<task_and_content>
<!-- Here is the task that is to be accomplished with the provided materials -->
<instructions>

</instructions>

<!-- Here is the error that was faced upon running the codebase -->
<errors>

</errors>
</task_and_content>

<custom_instructions>
# debugging_and_analysis_instructions

This mantra will guide our development journey:  
Simplicity's bloom  
Intuition's guiding light  
Robust, timeless code

When addressing complex issues or when explicitly requested:

## meta_thinking_approach

Step back and analyze:

- Identify the immediate problem and its direct cause.
- Then, search beyond this surface level to uncover any underlying misunderstandings, inefficiencies, or subtly wrong upstream processes that cause or interact with the immediate error.
- Question if the current approach is the most effective way to achieve the goal.

Prioritize simplicity and maintainability:

- Always strive to minimize complexity in solutions.
- Maximize simplicity, robustness, and long-term maintainability.
- Consider how easily the solution can be understood and modified in the future.

Proactively improve:

- Don't just patch problems; address fundamental issues.
- Suggest refactoring or restructuring if it leads to a cleaner, more maintainable solution.
- Propose alternative approaches that might better align with best practices or  elegant design patterns.

Consider long-term implications:

- Evaluate how proposed solutions will scale and evolve with the project.
- Anticipate potential future issues or limitations.
- Ensure solutions contribute to the overall stability and flexibility of the system.

Balance immediate fixes with strategic improvements:

- While addressing the immediate issue, always consider opportunities for broader enhancements (particularly with regards to ruthlessly eliminating complexity while achieving project goals). These suggestions may be assertive, and Claude as the co-developer is allowed (and expected!) to actively suggest ideas that would improve the project along the three critical axes of complexity (minimize), intuitive structuring (maximize), and long-term maintainability (maximize).
- Propose incremental steps towards a more robust architecture if a complete overhaul isn't feasible.

The following principles may be considered where applicable to improve code quality, maintainability and scalability:


1. SOLID Principles:
   - Single Responsibility
   - Open/Closed
   - Liskov Substitution
   - Interface Segregation
   - Dependency Inversion
   Best for: Large, complex systems requiring maintainability and extensibility.

2. DRY (Don't Repeat Yourself):
   Best for: Reducing code duplication and improving maintainability.

3. KISS (Keep It Simple, Stupid):
   Best for: Ensuring code readability and reducing complexity.

4. Composition Over Inheritance:
   Best for: Flexible object design and avoiding deep inheritance hierarchies.

5. Zen of Python:
   Best for: Writing idiomatic Python code.

6. YAGNI (You Ain't Gonna Need It):
   Best for: Avoiding premature optimization and unnecessary features.

7. Separation of Concerns:
   Best for: Modular design and clear code organization.

When engaging in this meta-thinking process, print the tag **#MT!** when you explicitly intend to use these methods/principles to deeply analyze the problem and its context. This approach should be the default mode of operation, consistently questioning and improving the project's foundations without needing explicit prompting from the user. If the user determines that the spirit of this approach is not being adequately embodied, they will include the **#MT!** in their message tag to strongly nudge you in this direction and encourage you to take charge and be proactive for the good of the project.

## project_context_certainty

The full contents of the repository will generally be provided with markdown tags that helpfully show the project's structure:

- Be absolutely certain and decisive about what is and is not implemented in the project.
- Never use hedging language like "maybe", "might be", or "possibly" when referring to existing project structure, files, or implementations.
- If a file, function, or feature exists in the provided context, state it as a fact without any ambiguity.
- If something does not exist in the provided context, state clearly that it is not present or implemented.
- Do not suggest creating files or implementing features that already exist in the project.
- Always verify the project structure and contents before making any statements or suggestions about implementation.
- If you're unsure about something not explicitly shown in the context, clearly state that the information is not available in the provided context.

You will print the tag **#CC!** when thinking carefully about what does or does not exist in the project so that your suggestions are precise, intelligently aware of what is actually in the project, and non-redundant. If the user determines that you are not being rigorous and certain enough about the project context, they will print the **#CC!** tag to strongly encourage you to ground your reasoning and suggestions in what is actually implemented or not implemented in the project.

### project_structure
- Reflect on the project structure.
- Print ASCII representation of project structure if applicable.
- Identify and isolate project files relevant to the query or error.

### problem_statement
Clearly articulate the problem, including any error messages or unexpected behaviors.

### code_trace
Walk through the code execution path, step-by-step:

- Present relevant code snippets.
- Show corresponding output or log entries from the stack trace.
- Highlight discrepancies between expected and actual behavior at each step.
- Preserve all existing logging unless explicitly asked to remove it.

### data_flow_analysis
Trace the flow and transformation of data through the system:

- Identify input sources and initial data states.
- Track how data is modified at each step.
- Note any unexpected data states or transformations.
- Importantly print any important data or snippets from the stack trace. It helps to print out precisely what you are talking about so it makes sense to all.

### critical_point_identification
Pinpoint the exact line or function where behavior deviates from expected:

- Show the specific code and corresponding output.
- Explain why this point is critical to the problem.

### root_cause_analysis
Formulate and evaluate hypotheses about the root cause:

- Present evidence supporting each hypothesis.
- Eliminate unlikely causes based on the evidence.
- Identify the most probable root cause.
- Be as precise as possible in this identification. If applicable, point out the precise line(s) of code or function(s) that causes the problem and how they interact with the rest of the code.

### solution_proposal
Propose a detailed solution addressing the root cause:

- Provide modified code snippets.
- Explain how the changes resolve the issue.
- Discuss any potential side effects or considerations.
- Preserve all existing logging, comments, and unrelated code unless there's a specific reason to change them.
- When providing any code, especially full code, include the location of the file relative to the project root dir as the first comment in the code.

### verification_strategy
Suggest a strategy to verify the solution:

- Propose specific tests or checks.
- Outline expected outcomes that would confirm the fix.

For all tasks:

- Adapt the depth of analysis to the complexity of the problem.
- Ground all reasoning in specific details and observed behaviors.
- Prioritize addressing root causes over symptom management.
- Clearly state if more information is needed and why it's crucial.

When you see **#FC!** in any user message:

- Provide the complete, corrected code for all relevant files.
- Include all necessary imports and dependencies.
- Do not use abbreviations, placeholders, or truncations.
- Maintain and correct all existing comments, docstrings, and logging.
- If multiple files need changes, provide the full content of each file.
- Print **#FC!** right before you start writing any code to acknowledge.

## thinking_tag
Use this tag: <thinking></thinking> to decide on the approach you will take to solve a problem. Reflect on the complexity of the issue and the most appropriate strategy for analysis.

This systematic process is not limited to code and can be applied to any situation that would benefit from this kind of analysis or when the user and model seem to be hitting a wall in their discussion.

Always maintain a balance between thoroughness and relevance, focusing on the most critical aspects of the problem.

Example of the expected analysis process:

1. Analyze a piece of code and state its expected output.
2. Print and analyze the actual stack trace or output.
3. Compare expected and actual results, noting any discrepancies.

For instance:

### code_analysis
Let's examine the `get_data_with_history` function:

```python
def get_data_with_history(table_name):
    end_time = datetime.now(pytz.UTC)
    start_time = end_time - timedelta(days=HISTORICAL_DAYS)

    query = f"""
    SELECT *
    FROM `{PROJECT_ID}.weather.{table_name}`
    WHERE TIMESTAMP BETWEEN '{start_time}' AND '{end_time}'
    ORDER BY TIMESTAMP
    """

    logger.info(f"Executing query for {table_name}:
{query}")
    df = client.query(query).to_dataframe()
    return df
```

Expected behavior: This function should retrieve weather data for the specified table, logging the query and returning a DataFrame.

### trace_analysis
From the stack trace:

```
2024-07-06 15:15:55,042 - INFO - Executing query for current-weather-mesonet:
    SELECT *
    FROM `crop2cloud24.weather.current-weather-mesonet`
    WHERE TIMESTAMP BETWEEN '2024-06-06 20:15:55.042018+00:00' AND '2024-07-06 20:15:55.042018+00:00'
    ORDER BY TIMESTAMP

2024-07-06 15:15:58,879 - INFO - Raw data retrieved for current-weather-mesonet. Shape: (60212, 42)
```

The actual output matches the expected behavior. The query is logged correctly, and data is retrieved successfully.

### data_flow_analysis
Next, let's trace how this data is processed:

```python
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], utc=True, format='mixed')
df = df.dropna(subset=['TIMESTAMP'])
df = merge_duplicate_timestamps(df)
```

Expected behavior: This should convert timestamps to datetime objects, remove rows with null timestamps, and merge any duplicate timestamps.

From the stack trace:

```
2024-07-06 15:15:58,920 - INFO - Number of null timestamps after conversion: 0
2024-07-06 15:15:58,933 - INFO - Shape after dropping null timestamps: (60212, 42)
2024-07-06 15:15:59,004 - INFO - Merged 17280 duplicate timestamp rows. Final shape: (42932, 42)
```

The actual output aligns with expectations. Timestamps are processed correctly, and duplicate rows are merged.

Continue this process, analyzing each step of the data flow until you identify the point where behavior deviates from expected, then proceed with root cause analysis and solution proposal.

When following this flow there is no need to explicitly print the markdown tags. Write naturally.

The key emphasis here is the comparison between expected and actual behavior at each stage of the code execution.

## rigorous_debugging_approach

When standard debugging approaches fail to isolate the root cause of an issue, employ this exhaustive debugging strategy:

Use the tag **#RD!** to indicate the start of this rigorous debugging process. RD stands for Rigorous Debugging.

1. Implement extensive logging and print statements throughout the codebase.
2. Inspect and log the contents of relevant files.
3. Examine and log the types and contents of imported functions and modules.
4. Capture and log detailed tracebacks for any exceptions.
5. Trace the execution flow in minute detail.

Example of rigorous debugging implementation:

```python
#RD!
import logging
import inspect
import traceback
import sys
import os

# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def print_file_contents(filename):
    try:
        with open(filename, 'r') as file:
            logger.debug(f"Contents of {filename}:")
            logger.debug(file.read())
    except Exception as e:
        logger.error(f"Error reading file {filename}: {str(e)}")

def inspect_object(obj, name):
    logger.debug(f"Inspecting object: {name}")
    logger.debug(f"Type: {type(obj)}")
    logger.debug(f"Dir: {dir(obj)}")
    if inspect.isfunction(obj) or inspect.ismethod(obj):
        logger.debug(f"Function signature: {inspect.signature(obj)}")
        logger.debug(f"Function source:
{inspect.getsource(obj)}")

def debug_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger.debug(f"Entering {func.__name__}")
        logger.debug(f"Args: {args}")
        logger.debug(f"Kwargs: {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"Exiting {func.__name__}. Result: {result}")
            return result
        except Exception as e:
            logger.error(f"Exception in {func.__name__}: {str(e)}")
            logger.error("Detailed traceback:")
            logger.error(traceback.format_exc())
            raise
    return wrapper

# Inspect imported modules and functions
logger.debug("Inspecting imported modules and functions:")
for name, obj in globals().items():
    if not name.startswith("__"):
        inspect_object(obj, name)

# Print contents of relevant files
print_file_contents("src/automated_reachouts/campaign_manager.py")
print_file_contents("src/automated_reachouts/s5_send_emails.py")

@debug_decorator
def problematic_function(param1, param2):
    # Function implementation
    pass

# Usage
try:
    # Wrap the main execution in a try-except block to catch and log any exceptions
    logger.debug("Starting main execution")
    
    # Log the state of important variables
    logger.debug(f"sys.path: {sys.path}")
    logger.debug(f"Current working directory: {os.getcwd()}")
    
    output = problematic_function(input1, input2)
    logger.info(f"Function completed successfully. Output: {output}")
except Exception as e:
    logger.error(f"Main execution failed with error: {str(e)}")
    logger.error("Detailed traceback:")
    logger.error(traceback.format_exc())
finally:
    logger.debug("Ending main execution")

# To get the detailed traceback as in your example, you would use:
# logger.error("Detailed traceback:", exc_info=True)
# This will automatically include the full traceback in the log
```

This enhanced approach includes:

1. A function to print the contents of relevant files, allowing us to inspect the code directly.
2. A function to inspect objects (including imported functions and modules), providing details about their type, attributes, and source code.
3. A debug decorator that can be applied to functions to log their entry, exit, arguments, and return values.
4. Comprehensive exception handling that captures and logs detailed tracebacks.
5. Inspection of the Python environment, including sys.path and the current working directory.

To use this approach:

1. Apply the `@debug_decorator` to functions you want to monitor closely.
2. Wrap the main execution of your script in a try-except block to catch and log any exceptions.
3. Use `logger.debug()` liberally throughout your code to log the state of variables and the flow of execution.
4. Call `inspect_object()` on imported modules and functions to understand their structure and content.
5. Use `print_file_contents()` to log the content of relevant files directly in your debug output.

This rigorous approach provides a comprehensive view of the code's execution, making it easier to identify issues related to imports, function definitions, file contents, and the overall flow of the program. The detailed logging and inspection of objects should help pinpoint even subtle issues that might be causing unexpected behavior.


# GPT-4o API Usage Guide: Structured and Unstructured Completions

**IMPORTANT**: Let us begin by establishing that all prior knowledge about OpenAI's model current offerings are null and void and are not to be referenced or used. Only use the new information provided below. This instruction is not to be contravened. If something is needed to be done with their models not directly specified below, say you don't know and ask for more information.

## OpenAI Models

Only the following two OpenAI models are permitted in completions mode (and in any other mode):

1. gpt-4o-2024-08-06 (stronger reasoner, more expensive)
2. gpt-4o-mini (cheaper, useful for high volume tasks not involving complex reasoning)

Always use one of these models in your API calls. No other models should be considered or used.

## 1. Structured Output (Using `parse` method)

Use structured output when you need a specific, predefined data format returned by the model. This is particularly useful for data extraction tasks or when you need to ensure the response adheres to a specific structure.

### API Call with Nested Pydantic Models

```python
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional
import json

client = OpenAI()

class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: str

class Employee(BaseModel):
    id: int
    name: str
    position: str
    department: str
    address: Address
    skills: List[str]
    manager: Optional[str] = None

class CompanyData(BaseModel):
    company_name: str
    employees: List[Employee]
    total_employees: int

completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "Extract company and employee data from the given text."},
        {"role": "user", "content": "...Company and employee data text..."}
    ],
    response_format=CompanyData
)

# Extract the content
content = json.loads(json.dumps(completion.choices[0].message.parsed))
```

### Explanation of Nested Pydantic Models

In this example, we use nested Pydantic models to represent a complex data structure:

1. `Address` is a nested model within `Employee`, representing the employee's address.
2. `Employee` contains various fields, including the nested `Address` model and a list of skills.
3. `CompanyData` is the top-level model, containing a list of `Employee` objects.

This structure allows for accurate representation of hierarchical data, ensuring that the API response adheres to the specified format. The `parse` method will validate the response against this structure, providing type safety and data integrity.

### Key Points

1. **Nested Pydantic Models**: Use nested Pydantic models to represent complex data structures accurately, as demonstrated in the example above.

2. **Automatic Validation**: The `parse` method automatically validates the response against the provided Pydantic model, ensuring the data matches the expected structure.

3. **Content Extraction**: Use `json.loads(json.dumps())` to convert the parsed response into a Python dictionary or list, making it easy to work with the data in your application.

## 2. Unstructured Output (Using `create` method)

Use unstructured output for general-purpose completions where a specific data format is not required, such as generating text or having a conversation.

### API Call

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the capital of France?"}
    ]
)

# Extract the content
content = response.choices[0].message.content
```

### Key Points

1. **Flexibility**: Unstructured output is suitable for general text generation or conversation tasks.

2. **Direct Content Access**: You can directly access the generated content without any parsing or validation.

3. **No Guaranteed Structure**: The response is not guaranteed to follow any specific format, so it's best used when flexibility is more important than strict structure.

## When to Use Each Method

- **Use Structured Output (`parse`) when:**
  - You need data in a specific format (e.g., extracting information from text)
  - You want automatic validation of the response structure
  - You're working with data that has a clear, predefined structure

- **Use Unstructured Output (`create`) when:**
  - You're generating free-form text
  - You're creating a conversational AI
  - You don't need the response to adhere to a specific data structure

## Important Notes

1. Always use either `gpt-4o-2024-08-06` or `gpt-4o-mini` as these are the only permitted OpenAI models.

2. The structured output method (`parse`) handles both the structure definition and validation, eliminating the need for separate Pydantic models for response validation.

3. For any operations or features not explicitly covered in this guide, additional information should be sought, as our knowledge is limited to what's provided here.
</custom_instructions>

<repository_structure>
    <timestamp>20240918_155837</timestamp>
<directory name="Automated_Lit_Revs">
    <file>
        <name>.dockerignore</name>
        <path>.dockerignore</path>
        <content>
**/__pycache__
**/.venv
**/.classpath
**/.dockerignore
**/.env
**/.git
**/.gitignore
**/.project
**/.settings
**/.toolstarget
**/.vs
**/.vscode
**/*.*proj.user
**/*.dbmdl
**/*.jfm
**/bin
**/charts
**/docker-compose*
**/compose*
**/Dockerfile*
**/node_modules
**/npm-debug.log
**/obj
**/secrets.dev.yaml
**/values.dev.yaml
LICENSE
README.md

        </content>
    </file>
    <file>
        <name>.env</name>
        <path>.env</path>
        <content>
OPENAI_API_KEY=********
CORE_API_KEY=********
ENVIRONMENT=********



        </content>
    </file>
    <file>
        <name>.gitignore</name>
        <path>.gitignore</path>
        <content>
__pycache__/
*.pyc
.env
venv/
.venv
        </content>
    </file>
    <file>
        <name>analyze_papers.py</name>
        <path>analyze_papers.py</path>
        <content>
from misc_utils import get_api_keys
import asyncio
import re
from llm_api_handler import LLM_APIHandler
from prompts import get_prompt
import aiohttp
import json

from logger_config import get_logger

logger = get_logger(__name__)


class PaperRanker:
    def __init__(self, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)
        self.max_retries = max_retries

    async def process_query(self, query_key, query_data, point_context):
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                template_name="rank_papers",
                full_text=query_data.get("full_text", ""),
                point_context=point_context,
                query_rationale=query_data.get("query_rationale", ""),
            )
            try:
                print(f"Processing queries for {point_context}...")
                response = await self.llm_api_handler.generate_openai_content(prompt)
                print(f"Response: {response}")
                if response is None:
                    logger.warning(
                        "Received None response from the Gemini API. Skipping query."
                    )
                    return None

                try:
                    # Extract the relevance score using the specified token format
                    relevance_score_match = re.search(
                        r"<<relevance>>(\d+\.\d+)<<relevance>>",
                        response,
                    )

                    if relevance_score_match:
                        relevance_score_str = relevance_score_match.group(1)
                        try:
                            relevance_score = float(relevance_score_str)
                            if relevance_score > 0.5:
                                logger.debug(f"Successfully processed query.")
                                return {
                                    "DOI": query_data.get("DOI", ""),
                                    "title": query_data.get("title", ""),
                                    "analysis": response,
                                    "relevance_score": relevance_score,
                                }
                            else:
                                logger.debug(
                                    f"Relevance score {relevance_score} is below the threshold. Skipping query."
                                )
                                return None
                        except ValueError:
                            logger.warning(
                                f"Extracted relevance score '{relevance_score_str}' is not a valid float. Retrying..."
                            )
                            retry_count += 1
                    else:
                        logger.warning(
                            f"No relevance score found between <|relevance|> tokens in the response for query {query_key}. Response: {response}"
                        )
                        retry_count += 1
                except Exception as e:
                    logger.warning(
                        f"Error extracting relevance score for query {query_key}: {str(e)}. Retrying..."
                    )
                    retry_count += 1
            except Exception as e:
                logger.exception(f"Error processing query {query_key}: {str(e)}")
                retry_count += 1

        logger.error(f"Max retries reached for query {query_key}. Skipping query.")
        return None

    async def process_queries(self, input_json, point_context):
        tasks = []
        for query_key, query_data in input_json.items():
            task = asyncio.create_task(
                self.process_query(query_key, query_data, point_context)
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        output_json = {}
        for query_key, result in zip(input_json.keys(), results):
            if result and isinstance(result, dict):
                output_json[query_key] = result

        return output_json


async def main(input_json, point_context):
    async with aiohttp.ClientSession() as session:
        ranker = PaperRanker(session)
        logger.info("Starting paper ranking process...")
        output_json = await ranker.process_queries(input_json, point_context)
        logger.info("Paper ranking process completed.")
        return output_json


if __name__ == "__main__":
    input_json = {
        "query_1": {
            "DOI": "https://doi.org/10.1007/bf00281114",
            "authors": ["Jarrett Rj"],
            "citation_count": 156,
            "journal": "Diabetologia",
            "pdf_link": "https://link.springer.com/content/pdf/10.1007%2FBF00281114.pdf",
            "publication_year": 1984,
            "title": "Type 2 (non-insulin-dependent) diabetes mellitus and coronary heart disease ? chicken, egg or neither?",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query aims to understand the relationship between Type 2 diabetes and coronary heart disease in chickens.",
        },
        "query_2": {
            "DOI": "https://doi.org/10.1001/jamainternmed.2019.6969",
            "authors": [
                "Victor W. Zhong",
                "Linda Van Horn",
                "Philip Greenland",
                "Mercedes R. Carnethon",
                "Hongyan Ning",
                "John T. Wilkins",
                "Donald M. Lloyd‐Jones",
                "Norrina B. Allen",
            ],
            "citation_count": 221,
            "journal": "JAMA internal medicine",
            "pdf_link": "https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf",
            "publication_year": 2020,
            "title": "Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query investigates the associations between different types of meat intake, including poultry, and cardiovascular disease and mortality.",
        },
        "query_3": {
            "DOI": "https://doi.org/10.1016/s0034-5288(18)33737-8",
            "authors": ["S.F. Cueva", "H. Sillau", "Abel Valenzuela", "H. P. Ploog"],
            "citation_count": 181,
            "journal": "Research in Veterinary Science/Research in veterinary science",
            "publication_year": 1974,
            "title": "High Altitude Induced Pulmonary Hypertension and Right Heart Failure in Broiler Chickens",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query focuses on the effects of high altitude on pulmonary hypertension and right heart failure specifically in broiler chickens.",
        },
        "query_4": {
            "DOI": "https://doi.org/10.2307/1588087",
            "authors": ["Sherwin A. Hall", "Nicanor Machicao"],
            "citation_count": 58,
            "journal": "Avian diseases",
            "publication_year": 1968,
            "title": "Myocarditis in Broiler Chickens Reared at High Altitude",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query examines myocarditis in broiler chickens reared at high altitude, providing insight into heart disease in this specific context.",
        },
        "query_5": {
            "DOI": "https://doi.org/10.1038/srep14727",
            "authors": [
                "Huaguang Lu",
                "Yi Tang",
                "Patricia A. Dunn",
                "Eva Wallner-Pendleton",
                "Lin Lin",
                "Eric A. Knoll",
            ],
            "citation_count": 82,
            "journal": "Scientific reports",
            "pdf_link": "https://www.nature.com/articles/srep14727.pdf",
            "publication_year": 2015,
            "title": "Isolation and molecular characterization of newly emerging avian reovirus variants and novel strains in Pennsylvania, USA, 2011–2014",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query looks at the isolation and characterization of new avian reovirus variants and strains, which may have implications for chicken health.",
        },
    }
    point_context = "Heart disease in chickens."

    output_json = asyncio.run(main(input_json, point_context))
    print(json.dumps(output_json, indent=2))

        </content>
    </file>
    <file>
        <name>app.py</name>
        <path>app.py</path>
        <content>
import sys
import os
import aiohttp
from fastapi import FastAPI, Form, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
import logging
import pkg_resources

sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from get_search_queries import QueryGenerator
from analyze_papers import PaperRanker
from synthesize_results import QueryProcessor
from core_search import CORESearch
from misc_utils import get_api_keys

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def log_all_versions():
    installed_packages = pkg_resources.working_set
    installed_packages_list = sorted([f"{i.key}=={i.version}" for i in installed_packages])
    logging.info("Installed packages:")
    for package in installed_packages_list:
        logging.info(package)

# Call this function at the start of your application
log_all_versions()

app = FastAPI()

# Allow CORS for development
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)


class ResearchQueryProcessor:
    def __init__(self):
        self.api_keys = get_api_keys(source="local" if os.getenv("ENVIRONMENT") == "local" else "env")

    async def chatbot_response(self, message: str) -> str:
        async with aiohttp.ClientSession() as session:
            logger.info("Generating search queries...")
            query_generator = QueryGenerator(session)
            search_queries = await query_generator.generate_queries(message)

            logger.info("Searching in CORE...")
            core_search = CORESearch(max_results=5)
            search_results = await core_search.search_and_parse_json(search_queries)

            logger.info("Analyzing papers...")
            paper_ranker = PaperRanker(session)
            analyzed_papers = await paper_ranker.process_queries(
                search_results, message
            )

            logger.info("Synthesizing results...")
            query_processor = QueryProcessor(session)
            synthesized_results = await query_processor.process_query(
                message, analyzed_papers
            )

            final_response = f"{synthesized_results}"
            logger.info(f"Final response: {final_response}")
            return final_response


processor = ResearchQueryProcessor()


@app.get("/", response_class=HTMLResponse)
async def get_root():
    html_content = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>AI-Powered Literature Review Assistant</title>
        <style>
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                background-color: #f0f4f8;
                margin: 0;
                padding: 0;
                display: flex;
                justify-content: center;
                min-height: 100vh;
                font-size: 16px;
            }
            .container {
                background-color: white;
                border-radius: 10px;
                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                padding: 1.5rem;
                margin: 1rem;
                max-width: 800px;
                width: 100%;
            }
            h1 {
                color: #2c3e50;
                text-align: center;
                margin-bottom: 0.5rem;
                font-size: 1.8em;
            }
            .description {
                color: #34495e;
                text-align: center;
                margin-bottom: 1rem;
            }
            textarea {
                width: 100%;
                padding: 0.5rem;
                margin-bottom: 0.5rem;
                border: 1px solid #bdc3c7;
                border-radius: 5px;
                resize: vertical;
                font-size: 1em;
            }
            button {
                background-color: #3498db;
                color: white;
                border: none;
                padding: 0.5rem 1rem;
                border-radius: 5px;
                cursor: pointer;
                font-size: 1em;
                transition: background-color 0.3s;
            }
            button:hover {
                background-color: #2980b9;
            }
            #result {
                margin-top: 1rem;
                white-space: pre-wrap;
                background-color: #ecf0f1;
                padding: 1rem;
                border-radius: 5px;
                display: none;
                font-size: 1em;
            }
            .loader {
                text-align: center;
                margin: 20px auto;
                display: none;
            }
            .loader-text {
                margin-bottom: 10px;
            }
            .spinner {
                border: 5px solid #f3f3f3;
                border-top: 5px solid #3498db;
                border-radius: 50%;
                width: 40px;
                height: 40px;
                animation: spin 1s linear infinite;
                margin: 0 auto;
            }
            @keyframes spin {
                0% { transform: rotate(0deg); }
                100% { transform: rotate(360deg); }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>AI-Powered Literature Review Assistant</h1>
            <p class="description">
                Welcome to our advanced research tool! Here's how it works:
                <br>
                1. Enter your research query in the box below.
                <br>
                2. Our AI will search the CORE academic database (currently limited to open access papers).
                <br>
                3. You'll receive a synthesized answer based on the most relevant peer-reviewed research.
            </p>
            <form id="queryForm">
                <textarea id="queryText" rows="4" placeholder="Enter your research query here (e.g., 'What are the latest treatments for heart disease?')" required></textarea>
                <button type="button" onclick="submitQuery()">Submit Query</button>
            </form>
            <div class="loader" id="loader">
                <p class="loader-text" id="loaderText">Searching database...</p>
                <div class="spinner"></div>
            </div>
            <div id="result"></div>
        </div>
        <script>
            async function submitQuery() {
                const query = document.getElementById('queryText').value;
                document.getElementById('result').style.display = 'none';
                document.getElementById('loader').style.display = 'block';
                
                const loadingSteps = [
                    "Searching database...",
                    "Analyzing papers...",
                    "Synthesizing results..."
                ];
                let currentStep = 0;
                
                const loadingInterval = setInterval(() => {
                    document.getElementById('loaderText').innerText = loadingSteps[currentStep];
                    currentStep = (currentStep + 1) % loadingSteps.length;
                }, 3000);
                
                try {
                    const response = await fetch('/process_query/', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/x-www-form-urlencoded'
                        },
                        body: 'query=' + encodeURIComponent(query)
                    });
                    const result = await response.json();
                    clearInterval(loadingInterval);
                    document.getElementById('loader').style.display = 'none';
                    document.getElementById('result').style.display = 'block';
                    document.getElementById('result').innerHTML = result.result;
                } catch (error) {
                    clearInterval(loadingInterval);
                    document.getElementById('loader').style.display = 'none';
                    document.getElementById('result').style.display = 'block';
                    document.getElementById('result').innerHTML = "An error occurred. Please try again.";
                }
            }
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)


@app.post("/process_query/")
async def process_query(query: str = Form(...)):
    try:
        logger.info(f"Received query: {query}")
        result = await processor.chatbot_response(query)
        logger.info(f"Processed query successfully")
        return {"result": result}
    except Exception as e:
        logger.exception(f"Error processing query: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal Server Error")

        </content>
    </file>
    <file>
        <name>core_search.py</name>
        <path>core_search.py</path>
        <content>
import aiohttp
import asyncio
import json
from misc_utils import get_api_keys

from logger_config import get_logger

logger = get_logger(__name__)


class CORESearch:
    def __init__(self, max_results):
        self.api_keys = get_api_keys()
        self.base_url = "https://api.core.ac.uk/v3"
        self.max_results = max_results
        self.core_api_key = self.api_keys["CORE_API_KEY"]

    async def search(self, search_query):
        headers = {
            "Authorization": f"Bearer {self.core_api_key}",
            "Accept": "application/json",
        }

        params = {
            "q": search_query,
            "limit": self.max_results,
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/search/works", headers=headers, json=params
            ) as response:
                if response.status == 200:
                    logger.info("CORE API request successful.")
                    response_json = await response.json()
                    return response_json
                else:
                    logger.warning(
                        f"CORE API request failed with status code: {response.status}"
                    )
                    return None

    async def search_and_parse(self, query):
        try:
            search_query = query["search_query"]
            response = await self.search(search_query)

            if response is None:
                logger.warning(f"Empty API response for query: {search_query}")
                return {}

            results = response.get("results", [])
            parsed_result = {}

            if results:
                print(results)
                entry = results[0]
                parsed_result = {
                    "DOI": entry.get("doi", ""),
                    "authors": [author["name"] for author in entry.get("authors", [])],
                    "citation_count": entry.get("citationCount", 0),
                    "journal": entry.get("publisher", ""),
                    "pdf_link": entry.get("downloadUrl", ""),
                    "publication_year": entry.get("publicationYear"),
                    "title": entry.get("title", ""),
                    "full_text": entry.get("fullText", ""),
                    "query_rationale": query["query_rationale"],
                }

            return parsed_result
        except Exception as e:
            logger.error(
                f"An error occurred while searching and parsing results for query: {search_query}. Error: {e}"
            )
            return {}

    async def search_and_parse_json(self, input_json):
        try:
            updated_json = {}
            for query_id, query in input_json.items():
                parsed_result = await self.search_and_parse(query)
                updated_json[query_id] = parsed_result
            return updated_json
        except Exception as e:
            logger.error(
                f"An error occurred while processing the input JSON. Error: {e}"
            )
            return {}


async def main():
    max_results = 1

    core_search = CORESearch(max_results)

    input_json = {
        "query_1": {
            "search_query": "climate change, water resources",
            "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic.",
        },
        "query_2": {
            "search_query": "water scarcity, (hydrologist OR water expert)",
            "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources.",
        },
        "query_3": {
            "search_query": "sea level rise, coastal erosion",
            "query_rationale": "This query is crucial to understand the impact of climate change on coastal regions and the resulting effects on global water resources.",
        },
        "query_4": {
            "search_query": "water conservation, climate change mitigation, environmental studies",
            "query_rationale": "This query is important to identify strategies for water conservation and their role in mitigating the effects of climate change on global water resources.",
        },
        "query_5": {
            "search_query": "glacier melting, cryosphere",
            "query_rationale": "This query is necessary to understand the impact of climate change on glaciers and the resulting effects on global water resources.",
        },
    }

    updated_json = await core_search.search_and_parse_json(input_json)

    # Remove the "full_text" key from each query result
    for query_result in updated_json.values():
        query_result.pop("full_text", None)

    print(json.dumps(updated_json, indent=2))


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
    <file>
        <name>Dockerfile</name>
        <path>Dockerfile</path>
        <content>
   # Use an official Python runtime as a parent image
   FROM python:3.9-slim

   # Set environment variables
   ENV PYTHONUNBUFFERED=1 \
       PYTHONDONTWRITEBYTECODE=1 \
       PIP_NO_CACHE_DIR=off \
       PIP_DISABLE_PIP_VERSION_CHECK=on \
       PIP_DEFAULT_TIMEOUT=100 \
       POETRY_VERSION=1.8.3 \
       POETRY_HOME="/opt/poetry" \
       POETRY_VIRTUALENVS_IN_PROJECT=true \
       POETRY_NO_INTERACTION=1 \
       PYSETUP_PATH="/opt/pysetup" \
       VENV_PATH="/opt/pysetup/.venv"

   # prepend poetry and venv to path
   ENV PATH="$POETRY_HOME/bin:$VENV_PATH/bin:$PATH"

   # Install poetry
   RUN pip install "poetry==$POETRY_VERSION"

   # Set the working directory in the container
   WORKDIR $PYSETUP_PATH

   # Copy only requirements to cache them in docker layer
   COPY pyproject.toml poetry.lock* ./

   # Install dependencies
   RUN poetry install --no-root

   # Copy the current directory contents into the container
   COPY . .

   # Make port 8080 available to the world outside this container
   EXPOSE 8080

   # Run app.py when the container launches
   CMD ["poetry", "run", "gunicorn", "--workers", "2", "--worker-class", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8080", "--timeout", "300", "app:app"]
        </content>
    </file>
    <file>
        <name>get_repo_context.py</name>
        <path>get_repo_context.py</path>
        <content>
import os
import datetime
from pathlib import Path

# Global variables with docstrings
CUSTOM_INSTRUCTIONS = """
# debugging_and_analysis_instructions

This mantra will guide our development journey:  
Simplicity's bloom  
Intuition's guiding light  
Robust, timeless code

When addressing complex issues or when explicitly requested:

## meta_thinking_approach

Step back and analyze:

- Identify the immediate problem and its direct cause.
- Then, search beyond this surface level to uncover any underlying misunderstandings, inefficiencies, or subtly wrong upstream processes that cause or interact with the immediate error.
- Question if the current approach is the most effective way to achieve the goal.

Prioritize simplicity and maintainability:

- Always strive to minimize complexity in solutions.
- Maximize simplicity, robustness, and long-term maintainability.
- Consider how easily the solution can be understood and modified in the future.

Proactively improve:

- Don't just patch problems; address fundamental issues.
- Suggest refactoring or restructuring if it leads to a cleaner, more maintainable solution.
- Propose alternative approaches that might better align with best practices or  elegant design patterns.

Consider long-term implications:

- Evaluate how proposed solutions will scale and evolve with the project.
- Anticipate potential future issues or limitations.
- Ensure solutions contribute to the overall stability and flexibility of the system.

Balance immediate fixes with strategic improvements:

- While addressing the immediate issue, always consider opportunities for broader enhancements (particularly with regards to ruthlessly eliminating complexity while achieving project goals). These suggestions may be assertive, and Claude as the co-developer is allowed (and expected!) to actively suggest ideas that would improve the project along the three critical axes of complexity (minimize), intuitive structuring (maximize), and long-term maintainability (maximize).
- Propose incremental steps towards a more robust architecture if a complete overhaul isn't feasible.

The following principles may be considered where applicable to improve code quality, maintainability and scalability:


1. SOLID Principles:
   - Single Responsibility
   - Open/Closed
   - Liskov Substitution
   - Interface Segregation
   - Dependency Inversion
   Best for: Large, complex systems requiring maintainability and extensibility.

2. DRY (Don't Repeat Yourself):
   Best for: Reducing code duplication and improving maintainability.

3. KISS (Keep It Simple, Stupid):
   Best for: Ensuring code readability and reducing complexity.

4. Composition Over Inheritance:
   Best for: Flexible object design and avoiding deep inheritance hierarchies.

5. Zen of Python:
   Best for: Writing idiomatic Python code.

6. YAGNI (You Ain't Gonna Need It):
   Best for: Avoiding premature optimization and unnecessary features.

7. Separation of Concerns:
   Best for: Modular design and clear code organization.

When engaging in this meta-thinking process, print the tag **#MT!** when you explicitly intend to use these methods/principles to deeply analyze the problem and its context. This approach should be the default mode of operation, consistently questioning and improving the project's foundations without needing explicit prompting from the user. If the user determines that the spirit of this approach is not being adequately embodied, they will include the **#MT!** in their message tag to strongly nudge you in this direction and encourage you to take charge and be proactive for the good of the project.

## project_context_certainty

The full contents of the repository will generally be provided with markdown tags that helpfully show the project's structure:

- Be absolutely certain and decisive about what is and is not implemented in the project.
- Never use hedging language like "maybe", "might be", or "possibly" when referring to existing project structure, files, or implementations.
- If a file, function, or feature exists in the provided context, state it as a fact without any ambiguity.
- If something does not exist in the provided context, state clearly that it is not present or implemented.
- Do not suggest creating files or implementing features that already exist in the project.
- Always verify the project structure and contents before making any statements or suggestions about implementation.
- If you're unsure about something not explicitly shown in the context, clearly state that the information is not available in the provided context.

You will print the tag **#CC!** when thinking carefully about what does or does not exist in the project so that your suggestions are precise, intelligently aware of what is actually in the project, and non-redundant. If the user determines that you are not being rigorous and certain enough about the project context, they will print the **#CC!** tag to strongly encourage you to ground your reasoning and suggestions in what is actually implemented or not implemented in the project.

### project_structure
- Reflect on the project structure.
- Print ASCII representation of project structure if applicable.
- Identify and isolate project files relevant to the query or error.

### problem_statement
Clearly articulate the problem, including any error messages or unexpected behaviors.

### code_trace
Walk through the code execution path, step-by-step:

- Present relevant code snippets.
- Show corresponding output or log entries from the stack trace.
- Highlight discrepancies between expected and actual behavior at each step.
- Preserve all existing logging unless explicitly asked to remove it.

### data_flow_analysis
Trace the flow and transformation of data through the system:

- Identify input sources and initial data states.
- Track how data is modified at each step.
- Note any unexpected data states or transformations.
- Importantly print any important data or snippets from the stack trace. It helps to print out precisely what you are talking about so it makes sense to all.

### critical_point_identification
Pinpoint the exact line or function where behavior deviates from expected:

- Show the specific code and corresponding output.
- Explain why this point is critical to the problem.

### root_cause_analysis
Formulate and evaluate hypotheses about the root cause:

- Present evidence supporting each hypothesis.
- Eliminate unlikely causes based on the evidence.
- Identify the most probable root cause.
- Be as precise as possible in this identification. If applicable, point out the precise line(s) of code or function(s) that causes the problem and how they interact with the rest of the code.

### solution_proposal
Propose a detailed solution addressing the root cause:

- Provide modified code snippets.
- Explain how the changes resolve the issue.
- Discuss any potential side effects or considerations.
- Preserve all existing logging, comments, and unrelated code unless there's a specific reason to change them.
- When providing any code, especially full code, include the location of the file relative to the project root dir as the first comment in the code.

### verification_strategy
Suggest a strategy to verify the solution:

- Propose specific tests or checks.
- Outline expected outcomes that would confirm the fix.

For all tasks:

- Adapt the depth of analysis to the complexity of the problem.
- Ground all reasoning in specific details and observed behaviors.
- Prioritize addressing root causes over symptom management.
- Clearly state if more information is needed and why it's crucial.

When you see **#FC!** in any user message:

- Provide the complete, corrected code for all relevant files.
- Include all necessary imports and dependencies.
- Do not use abbreviations, placeholders, or truncations.
- Maintain and correct all existing comments, docstrings, and logging.
- If multiple files need changes, provide the full content of each file.
- Print **#FC!** right before you start writing any code to acknowledge.

## thinking_tag
Use this tag: <thinking></thinking> to decide on the approach you will take to solve a problem. Reflect on the complexity of the issue and the most appropriate strategy for analysis.

This systematic process is not limited to code and can be applied to any situation that would benefit from this kind of analysis or when the user and model seem to be hitting a wall in their discussion.

Always maintain a balance between thoroughness and relevance, focusing on the most critical aspects of the problem.

Example of the expected analysis process:

1. Analyze a piece of code and state its expected output.
2. Print and analyze the actual stack trace or output.
3. Compare expected and actual results, noting any discrepancies.

For instance:

### code_analysis
Let's examine the `get_data_with_history` function:

```python
def get_data_with_history(table_name):
    end_time = datetime.now(pytz.UTC)
    start_time = end_time - timedelta(days=HISTORICAL_DAYS)

    query = f\"""
    SELECT *
    FROM `{PROJECT_ID}.weather.{table_name}`
    WHERE TIMESTAMP BETWEEN '{start_time}' AND '{end_time}'
    ORDER BY TIMESTAMP
    \"""

    logger.info(f"Executing query for {table_name}:
{query}")
    df = client.query(query).to_dataframe()
    return df
```

Expected behavior: This function should retrieve weather data for the specified table, logging the query and returning a DataFrame.

### trace_analysis
From the stack trace:

```
2024-07-06 15:15:55,042 - INFO - Executing query for current-weather-mesonet:
    SELECT *
    FROM `crop2cloud24.weather.current-weather-mesonet`
    WHERE TIMESTAMP BETWEEN '2024-06-06 20:15:55.042018+00:00' AND '2024-07-06 20:15:55.042018+00:00'
    ORDER BY TIMESTAMP

2024-07-06 15:15:58,879 - INFO - Raw data retrieved for current-weather-mesonet. Shape: (60212, 42)
```

The actual output matches the expected behavior. The query is logged correctly, and data is retrieved successfully.

### data_flow_analysis
Next, let's trace how this data is processed:

```python
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], utc=True, format='mixed')
df = df.dropna(subset=['TIMESTAMP'])
df = merge_duplicate_timestamps(df)
```

Expected behavior: This should convert timestamps to datetime objects, remove rows with null timestamps, and merge any duplicate timestamps.

From the stack trace:

```
2024-07-06 15:15:58,920 - INFO - Number of null timestamps after conversion: 0
2024-07-06 15:15:58,933 - INFO - Shape after dropping null timestamps: (60212, 42)
2024-07-06 15:15:59,004 - INFO - Merged 17280 duplicate timestamp rows. Final shape: (42932, 42)
```

The actual output aligns with expectations. Timestamps are processed correctly, and duplicate rows are merged.

Continue this process, analyzing each step of the data flow until you identify the point where behavior deviates from expected, then proceed with root cause analysis and solution proposal.

When following this flow there is no need to explicitly print the markdown tags. Write naturally.

The key emphasis here is the comparison between expected and actual behavior at each stage of the code execution.

## rigorous_debugging_approach

When standard debugging approaches fail to isolate the root cause of an issue, employ this exhaustive debugging strategy:

Use the tag **#RD!** to indicate the start of this rigorous debugging process. RD stands for Rigorous Debugging.

1. Implement extensive logging and print statements throughout the codebase.
2. Inspect and log the contents of relevant files.
3. Examine and log the types and contents of imported functions and modules.
4. Capture and log detailed tracebacks for any exceptions.
5. Trace the execution flow in minute detail.

Example of rigorous debugging implementation:

```python
#RD!
import logging
import inspect
import traceback
import sys
import os

# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def print_file_contents(filename):
    try:
        with open(filename, 'r') as file:
            logger.debug(f"Contents of {filename}:")
            logger.debug(file.read())
    except Exception as e:
        logger.error(f"Error reading file {filename}: {str(e)}")

def inspect_object(obj, name):
    logger.debug(f"Inspecting object: {name}")
    logger.debug(f"Type: {type(obj)}")
    logger.debug(f"Dir: {dir(obj)}")
    if inspect.isfunction(obj) or inspect.ismethod(obj):
        logger.debug(f"Function signature: {inspect.signature(obj)}")
        logger.debug(f"Function source:
{inspect.getsource(obj)}")

def debug_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger.debug(f"Entering {func.__name__}")
        logger.debug(f"Args: {args}")
        logger.debug(f"Kwargs: {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"Exiting {func.__name__}. Result: {result}")
            return result
        except Exception as e:
            logger.error(f"Exception in {func.__name__}: {str(e)}")
            logger.error("Detailed traceback:")
            logger.error(traceback.format_exc())
            raise
    return wrapper

# Inspect imported modules and functions
logger.debug("Inspecting imported modules and functions:")
for name, obj in globals().items():
    if not name.startswith("__"):
        inspect_object(obj, name)

# Print contents of relevant files
print_file_contents("src/automated_reachouts/campaign_manager.py")
print_file_contents("src/automated_reachouts/s5_send_emails.py")

@debug_decorator
def problematic_function(param1, param2):
    # Function implementation
    pass

# Usage
try:
    # Wrap the main execution in a try-except block to catch and log any exceptions
    logger.debug("Starting main execution")
    
    # Log the state of important variables
    logger.debug(f"sys.path: {sys.path}")
    logger.debug(f"Current working directory: {os.getcwd()}")
    
    output = problematic_function(input1, input2)
    logger.info(f"Function completed successfully. Output: {output}")
except Exception as e:
    logger.error(f"Main execution failed with error: {str(e)}")
    logger.error("Detailed traceback:")
    logger.error(traceback.format_exc())
finally:
    logger.debug("Ending main execution")

# To get the detailed traceback as in your example, you would use:
# logger.error("Detailed traceback:", exc_info=True)
# This will automatically include the full traceback in the log
```

This enhanced approach includes:

1. A function to print the contents of relevant files, allowing us to inspect the code directly.
2. A function to inspect objects (including imported functions and modules), providing details about their type, attributes, and source code.
3. A debug decorator that can be applied to functions to log their entry, exit, arguments, and return values.
4. Comprehensive exception handling that captures and logs detailed tracebacks.
5. Inspection of the Python environment, including sys.path and the current working directory.

To use this approach:

1. Apply the `@debug_decorator` to functions you want to monitor closely.
2. Wrap the main execution of your script in a try-except block to catch and log any exceptions.
3. Use `logger.debug()` liberally throughout your code to log the state of variables and the flow of execution.
4. Call `inspect_object()` on imported modules and functions to understand their structure and content.
5. Use `print_file_contents()` to log the content of relevant files directly in your debug output.

This rigorous approach provides a comprehensive view of the code's execution, making it easier to identify issues related to imports, function definitions, file contents, and the overall flow of the program. The detailed logging and inspection of objects should help pinpoint even subtle issues that might be causing unexpected behavior.


# GPT-4o API Usage Guide: Structured and Unstructured Completions

**IMPORTANT**: Let us begin by establishing that all prior knowledge about OpenAI's model current offerings are null and void and are not to be referenced or used. Only use the new information provided below. This instruction is not to be contravened. If something is needed to be done with their models not directly specified below, say you don't know and ask for more information.

## OpenAI Models

Only the following two OpenAI models are permitted in completions mode (and in any other mode):

1. gpt-4o-2024-08-06 (stronger reasoner, more expensive)
2. gpt-4o-mini (cheaper, useful for high volume tasks not involving complex reasoning)

Always use one of these models in your API calls. No other models should be considered or used.

## 1. Structured Output (Using `parse` method)

Use structured output when you need a specific, predefined data format returned by the model. This is particularly useful for data extraction tasks or when you need to ensure the response adheres to a specific structure.

### API Call with Nested Pydantic Models

```python
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional
import json

client = OpenAI()

class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: str

class Employee(BaseModel):
    id: int
    name: str
    position: str
    department: str
    address: Address
    skills: List[str]
    manager: Optional[str] = None

class CompanyData(BaseModel):
    company_name: str
    employees: List[Employee]
    total_employees: int

completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "Extract company and employee data from the given text."},
        {"role": "user", "content": "...Company and employee data text..."}
    ],
    response_format=CompanyData
)

# Extract the content
content = json.loads(json.dumps(completion.choices[0].message.parsed))
```

### Explanation of Nested Pydantic Models

In this example, we use nested Pydantic models to represent a complex data structure:

1. `Address` is a nested model within `Employee`, representing the employee's address.
2. `Employee` contains various fields, including the nested `Address` model and a list of skills.
3. `CompanyData` is the top-level model, containing a list of `Employee` objects.

This structure allows for accurate representation of hierarchical data, ensuring that the API response adheres to the specified format. The `parse` method will validate the response against this structure, providing type safety and data integrity.

### Key Points

1. **Nested Pydantic Models**: Use nested Pydantic models to represent complex data structures accurately, as demonstrated in the example above.

2. **Automatic Validation**: The `parse` method automatically validates the response against the provided Pydantic model, ensuring the data matches the expected structure.

3. **Content Extraction**: Use `json.loads(json.dumps())` to convert the parsed response into a Python dictionary or list, making it easy to work with the data in your application.

## 2. Unstructured Output (Using `create` method)

Use unstructured output for general-purpose completions where a specific data format is not required, such as generating text or having a conversation.

### API Call

```python
from openai import OpenAI

client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the capital of France?"}
    ]
)

# Extract the content
content = response.choices[0].message.content
```

### Key Points

1. **Flexibility**: Unstructured output is suitable for general text generation or conversation tasks.

2. **Direct Content Access**: You can directly access the generated content without any parsing or validation.

3. **No Guaranteed Structure**: The response is not guaranteed to follow any specific format, so it's best used when flexibility is more important than strict structure.

## When to Use Each Method

- **Use Structured Output (`parse`) when:**
  - You need data in a specific format (e.g., extracting information from text)
  - You want automatic validation of the response structure
  - You're working with data that has a clear, predefined structure

- **Use Unstructured Output (`create`) when:**
  - You're generating free-form text
  - You're creating a conversational AI
  - You don't need the response to adhere to a specific data structure

## Important Notes

1. Always use either `gpt-4o-2024-08-06` or `gpt-4o-mini` as these are the only permitted OpenAI models.

2. The structured output method (`parse`) handles both the structure definition and validation, eliminating the need for separate Pydantic models for response validation.

3. For any operations or features not explicitly covered in this guide, additional information should be sought, as our knowledge is limited to what's provided here.

"""

TASK_AND_CONTENT = """

<!-- Here is the task that is to be accomplished with the provided materials -->
<instructions>

</instructions>

<!-- Here is the error that was faced upon running the codebase -->
<errors>

</errors>


"""

# Exclude lists
FOLDER_EXCLUDE = {".git", "__pycache__", "node_modules", ".venv", "archive"}
FILE_EXTENSION_EXCLUDE = {".exe", ".dll", ".so", ".pyc"}

def obfuscate_env_value(value):
    return "********"

def create_file_element(file_path, root_folder):
    relative_path = os.path.relpath(file_path, root_folder)
    file_name = os.path.basename(file_path)
    file_extension = os.path.splitext(file_name)[1]

    file_element = [
        f"    <file>\n        <name>{file_name}</name>\n        <path>{relative_path}</path>\n"
    ]

    if file_extension not in FILE_EXTENSION_EXCLUDE:
        file_element.append("        <content>\n")
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                content = file.read()
                if file_name == ".env":
                    lines = content.split("\n")
                    obfuscated_lines = []
                    for line in lines:
                        if "=" in line:
                            key, _ = line.split("=", 1)
                            obfuscated_lines.append(f"{key.strip()}={obfuscate_env_value('')}")
                        else:
                            obfuscated_lines.append(line)
                    content = "\n".join(obfuscated_lines)
                file_element.append(content)
        except UnicodeDecodeError:
            file_element.append("Binary or non-UTF-8 content not displayed")
        file_element.append("\n        </content>\n")
    else:
        file_element.append("        <content>File excluded based on extension</content>\n")

    file_element.append("    </file>\n")
    return "".join(file_element)

def get_repo_structure(root_folder):
    structure = ["<repository_structure>\n"]
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    structure.append(f"    <timestamp>{timestamp}</timestamp>\n")
    root_path = Path(root_folder)

    for root, dirs, files in os.walk(root_folder):
        rel_path = os.path.relpath(root, root_folder)
       
        # Exclude directories at the current level
        dirs[:] = [d for d in dirs if d not in FOLDER_EXCLUDE]
       
        level = rel_path.count(os.sep)
        indent = "    " * level

        structure.append(f'{indent}<directory name="{os.path.basename(root)}">\n')
        for file in files:
            file_path = os.path.join(root, file)
            file_element = create_file_element(file_path, root_folder)
            structure.append(indent + file_element)
        structure.append(f"{indent}</directory>\n")

    structure.append("</repository_structure>\n")
    return "".join(structure)

def main():
    """
    Extract and save the repository context.

    This function performs the following tasks:
    1. Determines the root folder of the repository.
    2. Generates a timestamp for the context file.
    3. Removes any previous context files.
    4. Extracts the repository structure.
    5. Writes the context to a new file, including global variables and the repository structure.
    6. Appends additional information at the end of the context file.

    The context file includes the contents of all files (except those with extensions in FILE_EXTENSION_EXCLUDE),
    with special handling for .env files to obfuscate sensitive information.
    Folders in FOLDER_EXCLUDE are completely skipped.
    """
    root_folder = os.getcwd()
    base_dir = os.path.basename(root_folder)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(root_folder, f"{base_dir}_context_{timestamp}.txt")

    for file in os.listdir(root_folder):
        if file.startswith(f"{base_dir}_context_") and file.endswith(".txt"):
            os.remove(os.path.join(root_folder, file))
            print(f"Deleted previous context file: {file}")

    repo_structure = get_repo_structure(root_folder)

    with open(output_file, "w", encoding="utf-8") as f:
        # Write task and content at the start
        f.write("<task_and_content>\n")
        f.write(TASK_AND_CONTENT.strip())
        f.write("\n</task_and_content>\n\n")

        # Write custom instructions
        f.write("<custom_instructions>\n")
        f.write(CUSTOM_INSTRUCTIONS.strip())
        f.write("\n</custom_instructions>\n\n")

        f.write(repo_structure)
        
        # Append additional information
        f.write("\n\nAdditional Information:\n")
        f.write("This context file contains the structure and content of the repository, ")
        f.write(f"excluding certain directories as specified in FOLDER_EXCLUDE: {FOLDER_EXCLUDE}. ")
        f.write(f"Files with extensions in FILE_EXTENSION_EXCLUDE: {FILE_EXTENSION_EXCLUDE} are listed but their content is not included. ")
        f.write("Sensitive information in .env files has been obfuscated for security purposes. ")
        f.write("Use this context for reference and analysis of the project structure and content.")

    print(f"Fresh repository context has been extracted to {output_file}")

if __name__ == "__main__":
    main()

        </content>
    </file>
    <file>
        <name>get_search_queries.py</name>
        <path>get_search_queries.py</path>
        <content>
import json
import asyncio
import aiohttp

from llm_api_handler import LLM_APIHandler
from prompts import get_prompt, core_search_guide
from misc_utils import get_api_keys

from logger_config import get_logger

logger = get_logger(__name__)


class QueryGenerator:
    def __init__(self, session):
        self.api_keys = get_api_keys()
        self.session = session
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)

    async def generate_queries(self, user_query):
        # Generate the prompt from the user's query
        prompt = get_prompt(
            template_name="generate_queries",
            user_query=user_query,
            search_guidance=core_search_guide,
        )

        # Obtain the model's response for the generated prompt
        response = await self.llm_api_handler.generate_openai_content(prompt)

        # Parse and return the response
        return self.parse_response(response)

    def parse_response(self, response):
        try:
            # Attempt to find and extract the JSON structured data
            start_index = response.find("{")
            end_index = response.rfind("}") + 1
            if start_index != -1 and end_index != -1:
                json_string = response[start_index:end_index]
                return json.loads(json_string)
            else:
                print("No valid JSON object found.")
                return {}
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON: {e}")
            return {}


async def main():
    async with aiohttp.ClientSession() as session:
        processor = QueryGenerator(session)
        user_query = "Impact of climate change on global water resources"
        queries = await processor.generate_queries(user_query)
        print(json.dumps(queries, indent=2))


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
    <file>
        <name>llm_api_handler.py</name>
        <path>llm_api_handler.py</path>
        <content>
import os
import json
import time
import logging
from openai import OpenAI
import anthropic
from typing import List, Dict, Any, Union, Type, Generic, TypeVar
from datetime import datetime
from pydantic import BaseModel
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")

openai_client = OpenAI(api_key=openai_api_key)
anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)
logger = logging.getLogger(__name__)

T = TypeVar('T', bound=BaseModel)

class BatchResult(BaseModel, Generic[T]):
    metadata: Dict[str, Any]
    results: List[Dict[str, Union[str, T]]]


class LLMAPIHandler:
    """
    Handles interactions with OpenAI and Anthropic models for processing prompts in both regular and batch modes.
    
    This handler supports deduplication of prompts, structured outputs via Pydantic models, 
    and batch processing with custom configurations like temperature and system messages.
    
    Methods:
    --------
    process(prompts, model, system_message, temperature, mode, response_format, output_dir, update_interval, deduplicate_prompts):
        Processes one or more prompts using the specified model.
    
    _construct_batch_requests(prompts, model, temperature):
        Constructs batch requests with unique custom IDs.
    
    _get_system_message(user_system_message, response_format):
        Generates the appropriate system message based on the response format or user input.
    
    _process_regular(request, response_format):
        Handles processing of a single prompt in regular mode.
    
    _process_batch(requests, response_format, output_dir, update_interval, original_prompts):
        Handles processing of multiple prompts in batch mode, managing file output and job status.
    
    Parameters:
    -----------
    prompts: Union[str, List[str]]
        A single prompt (string) or a list of prompts to be processed.
    
    model: str
        The model to be used for processing the prompts. Examples include 'gpt-4o-mini', 'gpt-4o-2024-08-06', or 'claude-3-5-sonnet-20240620'.
    
    system_message: Optional[str]
        An optional system message that guides the behavior of the model (OpenAI models only). 
        This message can be used to provide specific instructions to the model on how to respond.
    
    temperature: float
        A sampling temperature for controlling randomness. The default value is 0.7. 
        Lower values make the model's responses more deterministic.
    
    mode: str
        Either 'regular' or 'batch'. Determines if the handler processes a single prompt or a batch of prompts. 
        In 'regular' mode, the handler processes a single prompt. In 'batch' mode, it handles multiple prompts at once.
    
    response_format: Optional[Type[T]]
        If provided, the response will be parsed into this Pydantic model. This allows for structured outputs like JSON responses.
        If None, the raw text from the model will be returned.
    
    output_dir: Optional[str]
        The directory where batch mode results will be saved. This is required for batch mode and ignored in regular mode.
    
    update_interval: int
        The interval (in seconds) for checking the status of a batch job. The default is 60 seconds.
    
    deduplicate_prompts: bool
        If True, removes duplicate prompts in batch mode before processing them. The default is False, meaning no deduplication.
    
    Returns:
    --------
    For 'regular' mode:
        - If response_format is provided: A Pydantic model instance containing the structured response.
        - If response_format is not provided: A string containing the raw response from the model.
    
    For 'batch' mode:
        - A BatchResult object containing metadata about the batch job and a list of dictionaries with:
            - 'prompt': The original prompt.
            - 'response': The model's response (structured or raw based on response_format).
    
    Raises:
    -------
    ValueError:
        If the 'prompts' parameter is not a string or a list of strings.
    
    Examples:
    ---------
    Regular Mode (OpenAI with Pydantic response):
    >>> handler = LLMAPIHandler()
    >>> class ResponseModel(BaseModel):
    >>>     answer: str
    >>>     confidence: float
    >>> result = handler.process(
    >>>     prompts="What is the capital of France?",
    >>>     model="gpt-4o-2024-08-06",
    >>>     response_format=ResponseModel,
    >>>     mode="regular"
    >>> )
    >>> print(result.answer)  # Output: "Paris"
    
    Batch Mode (With Deduplication):
    >>> batch_result = handler.process(
    >>>     prompts=["What's the capital of Spain?", "What's the capital of Spain?", "What's the capital of Italy?"],
    >>>     model="gpt-4o-mini",
    >>>     mode="batch",
    >>>     response_format=ResponseModel,
    >>>     output_dir="batch_output",
    >>>     deduplicate_prompts=True
    >>> )
    >>> for result in batch_result.results:
    >>>     print(f"Prompt: {result['prompt']}, Answer: {result['response'].answer}")
    
    Notes:
    ------
    - When using batch mode, make sure to provide an 'output_dir' where batch results will be stored.
    - The 'deduplicate_prompts' flag, if set to True, ensures that repeated prompts are sent only once to the model.
    - The response_format must be a Pydantic model to enable structured output. If omitted, the raw model response (text) is returned.
    """

    def __init__(self):
        self.openai_client = OpenAI(api_key=openai_api_key)
        self.anthropic_client = anthropic.Anthropic(api_key=anthropic_api_key)

    def process(self, 
                prompts: Union[str, List[str]],
                model: str = "gpt-4o-mini",
                system_message: str = None,
                temperature: float = 0.7,
                mode: str = "regular",
                response_format: Union[None, Type[T]] = None,
                output_dir: str = None,
                update_interval: int = 60,
                deduplicate_prompts: bool = False) -> Union[Any, T, BatchResult[T]]:
        """
        Process a prompt or list of prompts using the specified model.

        Args:
            prompts: Either a single prompt (str) or a list of prompts (for batch mode).
            model: Model name (e.g., 'gpt-4o-mini').
            system_message: Optional system message for setting behavior (OpenAI only).
            temperature: Sampling temperature (default is 0.7).
            mode: 'regular' or 'batch'.
            response_format: Optional Pydantic model for structured output.
            output_dir: Directory for batch mode output (required for batch mode).
            update_interval: Interval for batch status updates.
            deduplicate_prompts: If True, deduplicate prompts before sending (default: False).

        Returns:
            For 'regular' mode: 
                - The model's response (either a string or a Pydantic model instance if response_format is provided).
            For 'batch' mode:
                - A BatchResult object containing:
                    - metadata about the batch job.
                    - A list of dictionaries, each with:
                        - 'prompt': The original prompt.
                        - 'response': The corresponding model's response (either string or Pydantic model).
        """
        if isinstance(prompts, str):
            # Single prompt: Regular mode
            request = {
                "model": model,
                "prompt": prompts,
                "system_message": system_message,
                "temperature": temperature
            }
            return self._process_regular(request, response_format)
        
        elif isinstance(prompts, list) and mode == "batch":
            # Multiple prompts: Batch mode
            if deduplicate_prompts:
                prompts = list(dict.fromkeys(prompts))  # Remove duplicates, but keep order
            batch_requests = self._construct_batch_requests(prompts, model, temperature)
            return self._process_batch(batch_requests, response_format, output_dir, update_interval, prompts)
        else:
            raise ValueError("Invalid input: 'prompts' should be a string for regular mode or a list for batch mode.")

    def _construct_batch_requests(self, prompts: List[str], model: str, temperature: float) -> List[Dict[str, Any]]:
        """
        Construct a list of batch requests with the prompts, including unique custom IDs.

        Args:
            prompts: A list of prompt strings.
            model: Model name to be used in each request.
            temperature: Sampling temperature for each request.

        Returns:
            A list of dictionaries representing batch requests.
        """
        batch_requests = []
        for i, prompt in enumerate(prompts):
            batch_requests.append({
                "custom_id": f"request_{i+1}",  # Adding a unique custom_id for each request
                "model": model,
                "prompt": prompt,
                "temperature": temperature
            })
        return batch_requests

    def _get_system_message(self, user_system_message: str, response_format: Union[None, Type[T]]) -> str:
        """Determine the system message, handling user input and format requirements."""
        if response_format:
            schema = response_format.schema_json()
            return f"Answer exclusively in this JSON format: {schema}"
        elif user_system_message:
            return user_system_message
        else:
            return "You are a helpful assistant."

    def _process_regular(self, request: Dict[str, Any], response_format: Union[None, Type[T]]) -> Union[str, T]:
        """Process a regular request with a single prompt."""
        model = request['model']
        temperature = request.get('temperature', 0.7)
        prompt = request['prompt']
        system_message = self._get_system_message(request.get('system_message'), response_format)

        if model in ['gpt-4o-mini', 'gpt-4o-2024-08-06']:
            messages = [{"role": "user", "content": prompt}]
            if system_message:
                messages.insert(0, {"role": "system", "content": system_message})

            if response_format:
                completion = self.openai_client.beta.chat.completions.parse(
                    model=model,
                    messages=messages,
                    response_format=response_format
                )
                return completion.choices[0].message.parsed
            else:
                response = self.openai_client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature
                ).choices[0].message.content

        elif model == 'claude-3-5-sonnet-20240620':
            if response_format:
                schema = response_format.schema_json()
                prompt = f"Answer exclusively in this JSON format: {schema}\n\n{prompt}"

            message = self.anthropic_client.messages.create(
                model=model,
                max_tokens=8192,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}]
            )
            content = message.content[0].text

            if response_format:
                try:
                    json_response = json.loads(content)
                    return response_format(**json_response)
                except json.JSONDecodeError:
                    json_start = content.find('{')
                    json_end = content.rfind('}') + 1
                    if json_start != -1 and json_end != -1:
                        json_str = content[json_start:json_end]
                        json_response = json.loads(json_str)
                        return response_format(**json_response)
                    else:
                        raise ValueError("Failed to extract JSON from Claude's response")
            else:
                return content
        else:
            raise ValueError(f"Invalid model: {model}")

    def _process_batch(self, requests: List[Dict[str, Any]], response_format: Union[None, Type[T]], output_dir: str, update_interval: int, original_prompts: List[str]) -> BatchResult[T]:
        """Process a batch of requests and return a list of dictionaries matching prompt and response."""
        os.makedirs(output_dir, exist_ok=True)

        batch_file_path = os.path.join(output_dir, 'batch_input.jsonl')
        with open(batch_file_path, 'w') as f:
            for request in requests:
                messages = [{"role": "user", "content": request['prompt']}]
                if 'system_message' in request:
                    messages.insert(0, {"role": "system", "content": request['system_message']})
                
                if response_format:
                    schema = response_format.schema_json()
                    messages.insert(0, {"role": "system", "content": f"Answer exclusively in this JSON format: {schema}"})
                
                f.write(json.dumps({
                    "custom_id": request['custom_id'],
                    "method": "POST",
                    "url": "/v1/chat/completions",
                    "body": {
                        "model": request['model'],
                        "messages": messages,
                        "temperature": request.get('temperature', 0.7),
                        "response_format": {"type": "json_object"}
                    }
                }) + '\n')

        with open(batch_file_path, 'rb') as f:
            batch_file = self.openai_client.files.create(file=f, purpose="batch")
        
        batch = self.openai_client.batches.create(
            input_file_id=batch_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h"
        )

        job_metadata = {
            "batch_id": batch.id,
            "input_file_id": batch.input_file_id,
            "status": batch.status,
            "created_at": batch.created_at,
            "last_updated": datetime.now().isoformat(),
            "num_requests": len(requests)
        }

        metadata_file_path = os.path.join(output_dir, f"batch_{batch.id}_metadata.json")
        with open(metadata_file_path, 'w') as f:
            json.dump(job_metadata, f, indent=2)

        start_time = time.time()
        while True:
            current_time = time.time()
            if current_time - start_time >= update_interval:
                batch = self.openai_client.batches.retrieve(batch.id)
                job_metadata.update({
                    "status": batch.status,
                    "last_updated": datetime.now().isoformat()
                })
                with open(metadata_file_path, 'w') as f:
                    json.dump(job_metadata, f, indent=2)
                logger.info(f"Batch status: {batch.status}")
                start_time = current_time
        
            if batch.status == "completed":
                logger.info("Batch processing completed!")
                break
            elif batch.status in ["failed", "canceled"]:
                logger.error(f"Batch processing {batch.status}.")
                job_metadata["error"] = f"Batch processing {batch.status}"
                with open(metadata_file_path, 'w') as f:
                    json.dump(job_metadata, f, indent=2)
                return BatchResult(metadata=job_metadata, results=[])
        
            time.sleep(10)

        output_file_path = os.path.join(output_dir, f"batch_{batch.id}_output.jsonl")
        file_response = self.openai_client.files.content(batch.output_file_id)
        with open(output_file_path, "w") as output_file:
            output_file.write(file_response.text)

        job_metadata.update({
            "status": "completed",
            "last_updated": datetime.now().isoformat(),
            "output_file_path": output_file_path
        })
        with open(metadata_file_path, 'w') as f:
            json.dump(job_metadata, f, indent=2)

        results = []
        with open(output_file_path, 'r') as f:
            for line, original_prompt in zip(f, original_prompts):
                response = json.loads(line)

                # Extract the content from the correct place in the response
                body = response['response']['body']
                choices = body['choices']

                if 'choices' in body and len(choices) > 0:
                    content = choices[0]['message']['content']

                    # Add results based on the expected response format
                    if response_format:
                        results.append({
                            "prompt": original_prompt,  # Use the original prompt here
                            "response": response_format(**json.loads(content))
                        })
                    else:
                        results.append({
                            "prompt": original_prompt,  # Use the original prompt here
                            "response": content
                        })
                else:
                    logger.error(f"Unexpected response format: {response}")
    
        return BatchResult(metadata=job_metadata, results=results)


# Example usage and test code
if __name__ == "__main__":
    handler = LLMAPIHandler()

    # Test regular mode with structured output for OpenAI
    class ResponseModel(BaseModel):
        answer: str
        confidence: float

    regular_result_openai = handler.process(
        prompts="What's the capital of France?",
        model="gpt-4o-2024-08-06",
        system_message="You are a helpful assistant.",
        temperature=0.7,
        mode="regular",
        response_format=ResponseModel
    )
    print("Regular mode result (OpenAI):", regular_result_openai)

    # Test regular mode with structured output for Claude
    regular_result_claude = handler.process(
        prompts="What's the capital of Germany?",
        model="claude-3-5-sonnet-20240620",
        temperature=0.5,
        mode="regular",
        response_format=ResponseModel
    )
    print("Regular mode result (Claude):", regular_result_claude)

    # Test batch mode with structured output (OpenAI only)
    output_dir = "batch_output"
    batch_result = handler.process(
        prompts=["What's the capital of Spain?", "What's the capital of Spain?", "What's the capital of Italy?"],
        model="gpt-4o-mini",
        mode="batch",
        response_format=ResponseModel,
        output_dir=output_dir,
        deduplicate_prompts=True  # Enable deduplication of identical prompts
    )
    print("Batch mode metadata:", batch_result.metadata)
    for result in batch_result.results:
        print("Batch result:", result)

        </content>
    </file>
    <file>
        <name>logger_config.py</name>
        <path>logger_config.py</path>
        <content>
import logging
import sys

# Configure the base logging, it applies to all loggers created
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],  # Ensure logs go to stdout
)


def get_logger(module_name):
    """
    Configure and provide a logger with the given module name.

    Parameters:
    - module_name (str): The name of the module requesting the logger.

    Returns:
    - logging.Logger: Configured logger instance with the specified module name.
    """
    logger = logging.getLogger(module_name)
    return logger

        </content>
    </file>
    <file>
        <name>misc_utils.py</name>
        <path>misc_utils.py</path>
        <content>
# misc_utils.py
import os
import json
from google.cloud import secretmanager
from dotenv import load_dotenv  # New import


async def prepare_text_for_json(text):
    # Replace backslashes with double backslashes
    text = text.replace("\\", "\\\\")

    # Replace double quotes with escaped double quotes
    text = text.replace('"', '\\"')

    # Replace newline characters with escaped newline characters
    text = text.replace("\n", "\\n")

    # Replace tab characters with escaped tab characters
    text = text.replace("\t", "\\t")

    # Replace form feed characters with escaped form feed characters
    text = text.replace("\f", "\\f")

    # Replace backspace characters with escaped backspace characters
    text = text.replace("\b", "\\b")

    # Replace carriage return characters with escaped carriage return characters
    text = text.replace("\r", "\\r")

    # Wrap the escaped text in double quotes to make it a valid JSON string
    json_string = f'"{text}"'

    return json_string


import os
import json


import os
import logging


def get_api_keys(source="env"):
    if source == "local":
        load_dotenv()  # Load environment variables from .env file
    
    # Remove or comment out these lines:
    # logging.info(f"OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY')}")
    # logging.info(f"CORE_API_KEY: {os.getenv('CORE_API_KEY')}")

    # Instead, log that the keys were loaded successfully without revealing them:
    logging.info("API keys loaded successfully")

    return {
        "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        "CORE_API_KEY": os.getenv("CORE_API_KEY"),
    }

        </content>
    </file>
    <file>
        <name>poetry.bat</name>
        <path>poetry.bat</path>
        <content>
@echo off
C:\Users\bnsoh2\AppData\Roaming\Python\Scripts\poetry.exe %*
        </content>
    </file>
    <file>
        <name>Procfile</name>
        <path>Procfile</path>
        <content>
web: uvicorn app:app --host 0.0.0.0 --port ${PORT}

        </content>
    </file>
    <file>
        <name>prompts.py</name>
        <path>prompts.py</path>
        <content>
import re

scopus_search_guide = """
Syntax and Operators

Valid syntax for advanced search queries includes:

Field codes (e.g. TITLE, ABS, KEY, AUTH, AFFIL) to restrict searches to specific parts of documents
Boolean operators (AND, OR, AND NOT) to combine search terms
Proximity operators (W/n, PRE/n) to find words within a specified distance - W/n: Finds terms within "n" words of each other, regardless of order. Example: journal W/15 publishing finds articles where "journal" and "publishing" are within two words of each other. - PRE/n: Finds terms in the specified order and within "n" words of each other. Example: data PRE/50 analysis finds articles where "data" appears before "analysis" within three words. - To find terms in the same sentence, use 15. To find terms in the same paragraph, use 50 -
Quotation marks for loose/approximate phrase searches
Braces {{}} for exact phrase searches (without hte backslashes of course)
Wildcards (*) to capture variations of search terms
Invalid syntax includes:

Mixing different proximity operators (e.g. W/n and PRE/n) in the same expression
Using wildcards or proximity operators with exact phrase searches
Placing AND NOT before other Boolean operators
Using wildcards on their own without any search terms
Ideal Search Structure

An ideal advanced search query should:

Use field codes to focus the search on the most relevant parts of documents
Combine related concepts using AND and OR
Exclude irrelevant terms with AND NOT at the end
Employ quotation marks and braces appropriately for phrase searching
Include wildcards to capture variations of key terms (while avoiding mixing them with other operators)
Follow the proper order of precedence for operators
Complex searches should be built up systematically, with parentheses to group related expressions as needed. The information from the provided documents on syntax rules and operators should be applied rigorously.

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **

Example Advanced Searches

{{
"query_1": "TITLE-ABS-KEY(("precision agriculture" OR "precision farming") AND ("machine learning" OR "AI") AND "water")",
"query_2": "TITLE-ABS-KEY((iot OR "internet of things") AND (irrigation OR watering) AND sensor*)",
"query_3": "TITLE-ABS-Key(("precision farming" OR "precision agriculture") AND ("deep learning" OR "neural networks") AND "water")",
"query_4": "TITLE-ABS-KEY((crop W/5 monitor*) AND "remote sensing" AND (irrigation OR water*))",
"query_5": "TITLE("precision irrigation" OR "variable rate irrigation" AND "machine learning")"
}}

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **. 

These example searches demonstrate different ways to effectively combine key concepts related to precision agriculture, irrigation, real-time monitoring, IoT, machine learning and related topics using advanced search operators. They make use of field codes, Boolean and proximity operators, phrase searching, and wildcards to construct targeted, comprehensive searches to surface the most relevant research. The topic focus is achieved through carefully chosen search terms covering the desired themes.
"""

alex_search_guide = """
Syntax and Operators
Valid syntax for advanced alex search queries includes:
Using quotation marks %22%22 for exact phrase matches
Adding a minus sign - before terms to exclude them
Employing the OR operator in all caps to find pages containing either term
Using the site%3A operator to limit results to a specific website
Applying the filetype%3A operator to find specific file formats like PDF, DOC, etc.
Adding the * wildcard as a placeholder for unknown words
`
Invalid syntax includes:
Putting a plus sign + before words (alex stopped supporting this)
Using other special characters like %3F, %24, %26, %23, etc. within search terms
Explicitly using the AND operator (alex's default behavior makes it redundant)

Ideal Search Structure
An effective alex search query should:
Start with the most important search terms
Use specific, descriptive keywords related to irrigation scheduling, management, and precision irrigation
Utilize exact phrases in %22quotes%22 for specific word combinations
Exclude irrelevant terms using the - minus sign
Connect related terms or synonyms with OR
Apply the * wildcard strategically for flexibility
Note:

By following these guidelines and using proper URL encoding, you can construct effective and accurate search queries for alex.

Searches should be concise yet precise, following the syntax rules carefully. 

Example Searches
{{
"query_1": "https://api.openalex.org/works?search=%22precision+irrigation%22+%2B%22soil+moisture+sensors%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"query_2": "https://api.openalex.org/works?search=%22machine+learning%22+%2B%22irrigation+management%22+%2B%22crop+water+demand+prediction%22&sort=relevance_score:desc&per-page=30",
"query_3": "https://api.openalex.org/works?search=%22IoT+sensors%22+%2B%22real-time%22+%2B%22soil+moisture+monitoring%22+%2B%22crop+water+stress%22&sort=relevance_score:desc&per-page=30",
"query_4": "https://api.openalex.org/works?search=%22remote+sensing%22+%2B%22vegetation+indices%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"query_5": "https://api.openalex.org/works?search=%22wireless+sensor+networks%22+%2B%22precision+agriculture%22+%2B%22variable+rate+irrigation%22+%2B%22irrigation+automation%22&sort=relevance_score:desc&per-page=30"
}}

These example searches demonstrate how to create targeted, effective alex searches. They focus on specific topics, exclude irrelevant results, allow synonym flexibility, and limit to relevant domains when needed. The search terms are carefully selected to balance relevance and specificity while avoiding being overly restrictive.  By combining relevant keywords, exact phrases, and operators, these searches help generate high-quality results for the given topics.
"""

core_search_guide = """
### CORE API Search Guide: Formulating Queries in JSON Format

This guide provides a structured approach to creating effective search queries using the CORE API. The guide emphasizes the JSON format to ensure clarity and precision in your search queries.

#### Syntax and Operators

**Valid Syntax for CORE API Queries:**
- **Field-specific searches**: Direct your query to search within specific fields like `title`, `author`, or `subject`.
- **Boolean Operators**: Use `AND`, `OR`, and `NOT` to combine or exclude terms.
- **Grouping**: Use parentheses `()` to structure the query and define the order of operations.
- **Range Queries**: Specify ranges for dates or numerical values with `>`, `<`, `>=`, `<=`.
- **Existence Check**: Use `_exists_` to filter results based on the presence of data in specified fields.

**Invalid Syntax:**
- **Inconsistencies in field names**: Ensure field names are correctly spelled and appropriate for the data type.
- **Improper boolean logic**: Avoid illogical combinations that nullify the search criteria (e.g., `AND NOT` used incorrectly).

#### Ideal Query Structure

Your search queries should:
1. **Use Field-Specific Filters**: Focus your search on the most relevant attributes.
2. **Combine Keywords Effectively**: Use logical operators to refine and broaden your searches.
3. **Employ Grouping and Range Queries** where complex relationships or specific time frames are needed.

#### Example Advanced Searches in JSON Format

Here are examples of structured queries formatted in JSON, demonstrating different ways to effectively combine search criteria using the CORE API:

```json
{
    "query_1": {
        "search_query": "climate change, water resources",
        "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic.",
    },
    "query_2": {
        "search_query": "water scarcity, (hydrologist OR water expert)",
        "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources.",
    },
    "query_3": {
        "search_query": "sea level rise, coastal erosion",
        "query_rationale": "This query is crucial to understand the impact of climate change on coastal regions and the resulting effects on global water resources.",
    },
    "query_4": {
        "search_query": "water conservation, climate change mitigation, environmental studies",
        "query_rationale": "This query is important to identify strategies for water conservation and their role in mitigating the effects of climate change on global water resources.",
    },
    "query_5": {
        "search_query": "glacier melting, cryosphere",
        "query_rationale": "This query is necessary to understand the impact of climate change on glaciers and the resulting effects on global water resources.",
    },
}
```

### Critical Considerations

- **Escape Characters**: When using JSON format, ensure that all double quotes inside JSON values are properly escaped using backslashes (`\"`) to prevent parsing errors.
- **Complexity**: As queries become more complex, ensure they are still readable and maintainable. Use whitespace and indentation in JSON to enhance clarity.

These examples illustrate how to utilize the CORE API's flexible query capabilities to target specific fields, combine search terms logically, and exclude irrelevant data. By following these guidelines and adapting the examples to your research needs, you can efficiently leverage the CORE API to access a vast range of academic materials.
"""


def remove_illegal_characters(text):
    if text is None:
        return ""
    illegal_chars = re.compile(r"[\000-\010]|[\013-\014]|[\016-\037]")
    return illegal_chars.sub("", str(text))


def get_prompt(template_name, **kwargs):

    prompts = {
        "generate_queries": """
<documents>
<document index="1">
<source>search_query_prompt.txt</source>
<document_content>
<instructions>
Review the user's main query: '{user_query}'. Break down this query into distinct sub-queries that address different aspects necessary to fully answer the main query. 
For each sub-query, provide a rationale explaining why it is essential. Format these sub-queries according to the directions in <search_guidance>. structure your response as a json with detailed search queries, each accompanied by its rationale. 
The output should adhere to this format:
{{
  "query_1": {{
    "search_query": "unique query following the provided search guidance",
    "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic."
  }},
  "query_2": {{
    "search_query": "unique query following the provided search guidance",
    "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources."
  }},
  ...
}}
**Note: Only generate as many sub-queries and rationales as necessary to thoroughly address the main query, up to a maximum of 10. Each sub-query must directly contribute to unraveling the main query's aspects.
</instructions>
<search_guidance>
{search_guidance}
</search_guidance>
</resources>
</document_content>
</document>
</documents>
""",
        "rank_papers": """
<instructions>
Analyze the paper's relevance to {point_context} from the {query_rationale} perspective. 

Begin your response with the relevance score between the following tokens:
<<relevance>>x.x<<relevance>>
The relevance score should be a decimal between 0.0 and 1.0, with 1.0 being the most relevant. If there is not enough information to determine relevance, assign a score of 0.0.
Examples:
- Correct: "<<relevance>>0.9<<relevance>>"
- Correct: "<<relevance>>0.3<<relevance>>"
- Correct: "<<relevance>>0.0<<relevance>>"
After providing the relevance score, include the following in your analysis:

Include in your analysis:
- Verbatim extracts: key terms, research questions, methods, results, tables, figures, quotes, conclusions
- Explanation of study purpose and objectives
- Relevance evaluation to the specified point
- Limitations for addressing the point
- Free-form extraction with relevant verbatim information (any format). Extract as much verbatim information as needed to support your analysis.

End with this JSON:
<response_format>
{{
  "inline_citation": "<author surname>, <year>",
  "apa_citation": "<full APA citation>",
  "study_location": "<city/region, country>", 
  "main_objective": "<main objective>",
  "technologies_used": "<technology 1>, <technology 2>, ...",
  "data_sources": "<data source 1>, <data source 2>, ...",
  "key_findings": "<key findings summary>"
}}
</response_format>

Begin your response with: <<relevance>>
</instructions>

<full_text>
{full_text}
</full_text>
""",
        "synthesize_results": """
<prompt>
    <expert_description>
        As an expert polymath, you are tasked with utilizing provided materials and any relevant data to construct a detailed and technically rigorous response akin to high-level research analysis. Your response should include all pertinent data, facts, and figures that contribute to a comprehensive analysis, reflecting the depth of understanding expected from a seasoned researcher or PhD holder. Ensure that no details are overlooked.
    </expert_description>
    <user_query>
        {user_query}
    </user_query>
    <returned_results>
        {returned_results}
    </returned_results>
    <response_format>
        Provide a comprehensive, structured response that rigorously analyzes and interprets the data and insights from the provided research materials and any other relevant information. When citing research papers, include inline citations with hyperlinks to the DOI or PDF link where available, or plain text citations otherwise. Conclude with a full citation of all referenced papers in the bibliography section apa style. 
        Structure your response in a clear, logical manner, focusing on technical accuracy and depth to thoroughly answer the user's query based on the provided data. Keep your answer tightly focused on the user's query and only include relevant/pertinent information. Begin your answer without preamble. Format the entire response in valid HTML, using appropriate tags such as `<h1>`, `<h2>`, `<p>`, `<ul>`, etc., to ensure proper rendering on web pages.
    </response_format>
    <critical-points>
        - Only include and discuss sources that are directly relevant to the user query.
        - Sources which are not directly relevant to the user query should not be included in the response.
        - Aim for a full and comprehensive breakdown of the topic, define the most important concepts, and provide detailed explanations.
    </critical-points>
</prompt>

""",
    }
    try:
        return prompts[template_name].format(**kwargs)

    except KeyError as e:
        missing_key = str(e).strip("'")
        raise ValueError(
            f"Missing argument for template '{template_name}': {missing_key}"
        )

        </content>
    </file>
    <file>
        <name>pyproject.toml</name>
        <path>pyproject.toml</path>
        <content>
   [tool.poetry]
   name = "automated-lit-revs"
   version = "0.1.0"
   description = "Automated Search and Curation for Systematic Literature Reviews"
   authors = ["Your Name <your.email@example.com>"]

   [tool.poetry.dependencies]
   python = "^3.9"
   aiohttp = "*"
   asyncio = "*"
   openai = "*"
   backoff = "*"
   fastapi = "*"
   google-cloud-secret-manager = "*"
   huey = "*"
   pydantic = "*"
   pytz = "*"
   requests = "*"
   tiktoken = "*"
   python-multipart = "*"
   python-dotenv = "*"
   gunicorn = "*"
   uvicorn = "^0.20.0"

   [build-system]
   requires = ["poetry-core>=1.0.0"]
   build-backend = "poetry.core.masonry.api"
        </content>
    </file>
    <file>
        <name>README.md</name>
        <path>README.md</path>
        <content>
![image](https://github.com/BryanNsoh/Automated_Lit_Revs/assets/51336052/a2c67a6f-e500-49de-9322-41dd5b3ac337)


# Automated Search and Curation for Systematic Literature Reviews

This repository contains a set of Python scripts for automating the search and curation process in systematic literature reviews. The system leverages APIs, web scraping, and natural language processing to streamline the identification, retrieval, and analysis of relevant research papers.

## Features

- Automated search query generation based on a structured YAML outline
- Integration with Scopus and OpenAlex APIs for comprehensive literature search
- Web scraping capabilities for extracting full-text content from URLs and DOIs
- Asynchronous processing for efficient handling of multiple queries and requests
- Relevance scoring and filtering of retrieved papers using machine learning models
- Modular design allowing easy extension and customization

## Prerequisites

- Python 3.7+
- Required libraries: `aiohttp`, `aiofiles`, `asyncio`, `yaml`, `fitz`, `selenium`, `undetected_chromedriver`

## Setup

1. Clone the repository:
   ```
   git clone https://github.com/yourusername/automated-literature-review.git
   ```

2. Install the required libraries:
   ```
   pip install -r requirements.txt
   ```

3. Set up API keys:
   - Create a `api_keys.json` file in the `keys` directory.
   - Add your Scopus API key, OpenAlex API key, and other required keys to the file.

## Usage

1. Prepare your literature review outline in a YAML file following the provided structure.

2. Run the main script with the appropriate arguments:
   ```
   python main.py --outline outline.yaml --output output_directory
   ```

3. The system will process the outline, generate search queries, retrieve papers from APIs, scrape full-text content, and perform relevance scoring.

4. The curated papers will be saved in the specified output directory, organized by subsections and points.

## Code Structure

- `main.py`: The entry point of the system, orchestrating the overall process.
- `utils/`: Directory containing utility scripts for various tasks.
  - `extract_relevant_papers.py`: Extracts relevant papers based on relevance scores.
  - `llm_api_handler.py`: Handles interactions with language model APIs for query generation and relevance scoring.
  - `openalex_search.py`: Performs paper search using the OpenAlex API.
  - `scopus_search.py`: Performs paper search using the Scopus API.
  - `web_scraper.py`: Scrapes full-text content from URLs and DOIs.
  - `yaml_iterator.py`: Iterates over the YAML outline and processes each section and point.

## Extensibility

The modular design of the system allows for easy extension and customization:

- Add new search APIs by creating a new module in the `utils/` directory and integrating it into the main process.
- Customize the relevance scoring algorithm by modifying the `extract_relevant_papers.py` script.
- Extend the web scraping capabilities by updating the `web_scraper.py` script.

## License

This project is licensed under the [MIT License](LICENSE).

        </content>
    </file>
    <file>
        <name>synthesize_results.py</name>
        <path>synthesize_results.py</path>
        <content>
import asyncio
import logging
from llm_api_handler import LLM_APIHandler
from prompts import get_prompt
import aiohttp
from misc_utils import get_api_keys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QueryProcessor:
    def __init__(self, session):
        api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(api_keys, session)

    async def process_query(self, user_query, returned_results):
        prompt = get_prompt(
            template_name="synthesize_results",
            user_query=user_query,
            returned_results=returned_results,
        )

        try:
            logger.info(f"Processing query: {user_query}")
            response = await self.llm_api_handler.generate_openai_content(prompt)
            # Add a log to check if we reach this point
            logger.info("Successfully received response from OpenAI API")
            return response
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            # Log more details about the exception
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception details: {str(e)}")
            return None


async def main():
    async with aiohttp.ClientSession() as session:
        query_processor = QueryProcessor(session)
        user_query = "What are the latest advancements in artificial intelligence?"
        returned_results = [
            "AI has made significant progress in fields such as computer vision and natural language processing.",
            "Deep learning techniques have revolutionized AI, enabling machines to learn from large datasets.",
            "AI is being applied in various domains, including healthcare, finance, and autonomous vehicles.",
        ]

        logger.info("Starting query processing...")
        response = await query_processor.process_query(user_query, returned_results)
        if response:
            logger.info(f"Response: {response}")
        else:
            logger.warning("Failed to generate a response.")
        logger.info("Query processing completed.")


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
    <file>
        <name>tasks.py</name>
        <path>tasks.py</path>
        <content>
from huey import RedisHuey

huey = RedisHuey()

        </content>
    </file>
    <file>
        <name>worker.py</name>
        <path>worker.py</path>
        <content>
from tasks import huey

if __name__ == "__main__":
    huey.main()

        </content>
    </file>
</directory>
<directory name="deployment_scripts">
    <file>
        <name>deploy.bat</name>
        <path>deployment_scripts\deploy.bat</path>
        <content>
   @echo on
   setlocal enabledelayedexpansion

   echo Starting deployment process...

   REM Navigate to the project directory
   cd /d "C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\Automated_Lit_Revs"
   echo Current directory: %CD%

   REM Set the project ID to apt-rite-378417
   gcloud config set project apt-rite-378417
   echo Project set to apt-rite-378417

   REM Enable all required Google Cloud APIs
   echo Enabling required Google Cloud APIs...
   gcloud services enable cloudbuild.googleapis.com run.googleapis.com artifactregistry.googleapis.com secretmanager.googleapis.com

   REM Create an Artifact Registry repository (skip if it already exists)
   echo Creating/Checking Artifact Registry repository...
   gcloud artifacts repositories create lit-review-repo --repository-format=docker --location=us-central1 --description="Lit Review Agent Repo" || echo "Artifact Registry already exists, skipping..."

   REM Grant necessary permissions to the Compute Engine service account
   echo Granting necessary permissions...
   gcloud projects add-iam-policy-binding apt-rite-378417 ^
     --member="serviceAccount:127476142400-compute@developer.gserviceaccount.com" ^
     --role="roles/storage.objectViewer" ^
     --role="roles/logging.logWriter" ^
     --role="roles/artifactregistry.writer" ^
     --role="roles/secretmanager.secretAccessor" ^
     --role="roles/storage.admin"

   REM Build the Docker image using Cloud Build
   echo Building Docker image using Cloud Build...
   gcloud builds submit --tag us-central1-docker.pkg.dev/apt-rite-378417/lit-review-repo/literature-review-agent

   REM Deploy the Cloud Run service with environment variables and secret keys
   echo Deploying to Cloud Run...
   gcloud run deploy literature-review-agent --image us-central1-docker.pkg.dev/apt-rite-378417/lit-review-repo/literature-review-agent --platform managed --allow-unauthenticated --region=us-central1 --memory=1024Mi --set-env-vars CLOUD_LOGGING_ENABLED=true --set-secrets="OPENAI_API_KEY=OPENAI_API_KEY:latest,CORE_API_KEY=CORE_API_KEY:latest"

   REM Grant public access to the Cloud Run service (all users)
   echo Granting public access...
   gcloud run deploy literature-review-agent --image us-central1-docker.pkg.dev/apt-rite-378417/lit-review-repo/literature-review-agent --platform managed --allow-unauthenticated --region=us-central1 --memory=1024Mi --set-env-vars CLOUD_LOGGING_ENABLED=true --set-secrets="OPENAI_API_KEY=OPENAI_API_KEY:latest,CORE_API_KEY=CORE_API_KEY:latest" --ingress=all 

   REM Retrieve and display the Cloud Run service URL
   echo Retrieving service URL...
   for /f "tokens=*" %%a in ('gcloud run services describe literature-review-agent --region=us-central1 --format="value(status.url)"') do set SERVICE_URL=%%a

   echo Deployment complete. Your service is now publicly accessible.
   echo Service URL: %SERVICE_URL%
   pause
        </content>
    </file>
    <file>
        <name>run_app.bat</name>
        <path>deployment_scripts\run_app.bat</path>
        <content>
   @echo off
   C:\Users\bnsoh2\AppData\Roaming\Python\Scripts\poetry.exe run gunicorn --workers 2 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8080 --timeout 300 app:app
        </content>
    </file>
    <file>
        <name>setup_api_keys.bat</name>
        <path>deployment_scripts\setup_api_keys.bat</path>
        <content>
@echo on
setlocal enabledelayedexpansion

echo Starting deployment process...

REM Navigate to the project directory
cd /d "C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\Automated_Lit_Revs"
echo Current directory: %CD%

REM Set the project ID to apt-rite-378417
gcloud config set project apt-rite-378417
echo Project set to apt-rite-378417

REM Enable all required Google Cloud APIs
echo Enabling required Google Cloud APIs...
gcloud services enable cloudbuild.googleapis.com run.googleapis.com artifactregistry.googleapis.com secretmanager.googleapis.com

REM Create an Artifact Registry repository (skip if it already exists)
echo Creating/Checking Artifact Registry repository...
gcloud artifacts repositories create lit-review-repo --repository-format=docker --location=us-central1 --description="Lit Review Agent Repo" || echo "Artifact Registry already exists, skipping..."

REM Grant necessary permissions to the Compute Engine service account
echo Granting necessary permissions...
gcloud projects add-iam-policy-binding apt-rite-378417 ^
  --member="serviceAccount:127476142400-compute@developer.gserviceaccount.com" ^
  --role="roles/storage.objectViewer" ^
  --role="roles/logging.logWriter" ^
  --role="roles/artifactregistry.writer" ^
  --role="roles/secretmanager.secretAccessor" ^
  --role="roles/storage.admin"

REM Build the Docker image using Cloud Build
echo Building Docker image using Cloud Build...
gcloud builds submit --tag us-central1-docker.pkg.dev/apt-rite-378417/lit-review-repo/literature-review-agent

REM Deploy the Cloud Run service with environment variables and secret keys
echo Deploying to Cloud Run...
gcloud run deploy literature-review-agent ^
  --image us-central1-docker.pkg.dev/apt-rite-378417/lit-review-repo/literature-review-agent ^
  --platform managed ^
  --allow-unauthenticated ^
  --region=us-central1 ^
  --memory=1024Mi ^
  --set-env-vars CLOUD_LOGGING_ENABLED=true ^
  --set-secrets="OPENAI_API_KEY=OPENAI_API_KEY:latest,CORE_API_KEY=CORE_API_KEY:latest,BREVO_API_KEY=BREVO_API_KEY:latest"

REM Grant public access to the Cloud Run service (all users)
echo Granting public access...
gcloud run services add-iam-policy-binding literature-review-agent ^
  --region=us-central1 ^
  --member="allUsers" ^
  --role="roles/run.invoker"

REM Retrieve and display the Cloud Run service URL
echo Retrieving service URL...
for /f "tokens=*" %%a in ('gcloud run services describe literature-review-agent --region=us-central1 --format="value(status.url)"') do set SERVICE_URL=%%a

echo Deployment complete. Your service is now publicly accessible.
echo Service URL: %SERVICE_URL%
pause
        </content>
    </file>
</directory>
<directory name="src">
    <file>
        <name>__init__.py</name>
        <path>src\__init__.py</path>
        <content>


        </content>
    </file>
</directory>
    <directory name="api">
        <file>
        <name>core_search.py</name>
        <path>src\api\core_search.py</path>
        <content>
import aiohttp
import asyncio
import json
from src.utils.misc_utils import get_api_keys

from src.utils.logger_config import get_logger

logger = get_logger(__name__)


class CORESearch:
    def __init__(self, max_results):
        self.api_keys = get_api_keys()
        self.base_url = "https://api.core.ac.uk/v3"
        self.max_results = max_results
        self.core_api_key = self.api_keys["CORE_API_KEY"]

    async def search(self, search_query):
        headers = {
            "Authorization": f"Bearer {self.core_api_key}",
            "Accept": "application/json",
        }

        params = {
            "q": search_query,
            "limit": self.max_results,
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/search/works", headers=headers, json=params
            ) as response:
                if response.status == 200:
                    logger.info("CORE API request successful.")
                    response_json = await response.json()
                    return response_json
                else:
                    logger.warning(
                        f"CORE API request failed with status code: {response.status}"
                    )
                    return None

    async def search_and_parse(self, query):
        try:
            search_query = query["search_query"]
            response = await self.search(search_query)

            if response is None:
                logger.warning(f"Empty API response for query: {search_query}")
                return {}

            results = response.get("results", [])
            parsed_result = {}

            if results:
                print(results)
                entry = results[0]
                parsed_result = {
                    "DOI": entry.get("doi", ""),
                    "authors": [author["name"] for author in entry.get("authors", [])],
                    "citation_count": entry.get("citationCount", 0),
                    "journal": entry.get("publisher", ""),
                    "pdf_link": entry.get("downloadUrl", ""),
                    "publication_year": entry.get("publicationYear"),
                    "title": entry.get("title", ""),
                    "full_text": entry.get("fullText", ""),
                    "query_rationale": query["query_rationale"],
                }

            return parsed_result
        except Exception as e:
            logger.error(
                f"An error occurred while searching and parsing results for query: {search_query}. Error: {e}"
            )
            return {}

    async def search_and_parse_json(self, input_json):
        try:
            updated_json = {}
            for query_id, query in input_json.items():
                parsed_result = await self.search_and_parse(query)
                updated_json[query_id] = parsed_result
            return updated_json
        except Exception as e:
            logger.error(
                f"An error occurred while processing the input JSON. Error: {e}"
            )
            return {}


async def main():
    max_results = 1

    core_search = CORESearch(max_results)

    input_json = {
        "query_1": {
            "search_query": "climate change, water resources",
            "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic.",
        },
        "query_2": {
            "search_query": "water scarcity, (hydrologist OR water expert)",
            "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources.",
        },
        "query_3": {
            "search_query": "sea level rise, coastal erosion",
            "query_rationale": "This query is crucial to understand the impact of climate change on coastal regions and the resulting effects on global water resources.",
        },
        "query_4": {
            "search_query": "water conservation, climate change mitigation, environmental studies",
            "query_rationale": "This query is important to identify strategies for water conservation and their role in mitigating the effects of climate change on global water resources.",
        },
        "query_5": {
            "search_query": "glacier melting, cryosphere",
            "query_rationale": "This query is necessary to understand the impact of climate change on glaciers and the resulting effects on global water resources.",
        },
    }

    updated_json = await core_search.search_and_parse_json(input_json)

    # Remove the "full_text" key from each query result
    for query_result in updated_json.values():
        query_result.pop("full_text", None)

    print(json.dumps(updated_json, indent=2))


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
        <file>
        <name>llm_api_handler.py</name>
        <path>src\api\llm_api_handler.py</path>
        <content>
import asyncio
import aiohttp
import anthropic
import backoff
import tiktoken
import time
import cohere
import requests
from src.utils.misc_utils import get_api_keys
from openai import AsyncOpenAI

from src.utils.logger_config import get_logger

logger = get_logger(__name__)


def count_tokens(text, encoding_name="cl100k_base"):
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(text))
    return num_tokens


def clip_prompt(prompt, max_tokens, encoding_name="cl100k_base"):
    encoding = tiktoken.get_encoding(encoding_name)
    tokens = encoding.encode(prompt)
    if len(tokens) > max_tokens:
        clipped_tokens = tokens[:max_tokens]
        clipped_prompt = encoding.decode(clipped_tokens)
        return clipped_prompt
    return prompt


class RateLimiter:
    def __init__(self, rps, window_size):
        self.rps = rps
        self.window_size = window_size
        self.window_start = time.monotonic()
        self.request_count = 0
        self.semaphore = asyncio.Semaphore(rps)

    async def acquire(self):
        current_time = time.monotonic()
        elapsed_time = current_time - self.window_start
        if elapsed_time > self.window_size:
            self.window_start = current_time
            self.request_count = 0
        if self.request_count >= self.rps:
            await asyncio.sleep(self.window_size - elapsed_time)
            self.window_start = time.monotonic()
            self.request_count = 0
        self.request_count += 1
        await self.semaphore.acquire()

    def release(self):
        self.semaphore.release()


class LLM_APIHandler:
    def __init__(self, api_keys, session, rps=0.5, window_size=60):
        self.set_api_keys(api_keys)
        self.claude_rate_limiter = RateLimiter(rps, window_size)
        self.openai_rate_limiter = RateLimiter(rps, window_size)
        self.cohere_rate_limiter = RateLimiter(75, 60)  # 75 calls per minute
        self.llama_rate_limiter = RateLimiter(rps, window_size)
        self.qwen_rate_limiter = RateLimiter(rps, window_size)
        self.claude_client = anthropic.Anthropic(api_key=self.claude_api_key)
        self.cohere_client = cohere.Client(self.cohere_api_key)
        self.session = session

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()  # Close the session when done

    def set_api_keys(self, api_keys):
        self.claude_api_key = api_keys["CLAUDE_API_KEY"]
        self.openai_api_key = api_keys["OPENAI_API_KEY"]
        self.cohere_api_key = api_keys["COHERE_API_KEY"]
        self.together_api_key = api_keys["TOGETHER_API_KEY"]

    @backoff.on_exception(
        backoff.expo,
        (anthropic.APIError, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_opus_content(
        self,
        prompt,
        system_prompt=None,
        model="claude-3-opus-20240229",
        max_tokens=3000,
    ):
        await self.claude_rate_limiter.acquire()
        try:
            if model not in [
                "claude-3-opus-20240229",
                "claude-3-sonnet-20240229",
                "claude-3-haiku-20240307",
            ]:
                raise ValueError(f"Invalid model: {model}")
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            messages = [{"role": "user", "content": clipped_prompt}]
            if system_prompt is None:
                system_prompt = "Directly fulfill the user's request without preamble, paying very close attention to all nuances of their instructions."
            try:
                response = self.claude_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    system=system_prompt,
                    messages=messages,
                )
                return response.content[0].text
            except anthropic.APIError as e:
                logger.error(
                    f"Max retries reached. Unable to generate content with Claude API. Error: {e}. Moving on."
                )
                return None
        finally:
            self.claude_rate_limiter.release()

    @backoff.on_exception(
        backoff.expo,
        (anthropic.APIError, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_haiku_content(
        self,
        prompt,
        system_prompt=None,
        model="claude-3-haiku-20240307",
        max_tokens=3000,
    ):
        await self.claude_rate_limiter.acquire()
        try:
            if model not in [
                "claude-3-opus-20240229",
                "claude-3-sonnet-20240229",
                "claude-3-haiku-20240307",
            ]:
                raise ValueError(f"Invalid model: {model}")
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            messages = [{"role": "user", "content": clipped_prompt}]
            if system_prompt is None:
                system_prompt = "Directly fulfill the user's request without preamble, paying very close attention to all nuances of their instructions."
            try:
                response = self.claude_client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    system=system_prompt,
                    messages=messages,
                )
                return response.content[0].text
            except anthropic.APIError as e:
                logger.error(
                    f"Max retries reached. Unable to generate content with Claude API. Error: {e}. Moving on."
                )
                return None
        finally:
            self.claude_rate_limiter.release()

    async def generate_cohere_content(
        self,
        prompt,
        model="command-r",
        temperature=0.3,
        prompt_truncation="AUTO",
        connectors=None,
    ):
        await self.cohere_rate_limiter.acquire()
        try:
            clipped_prompt = clip_prompt(prompt, max_tokens=180000)
            response = self.cohere_client.chat(
                model=model,
                message=clipped_prompt,
                temperature=temperature,
                chat_history=[],
                prompt_truncation=prompt_truncation,
                connectors=connectors,
            )
            return response.text
        except cohere.CohereError as e:
            logger.error(
                f"Unable to generate content with Cohere API. Error: {e}. Moving on."
            )
            return None
        finally:
            self.cohere_rate_limiter.release()

    @backoff.on_exception(
        backoff.expo,
        (requests.exceptions.RequestException, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_llama_content(
        self,
        prompt,
        max_tokens=1024,
        temperature=0.5,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
    ):
        endpoint = "https://api.together.xyz/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.together_api_key}",
        }
        clipped_prompt = clip_prompt(prompt, max_tokens=5000)
        messages = [{"content": clipped_prompt, "role": "user"}]
        data = {
            "model": "meta-llama/Llama-3-70b-chat-hf",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "repetition_penalty": repetition_penalty,
            "stop": ["<|eot_id|>"],
            "messages": messages,
        }

        await self.llama_rate_limiter.acquire()  # Acquire the rate limiter
        try:
            async with self.session.post(
                endpoint, json=data, headers=headers
            ) as response:
                response.raise_for_status()
                result = await response.json()
                content = result["choices"][0]["message"]["content"]
                return content
        except (requests.exceptions.RequestException, ValueError, KeyError) as e:
            logger.error(
                f"Unable to generate content with Llama API. Error: {e}. Moving on."
            )
            return None
        finally:
            self.llama_rate_limiter.release()  # Release the rate limiter

    @backoff.on_exception(
        backoff.expo,
        (requests.exceptions.RequestException, ValueError),
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_qwen_content(
        self,
        prompt,
        max_tokens=2048,
        temperature=0.1,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
    ):
        endpoint = "https://api.together.xyz/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.together_api_key}",
        }
        clipped_prompt = clip_prompt(prompt, max_tokens=25000)
        messages = [{"content": clipped_prompt, "role": "user"}]
        data = {
            "model": "meta-llama/Llama-3-70b-chat-hf",
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "repetition_penalty": repetition_penalty,
            "stop": ["<|im_end|>", "<|im_start|>"],
            "messages": messages,
        }

        await self.qwen_rate_limiter.acquire()  # Acquire the rate limiter
        try:
            async with self.session.post(
                endpoint, json=data, headers=headers
            ) as response:
                response.raise_for_status()
                result = await response.json()
                content = result["choices"][0]["message"]["content"]
                return content
        except (requests.exceptions.RequestException, ValueError, KeyError) as e:
            logger.error(
                f"Unable to generate content with Llama API. Error: {e}. Moving on."
            )
            return None
        finally:
            self.qwen_rate_limiter.release()  # Release the rate limiter

    @backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=5,
        max_time=60,
        jitter=backoff.full_jitter,
    )
    async def generate_openai_content(self, prompt, model="gpt-4o"):
        client = AsyncOpenAI(api_key=self.openai_api_key)

        # Format the single prompt string into the correct structure expected by the API
        messages = [{"role": "user", "content": prompt}]

        try:
            response = await client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,
                top_p=1,
                n=1,
                stream=False,
                max_tokens=None,
                presence_penalty=0,
                frequency_penalty=0,
                user=None,
            )

            if response:
                # Assuming non-streaming response for simplicity
                return response.choices[0].message.content
            else:
                return "No response from API."

        except Exception as e:
            logger.error(
                f"Unable to generate content with OpenAI API. Error: {e}. Moving on."
            )
            return None


async def main():
    api_keys = get_api_keys()
    rps = 5  # Requests per second
    window_size = 60  # Window size in seconds
    async with aiohttp.ClientSession() as session:
        async with LLM_APIHandler(api_keys, session, rps, window_size) as api_handler:
            tasks = []

            # Llama API
            test_qwen = False
            if test_qwen:
                qwen_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_qwen_content(prompt=qwen_prompt))

            # OpenAI API
            test_openai = True
            if test_openai:
                openai_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_openai_content(openai_prompt))

            # Claude API
            test_claude = False
            if test_claude:
                claude_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_opus_content(claude_prompt))

            # Cohere API
            test_cohere = False
            if test_cohere:
                cohere_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_cohere_content(cohere_prompt))

            # Llama API
            test_llama = False
            if test_llama:
                llama_prompt = "What is the meaning of life?"
                tasks.append(api_handler.generate_llama_content(prompt=llama_prompt))

            responses = await asyncio.gather(*tasks)

            for response, api_name in zip(
                responses, ["Llama", "Cohere", "Claude", "OpenAI", "qwen"]
            ):
                if response is not None:
                    print(f"{api_name} Response:", response)


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
        <file>
        <name>openalex_search.py</name>
        <path>src\api\openalex_search.py</path>
        <content>
from src.utils.misc_utils import get_api_keys
import asyncio
import re
from src.api.llm_api_handler import LLM_APIHandler
from src.core.prompts import get_prompt
import aiohttp
import json

from src.utils.logger_config import get_logger

logger = get_logger(__name__)


class PaperRanker:
    def __init__(self, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)
        self.max_retries = max_retries

    async def process_query(self, query_key, query_data, point_context):
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                template_name="rank_papers",
                full_text=query_data["full_text"],
                point_context=point_context,
                query_rationale=query_data["query_rationale"],
            )
            try:
                print(f"Processing queries for {point_context}...")
                response = await self.llm_api_handler.generate_cohere_content(prompt)
                print(f"Response: {response}")
                if response is None:
                    logger.warning(
                        "Received None response from the Gemini API. Skipping query."
                    )
                    return None

                try:
                    # Extract the relevance score using the specified token format
                    relevance_score_match = re.search(
                        r"<<relevance>>(\d+\.\d+)<<relevance>>",
                        response,
                    )

                    if relevance_score_match:
                        relevance_score_str = relevance_score_match.group(1)
                        try:
                            relevance_score = float(relevance_score_str)
                            if relevance_score > 0.5:
                                logger.debug(f"Successfully processed query.")
                                return {
                                    "DOI": query_data["DOI"],
                                    "title": query_data["title"],
                                    "analysis": response,
                                    "relevance_score": relevance_score,
                                }
                            else:
                                logger.debug(
                                    f"Relevance score {relevance_score} is below the threshold. Skipping query."
                                )
                                return None
                        except ValueError:
                            logger.warning(
                                f"Extracted relevance score '{relevance_score_str}' is not a valid float. Retrying..."
                            )
                            retry_count += 1
                    else:
                        logger.warning(
                            f"No relevance score found between <|relevance|> tokens in the response for query {query_key}. Response: {response}"
                        )
                        retry_count += 1
                except Exception as e:
                    logger.warning(
                        f"Error extracting relevance score for query {query_key}: {str(e)}. Retrying..."
                    )
                    retry_count += 1
            except Exception as e:
                logger.exception(f"Error processing query {query_key}: {str(e)}")
                retry_count += 1

        logger.error(f"Max retries reached for query {query_key}. Skipping query.")
        return None

    async def process_queries(self, input_json, point_context):
        tasks = []
        for query_key, query_data in input_json.items():
            if not query_data.get("DOI") or not query_data.get("title"):
                logger.warning(
                    f"Discarding query {query_key} due to missing DOI or title."
                )
                continue
            task = asyncio.create_task(
                self.process_query(query_key, query_data, point_context)
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        output_json = {}
        for query_key, result in zip(input_json.keys(), results):
            if result and isinstance(result, dict):
                output_json[query_key] = result

        return output_json


async def main(input_json, point_context):
    async with aiohttp.ClientSession() as session:
        ranker = PaperRanker(session)
        logger.info("Starting paper ranking process...")
        output_json = await ranker.process_queries(input_json, point_context)
        logger.info("Paper ranking process completed.")
        return output_json


if __name__ == "__main__":
    input_json = {
        "query_1": {
            "DOI": "https://doi.org/10.1007/bf00281114",
            "authors": ["Jarrett Rj"],
            "citation_count": 156,
            "journal": "Diabetologia",
            "pdf_link": "https://link.springer.com/content/pdf/10.1007%2FBF00281114.pdf",
            "publication_year": 1984,
            "title": "Type 2 (non-insulin-dependent) diabetes mellitus and coronary heart disease ? chicken, egg or neither?",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query aims to understand the relationship between Type 2 diabetes and coronary heart disease in chickens.",
        },
        "query_2": {
            "DOI": "https://doi.org/10.1001/jamainternmed.2019.6969",
            "authors": [
                "Victor W. Zhong",
                "Linda Van Horn",
                "Philip Greenland",
                "Mercedes R. Carnethon",
                "Hongyan Ning",
                "John T. Wilkins",
                "Donald M. Lloyd‐Jones",
                "Norrina B. Allen",
            ],
            "citation_count": 221,
            "journal": "JAMA internal medicine",
            "pdf_link": "https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf",
            "publication_year": 2020,
            "title": "Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query investigates the associations between different types of meat intake, including poultry, and cardiovascular disease and mortality.",
        },
        "query_3": {
            "DOI": "https://doi.org/10.1016/s0034-5288(18)33737-8",
            "authors": ["S.F. Cueva", "H. Sillau", "Abel Valenzuela", "H. P. Ploog"],
            "citation_count": 181,
            "journal": "Research in Veterinary Science/Research in veterinary science",
            "publication_year": 1974,
            "title": "High Altitude Induced Pulmonary Hypertension and Right Heart Failure in Broiler Chickens",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query focuses on the effects of high altitude on pulmonary hypertension and right heart failure specifically in broiler chickens.",
        },
        "query_4": {
            "DOI": "https://doi.org/10.2307/1588087",
            "authors": ["Sherwin A. Hall", "Nicanor Machicao"],
            "citation_count": 58,
            "journal": "Avian diseases",
            "publication_year": 1968,
            "title": "Myocarditis in Broiler Chickens Reared at High Altitude",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query examines myocarditis in broiler chickens reared at high altitude, providing insight into heart disease in this specific context.",
        },
        "query_5": {
            "DOI": "https://doi.org/10.1038/srep14727",
            "authors": [
                "Huaguang Lu",
                "Yi Tang",
                "Patricia A. Dunn",
                "Eva Wallner-Pendleton",
                "Lin Lin",
                "Eric A. Knoll",
            ],
            "citation_count": 82,
            "journal": "Scientific reports",
            "pdf_link": "https://www.nature.com/articles/srep14727.pdf",
            "publication_year": 2015,
            "title": "Isolation and molecular characterization of newly emerging avian reovirus variants and novel strains in Pennsylvania, USA, 2011–2014",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query looks at the isolation and characterization of new avian reovirus variants and strains, which may have implications for chicken health.",
        },
    }
    point_context = "Heart disease in chickens."

    output_json = asyncio.run(main(input_json, point_context))
    print(json.dumps(output_json, indent=2))

        </content>
    </file>
        <file>
        <name>scopus_search.py</name>
        <path>src\api\scopus_search.py</path>
        <content>
"""
Scopus Search Program

This program performs searches on the Scopus API based on the provided JSON queries.
It retrieves relevant data for the first entry with successfully scraped full text,
including title, DOI, description, journal, authors, citation count, and full text.
The results are returned in the updated JSON format.

Usage:
    scopus_search = ScopusSearch(doi_scraper)
    updated_json = scopus_search.search_and_parse_json(input_json)

Parameters:
    - doi_scraper: A scraper object capable of retrieving full text content given a DOI.
    - input_json: A JSON object containing the search queries in the specified format.

Returns:
    - updated_json: The updated JSON object with the search results and additional data.
"""

import aiohttp
import asyncio
import json
import time
from collections import deque
from src.utils.misc_utils import prepare_text_for_json, get_api_keys
from src.utils.web_scraper import WebScraper

from src.utils.logger_config import get_logger

logger = get_logger(__name__)


class ScopusSearch:
    def __init__(self, doi_scraper, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.base_url = "http://api.elsevier.com/content/search/scopus"
        self.request_times = deque(maxlen=6)
        self.scraper = doi_scraper
        self.session = session
        self.max_retries = max_retries

    async def search(self, query, count=25, view="COMPLETE", response_format="json"):
        headers = {
            "X-ELS-APIKey": self.api_keys["SCOPUS_API_KEY"],
            "Accept": (
                "application/json"
                if response_format == "json"
                else "application/atom+xml"
            ),
        }

        params = {
            "query": query["search_query"].replace("\\", ""),
            "count": count,
            "view": view,
        }

        retry_count = 0
        while retry_count < self.max_retries:
            try:
                # Ensure compliance with the rate limit
                while True:
                    current_time = time.time()
                    if (
                        not self.request_times
                        or current_time - self.request_times[0] >= 1
                    ):
                        self.request_times.append(current_time)
                        break
                    else:
                        await asyncio.sleep(0.2)

                async with self.session.get(
                    self.base_url, headers=headers, params=params
                ) as response:
                    if response.status == 200:
                        logger.info("Scopus API request successful.")
                        if response_format == "json":
                            return await response.json()
                        else:
                            return await response.text()
                    else:
                        logger.warning(
                            f"Scopus API request failed with status code: {response.status}"
                        )
                        return None
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                retry_count += 1
                wait_time = 2**retry_count
                logger.warning(
                    f"Error occurred while making Scopus API request: {e}. Retrying in {wait_time} seconds... (Attempt {retry_count}/{self.max_retries})"
                )
                await asyncio.sleep(wait_time)  # Exponential backoff

        logger.error(
            f"Max retries ({self.max_retries}) exceeded. Unable to fetch data from the Scopus API for query: {query}"
        )
        return None

    async def search_and_parse(self, query, query_id, count=25, view="COMPLETE"):
        try:
            results = await self.search(query, count, view, response_format="json")

            if (
                results is None
                or "search-results" not in results
                or "entry" not in results["search-results"]
            ):
                logger.warning(f"No results found for query: {query}")
                return {}
            else:
                for entry in results["search-results"]["entry"]:
                    title = entry.get("dc:title")
                    doi = entry.get("prism:doi")
                    description = entry.get("dc:description")
                    journal = entry.get("prism:publicationName")
                    citation_count = entry.get("citedby-count", "0")
                    authors = [
                        author.get("authname")
                        for author in entry.get("author", [])
                        if author.get("authname") is not None
                    ]

                    full_text = None
                    if doi:
                        logger.info(f"Scraping full text for DOI: {doi}")
                        try:
                            full_text = await self.scraper.get_url_content(doi)
                            full_text = await prepare_text_for_json(full_text)
                            logger.info(
                                f"Full text scraped successfully for DOI: {doi}"
                            )
                        except Exception as e:
                            logger.warning(
                                f"Error occurred while scraping full text for DOI: {doi}. Error: {e}"
                            )
                            continue

                    parsed_result = {
                        "search_query": query["search_query"],
                        "query_rationale": query["query_rationale"],
                        "title": title,
                        "DOI": doi,
                        "description": description,
                        "journal": journal,
                        "authors": authors,
                        "citation_count": citation_count,
                        "full_text": full_text or "",
                    }

                    return parsed_result

                logger.warning(f"No full text successfully scraped for query: {query}")
                return {}
        except Exception as e:
            logger.error(
                f"An error occurred while searching and parsing results for query: {query}. Error: {e}"
            )
            return {}

    async def search_and_parse_json(self, input_json):
        try:
            updated_json = {}
            for query_id, query in input_json.items():
                parsed_result = await self.search_and_parse(query, query_id)
                updated_json[query_id] = parsed_result
            return json.dumps(updated_json, ensure_ascii=False)
        except Exception as e:
            logger.error(
                f"An error occurred while processing the input JSON. Error: {e}"
            )
            return json.dumps({})


async def main():
    async with aiohttp.ClientSession() as session:
        doi_scraper = WebScraper(session)
        # Create an instance of the ScopusSearch class
        scopus_search = ScopusSearch(doi_scraper, session)

        # Example usage
        input_json = {
            "query_1": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND pathology)",
                "query_rationale": "This query is essential to understand the pathology of heart disease in chickens, providing a foundation for further investigation.",
            },
            "query_2": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND epidemiology)",
                "query_rationale": "This query is necessary to identify the prevalence and distribution of heart disease in chicken populations, informing strategies for disease control and prevention.",
            },
            "query_3": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND nutrition)",
                "query_rationale": "This query is important to explore the relationship between nutrition and heart disease in chickens, potentially identifying dietary factors that contribute to the development of the disease.",
            },
            "query_4": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND genetics)",
                "query_rationale": "This query is crucial to investigate the genetic factors that predispose chickens to heart disease, enabling the development of breeding programs that reduce the incidence of the disease.",
            },
            "query_5": {
                "search_query": "TITLE-ABS-KEY(heart disease AND chickens AND diagnosis)",
                "query_rationale": "This query is essential to identify effective diagnostic methods for heart disease in chickens, ensuring accurate and timely detection of the disease.",
            },
        }

        # Call the search_and_parse_json method
        updated_json = await scopus_search.search_and_parse_json(input_json)
        print(updated_json)


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
        <file>
        <name>__init__.py</name>
        <path>src\api\__init__.py</path>
        <content>

        </content>
    </file>
    </directory>
    <directory name="config">
        <file>
        <name>settings.py</name>
        <path>src\config\settings.py</path>
        <content>
import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
PROMPTS_DIR = os.path.join(BASE_DIR, 'prompts')
CONFIG_DIR = os.path.join(BASE_DIR, 'config')

        </content>
    </file>
    </directory>
    <directory name="core">
        <file>
        <name>paper_analyzer.py</name>
        <path>src\core\paper_analyzer.py</path>
        <content>
from src.utils.misc_utils import get_api_keys
import asyncio
import re
from src.api.llm_api_handler import LLM_APIHandler
from src.core.prompts import get_prompt
import aiohttp
import json

from src.utils.logger_config import get_logger

logger = get_logger(__name__)


class PaperRanker:
    def __init__(self, session, max_retries=4):
        self.api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)
        self.max_retries = max_retries

    async def process_query(self, query_key, query_data, point_context):
        retry_count = 0
        while retry_count < self.max_retries:
            prompt = get_prompt(
                template_name="rank_papers",
                full_text=query_data.get("full_text", ""),
                point_context=point_context,
                query_rationale=query_data.get("query_rationale", ""),
            )
            try:
                print(f"Processing queries for {point_context}...")
                response = await self.llm_api_handler.generate_cohere_content(prompt)
                print(f"Response: {response}")
                if response is None:
                    logger.warning(
                        "Received None response from the Gemini API. Skipping query."
                    )
                    return None

                try:
                    # Extract the relevance score using the specified token format
                    relevance_score_match = re.search(
                        r"<<relevance>>(\d+\.\d+)<<relevance>>",
                        response,
                    )

                    if relevance_score_match:
                        relevance_score_str = relevance_score_match.group(1)
                        try:
                            relevance_score = float(relevance_score_str)
                            if relevance_score > 0.5:
                                logger.debug(f"Successfully processed query.")
                                return {
                                    "DOI": query_data.get("DOI", ""),
                                    "title": query_data.get("title", ""),
                                    "analysis": response,
                                    "relevance_score": relevance_score,
                                }
                            else:
                                logger.debug(
                                    f"Relevance score {relevance_score} is below the threshold. Skipping query."
                                )
                                return None
                        except ValueError:
                            logger.warning(
                                f"Extracted relevance score '{relevance_score_str}' is not a valid float. Retrying..."
                            )
                            retry_count += 1
                    else:
                        logger.warning(
                            f"No relevance score found between <|relevance|> tokens in the response for query {query_key}. Response: {response}"
                        )
                        retry_count += 1
                except Exception as e:
                    logger.warning(
                        f"Error extracting relevance score for query {query_key}: {str(e)}. Retrying..."
                    )
                    retry_count += 1
            except Exception as e:
                logger.exception(f"Error processing query {query_key}: {str(e)}")
                retry_count += 1

        logger.error(f"Max retries reached for query {query_key}. Skipping query.")
        return None

    async def process_queries(self, input_json, point_context):
        tasks = []
        for query_key, query_data in input_json.items():
            task = asyncio.create_task(
                self.process_query(query_key, query_data, point_context)
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)
        output_json = {}
        for query_key, result in zip(input_json.keys(), results):
            if result and isinstance(result, dict):
                output_json[query_key] = result

        return output_json


async def main(input_json, point_context):
    async with aiohttp.ClientSession() as session:
        ranker = PaperRanker(session)
        logger.info("Starting paper ranking process...")
        output_json = await ranker.process_queries(input_json, point_context)
        logger.info("Paper ranking process completed.")
        return output_json


if __name__ == "__main__":
    input_json = {
        "query_1": {
            "DOI": "https://doi.org/10.1007/bf00281114",
            "authors": ["Jarrett Rj"],
            "citation_count": 156,
            "journal": "Diabetologia",
            "pdf_link": "https://link.springer.com/content/pdf/10.1007%2FBF00281114.pdf",
            "publication_year": 1984,
            "title": "Type 2 (non-insulin-dependent) diabetes mellitus and coronary heart disease ? chicken, egg or neither?",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query aims to understand the relationship between Type 2 diabetes and coronary heart disease in chickens.",
        },
        "query_2": {
            "DOI": "https://doi.org/10.1001/jamainternmed.2019.6969",
            "authors": [
                "Victor W. Zhong",
                "Linda Van Horn",
                "Philip Greenland",
                "Mercedes R. Carnethon",
                "Hongyan Ning",
                "John T. Wilkins",
                "Donald M. Lloyd‐Jones",
                "Norrina B. Allen",
            ],
            "citation_count": 221,
            "journal": "JAMA internal medicine",
            "pdf_link": "https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2759737/jamainternal_zhong_2020_oi_190112.pdf",
            "publication_year": 2020,
            "title": "Associations of Processed Meat, Unprocessed Red Meat, Poultry, or Fish Intake With Incident Cardiovascular Disease and All-Cause Mortality",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query investigates the associations between different types of meat intake, including poultry, and cardiovascular disease and mortality.",
        },
        "query_3": {
            "DOI": "https://doi.org/10.1016/s0034-5288(18)33737-8",
            "authors": ["S.F. Cueva", "H. Sillau", "Abel Valenzuela", "H. P. Ploog"],
            "citation_count": 181,
            "journal": "Research in Veterinary Science/Research in veterinary science",
            "publication_year": 1974,
            "title": "High Altitude Induced Pulmonary Hypertension and Right Heart Failure in Broiler Chickens",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query focuses on the effects of high altitude on pulmonary hypertension and right heart failure specifically in broiler chickens.",
        },
        "query_4": {
            "DOI": "https://doi.org/10.2307/1588087",
            "authors": ["Sherwin A. Hall", "Nicanor Machicao"],
            "citation_count": 58,
            "journal": "Avian diseases",
            "publication_year": 1968,
            "title": "Myocarditis in Broiler Chickens Reared at High Altitude",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query examines myocarditis in broiler chickens reared at high altitude, providing insight into heart disease in this specific context.",
        },
        "query_5": {
            "DOI": "https://doi.org/10.1038/srep14727",
            "authors": [
                "Huaguang Lu",
                "Yi Tang",
                "Patricia A. Dunn",
                "Eva Wallner-Pendleton",
                "Lin Lin",
                "Eric A. Knoll",
            ],
            "citation_count": 82,
            "journal": "Scientific reports",
            "pdf_link": "https://www.nature.com/articles/srep14727.pdf",
            "publication_year": 2015,
            "title": "Isolation and molecular characterization of newly emerging avian reovirus variants and novel strains in Pennsylvania, USA, 2011–2014",
            "full_text": "This is the full text of the paper...",
            "query_rationale": "This query looks at the isolation and characterization of new avian reovirus variants and strains, which may have implications for chicken health.",
        },
    }
    point_context = "Heart disease in chickens."

    output_json = asyncio.run(main(input_json, point_context))
    print(json.dumps(output_json, indent=2))

        </content>
    </file>
        <file>
        <name>prompts.py</name>
        <path>src\core\prompts.py</path>
        <content>
import re
import os
from src.config import settings

scopus_search_guide = """
Syntax and Operators

Valid syntax for advanced search queries includes:

Field codes (e.g. TITLE, ABS, KEY, AUTH, AFFIL) to restrict searches to specific parts of documents
Boolean operators (AND, OR, AND NOT) to combine search terms
Proximity operators (W/n, PRE/n) to find words within a specified distance - W/n: Finds terms within "n" words of each other, regardless of order. Example: journal W/15 publishing finds articles where "journal" and "publishing" are within two words of each other. - PRE/n: Finds terms in the specified order and within "n" words of each other. Example: data PRE/50 analysis finds articles where "data" appears before "analysis" within three words. - To find terms in the same sentence, use 15. To find terms in the same paragraph, use 50 -
Quotation marks for loose/approximate phrase searches
Braces {{}} for exact phrase searches (without hte backslashes of course)
Wildcards (*) to capture variations of search terms
Invalid syntax includes:

Mixing different proximity operators (e.g. W/n and PRE/n) in the same expression
Using wildcards or proximity operators with exact phrase searches
Placing AND NOT before other Boolean operators
Using wildcards on their own without any search terms
Ideal Search Structure

An ideal advanced search query should:

Use field codes to focus the search on the most relevant parts of documents
Combine related concepts using AND and OR
Exclude irrelevant terms with AND NOT at the end
Employ quotation marks and braces appropriately for phrase searching
Include wildcards to capture variations of key terms (while avoiding mixing them with other operators)
Follow the proper order of precedence for operators
Complex searches should be built up systematically, with parentheses to group related expressions as needed. The information from the provided documents on syntax rules and operators should be applied rigorously.

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **

Example Advanced Searches

{{
"query_1": "TITLE-ABS-KEY(("precision agriculture" OR "precision farming") AND ("machine learning" OR "AI") AND "water")",
"query_2": "TITLE-ABS-KEY((iot OR "internet of things") AND (irrigation OR watering) AND sensor*)",
"query_3": "TITLE-ABS-Key(("precision farming" OR "precision agriculture") AND ("deep learning" OR "neural networks") AND "water")",
"query_4": "TITLE-ABS-KEY((crop W/5 monitor*) AND "remote sensing" AND (irrigation OR water*))",
"query_5": "TITLE("precision irrigation" OR "variable rate irrigation" AND "machine learning")"
}}

** Critical: all double quotes other than the outermost ones should be preceded by a backslash (") to escape them in the JSON format. Failure to do so will result in an error when parsing the JSON string. **. 

These example searches demonstrate different ways to effectively combine key concepts related to precision agriculture, irrigation, real-time monitoring, IoT, machine learning and related topics using advanced search operators. They make use of field codes, Boolean and proximity operators, phrase searching, and wildcards to construct targeted, comprehensive searches to surface the most relevant research. The topic focus is achieved through carefully chosen search terms covering the desired themes.
"""

alex_search_guide = """
Syntax and Operators
Valid syntax for advanced alex search queries includes:
Using quotation marks %22%22 for exact phrase matches
Adding a minus sign - before terms to exclude them
Employing the OR operator in all caps to find pages containing either term
Using the site%3A operator to limit results to a specific website
Applying the filetype%3A operator to find specific file formats like PDF, DOC, etc.
Adding the * wildcard as a placeholder for unknown words
`
Invalid syntax includes:
Putting a plus sign + before words (alex stopped supporting this)
Using other special characters like %3F, %24, %26, %23, etc. within search terms
Explicitly using the AND operator (alex's default behavior makes it redundant)

Ideal Search Structure
An effective alex search query should:
Start with the most important search terms
Use specific, descriptive keywords related to irrigation scheduling, management, and precision irrigation
Utilize exact phrases in %22quotes%22 for specific word combinations
Exclude irrelevant terms using the - minus sign
Connect related terms or synonyms with OR
Apply the * wildcard strategically for flexibility
Note:

By following these guidelines and using proper URL encoding, you can construct effective and accurate search queries for alex.

Searches should be concise yet precise, following the syntax rules carefully. 

Example Searches
{{
"query_1": "https://api.openalex.org/works?search=%22precision+irrigation%22+%2B%22soil+moisture+sensors%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"query_2": "https://api.openalex.org/works?search=%22machine+learning%22+%2B%22irrigation+management%22+%2B%22crop+water+demand+prediction%22&sort=relevance_score:desc&per-page=30",
"query_3": "https://api.openalex.org/works?search=%22IoT+sensors%22+%2B%22real-time%22+%2B%22soil+moisture+monitoring%22+%2B%22crop+water+stress%22&sort=relevance_score:desc&per-page=30",
"query_4": "https://api.openalex.org/works?search=%22remote+sensing%22+%2B%22vegetation+indices%22+%2B%22irrigation+scheduling%22&sort=relevance_score:desc&per-page=30",
"query_5": "https://api.openalex.org/works?search=%22wireless+sensor+networks%22+%2B%22precision+agriculture%22+%2B%22variable+rate+irrigation%22+%2B%22irrigation+automation%22&sort=relevance_score:desc&per-page=30"
}}

These example searches demonstrate how to create targeted, effective alex searches. They focus on specific topics, exclude irrelevant results, allow synonym flexibility, and limit to relevant domains when needed. The search terms are carefully selected to balance relevance and specificity while avoiding being overly restrictive.  By combining relevant keywords, exact phrases, and operators, these searches help generate high-quality results for the given topics.
"""

core_search_guide = """
### CORE API Search Guide: Formulating Queries in JSON Format

This guide provides a structured approach to creating effective search queries using the CORE API. The guide emphasizes the JSON format to ensure clarity and precision in your search queries.

#### Syntax and Operators

**Valid Syntax for CORE API Queries:**
- **Field-specific searches**: Direct your query to search within specific fields like `title`, `author`, or `subject`.
- **Boolean Operators**: Use `AND`, `OR`, and `NOT` to combine or exclude terms.
- **Grouping**: Use parentheses `()` to structure the query and define the order of operations.
- **Range Queries**: Specify ranges for dates or numerical values with `>`, `<`, `>=`, `<=`.
- **Existence Check**: Use `_exists_` to filter results based on the presence of data in specified fields.

**Invalid Syntax:**
- **Inconsistencies in field names**: Ensure field names are correctly spelled and appropriate for the data type.
- **Improper boolean logic**: Avoid illogical combinations that nullify the search criteria (e.g., `AND NOT` used incorrectly).

#### Ideal Query Structure

Your search queries should:
1. **Use Field-Specific Filters**: Focus your search on the most relevant attributes.
2. **Combine Keywords Effectively**: Use logical operators to refine and broaden your searches.
3. **Employ Grouping and Range Queries** where complex relationships or specific time frames are needed.

#### Example Advanced Searches in JSON Format

Here are examples of structured queries formatted in JSON, demonstrating different ways to effectively combine search criteria using the CORE API:

```json
{
    "query_1": {
        "search_query": "climate change, water resources",
        "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic.",
    },
    "query_2": {
        "search_query": "water scarcity, (hydrologist OR water expert)",
        "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources.",
    },
    "query_3": {
        "search_query": "sea level rise, coastal erosion",
        "query_rationale": "This query is crucial to understand the impact of climate change on coastal regions and the resulting effects on global water resources.",
    },
    "query_4": {
        "search_query": "water conservation, climate change mitigation, environmental studies",
        "query_rationale": "This query is important to identify strategies for water conservation and their role in mitigating the effects of climate change on global water resources.",
    },
    "query_5": {
        "search_query": "glacier melting, cryosphere",
        "query_rationale": "This query is necessary to understand the impact of climate change on glaciers and the resulting effects on global water resources.",
    },
}
```

### Critical Considerations

- **Escape Characters**: When using JSON format, ensure that all double quotes inside JSON values are properly escaped using backslashes (`\"`) to prevent parsing errors.
- **Complexity**: As queries become more complex, ensure they are still readable and maintainable. Use whitespace and indentation in JSON to enhance clarity.

These examples illustrate how to utilize the CORE API's flexible query capabilities to target specific fields, combine search terms logically, and exclude irrelevant data. By following these guidelines and adapting the examples to your research needs, you can efficiently leverage the CORE API to access a vast range of academic materials.
"""


def remove_illegal_characters(text):
    if text is None:
        return ""
    illegal_chars = re.compile(r"[\000-\010]|[\013-\014]|[\016-\037]")
    return illegal_chars.sub("", str(text))


def get_prompt(template_name, **kwargs):
    prompts = {
        "generate_queries": """
<documents>
<document index="1">
<source>search_query_prompt.txt</source>
<document_content>
<instructions>
Review the user's main query: '{user_query}'. Break down this query into distinct sub-queries that address different aspects necessary to fully answer the main query. 
For each sub-query, provide a rationale explaining why it is essential. Format these sub-queries according to the directions in <search_guidance>. structure your response as a json with detailed search queries, each accompanied by its rationale. 
The output should adhere to this format:
{{
  "query_1": {{
    "search_query": "unique query following the provided search guidance",
    "query_rationale": "This query is essential to understand the overall impact of climate change on global water resources, providing a broad understanding of the topic."
  }},
  "query_2": {{
    "search_query": "unique query following the provided search guidance",
    "query_rationale": "This query is necessary to identify areas with high water scarcity and how climate change affects the global distribution of water resources."
  }},
  ...
}}
**Note: Only generate as many sub-queries and rationales as necessary to thoroughly address the main query, up to a maximum of 10. Each sub-query must directly contribute to unraveling the main query's aspects.
</instructions>
<search_guidance>
{search_guidance}
</search_guidance>
</resources>
</document_content>
</document>
</documents>
""",
        "rank_papers": """
<instructions>
Analyze the paper's relevance to {point_context} from the {query_rationale} perspective. 

Begin your response with the relevance score between the following tokens:
<<relevance>>x.x<<relevance>>
The relevance score should be a decimal between 0.0 and 1.0, with 1.0 being the most relevant. If there is not enough information to determine relevance, assign a score of 0.0.
Examples:
- Correct: "<<relevance>>0.9<<relevance>>"
- Correct: "<<relevance>>0.3<<relevance>>"
- Correct: "<<relevance>>0.0<<relevance>>"
After providing the relevance score, include the following in your analysis:

Include in your analysis:
- Verbatim extracts: key terms, research questions, methods, results, tables, figures, quotes, conclusions
- Explanation of study purpose and objectives
- Relevance evaluation to the specified point
- Limitations for addressing the point
- Free-form extraction with relevant verbatim information (any format). Extract as much verbatim information as needed to support your analysis.

End with this JSON:
<response_format>
{{
  "inline_citation": "<author surname>, <year>",
  "apa_citation": "<full APA citation>",
  "study_location": "<city/region, country>", 
  "main_objective": "<main objective>",
  "technologies_used": "<technology 1>, <technology 2>, ...",
  "data_sources": "<data source 1>, <data source 2>, ...",
  "key_findings": "<key findings summary>"
}}
</response_format>

Begin your response with: <<relevance>>
</instructions>

<full_text>
{full_text}
</full_text>
""",
        "synthesize_results": """
<prompt>
    <expert_description>
        As an expert polymath, you are tasked with utilizing provided materials and any relevant data to construct a detailed and technically rigorous response akin to high-level research analysis. Your response should include all pertinent data, facts, and figures that contribute to a comprehensive analysis, reflecting the depth of understanding expected from a seasoned researcher or PhD holder. Ensure that no details are overlooked.
    </expert_description>
    <user_query>
        {user_query}
    </user_query>
    <returned_results>
        {returned_results}
    </returned_results>
    <response_format>
        Provide a comprehensive, structured response that rigorously analyzes and interprets the data and insights from the provided research materials and any other relevant information. When citing research papers, include inline citations with hyperlinks to the DOI or PDF link where available, or plain text citations otherwise. Conclude with a full citation of all referenced papers in the bibliography section apa style. 
        Structure your response in a clear, logical manner, focusing on technical accuracy and depth to thoroughly answer the user's query based on the provided data. Keep your answer tightly focused on the user's query and only include relevant/pertinent information. Begin your answer without preamble. Format the entire response in valid HTML, using appropriate tags such as `<h1>`, `<h2>`, `<p>`, `<ul>`, etc., to ensure proper rendering on web pages.
    </response_format>
    <critical-points>
        - Only include and discuss sources that are directly relevant to the user query.
        - Sources which are not directly relevant to the user query should not be included in the response.
        - Aim for a full and comprehensive breakdown of the topic, define the most important concepts, and provide detailed explanations.
    </critical-points>
</prompt>

""",
    }
    
    # If loading external prompt files
    prompt_file_path = os.path.join(settings.PROMPTS_DIR, f"{template_name}.txt")
    if os.path.exists(prompt_file_path):
        with open(prompt_file_path, 'r') as f:
            return f.read().format(**kwargs)
    
    try:
        return prompts[template_name].format(**kwargs)

    except KeyError as e:
        missing_key = str(e).strip("'")
        raise ValueError(
            f"Missing argument for template '{template_name}': {missing_key}"
        )

        </content>
    </file>
        <file>
        <name>query_generator.py</name>
        <path>src\core\query_generator.py</path>
        <content>
import json
import asyncio
import aiohttp

from src.api.llm_api_handler import LLM_APIHandler
from src.core.prompts import get_prompt, core_search_guide
from src.utils.misc_utils import get_api_keys

from src.utils.logger_config import get_logger

logger = get_logger(__name__)


class QueryGenerator:
    def __init__(self, session):
        self.api_keys = get_api_keys()
        self.session = session
        self.llm_api_handler = LLM_APIHandler(self.api_keys, session)

    async def generate_queries(self, user_query):
        # Generate the prompt from the user's query
        prompt = get_prompt(
            template_name="generate_queries",
            user_query=user_query,
            search_guidance=core_search_guide,
        )

        # Obtain the model's response for the generated prompt
        response = await self.llm_api_handler.generate_openai_content(prompt)

        # Parse and return the response
        return self.parse_response(response)

    def parse_response(self, response):
        try:
            # Attempt to find and extract the JSON structured data
            start_index = response.find("{")
            end_index = response.rfind("}") + 1
            if start_index != -1 and end_index != -1:
                json_string = response[start_index:end_index]
                return json.loads(json_string)
            else:
                print("No valid JSON object found.")
                return {}
        except json.JSONDecodeError as e:
            print(f"Error parsing JSON: {e}")
            return {}


async def main():
    async with aiohttp.ClientSession() as session:
        processor = QueryGenerator(session)
        user_query = "Impact of climate change on global water resources"
        queries = await processor.generate_queries(user_query)
        print(json.dumps(queries, indent=2))


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
        <file>
        <name>result_synthesizer.py</name>
        <path>src\core\result_synthesizer.py</path>
        <content>
import asyncio
import logging
from src.api.llm_api_handler import LLM_APIHandler
from src.core.prompts import get_prompt
import aiohttp
from src.utils.misc_utils import get_api_keys

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QueryProcessor:
    def __init__(self, session):
        api_keys = get_api_keys()
        self.llm_api_handler = LLM_APIHandler(api_keys, session)

    async def process_query(self, user_query, returned_results):
        prompt = get_prompt(
            template_name="synthesize_results",
            user_query=user_query,
            returned_results=returned_results,
        )

        try:
            logger.info(f"Processing query: {user_query}")
            response = await self.llm_api_handler.generate_openai_content(prompt)
            return response
        except Exception as e:
            logger.error(f"Error processing query: {e}")
            return None


async def main():
    async with aiohttp.ClientSession() as session:
        query_processor = QueryProcessor(session)
        user_query = "What are the latest advancements in artificial intelligence?"
        returned_results = [
            "AI has made significant progress in fields such as computer vision and natural language processing.",
            "Deep learning techniques have revolutionized AI, enabling machines to learn from large datasets.",
            "AI is being applied in various domains, including healthcare, finance, and autonomous vehicles.",
        ]

        logger.info("Starting query processing...")
        response = await query_processor.process_query(user_query, returned_results)
        if response:
            logger.info(f"Response: {response}")
        else:
            logger.warning("Failed to generate a response.")
        logger.info("Query processing completed.")


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
        <file>
        <name>__init__.py</name>
        <path>src\core\__init__.py</path>
        <content>

        </content>
    </file>
    </directory>
    <directory name="ui">
        <file>
        <name>user_interface_local.py</name>
        <path>src\ui\user_interface_local.py</path>
        <content>
# utils/user_interface.py
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import gradio as gr
import json
import aiohttp
import logging
from src.core.query_generator import QueryGenerator
from src.api.scopus_search import ScopusSearch
from src.core.paper_analyzer import PaperRanker
from src.core.result_synthesizer import QueryProcessor
from src.utils.web_scraper import WebScraper
from src.api.core_search import CORESearch
from src.utils.misc_utils import get_api_keys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("research_query_processor.log"),
        logging.StreamHandler(),
    ],
)


class ResearchQueryProcessor:
    def __init__(self):
        self.api_keys = get_api_keys()
        self.session = None

    async def chatbot_response(self, message, history):
        async with aiohttp.ClientSession() as session:
            self.session = session
            logging.info(f"Received message: {message}")

            yield "Generating search queries..."
            query_generator = QueryGenerator(self.session)
            search_queries = await query_generator.generate_queries(message)
            logging.info(f"Generated search queries: {search_queries}")

            yield "Searching in Scopus..."
            doi_scraper = WebScraper(self.session)
            scopus_search = ScopusSearch(doi_scraper, self.session)
            search_results = await scopus_search.search_and_parse_json(search_queries)
            search_results = json.loads(search_results)
            logging.info(f"Scopus search results: {search_results}")

            yield "Analyzing papers..."
            paper_ranker = PaperRanker(self.session)
            analyzed_papers = await paper_ranker.process_queries(
                search_results, message
            )
            logging.info(f"Analyzed papers: {analyzed_papers}")

            yield "Synthesizing results..."
            query_processor = QueryProcessor(self.session)
            synthesized_results = await query_processor.process_query(
                message, analyzed_papers
            )
            logging.info(f"Synthesized results: {synthesized_results}")

            yield f"{synthesized_results}"


def create_app():
    processor = ResearchQueryProcessor()

    chat_interface = gr.ChatInterface(
        fn=processor.chatbot_response,
        title="Literature Review Agent",
        description="Enter your research query below.",
        theme=gr.themes.Soft(),
    )

    return chat_interface


# Run the app
if __name__ == "__main__":
    app = create_app()
    app.launch()

        </content>
    </file>
        <file>
        <name>__init__.py</name>
        <path>src\ui\__init__.py</path>
        <content>

        </content>
    </file>
    </directory>
    <directory name="utils">
        <file>
        <name>logger_config.py</name>
        <path>src\utils\logger_config.py</path>
        <content>
import logging
import sys

# Configure the base logging, it applies to all loggers created
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(name)s] %(levelname)s: %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],  # Ensure logs go to stdout
)


def get_logger(module_name):
    """
    Configure and provide a logger with the given module name.

    Parameters:
    - module_name (str): The name of the module requesting the logger.

    Returns:
    - logging.Logger: Configured logger instance with the specified module name.
    """
    logger = logging.getLogger(module_name)
    return logger

        </content>
    </file>
        <file>
        <name>web_scraper.py</name>
        <path>src\utils\web_scraper.py</path>
        <content>
import asyncio
import random
import aiohttp
from playwright.async_api import async_playwright
from fake_useragent import UserAgent
import logging
import sys


class WebScraper:
    def __init__(self, session, max_concurrent_tasks=120):
        self.semaphore = asyncio.Semaphore(max_concurrent_tasks)
        self.user_agent = UserAgent()
        self.browser = None
        self.session = session
        self.logger = logging.getLogger(__name__)

    async def initialize(self):
        try:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
            self.logger.info("Browser initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize browser: {str(e)}")
            raise

    async def close(self):
        if self.browser:
            await self.browser.close()
            self.logger.info("Browser closed")

    async def scrape_url(self, url, max_retries=3):
        if not self.browser:
            await self.initialize()  # Ensure the browser is initialized

        retry_count = 0
        while retry_count < max_retries:
            try:
                context = await self.browser.new_context(
                    user_agent=self.user_agent.random,
                    viewport={"width": 1920, "height": 1080},
                    ignore_https_errors=True,
                )
                page = await context.new_page()
                await self.navigate_to_url(page, url)
                content = await self.extract_text_content(page)
                self.logger.info(f"Successfully scraped URL: {url}")
                await page.close()
                await context.close()
                return content
            except Exception as e:
                self.logger.error(
                    f"Error occurred while scraping URL: {url}. Error: {str(e)}"
                )
                retry_count += 1
                await asyncio.sleep(
                    random.uniform(1, 5)
                )  # Random delay between retries
            finally:
                try:
                    await page.close()
                    await context.close()
                except Exception as e:
                    self.logger.warning(
                        f"Error occurred while closing page or context: {str(e)}"
                    )
        self.logger.warning(f"Max retries exceeded for URL: {url}")
        return ""

    async def get_url_content(self, url):
        async with self.semaphore:
            return await self.scrape_url(url)

    async def navigate_to_url(self, page, url, max_retries=3):
        if not url.startswith("http"):
            url = f"https://doi.org/{url}"
        retry_count = 0
        while retry_count < max_retries:
            try:
                await page.goto(url, wait_until="networkidle", timeout=30000)
                await asyncio.sleep(1)  # Minor delay to ensure page loads completely
                return
            except Exception as e:
                self.logger.warning(
                    f"Retrying URL: {url}. Remaining retries: {max_retries - retry_count}"
                )
                retry_count += 1
                await asyncio.sleep(
                    random.uniform(1, 5)
                )  # Random delay between retries
        self.logger.error(
            f"Failed to navigate to URL: {url} after {max_retries} retries"
        )
        raise e

    async def extract_text_content(self, page):
        try:
            paragraphs = await page.query_selector_all("p")
            text_content = "".join(
                [await paragraph.inner_text() + "\n" for paragraph in paragraphs]
            )
            return text_content.strip()
        except Exception as e:
            self.logger.error(f"Failed to extract text content. Error: {str(e)}")
            return ""


# Usage
async def main():
    log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler("scraper.log"),
            logging.StreamHandler(sys.stdout),
        ],
    )

    async with aiohttp.ClientSession() as session:
        scraper = WebScraper(session=session)
        try:
            await scraper.initialize()
        except Exception as e:
            logging.error(f"Initialization failed: {e}")
            return  # Early return if initialization fails

        urls = [
            "10.1016/j.ifacol.2020.12.237",
            "10.1016/j.agwat.2023.108536",
            "10.1016/j.atech.2023.100251",
            "10.1016/j.atech.2023.100179",
            "10.1016/j.ifacol.2023.10.677",
            "10.1016/j.ifacol.2023.10.1655",
            "10.1016/j.ifacol.2023.10.667",
            "10.1002/cjce.24764",
            "10.3390/app13084734",
            "10.1016/j.atech.2022.100074",
            "10.1007/s10668-023-04028-9",
            "10.1109/IJCNN54540.2023.10191862",
            "10.1201/9780429290152-5",
            "10.1016/j.jprocont.2022.10.003",
            "10.1016/j.rser.2022.112790",
            "10.1007/s11269-022-03191-4",
            "10.3390/app12094235",
            "10.3390/w14060889",
            "10.3390/su14031304",
        ]

        scrape_tasks = []
        for url in urls:
            scrape_task = asyncio.create_task(scraper.get_url_content(url))
            scrape_tasks.append(scrape_task)

        scraped_contents = await asyncio.gather(*scrape_tasks)

        for url, content in zip(urls, scraped_contents):
            logging.info(f"Scraped content for URL: {url}")
            logging.info(f"Content: {content}")

        await scraper.close()


if __name__ == "__main__":
    asyncio.run(main())

        </content>
    </file>
        <file>
        <name>__init__.py</name>
        <path>src\utils\__init__.py</path>
        <content>

        </content>
    </file>
    </directory>
</repository_structure>


Additional Information:
This context file contains the structure and content of the repository, excluding certain directories as specified in FOLDER_EXCLUDE: {'__pycache__', 'archive', '.git', '.venv', 'node_modules'}. Files with extensions in FILE_EXTENSION_EXCLUDE: {'.exe', '.so', '.pyc', '.dll'} are listed but their content is not included. Sensitive information in .env files has been obfuscated for security purposes. Use this context for reference and analysis of the project structure and content.