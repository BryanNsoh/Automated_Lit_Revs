- analysis: '>'
  authors:
  - Eron F.
  - Noman M.
  - de Oliveira R.R.
  - Chalfun-Junior A.
  citation_count: '0'
  description: Earlier, researchers have employed computer vision-based automatic
    fruit detection to estimate coffee yield at the time of harvest, however, studies
    on the on-plant quantification of the coffee fruit are scarce. In this study,
    the latest version of the state-of-the-art algorithm YOLOv7 (You Only Look Once)
    was used for the first time. YOLOv7 was trained with 324 annotated images of fruit
    bearing coffee branches followed by its evaluation with 82 annotated images as
    validation data (supervised method) and then tested through raw images (unannotated)
    as test data. Meanwhile, the K-means models were trained which led to machine-generated
    color classes of coffee fruit for semi-supervised image annotation. Consequently,
    the developed model efficiently analyzed the test data with an mAP@.5 (mean average
    precision) of 0.89. Strikingly, our innovative semi-supervised method with an
    mAP@.5 of 0.77 for multi-class mode surpassed the supervised method which had
    mAP@.5 of only 0.60, leading to faster and more accurate annotation. While testing
    the yield estimation of the model in two plots, an average error of 3.78% was
    recorded between the predicted (pre-harvest) and ground truth (harvest) data for
    binary class mode. The average error for multi-class data was recorded as 3.87%
    for green, 3.445% for green-yellow, 5.09% for cherry-raisin, and 2.51% for dry
    fruits. This AI-based technology when integrated with other tools such as UAV
    would efficiently remotely monitor coffee field for informed decision about irrigation,
    fertilizer application and other measures of timely field management, hence advancing
    precision agriculture. Moreover, this machine learning intelligent model can be
    tailored for various other fruit farming.
  doi: 10.1016/j.scienta.2024.112847
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and Methods
    3. Results 4. Discussion 5. Conclusions CRediT authorship contribution statement
    Declaration of competing interest Supplementary Materials Funding acquisition
    Acknowledgements Appendix. Supplementary materials Data availability References
    Show full outline Figures (9) Show 3 more figures Tables (1) Table 1 Extras (1)
    Document Scientia Horticulturae Volume 327, 1 March 2024, 112847 Computer Vision-Aided
    Intelligent Monitoring of Coffee: Towards Sustainable Coffee Production Author
    links open overlay panel Francisco Eron a #, Muhammad Noman a #, Raphael Ricon
    de Oliveira a #, Antonio Chalfun-Junior a Show more Add to Mendeley Share Cite
    https://doi.org/10.1016/j.scienta.2024.112847 Get rights and content Highlights
    • The latest state-of-the-art Convolutional Neural Network (CNN)-based object
    detection model YOLOv7 (You Only Look Once) was trained for on-plant coffee fruit
    quantification and classification. • A novel fast and high-throughput semi-supervised
    method was developed on the basis of machine-generated color classes of coffee
    fruit for accelerated data annotation. • This computer vision system can efficiently
    track coffee ripening, estimate yield and harvest time as well as plant health.
    • Besides coffee, this ML intelligent monitoring model can be tailored to apply
    for various other fruits. Abstract Earlier, researchers have employed computer
    vision-based automatic fruit detection to estimate coffee yield at the time of
    harvest, however, studies on the on-plant quantification of the coffee fruit are
    scarce. In this study, the latest version of the state-of-the-art algorithm YOLOv7
    (You Only Look Once) was used for the first time. YOLOv7 was trained with 324
    annotated images of fruit bearing coffee branches followed by its evaluation with
    82 annotated images as validation data (supervised method) and then tested through
    raw images (unannotated) as test data. Meanwhile, the K-means models were trained
    which led to machine-generated color classes of coffee fruit for semi-supervised
    image annotation. Consequently, the developed model efficiently analyzed the test
    data with an mAP@.5 (mean average precision) of 0.89. Strikingly, our innovative
    semi-supervised method with an mAP@.5 of 0.77 for multi-class mode surpassed the
    supervised method which had mAP@.5 of only 0.60, leading to faster and more accurate
    annotation. While testing the yield estimation of the model in two plots, an average
    error of 3.78% was recorded between the predicted (pre-harvest) and ground truth
    (harvest) data for binary class mode. The average error for multi-class data was
    recorded as 3.87% for green, 3.445% for green-yellow, 5.09% for cherry-raisin,
    and 2.51% for dry fruits. This AI-based technology when integrated with other
    tools such as UAV would efficiently remotely monitor coffee field for informed
    decision about irrigation, fertilizer application and other measures of timely
    field management, hence advancing precision agriculture. Moreover, this machine
    learning intelligent model can be tailored for various other fruit farming. Previous
    article in issue Next article in issue Keywords Coffeeprecision agriculturemachine
    learningartificial intelligencedigital phenotyping 1. Introduction Coffee is a
    highly traded commodity globally ranking second only to oil in terms of traded
    value (FAO, 2023). The crop is a major contributor to the socio-economic development
    of tropical developing countries, with millions of people globally depending on
    it for their livelihoods. Aside from its contribution to agricultural GDP, coffee
    production is a significant employer and supports poverty alleviation (Chemura
    et al., 2016; Läderach et al., 2017). Thus, coffee cultivation is considered an
    avenue for realizing several of the Sustainable Development Goals (SDGs), such
    as generating income, creating rural employment, and poverty alleviation (FAO,
    2023). Coffee cultivation takes place in over 60 countries, primarily in tropical
    regions that are conducive to its growth. Brazil, Vietnam, and Colombia are the
    leading producers worldwide, with Brazil alone accounting for 36% of global coffee
    production (USDA), while the U.S, Brazil and Europe are its top consumers. Additionally,
    coffee plantations, especially shaded farms, provide crucial ecosystem services
    such as biodiversity conservation (Jha et al., 2014), carbon sequestration (van
    Rikxoort et al., 2014), and soil protection (Meylan et al., 2017). The coffee
    market is subject to recurrent supply-demand imbalances and uneven income distribution
    along the value chain. The global exports of coffee were recorded 10.88 million
    bags by December 2022 (ICO, 2023). Per data provided by the International Coffee
    Organization (ICO) in 2023, global coffee production was estimated to have reached
    169.34 million bags, with each bag weighing 60 kg, signifying a decline of 2.2%
    compared to the previous year. In 2021, Brazil suffered a 21.7% drop in coffee
    production, which amounted to an estimated 67.2 million bags due to weather-associated
    factors such as drought and frost. The sustainability of coffee bean production
    and the impact of climate change are key sources of uncertainty for the coffee
    industry. Climatic conditions, especially during the vegetative and reproductive
    phases of the coffee plant, significantly influence coffee yield (Tavares et al.,
    2018). Rising temperatures and precipitation shortages affect flowering, fruiting,
    and bean quality. Furthermore, climate variability is a key factor influencing
    the incidence of severe pests and diseases such as coffee leaf rust and coffee
    berry borer, which can decrease coffee yield and quality and increase production
    costs (Krishnan, 2017). Globally, Coffea arabica and Coffea canephora, commonly
    referred to as Arabica and Robusta coffees respectively, constitute approximately
    99% of the coffee production (Jayakumar et al., 2017). The quality of beans and
    yield of both species declines when outside these optimal temperature ranges (18-22°C
    for Arabica, while 22-28°C for Robusta), suggesting significant sensitivity to
    climatic changes (Magrach and Ghazoul, 2015). Therefore, from a socio-economic
    standpoint, it is crucial to comprehend the degree of climate-driven impacts on
    coffee production and the advantages of potential adaptation strategies to maintain
    and enhance coffee productivity and profitability while sustaining the livelihoods
    of smallholder producers globally. To protect coffee farms from adverse climatic
    conditions, keep a sustainable production and even enhance coffee yield and productivity,
    coffee farms demand continuous monitoring of every aspect. Nonetheless, the phenomenon
    of asynchronous flowering poses a significant challenge for coffee growers, leading
    to irregular fruit ripening (López et al., 2021). Consequently, this causes problems
    during the harvesting process, as careful attention must be paid to ensure optimal
    timing. Oftentimes, for quality coffee production, coffee farmers resort to the
    practice of lapsed harvesting, wherein they must wait for the next batch of cherries
    to ripen before harvesting. This approach is not only time-consuming but also
    labor-intensive, requiring frequent visits and manual screening of the fruits.
    It is important to note that the quality of coffee is largely dependent on the
    ripeness of the fruits (Thompson et al., 2012). Coffee fruits, commonly referred
    to as red cherries, undergo a color transformation during the ripening process
    (Haile and Kang, 2019). The term \"red cherry\" is used to describe the fruit''s
    epidermis when it achieves a uniform and intense red color at full ripeness, having
    progressed through various shades of green, orange, and pink. Overripe cherries
    turn dark violet, while the presence of green, overripe, or dry cherries in the
    harvested mass negatively impacts the quality of the beverage and subsequently,
    its value in the international market (Velásquez et al., 2019). In particular,
    the proportion of green cherries in the harvested mass can significantly affect
    the beverage''s acidity. To maintain high-quality standards and command a premium
    price, it is crucial to ensure that at least 98% of the harvested cherries are
    fully ripe (Leroy et al., 2006). In the current landscape, the adoption of new
    technologies and innovation is imperative for the beverage industry to increase
    productivity and competitiveness. To this end, the scientific community is making
    significant efforts to develop automatic systems that can enhance the inspection
    process. Numerous studies have already been conducted, resulting in the development
    of various applications that have improved for example sorting processes for different
    fruits and vegetables (Hameed et al., 2018). Technological advancements in precision
    agriculture play a vital role in obtaining accurate and reliable measurements
    for crop monitoring. Precision agricultural practices can enhance the potential
    of a region to produce crops with higher yield and quality at less cost. Remote
    sensing has emerged as a promising technology for coffee management, with studies
    demonstrating its efficacy in evaluating coffee leaf rust levels through the use
    of Sentinel 2 sensor and Random Forest (RF) algorithms combined with vegetation
    indices, as described earlier (Chemura et al., 2017). Convolutional Neural Networks
    (CNNs) (Sultana et al., 2020) are a type of deep learning model that are inspired
    by the human visual system and consist of multiple convolutional and pooling layers
    followed by fully connected layers. The input layer of a CNN takes in the input
    data, which could be an image, and passes it through a series of convolutional
    and pooling layers. The convolutional layers apply convolution operations to the
    input data to extract features such as edges, corners, and textures, which are
    important for object recognition. The pooling layers reduce the spatial dimensions
    of the feature maps obtained from the convolutional layers, which helps in reducing
    computational complexity and improving the model''s ability to generalize. The
    output of the last pooling layer is then flattened into a vector, which is passed
    through fully connected layers. These fully connected layers learn complex patterns
    and relationships between the features extracted from the convolutional layers,
    and produce the final prediction output (Bellocchio et al., 2019). During the
    training process, the forward computation is performed to make predictions, and
    the backward computation is performed to compute the gradients of the model parameters
    based on the prediction output and the labeled ground-truth. The gradients are
    then used to update the parameters of the model, typically using optimization
    algorithms such as gradient descent, which iteratively adjust the parameters to
    minimize the loss or cost function. The training process continues for a determined
    number of iterations of forward and backward stages, also known as epochs, until
    a stopping criterion is met, such as reaching a certain level of accuracy or a
    maximum number of epochs. This helps the model learn the optimal parameters for
    making accurate predictions on the training data (Liu et al., 2015). Computer
    vision has enabled the implementation of non-destructive techniques for detecting
    and identifying vegetative structures in the field using images. These techniques
    have been successfully applied to a wide range of crops including corn (Guerrero
    et al., 2013), tomatoes (Verma et al., 2014), and oranges (Patel et al., 2011).
    Besides, these techniques have also been implemented with grapes (Dey et al.,
    2012), pineapples (Moonrinta et al., 2010), and vegetable crops (Jay et al., 2015).
    Efficient decision-making on the appropriate harvesting period for coffee fruits
    can be facilitated by tracking their maturation stages through digital phenotyping.
    Ramos (2018) suggests that this can be achieved by determining the percentage
    of mature fruits on tree branches (Ramos et al., 2018). While previous studies
    have relied on destructive sampling, mainly post-harvest, to quantify and classify
    fruit for yield estimation (Carrillo and Penaloza, 2009; de Oliveira et al., 2016),
    only a limited number of studies have explored the classification of coffee fruits
    before harvest, which can significantly benefit coffee farmers decision-making.
    Earlier, Avendano et al. (2017)) developed a system that constructs a 3D representation
    of coffee branches and classifies their vegetative structures (Avendano et al.,
    2017). In this pursuit, another group came with a brilliant idea of developing
    a CV-based non-destructive method of fruit counting and classification similar
    to ours, with a few differences including a specific holder for smartphone during
    imaging, bit larger dataset to reproduce, and data modelling (Ramos et al., 2017).
    Few advancements were recently seen in this field. For example, a study developed
    a vegetation index (VI) for coffee ripeness based on the imaging data obtained
    from coffee fields through an RGB and a five-band multi-spectral cameras, each
    fixed on a separate UAV (Nogueira Martins et al., 2021). Similarly, Rodriguez
    et al. earlier came with a classic computer vision approach, however, it involved
    many instruments for image acquisition, a complex image processing system with
    precision values (Rodríguez et al., 2020). Although this technique requires the
    extraction of various features and their input into a classification algorithm,
    recent advancements in computer vision systems based on deep learning allow for
    the automatic extraction of multiple features. These techniques have gained popularity
    due to their speed and accuracy. Some recent studies devised yield mapping technique
    during harvest based on imaging from a camera mounted over the harvesting machine,
    using YOLOv4 (Bazame et al., 2022, 2021; Martello et al., 2022). The current study
    aimed at implementing the state-of-the-art CNN-based computer vision algorithm
    YOLOv7, the latest version (Wang et al., 2022), to detect and classify coffee
    fruits on tree branches at different maturation stages. The algorithm was first
    trained with training data (324 annotated images) and then evaluated with test
    data (82 annotated images), followed by testing with raw images (unannotated).
    We aimed at introducing a novel semi-supervised method for annotating the training
    data, which would save time and be able to handle large sets of data. This AI-based
    technology when integrated with other tools such as UAV would efficiently remotely
    monitor coffee field for informed decision about irrigation, fertilizer application
    and other measures of timely field management, implementing precision agriculture
    in sustainable quality coffee production chain. Nonetheless, this machine learning
    intelligent model could also be tailored for various other fruit farming. 2. Materials
    and Methods 2.1. Data Preparation We began by collecting images from various coffee
    farms at Lavras, and surrounding areas, Minas Gerais, which is the highest coffee
    producing area in Brazil. The areas included 21°09′51.7\"S 44°59′31.7\"W, 21°
    13′ 33.0852′'' S44° 58′ 17.7312′'' W, and 21° 13′ 44.4144′'' S 44° 57′ 47.0916′''
    W. The flowering in coffee is asynchronized, leading to varied dates of anthesis
    events (flower opening). In the mentioned location, a full bloom was observed
    in early September (first week), while following the fruit set, the branches were
    imaged once every week throughout the course of ripening, starting from November
    up to harvest in June and early July. For a broad applicability, the images used
    in this study included those photographing coffee fruits (branches) at every stage
    as the images were taken along a course of time from unripe green fruits to ripe
    cherries and raisin ones. The fruit bearing branches were photographed using various
    smartphone cameras at different angles to achieve representative data. The images
    were shot with default parameters of the smartphone camera, from an approximate
    distance of 30 cm (or including 7 to 10 nodes) in an angle of mostly 90° in relation
    to the coffee plagiotropic branch. The images were not calibrated or processed
    so that the algorithm would be able to detect fruits in a very easy and precise
    way independently of the equipment. All the images taken in the field were properly
    tagged and stored in a local database. The dataset of 406 images was split into
    training and validation sets, with a training split of 80% (324 images, 33,717
    fruits) and a validation split of 20% (82 images, 10,094 fruits). This random
    division of the dataset ensures that the models are trained on a diverse set of
    images and can generalize well to new data. In order to train the model for a
    diverse range of images, we deliberately included images of coffee plant branches
    in shade and those exposed to sunlight, and even with varied angles and distance.
    The images were manually annotated through bounding boxes using Label Studio (Label
    Studio), an open-source platform for creating labeled datasets, to accurately
    identify the coffee cherries on the tree canopy. The software used included Label
    Studio version 1.6.0 (HumanSignal, Inc.), and Python version 3.10.9 (Python Software
    Foundation) with Python libraries Pytorch (version 1.0.2), numpy (1.20.1), scikit-learn
    (0.24.1), scipy (1.6.2), seaborn (0.11.1), pandas (1.2.4), matplotlib (3.3.4).
    In the process of annotation, the color scale (green, yellow, red, dark-red and
    dry (black)) defined previously (Ságio, 2009), was used as reference. 2.2. Training
    the Model To train a computer vision model, for example to identify the target
    object in an image in this case, the algorithm should be first fed with annotated
    images. Here we used YOLO, which is a part of a family of one-stage object detectors
    and is popular for its speed and accuracy. We first evaluated and compared the
    efficiency of YOLOv5 (Jocher et al., 2022), YOLOv5m6 (Li et al., 2023) and YOLOv7,
    which are the latest versions and have not been employed for this purpose before.
    An ideal state-of-the-art model should have (1) a faster and stronger network
    architecture; (2) a more effective feature integration method; (3) a more accurate
    detection method; (4) a more robust loss function; (5) a more efficient label
    assignment method; and (6) a more efficient training method. As compared to YOLOv4,
    YOLOv7 has been proved to be more efficient even with 75% less parameters and
    36% less computation (Wang et al., 2022). To facilitate model training, all images
    were resized to 640 × 640 pixels. To further improve the model ability to generalize,
    default data augmentation techniques specific to the implemented models were used.
    In particular, we used mosaic augmentation, as described earlier (Bochkovskiy
    et al., 2020), to randomly combine multiple images into a single training sample.
    Mosaic augmentation works by stitching four images together at random positions
    in a larger canvas to create one larger mosaic image. The ground truth annotations
    of individual images in the mosaic image are then adjusted. A diverse set of images
    in the training data helped the models learn to better handle occlusions and other
    challenging conditions. Notably, the collected images contained a certain level
    of noise, reflecting the reality of field data collection and further challenges
    the models ability to generalize learning. Equations 1 and 2 were used in comparing
    the three different models. (1) where; IoU - Intersection over Union, A - Ground
    Truth Boxes (The manual labeled bounding box of a particular object in an image),
    B - Predicted Boxes (The predicted bounding box from the detection and classification
    algorithm), (2) where; AP - Average Precision, P – Precision, R – Recall “Precision”
    measures the number of correct positive predictions. “Recall” measures the number
    of correctly identified positive class samples present in the dataset. 2.3. Developing
    a semi-supervised annotation system To accelerate the annotation process, we utilized
    the annotation text-file in YOLO format (used for training the models in binary
    and multiclass mode) to collect objects (coffee fruits) in images, which were
    then subjected to cropping and resizing to 28 × 28 × 3 dimensions. To address
    potential lighting variations, we converted the resized RGB (Red, Green, Blue)
    images into the LAB color-space and extracted the A and B color channels for further
    analysis, eliminating the L channel, since the lightning variation could enhance
    bias into categorizing fruits based on shadow and light variations. The AB color
    space images were represented as vectors in a multidimensional space, and K-means
    models were trained with different k-sizes (2 to 7) on a dataset of approximately
    36,000 fruits randomly selected from the dataset to create different classes of
    colors of fruits. The sketch of the strategy is shown below. To enable semi-supervised
    learning, we curated annotations comprising of manually annotated bounding boxes
    but unsupervised sub-categories of fruits. By leveraging these annotations, we
    performed semi-supervised learning in object detection tasks in the selected model
    (YOLOv7), which is crucial for real-world applications. By allowing the creation
    of in-demand complex subcategories of objects, the selected model was trained
    in the semi-supervised learned sub-categories of fruits and contrasted with the
    performance from the model of supervised learning from the same number of categories
    and hyperparameters. All three object detection models (YOLOv5, YOLOv5m6 and YOLOv7)
    were trained using the default hyperparameters specified in the respective papers
    or repositories, except for the batch size and number of epochs. In this study,
    a batch size of 16 and 100 epochs were used for all models. Batch size determines
    the number of images processed in a single forward and backward pass to calculate
    the gradient for weight updates. It affects memory usage and computational efficiency,
    with larger batches potentially speeding up training. Epochs denote the number
    of complete passes of the entire dataset through the neural network during training.
    The selection of epochs influences the duration of training and the model''s ability
    to generalize. The training and evaluation were conducted on a Tesla T4 GPU available
    in Google Colab (Bisong and Bisong, 2019). At the Google CoLab, the online GPUs
    allow for faster execution of the code and developing of the model. The evaluation
    metrics used in this study included Precision, Recall, and mAP (He and Garcia,
    2009). 2.4. Validation of the model The selected model was used to quantify coffee
    fruits and their specific class based on their maturity level, such as unripe
    and ripe. The model was trained on a large dataset of coffee fruit (324) images
    with labels indicating their class. The model was able to learn features and patterns
    that distinguish different maturation stages of coffee fruits from each other.
    Afterwards, the model was applied to images of the entire dataset to predict their
    class and count the number of fruits in each image. The performance of the model
    was evaluated by comparing its predictions with the ground truth labels obtained
    from the manual annotation process. The accuracy and precision of the model were
    reported as metrics of its effectiveness in quantifying coffee fruits and their
    specific class. To estimate the percentage of ripeness and unripeness in the plant,
    equation 3 and 4 were used. (3) (4) To check the yield estimation efficiency of
    the model, the automatic counting data (number) was correlated with the harvested
    and manually measured (weight) data. The yield data was analyzed both in binary
    and multiclass modes. 3. Results 3.1. Label Studio and Google CoLab are efficient
    platforms for data annotation and machine learning While annotating the data,
    the Label Studio proved efficient both for aiming the object such as where the
    object in the image is located as well as classifying it such as what the object
    is and to which category it belongs. Similarly, following data annotation, Google
    CoLab also displayed all the necessary features to execute the YOLO through script.
    A total of 9416 fruits were labelled as ripe and 34395 as unripe, for binary class
    data. Similarly, for multiclass data, 23886 fruits were labelled as green, 10509
    as green-yellow, 4303 as cherry, 4265 as raisin, 848 as dry. Fig. 1 shows a random
    training data image of coffee fruit bearing branch captured with a smartphone
    camera, before (1A) and after (1B) annotation with Label Studio. Different colored
    fruit were labeled with different colored boxes. Download : Download high-res
    image (948KB) Download : Download full-size image Fig. 1. Annotation of an image
    using Label Studio. The unannotated image (A) depicts a branch of coffee plant
    bearing fruits at various stages such as green, green-yellow, cherry and raisin.
    Using Label Studio, all the visible fruits in image were annotated (B). Different
    colored boxes were used to classify the fruit at various stages such as unripe,
    yellow, cherry, raisin and dry. The whole training dataset (324 images) was annotated
    in the same way. 3.2. Comparison of various versions of YOLO to get the best mAP
    In the dataset of total 406 images, 324 were used as training data while 82 as
    validation data. After training the algorithm, the selected models (YOLOv5, YOLOv5m6
    and 7), chosen for their high mean average precision at 50% intersection over
    union (mAP@.5) and real-time object detection capabilities in COCO dataset (13;
    14), were trained on our training dataset (324 images). While comparing the object
    detection efficiency of three different YOLO versions, the results showed that
    YOLOv7 achieved the highest mAP@.5 values in all modes, followed by Yolov5 and
    Yolov5m6 (Table 1). YOLOv7 showed mAP@.5 value of 0.904 for mono class (only fruit),
    0.892 for binary class (unripe/ripe) while 0.605 for multiclass (green, yellow-green,
    cherry, raisin, dry). The rest two versions showed lesser values for all categories
    (Table 1). Fig. 2 further demonstrates the better performance of YOLOv7 compared
    to the other two for mono, binary and multiclass. Table 1. Comparison of object
    detection performance of three different YOLO versions in three different modes
    (Mono, Binary, and Multiclass). The values of precision (P), recall (R) and mAP@.5
    are calculated using the test data. The parameters values indicate the complexity
    of the models. Model P R mAP@.5val Parameters Yolov7 (Mono) 0.852 0.871 0.904
    36.9M Yolov7 (Binary) 0.845 0.852 0.892 36.9M Yolov7 (Multiclass) 0.627 0.682
    0.605 36.9M Yolov5 (Mono) 0.875 0.819 0.885 21.2M Yolov5 (Binary) 0.844 0.821
    0.866 21.2M Yolov5 (Multiclass) 0.64 0.562 0.555 21.2M Yolov5m6 (Mono) 0.873 0.833
    0.898 35.7M Yolov5m6 (Binary) 0.848 0.821 0.875 35.7M Yolov5m5 (Multiclass) 0.721
    0.547 0.556 35.7M Download : Download high-res image (366KB) Download : Download
    full-size image Fig. 2. Comparison of the performance of three different YOLO
    versions. For the mean average precision at 50% intersection over union (mAP@.5)
    in three modes of the dataset: Mono (only fruits), Binary (unripe and ripe fruits),
    and Multiclass (continuous classification scale - unripe, yellow, cherry, raisin
    and dry), YOLOv7 outperformed YOLOv5 and YOLOv5m6. 3.3. Development of a novel
    semi-supervised annotation system Through manual bounding boxes and automatic
    classification, a semi-supervised method was developed for the annotation of training
    data. Here, K-means clustering was used to create categories of coffee fruits
    based on their color. We trained K-means models with different k-sizes ranging
    from 2 to 7 and evaluated their performance based on their ability to identify
    distinct color clusters. To our interest, the K-means model efficiently identified
    distinct color clusters within the high-dimensional (28*28*2 axis) and created
    categories of the various stages of coffee fruits that were visually distinguishable.
    In this novel approach, the categories were composed of coffee fruits with similar
    color representing similar ripening stage, which can be useful for further analysis
    and classification. After categorization, we performed PCA in the high-dimensional
    vector to produce visualization of the boundaries among clusters (Fig. 3). Download
    : Download high-res image (855KB) Download : Download full-size image Fig. 3.
    Principal Component analysis of dataset annotated through semi-supervised approach.
    Various clusters in each class are visible in different colors. As the number
    of classes increased from two to seven classes, the mAP@.5 value also increased,
    however 4 classes appeared to be optimum. The seven various color classes of the
    coffee fruit ripening stage created through k-means are shown in Fig. 4. Employing
    semi-supervised approach, we attempted to annotate the same training and validation
    datasets which was earlier annotated using supervised method. Interestingly, the
    precision of detection increased with the increase in number of classes, whereas
    the optimal number of classes was determined to be 4, which was also visualized
    through Elbow method shown in Figure S1. Download : Download high-res image (2MB)
    Download : Download full-size image Fig. 4. K-means-based machine–generated color
    classes as semi-supervised method. There were up to 7 classes generated keeping
    k = 7, however the optimal number of classes was determined as 4. 3.4. Comparison
    of the supervised and semi-supervised annotation methods To validate the efficiency
    as well as consistency of the novel machine learning method, we compared the performance
    of semi-supervised model with the supervised one. Fig. 5 shows the comparative
    performance of supervised and semi-supervised annotations in binary as well as
    multiclass modes. To our interest, the semi-supervised model performed faster
    and more accurate annotation than the supervised one. Fig. 5A depicts output of
    the supervised (5A) and semi-supervised (5B) method for binary class annotation.
    The number of ripe (R) and unripe (UR) fruit through ML-based annotation is written
    above each image. The output of comparison of supervised and semi-supervised methods
    in multi-class mode is shown in Fig. 5C and 5D, respectively. The number as well
    as class of the fruit detected are mentioned in the figure. The images in both
    the binary and multiclass represent four different time points of fruit ripening
    (from earlier (A, B) to later stages (C, D)). Comparing both the cases (Fig. 5A,
    B, C, and D), it is clear that semi-supervised annotation surpassed the supervised
    annotation in terms of speed and accuracy. This is further simplified graphically
    in Fig. 6. For binary class, the supervised and semi-supervised training models
    had an equal mAP@.5 of .89 (Fig. 6A), showing similar performance for both methods.
    However, for multi-class detection (Fig. 6B), the mAP@.5 was 0.77 in case of semi-supervised
    model, which was only 0.6 with the supervised method, keeping the number of categories
    the 4 in both cases. It proves the high resolving power of the semi-supervised
    annotation. Moreover, its faster and more accurate annotation feature will aid
    in machine learning of large dataset, in less time. This is a novel and rigorous
    approach to analyze large-scale coffee-fruits datasets, which can have significant
    implications for various fields such as computer vision, image processing, and
    machine learning. Download : Download high-res image (3MB) Download : Download
    full-size image Fig. 5. Comparative performance of supervised and semi-supervised
    methods. The novel method of semi-supervised annotation was compared with the
    supervised for binary (Supervised – A Semi-supervised – B) and multi-class (Supervised
    – A, Semi-supervised – B) modes. The numerals show fruit counts while letters
    denote fruit type as UR – Unripe, RP – Ripe, C – Cherry G – Green, GY – Green-yellow,
    R – Raisin. Download : Download high-res image (256KB) Download : Download full-size
    image Fig. 6. Graphical representation of the comparative performance of supervised
    and semi-supervised methods. For binary class, the supervised and semi-supervised
    training models had an equal mAP@.5 of .89, showing similar performance for both
    methods (A). However, for multi-class annotation, the semi-supervised method displayed
    higher mAP@.5 value of 0.77 as compared to 0.66 of the supervised method (B),
    showing better performance. 3.5. Validation of the established model To check
    the efficiency of our trained model, we initially tested it by feeding raw images,
    not included in our initial dataset. Afterwards, we also tracked the ripening
    of coffee fruits in real time. Both the approaches proved the higher image processing
    efficiency of our established model. Fig. 7 depicts raw images not originally
    included in our dataset. The raw images from the field were analyzed with the
    model where; Fig. 7A shows the binary class detection counting only ripe and unripe
    fruits. However, multi-class fruit detection and quantification, classifying them
    into green, green-yellow, cherry and raisin is also shown in Fig. 7B. The number
    and category of the fruit are written above each image. This proved the model
    was successful in image processing. A collection of data like this will provide
    a broad picture of the fruit ripening pattern, estimate yield and harvesting time.
    The big data will eventually aid in informed decision on coffee crop management
    specially plans for harvest and post-harvest measures. Download : Download high-res
    image (2MB) Download : Download full-size image Fig. 7. Evaluation of the established
    model using test images outside of dataset. Raw images from coffee field were
    analyzed by the for binary (A) and multi-class (B) fruit detection. The numerals
    show fruit counts while letters denote fruit type as UR – Unripe, RP – Ripe, C
    – Cherry G – Green, GY – Green-yellow, R – Raisin. For further validation, we
    tracked and analyzed the fruit ripening in a coffee field in real time for 90
    days. Fig. 8A shows the ripening in binary mode (unripe and ripe) over the said
    time duration. Fig. 8B depicts the ripening information of the same data in multi-class
    mode over the 3-month period. It is obvious from this example that for about the
    initial 40 days there is higher percentage of unripe fruit which turn ripe after
    this period. We additionally demonstrated the potential of this algorithm to be
    used in crop analysis and estimation, utilizing regression and ridgeline plots
    (Figure S2) to illustrate the variation in mature fruit proportions over the growing
    season. Download : Download high-res image (601KB) Download : Download full-size
    image Fig. 8. Tracking coffee fruit ripening with the developed model over a period
    of 3 months. Plots show the percentage of ripe and unripe fruits over time in
    binary (A) and multi-class (B) modes. Fig. 9 shows correlation of the automatic
    counting data with yield data in two plots (T1, T2). The automatic counting data
    (9A) can be compared with manually measured data (9B) in binary mode. While the
    respective data can be seen in multi-class mode (9C, 9D). An average error of
    3.78% was recorded between the predicted (pre-harvest) and ground truth (harvest)
    data for binary class mode (Table S1). The average error for multi-class data
    (Table S2) was recorded as 3.87% for green, 3.445% for green-yellow, 5.09% for
    cherry-raisin, and 2.51% for dry fruits. Download : Download high-res image (571KB)
    Download : Download full-size image Fig. 9. Yield estimation data of the model.
    The automatic counting data (pre-harvest) was correlated with manual (post-harvest)
    data. (A) Automatic counting data in binary mode. (B) Manually measured data in
    binary mode. (C) Automatic counting data in multi-class mode. (D) Manually measured
    data in multiclass mode. 4. Discussion Traditional methods of yield estimation
    and decision about harvest time for crops like coffee involve manual counting
    of fruits (usually through destructive sampling), which is laborious, time-consuming,
    and often error-prone. Federación Nacional de Cafeteros (FNC) reported a significant
    portion of coffee fruit is wasted by the destructive sampling which includes 60
    coffee trees per hectare in an area of 2000 hectares, for coffee yield estimation
    (Ramos, et al., 2016). To rapidly monitor coffee crop specially during fruit ripening
    time which is usually from January to May and June, estimate the yield and harvest
    time, forecasting market supply, and reducing production costs, precision agricultural
    approaches present an efficient and reliable solution. These take help of the
    artificial intelligence technology especially computer vision. In this study,
    we present a novel computer vision-based approach for estimating coffee crop yields
    as well as harvest time. Our approach involves training of a model for which the
    image dataset was annotated using online tools. Our dataset included images of
    fruit bearing branches of coffee plants. While addressing the above-mentioned
    issues, we trained the state-of-the-art object detection model YOLOv7 to count
    and classify coffee fruit in images. YOLO is CNN-based object detection model,
    where CNN is being used in numerous areas such as the facial recognition system
    the software of webcams and surveillance cameras. The annotation results (Fig.
    1) of our training and validation datasets demonstrated that the online platform
    Label Studio for annotating the images, possesses multiple features such as shared
    working, allowing more than one person to deal a common project. Comparing YOLOv7
    with the two other versions, YOLOv5 and YOLOv5m6 (Table 1), we obtained highest
    mAP value with YOLOv7, which is also obvious in Fig. 2. Ramos et al (2018) while
    using an alternative approach of using 3D images (via Structure from Motion (SfM),
    which is a computational technique, to generate 3D structures (point clouds),
    from corresponding points in multiple views of an object in space) in Support
    Vector Machine (SVM), constructing statistical estimation model for each ripening
    stage obtained a maturation index with 83% efficiency. While comparing with YOLOv3
    for all network sized considered, an earlier study obtained mAP of 83.5% with
    YOLOv4 (Bazame et al., 2022). However, it is lower than the mAP we obtained with
    YOLOv7, which is >89%. Moreover, they reported an 8.8% lower Floating Points Operations
    (FLOPs) for YOLOv4, as compared to YOLOv3. The same group earlier executed a full-fledged
    experiment using YOLOv3-tiny, yet obtained mAP of 84%, while AP of 86%, 85% and
    80% for unripe, ripe and overripe coffee fruits, respectively (Bazame et al.,
    2021). Nonetheless, the group estimated coffee yield only at harvest stage, while
    also using older version of YOLO. This also undermines the CV-aided estimation
    of the harvest time. A very recent study while comparing YOLOv7 and YOLOv4 models,
    based on average precisions (APs) for four apple flower bud growth stages and
    mean APs (mAPs), showed that YOLOv7 outperformed YOLOv4 in all growth stages and
    at all training image annotation quality levels. When trained with 100% training
    image annotation quality, YOLOv7 achieved a mAP of 0.80, whereas YOLOv4 achieved
    a mAP of 0.63 with only 5% training image annotation quality. The improvement
    in APs and mAPs varied depending on the growth stage and training image annotation
    quality. YOLOv7 showed improvements in APs ranging from 1.52% to 166.48% and mAPs
    ranging from 3.43% to 53.45% compared to YOLOv4, again depending on the specific
    growth stage and training image annotation quality (Yuan, 2023). YOLOv7 with the
    SENet attention mechanism and a weighted loss function, was effective in addressing
    the challenges of semantic segmentation of standing tree images, and outperformed
    other methods in terms of detection speed and segmentation accuracy by showing
    mean pixel accuracy (MPA) of 94.69% and mean intersection over union (MIoU) of
    91.17% (Cao et al., 2023). In addition to its yield estimation capabilities, the
    Yolov7 model can also extract valuable information about the ripening process
    of crops over time. By calculating the percentage of ripe fruits over months,
    we can create plots that visualize the progression of ripeness levels as the crops
    mature, as well as the categorization of the pattern of maturation present in
    the farm. These plots provide farmers and researchers with valuable insights into
    the development of the crop, enabling them to plan harvesting schedules, optimize
    yields, and better understand the underlying biological processes. By quantifying
    ripeness in this way, we can improve our ability to predict and manage crop yields,
    ultimately leading to more efficient and sustainable agricultural practices. Furthermore,
    the data collected from the model allows for an analysis of the distribution of
    ripe and unripe fruits throughout the growing season. One of the challenges of
    annotation using an adapted scale in the field is the increase of confusion between
    classes due to the overlapping of features present in the classes. This can be
    seen by comparing the confusion matrices of binary (Figure S4) and multiclass
    (Figure S5) annotation schemes. This confusion can affect the reliability and
    validity of the annotation process and compromise the quality of the data. Therefore,
    it is important to create mathematically optimized scales that can reduce ambiguity
    and increase consistency among annotators. Such scales can also facilitate the
    analysis and interpretation of the data and enhance the scientific contribution
    of the research. Although YOLOv7 displayed higher values even in multiclass mode,
    yet it needs further improvement to achieve the best results. Enlarging and further
    diversifying the training dataset could also further improve its performance.
    The semi supervised model we devised is a novel method for annotating coffee images
    in training as well as validation data. The semi-supervised learning categories
    can be more representative of the mathematical process of categorization in AI
    and avoid human error being propagated through the machine learning metrics by
    imposing categories or scales. Besides that, the semi-supervised learned categories
    accelerate the process of annotations and can be used to create mathematically-optimized
    models and scales (van Engelen and Hoos, 2020). However, care should be taken
    in the semi-supervised approach as the machine-generated color categories of coffee
    fruits could sometimes be confused and mixed. This problem could be minimized
    by enlarging as well as diversifying the training data. Our choice of employing
    K-means clustering to create categories of coffee fruits based on color was driven
    by its ability to uncover distinct color clusters within the intricate high-dimensional
    space (28282 axis) of coffee fruit images. The resulting categories, which visually
    represent various ripening stages, offer a valuable resource for further analysis
    and classification. This approach holds promise for enhancing the understanding
    of coffee fruit development, as it groups fruits with similar color profiles into
    categories indicative of analogous ripening stages. The utilization of Principal
    Component Analysis (PCA) to visualize the boundaries among these clusters provided
    a tangible representation of the model''s performance and the inherent color patterns
    captured within the dataset. Our findings also extended to the realm of model
    optimization. Through the semi-supervised approach, we sought to annotate the
    same training and validation datasets initially annotated using a supervised method.
    Notably, the precision of detection exhibited an intriguing trend, with an increase
    in the number of classes correlating with enhanced precision. Importantly, our
    investigation pinpointed the optimal number of classes as four, highlighting the
    model''s capacity to discern meaningful distinctions and subsequently refine detection
    precision. Here, it is important to note that the categories created by K-means
    were based solely on color and did not necessarily correspond to different types
    or varieties of coffee fruits, such as maturity. Therefore, further analysis and
    classification was required to accurately identify the different types of coffee
    fruits being represented in each category. Further analysis to acknowledge this
    problem was performed by correlating the output categories with the Moraes categories
    used in the annotation process. By doing this, we defined categories in a crescent
    order of maturity. It holds enormous potential in handling a dataset despite its
    size. One thing to note is number of classes at which the color classification
    is optimum. In our case, we determined the optimal number of color classes to
    be 4. Our semi-supervised method outperformed the supervised method as it gave
    a mAP@.5 of 0.77 compared to 0.60 of the supervised method in multiclass detection
    mode. It proves the high resolving power of the semi-supervised annotation. Ramos
    et al. (2017), developed one of the pioneer CV system, which they named MVS for
    coffee fruit detection and counting, where they also considered using masking
    technique which aided in detecting the occluded fruits in images. The same masking
    technique if applied to our model could enhance the precision of our system. This
    should be addressed in future. While the supervised method showcased competitive
    results in binary classification, its performance lagged behind the semi-supervised
    counterpart in the more intricate landscape of multi-class detection. As such,
    researchers navigating multi-class classification tasks may find the semi-supervised
    approach to be a valuable tool for achieving higher accuracy and robustness. Moreover,
    its faster and more accurate annotation feature will aid in machine learning of
    large dataset, in less time. This is a novel and rigorous approach to analyze
    large-scale coffee-fruits datasets, which can have significant implications for
    various fields such as computer vision, image processing, and machine learning.
    This performance could even be enhanced in future as newer improved versions of
    YOLO become available. However, increasing this value should be addressed in future
    CV studies regarding coffee. Once the imaging data has been collected, this technology
    can be efficiently applied to quantify fruit in a non-destructive, cost-effective
    and non-laborious way. It not only informs about the yield and harvest time, but
    also monitors plant health. The ripening stage as well as health of coffee fruit
    are quite obvious from its color, for example, unripe fruit are green and ripe
    are red but overripe turn black and even dry if not harvested on time. Similarly,
    pre-mature blackening of the coffee fruit means abnormality which may be due to
    temperature changes, osmotic stress or malnutrition. The observation at the early
    stage of such a problem can rescue the crop by taking timely measures such as
    irrigation and fertilizer application, accordingly. In short, it is a simple yet
    very useful system for coffee farming and would prove revolutionary in enhancing
    coffee yield and quality in the coffee producing areas such as Minas Gerais, Brazil.
    This allows us to accurately monitor the plant health, estimate yield and harvest
    time through images showing coffee fruit on branches, in a non-destructive way
    with minimum human intervention. Bazame et al. (2021) also used their model for
    videos recorded during the coffee harvest, making it possible to map the qualitative
    attribute of the maturation stage of coffee over the experimental area, We also
    envision to integrate the video function which would be useful in faster and more
    convenient data collection in addition to farm monitoring via UAV. In precision
    agriculture, our model''s accurate pre-harvest classification can empower farmers
    with timely insights, enabling them to make informed decisions about harvesting
    schedules, resource allocation, and crop management practices, which will optimize
    yield, minimize waste, and enhance overall productivity. Moreover, the impact
    extends to supply chain management, where reliable pre-harvest classification
    offers an advantage. However, variability in environmental conditions, lighting,
    and image quality could potentially impact the model''s performance under real-world
    circumstances. As a result, rigorous testing and validation across diverse scenarios
    are essential to ensure its reliability and robustness. Furthermore, the model''s
    generalization beyond the training data, particularly to new and unseen datasets,
    should be rigorously assessed to ascertain its practical applicability. Continuously
    collecting data across diverse environments and lighting conditions can contribute
    to refining the model''s adaptability and performance. Integrating additional
    modalities, such as hyperspectral or thermal data, hold the potential to provide
    supplementary insights and enhance overall classification accuracy. Exploring
    ensemble approaches, where predictions from multiple models or modalities are
    combined, could further elevate the model''s accuracy and reliability. In terms
    of practical deployment, assessing the model''s real-time performance in scenarios
    like automated fruit sorting systems is crucial to validate its feasibility and
    efficiency in dynamic agricultural settings. Additionally, user interface development
    plays key role, as creating intuitive and user-friendly interfaces can facilitate
    easy interaction with the model''s predictions, enabling farmers and stakeholders
    to make practical and timely decisions. 5. Conclusions As a temperature sensitive
    tropical plant, weather changes can greatly affect coffee yield. Thus, the continuous
    monitoring of coffee specially during fruit ripening is indispensable for its
    production in a sustainable manner. Under these scenarios, computer vision-aided
    coffee fruit quantification and yield estimation studies have been carried out
    in the recent past, however some of them used expensive machinery and complex
    image processing while others used outdated machine learning models. The most
    important results of our work include, 1) using the latest state-of-the-art YOLOv7,
    we obtained an mAP@.5 of 0.89, the highest ever so far. 2) We devised a semi-supervised
    method of annotating coffee images (training data), which would greatly aid in
    handling large datasets in less time. 3) The real-time tracking of coffee fruit
    ripening over a period of 90 days validated the efficiency of our system in predicting
    harvest time and yield estimate. With the integration of UAV and other value addition,
    the developed system holds enormous potential to be used in monitoring coffee
    farms for informed decision on timely field management, harvest time and post-harvest
    measures, which will ultimately enhance coffee yield and contribute to sustainable
    coffee production. CRediT authorship contribution statement Francisco Eron: Formal
    analysis, Methodology, Software, Visualization, Writing – original draft. Muhammad
    Noman: Methodology, Supervision, Writing – original draft, Writing – review &
    editing. Raphael Ricon de Oliveira: Conceptualization, Formal analysis, Supervision,
    Writing – review & editing. Antonio Chalfun-Junior: Conceptualization, Funding
    acquisition, Project administration, Resources, Supervision, Writing – review
    & editing. Declaration of competing interest The authors declare that they have
    no competing interest. Supplementary Materials All the supplementary materials
    are available at www.xyz.com. Funding acquisition The authors thank the members
    of the Laboratory of Plant Molecular Physiology (LFMP, UFLA/Brazil) for structural
    support of the experiments; and Instituto Brasileiro de Ciência e Tecnologia do
    Café (INCT-Café) to support plant material and field experiments; Fundação de
    Amparo à Pesquisa do Estado de Minas Gerais (FAPEMIG, grant number CAP APQ 03605/17),
    Conselho Nacional de Desenvolvimento Científico e Tecnológico and INCT-Café for
    the financial support; CNPq for scholarship research for ACJ under grant number
    310216/2019-2 and 2021/06968-3, São Paulo Research Foundation (FAPESP) for financial
    support. Acknowledgements The authors thank the members of the Laboratory of Plant
    Molecular Physiology (LFMP, UFLA/Brazil) for structural support of the experiments,
    and Instituto Brasileiro de Ciência e Tecnologia do Café (INCT-Café) to support
    plant material and field experiments, Fundação de Amparo à Pesquisa do Estado
    de Minas Gerais (FAPEMIG), Conselho Nacional de Desenvolvimento Científico e Tecnológico
    and INCT-Café for the financial support; CNPq for scholarship research for ACJ,
    and São Paulo Research Foundation (FAPESP). Appendix. Supplementary materials
    Download : Download Word document (680KB) Data availability The code developed
    is available at http://github.com/EronDS/CoffeApp. References Avendano, Ramos
    and Prieto, 2017 J. Avendano, P.J. Ramos, F.A. Prieto A system for classifying
    vegetative structures on coffee branches based on videos recorded in the field
    by a mobile device Expert Syst. Appl., 88 (2017), pp. 178-192, 10.1016/j.eswa.2017.06.044
    View PDFView articleView in ScopusGoogle Scholar Bazame et al., 2022 H.C. Bazame,
    J.P. Molin, D. Althoff, M. Martello, L.D.P. Corrêdo Mapping coffee yield with
    computer vision Precis. Agric., 23 (2022), pp. 2372-2387, 10.1007/s11119-022-09924-0
    View in ScopusGoogle Scholar Bazame, Molin, Althoff and Martello, 2021 H.C. Bazame,
    J.P. Molin, D. Althoff, M. Martello Detection, classification, and mapping of
    coffee fruits during harvest with computer vision Comput. Electron. Agric., 183
    (2021), Article 106066, 10.1016/j.compag.2021.106066 View PDFView articleView
    in ScopusGoogle Scholar Bellocchio, Ciarfuglia, Costante and Valigi, 2019 E. Bellocchio,
    T.A. Ciarfuglia, G. Costante, P. Valigi Weakly Supervised Fruit Counting for Yield
    Estimation Using Spatial Consistency IEEE Robot. Autom. Lett., 4 (2019), pp. 2348-2355,
    10.1109/LRA.2019.2903260 View in ScopusGoogle Scholar Bisong and Bisong, 2019
    E. Bisong, E. Bisong Google colaboratory Build. Mach. Learn. Deep Learn. Model.
    google cloud Platf. a Compr. Guid. beginners (2019), pp. 59-64 CrossRefView in
    ScopusGoogle Scholar Bochkovskiy, Wang and Liao, 2020 A. Bochkovskiy, C.-Y. Wang,
    H.-Y.M. Liao Yolov4: Optimal speed and accuracy of object detection arXiv Prepr.
    arXiv2004.10934 (2020) Google Scholar Cao, Zheng and Fang, 2023 Cao, L., Zheng,
    X., Fang, L., 2023. The Semantic Segmentation of Standing Tree Images Based on
    the Yolo V7 Deep Learning Algorithm. Electronics. https://doi.org/10.3390/electronics12040929.
    Google Scholar Carrillo and Penaloza, 2009 E. Carrillo, A.A. Penaloza Artificial
    vision to assure coffee-Excelso beans quality Proceedings of the 2009 Euro American
    Conference on Telematics and Information Systems: New Opportunities to Increase
    Digital Citizenship (2009), pp. 1-8 CrossRefGoogle Scholar Chemura, Kutywayo,
    Chidoko and Mahoya, 2016 A. Chemura, D. Kutywayo, P. Chidoko, C. Mahoya Bioclimatic
    modelling of current and projected climatic suitability of coffee (Coffea arabica)
    production in Zimbabwe Reg. Environ. Chang., 16 (2016), pp. 473-485, 10.1007/s10113-015-0762-9
    View in ScopusGoogle Scholar Chemura, Mutanga and Dube, 2017 A. Chemura, O. Mutanga,
    T. Dube Separability of coffee leaf rust infection levels with machine learning
    methods at Sentinel-2 MSI spectral resolutions Precis. Agric., 18 (2017), pp.
    859-881 CrossRefView in ScopusGoogle Scholar de Oliveira et al., 2016 E.M. de
    Oliveira, D.S. Leme, B.H.G. Barbosa, M.P. Rodarte, R.G.F.A. Pereira A computer
    vision system for coffee beans classification based on computational intelligence
    techniques J. Food Eng., 171 (2016), pp. 22-27, 10.1016/j.jfoodeng.2015.10.009
    View PDFView articleView in ScopusGoogle Scholar Dey, Mummert and Sukthankar,
    2012 D. Dey, L. Mummert, R. Sukthankar Classification of plant structures from
    uncalibrated image sequences 2012 IEEE Workshop on the Applications of Computer
    Vision (WACV), IEEE (2012), pp. 329-336 CrossRefView in ScopusGoogle Scholar FAO
    2023 FAO, 2023. Food and Agriculture Organization [WWW Document]. URL https://www.fao.org/markets-and-trade/commodities/coffee/en/(accessed
    4.9.23). Google Scholar Guerrero et al., 2013 J.M. Guerrero, M. Guijarro, M. Montalvo,
    J. Romeo, L. Emmi, A. Ribeiro, G. Pajares Automatic expert system based on images
    for accuracy crop row detection in maize fields Expert Syst. Appl., 40 (2013),
    pp. 656-664 View PDFView articleView in ScopusGoogle Scholar Haile and Kang, 2019
    M. Haile, W.H. Kang The harvest and post-harvest management practices’ impact
    on coffee quality Coffee-Production Res (2019), pp. 1-18 View in ScopusGoogle
    Scholar Hameed, Chai and Rassau, 2018 K. Hameed, D. Chai, A. Rassau A comprehensive
    review of fruit and vegetable classification techniques Image Vis. Comput., 80
    (2018), pp. 24-44 View PDFView articleView in ScopusGoogle Scholar He and Garcia,
    2009 H. He, E.A. Garcia Learning from imbalanced data IEEE Trans. Knowl. Data
    Eng., 21 (2009), pp. 1263-1284 View in ScopusGoogle Scholar ICO 2023 ICO, 2023.
    International Coffee Organization [WWW Document]. URL https://www.ico.org/(accessed
    3.27.23). Google Scholar Jay et al., 2015 S. Jay, G. Rabatel, X. Hadoux, D. Moura,
    N. Gorretta In-field crop row phenotyping from 3D modeling performed using Structure
    from Motion Comput. Electron. Agric., 110 (2015), pp. 70-77 View PDFView articleView
    in ScopusGoogle Scholar Jayakumar et al., 2017 M. Jayakumar, M. Rajavel, U. Surendran,
    G. Gopinath, K. Ramamoorthy Impact of climate variability on coffee yield in India—with
    a micro-level case study using long-term coffee yield data of humid tropical Kerala
    Clim. Change, 145 (2017), pp. 335-349, 10.1007/s10584-017-2101-2 View in ScopusGoogle
    Scholar Jha et al., 2014 S. Jha, C.M. Bacon, S.M. Philpott, V. Ernesto Méndez,
    P. Läderach, R.A. Rice Shade Coffee: Update on a Disappearing Refuge for Biodiversity
    Bioscience, 64 (2014), pp. 416-428, 10.1093/biosci/biu038 View in ScopusGoogle
    Scholar Jocher et al., 2022 G. Jocher, A. Chaurasia, A. Stoken, J. Borovec, Y.
    Kwon ultralytics/yolov5: V6. 1-TensorRT TensorFlow edge TPU and OpenVINO export
    and inference Zenodo, 2 (2) (2022) Google Scholar Krishnan, 2017 Krishnan, S.,
    2017. Sustainable Coffee Production. https://doi.org/10.1093/acrefore/9780199389414.013.224.
    Google Scholar Läderach et al., 2017 P. Läderach, J. Ramirez–Villegas, C. Navarro-Racines,
    C. Zelaya, A. Martinez–Valle, A. Jarvis Climate change adaptation of coffee production
    in space and time Clim. Change, 141 (2017), pp. 47-62, 10.1007/s10584-016-1788-9
    View in ScopusGoogle Scholar López et al., 2021 López, M.E., Santos, I.S., de
    Oliveira, R.R., Lima, A.A., Cardon, C.H., Chalfun-Junior, A., n.d. An overview
    of the endogenous and environmental factors related to the Coffea arabica flowering
    process. Beverage Plant Res. 1, 1–16. https://doi.org/10.48130/BPR-2021-0013.
    Google Scholar Leroy et al., 2006 T. Leroy, F. Ribeyre, B. Bertrand, P. Charmetant,
    M. Dufour, C. Montagnon, P. Marraccini, D. Pot Genetics of coffee quality Brazilian
    J. plant Physiol., 18 (2006), pp. 229-242 View in ScopusGoogle Scholar Li et al.,
    2023 C. Li, L. Li, Y. Geng, H. Jiang, M. Cheng, B. Zhang, Z. Ke, X. Xu, X. Chu
    YOLOv6 v3. 0: A Full-Scale Reloading arXiv Prepr. arXiv2301.05586 (2023) Google
    Scholar Liu et al., 2015 T. Liu, S. Fang, Y. Zhao, P. Wang, J. Zhang Implementation
    of training convolutional neural networks arXiv Prepr. arXiv1506.01195 (2015)
    Google Scholar Magrach and Ghazoul, 2015 A. Magrach, J. Ghazoul Climate and Pest-Driven
    Geographic Shifts in Global Coffee Production: Implications for Forest Cover,
    Biodiversity and Carbon Storage PLoS One, 10 (2015), Article e0133071 CrossRefView
    in ScopusGoogle Scholar Martello, Molin and Bazame, 2022 Martello, M., Molin,
    J.P., Bazame, H.C., 2022. Obtaining and Validating High-Density Coffee Yield Data.
    Horticulturae. https://doi.org/10.3390/horticulturae8050421. Google Scholar Martins
    et al., 2021 Nogueira Martins, R., de Carvalho Pinto, F.D., Marçal de Queiroz,
    D., Magalhães Valente, D.S., Fim Rosas, J.T., 2021. A Novel Vegetation Index for
    Coffee Ripeness Monitoring Using Aerial Imagery. Remote Sens. https://doi.org/10.3390/rs13020263.
    Google Scholar Meylan et al., 2017 L. Meylan, C. Gary, C. Allinne, J. Ortiz, L.
    Jackson, B. Rapidel Evaluating the effect of shade trees on provision of ecosystem
    services in intensively managed coffee plantations Agric. Ecosyst. Environ., 245
    (2017), pp. 32-42, 10.1016/j.agee.2017.05.005 View PDFView articleView in ScopusGoogle
    Scholar Moonrinta, Chaivivatrakul, Dailey and Ekpanyapong, 2010 J. Moonrinta,
    S. Chaivivatrakul, M.N. Dailey, M. Ekpanyapong Fruit detection, tracking, and
    3D reconstruction for crop mapping and yield estimation 2010 11th International
    Conference on Control Automation Robotics & Vision. IEEE (2010), pp. 1181-1186
    CrossRefView in ScopusGoogle Scholar Patel, Jain and Joshi, 2011 H.N. Patel, R.K.
    Jain, M.V Joshi Fruit detection using improved multiple features based algorithm
    Int. J. Comput. Appl., 13 (2011), pp. 1-5 CrossRefGoogle Scholar Ramos et al.,
    2016 P. Ramos, F. Prieto, C. Oliveros, N.F. Aleixos, F. Albert Medición del porcentaje
    de madurez en ramas de café mediante dispositivos móviles y visión por computador
    VIII Congreso Ibérico de Agroingeniería. Retos de La Nueva Agricultura Mediterránea
    (2016), pp. 917-925 View in ScopusGoogle Scholar Ramos, Avendaño and Prieto, 2018
    P.J. Ramos, J. Avendaño, F.A. Prieto Measurement of the ripening rate on coffee
    branches by using 3D images in outdoor environments Comput. Ind., 99 (2018), pp.
    83-95, 10.1016/j.compind.2018.03.024 View PDFView articleView in ScopusGoogle
    Scholar Ramos, Prieto, Montoya and Oliveros, 2017 P.J. Ramos, F.A. Prieto, E.C.
    Montoya, C.E. Oliveros Automatic fruit count on coffee branches using computer
    vision Comput. Electron. Agric., 137 (2017), pp. 9-22, 10.1016/j.compag.2017.03.010
    View PDFView articleView in ScopusGoogle Scholar Rodríguez, Corrales, Aubertot
    and Corrales, 2020 J.P. Rodríguez, D.C. Corrales, J.-N. Aubertot, J.C. Corrales
    A computer vision system for automatic cherry beans detection on coffee trees
    Pattern Recognit. Lett., 136 (2020), pp. 142-153, 10.1016/j.patrec.2020.05.034
    View PDFView articleView in ScopusGoogle Scholar Ságio, 2009 Ságio, S.A., 2009.
    Características fisiológicas e bioquímicas de frutos de duas cultivares de café
    de ciclos de maturação precoce e tardio. Google Scholar Sultana, Sufian and Dutta,
    2020 Sultana, F., Sufian, A., Dutta, P., 2020. A Review of Object Detection Models
    Based on Convolutional Neural Network BT - Intelligent Computing: Image Processing
    Based Applications, in: Mandal, J.K., Banerjee, S. (Eds.), . Springer Singapore,
    Singapore, pp. 1–16. https://doi.org/10.1007/978-981-15-4288-6_1. Google Scholar
    Tavares et al., 2018 P. Tavares, S. da, A. Giarolla, S.C. Chou, A.J. Silva, P.
    de, A. Lyra, A. de Climate change impact on the potential yield of Arabica coffee
    in southeast Brazil Reg. Environ. Chang., 18 (2018), pp. 873-883, 10.1007/s10113-017-1236-z
    View in ScopusGoogle Scholar Thompson, Miller, Lopez and Camu, 2012 S.S. Thompson,
    K.B. Miller, A.S. Lopez, N. Camu Cocoa and coffee Food Microbiol. Fundam. Front.
    (2012), pp. 881-899 Google Scholar van Engelen and Hoos, 2020 J.E. van Engelen,
    H.H. Hoos A survey on semi-supervised learning Mach. Learn., 109 (2020), pp. 373-440,
    10.1007/s10994-019-05855-6 View in ScopusGoogle Scholar van Rikxoort, Schroth,
    Läderach and Rodríguez-Sánchez, 2014 H. van Rikxoort, G. Schroth, P. Läderach,
    B. Rodríguez-Sánchez Carbon footprints and carbon stocks reveal climate-friendly
    coffee production Agron. Sustain. Dev., 34 (2014), pp. 887-897, 10.1007/s13593-014-0223-8
    View in ScopusGoogle Scholar Velásquez et al., 2019 S. Velásquez, N. Peña, J.C.
    Bohórquez, N. Gutierrez, G.L. Sacks Volatile and sensory characterization of roast
    coffees–Effects of cherry maturity Food Chem, 274 (2019), pp. 137-145 View PDFView
    articleView in ScopusGoogle Scholar Verma et al., 2014 U. Verma, F. Rossant, I.
    Bloch, J. Orensanz, D. Boisgontier Shape-based segmentation of tomatoes for agriculture
    monitoring ICPRAM (2014) Google Scholar Wang, Bochkovskiy and Liao, 2022 C.-Y.
    Wang, A. Bochkovskiy, H.-Y.M. Liao YOLOv7: Trainable bag-of-freebies sets new
    state-of-the-art for real-time object detectors arXiv Prepr. arXiv2207.02696 (2022)
    Google Scholar Yuan, 2023 Yuan, W., 2023. Accuracy Comparison of YOLOv7 and YOLOv4
    Regarding Image Annotation Quality for Apple Flower Bud Classification. AgriEngineering.
    https://doi.org/10.3390/agriengineering5010027. Google Scholar Cited by (0) #
    Equal contribution View Abstract © 2024 Elsevier B.V. All rights reserved. Part
    of special issue Sensing advances for tree and fruit physiological traits definition
    in horticulture Edited by Dr. Nikos Tsoulias, Dr. Luigi Manfrini, Dr. Trim Bresilla
    View special issue Recommended articles Artificial hybridization in the Salvia
    genus (S. aramiensis Rech. Fil., S. fruticosa Mill. and L.) for herbal tea production,
    determination of some morphologic and quality properties of chosen hybrids Scientia
    Horticulturae, Volume 327, 2024, Article 112875 Nadire Pelin Bahadirli, Filiz
    Ayanoglu View PDF ACC (1-aminocyclopropane-1-carboxylic acid) deaminase producing
    endophytic bacteria improve hydroponically grown lettuce in the greenhouse during
    summer season Scientia Horticulturae, Volume 327, 2024, Article 112862 Robert
    L. Chretien, …, Chuansheng Mei View PDF Fine mapping an AUXIN RESPONSE FACTOR,
    SmARF18, as a candidate gene of the PRICKLE LOCUS that controls prickle absence/presence
    on various organs in eggplant ( L.) Scientia Horticulturae, Volume 327, 2024,
    Article 112874 Shaohang Li, …, Huoying Chen View PDF Show 3 more articles Article
    Metrics Captures Readers: 54 View details About ScienceDirect Remote access Shopping
    cart Advertise Contact and support Terms and conditions Privacy policy Cookies
    are used by this site. Cookie settings | Your Privacy Choices All content on this
    site: Copyright © 2024 Elsevier B.V., its licensors, and contributors. All rights
    are reserved, including those for text and data mining, AI training, and similar
    technologies. For all open access content, the Creative Commons licensing terms
    apply."'
  inline_citation: '>'
  journal: Scientia Horticulturae
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Computer Vision-Aided Intelligent Monitoring of Coffee: Towards Sustainable
    Coffee Production'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Goyal P.
  - Sharda R.
  - Saini M.
  - Siag M.
  citation_count: '1'
  description: Neural computing methods pose an edge over conventional methods for
    drought stress identification because of their ease of implementation, accuracy,
    non-invasive approach, cost-effectiveness, and ability to predict in real time.
    To ensure proper irrigation scheduling and prevent major yield losses, the objective
    was to develop a deep learning (DL)-based custom convolutional neural network
    (CNN) framework for in situ identification and classification of drought stress
    in maize crops. An original image dataset was created by acquiring 2703 RGB images
    of maize crops under natural daylight conditions to incorporate noise and varied
    backgrounds. The dataset was augmented and divided in a ratio of 7:2:1 for the
    training, validation, and test sets. A custom-CNN model was built using feature
    blocks, fully connected layers, and dense layers, and compared with five state-of-the-art
    CNN architectures, i.e. InceptionV3, Xception, ResNet50, DenseNet121 and EfficientNetB1.
    The results revealed that the custom CNN model achieved accuracies of 98.71% and
    98.53% on the training and test sets, respectively. In comparison, the ResNet50
    and EfficientNetB1 transfer-learned CNN architectures achieved an equivalent accuracy
    of 99.26% each, followed by DenseNet121 with a 98.90% accuracy on the test set.
    The Xception model performed the worst, with the highest accuracy of 91.91% on
    the test set. The results demonstrate that the developed custom CNN model should
    be adopted for real-time implementation on resource-constrained edge devices because
    of the lower number of parameters (0.65 million parameters) compared to other
    state-of-the-art architectures.
  doi: 10.1007/s00521-023-09219-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Neural Computing and Applications
    Article A deep learning approach for early detection of drought stress in maize
    using proximal scale digital images Original Article Published: 17 November 2023
    Volume 36, pages 1899–1913, (2024) Cite this article Download PDF Access provided
    by University of Nebraska-Lincoln Neural Computing and Applications Aims and scope
    Submit manuscript Pooja Goyal, Rakesh Sharda , Mukesh Saini & Mukesh Siag  321
    Accesses Explore all metrics Abstract Neural computing methods pose an edge over
    conventional methods for drought stress identification because of their ease of
    implementation, accuracy, non-invasive approach, cost-effectiveness, and ability
    to predict in real time. To ensure proper irrigation scheduling and prevent major
    yield losses, the objective was to develop a deep learning (DL)-based custom convolutional
    neural network (CNN) framework for in situ identification and classification of
    drought stress in maize crops. An original image dataset was created by acquiring
    2703 RGB images of maize crops under natural daylight conditions to incorporate
    noise and varied backgrounds. The dataset was augmented and divided in a ratio
    of 7:2:1 for the training, validation, and test sets. A custom-CNN model was built
    using feature blocks, fully connected layers, and dense layers, and compared with
    five state-of-the-art CNN architectures, i.e. InceptionV3, Xception, ResNet50,
    DenseNet121 and EfficientNetB1. The results revealed that the custom CNN model
    achieved accuracies of 98.71% and 98.53% on the training and test sets, respectively.
    In comparison, the ResNet50 and EfficientNetB1 transfer-learned CNN architectures
    achieved an equivalent accuracy of 99.26% each, followed by DenseNet121 with a
    98.90% accuracy on the test set. The Xception model performed the worst, with
    the highest accuracy of 91.91% on the test set. The results demonstrate that the
    developed custom CNN model should be adopted for real-time implementation on resource-constrained
    edge devices because of the lower number of parameters (0.65 million parameters)
    compared to other state-of-the-art architectures. Similar content being viewed
    by others Automatic Flood Detection from Sentinel-1 Data Using a Nested UNet Model
    and a NASA Benchmark Dataset Article Open access 12 March 2024 The improved YOLOv8
    algorithm based on EMSPConv and SPE-head modules Article 02 January 2024 Plant
    disease detection using drones in precision agriculture Article Open access 28
    March 2023 1 Introduction Maize (Zea mays) or corn is the most widely cultivated
    and consumed crop throughout the world. The annual production of maize in the
    year 2021–22 reached 1.2 billion metric tons, with the USA being the largest producer
    [1]. It is the third most important cereal crop after wheat and rice, which contributes
    toward the global food security. However, maize crop is extremely sensitive to
    heat and drought stress [2] and its production will be significantly impacted
    by the climate change. Climate change will result in increased temperatures, change
    precipitation distribution patterns, and reduce the amount of rainfall received
    in a year, which will further accelerate the frequency of droughts worldwide [3].
    Maize, an extremely heat- and drought-sensitive crop [2], is one of the most negatively
    impacted crops [4]. In general, plant stress disrupts plant metabolism in many
    ways [5], resulting in reduced growth and significant crop yield reductions [6,7,8,9].
    Abiotic stress factors are also expected to become more frequent and intense as
    a result of climate change [10]. Among the various abiotic stresses, water/drought
    stress is the most prominent factor that severely impacts crop growth, physiology
    and ultimately affects crop yields. The physiological response of plants to an
    insufficient supply of water is referred to as drought stress. Drought was found
    to be responsible for an approximately 20–30% reduction in maize yield in China
    and India [11]. Another study reported a yield loss of 40% on a global scale [12].
    Drought stress resulted in 45–92% yield reductions in different crops [13]. The
    onset of drought varies with crop type, variety, and growth stage [14]. The identification
    of this stress is especially important for water-sensitive crops, such as maize
    [13]. Exposure of maize to drought conditions at the tasseling stage results in
    significant yield loss [15]. Maize crops may respond in either elastic (reversible)
    or plastic (irreversible) manner, depending on the degree of stress [10]. Therefore,
    it is pertinent to identify drought stress at a stage where the effects of stress
    can be reversed with timely irrigation to maximize the yield potential of the
    crop. However, identifying drought stress, particularly during the initial stages,
    is a challenging task. Traditionally, various direct and indirect methods have
    also been used for stress measurement. Lab-based methods that measure leaf water
    potential or leaf water content often involve destructive sampling. Other direct
    methods involve the installation of sophisticated intricate sensors in the field.
    These sensors are generally expensive and time-consuming, and the measurements
    are point-based and may not consider spatial variability in actual field conditions.
    The lack of continuous in situ monitoring has given way to indirect methods, such
    as remote or proximal sensing, which offer better alternatives for the precise
    and accurate identification of water stress. Imaging techniques offer a noninvasive
    and nondestructive method for plant stress detection. Depending on the range of
    the sensor, imaging can be either simple red-blue-green (RGB) imagery (acquired
    using digital cameras in the visible range of 400–700 nm) or thermal, multispectral,
    hyperspectral, or florescence imaging. Apart from visible-range imaging, other
    imaging options serve as expensive alternatives and are easily influenced by atmospheric
    variations. Digital RGB images captured on a proximal scale can be combined with
    advanced computer-vision methods to provide important insights into plant stress
    analyses. With recent advancements in the field of computer vision, artificial
    intelligence (AI)-based techniques such as machine learning (ML) and image processing
    techniques have been used widely for the detection and identification of different
    types of biotic and abiotic stresses using digital image-based datasets [16,17,18,19,20].
    Zhuang et al. [19], used SVM and GBDT to detect water stress in maize crop by
    creating a dataset comprising of three categories of images, i.e., well-watered,
    reduced water and drought stressed. The developed model achieved an accuracy of
    90.39% and gave comparable accuracy when tested on images from open-field conditions
    with variable illumination intensity. Although ML models have demonstrated great
    success in the recognition of plant stress, the process of manual feature extraction
    fails to generalize to a different task, causing a lack of automation. Therefore,
    the developed ML model is unsuitable for real-time field implementation. Deep
    learning (DL), which is essentially a subset of ML, simplifies the learning process
    by eliminating the need for manual feature extraction. Deep learning typically
    solves this problem by using various forms of neural networks. It performs hierarchical
    feature extraction using various convolution layers that automatically extract
    useful information from an image. Image classification saw a breakthrough through
    convolutional neural networks (CNN), a type of DL network, that has been used
    extensively for the identification and classification of different types of biotic
    and abiotic stresses using digital image datasets [21,22,23,24,25,26,27]. This
    network has alternate convolutional and pooling layers, followed by fully connected
    layers. This network extracts image features by dividing an image into small blocks
    and assigning individual weights. It has been documented that DL models generally
    provide higher accuracy than ML models. In a follow up study by [19], An et al.,
    [25], used transfer learning approach with ResNet50 and ResNet152 models to improve
    drought stress detection accuracy with the same dataset. The results revealed
    that the models trained from scratch took longer and were less accurate than the
    transfer-learned models. The transfer-learned ResNet50 and ResNet152 models achieved
    accuracies of 96% and 95.09%, respectively, and performed better than the SVM
    and GBDT. Azimi et al., [28] created a dataset of chickpea crop under controlled
    conditions to detect water stress at 3 stages, i.e., control, young seedling and
    before flowering. They utilized a variant of CNN-Long short-term memory (CNN-LSTM),
    which achieved an accuracy of 98.52% and 97.78% on the JG-62 and Pusa-372 varieties,
    respectively. Earlier studies leveraged freely available datasets to develop classification
    models for drought stress detection in different crops. However, most studies
    employed datasets that were built under strict imaging protocols, where the lighting,
    background, illumination, and angles were predefined. There is no doubt that these
    imaging protocols help reduce background noise and aid in enhancing accuracy,
    resulting in good models. However, these models lack robustness and perform poorly
    under actual field conditions, which limits the deployment of the developed model
    in real-time scenarios. Very few studies have reported the use of in situ image
    datasets with varied backgrounds and lighting intensities for plant stress detection
    [21, 29, 30]. The objective of this study was to develop a robust image classification
    model built on a maize crop dataset acquired in real field experiments to replicate
    actual field conditions. A custom CNN pipeline that can be used in real time will
    be built from scratch to detect drought stress in maize crops. Along with the
    custom CNN model, various state-of-the-art CNN architectures such as InceptionV3,
    Xception, ResNet50, DenseNet121, and EfficientNetB1 were transfer-learned and
    compared with our custom-CNN architecture to arrive at the best performing model.
    2 Methodology 2.1 Experimental setup The maize crop was planted in an open field
    environment at the Research Farm of the Department of Soil and Water Engineering,
    Ludhiana, during the spring (2021) and Kharif (2021) seasons. Seeds were sown
    on February 12, 2021, and July 3, 2021, for the spring and kharif seasons, respectively.
    The experimental fields were divided into four plots. Each main plot was further
    divided into sub-plots such that three different types of maize varieties (i.e.,
    PMH-08, PMH-10 and DKC-9108 in spring season and PMH-1, PMH-11, PMH-13 during
    Kharif season) were grown under full irrigation (100% of the recommended dose
    of irrigation) and deficit irrigation (20% deficit of the recommended dose of
    irrigation) under mulched conditions. The main plots were replicated four times.
    Surface drip irrigation was used to irrigate the raised beds. The seeds were planted
    on raised beds in paired rows. Different varieties of maize were sown manually
    using the dibbling method. The field soil was first tested for the available nutrients
    in the soil, and fertigation was applied in accordance with the package of practices
    recommended by PAU, Ludhiana. Full irrigation was applied in the initial stages
    of crop growth in both treatments to ensure normal sprouting. 2.2 Image acquisition
    Digital images were captured manually at regular intervals using a Sony DSC-HX400V
    RGB camera to capture healthy and drought-stressed images of maize crops under
    varied background and lighting conditions. The other camera specifications include
    an ISO of 80, shutter speed of 1/60, and aperture of f/5. For image acquisition,
    the camera angle was maintained at approximately 45° from the vertical axis of
    the plant. Because the images were captured in natural daylight conditions, the
    image background varied considerably from soil, mulch, sky, building, human subjects
    such as hands, dirt, etc., including the illumination intensity throughout the
    day. The incorporation of such varied backgrounds, along with sunlight intensity,
    would enhance the robustness of the proposed model and incorporate it directly
    without significant alterations. Most of the images were captured throughout the
    day, starting from the high-knee stage of the maize crop until the start of the
    tasseling stage. The crop growth period was selected based on the sensitivity
    of the maize crop to water stress during this stage. 2.3 Dataset preparation A
    total of 2704 images, each with a size of 5184 × 3888, belonging to two classes,
    the first being the healthy class (1421 images) and the second being the drought-stressed
    class (1283 images), were captured and labeled to create the original dataset.
    An image dataset comprising 1000 images per class is considered satisfactory for
    classification tasks using CNNs, as the world’s largest collection of datasets,
    called ‘ImageNet’, also consists of an average of 1000 images per class [31].
    [32] also used an original dataset comprising 656 original images to train a CNN
    model. The collected images were resized to (224 × 224) and rescaled to 1/255
    pixels for the custom CNN model. The entire dataset was divided in a ratio of
    7:2:1 for the training, validation, and test sets. The batch size was maintained
    at 32. The training set provides the data for the learning process to fit the
    model parameters, the validation set aids in feature selection, and fine-tunes
    the hyperparameters associated with the model. The developed model was then tested
    on the test set to obtain an unbiased evaluation of the developed model. The images
    in the training and validation sets were subjected to preprocessing techniques,
    as shown in Fig. 1. Some image augmentation techniques included horizontal and
    vertical flipping, rotation between 0–10 degrees, image zoom to the range of 20%,
    and width and height shift range maintained at 0.2, which shears the images to
    20% width and height. Augmentation was applied batch-wise on the fly. This means
    that each image in a training batch is subjected to random transformations and
    returns newly transformed data on the go. Data augmentation was applied to the
    training and validation sets, while the test set was not augmented. This process
    of applying random transformations makes the model robust and allows for multiprocessing,
    parallelization of the loading process, and faster computation. All models were
    run for 50 epochs. Fig. 1 Different types of pre-processing applied to the training
    and validation dataset Full size image 2.4 Development of the custom CNN framework
    The custom and other CNN architectures were written and executed in Python 3.8.8,
    using TensorFlow (2.8). TensorFlow is an open-source machine-learning library
    used to perform image classification using Keras. The software was run on a Windows
    PC with an Intel Core i5-6200U CPU 2.30 GHz and 8 GB RAM. The custom CNN framework
    is a sequential pipeline containing a stack of convolutional layers, which form
    the major building block of the CNN. It has a group of filters, also called kernels,
    that extract features, such as color, edges, or textures, which are useful for
    image characterization and form a feature or activation map. Activation functions
    add a nonlinear component to the output of the previous layer for faster convergence
    and to prevent the problem of vanishing gradients. A Rectified Linear Unit (ReLU)
    was used as an activation function in the CNN layers. Two convolutional layers
    were followed by a max-pooling layer, which selects the maximum value from each
    patch of the feature map generated by the convolutional filter. The output of
    this operation is a feature map that contains the most prominent features of the
    previous layers. This is followed by a dropout layer, which intentionally drops
    random information from the neurons of the previous layers before heading to the
    next layer to improve processing, reduce computational time, and avoid model overfitting.
    The dropped neurons were not updated during the backward pass. Thus, the custom
    CNN model consisted of four feature blocks, followed by a flattened layer. The
    flatten layer serves as a connection between the convolutional layers and the
    fully connected layer, which converts the resultant 2D arrays from the pooled
    feature maps to a single long continuous linear vector. Finally, the fully connected
    layers, also called dense layers, contain several neurons that receive input from
    all the neurons of the previous layer. The last dense layer contains two neurons,
    and it applies another activation function called “softmax” to assign the outputs
    of the previous layers to their specified target class. The Adaptive Moment Estimation
    (Adam), an advanced form of the stochastic gradient descent method, was used as
    an optimizer to adjust the learning rate (initially kept at 0.001) of the weights
    of the neural network. The sparse categorical cross-entropy loss function was
    used to calculate the model loss. This tends to minimize the difference between
    the actual and predicted outputs. For all models, the model weights were saved
    at every epoch when the validation accuracy improved. Similar architectures have
    been used by other researchers to customize CNN model architecture [32]. The custom-CNN
    model architecture used in this study is illustrated in Fig. 2. Fig. 2 Frame work
    of the custom CNN Model Full size image 2.5 Transfer learning architectures employed
    The custom CNN model was compared with five state-of-the-art CNN architectures:
    InceptionV3, Xception, ResNet50, DenseNet121, and EfficientNetB1. The transfer
    learning approach was used to leverage the learnings of these architectures trained
    on a ‘ImageNet’ dataset to solve a similar but different problem. In this approach,
    these pre-trained models are used as a starting point to solve the current problem
    instead of training the model from scratch. This approach improves the existing
    models for a specific use case and significantly reduces storage and computational
    resources to a great extent. The initial architecture is generally frozen to avoid
    any loss of information that these models have learned from previous tasks. Additional
    trainable layers were added on top of the frozen layers to apply the old features
    to the predictions of a new dataset. In our case, a max-pooling, flatten, and
    dense layer was added to the stack of frozen layers, and the models were then
    trained for 50 epochs. These CNN architectures employed for transfer learning
    require image size in specific formats (as mentioned in Table 1) and specific
    input data preprocessing, which were adjusted for individual models. The architecture
    of these models is briefly explained in the following paragraphs. Table 1 Key
    specifications of the architectures employed (obtained from the official Keras
    documentation) Full size table InceptionV3 is a 48-layer deep CNN model first
    introduced by [33]. This is an advanced version of GoogLeNet [34], which belongs
    to the family of Inception networks. InceptionV3 uses multiple convolution filters
    of different sizes that run in parallel layers, allowing it to extract input features
    at multiple scales. The factorized 7 × 7 convolutions, along with auxiliary classifiers,
    allow the propagation of label information down the network. The auxiliary classifiers
    as regularizers deal with the problem of vanishing gradients. Thus, InceptionV3
    allows efficient utilization of computational resources with efficient dimensionality
    reduction without compromising accuracy. Xception, also known as the extreme version
    of the Inception model, was first introduced by [35]. Its architecture comprises
    a linear stack of depth-wise separable convolutions with residual/skip connections.
    It first uses pointwise convolution to map the cross-channel correlations and
    maps the spatial correlations of every output channel separately. The ability
    of the model to look at the channel and spatial correlations independently in
    successive steps allows it to maintain cross-channel features along with a reduced
    parameter count. ResNet50 is a 50-layer deep CNN that uses residual connections
    to perform identity mapping [36]. The layers learn the residual functions with
    reference to the input layer instead of learning the unreferenced functions. The
    stacked residual layers that learn the residual mapping are added element-wise
    to form the network. Here, the state is passed from one ResNet module to another;
    ResNets can achieve higher accuracy, even at considerably high depths. DenseNets
    contain a series of densely connected blocks connected by transition layers. Each
    layer obtains “collective knowledge” from all preceding layers and passes on its
    own feature maps to all subsequent layers [37]. The feature maps of the previous
    maps were concatenated and used as inputs. Transition layers apply batch normalization,
    a 1 × 1 convolution and 2 × 2 pooling layers to downsample the information from
    the previous dense block. All layers, i.e., those within the same dense block
    and transition layers, spread their weights over multiple inputs which allows
    deeper layers to use features extracted early on. DenseNet-121 has 120 convolutions
    and 4 average pooling layers. Because of their unique connections between layers,
    they alleviate the problem of vanishing gradients with reduced number of parameters.
    EfficientNets are the latest developments in state-of-the-art CNN architectures.
    Originally proposed by [38], it uses a unique compound coefficient approach to
    uniformly scale all three dimensions of depth, width, and resolution instead of
    using a fixed set of scaling coefficients for optimal performance improvement.
    This architecture adjusts the layers of the network using scaling coefficients
    that are determined using a grid search method. The resulting coefficients were
    used to scale the baseline network to the desired target model size. This novel
    scaling in a structured manner renders its accuracy superior to that of other
    CNN architectures. The base EfficientNetB0 network is based on the mobile inverted
    bottleneck convolutions (MBConv), similar to MobileNetV2. The network used in
    this study was EfficientNetB1, which was obtained by scaling the EfficientNetB0
    network. A flowchart of the methodology used in this study is shown in Fig. 3.
    Fig. 3 Flowchart of the steps taken for classification of the drought stress in
    maize Full size image 2.6 Model evaluation metrics The performance of the custom
    and other CNN architectures employed was evaluated on the basis of evaluation
    metrics such as accuracy, sensitivity, specificity, precision, and F1-score. All
    these parameters are calculated in terms of true positives (TP), true negatives
    (TN), false positives (FP) and false negatives (FN). Here, TP and TN represent
    the number of correct predictions of the water-stressed and healthy maize crop,
    respectively. FP, also known as type 1 error, predicts the healthy maize class
    as being water stressed. FN, also known as type 2 error, predicts water-stressed
    maize plants to be healthy. The classification accuracy represents the ratio of
    the correct predictions of both stressed and healthy maize images to the total
    number of images in the test set, as given by Eq. (1). $${\\text{Accuracy}} =
    \\frac{{\\text{TP + TN}}}{{\\text{TP + TN + FP + FN}}}$$ (1) $${\\text{Precision}}
    = \\frac{{{\\text{TP}}}}{{\\text{TP + FP}}}$$ (2) $${\\text{Sensitivity}} = {\\text{Recall}}
    = \\frac{{{\\text{TP}}}}{{\\text{TP + FN}}}$$ (3) $${\\text{Specificity}} = \\frac{{{\\text{TN}}}}{{\\text{TN
    + FP}}}$$ (4) $$F1_{{{\\text{SCORE}}}} = \\frac{{2*{\\text{Precision}}*{\\text{Recall}}}}{{{\\text{Precision}}
    + {\\text{Recall}}}}$$ (5) The confusion matrix extends beyond the simple accuracy.
    It is a tabular representation of the model predictions versus the ground-truth
    labels. Here, each row represents the instances in the actual class whereas the
    columns represent the instances of the predicted class. In the case of binary
    classification, there is a 2 × 2 matrix and diagonal elements if the matrix represents
    the correct predictions in each category. A representation of the different elements
    of the confusion matrix for our problem is shown in Fig. 4. Precision evaluates
    the model based on positive predictions. It is the ratio of the correctly predicted
    stressed crop from the overall predictions of stressed crop. Recall or sensitivity
    refers to the ratio of the correct predictions of the stressed crop to the actual
    number of stressed crop images. Thus, the model was evaluated based on its ability
    to predict the stressed class. Specificity focuses on healthy class predictions.
    It measures the proportion of the healthy class correctly predicted as healthy.
    Both sensitivity and specificity are similar; however, sensitivity calculates
    the true stress rate, whereas specificity calculates the false stress rate. The
    F1-score represents the weighted average of precision and recall. It is a more
    useful measure for evaluating the accuracy of binary classification models, especially
    in the case of an imbalanced data distribution in the dataset. The best value
    for the F1-score is 1, and the worst is 0. All metrics are defined in Eqs. (1–5).
    Fig. 4 Different elements of the confusion matrix as represented in the result
    section Full size image 3 Results The results of the different models during the
    training and validation phases during the first 10 epochs are presented in Fig.
    5. The graphs show that almost all models had a smooth transition from low training
    accuracy during the first epoch to fairly high accuracy in the 10th epoch. Comparing
    the different models, the Xception and custom-CNN models started with lowest training
    accuracy of 61.66% and 69.55%, respectively, while the other 4 models started
    with comparatively higher initial accuracies. The DenseNet121, EfficientNetB1,
    InceptionV3 and ResNet50 models started with an initial training accuracy of 85.53%,
    89.03%, 91.40% and 93.33%, respectively, moving gradually to 98.92%, 99.09%, 98.93%
    and 98.82%, respectively, at the end of 10th epoch. Similar to the training accuracy
    trend, the training loss also decreased gradually with an increase in the number
    of epochs. The initial cross-entropy training loss was highest for the Xception
    model at 0.89, closely followed by DenseNet121, custom-CNN, ResNet50, InceptionV3
    and EffcientNetB1 having training losses of 0.62, 0.58, 0.56, 0.50, and 0.40,
    respectively. At the end of 10th epoch, the training loss was highest for Xception
    model at 0.30, followed by ResNet50, custom-CNN, InceptionV3, DenseNet121 and
    EfficientNetB1 where the training loss reduced to 0.15, 0.14, 0.04, 0.04 and 0.03,
    respectively. Fig. 5 Comparative performance of various DL models in terms of
    accuracy and loss during the training and validation phase for the first 10 epochs
    Full size image Despite the smooth transitions of accuracy and cross-entropy loss
    in the training phase, the validation accuracy and loss graphs were characterized
    by a fair share of peaks and troughs during the initial training phase of 10 epochs.
    Validation accuracy and validation loss kept fluctuating for Xception and custom-CNN
    models, while the other four models, i.e., ResNet50, InceptionV3, DenseNet121
    and EfficientNetB1 were relatively stable. Toward the end of 10th epoch, the validation
    accuracy was the highest for DenseNet121 at 99.61%, closely followed by InceptionV3,
    EfficientNetB1, ResNet50 and custom-CNN having validation accuracy of 98.82%,
    98.44%, 98.05% and 97.66%, respectively. Xception model had the least validation
    accuracy of 88.48% at the end of 10th epoch. A similar trend was observed for
    validation loss curves, where the validation loss was highest for Xception model
    at 0.277, followed by ResNet50, IncpetionV3, custom-CNN and EfficientNetB1 having
    validation loss of 0.20, 0.16, 0.083, 0.081 and 0.065, respectively. DenseNet121
    showed the least validation loss of 0.0112 at the end of the 10th epoch. The learning
    curves of all investigated models for the entire training duration of 50 epochs
    are shown in Fig. 6. Model checkpoints were added after every epoch to monitor
    the validation accuracy, and model weights were saved at every instance of increase
    in the validation accuracy. The graphs clearly show that the training and validation
    accuracy curves of all the models follow a smooth trend and are on an increasing
    trend after the 10th epoch. The training and validation accuracy curves of the
    custom-CNN, Xception, DenseNet121 and EfficientNetB1 were comparatively smoother
    than those of the InceptionV3 and ResNet50 models, which still had minor troughs
    as the training continued. The custom-CNN model achieved a train accuracy of 96.07%
    at the 13th epoch, followed by DenseNet121 (96.34% at 2nd epoch), InceptionV3
    (96.45% at 2nd epoch), EfficientNetB1 (97.79% at 2nd epoch) and ResNet50 (98.60%
    at 3rd epoch). The Xception model could not surpass an accuracy of 96% even after
    50 epochs. Owing to the model checkpoints, the best model was saved at the last
    increase in validation accuracy. Therefore, the last model weights for the model
    custom-CNN, Xception, ResNet50, InceptionV3, DenseNet121 and EfficientNetB1 were
    saved at 48th, 30th, 42nd, 38th, 25th and 26th epoch, respectively. These epochs
    correspond to a train-validation accuracy of (98.44–99.02%), (92.25–94.53%), (99.52–100%),
    (99.24–99.80%), (98.92–100%), (99.03–98.24%) for the custom-CNN, Xception, ResNet50,
    InceptionV3, DenseNet121 and EfficientNetB1 models, respectively. Fig. 6 Percentage
    change in accuracy and loss during the training and validation phase when the
    models were trained for 50 epochs a Custom-CNN, b Xception, c InceptionV3 d ResNet50,
    e DenseNet121 f EfficientNetB1 Full size image Upon close inspection, the learning
    curves for training and validation loss for the different models vary significantly
    from each other. The training and validation losses for the custom-CNN and Xception
    models decreased gradually over time with no major differences between them. Although,
    the training and validation loss were higher for Xception model, i.e. 0.18 and
    0.18 compared to the custom-CNN, i.e., 0.05 and 0.03, respectively, toward the
    end of 50th epoch, the training and validation loss for the InceptionV3 and ResNet50
    model did not decrease gradually. The graphs for both these models were characterized
    by higher peaks for the validation loss compared to the training loss, indicating
    instances of overfitting. The ResNet50 model showed higher instances of overfitting
    because of large gap between the train and validation loss, both in between the
    epochs and also toward the end of 50th epoch, compared to the InceptionV3 model.
    The train and validation loss at the end of the 50th epoch for the ResNet50 model
    was 0.03 and 0.27, making a generalization gap of 0.24 between the two loss curves.
    On the other hand, the training and validation loss for InceptionV3 model was
    0.08 and 0.09, respectively, much lesser gap than the ResNet50 model. The DenseNet121
    and EfficientNetB1 models were comparatively more stable and performed relatively
    better than the rest of the four models in terms of training and validation loss
    curves. The train and validation loss at the end of 50th epoch for DenseNet121
    was only 0.02 and 0.01, and for EfficientNetB1, it was 0.02 and 0.09, respectively.
    Although EfficientNetB1 showed higher training accuracy in the initial phases,
    it showed slight instances of overfitting as the plot for validation loss decreased
    initially but started increasing toward the end of the 50th epoch. Since the model
    weights for EfficientNetB1 were saved at 26th epoch, it was able to achieve lower
    training and validation loss. The generalization gap for EfficientNetB1 was also
    higher at 6.75% compared to DenseNet121, i.e.1.44%. From an overall perspective
    of both training and validation accuracy and loss curves, the DenseNet121 model
    achieved better performance compared to other networks. The trained models were
    evaluated on the test set containing 10% (272 images) of the total image dataset,
    which was not used previously in the model training process. The confusion matrices
    generated by the different models are shown in Fig. 7. The elements of the confusion
    matrix were in accordance with the representation shown in Fig. 4. Continuing
    a similar trend as exhibited by the previous graphs, the Xception model performed
    the worst on the test dataset, as it had the lowest predictions of TNs and TPs.
    The type-1 error, i.e., FP was also highest for Xception, followed by custom-CNN
    and DenseNet121, and nil for InceptionV3, ResNet50 and EfficientNetB1. In terms
    of type-2 error, i.e., FNs, was also highest for Xception, followed by InceptionV3
    and custom-CNN and least for ResNet50, DenseNet121 and EfficientNetB1 models.
    Different evaluation metrics were calculated from the generated confusion matrix
    on the test set, the values for which are given in Table 2. The test accuracy
    and F1-score were the highest for the ResNet50 and EfficientNetB1 models, followed
    by DenseNet121, custom-CNN, InceptionV3 and Xception models. The test accuracy
    and F1-score were the highest for the ResNet50 and EfficientNetB1 models, followed
    by DenseNet121, custom-CNN and InceptionV3. Xception achieved the lowest test
    accuracy of 91.91%. In terms of precision and specificity, which represent stressed
    crop and healthy class predictions, respectively, ResNet50 and EfficientNetB1
    scored the maximum percentage, followed by custom-CNN and DenseNet121 models.
    Xception model resulted in the least precision (97.35%) and specificity (97.90%)
    values. Recall/sensitivity was the highest for InceptionV3 model (98.53%), followed
    by ResNet50, DenseNet121 and EfficientNetB1, where the recall values were 98.45%
    each. Xception had the least specificity (85.27%). Although DenseNet121 performed
    better than the ResNet50 and EfficientNetB1 models on the training and validation
    datasets, the higher accuracies of the ResNet50 and EfficientNetB1 models can
    be attributed to the fact that the best model weights were saved whenever there
    was an improvement in the validation accuracy despite the peaked values in their
    validation loss curves. Fig. 7 Results of the confusion matrix as applied on the
    10% test dataset containing 272 images of the model a Custom-CNN b Xception c
    InceptionV3 d ResNet50 e DenseNet121 f EfficientNetB Full size image Table 2 Evaluation
    metrics of the different models Full size table Some of the sample predictions
    for the test set given by the custom-CNN model are shown in Fig. 8. The results
    showed that the custom-CNN model built from starch fairs well among the other
    transfer-learned architectures. Similar custom CNN models, which involve much
    lesser number of layers and have lower computational complexity have been developed
    by other authors as well, which performed well for the respective datasets. Zhuang
    et al. [39], used a shallow CNN with 6 convolutional layers to classify 3 categories
    of water stress recognition in maize. Similarly, a lightweight 6 layered CNN network
    was used by [40] to classify sick and healthy grapevine plants. Azimi et al. [27],
    also reported similar performance of the custom-CNN architecture with the VGG16
    and InceptionV3 models when applied for water stress detection in chickpea plant
    shoot images. The comparative evaluation metrics of the different models showed
    that the state-of-the-art transfer-learned architectures performed slightly better
    than the custom-CNN model. The results revealed that ResNet50 and EfficientNetB1
    models achieved highest F1-score of > 99% on the test sets. Different forms of
    ResNet architectures have been applied successfully in different studies and have
    achieved acceptable accuracies. ResNet50 achieved the highest accuracy among the
    seven CNN architectures employed for the classification of sugarcane lodged images
    from unlodged ones [41]. Esgario et al., [42] compared AlexNet, GoogleNet, VGG16,
    ResNet50 and MobileNetV2 for biotic stress classification and severity estimation
    in coffee leaves, where the ResNet50 architecture obtained the highest accuracy
    of 95.24% and 86.51%, respectively, on both the tasks and outperformed the rest
    of the architectures. Sunil et al. [43], used ResNet50 and VGG16 architectures
    to classify weeds from the crop under different backgrounds in greenhouse conditions,
    where the ResNet50 achieved higher average F1-score of 99% compared to the 92%
    in case of the VGG16 model. Fig. 8 Sample predictions of the custom-CNN model
    on the test set along with the confidence percentage Full size image EfficientNets
    are the most recent developments in the transfer-learning domain and have not
    been used extensively. In our study, although the EfficientNetB1 network became
    unstable during the later stages of the training, however, since the best weights
    were saved during the initial epochs, the model performance was better when applied
    to the test set. Atila et al. [44], compared VGG16, ResNet50, Inception V3 and
    all variants of EfficientNet (B0-B7) to classify 39 classes of Plant Village dataset,
    where the B4 and B5 variants of EfficientNet model outperformed the rest of the
    models under original as well as augmented dataset conditions. Candido-Mireless
    et al. [45], achieved an average accuracy and precision of 97.2% and 95.8% when
    the used EfficientNetB3 architecture to classify stressed images from healthy
    grape vineyard images in natural illumination conditions, compared to the other
    3 transfer learned architectures, i.e., VGG16, ResNet50 and InceptionV3. In addition,
    similar to our observations, the ResNet50 model showed higher variability during
    experimentation compared to other architectures. In another study, a variant of
    EfficientNet, called EfficientNetB4 was applied to classify nutritional deficiencies
    using RGB imagery in sugar beet and orange trees, where it achieved Top-1 accuracy
    of 98.65% and 98.52%, respectively, on both datasets [46]. Jung et al. [47], compared
    five different transfer learned architectures for crop classification, disease
    detection and disease classification in three different crops, out of which the
    EfficientNet model gave the best results. Shahoveisi et al., [48] classified rust
    disease in 3 field crops using Xception, ResNet50, EfficientNetB4, and MobileNet
    architectures, where the best results were attained by the EfficientNetB4 model
    (accuracy = 94.29%) followed by the ResNet50 (93.52%) model. In terms of stability,
    DenseNet121 was found to be the most stable model among all transfer-learned models
    used in this study. The performance of DenseNet121 (F1-score- 98.83%) was comparable
    to the best models in our use case. In another study, the accuracy of DenseNet121
    was second best (92.41%) after the ResNet50 (92.61%) on the test set, which achieved
    the highest accuracy among the 5 transfer-learned architectures for classification
    of biotic stresses in paddy crop [49]. DenseNets have also been found to perform
    well for leaf disease classification [50]. The results obtained after training
    the custom-CNN as well as other transfer-learned architectures reveal that all
    the models achieved an acceptable accuracy and precision levels for drought/water
    stress identification for maize crop and can be successfully deployed in real-time
    actual field conditions to quantify the maize stress in the form of an application
    on the IoT embedded and other mobile devices. However, since the majority of the
    IoT devices are characterized by constrained resources in terms of memory and
    computational requirements, the model selection for the deployment on these devices
    should also consider the size of the model. Figure 9 shows the number of trainable,
    non-trainable, and total parameters for the different CNN architectures evaluated
    in this study. The inference time of a deep learning model is directly proportional
    to the number of trainable parameters. By examining the trainable parameters,
    InceptionV3 had the lowest trainable parameters at 16,386, closely followed by
    DenseNet121 at 18,434 and EfficientNetB1 at 23,042. Custom-CNN had the highest
    number of trainable parameters, at 653,266, which was also the total number of
    parameters in this model. However, another factor to note from the graph is that
    the custom-CNN model has a significantly lower number of total parameters than
    the other transfer-learned architectures employed. Therefore, it is expected that
    although the inference time of the custom-CNN model might be slightly higher,
    it will require less energy and disk storage space in contrast to the other models
    which have a much higher number of parameters. The performance of a 23-layered
    simple custom-CNN architecture was comparable to ResNet and NasNet models, which
    are much larger in size for classification of three levels of nitrogen stress
    in sorghum crop [51]. Razfar et al. [52], also showed that a 5-layered custom
    CNN architecture showed higher detection accuracy (97.7%) with lower latency and
    memory usage for weed detection in soybean crops and were therefore recommended
    for edge-based vision systems. If the initial frozen layers of the employed state-of-the-art
    architectures were also fine-tuned instead of transfer learning, the number of
    trainable parameters in these architectures would have been much higher than that
    in the custom-CNN model. Also, the test accuracy of the best models, i.e., ResNet50
    and EfficientNetB1 was only higher by 1.03% compared to the custom-CNN model.
    Therefore, the developed custom-CNN model is a perfect fit for deployment on resource-constrained
    devices because it is small in size without much compromise on the accuracy front.
    Fig. 9 Graph showing the number of trainable, non-trainable and total parameters
    of the different CNN architectures Full size image One of the highlights of this
    study is that the dataset was acquired in actual field conditions and that too
    manually with a handheld camera; therefore, most of the images in the dataset
    do not possess a specific background or image of a particular plant at a fixed
    angle during different time steps. Variability in illumination intensity, background,
    and other objects surrounding the plant makes it suitable for mobile applications.
    The custom-CNN model can potentially achieve a high degree of automation. This
    will prove beneficial to potential farmers and researchers for timely intervention
    and mitigation of problems by applying proper crop management strategies that
    can effectively boost crop yields. 4 Conclusions The present work discusses the
    successful implementation of a custom-built CNN model from scratch with fewer
    layers having much fewer parameters and compares it against well-established transfer-learned
    architectures such as InceptionV3, Xception, ResNet50, DenseNet121, and EfficientNetB1
    to classify drought stress during the initial stages in a maize-crop dataset acquired
    during actual field experimentation under open conditions. This comprehensive
    assessment of different architectures helps us understand the superior model among
    the rest. Considering all the evaluation metrics applied to the test set, the
    best results were achieved by the InceptionV3, ResNet50, and EfficientNetB1 models.
    However, on the combined front of accuracy and stability, DenseNet121 was found
    to be a better choice among the rest of the models. The performance of Xception
    model was found to be the worst. The results exhibit that the performance of the
    custom-CNN model was equivalent to the other deeper and much heavier transfer-learned
    models, without significantly compromising on the accuracy front. The proposed
    custom-CNN model can be further integrated with real-time mobile applications
    to provide timely information regarding the onset of water stress. This study
    will be especially useful for farmer communities and irrigation managers, who
    can leverage this technology for drought stress detection at the field level in
    real time. Data availability Not applicable. References Anonymous (2022) https://www.statista.com/statistics/1156213/global-corn-production/#:~:text=In%202021%2F22%20marketing%20year,followed%20by%20China%20and%20Brazil
    Zhao F, Zhang D, Zhao Y, Wang W, Yang H, Tai F, Hu X (2016) The difference of
    physiological and proteomic changes in maize leaves adaptation to drought, heat,
    and combined both stresses. Front Plant Sci 7:1471. https://doi.org/10.3389/fpls.2016.01471
    Article   Google Scholar   Mukherjee S, Mishra A, Trenberth KE (2018) Climate
    change and drought: a perspective on drought indices. Current Climate Change Reports
    4(2):145–163 Article   Google Scholar   Tebaldi C, Lobell DJ (2018) Differences,
    or lack thereof, in wheat and maize yields under three low-warming scenarios.
    Environ Res Lett 13:065001 Article   Google Scholar   Zhang H, Sonnewald U (2017)
    Differences and commonalities of plant responses to single and combined stresses.
    Plant J 90:839–855. https://doi.org/10.1111/tpj.13557 Article   Google Scholar   Rejeb
    IB, Pastor V, Mauch-Mani B (2014) Plant responses to simultaneous biotic and abiotic
    stress: molecular mechanisms. Plants 3:458–475. https://doi.org/10.3390/plants3040458
    Article   Google Scholar   Pandey P, Irulappan V, Bagavathiannan MV, Senthil-Kumar
    M (2017) Impact of combined abiotic and biotic stresses on plant growth and avenues
    for crop improvement by exploiting physio–morphological traits. Front Plant Sci
    8:537. https://doi.org/10.3389/fpls.2017.00537 Article   Google Scholar   Ahmad
    B, Raina A, Khan S (2019) Impact of biotic and abiotic stresses on plants, and
    their responses. In: Wani S (ed) Disease resistance in crop plants. Springer,
    Cham, pp 1–20 Google Scholar   Cohen I, Zandalinas SI, Huck C, Fritschi FB, Mittler
    R (2021) Metaanalysis of drought and heat stress combination impact on crop yield
    and yield components. Physiol Plant 171:66–76. https://doi.org/10.1111/ppl.13203
    Article   Google Scholar   Salika R, Riffat J (2021) Abiotic stress responses
    in maize: a review. Acta Physiol Plantarum 43(9):130 Article   Google Scholar   Zaidi
    P, Yadav M, Maniselvan P, Khan R, Shadakshari T, Singh R, Pal D (2010) Morpho-physiological
    traits associated with cold stress tolerance in tropical maize (Zea mays L.).
    Maydica 55:201–208 Google Scholar   Daryanto S, Wang L, Jacinthe PA (2016) Global
    synthesis of drought effects on maize and wheat production. PLoS ONE 11:e0156362.
    https://doi.org/10.1371/journal.pone.0156362 Article   Google Scholar   Fahad
    S, Bajwa AA, Nazir U, Anjum SA, Farooq A., Zohaib A, Sadia S, Nasim W, Adkins
    S, Saud S, Ihsan M Z (2017) Crop production under drought and heat stress: plant
    responses and management options. Front Plant Sci p.1147. Demirevska K, Zasheva
    D, Dimitrov R, Simova-Stoilova L, Stamenova M, Feller U (2009) Drought stress
    effects on Rubisco in wheat: changes in the Rubisco large subunit. Acta Physiol
    Plant 31:1129–1138. https://doi.org/10.1007/s11738-009-0331-2 Article   Google
    Scholar   Anjum SA, Wang LC, Farooq M, Hussain M, Xue LL, Zou CM (2011) Brassinolide
    application improves the drought tolerance in maizethrough modulation of enzymatic
    antioxidants and leaf gas exchange. J Agron Crop Sci 197:177–185. https://doi.org/10.1111/j.1439-037X.2010.00459
    Article   Google Scholar   Singh AK, Ganapathysubramanian B, Singh AK, Sarkar
    S (2016) Machine learning for high throughput stress phenotyping in plants. Trends
    Plant Sci 21(2):110–124 Article   Google Scholar   Naik HS, Zhang J, Lofquist
    A, Assefa T, Sarkar S, Ackerman D, Singh A, Singh AK, Ganapathysubramanian B (2017)
    A real-time phenotyping framework using machine learning for plant stress severity
    rating in soybean. Plant Methods 13(1):1–2 Article   Google Scholar   Pantazi
    XE, Moshou D, Oberti R, West J, Mouazen AM, Bochtis D (2017) Detection of biotic
    and abiotic stresses in crops by using hierarchical self-organizing classifiers.
    Precis Agric 18(3):383–393 Article   Google Scholar   Zhuang S, Wang P, Jiang
    B, Li M, Gong Z (2017) Early detection of water stress in maize based on digital
    images. Comp Elect Agri 140:461–468 Article   Google Scholar   Anami BS, Malvade
    NN, Palaiah S (2020) Classification of yield affecting biotic and abiotic paddy
    crop stresses using field images. Info Process Agri 7(2):272–285 Google Scholar   Mohanty
    SP, Hughes DP, Salathe M (2016) Using deep learning for image-based plant disease
    detection. Front Plant Sci 7:1419 Article   Google Scholar   DeChant C, Wiesner-Hanks
    T, Chen S, Stewart EL, Yosinski J, Gore MA, Lipson H (2017) Automated identification
    of northern leaf blight-infected maize plants from field imagery using deep learning.
    Phytopathol 107(11):1426–1432 Article   Google Scholar   Ghosal S, Blystone D,
    Singh AK, Ganapathysubramanian B, Singh A, Sarkar S (2018) An explainable deep
    machine vision framework for plant stress phenotyping. Proc Natl Acad Sci 115(18):4613–4618
    Article   Google Scholar   Too EC, Yujian L, Njuki S, Yingchun L (2019) A comparative
    study of fine-tuning deep learning models for plant disease identification. Comp
    Elect Agri 161:272–279 Article   Google Scholar   An JY, Li WY, Li MS, Cui SR,
    Yue HR (2019) Identification and classification of maize drought stress using
    deep convolutional neural network. Symmetry 11(2):256 Article   Google Scholar   Picon
    A, Seitz M, Alvarez-Gila A, Mohnke P, Ortiz-Barredo A, Echazarra J (2019) Crop
    conditional convolutional neural networks for massive multi-crop plant disease
    classification over cell phone acquired images taken on real field conditions.
    Comp Elect Agri 167:105093 Article   Google Scholar   Anami BS, Malvade NN, Palaiah
    S (2020) Deep learning approach for recognition and classification of yield affecting
    paddy crop stresses using field images. Artif Intl Agri 4:12–20 Google Scholar   Azimi
    S, Wadhawan R, Gandhi TK (2021) Intelligent monitoring of stress induced by water
    deficiency in plants using deep learning. IEEE Trans Instrum Meas 70:1–13 Article   Google
    Scholar   Zhang X, Yue Q, Fanfeng M, Chengguo F, Mingming Z (2018) Identification
    of maize leaf diseases using improved deep convolutional neural networks. IEEE
    Access 6:30370–30377 Article   Google Scholar   Chandel NS, Chakraborty SK, Rajwade
    YA, Dubey K, Tiwari MK, Jat D (2020) Identifying crop water stress using deep
    learning models. Neural Comput Appl 17:1–15 Google Scholar   Deng J, Dong W, Socher
    R, Li L J, Li K, Fei-Fei L (2009) ImageNet: A large-scale hierarchical image database.
    In: Proceedings of the IEEE conference on computer vision and pattern recognition.
    Miami, FL, USA, pp 248–255 Zhuang S, Wang P, Jiang B, Li M (2020) Learned features
    of leaf phenotype to monitor maize water status in the fields. Comp Elect Agri
    172:105347 Article   Google Scholar   Szegedy C, Vanhoucke V, Ioffe S, Shlens
    J, Wojna Z (2016) Rethinking the inception architecture for computer vision. In:
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp 2818–2826 Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Rabinovich
    A (2015) Going deeper with convolutions. In: Proceedings of the IEEE conference
    on computer vision and pattern recognition, pp 1–9 Chollet F (2017) Xception:
    deep learning with depthwise separable convolutions. In: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 1251–1258 He K, Zhang X,
    Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings
    of the IEEE conference on computer vision and pattern recognition, pp 770–778
    Huang G, Liu Z, Van Der Maaten L, Weinberger K Q (2017) Densely connected convolutional
    networks. In: Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp 4700–4708 Tan M, Le Q (2019) Efficientnet: rethinking model scaling
    for convolutional neural networks. In: International conference on machine learning,
    pp 6105–6114. PMLR Zhuang S, Ping W, Boran J, Maosong L (2020) Learned features
    of leaf phenotype to monitor maize water status in the fields. Comp Elect Agri
    172:105347 Article   Google Scholar   Nagi R, Tripathy SS (2022) Deep convolutional
    neural network based disease identification in grapevine leaf images. Multimed
    Tools Appls 81(18):24995–25006 Article   Google Scholar   Modi RU, Chandel AK,
    Chandel NS, Dubey K, Subeesh A, Singh AK, Kancheti M (2023) State-of-the-art computer
    vision techniques for automated sugarcane lodging classification. Field Crops
    Res 291:108797 Article   Google Scholar   Esgario JG, Krohling RA, Ventura JA
    (2020) Deep learning for classification and severity estimation of coffee leaf
    biotic stress. Comp Elect Agri 169:105162 Article   Google Scholar   Sunil GC,
    Koparan C, Ahmed MR, Zhang Y, Howatt K, Sun X (2022) A study on deep learning
    algorithm performance on weed and crop species identification under different
    image background. Artif Intell Agri 6:242–256 Google Scholar   Atila U, Ucar M,
    Akyol K, Ucar E (2021) Plant leaf disease classification using EfficientNet deep
    learning model. Ecol Inform 61:101182. https://doi.org/10.1016/j.ecoinf.2020.101182
    Article   Google Scholar   Candido-Mireles M, Hernandez-Gama R, Salas J (2023)
    Detecting vineyard plants stress in situ using deep learning. Comp Elect Agri
    210:107837 Article   Google Scholar   Espejo-Garcia B, Malounas I, Mylonas N,
    Kasimati A, Fountas S (2022) Using EfficientNet and transfer learning for image-based
    diagnosis of nutrient deficiencies. Comp Elect Agri 196:106868 Article   Google
    Scholar   Jung M, Song JS, Shin AY, Choi B, Go S, Kwon SY, Park J, Park SG, Kim
    YM (2023) Construction of deep learning-based disease detection model in plants.
    Sci Rep 13(1):7331 Article   Google Scholar   Shahoveisi F, Taheri Gorji H, Shahabi
    S, Hosseinirad S, Markell S, Vasefi F (2023) Application of image processing and
    transfer learning for the detection of rust disease. Sci Rep 13(1):5133 Article   Google
    Scholar   Malvade NN, Yakkundimath R, Saunshi G, Elemmi MC, Baraki P (2022) A
    comparative analysis of paddy crop biotic stress classification using pre-trained
    deep neural networks. Artif Intell Agri 6:167–175 Google Scholar   Sai Reddy B,
    Neeraja S (2022) Plant leaf disease classification and damage detection system
    using deep learning models. Multimed Tools Appl 81(17):24021–24040 Article   Google
    Scholar   Azimi S, Kaur T, Gandhi TK (2021) A deep learning approach to measure
    stress level in plants due to Nitrogen deficiency. Measurement 173:108650 Article   Google
    Scholar   Razfar N, True J, Bassiouny R, Venkatesh V, Kashef R (2022) Weed detection
    in soybean crops using custom lightweight deep learning models. J Agri Food Res
    8:100308 Google Scholar   Download references Acknowledgements This research was
    supported by IIT Ropar Technology and Innovation Foundation (iHub – AWaDH) for
    Agriculture and Water Technology Development Hub, established by the Department
    of Science & Technology (DST), Government of India, at the Indian Institute of
    Technology, Ropar in the framework of National Mission on Interdisciplinary Cyber
    Physical Systems (NM – ICPS). Author information Authors and Affiliations Department
    of Soil and Water Engineering, Punjab Agricultural University (PAU), Ludhiana,
    India Pooja Goyal, Rakesh Sharda & Mukesh Siag Department of Computer Science
    and Engineering, IIT Ropar, Rupnagar, India Mukesh Saini Corresponding author
    Correspondence to Rakesh Sharda. Ethics declarations Conflict of interest The
    authors declare that they have no known competing financial interests or personal
    relationships that could influence the work reported in this study. Additional
    information Publisher''s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Springer Nature or its licensor (e.g. a society or other partner) holds exclusive
    rights to this article under a publishing agreement with the author(s) or other
    rightsholder(s); author self-archiving of the accepted manuscript version of this
    article is solely governed by the terms of such publishing agreement and applicable
    law. Reprints and permissions About this article Cite this article Goyal, P.,
    Sharda, R., Saini, M. et al. A deep learning approach for early detection of drought
    stress in maize using proximal scale digital images. Neural Comput & Applic 36,
    1899–1913 (2024). https://doi.org/10.1007/s00521-023-09219-z Download citation
    Received 30 May 2023 Accepted 21 October 2023 Published 17 November 2023 Issue
    Date February 2024 DOI https://doi.org/10.1007/s00521-023-09219-z Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords CNN Transfer learning Abiotic stress Computer vision Image classification
    Use our pre-submission checklist Avoid common mistakes on your manuscript. Sections
    Figures References Abstract Introduction Methodology Results Conclusions Data
    availability References Acknowledgements Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Neural Computing and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A deep learning approach for early detection of drought stress in maize using
    proximal scale digital images
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Leepkaln R.L.
  - Ré A.M.d.
  - Wiggers K.L.
  citation_count: '0'
  description: Potato is a widely consumed food worldwide, and its productivity has
    increased due to new varieties and the use of technologies related to irrigation,
    nutrition, and soil preparation, among others. However, diseases such as late
    blight disease can often affect the crop, impacting many farmers around the world.
    As a way to help production, technology in agriculture is increasing. Among the
    various computational techniques that can be applied, those based on digital image
    processing associated with machine learning algorithms stand out, producing excellent
    results. This work aimed to develop a methodology for recognizing late blight
    disease in potato leaves using digital image processing techniques and machine
    learning algorithms. It was possible to obtain promising results. The experiments
    were carried out in a set of images from a public database containing images of
    healthy and unhealthy leaves (with late blight). We compare the performance of
    machine learning algorithms using feature vectors obtained with SIFT algorithm
    and RGB descriptors. The best performance was using the Decision Tree algorithm
    and SIFT vectors, with 99.24% of accuracy.
  doi: 10.1007/978-3-031-53036-4_12
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Optimization,
    Learning Algorithms and Applications OL2A 2023: Optimization, Learning Algorithms
    and Applications pp 164–177Cite as Home Optimization, Learning Algorithms and
    Applications Conference paper Identification of Late Blight in Potato Leaves Using
    Image Processing and Machine Learning Renan Lemes Leepkaln , Angelita Maria de
    Ré & Kelly Lais Wiggers   Conference paper First Online: 03 February 2024 63 Accesses
    Part of the book series: Communications in Computer and Information Science ((CCIS,volume
    1982 )) Abstract Potato is a widely consumed food worldwide, and its productivity
    has increased due to new varieties and the use of technologies related to irrigation,
    nutrition, and soil preparation, among others. However, diseases such as late
    blight disease can often affect the crop, impacting many farmers around the world.
    As a way to help production, technology in agriculture is increasing. Among the
    various computational techniques that can be applied, those based on digital image
    processing associated with machine learning algorithms stand out, producing excellent
    results. This work aimed to develop a methodology for recognizing late blight
    disease in potato leaves using digital image processing techniques and machine
    learning algorithms. It was possible to obtain promising results. The experiments
    were carried out in a set of images from a public database containing images of
    healthy and unhealthy leaves (with late blight). We compare the performance of
    machine learning algorithms using feature vectors obtained with SIFT algorithm
    and RGB descriptors. The best performance was using the Decision Tree algorithm
    and SIFT vectors, with 99.24% of accuracy. Keywords Automatic disease recognition
    Digital Image Machine Learning Computer Vision Access provided by University of
    Nebraska-Lincoln. Download conference paper PDF 1 Introduction Potatoes are one
    of the most consumed foods on the planet, second only to dairy products, wheat,
    and rice. The tuber is planted in more than 130 countries, covering about 20 million
    hectares and producing more than 400 million tons of potatoes annually. In Brazil,
    it is estimated that there are 100,000 hectares of the crop spread across different
    states of Brazil, generating a production of 3.8 million tons, according to the
    Brazilian Institute of Geography and Statistics [1]. Productivity has been increasing
    due to more productive varieties and new technologies related to irrigation, nutrition,
    soil preparation, and the use of quality seed potatoes. Most of this production
    is sold fresh, with around 25–30% destined for industrial processing, in the form
    of frozen pre-fried (500,000 tons), chips, and straw potatoes (250,000 tons) [22].
    However, diseases can affect the plantation, and these are usually caused by microorganisms,
    such as bacteria, fungi, nematodes, and viruses, but a lack or excess of essential
    factors for plant growth, such as nutrients, water, and light can also cause them.
    The growing maturation and development of agricultural technology, in general,
    is remarkable, with the emergence of new applications and software to facilitate
    the day-to-day life of agricultural producers. Among these, techniques based on
    digital image processing are widely used. Image processing performs operations
    on an image to obtain an improved image or extract some useful information from
    it. It is a type of signal processing where the input is an image, and the output
    can be an image or features associated with that image. Several techniques can
    be explored, mainly in agriculture, due to the variety of plants that can present
    diseases. Furthermore, along with these processing techniques, machine learning
    algorithms can help detect diseases, pests, or even production estimates, such
    as the work of Kadir et al. [14] and Trindade and Basso [25]. Specifically in
    disease identification, Arnaud et al. [2] aimed to develop an application that
    identifies disease diagnoses using deep learning techniques. Pallathadka et al.
    [19] detected grape diseases. Lawrence et al. [18] to recognize leaf diseases
    using individual lesions. Carmo et al. [6] identified the best time and sensor
    to detect the disease caused by Pectobacterium carotovorum in lettuce, using images
    obtained by multispectral sensors in Unmanned Aerial Vehicle (UAV). Therefore,
    this paper aims to develop a methodology for recognizing late blight disease in
    potato leaves by exploring digital image processing techniques and machine learning
    algorithms. This way, image samples of healthy potato leaves and leaves with late
    blight disease were selected to form a database. We performed experiments using
    image processing techniques to obtain the best representation from this base.
    Finally, relevant algorithms from the literature were trained and evaluated, allowing
    the definition of a promising approach to identifying the disease. The main contribution
    was finding a feature extraction strategy and a traditional machine learning algorithm,
    which bring promising results, avoiding more complex structures, such as deep
    learning approaches. The rest of the paper is organized as follows: Sect. 2 describes
    the previous works of disease detection of different plants and Sect. 3 explains
    the dataset and the proposed approach. The experimental results are illustrated
    in Sect. 4. Finally, Sect. 5 concludes the work, including the future plans. 2
    Related Work Machine learning algorithms involving classification tasks are increasingly
    important and popular. The main goal of algorithms is to learn a general rule
    that maps inputs to outputs correctly. Several techniques have been used to detect
    and categorize plant diseases. These various techniques could be observed in the
    relevant and recent literature articles that we found, in which the authors performed
    disease classification on images of agricultural data. Hughes [16] created a database
    with 50,000 expertly curated images of agricultural leaves available on the PlantVillage
    online platform. The goal was to develop an application that identifies disease
    diagnoses using machine learning techniques. This database allows several researchers
    to use it in different experiments. For example, Pallathadka et al. [19] used
    Support Vector Machine (SVM), Naïve Bayes, and Convolution Neural Network (CNN)
    to detect diseases of rice leaves. The SVM was the algorithm with the better performance,
    with 96.2% accuracy. Ngugi et al. [18] recognize leaf diseases using individual
    lesions. A segmentation algorithm was trained using only 90 hand-labeled critical
    illness images. However, manually labeling or cutting lesions that do not have
    clearly defined borders but gradually merge with healthy leaf tissue makes it
    difficult to be consistent when labeling lesions manually. The proposed automatic
    lesion segmentation techniques work in experimental settings. Carmo et al. [6]
    identified the best time and the best sensor to detect the disease caused by Pectobacterium
    carotovorum in lettuce, using images obtained by multispectral sensors in an Unmanned
    Aerial Vehicle (UAV). For this, an experiment was installed in a greenhouse at
    the Federal University of Uberlândia, Mount Carmel Campus. The authors used Support
    Vector Machine (SVM) and Naive Bayes (NB) classifiers to evaluate groups of data
    composed of spectral bands, vegetation indices, and the combination of bands and
    indices obtained from a conventional visible camera and a Mapir Survey 3W multispectral
    camera, as well as agronomic parameters. Trindade et al. [25] investigate three
    computational methods to detect soybean leaves. Thus, they carried out a systematic
    literature review to map the methods applicable to the problem, characterizing
    techniques for detecting plant diseases and generating important information.
    They selected three methods for training Neural Networks Convolutional (RNCs),
    and each one was tested and compared, generating very motivating results that
    prove the efficiency of using image data augmentation techniques as an essential
    pre-processing step. In the case of potatoes, Biondo et al. [4] present a system
    developed to classify the types of potato disease, such as late blight and early
    blight, based on leaf conditions, using Deep Learning as a model of conventional
    neural network architecture, through the proposed model had an accuracy of 92.57%
    in identifying diseases. Islam et al. [13] implemented AVM to classify healthy
    leaves and those affected by late and early blight diseases using 300 potato images.
    The system’s cross-validated accuracy was 95%. Sanjeev et al. [24] implemented
    a classifier based on a neural network to predict and classify potato image samples.
    The Feed Forward Neural Network (FFNN) Model was used to predict and classify
    unknown leaves. The accuracy of the model is achieved at 96.5%. Pires et al. [21]
    used Convolutional Neural Networks (CNN) to classify tree species by leaf images.
    The work compared Darknet-19 and GoogLeNet (Inception-v3). The Darknet and GoogLeNet
    models achieved recognition rates of 86.2% and 90.3%, respectively. Generally,
    many researchers use ANN and Deep Learning as algorithms for training the classifier.
    However, this may require more computational resources and relies on a large volume
    of data for training. Thus, we decide to explore some traditional feature extraction
    techniques and machine learning algorithms to classify late blight disease and
    compare the results of our approaches. Fig. 1. Development Stages Full size image
    3 Methodology Machine learning is one of the areas of artificial intelligence
    that uses algorithms and mathematics, specifically statistics, to accomplish learning
    tasks. Currently, various databases are available where experiments can be performed
    integrating techniques for automatic recognition. Thus, the need arose to process
    and obtain useful information from these databases. Consequently, the machine
    learning area gained prominence since it is impracticable to process and analyze
    data currently available manually, and tasks can be automated, simulating human
    behavior [8]. However, it is necessary to carry out studies and define stages
    of data preparation, as well as the algorithms and parameters to be implemented.
    Therefore, for the development of this work, four main stages were defined: 1.
    Image search and selection; 2. Preprocessing; 3. Feature extraction and machine
    learning algorithm training, and 4. analysis of results. The developed method
    includes exploratory experiments with digital image processing techniques to allow
    the formation of feature vectors for each image in the database. With these vectors,
    experiments were carried out with supervised machine learning algorithms to allow
    the automatic identification of leaves that have late blight disease. Figure 1
    shows the schematic of the order in which the development steps are carried out,
    and the descriptions are presented below. The technologies used for the development
    of this project were: Python programming language; Libraries: Numpy; Pandas; csv;
    Tqdm; Sklearn; Skimage; OpenCV; Matplotlib; posthoc. 3.1 Image Search and Selection
    The database used in the experiments is formed by selecting public images from
    the Plant Village website Footnote 1. This database is a collection of plant and
    leaf images and corresponding metadata, which are used to train machine learning
    models in plant disease recognition and diagnosis tasks. In this way, there are
    4864 images in the selected dataset. The first class is formed by 2432 images
    of healthy leaves. The second class is formed by 1216 slightly diseased and 1216
    extremely diseased. Figure 2 shows healthy and late blight leaf examples, respectively.
    Fig. 2. Example of selected leaf images from PlantVillage database Full size image
    3.2 Preprocessing At this stage, firstly, all images were resized to 256\\(\\,\\times
    \\,\\)256, since, in the Plant Village database, by default, the photos are of
    different sizes or even show deformation. Moreover, all images must have the same
    size for an effective feature extraction process. Furthermore, the images of the
    database are in RGB color space. RGB is the most common color space used in image
    processing, with one channel each for red, green, and blue colors. All other colors
    are produced only by the proportions of these three colors, where zero corresponds
    to black, and as the value increases, the intensity increases. However, considering
    that the images may have luminosity, contrast, and color problems, the images
    were transformed from the RGB color space to HSV (Hue, Saturation, Value) to improve
    the luminosity. In color space HSV, the image consists of three channels: Hue,
    Saturation, and Value. A color component represents the H; the S suggests the
    percentage of the color; usually, this value is between 0 and 1, and the V represents
    the intensity of the chosen color and varies from 0 to 100 [10]. Thus, we equalized
    the S channel histogram after transforming the image to HSV. Subsequently, the
    images were converted back to RGB. The histogram equalization aims to balance
    the distribution of pixel frequency values, reducing accentuated differences and
    highlighting previously unnoticed details. The algorithm normalizes the brightness
    and increases the contrast of the image. Figure 3 presents an example of equalizing
    the histogram of an RGB image. Thus, given an image src, the equalization is given
    by steps 1 to 4, according [10] and OpenCV implementation Footnote 2. 1. Calculate
    the histogram H for src. 2. Normalize the histogram so that the sum of histogram
    bins is 255. 3. Compute the integral of histogram using: $$\\begin{aligned} H_{i}^{''}
    = \\sum _{0\\le i\\le L}^{}H(i) \\end{aligned}$$ (1) 4. Transform the image using
    \\(H^{''}\\) as a look-up table: $$\\begin{aligned} img_{eq}(x,y) = H^{''}(src(x,y))
    \\end{aligned}$$ (2) Fig. 3. Example of histogram equalization. Source: the authors.
    Full size image Fig. 4. Example of points of interest found by the SIFT extractor.
    Source: the authors. Full size image 3.3 Feature Extraction The feature extraction
    stage aimed to extract the information of interest (sheet), generating the feature
    vectors of the images. For this work, we evaluated two types of characteristics:
    Automatic vectors extracted by the Scale-Invariant Feature Transform (SIFT) algorithm:
    It starts by identifying the patterns, called descriptors, that is, characteristic
    points, looking for the edges of each object. Edges in the image are discovered
    by calculating the magnitude of the x- and y-axis frequencies using a kind of
    “high-pass\" filter to enhance the edges of the image. Once this task is completed,
    the method tests and evaluates the possibility of image standard deviation creating
    different highlights in the image [15]. However, this algorithm requires the formation
    of a vocabulary to generate the feature vectors using the K-means algorithm. Figure
    4 shows an example of identifying SIFT descriptors in a leaf image. Extracted
    vectors based on color information: feature vector from the RGB color space: This
    is the simplest form of representation. The number of features will be a one-dimensional
    matrix containing the color value of the pixels of the images. For this scenario,
    as previously described, the images have dimensions 256 in width and 256 in height,
    formed by 3-channel gray scales, which are the R, G, and B channels. The total
    number of pixels would be 256\\(\\,\\times \\,\\)256\\(\\,\\times \\,\\)3, the
    average values of the images for each pixel i of each channel defined by Eq. 3.
    $$\\begin{aligned} \\begin{matrix} RGB_{vector} = \\sum _{n=0}^{i}\\frac{R_i +
    G_i + B_i}{3}; & with & 0< i\\le L \\end{matrix} \\end{aligned}$$ (3) R is the
    red channel, G is the green channel, B is the blue channel, and L is the limit
    edge of the 256-pixel image, then transformed into a single 1-dimensional channel
    with the same width and height. 3.4 Machine Learning Machine learning models were
    selected to carry out the experiments in this step. Therefore, the objective is
    to identify patterns based on the features provided by the images in the database.
    As an experimental protocol, we split the database into 70% for training and validation,
    and 30% for tests. Also, in the training set, we performed ten runs to analyze
    the results [7], and cross-validation (k-folds) with different values of K. This
    technique consists of randomly dividing the database into K subsets (K is previously
    defined), with approximately the same amount of samples in each of them. At each
    iteration, training, and validation, a set formed by K-1 subsets is used for training,
    and the remaining subset will be used for validation, generating a metric result
    for evaluation. For the experiments, the values of K = 5 were defined, with sets
    formed in a stratified manner. These experiments were performed to avoid overfitting
    or underfitting. Overfitting means that the model has a high accuracy score on
    the training data but a low score on the test data, i.e. the model is not generalized.
    On the other hand, underfitting occurs when the algorithm used to build the prediction
    model has low performance already on the training data [5]. After some experiments
    and state-of-the-art studies, we define the machine learning models: Random Forest
    (RF) [20], Decision Tree (DT) [17], K-Nearest Neighbors (KNN) [3], Support Vector
    Machine (SVM), LinearSVM [12] and Stochastic Gradient Descent (SGD) [23]. 3.5
    Metrics for Evaluating Results The experiments were evaluated using statistical
    analysis through Accuracy (Eq. 4), Precision (Eq. 5), Recall (Eq. 6), and Confusion
    Matrix, because they are essential tools for evaluating the performance of a classification
    algorithm. [9, 11]. $$\\begin{aligned} accuracy = \\frac{TP + TN}{TP + TN+ FP
    +FN} \\end{aligned}$$ (4) where TP: true positive, TN: true negative, FP: false
    positive, and FN: false negative $$\\begin{aligned} precision = \\frac{TP}{TP
    + FP} \\end{aligned}$$ (5) $$\\begin{aligned} recall = \\frac{TP}{TP + FN} \\end{aligned}$$
    (6) The Kruskal-Wallis statistical test, with a significance of 0.5, was applied
    to the results. The Kruskal-Wallis tests the null hypothesis that the population
    median of all algorithms being compared is equal. The test works on two or more
    independent samples with different sizes. If the value (p) is less than or equal
    to 0.05, the hypothesis that all samples have equal medians is rejected with 95%
    confidence, i.e., there are at least one pair of algorithms that is significantly
    different between them. Otherwise, the hypothesis is accepted, the performance
    of all algorithms is statistically equivalent. Rejecting the null hypothesis does
    not indicate which of the algorithms is different. Thus, post hoc comparisons
    between algorithms are needed to determine which algorithms differ. This work
    used Conover [7] as a post hoc test. 4 Experimental Results Firstly, the images
    were read using the OpenCV library and stored as a tuple containing the image
    name and the image itself. Then, experiments were carried out using pre-processing
    and image feature extraction techniques. Table 1 shows the database’s pre-processing
    results applied to healthy and late blight leaves. Thus, it is possible to observe
    that regions of the leaf that present late blight disease were more intensified
    in the pre-processed image. In addition, green color pigments and sparkles also
    had increased contrast, mainly in disease leaves. Table 1. Examples of preprocessing
    results applied in dataset imagens Full size table Next, we describe the results
    using feature extraction techniques and machine learning algorithms. The first
    technique used was the SIFT algorithm to detect the points of interest in each
    image and extract the corresponding descriptors, totaling 2214 descriptors. The
    descriptors were stored in a list. Subsequently, we used the K-Means clustering
    algorithm with 50 clusters to group the descriptors into similar clusters, creating
    a vocabulary to describe the features of the images. Once the descriptors were
    grouped into clusters, a histogram was created for each image using the results
    of running K-Means. Each descriptor is assigned to the closest cluster, and the
    histogram is created by counting the occurrence in the image. Finally, we saved
    the histograms in a text file for later use in the classification task. Therefore,
    4864 training samples were created, each with 50 feature descriptors. Afterward,
    the base was separated for training, validation, and test set. Thus, 3404 samples
    were used for training/validation, k-folds with k = 5, and 1460 images for testing.
    All analyzes are based on ten runs independent of the algorithms and calculated
    the mean values for accuracy, precision, and recall. The results of the experiments
    on the test samples are in Table 2. Table 2. Results of experiments using SIFT
    vectors (in %) Full size table It is possible to observe in Table 2 that the algorithms
    SGDClassifier and LinearSVM presented lower performance. The best results use
    Decision Tree and Knn algorithms, with 99.24% accuracy. In general, the results
    obtained were satisfactory, as using the SIFT algorithm allowed the creation of
    robust image descriptors. Applying the K-Means clustering algorithm allowed the
    identification of similar features between the images, mainly observing the proportion
    between precision and recall. In this way, it is possible to conclude that using
    SIFT descriptors is a promising alternative for identifying late blight through
    images of the leaves. The Decision Tree model was selected to analyze the confusion
    matrix (Table 3). Thus, 1460 images were selected for test samples. There was
    low confusion in differentiating healthy leaves from diseased ones; only 11 leaves
    were classified incorrectly. Table 3. Confusion matrix using SIFT descriptors
    and Decision Tree algorithm Full size table The second set of experiments used
    RGB descriptors as feature vectors. Table 4 shows that the best performance was
    obtained using the SGD algorithm, with approximately 79% accuracy, followed by
    the LinearSVM with only 0.05% difference. Analyzing the SGD confusion matrix (Table
    5), 759 healthy leaves and 392 diseased leaves were classified correctly. However,
    the major confusion was in 305 samples that were classified incorrectly as healthy
    leaves. In addition, the results obtained using vectors with the average of RGB
    colors presented better precision, however, low recall. This shows that color
    features may not be able to correctly distinguish diseased parts of potato leaves.
    Thus more experiments must be performed in further works, preparing the feature
    vectors. Table 4. Results of experiments using RGB descriptors (in %) Full size
    table Table 5. Confusion matrix using RGB descriptors and SGD algorithm algorithm
    Full size table Fig. 5. Pair-by-pair analysis of post-hoc testing of machine learning
    algorithms on the data set Full size image Figures 5a and 5b show the statistical
    analysis, and when \\(p < 0.05\\) (p-value) means that there is a statistical
    difference in favor of the method that has highest mean accuracy. In these figures,
    the p values represented by the green colors (the greater the color intensity,
    the greater the difference between the algorithms) indicate statistical differences
    between the pair of algorithms in row x column. The pink color represents statistical
    equivalence. For both sets of descriptors, the statistical tests show that there
    is a statistical difference between some pairs of compared versions. Except for
    the SIFT-KNN and SIFT-DT, RGB-LinearSVM and RGB-SGD, and RGB-RF and RGB-SVM. Many
    papers with similar problems do not provide the bases used in their experiments.
    However, with the samples selected for the experiments, promising results were
    obtained in identifying late blight on potato leaves. For comparison, the most
    recent papers that classify potato diseases were in Table 6. Though methods like
    artificial neural networks and deep neural networks, applied by [4] and [24],
    show good accuracies, they come with high computational costs. In addition, [4]
    used a deep learning approach, however, with few samples of healthy and diseased
    leaves. This implies performing data augmentation, with we did not perform in
    our work. Thus, we can observe that our proposed method, using Decision Tree and
    SIFT vectors, outperformed the results. Table 6. Comparison of the state of the
    art Full size table 5 Conclusion The present study aimed to develop a methodology
    for recognizing late blight disease in potato leaves using digital image processing
    techniques and machine learning algorithms. Firstly, we collected images from
    a public database. Through a bibliographic survey, available images were found,
    as well as the main image processing techniques used. We used two feature extraction
    techniques to perform comparative experiments. We observe that using vectors of
    the RGB color space with the use of the SGD algorithm presented promising results.
    However, the SIFT points presented robust feature vectors, which increased the
    results, mainly using DT and KNN model. In addition, we evaluated the significant
    difference between machine learning models and concluded some significant differences
    between the algorithms evaluated. In future work, we intend to conduct experiments
    on a specific database, created from the collection of images of greenhouse potato
    leaves located in the Cedeteg campus of Unicentro, Paraná. In addition, further
    research is needed to identify other models of automatic feature extraction and
    machine learning algorithms, allowing to increase in the amount of the image database,
    with greater data variability. Notes 1. https://plantvillage.psu.edu/. 2. https://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html?highlight=equalizehist.
    References Abba: Situação atual da produção de batata no Brasil. Batata Show 20(58)
    (2020) Google Scholar   Arnaud, S.E., Rehema, N., Aoki, S., Kananu, M.L.: Comparison
    of deep learning architectures for late blight and early blight disease detection
    on potatoes. Open J. Appl. Sci. 12(5), 723–743 (2022) Google Scholar   AWS: Amazon
    sagemaker documentation (2023). https://docs.aws.amazon.com/sagemaker/index.html
    Biodo, D.R.: Classificação de doenças em batata baseado em imagens das folhas
    de batata utilizando Deep Learning. Masters, UFSCar (2021) Google Scholar   Bonaccorso,
    G.: Machine Learning Algorithms. Packt Publishing, Birmingham (2017) Google Scholar   Carmo,
    G., Castoldi, R., Martins, G., Castoldi, R., Zilvani, A.: Detecção de podridão
    mole em alface por Pectobacterium carotovorum subsp. carotovorum por algoritmos
    de aprendizado de máquina a partir de imagens multiespectrais. Master in agriculture
    and geospatial information, Universidade Federal de Uberlândia (2021) Google Scholar   Conover,
    W.J.: Practical Nonparametric Statistics, 3rd edn. Wiley, Hoboken (1999) Google
    Scholar   Escovedo, T.: Machine learning; conceitos e modelos - parte i: Aprendizado
    supervisionado (2020). https://tatianaesc.medium.com/machine-learning-conceitos-e-modelos-f0373bf4f445
    Ferri, C., Hernández-Orallo, J., Modroiu, R.: An experimental comparison of performance
    measures for classification. Pattern Recogn. Lett. 30(1), 27–38 (2009) Article   Google
    Scholar   Gonzalez, R., Woods, R.: Processamento digital de imagens, vol. 3. Pearson
    Prentice Hall, Upper Saddle River (2010) Google Scholar   Hossin, M., Sulaiman,
    M.N.: A review on evaluation metrics for data classification evaluations. Int.
    J. Data Mining Knowl. Manag. Process 5(2), 1 (2015) Article   Google Scholar   Iniyan,
    S., Jebakumar, R., Mangalraj, P., Mohit, M., Nanda, A.: Plant disease identification
    and detection using support vector machines and artificial neural networks. In:
    Dash, S.S., Lakshmi, C., Das, S., Panigrahi, B.K. (eds.) Artificial Intelligence
    and Evolutionary Computations in Engineering Systems. AISC, vol. 1056, pp. 15–27.
    Springer, Singapore (2020). https://doi.org/10.1007/978-981-15-0199-9_2 Chapter   Google
    Scholar   Islam, M., Dinh, A., Wahid, K., Bhowmik, P.: Detection of potato diseases
    using image segmentation and multiclass support vector machine, pp. 1–4 (2017).
    https://doi.org/10.1109/CCECE.2017.7946594 Kadir, A., Nugroho, L., Susanto, A.:
    Performance improvement of leaf identification system using principal component
    analysis. J. Theor. Appl. Inf. Technol. 44, 113–124 (2021) Google Scholar   Lowe,
    D.G.: Distinctive image features from scale-invariant keypoints. Int. J. Comput.
    Vision 60, 91–110 (2004). https://doi.org/10.1023/B:VISI.0000029664.99615.94 Article   Google
    Scholar   Hughes, D., Salathe, M.: An open access repository of images on plant
    health to enable the development of mobile disease diagnostics (2015). https://arxiv.org/abs/1511.08060
    Mada, M.S.: Decision trees algorithms (2017). https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1
    Ngugi, L.C., Abdelwahab, M., Abo-Zahhad, M.: A new approach to learning and recognizing
    leaf diseases from individual lesions using convolutional neural networks. Inf.
    Process. Agric. 10(1), 11–27 (2023) Google Scholar   Pallathadka, H., Ravipat,
    P., Phashinam, G.S.K., Kassanuk, T., Sanchez, T.: Application of machine learning
    techniques in rice leaf disease detection. Mater. Today Proc. 51, 2277–2280 (2022)
    Article   Google Scholar   Paul, A., Mukherjee, D.P., Das, P., Gangopadhyay, A.,
    Chintha, A.R., Kundu, S.: Improved random forest for classification. IEEE Trans.
    Image Process. 27(8), 4012–4024 (2018). https://doi.org/10.1109/TIP.2018.2834830
    Article   MathSciNet   Google Scholar   Pires, W.O., Fernandes, R.C., de Paula
    Filho, P.L., Candido Junior, A., Teixeira, J.P.: Leaf-based species recognition
    using convolutional neural networks. In: Pereira, A.I., et al. (eds.) OL2A 2021.
    CCIS, vol. 1488, pp. 367–380. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-91885-9_27
    Chapter   Google Scholar   Rampazo, A.: Cenário atual da cultura da batata e os
    principais desafios (2020). https://www.agrolink.com.br Russel, S., Norvig, P.:
    Artificial Intelligence - A Modern Approach, 4th edn. Pearson, Boston (2022) Google
    Scholar   Sanjeev, K., Gupta, N.K., Jeberson, W., Paswan, S.: Early prediction
    of potato leaf diseases using ANN classifier. Orient. J. Comput. Sci. Technol.
    13(2), 129–134 (2021) Article   Google Scholar   Trindade, L., Basso, F.: Investigando
    técnicas de processamento de imagens com IA na detecção de ferrugem em folhas
    de soja. Professional master’s in software engineering, Universidade Federal do
    Pampa (2021) Google Scholar   Download references Author information Authors and
    Affiliations Midwestern Parana State University, Guarapuava, Paraná, Brazil Renan
    Lemes Leepkaln & Angelita Maria de Ré Federal Technological University of Paraná,
    Pato Branco, Paraná, Brazil Kelly Lais Wiggers Corresponding author Correspondence
    to Kelly Lais Wiggers . Editor information Editors and Affiliations Instituto
    Politécnico de Bragança, Bragança, Portugal Ana I. Pereira University of Azores,
    Ponta Delgada, Portugal Armando Mendes Instituto Politécnico de Bragança, Bragança,
    Portugal Florbela P. Fernandes Instituto Politécnico de Bragança, Bragança, Portugal
    Maria F. Pacheco Instituto Politécnico de Bragança, Bragança, Portugal João P.
    Coelho Instituto Politécnico de Bragança, Bragança, Portugal José Lima Rights
    and permissions Reprints and permissions Copyright information © 2024 The Author(s),
    under exclusive license to Springer Nature Switzerland AG About this paper Cite
    this paper Leepkaln, R.L., Ré, A.M.d., Wiggers, K.L. (2024). Identification of
    Late Blight in Potato Leaves Using Image Processing and Machine Learning. In:
    Pereira, A.I., Mendes, A., Fernandes, F.P., Pacheco, M.F., Coelho, J.P., Lima,
    J. (eds) Optimization, Learning Algorithms and Applications. OL2A 2023. Communications
    in Computer and Information Science, vol 1982 . Springer, Cham. https://doi.org/10.1007/978-3-031-53036-4_12
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-53036-4_12
    Published 03 February 2024 Publisher Name Springer, Cham Print ISBN 978-3-031-53035-7
    Online ISBN 978-3-031-53036-4 eBook Packages Computer Science Computer Science
    (R0) Share this paper Anyone you share the following link with will be able to
    read this content: Get shareable link Provided by the Springer Nature SharedIt
    content-sharing initiative Publish with us Policies and ethics Download book PDF
    Download book EPUB Sections Figures References Abstract Introduction Related Work
    Methodology Experimental Results Conclusion Notes References Author information
    Editor information Rights and permissions Copyright information About this paper
    Publish with us Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Communications in Computer and Information Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Identification of Late Blight in Potato Leaves Using Image Processing and Machine
    Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chang C.L.
  - Chen H.W.
  - Ke J.Y.
  citation_count: '0'
  description: Complex farmland backgrounds and varying light intensities make the
    detection of guidance paths more difficult, even with computer vision technology.
    In this study, a robust line extraction approach for use in vision-guided farming
    robot navigation is proposed. The crops, drip irrigation belts, and ridges are
    extracted through a deep learning method to form multiple navigation feature points,
    which are then fitted into a regression line using the least squares method. Furthermore,
    deep learning-driven methods are used to detect weeds and unhealthy crops. Programmed
    proportional–integral–derivative (PID) speed control and fuzzy logic-based steering
    control are embedded in a low-cost hardware system and assist a highly maneuverable
    farming robot in maintaining forward movement at a constant speed and performing
    selective spraying operations efficiently. The experimental results show that
    under different weather conditions, the farming robot can maintain a deviation
    angle of 1 degree at a speed of 12.5 cm/s and perform selective spraying operations
    efficiently. The effective weed coverage (EWC) and ineffective weed coverage (IWC)
    reached 83% and 8%, respectively, and the pesticide reduction reached 53%. Detailed
    analysis and evaluation of the proposed scheme are also illustrated in this paper.
  doi: 10.3390/agriculture14010057
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all                         Journals Topics Information Author Services
    Initiatives About Sign In / Sign Up Submit   Search for Articles: Agriculture
    All Article Types Advanced   Journals Agriculture Volume 14 Issue 1 10.3390/agriculture14010057
    Submit to this Journal Review for this Journal Propose a Special Issue Article
    Menu Academic Editor Jiyu Li Subscribe SciFeed Recommended Articles Related Info
    Link More by Authors Links Article Views 878 Table of Contents Abstract Introduction
    Autonomous Navigation and Selective Spraying Scheme Description of the Farming
    Robot Experiments and Results Conclusions Author Contributions Funding Institutional
    Review Board Statement Informed Consent Statement Data Availability Statement
    Acknowledgments Conflicts of Interest References share Share announcement Help
    format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse textsms
    Comment first_page settings Order Article Reprints Open AccessArticle Robust Guidance
    and Selective Spraying Based on Deep Learning for an Advanced Four-Wheeled Farming
    Robot by Chung-Liang Chang *, Hung-Wen Chen and Jing-Yun Ke Department of Biomechatronics
    Engineering, National Pingtung University of Science and Technology, Neipu 91201,
    Taiwan * Author to whom correspondence should be addressed. Agriculture 2024,
    14(1), 57; https://doi.org/10.3390/agriculture14010057 Submission received: 5
    December 2023 / Revised: 24 December 2023 / Accepted: 26 December 2023 / Published:
    28 December 2023 (This article belongs to the Special Issue Advances in Modern
    Agricultural Machinery) Download keyboard_arrow_down     Browse Figures Versions
    Notes Abstract Complex farmland backgrounds and varying light intensities make
    the detection of guidance paths more difficult, even with computer vision technology.
    In this study, a robust line extraction approach for use in vision-guided farming
    robot navigation is proposed. The crops, drip irrigation belts, and ridges are
    extracted through a deep learning method to form multiple navigation feature points,
    which are then fitted into a regression line using the least squares method. Furthermore,
    deep learning-driven methods are used to detect weeds and unhealthy crops. Programmed
    proportional–integral–derivative (PID) speed control and fuzzy logic-based steering
    control are embedded in a low-cost hardware system and assist a highly maneuverable
    farming robot in maintaining forward movement at a constant speed and performing
    selective spraying operations efficiently. The experimental results show that
    under different weather conditions, the farming robot can maintain a deviation
    angle of 1 degree at a speed of 12.5 cm/s and perform selective spraying operations
    efficiently. The effective weed coverage (EWC) and ineffective weed coverage (IWC)
    reached 83% and 8%, respectively, and the pesticide reduction reached 53%. Detailed
    analysis and evaluation of the proposed scheme are also illustrated in this paper.
    Keywords: agricultural robot; deep learning; selective spraying; autonomous navigation;
    agricultural practices 1. Introduction In the rapid development of smart technologies,
    their integration into agriculture has been critical to combating labor scarcity
    and an aging workforce. Contemporary practices in crop management, including plant
    monitoring, watering, pesticide spraying, and fertilizing, are still often performed
    manually using machines and tools. This requires farmers to focus on field operations
    for a long time, which results in long working hours and high labor costs. The
    proven efficacy of automation in performing monotonous tasks has seen its adoption
    across various sectors, with its applicability in agriculture being equally comprehensive,
    encompassing activities like weeding and harvesting [1]. Traditional small-scale
    agricultural robots were designed to navigate based on sensor fusion methods,
    which are suitable for structured environments [2]. Relying on the improvement
    of precision agriculture technology, the real-time kinemetric (RTK) global navigation
    satellite system (GNSS) (RTK-GNSS) and machine vision have played an important
    role in automatic guidance technology. RTK-GNSS has been pivotal in providing
    precise metrics for positioning, velocity, and timing, which assists users in
    planning the robot’s movement path [3,4]. In particular, after the user can define
    multiple specific positions to form a path in a known field environment, the robot
    can then autonomously move to that point to perform field operations and reduce
    errors through heading control [5,6]. With the improvement of computer computing
    performance, machine vision technology has been used to identify, track, and measure
    targets and perform image processing [7]. Its technology has low development costs,
    is easy to maintain, and has wide applicability [8,9,10,11,12,13,14]. Morphology-based
    methods have been used to extract guide lines from rice field images, enabling
    autonomous weeding robots to operate without damaging crops [9,10]. This approach
    initially involved grayscale or CIE-Lab color space images, followed by Otsu thresholding
    and thinning processes to extract the edges of plant objects. The traditional
    crop row detection method often fails due to the influence of excessive ambient
    light or crop occlusion and other issues. Therefore, the method of using soil
    distribution information to find guidance points has been proven to be able to
    correctly find crop lines [11]. Post-identification Hough transform operations
    are utilized for edge detection, and the median lines between them serve as navigation
    lines, guiding unmanned vehicles through field operations autonomously [12]. The
    method of obtaining path fitting from grayscale images of specific areas of interest
    through image segmentation, navigation point extraction, and predicted point Hough
    transforms has been proven to be effective to improve the computational efficiency
    of the traditional Hough transform [13]. Meanwhile, this method can also solve
    the problem of insufficient accuracy caused by using the least squares method.
    Based on the above description, the Hough transform and least squares method were
    the most commonly used path fitting methods in crop row identification. Among
    them, Hough processing easily extracts feature edge lines and then obtains the
    crop lines. The least squares rule is a statistical method of regression analysis
    which can fit a navigation path with acceptable accuracy. Although both methods
    can detect row guide lines, different environmental conditions, such as variations
    in color, brightness, saturation, contrast, reflection, shadow, occlusion, and
    noise in the same scene, can lead to the failure of guidance line extraction [15,16].
    Secondly, differences in wind intensity in the field will also cause plant movement,
    which will blur the plant image and cause inaccurate crop center point detection
    [17,18,19]. With the advancements in high-speed computing technology, employing
    deep learning for navigation line extraction has gained traction. A U-Net deep
    learning model has been used to detect crops without interruption in the field,
    given favorable weather conditions, suitable lighting, few weeds, and neatly arranged
    crops. Finally, the Hough transform operation was used to identify the guidance
    lines [20]. The crop row extraction method based on Tiny-YOLOv4 can quickly detect
    multiple objects in an image and extract crop feature points within the frame
    through binarization operations and mean filtering operations. Finally, regression
    analysis with the least squares method was used to fit a guidance line [21]. An
    object detection method combining YOLO-R and density-based spatial clustering
    of applications with noise (DBSCAN) can quickly identify the number of crop rows
    and the crops in each row. Crop row lines can be found through the least squares
    method, and under different rice growth stages, the crop row recognition rate
    reaches at least 89.87% [22]. Deep learning methods have also been frequently
    employed in robotics for the identification of weeds and crops. Various research
    endeavors highlight the effectiveness of these advanced techniques in precision
    agriculture. Among them, the YOLO-based method has been commonly used to detect
    weeds in the field [23,24,25,26,27,28]. Ruigrok et al. [23] used the trained YOLO
    v3 model to detect weeds and spray them. The results showed that 96% of the weeds
    were controlled, but about 3% of the crops were sprayed by mistake. Twelve types
    of weeds in rice fields were detected by the trained YOLO v4 model, with an accuracy
    of 97% and an average detection time of 377 ms [24]. A weeding robot with deep
    learning developed by Chang et al. [25] could remove weeds at a speed of 15 cm
    per second, with an efficiency rating of 88.6%. The trained YOLO v5 model was
    utilized for weed (Solanum rostratum Dunal) detection, and the accuracy and recall
    rate of the model were 95% and 90%, respectively [26]. The YOLO-sesame model was
    used to identify weeds and crops in sesame fields, and its results showed a mean
    accuracy (mAP) of 96.1% at a frame rate of 36.8 frames per second (FPS) [27].
    Utilizing the YOLO v3 model for weed detection, as detailed by Ruigrok et al.,
    they trained it on image data from 20 different fields and tested it in 5 different
    arable fields [28]. The results indicated that increasing the variance in training
    data while keeping the sample size constant could reduce the generalization error
    during detection. Five deep learning models were used to detect weeds in soybean
    fields, with a custom five-layer CNN architecture showing a high detection accuracy
    of 97.7% and the lowest latency and memory usage [29]. With the four-wheel steering
    mechanism and flexible steering control method, the robot can move on any terrain
    on the site with high maneuverability and avoid slipping. Common steering control
    methods are based on Ackerman steering principle technology [30,31] combined with
    proportional–integral–derivative (PID), fuzzy logic, and sliding mode control
    [5,6,12,32]. The purpose of efficient guidance and control systems for agricultural
    robots is to accurately perform tasks such as spraying and weeding. However, a
    large part of existing research is still limited to the field of offline simulation
    or laboratory experiments, or they were only used to demonstrate crop row guidance
    performance, with little empirical evidence to support their applicability in
    real-world agricultural task operations. In the real field, the surface appearance
    of field soil is constantly changing in farmland. During fallow periods, farmland
    may exhibit only furrows or what are referred to as drip irrigation belts interspersed
    between ridges. In contrast, the planting season may present a mix of crops and
    ridges without the consistent presence of irrigation belts or, in some instances,
    exclusively crops, contingent upon individual agricultural practices. Many studies
    often focus on feature extraction of single objects in the field. Once the features
    of objects in the field are unclear or do not exist, the method used often loses
    the guidance line, especially in low-light environments. Compounding these challenges
    is the reliance on a singular type of object for training datasets, thereby critically
    hampering the universality and adaptability of the detection models. Aside from
    that, open field images are often used for crop line detection. In practice, these
    images are often exposed, causing the detection model to be unable to identify
    crop row lines. It is uncertain whether these methods can be used for detection
    during robot motion or achieve the same detection performance. Furthermore, field
    testing and validation of these integrated approaches for steering control and
    task execution remains challenging. In this study, the proposed scheme was used
    to automatically detect potential guidance lines on field ridges with deep learning
    and least squares regression, using a PID controller and fuzzy logic controller
    (FLC) to maintain the travel speed and heading angle. By adopting the one-stage
    object detection framework, the robot operation system was tailored for various
    object recognition tasks such as crop identification, drip irrigation belt detection,
    ridge recognition, weed detection, and identification of crops with nutrient deficiencies.
    It was also specifically designed to analyze and compare the object detection
    performance of the trained models at different FPS and obtain the real-time processing
    performance of the detection model in a field under different weather conditions.
    In terms of field operations, the smart sprayers were designed to spray nutrient-deficient
    crops as well as weeds. The organization of this paper is as follows. Section
    2 introduces the methodology, including the motion model of a farming robot, guidance
    line generation, methods for controlling the speed and heading of the robot, and
    spraying operation. Section 3 describes the configuration of each module within
    the robot.Section 4 discusses the experimental results, including tests of autonomous
    guidance of the robot, identification of weeds and unhealthy crops, and tests
    of selective spraying system performance. Finally, Section 5 provides the conclusions,
    summarizing the main findings of this study. 2. Autonomous Navigation and Selective
    Spraying Scheme 2.1. Motion Model Given the constant and relatively slow travel
    speed of the robot, along with its rigid tires, its motion state at any given
    moment can be described using a bicycle model [33] as shown in Figure 1a. The
    global X-Y coordinate plane is a fixed horizontal plane upon which the robot moves
    and is used to describe its motion. It was assumed that 𝑂 , 𝑂 𝑓 , and 𝑂 𝑟 represent
    the center of gravity of the robot, the center of the rear wheel, and the center
    of the front wheel, respectively. The distance between the center of the front
    wheel and the robot’s center of gravity is denoted as 𝐿 𝑓 , while 𝐿 𝑟 is the distance
    between the center of the rear wheel and the center of gravity of the robot. The
    slip angle was represented by α, the heading angle by θ, and the speed at the
    center of gravity by ν, with its component velocities being 𝑥 ˙ and 𝑦 ˙ . This
    motion model is based on front-wheel steering, assuming the direction of the rear
    wheels is parallel to the robot body. The kinematic model of the robot is represented
    by ( 𝑥 ˙ , 𝑦 ˙ , 𝜃 ˙ ): 𝑥 ˙ =𝑣cos(𝜃+𝛼) (1) 𝑦 ˙ =𝑣sin(𝜃+𝛼) (2) 𝜃 ˙ = 𝑣cos𝛼tan𝛿
    𝐿 𝑓 + 𝐿 𝑟 (3) Figure 1. The motion mode of the robot. (a) A bicycle model for
    straight-line movement of the robot. (b) The steering angle of the four wheels
    in spin-on-the-spot mode. |·| stands for an absolute value operation. The velocity
    𝑣 is given by 𝑣= 𝑣 𝑓 cos𝛿 +𝑣 𝑟 2cos𝛼 , where 𝑣 𝑓 and 𝑣 𝑟 represent the velocities
    of the front and rear wheel, respectively. The slip angle 𝛼 is calculated as 𝛼=
    tan −1 ( 𝐿 𝑟 tan𝛿 𝐿 𝑓 + 𝐿 𝑟 ) . When the robot moves in a straight line, the steering
    angle δ of both of the front wheels is within the range from −δmax to +δmax, and
    the steering angle of both the rear wheels is fixed at zero. Moreover, during
    turning maneuvers, all wheels assumed the same steering angle |𝛿| , putting the
    robot in a state of on-the-spot rotation (Figure 1b). 2.2. Guidance Line Generation
    First, the top in-view image of the field is captured by a digital camera. The
    target objects in the image consist of ridges, crops, or a drip irrigation belt
    (Figure 2a). In order to form a guidance line, the original image is divided into
    two sub-images through a masking operation, which covers the long strip area in
    the center of the image with a white color (see Figure 2b). Subsequently, deep
    learning techniques are utilized to detect the ridges, crops, or drip irrigation
    belt within the image, as shown in Figure 2c. Meanwhile, the center point of each
    object is also extracted. Figure 2. An xample of object detection in the image
    of the ridge. (a) The original image. (b) The original image divided into two
    sub-images through masking processing. (c) An example of object detection results.
    Red dot = center point of object; green box = detected objects (ridges, crops,
    or drip irrigation belt). The tool used for labeling the ridges, crops, and drip
    irrigation belt in the image was LabelImg, which executes the LabelImg script
    through Anaconda. This tool marks the positions of objects in each image and generates
    an XML file containing information about the objects and their positions, providing
    training data for the dataset. In this study, YOLO v4 was used as the object detector
    [34], and its architecture is based on YOLO v3, which was proposed by Joseph Redmon
    [35]. YOLO v4 is proficient at identifying small objects at high speeds while
    maintaining a certain level of recognition accuracy. The architecture of YOLO
    v4 includes three core parts: the backbone, the neck, and the detection head.
    As shown in Figure 3, CSPDarknet53 serves as the backbone network for the object
    detector. Its structure is based on DenseNet, which functions to connect layers
    in a convolutional neural network and adds a cross-stage partial network (CSPNet)
    [36]. Splitting and merging techniques are used to obtain a more efficient flow
    of gradient information and improve the accuracy of gradient calculations. The
    deep features of the image are then introduced into the neck layer, which separates
    the smallest-scale features from the backbone and pools multiple sizes to increase
    the receptive field. Figure 3. The framework of YOLO v4. The Path Aggregation
    Network (PANet) uses feature maps formulated through spatial pyramid pooling (SPP)
    [37] and CSP-Darknet53 at each level to perform multiple scaling operations sequentially.
    It transfers spatial information from the lower layers to the top ones with minimal
    loss to achieve more precise localization. YOLO v4, similar to YOLO v3, employs
    one-stage object detectors as detection heads. These YOLO heads are used for fusion
    and interaction with feature maps of different scales to detect objects. In YOLO
    v4, the “bag of specials” (BoS) [34] and “bag of freebies” (BoF) [38] tools are
    deployed to improve the network performance. The use of BoS tools increases the
    inference time but can significantly enhance the performance of the network. In
    contrast, BoF contains several data augmentation techniques that improve the model
    accuracy without increasing the inference time. The complete intersection over
    union (CIOU) loss, drop block regularization, cutMix, mosaic augmentation techniques,
    etc. are packaged in BoF. The BoS features include Mish activation, SPP, a spatial
    attention module (SAM), DIOU-NMS [39], and PANet blocks. The loss function is
    an important indicator for evaluating the quality of the detection model in object
    detection [40]. In YOLO v4, the total loss, 𝑃 𝐿𝑜𝑠𝑠 , comprising object classification
    loss ( L OC ), confidence loss ( L OF ), and regression loss (LOCI), is defined
    by Equation (4): 𝑃 𝐿𝑜𝑠𝑠 = 𝜖 1 L OC + 𝜖 2 L OF + 𝜖 3 L OCI (4) where 𝜖 1 , 𝜖 2
    , and 𝜖 3 represent the balancing coefficients, which are usually all set to one.
    L OC and L OF are measured using the cross-entropy operation, similar to YOLO
    v3 [35]. L OCI is predicted based on the CIOU algorithm, which calculates the
    positional loss between the predicted bounding box ( 𝜑 ′ ) and ground truth (
    𝜑 ), as illustrated in Figure 4, while 𝜑       denotes the minimum outer
    bounding box encompassing both the predicted bounding box and the ground truth:
    CIOU=IOU(𝜑,  𝜑 ′ )− 𝑢 2 𝑐 2 −𝜀𝜌 (5) where 𝑢 represents the distance between the
    center point of 𝜑 ′ and 𝜑 while 𝑐 is the diagonal distance between 𝜑     
     and 𝜀=𝜌/(1−IOU) + 𝜌 . The intersection over union (IOU) is given by IOU=|𝜑∩
    𝜑 ′ |/|𝜑∪ 𝜑 ′ | . The symbols “ ∩ ” and “ ∪ ” depict the intersection and union
    operation, respectively. The adjustment factor 𝜌 is shown in Equation (6): 𝜌=
    (tan −1 (W g /H g )− tan −1 (W p /H p )) 2 0.25 𝜋 2 (6) where W g and  H g denote
    the width and height of 𝜑 , respectively, while W p and H p denote the width and
    height of 𝜑 ′ , respectively. In Equation (4), the regression loss component L
    OCI can be measured as 1 − CIOU. Furthermore, the accuracy of the bounding box
    is presented through the IOU of the predicted box and the actual box. The confidence
    level of the bounding box is measured by Equation (7): C 𝑓 = P 𝑟 (Obj)×IOU,  P
    r (Obj)∈{0,1} (7) where P r (Obj) denotes the probability of an “object” being
    present within a bounding box. When that bounding box contains the target object,
    P r (Obj)=1 ; otherwise, if the bounding box does not contain the target object,
    then P r (Obj)=0 . Figure 4. Illustration of the prediction bounding box and the
    ground truth. The red dotted box ( 𝜑 ′ ) represents the prediction bounding box,
    the green dotted box ( 𝜑 ) indicates the ground truth, and 𝜑       denotes
    the minimum outer bounding box encompassing both 𝜑 ′ and 𝜑 . During the movement
    of the robot, challenges such as uneven terrain and variable external lighting
    may cause the center point of the object to be distorted or disappear. These factors
    can disrupt the accuracy of object bounding box determination in deep learning
    applications, leading to deviations in center point detection. To address this,
    the object center points are extracted from multiple images, and a least square
    regression analysis is performed. This analysis aims to determine the regression
    line that fits the distribution of these points while minimizing the sum of the
    squared vertical distances between the line and the points [41]. Given 𝑘 data
    pairs, represented as {( 𝑥 𝑖 , 𝑦 𝑖 ), 𝑖=1,…, 𝑘 }, the relation between 𝑥 𝑖 and
    𝑦 𝑖 is discerned, factoring in an error term 𝜀 𝑖 to account for any uncertainties
    or deviations: 𝑦 𝑖 =𝑎 𝑥 𝑖 +𝑏+ 𝜀 𝑖 (8) Assuming that 𝑎 ̂ and 𝑏 ̂ represent the
    approximations of parameters 𝑎 and 𝑏 , these are utilized in a subsequent problem,
    aiming to achieve the most suitable fit for the data points: min 𝑎,𝑏 ∑ 𝑘 𝑖=1 𝜀
    ̂ 2 𝑖 (9) If 𝑏 is assumed to be zero, then the ordinary least squares method is
    used for estimating 𝑎 ̂ to minimize Equation (9): 𝑎 ̂ = ∑ 𝑘 𝑖=1 𝑥 𝑖 𝑦 𝑖 ∑ 𝑘 𝑖=1
    𝑥 2 𝑖 (10) Once the slope 𝑎 ̂ is obtained, this indicates that a regression line
    has been formed (see the example in Figure 5). Figure 5. An example of generating
    a regression line using the least squares method with multiple data points in
    pixels, with each represented by a red dot. The green frame and red dot represent
    the continuously detected object frame and its center point respectively. It should
    be noted that during the object recognition process using the YOLO v4 model, detected
    objects of the same type are assigned corresponding identification numbers, and
    the center point position of each object is continuously recorded. Common performance
    indicators for evaluating YOLO v4 models include the precision (PR), recall (RCA),
    and F1−score , as outlined in Equations (11)–(13), respectively. Among these,
    truth positive (TP) represents samples where the model correctly predicts a positive
    outcome, false positive (FP) represents samples where the model incorrectly predicts
    a positive outcome, true negative (TN) represents samples where the model correctly
    predicts a negative outcome, and false negative (FN) represents samples where
    the model incorrectly predicts a negative outcome. It is worth noting that for
    each image, if the IoU exceeds a predetermined threshold, then it is classified
    as TP; otherwise, it is classified as FP: PR= TP TP+FP (11) RCA= TP TP+FN (12)
    F1−score= 2(PR×RCA) PR+RCA (13) 2.3. Guidance and Control 2.3.1. Heading Control
    Using FLC As described in the previous section, linear regression fitting is performed
    on the centers of multiple objects, measured in pixels, to obtain a regression
    line (see Figure 6). This process can generate up to three regression lines. Additionally,
    the average heading angle can be calculated using Equation (14): 𝜃= ∑ 𝑛=1 𝑁 tan
    −1 𝑎 ⌢ 𝑛 /𝑁 (14) where N represents the number of object categories. Figure 6.
    Illustration of multiple regression line and heading angle. The dotted line represents
    the vertical line in the image. The black line signifies the regression line.
    In practical operation, these regression lines, constructed from pixel coordinates,
    are used to estimate their slope. When the guidance line is parallel to any vertical
    line on the image, this indicates that the robot is moving straight forward without
    deviating from the desired heading. However, the presence of positive or negative
    slopes, as well as changes in the slope, typically signify a deviation in the
    heading to the right or left, respectively. Fuzzy logic is a well-known technique
    that involves expert opinion in decision making and is particularly suitable for
    finding effective solutions when information is insufficient. In this study, an
    FLC is utilized to adjust the heading angle of the robot. Its components include
    fuzzification, fuzzy decision making, defuzzification, and a knowledge base [42].
    First, the role of fuzzification is to map input crisp values, denoted as “v”,
    to fuzzy sets. This involves defining a linguistic term 𝐴 ̃ to represent a fuzzy
    set, viewed as a membership function. The most common membership functions are
    triangular and trapezoidal. As shown in Figure 7a, the triangular function and
    its mathematical representation consist of three parameters. The lower boundaries
    on the left and right are represented by 𝛼 and 𝛽 , respectively, while 𝛾 denotes
    the peak of the triangle. When the input crisp value “v” falls between 𝛼 and 𝛾
    , its degree of membership 𝜇 𝐴 ̃ (𝑣) is nonzero, while it is one when “v” equals
    𝛾 and zero when “v” is less than α or greater than β. This implies that the closer
    “v” is to 𝛾 , the higher its degree of membership. Similarly, Figure 7b depicts
    the trapezoidal membership function and its mathematical representation, consisting
    of four parameters: 𝛼′ , 𝛾 1 , 𝛾 2 , and β’. Figure 7. Illustration of membership
    function and their mathematical expressions. (a) Triangular membership function.
    (b) Ladder membership function. In the FLC, the heading angle (θ) and the rate
    of change of the heading angle ( 𝜃 ˙ ) are the two input variables. The heading
    angle corresponds to three fuzzy linguistic terms: left offset (LO), middle (M),
    and right offset (RO). Additionally, the rate of change of the heading angle is
    represented by three fuzzy linguistic terms: negative (N), zero (Z), and positive
    (P). The output variable is the steering angle (δ), defined by the fuzzy linguistic
    terms left (L), mid (M), and right (R). Table 1 compiles the input and output
    variable values, corresponding to the fuzzy logic statements, and the parameter
    values of their membership functions for the FLC. Table 1. Parameters of membership
    function of input and output variables in FLC. Second, the knowledge base consists
    of rules that primarily use [IF—THEN] statements to describe the relationships
    between the input and output variables. When multiple input variables are involved
    in the FLC, [IF—AND—THEN] statements are used. In this study, nine rules have
    been defined based on two input variables (θ and 𝜃 ˙ ) and one output variable
    (δ): Rule 1: IF ( 𝜃 is LO) AND ( 𝜃 ˙ is N) THEN ( 𝛿 is L); Rule 2: IF ( 𝜃 is LO)
    AND ( 𝜃 ˙ is Z) THEN ( 𝛿 is M); Rule 3: IF ( 𝜃 is LO) AND ( 𝜃 ˙ is P) THEN ( 𝛿
    is R); Rule 4: IF ( 𝜃 is M) AND ( 𝜃 ˙ is N) THEN ( 𝛿 is L); Rule 5: IF ( 𝜃 is
    M) AND ( 𝜃 ˙ is Z) THEN ( 𝛿 is M); Rule 6: IF ( 𝜃 is M) AND ( 𝜃 ˙ is P) THEN (
    𝛿 is R); Rule 7: IF ( 𝜃 is RO) AND ( 𝜃 ˙ is N) THEN ( 𝛿 is L); Rule 8: IF ( 𝜃
    is RO) AND ( 𝜃 ˙ is Z) THEN ( 𝛿 is M); Rule 9: IF ( 𝜃 is RO) AND ( 𝜃 ˙ is P) THEN
    ( 𝛿 is R). For example, Rule 1 states that when θ belongs to the “left offset”
    category, and 𝜃 ˙ indicates a “negative rate of change”, the wheels should turn
    left. Subsequently, the fuzzy decision and defuzzification are established. For
    decision making, the Mamdani model, also known as the “max-min composition method”,
    is employed. The principle of this model involves selecting the minimum membership
    degree corresponding to the fuzzy set of the antecedent condition in the activated
    rules and assigning it to the corresponding fuzzy set in the consequent condition
    based on the input values (“minimum” operation). The output fuzzy sets of all
    the activated rules are then combined using a union operation (i.e., the “maximum”
    operation). After the comprehensive inference, the final result comprises a series
    of fuzzy sets with varying degrees. For defuzzification, the centroid method is
    used to convert these fuzzy values into crisp values. 2.3.2. Speed Control Using
    a PID Controller The PID controller is commonly used to regulate the speed of
    a robot. It maintains the stable motion of the robot by adjusting three parameters:
    the proportional gain ( 𝐾 𝑝 ), integral gain ( 𝐾 𝑖 ) and differential gain ( 𝐾
    𝑑 ). When the “P” term increases, the output in response to an error also increases,
    and vice versa. However, using only the “P” term, the system may exhibit a steady
    state error. To eliminate this offset, the “I” term was introduced, which works
    by integrating the error over time to accelerate the system’s response in reaching
    the target state. As time progresses, the “I” term accumulates, meaning that even
    with smaller errors, its contribution grows due to the passage of time until the
    steady state error is eliminated. On the other hand, the “D” term adjusts based
    on the rate of change of the error relative to time. In this study, four sets
    of PID controllers were used to control the rotation speed of the four motors
    of a four-wheeled robot, with each dedicated to one motor. This approach ensures
    precise adjustments, responding to the specific needs and conditions of each wheel
    and thereby enhancing the robot’s overall stability and performance in varying
    operational contexts. The output of the PID controller, denoted as 𝑢(𝑡) , is calculated
    as follows: 𝑢(𝑡)= 𝐾 𝑝 𝑒(𝑡) ⏟ P + 𝐾 𝑖 ∫ 𝑡 0 𝑒(𝑡)𝑑𝑡      I + 𝐾 𝑑 𝑑 𝑑𝑡 𝑒(𝑡)
         𝐷 (15) where 𝑒(𝑡) represents the error between the desired velocity
    and estimated velocity at time 𝑡 . The PID parameters are defined in Table 2.
    Initially, a trial-and-error method is employed to determine the parameter 𝐾 𝑝
    that brings the system to a marginally stable state. This process yields a proportional
    gain parameter, denoted as 𝐾 𝑝𝑐 . 𝐾 𝑝𝑐 , in combination with a proportional coefficient,
    which is used to set the parameter 𝐾 𝑖 . Additionally, the system’s time period,
    denoted as 𝑇 𝑐 , is measured and employed to obtain the parameter 𝐾 𝑑 . Table
    2. Definition of parameters of PID controller for speed control of a robot. 2.4.
    Selective Spraying The design concept of selective spraying is depicted in Figure
    8. A farm robot carries a camera and three nozzles for dispensing herbicide. The
    camera is positioned approximately 40 cm above the ground to ensure a comprehensive
    top-down field of view (depicted as the bold black box in Figure 8). The nozzles
    are positioned approximately 20 cm above the ground. The spray areas are divided
    into three sub-areas, represented by the symbols “❶”, “❷”, and “❸”, with each
    corresponding to a set on the ridge. As mentioned in Section 2.2, the YOLO v4
    model was used to detect drip irrigation belts, crops, and field ridges from images.
    A separate YOLO v4 model is also trained to specifically detect weeds and unhealthy
    crops, identifying the presence of tiny weeds on the ridge. When weeds or unhealthy
    crops are detected, the center point position of the object in pixels is estimated.
    As the robot travels, these tracked center points within the image cross trigger
    lines on the screen of the camera, as shown by the dotted line in Figure 9, activating
    the corresponding nozzles to deliver chemicals or nutrient solutions to the targeted
    objects (weeds or unhealthy crops). The middle nozzle can be used to deliver nutrient
    solutions or pesticides. It is important to note that the nozzle type and pressure
    are adjusted to cover their corresponding areas. The distance between the camera
    and nozzles, denoted as s d , is crucial. When the sprayer is active, the center
    point of the object now crossing the spray line can be ignored to reduce the possibility
    of spray failure. During spraying, the distance traveled by the robot can be regarded
    as s w =𝑣 𝑡 𝑠 , where 𝑡 𝑠 represents the spraying time. Light blue indicates the
    sprayed areas, and w R , w M , and w L denote the width of the right-hand, middle,
    and left-hand ridge in cm, respectively. Figure 8. Design concept of selective
    spraying based on deep learning for an agricultural robot (depicted in gray color)
    in the field. Symbols “❶”, “❷”, and “❸” represent the right, middle, and left
    spray areas, respectively, each corresponding to one of the three nozzles. Purple
    color indicates lettuce; green color signifies weeds; and light blue color denotes
    areas that have been sprayed. Figure 9. The experimental platform of a farming
    robot. Key components are labeled as follows: ❶ = two GNSS antennas; ❷ = control
    box; ❸ = installation space of spraying module; ❹ = linear electric actuator;
    ❺ = battery; ❻ = DC blushless motor; and ❼ = water tank. In practice, the nozzles
    deliver chemicals to the weeds as soon as the center point of a detected weed
    object crosses the spray line. Due to varying weed sizes and dispersion of weeds,
    the object detector, even with lower recognition ability, still ensures that most
    of the weeds are covered with herbicides. The relationship between 𝑡 𝑠 , the width
    of the object s ‘object’ , the delay time for starting the sprayer 𝑡 𝑑𝑒𝑙𝑎𝑦 , and
    the speed of the robot 𝑣 is as follows: 𝑠 𝑑 −𝜆 s ‘object’ <( 𝑡 𝑠 + 𝑡 𝑑𝑒𝑙𝑎𝑦 )𝑣<
    𝑠 𝑑 +(1−𝜆) s ‘object’ (16) where 0≤𝜆 < 1 is the regulation factor and object∈(crop,
    weed) . Two evaluation metrics, the effective weed coverage (EWC) and ineffective
    weed coverage (IWC), are used to evaluate the coverage area of effective spraying
    and the area of ineffective spraying, given a known weed detection rate. It is
    assumed that there are three nozzles with fixed heights and arrangements: EWC
    (%)= N T R s w w R + N T M s w w M +N T L s w w L L(w R + w M +w L )− s crop w
    M (17) IWC (%)= N F R s w w R + N F M s w w M + N F L s w w L L(w R + w M +w L
    )− s crop w M (18) where L represents the total length of the selective spraying
    experimental site and N T R , N T M , and N T L represent the number of times
    the right, middle, and left nozzles correctly deliver the pesticide to the weeds,
    respectively. Conversely, N F R , N F M , and N F L represent the number of times
    the right, middle, and left nozzles incorrectly deliver the pesticide, respectively.
    These values are estimated through spray control systems and experiments. It is
    particularly noteworthy that the process of spraying unhealthy crops is distinct
    from that of spraying weeds. The spraying rate sprayC (%)= N U / N C is used to
    evaluate the spraying efficiency, where N U and N C represent the number of sprayed
    unhealthy crops and the actual number of unhealthy crops, respectively. Finally,
    the amount of pesticide consumed by selective spraying C sel and the amount of
    traditional spraying C full are compared to determine the pesticide reduction
    ratio C (%)= (C full − C sel )/C full . 3. Description of the Farming Robot A
    four-wheel-drive (4WD) and four-wheel-steering (4WS) farming robot was utilized
    to evaluate the autonomous navigation and selective spraying with deep learning
    approach. The mechanism of the robot and software and hardware system configuration
    are explained below. 3.1. Mechatronics System The experimental platform, shown
    in Figure 9, features a chassis composed of multiple modular mechanical components
    [43]. The height of the robot is available in two types: 80 cm and 200 cm. Its
    width is adjustable through a platinum connecting element. The shock absorber,
    forming a double A-arm shock absorber module, is 21 cm long with a 4 cm compression
    stroke. The wheels, made of hard rubber, have a diameter of 65 cm and a width
    of 8 cm. Each wheel is powered by a DC brushless motor (model: 9B200P-DM, TROY
    Enterprise Co., Ltd., Wugu, New Taipei City, Taiwan) coupled to a reduction gear
    set (model: 9VD360H, TROY Enterprise Co., Ltd., Wugu, New Taipei City, Taiwan)
    with a 360:1 reduction ratio and controlled by a motor drive (model: AGV-BLD-1S-200W,
    TROY Enterprise Co., Ltd., Wugu, New Taipei City, Taiwan). Additionally, four
    steering drives (model: CSBL1400, CSIM Inc., Xinzhuang, New Taipei City, Taiwan)
    are connected to four servo motors (model: CS60-150C8AE, CSIM Inc., Xinzhuang,
    New Taipei City, Taiwan) built into linear electric actuators (model: LOE-40-100-C-L5,
    LIND Inc., Taiping, Taichung County, Taiwan). The robot’s embedded board and peripheral
    electronic components are housed inside a control box. Two sets of RTK-GNSS modules
    (model: C099-F9P, u-Blox Inc., Thalwil, canton of Zürich, Switzerland) with two
    antennas (model: VEXXIS Antennas GNSS-502, NovAtel Inc., Issy-les-Moulineaux,
    France) are installed on the front and rear brackets at the top of the robot.
    An embedded board (model: Jetson Xavier NX, NVIDIA Inc., Sunnyvale, CA, USA),
    serving as the main controller of the robot’s operating system, executes deep
    learning algorithms for selective spraying and enables autonomous operation based
    on programmed instructions. A camera (model: BRIO 4K Ultra HD, Logitech Inc.,
    Lausanne, Switzerland) is mounted under the central frame to capture images of
    field ridges, crops, weeds, or drip irrigation belts. The spray module, housed
    in a waterproof box attached to the side bracket, is connected by hoses to nozzles
    at the rear of the central bracket. The nozzles, directed toward the ground, cover
    the left, center, and right areas of the camera’s field of view. Data transfer
    connectivity utilizes Universal Serial Bus (USB), General-Purpose Input/Output
    (GPIO), RS485, and RS-232 protocols, providing an interface between the robot’s
    operating system and electronic components such as the camera, GNSS receivers,
    drivers, spraying module, and other peripherals (Figure 10). The detailed specifications
    of the electronic components are presented in Table 3. Figure 10. Hardware architecture
    of robotic control system. Table 3. Component specification for an agricultural
    robot. 3.2. Steering Mechanism The linear electric actuator, characterized by
    its high output torque, is ideally suited for assembly in agricultural robots,
    particularly for steering control. It comprises a servo motor and a screw mechanism,
    which convert the rotational motion of the motor shaft into the linear motion
    of the piston rod. This steering mechanism is used to adjust the steering angle
    δ (as shown in Figure 11), which is defined by Equation (19): 𝛿= cos −1 ( 𝑟 2
    + 𝑓 2 −𝑑 ′ 2 2𝑟𝑓 )− cos −1 ( 𝑟 2 + 𝑓 2 − 𝑑 2 2𝑟𝑓 ) (19) where 𝑟 denotes the length
    from the center point of the link slider to the endpoint of the piston rod and
    f represents the distance between the center point of the link slider and the
    base end of the electric actuator, while 𝑑 and 𝑑 ′ signify the original length
    and the extended length from the front end to the base end of the electric actuator,
    respectively. In this study, 𝑟 = 8.07 cm, 𝑓=46.8 cm , and 𝑑=39.6 cm . Figure 11.
    Steering mechanism of robot. (a) Appearance of steering mechanism. (b) Relationship
    between the steering and linear electronic actuators. 3.3. Spraying Module The
    circuit of the spray module is shown in Figure 12a. The main controller operates
    the spray program and sends a start or stop command to the relay module through
    the GPIO interface. This process controls the activation or deactivation of the
    solenoid valve, thereby regulating the timing of the spraying. A webcam is utilized
    to capture images on the ridge. The control box, shown in the upper part of Figure
    12b, includes relays, DC-DC converters, transformers, and peripheral circuit components.
    The internal components of the spray box, depicted in the lower part of Figure
    12b, comprise pumps, solenoid valves, connectors, and plastic water pipes. Five
    sets of spray connectors are installed on both the left and right sides of the
    box. Both the external appearance and the internal configuration of the spray
    nozzle used in the agricultural robot are shown in Figure 13. Figure 12. Spray
    module. (a) A diagram illustrating the spraying circuit. (b) The internal configuration
    of the control box showing peripheral components (top) and the arrangement of
    water pipe connections inside the spray box (bottom). Figure 13. The external
    appearance and the internal configuration of the spray nozzle. 4. Experiments
    and Results 4.1. Environmental Conditions and Parameters The experimental site
    was an empty field located in front of the Department of Biomechanical Engineering
    building (longitude: 120.6059°, latitude: 22.6467°). The experiments were conducted
    from summer to autumn. The field spans approximately 10 m in length, with individual
    ridge widths measuring 80 cm. Due to the limited farming area, we only allowed
    the farming robot to move between two ridges (as shown in Figure 14). The experimental
    site is surrounded by green trees. The left and right wheels of the robot straddled
    the sides of a strip-shaped farmland, with both ends of the farmland serving as
    turning points (marked by star-shaped dots). When the position of the robot fell
    into the set range to be turned, the robot stopped and performed a 90 degree on-the-spot
    rotation in place to keep the head of the robot facing the forward direction.
    The motion behavior was repeated until the robot returned to the origin point
    (see “ST/END” in Figure 14). Figure 14. Schematic of movement behavior of the
    robot in the field. Star-shaped dots represent turning points, as well as start
    and end (ST/END) points. The GNSS receiver, enhanced with RTK capabilities, produces
    navigation data in a format established by the National Marine Electronics Association
    (NMEA), offering highly accurate longitudinal and latitudinal details [44]. Two
    GNSS receivers were employed to record the robot position, separated by 0.65 m.
    The latitude and longitude of the location obtained from the two receivers were
    each converted into two TWD97-based positions [45]. Generally speaking, a typical
    video frame rate is 15–30 FPS [46]. Considering hardware limitations and expected
    objects, in this study, FPS values between 1 and 13 were evaluated with an image
    size of 416 × 416. The experimental periods were divided into morning (9:00–11:00
    a.m.), noon (12:00–2:00 p.m.), and afternoon (3:00–5:00 p.m.). The weather conditions
    during these periods varied and may have been sunny (9800~35,000 lux), partly
    sunny (3000~9800 lux), or cloudy (0~3000 lux). The experimental period spanned
    3 months. Since lettuce crops are harvested approximately 20–30 days after sowing,
    a total of three harvests were performed during the trial, and the cultivated
    land was reseeded after each harvest. During the planting process, the amount
    of watering for each crop was adjusted, resulting in differences in growth conditions.
    4.2. Preliminary Test Red leaf lettuce (model: HV-067, Taiwan Known-You Seed Co.,
    Ltd., Dashu Kaohsiung city, Taiwan) was selected as the target crop. Two YOLO
    v4 models were trained and employed for guidance line detection (DetModel #1)
    and the detection of weeds and unhealthy crops (DetModel #2). The users captured
    image samples randomly at the experimental site every day using cameras. These
    images encompassed weeds, both healthy and abnormal crops, drip irrigation belts,
    and ridges. A total of 5800 images were collected in the experimental farm area
    of Pingtung University of Science and Technology during the autumn and winter
    of 2023. A multi-channel data logger (model: WatchDog 1650, Spectrum Technologies,
    Inc., Aurora, IL, USA) was used to record the light intensity. These images were
    taken through the camera on the robot platform at different times and under different
    weather conditions. Each image showed drip irrigation belts, field ridges, crops,
    unhealthy crops, or weeds. These images were then processed through image argumentation
    to obtain a total of 9500 images, which were used to build DetModel #1 and DetModel
    #2. The images were divided into a training set, test set, and validation set
    according to the ratio of 7:2:1. The images in the training set were manually
    annotated by using the open-source image annotation tool LabelImg to mark objects
    within the images. The abnormal growth of crops was characterized by symptoms
    such as wrinkled leaves, as depicted in Figure 15a,b. The type of weed at the
    experimental site is also shown in Figure 15c. Figure 15. Leaf appearance for
    unhealthy crops and weeds. (a) Wrinkled leaves (sample 1). (b) Wrinkled leaves
    (sample 2). (c) The type of weed. The hyperparameters for both YOLO v4 models
    (DetModel #1 and #2) were configured as follows: the batch size set to 64, subdivisions
    set to 32, image size of 416 × 416 (width × height), decay rate of 0.0005, momentum
    of 0.949, learning rate set to 0.001, and maximum number of batches set to 10,000.
    The training iterations for DetModel #1 and DetModel #2 were stopped after reaching
    10,000 and 6000 iterations, with 𝑃 𝐿𝑜𝑠𝑠 values of 1.0719 and 14.8856, respectively.
    The mean average precision (mAP) values of DetModel #1 and DetModel #2 were 99.0%
    and 92.7%, respectively (Figure 16). Figure 16. Identification of objects in the
    ridge using trained YOLO v4 models. (a) Detection results for the ridge, drip
    irrigation belt, and crops using DetModel #1. (b) Detection results for weeds
    and unhealthy crops using DetModel #2, featuring weed detection results (purple
    bounding box on the left side), the loss convergence curve (in the middle), unhealthy
    crops (green box on the right side), and weeds (purple bounding box). The performance
    comparison results of the two detection models, DetModel #1 and DetModel #2, under
    different weather conditions are shown in Table 4. The results indicate that the
    recognition rates of DetModel #1 for detecting drip irrigation belts, crops, and
    ridges ranged from about 96 to 99%, 93 to 98%, and 93 to 97% respectively. Under
    the sunny conditions and during time periods from 3:00 to 5:00 p.m., DetModel
    #1 achieved the best recognition rate of 99% for identifying drip irrigation belts.
    On cloudy days and between 3:00 and 5:00 p.m., the average precision for ridges
    dropped to 93%. Overall, using DetModel #1, the average accuracy was about 98%.
    Secondly, the accuracy of DetModel #2 for detecting unhealthy crops and weeds
    ranged from 84 to 92% and 86 to 92%, respectively. Among these, the highest accuracy
    rate for weed detection was 93% on sunny days between 9:00 and 11:00 a.m. In contrast,
    on cloudy days during the same time period, the detection rate fell to 84%. Using
    DetModel #2, the average accuracy for weeds and unhealthy crops was about 88%.
    Table 4. Comparison of the performance of detection models in identifying different
    types of objects under different weather conditions. 4.3. Experimental Results
    Two scenarios, comprising the autonomous guidance and selective spraying experiments,
    were conducted to evaluate the robustness of the proposed scheme. 4.3.1. Scenario
    1 A total of 60 crops were planted in two rows of farmland. The speed of the robot
    was 12.5 cm/s. During the autonomous navigation of the farming robot, data such
    as the velocity of each motor, heading angle, and the output value of the FLC
    were recorded. The experimental time was from 9:00 to 11:00 a.m., and the weather
    conditions were sunny. Three types of guidance lines were measured to estimate
    the deviation angle (N = 3), and the angles were averaged to determine the real-time
    heading angle of the robot. PID control and a fuzzy logic-based steering control
    program embedded in the guidance system were executed to continuously maintain
    the speed of the robot and correct its heading angle. The movement trajectory
    of the robot was obtained by two GNSS receivers as shown in Figure 17a. The change
    in velocity of each wheel of the robot is shown in Figure 17b. The control parameters
    of the PID controller for the four motor drivers, 𝐾 𝑝 , 𝐾 𝑖 , and 𝐾 𝑑 , were all
    set to 0.5, 0.1, and 0.6, respectively. Specifically, when the robot moved forward,
    the required motor speed for the four wheels was maintained at approximately 700
    rpm (regardless of the reduction ratio). A speed overshoot occurred briefly when
    the motor speed value was switched. Additionally, this phenomenon also occurred
    when the velocity of the four motors was maintained at about 1000 rpm during in-place
    rotation for the robot. Figure 17. The velocity control of the farming robot in
    the farmland using PID control. (a) The movement trajectory of the robot (depicted
    by blue and green dotted lines) obtained using two GNSS receivers. (b) A comparison
    of the speed variation range and motion behavior of each wheel in relation to
    the positioning trajectory shown in (a). Numbers 1 through 8 correspond to the
    respective rotational speed changes of the four motors when the robot is in motion.
    Secondly, the results of using different guidance lines for estimating the heading
    angle and correcting it through the FLC were compared, as shown in Figure 18a.
    The speed of the robot was 12.5 cm/s. The changes in the heading angles, measured
    from the regression lines generated by fitting different types of objects, were
    observed. It was found that when the crop line was used as the reference guidance
    line, it showed the largest change in heading angle (Figure 18a). Conversely,
    using the irrigation line as a guidance line resulted in minimal variation in
    the heading angle. Figure 18b shows the steering angle obtained by using the FLC
    when the crop line was the guidance line. The output value of the FLC ranged between
    ±2 degrees, with a few output values reaching ±6 degrees. These larger steering
    angle peaks appeared within the unit time range of 50~60, 120~130, or 225~235.
    These values reflect the heading angle and its changes during the corresponding
    time intervals when the robot traveled along the crop line (Figure 18a). The reason
    for these instantaneous changes in angles is that the planting position of the
    crops was deviated due to human factors, or crops that were not growing well (elongated)
    caused the center point of the labeled object to deviate too far from the expected
    position. Figure 18. Changes in heading angle using different types of guidance
    lines. (a) The average angular variation in the heading angle was approximately
    1 degree, indicated by the orange square. (b) The steering angle obtained using
    the FLC when the crop line served as a navigation line. On the other hand, the
    impact of different speeds of the robot on the fitting results of the regression
    under varying weather conditions was observed. As shown in Figure 19, it is evident
    that the use of DetModel #1 enabled object detection and achieved a mean average
    precision (mAP) of 97% when the speed of the robot was 12.5 cm/s. However, as
    the speed of the robot increased, the mAP gradually decreased. Specifically, at
    a speed of 19 cm/s, the mAP dropped to below 75%, and when the speed of the robot
    reached 35 cm/s, the mAP decreased to below 50%. At a travel speed of 12.5 cm/s,
    there was no significant difference in the mAP under different weather conditions.
    Figure 19. Comparison of the robot’s object detection performance at different
    robot speeds. A snapshot of the results from using DetModel #1 to continuously
    detect objects of different categories (FPS = 7) and using the least squares method
    to generate guidance lines under different weather conditions is shown in Figure
    20. Although Figure 20a,b displays an uneven brightness distribution, three lines
    were still generated: the irrigation line (red), the crop line (orange), and the
    field border line (blue). The same results were also observed in low-brightness
    environments, as demonstrated in Figure 20c. Figure 20. Snapshots showcasing guidance
    line generation and object detection results under various weather conditions
    (FPS = 7; speed = 12.5 cm/s). (a) Sunny weather. (b) Partly sunny conditions.
    (c) Cloudy conditions. Black line represents the vertical line in the center of
    the image; orange line indicates crop line; red line denotes irrigation line;
    blue line signifies ridge line; and green frame highlights the detected object
    frame. A guidance line was generated after fitting the measured data through least
    squares regression. The effect of the number of FPS on the reliability of regression
    line generation was evaluated. Videos recorded by remotely controlling the robot
    while it traveled at different speeds in the field were used to evaluate the impact
    of different frame rates on changes in the heading angle. The heading angles obtained
    from the three fitted regression lines were averaged and compared with the FPS
    values. When the speed of the robot was maintained at 12.5 cm/s, and the FPS was
    greater than seven, the variation range in heading angle was about two degrees
    (Figure 21). Similar results were also presented when the speed of the robot was
    equal to 19 cm/s and the FPS was equal to 11 and 13. When the speed of the robot
    was maintained at 24 cm/s, and the FPS was greater than 5, its heading angle changed
    by about 3.5–5.5 degrees. When the robot speed was 35 cm/s, the FPS had to be
    increased to above nine to obtain the heading angle. However, in this case, the
    range of variation in the heading angle was the largest compared with other scenarios.
    Figure 21. Variation range in heading angle versus FPS. The heading angle could
    not be obtained when the speed of the robot was 19 cm/s, 24 cm/s, or 35 cm/s.
    4.3.2. Scenario 2 In the selective spraying experiment, water-soluble pigments
    were used as the spraying solution. After each spraying experiment, weeds were
    removed manually. About a week later, once the weeds had regrown, the spraying
    experiment was carried out again. Equations (16) and (17) were used to estimate
    the EWC and IWC, respectively. The realism of the sprayed area was determined
    by visually inspecting whether water dripped onto the weeds or crops. This procedure
    was executed in three replicates, where the total amount of solution applied in
    each test was quantified through a water storage bucket on the side of the robot.
    According to the weed detection results presented in Table 4, the weather conditions
    during this experiment were sunny, and the time period was from 9:00 to 11:00
    a.m. There were two strips of cultivated land. Among them, w R =w L =12 cm , w
    M =15 cm , s d =35 cm , and s crop =12 cm. When the speed of the robot was 12.5
    cm/s, 19 cm/s, 24 cm/s and 35 cm/s, the delay time for starting the sprayer was
    set to 2.5 s, 1.5 s, 1 s, and 0.5 s, respectively. The spray time of the sprayer
    was set to 0.5 s ( 𝑡 𝑠 =0.05 ) for each operation. When the spraying program was
    executed in the robot operating system, it recorded the number of executions of
    the left, middle, and right nozzles, estimated the spraying area and volume, and
    compared them with the volume obtained using traditional spraying methods to calculate
    the pesticide reduction. It is important to note that during the weeding spraying
    experiments, the robot was remotely controlled by a human at a constant speed,
    and only the central nozzle sprayed the weeds. Unhealthy crops were not sprayed
    during these experiments. A windless environment was ensured when performing the
    spray tests. The impact of different FPS values on the detection performance of
    DetModel #2 for weeds and unhealthy crops was also evaluated, with the speed of
    the robot set to 12.5 cm/s. Figure 22 shows that when the FPS was greater than
    seven, the average accuracy of detecting weeds and unhealthy crops ranged from
    89% to 92% and from 88% to 90%, respectively. However, when the FPS was less than
    seven, the accuracy of detecting weeds and unhealthy crops dropped significantly.
    Figure 22. Comparison of the detection accuracy for weeds and unhealthy crops
    at different FPS values. The results for the EWC, IWC, and pesticide reduction
    rate at different robot speeds with a detection time of 143 ms per image for DetModel
    #2 are illustrated in Table 5. Pesticide reduction refers to the decrease in pesticide
    usage achieved by selective spraying compared with traditional uniform spraying.
    It can be seen from this table that as the speed of the robot increased, the EWC
    gradually decreased, and the IWC gradually increased. However, limited by the
    detection performance of DetModel #2, when the robot speed reached 35 cm/s, the
    EWC was at its lowest, and the IWC was at its highest. In this scenario, although
    pesticide usage was significantly reduced (about 63%), most weeds were not sprayed.
    Table 5. Comparison of effective and ineffective weeding coverage and pesticide
    reduction rates at different robot speeds ( 𝑡 𝑠 =0.5 s). The performance of selective
    spraying for unhealthy crops was evaluated. Each spray time of the sprayer was
    also set to 0.5 s ( 𝑡 𝑠 =0.5 s). The delay time for starting the sprayer was the
    same as that in the weed spraying experiment. This spraying experiment was conducted
    at different time periods and under different weather conditions, with the experimental
    procedures for each time period being repeated three times. The comparison results
    of the spray rate of the robot at different speeds are demonstrated in Figure
    23. When the speed of the robot was 12.5 cm/s, 19 cm/s, 24 cm/s, and 35 cm/s,
    the spraying rates (SprayC) were between about 85 and 92%, 83% and 88%, 70 and
    88%, and 40 and 65%, respectively. A snapshot of an unhealthy crop after spraying
    is shown in Figure 24. These images were all captured by cameras after selective
    spraying tests under different weather conditions. The bounding boxes in these
    images were labeled offline with DetModel #2 and confirm the spray behavior on
    unhealthy crops or weeds. Figure 23. Comparison of spray rates (SprayC) for the
    robot at different speeds and FPS values: (a) 12.5 cm/s; (b) 19 cm/s; (c) 24 cm/s;
    and (d) 35 cm/s. Figure 24. Spraying results under different weather conditions
    (speed of 12.5 cm/s; FPS = 7): (a) sunny (zoomed in); (b) partly sunny; (c) cloudy
    (Example 1); and (d) coludy (Example 2). The bounding boxes were marked by DetModel
    #2 after spraying. 4.4. Discussion Based on the comprehensive analysis of our
    experimental results, we have summarized the following points: The fitting result
    of the guidance line is related to the speed of the robot, the detection performance
    of DetModel #1, and the FPS value. As shown in Table 4, the accuracy of using
    DetModel #1 to detect ridges, crops, or drip irrigation belts ranged from 92 to
    99%, with an average accuracy (AP) of about 95%. The accuracy of identifying drip
    irrigation belts was the highest, reaching up to 99%, followed by crops at 98%
    and field ridges at 97%. Since the training set samples came from images captured
    in a real environment from different angles and under various weather conditions,
    such realistic datasets can enhance the object recognition ability of the model
    [47,48]. The accuracy of navigation line extraction depends on the performance
    of the trained model in detecting targets. When the same type of object of the
    image is successfully detected multiple times, a line can be fitted using the
    least squares method. Soil images under different climate conditions were collected,
    and even overexposed images were used as training samples for modeling. The experimental
    results show that the trained model can effectively improve its generalization
    performance, especially under different climate conditions. In practical operation,
    as long as the center points of at least two objects can be detected, the navigation
    line can be extracted. When the robot traveled at a speed of 12.5 cm/s, and the
    FPS is set to 7, its mAP could reach 98%. As the robot speed increased, the mAP
    gradually decreased. When comparing the relationship between the mAP and heading
    angle variation in Figure 19 and Figure 21, it can be observed that a lower mAP
    would lead to an increased range of heading angle variation, producing more dubious
    heading angles. Increasing the FPS can help reduce false detection for objects
    caused by instantaneous strong ambient light and maintain a certain mAP even as
    the speed of robot increases. However, it also increases the computing load of
    the system, leading to the risk of overheating of the hardware system. Under conditions
    that meet various climatic requirements while maintaining the average object detection
    performance, the average detection time of DetModel #1 was 143 ms/per image. Under
    different weather conditions, DetModel #2 was used to identify unhealthy crops
    and weeds, achieving average PRs of about 84% and 93%, respectively. In the afternoon
    on a cloudy day, the PR was slightly reduced to 84~89%, demonstrating that the
    deep learning model exhibited better adaptability to images with weaker light
    intensities. This adaptability improves upon the limitations of traditional machine
    learning techniques [49]. However, the limited dynamic illumination range of RGB
    cameras can easily lead to image color distortion when this limit is exceeded
    or not met [50], making object recognition difficult, especially with overexposed
    or insufficiently bright images. Rapid movement can result in specific frames
    failing to capture the target. Objects often have features significantly different
    from those encountered during the training process, making them difficult detect
    [25]. Changes in ambient lighting alter the tones and shadows in the image frame,
    affecting an object’s color, and pixel edge blur or shadows can also significantly
    impact detection accuracy [51,52]. Adding more feature information to the training
    of the deep learning model can improve its detection performance [53]. In this
    study, more field images, including soil types in low-light environments and even
    overexposed images, were used as samples. The experimental results show that the
    trained deep learning model had better generalization performance. Limited by
    the soil hardness and site flatness, in this study, the 𝐾 𝑝 value of the PID controller
    was set to 0.5, enabling the four DC motors to reach the required speed quickly.
    Although there is a short-term small oscillation when the motor starts, its impact
    on the traveling speed of the robot is minimal. On the other hand, the FLC can
    smoothly adjust the heading angle and improve the two-wheel speed difference control
    method [54]. The experimental results show that when the robot moved in a guidance
    line at a speed of 12.5 cm/s, the variation in the heading angle stayed within
    one degree. Although no drip irrigation belt was present on the field, the average
    variation range of the heading angle remained within ±3.5 degrees (Figure 18a).
    Assuming that the planting position of the crops in the image was not on the vertical
    line in the center of the field ridge, it would also affect the estimated heading
    angle when the robot moved. For instance, in Figure 18a, at time indices of about
    55, 130, 125, and 232, the heading angle estimated based on the crop line changed
    instantly by more than four degrees. Fortunately, this result has to be averaged
    with the heading angles obtained from the other regression lines, preventing ridge
    damage due to overcorrection of the wheels and avoiding misjudgment of the heading
    angle. When the robot rotated, a higher motor speed output value (1000 rpm) was
    used, ensuring greater torque was instantly available and allowing the four drives
    to power the motors. The advantage of the 4 WD/4 WS system is that it enables
    the robot to achieve a turning radius of zero. Compared with common car drive
    systems, this steering mode reduces the space required for turning and improves
    land use efficiency. Moreover, relying on RTK-GNSS receivers for centimeter-level
    autonomous guidance of a farming robot is a common practice [55]. This centimeter-level
    positioning error is acceptable when a fixed ambiguity solution is obtained [56].
    However, there are risks in autonomous guiding operations on narrow farming areas.
    If the positioning signal is interrupted, or the positioning solution is a floating
    solution, then the robot may damage the field during movement. During spraying
    test operation, setting the delay time of the sprayer according to the speed of
    the robot can avoid ineffective spraying. In this study, when the robot speed
    was 12.5 cm/s, with a sprayer time of about 0.5 s and a delay time of 2.5 s, the
    EWC and IWC reached 83% and 8% respectively, and the pesticide reduction reached
    53%. Similar results were also presented in [57], with a 34.5% reduction in pesticide
    usage compared with traditional uniform spraying methods. As the speed of the
    robot increased, the EWC decreased, and the IWC increased. At a speed of 35 cm/s,
    the average precision was too low, which indirectly resulted in a reduction in
    the number of pesticide applications. On the other hand, the unhealthy crops were
    about 7–12 cm in diameter. Even with the appropriate delay time set, there were
    still some unhealthy crops or weeds that could not be sprayed correctly. 5. Conclusions
    This study offers insights into the challenges and potential solutions for agricultural
    robots in real-world applications. Agricultural robots must rely on high-precision
    positioning systems to complete autonomous operations in the field. In practice,
    4WS/4WD agricultural robots are utilized to navigate strip fields, turning on
    the spot to transition to other farming areas. Firstly, the experimental results
    confirmed that the deep learning model can detect drip irrigation belts, field
    ridges, crops, unhealthy crops, and weed objects with an accuracy rate ranging
    from 83 to 99%. This enables the implementation of various operations in climatically
    diverse environments, such as inter-row line tracking and selective spraying.
    In terms of robot guidance line tracking, this study presents an innovative method
    that estimates the robot’s heading angle using multiple regression lines. This
    method has been proven to be more reliable than conventional crop row extraction
    techniques, significantly reducing the risk of oversteering and potential damage
    to crops and field ridges during the robot’s movement. The deviation angle was
    within one degree at a speed of 12.5 cm/s, assisted by multiple guidance lines
    and using a PID controller and FLC. Minimizing human factors, such as misaligned
    crop planting and uneven crop spacing, will enhance the accuracy of the heading
    angle. Secondly, an excessively fast robot movement speed causes the object detection
    rate to decrease. The reason involves limited computing resources leading to insufficient
    FPS values. By allowing the robot to operate at a slower speed, we ensure the
    maintenance of a certain level of object detection performance for selective spraying
    operations. At a frame processing rate of 7 FPS and a robot speed of 12.5 cm/s,
    the mAP for detecting weeds and unhealthy crops ranged from 93 to 98%. The accuracy
    of spraying unhealthy crops could reach up to 92%. Considering the sprayer’s fixed
    delay time, the autonomous robot achieved an effective weed coverage rate of 83%
    and a pesticide saving rate of 53% while operating at a speed of 12.5 cm/s. The
    applicability of these advancements to low-cost hardware expands their impact
    across various agricultural settings, particularly benefiting small-scale and
    resource-limited farming. Future research will concentrate on integrating adaptive
    FPS and minimizing spray start latency in autonomous agricultural robots, enabling
    them to perform a variety of tasks in a decentralized fashion. Author Contributions
    Conceptualization, C.-L.C.; methodology, C.-L.C.; software, C.-L.C., J.-Y.K. and
    H.-W.C.; verification, J.-Y.K. and H.-W.C.; data management, J.-Y.K. and H.-W.C.;
    writing—manuscript preparation, C.-L.C. and H.-W.C.; writing—review and editing,
    C.-L.C.; visualization, C.-L.C.; supervision, C.-L.C.; project management, C.-L.C.;
    fund acquisition, C.-L.C. All authors have read and agreed to the published version
    of the manuscript. Funding This research was supported by the National Science
    and Technology Council (grant number NSTC 112-2221-E-020-013). Institutional Review
    Board Statement Not applicable. Informed Consent Statement Not applicable. Data
    Availability Statement The data that support the findings of this study are available
    from the corresponding author, C.-L.C., upon reasonable request. Acknowledgments
    Many thanks are due to the editors and reviewers for their valuable comments to
    refine this paper. Conflicts of Interest The authors declare no conflicts of interest.
    References Spykman, O.; Gabriel, A.; Ptacek, M.; Gandorfer, M. Farmers’ perspectives
    on field crop robots—Evidence from Bavaria, Germany. Comput. Electron. Agric.
    2021, 186, 106176. [Google Scholar] [CrossRef] Wu, J.; Jin, Z.; Liu, A.; Yu, L.;
    Yang, F. A survey of learning-based control of robotic visual servoing systems.
    J. Franklin Inst. 2022, 359, 556–577. [Google Scholar] [CrossRef] Kato, Y.; Morioka,
    K. Autonomous robot navigation system without grid maps based on double deep Q-Network
    and RTK-GNSS localization in outdoor environments. In Proceedings of the 2019
    IEEE/SICE International Symposium on System Integration (SII), Paris, France,
    14–16 January 2019; pp. 346–351. [Google Scholar] Galati, R.; Mantriota, G.; Reina,
    G. RoboNav: An affordable yet highly accurate navigation system for autonomous
    agricultural robots. Robotics 2022, 11, 99. [Google Scholar] [CrossRef] Chien,
    J.C.; Chang, C.L.; Yu, C.C. Automated guided robot with backstepping sliding mode
    control and its path planning in strip farming. Int. J. iRobotics 2022, 5, 16–23.
    [Google Scholar] Zhang, L.; Zhang, R.; Li, L.; Ding, C.; Zhang, D.; Chen, L. Research
    on virtual Ackerman steering model based navigation system for tracked vehicles.
    Comput. Electron. Agric. 2022, 192, 106615. [Google Scholar] [CrossRef] Tian,
    H.; Wang, T.; Liu, Y.; Qiao, X.; Li, Y. Computer vision technology in agricultural
    automation—A review. Inf. Process. Agric. 2020, 7, 1–19. [Google Scholar] [CrossRef]
    Leemans, V.; Destain, M.F. Application of the Hough transform for seed row localisation
    using machine vision. Biosyst. Eng. 2006, 94, 325–336. [Google Scholar] [CrossRef]
    Choi, K.H.; Han, S.K.; Han, S.H.; Park, K.-H.; Kim, K.-S.; Kim, S. Morphology-based
    guidance line extraction for an autonomous weeding robot in paddy fields. Comput.
    Electron. Agric. 2015, 113, 266–274. [Google Scholar] [CrossRef] Zhou, X.; Zhang,
    X.; Zhao, R.; Chen, Y.; Liu, X. Navigation line extraction method for broad-leaved
    plants in the multi-period environments of the high-ridge cultivation mode. Agriculture
    2023, 13, 1496. [Google Scholar] [CrossRef] Suriyakoon, S.; Ruangpayoongsak, N.
    Leading point-based interrow robot guidance in corn fields. In Proceedings of
    the 2017 2nd International Conference on Control and Robotics Engineering (ICCRE),
    Bangkok, Thailand, 1–3 April 2017; pp. 8–12. [Google Scholar] Bonadiesa, S.; Gadsden,
    S.A. An overview of autonomous crop row navigation strategies for unmanned ground
    vehicles. Eng. Agric. Environ. Food 2019, 12, 24–31. [Google Scholar] [CrossRef]
    Chen, J.; Qiang, H.; Wu, J.; Xu, G.; Wang, Z. Navigation path extraction for greenhouse
    cucumber-picking robots using the prediction-point Hough transform. Comput. Electron.
    Agric. 2021, 180, 105911. [Google Scholar] [CrossRef] Ma, Z.; Tao, Z.; Du, X.;
    Yu, Y.; Wu, C. Automatic detection of crop root rows in paddy fields based on
    straight-line clustering algorithm and supervised learning method. Biosyst. Eng.
    2021, 211, 63–76. [Google Scholar] [CrossRef] Shi, J.; Bai, Y.; Diao, Z.; Zhou,
    J.; Yao, X.; Zhang, B. Row detection-based navigation and guidance for agricultural
    robots and autonomous vehicles in row-crop fields: Methods and applications. Agronomy
    2023, 13, 1780. [Google Scholar] [CrossRef] Zhang, S.; Wang, Y.; Zhu, Z.; Li,
    Z.; Du, Y.; Mao, E. Tractor path tracking control based on binocular vision. Inf.
    Process. Agric. 2018, 5, 422–432. [Google Scholar] [CrossRef] Mavridou, E.; Vrochidou,
    E.; Papakostas, G.A.; Pachidis, T.; Kaburlasos, V.G. Machine vision systems in
    precision agriculture for crop farming. J. Imaging 2019, 5, 89. [Google Scholar]
    [CrossRef] Gu, Y.; Li, Z.; Zhang, Z.; Li, J.; Chen, L. Path tracking control of
    field information-collecting robot based on improved convolutional neural network
    algorithm. Sensors 2020, 20, 797. [Google Scholar] [CrossRef] Pajares, G.; García-Santillán,
    I.; Campos, Y.; Montalvo, M.; Guerrero, J.M.; Emmi, L.; Romeo, J.; Guijarro, M.;
    González-de-Santos, P. Machine-vision systems selection for agricultural vehicles:
    A guide. J. Imaging 2016, 2, 34. [Google Scholar] [CrossRef] de Silva, R.; Cielniak,
    G.; Gao, J. Towards agricultural autonomy: Crop row detection under varying field
    conditions using deep learning. arXiv 2021, arXiv:2109.08247. [Google Scholar]
    Hu, Y.; Huang, H. Extraction method for centerlines of crop row based on improved
    lightweight Yolov4. In Proceedings of the 2021 6th International Symposium on
    Computer and Information Processing Technology (ISCIPT), Changsha, China, 11–13
    June 2021; pp. 127–132. [Google Scholar] Ruan, Z.; Chang, P.; Cui, S.; Luo, J.;
    Gao, R.; Su, Z. A precise crop row detection algorithm in complex farmland for
    unmanned agricultural machines. Biosyst. Eng. 2023, 232, 1–12. [Google Scholar]
    [CrossRef] Ruigrok, T.; van Henten, E.; Booij, J.; van Boheemen, K.; Kootstra,
    G. Application-specific evaluation of a weed-detection algorithm for plant-specific
    spraying. Sensors 2020, 20, 7262. [Google Scholar] [CrossRef] Hu, D.; Ma, C.;
    Tian, Z.; Shen, G.; Li, L. Rice Weed detection method on YOLOv4 convolutional
    neural network. In Proceedings of the 2021 International Conference on Artificial
    Intelligence, Big Data and Algorithms (CAIBDA), Xi’an, China, 28–30 May 2021;
    pp. 41–45. [Google Scholar] Chang, C.L.; Xie, B.X.; Chung, S.C. Mechanical control
    with a deep learning method for precise weeding on a farm. Agriculture 2021, 11,
    1049. [Google Scholar] [CrossRef] Wang, Q.; Cheng, M.; Huang, S.; Cai, Z.; Zhang,
    J.; Yuan, H. A deep learning approach incorporating YOLO v5 and attention mechanisms
    for field real-time detection of the invasive weed Solanum rostratum Dunal seedlings.
    Comput. Electron. Agric. 2022, 199, 107194. [Google Scholar] [CrossRef] Chen,
    J.; Wang, H.; Zhang, H.; Luo, T.; Wei, D.; Long, T.; Wang, Z. Weed detection in
    sesame fields using a YOLO model with an enhanced attention mechanism and feature
    fusion. Comput. Electron. Agric. 2022, 202, 107412. [Google Scholar] [CrossRef]
    Ruigrok, T.; van Henten, E.J.; Kootstra, G. Improved generalization of a plant-detection
    model for precision weed control. Comput. Electron. Agric. 2023, 204, 107554.
    [Google Scholar] [CrossRef] Razfar, N.; True, J.; Bassiouny, R.; Venkatesh, V.;
    Kashef, R. Weed detection in soybean crops using custom lightweight deep learning
    models. J. Agric. Food Res. 2022, 8, 100308. [Google Scholar] [CrossRef] Qiu,
    Q.; Fan, Z.; Meng, Z.; Zhang, Q.; Cong, Y.; Li, B.; Wang, N.; Zhao, C. Extended
    Ackerman steering principle for the co-ordinated movement control of a four wheel
    drive agricultural mobile robot. Comput. Electron. Agric. 2018, 152, 40–50. [Google
    Scholar] [CrossRef] Bak, T.; Jakobsen, H. Agricultural robotic platform with four
    wheel steering for weed detection. Biosyst. Eng. 2004, 87, 125–136. [Google Scholar]
    [CrossRef] Tu, X.; Gai, J.; Tang, L. Robust navigation control of a 4WD/4WS agricultural
    robotic vehicle. Comput. Electron. Agric. 2019, 164, 104892. [Google Scholar]
    [CrossRef] Wang, D.; Qi, F. Trajectory planning for a four-wheel-steering vehicle.
    In Proceedings of the 2001 ICRA. IEEE International Conference on Robotics and
    Automation, Seoul, Republic of Korea, 21–26 May 2001; Volume 4, pp. 3320–3325.
    [Google Scholar] Bochkovskiy, A.; Wang, C.-Y.; Liao, H.M. YOLOv4: Optimal speed
    and accuracy of object detection. arXiv 2020, arXiv:2004.10934. [Google Scholar]
    Redmon, J.; Farhadi, A. YOLOv3: An incremental improvement. arXiv 2018, arXiv:1804.02767.
    [Google Scholar] Wang, C.Y.; Liao, H.Y.M.; Wu, Y.H.; Chen, P.Y.; Hsieh, J.W.;
    Yeh, I.H. CSPNet: A new backbone that can enhance learning capability of CNN.
    In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition Work-shops (CVPRW), Seattle, WA, USA, 14–19 June 2020; pp. 1571–1580.
    [Google Scholar] He, K.; Zhang, X.; Ren, S.; Sun, J. Spatial pyramid pooling in
    deep convolutional networks for visual recognition. IEEE Trans. Pattern Anal.
    Mach. Intell. 2015, 37, 1904–1916. [Google Scholar] [CrossRef] [PubMed] Zhang,
    Z.; He, T.; Zhang, H.; Zhang, Z.; Xie, J.; Li, M. Bag of freebies for training
    object detection neural networks. arXiv 2019, arXiv:1902.04103. [Google Scholar]
    Zheng, Z.; Wang, P.; Liu, W.; Li, J.; Ye, R.; Ren, D. Distance-IoU loss: Faster
    and better learning for bounding box regression. AAAI Tech. Track Vis. 2020, 34,
    12993–13000. [Google Scholar] [CrossRef] Roy, A.M.; Bhaduri, J. Real-time growth
    stage detection model for high degree of occultation using DenseNet-fused YOLOv4.
    Comput. Electron. Agric. 2022, 193, 106694. [Google Scholar] [CrossRef] Chang,
    C.L.; Chen, H.W. Straight-line generation approach using deep learning for mobile
    robot guidance in lettuce fields. In Proceedings of the 2023 9th International
    Conference on Applied System Innovation (ICASI), Chiba, Japan, 21–25 April 2023.
    [Google Scholar] Lee, C.C. Fuzzy logic in control system: Fuzzy logic controller.
    IEEE Trans. Syst. Man Cybern. Syst. 1990, 20, 404–418. [Google Scholar] [CrossRef]
    Yu, C.C.; Tsen, Y.W.; Chang, C.L. Modeled Carrier. TW Patent No. I706715, 11 October
    2020. [Google Scholar] Bennett, P. The NMEA FAQ (Fragen und Antworten zu NMEA),
    Ver. 6.1; Sepember 1997. Available online: https://www.geocities.ws/lrfernandes/gps_project/Appendix_E_NMEA_FAQ.pdf
    (accessed on 30 January 2023). Shih, P.T.-Y. TWD97 and WGS84, datum or map projection?
    J. Cadastr. Surv. 2020, 39, 1–12. [Google Scholar] Lee, J.; Hwang, K.I. YOLO with
    adaptive frame control for real-time object detection applications. Multimed.
    Tools Appl. 2022, 81, 36375–36396. [Google Scholar] [CrossRef] Hasan, R.I.; Yusuf,
    S.M.; Alzubaidi, L. Review of the state of the art of deep learning for plant
    diseases: A broad analysis and discussion. Plants 2020, 9, 1302. [Google Scholar]
    [CrossRef] Arsenovic, M.; Karanovic, M.; Sladojevic, S.; Anderla, A.; Stefanovic,
    D. Solving current limitations of deep learning based approaches for plant disease
    detection. Symmetry 2019, 11, 939. [Google Scholar] [CrossRef] Zhang, Y.; Chen,
    H.; He, Y.; Ye, M.; Cai, X.; Zhang, D. Road segmentation for all-day outdoor robot
    navigation. Neurocomputing 2018, 314, 316–325. [Google Scholar] [CrossRef] Liu,
    J.; Wang, X. Plant diseases and pests detection based on deep learning: A review.
    Plant Methods 2021, 17, 22–35. [Google Scholar] [CrossRef] [PubMed] Jiao, L.;
    Zhang, R.; Liu, F.; Yang, S.; Hou, B.; Li, L.; Tang, X. New generation deep learning
    for video object detection: A survey. IEEE Trans. Neural Netw. Learn. Syst. 2021,
    33, 3195–3215. [Google Scholar] [CrossRef] [PubMed] Li, D.; Wang, R.; Xie, C.;
    Liu, L.; Zhang, J.; Li, R.; Wang, F.; Zhou, M.; Liu, W. A recognition method for
    rice plant diseases and pests video detection based on deep convolutional neural
    network. Sensors 2020, 20, 578. [Google Scholar] [CrossRef] Altalak, M.; Ammad
    uddin, M.; Alajmi, A.; Rizg, A. Smart agriculture applications using deep learning
    technologies: A survey. Appl. Sci. 2022, 12, 5919. [Google Scholar] [CrossRef]
    Chang, C.L.; Chen, H.W.; Chen, Y.H.; Yu, C.C. Drip-tape-following approach based
    on machine vision for a two-wheeled robot trailer in strip farming. Agriculture
    2022, 12, 428. [Google Scholar] [CrossRef] del Rey, J.C.; Vega, J.A.; Pérez-Ruiz,
    M.; Emmi, L. Comparison of positional accuracy between RTK and RTX GNSS based
    on the autonomous agricultural vehicles under field conditions. Appl. Eng. Agric.
    2014, 30, 361–366. [Google Scholar] Han, J.H.; Park, C.H.; Park, Y.J.; Kwon, J.H.
    Preliminary results of the development of a single-frequency GNSS RTK-based autonomous
    driving system for a speed sprayer. J. Sens. 2019, 2019, 4687819. [Google Scholar]
    [CrossRef] Gonzalez-de-Soto, M.; Emmi, L.; Perez-Ruiz, M.; Aguera, J.; Gonzalez-de-Santos,
    P. Autonomous systems for precise spraying—Evaluation of a robotised patch sprayer.
    Biosyst. Eng. 2016, 146, 165–182. [Google Scholar] [CrossRef]                 Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2023 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Chang, C.-L.; Chen, H.-W.; Ke, J.-Y. Robust
    Guidance and Selective Spraying Based on Deep Learning for an Advanced Four-Wheeled
    Farming Robot. Agriculture 2024, 14, 57. https://doi.org/10.3390/agriculture14010057
    AMA Style Chang C-L, Chen H-W, Ke J-Y. Robust Guidance and Selective Spraying
    Based on Deep Learning for an Advanced Four-Wheeled Farming Robot. Agriculture.
    2024; 14(1):57. https://doi.org/10.3390/agriculture14010057 Chicago/Turabian Style
    Chang, Chung-Liang, Hung-Wen Chen, and Jing-Yun Ke. 2024. \"Robust Guidance and
    Selective Spraying Based on Deep Learning for an Advanced Four-Wheeled Farming
    Robot\" Agriculture 14, no. 1: 57. https://doi.org/10.3390/agriculture14010057
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 10. Jan 20. Jan 30. Jan 9.
    Feb 19. Feb 29. Feb 10. Mar 20. Mar 30. Mar 0 1000 250 500 750 For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Agriculture, EISSN 2077-0472, Published by MDPI RSS
    Content Alert Further Information Article Processing Charges Pay an Invoice Open
    Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For
    Editors For Librarians For Publishers For Societies For Conference Organizers
    MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia
    JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive
    issue release notifications and newsletters from MDPI journals Select options
    Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated Disclaimer
    Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Agriculture (Switzerland)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Robust Guidance and Selective Spraying Based on Deep Learning for an Advanced
    Four-Wheeled Farming Robot
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Wolter-Salas S.
  - Canessa P.
  - Campos-Vargas R.
  - Opazo M.C.
  - V. Sepulveda R.
  - Aguayo D.
  citation_count: '0'
  description: Lettuce (Lactuca sativa L.) is highly susceptible to drought and water
    deficits, resulting in lower crop yields, unharvested areas, reduced crop health
    and quality. To address this, we developed a High-Throughput Phenotyping platform
    using Deep Learning and infrared images to detect stress stages in lettuce seedlings,
    which could help to apply real-time agronomical decisions from data using variable
    rate irrigation systems. Accordingly, a comprehensive database comprising infrared
    images of lettuce grown under drought-induced stress conditions was built. In
    order to capture the required data, we deployed a Raspberry Pi robot to autonomously
    collect infrared images of lettuce seedlings during an 8-day drought stress experiment.
    This resulted in the generation of a database containing 2119 images through augmentation.
    Leveraging this data, a YOLOv8 model was trained (WS-YOLO), employing instance
    segmentation for accurate stress level detection. The results demonstrated the
    efficacy of our approach, with WS-YOLO achieving a mean Average Precision (mAP)
    of 93.62% and an F1 score of 89.31%. Particularly, high efficiency in early stress
    detection was achieved, being a critical factor for improving food security through
    timely interventions. Therefore, our proposed High-Throughput Phenotyping platform
    holds the potential for high-yield lettuce breeding, enabling early stress detection
    and supporting informed decision-making to mitigate losses. This interdisciplinary
    approach highlights the potential of AI-driven solutions in addressing pressing
    challenges in food production and sustainability. This work contributes to the
    field of precision agricultural technology, providing opportunities for further
    research and implementation of cutting-edge Deep Learning techniques for stress
    detection in crops.
  doi: 10.1007/978-3-031-48858-0_27
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Advanced Research in Technologies,
    Information, Innovation and Sustainability Conference paper WS-YOLO: An Agronomical
    and Computer Vision-Based Framework to Detect Drought Stress in Lettuce Seedlings
    Using IR Imaging and YOLOv8 Conference paper First Online: 20 December 2023 pp
    339–351 Cite this conference paper Access provided by University of Nebraska-Lincoln
    Download book PDF Download book EPUB Advanced Research in Technologies, Information,
    Innovation and Sustainability (ARTIIS 2023) Sebastian Wolter-Salas , Paulo Canessa
    , Reinaldo Campos-Vargas , Maria Cecilia Opazo , Romina V. Sepulveda & Daniel
    Aguayo  Part of the book series: Communications in Computer and Information Science
    ((CCIS,volume 1935)) Included in the following conference series: International
    Conference on Advanced Research in Technologies, Information, Innovation and Sustainability
    160 Accesses Abstract Lettuce (Lactuca sativa L.) is highly susceptible to drought
    and water deficits, resulting in lower crop yields, unharvested areas, reduced
    crop health and quality. To address this, we developed a High-Throughput Phenotyping
    platform using Deep Learning and infrared images to detect stress stages in lettuce
    seedlings, which could help to apply real-time agronomical decisions from data
    using variable rate irrigation systems. Accordingly, a comprehensive database
    comprising infrared images of lettuce grown under drought-induced stress conditions
    was built. In order to capture the required data, we deployed a Raspberry Pi robot
    to autonomously collect infrared images of lettuce seedlings during an 8-day drought
    stress experiment. This resulted in the generation of a database containing 2119
    images through augmentation. Leveraging this data, a YOLOv8 model was trained
    (WS-YOLO), employing instance segmentation for accurate stress level detection.
    The results demonstrated the efficacy of our approach, with WS-YOLO achieving
    a mean Average Precision (mAP) of 93.62% and an F1 score of 89.31%. Particularly,
    high efficiency in early stress detection was achieved, being a critical factor
    for improving food security through timely interventions. Therefore, our proposed
    High-Throughput Phenotyping platform holds the potential for high-yield lettuce
    breeding, enabling early stress detection and supporting informed decision-making
    to mitigate losses. This interdisciplinary approach highlights the potential of
    AI-driven solutions in addressing pressing challenges in food production and sustainability.
    This work contributes to the field of precision agricultural technology, providing
    opportunities for further research and implementation of cutting-edge Deep Learning
    techniques for stress detection in crops. Keywords Digital Agriculture Computer
    Vision High-Throughput Phenotyping Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others Automatic
    non-destructive multiple lettuce traits prediction based on DeepLabV3 + Article
    25 October 2022 IRPD: In-Field Radish Plant Dataset Chapter © 2023 Plant Stress
    Recognition Using Deep Learning and 3D Reconstruction Chapter © 2023 1 Introduction
    Nowadays, the agricultural sector has been severely affected by water shortages
    affecting horticultural production. The effects of water stress on horticultural
    crops can induce physiological stress in plants, leading to stunted growth, diminished
    produce quality, and increased susceptibility to pests and diseases (Molina-Montenegro
    et al., 2011; Knepper y Mou, 2015; Kumar et al. 2021). The resulting water scarcity
    has significantly affected lettuce (Lactuca sativa L.) an extensively cultivated
    leafy vegetable, requiring an adequate water supply for optimal growth and quality.
    Lettuce stands as one of the most extensively cultivated leafy vegetables globally,
    encompassing a cultivation area of 1.3 million hectares and yielding approximately
    29 million tons (Kim et al. 2016; Chen et al. 2019). Various cultivation methods
    are employed for lettuce, including hydroponic systems, greenhouses, and plant
    factories, while open-field cultivation remains prevalent (Donoso 2021). However,
    decreased water availability necessitates appropriate water management practices,
    influencing irrigation strategies and crop performance. Due to the water-intensive
    nature of lettuce cultivation, it is especially susceptible to water stress (Kumar
    et al. 2021). Optimal irrigation management during the seedling stage is closely
    linked to future productivity and the provision of healthy, uniform seedlings,
    thereby impacting the overall yield of horticultural crops (Shin et al. 2021).
    In seedling farms, the determination of irrigation management predominantly relies
    upon cultivation techniques and the visual discernment of the crop manager (Chen
    et al. 2014; Yang et al. 2020). Nonetheless, deficient and subjective cultivation
    methods yield undesirable consequences, including escalated labour and temporal
    requirements. In the context of water scarcity, the incorporation of Artificial
    Intelligence (AI) methodologies, specifically Deep Learning (DL), holds the potential
    for improving the well-being of vegetable crops, such as lettuce (Das Choudhury
    et al. 2019). DL algorithms excel at analyzing large volumes of data and extracting
    meaningful patterns, which can aid in optimizing irrigation management, predicting
    crop water needs, and improving resource-use efficiency (Cheng et al. 2020; Xiao
    et al. 2022; Gill et al. 2022). By leveraging the power of DL, it is possible
    to create predictive models that incorporate various environmental and crop-specific
    parameters to optimize irrigation scheduling, thereby reducing water usage and
    ensuring optimal plant growth (Kamarudin et al. 2021). In addition, DL algorithms
    can aid in the early detection and identification of plant stress symptoms caused
    by a lack of water, allowing for prompt intervention and mitigating yield losses
    (Kamarudin and Ismail 2022). For example, by analyzing Infrared (IR) imagery,
    these algorithms can detect minute changes in the observed phenotype, allowing
    for the early detection of crop stress phenotypes (Paulo et al. 2023). This early
    detection enables producers to implement targeted irrigation strategies, optimize
    resource allocation, and mitigate the adverse effects of water stress on crop
    health and yield (Islam and Yamane 2021; Chen et al. 2014). Thus, the precise
    and effective identification of stress-induced characteristics is imperative for
    the progression of our understanding of plant reactions to environmental stressors
    and the development of practical mitigation approaches. DL models, such as YOLO,
    have become prominent in the field of automated and High-Throughput Phenotyping
    (HTP) (Buzzy et al. 2020; Zhang and Li 2022; Cardellicchio et al. 2023; Xu et
    al. 2023). YOLO is a sophisticated object detection and instance segmentation
    model that utilizes deep neural networks to identify and precisely locate objects
    within images (Song et al. 2021). The cutting-edge architecture has exhibited
    exceptional efficacy in diverse Computer Vision (CV) assignments (Chen et al.
    2021). Using YOLO in plant science has significantly transformed phenotyping,
    providing a potent approach for automating stress-related phenotype identification
    and measurement (Chen et al. 2021, Xu et al. 2023). Through the process of training
    YOLO on extensive collections of plant images, the model can acquire the ability
    to identify and precisely locate stress symptoms (Mota-Delfin et al. 2022). Its
    real-time processing capability enables rapid analysis of large-scale datasets,
    facilitating HTP (James et al. 2022). This speed is critical for real-time monitoring
    of plant responses to stress and allows for timely interventions to mitigate damage
    and optimize crop management. Detailed knowledge of stress patterns in a plant
    population can guide targeted breeding tasks and precise agricultural interventions.
    1.1 Related Work  In recent years, there have been notable advancements in the
    field of crop phenotyping through the utilization of CV and DL approaches (Wang
    and Su 2022; Jiand and Li 2020; Chandra et al. 2020; Li et al. 2020). Particularly,
    instance segmentation refines the classic object detection task by identifying
    individual object instances and segmenting them pixel-wise. Recent studies have
    investigated the utilization of YOLO models in order to perform instance segmentation
    and object detection tasks specifically for plant phenotyping. In recent studies
    conducted by Khalid et al. (2023) and Qiu et al. (2022), a comparable methodology
    was employed, wherein numerous YOLO models were utilized for the timely identification
    of pests and illnesses in the field of agriculture. The primary objective of the
    research conducted by Khalid et al. (2023) is to discern and classify thistle
    caterpillars, red beetles, and citrus psylla pests. This was achieved through
    the utilization of a dataset of 9875 images, which were acquired under different
    lighting conditions. The YOLOv8 model demonstrates superior performance in the
    detection of tiny pests, surpassing prior studies with a mean Average Precision
    (mAP) of 84.7% and an average loss of 0.7939. Similarly, Rong et al. (2023) present
    a visual methodology for efficient point cloud processing of tomato organs, essential
    for automated crop management. The method involves segmenting tomato organs using
    instance segmentation and a strategy that utilizes point cloud constraints to
    match the organs. YOLOv5-4D detects the region of interest on tomatoes, achieving
    a mAP of 0.953, being slightly more accurate than the native YOLOv8 model. The
    proposed point cloud constraint-based search method effectively matches tomato
    organs in 3D space, yielding an 86.7% success rate in multiple real scenarios.
    However, the utilization of YOLOv5 in the agricultural sector, particularly in
    crop phenotype research, has gained significant traction and reached a level of
    maturity (Kong et al. 2023). Liu et al. (2023) proposed Small-YOLOv5 for automatically
    identifying the growth period of rice, which is crucial for producing high-yield
    and high-quality rice. The Small-YOLOv5 approach utilizes MobileNetV3 as the backbone
    feature extraction network, resulting in smaller model size and fewer parameters,
    thus improving the detection speed. Experimental results demonstrate that Small-YOLOv5
    outperforms other popular lightweight models, achieving a 98.7% mAP value at a
    threshold of 0.5 and a 94.7% mAP at a threshold range of 0.5 to 0.95. Moreover,
    Small-YOLOv5 significantly reduces the model parameters and volume. This is still
    project-dependent, as the work of Blekos et al. (2023) achieves the second-highest
    bounding box accuracy using YOLOv8 for grape maturity estimations utilizing their
    custom dataset of 2500 images. This result outperforms all YOLO versions, with
    an 11% accuracy margin compared to YOLOv3. Previous studies in this domain have
    not explored the application of YOLO for this purpose in lettuce. The closest
    related research is the work of Wang et al. (2023), which shares similarities
    but focuses on microscopic imaging of stomatal opening and closure. This excellent
    investigation has limitations, mainly in data acquisition and in the stage of
    abiotic stress. However, in our study, an IR camera is utilized to capture a complete
    frontal view of the entire plant. This approach eliminates the need for costly
    microscopic cameras and allows the use of ground drones for phenotyping purposes,
    enhancing water usage through the early detection of water stress. Herein we develop
    an HTP platform for the early detection of drought stress in lettuce using the
    DL model YOLOv8 by IR imaging. Accordingly, we developed an autonomous platform
    to generate a database that comprehends the different levels of stress that can
    affect lettuce over an extended period of water deficit. This database was used
    to train a YOLOv8 model that successfully from IR images lettuces exposed to different
    water stress levels. This development can be used to build novel strategies for
    efficient water use based on automatic stress detection. 2 Methodology 2.1 Plant
    Material and Experimental Conditions In order to define the state of water stress,
    it is necessary to grow lettuce seedlings. Lettuce seedlings (Lactuca sativa L.
    var. Capitata (L.) Janchen) were grown in a greenhouse under a controlled environment
    and irrigation. The lettuce seedlings were grown up to 10 days after the appearance
    of their true leaves continuing to the experimental phase. In this phase water
    was not administered to the experimental group for 8 days, maintaining a control
    group with normal water administration. A total of 72 individuals corresponded
    to the control group, meanwhile, 60 individuals corresponded to the experimental
    group. 2.2 Database Collection The water deficit stress level was defined using
    the morphological state of the lettuce seedlings. Based on this, an autonomous
    robot (named as High-Performance Autonomous Phenotyper, HPAP) with a camera capable
    of detecting and capturing digital images of lettuce seedlings in motion was built
    using Arduino. The images were using the Camera Module 2 Pi NoIR with a Sony IMX219
    8-megapixel sensor and calibrated with OpenCV library using Python. In addition,
    3 ultrasound sensor modules connected to the Raspberry Pi 3B were connected via
    a prototyping board and used to census the distance of its surroundings and make
    decisions on its trajectory automatically and correctively. HPAP is programmed
    to stay between 12 to 14 cm (cm) away from the surface where the lettuce seedlings
    are located, so if it is outside the threshold, it can correct its trajectory
    to maintain the proximity margin (Fig. 1). 2.3 Stress Detection The stress detection
    was assessed using YOLOv8 instance segmentation using a total of 2119 images (named
    Water Stress - YOLO, or WS-YOLO). The image resolution used was 640 × 480 pixels.
    The image annotation was automated using Supervision and fine-tuned Segment Anything
    Model (SAM) model using 4 states: ‘healthy lettuce’, ‘lettuce with mild stress’,
    ‘lettuce with moderate stress’ and ‘lettuce with severe stress’ based on the morphological
    characteristics and literature. The control group was annotated as ‘healthy’ as
    long as it did not show symptoms of water stress. Lettuces were annotated as ‘moderately
    stressed’ when they exhibited the first symptoms of water stress, such as slight
    wilting of the leaves and reduced turgor. In the case of ‘severe stress’ (or plant
    death) lettuces were annotated when they had traits such as wilting, reduced leaf
    area, leaf decay and loss of biomass. Finally, ‘mild’ (or early) stressed plants
    belonging to the experimental group (without irrigation) were annotated when they
    exhibited no stress symptoms since day 1. This was corroborated in the literature
    (see Discussion). Furthermore, the dataset was augmented by applying a horizontal
    flip, crop with a minimum zoom of 13% and a maximum zoom of 50%, rotation of 20°,
    saturation of 20%, exposure of 15%, and up to 5% pixel noise. The dataset was
    divided into training, validation, and test sets (70:20:10). The hyperparameters
    used in WS-YOLO model were defined to achieve a trade-off between the precision
    of the model and its computational efficiency. A batch size of 6 was used, resulting
    in good training times while preserving model stability. Additionally, an initial
    learning rate of 1*10^5 was implemented using ‘AdamW’ optimizer during 25 epochs.
    The incorporation of momentum of 0.85 and weight decay of 1*10^4 was implemented
    to expedite convergence and forestall overfitting. The PyTorch DL framework was
    utilized to implement the WS-YOLO model, which underwent training on a Windows
    System equipped with a 14-core Intel i5 CPU and an NVIDIA RTX 3070 Ti graphics
    card. The pre-trained model used was YOLOv8x-seg. 2.4 Model Evaluation The efficacy
    of the performed experiments on the WS-YOLO model was evaluated using Precision,
    Recall, F1-score, and Mean Average Precision (mAP) of 50% and between 50 and 95%
    of the Intersection over Union (IoU) as the evaluation metrics. The methodology
    for calculating is presented in the Eqs. (1–4). The abbreviations of these equations
    are: True positive (TP), False Positive (FP), False Negative (FN) and Average
    Precision (AP). $$\\mathbf{P}\\mathbf{r}\\mathbf{e}\\mathbf{c}\\mathbf{i}\\mathbf{s}\\mathbf{i}\\mathbf{o}\\mathbf{n}=
    \\frac{{\\text{TP}}}{\\mathrm{TP }+\\mathrm{ FP}}$$ (1) $$\\mathbf{R}\\mathbf{e}\\mathbf{c}\\mathbf{a}\\mathbf{l}\\mathbf{l}=
    \\frac{{\\text{TP}}}{\\mathrm{TP }+\\mathrm{ FN}}$$ (2) $$\\mathbf{F}1= \\frac{2
    *\\mathrm{ Precision }*\\mathrm{ Recall}}{\\mathrm{Precision }+\\mathrm{ Recall}}$$
    (3) $$\\mathbf{m}\\mathbf{A}\\mathbf{P}= \\frac{1}{{\\text{n}}}\\sum_{\\mathrm{k
    }= 1}^{\\mathrm{k }=\\mathrm{ n}}{{\\text{AP}}}_{\\mathrm{ k}}$$ (4) 3 Results
    and Discussion The trained WS-YOLO model successfully detects the experimentally-defined
    stress levels of lettuce seedlings. The model was evaluated using Precision, Recall,
    F1-score, mAP (50%), and mAP (50–95%) of the lettuce image analysis for evaluating
    the performance and effectiveness. Table 1 displays the experimental outcomes.
    Table 1. WS-YOLO performance. Full size table The calculated precision shows that
    90.63% of the instances predicted by the model are correct. This high precision
    value indicates a low false positive rate for this model. Furthermore, the recall
    value indicates that the model correctly identified 88.08% of real instances.
    This observation suggests a significant level of recall, indicating that the model
    exhibits a low rate of false negatives (Fig. 2). The F1 score is the harmonic
    mean of precision and recall, providing a balanced measure of both metrics. With
    an F1 score of 89.31%, this model has a good balance between precision and recall,
    obtaining the best score with 0.593 confidence (Fig. 3). The mAP at an IoU threshold
    of 50% measures the average precision across different segmented categories. A
    score of 93.62% indicates that the model achieves a high precision-recall tradeoff
    on average across the segmented categories. This result indicates good overall
    performance. The mAP assessed across a range of IoU thresholds from 50% to 95%
    provides a more stringent evaluation. The model demonstrates satisfactory performance
    in terms of precision and recall throughout a broader spectrum of IoU thresholds,
    albeit slightly lower than the mAP at 50%, with a value of 74.07%. With respect
    to other plant phenotyping research, the values presented in this study are within
    good parameters. In example, the work conducted by Lin et al. (2023) introduces
    YOLO-Tobacco, an improved YOLOX-Tiny network designed for detecting fungal-induced
    diseases. This network achieved a detection accuracy of 80.45% AP, a recall of
    69.27%, a precision of 86.25%, and an F1-score of 0.7683. Although these results
    are slightly lower compared to our findings (Table 1), this discrepancy may be
    attributed to the limited number of images used for model training. Conversely,
    Wang and He (2021) utilized the YOLOv5s model to successfully detect apple fruitlets
    before fruit thinning. Their study yielded favourable outcomes by employing a
    dataset containing 3165 images, resulting in a recall of 87.6%, precision of 95.8%,
    and an F1-score of 91.5%. In contrast to the previous example, these results surpass
    those achieved by the WS-YOLO model. Similarly, Isaac et al. (2023) employed the
    YOLOv5 model to detect cotton bolls. Their research achieved a precision of 0.84,
    a recall of 0.99, and an F1-score of 0.904 by utilizing a dataset comprising 9000
    images. Therefore, obtaining a larger image dataset is essential for attaining
    improved results. The WS-YOLO model visualization (Fig. 4) was carried out to
    view the segmented and detected lettuces with their stress levels across the experiment
    duration. According to our experimental design, the model successfully segmented
    healthy lettuces on day 8. This consistency is maintained throughout the experiment.
    In turn, Fig. 3 shows evidence of the detection of mild stress in lettuce during
    day 2. Mild water stress was detected on the second day using IR images, which
    provide different information compared to conventional images. Numerous phenotypic
    alterations, such as changes in biomass, are often observable following stress
    treatment, although certain changes, like water content dynamics, are less obvious
    or subtle to discern without specialized instruments. Previous studies have mentioned
    that IR images would be particularly effective for analyzing water stress (Berger
    et al. 2010; Munns et al. 2010; Chen et al. 2014). IR cameras possess high spectral
    sensitivity, capturing wavelengths between 400 and 1000 nm. This range allows
    for capturing information about leaf width, which is influenced by their water
    content (Fahlgren et al. 2015). Specifically, wavelengths between 700 and 1000
    nm exhibit higher reflectance in plant tissues compared to visible light, whose
    reflection is affected by leaf thickness (Ma et al. 2022). Osco et al. (2019)
    effectively identified physiological alterations resulting from water stress by
    employing hyperspectral imaging. Furthermore, they employed artificial neural
    network algorithms to classify the obtained images on the initial day of the experiment.
    Additionally, Basahi et al. (2014) determined a decrease in the relative water
    content in lettuce after 2 days of water stress. Moreover, the study conducted
    by Knepper and Mou (2015) supports this observation, reporting similar findings
    in three distinct lettuce varieties. Notably, only one of these strains exhibited
    a significant reduction in the relative water content of its leaves upon the initial
    day of drought stress induction. Based on this, it can be inferred that lettuce
    begins to experience water stress within a few days of initiating the experimental
    phase. Accordingly, on the fourth day, the model detected the first signs of wilting
    in the leaves of lettuce subjected to water stress (Fig. 4). Leaf wilting, as
    a morphological trait of stress, is caused by the loss of turgidity in the leaves,
    which eventually yields due to a lack of cellular elasticity (Seleiman et al.
    2021). This finding aligns with the study conducted by Shin et al. 2020, where
    similar leaf morphology was detected on the sixth day. However, it is important
    to consider that environmental differences and variations in lettuce strains may
    influence these observations. As discussed earlier, stress begins to manifest
    early depending on the resistance of the specific strain. Additionally, it is
    worth noting that not all lettuce plants in Fig. 4 exhibit moderate stress. Some
    lettuce plants do not show notable morphological characteristics of this stress,
    which may be attributed to genetic differences. These differences can result in
    slightly more resistant lettuce plants compared to others in response to drought
    stress (Lafta et al. 2021; Park et al. 2021). By the eighth day of water stress,
    a complete leaf drooping is observed, with reduced leaf area and length, indicating
    severe stress or plant death. This condition is attributed to the low soil moisture
    content and insufficient physiological responses to cope with advanced water stress.
    The novelty of our study resides in the integration of CV techniques to tackle
    the difficult task of detecting early stress in crops with a vision of inexpensive
    agronomic management. Although previous work explored the use of CV methods to
    determine plant phenotypes, the use of the YOLOv8 (WS-YOLO) model to assess stress
    levels in lettuce by IR cameras, to our knowledge, is unprecedented. This innovative
    approach leverages YOLOv8''s segmentation capabilities, allowing for precise identification
    and characterization of stress-induced phenotypic changes through IR imaging with
    good accuracy. In addition, the creation of a comprehensive database of IR images
    obtained through automated data collection of the Raspberry Pi robot expands the
    range of alternatives for data collection for similar research studies. As a result,
    our research innovates by combining the most advanced DL techniques with automated
    IR data collection, leading to alternatives for precision irrigation, food safety
    and sustainability. While there have been notable advancements in crop phenotyping
    and instance segmentation using YOLO-based models, there remain challenges in
    handling diverse environmental conditions, scale variations, and occlusions in
    crop images. Our study has revealed promising findings regarding the use of YOLOv8
    for stress detection in lettuce. However, there are several areas that warrant
    further investigation in future research. Firstly, it would be beneficial to expand
    the application of this approach to different crop species to assess its generalizability
    and effectiveness. Water stress caused by drought is already causing issues in
    the cultivation of various leafy vegetables (Khalid et al. 2022). Additionally,
    an intriguing avenue for future research involves incorporating temporal dynamics
    into the model. This would involve training the WS-YOLO model on sequential imagery,
    enabling real-time stress monitoring throughout the entire lifecycle of a crop.
    Additionally, considering the ever-evolving landscape of deep learning architectures,
    it would be valuable for future studies to evaluate the performance of emerging
    and older models and architectures in comparison to WS-YOLO. This could potentially
    lead to even higher levels of accuracy in stress detection. Finally, it is necessary
    to assess the phenotypic state using omics techniques. This has the potential
    to enhance the classification and robustness of this study. Fig. 1. Experimental
    design for WS-YOLO development. Full size image Fig. 2. Normalized Confusion Matrix
    of WS-YOLO. Full size image Fig. 3. F1 score over Confidence in different categories
    of stress detection of WS-YOLO. Full size image Fig. 4. WS-YOLO detection on Control
    and Water Stress group through the duration of the experiment. Full size image
    4 Conclusion and Future Perspectives This research contributes to the field of
    agricultural technology and stress detection in lettuce. By introducing a novel
    HTP platform that leverages DL, Robotics, and CV, the study addresses the critical
    challenge of early stress detection through IR imaging in lettuce, crucial for
    ensuring food security and mitigating yield losses. The application of WS-YOLO
    model with instance segmentation demonstrates promising results, achieving a mAP
    of 93.62% and an F1 score of 89.31%. These findings showcase the efficacy and
    potential of AI-driven solutions in tackling pressing challenges in food production
    and sustainability. Moreover, the creation of a comprehensive database of IR images
    through autonomous data collection further enriches the scientific knowledge base
    and opens opportunities for further research in cutting-edge DL techniques for
    stress detection in crops. Nonetheless, effectively demonstrating water stress
    in lettuce through experimental analysis is crucial. This approach would provide
    greater robustness in phenotype detection and enable the characterization of the
    physiology of this lettuce strain. References Basahi, J.: Effects of Enhanced
    UV-B Radiation and Drought Stress on Photosynthetic Performance of Lettuce (Lactuca
    sativa L. Romaine) Plants. Ann. Res. Rev. Biol. 4, 1739–1756 (2014) Google Scholar   Berger,
    B., Parent, B., Tester, M.: High-throughput shoot imaging to study drought responses.
    J. Exp. Bot. 61, 3519–3528 (2010) Article   Google Scholar   Blekos, A., et al.:
    A grape dataset for instance segmentation and maturity estimation. Agronomy 13,
    1995 (2023) Article   Google Scholar   Buzzy, M., Thesma, V., Davoodi, M., Mohammadpour
    Velni, J.: Real-time plant leaf counting using deep object detection networks.
    Sensors 20, 6896 (2020) Google Scholar   Cardellicchio, A., et al.: Detection
    of tomato plant phenotyping traits using YOLOv5-based single stage detectors.
    Comput. Electron. Agric.. Electron. Agric. 207, 107757 (2023) Article   Google
    Scholar   Chandra, A.L., Desai, S.V., Guo, W., Balasubramanian, V.N.: Computer
    Vision with Deep Learning for Plant Phenotyping in Agriculture: A Survey. arXiv
    1–26 (2020). https://doi.org/10.48550/arXiv.2006.11391 Chen, D., et al.: Dissecting
    the phenotypic components of crop plant growth and drought responses based on
    high-throughput image analysis. Plant Cell 26, 4636–4655 (2014) Article   Google
    Scholar   Chen, W., Zhang, J., Guo, B., Wei, Q., Zhu, Z.: An apple detection method
    based on Des-YOLO v4 algorithm for harvesting robots in complex environment. Math.
    Probl. Eng.Probl. Eng. 2021, 1–12 (2021) Google Scholar   Chen, Z., et al.: Assessing
    the performance of different irrigation systems on lettuce (Lactuca sativa L.)
    in the greenhouse. PLOS ONE 14, e0209329 (2019) Google Scholar   Cheng, Q., Zhang,
    S., Bo, S., Chen, D., Zhang, H.: Augmented reality dynamic image recognition technology
    based on deep learning algorithm. IEEE Access 8, 137370–137384 (2020) Article   Google
    Scholar   Das Choudhury, S., Samal, A., Awada, T.: Leveraging image analysis for
    high-throughput plant phenotyping. Front. Plant Sci. 10 (2019) Google Scholar   Donoso,
    G.: Management of water resources in agriculture in chile and its challenges.
    Int. J. Agric. Natural Resources 48, 171–185 (2021) Article   Google Scholar   Fahlgren,
    N., Gehan, M.A., Baxter, I.: Lights, camera, action: high-throughput plant phenotyping
    is ready for a close-up. Curr. Opin. Plant Biol.. Opin. Plant Biol. 24, 93–99
    (2015) Article   Google Scholar   Gill, T., Gill, S.K., Saini, D.K., Chopra, Y.,
    de Koff, J.P., Sandhu, K.S.: A comprehensive review of high throughput phenotyping
    and machine learning for plant stress phenotyping. Phenomics 2, 156–183 (2022)
    Article   Google Scholar   Islam, M.P., Yamane, T.: HortNet417v1—a deep-learning
    architecture for the automatic detection of pot-cultivated peach plant water stress.
    Sensors 21, 7924 (2021) Article   Google Scholar   James, K.M.F., Sargent, D.J.,
    Whitehouse, A., Cielniak, G.: High-throughput phenotyping for breeding targets—Current
    status and future directions of strawberry trait automation. Plants, People, Planet
    4, 432–443 (2022) Article   Google Scholar   Kamarudin, M.H., Ismail, Z.H.: Lightweight
    deep CNN models for identifying drought stressed plant. IOP Conf. Ser. Earth Environ.
    Sci. 1091, 012043 (2022) Article   Google Scholar   Kamarudin, M.H., Ismail, Z.H.,
    Saidi, N.B.: Deep learning sensor fusion in plant water stress assessment: a comprehensive
    review. Appl. Sci. 11, 1403 (2021) Article   Google Scholar   Khalid, M.F., et
    al.: Alleviation of drought and salt stress in vegetables: crop responses and
    mitigation strategies. Plant Growth Regul.Regul. 99, 177–194 (2022) Article   Google
    Scholar   Khalid, S., Oqaibi, H.M., Aqib, M., Hafeez, Y.: Small pests detection
    in field crops using deep learning object detection. Sustainability 15, 6815 (2023)
    Article   Google Scholar   Kim, M. J., Moon, Y., Tou, J. C., Mou, B., Waterland,
    N.L.: Nutritional value, bioactive compounds and health benefits of lettuce (Lactuca
    sativa L.). J. Food Composition Anal. 49, 19–34 (2016) Google Scholar   Knepper,
    C., Mou, B.: Semi-high throughput screening for potential drought-tolerance in
    lettuce (lactuca sativa) germplasm collections. J. Vis. Exp. 98, 1–6 (2015) Google
    Scholar   Kong, S., Li, J., Zhai, Y., Gao, Z., Zhou, Y., Xu, Y.: Real-time detection
    of crops with dense planting using deep learning at seedling stage. Agronomy 13,
    1503 (2023) Article   Google Scholar   Kumar, P., Eriksen, R. L., Simko, I., Mou,
    B.: Molecular mapping of water-stress responsive genomic loci in lettuce (Lactuca
    spp.) using kinetics Chlorophyll fluorescence, hyperspectral imaging and machine
    learning. Front. Genetics 12 (2021) Google Scholar   Lafta, A., Sandoya, G., Mou,
    B.: Genetic variation and genotype by environment interaction for heat tolerance
    in crisphead lettuce. HortScience 56, 126–135 (2021) Article   Google Scholar   Li,
    Z., Guo, R., Li, M., Chen, Y., Li, G.: A review of computer vision technologies
    for plant phenotyping. Comput. Electron. Agric.. Electron. Agric. 176, 105672
    (2020) Article   Google Scholar   Lin, J., et al.: Improved YOLOX-Tiny network
    for detection of tobacco brown spot disease. Front. Plant Sci. 14 (2023) Google
    Scholar   Liu, K., Wang, J., Zhang, K., Chen, M., Zhao, H., Liao, J.: A lightweight
    recognition method for rice growth period based on improved YOLOv5s. Sensors 23,
    6738 (2023) Article   Google Scholar   Ma, Z., et al.: A review on sensing technologies
    for high-throughput plant phenotyping. IEEE Open J. Instr. Measure. 1, 1–21 (2022)
    Article   Google Scholar   Mota-Delfin, C., López-Canteñs, G. de J., López-Cruz,
    I.L., Romantchik-Kriuchkova, E., Olguín-Rojas, J.C.: Detection and counting of
    corn plants in the presence of weeds with convolutional neural networks. Remote
    Sensing 14, 4892 (2022) Google Scholar   Munns, R., James, R.A., Sirault, X.R.R.,
    Furbank, R.T., Jones, H.G.: New phenotyping methods for screening wheat and barley
    for beneficial responses to water deficit. J. Exp. Bot. 61, 3499–3507 (2010) Article   Google
    Scholar   Osco, L.P., et al.: Modeling hyperspectral response of water-stress
    induced lettuce plants using artificial neural networks. Remote Sensing 11, 2797
    (2019) Article   Google Scholar   Park, S., Kumar, P., Shi, A., Mou, B.: Population
    genetics and genome‐wide association studies provide insights into the influence
    of selective breeding on genetic variation in lettuce. The Plant Genome 14 (2021)
    Google Scholar   de Paulo, R.L., Garcia, A.P., Umezu, C.K., de Camargo, A.P.,
    Soares, F.T., Albiero, D.: Water stress index detection using a low-cost infrared
    sensor and excess green image processing. Sensors 23, 1318 (2023) Article   Google
    Scholar   Qiu, R.-Z., et al.: An automatic identification system for citrus greening
    disease (Huanglongbing) using a YOLO convolutional neural network. Frontiers in
    Plant Science 13 (2022) Google Scholar   Rong, J., Yang, Y., Zheng, X., Wang,
    S., Yuan, T., Wang, P.: Three-Dimensional Plant Pivotal Organs Photogrammetry
    on Cherry Tomatoes Using an Instance Segmentation Method and a Spatial Constraint
    Search Strategy. (2023). https://doi.org/10.2139/ssrn.4482155 Article   Google
    Scholar   Seleiman, M.F., et al.: Drought stress impacts on plants and different
    approaches to alleviate its adverse effects. Plants 10, 259 (2021) Article   Google
    Scholar   Song, P., Wang, J., Guo, X., Yang, W., Zhao, C.: High-throughput phenotyping:
    breaking through the bottleneck in future crop breeding. Crop J. 9, 633–645 (2021)
    Article   Google Scholar   Wang, D., He, D.: Channel pruned YOLO V5s-based deep
    learning approach for rapid and accurate apple fruitlet detection before fruit
    thinning. Biosys. Eng.. Eng. 210, 271–281 (2021) Article   Google Scholar   Wang,
    J., Renninger, H., Ma, Q., Jin, S.: StoManager1: An Enhanced, Automated, and High-throughput
    Tool to Measure Leaf Stomata and Guard Cell Metrics Using Empirical and Theoretical
    Algorithms. arXiv 1–15 (2023). https://doi.org/10.48550/arXiv.2304.10450 Wang,
    Y., et al.: Insights into the stabilization of landfill by assessing the diversity
    and dynamic succession of bacterial community and its associated bio-metabolic
    process. Sci. Total. Environ. 768, 145466 (2021) Article   Google Scholar   Wang,
    Y.-H., Su, W.-H.: Convolutional neural networks in computer vision for grain crop
    phenotyping: a review. Agronomy 12, 2659 (2022) Article   Google Scholar   Xiao,
    Q., Bai, X., Zhang, C., He, Y.: Advanced high-throughput plant phenotyping techniques
    for genome-wide association studies: a review. J. Adv. Res. 35, 215–230 (2022)
    Article   Google Scholar   Xu, J., Yao, J., Zhai, H., Li, Q., Xu, Q., Xiang, Y.,
    Liu, Y., Liu, T., Ma, H., Mao, Y., Wu, F., Wang, Q., Feng, X., Mu, J. & Lu, Y.
    TrichomeYOLO: A Neural Network for Automatic Maize Trichome Counting. Plant Phenomics
    5, (2023) Google Scholar   Yang, W., et al.: Crop phenomics and high-throughput
    phenotyping: past decades, current challenges, and future perspectives. Mol. Plant
    13, 187–214 (2020) Article   Google Scholar   Zhang, P., Li, D.: YOLO-VOLO-LS:
    a novel method for variety identification of early lettuce seedlings. Front. Plant
    Sci. 13 (2022) Google Scholar   Download references Acknowledgement This research
    was funded by ANID BECAS/DOCTORADO NACIONAL (2023) 21231516 (S.W.S.), ANID/FONDECYT
    1200260 (R.C.V.), FONDEF ID19I10160 (D.A.), Proyecto interno UDLA DI-08/22 (C.O.),
    ANID/Millennium Science Initiative Program ICN17_022 and ANID/FONDECYT 1190611
    (P.C.). Author information Authors and Affiliations Center for Bioinformatics
    and Integrative Biology, Facultad de Ciencias de la Vida, Universidad Andrés Bello,
    Av. República 330, 8370186, Santiago, Chile Sebastian Wolter-Salas & Romina V.
    Sepulveda Centro de Biotecnología Vegetal, Facultad de Ciencias de la Vida, Universidad
    Andrés Bello, Av. República 330, 8370186, Santiago, Chile Paulo Canessa ANID–Millennium
    Science Initiative–Millennium Institute for Integrative Biology (iBIO), Av. Libertador
    Bernardo O’Higgins 340, 7500565, Santiago, Chile Paulo Canessa Centro de Estudios
    Postcosecha, Facultad de Ciencias Agronómicas, Universidad de Chile, Av. Santa
    Rosa 11315, 8831314, Santiago, Chile Reinaldo Campos-Vargas Instituto de Ciencias
    Naturales, Facultad de Medicina Veterinaria y Agronomía, Universidad de Las Américas,
    Av. Manuel Montt 948, 7500000, Santiago, Chile Maria Cecilia Opazo Instituto de
    Tecnología para la Innovación en Salud y Bienestar, Facultad de Ingeniería, Universidad
    Andrés Bello, Quillota 980, 2531015, Viña del Mar, Chile Daniel Aguayo Corresponding
    author Correspondence to Romina V. Sepulveda . Editor information Editors and
    Affiliations Universidad Estatal Peninsula de Santa Elena Campus Matriz, La Libertad,
    Ecuador Teresa Guarda Algoritmi Research Centre, University of Minho, Guimarães,
    Portugal Filipe Portela Universidad a Distancia de Madrid, Madrid, Spain Jose
    Maria Diaz-Nafria Rights and permissions Reprints and permissions Copyright information
    © 2024 The Author(s), under exclusive license to Springer Nature Switzerland AG
    About this paper Cite this paper Wolter-Salas, S., Canessa, P., Campos-Vargas,
    R., Opazo, M.C., V. Sepulveda, R., Aguayo, D. (2024). WS-YOLO: An Agronomical
    and Computer Vision-Based Framework to Detect Drought Stress in Lettuce Seedlings
    Using IR Imaging and YOLOv8. In: Guarda, T., Portela, F., Diaz-Nafria, J.M. (eds)
    Advanced Research in Technologies, Information, Innovation and Sustainability.
    ARTIIS 2023. Communications in Computer and Information Science, vol 1935. Springer,
    Cham. https://doi.org/10.1007/978-3-031-48858-0_27 Download citation .RIS.ENW.BIB
    DOI https://doi.org/10.1007/978-3-031-48858-0_27 Published 20 December 2023 Publisher
    Name Springer, Cham Print ISBN 978-3-031-48857-3 Online ISBN 978-3-031-48858-0
    eBook Packages Computer Science Computer Science (R0) Share this paper Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Publish
    with us Policies and ethics Sections Figures References Abstract Introduction
    Methodology Results and Discussion Conclusion and Future Perspectives References
    Acknowledgement Author information Editor information Rights and permissions Copyright
    information About this paper Publish with us Discover content Journals A-Z Books
    A-Z Publish with us Publish your research Open access publishing Products and
    services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Communications in Computer and Information Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'WS-YOLO: An Agronomical and Computer Vision-Based Framework to Detect Drought
    Stress in Lettuce Seedlings Using IR Imaging and YOLOv8'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Narayana G.V.S.
  - Kuanar S.K.
  - Patel P.
  citation_count: '0'
  description: A weed is a wild, undesired plant that grows naturally along with the
    desired crop. For their growth, they compete with the main crop for various resources
    like space, sunlight, irrigation, nutrients, etc. This leads to an enormous loss
    in the yield of the main crop and hence needs to be selectively controlled. Human
    intervention in the process of identification of weeds and subsequently its removal
    is extremely tedious and time-consuming too. Achieving the desired level of accuracy
    and preciseness in the manual weed identification process is impracticable. In
    recent years, researchers have proposed computer vision-based methods for automatic
    weed identification in precision agriculture. In this paper, we have used YOLOv7-X
    for automatically detecting weeds in cotton production systems. YOLOv7-X is a
    relatively new addition to the You Only Look Once (YOLO) family of fast and accurate
    algorithms. The benchmark dataset used for the purpose of validating our results
    is CottonWeedid-15. This dataset is customized with annotations suitable for YOLOv7-X
    by using the roboflow tool. The experimental study demonstrates that the YOLOv7-X
    model’s mean average precision (mAP@.5) can attain 96.6 %. The average precision
    and average recall of the model were 0.914 and 0.953 respectively. This model
    can also be used to classify several weeds in various crops.
  doi: 10.1007/978-981-99-3932-9_27
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Intelligent Systems Conference
    paper Weed Detection in Cotton Production Systems Using Novel YOLOv7-X Object
    Detector Conference paper First Online: 06 October 2023 pp 303–314 Cite this conference
    paper Access provided by University of Nebraska-Lincoln Download book PDF Download
    book EPUB Intelligent Systems (ICMIB 2023) G. V. S. Narayana, Sanjay K. Kuanar
    & Punyaban Patel  Part of the book series: Lecture Notes in Networks and Systems
    ((LNNS,volume 728)) Included in the following conference series: International
    Conference on Machine Learning, IoT and Big Data 135 Accesses Abstract A weed
    is a wild, undesired plant that grows naturally along with the desired crop. For
    their growth, they compete with the main crop for various resources like space,
    sunlight, irrigation, nutrients, etc. This leads to an enormous loss in the yield
    of the main crop and hence needs to be selectively controlled. Human intervention
    in the process of identification of weeds and subsequently its removal is extremely
    tedious and time-consuming too. Achieving the desired level of accuracy and preciseness
    in the manual weed identification process is impracticable. In recent years, researchers
    have proposed computer vision-based methods for automatic weed identification
    in precision agriculture. In this paper, we have used YOLOv7-X for automatically
    detecting weeds in cotton production systems. YOLOv7-X is a relatively new addition
    to the You Only Look Once (YOLO) family of fast and accurate algorithms. The benchmark
    dataset used for the purpose of validating our results is CottonWeedid-15. This
    dataset is customized with annotations suitable for YOLOv7-X by using the roboflow
    tool. The experimental study demonstrates that the YOLOv7-X model’s mean average
    precision (mAP@.5) can attain 96.6 %. The average precision and average recall
    of the model were 0.914 and 0.953 respectively. This model can also be used to
    classify several weeds in various crops. Keywords YOLO Weed detection Artificial
    intelligence Deep learning Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others Weeds Detection
    Using Mask R-CNN and Yolov5 Chapter © 2024 Pest Detection Using YOLO V7 Model
    Chapter © 2024 Crop and Weed Detection From Images Using YOLOv5 Family Chapter
    © 2023 1 Introduction A weed is an unwanted plant growing wildly along with the
    desired crop. The impact of weeds on agricultural output is significant. Weeds
    are the root cause of problematic insects and agricultural illnesses. They also
    carry various viruses and insects that destroy the desired crop plants. Weeds
    and crops compete for various resources like space, sunshine, irrigation, and
    nutrients. Just like insects and pests, weeds also harm crop cultivation and can
    have a greater impact on crop output reduction. India has the second-largest agricultural
    land area in the world, and agriculture provides a living for almost 60% of rural
    Indian people [1]. In the majority of the country, cotton is a Kharif crop. One
    of India’s most significant fiber and commercial crops is cotton. The nation’s
    cotton production has increased significantly quantitatively throughout the years.
    India, one of the top cotton producers in the world, produces about 21% of the
    cotton used worldwide. The current production per kg per hectare, 445 kg/ha, is
    still less than the global average yield of 765 kg/ha [22]. Although the yield
    per hectare is still lower than the global average of 765 kg per hectare, significant
    adjustments being made to the country’s cotton farming practices have the potential
    to bring the current productivity level closer to the global average in the near
    future [22]. The effectiveness of weed control is very crucial to enhancing cotton
    production in the country. The first six to eight weeks after planting are crucial
    for controlling weeds because during this time weeds for nutrients and water aggressively
    compete with the crop. The percentage of loss in crop production varies based
    on the type of weeds. However, zero cultivation loss from weeds is quite uncommon.
    Weed detection by using an appropriate mechanism is the first fundamental need
    for effective control. Farmers tend to use herbicides compared to manual weeding
    to cut down the expenditure. The use of continuous herbicides can cause an impact
    on the environment. Automation of weed detection has the potential to improve
    system accuracy and calculated density. To identify weeds using automation, there
    were primarily three methods used: Biological morphology, Spectral characteristics,
    and Visual texture. Several techniques existed for weed identification and precise
    application of herbicides such as weed detection from photos using machine vision,
    Global Positioning System (GPS)-guided patch application, Herbicide applicator
    with a variable rate (VRA), applying herbicide with drones (Unmanned Aerial Vehicle-UAVs),
    Online spraying system with machine vision, Robots and weeding machines using
    sensor-based machine vision, weeding in a microwave, and weeders using a laser
    thermal beam. Few studies are available on the detection of multi-class weeds
    that significantly affect cotton fields. A comprehensive benchmark of YOLO detectors
    on multi-class weed detection would be extremely helpful to the research community
    given the current state of YOLO detector advances. In order to discover multi-class
    weeds in the cotton production industry, this research employs a performance evaluation
    of the innovative YOLOv7, which is our motivation for this research. The next
    sections of the paper are structured as follows: Sect. 2 presents current techniques
    for weed detection, Sect. 3 describes the methodology, used dataset, and metrics
    for performance assessment, and Sect. 4 discusses the result of the YOLOv7-X model.
    In the end, we presented the conclusion in Sect. 5. 2 Literature Review Currently,
    computer vision technology is applied to various smart agriculture activities,
    including the detection of plant diseases, crop productivity prediction, taxonomic
    identification, weed detection, and irrigation and soil conservation [2,3,4].
    For future standardized and consistent weed management that makes use of a variety
    of control tactics, computer vision technologies are crucial [5]. The effectiveness
    of computer vision algorithms depends on large-scale tagged image data-set and
    it boosts the performance of algorithms [6]. Good datasets for weed recognition
    should adequately capture essential weed species, environmental factors (such
    as temperature, illumination conditions, and soil composition), and the development
    of morphological or physiological differences. Preparing such databases is time-consuming,
    expensive, and requires domain expertise in weed detection. Recent research has
    been done to create image databases for weed control [7], e.g., DeepWeeds [8],
    Early crop weed data-set [9], CottonWeedID15 [10], the Eden Library [11], and
    Weed-AI. To the best of our knowledge, only CottonWeedID15 [10] was developed
    for the identification of weeds particular to cotton-producing systems. This dataset
    only has image-level annotations, making it unsuitable for weed recognition tasks
    that need bounding box annotations for weed instances in the images. Applications
    for YOLO object detectors in weed detection are numerous. YOLOv3-tiny (a condensed
    version) was used by Gao et al. [12] to identify Calystegia sepium in Beta vulgaris
    fields and achieved mean average precision (mAP) of 0.829. In Sharpe et al. [13]
    on Eleusine indica detection, YOLOv3-tiny attained the F1-ratings of 0.85 in the
    field of Fragaria and 0.65 in the field of tomato. A total mAP of 54.3% was calculated
    by Ahmad et al. [14] adopting YOLOv3 to identify four weed categories that are
    prevalent in the United States. Li et al. [15] used a publicly available dataset
    (Sudars et al. [16]) by YOLOX to examine eight object detectors for weed identification
    and achieved the highest mAP of 79.6%. (Ge et al. [17]). With the exception of
    Ahmad et al., all studies only looked at a single weed class, but they demonstrate
    the effectiveness of YOLO detectors. Chen et al. [18], this paper’s research focuses
    on Deep Transfer Learning(DTL) techniques. In this study, they built a comprehensive
    benchmark for the weed identification task under consideration and examined 27
    cutting-edge deep learning techniques using transfer learning. DTL required only
    a minimal amount of training time across models to obtain excellent classification
    precision of F1 scores above 95%. ResNet101 had the highest F1 score (99.1%),
    while 14 out of the 27 models had F1 values that were higher than 98.0%. Thuan
    D [19], in this publication, research is provided that emphasizes the sustainable
    development of crop cultivation can be improved by using intelligent weeding systems
    to carry out plant-specific procedures. These systems must be able to accurately
    identify and classify weeds in order to prevent unintentional spraying that could
    hamper the nearest species. Among the standard challenges in computer vision is
    object detection. The object in the photograph is classified as well as localized.
    The approaches employed in decades past to solve this issue were split into two
    phases: 1. Detects various portions of the image using sliding windows of multiple
    lengths and 2. Apply the classification issue to ascertain the items’ class by
    using it. For the first time ever, researcher Joseph Redmon and colleagues developed
    the YOLO method in 2015. An object identification system completes all necessary
    steps to recognize an object using a single computational model (the term You
    Only Look Once). The YOLO algorithms were created on the PyTorch framework, and
    a month following the release of YOLOv4, researcher Glenn Jocher, and his Ultralytics
    Limited Liability Company (LLC) research division published YOLOv5 with a couple
    of modifications and enhancements. Olanivi et al. [20], the research presented
    in this paper focuses on the vision-based community that developed the generative
    adversarial network (GAN) in 2014, and it offers a variety of methods and effects
    that can produce highly accurate samples and generate accurate data representations.
    However, it might be challenging to find large-scale, unbiased, and tarmac image
    data–sets to support the creation of sophisticated, high-performance classifiers.
    Myloans et al. [11], the work in this paper is focusing on precision agriculture
    using deep learning models to boost computer vision and Artificial intelligence
    (AI)-based applications. These methods, however, require large amounts of data,
    which are typically in limited supply in agriculture, in order to achieve outstanding
    performances. The Eden Library, a platform for addressing the current gap in open-access
    agriculture databases that include regional and aerial images, is presented in
    this article. The offered datasets are believed to be sustained and enhanced over
    time rather than simply remaining as a static study product addressing only particular
    species, life-cycle phases, and environments, making this procedure pertinent.
    The Eden Library has information on 30 ailments, 15 distinct crops, and 9 weeds.
    3 Proposed Methodology Here, we elaborate on the methodology for multi-class weed
    identification by using the YOLOv7-X model, used dataset (CottonWeedID15), and
    metrics for performance assessment. 3.1 Methodology Figure 1 depicts our proposed
    methodology for identifying weeds. The input image annotations do not follow the
    format of YOLO detectors. Consequently, the input needs to be translated into
    the format of object detectors by using roboflow. After converting to a new format,
    the dataset was randomly split into the train, validation, and test subsets using
    the partition ratios of 70%-20%-10%, respectively. To meet the requirements of
    YOLO deep learning architectures, the actual pictures were downsized to 640\\(\\,\\times
    \\,\\)640 pixels for training inputs. Fig. 1 The pipeline for YOLO feature extraction
    algorithms to detect weeds Full size image 3.2 CottonWeedID15 In this experiment,
    we used the CottonWeedID15 dataset, which comprises 5187 color photos of 15 weed
    classes taken in cotton crops, mostly in North Carolina, in natural lighting conditions.
    The current sample for weed detection is more than ten times larger than the one
    for four weed species in terms of the number of images (Ahmad, 2021). Figure 2
    represents the number of images in each class of the CottonWeedID15 dataset. Fig.
    2 pie chart showing the diversity of the cotton weed dataset Full size image 3.3
    Metrics for Performance Assessment In computer vision, Intersection over Union
    (IoU) and mean average precision (mAP) are popular evaluation metrics used for
    object detection. 3.3.1 Intersection over Union We calculate the overlap between
    the predicted bounding box and the actual bounding box for every bounding box.
    This is determined via intersection over union (IoU) [21] and represented in Eq.
    1 3.3.2 Mean Average Precision (mAP) The mAP is a well-known assessment measure
    in computer vision that is employed for object recognition tasks including localization
    and classification. By applying bounding box coordinates, localization pinpoints
    an instance’s position, and classification identifies the particular weed class
    that it belongs to. The ratio of true positives to all positive predictions is
    known as Precision. Equation 2 represents the Precision Formula. The recall is
    calculated as the ratio between the number of Positive samples correctly classified
    as Positive to the total number of Positive samples. Equation 3 represents the
    Recall formula. $$\\begin{aligned} IoU = \\frac{Area of Overlap}{Area of Union}
    \\end{aligned}$$ (1) $$\\begin{aligned} P = \\frac{TP}{TP + FP} \\end{aligned}$$
    (2) $$\\begin{aligned} R = \\frac{TP}{TP + FN} \\end{aligned}$$ (3) where the
    following abbreviations stand for the number of true positive, false positive,
    and false negative predictions at the specified IoU cutoff, respectively: TP,
    FP, and FN. Next, the below finite sum is used to compute the Average Precision
    score, which represents the area covered by the curve. By averaging the AP results
    throughout all object classes, the mean Average Precision is calculated. Equation
    4 represents AP and Eq. 5 represents mAP respectively. $$\\begin{aligned} AP =
    \\sum _{n} (R_{n}-R_{n-1})P_{n} \\end{aligned}$$ (4) $$\\begin{aligned} mAP =\\frac{1}{N}
    \\sum ^{N}_{i=1} AP_{i} \\end{aligned}$$ (5) 4 Results and Discussion Here, we
    elaborate on the results of the YOLOv7-X model, Analysis of the Confusion Matrix
    and F1 Curve, Precision and Recall, and PR curve. 4.1 Results of YOLOv7-X Model
    We used the google colab to train the YOLOv7-X model for 200 epochs. We are able
    to run 40 to 50 epochs in 8 h. To run 200 epochs, it took 5 days by resuming the
    code. After completion of all epochs, we got the results for various parameters
    like Precision, Recall, Classification status, etc. The complete result is shown
    in Fig. 3. Table 1 shows the number of labels, Precision, Recall, mAP@.5, and
    mAP@.5:.95 for each class. We got the highest precision for Ragweed, and SqurredAnoda
    classes and the lowest precision for the Prickly Sida class. We got the highest
    recall for Crabgrass, and Swinecress classes and the lowest recall for the Nutsedge
    class. Figure 4 represents qualitative weed detection by using bounding boxes.
    Table 1 Model’s Accuracy for the Weed Classes. Full size table Fig. 3 Results
    of YOLOv7-X Full size image Fig. 4 Weed detection through bounding boxes Full
    size image 4.2 Analysis of the Confusion Matrix and F1 Score The Confusion Matrix
    displays both the actual labels and the expected labels. We have generated a 15*15
    confusion matrix representing each class of weed in our dataset. As per the confusion
    matrix, the highest classification rate (100%) for Crabgrass, Sicklepod, SqurredAnoda,
    and Swinecress classes and the lowest (84%) for the Nutsedge class. The confusion
    matrix’s diagonal elements indicate how closely the predicted labels correspond
    with the actual labels. F1 Score combines the results of precision and recall
    into a single metric by considering their harmonic mean. The F1 score varies from
    0 to 1, with 0 representing low accuracy and 1 representing high accuracy prediction.
    The confusion matrix and F1 Score are illustrated in Fig. 4. Fig. 5 Confusion
    Matrix and F1 Curve Full size image 4.3 Precision, Recall and PR Curves In the
    P curve, confidence is shown along the x-axis, and precision is shown along the
    y-axis. We got the model’s precision as 1.00 at a confidence of 0.951. In the
    R curve, confidence is shown on the x-axis, and recall is shown on the y-axis.
    We got the model’s recall as 0.98 at a confidence of 0.00. By adjusting the IoU
    threshold, a P-R slope or pattern is produced. The PR curve displays the recall
    on the x-axis and precision on the y-axis. The area under the PR curve is one
    square unit. According to the PR curve, the classes Crabgrass, Ragweed, and Square-Anoda
    have the highest Area Under Curves (AUC), whereas Eclipta has the lowest AUC.
    Figure 5 represents the P Curve and R Curve and Fig. 6 represents the PR Curve
    (Fig. 7). Fig. 6 Precision and Recall Curve Full size image Fig. 7 PR Curve Full
    size image 5 Conclusion For site-specific weed management, accurate localization
    of weeds by computer vision systems depends on weed detection and categorization.
    The creation of supervised deep learning on the weed image database and the curation
    of comprehensive, properly specific data on weeds are crucial parts of efficient
    weed identification. The dataset for weed detection that is currently most exhaustive
    and useful to the agriculture development systems is provided in this research.
    It consists of 5187 images of 15 weed classes. The mAP@0.5 ratings for the YOLO
    detection algorithms varied from 88.14% for YOLOv3-tiny to 95.22% for YOLOv4.We
    utilized the YOLOv7-X model to identify the multi-weed class in this paper. CottonWeedID15
    is the benchmark dataset used for object detection. This YOLOv7-X model gives
    an accuracy of 0.966 at map@0.50. In the future, our current work can be further
    extended to incorporate multi-weed detection with intra-similarity between weeds
    and inter-similarity between weed & crop. References May P, Ehrlich H-C, Steinke
    T (2006) ZIB structure prediction pipeline: composing a complex biological workflow
    through web services. In: Nagel WE, Walter WV, Lehner W (eds) Euro-Par 2006, vol
    4128. LNCS. Springer, Heidelberg, pp 1148–1158. https://doi.org/10.1007/11823285_121
    Chapter   Google Scholar   Tian H, Wang T, Liu Y, Qiao X, Li Y (2020) Computer
    vision technology in agricultural automation-A review. Inf Proc Agric 7(1):1–19
    Google Scholar   Mavridou E, Vrochidou E, Papakostas GA, Pachidis T, Kaburlasos
    VG (2019) Machine vision systems in precision agriculture for crop farming. J.
    Imaging 5(12):89 Article   Google Scholar   Zhang S, Huang W, Wang Z (2021) Combing
    modified Grabcut, K-means clustering and sparse representation classification
    for weed recognition in wheat field. Neurocomputing 452:665–674 Article   Google
    Scholar   Young SL (2018) Beyond precision weed control: a model for true integration.
    Weed Technol 32(1):7–10 Article   Google Scholar   Sun C, Shrivastava A, Singh
    S, Gupta A (2017) Revisiting unreasonable effectiveness of data in deep learning
    era. In: Proceedings of the IEEE international conference on computer vision,
    pp 843-852 Google Scholar   Lu Y, Young S (2020) A survey of public datasets for
    computer vision tasks in precision agriculture. Comput Electron Agric 178:105760
    Article   Google Scholar   Olsen A et al (2019) DeepWeeds: a multiclass weed species
    image dataset for deep learning. Sci Rep 9(1):1–12 Article   Google Scholar   Espejo-Garcia
    B, Mylonas N, Athanasakos L, Fountas S, Vasilakoglou I (2020) Towards weeds identification
    assistance through transfer learning. Comput Electron Agric 171:105306 Article   Google
    Scholar   Chen D, Yuzhen L, Li Z, Young S (2022) Performance evaluation of deep
    transfer learning on multi-class identification of common weed species in cotton
    production systems. Comput Electron Agric 198:107091 Article   Google Scholar   Mylonas
    N, Malounas I, Mouseti S, Vali E, Espejo-Garcia B, Fountas S (2022) Eden library:
    a long-term database for storing agricultural multi-sensor datasets from UAV and
    proximal platforms. Smart Agric Technol 2:100028 Article   Google Scholar   Gao
    J, French AP, Pound MP, He Y, Pridmore TP, Pieters JG (2020) Deep convolutional
    neural networks for image-based Convolvulus Sepium detection in sugar beet fields.
    Plant Methods 16(1):1–12 Article   Google Scholar   Sharpe SM, Schumann AW, Boyd
    NS (2020) Goosegrass detection in strawberry and tomato using a convolutional
    neural network. Sci Rep 10(1):1–8 Article   Google Scholar   Ahmad A, Saraswat
    D, Aggarwal V, Etienne A, Hancock B (2021) Performance of deep learning models
    for classifying and detecting common weeds in corn and soybean production systems.
    Comput Electron Agric 184:106081 Article   Google Scholar   Li Y, Guo Z, Shuang
    F, Zhang M, Li X (2022) Key technologies of machine vision for weeding robots:
    a review and benchmark. Comput Electron Agric 196:106880 Article   Google Scholar   Sudars
    K, Jasko J, Namatevs I, Ozola L, Badaukis N (2020) Dataset of annotated food crops
    and weed images for robotic computer vision control. Data Brief 31:105833 Article   Google
    Scholar   Ge Z, Liu S, Wang F, Li Z, Sun J (2021) YOLOX: exceeding yolo series
    in 2021. arXiv preprint: arXiv:2107.08430 Chen Q, Wang Y, Yang T, Zhang X, Cheng
    J, Sun J (2021) You only look one-level feature. In: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, pp 13039-13048 Google Scholar   Thuan,
    D (2021) Evolution of YOLO algorithm and YOLOv5: the state-of-the-art object detention
    algorithm Google Scholar   Olaniyi E, Chen D, Lu Y, Huang Y (2022) Generative
    adversarial networks for image augmentation in agriculture: a systematic review.
    arXiv preprint: arXiv:2204.04707 Padilla R, Netto SL, Da Silva EA (2020) A survey
    on performance metrics for object-detection algorithms. In: 2020 international
    conference on systems, signals and image processing (IWSSIP), pp 237-242. IEEE
    Google Scholar   MacCarthy DS et al (2021) Potential impacts of agricultural intensification
    and climate change on the livelihoods of farmers in Nioro, Senegal, West Africa
    Google Scholar   Download references Author information Authors and Affiliations
    GIET University, Gunupur, Odisha, India G. V. S. Narayana & Sanjay K. Kuanar CMR
    Technical Campus,Kandlakoya(V), Medchal (M), Hyderabad, Telangana State, India
    Punyaban Patel Corresponding author Correspondence to G. V. S. Narayana . Editor
    information Editors and Affiliations School of Computer and Information Sciences,
    University of Hyderabad, Hyderabad, Telangana, India Siba K. Udgata Department
    of Computer Science and Engineering, Indira Gandhi Institute of Technology, Dhenkanal,
    Odisha, India Srinivas Sethi Department of Computer Science, University of Eastern
    Finland, Kuopio, Finland Xiao-Zhi Gao Rights and permissions Reprints and permissions
    Copyright information © 2024 The Author(s), under exclusive license to Springer
    Nature Singapore Pte Ltd. About this paper Cite this paper Narayana, G.V.S., Kuanar,
    S.K., Patel, P. (2024). Weed Detection in Cotton Production Systems Using Novel
    YOLOv7-X Object Detector. In: Udgata, S.K., Sethi, S., Gao, XZ. (eds) Intelligent
    Systems. ICMIB 2023. Lecture Notes in Networks and Systems, vol 728. Springer,
    Singapore. https://doi.org/10.1007/978-981-99-3932-9_27 Download citation .RIS.ENW.BIB
    DOI https://doi.org/10.1007/978-981-99-3932-9_27 Published 06 October 2023 Publisher
    Name Springer, Singapore Print ISBN 978-981-99-3931-2 Online ISBN 978-981-99-3932-9
    eBook Packages Engineering Engineering (R0) Share this paper Anyone you share
    the following link with will be able to read this content: Get shareable link
    Provided by the Springer Nature SharedIt content-sharing initiative Publish with
    us Policies and ethics Sections Figures References Abstract Introduction Literature
    Review Proposed Methodology Results and Discussion Conclusion References Author
    information Editor information Rights and permissions Copyright information About
    this paper Publish with us Discover content Journals A-Z Books A-Z Publish with
    us Publish your research Open access publishing Products and services Our products
    Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio
    BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state
    privacy rights Accessibility statement Terms and conditions Privacy policy Help
    and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University
    of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Networks and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Weed Detection in Cotton Production Systems Using Novel YOLOv7-X Object Detector
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chamara N.
  - Bai G.
  - Ge Y.
  citation_count: '1'
  description: 'Precision Agriculture (PA) promises to meet the future demands for
    food, feed, fiber, and fuel while keeping their production sustainable and environmentally
    friendly. PA relies heavily on sensing technologies to inform site-specific decision
    supports for planting, irrigation, fertilization, spraying, and harvesting. Traditional
    point-based sensors enjoy small data sizes but are limited in their capacity to
    measure plant and canopy parameters. On the other hand, imaging sensors can be
    powerful in measuring a wide range of these parameters, especially when coupled
    with Artificial Intelligence. The challenge, however, is the lack of computing,
    electric power, and connectivity infrastructure in agricultural fields, preventing
    the full utilization of imaging sensors. This paper reported AICropCAM, a field-deployable
    imaging framework that integrated edge image processing, Internet of Things (IoT),
    and LoRaWAN for low-power, long-range communication. The core component of AICropCAM
    is a stack of four Deep Convolutional Neural Networks (DCNN) models running sequentially:
    CropClassiNet for crop type classification, CanopySegNet for canopy cover quantification,
    PlantCountNet for plant and weed counting, and InsectNet for insect identification.
    These DCNN models were trained and tested with >43,000 field crop images collected
    offline. AICropCAM was embodied on a distributed wireless sensor network with
    its sensor node consisting of an RGB camera for image acquisition, a Raspberry
    Pi 4B single-board computer for edge image processing, and an Arduino MKR1310
    for LoRa communication and power management. Our testing showed that the time
    to run the DCNN models ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet,
    and power consumption ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet.
    The classification model CropClassiNet reported 94.5 % accuracy, and the segmentation
    model CanopySegNet reported 92.83 % accuracy. The two object detection models
    PlantCountNet and InsectNet reported mean average precision of 0.69 and 0.02 for
    the test images. Predictions from the DCNN models were transmitted to the ThingSpeak
    IoT platform for visualization and analytics. We concluded that AICropCAM successfully
    implemented image processing on the edge, drastically reduced the amount of data
    being transmitted, and could satisfy the real-time need for decision-making in
    PA. AICropCAM can be deployed on moving platforms such as center pivots or drones
    to increase its spatial coverage and resolution to support crop monitoring and
    field operations.'
  doi: 10.1016/j.compag.2023.108420
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusion and future perspectives Funding
    CRediT authorship contribution statement Declaration of Competing Interest Acknowledgements
    Data availability References Show full outline Cited by (1) Figures (12) Show
    6 more figures Tables (7) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show
    all tables Computers and Electronics in Agriculture Volume 215, December 2023,
    108420 AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge Author links open overlay
    panel Nipuna Chamara a, Geng Bai a, Yufeng Ge a b Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.compag.2023.108420 Get rights and content Under
    a Creative Commons license open access Highlights • We developed AICropCAM, an
    edge-computing enabled camera system for crop monitoring. • It integrated Raspberry
    Pi and Arduino for image processing and LoRa communication. • It ran a stack of
    four deep neural networks to characterize multiple plant/canopy parameters. •
    We quantified the power consumption and speed of the system for edge image-processing.
    • AICropCAM is a next-generation enabling technology for real-time, low-latency
    Ag applications. Abstract Precision Agriculture (PA) promises to meet the future
    demands for food, feed, fiber, and fuel while keeping their production sustainable
    and environmentally friendly. PA relies heavily on sensing technologies to inform
    site-specific decision supports for planting, irrigation, fertilization, spraying,
    and harvesting. Traditional point-based sensors enjoy small data sizes but are
    limited in their capacity to measure plant and canopy parameters. On the other
    hand, imaging sensors can be powerful in measuring a wide range of these parameters,
    especially when coupled with Artificial Intelligence. The challenge, however,
    is the lack of computing, electric power, and connectivity infrastructure in agricultural
    fields, preventing the full utilization of imaging sensors. This paper reported
    AICropCAM, a field-deployable imaging framework that integrated edge image processing,
    Internet of Things (IoT), and LoRaWAN for low-power, long-range communication.
    The core component of AICropCAM is a stack of four Deep Convolutional Neural Networks
    (DCNN) models running sequentially: CropClassiNet for crop type classification,
    CanopySegNet for canopy cover quantification, PlantCountNet for plant and weed
    counting, and InsectNet for insect identification. These DCNN models were trained
    and tested with >43,000 field crop images collected offline. AICropCAM was embodied
    on a distributed wireless sensor network with its sensor node consisting of an
    RGB camera for image acquisition, a Raspberry Pi 4B single-board computer for
    edge image processing, and an Arduino MKR1310 for LoRa communication and power
    management. Our testing showed that the time to run the DCNN models ranged from
    0.20 s for InsectNet to 20.20 s for CanopySegNet, and power consumption ranged
    from 3.68 W for InsectNet to 5.83 W for CanopySegNet. The classification model
    CropClassiNet reported 94.5 % accuracy, and the segmentation model CanopySegNet
    reported 92.83 % accuracy. The two object detection models PlantCountNet and InsectNet
    reported mean average precision of 0.69 and 0.02 for the test images. Predictions
    from the DCNN models were transmitted to the ThingSpeak IoT platform for visualization
    and analytics. We concluded that AICropCAM successfully implemented image processing
    on the edge, drastically reduced the amount of data being transmitted, and could
    satisfy the real-time need for decision-making in PA. AICropCAM can be deployed
    on moving platforms such as center pivots or drones to increase its spatial coverage
    and resolution to support crop monitoring and field operations. Graphical abstract
    Download : Download high-res image (227KB) Download : Download full-size image
    Previous article in issue Next article in issue Keywords Artificial intelligenceComputer
    visionEdge computingInternet of thingsLoRaWANPrecision agriculture 1. Introduction
    The demands for food, feed, fiber, and fuel increase rapidly due to the fast expansion
    of the global population, income growth, technological advancement, and transport
    and logistics improvements (van Dijk et al., 2021). Precision agriculture (PA),
    which seeks to apply the right amount of inputs (fertilizers, irrigation water,
    pesticides, and other chemicals) in the right location at the right time, is essential
    to meet the requirements of future global food production, as well as environmental
    sustainability and climate resilience. PA is predicated on accurate sensor measurements,
    timely and sound decision-making, and automated actuators. The backbone of PA
    is the Internet of Things (IoT) technology that automates data collection, data
    analytics, data presentation, control, and efficient data communication (Chamara
    et al., 2022). Imaging sensors or digital cameras are essential for PA as they
    can capture more information than traditional scalar or vector sensors. Images
    can capture crop phenology for precise decision-making (Taylor and Browning, 2022,
    Tian et al., 2020). Cyclic events such as vegetative growth, flowering, leaf count
    and color change, maturation, and senescence are studied in crop phenology, which
    is essential to PA as it determines the management inputs required by crops. Moreover,
    images have rich information on the scene that allows for pest pressure evaluation.
    At present, a limited number of sensors are available for pest identification
    and pest pressure estimation. Among them, imaging sensors provide the most promising
    solution. Conventional (handcrafted feature extraction) and Artificial Intelligence
    (AI)-based image processing are the two branches of image processing. Traditional
    approaches extract image features defined by shape, texture, and color (Anubha
    et al., 2019, Yuan et al., 2019). The AI-based methods use Convolutional Neural
    Networks (CNN) to extract features from images (Luis et al., 2020). CNN models
    with multiple hidden layers for feature extraction and learning are considered
    Deep Convolutional Neural Networks (DCNN) (LeCun et al., 1998). Conventional imaging
    platforms in PA store images locally using onboard storage memories. Post processing
    refers to the processing of images stored at the central data storage in batches
    at a later time to extract useful information (Aasen et al., 2020). Imaging platforms
    that can access the internet through a stable connection with high bandwidth can
    automatically upload images to Cloud data storage. The vast majority of farmlands
    worldwide are in rural and remote areas with poor access to electric power and
    internet connectivity. This represents a big challenge for camera systems deployed
    in rural farmlands for high-speed image processing, data transmission, and low-latency
    decision-making (Richardson, 2019). Post-processing of crop images has been used
    for the estimation of leaf area index (Aasen et al., 2020), growth rate (Sakamoto
    et al., 2012), leaf chlorophyll and nitrogen content (Wang et al., 2014), fruit
    counts (Wang et al., 2014), and plant height (Sritarapipat et al., 2014). Further
    post-image processing allows for the assessment of biotic stress, such as pest
    density (Barbedo, 2014; Park et al., 2007) and weed pressure (Wang et al., 2019),
    as well as abiotic stress, such as nutrient deficiency (Ghorai et al., 2021).
    Richardson (2019) suggested that deep learning-based methods have the potential
    to facilitate the extraction of more sophisticated phenological data from both
    new and previously archived camera imagery compared to conventional image processing.
    Semantic segmentation-based canopy coverage (CC) estimation (Chamara et al., 2021;
    Liang et al., 2023), image classification-based crop identification (Anubha et
    al., 2019), disease identification (Sharma et al., 2020), growth stage prediction
    (Yasrab et al., 2021) and object detection-based plant feature identification
    (A. Wang et al., 2019) are examples of DCNN applications in agriculture. Conventional
    image processing requires less computational power and less energy, but they are
    limited in adaption to new scenarios, while deep learning requires high computational
    power and consumes more energy. DCNN models require large memory due to the large
    number of parameters these models hold. Therefore, it is not easy to implement
    these models practically in embedded systems that have less memory and computation
    power. These models also require a large amount of data to train to predict with
    high accuracy. Therefore, it is resource intensive. Edge image processing is the
    image processing done on image-capturing devices. The main advantage of edge image
    computing is that it lowers the high throughput data transmission requirement
    over a wireless IoT-enabled imaging network (Cao et al., 2020). Wang et al. (2022a)
    demonstrated the capability of identifying potted flowers with precision above
    89 % in real-time in a Jetson TX 2 computing module based on a DCNN algorithm.
    These authors suggested that a cloud-edge collaborative framework could achieve
    real-time and automatic learning for the DCNN model they have developed. Wang
    et al. (2022b) proposed a real-time weed detection model run on Jetson AGX Xavier
    for field robots. The authors proved it was possible to do real-time weed detection
    with a precision above 90 % yet required expensive hardware. Wang et al. (2022a)
    reviewed Raspberry Pi single-board computer-based real-time image processing applications.
    They concluded that Raspberry Pi (Datasheet Raspberry Pi Model B, 2019) is a cost-effective
    edge computing unit that could potentially be used as an edge image processing
    unit, and the capability of integrating it with IoT was also discussed. Zualkernan
    et al. (2022) demonstrated an edge image processing platform for the classification
    of animals and transmitting the identified animal and time of identification via
    LoRa for a camera trap. Past literature on IoT and image processing applications
    in agriculture has highlighted a research gap in edge image processing with IoT-enabled
    crop monitoring cameras. In-field crop cameras are expected to make real-time
    crop management decisions based on real-time image processing; however, poor internet
    connectivity in agricultural fields severely limits their capability. To address
    this gap, we have developed a novel imaging platform named AICropCAM that extracts
    plant and crop canopy level parameters through DCNN and uploads them to the Cloud
    via low-power, low-throughput communication protocols. We also demonstrated AICropCAM
    on an IoT-enable wireless sensor network in corn and soybean fields. A technology
    that addresses image processing at the lowest level (edge) and transmits only
    useful information can revolutionize real-time decision-making in PA. Therefore,
    the main objective of this paper is to demonstrate AICropCAM to perform edge image
    processing and low-throughput, low-power, and long-range data transmission through
    IoT technology. In this novel AICropCAM platform, multiple DCNN image processing
    algorithms run in series to extract plant-level and canopy-level features in an
    embedded system. Image classification, object detection with classification, and
    image segmentation are the three major applications of DCNN image processing,
    and all three are included in AICropCAM to demonstrate the capabilities of DCNN
    for image processing in PA. AICropCAM has trained models for canopy segmentation,
    crop classification, plant growth stage identification, plant counting, weed counting,
    and plant type identification. All the protocols that transmit data from AICropCAM
    to the Cloud were custom designed. AICropCAM sends the generated data to a cloud
    platform for logging, visualization, and analysis. Furthermore, this paper explains
    the DCNN model training process, model performance, and test results. We reported
    the model training comprehensively because it was essential for AICropCAM development.
    2. Materials and methods Essential activities in this research were data/image
    collection and preprocessing, hardware design for AICropCAM, software design for
    data transmission between the edge and the Cloud, deep learning model design,
    and model training and optimization (Fig. 1). AICropCAM was implemented in a corn
    and soybean field at the field phenotyping facility in Mead, Nebraska, USA (Bai
    et al., 2019). We demonstrated the training of the following DCNNs: CropClassiNet
    for classifying images based on image quality and crop type, CanopySegNet for
    segmenting crop canopy from the background, PlantCountNet for classifying and
    counting soybean and weed plants, and InsectNet for identifying insects and counting
    them. Download : Download high-res image (412KB) Download : Download full-size
    image Fig. 1. Steps of edge image processing program deployment on the embedded
    system (edge devices). 2.1. Image collection, annotation, preprocessing, and augmentation
    Image collection for DCNN model training occurred in four growing seasons using
    three different types of cameras: (i) commercially available Meidas SL122 trail
    cameras in 2019 (Meidas Trail Cameras, 2022), (ii) OV5642 imaging sensors with
    ArduCAM camera shields in 2020, and (iii) Raspberry Pi Camera Module V2 with Raspberry
    Pi Zero in 2021 and 2022 (Chamara, 2021). All the cameras were mounted on the
    bars horizontally extended and fixed on stationary poles erected vertically in
    the fields, as shown in Fig. 2A. The distance between the crop canopy and the
    cameras was maintained between 0.5 and 1.5 m throughout the growing seasons. Images
    used for training the InsectNet were also captured with smartphones as we could
    not collect enough images with insects from the three types of cameras mentioned
    above. Download : Download high-res image (338KB) Download : Download full-size
    image Fig. 2. Left: An Illustration of how AICropCAM was set up in the field for
    image collection. In addition to the camera, other components such as the solar
    panel and data logger were also shown. Right: A close-up view of AICropCAM and
    its hardware components. All three standard image annotation techniques in deep
    learning model training were utilized: (1) folder labeling for the image classification
    models, (2) pixel-level annotation for the semantic segmentation model, and (3)
    bounding boxes for object detection models. Images belonging to the same class
    were grouped into a single folder, and five distinct classes (or folders) were
    created: rejected, corn, soybean, grass, and night. Separating the crop canopy
    from the soil was done with pixel-level annotation and semantic segmentation.
    Bounding boxes, the smallest rectangle around an object, were drawn for corn plants,
    soybean plants, weed plants, and insects. Table 1 explains each type of annotation
    used in the model training. Table 1. Annotation criteria used to generate labels
    from the images to train and test the four deep convolutional neural network models
    in AICropCAM. Labeling Type Class Description Image classification (CropClassiNet)
    Rejected Images were labeled as rejected due to multiple reasons: blurred images
    caused by water droplets on the lens; the cameras turned away from the targeted
    crop; crops growing up to the camera blocking the view or capturing only a few
    leaves; people present in the images; lens covered with different stuff; and the
    camera was not installed in the field. Corn Images entirely covered by corn plants
    at different growth stages. Soybean Images entirely covered by soybean plants
    at different growth stages. Grass/Weed Images only comprise grass/weed plants
    at different growth stages. Night Images captured under low lighting conditions.
    Most of the cameras were not programmed to stop collecting images under low light.
    Crop canopy and background (CanopySegNet) Canopy Pixel labeling was done on the
    crop canopy. We used assisted freehand tool and the superpixel option in the MATLAB
    image labeler. Background Pixel labeling was done on the crop canopy. We used
    assisted freehand tool and the superpixel option in the MATLAB image labeler.
    Plant-type (PlantCountNet) Weed Weed present in the image was labeled using bounding
    boxes. It was challenging to locate the weed after the corn or soybean canopy
    was closed. Soybean Soybean plants present in the image were labeled using bounding
    boxes. Insects (InsectNet) Insects During the labeling process, without distinguishing
    insects based on their type, all the insects present in the images were labeled
    using bounding boxes. Image preprocessing is necessary for DCNN model training
    and real-time edge image processing. Differences in the input layer size in different
    DCNN models demand that images be resized before passing through the model for
    training or prediction purposes. High-resolution images improve accuracy but require
    more computational power. For specific applications, labeled datasets were only
    limitedly available. Therefore, image augmentation techniques were used to increase
    the number of image data sets, including scaling, flipping, cropping, rotation,
    color transformation, PCA color augmentation, and noise rejection (Paymode and
    Malode, 2022). Multiple augmentation techniques were used for each model, as detailed
    in Table 2. Additionally, Table 2 provides the numbers of images in training,
    validation, and testing for the four DCNN models. Table 2. DCNN model image allocation
    and image augmentation. Model Number of images Data Augmentation Techniques Total
    Training Validation Test CropClassiNet 43,611 30,528 9,810 3,273 Random rotation,
    random X  and Y reflection CanopySegNet 51 31 10 10 Transformation (random left/right
    reflection and random X/Y translation of ±10 pixels) PlantCountNet 110 88 11 11
    Transformation (same as CanopySegNet) InsectNet 542 326 108 108 Transformation
    (same as CanopySegNet) Our main objective was not to make the most accurate prediction
    for the DCNN models but to demonstrate the concept of implementing edge image
    processing and transmitting the results to the Cloud for decision-making. Therefore,
    we selected a limited number of images for CanopySegNet, PlantCountNet, and InsectNet,
    which were sufficient to train models with a reasonable degree of accuracy. 2.2.
    DCNN model architecture selection, training, evaluation, and deployment on the
    edge device The steps to select model architecture/model backbone weights and
    image input sizes to train the best model for CropClassiNet, CanopySegNet, PlantCountNet,
    and InsectNet are summarized in Fig. 3. Unlike many DCNN applications that prioritize
    higher accuracy, our application focused on finding the balance between accuracy
    and model deployability on the edge device. Download : Download high-res image
    (771KB) Download : Download full-size image Fig. 3. DCNN model selection process
    during the training and testing by attempting different model architectures, model
    backdone weights, and input image sizes. For example, in the development of CropSegNet
    (Segmentation), we selected DeepLabv3+ (Firdaus-Nawi et al., 2018) with weights
    initialized from pre-trained networks of ResNet18 (He et al., 2016), ResNet50,
    Xception, InceptionresnetV2, and MobileNetV2. The input image sizes tested were
    512 × 512 × 3 and 256 × 256 × 3, and training options were kept constant to find
    the best-performing networks, which should also be deployable to Raspberry Pi
    4B. This process identified DeepLabv3 + with ResNet50 as the most suitable model
    for CropSegNet, with an input image size of 512 × 512 × 3. Table 3 summarizes
    the hyperparameter values and training options for the final DCNN models deployed
    to the edge device. (1) (2) (3) (4) (5) (6) (7) Table 3. Hyperparameter values
    and training options for the best models (SGDM - stochastic gradient descent with
    momentum, RMSProp - Root mean square propagation). Training option and the function/Hyperparameters
    Values for CropClassiNet Values for CanopySegNet Values for InsectNet (320 × 320
    × 3) Values for PlantCountNet (320 × 320 × 3) Optimizer SGDM SGDM SGDM RMSProp
    Momentum 0.9 0.9 0.99 NA Initial learning rate 0.001 0.001 0.001 0.001 Learn rate
    schedule Piecewise Piecewise Piecewise Piecewise Learn rate drop period 10 10
    10 10 Learn rate drop factor 0.3 0.3 0.1 0.3 Minibatch size 16 4 16 32 L2Regularization
    NA 0.005 0.005 0.005 Validation frequency 3 3 3 10 Shuffle Every epoch Every epoch
    Every epoch Every epoch Validation patience 4 10 10 10 Max epochs 100 300 1000
    100 Execution environment Multi GPU Multi GPU GPU GPU The performance of the four
    DCNN models was evaluated using the indices calculated from Eq. (1), (2), (3),
    (4), (5), (6), (7). Accuracy, Precision, Recall, F1 score, and Jaccard index were
    used for the classification models CropClassiNet and CropSegNet, whereas IoU and
    mAP (Mean Average Precision) were used for PlantCountNet and InsectNet. Jaccard
    index gives the proportion of correctly predicted labels to the total number of
    labels. Model training was performed on an NVIDIA GeForce GTX 1650 Ti Mobile processor,
    a dedicated mid-range graphics card with 4 GB GDDR6 memory on a Dell XPS 15 9500
    Laptop. The laptop had an Intel Core i7-10750H 10th Gen processor,16 GB DDR4 RAM,
    and 1 TB SSD hard disk. 2.3. Hardware and software of AICropCAM The IoT data transmission
    and edge image processing hardware comprised the following major components: a
    Raspberry Pi 4B single-board computer, an Arduino MKR1310 development board, an
    Arduino MKR Relay Proto Shield, and a Dragino OLG02 outdoor dual channels LoRa
    Gateway (Fig. 4). The 12 V 8Ah battery powered the Raspberry Pi 4B, controlled
    through the relay shield managed by the Arduino MKR1310. A 3.7 V lithium polymer
    battery powered the Arduino MKR1310 board. There are two advantages of having
    a separate Arduino board. First, the Arduino board consumes less power than the
    Raspberry Pi 4B module. It can be switched on and off according to user requirements.
    Second, it allows uninterrupted communication between the edge node and the Cloud
    with low power. Download : Download high-res image (303KB) Download : Download
    full-size image Fig. 4. Hardware overview of AICropCAM and data flow. AICropCAM
    required programming on two hardware platforms. Arduino was programmed using C++
    in Arduino’s Integrated Development Environment. Raspberry Pi imaging and image
    processing program was developed in MATLAB and deployed onto the Raspberry Pi
    4B using the MATLAB Coder and MATLAB Compiler. A python program was designed to
    read the saved data in the Raspberry Pi 4B and serially communicate to the Arduino
    MKR1310. The primary functions of the MRK1310 program were to (1) turn on the
    Raspberry Pi 4B module based on the user-defined time intervals, (2) get the processed
    data, including the results of DCNN model predictions, through serial communication
    from the Raspberry Pi 4B, and (3) transmit the data to the ThingSpeak Cloud channel
    through the LoRa gateway. All the DCNN models were trained using the MATLAB deep
    learning toolbox. In the edge deployment, a MATLAB program runs multiple models
    logically depending on the prediction result of the previous model estimation,
    as shown in Fig. 5. MATLAB coder generated the C and C++ code derived from the
    program we developed to run on the Raspberry Pi. MATLAB Compiler generated the
    standalone application on the Raspberry Pi (The MathWorks, 2022). Download : Download
    high-res image (477KB) Download : Download full-size image Fig. 5. Overall sequential
    image processing and data generation flow chart. Table 4 lists the parameters
    generated by the models in AICropCAM. The abbreviations in Table 4 are fields
    holding data in the program to reduce the complexity of system development and
    maintain a common standard among different platforms. Fig. 6 shows the data generation
    from images. According to Fig. 6, the size of the images were around 2 MB before
    being fed into the image processing pipeline. The output message contains the
    crop type (CT), plant count (PC), weed count (WC), canopy coverage (CC), and pest
    count (PstC). The resulting message is typically less than 100 bytes. This represents
    a substantial reduction of memory size with the output being 0.00005 times the
    size of the original image. Consequently, this message can be transmitted in a
    single message via LoRa as the maximum LoRa packet size is around 256 bytes. Table
    4. List of parameters used to represent information in the images. Parameter Abbreviation
    Represent information Image location LOC Node ID manually entered/Global positioning
    system location coordinates Image orientation IO Accelerometer/Manually feed/Gravity
    switch Image quality/Crop type CT Image classification based on image quality
    and the crop type Plant count/Weed count PC/WC Multiclass object detection/classification
    Crop canopy coverage CC Semantic segmentation Pest count PstC Multiclass object
    detection/classification Download : Download high-res image (2MB) Download : Download
    full-size image Fig. 6. Examples of message generation and data size reduction
    for LoRa transmission. 2.4. Data transmission, visualization, and storage The
    data generated after image processing were saved on the Raspberry Pi 4B SD card,
    allowing access to the data remotely or through manual retrieval during field
    visits. Two options for transmitting the collected data to the ThingSpeak IoT
    platform are available. Firstly, the data can be uploaded directly from the Raspberry
    Pi 4B if internet connectivity is available for growers with Wi-Fi access. Secondly,
    the Raspberry Pi 4B transmits the recently acquired data to the Arduino MKR1310.
    The Arduino MKR1310 decodes the data received from the Raspberry Pi 4B and forwards
    it to the ThingSpeak. The second method is for low-rate, long-range communication
    beyond the limit of Wi-Fi. A single message receivable to the ThingSpeak server
    includes data for eight fields. In our demonstration, a single message was enough
    to transmit the data generated. Fields 1 and 2 are reserved for geographic coordinates
    (namely, latitude and longitude) to represent the device''s location. The third
    field was for camera orientation. Image quality/crop type, plant count, weed count,
    insect count, and crop canopy coverage were allocated from fields four to eight.
    ThingSpeak supports eight channels per gateway. If additional data is generated
    in the future, we have to create new channels to accommodate them. However, only
    data in a single channel can be passed through a single message. The Arduino-LoRa
    library was used to prepare the LoRa messages forwarded to the gateway (Mistry,
    2016). The message generated from the Arduino MKR1310 includes the device identification
    number and the data with the field number. Once the gateway receives this message,
    it adds the target client ID (generated by ThingSpeak when defining a device),
    host address (mqtt://mqtt3.thingspeak.com), server port number, username and password,
    channel ID, and the data in each field according to the Message Queuing Telemetry
    Transport (MQTT) protocol. Username and password ensure that only authorized devices
    can transmit data to the ThingSpeak platform. ThingSpeak provides two ways to
    interact with its platform, REST (Representative State Transfer) and MQTT protocols.
    The advantages of using MQTT over REST protocol are that it supports ThingSpeak
    data publishing, including immediate and minimum power consumption and data transmission
    over limited bandwidth, which encouraged us to select the MQTT protocol in our
    demonstration. 3. Results and discussion 3.1. DCNN model performance CropClassiNet
    had a test accuracy of 91.26 %, a Jaccard Index of 0.77, and an F1-score of 0.91;
    the confusion matrix is given in Fig. 7. The highest precision is for the “grass”
    class (100 %), and the lowest is for “soybean” (92.0 %). The highest recall is
    for the “corn” class (99.9 %), and the lowest is for “grass” (67.1 %). The primary
    goal of CropClassiNet is to determine the quality of new images and direct them
    for subsequent processing (Fig. 5). This step has never been executed in an image-based
    crop monitoring platform before. Further, CropClassiNet can eliminate erroneous
    images when humans are present in the camera’s field of view or when the camera
    is misaligned due to external factors. AICropCAM can send maintenance requests
    through IoT analytics if rejected images are continuously generated. Download
    : Download high-res image (275KB) Download : Download full-size image Fig. 7.
    Confusion matrix for test images by CropClassiNet. CanopySegNet on the test images
    achieved a global accuracy of 0.93, a weighted IoU of 0.87, and a mean BF score
    of 0.73. Fig. 8 shows an example of an original soybean image and the corresponding
    segmentation result by CanopySegNet, which estimated CC to be 18.72 %. Season-long,
    time-series images can be fed into CanopySegNet to generate diurnal and seasonal
    curves of crop CC, as shown in Fig. 9. Download : Download high-res image (621KB)
    Download : Download full-size image Fig. 8. An image of soybean crop and the segmentation
    result by CropSegNet to calculate canopy coverage. Download : Download high-res
    image (367KB) Download : Download full-size image Fig. 9. Examples of diurnal
    and seasonal variations of canopy coverage as computed by CropSegNet. According
    to Fig. 9, canopy coverage percentage variation is low during the daytime and
    reaches zero at night. This verifies the need to eliminate low-light images before
    segmenting. As shown in Fig. 5, it is possible to eliminate the generation of
    false values when the camera captures images under low light conditions by halting
    the process of running CanopySegNet. There are three diurnal variation series
    on 6/8/2021, 6/26/2021, and 7/12/2021 in Fig. 9. The CC increased from 8 % to
    95 % between 6/8/2021 to 7/12/2021. The seasonal trend showed that the CC reached
    a maximum around 7/8/2021. These results suggest that the proposed stacked models
    can track the daily and seasonal CC variation and eliminate the effect of lighting
    conditions on false value generation. Table 5. Performance of PlantCountNet and
    InsectNet on the test image set (Root mean square error (RMSE)/Final validation
    loss (FVL)). Model Name Architecture Input size Validation RMSE/FVL Mean average
    precision Object class PlantCountNet YOLOv2 320 × 320 × 3 0.888 (RMSE) 0.66 Soybean
    0.86 Weed InsectNet YOLOv4 320 × 320 × 3 26.2 (FVL) 0.02 Insect The overall performance
    of the PlantCountNet and InsectNet is given in Table 5. Fig. 10(A) and 10(B) show
    the result obtained by PlantCountNet for a soybean image at an early vegetative
    stage (V3). Meanwhile Fig. 10(C) and 10(D) shows the result at a reproductive
    stage (R1). It can be seen that, at V3 stage, the model outputs matched the labels
    of soybean and weed plants well, indicating a level of high accuracy. Download
    : Download high-res image (1MB) Download : Download full-size image Fig. 10. The
    result of PlantCountNet for soybean and weed counting: Manually annotated vs.
    model-predicted bounding boxes at V3 growth stage (A and B); manually annotated
    vs. model-predicted bounding boxes at R1 growth stage (C and D). The size of insects
    is very small compared to the size of images (Fig. 11), which is the main reason
    for the low mAP for InsectNet (Table 5). Increasing input image resolution beyond
    480 × 480 × 3 is impractical as it exceeds the memory limitation to load models
    into Raspberry Pi 4B. A potential solution could be to increase the resolution
    of the region of interest by splitting the original image while keeping the resolution
    the same. Also, we suggest using the approach recommended by Tetila et al., 2020a,
    Tetila et al., 2020b in the future on Raspberry Pi model 4B. As technology advances,
    we expect the memory capacities will increase for edge computing units. At the
    same time, the state-of-the-art object detection algorithms will improve the accuracy
    for small object detection. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 11. The result of InsectNet for insect counting
    in soybean. The top row shows a situation of high false positives and low false
    negatives: (A) and (B) are manually annotated and model-predicted insect labels,
    respectively. The bottom row shows a situation of low false positive and high
    false negative: (C) and (D) are manually annotated and model-predicted insect
    labels. 3.2. Power consumption for Raspberry Pi 4B Since edge cameras in farmlands
    have limited access to electric power, information on their power consumption
    is essential for designing IoT devices and systems. AICropCAM is designed to be
    energized by solar power. It runs on a rechargeable battery when there is no solar
    power. We monitored the maximum energy consumption of each task performed by AICropCAM,
    and the result is presented in Table 6. Four main strategies are available for
    the power management of IoT edge devices: Selecting power-efficient hardware,
    maintaining low power modes, dynamic power management, and cloud-based management.
    Raspberry Pi 4B is an affordable power-efficient single-board computer suitable
    for our application, but it does not naturally support low-power modes. Therefore,
    we introduced the Arduino MKR1310 LoRa module for the Raspberry Pi 4B dynamic
    power management. Furthermore, this Arduino module allows us to perform cloud-based
    central management independently. Table 6. Electrical power consumption of the
    Raspberry Pi 4B and the Arduino MKR1310 during edge image processing. Device Activity
    The maximum current range and the voltage recorded Raspberry Pi 4B Idle run 5.25
    V × (0.45 – 0.53) A Image classification 5.25 V × (0.97 – 1.04) A Image segmentation
    5.25 V × (0.98 – 1.11) A Weed and plant detection 5.25 V × (0.62 – 0.70) A Insect
    detection 5.25 V × (0.62 – 0.70) A Arduino MKR1310 Sleep mode <0.01A Serial communication
    <0.01A LoRa transmission <0.01A For our measurements, we used a Raspberry Pi 4B
    with 8 GB of RAM, connected to an HDMI monitor, a USB keyboard, and a USB mouse,
    and ran a MathWorks® Raspbian image (file used to boot the Raspberry Pi 4B). The
    Raspberry Pi 4B was operated at room temperature and connected to a wireless LAN
    access point and a laptop via an Ethernet cable. The electric current consumption
    for running each DCNN model was recorded during the test. CropClassiNet had the
    highest current consumption, while the PlantCountNet and InsectNet models had
    the lowest. As for LoRa transmission, we could not measure its current consumption
    because the lowest value our instrument could measure was 0.01A. Based on the
    manufacturer''s specifications, the Arduino MKR1310 consumes 104 uA at 5 V. The
    average time to run the DCNN models is essential to estimate the energy consumed
    for each prediction. These parameters listed in Table 7 provide essential guidelines
    for designing IoT sensor nodes with suitable batteries and power sources. We also
    noticed that typically the first prediction of a model took the longest time,
    but the rest take a considerably shorter time to predict. Table 7. Time duration
    needed for the selected DCNN models deployed in the Raspberry Pi 4B. Model/Task
    Input image size Time for predicting results (s) The maximum power demand for
    the activity (W) CropClassiNet/Image quality evaluation and crop classification
    224 × 224 × 3 6.44 5.46 CanopySegNet/Semantic segmentation to separate canopy
    from background 512 × 512 × 3 20.20 5.83 PlantCountNet/Weed and plant detection,
    classification, and counting 320 × 320 × 3 14.38 3.68 InsectNet/Insect detection
    320 × 320 × 3 0.20 3.68 Semantic segmentation was the most power-demanding activity,
    while insect detection was the least. Changing the order of the image processing
    models and adding new models or dropping existing models is possible during regular
    operation. It enables dynamic power management within the Raspberry Pi module.
    The main advantage of AICropCAM is that it implements a stack of four DCNN-based
    image processing models with multiple objectives. To the best of our knowledge,
    this is the first time such a system has been developed for a field crop monitoring
    camera. AICropCAM has applications such as setting up smart in-field or greenhouse
    IoT camera networks with edge computing capability, monitoring crops by attaching
    them to sprinkler irrigation systems (pivots and linear moves), or collecting
    crop information through ground or aerial mobile robots. The relatively short
    time to run each DCNN model makes the system suitable for real-time applications,
    including variable rate irrigation, fertilization, and spraying. For example,
    a pivot irrigated multi-cropping system with AICropCAM can automate irrigation
    or fertigation transition between different crops or crops at different growth
    stages by automatically providing the crop type or growth stage information to
    the irrigation controller. Additionally, existing herbicide or pesticide sprayers
    can get the feedback of the PlantCountNet and InsectNet in the AICropCAM for precision
    spraying. 4. Conclusion and future perspectives This paper outlines the essential
    components of constructing a functional edge image processing framework for real-time
    crop monitoring. From a software standpoint, CropClassiNet can categorize captured
    images according to image quality and detect the presence of specific crop types
    for further processing. CanopySegNet can further quantify the degree of canopy
    coverage; PlantCountNet can count the number of plants and weeds in the image;
    and finally, InsectNet can count the number of insects in the image. These four
    DCNN models, when implemented on edge devices, can extract an array of important
    crop and canopy parameters from field images and enable real-time, low-latency
    decision making and applications. Deep learning-based image processing on the
    edge has excellent potential in PA. Applications of AICropCAM are not limited
    to image classification, segmentation, plant counting, or weed counting. Potential
    future applications include insect classification and crop damage estimation,
    weed classification and pressure estimation, fruit identification and yield estimation,
    decision on replanting (Whigham et al., 2000), and disease identification and
    disease damage estimation in real time using actual field images collected by
    AICropCAM. AICropCAM shows excellent potential in enhancing crop management through
    crop monitoring. However, the current demonstration requires significant improvements
    on both hardware and software fronts. Customized circuitry and modular design
    are required to put AICropCAM in commercial farm applications. The full potential
    of the AICropCAM can be achieved by putting this camera on a moving platform like
    a center pivot with a GPS receiver to generate spatiotemporal data. Crop classification
    must include more crop types, and segmentation models need training data from
    other crop types. The DCNN models for weed and insect identification require the
    capability to identify different weed types, their growth stage, different insect
    types, and their growth stages to generate effective pest control decisions. Additionally,
    improving the models’ accuracy in image classification, segmentation, and object
    detection is crucial. It can be achieved by increasing the number of training
    image data sets. We also planned to expand the research for multiple edge architecture
    evaluation. Architectures such as a high-performance edge computer that accepts
    images from multiple edge devices through short-range, high-speed communication
    (e.g., Wi-Fi) and can run more accurate deep learning models with higher numbers
    of parameters, might be a better solution for the primary objectives addressed
    in this paper. We aim to expand the AICropCAM applications to other crops beyond
    corn and soybean. By making these improvements, AICropCAM will become a more effective
    tool for crop management, potentially revolutionizing how we grow and manage crops.
    Funding This work was supported by the United States Department of Agriculture
    – National Institute of Food and Agriculture grants [Award 2020-68013-32371 to
    YG and GB, Award 2021-67021-34417 to YG]. CRediT authorship contribution statement
    Nipuna Chamara: Methodology, Software, Visualization. Geng Bai: Conceptualization,
    Methodology, Resources. Yufeng Ge: Conceptualization, Resources, Supervision,
    Project administration, Funding acquisition. Declaration of Competing Interest
    The authors declare the following financial interests/personal relationships which
    may be considered as potential competing interests: Nipuna Chamara, Yufeng Ge,
    Geng Bai has patent pending to University of Nebraska-Lincoln. Acknowledgements
    Jianxin Sun assisted in developing the imaging device with Raspberry Pi Zero used
    for image acquisition. David Scoby helped the field management and AICropCAM installation.
    Junxiao Zhang supported the field installation of AICropCAM and smart-phone based
    acquisition of crop images with insects. Data availability Data will be made available
    on request. References Aasen et al., 2020 H. Aasen, N. Kirchgessner, A. Walter,
    F. Liebisch PhenoCams for field phenotyping: using very high temporal resolution
    digital repeated photography to investigate interactions of growth, phenology,
    and harvest traits Front. Plant Sci., 11 (June) (2020), pp. 1-16, 10.3389/fpls.2020.00593
    Google Scholar Anubha et al., 2019 P.S. Anubha, V. Sathiesh Kumar, S. Harini A
    study on plant recognition using conventional image processing and deep learning
    approaches J. Intell. Fuzzy Syst., 36 (3) (2019), pp. 1997-2004, 10.3233/JIFS-169911
    Google Scholar ArduCAM, 2016 ArduCAM ESP8266 UNO board User Guide (pp. 0–9). (2016).
    www.ArduCAM.com. Google Scholar Bai et al., 2019 G. Bai, Y. Ge, D. Scoby, B. Leavitt,
    V. Stoerger, N. Kirchgessner, S. Irmak, G. Graef, J. Schnable, T. Awada NU-Spidercam:
    A large-scale, cable-driven, integrated sensing and robotic system for advanced
    phenotyping, remote sensing, and agronomic research Comput. Electron. Agric.,
    160 (March) (2019), pp. 71-81, 10.1016/j.compag.2019.03.009 View PDFView articleView
    in ScopusGoogle Scholar Barbedo, 2014 J.G.A. Barbedo Using digital image processing
    for counting whiteflies on soybean leaves J. Asia Pac. Entomol., 17 (4) (2014),
    pp. 685-694, 10.1016/j.aspen.2014.06.014 View PDFView articleView in ScopusGoogle
    Scholar Cao et al., 2020 K. Cao, Y. Liu, G. Meng, Q. Sun An Overview on Edge Computing
    Research IEEE Access, 8 (2020), pp. 85714-85728, 10.1109/ACCESS.2020.2991734 View
    in ScopusGoogle Scholar Chamara et al., 2021 N. Chamara, K. Alkhadi, H. Jin, F.
    Bai, A. Samal, Y. Ge A deep convolutional neural network based image processing
    framework for monitoring the growth of soybean crops. 2021 ASABE Annual International
    Meeting, 2100259 (2021), 10.13031/aim.202100259 Google Scholar Chamara et al.,
    2022 N. Chamara, M.D. Islam, G.F. Bai, Y. Shi, Y. Ge Ag-IoT for crop and environment
    monitoring: Past, present, and future Agr. Syst., 203, 103497 (2022), 10.1016/j.agsy.2022.103497
    Google Scholar Chamara, 2021 N. Chamara Development of an Internet of Things (IoT)
    Enabled Novel Wireless Multi-Sensor Network for Infield Crop Monitoring. Master’s
    Thesis, Department of Biological Systems Engineering, University of Nebraska-Lincoln
    (2021) Google Scholar Datasheet Raspberry Pi Model, 2019 Datasheet Raspberry Pi
    Model B, 2019. https://datasheets.raspberrypi.org. Accessed 11 November 2023.
    Google Scholar Firdaus-Nawi et al., 2018 Firdaus-Nawi, M., Noraini, O., Sabri,
    M.Y., Siti-Zahrah, A., Zamri-Saad, M., Latifah, H., 2018. DeepLabv3+_Encoder-Decoder
    with Atrous Separable Convolution for Semantic Image Segmentation. In: Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 801–818. Google Scholar
    Ghorai et al., 2021 A.K. Ghorai, A.R. Barman, B. Chandra, K. Viswavidyalaya, S.
    Jash, B. Chandra, K. Viswavidyalaya, B. Chandra, K. Viswavidyalaya Image processing
    based detection of diseases and nutrient deficiencies in plants SATSA Mukhapatra,
    25 (1) (2021), pp. 1-24 Google Scholar He et al., 2016 He, K., Zhang, X., Ren,
    S., Sun, J., 2016. Deep residual learning for image recognition kaiming. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778.
    doi: 10.1002/chin.200650130. Google Scholar LeCun et al., 1998 LeCun, Y., Bottou,
    L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to document
    recognition. Proc. IEEE 86(11), 2278–2323. doi: 10.1109/5.726791. Google Scholar
    Liang et al., 2023 Liang, W. Z., Oboamah, J., Qiao, X., Ge, Y., Harveson, B.,
    Rudnick, D. R., Wang, J., Yang, H., Gradiz, A., 2023. CanopyCAM – an edge-computing
    sensing unit for continuous measurement of canopy cover percentage of dry edible
    beans. Comput. Electron. Agric. 204 (January), 107498. https://doi.org/10.1016/j.compag.2022.107498.
    Google Scholar Luis et al., 2020 Luis, S., Filipe, N.S., Paulo, M.O., Pranjali,
    S., 2020. Deep Learning applications in agriculture: a short review. Deep Learning
    Applications in Agriculture: A Short Review, 1092 AISC(January), C1. doi: 10.1007/978-3-030-35990-4.
    Google Scholar Meidas Trail Cameras, 2022 Meidas Trail Cameras, 2022. https://www.meidase.com/product-category/trail-cameras/.
    Accessed 11 November 2023. Google Scholar Mistry, 2016 Mistry, S., 2016. Arduino
    LoRa. MIT License. https://github.com/sandeepmistry/arduino-LoRa. Accessed 11
    November 2023. Google Scholar Park et al., 2007 Y. Park, R.K. Krell, M. Carroll
    Theory, technology, and practice of site-specific insect pest management J. Asia
    Pac. Entomol., 10 (2) (2007), pp. 89-101 View PDFView articleView in ScopusGoogle
    Scholar Paymode and Malode, 2022 A.S. Paymode, V.B. Malode Transfer learning for
    multi-crop leaf disease image classification using convolutional neural network
    VGG Artif. Intell. Agric., 6 (2022), pp. 23-33, 10.1016/j.aiia.2021.12.002 View
    PDFView articleView in ScopusGoogle Scholar Richardson, 2019 A.D. Richardson Tracking
    seasonal rhythms of plants in diverse ecosystems with digital camera imagery New
    Phytol., 222 (4) (2019), pp. 1742-1750, 10.1111/nph.15591 View in ScopusGoogle
    Scholar Sakamoto et al., 2012 T. Sakamoto, A.A. Gitelson, A.L. Nguy-Robertson,
    T.J. Arkebauer, B.D. Wardlow, A.E. Suyker, S.B. Verma, M. Shibayama An alternative
    method using digital cameras for continuous monitoring of crop status Agric. For.
    Meteorol., 154–155 (2012), p. 113, 10.1016/j.agrformet.2011.10.014 View PDFView
    articleView in ScopusGoogle Scholar Sharma et al., 2020 P. Sharma, Y.P.S. Berwal,
    W. Ghai Performance analysis of deep learning CNN models for disease detection
    in plants using image segmentation Inf. Process. Agric., 7 (4) (2020), pp. 566-574,
    10.1016/j.inpa.2019.11.001 View PDFView articleView in ScopusGoogle Scholar Sritarapipat
    et al., 2014 T. Sritarapipat, P. Rakwatin, T. Kasetkasem Automatic rice crop height
    measurement using a field server and digital image processing Sensors (Switzerland),
    14 (1) (2014), pp. 900-926, 10.3390/s140100900 View in ScopusGoogle Scholar Taylor
    and Browning, 2022 S.D. Taylor, D.M. Browning Classification of daily crop phenology
    in phenocams using deep learning and hidden markov models Remote Sens. (Basel),
    14 (2) (2022), pp. 1-22, 10.3390/rs14020286 Google Scholar Tetila et al., 2020a
    Tetila, E.C., Machado, B.B., Astolfi, G., Belete, N.A.S., Amorim, W.P., Roel,
    A.R., Pistori, H., 2020. Detection and classification of soybean pests using deep
    learning with UAV images. Computers and Electronics in Agriculture, 179(May).
    doi: 10.1016/j.compag.2020.105836. Google Scholar Tetila et al., 2020b E.C. Tetila,
    B.B. MacHado, G.V. Menezes, N.A. De Souza Belete, G. Astolfi, H. Pistori A deep-learning
    approach for automatic counting of soybean insect pests IEEE Geosci. Remote Sens.
    Lett., 17 (10) (2020), pp. 1837-1841, 10.1109/LGRS.2019.2954735 View in ScopusGoogle
    Scholar The MathWorks, 2022 The MathWorks, I., 2022. MATLAB Coder - MATLAB. MathWorks.
    https://www.mathworks.com/products/matlab-coder.html. Google Scholar Tian et al.,
    2020 H. Tian, T. Wang, Y. Liu, X. Qiao, Y. Li Computer vision technology in agricultural
    automation—a review Inf. Process. Agric., 7 (1) (2020), pp. 1-19, 10.1016/j.inpa.2019.09.006
    View PDFView articleView in ScopusGoogle Scholar van Dijk et al., 2021 M. van
    Dijk, T. Morley, M.L. Rau, Y. Saghai A meta-analysis of projected global food
    demand and population at risk of hunger for the period 2010–2050 Nat. Food, 2
    (7) (2021), pp. 494-501, 10.1038/s43016-021-00322-9 View in ScopusGoogle Scholar
    Wang et al., 2022b Q. Wang, M. Cheng, S. Huang, Z. Cai, J. Zhang, H. Yuan A deep
    learning approach incorporating YOLO v5 and attention mechanisms for field real-time
    detection of the invasive weed Solanum rostratum Dunal seedlings Comput. Electron.
    Agric., 199 (July) (2022), Article 107194, 10.1016/j.compag.2022.107194 View PDFView
    articleView in ScopusGoogle Scholar Wang et al., 2022a J. Wang, Z. Gao, Y. Zhang,
    J. Zhou, J. Wu, P. Li Real-time detection and location of potted flowers based
    on a ZED camera and a YOLO V4-tiny deep learning algorithm Horticulturae, 8 (1)
    (2022), 10.3390/horticulturae8010021 Google Scholar Wang et al., 2014 Y. Wang,
    D. Wang, P. Shi, K. Omasa Estimating rice chlorophyll content and leaf nitrogen
    concentration with a digital still color camera under natural light Plant Methods,
    10 (3) (2014), pp. 273-286, 10.1016/S0378-4290(99)00063-5 View in ScopusGoogle
    Scholar Wang et al., 2019 A. Wang, W. Zhang, X. Wei A review on weed detection
    using ground-based machine vision and image processing techniques Comput. Electron.
    Agric., 158 (January) (2019), pp. 226-240, 10.1016/j.compag.2019.02.005 View PDFView
    articleView in ScopusGoogle Scholar Whigham et al., 2000 K. Whigham, D. Farnham,
    J. Lundvall, D. Tranel Soybean replant decision, Department of Agronomy, Iowa
    State University (2000) Google Scholar Yasrab et al., 2021 R. Yasrab, J. Zhang,
    P. Smyth, M.P. Pound Predicting plant growth from time-series data using deep
    learning Remote Sens. (Basel), 13 (3) (2021), pp. 1-17, 10.3390/rs13030331 View
    in ScopusGoogle Scholar Yuan et al., 2019 W. Yuan, N.K. Wijewardane, S. Jenkins,
    G. Bai, Y. Ge, G.L. Graef Early prediction of soybean traits through color and
    texture features of canopy RGB imagery Sci. Rep., 9 (2019), p. 14089, 10.1038/s41598-019-50480-x
    View in ScopusGoogle Scholar Zualkernan et al., 2022 I. Zualkernan, S. Dhou, J.
    Judas, A.R. Sajun, B.R. Gomez, L.A. Hussain An IoT system using deep learning
    to classify camera trap images on the edge Computers, 11 (1) (2022), pp. 1-24,
    10.3390/computers11010013 Google Scholar Cited by (1) YOLO performance analysis
    for real-time detection of soybean pests 2024, Smart Agricultural Technology Show
    abstract © 2023 The Authors. Published by Elsevier B.V. Part of special issue
    Agricultural Cybernetics: A New Methodology of Analysis and Development for Modern
    Agricultural Production Systems Edited by Yanbo Huang, Manoj Karkee, Lie Tang,
    Dong Chen View special issue Recommended articles LSCA-net: A lightweight spectral
    convolution attention network for hyperspectral image processing Computers and
    Electronics in Agriculture, Volume 215, 2023, Article 108382 Ziru Yu, Wei Cui
    View PDF Joint control method based on speed and slip rate switching in plowing
    operation of wheeled electric tractor equipped with sliding battery pack Computers
    and Electronics in Agriculture, Volume 215, 2023, Article 108426 Qi Wang, …, Yongjie
    Cui View PDF Monitoring maize lodging severity based on multi-temporal Sentinel-1
    images using Time-weighted Dynamic time Warping Computers and Electronics in Agriculture,
    Volume 215, 2023, Article 108365 Xuzhou Qu, …, Yuchun Pan View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 1 Captures Readers: 19 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hashmi M.F.
  - Keskar A.G.
  citation_count: '1'
  description: Machine Learning and Deep Learning for Smart Agriculture and Applications
    delves into the captivating realm of artificial intelligence and its pivotal role
    in transforming the landscape of modern agriculture. With a focus on precision
    agriculture, digital farming, and emerging concepts, this book illuminates the
    significance of sustainable food production and resource management in the face
    of evolving digital hardware and software technologies. Geospatial technology,
    robotics, the Internet of Things (IoT), and data analytics converge with machine
    learning and big data to unlock new possibilities in agricultural management.
    This book explores the synergy between these disciplines, offering cutting-edge
    insights into data-intensive processes within operational agricultural environments.
    From automated irrigation systems and agricultural drones for field analysis to
    crop monitoring and precision agriculture, the applications of machine learning
    are far-reaching. Animal identification and health monitoring also benefit from
    these advanced techniques. One of the book's key focuses is the critical role
    of health monitoring for plants and fruits in achieving sustainable agriculture.
    Plant diseases pose significant financial challenges in the farming industry worldwide.
    By leveraging sophisticated image processing and advanced computer vision techniques,
    automated detection and identification of plant diseases are revolutionized, enabling
    precise and rapid identification while minimizing human effort and labor costs.
    For researchers involved in image processing and computer vision for smart agriculture,
    this book offers invaluable insights. It covers the most important fields of image
    processing in the agricultural domain, encompassing computer vision applications,
    machine learning, and deep learning approaches. From the analysis of agricultural
    data using machine learning to the implementation of bio-inspired algorithms,
    the book explores the breadth and depth of agricultural modernization through
    the lens of AI technologies. With practical case studies on vegetable and fruit
    leaf disease detection, drone-based agriculture, and the impact of pesticides
    on plants, this book provides a comprehensive understanding of the applications
    of machine learning and deep learning in smart agriculture. It also examines various
    modeling techniques employed in this field and showcases how artificial intelligence
    can revolutionize plant disease detection. This book serves as a comprehensive
    guide for researchers, practitioners, and students seeking to harness the power
    of AI in transforming the agricultural landscape.
  doi: 10.4018/978-1-6684-9975-7
  full_citation: '>'
  full_text: '>

    "Login Register Language: English Welcome to the InfoSci Platform University of
    Nebraska - Lincoln Database Search Research Tools User Resources Reference Hub1
    Indices1 Machine Learning and Deep Learning for Smart Agriculture and Applications
    Mohamamd Farukh Hashmi, Avinash G. Kesakr Copyright: © 2023 |Pages: 257 ISBN13:
    9781668499757|ISBN10: 1668499754|EISBN13: 9781668499764 DOI: 10.4018/978-1-6684-9975-7
    Cite Book Favorite Full-Book Download Machine Learning and Deep Learning for Smart
    Agriculture and Applications delves into the captivating realm of artificial intelligence
    and its pivotal role in transforming the landscape of modern agriculture. With
    a focus on precision agriculture, digital farming, and emerging concepts, this
    book illuminates the significance of sustainable food production and resource
    management in the face of evolving digital hardware and software technologies.
    Geospatial technology, robotics, the Internet of Things (IoT), and data analytics
    converge with machine learning and big data to unlock new possibilities in agricultural
    management. This book explores the synergy between these disciplines, offering
    cutting-edge insights into data-intensive processes within operational agricultural
    environments. From automated irrigation systems and agricultural drones for field
    analysis to crop monitoring and precision agriculture, the applications of machine
    learning are far-reaching. Animal identification and health monitoring also benefit
    from these advanced techniques. With practical case studies on vegetable and fruit
    leaf disease detection, drone-based agriculture, and the impact of pesticides
    on plants, this book provides a comprehensive understanding of the applications
    of machine learning and deep learning in smart agriculture. It also examines various
    modeling techniques employed in this field and showcases how artificial intelligence
    can revolutionize plant disease detection. This book serves as a comprehensive
    guide for researchers, practitioners, and students seeking to harness the power
    of AI in transforming the agricultural landscape. Table of Contents Reset Front
    Materials PDF HTML Title Page PDF HTML Copyright Page PDF HTML Advances in Environmental
    Engineering and Green Technologies (AEEGT) Book Series PDF HTML Preface Chapters
    PDF HTML Chapter 1 Deep Learning Techniques for Smart Agriculture Applications  (pages
    1-23) Ankita Mishra, Sourik Banerjee, Brijendra Singh With an emphasis on the
    rapid and accurate diagnosis of plant and fruit diseases, researchers have been
    looking into sustainable agriculture utilizing cutting-edge deep learning techniques.
    The objective is to show how effective deep... PDF HTML Chapter 2 Review Work
    of Automation Agricultural Robot System Using Machine Learning and Deep Learning  (pages
    24-33) R. Felshiya Rajakumari, M. Siva Ramkumar An occupation based on automatic
    system using machine learning and deep learning techniques is developed in this
    chapter. The agricultural land and automatic systems are worked mutually to defeat
    the concern by integration with solar... PDF HTML Chapter 3 Applications of Deep
    Learning and Machine Learning in Smart Agriculture: A Survey  (pages 34-57) Amrit
    pal Kaur, Devershi Pallavi Bhatt, Linesh Raja Machine learning (ML) and deep learning
    can be used in the smartest way possible to improve productivity in agriculture.
    The Food and Agriculture Organization''s research shows that the crop''s production
    is rising. One of the... PDF HTML Chapter 4 Plant Disease Classification in Segmented
    Images Using Computer Vision  (pages 58-92) Rajashri Roy Choudhury, Piyal Roy,
    Shivnath Ghosh Agriculture productivity has a significant impact on the lives
    of people and economies because of the growing human population. In agriculture,
    plant diseases are a big problem since they result in severe crop losses and financial...
    PDF HTML Chapter 5 Enhancing Tomato Fruit Detection and Counting Through AI-Enabled
    Agricultural Innovations  (pages 93-105) S Gandhimathi Alias Usha This chapter
    aims to enhance tomato fruit detection and counting in agricultural practices
    through AI-enabled innovations. Traditional manual methods for fruit detection
    and counting are labor-intensive and time-consuming. By... PDF HTML Chapter 6
    Harnessing Environmental Intelligence to Enhance Crop Management by Leveraging
    Deep Learning Technique  (pages 106-123) S. Gandhimathi Alias Usha This chapter
    aims to enhance crop management practices by harnessing environmental intelligence
    through the power of deep learning techniques. Efficient and sustainable crop
    management is crucial for meeting the increasing demand for... PDF HTML Chapter
    7 Crop Prediction for Smart Agriculture Using Ensemble of Classifiers  (pages
    124-141) Khushal Kindra, Bhuvaneswari Amma N. G. Nowadays due to the advancement
    in technology, smart agriculture is in the evolving stage. Agricultural farmers
    worldwide commonly utilize the process of cultivating and harvesting crops to
    produce food and fiber. Therefore, crop... PDF HTML Chapter 8 An Integrated Approach
    for Selection and Design of Sustainable Farmers'' Protective-Hat in India  (pages
    142-172) Suchismita Satapathy Normally, the farmers in India are required to work
    in adverse climatic conditions while performing their agricultural activities.
    Though a number of preventive measures are available for the protection of farmers,
    the nominal and... PDF HTML Chapter 9 Smart Crop Protection System From Wild Animals
    Using Artificial Intelligence  (pages 173-191) Shailaja S. Mudengudi, Muktha S.
    Patil, Neetha S. Mudaraddi The first major threat to the farmers is drought. Crop
    vandalization by animals is the second major threat after drought. Crops are vulnerable
    to animals. Therefore, it is very important to monitor the nearby presence of
    animals. The... PDF HTML Chapter 10 GUI-Based End-to-End Deep Learning Model for
    Corn Leaf Disease Classification  (pages 192-213) G. Revathy, J. Jeyabharathi,
    Madonna Arieth, A. Ramalingam Food security is a major problem worldwide. Ensuring
    that the crops produced are both safe and wholesome is crucial not only for people
    as the ultimate consumers of the crops, but also for farmers. Plant diseases are
    responsible for... This content was retracted PDF HTML Chapter 11 A Transfer Learning
    Approach for Detecting Plant Leaf Diseases With Convolutional Neural Networks  (pages
    214-224) P. Valarmathi, N. G. Bhuvaneswari Amma, Vasu Bhasin Agriculture is an
    indispensable sector for the continuity of homo sapiens. In the Indian context
    where agriculture contributes 19.9 percent of GDP and engages almost 54.6 percent
    of the population, it requires great measures to be... Back Materials PDF HTML
    Compilation of References PDF HTML About the Contributors PDF Index View All Books
    Request Access You do not own this content. Please login to recommend this title
    to your institution''s librarian or purchase it from the IGI Global bookstore.
    Username or email:   Password:   Log In   Forgot individual login password? Create
    individual account Research Tools Database Search | Help | User Guide | Advisory
    Board User Resources Librarians | Researchers | Authors Librarian Tools COUNTER
    Reports | Persistent URLs | MARC Records | Institution Holdings | Institution
    Settings Librarian Resources Training | Title Lists | Licensing and Consortium
    Information | Promotions Policies Terms and Conditions     Copyright © 1988-2024,
    IGI Global - All Rights Reserved"'
  inline_citation: '>'
  journal: Machine Learning and Deep Learning for Smart Agriculture and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Machine learning and deep learning for smart agriculture and applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Devi N.
  - Sarma K.K.
  - Laskar S.
  citation_count: '10'
  description: Crop health monitoring and weed removal are two crucial elements dictating
    efficient, productive and resilient cultivation. Due to frequent attacks by pest
    and pathogens, the crops become diseased resulting in degradation of the quality
    and quantity of the production. The process of continuous monitoring of crop health
    is challenging and requires the involvement of information and communication technologies
    (ICT). The outcome is precision agriculture where the Internet of Things (IoT)
    and Artificial Intelligence (AI) techniques are vital ingredients. The design
    of an integrated approach of precision agriculture based on IoT and AI is discussed
    here which is tailored for real time crop health monitoring and performs various
    other operations like weed detection, ambient air sensing, watering the vegetation
    automatically at regular intervals of time, spraying of pesticides etc. The proposed
    system is a combination of an IoT formed using sensors and devices, image processing
    and machine learning (ML)/ deep learning (DL) techniques confined to the cultivation
    of fifteen varieties of beans found in India. The work involves two intelligent
    learning models configured to capture spatio-temporal attributes of image samples
    and sensor inputs and for real time discrimination between healthy and diseased
    bean leaves, detection of weeds growing around the cultivation land and also for
    process control. The first approach employs a DL structure named EfficientNetB7
    along with a Bidirectional Long Short Term Memory (BiLSTM) while the second method
    adopts a VGG16 with an integrated attention mechanism. Also experiments have been
    carried out using benchmark ML classifiers like Support Vector Machine (SVM),
    Random Forest (RF), K-Nearest Neighbor (KNN), Multi-Layer Perceptron (MLP) and
    Time Delay Neural Network (TDNN) combined with feature extraction techniques.
    Segmentation methods have been used to separate out the diseased sections of the
    leaves which are then used as apriori labels for the classifiers to reinforce
    the previously known details of the bean varieties. Subsequently, the trained
    networks are tested with bean leaf samples collected from cultivation farms. Results
    show that our proposed DL models could accurately predict the health state of
    the bean leaves with less computation time. With an automated approach of bean
    leaf health discrimination, weed detection and process control, the cost effectiveness
    of the overall effort is enhanced. Further, the sensor pack also provides precise
    thresholds at which water sprinkling could be initiated resulting in water conservation.
  doi: 10.1016/j.ecoinf.2023.102044
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methodology
    3. Design details of the proposed models 4. Experimental results 5. Conclusion
    Ethics approval Consent to participate Consent for publication Availability of
    data Grants and funding Author''s contribution Declaration of Competing Interest
    Acknowledgements Data availability References Show full outline Cited by (11)
    Figures (15) Show 9 more figures Tables (12) Table 1 Table 2 Table 3 Table 4 Table
    5 Table 6 Show all tables Ecological Informatics Volume 75, July 2023, 102044
    Design of an intelligent bean cultivation approach using computer vision, IoT
    and spatio-temporal deep learning structures Author links open overlay panel Nilakshi
    Devi a, Kandarpa Kumar Sarma b, Shakuntala Laskar a Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.ecoinf.2023.102044 Get rights and content
    Highlights • The design of EfficientNetB7 with BiLSTM and VGG16 with attention
    has been proposed. • The models capture the spatio-temporal attributes of the
    bean leaf samples. • Segmentation is performed to separate the diseased area of
    the leaves. • DL-IoT set up to monitor the environmental parameters and execute
    process control. Abstract Crop health monitoring and weed removal are two crucial
    elements dictating efficient, productive and resilient cultivation. Due to frequent
    attacks by pest and pathogens, the crops become diseased resulting in degradation
    of the quality and quantity of the production. The process of continuous monitoring
    of crop health is challenging and requires the involvement of information and
    communication technologies (ICT). The outcome is precision agriculture where the
    Internet of Things (IoT) and Artificial Intelligence (AI) techniques are vital
    ingredients. The design of an integrated approach of precision agriculture based
    on IoT and AI is discussed here which is tailored for real time crop health monitoring
    and performs various other operations like weed detection, ambient air sensing,
    watering the vegetation automatically at regular intervals of time, spraying of
    pesticides etc. The proposed system is a combination of an IoT formed using sensors
    and devices, image processing and machine learning (ML)/ deep learning (DL) techniques
    confined to the cultivation of fifteen varieties of beans found in India. The
    work involves two intelligent learning models configured to capture spatio-temporal
    attributes of image samples and sensor inputs and for real time discrimination
    between healthy and diseased bean leaves, detection of weeds growing around the
    cultivation land and also for process control. The first approach employs a DL
    structure named EfficientNetB7 along with a Bidirectional Long Short Term Memory
    (BiLSTM) while the second method adopts a VGG16 with an integrated attention mechanism.
    Also experiments have been carried out using benchmark ML classifiers like Support
    Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbor (KNN), Multi-Layer
    Perceptron (MLP) and Time Delay Neural Network (TDNN) combined with feature extraction
    techniques. Segmentation methods have been used to separate out the diseased sections
    of the leaves which are then used as apriori labels for the classifiers to reinforce
    the previously known details of the bean varieties. Subsequently, the trained
    networks are tested with bean leaf samples collected from cultivation farms. Results
    show that our proposed DL models could accurately predict the health state of
    the bean leaves with less computation time. With an automated approach of bean
    leaf health discrimination, weed detection and process control, the cost effectiveness
    of the overall effort is enhanced. Further, the sensor pack also provides precise
    thresholds at which water sprinkling could be initiated resulting in water conservation.
    Previous article in issue Next article in issue Keywords Artificial intelligenceConvolution
    neural networkLearning based systemSmart farming 1. Introduction Precision agriculture
    (PA) has become essential to increase productivity of traditional cultivation
    (Naik et al., 2022). Due to various factors like environment, pest and pathogens,
    soil etc. crop cultivation is severely affected which decrease their quality as
    well as the production quantity (Naik et al., 2022). It leads to severe financial
    loss and food crisis. In a recent report, it has been estimated that a sizeable
    percentage of the crop yields globally have been decreased due to pests and pathogens
    (CABI, 2022). Thus, it is has become essential that the farmers should be well
    equipped with the state-of-the-art (SOTA) technologies for proper monitoring of
    the health of the cultivated plants on regular basis. Artificial Intelligence
    (AI) in combination with Internet of Things (IoT) have become popular in agriculture
    due to several factors like continuous process monitoring, accurate control, quick
    decision making, automation, reliability etc(Jha et al., 2019). The use of AI
    tools is also beneficial since these are adaptive systems that can understand
    its surroundings, capture the relevant details and retain the knowledge to find
    solutions for the real world problems. AI is already triggering major transformations
    in the agriculture sector helping the farmers to cope up with several challenges
    like monitoring plant diseases, weed detection, identification and control of
    pest and pathogens, crop harvesting etc. with minimum loss of time (Jha et al.,
    2019). Further, AI approaches constituted by machine learning (ML) and deep learning
    (DL) tools provide enough capability to handle real life challenges and demonstrate
    the ability to generate human like decision making (Hapsari et al., 2022) in case
    of agriculture. Over the years researchers have attempted several approaches for
    accurate plant disease identification and detection. Use of ML methods as part
    of precision agriculture including plant disease detection (Pantazi et al., 2019)
    has been widely reported. ML techniques like Support Vector Machine (SVM), Random
    Forest (RF), K-Nearest Neighbor (KNN), Artificial Neural Network (ANN), Naive
    Bayes etc. have been extensively used (Rudagi et al., 2022) in agriculture. A
    technique for classification of rice disease has been developed using SVM like
    ML algorithms (Maione et al., 2018). Pantazi et al. have developed a system for
    detection of yellow rust disease in wheat leaves using hyper spectral imaging
    data and SVM technique (Pantazi et al., 2019). In another study, the authors (Binch
    and Fox, 2017) have reported a method for crop and weed detection using SVM technique.
    Three different wheat disease detection methods using Naïve Baye''s classifier
    have been reported by (Johannes et al., 2017). In another approach, an ANN model
    has been used (Johann et al., 2016) for crop monitoring and estimation of soil
    parameters. An ANN based system for weed detection using multispectral images
    captured by unmanned aircraft system (UAS) has been reported in (Kashefi et al.,
    2017). These ML approaches could provide satisfactory results but it has been
    observed that the efficiency of these models decline significantly as the size
    (Sibiya et al., 2019) and diversity (Sibiya et al., 2019) of the data increases.
    Moreover, the manual process of feature extraction in a ML model is much time-consuming
    and erroneous at times which also act as a limitation of this class of techniques
    (Goodfellow et al., 2016). Thus, to overcome these crucial limitations, the attention
    shifted towards the DL models as alternatives to ML methods (Singh et al., 2020)
    Further it is also a part of the global trend of replacing ML approaches with
    DL techniques. DL being a specialized subset of the ML techniques enables a computerized
    set-up to thrive upon larger volumes of data, use a complex stage of objective
    driven in built feature extraction process and catalyze a tunable classifier block
    to demonstrate the ability of superior decision making while handling real life
    situations (Goodfellow et al., 2016). Further, due to the availability of Graphical
    Processing Units (GPUs) and other high performance architectures that can process
    huge volumes of raw data without the traditional feature extraction stage dependent
    on human intervention, Sutaji and Yildiz (2022) DL supports self- sustaining and
    continuous learning. Popularly, DL methods consist of two approaches: one is to
    develop the whole model from scratch and the other option is the adoption of transfer
    learning (TL) (Goodfellow et al., 2016). The former method is time consuming and
    requires a huge amount of data to train the network which is not always accessible
    (Shah et al., 2022). Thus, the latter method started to become attractive. Lately,
    transfer learning has turned out to be a process where the model is trained on
    a huge dataset and the knowledge acquired during the training is stored for solving
    similar task (Geron, 2019) and passed on to networks configured for identical
    chores. It popularized the pre-trained models (PTMs) that utilize the concept
    of transfer learning and accelerated their applications in a range of real world
    scenarios. The PTMs are emerging as efficient and expandable frameworks in solving
    many image classifications and computer vision problems (Shah et al., 2022) including
    those relevant to precision agriculture and ecological informatics. The PTMs reduce
    the time and effort considerably and offer a higher learning rate during training
    since the models are already accustomed for similar assignments (Geron, 2019).
    With the increasing popularity of the DL architectures, Convolution Neural Network
    (CNN) – a common DL architecture, has been explored extensively for precision
    agriculture with special focus on efficient plant disease detection and classification
    (Akram et al., 2017) (Ferentinos, 2018). As DL architectures evolved with time,
    models like VGG16, ResNet, DenseNet etc. have been successfully used for plant
    disease detection with higher classification accuracies (CA) (Chen et al., 2020).
    The Residual Neural Network (ResNet) has been fruitful in addressing the notorious
    vanishing gradient problem of the DL networks (Tool et al., 2019) and has been
    applied extensively in pattern recognition problems (Tool et al., 2019). Other
    CNN structures like DenseNet, SqueezeNet, Xception etc. have also been commonly
    used for plant disease detection (Yadav et al., 2021). CNN architectures like
    AlexNet, GoogleNet etc. have been explored for disease classification of different
    vegetables like tomato, potato, lady''s finger, beans, spinach etc. (Yadav et
    al., 2021) (Pandey, 2022). As indicated above, there are many instances of different
    DL structures being adopted for accurate plant disease detection (Picon, 2019;
    Singh et al., 2020; Shah et al., 2022; Thi et al., 2022, Manjula et al., 2022).
    Another study reports a deep transfer learning method for Casava plant disease
    detection (Shah et al., 2022). Too et al. proposed a plant disease detection method
    using ResNet50, ResNet101 and InceptionV1 (Tool et al., 2019). Further, computer
    vision and ML/DL techniques have also been successfully explored in many plant
    disease detection applications (Traore et al., 2019), ResNet50 based classification
    of five crop diseases (Das et al., 2022), DenseNet and Inception nets for rice
    crop disease detection (Hapsari et al., 2022), papaya leaf disease detection using
    ResNet (Veeraballi, 2020), grape leaf disease detection using InceptionV1 and
    ResNetV2 (Xie et al., 2020), attention dense learning (ADL) mechanism for classification
    of leaf health conditions of non-identical plants (Akshay Pandey and Kamal Jain,
    2021) etc. In another study, an IoT based system has been reported for remote
    checking of agriculture parameters (Nicola and Pisana, 2021). Asmita et al. reported
    the use of ML and DL techniques integrated with IoT for crop monitoring (Asmita
    Hobisiyashi, 2022). Joshi et al. proposed a CNN based system for detection of
    disease in mungo bean leaves (Rakesh et al., 2021). Another study involves a CNN
    aided work for disease detection in peach crops (Yadav et al., 2021). Gokulnath
    and Usha (2021) have proposed a method for plant disease identification using
    LF-CNN. Tiwari et al. proposed a work on multi class plant disease detection and
    classification using leaf images with a dense CNN (Tiwari et al., 2021). A DL
    approach for tomato leaf disease detection has been reported which employs EfficientNetB1
    with different classifiers like RF and SVM (Chug et al., 2022). Sutaji and Yildiz
    (2022) have proposed MobileNetV2 and Xception models for prediction of plant diseases.
    Naik et al. have designed a squeeze and excitation based CNN model for chili leaf
    disease detection (Naik et al., 2022). Zan et al. have developed a CNN model named
    MatDet for tomato maturity detection and classification of tomato leaf diseases
    (Zan et al., 2022). Limei et al. have developed an R-CNN model for estimation
    of strawberry leaf scorch severity (Xia et al., 2022). Ali et al. have developed
    a DL based prediction model for plant disease identification (Hobisiyashi and
    Yadav, 2022). Olivia et al. have proposed a BLeafNet model for plant disease detection
    using leaf RGB images (Olivia et al., 2022). Mesut Togacar (2022) has proposed
    a method to detect weeds growing along with seedlings using DarkNet models and
    meta-heuristic algorithms. In another study, Bhagat et al. have proposed a method
    for plant leaf segmentation using Eff-UNet model (Bhagat et al., 2022). Olfa et
    al. (2023) proposed a DL based segmentation method for plant disease detection.
    Kaya et al. have proposed a multi head CNN model for identification of plant diseases
    using RGB images (Kaya et al., 2023). In another study, a DenseNet model is proposed
    for classification of maize disease (Wang et al., 2023a, Wang et al., 2023b).
    Vimal et al. have proposed a fine tuned CNN model for classification of beans
    plant leaf diseases (Vimal et al., 2023). The performances of these models have
    been evaluated using metrics like CA, precision, recall and F1 score. Several
    works also have used other metrics like sensitivity, specificity, type I and type
    II error, micro and macro F1 score to evaluate the performance of the models.
    The above works have discussed several dimensions related to precision agriculture,
    yet there are ample of opportunities to explore new approaches to develop effective
    solutions and lend a helping hand to the farming community. Further, there are
    sufficient scopes to formulate mechanisms for designing intelligent decision support
    and process control systems for more accurate detection and prediction of plant
    leaf health, weed detection, water sprinkling, conservation of resources, optimized
    use of chemicals for control of unwanted vegetation growth etc. Especially the
    improvements in performance that can be obtained by adopting innovative DL networks
    combined with computer vision methods and IoT set-ups in an integrated approach
    need to be explored. DL structures that can handle spatial and temporal variations
    in samples are expected to contribute towards better performance especially with
    real-time inputs. The CNN based structures are reliable in capturing spatial content
    of samples especially those from visual inputs but are inefficient in handling
    details with time varying characteristics (Goodfellow et al., 2016). For temporal
    attributes, recurrent structures like the long short term memory (LSTM) cells
    are considered to be efficient (Goodfellow et al., 2016). Especially, LSTMs configured
    in a two way processing mode called bi-directional LSTM (BiLSTM) are regarded
    to be competent in capturing sequences and temporal variations (Goodfellow et
    al., 2016). Moreover, integrated and AI aided frameworks with the ability to extract
    spatial and temporal variations while employed for plant leaf health monitoring,
    weed detection and water sprinkling as per continuously captured on-field sensor
    data have relevance not only for the farming community but also for ecological
    conservation. Here, we discuss the design of an approach that extracts both spatial
    and temporal attributes of samples using a set-up that combines multiple DL methods
    and an IoT arrangement for application in precision agriculture and especially
    configured for the cultivation of several varieties of beans found in different
    parts of India. The system is designed to determine the status of the bean cultivation
    by looking into the health of the leaves, execute varied parameter monitoring
    and process control and segregate weed growth from the crop plantation. The main
    constituent of the system is a DL block formed by a CNN based EfficientNetB7 with
    a BiLSTM structure and a VGG16 with an attention layer. Here, the EfficientNetB7
    and VGG16 capture the spatial contents while the BiLSTM and attention blocks deal
    with the temporal attributes of the input respectively. These two structures have
    been trained with images of fifteen varieties of bean leaves which are obtained
    from the “Beans” database. Further, these samples are augmented with the bootstrapping
    method and tested with actual samples collected during field visits to the bean
    cultivation farms. The performances of the networks are compared with that obtained
    from SVM, RF, KNN, Multi-Layer Perceptron (MLP) and Time Delay Neural Network
    (TDNN). Moreover, physiological attributes of the leaves are captured using features
    like Gray Level Co-occurrence Matrix (GLCM), Local Binary Pattern (LBP) and Local
    Binary Gray Level Co-occurrence Matrix (LBGLCM) while extracted sections (region
    of interest (ROI)) of the leaves in healthy and diseased forms are obtained using
    a few segmentation techniques including Fuzzy C-Means clustering (FCM) for extracting
    labels. These ROIs are used as targets during the training of the classifiers.
    The above ML/ DL methods are trained and the best approach is determined from
    a series of validation cycles. Further, on-field testing is carried out with samples
    collected using near infrared (NIR) camera. Subsequently, data from soil moisture
    sensor, temperature sensor and humidity sensor are connected to a processing node
    with WiFi access which forms an IoT pack for continuous monitoring. The system
    accepts camera and sensor inputs to provide discrimination decision regarding
    plant health, weed growth and also triggers process controls activities like watering
    the plants. The proposed DL models have consistently demonstrated accuracies around
    96%‐98% along with several statistical parameters indicating satisfactory levels
    of reliabilities. The performances of the proposed approaches are also compared
    with previously reported works in the area of precision agriculture and ecological
    informatics. Also, the computation complexity of the proposed models has been
    analyzed to validate the ability of the approaches in solving complex problems
    with better response times. The proposed systems can be integrated to UAS such
    as drones to monitor large patches of agricultural land. Further, an impact analysis
    of the work has been carried out by certain on-field experiments and the outcomes
    are discussed with linkages to ecological and resource conservation. The rest
    of the paper is categorized as follows: In Section 2, certain aspects related
    to the materials used and methodology adopted as part of the work have been discussed.
    Design details are covered in Section 3. Experimental results and discussion are
    presented in Section 4. Section 5 concludes the manuscript. 2. Materials and methodology
    Here we provide the details of the materials used and methodology adopted as part
    of the work including a short discussion on related considerations. 2.1. Related
    considerations Fifteen varieties of beans (kidney beans, black beans, cranberry
    beans, chickpeas, lima beans, soyabean, red beans, mung beans etc.) are grown
    in India which requires tropical and subtropical higher temperatures (20 to 30
    degree centigrade), humidity for plant growth with fruiting during winter and
    takes around three months from germination. It supplies green pods through winter
    and spring in loam soil but grows well in alkaline and saline soils tolerating
    pH value up to 8.5. Normalized Difference Vegetation Index (NDVI) is used to quantify
    the health of the bean leaf. The NDVI is a measure to find the amount of green
    vegetation a land contains (Vido et al., 2020). It works on the principle that
    healthy vegetation reflects more of visible light compared to unhealthy vegetation.
    NDVI can be calculated using Eq. (1) (1) For continuous monitoring of plants,
    several sensors are required that are connected and operated in a uniform platform
    constituting an IoT. IoT is an arrangement of certain devices and sensors that
    collect, processes and transmits the data to another arrangement through the Internet
    or using any other network (Almadhor and Rauf, 2021). With an IoT arrangement,
    ML/DL components can be integrated for constituting an efficient decision making
    system. A simple arrangement of an IoT network connected with a DL based decision
    support system (DL-DSS) and process control is shown in Fig. 1. Download : Download
    high-res image (324KB) Download : Download full-size image Fig. 1. IoT based platform
    connected with DL-DSS and process control. Image segmentation is an important
    aspect of the work used to extract the ROIs from the samples. This work adopts
    K-Means clustering (KMC), Fuzzy C-means clustering (FCM) and Region Growing methods
    (Goodfellow et al., 2016) along with feature extraction to capture physiological
    attributes of the bean leaf samples. Feature extraction methods like GLCM, LBP
    and LBGLCM have been explored for studying the leaf samples (Ramesh et al., 2018).
    GLCM studies the spatial relationship among the pixels and is a second order statistical
    texture analysis method (Ramesh et al., 2018). A schematic of the feature extraction
    process using GLCM has been depicted in Fig. 2. Download : Download high-res image
    (261KB) Download : Download full-size image Fig. 2. Process of feature extraction
    necessary for the classifiers. Using GLCM, we have analyzed a set of features
    like energy, homogeneity, contrast and correlation which are useful in obtaining
    the details of the texture of the input images (Ramesh et al., 2018). The LBP
    features describe the statistical and structural model of an image. Further, the
    LBP and GLCM features are combined to generate the LBGLCM feature which is helpful
    in studying the composition of an image. Energy, Homogeneity, Contrast, Entropy
    and Correlation are some of the attributes that are captured by the combined LBP
    and GLCM feature set (Ramesh et al., 2018). Some of the benchmark classifiers
    used is CNN, SVM, MLP, TDNN, RF, etc. Further, we have used certain PTMs like
    Resnet152, InceptionV3, MobileNetV2 and proposed the EfficientNetB7 with BiLSTM
    and VGG16 with an attention mechanism that also have the abilities to carry out
    efficient disease detection, weed identification and process control. 2.2. Proposed
    methodology The block diagram of the complete work is summarized in Fig. 1. As
    already indicted above, the main constituent of the system is a DL block formed
    by two separate frameworks. The first one is formed using a CNN based EfficientNetB7
    with a BiLSTM network and the second one consists of a VGG16 with an attention
    layer. These two structures have been trained with fifteen varieties of bean leaves.
    A sizeable portion of the samples are obtained from the “Beans” database (Makerere
    AI Lab, January 2020) which is augmented with bootstrapping method. The performances
    of these networks are compared with that obtained from SVM, RF, KNN, MLP and TDNN
    classifiers. The performances of each of these classifiers are compared for ascertaining
    the most effective method for performing the stated objectives. Further, physiological
    attributes of the bean leaves are captured using features like GLCM, LBP and LBGLCM
    while extracted sections of the leaves in healthy and diseased forms are obtained
    using image segmentation. Fuzzy clustering is used for obtaining apriori labels
    for the classifiers. After these discrimination methods are trained, the best
    approach is determined from a series of validation cycles and then on-field testing
    is carried out with samples collected using NIR camera. The system accepts camera
    and sensor inputs to provide discriminator decision regarding plant health, weed
    growth and also triggers process controls activities like watering the plants
    at regular intervals of time. The proposed DL models have consistently demonstrated
    accuracies around 96%–98% along with several statistical parameters indicating
    satisfactory levels of accuracy. As already reported, a DL-DSS has been designed
    for real time monitoring of the health of the crops, weed detection and process
    control using AI and IoT as shown in Fig. 3. Download : Download high-res image
    (227KB) Download : Download full-size image Fig. 3. Expanded depiction of the
    proposed approach of precision agriculture. Leaf images of different bean species
    have been collected during visits to cultivation farms. A few of them are shown
    in Fig. 4. About 600 leaf images of different bean species have been captured
    by the NIR camera during visits to cultivation farms. Data bootstrapping techniques
    have been used to increase the number of training samples. This process enhances
    the learning efficiency of the DL network and other classifiers. Further, KMC,
    FMC and Region Growing methods have been used to extract and analyze the diseased
    region of the non-healthy bean leaves. Feature extraction has been performed using
    GLCM and LBP which captures the texture of an image through its pixel values.
    Features like energy, homogeneity, contrast, correlation etc. are obtained with
    the help of the GLCM matrix. These features are then used to study the physiological
    attributes of the leaf images. Classification is performed using ML classifiers
    like SVM, RF, KNN, MLP and TDNN along with the two proposed deep learning models
    EfficientNetB7-BiLSTM and VGG19 with attention for real time monitoring of the
    crop plants. Only the ML classifiers use the features extracted manually. In case
    of the DL models, the manually extracted features supplement the inbuilt feature
    extraction mechanism. Further, as the system is trained to discriminate between
    bean plants and weeds, it contributes towards demarcation of actual and wasteful
    growth. The IoT based framework continuously gathers data regarding the health
    of the plants and other environmental conditions. The collected information is
    sent to the cloud for storage in a database which can be easily accessed by the
    farmers through mobile app. Download : Download high-res image (356KB) Download
    : Download full-size image Fig. 4. Data augmentation performed to increase the
    number of training samples. The NDVI values are calculated for both healthy and
    non-healthy bean leaf samples. The NDVI value of a healthy leaf is found to be
    0.91 whereas of non-healthy i.e. diseased leaf, the value is 0.41. A schematic
    of this processing is shown in Fig. 5. Download : Download high-res image (309KB)
    Download : Download full-size image Fig. 5. Steps of leaf image processing and
    classification using ML classifiers. In the subsequent sections, the details of
    each of the proposed methods are presented. 3. Design details of the proposed
    models The main constituent of the system is a DL block formed by two separate
    frameworks. Each of these two frameworks is discussed in details. 3.1. Proposed
    DL Model I (EfficientNetB7 with a BiLSTM) This model employs an EfficientNetB7
    with a BiLSTM for performing operations like deep feature extraction, multi-class
    image classification using segments of bean leaf samples extracted from a frame,
    detect weed growth and also trigger process controls activities like watering
    the plants at regular intervals of time. This model has three modules: pre-processing,
    deep feature extraction and multi-class classification as shown in Fig. 6. Download
    : Download high-res image (263KB) Download : Download full-size image Fig. 6.
    Architecture of the proposed model I. 3.1.1. EfficientNet The EfficientNet is
    trained on the ImageNet database that contains about 15 million labeled images
    of different categories (Thi et al., 2022). This network uniformly scales the
    three most important parameters, the depth, width and resolution through a compound
    coefficient. The EfficientNet has been effectively used in image processing task
    as a SOTA technique (Thi et al., 2022). The number of layers in any network is
    determined by the requirements of a given complex problem. The EfficientNet has
    different baseline networks starting from B0 to B7. With a layer of 237 B0 baseline
    network has the lowest number of layers whereas the B7 network has the highest
    number of layers 837 (Thi et al., 2022). As already mentioned, the EfficientNet
    B7 captures the spatio attributes of the samples. 3.1.2. BiLSTM A BiLSTM model
    consists of two LSTMs: one taking the input in the forward path and the other
    in the backward flow (Yang et al., 2019) while facilitating processing in both
    directions. LSTM is a kind of Recurrent Neural Network (RNN) where the output
    from the last step is fed as input to the current step. LSTMs are much preferred
    over RNN due to their ability to grasp the vanishing gradient problem and process
    samples with temporal attributes (Goodfellowet al. 2016; Yang et al., 2019). The
    LSTM has a structure containing four learning layers and different memory blocks
    called cells. The structure consists of three gates namely: forget gate, input
    gate and output gate. Each gate performs its unique function. The inputs are fed
    to the network through the input gate which plays the role of adding useful information
    using appropriate function and preserving the relevant information. The forget
    gate discards all the unwanted information and finally, the output gate retains
    all the relevant information from the current cell state, which is then send to
    the next cell after point wise multiplication. This way the input flows in one
    direction only in a LSTM network. This process is not always suitable for certain
    classification tasks which require both forward and backward contextual relationship
    for efficiency (Goodfellow et al., 2016; Yang et al., 2019). Thus, to overcome
    this limitation, a single layer stack of BiLSTM is used in our work which consists
    of two distinct hidden layers to model the input in both the forward and backward
    direction. The BiLSTM layer deals with the temporal attributes of the samples,
    retains and circulates the contextual content throughout the network. The output
    of the BiLSTM, then passes through the fully connected (FC) layers and finally
    moves to the softmax classifier at the output layer. 3.1.3. Working of the proposed
    Model I The input images are first normalized and resized to a fixed dimension
    of 224 × 224 × 3 to reduce computation complexity. These input images are then
    fed to the EfficientNetB7 where each of the input images is processed through
    a stack of 813 layers followed by a BiLSTM layer. The EfficientNetB7 then generates
    an output of feature vectors of 7 × 7 × 2560 dimensions for each image. These
    feature vectors are fed as input to the BiLSTM network to execute the training
    and extract the spatio-temporal attributes of the samples. Thereafter, the flattening
    layer is added to convert the vectors to a 1-dimensional vector. A dropout layer
    is added immediately after the flattening layer with a dropout rate of 30% to
    prevent the model from over fitting. While we have experimented with various dropout
    rates such as 10%, 20%, 30%, 40%, efficient results have been obtained with a
    dropout of 30% (value = 0.3). The speed-up of the computation of the EfficientNetB7
    is attributed to the optimized structure and learning achieved using the dynamic
    architectural trimming which is facilitated by this dropout mechanism. The next
    layer is the dense layer consisting of 4096 neurons with Rectified Linear Activation
    (ReLU). Finally, the network has the softmax layer with three neurons representing
    three different classes (angular leaf spot, bean rust and healthy). The training
    is supervised and is carried out using the apriori labels obtained from the segmented
    ROI''s of the bean leaves. The configuration of the proposed model is provided
    in the Table 1. Table 1. Details of network configuration and parameters of the
    Proposed Model I. Layer Type Size Layer 1 Input Layer 500 × 500 × 3 Layer 2 Resize
    Layer 224 × 224 × 3 Layer 3 EfficientNetB7 Layer 7 × 7 × 2560 Layer 4 Bidirectional
    LSTM 256 Layer 5 Flatten 5632 Layer 6 Dropout layer (value = 0.3) 4096 Layer 7
    Fully Connected (Dense) 4096 Layer 8 Softmax Layer 3 Total Parameters: 215765402
    Trainable Parameters: 64097687 Non- Trainable Parameters: 151667715 3.2. Proposed
    Model II (VGG16 with an attention layer) The second proposed model is based on
    a pre-trained CNN named VGG16 integrated with an attention mechanism. The attention
    mechanism has been integrated at each stage of the network for learning of the
    class features with greater focus so as to enhance the feature awareness and ensure
    spatio-temporal extraction of the relevant details. The schematic is shown in
    Fig. 7. Download : Download high-res image (305KB) Download : Download full-size
    image Fig. 7. Architecture of the Proposed Model II. 3.2.1. VGG16 The VGG16 with
    16 layers is trained on the ImageNet dataset (over 15 million images of 1000 different
    classes) (Ashish et al., 2020). The architecture of VGG16 consists of small convolution
    filters with each hidden layers using ReLU activation function. Finally, the network
    works with three fully connected layers where the first two layers have 4096 channels
    and the third layer has 1000 channels for each class. In our work, the VGG16 has
    been integrated with multiple attention layers each reinforcing the class details
    allowing effective training to take place. The attention layer is required to
    ensure precise focus on specific parts of a sequence when processing a large assortment
    of data. In DL, the use of attention mechanism helps the network in remembering
    long sequences of data for a longer period of time (Goodfellow et al., 2016).
    The illustration of the attention mechanism is shown in Fig. 8. Download : Download
    high-res image (321KB) Download : Download full-size image Fig. 8. Illustration
    of the attention mechanism. In Fig. 5 [X] is the input, [Y] is the output, Yt
    is the target (apriori ROI segmented out of the leaf samples) and μ1, μ2 …μn are
    the alignment scores. There are two rows of LSTM cells which are grouped with
    processing sequence going from the forward to the backward direction and vice-versa.
    The output of these two channels of LSTM cells are combined and weighted with
    alignment scores which are then compared with the target vector Yt. The difference
    or error vector drives the learning process of the attention mechanism and a gradient
    descent process is updated till the goals are met. 3.2.2. Implementation of the
    proposed Model II Similar to the proposed Model I, here also the input RGB images
    are normalized to reduce the computational complexity of the system and then resized
    to a fixed dimension of 224 × 224 × 3. Next, these pre-processed images are passed
    through the usual layers of the VGG16 network having 13 convolution layers and
    three fully connected layers. At the indicated stages, the attention mechanisms
    are implemented for detailed learning of the class features and embedding these
    into the training process of the network. This way identical numbers of attention
    layers for the three classes are implemented. Finally, the classifier section
    is put in place which is constituted by the softmax activation function with three
    neurons. The configuration of the proposed model is shown in Table 2. The softmax
    activation function is defined as follows: (2) Table 2. Details of network configuration
    and parameters of the Proposed Model II. Convolution Block Type Size Input Resize
    Image 224 × 224 × 3 Block 1 Convolution Convolution Pooling conv3–64 conv3–64
    Block 2 Convolution Convolution Pooling conv3–128 conv3–128 Block 3 Convolution
    Convolution Convolution Pooling conv3‐256 conv3–256 conv3––256 Block 4 Convolution
    Convolution Convolution Pooling conv3‐512 conv3–512 conv3––512 Block 5 Convolution
    Convolution Convolution Pooling conv3‐512 conv3–512 conv3––512 Fully Connected
    Layers Layer 1 Layer2 Layer3 4096 4096 1000 Output Layer Softmax 3 Total Parameters:
    138,357,544 Trainable Parameters: 138, 357, 544 Non-Trainable Parameters: 0 The
    learning schedule is extended to a few more classifiers which are used for performance
    benchmarking. 3.3. Experimental environment and computation The proposed models
    have been implemented in Python using Keras application programming interface
    (API) that runs on top of Google''s Tensor Flow open source library (Geron, 2019).
    The Google Colab integrated development environment (IDE) is used for writing
    and implementing the Python codes for the proposed deep learning models. A computer
    system with Intel i7 processor and 16 GB RAM has been used as the host setup.
    3.4. Design of an IoT based framework for on-field testing The proposed framework
    comprises of temperature, humidity and soil moisture sensors for gathering the
    required information contributing to the productivity of the farming land. These
    sensors are then directly connected with the Arduino UNO board. An application
    has been designed using the Blync app that sends the collected information to
    the cloud stage for further investigation. Further, we have used ThingSpeak platform
    for data presentation and analysis for visualization of real data captured using
    the IoT platform. Schematic of the proposed framework is depicted in Fig. 9. Download
    : Download high-res image (117KB) Download : Download full-size image Fig. 9.
    IoT framework. The sensors are connected to the Arduino board and the output is
    directly forwarded to the server, which is then saved automatically in the database
    of the app. The DHT11 sensor is used to obtain the temperature and humidity values.
    Moreover, since our proposed system is designed to work in hot and humid conditions,
    this sensor is a proper choice to obtain the temperature details. The soil moisture
    sensor helps in moisture sensing by measuring the water content in the soil. The
    sensor has advantages like anti-rusting property and has a long power life. The
    block diagram of the proposed IoT framework for an intelligent farming system
    is shown in Fig. 10. Download : Download high-res image (315KB) Download : Download
    full-size image Fig. 10. Block diagram showing the connections of the different
    sensors forming the IoT set-up. The Arduino UNO collects the real-time data from
    different sensors and integrates them enabling automated decision making and control
    of the motors driving the processes whenever required. LEDs are used to indicate
    the working status of the sensors. The red LED placed near the sensor indicates
    any technical fault in the system whilethe green LED indicates the working condition
    of the system. The data gathered from the sensors is sent to the ThingSpeak platform
    and the mobile app. The flowchart of the proposed system is shown in Fig. 11.
    For testing our proposed system, we visited a few bean cultivation farms and gathered
    leaf samples of fifteen different bean species found in India like kidney beans,
    black beans, cranberry beans, chickpeas, lima beans, soyabean, red beans, mung
    beans etc. It was observed that our proposed systems can efficiently discriminate
    between healthy and non-healthy bean leaves as well as accurately detect common
    bean diseases like bean rust and angular leaf spot in the leaves. Further, the
    system is also able to predict environmental parameters and identify land areas
    having weeds and cultivable land. Download : Download high-res image (436KB) Download
    : Download full-size image Fig. 11. Flowchart highlighting the working of the
    real-time monitoring and process control system. 3.5. DSS for weed control The
    proposed system is trained to detect weed and bean crop. Initially we took samples
    of the weed and the bean cultivation land and labeled them. The labeled samples
    are used to train the classifier (VGG-16) to identify the weed area and crop section.
    This ability of the classifier will help the farmer to mark the region of the
    plot of land where weed removal measures can be taken. It has been recorded that
    at least 37 types of weeds grow around fifteen types of bean cultivations in India.
    Weeds suck the nutrients of the soil making it infertile. Hence, proper weed detection
    is a crucial attribute of the proposed system. It contributes towards lowering
    of accidental mechanical cutting of bean plants assuming to be weed growth. Further,
    with proper identification of wild growth, control spreading of weed removing
    chemicals can be carried out which protects the fertility of the land and also
    saves the bean leaves from damage. 3.6. Fertilizer and disinfectant spreading
    The IoT-DSS has been programmed to demonstrate the spreading of fertilizer and
    disinfectant at multiple locations where and when required. This process is expected
    to assist the farmers as a measure to pest control for healthy cultivation. 4.
    Experimental results A series of experiments have been performed to validate the
    work and check the reliability of our proposed system. The experimental parameters
    are summarized in Table 3. Initial steps like noise removal, normalization, resizing
    have been performed as a part of the pre-processing procedure. After pre-processing,
    feature extraction using GLCM, LBP and LBGLCM has been carried out. Features like
    homogeneity, energy, contrast, correlation and entropy are considered to analyze
    the texture of the processed images. The GLCM analysis the statistical measures
    from the images. Different classifiers like SVM, TDNN, RF, MLP and KNN have been
    used for image classification. Further, we created a graphical user interface
    (GUI) for carrying the required experiments. After image pre-processing, as shown
    in Fig. 12, segmentation has been performed using FCM and KMC techniques. Two
    important parameters namely Intersection over Union (IOU) (Eq. (3)) and Pixel
    Accuracy (PA) (Eq. (4)) are utilized to establish the reliability of our system
    spatial domain. IOU is an important measurement that indicates how accurately
    the system performs while pixel accuracy is calculated to observe the number of
    correctly classified pixels. Further, the segmentation accuracy (Acc) is also
    calculated through pixel to pixel match. Table 4 shows the results (IoU, Acc and
    PA) obtained from FMC, KMC and Region Growing segmentation methods when extracting
    ROIs using SVM, RF, KNN, MLP and TDNN. It can be clearly observed that FMC-TDNN
    is the most reliable combination in terms of Acc, IOU and PA. (3) (4) Table 3.
    Experimental Parameters. Description Type Pre-processing Noise removal, normalization,
    resizing Segmentation FCM, KMC, Region Growing Classifiers SVM,RF,MLP,TDNN and
    KNN Parameters Bean leaf images, NDVI, temperature, humidity, soil moistureand
    air quality Data Used Training set of 10,000 mixed bean leaf images Download :
    Download high-res image (294KB) Download : Download full-size image Fig. 12. Operations
    of pre-processing on the (a) diseased leaf, (b) segmentation, (c) edge detection
    and (d), (e), (f) selection of region of interest (ROI). Table 4. Summary of IOU,
    Acc and PA obtained using FMC, KMC and Region Growing clustering in percentage
    (%). Method SVM RF KNN MLP TDNN Acc IOU PA Acc IOU PA Acc IOU PA Acc IOU PA Acc
    IOU PA FMC 84 82 83 84 84 85 83 82 83 87 82 83 92 89 90 KMC 84 81 82 83 83 84
    83 81 83 86 81 83 93 88 89 Region Growing 83 81 82 83 82 82 83 82 82 84 82 81
    89 86 87 Table 5 shows the segmentation accuracy achieved using three different
    feature sets. It can be observed that the GLCM-SVM and GLCM-TDNN gives the best
    results but as TDNN is trainable and robust under varied conditions, the GLCM-TDNN
    combination is used for benchmarking. Table 5. Summary of the segmentation accuracy
    obtained from different feature sets. Method SVM (%) RF(%) KNN(%) MLP(%) TDNN(%)
    GLCM 84 84 83 87 89 LBP 83 84 82 88 88 LBGLCM 83 83 81 89 90 From the experiments,
    a few statistical parameters are obtained and the results are summarized in Table
    6. The results have been derived using GLCM features and FCM segmentation techniques.
    It has been observed that while the learning based methods provide reliability
    still the performance cannot be extended beyond certain limits. Further, with
    diversity in the content, the reliability suffers, which clearly indicates the
    limitations of ML based approaches. Hence, a set of experiments have been carried
    out using DLbased methods. Table 6. Summary of Performance Metrics of Different
    Classifiers. Model CA Precision Recall Specificity Type I Error Type II Error
    F1 Score Micro Avg Macro Avg TDNN 88% 0.88 0.87 0.89 0.17 0.20 0.86 0.88 0.88
    MLP 86% 0.86 0.85 0.88 0.19 0.22 0.86 0.86 0.86 KNN 76% 0.76 0.75 0.80 0.24 0.26
    0.76 0.76 0.75 RF 78% 0.78 0.78 0.84 0.22 0.21 0.78 0.78 0.78 SVM 79% 0.79 0.79
    0.82 0.21 0.22 0.78 0.79 0.79 Table.7 summarizes the results obtained using existing
    PTMs. It can be observed that MobileNetV2 has higher classification accuracy(CA)
    in comparison with other PTMs InceptionV3, ResNet152 etc. Table 7. Performance
    Metrics using some existing PTMs and proposed models. Models CA Precision Recall
    Specificity Type I Error Type II Error F1 Score Micro Avg Macro Avg ResNet152
    92% 0.92 0.91 0.94 0.02 0.22 0.92 0.91 0.92 InceptionV3 90% 0.90 0.90 0.92 0.05
    0.24 0.90 0.89 0.90 MobileNetV2 93% 0.93 0.93 0.93 0.06 0.36 0.93 0.93 0.92 Proposed
    Model I 96% 0.96 0.95 0.95 0.04 0.08 0.96 0.95 0.95 Proposed Model II 98% 0.98
    0.97 0.96 0.04 0.09 0.98 0.97 0.98 Fig. 13 shows the accuracy achieved during
    network training and model loss suffered by the Proposed Model I (EfficientNetB7
    with BiLSTM). The performance has been achieved in 100 epochs which indicates
    the framework is computationally efficient in completing the training in less
    amount of time. It has been observed that initially the training starts with some
    fluctuations but reaches satisfactory performance levels without taking high numbers
    of epochs. During the first 20 epochs, the accuracy of the model with both train
    and test data reaches the mid 80''s and 90''s in percentage range. This clearly
    indicates the proposed model EfficientNetB7 with BiLSTM has capacity of fast processing.
    The epochs are further extended to see where the model''s performance saturates.
    It was observed that beyond 20 epochs, the performance shows no major improvement.
    This plot is obtained from average values of hundred trials using training and
    testing data. Download : Download high-res image (256KB) Download : Download full-size
    image Fig. 13. (a) Training Accuracy and (b) Model Loss graphs of Proposed Model
    I (EfficientNetB7 with BiLSTM). Another set of graphs has been obtained for the
    Proposed Model II (VGG16 with attention layer) when subjecting it to a series
    of training cycles. The average performance derived in terms of accuracy and model
    loss for 100 epochs is shown in Fig. 14. It can be observed clearly that the accuracy
    values attain stable state beyond 5 epochs. The accuracy performance reach mid
    80''s and crosses into the 90''s percentage range beyond 10 epochs. Same is the
    case with the model loss graph. The model loss saturates beyond 20 epochs. Download
    : Download high-res image (211KB) Download : Download full-size image Fig. 14.
    (a) Training Accuracy and (b) Model Loss graphs of Proposed Model II (VGG16 with
    attention layer). Table.8shows the computation load associated with our proposed
    models and the PTMs that are used as benchmark methods. The performance associated
    with 100 epoch cycles has been presented in Table 8. It has been found that EfficientNetB7
    with BiLSTM takes about 1100 s to complete with 11 s per epoch, which is over
    35% better compared to the VGG16 with attention layer based approach. Compared
    to Resnet152, the proposed model I is computationally two times less demanding
    and compared to MobileNetV2 and InceptionV3 it is about 2.18 and 2.54 times more
    efficient respectively. This clearly indicates the advantage of the EfficientNetB7
    based method. The specific model parameters of both the approaches are shown in
    Table 9. With learning rate of 0.001, Adam optimizer, 30% dropout rate and batch
    size of 32, categorical cross entropy used as the loss function to run training
    cycles of 100 epochs, the proposed approaches appear reliable, robust and efficient.
    Table.10 shows the comparison of our proposed models with existing SOTA models
    using leaf samples of different vegetables. Here also we can clearly observe that
    our proposed approaches performed fairly well in comparison with other methods.
    Table 8. Computation Time of the Proposed Models and other PTM. Model Epochs Computation
    Time (sec) Computation time per step (msec) Proposed Model I (EfficientNetB7 with
    BiLSTM) 100 11 s per epoch 72 ms Proposed Model II (VGG16 with attention) 100
    17 s per epoch 88 ms ResNet152 100 22 s per epoch 122 ms MobileNetV2 100 24 s
    per epoch 142 ms InceptionV3 100 28 s per epoch 288 ms Table 9. Model Parameters:
    Train-Test Split ratio: 80:20 Learning rate: 0.001 Choice of Optimizer: Adam Loss
    Function: Categorical Cross Entropy Dropout rate: 30%, 40%, 50% Epochs: 100 Batch
    size: 32 Table 10. Comparison of our proposed models with SOTA models applied
    in precision agriculture of several crops with leaf taken as input. Model Leaf
    Classification accuracy (CA) f1-score EfficientNetB7 with KNN, RF (Chug et al.,
    2022) Tomato 88% 0.87 Inception,ResnetV2(Singh et al., 2020) Multiple crops 70%
    0.70 InceptionV1,ResNet50 (Xie et al., 2020) Grape 78% 0.79 ResNet50 (Veeraballi,
    2020) Papaya 85% – Naïve''s Bayes (Sahoo et al., 2020) Maize 77% – Proposed Model
    I (EfficientNetB7 with BiLSTM) Bean 96% 0.96 Proposed Model II (VGG16 with attention)
    Bean 98% 0.98 Table.11 shows the comparison of our proposed models with existing
    SOTA techniques using only bean leaves. It can be clearly observed that our proposed
    approaches demonstrate 1–7% accuracy improvements compared with the other existing
    methods. Same is the case with F1-score. The confusion matrix obtained from the
    original Beans dataset using proposed model I and II are shown in Fig. 15. Table
    11. Comparison of our proposed models with SOTA models with Beans dataset. Model
    Leaf CA f1-score VirLeafNet1(Rakesh et al., 2021) Bean 91.23% – AlexNet, GoogleNet
    (Chen et al., 2020) Bean 94% 0.93 CNN(Himadriet al., 2022) Bean 93% – GoogleNet
    (Amit et al., 2021) Bean 95% 0.95 MobileNetV2(Recep and Lahcen, 2022) Bean 92%
    – Proposed Model I (EfficientNetB7 with BiLSTM) Bean 96% 0.96 Proposed Model II
    (VGG16 with attention) Bean 98% 0.98 Download : Download high-res image (84KB)
    Download : Download full-size image Fig. 15. Confusion Matrix of (a) proposed
    model I and (b) proposed II. Proposed model I is able to classify diseased and
    healthy leaves efficiently with an accuracy of 96% with an F1-score of 0.96 and
    proposed model II could successfully discriminate between diseased and healthy
    leaves with an accuracy of 98% with an F1-score of 0.98 respectively. Further,
    the two proposed approaches perform better than a number of PTMs like ResNet152,
    MobileNetV2 and InceptionV3 as shown in Table.7. 4.1. Performance comparison with
    existing SOTA models The performance of the proposed models has been compared
    with the existing SOTA models to understand how well our system performed in accurate
    detection of plant diseases. Comparison of performance of our proposed approaches
    has been shown in Table.10. Table.11 summarizes the comparison results of the
    existing models in literature employed for image based plant disease detection
    with our proposed models using only Bean leaves. The performance of the system
    designed using the EfficientNetB7 with BiLSTM and VGG16 with an integrated attention
    mechanism have been compared with that obtained using SVM, RF, KNN, MLP and TDNN.
    Subsequently, the trained network has been tested on real field samples collected
    using an IoT based approach which also monitors temperature, humidity and soil
    moisture. Further, experiments have also been performed using some existing CNNs
    like ResNet152, MobileNetV2 and InceptionV3. Results show that our proposed DL
    models could classify healthy and diseased bean leaves with accuracy of 96% and
    98%respectively using less computation time. Further, our proposed models outperform
    some existing models in literature like VirLeafNet (Rakesh et al., 2021), DADCNN-5
    (Akshay and Kamal, 2022) and R-CNN (Zan et al., 2022) that have been employed
    for classification and identification of plant leaf diseases and are related to
    ecological informatics. Furthermore, the average processing time of a single bean
    leaf image is 0.011 s demonstrated by our proposed approach which is less than
    the processing time utilized by some existing models for plant disease detection
    (Rakesh et al., 2021) and (Ramesh et al., 2018). 4.2. Impact analysis and discussion
    The impact analysis of the proposed approach is presented in terms of reliability
    of identification of bean leaf health, weed detection and water sprinkling compared
    to the tasks executed by a few human volunteers (of three different experience
    categories). An area of 20 ft × 20 ft with about 400 bean plants nurtured in certain
    number of rows is considered for the study. Each row has a sensor pack consisting
    of air quality sensor (MQ135), temperature and humidity sensor (DHT11) and soil
    moisture sensor connected to a few Arduino UNO boards which are linked up to a
    host computer using Wi-Fi access. Further, a NIR camera with a Wi-Fi module is
    placed over a slider arrangement laid at a height of 5 ft that can be mechanically
    moved over each row. This arrangement (sensor pack and the NIR camera) is used
    as the data capture block to feed samples to the two proposed approaches and subjected
    to performance evaluation for identification of bean leaf health, weed detection,
    ascertaining air quality and soil moisture condition and initiation of water sprinkling.
    The above performances are compared with that obtained using human volunteers
    (of three different experience categories). The first category is a volunteer
    with about one year bean cultivation experience with knowledge of the requirements
    but with temporary involvement with the effort. The second and third categories
    of volunteers have adequate knowhow about bean cultivation, have been involved
    continuously with the process for over two years and are regularly associated.
    The data have been compiled over a period of seven days and is summarized in Table.12.
    Table 12. Summary results of performance evaluation of the proposed system in
    terms of accuracy compared to human observers. Sl no. Method Accuracy in % Bean
    leaf health identification Weed detection Air quality assessment Soil moisture
    assessment Water sprinkling 1 Proposed approach 1 95 95 94 94 94 2 Proposed approach
    2 96 96 95 95 96 3 MobileNetV2 (Recep and Lahcen, 2022) 92 93 92 92 92 4 GoogleNet
    (Amit et al., 2021) 94 93 93 93 93 5 Human with 1 year experience 58 52 50 55
    61 6 Human with 2 years'' experience 63 65 62 64 63 7 Human with 5 years'' experience
    80 79 71 77 79 During this period, the observation accuracies notched by the proposed
    approaches and three persons of different bean cultivation experiences in case
    of bean leaf health identification, weed detection, air quality assessment, soil
    moisture estimation and water sprinkling have been recorded. A factor that appears
    to be obvious is the fact that continuous monitoring of agricultural produce is
    strength of automation frameworks and is not convenient for human beings. While
    bean leaf health identification and weed detection at microscopic level can be
    meticulously carried out by the proposed sensor pack integrated to the AI aided
    decision support system, the same turns out to be highly repetitive and tedious
    tasks for a human where errors are likely to take place. This is established by
    the fact that human errors are between 37% and 15% when compared with the proposed
    approach 1 while it is between 38% and 16% for the proposed approach 2. The most
    experience human volunteer notches up average accuracies of around 80% which is
    at least 15% less than the AI based methods. With lower errors in monitoring and
    timely human intervention, the possibility of rise in productivity and decrease
    in financial involvement are the logical spinoffs despite the fact that an automated
    system is likely to suffer breakdown at times and are constrained by availability
    of uninterrupted electricity supply, hermetically tight packaging and flexible
    deployment to prevent damage of the components due to environment factors etc.
    While bean leaf health identification is crucial for better productivity of a
    plot of land, more important is early detection of the disease affecting the plants
    and timely intervention. An early detection is possible by the application of
    the proposed approaches and timely measures to reduce the loss can be initiated.
    The monitoring and detection process can be executed in a continuous manner with
    very little human involvement except while initiating the measures to use medicines
    to reduce the leaf diseases. Similarly, weed growth can be a major challenge for
    the cultivators during the initial and subsequent periods as it perennially threatens
    to encroach on the resources earmarked for the bean farming Togacar, 2022). Weeds
    are unwanted growth that invades the cultivation land and damage the food crops.
    Moreover, accurate detection of weed and demarcation of such areas can lower contamination
    due to unregulated spraying of chemicals for weed control and prevent damage to
    the fertile land. Further, precise demarcation of weed filled areas can also help
    the farmers to use mechanical means of weed cutting which can prevent damage to
    the bean cultivation and the fertile land due to use of chemicals for removal
    of the unwanted vegetation growth. This is pertinent due to the fact that over
    37 different types of weeds like pig weed, crab grass, goose grass etc. are observed
    around bean cultivation areas in India. Weeds always threaten to deprive the bean
    crops from adequate amount of sunlight, nutrients, water (Togacar, 2022). Moreover,
    many weeds are also host of plant disease organisms (Gokulnath and Usha, 2021).
    Hence an automated approach for weed detection is an essential tool for the farmers.
    With an automated approach of bean leaf health identification and weed detection,
    the volume of irrelevant manpower required can be reduced which increases the
    cost effectiveness of the overall effort. In addition to the above, the execution
    time is short and the decision making of the proposed approaches is fast and reliable
    which is not possible in all cases to be matched by personnel employed for the
    purpose. The sensor pack provides precise thresholds at which water sprinkling
    could be initiated. It helps in preventing water wastage as well. The accurate
    timings and amount of water sprinkling might be not always maintained with human
    involvement. In view of the above, the role of an AI assisted agriculture system
    configured for bean cultivation working in complementary supplementary roles to
    the human cultivator is widespread and its impact on the cultivation process and
    the surrounding ecology shall be far-reaching. 5. Conclusion The proposed system
    is a combination of IoT devices, image processing techniques and ML/DL based systems
    applied as part of a precision agriculture setup related to several varieties
    of bean species found in India. In this work, we have proposed two DL models for
    real time detection of healthy and non-healthy (diseased) bean leaves, weeds growing
    around the cultivation land and monitoring of related environment parameters and
    controlled sprinkling of water. The first approach uses EfficientNetB7 along with
    a BiLSTM layer and the second approach employs VGG16 with an integrated attention
    mechanism. Further, experiments have been carried using SVM, RF, KNN, MLP and
    TDNN using features GLCM, LBP and LBGLCM to capture physiological attributes of
    the bean leaf samples which in combination with different segmentation methods
    separates the diseased areas of the leaves. These are then used as apriori labels
    for the classifiers to reinforce the previously known details of the bean varieties.
    Subsequently, the trained network is tested using samples collected during visits
    to bean cultivation farms. Moreover, experiments have also been performed using
    some existing CNNs like ResNet152, MobileNetV2 and InceptionV3. The proposed methods
    have been compared with existing SOTA techniques and it has been observed that
    our proposed DL models could classify healthy and diseased bean leaves with accuracy
    1–7% better than the existing methods. Our proposed models I and II consistently
    demonstrated classification accuracies of 96% and 98% respectively. The computational
    framework of the proposed models has also been analyzed and we observed that the
    computation time of models I and II have been 11 s and 17sper epoch respectively.
    Also, the processing time of a single bean leaf image is 0.011 s. Compared to
    other PTMs, our proposed models are found to be computationally less demanding.
    This clearly highlights the computational efficiency of our proposed models. These
    systems can be integrated to UAVs for extensive crop monitoring in large patches
    of agricultural land in less amount of time. The key novelty of the work is an
    AI aided accurate and efficient decision support mechanism that reliably identifies
    bean leaf disease, recognizes weed growth with proper demarcation of the area
    under bean cultivation, continuous monitoring of the ambient conditions and air
    which triggers regulated water sprinkling. With continuous and automated monitoring
    of the health state of the bean leaves, the farmer obtains considerable support
    to enhance productivity. The weed monitoring helps to prevent encroachment of
    nutrients by wild growth and ensures better output from the bean cultivation.
    With weed area demarcation, use of mechanical means to remove the wild grass prevents
    chemical contamination of the fertile land. Controlled sprinkling of water prevents
    waste of a precious commodity like water. Further, the continuous monitoring of
    the air enables the farmers to visualize the best ambiance for seed germination
    and growth. In view of the above, the proposed system is expected to have decisive
    impact not only in assisting the farmer to enhance productivity but also to contribute
    towards ecological preservation. Ethics approval Not Applicable. Consent to participate
    All the authors approved to participate in this research. Consent for publication
    All the authors approved the publication of this research. Availability of data
    The authors do not have the permission to share the data. Grants and funding The
    authors did not receive any funds or grants from any organization. Author''s contribution
    Nilakshi Devi- experimental work, manuscript preparation, result generation and
    analysis; Kandarpa Kumar Sarma- conceptualization, editing of manuscript, supervision
    of experimental work and analysis; ShakuntalaLashkar- Supervision and analysis.
    Declaration of Competing Interest The authors declared that they have no competing
    interest. Acknowledgements The authors would like to thank Nayan Bean Farm, Sonapur,
    Guwahati, Assam for providing the bean leaf samples. Data availability The authors
    do not have permission to share data. References Akram et al., 2017 T. Akram,
    Sayeed Rameez, Mohammad Kmaran Towards real time crops surveillance for disease
    classification: exploiting parallelism in computer vision Comput. Electr. Eng.,
    59 (2017), pp. 15-26 View PDFView articleView in ScopusGoogle Scholar Akshay Pandey
    and Kamal Jain, 2021 Akshay Pandey, Kamal Jain A robust deep attention dense convolutional
    neural network for plant leaf disease identification and classification from smart
    phone captured real world images Ecological Informatics.vol.70 (2021) Google Scholar
    Almadhor and Rauf, 2021 A. Almadhor, H.T. Rauf AI-driven framework for recognition
    of guava plant diseases through machine learning from DSLR camera sensor based
    high resolution imagery Sensors., 21 (2021), p. 3830 CrossRefView in ScopusGoogle
    Scholar Amit et al., 2021 Prakash Amit, Sahu Priyanka, Singh Dinesh Deep learning
    models for beans crop diseases: classification and visualization techniques Int.
    J. Modern Agric., 10 (2021) Google Scholar Ashish, 2020 Kumar Ashish, et al. Res-VGG:
    A novel model for plant disease detection by fusing VGG16 and ResNet models International
    Conference on Machine Learning, Image Processing, Network Security and Data Science
    (2020), pp. 383-400 Google Scholar Asmita Hobisiyashi, 2022 Asmita Hobisiyashi
    ShivamYadav Cloud Based IoT controlled System Model for Plant Disease Monitoring.Predictive
    Analytics in Cloud, Fog and Edge Computing, Springer (2022) Google Scholar Bhagat
    et al., 2022 Sandesh Bhagat, et al. Eff-UNet++: a novel architecture for plant
    leaf segmentation and counting Ecol. Inform., 68 (2022) Google Scholar Binch and
    Fox, 2017 A. Binch, C.W. Fox Controlled comparison of machine vision algorithms
    for Rumex and Urtica detection in grassland Comput. Electron. Agric., 140 (2017),
    pp. 123-138 View PDFView articleView in ScopusGoogle Scholar CABI, 2022 CABI https://www.cabi.org/
    (2022) (Last checked on June 2022) Google Scholar Chen et al., 2020 Junde Chen,
    Defu Zhang, Jinxiu Chen Using deep transfer learning for image based plant disease
    identification Comput. Electron. Agric., 173 (2020) Google Scholar Chug et al.,
    2022 Anuradha Chug, Anshul Bhatia, Amit Prakash, Dinesh Singh A novel framework
    for image-based plant disease detection using hybrid deep learning approach J.
    Soft Comput. (2022), pp. 234-242 Springer Google Scholar Das et al., 2022 Amit
    Das, Himadri Saha, Amlan Chakrabarti Deep learning based automated disease detection
    and pests classification in Indian mung bean MultiMedia Tools and Applications,
    Springer (2022) Google Scholar Ferentinos, 2018 K.P. Ferentinos Deep learning
    models for plant disease detection and diagnosis Comput. Electron. Agric., 114
    (2018), pp. 311-318 View PDFView articleView in ScopusGoogle Scholar Geron, 2019
    Aurelin Geron Hands-on Machine Learning with Scikit-Learn, Keras and Tensor Flow
    O’ReillyPublisher (2019), pp. 234-345 Google Scholar Gokulnath and Usha, 2021
    B.V. Gokulnath, Devi Usha Identifying and classifying plant disease using resilient
    LF-CNN Ecol. Inform., 63, Elsevier (2021) Google Scholar Goodfellow et al., 2016
    Ian Goodfellow, Yoshua Bengio, Aaron Courville Deep Learning MIT Press (2016)
    Google Scholar Hapsari et al., 2022 Rinci Kembang Hapsari, Gan Hong Seng, Miswanto
    Miswanto Modified Gray Level Haralick Texture Features For Early Detection of
    Diabetes Mellitus and High Cholestrol in Iris Image vol. 2 (2022), p. 11 Google
    Scholar Hobisiyashi and Yadav, 2022 Asmita Hobisiyashi, Shivam Yadav Cloud based
    IoT controlled system model for plant disease monitoring Predictive Analytics
    in Cloud, Fog and Edge Computing, Springer (2022) Google Scholar Jha et al., 2019
    K. Jha, A. Doshi, P. Patel, M. Shah A comprehensive review on automation in agriculture
    using artificial intelligence Artif. Intelligence Agric., 2 (2019), pp. 1-12 June
    View PDFView articleView in ScopusGoogle Scholar Johann, 2016 Andre L. Johann,
    et al. Soil moisture modeling based on stochastic behavior of forces on a no-till
    chisel opener Comput. Electron. Agric., 121 (2016), pp. 420-428 View PDFView articleView
    in ScopusGoogle Scholar Johannes et al., 2017 A. Johannes, et al. Automatic plant
    disease diagnosis using mobile capture devices applied on a wheat use case Comput.
    Electron. Agric. (2017), pp. 200-209 View PDFView articleView in ScopusGoogle
    Scholar Kashefi et al., 2017 Javed Kashefi, et al. Novelty Detection Classifiers
    in Weed Mapping: Silybummarianum Detection on UAV Multispectral Images vol. 17
    (2017) Google Scholar Kaya et al., 2023 Yasin Kaya, et al. A novel multi-head
    CNN design to identify plant diseases using the fusion of RGB images Ecol. Inform.,
    75 (2023) Google Scholar Maione et al., 2018 Camila Maione, et al. Recent applications
    of multivariate data analysis methods in the authentication of rice and the most
    analyzed parameters: a review Taylor &Francis (2018), pp. 1868-1879 Online Google
    Scholar Manjula et al., 2022 Manjula, et al. Plant disease detection using deep
    learning Lecture Notes in Electrical Engineering, Springer (2022), pp. 1389-1396
    CrossRefView in ScopusGoogle Scholar Naik et al., 2022 Naik, et al. Detection
    and classification of chilli leaf disease using a squeeze-and-excitation-based
    CNN model Ecol. Inform., 69 (2022) Google Scholar Nicola and Pisana, 2021 Papini
    Nicola, Placidi Pisana Monitoring Soil and Ambient Parameters in IoT Precision
    Agriculture Sceneriao: An Original Modelling Approach Dedicated to Low Cost Water
    Content Sensors vol. 21 (2021), p. 5110 Google Scholar Olivia et al., 2022 Diego
    Olivia, et al. BLeafNet: a Bonferroni mean operator based fusion of CNN models
    for plant identification using leaf image classification Ecol. Inform., 69 (2022)
    Google Scholar Pandey, 2022 A. Pandey, et al. An intelligent system for crop identification
    and classification from UAV images using conjugated dense convolutional neural
    network Comput. Electron. Agric. (2022), p. 567 Google Scholar Pantazi et al.,
    2019 X.E. Pantazi, et al. Automated Leaf Disease Detection in Different Crop Species
    through Image Features Analysis and One Class Classifiers Computers and Electronics
    in Agriculture (2019) Google Scholar Picon, 2019 A. Picon, et al. Deep convolutional
    neural networks for mobile capture device-based crop disease classification in
    the wild Comput. Electron. Agric..pp.235 (2019) Google Scholar Rakesh et al.,
    2021 Chandra Joshi Rakesh, Kaushik Manoj, Kishore Dutta Malay, Srivastava Ashish,
    Choudhury Nandlal VirLeafNet: automatic analysis and viral disease diagnosis using
    deep learning in VignaMungo plant Ecol. Inform., 61 (2021) Elsevier Google Scholar
    Ramesh et al., 2018 Shima Ramesh, et al. Plant disease detection using machine
    learning International Conference on Design Innovation for 3Cs Compute Communicate
    Control (ICDI3C), IEEE (2018) Google Scholar Recep and Lahcen, 2022 Eryigit Recep,
    Elfatimi Lahcen Bean Leaf Disease Classification Using Mobile Net Models IEEE
    Access, 10 (2022), pp. 9471-9482 Google Scholar Rudagi et al., 2022 Jayashri Rudagi,
    et al. Plant leaf disease detection using computer vision and machine learning
    algorithms Glob. Trans. Proc., 3 (2022), pp. 305-310 Google Scholar Sahoo et al.,
    2020 Sahoo, et al. Maize leaf disease detection and classification using machine
    learning algorithms Progress in Computing, Analytics and Networking (2020), pp.
    659-669 Google Scholar Shah et al., 2022 Deshna Shah, et al. Image based plant
    disease detection Data Intelligence and Cognitive Informatics, Springer (2022),
    pp. 651-666 CrossRefGoogle Scholar Sibiya et al., 2019 Sibiya, et al. A computational
    procedure for the recognition and classification of maize leaf disease out of
    healthy leaves using convolution neural networks J. Agr. Eng. (2019), pp. 119-131
    CrossRefView in ScopusGoogle Scholar Singh et al., 2020 Singh, et al. PlantDoc:
    A dataset for visual plant disease detection Proceedings of the 7th ACM IKDD CoDS
    and 25th COMAD (2020), pp. 249-253 Google Scholar Sutaji and Yildiz, 2022 Deni
    Sutaji, Oktay Yildiz LEMOXINET: Lite ensemble MobileNetV2 and Xception models
    to predict plant disease Ecol. Inform., 70 (2022) Google Scholar Thi et al., 2022
    Hanh Bui Thi, et al. Enhancing the performance of transferred efficient net models
    in leaf image-based plant disease classification J. Plant Dis. Protect. (2022),
    pp. 623-634 Google Scholar Tiwari et al., 2021 Vaibhav Tiwari, Rakesh Chandra
    Joshi, Malay Kishore Dutta Dense convolutional neural network basedmulti class
    plant disease detection and classification using leaf images Ecol. Inform., 63
    (2021) Google Scholar Togacar, 2022 Mesut Togacar Using DarkNet models and metaheuristic
    optimization methods together to detect weeds growing along with seedlings Ecol.
    Inform., 68 (2022) Google Scholar Tool et al., 2019 E.C. Tool, et al. A comparative
    analysis of fine tuning deep learning models for plant disease identification
    Comput. Electron. Agric. (2019), pp. 272-279 Google Scholar Traore et al., 2019
    D. Traore, et al. Deep neural networks with transfer learning in millet crop images
    Comput. Ind., 108 (2019), pp. 115-120 Google Scholar Veeraballi, 2020 Veeraballi,
    et al. Deep learning based approach for classification and detection of papaya
    leaf diseases Adv. Intelligent Syst. Comput..pp.567 (2020) Google Scholar Vimal
    et al., 2023 Vimal, et al. Classification of beans leaf diseases using fine tuned
    CNN model Procedia Comput. Sci., 218 (2023) Google Scholar Wang et al., 2023a
    Wang, et al. Sweet potato leaf detection in a natural scene based on faster R-CNN
    with a visual attention mechanism and DIoU-NMS Ecol. Inform., 73 (2023) Google
    Scholar Wang et al., 2023b Wang, et al. A novel deep learning method for maize
    disease identification based on small sample-size and complex background datasets
    Ecol. Inform., 75 (2023) Google Scholar Xia, 2022 Limei Xia Automatic strawberry
    leaf scorch severity estimation via faster R-CNN and few-shot learning Ecol. Inform.,
    70 (2022) Google Scholar Xie et al., 2020 Xie, et al. A deep learning real time
    detector for grape leaf disease using improved convolutional neural networks Front.
    Plant Sci. (2020), pp. 1-14 Google Scholar Yadav et al., 2021 S. Yadav, et al.
    Identification of disease using deep learning and evaluation of bacteriosis in
    peach leaf Ecol. Inform., 61 (2021) Google Scholar Yang et al., 2019 Yang Yang,
    et al. Disease prediction model based on BiLSTM and attention mechanism IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM), IEEE (2019) Google Scholar
    Zan, 2022 Wang Zan, et al. An improved faster R-CNN model for multi-object tomato
    maturity detection in complex scenarios Ecol. Inform., 72 (2022) Google Scholar
    Olfa, 2023 Olfa, et al. Deep learning-based segmentation for disease identification
    Ecol. Inform. (2023), p. 34 Available online on January 2023. Google Scholar Cited
    by (11) Image patch-based deep learning approach for crop and weed recognition
    2023, Ecological Informatics Show abstract Strawberry R-CNN: Recognition and counting
    model of strawberry based on improved faster R-CNN 2023, Ecological Informatics
    Show abstract Identification of suitable location to cultivate grape based on
    disease infestation using multi-criteria decision-making (MCDM) and remote sensing
    2023, Ecological Informatics Show abstract Sustainable integrated farming in agriculture
    2023, Water-Soil-Plant-Animal Nexus in the Era of Climate Change Machine Learning
    for Weather-Driven Energy Consumption Forecasting and Optimization in Moroccan
    Agricultural Greenhouses 2023, SSRN Monitoring and Sensing of Real-Time Data with
    Deep Learning Through Micro- and Macro-analysis in Hardware Support Packages 2023,
    SN Computer Science View all citing articles on Scopus View Abstract © 2023 Elsevier
    B.V. All rights reserved. Recommended articles Development of artificial intelligence
    based systems for prediction of hydration characteristics of wheat Computers and
    Electronics in Agriculture, Volume 128, 2016, pp. 34-45 S.M. Shafaei, …, S. Kamgar
    View PDF Weekly carbon dioxide exchange trend predictions in deciduous broadleaf
    forests from site-specific influencing variables Ecological Informatics, Volume
    75, 2023, Article 101996 David A. Wood View PDF Expansion risk of the toxic dinoflagellate
    Gymnodinium catenatum blooms in Chinese waters under climate change Ecological
    Informatics, Volume 75, 2023, Article 102042 Changyou Wang, …, Zhuhua Luo View
    PDF Show 3 more articles Article Metrics Citations Citation Indexes: 9 Captures
    Readers: 66 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Ecological Informatics
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Design of an intelligent bean cultivation approach using computer vision,
    IoT and spatio-temporal deep learning structures
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Phade G.
  - Kishore A.T.
  - Omkar S.
  - Kumar M.S.
  citation_count: '0'
  description: Farmers of the twentieth century are adopting the use of leading technical
    facilities in the field of agriculture to sustain the competitiveness of the global
    market economy. With the help of modern technology, they are trying to reduce
    production costs and improve crop yield with better product quality. A new era
    in which many traditional agricultural practices are replaced by cutting-edge
    technologies like Precision Farming (PF), which entails applying the agronomic
    variables in the right place, at the right time, has been ushered in by technological
    advancements made in monitoring, supervision, management, and control systems.
    One of the methods that will enable quick and non-destructive analysis of agricultural
    data will turn out to be an IoT integrated smart agriculture drone. This includes
    taking pictures to analyze crop behavior, finding out how much water the soil
    can contain, managing irrigation systems, etc. Numerous engineering disciplines,
    including aerodynamics, electronics, computer programming, and economics are integrated
    in the design, development, and implementation of drone-based agricultural systems.
    In considering the above thought, a problem taken into consideration is for a
    grape field of approximately 10 acres located near Nashik (28 36'N, 77 12'E) in
    the state of Maharashtra to analyze the performance of IoT enabled agriculture
    drones for PF. Main objective is to spray the insecticide with an agriculture
    drone only on the detection of the green zone of the crop or canopy for effective
    spraying and hence to reduce the wastage and cost of spraying. Drone image sensor
    will capture the field images, atmospheric parameters like, temperature, humidity
    in the field and analyze it on the cloud platform for analyzing the crop health
    and related parameters. An agriculture drone of 16 L capacity is proposed for
    spraying 1 acre of land. It could take 7-10 minutes of flight time of the drone
    to spray the insecticide which can lead to reducing the labor cost by 65%, spraying
    time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism
    is to be synchronized to achieve the objective through IoT platform. For effective
    spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying
    is proposed. Further, time series analysis of the photographic images and video
    footage captured by the drone camera can be done for the prediction, modeling
    of crop yields and auditing them with computer vision capabilities of UAV, and
    developing sufficient datasets. These data sets can be used for machine learning
    algorithms for AI or DL and for future research related to crop disease forecasting,
    canopy cover, stress management, and be able to guide farmers for taking corrective
    actions in advance and evolve best practices for overall improvement in the yield.
  doi: 10.1002/9781394168002.ch12
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Drone Technology: Future Trends and Practical Applications Chapter
    12 IoT-Enabled Unmanned Aerial Vehicle An Emerging Trend in Precision Farming
    Gayatri Phade,  A. T. Kishore,  S. Omkar,  M. Suresh Kumar Book Editor(s):Sachi
    Nandan Mohanty,  J.V.R. Ravindra,  G. Surya Narayana,  Chinmaya Ranjan Pattnaik,  Y.
    Mohamed Sirajudeen First published: 12 May 2023 https://doi.org/10.1002/9781394168002.ch12
    PDF TOOLS SHARE Summary Farmers of the twentieth century are adopting the use
    of leading technical facilities in the field of agriculture to sustain the competitiveness
    of the global market economy. With the help of modern technology, they are trying
    to reduce production costs and improve crop yield with better product quality.
    A new era in which many traditional agricultural practices are replaced by cutting-edge
    technologies like Precision Farming (PF), which entails applying the agronomic
    variables in the right place, at the right time, has been ushered in by technological
    advancements made in monitoring, supervision, management, and control systems.
    One of the methods that will enable quick and non-destructive analysis of agricultural
    data will turn out to be an IoT integrated smart agriculture drone. This includes
    taking pictures to analyze crop behavior, finding out how much water the soil
    can contain, managing irrigation systems, etc. Numerous engineering disciplines,
    including aerodynamics, electronics, computer programming, and economics are integrated
    in the design, development, and implementation of drone-based agricultural systems.
    In considering the above thought, a problem taken into consideration is for a
    grape field of approximately 10 acres located near Nashik (28 36''N, 77 12''E)
    in the state of Maharashtra to analyze the performance of IoT enabled agriculture
    drones for PF. Main objective is to spray the insecticide with an agriculture
    drone only on the detection of the green zone of the crop or canopy for effective
    spraying and hence to reduce the wastage and cost of spraying. Drone image sensor
    will capture the field images, atmospheric parameters like, temperature, humidity
    in the field and analyze it on the cloud platform for analyzing the crop health
    and related parameters. An agriculture drone of 16 L capacity is proposed for
    spraying 1 acre of land. It could take 7–10 minutes of flight time of the drone
    to spray the insecticide which can lead to reducing the labor cost by 65%, spraying
    time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism
    is to be synchronized to achieve the objective through IoT platform. For effective
    spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying
    is proposed. Further, time series analysis of the photographic images and video
    footage captured by the drone camera can be done for the prediction, modeling
    of crop yields and auditing them with computer vision capabilities of UAV, and
    developing sufficient datasets. These data sets can be used for machine learning
    algorithms for AI or DL and for future research related to crop disease forecasting,
    canopy cover, stress management, and be able to guide farmers for taking corrective
    actions in advance and evolve best practices for overall improvement in the yield.
    References Drone Technology: Future Trends and Practical Applications References
    Related Information Recommended Unmanned Aerial Vehicles for Agriculture: an Overview
    of IoT‐Based Scenarios Bacco Manlio,  Barsocchi Paolo,  Gotta Alberto,  Ruggeri
    Massimiliano Autonomous Airborne Wireless Networks, [1] Role of AI and Big Data
    Analytics in UAV‐Enabled IoT Applications for Smart Cities Madhuri S. Wakode Unmanned
    Aerial Vehicles for Internet of Things (IoT): Concepts, Techniques, and Applications,
    [1] Unmanned Aerial Vehicle (UAV) Mengxiang Li,  Liang Tang International Encyclopedia
    of Geography: People, the Earth, Environment and Technology, [1] Review on unmanned
    aerial vehicles, remote sensors, imagery processing, and their applications in
    agriculture Daniel Olson,  James Anderson Agronomy Journal Unmanned Aerial Vehicle
    (UAV): A Comprehensive Survey Rohit Chaurasia,  Vandana Mohindru Unmanned Aerial
    Vehicles for Internet of Things (IoT): Concepts, Techniques, and Applications,
    [1] Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About
    Cookies Manage Cookies Accessibility Wiley Research DE&I Statement and Publishing
    Policies Developing World Access HELP & SUPPORT Contact Us Training and Support
    DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers & Corporate
    Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright © 1999-2024
    John Wiley & Sons, Inc or related companies. All rights reserved, including rights
    for text and data mining and training of artificial technologies or similar technologies."'
  inline_citation: '>'
  journal: 'Drone Technology: Future Trends and Practical Applications'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'IoT-enabled unmanned aerial vehicle: An emerging trend in precision farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sharma S.
  - Verma K.
  - Hardaha P.
  citation_count: '27'
  description: Agriculture has a significant contribution to the economy. Agricultural
    automation is a major cause of concern and a relatively new phenomenon throughout
    the world. The world’s population is quickly growing, resulting in increased demand
    for food and labor. Farmers’ customary techniques were insufficient to achieve
    these goals. As a result, new automated techniques were developed. These creative
    initiatives met food demands while also providing work opportunities for a large
    number of people. Agriculture has changed as a result of artificial intelligence
    (AI). This strategy has shielded agricultural production from a variety of threats
    such as weather, population growth, labor rights, and food security concerns.
    The major issue of this is the numerous applications of AI in agriculture, such
    as irrigation, weeding, and spraying with different sensors or other ways implanted
    in robots and drones. These technologies limit the use of water, pesticides, and
    herbicides, preserve soil fertility, and help in the effective use of labor, resulting
    in increased output and quality. Many researchers make efforts to gain a quick
    overview of the present state of automation in agriculture, including weeding
    systems using robots and drones. Two automated weeding strategies as well as several
    soil water sensing technologies are explored. The drones are employed for the
    numerous methods for spraying and crop monitoring. In this paper, we also discuss
    how AI should be combined with other technologies and applications of AI for solving
    farming challenges.
  doi: 10.47852/bonviewJCCE2202174
  full_citation: '>'
  full_text: '>

    "Journal of Computational and Cognitive Engineering HOME ABOUT BROWSE CONTRIBUTE
    EDITORIAL BOARD SPECIAL ISSUES Search Register BROWSE Login Home / Archives /
    Vol. 2 No. 2 (2023) / Research Articles Implementation of Artificial Intelligence
    in Agriculture Shivangi Sharma Department of Computer Science and Engineering,
    Lakshmi Narain College of Technology and Sciences, India Kirti Verma Department
    of Engineering Mathematics, Lakshmi Narain College of Technology, India .st0{fill:#A6CE39;}
    .st1{fill:#FFFFFF;} https://orcid.org/0000-0003-4658-9722 Palak Hardaha Department
    of Computer Science and Engineering, Lakshmi Narain College of Technology and
    Sciences, India DOI: https://doi.org/10.47852/bonviewJCCE2202174 Keywords: artificial
    intelligence, herbicide, pesticide, automation, irrigation, machine learning,
    anomaly detection, computer vision, natural language processing, conversational
    AI Abstract Agriculture has a significant contribution to the economy. Agricultural
    automation is a major cause of concern and a relatively new phenomenon throughout
    the world. The world''s population is quickly growing, resulting in increased
    demand for food and labor. Farmers'' customary techniques were insufficient to
    achieve these goals. As a result, new automated techniques were developed. These
    creative initiatives met food demands while also providing work opportunities
    for a large number of people. Agriculture has changed as a result of artificial
    intelligence. This strategy has shielded agricultural production from a variety
    of threats such as weather, population growth, labour rights, and food security
    concerns. The major issue of this is the numerous applications of AI in agriculture,
    such as irrigation, weeding, and spraying with different sensors or other ways
    implanted in robots and drones. These technologies limit the use of water, pesticides,
    and herbicides, preserve soil fertility and help in the effective use of labour,
    resulting in increased output and quality. Many researchers efforts to gain a
    quick overview of the present state of automation in agriculture, including weeding
    systems using robots and drones. Two automated weeding strategies are explored,
    as well as several soil water sensing technologies. The utilization of drones,
    as well as the numerous methods for spraying and crop monitoring that drones employ.
    In this Paper we also discuss how AI should be combined with other technologies
    and applications of AI in solving farming challenges.   Received: 18 February
    2022 | Revised: 2 June 2022 | Accepted: 2 July 2022   Conflicts of Interest The
    authors declare that they have no conflicts of interest to this work. Metrics
    PDF views 3,226 Jul 04 ''22 Jul 07 ''22 Jul 10 ''22 Jul 13 ''22 Jul 16 ''22 Jul
    19 ''22 Jul 22 ''22 Jul 25 ''22 Jul 28 ''22 Jul 31 ''22 Aug 01 ''22 32 daily (first
    30) | monthly  We recommend Revolutionizing Sustainable Energy Production with
    Quantum Artificial Intelligence: Applications in Autonomous Robotics and Data
    Management Luka Baklaga, Green and Low-Carbon Economy, 2023 Thriving in the Entrepreneurial
    Landscape of Sustainability and Intelligent Automation Era Narcisa Roxana Moşteanu,
    Green and Low-Carbon Economy, 2023 Topological Data Analysis of COVID-19 Using
    Artificial Intelligence and Machine Learning Techniques in Big Datasets of Hausdorff
    Spaces Allan Onyango et al., Journal of Data Science and Intelligent Systems,
    2023 Machine learning for weed–plant discrimination in agriculture 5.0: An in-depth
    review Filbert H. Juwono et al., Artificial Intelligence in Agriculture, 2023
    ARTIFICIAL INTELLIGENCE-2023 Medigy Innovation Network 553 The role of artificial
    intelligence in pediatric injuries-a scoping review Maleeha Naseem et al., Inj
    Prev, 2022 Powered by PDF Published 2022-07-04 How to Cite Sharma, S., Verma,
    K., & Hardaha, P. (2022). Implementation of Artificial Intelligence in Agriculture.
    Journal of Computational and Cognitive Engineering, 2(2), 155–162. https://doi.org/10.47852/bonviewJCCE2202174
    More Citation Formats Issue Vol. 2 No. 2 (2023) Section Research Articles License
    Copyright (c) 2022 Authors This work is licensed under a Creative Commons Attribution
    4.0 International License. Journal Information Editor-in-Chief: Harish Garg Thapar
    Institute of Engineering and Technology, India Frequency: Quarterly Submission
    to final decision: 65 days Acceptance to publication: 15 days Acceptance rate:
    29% eISSN: 2810-9503 pISSN: 2810-9570   © 2024  Bon View Publishing Pte Ltd. Make
    a Submission Announcements Bon View Publishing Formally Joined Open Access Scholarly
    Publishing Association(OASPA) April 7, 2024 Bon View Publishing Pte. Ltd. has
    recently become a member of Open Access Scholarly Publishing Association(OASPA).
    As the member of OASPA, Bon View Publishing is intrinsic to fulfilling the mission
    of encouraging and enabling open access as the predominant model of communication
    for scholarly outputs. JCCE is formally accepted by SCOPUS March 12, 2024 We are
    delighted to share with you that Journal of Computational and Cognitive Engineering
    has been accepted by SCOPUS and articles published in this journal will be indexed
    from April, 2024. JCCE Published Volume 3, Issue 1 on February 23, 2024 February
    23, 2024 We are excited to announce that Journal of Computational and Cognitive
    Engineering (JCCE) published Volume 3 Issue 1 on February 23, 2024. Keywords reliability
    decision-making soft set decision making availability Internet of Things optimal
    solution neutrosophic set fuzzy set BERT big data new NeutroGroupoid new NeutroSemiGroup
    solar bipolar neutrosophic set Most Read Implementation of Artificial Intelligence
    in Agriculture 3033 A Systematic Review on Intelligent Transport Systems 1138
    Spam Detection Using Bidirectional Transformers and Machine Learning Classifier
    Algorithms 1103 Comparing BERT Against Traditional Machine Learning Models in
    Text Classification 873 Machine Learning-Based Intrusion Detection System: An
    Experimental Comparison 848 All site content, except where otherwise noted, is
    licensed under a Creative Commons Attribution 4.0 International License. pISSN
    2810-9570, eISSN 2810-9503 | Published by Bon View Publishing Pte Ltd. Member
    of                        "'
  inline_citation: '>'
  journal: Journal of Computational and Cognitive Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Implementation of Artificial Intelligence in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Singh S.
  - Babu K.V.S.
  citation_count: '0'
  description: The agricultural sector allowed for very diverse management in the
    global economy, where the sector is being strengthened to be part of the commercial
    growth engine. The firm belief in incorporating information and communication
    technology (ICT) with agricultural systems influenced the expansion of a mechanized
    system to classify and organize agricultural products. Conventional farming is
    based on observations and is highly familiar which is quite laborious and time-consuming,
    consequently, the need for continuous monitoring of crops can be a difficulty
    for the farmers. The technologically advanced system initiates the monitoring
    and mapping process by capturing and predicting the general characterization.
    The integration of the Internet of Things (IoT) and artificial intelligence (AI)
    plays a dynamic role in the concept of smart farming, using such applications
    as monitoring systems to observe crop yield estimation, irrigation, nutrient management,
    disease identification, and weather forecast. This paper proposes a framework
    to enable advanced AI according to user-defined variables, of which sensors are
    an important feature and contributor. As an interface between a sensor and IoT
    as a medium, it offers great potential for outstanding performance. The results
    obtained using this integrated approach are very promising and can be used significantly
    for any other application of precision agriculture.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: AIoT Technologies and Applications for Smart Environments
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Adaptive smart farming system using Internet of Things (IoT) and artificial
    intelligence (AI) modeling
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Mahmud M.S.
  - Zahid A.
  - Das A.K.
  citation_count: '5'
  description: The ornamental crop industry is an important contributor to the economy
    in the United States. The industry has been facing challenges due to continuously
    increasing labor and agricultural input costs. Sensing and automation technologies
    have been introduced to reduce labor requirements and to ensure efficient management
    operations. This article reviews current sensing and automation technologies used
    for ornamental nursery crop production and highlights prospective technologies
    that can be applied for future applications. Applications of sensors, computer
    vision, artificial intelligence (AI), machine learning (ML), Internet-of-Things
    (IoT), and robotic technologies are reviewed. Some advanced technologies, including
    3D cameras, enhanced deep learning models, edge computing, radio-frequency identification
    (RFID), and integrated robotics used for other cropping systems, are also discussed
    as potential prospects. This review concludes that advanced sensing, AI and robotic
    technologies are critically needed for the nursery crop industry. Adapting these
    current and future innovative technologies will benefit growers working towards
    sustainable ornamental nursery crop production.
  doi: 10.3390/s23041818
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 23 Issue 4 10.3390/s23041818 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Subscribe SciFeed Recommended
    Articles Related Info Links More by Authors Links Article Views 3808 Citations
    5 Table of Contents Abstract Introduction Sensing and Automation Technologies
    for Ornamental Crops Future Prospects/Directions Discussion and Conclusions Author
    Contributions Funding Conflicts of Interest References share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessReview Sensing
    and Automation Technologies for Ornamental Nursery Crop Production: Current Status
    and Future Prospects by Md Sultan Mahmud 1,2,*, Azlan Zahid 3 and Anup Kumar Das
    4 1 Department of Agricultural and Environmental Sciences, Tennessee State University,
    Nashville, TN 37209, USA 2 Otis L. Floyd Nursery Research Center, Tennessee State
    University, McMinnville, TN 37110, USA 3 Department of Biological and Agricultural
    Engineering, Texas A&M AgriLife Research, Texas A&M University System, Dallas,
    TX 75252, USA 4 Department of Agricultural and Biosystems Engineering, North Dakota
    State University, Fargo, ND 58102, USA * Author to whom correspondence should
    be addressed. Sensors 2023, 23(4), 1818; https://doi.org/10.3390/s23041818 Submission
    received: 26 November 2022 / Revised: 11 January 2023 / Accepted: 1 February 2023
    / Published: 6 February 2023 (This article belongs to the Section Smart Agriculture)
    Download keyboard_arrow_down     Browse Figures Versions Notes Abstract The ornamental
    crop industry is an important contributor to the economy in the United States.
    The industry has been facing challenges due to continuously increasing labor and
    agricultural input costs. Sensing and automation technologies have been introduced
    to reduce labor requirements and to ensure efficient management operations. This
    article reviews current sensing and automation technologies used for ornamental
    nursery crop production and highlights prospective technologies that can be applied
    for future applications. Applications of sensors, computer vision, artificial
    intelligence (AI), machine learning (ML), Internet-of-Things (IoT), and robotic
    technologies are reviewed. Some advanced technologies, including 3D cameras, enhanced
    deep learning models, edge computing, radio-frequency identification (RFID), and
    integrated robotics used for other cropping systems, are also discussed as potential
    prospects. This review concludes that advanced sensing, AI and robotic technologies
    are critically needed for the nursery crop industry. Adapting these current and
    future innovative technologies will benefit growers working towards sustainable
    ornamental nursery crop production. Keywords: agricultural mechanization; artificial
    intelligence; computer vision; digital agriculture; internet-of-things; plant
    biometrics; smart irrigation; smart spraying; stress detection 1. Introduction
    The nursery and greenhouse industry contributes nearly $14 billion in annual sales
    to the U.S. economy [1]. This industry produces more than 2000 ornamental plant
    species, covering most of the U.S.’ ornamental plants [2]. Nurseries are, in general,
    open-air operations where plants grow in the ground or in containers [3]. Greenhouses
    are typically enclosed environments where growth conditions (e.g., lighting, temperature,
    humidity, and irrigation) can be controlled [4]. Rapidly increasing production
    cost due to the increased labor expense, difficulty in obtaining skilled labor,
    and inappropriate application of agricultural resources are rising concerns for
    the ornamental industry [5,6]. Operations such as planting, growing, and harvesting
    nursery crops are heavily dependent on labor. These operations account for 43%
    of total production expenses [7]. It is becoming increasingly difficult for the
    industry to obtain such labor, especially the skilled workforce required to grow
    ornamental crops [8]. Conventional practices apply agricultural resources (such
    as water, nutrients, fertilizers, and pesticides) excessively and inefficiently,
    increasing production costs. These conventional approaches not only increase the
    production cost but are also responsible for contaminating the environment and
    the ecosystem. The industry must look for alternative solutions, such as automated
    crop management technologies, to reduce labor needs and ensure the efficient use
    of crop production resources. In the current decade, sensing and automation technologies
    have been continually increasing their impact on different crop management operations
    [9,10,11,12,13]. These technologies are categorized into two groups: ground-based
    and aerial-based. Ground-based crop harvesting technologies have been tested on
    various crops, including sweet pepper [14], lettuce [15], tomato [16], strawberries
    [11], apples [9], and cherries [17]. Ground-based technologies have also been
    explored widely in automatic disease detection in different crops, such as: powdery
    mildew on strawberry leaves [18]; leaf blotch, stripe rust, powdery mildew, leaf
    rust, black chaff, and smut on wheat leaves [19]; Alternaria leaf spot, brown
    spot, mosaic, grey spot and rust on apple leaves [20]; and anthracnose, brown
    spot, mites, black rot, downy mildew, and leaf blight on grape leaves [10]. Recent
    evolutions in unmanned aerial vehicles (UAVs) show the potential of using them
    in different agricultural operations, thereby consuming less time than ground-based
    systems [12]. Until now, UAVs used for agriculture have been limited to only remote
    sensing applications, due to limited payload capacity and battery life. UAVs have
    been used in various crop management applications, including automatic canker
    disease monitoring in citrus [21], weed detection in wheat and oat fields [22],
    detecting and mapping tree seedlings and individual plants [23,24], and yield
    estimation in cotton [25]. However, the success of sensing and automation technologies
    largely depends on the types of sensors used to acquire crop data and the processing
    algorithms used to extract valuable information. Various sensors, such as soil
    moisture, temperature and humidity sensors, cameras (color, spectral, and infrared),
    together with computer algorithms are used to develop smart technologies for agricultural
    applications [5,18,21,26]. A prototype irrigation controller system was developed
    using nine soil moisture sensors on an IoT platform to automatically manage water
    application in crops [26]. You et al. [27] used an RGB-D camera system to develop
    an autonomous robot for pruning branches of sweet cherry trees. It should be noted
    that RGB-D cameras offer four channels (i.e., red, green, blue and depth) that
    were required to estimate the size of branches (by depth channel) to decide which
    ones need to prune. Abdulridha et al. [21] detected citrus disease at an early
    stage using a hyperspectral camera. Other cameras may not be suitable for detecting
    a particular disease at the asymptomatic stage. Liu et al. [28] used enhanced
    generative adversarial networks (GANs) to augment their data for grape leaf disease
    detection; other machine-learning models were not considered because of the requirement
    for a deeper network. In conclusion, identifying appropriate sensors and developing
    algorithms are necessary tasks that depend mainly on crop and soil characteristics
    and operational needs. In most cases, one automated technology is specific to
    one particular operation in a specific crop. Therefore, evaluating sensor and
    algorithm performances for different crops in a certain industry provide insights
    for choosing them generally, while developing technology for a particular production
    operation. Although the ornamental crop industry is in the initial phase of developing
    sensing and automation technologies, an overview of currently available technologies
    and prospects of advanced technologies utilized for other crop industries (for
    agronomic crops and tree fruits industry) will be helpful for future technology
    developments. 1.1. Scope of the Study A few of the available reviews for ornamental
    crops mainly reviewed water management technologies and barriers to technology
    adoption [6,29]. Lea-Cox et al. [29] studied the economic benefit, current and
    future challenges, and support issues of using wireless sensor networks (WSNs)
    for water management of ornamental crops. Rihn et al. [6] reviewed factors correlated
    with the nursery industry’s propensity to use automation and mechanization. Their
    study also discussed the barriers to adoption for currently available automated
    technologies. This review aims to cover available sensing and automation technologies
    used for ornamental crop production operations, along with the prospects of using
    some advanced technologies (used in other crop industries) that can be beneficial
    to this industry. To the author’s knowledge, this is the first review article
    that broadly discusses sensing and automation technologies for ornamental crops.
    1.2. Paper Organization This review aims to discuss the status and challenges
    of sensing and automation technologies for the ornamental crop industry. The organization
    of this article is as follows: Section 2 presents an overview of sensing and automation
    technologies used for ornamental crops. In Section 3, advanced technologies used
    for other cropping systems are discussed that could be valuable for developing
    future technologies for ornamental crops. Finally, Section 4 summarizes the overall
    discussion and conclusion of the article. 2. Sensing and Automation Technologies
    for Ornamental Crops Sensing and automation technologies are used in different
    operations relating to ornamental nursery crop production. The major operations
    are smart irrigation, plant stress detection, smart or variable-rate spraying,
    and plant biometrics measurements (Figure 1). This section presents detailed reviews
    of the currently applied sensing and automation technologies for those operations.
    The technologies have also been used in a few other areas and represented as other
    significant works. Figure 1. Areas where sensing and automation technologies are
    used for ornamental crop production. 2.1. Smart Irrigation Smart or precision
    irrigation technology determines the water requirement of crops using set-point
    control (using soil moisture data) or model-based control (using crop and environmental
    data) to maximize irrigation efficiency [4,29]. It helps reduce excessive water
    application while maintaining crop growth and development. Sensors-based irrigation
    technologies have been tested in different nurseries, including greenhouse, container,
    pot-in-pot, and field nurseries [30,31,32,33,34]. A schematic diagram of a smart
    irrigation system is presented in Figure 2. Figure 2. A schematic of an IoT-based
    smart irrigation system for water management in a container-based nursery. Table
    1 presents different sensor applications for automatic irrigation management in
    different nurseries. Wireless sensor networks (WSNs) were used to control irrigation
    water flow in three container-based nurseries [32]. Experiments were conducted
    in two phases: first, EM50R nodes with EC-5 sensors were used to monitor soil
    moisture; and second, nR5 nodes were used to monitor and control irrigation. The
    WSNs-based technology reduced water use by about 20% to 25%. Kim et al. [35] tested
    soil moisture and EC sensors to monitor and automatically implement irrigation
    protocols. Substrate moisture data were measured to reduce water usage of hydrangea
    by as much as 83%. Coates et al. [36] used a VH400 (Vegetronix, Sandy, UT, USA)
    sensor to monitor soil water content in container nurseries where pots contain
    hydrangea plants. Even though the VH400 sensor costs half as much as standard
    EC-5 sensors, the authors concluded the VH400 was unsuitable for nursery crop
    monitoring because its output varied by up to 29%. This type of sensor (VH400)
    shows a high sensitivity of ~34 mV rather than ~5 mV using EC-5 per % volumetric
    water content. Lea-Cox et al. [31] used a hybrid system consisting of a 12-node
    CMU network (developed by Carnegie Mellon University, United States) and Decagon
    Ech20 moisture sensors (Decagon Devices Inc., Pullman, WA, USA) to control water
    applications in real-time in a container nursery. The system was also tested in
    a greenhouse where a six-node CMU network was used. The results reported that
    both networks performed well, but encountered some networking challenges at remote
    sites. The authors noted the CMU network node is less costly than the commercial
    Decagon Ech20 sensor, but showed similar performance. Wheeler et al. [34] also
    tested a smart irrigation system in a container nursery and greenhouse. They used
    Decagon soil moisture sensors along with an nR5 wireless node to control irrigation.
    The study reported a water use reduction of approximately 50% when compared to
    grower-controlled irrigation. The same sensor system was trialed previously by
    Wheeler et al. [5] in a floriculture greenhouse. The WSNs are also used in pot-in-pot
    nurseries. Belayneh et al. [37] used this technology to control irrigation in
    dogwood (planted in 15-gal containers) and red maple (planted in 30-gal containers)
    nurseries. The EM50R nodes were used to monitor data from soil moisture, and environmental
    sensors and nR5 nodes were used for irrigation control. Volumetric water content-based
    sensors were utilized for monitoring soil moisture. The sensors were inserted
    at a 6-inch depth for dogwood and at 6 and 12 inches depth for red maple. The
    results showed that the WSNs-based irrigation method reduced water usage by ~34%
    and ~63% for red maple and dogwood, respectively. Lea-Cox and Belayneh [38] developed
    a smart battery-operated nR5 wireless sensor node using a series of soil moisture
    and environmental sensors to irrigate dogwood and red maple nursery blocks. The
    study reduced daily water application by about 62.9%. The authors concluded that
    this sensor-based irrigation technology resulted in nearly a three-fold increase
    in the efficiency of water without reducing the quality or growth of trees. Internet-of-Things
    (IoT)-based smart irrigation systems have also been used for ornamental crop production.
    Banda-Chávez et al. [39] developed an IoT-based sensor network to activate the
    irrigation system to irrigate ornamental plant using an IoT platform and soil
    moisture sensors (YL-69). In addition, Beeson and Brooks [40] used an evapotranspiration
    (ETo) model-based smart irrigation system for wax-leaf privet. The study reported
    that this model-based irrigation system could reduce water application by about
    22.22% annually, compared to the traditional overhead irrigation method. Although
    a limited number of studies have reported on the IoT-based automatic irrigation
    systems used for the ornamental industry, trends and current successes of this
    technology for other crop industries show promising potential for ornamental crop
    production. Although studies have reported the potential of using sensors-based
    technology for irrigation management, many factors impede this technology’s efficacy.
    Sensor-to-sensor variability in a particular environment could be one of them.
    The greatest variability among sensor readings occurred at volumetric water content
    levels just below the water-holding capacity of the substrate. Therefore, finding
    sensor-to-sensor variability in a particular nursery condition can greatly increase
    confidence in the data. Sensor positioning is another important factor that directly
    affects efficacy. Accurate positioning is needed in nursery conditions, particularly
    when measuring soil moisture content in container production. Sensors need to
    be placed in that part of the root zone where active water uptake occurs. Determination
    of optimal sensor numbers is another factor in specifying sensors for a nursery
    environment. The optimal number of sensors for a particular nursery depends primarily
    on the accuracy and repeatability of the sensors, variation among sensors, spatial
    variability of the nursery environment, and cost. Table 1. Summary of studies
    reported for smart nursery irrigation. 2.2. Plant Stress Detection Detection of
    stresses such as drought, disease infection, and pest pressure, recognizes unfavorable
    condition or substance that affects the growth, development or production of plants
    or crops using sensors and advanced technologies [41]. This detection helps growers
    to identify problems and take preventive actions before stresses significantly
    damage plants or crops. Two types of stresses have been identified in ornamental
    crop production: abiotic plant stress and biotic plant stress. Abiotic plant stress
    includes drought, nutrient deficiency, salinity problems, floods, etc., while
    biotic stress refers to damage caused by fungi, bacteria, insects, or weeds. Sensors,
    including RGB, thermal, and spectral, have been utilized to monitor stresses in
    ornamental crop production [42,43,44,45]. A schematic diagram of the sensor-based
    automatic crop disease detection procedure is presented in Figure 3. Figure 3.
    A schematic of a computer-vision-guided dogwood anthracnose leaf disease detection
    procedure. Table 2 represents different ornamental plant disease detection using
    advanced sensing technologies. Red-green-blue (RGB) imaging sensors with a spectrum
    range of 400–700 nm (visible range) are used to monitor ornamental plant stresses
    due to their affordability and application in other cropping systems. Velázquez-López
    et al. [42] developed an image processing-based powdery mildew disease detection
    system for rose plants by using the Open CV library. The system detected powdery
    mildew by converting RGB images to hue, saturation, and value (HSV) color space
    and achieved the highest disease region matching of 93.2% by segmenting with V
    channel using close captured images (captured at 10 cm from the rose canopies).
    Although this study achieved good performance with the traditional image segmentation
    method, the performance would not have been the same if the image capturing conditions
    had changed. This is considered a major limitation, especially for real-time disease
    detection, where multiple diseases would be present. Nuanmeesri [46] advanced
    the image processing technique from traditional image segmentation to deep learning-based
    detection in order to identify up to 15 different diseases. A hybrid deep learning
    model built by fusing convolutional neural networks (CNNs) and a support vector
    machine (SVM) were used. Researchers also tested the image registration approach
    of two imaging media for ornamental crop disease detection. Minaei et al. [45]
    registered RGB and thermal images to detect powdery mildew and gray mold disease
    on roses for developing a site-specific spraying system. A few studies have compared
    RGB imaging with spectral imaging for tulip disease detection [43,47]. The results
    reported that a spectral imaging system achieved better detection accuracies than
    RGB imaging while detecting tulip breaking virus (TBV). Hyperspectral imaging
    is a powerful tool that uses imaging and spectroscopy for detecting stresses at
    the early stage, gathering and processing feature information from a wide spectrum
    of light. Researchers have used hyperspectral sensors for ornamental crops, but
    mainly in laboratory applications due to their vulnerability in real-time field
    applications [43]. Polder et al. [48] identified Botrytis infected Cyclamen plants
    with selected features (bands) of 497, 635, 744, 839, 604, 728, 542, and 467 nm
    in a controlled greenhouse environment. Poona and Ismail [44] selected wavebands
    located across VIS, red edge, NIR, and SWIR regions to detect Fusarium circinatum
    infection in Pinus radiata seedlings at the asymptomatic stage. The study concluded
    that random forest (RF) is a good machine learning (ML) classifier to discriminate
    disease infection from spectral bands. Heim et al. [49] also used RF to differentiate
    myrtle rust-infected lemon myrtle plants and achieved an overall accuracy of 90%.
    The spectral wavebands (545, 555, 1505, and 2195 nm) were selected for discrimination.
    Considering hyperspectral systems’ slow data processing and expense, some studies
    have tried to find an alternative to hyperspectral imaging. A few studies have
    used the multispectral imaging system instead because of its faster data processing
    ability. Polder et al. [43] used an RGB-NIR-based multispectral system (range
    500–750 nm) to detect TBV disease in tulips and achieved a classification accuracy
    of 92%. They employed a linear discriminant classifier along with R, G, B, and
    NIR features to segment the plant and the soil. The author used features such
    the fraction of red pixels, mean normalized red value, mean normalized green value,
    and ratio of contour pixels of spots to classify disease in tulips. Pethybridge
    et al. [50] assessed ray blight disease (caused by Phoma ligulicola) intensity
    using a hand-held multispectral radiometer with 485, 560, 660, 830, and 1650 nm
    spectral band sensors. The study used vegetation indices, including normalized
    difference vegetative index (NDVI), green normalized difference vegetative index
    (GNDVI), difference vegetative index, and renormalized difference vegetative index
    to assess ray blight disease. Thermal imaging has also been tested for stress
    detection in ornamental plants, a technique which depicts the spatial distribution
    of temperature differences in a captured scene by converting infrared (IR) radiation
    into visible images. Jafari et al. [51] classified asymptomatic powdery mildew
    and gray mold disease on roses by fusing thermal images with visible-range captured
    images. Valuable thermal features were extracted, and artificial neural networks
    (ANN) and SVM were used to classify healthy and disease-infected rose plants.
    The thermal features include maximum, minimum, median, mode, standard deviation,
    maximum difference in temperature, skewness, kurtosis, sum of squared errors,
    and so on. Studies have been conducted for disease stress detection using thermal
    imaging; however, this type of sensing is more practical for water stress detection.
    Before conducting the above experiment, Jafari et al. [52] attempted to classify
    Botrytis cinerea infection on rose using thermal spectra and radial-basis neural
    networks. Buitrago et al. [53] analyzed the infrared spectra of plants for water
    stress detection and concluded that spectral changes in plant regions had a direct
    connection with the microstructure and biochemistry of leaves. Stress detection
    technologies are widely used in other crop industries, especially for agronomic
    crops (such as corn and soybean) and tree fruits (such as apple and citrus), but
    very few experiments have been conducted for ornamental crops (mostly in the floriculture
    industry). Very limited research, almost no studies, have been conducted for the
    woody ornamental industry. A few studies have been conducted to detect stress
    using RGB sensors because RGB cameras do not require deep technical knowledge
    to operate or use. Spectral sensors are necessary to detect stress at an asymptomatic
    or early stage. Spectral sensors have a huge potential for the ornamental industry,
    but not much progress has been previously reported. Currently, UAVs are very popular
    for crop stress detection and monitoring, but the applications of these systems
    are also very limited for the ornamental crop industry. De Castro et al. [54]
    used a UAV system to detect water stress in Cornus, Hydrangea, Spiraea, Buddleia
    and Physocarpus, and the results of this study show promise. The ornamental industry
    can benefit from using UAV-based sensing technologies for the timely detection
    and monitoring of stresses to enhance crop production. Table 2. Summary of studies
    reported for plant stress detection.    2.3. Smart Spraying Management of different
    pests and diseases is essential to ensure high quality ornamental nursery crop
    production meeting the market’s requirements [55]. Traditional management techniques
    include pruning the infected branches, removing dead or infected plants, monitoring
    diseases, trapping insects, growing pest-resistant cultivars, and pesticide applications
    [56]. Foliar pesticide application is the most effective method for preventing
    pest infestations and ensuring healthy and unblemished nursery plants [57]. In
    the United States, the greenhouse and nursery industries use about 1.3 million
    kg of pesticides every year, saving billions worth of crops [58]. Conventionally,
    radial air-assisted sprayers are the most used spray equipment for pesticide application
    in ornamental nurseries [59]. These sprayers apply pesticides to the entire field
    regardless of the plant structure, plant growth stage, and absence of plants in
    rows, thus, resulting in under- or over-spraying [60] as well as contaminating
    the environment, wasting pesticides, and increasing production cost [61]. This
    problem is more critical for the nursery industry, as there is great diversity
    in canopy structures and densities found in nursery crops. In field nursery production,
    it is a common practice that trees of different ages and cultivars are planted
    in the same row. The traditional sprayers cannot adjust sprayer settings to match
    the target tree requirements, reducing application efficiency. One way to improve
    spraying efficiency is to use sensing technologies to identify target trees for
    precise spraying applications, also referred to as smart/variable-rate-intelligent
    spraying (Figure 4). Figure 4. A schematic of a light detection and ranging (LiDAR)-guided
    variable-rate spraying system. Smart spraying is defined as the precise application
    of pesticides, performed by controlling the spray output of each nozzle based
    on the presence, structure, and canopy density of plants as obtained from sensors
    such as ultrasound, laser, and cameras [18]. In recent years, significant research
    has been conducted to develop smart spraying systems for the nursery industry.
    Different sensors, such as ultrasonic and laser, have been utilized to measure
    the canopy parameters for intelligent spraying in nursery crops. The summary of
    the reviewed studies is presented in Table 3. The initial efforts for smart nursery
    spraying were reported back in 2010 by a team of scientists from the United States
    [62]. The authors developed two precision sprayer prototypes: a hydraulic boom
    sprayer with an ultrasonic sensor for small narrow trees such as liners and an
    air-assisted sprayer with a laser scanner for other ornamental nursery species.
    The authors compared the spray consumption between a sensor-based sprayer and
    a conventional air blast sprayer at three growing stages and four travel speeds
    (3.2, 4.8, 6.4, and 8.0 km/h). The sensor-based air-assisted sprayer applied 70%,
    66%, and 52% fewer chemicals at different growth stages than conventional spraying.
    The results also reported a uniform spray deposit and coverage regardless of changes
    in the canopy size and travel speed. Jeon and Zhu [63] developed an ultrasonic-sensed
    real-time variable-rate vertical boom sprayer for nursery liners. The sprayer
    consisted of two booms with five pairs of equally spaced nozzles, with the ultrasonic
    sensor mounted 0.35 m ahead of the nozzles. Field tests were conducted for six
    different liner species at travel speeds from 3.2 to 8.0 km/h. The spray nozzles
    were triggered successfully from 4.5 to 12.5 cm ahead of the target, and the effects
    of travel speed on mean spray coverage and deposit were insignificant. Following
    this work, a study for the same precision sprayer was reported for performance
    evaluation based on spray coverage, deposit, and droplet density compared to conventional
    ones for all six-liner cultivars [64]. The reported results suggest that the spray
    coverage, deposit, and droplet density were lower in the sensor-based sprayer,
    and the spray volume was reduced by 86.4% compared to the conventional sprayer.
    Laser sensing is another technology used for precision spraying for many tree
    crops. A few studies have been reported that utilize laser scanning for smart
    spraying applications in nurseries. Chen et al. [57] developed a variable-rate
    air-assisted sprayer using a laser scanner. The authors reported that the spray
    coverage differences inside the canopies were not statistically significant at
    3.2 and 6.4 km/h travel speeds. Liu et al. [65] used a laser scanner to develop
    an intelligent variable-rate air-assisted sprayer and tested the system in a commercial
    nursery and grapevine orchard. The authors reported that the new sprayer reduced
    chemical usage by more than 50% compared to the conventional sprayer at a travel
    speed of 3.2 to 8.0 km/h. Shen et al. [66] developed an air-assisted laser-guided
    sprayer for Japanese maple nursery trees. The new sprayer consisted of a 270°
    radial-range laser scanner, embedded controller, and pulse-width-modulated (PWM)
    nozzles. The authors reported an accurate measurement of different trees and control
    of nozzles to match trees independently. The spray usage was reduced by 12 to
    43%, compared to the conventional spraying. In addition, a few studies have been
    reported for field validation of precision sprayers to control different diseases.
    Zhu et al. [59] validated the laser-guided air-assisted sprayer and reported a
    chemical saving of about 36% and 30% in the Prairifire crabapple and Honey locust
    nurseries, respectively. Chen et al. [67] also conducted a performance comparison
    of laser-guided air-assisted sprayers with conventional sprayers in commercial
    nurseries with different test plants. The author reported 56% and 52% chemical
    savings for two nurseries. Similarly, a few other studies have compared the performance
    of smart laser-guided sprayers with conventional sprayers and reported promising
    results for effective disease control in different nursery crops [61,68]. Table
    3. Summary of studies reported for smart nursery spraying.   Smart spraying for
    nursery crops using different sensing technologies, mainly ultrasonic and laser,
    has been reported in the last decade. Ultrasonic and laser sensors were integrated
    with conventional sprayers to detect the target (e.g., canopies). Although ultrasonic
    sensor-based sprayers exhibit significant chemical savings, their accuracy varies
    with temperature, humidity, and detection distance [57]. On the other hand, laser
    sensors are less influenced by weather conditions when detecting and measuring
    target characteristics [69]. Moreover, the nursery industry encounters several
    unique challenges, such as the lack of crop uniformity, varying shapes, sizes,
    growth patterns, and harvest schedules. Most existing sprayers have been developed
    for the orchard environment [59]; modifications may be required to make them usable
    for ornamental nursery crop production. Another challenge for the ornamental industry
    is its high aesthetic thresholds allowing for no visible infections. Thus, efforts
    are required to develop a smart spraying system based on the requirements of the
    nursery industry. 2.4. Plant Biometrics and Identification Information on plant
    physiology and responses to biotic/abiotic stresses are critical to determine
    the management practices required to improve productivity and sustainability in
    the nursery industry. Plant biometry (e.g., structural information) can assist
    in understanding the plant’s growth differences in diverse environments [70].
    Cultivar identification of nursery plants is also important for breeding, reproduction,
    and cultivation [71]. Plant biometry is a classification system that distinguishes
    a plant by defining its authenticity using physiological characteristics. The
    defined biometric for an individual plant should be universal, distinctive, permanent,
    and collectible [72]. Plant identification, inspection, and a precise count of
    each cultivar’s number and size distribution are essential for nursery management
    and efficiently marketing the trees [73] (Figure 5). Figure 5. A schematic of
    a UAV-based tree canopy characteristics measurement system. Different sensors,
    including cameras and LiDAR, have been utilized for nursery plant biometrics.
    The summary of the reviewed studies is presented in Table 4. The research for
    nursery plant identification using camera imaging systems started in the 1990s.
    Shearer and Holmes [74] used a camera vision system to identify tree species in
    the nursery. The study used color co-occurrence matrices derived from intensity,
    saturation, and hue to identify seven common containerized nursery plants. A total
    of 33 texture features were used for the analysis, and the reported classification
    accuracy was 91%. She et al. [75] developed a high-resolution imaging system to
    classify containerized Perennial peanut and Fire chief arborvitae plants for counting.
    he authors found that the classification accuracy of plants with flowers was higher
    (97%) than those without flowers (96%). Leiva et al. [76] developed an unmanned
    aircraft system (UAS)-based imaging system for counting container-grown Fire Chief
    arborvitae. The author developed a custom counting algorithm and tested it on
    different backgrounds, including gravel and black fabric. The reported results
    indicated counting errors of 8% and 2% for gravel and black fabric backgrounds,
    respectively. In another study, the authors used a depth camera for height measurements
    of nursery plants [77]. The authors implemented Ghostnet–YoloV4 Network for measuring
    height and counting different nursery plants, including spruce, Mongolian scotch
    pine, and Manchurian ash. They achieved an accuracy of more than 92% for measurement
    and counting. Gini et al. [78] used a UAS-based multispectral imaging system to
    classify eleven nursery plant species. The author implemented multiple grey level
    co-occurrence matrix algorithms to perform textural analysis of acquired images.
    A principal component analysis was used after feature extraction, achieving a
    classification accuracy of 87% for the selected plants. Likewise, a few studies
    have reported the application of LiDAR sensors to identify nursery plants. Weiss
    et al. [79] developed a method for identifying nursery plant species using a LiDAR
    sensor and supervised machine learning. The author used multiple machine learning
    classifiers and 83 features to identify six containerized nursery plant species,
    and achieved an accuracy of more than 98%. Similarly, LiDAR and light curtain
    sensors were used to develop a stem detection and classification system for almond
    nursery plants [73]. The authors developed a custom segmentation and thresholding
    algorithm, and the reported detection accuracies with the LiDAR and light curtain
    sensors were 95.7% and 99.48%, respectively. The success rates for dead/alive
    plant detection for the LiDAR and light curtain sensors were 93.75% and 94.16%,
    respectively. Additionally, a few other studies have reported the application
    of machine vision approaches using different machine learning and deep learning
    methodologies for detecting and classifying different flower nurseries [71,80,81,82,83,84].
    Table 4. Summary of studies reported for plant biometric measurements. Nursery
    crop management is time-consuming and labor-intensive, bringing a great need for
    automation, especially for large nursery production areas. Sensing-based plant
    biometrics, identification, and recognition are promising but challenging tasks.
    The rapid advancements in sensing, computation, artificial Intelligence (AI),
    and data analytics have allowed more detailed investigations in this domain. Research
    has been reported to identify tree species for management operations and counting
    plants for inventory control using different types of sensors, including RGB,
    multispectral, LiDAR, etc. A few recent studies have utilized state-of-art deep
    learning techniques for nursery plant classification; however, more efforts are
    needed to facilitate the growers’ use of such techniques for the profitability
    and sustainability of the nursery industry. 2.5. Other Significant Works The economics
    of production practices associated with fertilizer inputs, pest control needs,
    and labor requirements affect the nursery industry. Most nursery production operations
    are labor intensive. According to Gunjal et al. [85], labor accounts for 70% of
    the costs for nursery production. Though a few operations in nursery production
    have been mechanized, many others have not been automated. Advanced sensing and
    mechanization/automation could reduce resource consumption and labor dependence
    [73]. In this context, the ornamental nursery industry has witnessed some progress
    in different sensing, automation, and robotic applications. Table 5 presents the
    summary of works related to other sensing and automation applications for nursery
    crop production. Li et al. [86] developed a trimming robot for ornamental plants.
    The design includes a knife system and a rotary base, allowing the knife to rotate
    360 degrees to cut the plants into the desired shape. The robot was tested for
    five different nursery plant species (Aglaia odorata, Murraya exotica, Camellia
    oleifera, Osmanthus fragrans, and Radermachera sinica), and results indicated
    that the overall performance was above 93% with the time taken as 8.89 s. Zhang
    et al. [87] developed a path-planning scheme for a watering robot for containerized
    ornamental nursery plants. The authors optimized the robot’s path planning using
    a genetic algorithm with neighbor exchanging to test different watering strategies,
    and achieved promising results in terms of water savings. Sharma and Borse [88]
    developed an autonomous mobile robot to carry out different production operations
    in the nursery. The robot featured multiple sensor modules, including camera and
    climate monitoring, to perform real-time growth monitoring, disease detection,
    and the spraying of fertilizer, pesticide, and water. The platform was also equipped
    with a Zigbee communication framework to transmit the sensed data to the central
    control system. The system achieved the desired results for disease detection
    and growth monitoring; however, no technical details are provided. Similarly,
    a conceptual design of a cable-driven parallel robot (CDPR) to perform different
    operations, including seeding, weeding, and nutrition monitoring for plant nurseries
    has been presented [89]. The authors performed the operational and path planning
    simulation to execute seeding and weeding operations. Additionally, a pretrained
    VGG16 model was used for weed identification, and results showed promise, with
    an accuracy of 96.29% achieved during testing. Despite some progress, the status
    of research-based findings for robotic applications in the nursery industry lags
    far behind its contemporary industries. Table 5. Summary of works related to nursery
    production in other remaining areas. 3. Future Prospects/Directions 3.1. Advanced
    Camera Sensor Applications 3.1.1. ToF, LiDAR, and 3D Sensors Applications Advanced
    sensing technologies, such as depth cameras, time-of-flight (ToF) cameras, and
    multispectral and hyperspectral cameras, have been widely used in different agricultural
    applications. Kim et al. [90] implemented a binocular stereo-vision camera incorporated
    with a single-board computer for estimating crop height. Authors successfully
    estimated heights for Chinese cabbage, potato, sesame, radish, and soybean crops
    with a less than 5% of error in field conditions. Wang et al. [91] developed a
    ground-based remote imaging system comprised of an ultrasonic sensor, a LiDAR
    sensor, a Kinect camera, an imaging array of four digital cameras, and a custom-developed
    gimble and camera, respectively, for estimating sorghum plant height at plot level.
    The author observed that an ultrasonic sensor, a LiDAR sensor, and a Kinect camera
    resulted in strong correlations (r ≥ 0.90) between automatic and manual measurements
    for plant height estimation. The study concluded that the ground-based image acquisition
    system resulted in a comparatively higher correlation between automatic and manual
    measurements compared to the remote imaging system. They recommended LiDAR combined
    with high-resolution camera array technology, which can be an ideal methodology
    for measuring plant height effectively. The 3D/Depth cameras have found widespread
    usage in agriculture for a variety of purposes, including but not limited to yield
    estimation [92], plant phenotyping [93], and disease detection [94]. A vision-based
    under-canopy navigation and mapping system for corn and sorghum was developed
    by Gai et al. [95] using a ToF camera combined with a field robot, PhenoBot 3.0.
    They implemented linear programming techniques and developed a novel algorithm
    for reliable crop row detection and navigation. The developed system achieved
    mean absolute errors (MAE) of 3.4 cm and 3.6 cm in fields of corn and sorghum,
    respectively. Similarly, Gongal et al. [96] fused a color charge coupled device
    (CCD) camera and a ToF sensor to estimate apple fruit size under controlled lighting
    conditions. The developed system estimated apple fruit size with an accuracy of
    84.8% based on pixel size. A few of the most significant applications for ToF
    cameras in agriculture are plant height estimation [97,98], 3D reconstruction
    of the plant [99], 3D plant morphology [100], palm bunch grading [101], and so
    on. 3.1.2. Spectral Sensor Applications Cao et al. [102] developed a nitrogen
    monitoring system for tea plants using multispectral (wavelengths: 475 nm, 560
    nm, 668 nm, 717 nm, and 840 nm) and hyperspectral imaging systems. They fused
    data after preprocessing, which included multispectral image registration, calibration,
    information extraction and selection, and hyperspectral wavelength selection.
    After filtering the fused data, they feed them to regression models, including
    PLS regression, random forest regression (RFR), and support vector machine regression
    (SVR), to predict the nitrogen content of tea leaves. The support vector machine
    regression outperformed other models and achieved R2 (coefficient of determination)
    and root mean square error values of ~0.92 and ~0.06, respectively. Another researcher,
    Chandel et al. [103], also used simple linear regression models (LRs) to experiment
    on characterizing Alfalfa (Medicago sativa L.) crop vigor and yield by combining
    multispectral (465–860 nm) and thermal infrared (11,000 ± 3000 nm) image data
    collected from unmanned aerial vehicles. The model MLR-4 outperformed other models
    and achieved an R2 of 0.64. The aforementioned studies offer compelling evidence
    of increased success rates for agricultural applications of cutting-edge sensors,
    which suggest prospective uses for ornamental nursery crops. The advanced sensors
    can operate successfully in both indoor and outdoor environments. Therefore, in
    the future, automated systems for ornamental nursery corps can be developed using
    sophisticated camera sensors like 3D or depth cameras, ToF, multispectral, and
    hyperspectral. 3.2. Enhanced Deep Network Applications Due to the extraordinary
    ability to generate synthetic datasets with the same properties as training datasets,
    advanced computer vision-based techniques such as generative adversarial networks
    (GANs) and transformers are overtaking photometric and geometric-based augmentation
    approaches in a variety of agricultural problems. Abbas et al. [104] proposed
    a tomato plant disease detection system using a publicly available plant village
    tomato leaf dataset. The authors augmented the dataset using a conditional generative
    adversarial network (C-GAN) and fed the data to a pre-trained DenseNet network.
    The network successfully predicted tomato leaf diseases from healthy leaves and
    achieved an accuracy of 97.11%. The augmentation of the tomato leaf dataset improved
    the DenseNet network’s prediction by 2.77% compared to the accuracy of the original
    plant village tomato leaf dataset. Xiao et al. [105] implemented Texture Reconstruction
    Loss CycleGAN (TRL-GAN) to produce phenotypic data for the citrus greening disease
    and improve classification networks for the detection of diseased leaves. The
    authors observed that the TRL-GAN based method improved accuracy by 2.76% compared
    to the baseline model and 1.04% compared to the traditional augmentation methods
    (rotation and stretching). Zhang et al. [106] combined hyperspectral imaging with
    generative adversarial networks (DCGAN, CGAN) to expand the original dataset.
    They also observed that expansion of the dataset using GANs would improve the
    accuracy of k-nearest neighbor (kNN), SVM, and RF for haploid maize kernel classification
    by 12%, 20%, and 12%, respectively, compared to baseline models. Data enlargement
    using GANs allows for the development of detection, classification, and prediction
    models with less data on ornamental nursery crop images, which increases the model’s
    resilience in varying conditions and improves performances or accuracies. The
    augmented data can be incredibly useful when developing machine vision-based systems
    for nursery crops, such as leaf classification and disease assessment systems.
    Robots may be trained in a simulated environment using the data produced by GANs.
    3.3. Edge-AI Applications Embedded platforms combined with hardware accelerators
    and artificial intelligence-based sensing technology, called Edge Artificial Intelligence
    (Edge-AI), have made quick responses with low latency possible over cloud-based
    solutions. This technique has been adopted in different agricultural applications
    in recent years. Mazzia et al. [107] developed a real-time apple detection system
    using an Edge-AI technology. They implemented YOLOv3-Tiny algorithms on three
    different embedded platforms, including Raspberry Pi 3 B+ with Intel Movidius
    Neural Computing Stick (NCS), Nvidia’s Jetson Nano and Jetson AGX Xavier, and
    successfully detected apples in an orchard. Their system achieved an accuracy
    of 83.64% with a data processing speed up to 30 frames per second (fps) in complex
    situations. Zhang et al. [108] implemented YOLOv4-Tiny networks combined with
    improved cross stage partial networks (CSPNet) in the backbone for strawberry
    detection and implemented a developed model on the embedded platform Jetson Nano
    (NVIDIA Corporation, Santa Clara, CA, USA). Their optimized model (RTSD-Net) with
    TensorRT achieved about 25.20 fps and performed 15% faster than the original YOLOv4-tiny
    model on Jetson Nano without significant loss of accuracy. Other promising applications
    of Edge-AI are air temperature forecasting [109], environment monitoring [110],
    autonomous navigation systems [111] and so on. Edge-AI technology can potentially
    be applied for weeding, spraying, and robot navigation in ornament nursery crop
    production. Weed maps generated by UAVs may be combined into autonomous robots
    for site-specific weed management and pesticide applications in the field. Embedded
    hardwire (Raspberry Pi, Jetson Nano, and Jetson TX2) paired with sensors (color
    camera, depth camera), and AI may be implemented to develop Edge-AI technology
    for ornamental nursery crops. Vision-based robots using Edge-AI technology can
    be an aid to robot navigation for accomplishing site-specific applications in
    nursery crops. 3.4. Radio Frequency Identification Tagging Applications Radio
    frequency identification (RFID) technology has become popular in different fields
    of agriculture, including soil environment monitoring, soil moisture monitoring,
    soil solarization, and automation in irrigation. Deng et al. [112] designed and
    developed a novel system that integrates an RFID sensor with LoRa to provide a
    low-cost, low-power, and efficient soil environment monitoring solution. The authors
    embedded RFID tags at 60 cm into the soil; the tags can communicate with the monitoring
    center through radio communication (LoRa) placed in the patrol car. Their system
    would be able to establish communication within a range of 1.3 m without compromising
    relative measurement errors (temperature: 1.5% and soil moisture content: 1.0%).
    The study achieved a higher communication rate (above 90%) at a patrol speed of
    33 kmh−1. Luvisi et al. [113] developed a system that monitors different types
    of soil solarization (sandy, loam, and clay soils) using an RFID sensor and biodegradable
    films. They placed soil sensors at different depths (5 and 10 cm) along with a
    soil profile at different soil moisture holding capacities (10%, 50%, and 90%)
    and measured the effect of soil solarization treatment. In the second and third
    weeks of treatment, they found that the maximum soil temperature at depths of
    5 and 10 cm increased to 9–13 °C and 11–14 °C, respectively. They also found that
    the method was 90% reliable. Vellidis et al. [114] implemented soil moisture sensors
    (Watermark® granular resistive type) and thermocouple temperature sensors coupled
    with RFID tags (WhereNet®, Santa Clara, CA, USA) for developing sensor nodes to
    automate irrigation schedules for cotton crops. The nodes were connected to a
    laptop computer via wireless communication. The developed system contained an
    array of sensors, and data obtained from the sensors could assist in decision-making
    and scheduling irrigation for the cotton field. Several researchers also contributed
    to RFID-based soil moisture sensor developments [115,116,117,118,119]. RFID-based
    sensors also have other applications in agriculture, including tracking plants
    in pots in greenhouses [120], tracing food quality [121], and monitoring livestock
    [122]. The above studies and their success rates clearly show the potential of
    using RFID-based sensors in ornamental nursery crop applications. The potential
    applications of RFID-based sensors for nursery crops include soil environment
    monitoring, soil solarization, and automating irrigation scheduling, in indoor
    or field conditions. The RFID tags can be used in conjunction with soil monitoring
    sensors such as soil, moisture, soil micronutrient, gas, etc. to build sensor
    nodes and receive in-field data through wireless communications. Readings from
    sensor nodes may be used with machine learning and deep learning to make decisions
    in various field management operations. 3.5. Integrated Robotics Applications
    Robots integrated with computer vision have been widely adopted in many areas
    of agriculture, such as plant detection and mapping, fruit detection and localizations,
    robot-based harvesting, navigation, and obstacle detection systems. Weiss and
    Biber [123] developed a ground-based robot for maize plant recognition, mapping,
    and navigation using a 3D LiDAR sensor-based micro-electro-mechanical system (FX6
    3D LiDAR). The robot was constructed using modeled artificial maize plants and
    tested on a small corn field. The designed robot achieved detection and mapping
    accuracy of around 60%–70%. They measured a greater localization deviation in
    the direction of the row, measuring 1–2 cm. Ge et al. [124] developed a strawberry
    fruit localization method using a strawberry harvesting robot with an RGB-D camera.
    The authors implemented a convolutional neural network (i.e., Mask-RCNN) on RGB
    images for strawberry fruit segmentation and combined depth values to obtain 3D
    points of fruits. The 3D point was then used to obtain fruit localization using
    the shape completion method. The system achieved a minimum center deviation of
    6.9 mm between ground truths and automated measurements. Skoczeń et al. [125]
    also proposed a similar approach to develop an automatic obstacle-detection robot.
    They implemented an RGB-D camera (Intel RealSense D435i) for robot vision, reached
    obstacle segmentation accuracy of 98.11%, and obtained a depth measurement error
    of 38 cm. Ji et al. [126] developed a machine vision algorithm for a green pepper
    harvesting robot. The contrast values of images obtained by the camera (MX808)
    for various light conditions (normal, weak, and strong light) were then increased
    to make the green pepper stand out from the background leaf. The energy-driven
    sampling (SEEDS) algorithm is then fed the improved images to build super pixel
    blocks. The manifold ranking (MR) algorithm, the CART classifier, and the conditional
    random field (CRF) algorithm were used to recognize green pepper from super pixel
    blocks, followed by morphological processing. Classifiers were evaluated on 500
    images obtained from different lighting conditions. The algorithm manifold ranking
    outperformed other classifiers and achieved an accuracy of 83.6%; it took 116
    milliseconds to run the entire evaluation on Intel Core (TM) i5-4210U CPU (2.80
    GHz, 8 GB). Gai et al. [127] developed a cherry fruit detection system using a
    high-resolution Sony DSC-HX400 camera combined with a YOLO-V4 Dense Model network.
    The study compared the developed algorithm’s accuracy with the base model, YOLO-V3-dense,
    and YOLO-V4 and observed an improved detection rate (F1 scores: 94.70%). YOLO-V4
    Dense Model took 0.467 s on an Intel Core (TM) i7-7700 CPU (3.60 GHz, 4 GB) with
    a Tesla V100 GPU for processing an image of 1280 by 800 pixels. They found robotic
    intelligent picking is possible using the developed system. Jia et al. [128] also
    developed a robot vision using a high-resolution camera (6000 × 4000 pixel) for
    an apple harvesting robot using an optimized Mask R-CNN. The developed system
    achieved a high rate of detection (precision: 97.31%; recall: 95.70%). The development
    of robot vision using high-resolution camera sensors combined with deep learning
    techniques can be adopted to develop ornamental crop management robots. Applications,
    such as spraying, weeding, soil sampling, and digging could be effectively solved,
    enabling different operations in nursery crops. Robot vision combined with machine
    and deep learning may also be implemented in nurseries for plant counting, stem
    counting, and other essential tasks. 4. Discussion and Conclusions The ornamental
    crop industry in the U.S. depends largely on agricultural workers. Sensing and
    automation technologies offer a huge potential to reduce labor dependency and
    ensure the efficient use of resources required by the ornamental industry. In
    turn, the information in this article can aid the nursery industry in knowing
    about the specific area where technological development takes place and what those
    technologies are, and in considering what types of sensors, algorithms and tools
    are advantageous to develop effective technologies in different production operations.
    Current sensing and automation technology usage varies by production operations.
    For instance, smart irrigation has primarily relied on soil moisture sensors,
    and stress detection has largely depended on camera sensors. Despite the fact
    that not many studies have used IoT or Edge-AI-based IoT systems, these could
    be potential technologies for automating irrigation operations for ornamental
    crops. The Edge-AI-based systems and AI-of-things (AIoT) are relatively new concepts
    in agricultural applications, and successes in other cropping systems have shown
    promise for the ornamental nursery industry. Similar to irrigation, a very limited
    number of studies have been conducted for ornamental plant stress detection. One
    important fact regarding stresses is that they have to be detected early to minimize
    their effect on crops. Spectral cameras, including hyperspectral and multispectral
    devices, are the two sensors currently being used to detect stresses at the asymptomatic
    stage. However, the major challenge of detecting plant stresses is to detect them
    in real-time field conditions. Researchers have been trying to address challenges
    such as illumination variations, data processing speed, and environmental factors
    to make a viable system for real-time applications. More efforts are required,
    though, especially for the hyperspectral system, due to its slow data processing
    issues. Fluorescence sensors are another spectral technology that has not been
    explored much for ornamental crops, one which can provide improved spectroscopy
    data and can be useful for early plant stress detection. LiDAR is one of the powerful
    tools that can be used to accurately measure plant biometric information (plant
    height, width, canopy volume and density, etc.) to develop a smart or variable-rate
    spraying system. However, this tool cannot be used for spot spraying operations
    for disease management because the LiDAR sensor can only provide point cloud information
    (unlike cameras, it does not provide any color information). Integrated LiDAR
    and camera systems could potentially be tools for smart spraying systems for ornamental
    nursery crop production. The advantages and disadvantages of different sensors
    are presented in Table 6. Table 6. Advantages and disadvantages of different sensors
    for ornamental crops. Surprisingly, very few applications have been noticed for
    UAVs in ornamental crops, despite extensive implications these days in the agronomic,
    tree fruit and row crops. The low manufacturing cost and fast operation speed
    have opened up further research opportunities for UAVs. UAVs are becoming an essential
    part of remote sensing and can be an effective tool for ornamental plant stress
    detection and monitoring crop growth and development. The UAVs bring advantages
    over ground-based systems, such as their flexibility in capturing ultra-high spatial
    and temporal resolution data at any terrain conditions, and they require less
    time to collect data. However, developing manipulation systems for UAVs that can
    act with precision in fields is a challenging task requiring extensive investigations.
    The coordination between UAVs and ground-based systems has been receiving increasing
    attention in recent years, and has the potential to benefit the ornamental crop
    industry for site-specific management. Calibrating sensors is essential to reduce
    variability when multiple sensors are involved in a particular crop management
    operation. Recent advances in deep learning models (e.g., CNNs, GANs, transformers)
    have contributed significantly to different industries, including agriculture,
    but ornamental crops remain at the bottom user of these impressive innovations.
    These models can help predict stress, pest pressure, growth, yield, etc. RFID,
    a new crop tracking technology, can increase production operations’ efficacy and
    help nurseries to reduce the burden for growers or laborers by automating the
    inspections and recording accurate ornamental crop data instantly. Agricultural
    robotics is another critical area that can benefit the ornamental crop industry
    enormously. Currently, the agricultural workforce conducts most production operations,
    such as planting, pruning/shape forming, weeding, disease monitoring, and harvesting.
    These operations are vastly labor-intensive and cost a large portion of production
    expenses. Autonomous robotic systems can replace the humans conducting these operations.
    The systems will reduce time and production expenses in the long run. The ornamental
    industry lacks automation/robotic technologies; therefore, significant research
    needs to be done on these topics to develop some implementable robotic systems.
    As the majority of the ornamental crop farms are not so large compared to other
    major cropping industries, adopting advanced sensing and automation technologies
    would be a major challenge due to the initial high investment. Integrated multipurpose
    automated technologies will be helpful for this purpose. For instance, when a
    particular automated system can work for multiple operations (e.g., planting,
    pruning, and harvesting) for ornamental crops by replacing a few parts of the
    system, growers would be interested in buying and adopting those multipurpose
    systems. Researchers and manufacturers need to consider these points while working
    on or developing technologies for the ornamental nursery crop industry. Although
    not much progress in sensing and automation technologies has been observed for
    ornamental nursery crop production, a few mechanized systems are available for
    commercial scales. These include mixing systems to mix substrate or soil, potting
    systems to fill containers, tray filling systems to fill trays, planters to plant
    nursery liners in containers, seeding systems to sow and space out seeds on pots
    or containers, etc. Pack Manufacturing (http://packmfg.com/) (Pack Manufacturing
    Inc., McMinnville, TN, USA) is a leading company in the sale of these mechanized
    systems. A vital challenge in technology development for ornamental nursery crops
    is the substantial number of available plant species. Various ornamental plants
    have different morphologies, characteristics, canopy structures, and growth requirements.
    It is necessary to understand the types of plants grown and their production requirements
    to align the sensing and automation technologies with the production needs to
    facilitate industry operations. Author Contributions All authors contributed equally
    to this article. All authors have read and agreed to the published version of
    the manuscript. Funding This study was majorly supported by the United States
    Department of Agriculture (USDA)’s National Institute of Food and Agriculture
    (NIFA) Research Capacity Fund (Evans-Allen) under NIFA Accession No. 7003739 and
    Organizational Project No. TENX2203-CCOCP and partially supported by the USDA’s
    NIFA Federal Appropriations under TEX09954 and Accession No. 7002248. Conflicts
    of Interest The authors declare no conflict of interest. References USDA. U.S.
    Horticulture in 2014 (Publication ACH12-33); United States Department of Agriculture:
    Beltsville, MD, USA. Available online: https://www.agcensus.usda.gov/Publications/2012/Online_Resources/Highlights/Horticulture/Census_of_Horticulture_Highlights.pdf
    (accessed on 21 November 2022). Lea-Cox, J.D.; Zhao, C.; Ross, D.S.; Bilderback,
    T.E.; Harris, J.R.; Day, S.D.; Hong, C.; Yeager, T.H.; Beeson, R.C.; Bauerle,
    W.L.; et al. A Nursery and Greenhouse Online Knowledge Center: Learning Opportunities
    for Sustainable Practice. HortTechnology 2010, 20, 509–517. [Google Scholar] [CrossRef]
    Majsztrik, J.C.; Fernandez, R.T.; Fisher, P.R.; Hitchcock, D.R.; Lea-Cox, J.;
    Owen, J.S.; Oki, L.R.; White, S.A. Water Use and Treatment in Container-Grown
    Specialty Crop Production: A Review. Water. Air. Soil Pollut. 2017, 228, 151.
    [Google Scholar] [CrossRef] [PubMed] Majsztrik, J.; Lichtenberg, E.; Saavoss,
    M. Ornamental Grower Perceptions of Wireless Irrigation Sensor Networks: Results
    from a National Survey. HortTechnology 2013, 23, 775–782. [Google Scholar] [CrossRef]
    Wheeler, W.D.; Thomas, P.; van Iersel, M.; Chappell, M. Implementation of Sensor-Based
    Automated Irrigation in Commercial Floriculture Production: A Case Study. HortTechnology
    2018, 28, 719–727. [Google Scholar] [CrossRef] Rihn, A.L.; Velandia, M.; Warner,
    L.A.; Fulcher, A.; Schexnayder, S.; LeBude, A. Factors Correlated with the Propensity
    to Use Automation and Mechanization by the US Nursery Industry. Agribusiness 2022,
    39, 110–130. [Google Scholar] [CrossRef] USDA ERS. Farm Labor. Available online:
    https://www.ers.usda.gov/topics/farm-economy/farm-labor/ (accessed on 20 November
    2022). McClellan, M. Don’t Wait, Automate. Available online: https://www.nurserymag.com/article/five-tips-automation/
    (accessed on 20 November 2022). Silwal, A.; Davidson, J.R.; Karkee, M.; Mo, C.;
    Zhang, Q.; Lewis, K. Design, Integration, and Field Evaluation of a Robotic Apple
    Harvester. J. Field Robot. 2017, 34, 1140–1159. [Google Scholar] [CrossRef] Liu,
    B.; Ding, Z.; Tian, L.; He, D.; Li, S.; Wang, H. Grape Leaf Disease Identification
    Using Improved Deep Convolutional Neural Networks. Front. Plant Sci. 2020, 11,
    1082. [Google Scholar] [CrossRef] Xiong, Y.; Peng, C.; Grimstad, L.; From, P.J.;
    Isler, V. Development and Field Evaluation of a Strawberry Harvesting Robot with
    a Cable-Driven Gripper. Comput. Electron. Agric. 2019, 157, 392–402. [Google Scholar]
    [CrossRef] Ye, H.; Huang, W.; Huang, S.; Cui, B.; Dong, Y.; Guo, A.; Ren, Y.;
    Jin, Y. Recognition of Banana Fusarium Wilt Based on UAV Remote Sensing. Remote
    Sens. 2020, 12, 938. [Google Scholar] [CrossRef] Gajjar, R.; Gajjar, N.; Thakor,
    V.J.; Patel, N.P.; Ruparelia, S. Real-Time Detection and Identification of Plant
    Leaf Diseases Using Convolutional Neural Networks on an Embedded Platform. Vis.
    Comput. 2022, 38, 2923–2938. [Google Scholar] [CrossRef] Lehnert, C.; English,
    A.; Mccool, C.; Tow, A.W.; Perez, T. Autonomous Sweet Pepper Harvesting for Protected
    Cropping Systems. IEEE Robot. Autom. Lett. 2017, 2, 872–879. [Google Scholar]
    [CrossRef] Birrell, S.; Hughes, J.; Cai, J.Y.; Iida, F. A Field-Tested Robotic
    Harvesting System for Iceberg Lettuce. J. Field Robot. 2020, 37, 225–245. [Google
    Scholar] [CrossRef] [PubMed] Yasukawa, S.; Li, B.; Sonoda, T.; Ishii, K. Development
    of a Tomato Harvesting Robot. Proc. Int. Conf. Artif. Life Robot. 2017, 22, 408–411.
    [Google Scholar] [CrossRef] Amatya, S.; Karkee, M.; Gongal, A.; Zhang, Q.; Whiting,
    M.D. Detection of Cherry Tree Branches with Full Foliage in Planar Architecture
    for Automated Sweet-Cherry Harvesting. Biosyst. Eng. 2016, 146, 3–15. [Google
    Scholar] [CrossRef] Mahmud, M.S.; Zahid, A.; He, L.; Martin, P. Opportunities
    and Possibilities of Developing an Advanced Precision Spraying System for Tree
    Fruits. Sensors 2021, 21, 3262. [Google Scholar] [CrossRef] [PubMed] Lu, J.; Hu,
    J.; Zhao, G.; Mei, F.; Zhang, C. An In-Field Automatic Wheat Disease Diagnosis
    System. Comput. Electron. Agric. 2017, 142, 369–379. [Google Scholar] [CrossRef]
    Jiang, P.; Chen, Y.; Liu, B.; He, D.; Liang, C. Real-Time Detection of Apple Leaf
    Diseases Using Deep Learning Approach Based on Improved Convolutional Neural Networks.
    IEEE Access 2019, 7, 59069–59080. [Google Scholar] [CrossRef] Abdulridha, J.;
    Batuman, O.; Ampatzidis, Y. UAV-Based Remote Sensing Technique to Detect Citrus
    Canker Disease Utilizing Hyperspectral Imaging and Machine Learning. Remote Sens.
    2019, 11, 1373. [Google Scholar] [CrossRef] Torres-Sánchez, J.; Peña, J.M.; de
    Castro, A.I.; López-Granados, F. Multi-Temporal Mapping of the Vegetation Fraction
    in Early-Season Wheat Fields Using Images from UAV. Comput. Electron. Agric. 2014,
    103, 104–113. [Google Scholar] [CrossRef] Pearse, G.D.; Tan, A.Y.S.; Watt, M.S.;
    Franz, M.O.; Dash, J.P. Detecting and Mapping Tree Seedlings in UAV Imagery Using
    Convolutional Neural Networks and Field-Verified Data. ISPRS J. Photogramm. Remote
    Sens. 2020, 168, 156–169. [Google Scholar] [CrossRef] Zhang, C.; Atkinson, P.M.;
    George, C.; Wen, Z.; Diazgranados, M.; Gerard, F. Identifying and Mapping Individual
    Plants in a Highly Diverse High-Elevation Ecosystem Using UAV Imagery and Deep
    Learning. ISPRS J. Photogramm. Remote Sens. 2020, 169, 280–291. [Google Scholar]
    [CrossRef] Feng, A.; Zhou, J.; Vories, E.D.; Sudduth, K.A.; Zhang, M. Yield Estimation
    in Cotton Using UAV-Based Multi-Sensor Imagery. Biosyst. Eng. 2020, 193, 101–114.
    [Google Scholar] [CrossRef] Maja, J.M.J.; Robbins, J. Controlling Irrigation in
    a Container Nursery Using IoT. AIMS Agric. Food 2018, 3, 205–215. [Google Scholar]
    [CrossRef] You, A.; Parayil, N.; Krishna, J.G.; Bhattarai, U.; Sapkota, R.; Ahmed,
    D.; Whiting, M.; Karkee, M.; Grimm, C.M.; Davidson, J.R. An Autonomous Robot for
    Pruning Modern, Planar Fruit Trees. arXiv 2022, arXiv:220607201. [Google Scholar]
    Liu, B.; Tan, C.; Li, S.; He, J.; Wang, H. A Data Augmentation Method Based on
    Generative Adversarial Networks for Grape Leaf Disease Identification. IEEE Access
    2020, 8, 102188–102198. [Google Scholar] [CrossRef] Lea-Cox, J.D.; Bauerle, W.L.;
    van Iersel, M.W.; Kantor, G.F.; Bauerle, T.L.; Lichtenberg, E.; King, D.M.; Crawford,
    L. Advancing Wireless Sensor Networks for Irrigation Management of Ornamental
    Crops: An Overview. HortTechnology 2013, 23, 717–724. [Google Scholar] [CrossRef]
    Cornejo, C.; Haman, D.Z.; Yeager, T.H. Evaluation of Soil Moisture Sensors, and
    Their Use to Control Irrigation Systems for Containers in the Nursery Industry;
    ASAE Paper No. 054056; ASAE: St. Joseph, MI, USA, 2005. [Google Scholar] [CrossRef]
    Lea-Cox, J.D.; Ristvey, A.G.; Kantor, G.F. Using Wireless Sensor Technology to
    Schedule Irrigations and Minimize Water Use in Nursery and Greenhouse Production
    Systems ©. Comb. Proc. Int. Plant Propagators Soc. 2008, 58, 512–518. [Google
    Scholar] Chappell, M.; Dove, S.K.; van Iersel, M.W.; Thomas, P.A.; Ruter, J. Implementation
    of Wireless Sensor Networks for Irrigation Control in Three Container Nurseries.
    HortTechnology 2013, 23, 747–753. [Google Scholar] [CrossRef] van Iersel, M.W.;
    Chappell, M.; Lea-Cox, J.D. Sensors for Improved Efficiency of Irrigation in Greenhouse
    and Nursery Production. HortTechnology 2013, 23, 735–746. [Google Scholar] [CrossRef]
    Wheeler, W.D.; Chappell, M.; van Iersel, M.; Thomas, P. Implementation of Soil
    Moisture Sensor Based Automated Irrigation in Woody Ornamental Production. J.
    Environ. Hortic. 2020, 38, 1–7. [Google Scholar] [CrossRef] Kim, J.; Chappell,
    M.; Van Iersel, M.W.; Lea-Cox, J.D. Wireless Sensors Networks for Optimization
    of Irrigation, Production, and Profit in Ornamental Production. Acta Hortic. 2014,
    1037, 643–649. [Google Scholar] Coates, R.W.; Delwiche, M.J.; Broad, A.; Holler,
    M.; Evans, R.; Oki, L.; Dodge, L. Wireless Sensor Network for Precision Irrigation
    Control in Horticultural Crops; American Society of Agricultural and Biological
    Engineers: St. Joseph, MI, USA, 2012; Volume 3. [Google Scholar] Belayneh, B.E.;
    Lea-Cox, J.D.; Lichtenberg, E. Costs and Benefits of Implementing Sensor-Controlled
    Irrigation in a Commercial Pot-in-Pot Container Nursery. HortTechnology 2013,
    23, 760–769. [Google Scholar] [CrossRef] [Green Version] Lea-Cox, J.D.; Belayneh,
    B.E. Implementation of Sensor-Controlled Decision Irrigation Scheduling in Pot-in-Pot
    Nursery Production. Acta Hortic. 2013, 1034, 93–100. [Google Scholar] [CrossRef]
    Manuel Banda-Chávez, J.; Pablo Serrano-Rubio, J.; Osvaldo Manjarrez-Carrillo,
    A.; Maria Rodriguez-Vidal, L.; Herrera-Guzman, R. Intelligent Wireless Sensor
    Network for Ornamental Plant Care. In Proceedings of the IECON 2018—44th Annual
    Conference of the IEEE Industrial Electronics Society, Washington, DC, USA, 21–23
    October 2018; Volume 1. [Google Scholar] Beeson, R., Jr.; Brooks, J. Evaluation
    of a Model Based on Reference Crop Evapotranspiration (ETo) for Precision Irrigation
    Using Overhead Sprinklers during Nursery Production of Ligustrum Japonica. Proc.
    V Int. Symp. Irrig. Hortic. Crops 2006, 792, 85–90. [Google Scholar] Zubler, A.V.;
    Yoon, J.Y. Proximal Methods for Plant Stress Detection Using Optical Sensors and
    Machine Learning. Biosensors 2020, 10, 193. [Google Scholar] [CrossRef] Velázquez-López,
    N.; Sasaki, Y.; Nakano, K.; Mejía-Muñoz, J.M.; Kriuchkova, E.R. Detection of Powdery
    Mildew Disease on Rose Using Image Processing with Open CV. Rev. Chapingo Ser.
    Hortic. 2011, 17, 151–160. [Google Scholar] [CrossRef] Polder, G.; van der Heijden,
    G.W.A.M.; van Doorn, J.; Baltissen, T.A.H.M.C. Automatic detection of tulip breaking
    virus (TBV) in tulip fields using machine vision. Biosyst. Eng. 2014, 117, 35–42.
    [Google Scholar] [CrossRef] Poona, N.K.; Ismail, R. Using Boruta-Selected Spectroscopic
    Wavebands for the Asymptomatic Detection of Fusarium Circinatum Stress. IEEE J.
    Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 3764–3772. [Google Scholar] [CrossRef]
    Minaei, S.; Jafari, M.; Safaie, N. Design and Development of a Rose Plant Disease-Detection
    and Site-Specific Spraying System Based on a Combination of Infrared and Visible
    Images. J. Agric. Sci. Technol. 2018, 20, 23–36. [Google Scholar] Nuanmeesri,
    S. A Hybrid Deep Learning and Optimized Machine Learning Approach for Rose Leaf
    Disease Classification. Eng. Technol. Appl. Sci. Res. 2021, 11, 7678–7683. [Google
    Scholar] [CrossRef] Polder, G.; van der Heijden, G.W.A.M.; van Doorn, J.; Clevers,
    J.G.P.W.; van der Schoor, R.; Baltissen, A.H.M.C. Detection of the Tulip Breaking
    Virus (TBV) in Tulips Using Optical Sensors. Precis. Agric. 2010, 11, 397–412.
    [Google Scholar] [CrossRef] Polder, G.; Pekkeriet, E.; Snikkers, M. A Spectral
    Imaging System for Detection of Botrytis in Greenhouses. In Proceedings of the
    EFITA-WCCA-CIGR Conference “Sustainable Agriculture through ICT Innovation”, Turin,
    Italy, 24–27 June 2013. [Google Scholar] Heim, R.H.J.; Wright, I.J.; Allen, A.P.;
    Geedicke, I.; Oldeland, J. Developing a Spectral Disease Index for Myrtle Rust
    (Austropuccinia psidii). Plant Pathol. 2019, 68, 738–745. [Google Scholar] [CrossRef]
    Pethybridge, S.J.; Hay, F.; Esker, P.; Groom, T.; Wilson, C.; Nutter, F.W. Visual
    and Radiometric Assessments for Yield Losses Caused by Ray Blight in Pyrethrum.
    Crop Sci. 2008, 48, 343–352. [Google Scholar] [CrossRef] Jafari, M.; Minaei, S.;
    Safaie, N. Detection of Pre-Symptomatic Rose Powdery-Mildew and Gray-Mold Diseases
    Based on Thermal Vision. Infrared Phys. Technol. 2017, 85, 170–183. [Google Scholar]
    [CrossRef] Jafari, M.; Minaei, S.; Safaie, N.; Torkamani-Azar, F.; Sadeghi, M.
    Classification Using Radial-Basis Neural Networks Based on Thermographic Assessment
    of Botrytis Cinerea Infected Cut Rose Flowers Treated with Methyl Jasmonate. J.
    Crop Prot. 2016, 5, 591–602. [Google Scholar] [CrossRef] Buitrago, M.F.; Groen,
    T.A.; Hecker, C.A.; Skidmore, A.K. Changes in Thermal Infrared Spectra of Plants
    Caused by Temperature and Water Stress. ISPRS J. Photogramm. Remote. Sens. 2016,
    111, 22–31. [Google Scholar] [CrossRef] de Castro, A.; Maja, J.M.; Owen, J.; Robbins,
    J.; Peña, J. Experimental Approach to Detect Water Stress in Ornamental Plants
    Using SUAS-Imagery. In Proceedings of the Autonomous Air and Ground Sensing Systems
    for Agricultural Optimization and Phenotyping III, Orlando, FL, USA, 16–17 April
    2018; Volume 10664, pp. 178–188. [Google Scholar] Braman, S.; Chappell, M.; Chong,
    J.; Fulcher, A.; Gauthier, N.; Klingeman, W.; Knox, G.; LeBude, A.; Neal, J.;
    White, S.; et al. Pest Management Strategic Plan for Container and Field-Produced
    Nursery Crops: Revision 2015. In Proceedings of the Southern Nursery Integrated
    Pest Management Working Group (SNIPM), Mills River, NC, USA, 30–31 July 2009;
    Volume 236. [Google Scholar] Mizell, R.F.; Short, D.E. Integrated Pest Management
    in the Commercial Ornamental Nursery. 2015; Volume 8. Available online: https://site.caes.uga.edu/sehp/files/2020/03/UF-IPM-in-the-Commercial-Ornamental-Nursery.pdf
    (accessed on 20 November 2022). Chen, Y.; Zhu, H.; Ozkan, H.E. Development of
    a Variable-Rate Sprayer with Laser Scanning Sensor to Synchronize Spray Outputs
    to Tree Structures. Trans. ASABE 2012, 55, 773–781. [Google Scholar] [CrossRef]
    Hudson, W.G.; Garber, M.P.; Oetting, R.D.; Mizell, R.F.; Chase, A.R.; Bondari,
    K. Pest Management in the United States Greenhouse and Nursery Industry: V. Insect
    and Mite Control. HortTechnology 1996, 6, 216–221. [Google Scholar] [CrossRef]
    Zhu, H.; Rosetta, R.; Reding, M.E.; Zondag, R.H.; Ranger, C.M.; Canas, L.; Fulcher,
    A.; Derksen, R.C.; Ozkan, H.E.; Krause, C.R. Validation of a Laser-Guided Variable-Rate
    Sprayer for Managing Insects in Ornamental Nurseries. Trans. ASABE 2017, 60, 337–345.
    [Google Scholar] [CrossRef] Fox, R.D.; Derksen, R.C.; Zhu, H.; Brazee, R.D.; Svensson,
    S.A. A History of Air-Blast Sprayer Development and Future Prospects. Trans. ASABE
    2008, 51, 405–410. [Google Scholar] [CrossRef] Chen, L.; Zhu, H.; Horst, L.; Wallhead,
    M.; Reding, M.; Fulcher, A. Management of Pest Insects and Plant Diseases in Fruit
    and Nursery Production with Laser-Guided Variable-Rate Sprayers. HortScience 2021,
    56, 94–100. [Google Scholar] [CrossRef] Zhu, H.; Jeon, H.Y.; Gu, J.; Derksen,
    R.C.; Krause, C.R.; Ozkan, H.E.; Chen, Y.; Reding, M.E.; Ranger, C.M.; Cañas,
    L.; et al. Development of Two Intelligent Spray Systems for Ornamental Nurseries©.
    In Proceedings of the International Plant Propagators’ Society, Miami, FL, USA,
    1 August 2010; Volume 60, p. 322. [Google Scholar] Jeon, H.; Zhu, H. Development
    of a Variable-Rate Sprayer for Nursery Liner Applications. Trans. ASABE 2012,
    55, 303–312. [Google Scholar] [CrossRef] Jeon, H.Y.; Zhu, H.; Derksen, R.C.; Ozkan,
    H.E.; Krause, C.R.; Fox, R.D. Performance Evaluation of a Newly Developed Variable-Rate
    Sprayer for Nursery Liner Applications. Trans. ASABE 2011, 54, 773–781. [Google
    Scholar] Liu, H.; Zhu, H.; Shen, Y.; Chen, Y. Embedded Computer-Controlled Laser
    Sensor-Guided Air-Assisted Precision Sprayer Development. In Proceedings of the
    ASABE Annual International Meeting, New Orleans, LA, USA, 26–29 July 2015. [Google
    Scholar] Shen, Y.; Zhu, H.; Liu, H.; Chen, Y.; Ozkan, E. Development of a Laser-Guided,
    Embedded-Computercontrolled, Air-Assisted Precision Sprayer. Trans. ASABE 2017,
    60, 1827–1838. [Google Scholar] [CrossRef] Chen, L.; Wallhead, M.; Zhu, H.; Fulcher,
    A. Control of Insects and Diseases with Intelligent Variable-Rate Sprayers in
    Ornamental Nurseries. J. Environ. Hortic. 2019, 37, 90–100. [Google Scholar] [CrossRef]
    Fessler, L.; Fulcher, A.; Schneider, L.; Wright, W.C.; Zhu, H. Reducing the Nursery
    Pesticide Footprint with Laser-Guided, Variable-Rate Spray Application Technology.
    HortScience 2021, 141, 1572–1584. [Google Scholar] [CrossRef] Wei, J.; Salyani,
    M. Development of a Laser Scanner for Measuring Tree Canopy Characteristics: Phase
    1. Prototype Development. Trans. Am. Soc. Agric. Eng. 2004, 47, 2101–2107. [Google
    Scholar] [CrossRef] Campbell, J.; Sarkhosh, A.; Habibi, F.; Ismail, A.; Gajjar,
    P.; Zhongbo, R.; Tsolova, V.; El-sharkawy, I. Biometrics Assessment of Cluster-
    and Berry-related Traits of Muscadine Grape Population. Plants 2021, 10, 1067.
    [Google Scholar] [CrossRef] Zhang, R.; Tian, Y.; Zhang, J.; Dai, S.; Hou, X.;
    Wang, J.; Guo, Q. Metric Learning for Image-Based Flower Cultivars Identification.
    Plant Methods 2021, 17, 1–14. [Google Scholar] [CrossRef] [PubMed] Maltoni, D.;
    Maio, D.; Jain, A.K.; Prabhakar, S. Handbook of Fingerprint Recognition; Springer
    Science and Business Media: New York, NY, USA, 2009. [Google Scholar] [CrossRef]
    Garrido, M.; Perez-Ruiz, M.; Valero, C.; Gliever, C.J.; Hanson, B.D.; Slaughter,
    D.C. Active Optical Sensors for Tree Stem Detection and Classification in Nurseries.
    Sens. Switz. 2014, 14, 10783–10803. [Google Scholar] [CrossRef] [PubMed] Shearer,
    S.A.; Holmes, R.G. Plant identification using color co-occurrence matrices. Trans.
    ASAE 1990, 33, 1237–1244. [Google Scholar] [CrossRef] She, Y.; Ehsani, R.; Robbins,
    J.; Leiva, J.N.; Owen, J. Applications of High-Resolution Imaging for Open Field
    Container Nursery Counting. Remote Sens. 2018, 10, 2018. [Google Scholar] [CrossRef]
    Leiva, J.N.; Robbins, J.; Saraswat, D.; She, Y.; Ehsani, R. Evaluating Remotely
    Sensed Plant Count Accuracy with Differing Unmanned Aircraft System Altitudes,
    Physical Canopy Separations, and Ground Covers. J. Appl. Remote Sens. 2017, 11,
    036003. [Google Scholar] [CrossRef] Yuan, X.; Li, D.; Sun, P.; Wang, G.; Ma, Y.
    Real-Time Counting and Height Measurement of Nursery Seedlings Based on Ghostnet–YoloV4
    Network and Binocular Vision Technology. Forests 2022, 13, 1459. [Google Scholar]
    [CrossRef] Gini, R.; Sona, G.; Ronchetti, G.; Passoni, D.; Pinto, L. Improving
    Tree Species Classification Using UAS Multispectral Images and Texture Measures.
    ISPRS Int. J. Geo-Inf. 2018, 7, 315. [Google Scholar] [CrossRef] Weiss, U.; Biber,
    P.; Laible, S.; Bohlmann, K.; Zell, A. Plant Species Classification Using a 3D
    LIDAR Sensor and Machine Learning. In Proceedings of the 2010 Ninth International
    Conference on Machine Learning and Applications, Washington, DC, USA, 12–14 December
    2010; pp. 339–345. [Google Scholar] Alipour, N.; Tarkhaneh, O.; Awrangjeb, M.;
    Tian, H. Flower Image Classification Using Deep Convolutional Neural Network.
    In Proceedings of the 2021 7th International Conference on Web Research (ICWR),
    Tehran, Iran, 19–20 May 2021; pp. 1–4. [Google Scholar] Dharwadkar, S.; Bhat,
    G.; Subba Reddy, N.V.; Aithal, P.K. Floriculture Classification Using Simple Neural
    Network and Deep Learning. In Proceedings of the 2017 2nd IEEE International Conference
    on Recent Trends in Electronics, Information & Communication Technology (RTEICT),
    Bangalore, India, 19–20 May 2017; pp. 619–622. [Google Scholar] Malik, M.; Aslam,
    W.; Nasr, E.A.; Aslam, Z.; Kadry, S. A Performance Comparison of Classification
    Algorithms for Rose Plants. Comput. Intell. Neurosci. 2022, 2022, 1842547. [Google
    Scholar] [CrossRef] [PubMed] Narvekar, C.; Rao, M. Flower Classification Using
    CNN and Transfer Learning in CNN-Agriculture Perspective. In Proceedings of the
    2020 3rd International Conference on Intelligent Sustainable Systems (ICISS),
    Thoothukudi, India, 3–5 December 2020; pp. 660–664. [Google Scholar] Soleimanipour,
    A.; Chegini, G.R. A Vision-Based Hybrid Approach for Identification of Anthurium
    Flower Cultivars. Comput. Electron. Agric. 2020, 174, 105460. [Google Scholar]
    [CrossRef] Gunjal, S.; Waskar, D.; Dod, V.; Bhujbal, B.; Ambad, S.N.; Rajput,
    H.; Hendre, P.; Thoke, N.; Bhaskar, M. Horticulture Nursery Management. 2012.
    Available online: https://k8449r.weebly.com/uploads/3/0/7/3/30731055/horticulture_plant_nursery1-signed.pdf
    (accessed on 20 November 2022). Li, M.; Ma, L.; Zong, W.; Luo, C.; Huang, M.;
    Song, Y. Design and Experimental Evaluation of a Form Trimming Machine for Horticultural
    Plants. Appl. Sci. Switz. 2021, 11, 2230. [Google Scholar] [CrossRef] Zhang, M.;
    Guo, W.; Wang, L.; Li, D.; Hu, B.; Wu, Q. Modeling and Optimization of Watering
    Robot Optimal Path for Ornamental Plant Care. Comput. Ind. Eng. 2021, 157, 107263.
    [Google Scholar] [CrossRef] Sharma, S.; Borse, R. Automatic Agriculture Spraying
    Robot with Smart Decision Making. Adv. Intell. Syst. Comput. 2016, 530, 743–758.
    [Google Scholar] [CrossRef] Prabha, P.; Vishnu, R.S.; Mohan, H.T.; Rajendran,
    A.; Bhavani, R.R. A Cable Driven Parallel Robot for Nursery Farming Assistance.
    In Proceedings of the 2021 IEEE 9th Region 10 Humanitarian Technology Conference
    (R10-HTC), Bangalore, India, 30 September–2 October 2021; pp. 1–6. [Google Scholar]
    Kim, W.S.; Lee, D.H.; Kim, Y.J.; Kim, T.; Lee, W.S.; Choi, C.H. Stereo-Vision-Based
    Crop Height Estimation for Agricultural Robots. Comput. Electron. Agric. 2021,
    181, 105937. [Google Scholar] [CrossRef] Wang, X.; Singh, D.; Marla, S.; Morris,
    G.; Poland, J. Field-Based High-Throughput Phenotyping of Plant Height in Sorghum
    Using Different Sensing Technologies. Plant Methods 2018, 14, 1–16. [Google Scholar]
    [CrossRef] Andújar, D.; Ribeiro, A.; Fernández-Quintanilla, C.; Dorado, J. Using
    Depth Cameras to Extract Structural Parameters to Assess the Growth State and
    Yield of Cauliflower Crops. Comput. Electron. Agric. 2016, 122, 67–73. [Google
    Scholar] [CrossRef] Polder, G.; Hofstee, J.W. Phenotyping Large Tomato Plants
    in the Greenhouse Using a 3D Light-Field Camera. In Proceedings of the 2014 Montreal,
    Quebec, QC, Canada, 13–16 July 2014; American Society of Agricultural and Biological
    Engineers, 2014; p. 1. [Google Scholar] Kerkech, M.; Hafiane, A.; Canals, R.;
    Ros, F. Vine Disease Detection by Deep Learning Method Combined with 3d Depth
    Information. In Proceedings of the International Conference on Image and Signal
    Processing, 9th International Conference, ICISP 2020, Marrakesh, Morocco, 4–6
    June 2020; Springer: Berlin/Heidelberg, Germany, 2020; pp. 82–90. [Google Scholar]
    Gai, J.; Xiang, L.; Tang, L. Using a Depth Camera for Crop Row Detection and Mapping
    for Under-Canopy Navigation of Agricultural Robotic Vehicle. Comput. Electron.
    Agric. 2021, 188, 106301. [Google Scholar] [CrossRef] Gongal, A.; Karkee, M.;
    Amatya, S. Apple Fruit Size Estimation Using a 3D Machine Vision System. Inf.
    Process. Agric. 2018, 5, 498–503. [Google Scholar] [CrossRef] Vázquez-Arellano,
    M.; Paraforos, D.S.; Reiser, D.; Garrido-Izard, M.; Griepentrog, H.W. Determination
    of Stem Position and Height of Reconstructed Maize Plants Using a Time-of-Flight
    Camera. Comput. Electron. Agric. 2018, 154, 276–288. [Google Scholar] [CrossRef]
    Hämmerle, M.; Höfle, B. Direct Derivation of Maize Plant and Crop Height from
    Low-Cost Time-of-Flight Camera Measurements. Plant Methods 2016, 12, 50. [Google
    Scholar] [CrossRef] Vázquez-Arellano, M.; Reiser, D.; Paraforos, D.S.; Garrido-Izard,
    M.; Burce, M.E.C.; Griepentrog, H.W. 3-D Reconstruction of Maize Plants Using
    a Time-of-Flight Camera. Comput. Electron. Agric. 2018, 145, 235–247. [Google
    Scholar] [CrossRef] Li, J.; Tang, L. Developing a Low-Cost 3D Plant Morphological
    Traits Characterization System. Comput. Electron. Agric. 2017, 143, 1–13. [Google
    Scholar] [CrossRef] Pamornnak, B.; Limsiroratana, S.; Khaorapapong, T.; Chongcheawchamnan,
    M.; Ruckelshausen, A. An Automatic and Rapid System for Grading Palm Bunch Using
    a Kinect Camera. Comput. Electron. Agric. 2017, 143, 227–237. [Google Scholar]
    [CrossRef] Cao, Q.; Yang, G.; Duan, D.; Chen, L.; Wang, F.; Xu, B.; Zhao, C.;
    Niu, F. Combining Multispectral and Hyperspectral Data to Estimate Nitrogen Status
    of Tea Plants (Camellia sinensis (L.) O. Kuntze) under Field Conditions. Comput.
    Electron. Agric. 2022, 198, 107084. [Google Scholar] [CrossRef] Chandel, A.K.;
    Khot, L.R.; Yu, L.X. Alfalfa (Medicago sativa L.) Crop Vigor and Yield Characterization
    Using High-Resolution Aerial Multispectral and Thermal Infrared Imaging Technique.
    Comput. Electron. Agric. 2021, 182, 105999. [Google Scholar] [CrossRef] Abbas,
    A.; Jain, S.; Gour, M.; Vankudothu, S. Tomato Plant Disease Detection Using Transfer
    Learning with C-GAN Synthetic Images. Comput. Electron. Agric. 2021, 187, 106279.
    [Google Scholar] [CrossRef] Xiao, D.; Zeng, R.; Liu, Y.; Huang, Y.; Liu, J.; Feng,
    J.; Zhang, X. Citrus Greening Disease Recognition Algorithm Based on Classification
    Network Using TRL-GAN. Comput. Electron. Agric. 2022, 200, 107206. [Google Scholar]
    [CrossRef] Zhang, L.; Nie, Q.; Ji, H.; Wang, Y.; Wei, Y.; An, D. Hyperspectral
    Imaging Combined with Generative Adversarial Network (GAN)-Based Data Augmentation
    to Identify Haploid Maize Kernels. J. Food Compos. Anal. 2022, 106, 104346. [Google
    Scholar] [CrossRef] Mazzia, V.; Khaliq, A.; Salvetti, F.; Chiaberge, M. Real-Time
    Apple Detection System Using Embedded Systems With Hardware Accelerators: An Edge
    AI Application. IEEE Access 2020, 8, 9102–9114. [Google Scholar] [CrossRef] Zhang,
    Y.; Yu, J.; Chen, Y.; Yang, W.; Zhang, W.; He, Y. Real-Time Strawberry Detection
    Using Deep Neural Networks on Embedded System (Rtsd-Net): An Edge AI Application.
    Comput. Electron. Agric. 2022, 192, 106586. [Google Scholar] [CrossRef] Codeluppi,
    G.; Davoli, L.; Ferrari, G. Forecasting Air Temperature on Edge Devices with Embedded
    AI. Sensors 2021, 21, 3973. [Google Scholar] [CrossRef] Coppola, M.; Noaille,
    L.; Pierlot, C.; de Oliveira, R.O.; Gaveau, N.; Rondeau, M.; Mohimont, L.; Steffenel,
    L.A.; Sindaco, S.; Salmon, T. Innovative Vineyards Environmental Monitoring System
    Using Deep Edge AI. Artif. Intell. Digit. Ind.–Appl. 2022, 261–278. [Google Scholar]
    [CrossRef] Aghi, D.; Cerrato, S.; Mazzia, V.; Chiaberge, M. Deep Semantic Segmentation
    at the Edge for Autonomous Navigation in Vineyard Rows. In Proceedings of the
    2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
    Prague, Czech Republic, 27 September–1 October 2021; pp. 3421–3428. [Google Scholar]
    Deng, F.; Zuo, P.; Wen, K.; Wu, X. Novel Soil Environment Monitoring System Based
    on RFID Sensor and LoRa. Comput. Electron. Agric. 2020, 169, 105169. [Google Scholar]
    [CrossRef] Luvisi, A.; Panattoni, A.; Materazzi, A. RFID Temperature Sensors for
    Monitoring Soil Solarization with Biodegradable Films. Comput. Electron. Agric.
    2016, 123, 135–141. [Google Scholar] [CrossRef] Vellidis, G.; Tucker, M.; Perry,
    C.; Kvien, C.; Bednarz, C. A Real-Time Wireless Smart Sensor Array for Scheduling
    Irrigation. Comput. Electron. Agric. 2008, 61, 44–50. [Google Scholar] [CrossRef]
    Dey, S.; Bhattacharyya, R.; Karmakar, N.; Sarma, S. A Folded Monopole Shaped Novel
    Soil Moisture and Salinity Sensor for Precision Agriculture Based Chipless RFID
    Applications. In Proceedings of the 2019 IEEE MTT-S International Microwave and
    RF Conference (IMARC), Mumbai, India, 13–15 December 2019. [Google Scholar] [CrossRef]
    Wang, J.; Chang, L.; Aggarwal, S.; Abari, O.; Keshav, S. Soil Moisture Sensing
    with Commodity RFID Systems. In Proceedings of the MobiSys’20: The 18th Annual
    International Conference on Mobile Systems, Applications, and Services, Toronto,
    ON, Canada, 15–19 June 2020; Volume 13. [Google Scholar] [CrossRef] Aroca, R.V.;
    Hernandes, A.C.; Magalhães, D.V.; Becker, M.; Vaz, C.M.P.; Calbo, A.G. Calibration
    of Passive UHF RFID Tags Using Neural Networks to Measure Soil Moisture. J. Sens.
    2018, 2018, 3436503. [Google Scholar] [CrossRef] Hasan, A.; Bhattacharyya, R.;
    Sarma, S. Towards Pervasive Soil Moisture Sensing Using RFID Tag Antenna-Based
    Sensors. In Proceedings of the 2015 IEEE International Conference on RFID Technology
    and Applications (RFID-TA), Tokyo, Japan, 16–18 September 2015; pp. 165–170. [Google
    Scholar] Yong, W.; Shuaishuai, L.; Li, L.; Minzan, L.; Ming, L.; Arvanitis, K.;
    Georgieva, C.; Sigrimis, N. Smart Sensors from Ground to Cloud and Web Intelligence.
    IFAC-Pap. 2018, 51, 31–38. [Google Scholar] [CrossRef] Barge, P.; Gay, P.; Piccarolo,
    P.; Tortia, C. RFID Tracking of Potted Plants from Nursery to Distribution. In
    Proceedings of the International Conference Ragusa SHWA2010, Ragusa, Italy, 16–18
    September 2010. [Google Scholar] Sugahara, K. Traceability System for Agricultural
    Products Based on RFID and Mobile Technology. IFIP Adv. Inf. Commun. Technol.
    2009, 295, 2293–2301. [Google Scholar] [CrossRef] Voulodimos, A.S.; Patrikakis,
    C.Z.; Sideridis, A.B.; Ntafis, V.A.; Xylouri, E.M. A Complete Farm Management
    System Based on Animal Identification Using RFID Technology. Comput. Electron.
    Agric. 2010, 70, 380–388. [Google Scholar] [CrossRef] Weiss, U.; Biber, P. Plant
    Detection and Mapping for Agricultural Robots Using a 3D LIDAR Sensor. Robot.
    Auton. Syst. 2011, 59, 265–273. [Google Scholar] [CrossRef] Ge, Y.; Xiong, Y.;
    From, P.J. Symmetry-Based 3D Shape Completion for Fruit Localisation for Harvesting
    Robots. Biosyst. Eng. 2020, 197, 188–202. [Google Scholar] [CrossRef] Skoczeń,
    M.; Ochman, M.; Spyra, K.; Nikodem, M.; Krata, D.; Panek, M.; Pawłowski, A. Obstacle
    Detection System for Agricultural Mobile Robot Application Using RGB-D Cameras.
    Sensors 2021, 21, 5292. [Google Scholar] [CrossRef] Ji, W.; Gao, X.; Xu, B.; Chen,
    G.Y.; Zhao, D. Target Recognition Method of Green Pepper Harvesting Robot Based
    on Manifold Ranking. Comput. Electron. Agric. 2020, 177, 105663. [Google Scholar]
    [CrossRef] Gai, R.; Chen, N.; Yuan, H. A Detection Algorithm for Cherry Fruits
    Based on the Improved YOLO-v4 Model. Neural Comput. Appl. 2021, 1–12. [Google
    Scholar] [CrossRef] Jia, W.; Tian, Y.; Luo, R.; Zhang, Z.; Lian, J.; Zheng, Y.
    Detection and Segmentation of Overlapped Fruits Based on Optimized Mask R-CNN
    Application in Apple Harvesting Robot. Comput. Electron. Agric. 2020, 172, 105380.
    [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements, opinions
    and data contained in all publications are solely those of the individual author(s)
    and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2023 by
    the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Mahmud, M.S.; Zahid, A.; Das, A.K. Sensing and Automation Technologies
    for Ornamental Nursery Crop Production: Current Status and Future Prospects. Sensors
    2023, 23, 1818. https://doi.org/10.3390/s23041818 AMA Style Mahmud MS, Zahid A,
    Das AK. Sensing and Automation Technologies for Ornamental Nursery Crop Production:
    Current Status and Future Prospects. Sensors. 2023; 23(4):1818. https://doi.org/10.3390/s23041818
    Chicago/Turabian Style Mahmud, Md Sultan, Azlan Zahid, and Anup Kumar Das. 2023.
    \"Sensing and Automation Technologies for Ornamental Nursery Crop Production:
    Current Status and Future Prospects\" Sensors 23, no. 4: 1818. https://doi.org/10.3390/s23041818
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   4
    Web of Science   2 Scopus   5 Google Scholar   [click to view] Article Access
    Statistics Article access statistics Article Views 10. Jan 20. Jan 30. Jan 9.
    Feb 19. Feb 29. Feb 10. Mar 20. Mar 30. Mar 0k 1k 2k 3k 4k 5k For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Sensors, EISSN 1424-8220, Published by MDPI RSS Content
    Alert Further Information Article Processing Charges Pay an Invoice Open Access
    Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors
    For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives
    Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings
    Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release
    notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024
    MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions
    Privacy Policy"'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Sensing and Automation Technologies for Ornamental Nursery Crop Production:
    Current Status and Future Prospects'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Rodrigues L.
  - Magalhães S.A.
  - da Silva D.Q.
  - dos Santos F.N.
  - Cunha M.
  citation_count: '6'
  description: The efficiency of agricultural practices depends on the timing of their
    execution. Environmental conditions, such as rainfall, and crop-related traits,
    such as plant phenology, determine the success of practices such as irrigation.
    Moreover, plant phenology, the seasonal timing of biological events (e.g., cotyledon
    emergence), is strongly influenced by genetic, environmental, and management conditions.
    Therefore, assessing the timing the of crops’ phenological events and their spatiotemporal
    variability can improve decision making, allowing the thorough planning and timely
    execution of agricultural operations. Conventional techniques for crop phenology
    monitoring, such as field observations, can be prone to error, labour-intensive,
    and inefficient, particularly for crops with rapid growth and not very defined
    phenophases, such as vegetable crops. Thus, developing an accurate phenology monitoring
    system for vegetable crops is an important step towards sustainable practices.
    This paper evaluates the ability of computer vision (CV) techniques coupled with
    deep learning (DL) (CV_DL) as tools for the dynamic phenological classification
    of multiple vegetable crops at the subfield level, i.e., within the plot. Three
    DL models from the Single Shot Multibox Detector (SSD) architecture (SSD Inception
    v2, SSD MobileNet v2, and SSD ResNet 50) and one from You Only Look Once (YOLO)
    architecture (YOLO v4) were benchmarked through a custom dataset containing images
    of eight vegetable crops between emergence and harvest. The proposed benchmark
    includes the individual pairing of each model with the images of each crop. On
    average, YOLO v4 performed better than the SSD models, reaching an F1-Score of
    85.5%, a mean average precision of 79.9%, and a balanced accuracy of 87.0%. In
    addition, YOLO v4 was tested with all available data approaching a real mixed
    cropping system. Hence, the same model can classify multiple vegetable crops across
    the growing season, allowing the accurate mapping of phenological dynamics. This
    study is the first to evaluate the potential of CV_DL for vegetable crops’ phenological
    research, a pivotal step towards automating decision support systems for precision
    horticulture.
  doi: 10.3390/agronomy13020463
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agronomy All Article Types Advanced   Journals
    Agronomy Volume 13 Issue 2 10.3390/agronomy13020463 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Roberto
    Marani Subscribe SciFeed Recommended Articles Related Info Link More by Authors
    Links Article Views 2149 Citations 6 Table of Contents Abstract Introduction Materials
    and Methods Results Discussion Conclusions Author Contributions Funding Institutional
    Review Board Statement Informed Consent Statement Data Availability Statement
    Acknowledgments Conflicts of Interest References Altmetric share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessEditor’s
    ChoiceArticle Computer Vision and Deep Learning as Tools for Leveraging Dynamic
    Phenological Classification in Vegetable Crops by Leandro Rodrigues 1,2,*, Sandro
    Augusto Magalhães 2,3, Daniel Queirós da Silva 2,4, Filipe Neves dos Santos 2
    and Mário Cunha 1,2,* 1 Faculty of Sciences, University of Porto, Rua do Campo
    Alegre s/n, 4169-007 Porto, Portugal 2 INESC TEC-Institute for Systems and Computer
    Engineering, Technology and Science, Rua Dr. Roberto Frias s/n, 4200-465 Porto,
    Portugal 3 Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias
    s/n, 4200-465 Porto, Portugal 4 School of Science and Technology, University of
    Trás-os-Montes e Alto Douro (UTAD), 5000-801 Vila Real, Portugal * Authors to
    whom correspondence should be addressed. Agronomy 2023, 13(2), 463; https://doi.org/10.3390/agronomy13020463
    Submission received: 4 January 2023 / Revised: 30 January 2023 / Accepted: 1 February
    2023 / Published: 4 February 2023 Download keyboard_arrow_down     Browse Figures
    Versions Notes Abstract The efficiency of agricultural practices depends on the
    timing of their execution. Environmental conditions, such as rainfall, and crop-related
    traits, such as plant phenology, determine the success of practices such as irrigation.
    Moreover, plant phenology, the seasonal timing of biological events (e.g., cotyledon
    emergence), is strongly influenced by genetic, environmental, and management conditions.
    Therefore, assessing the timing the of crops’ phenological events and their spatiotemporal
    variability can improve decision making, allowing the thorough planning and timely
    execution of agricultural operations. Conventional techniques for crop phenology
    monitoring, such as field observations, can be prone to error, labour-intensive,
    and inefficient, particularly for crops with rapid growth and not very defined
    phenophases, such as vegetable crops. Thus, developing an accurate phenology monitoring
    system for vegetable crops is an important step towards sustainable practices.
    This paper evaluates the ability of computer vision (CV) techniques coupled with
    deep learning (DL) (CV_DL) as tools for the dynamic phenological classification
    of multiple vegetable crops at the subfield level, i.e., within the plot. Three
    DL models from the Single Shot Multibox Detector (SSD) architecture (SSD Inception
    v2, SSD MobileNet v2, and SSD ResNet 50) and one from You Only Look Once (YOLO)
    architecture (YOLO v4) were benchmarked through a custom dataset containing images
    of eight vegetable crops between emergence and harvest. The proposed benchmark
    includes the individual pairing of each model with the images of each crop. On
    average, YOLO v4 performed better than the SSD models, reaching an F1-Score of
    85.5%, a mean average precision of 79.9%, and a balanced accuracy of 87.0%. In
    addition, YOLO v4 was tested with all available data approaching a real mixed
    cropping system. Hence, the same model can classify multiple vegetable crops across
    the growing season, allowing the accurate mapping of phenological dynamics. This
    study is the first to evaluate the potential of CV_DL for vegetable crops’ phenological
    research, a pivotal step towards automating decision support systems for precision
    horticulture. Keywords: agricultural practices; phenology monitoring; phenotyping;
    precision horticulture; SSD; YOLO 1. Introduction Plant phenology comprises the
    seasonal timing of biological events (e.g., cotyledon emergence) and the causes
    of their timing concerning biotic and abiotic interactions (agricultural practices
    included) [1,2]. The timing of crops’ phenological phases (phenophases) provides
    valuable information for monitoring and simulating plant growth and development.
    Therefore, it allows the thorough planning and timely execution of agricultural
    practices, which are usually carried out according to specific phenophases (e.g.,
    timing of irrigation and harvest), leading to higher and more stable crop yields,
    sustainable practices, and improved food quality [3,4]. Field observation, crop
    growth models and remote sensing approaches are the most common methods for phenology
    monitoring. Field observation involves in situ plant assessment and the manual
    recording of phenophases (e.g., leaf unfolding). This observational approach is
    laborious and prone to error due to the large spatiotemporal variability of phenophases
    among plants [5,6]. Consequently, an increased sampling density is needed to adequately
    represent crop phenology at the field scale. Although crop growth models can simulate
    the timing of the phenophases of a particular genotype (individual), they cannot
    transpose the simulation to different spatial scales, as the heterogeneities in
    climate and management conditions are neglected [7,8,9]. Moreover, near-surface
    remote sensing techniques, such as digital repeat photography [10,11], and satellite
    remote sensing linked with vegetation indices (e.g., leaf area index) provide
    valuable data about phenology dynamics at a regional scale [12]. However, the
    ability of these methods to monitor phenology at the field or subfield scale is
    limited, especially in crops with hardly perceivable phenophase transitions such
    as vegetable crops [13]. Vegetable crops (see https://www.ishs.org/defining-horticulture,
    last accessed: 18 September 2022), are characterised by rapid growth, which leads
    to not very defined phenophases. Since most of them are fresh food, the timing
    of sensitive phenological events, such as cotyledon emergence, is of economic
    and technical concern to establish accurate practices, such as weed removal [14,15].
    Therefore, it is important to set alternative methods suitable for reliably and
    operationally assessing vegetable crops’ phenophases to support precision horticulture
    (PH) production systems. The dynamics of crop phenophases result from a set of
    structural, physiological, and performance-related phenotypic traits that characterise
    the given genotype and its interactions with the environment [16]. Computer vision
    (CV) has attracted growing interest in the agricultural domain as it comprises
    techniques that allow systems to automatically collect images and extract valuable
    information from them towards accurate and efficient practices [17]. Usually,
    CV-based systems include an image acquisition phase and image analysis techniques
    that can distinguish the regions of interest to be detected and classified (e.g.,
    whole plant, leaves) [18,19]. Among the plethora of image analysis techniques
    already developed, the one that best performs object detection in agriculture
    (e.g., disease detection [20], crop and weed detection [21], and fruit detection
    [22]) is deep learning (DL) [23]. DL is based on machine learning and has led
    to breakthroughs in image analysis, given its ability to automatically extract
    features from unstructured data [24]. In particular, convolutional neural networks
    (CNNs) are being tested for various tasks to support PH production systems, including
    phenology monitoring [25,26]. Thereby, it is hypothesised that high-throughput
    plant phenotyping techniques [27] based on CV coupled with DL (CV_DL) can assess
    the spatiotemporal dynamics of crop traits related to its phenophases in the context
    of PH [28]. Nevertheless, applications of CV and DL in phenology monitoring are
    mostly focused on arable crops (e.g., rice, barley, maize) [29,30,31,32], orchards
    [33,34], and forest trees [35,36,37] with limited studies into vegetable crops.
    Additionally, the developed models are crop-specific and tend to be based on well-defined
    phenophases, such as flowering [13,33,35], neglecting the spatiotemporal phenology
    dynamics. Vegetables usually grow as annual crops and are harvested during the
    vegetative phase. Thus, the phenotypic traits for phenological identification
    are restricted to leaves (e.g., number, size, and colour). Furthermore, considering
    that vegetable crops are often sown as mixed cropping, and the morphological traits
    of leaves are very similar between plants, especially in the early growth stages,
    it is difficult to identify the specific phenotypic traits in order to classify
    the phenophases of the corresponding plant (crops and weeds included) [38,39,40].
    This study primarily aims to develop an automatic approach for the dynamic phenological
    classification of vegetable crops using CV_DL techniques. Four state-of-the-art
    DL models were tested through a custom dataset that consists of RGB and greyscale
    annotated images corresponding to the main phenophases of eight vegetable crops
    between emergence and harvest. Thus, this work contributes to the state-of-the-art
    introducing a vegetable crop classification system that can identify multiple
    crops considering phenology dynamics throughout the growing season at the subfield
    scale. 2. Materials and Methods 2.1. Dataset Acquisition and Processing Eight
    vegetable crops were selected to build a varied dataset of phenophases (Table
    1), taking into account the length of the growing season, the intensity of agricultural
    practices (mainly weed removal), and the resistance to pests and diseases. Table
    1. Vegetable crops selected for the phenophase image dataset construction. Each
    crop was manually sown in a section of a 4.5 m2 plot at a greenhouse in Vairão,
    Vila do Conde, Portugal, according to the recommended sowing density. Weeds were
    manually removed at an early growth stage (less than four unfolded leaves), and
    plants were irrigated twice a day. Image acquisition occurred before the weed
    removal operation. Images in the RGB colour space were acquired between March
    and July 2021 by a smartphone (Huawei Mate 10 Lite, 16 MP resolution in Pro Mode)
    in different view angles and sunlight conditions, both procedures increasing the
    data representativeness. Depending on the crop, one or several growing seasons
    occurred during the image acquisition operation, and images were collected covering
    the main vegetative phenophases of the crops studied. The classification of the
    phenophases of each crop was based on the BBCH-scale [41]. In total, there were
    collected 4123 images with a resolution of 3456 × 4608 px each. Table 2 depicts
    the dataset: the number of images and plants of each crop and phenophase, accordingly.
    Table 2. Number of images acquired for each crop and the corresponding number
    of plants for each phenophase defined. All images were rescaled four times to
    a resolution of 864 × 1152 px, improving the processing operations. To evaluate
    the versatility of the DL models studied, the RGB images were also transformed
    into greyscale using OpenCV (see https://opencv.org, last accessed: 12 September
    2022) library. With this transformation, the complexity and the bias are reduced,
    as the model is not influenced by the colour in the images, while it is forced
    to learn features that are not specific to colour, improving the generalisation
    ability. By applying the luminosity method, it is possible to assign a weighted
    average of the colour components to each RGB channel (see https://docs.opencv.org/4.x/de/d25/imgproc_color_conversions.html,
    last accessed: 12 September 2022) (Equation (1)). An example of this transformation
    is presented in Figure 1. 𝐺𝑟𝑒𝑦𝑉𝑎𝑙𝑢𝑒=(0.299×𝑅𝑒𝑑)+(0.587×𝐺𝑟𝑒𝑒𝑛)+(0.114×𝐵𝑙𝑢𝑒) (1)
    Figure 1. Radish image acquired in RGB colour space (a) and after greyscale transformation
    (b). Since the training and evaluation of DL models involves supervised learning,
    the images need to be annotated. Following the BBCH-scale [41] and the phenophases
    defined for each crop (Table 2), images were manually annotated using the open
    source annotation tool CVAT (see https://www.cvat.ai/, last accessed: 18 September
    2022), indicating by rectangular bounding boxes the position and phenophase of
    each plant. After annotation, the images and the corresponding annotations were
    exported under the Pascal VOC [42] and YOLO formats [43] to train the SSD and
    YOLO models, respectively. All annotated images (RGB and greyscale) are publicly
    available at the open access digital repository Zenodo (see https://doi.org/10.5281/zenodo.7433286,
    last accessed: 5 November 2022) [44]. 2.2. Deep Learning Approach 2.2.1. Object
    Detection Models Object detection models are specialised DL models designed to
    detect the position and size of objects in an image. There are different kinds
    of DL architectures to detect objects, but the most common ones in the state of
    the art are based on single-shot CNN, such as SSD and YOLO. SSD was first introduced
    by Liu et al. [45]. This architecture comprises three main components: the feature
    extractor, the classifier, and the regressor. The feature extractor is a CNN frequently
    designated as the backbone and can be any classification network. Three state-of-the-art
    backbones were selected for this work: Inception v2, MobileNet v2, and ResNet50.
    These networks were selected due to the good results obtained in previous works
    on object detection in agriculture [46,47]. The image was then split into different
    sizes and positioned windows during the image processing stage. The classifier
    element will predict the object in each window, and the regressor will adjust
    the window (also called the bounding box) to the object’s position and size. Combining
    the three elements (feature extractor, classifier and regressor) leads to an object
    detection model. Inception V2, MobileNet v2, and ResNet 50 are three state-of-the-art
    CNNs. Inception v2 [48] is an improvement of GoogLeNet, also named Inception v1
    [49]. This network is made of inception modules and batch normalisation layers.
    MobileNet v2 is a CNN classifier designed for mobile and embedded applications
    [50]. In this network, the authors replaced the full convolutional operators with
    a factorised version that splits the convolution into two layers. The first layer
    is a lightweight filter, while the other layer is a point-wise operator which
    creates new features. As such, MobileNet v2 reduced the number of trainable parameters
    and training complexity. To improve the results of VGG DL models, He et al. [51]
    studied residual neural networks. ResNet is based on plain architectures with
    shortcuts for residual learning. This strategy reduced the number of trainable
    parameters and the training complexity against VGG. ResNet models allow depths
    between 18 and 152 layers, but ResNet 50 is the most common in the literature.
    You Only Look Once (YOLO) is also a well-known single-shot detector, serving most
    of the time as a reference object detector. YOLO v4 is the fourth generation of
    YOLO models [52], which delivered multiple features to the original YOLO version,
    improving the model’s performance and keeping the inference speed. YOLO uses a
    backbone of DarkNet (see https://pjreddie.com/darknet/, last accessed: 14 October
    2022). Among the selected models, YOLO v4 aims to be the one which provides the
    best balance between accuracy and inference speed. Therefore, this study covers
    the most current object detection architectures in the literature, especially
    concerning DL applications in agriculture. 2.2.2. Models Training For training
    purposes, the dataset was divided into three sets: training, validation, and test
    sets. Table 3 depicts the number of images and plants of each crop and phenophase,
    respectively, assigned for each set (considering only RGB images). The dataset
    split rate was determined by considering the corresponding number of images and
    plants in each phenophase. Table 3. Dataset split of each crop and phenophase
    images and plants for the training and evaluation of deep learning models (considering
    only RGB images). Data augmentation was used to increase the number of images,
    improving the overall learning procedure and performance by inputting variability
    into the dataset [23]. The transformations were carefully chosen, applying those
    that could happen under actual conditions, as displayed in Figure 2. The data
    transformations were applied to training and validation sets. The same transformations
    were applied to RGB images and the corresponding greyscale images (Table 4). Figure
    2. Representation of the transformations applied to the images. Table 4. Training
    and validation image sets’ composition after the data augmentation step, considering
    only RGB images. TensorFLow r.1.15.0 (see https://www.tensorflow.org/, last accessed:
    13 October 2022) was used for the training and evaluation scripts of SSD models,
    whereas Darknet (see https://pjreddie.com/darknet/, last accessed: 14 October
    2022) was used for YOLO v4. A GPU RTX3090 (VRAM of 24 GB) and a CPU Ryzen 9 5900X
    (12-core 3.7 GHz with a Turbo 4.8 GHz 70 MB SktAM4 with a RAM of 32 GB) were available
    to run the scripts. Through transfer learning, pre-trained models with Microsoft’s
    COCO dataset (see https://cocodataset.org/, last accessed: 8 October 2022) were
    fine-tuned to classify vegetable crops’ phenophases. Some changes to the default
    training pipeline were made to optimise the training dynamics and generalisation
    ability, such as adjusting the batch size. The standard input size for each model
    was maintained with the addition of a variation of SSD MobileNet v2 with an input
    size of 300 × 300 px. This information is shown in Table 5. Table 5. Input and
    batch size for the training pipeline of the deep learning (DL) models tested.
    Since data augmentation has already taken place, it was removed from the training
    pipeline. The SSD models’ training sessions ran for 50 000 steps, while the YOLO
    v4 training was conditioned by the number of classes of each crop (see https://github.com/AlexeyAB/darknet,
    last accessed: 4 November 2022). For example, YOLO v4 trained with lettuce (four
    classes) images ran for 8000 steps. An evaluation session occurred in the validation
    set according to the standard value used by the pre-trained models. These evaluation
    sessions are useful because they monitored the evolution of the training, i.e.,
    detecting if the evaluation loss begins to increase while the training loss decreases
    or stays constant, which means that the model is excessively complex and cannot
    be well generalised to new data. For benchmarking purposes, initially, training
    was carried out individually only with the RGB images corresponding to each crop
    and then replicated with greyscale images. After the individual performance evaluation
    of each model, the one that presented the best performance results was selected
    to be trained with all available data (RGB and greyscale images). 2.2.3. Models
    Performance Evaluation The performance of the DL models was evaluated through
    a comparison between the predictions made by the DL model and the ground truth
    data of the position and phenophase of each plant. The following evaluation metrics
    were considered: F1-Score, mean average precision (mAP), and balanced accuracy
    (BA) [42,43,53]. The cross-validation technique was applied according to Magalhães
    et al. [46] to optimise the DL model’s performance by optimising the confidence
    threshold. In the validation set, augmentations were removed, and the F1-Score
    was calculated for all the confidence thresholds from 0% to 100% into steps of
    1%. The confidence threshold with the best F1-score output was selected for the
    model’s normal operation. The performance evaluation procedure considered the
    test set images and occurred in a GPU RTX3090 with a VRAM of 24 GB. 3. Results
    Table 6 shows the results of the cross-validation technique applied to the validation
    set for both the models trained with RGB and greyscale images. The highest F1-score
    values are reported by SSD MobileNet v2 (300 × 300 px) and YOLO v4 trained with
    RGB and greyscale spinach images, respectively. SSD MobileNet v2 (300 × 300 px)
    trained with RGB carrot images and greyscale coriander images presented the lowest
    F1-scores of 58.4% and 32.0%, respectively. On average, the models with a higher
    F1-score are SSD ResNet 50 and YOLO v4, trained with RGB and greyscale images,
    respectively. Table 6. Confidence threshold that optimises the F1-score metric
    for each deep learning (DL) model tested in the validation set without augmentation.
    Figure 3 reports the mAP results for the best-computed confidence threshold. On
    average, this metric presents a value close to 70%, and the models trained with
    RGB images are slightly above (71.6%). Indeed, 25 of the 40 models trained with
    RGB images (solid black bars, Figure 3) present an mAP equal to or higher than
    the corresponding model trained with greyscale images (dashed bars, Figure 3).
    Figure 3. Mean average precision (mAP) for all models trained with RGB (solid
    black bars) and greyscale (dashed bars) images in the test set without augmentation.
    The models trained with carrot images present a lower mAP: close to 53% (RGB images)
    and 38% (greyscale images). Figure 4 shows the low performance of the SSD MobileNet
    v2 model (300 × 300 px) applied to carrot classification phenophases. This crop
    generally has erect and compound leaves (low soil cover), which facilitates weed
    growth and makes it more difficult to distinguish between the soil and plants
    (crop and weeds included). Figure 4. Carrot phenophase predictions made by SSD
    MobileNet v2 (300 × 300 px) using RGB (a) and the corresponding greyscale (b)
    image. Carrot phenophases are described as: coty (opened cotyledons), smallleaves
    (one to three unfolded leaves), and carrot (more than three unfolded leaves).
    The confusion matrix (Figure 5) for SSD MobileNet v2 (300 × 300 px) shows low
    performance. The model had considerable difficulty locating the carrot plants,
    especially when trained with greyscale images, given the number of undetected
    ground truths (false negatives). Figure 5. Confusion matrix for carrots’ phenophases
    based on SSD MobileNet v2 (300 × 300 px) trained with RGB (a) and greyscale (b)
    images. False positives and false negatives include improperly predicted plants
    and undetected plants, respectively. On the other hand, the models trained with
    spinach images are those with higher mAP (98.3% for RGB and 98.1% for greyscale
    images). The phenotypic traits of this crop test set may contribute to explaining
    the results. Figure 6 represents the spinach test set: images containing only
    one plant, with little weed or other plant presence and a clear distinction between
    the soil and the plant. Figure 6 is an example performed by ResNet 50, which was
    able to correctly classify all 57 ground truths of the test set. Figure 6. Spinach
    phenophase prediction made by SSD ResNet 50 based on RGB images. The spinach phenophase
    describes a spinach plant with less than nine unfolded leaves. The BA metric presented
    in Figure 7 is close to 77% (RGB images) and 79.2% (greyscale images), which are
    similar to the results of mAP (Figure 3). Figure 7. Balanced accuracy (BA) for
    all models trained with RGB (solid black bars) and greyscale (dashed bars) images
    in the test set without augmentation. The importance of BA relies on two aspects:
    it allows one to overcome biased conclusions due to unbalanced datasets [53] and
    it enables the analysis of correctly located objects, regardless of whether they
    are accurately classified. The latter is of particular interest in this work,
    as it is difficult, even for a skilled observer, to identify plants as small as
    cotyledons (Figure 8a) or to distinguish whether a plant has less (Figure 8b)
    or more (Figure 8c) than nine leaves. Figure 8. Lettuce phenophases: (a) coty
    (opened cotyledons); (b) minus9 (less than nine unfolded leaves); (c) plus9 (nine
    or more unfolded leaves). SSD ResNet 50 was the best-performing SSD model for
    both image groups, which is reflected not only in mAP results (76.0% for RGB and
    73.3% for greyscale images) but also in BA results (81.4% for RGB and 79.8% for
    greyscale images). However, SSD models have been outperformed by YOLO v4. This
    DL model presents, for both groups of images, not only a better performance when
    the analysis is based on mAP (76.2% for RGB and 83.5% for greyscale images) but
    also when the evaluation relies on BA (85.2% for RGB and 88.8% for greyscale images).
    Supported by these results, it was decided to train the YOLO v4 (416 × 416 px)
    model with all the images of the 24 defined phenophases, representing the growing
    season of the eight vegetable crops selected for the dataset construction. The
    dataset split resulted in 54,090 images for training, 19,532 images for validation,
    and 1524 images for test sets. The training and validation sets include the images
    resulting from the augmentation procedure. As previously mentioned, Darknet was
    used for the training and evaluation scripts, and the model training session ran
    for 48,000 steps. Furthermore, cross-validation (Figure 9) was applied with the
    model achieving an F1-Score of 83.6% for a confidence threshold higher than 53%.
    Figure 9. Evolution of the F1-score with the variation of the confidence threshold
    for YOLO v4 in the validation set without augmentation. The following performance
    analysis, summarised in Table 7, was conducted on the test set, considering the
    best-computed confidence rate (53%). The overall performance remained consistent,
    except for the slight differences compared with the average values of the YOLO
    v4 variations trained with the independent image groups. Table 7. Different evaluation
    metrics results for YOLO v4 model variations. The confusion matrix in Figure 10
    shows YOLO v4 performance. This DL model could discretise the phenotypic traits
    of the different phenophases for successful classification. The model was able
    to correctly classify 4123 plants out of a total of 5396 ground truths (Table
    2). Figure 10. Confusion matrix for YOLO v4 trained with all available data. False
    positives and false negatives include improperly predicted plants and undetected
    plants, respectively. Nevertheless, some phenophases have similar phenotypic traits,
    leading to misclassifications. For instance, the coty class is present in all
    crops except spinach, which presents similar morphological traits as those seen
    in Figure 11. Nevertheless, the number of false positives only between predictions
    in this class is relatively low, yet the respective number of false negatives
    cannot be neglected. Hence, the resulting mAP for coty is approximately 67%. Figure
    11. Examples of coty class images from the dataset under study. 4. Discussion
    Overall, both architectures were generic enough to successfully classify the vegetables’
    phenophases. SSD ResNet 50 was the best-performing SSD model for both image groups
    individually, and SSD MobileNet v2 (640 × 640 px) performed slightly better than
    the variation SSD MobileNet v2 (300 × 300 px), suggesting that the performance
    of this architecture is favoured by increasing the input size. The proposed SSD
    MobileNet v2 models outperformed the one presented by d’Andrimont et al. [31],
    which obtained an mAP close to 55%. The results presented by the SSD ResNet 50
    model align with those shown by Ofori et al. [39], where this model performed
    better than other DL models, including SSD Inception v2. Pearse et al. [35] and
    Samiei et al. [40] presented the ResNet model variations that performed slightly
    better than that proposed in this study (98% and 90%, respectively). However,
    the former application was for the well-defined Metrosideros excelsa flowering
    phenophase classification, and the latter used a dataset that did not represent
    actual conditions since the soil background was removed from the images. YOLO
    v4 was the model that presented a more consistent performance in both image groups.
    Hence, it was trained with all the images available in the dataset. The model
    performance was not significantly impacted, remaining close to 77% and 82% for
    mAP and BA, respectively. To the best of our knowledge, no work has tested a YOLO
    model for phenology monitoring. Correia et al. [36] is the only study where an
    object detector was developed, with lower performance when compared with the proposed
    YOLO v4 model. The greyscale transformation did not significantly hinder the models’
    performance, the differences being more notorious on the SSD Inception v2 and
    smaller on YOLO v4, where even the models trained with greyscale images showed
    a better performance. Furthermore, the DL models’ performance was not homogeneous
    with regard to different crops, with the models trained with carrot images and
    those trained with spinach images being the lowest and highest extremes, respectively.
    In this case, and in line with different authors [29,30], the images’ quantity
    and quality, particularly the specific phenotypic traits of each crop, may compromise
    the results. The models could classify vegetables at different phenological phases,
    even in non-structured environments, considering overlaps and variable sunlight
    conditions. For the comprehensive tracking of the crop cycle, it is crucial to
    monitor the phenology right after the cotyledons’ appearance and opening. Regarding
    coty (cotyledons’ opening), YOLO v4 classified this phenophase with an mAP of
    approximately 67%. Although slightly lower than the results reported by the literature
    [39,40], it is important to highlight that, in contrast to these, the images applied
    in this study represent actual conditions, without pre-processing operations,
    such as segmentation, to differentiate the plants. 5. Conclusions This paper presents
    a benchmark of four DL models for assessing the capability of CNN to leverage
    phenological classification in vegetable crops. A dataset with 4123 images was
    manually collected during various growing seasons of eight vegetable crops representing
    the main phenophases from emergence to harvest. For benchmarking purposes, each
    model was tested for the classification of each crop individually. YOLO v4 was
    the best-performing model compared to the SSD models. Therefore, this model was
    tested with all data obtaining a mAP of 76.6% and a BA of 81.7%. All assessed
    models delivered promising results, and were able to classify the phenological
    phases of individual plants between emergence and harvest. Furthermore, a single
    DL model could classify multiple vegetable crops considering phenology dynamics
    throughout the growing season at the subfield scale. As far as we have been able
    to ascertain, this is the first study to evaluate the potential of CV_DL for vegetable
    crops’ phenological research. Notwithstanding some limitations, this work is an
    important step towards sustainable agricultural practices. Further work needs
    to be done to (i) enlarge the dataset, balancing it with more images within less
    populated phenophases and increasing the number of vegetable crops; and (ii) assess
    the inference speed of the proposed models before transferring the detection framework
    to a robotic-assisted CV platform, thus allowing a more rigorous image collection
    as well as a more accurate ground truth identification; (iii) extend the benchmark
    with a new and more optimised backbone network, such as YOLO v7, and different
    DL architectures, such as faster R-CNN, confirming whether the SSD and YOLO architectures
    maintain better performance; and (iv) perform an ablation study in the best-performing
    model, YOLO v4, to understand the model behaviour and measure the contribution
    of each model component to the overall model performance. Furthermore, the proposed
    CV_DL framework can be framed in a modelling approach estimating the extent to
    which genetic, environmental, and management conditions impact the timing of phenophases
    and how they affect the efficiency of agricultural practices and crop yield. Author
    Contributions Conceptualisation, L.R., M.C. and F.N.d.S.; data curation, L.R.;
    investigation, L.R.; methodology, L.R., S.A.M. and D.Q.d.S.; software, L.R., S.A.M.
    and D.Q.d.S.; supervision, M.C. and F.N.d.S.; validation, M.C. and F.N.d.S.; visualisation,
    L.R.; writing—original draft, L.R.; writing—review and editing, S.A.M., D.Q.d.S.,
    M.C. and F.N.d.S. All authors read and agreed to the published version of the
    manuscript. Funding This work is partially financed by National Funds through
    the FCT—Fundação para a Ciência e a Tecnologia, I.P. (Portuguese Foundation for
    Science and Technology) within the project OmicBots, with reference PTDC/ASP-HOR/1338/2021.
    Besides, this work has received funding from the European Union’s Horizon 2020
    research and innovation programme under grant agreement No. 857202. Institutional
    Review Board Statement Not applicable. Informed Consent Statement Not applicable.
    Data Availability Statement The data presented in this study are openly available
    in the digital repository Zenodo: PixelCropRobot Dataset—https://doi.org/10.5281/zenodo.7433286
    (last accessed: 5 November 2022). Acknowledgments The authors would like to acknowledge
    the scholarships 9684/BI-M-ED_B2/2022 (L.R.), SFRH/BD/147117/2019 (S.A.M.), and
    UI/BD/152564/2022 (D.Q.d.S.) funded by National Funds through the Portuguese funding
    agency, FCT—Fundação para a Ciência e Tecnologia. Conflicts of Interest The authors
    declare no conflict of interest. The funders had no role in the design of the
    study; in the collection, analyses, or interpretation of data; in the writing
    of the manuscript, or in the decision to publish the results. References Lieth,
    H. Phenology and Seasonality Modeling. In Ecological Studies; Springer: Berlin/Heidelberg,
    Germany, 1974. [Google Scholar] [CrossRef] Liang, L. Phenology. In Reference Module
    in Earth Systems and Environmental Sciences; Elsevier: Amsterdam, The Netherlands,
    2019. [Google Scholar] [CrossRef] Ruml, M.; Vulić, T. Importance of phenological
    observations and predictions in agriculture. J. Agric. Sci. 2005, 50, 217–225.
    [Google Scholar] [CrossRef] Chmielewski, F.M. Phenology in Agriculture and Horticulture.
    In Phenology: An Integrative Environmental Science; Schwartz, M.D., Ed.; Springer:
    Dordrecht, The Netherlands, 2013; pp. 539–561. [Google Scholar] [CrossRef] Martín-Forés,
    I.; Casado, M.A.; Castro, I.; del Pozo, A.; Molina-Montenegro, M.; Miguel, J.M.D.;
    Acosta-Gallo, B. Variation in phenology and overall performance traits can help
    to explain the plant invasion process amongst Mediterranean ecosystems. NeoBiota
    2018, 41, 67–89. [Google Scholar] [CrossRef] Kato, A.; Carlson, K.M.; Miura, T.
    Assessing the inter-annual variability of vegetation phenological events observed
    from satellite vegetation index time series in dryland sites. Ecol. Indic. 2021,
    130, 108042. [Google Scholar] [CrossRef] Kasampalis, D.A.; Alexandridis, T.K.;
    Deva, C.; Challinor, A.; Moshou, D.; Zalidis, G. Contribution of Remote Sensing
    on Crop Models: A Review. J. Imaging 2018, 4, 52. [Google Scholar] [CrossRef]
    Fu, Y.; Li, X.; Zhou, X.; Geng, X.; Guo, Y.; Zhang, Y. Progress in plant phenology
    modeling under global climate change. Sci. China Earth Sci. 2020, 63, 1237–1247.
    [Google Scholar] [CrossRef] Kephe, P.; Ayisi, K.; Petja, B. Challenges and opportunities
    in crop simulation modelling under seasonal and projected climate change scenarios
    for crop production in South Africa. Agric. Food Secur. 2021, 10, 10. [Google
    Scholar] [CrossRef] Hufkens, K.; Melaas, E.K.; Mann, M.L.; Foster, T.; Ceballos,
    F.; Robles, M.; Kramer, B. Monitoring crop phenology using a smartphone based
    near-surface remote sensing approach. Agric. For. Meteorol. 2019, 265, 327–337.
    [Google Scholar] [CrossRef] Guo, Y.; Chen, S.; Fu, Y.H.; Xiao, Y.; Wu, W.; Wang,
    H.; Beurs, K.d. Comparison of Multi-Methods for Identifying Maize Phenology Using
    PhenoCams. Remote Sens. 2022, 14, 244. [Google Scholar] [CrossRef] Chacón-Maldonado,
    A.M.; Molina-Cabanillas, M.A.; Troncoso, A.; Martínez-Álvarez, F.; Asencio-Cortés,
    G. Olive Phenology Forecasting Using Information Fusion-Based Imbalanced Preprocessing
    and Automated Deep Learning. In Proceedings of the Hybrid Artificial Intelligent
    Systems Conference, Salamanca, Spain, 5–7 September 2022. [Google Scholar] [CrossRef]
    Milicevic, M.; Zubrinic, K.; Grbavac, I.; Obradovic, I. Application of Deep Learning
    Architectures for Accurate Detection of Olive Tree Flowering Phenophase. Remote
    Sens. 2020, 12, 2120. [Google Scholar] [CrossRef] Jing, H.; Xiujuan, W.; Haoyu,
    W.; Xingrong, F.; Mengzhen, K. Prediction of crop phenology—A component of parallel
    agriculture management. In Proceedings of the 2017 Chinese Automation Congress,
    Jinan, China, 20–22 October 2017; pp. 7704–7708. [Google Scholar] [CrossRef] Giordano,
    M.; Petropoulos, S.A.; Rouphael, Y. Response and Defence Mechanisms of Vegetable
    Crops against Drought, Heat and Salinity Stress. Agriculture 2021, 11, 463. [Google
    Scholar] [CrossRef] Dhondt, S.; Wuyts, N.; Inzé, D. Cell to whole-plant phenotyping:
    The best is yet to come. Trends Plant Sci. 2013, 18, 428–439. [Google Scholar]
    [CrossRef] Tripathi, M.K.; Maktedar, D.D. A role of computer vision in fruits
    and vegetables among various horticulture products of agriculture fields: A survey.
    Inf. Process. Agric. 2020, 7, 183–203. [Google Scholar] [CrossRef] Patrício, D.I.;
    Rieder, R. Computer vision and artificial intelligence in precision agriculture
    for grain crops: A systematic review. Comput. Electron. Agric. 2018, 153, 69–81.
    [Google Scholar] [CrossRef] Narvaez, F.; Reina, G.; Torres-Torriti, M.; Kantor,
    G.; Cheein, F. A Survey of Ranging and Imaging Techniques for Precision Agriculture
    Phenotyping. IEEE/ASME Trans. Mechatron. 2017, 22, 2428–2439. [Google Scholar]
    [CrossRef] Roy, A.M.; Bose, R.; Bhaduri, J. A fast accurate fine-grain object
    detection model based on YOLOv4 deep neural network. Neural Comput. Appl. 2022,
    34, 1–27. [Google Scholar] [CrossRef] Jin, X.; Che, J.; Chen, Y. Weed Identification
    Using Deep Learning and Image Processing in Vegetable Plantation. IEEE Access
    2021, 9, 10940–10950. [Google Scholar] [CrossRef] Aguiar, A.S.; Magalhães, S.A.;
    dos Santos, F.N.; Castro, L.; Pinho, T.; Valente, J.; Martins, R.; Boaventura-Cunha,
    J. Grape bunch detection at different growth stages using deep learning quantized
    models. Agronomy 2021, 11, 1890. [Google Scholar] [CrossRef] Kamilaris, A.; Prenafeta-Boldú,
    F.X. Deep learning in agriculture: A survey. Comput. Electron. Agric. 2018, 147,
    70–90. [Google Scholar] [CrossRef] Yang, B.; Xu, Y. Applications of deep-learning
    approaches in horticultural research: A review. Hortic. Res. 2021, 8, 123. [Google
    Scholar] [CrossRef] Katal, N.; Rzanny, M.; Mäder, P.; Wäldchen, J. Deep Learning
    in Plant Phenological Research: A Systematic Literature Review. Front. Plant Sci.
    2022, 13, 805738. [Google Scholar] [CrossRef] Potgieter, A.B.; Zhao, Y.; Zarco-Tejada,
    P.J.; Chenu, K.; Zhang, Y.; Porker, K.; Biddulph, B.; Dang, Y.P.; Neale, T.; Roosta,
    F.; et al. Evolution and application of digital technologies to predict crop type
    and crop phenology in agriculture. Silico Plants 2021, 3, diab017. [Google Scholar]
    [CrossRef] Yang, W.; Feng, H.; Zhang, X.; Zhang, J.; Doonan, J.; Batchelor, W.D.;
    Xiong, L.; Yan, J. Crop Phenomics and High-Throughput Phenotyping: Past Decades,
    Current Challenges, and Future Perspectives. Mol. Plant 2020, 13, 187–214. [Google
    Scholar] [CrossRef] [PubMed] Arya, S.; Sandhu, K.S.; Singh, J. Deep learning:
    As the new frontier in high-throughput plant phenotyping. Euphytica 2022, 218,
    1–22. [Google Scholar] [CrossRef] Yalcin, H. Plant phenology recognition using
    deep learning: Deep-Pheno. In Proceedings of the 6th International Conference
    on Agro-Geoinformatics, Fairfax, VA, USA, 7–10 August 2017; pp. 1–5. [Google Scholar]
    [CrossRef] Han, J.; Shi, L.; Yang, Q.; Huang, K.; Zha, Y.; Jin, Y. Real-time detection
    of rice phenology through convolutional neural network using handheld camera images.
    Precis. Agric. 2021, 22, 154–178. [Google Scholar] [CrossRef] d’Andrimont, R.;
    Yordanov, M.; Martinez-Sanchez, L.; van der Velde, M. Monitoring crop phenology
    with street-level imagery using computer vision. Comput. Electron. Agric. 2022,
    196, 106866. [Google Scholar] [CrossRef] Taylor, S.D.; Browning, D.M. Classification
    of Daily Crop Phenology in PhenoCams Using Deep Learning and Hidden Markov Models.
    Remote Sens. 2022, 14, 286. [Google Scholar] [CrossRef] Wang, X.; Tang, J.; Whitty,
    M. DeepPhenology: Estimation of apple flower phenology distributions based on
    deep learning. Comput. Electron. Agric. 2021, 185, 106123. [Google Scholar] [CrossRef]
    Molina, M.Á.; Jiménez-Navarro, M.J.; Martínez-Álvarez, F.; Asencio-Cortés, G.
    A Model-Based Deep Transfer Learning Algorithm for Phenology Forecasting Using
    Satellite Imagery. In Proceedings of the Hybrid Artificial Intelligent Systems,
    Bilbao, Spain, 22–24 September 2021; pp. 511–523. [Google Scholar] [CrossRef]
    Pearse, G.; Watt, M.S.; Soewarto, J.; Tan, A.Y. Deep Learning and Phenology Enhance
    Large-Scale Tree Species Classification in Aerial Imagery during a Biosecurity
    Response. Remote Sens. 2021, 13, 1789. [Google Scholar] [CrossRef] Correia, D.L.;
    Bouachir, W.; Gervais, D.; Pureswaran, D.; Kneeshaw, D.D.; De Grandpré, L. Leveraging
    Artificial Intelligence for Large-Scale Plant Phenology Studies From Noisy Time-Lapse
    Images. IEEE Access 2020, 8, 13151–13160. [Google Scholar] [CrossRef] Mann, H.M.R.;
    Iosifidis, A.; Jepsen, J.U.; Welker, J.M.; Loonen, M.J.J.E.; Høye, T.T. Automatic
    flower detection and phenology monitoring using time-lapse cameras and deep learning.
    Remote Sens. Ecol. Conserv. 2022, 8, 765–777. [Google Scholar] [CrossRef] Chavan,
    T.; Nandedkar, A. AgroAVNET for crops and weeds classification: A step forward
    in automatic farming. Comput. Electron. Agric. 2018, 154, 361–372. [Google Scholar]
    [CrossRef] Ofori, M.; El-Gayar, O. Towards Deep Learning for Weed Detection: Deep
    Convolutional Neural Network Architectures for Plant Seedling Classification.
    In Proceedings of the Americas Conference on Information Systems, Virtual, 10–14
    August 2020. [Google Scholar] Samiei, S.; Rasti, P.; Vu, J.; Buitink, J.; Rousseau,
    D. Deep learning-based detection of seedling development. Plant Methods 2020,
    16, 103. [Google Scholar] [CrossRef] Meier, U. (Ed.) Growth Stages of Mono- and
    Dicotyledonous Plants; Julius Kühn-Institut: Quedlinburg, Germany, 2018. [Google
    Scholar] Everingham, M.; Gool, L.V.; Williams, C.K.I.; Winn, J.M.; Zisserman,
    A. The Pascal Visual Object Classes (VOC) Challenge. Int. J. Comput. Vis. 2010,
    88, 303–338. [Google Scholar] [CrossRef] [Green Version] Padilla, R.; Passos,
    W.L.; Dias, T.L.B.; Netto, S.L.; da Silva, E.A.B. A Comparative Analysis of Object
    Detection Metrics with a Companion Open-Source Toolkit. Electronics 2021, 10,
    279. [Google Scholar] [CrossRef] Terra, F.; Rodrigues, L.; Magalhães, S.; Santos,
    F.; Moura, P.; Cunha, M. PixelCropRobot, a cartesian multitask platform for microfarms
    automation. In Proceedings of the 2021 International Symposium of Asian Control
    Association on Intelligent Robotics and Industrial Automation (IRIA), Goa, India,
    20–22 September 2021; pp. 382–387. [Google Scholar] [CrossRef] Liu, W.; Anguelov,
    D.; Erhan, D.; Szegedy, C.; Reed, S.; Fu, C.Y.; Berg, A.C. SSD: Single Shot MultiBox
    Detector. Lect. Notes Comput. Sci. 2016, 9905, 21–37. [Google Scholar] [CrossRef]
    Magalhães, S.A.; Castro, L.; Moreira, G.; dos Santos, F.N.; Cunha, M.; Dias, J.;
    Moreira, A.P. Evaluating the Single-Shot MultiBox Detector and YOLO Deep Learning
    Models for the Detection of Tomatoes in a Greenhouse. Sensors 2021, 21, 3569.
    [Google Scholar] [CrossRef] Moreira, G.; Magalhães, S.A.; Pinho, T.; dos Santos,
    F.N.; Cunha, M. Benchmark of Deep Learning and a Proposed HSV Colour Space Models
    for the Detection and Classification of Greenhouse Tomato. Agronomy 2022, 12,
    356. [Google Scholar] [CrossRef] Ioffe, S.; Szegedy, C. Batch Normalization: Accelerating
    Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of
    the 32nd International Conference on International Conference on Machine Learning,
    Lille, France, 6–11 July 2015; pp. 448–456. [Google Scholar] Szegedy, C.; Liu,
    W.; Jia, Y.; Sermanet, P.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich,
    A. Going deeper with convolutions. In Proceedings of the Conference on Computer
    Vision and Pattern Recognition, Boston, MA, USA, 7–12 June 2015; pp. 1–9. [Google
    Scholar] [CrossRef] Sandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; Chen, L.C.
    MobileNetV2: Inverted Residuals and Linear Bottlenecks. In Proceedings of the
    Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA,
    18–22 June 2018; pp. 4510–4520. [Google Scholar] [CrossRef] He, K.; Zhang, X.;
    Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. In Proceedings
    of the 2016 IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas,
    NV, USA, 26 June–1 July 2016; pp. 770–778. [Google Scholar] [CrossRef] Bochkovskiy,
    A.; Wang, C.Y.; Liao, H.Y.M. YOLOv4: Optimal speed and accuracy of object detection.
    arXiv 2020. [Google Scholar] [CrossRef] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.;
    Buhmann, J.M. The Balanced Accuracy and Its Posterior Distribution. In Proceedings
    of the 20th International Conference on Pattern Recognition, Istanbul, Turkey,
    23–26 August 2010; pp. 3121–3124. [Google Scholar] [CrossRef]      Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2023 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Rodrigues, L.; Magalhães, S.A.; da Silva, D.Q.;
    dos Santos, F.N.; Cunha, M. Computer Vision and Deep Learning as Tools for Leveraging
    Dynamic Phenological Classification in Vegetable Crops. Agronomy 2023, 13, 463.
    https://doi.org/10.3390/agronomy13020463 AMA Style Rodrigues L, Magalhães SA,
    da Silva DQ, dos Santos FN, Cunha M. Computer Vision and Deep Learning as Tools
    for Leveraging Dynamic Phenological Classification in Vegetable Crops. Agronomy.
    2023; 13(2):463. https://doi.org/10.3390/agronomy13020463 Chicago/Turabian Style
    Rodrigues, Leandro, Sandro Augusto Magalhães, Daniel Queirós da Silva, Filipe
    Neves dos Santos, and Mário Cunha. 2023. \"Computer Vision and Deep Learning as
    Tools for Leveraging Dynamic Phenological Classification in Vegetable Crops\"
    Agronomy 13, no. 2: 463. https://doi.org/10.3390/agronomy13020463 Note that from
    the first issue of 2016, this journal uses article numbers instead of page numbers.
    See further details here. Article Metrics Citations Crossref   6 Scopus   6 Web
    of Science   3 Google Scholar   [click to view] Article Access Statistics Article
    access statistics Article Views 10. Jan 20. Jan 30. Jan 9. Feb 19. Feb 29. Feb
    10. Mar 20. Mar 30. Mar 0 500 1000 1500 2000 2500 For more information on the
    journal statistics, click here. Multiple requests from the same IP address are
    counted as one view.   Agronomy, EISSN 2073-4395, Published by MDPI RSS Content
    Alert Further Information Article Processing Charges Pay an Invoice Open Access
    Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors
    For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives
    Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings
    Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release
    notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024
    MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions
    Privacy Policy"'
  inline_citation: '>'
  journal: Agronomy
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Computer Vision and Deep Learning as Tools for Leveraging Dynamic Phenological
    Classification in Vegetable Crops
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bawa A.
  - Samanta S.
  - Himanshu S.K.
  - Singh J.
  - Kim J.J.
  - Zhang T.
  - Chang A.
  - Jung J.
  - DeLaune P.
  - Bordovsky J.
  - Barnes E.
  - Ale S.
  citation_count: '6'
  description: Cotton boll count is an important phenotypic trait that aids in a better
    understanding of the genetic and physiological mechanisms of cotton growth. Several
    computer vision technologies are available for cotton boll segmentation. However,
    estimating the number of cotton bolls in a segmented cluster of cotton bolls is
    a challenging task due to the complex shapes of cotton bolls. This study proposed
    a combination of spectral-spatial and supervised machine learning based methods
    for cotton boll candidate recognition and counting from high resolution RGB images
    obtained from unmanned aerial vehicles (UAVs). An algorithm consisting of machine
    vision, band-mean filter, Otsu thresholding, red/blue band ratio filter, and geometrical
    characteristics-based error removal techniques, was employed to detect open cotton
    boll pixels under several environmental settings. In addition, a support vector
    machine (SVM) based encoding method was developed using geometric features of
    cotton boll candidates to predict the number of cotton bolls from the segmented
    cotton boll candidates. This algorithm was implemented over three experiment sites
    with three cotton varieties, two tillage practices, seven cover crop treatments,
    two irrigation regimes (irrigated and rainfed), 26 irrigation levels, and two
    sensors (DJI FC6310 RGB and MicaSense Rededge) capturing images at two spatial
    resolutions (0.75 cm and 1.07 cm) over two growing seasons (2019 and 2021). These
    different experimental settings allowed the proposed approaches to be validated
    against a variety of complex backgrounds. A visual inspection of 1000 randomly
    selected pixels revealed that the proposed cotton boll candidate recognition approach
    was highly effective in segmenting cotton bolls and background pixels, with high
    classification accuracy (> 95%) and a low number of falsely classified pixels
    (precision > 0.96; recall > 0.93). A high correlation between ground truth observations
    and predicted cotton boll count indicated that the use of geometric features of
    segmented candidates as predictors in association with the SVM model demonstrated
    a good performance in estimating boll count from recognized cotton boll candidates.
    Furthermore, linear regression analyses revealed that both boll count and candidate
    area are potential predictors of lint yield, with boll count being a better predictor
    than candidate area. Overall, the study demonstrated that machine vision/learning
    techniques can be potentially used on UAV images to count the number of cotton
    bolls and predict lint yield over large acreages with reasonable accuracy.
  doi: 10.1016/j.atech.2022.100140
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methods
    3. Results 4. Discussion 5. Conclusion Declaration of Competing Interest Acknowledgements
    Data availability References Show full outline Cited by (6) Figures (10) Show
    4 more figures Tables (5) Table 1 Table 2 Table 3 Table 4 Table 5 Smart Agricultural
    Technology Volume 3, February 2023, 100140 A support vector machine and image
    processing based approach for counting open cotton bolls and estimating lint yield
    from UAV imagery Author links open overlay panel Arun Bawa a b, Sayantan Samanta
    c, Sushil Kumar Himanshu b d, Jasdeep Singh b e, JungJin Kim b f, Tian Zhang b
    g, Anjin Chang h, Jinha Jung i, Paul DeLaune b, James Bordovsky j, Edward Barnes
    k, Srinivasulu Ale b Show more Share Cite https://doi.org/10.1016/j.atech.2022.100140
    Get rights and content Under a Creative Commons license open access Highlights
    • A simplified process for detecting cotton boll pixels from RGB images was developed.
    • Geometrical characteristics of candidates were used to improve image classification.
    • An SVR based method was developed to count open cotton bolls from cotton boll
    candidates. • The proposed methods were validated against a variety of complex
    backgrounds. • Lint yield relations with open cotton boll count and candidate
    area were developed. Abstract Cotton boll count is an important phenotypic trait
    that aids in a better understanding of the genetic and physiological mechanisms
    of cotton growth. Several computer vision technologies are available for cotton
    boll segmentation. However, estimating the number of cotton bolls in a segmented
    cluster of cotton bolls is a challenging task due to the complex shapes of cotton
    bolls. This study proposed a combination of spectral-spatial and supervised machine
    learning based methods for cotton boll candidate recognition and counting from
    high resolution RGB images obtained from unmanned aerial vehicles (UAVs). An algorithm
    consisting of machine vision, band-mean filter, Otsu thresholding, red/blue band
    ratio filter, and geometrical characteristics-based error removal techniques,
    was employed to detect open cotton boll pixels under several environmental settings.
    In addition, a support vector machine (SVM) based encoding method was developed
    using geometric features of cotton boll candidates to predict the number of cotton
    bolls from the segmented cotton boll candidates. This algorithm was implemented
    over three experiment sites with three cotton varieties, two tillage practices,
    seven cover crop treatments, two irrigation regimes (irrigated and rainfed), 26
    irrigation levels, and two sensors (DJI FC6310 RGB and MicaSense Rededge) capturing
    images at two spatial resolutions (0.75 cm and 1.07 cm) over two growing seasons
    (2019 and 2021). These different experimental settings allowed the proposed approaches
    to be validated against a variety of complex backgrounds. A visual inspection
    of 1000 randomly selected pixels revealed that the proposed cotton boll candidate
    recognition approach was highly effective in segmenting cotton bolls and background
    pixels, with high classification accuracy (> 95%) and a low number of falsely
    classified pixels (precision > 0.96; recall > 0.93). A high correlation between
    ground truth observations and predicted cotton boll count indicated that the use
    of geometric features of segmented candidates as predictors in association with
    the SVM model demonstrated a good performance in estimating boll count from recognized
    cotton boll candidates. Furthermore, linear regression analyses revealed that
    both boll count and candidate area are potential predictors of lint yield, with
    boll count being a better predictor than candidate area. Overall, the study demonstrated
    that machine vision/learning techniques can be potentially used on UAV images
    to count the number of cotton bolls and predict lint yield over large acreages
    with reasonable accuracy. Previous article in issue Next article in issue Keywords
    Machine learningSupervised classificationRemote sensingPhenotypingOtsu thresholding
    1. Introduction Cotton boll count is a valuable phenotypic trait, which most cotton
    breeders and producers use to develop a thorough understanding of the physiological
    and genetic crop growth [23]. Boll count also provides a means to assess crop
    growth conditions and facilitate timely crop management decisions to prevent yield
    losses. Cotton boll count is also an indicator of lint yield. Wells and Meredith
    [25] reported a positive correlation between bolls per unit area and lint yield.
    Yeom et al. [27] related area under boll pixels with cotton yield and reported
    a high correlation (R2 = 0.65). Rouze et al. [18] reported a moderate correlation
    (R2 = 0.49) between open boll count and cotton yield. Therefore, boll count can
    also serve as an important parameter for yield estimation and in gene-selection
    in plant breeding studies [8,25]. In addition, yield mapping using precision agriculture
    tools, can assist growers in overcoming in-field spatial variability [12]. Traditional
    boll counting methods are based on manual sampling and visual inspection, which
    are error-prone and also impractical for high acreage plant breeding programs
    [23]. With the technological advent of the Global Positioning System (GPS) integrated
    sensors in agriculture, it is now possible to monitor crop growth in real-time
    and automate farming operations [22,29]. A few studies were conducted over the
    last decade to automate cotton boll pixel extraction using remote electromagnetic
    radiation sensors mounted on Unmanned Aerial Vehicles (UAVs) or agri-robots. For
    example, using multispectral aerial imagery, Yeom et al. [27] proposed a region
    growing and Otsu threshold [16] based cotton boll pixel recognition method with
    high classification accuracy. Rouze et al. [18] generated open boll maps using
    a threshold for red/blue band ratio. Jung et al. [11] detected cotton bolls using
    a threshold of 190 for the red band in an 8-bit image. Although these studies
    have provided potential solutions for the segmentation of cotton boll pixels from
    the background, the segmented cotton boll pixels results in clusters of cotton
    bolls and none of the above mentioned study investigated the estimation of a number
    of cotton bolls from the detected cotton boll clusters. Sun et al. [23] proposed
    three geometric-based algorithms for cotton boll counting from high spatial resolution
    measurements from an agri-robot, but a similar study for UAV imagery was not found
    in the literature. Additionally, Sun et al. [23] boll counting method cannot be
    applied with UAV imagery due to the coarser spatial resolution of UAV measurements
    (a few cm) as compared to agri-robot measurements (< 1 mm). The UAV detected cotton
    boll clusters can have varied and complex shapes, mainly because of the cotton
    boll face orientation relative to the UAV sensor. Hence, simple area or size filters
    can not be applied to separate cotton bolls from the detected boll clusters. In
    this study, a novel approach was introduced to estimate the number of open cotton
    bolls from a red-green-blue (RGB) aerial imagery while segregating cotton boll
    pixels from the image background. Agricultural scientists are interested in rapidly
    collecting and analyzing a greater volume of high quality phenotypic trait data
    for crop improvement through breeding or other site-specific precision-agriculture
    approaches [20,22]. Because of their high spatio-temporal resolution, relatively
    lower operational cost, and less complexity in the data collection [20,29], UAVs
    have emerged as an intriguing remote sensing option for precision farming and
    agronomic research [19,29]. In addition, data science tools such as machine learning
    (ML) methods (e.g., support vector and random forests) are frequently used to
    extract information from the UAV imageries [17,28]. Such tools can be used in
    agricultural applications when classifying objects with complex and varied structures.
    For example, Yamamoto et al. [26] used X-means clustering algorithm to detect
    tomatoes and Li et al. [14] adopted random forest to achieve semantic labeling
    prediction for in-field cotton detection from images. These tools have the potential
    to automate the whole process, and hence they can be used in the development of
    decision support tools. In machine learning, support vector machines (SVMs) are
    high-dimensional hyperplane-based pattern recognition learning models that are
    used for the classification and regression analysis [4,7]. Due to SVMs’ capability
    to model complicated non-linear relationships, they are preferred over conventional
    classification and regression techniques such as logistic regression and linear
    or multiple regression models [13]. Varying sizes, complex shapes, orientation,
    and overlapping of cotton bolls along with a lack of linear relation between detected
    cotton boll candidate size and boll count, make cotton boll counting from UAV
    images a substantially challenging task. Therefore, the overall goal of this study
    was to evaluate an SVM regression (SVR)-based approach to establish non-linear
    relations among detected cotton boll clusters and cotton boll count across diverse
    environmental settings. Specific objectives of the study were to: (1) develop
    a simplified method for cotton boll candidate recognition from an image captured
    by an RGB sensor, (2) develop and validate an SVR-based method for counting cotton
    bolls within recognized cotton boll candidates, and (3) estimate cotton yield
    as a function of boll count and area under recognized boll candidates. 2. Materials
    and methods 2.1. Experiment setup In this study, UAV measurements were made over
    161 plots from three cotton experimental sites. Of these, two sites (I and II)
    are located at the Texas A&M AgriLife Research Station (34°15′ N, 99°30′ W), Chillicothe,
    Texas. The UAV measurements from these sites were collected during the 2021 cotton
    growing season. Experiment site I consisted of 12 irrigated plots (4 cover crop
    treatments X 3 replications) and experiment site II consisted of 21 dryland plots
    (7 cover crop treatments X 3 replications). The PHY 480 cultivar was used at both
    sites. More details about these field experiments can be found in DeLaune et al.
    [6] and DeLaune and Mubvumba [5]. These two cover crop experiments provided an
    opportunity to validate the approaches developed for cotton boll candidate recognition
    and boll count estimation across a variety of complex backgrounds. At the Texas
    A&M AgriLife Research Station (34°10′ N, 101°56′ W), Halfway, Texas experiment
    site (Site III), UAV images were collected from a cotton irrigation water use
    efficiency experiment. More details about the Halfway field experiment can be
    found in Bordovsky et al. [2], Himanshu et al. [10], and Himanshu et al. [9].
    The UAV measurements from this experiment allowed the validation of developed
    approaches against UAV images collected using a different sensor (described in
    Section 2.2) with different backgrounds from two distinct cotton varieties (FM2011
    and FM2484) over a different time period (2019 growing season), and at a different
    location as compared to sites I and II. Overall, this study used UAV measurements
    from three cotton varieties, two tillage practices, seven cover crop treatments,
    six different irrigation conditions, two different irrigation regimes (irrigated
    and rainfed), and two different years. 2.2. Data collection 2.2.1. UAV measurements
    and preprocessing At experiment site III, a Phantom 4 Pro (P4P; DJI, China) quadcopter
    equipped with a DJI FC6310 RGB (red, green, blue) (DJI, China) optical sensor
    (Fig. 1a) was used during the 2019 growing season. To collect UAV measurements
    from experiment sites I and II during the 2021 season, a Matrice 200 (M200; DJI,
    China) quadcopter equipped with a MicaSense Rededge multispectral (AgEagle Aerial
    Systems Inc, USA) optical sensor (Fig. 1b and 1c) was used. All of the flights
    were performed within one hour of solar noon, with little to no cloud cover and
    winds less than 15 km/h. The Pix4Dcapture mobile app, developed by Pix4D S.A.,
    Switzerland, was used to create the flight plans with the flight characteristics
    shown in Table 1. The flight altitude was set in order to obtain images with a
    spatial resolution close to 1 cm. Eighty-five percent front and side overlaps
    were employed to target oversampling and compensate for errors introduced by the
    uncalibrated/blurred/geometrically distorted images [1]. Download : Download high-res
    image (303KB) Download : Download full-size image Fig. 1. Unmanned Aerial Vehicle
    (UAV) imaging platforms and other instruments: (a) DJI Phantom 4 pro quadcopter
    equipped with DJI FC6310 RGB sensor, (b) Matrice 200 quadcopter equipped with
    MicaSense Rededge multispectral sensor, (c) MicaSense Rededge multispectral camera,
    (d) ground control point unit, (e) V-map dual-frequency global navigation satellite
    system, (f) MicaSense calibrated reflectance panel. Table 1. Characteristics of
    UAV flights and measurements. UAV measurements Year 2019 2021 Date October 21,
    2019 October 31, 2021 Drone Phantom 4 Pro Matrice 200 Sensor DJI FC6310 MicaSense
    Rededge Bands RGB RGB, RE, NIR Bands used in boll count RGB RGB Front overlap
    85% 85% Side overlap 85% 85% Height 30 m 15 m GCPs 8 8 Georeferencing RMSE 1.1
    cm 1.4 cm Spatial resolution 0.75 cm 1.07 cm Experiment site III I and II Note:
    R- Red; G- Green; B- Blue; RE- Red Edge; NIR- Near-Infrared; GCP- Ground Control
    Point; RMSE: Root Mean Square Error. All captured images were geotagged in real-time
    using the onboard GPS systems installed in the quadcopter platforms. In addition,
    eight ground control points (GCPs; Fig. 1d) were employed in the field to generate
    georeferenced data products with high precision. The coordinates of the GCPs were
    measured using the V-map dual frequency-post processed static (PPS) global navigation
    satellite system (GNSS; Fig. 1e; Micro Aerial Projects, USA) and they were used
    to georeference the orthomosaic images. As recommended by Rouze et al. [18], orthomosaic
    images with less than 1.5 times the pixel resolution georeferencing root mean
    square error (RMSE) were extracted in GeoTiff format for further image processing.
    Additionally, a MicaSense calibrated reflectance panel (Fig. 1f) supplied by MicaSense
    was used to radiometrically calibrate the captured images in order to perform
    an illumination adjustment and obtain more accurate reflectance values. During
    this calibration process, one image per band of the calibration panel was captured
    at the time of flight and the spectral reflectance of the processed images was
    adjusted based on the fixed reflectance values of the panel. More details about
    the calibration panel and spectral reflectance correction can be found at MicaSense
    knowledge base (https://support.micasense.com/). 2.2.2. Manual data collection
    for cotton boll count and cotton yield Ground truth data for cotton boll count
    and yield were collected from the experimental sites to validate the algorithm
    developed for counting cotton bolls. Cotton bolls were counted using a 1 m X 1
    m area in two middle rows (rows 4 and 5) per plot at sites I and II and from a
    set of five plants in one row per cotton cultivar per plot at the third experiment
    site. The coordinates of the ground truth data location area were recorded using
    V-map PPS GNSS. These locations were also marked with flags so that the same plants
    were monitored throughout the growing season and also to easily detect ground
    truth data locations in the UAV images and minimize errors associated with GPS
    instruments. Ground truth data for cotton boll count was collected one week before
    harvesting from 12 to 21 plots at experiment sites I and II, respectively, in
    2021 and from 48 plots at experiment site III in 2019. Whereas ground truth data
    for cotton yield was collected from 12 to 21 plots at experiment sites I and II,
    respectively, in 2021 and from 128 plots at experiment site III in 2019. This
    ground truth information on cotton yield was used for developing a relation between
    cotton boll count and cotton yield in this study. 2.3. Image processing The methodology
    proposed for cotton boll counting (Fig. 2) consists of two parts. First, the collected
    images were subjected to an image processing pipeline for the cotton boll candidate
    recognition. The goal of the cotton boll candidate recognition was to segment
    cotton boll pixels from the surrounding background, which included soil, weed,
    foliage/residuals, leaves, and branches. To remove non-cotton boll pixels, a python
    programming-based pipeline was developed, as described in Section 2.3.1. The output
    from this step was a binary image in which cotton boll candidate pixels were classified
    with pixel intensities of one and the non-cotton-boll pixels with pixel intensities
    of zero. This binary image was further converted into a polygon vector of cotton
    boll candidates. Download : Download high-res image (496KB) Download : Download
    full-size image Fig. 2. Flowchart of the processes involved in the proposed approach
    for counting cotton bolls. (Note: UAV- Unmanned Aerial Vehicle, SVR- Support Vector
    Regression). In the second stage, an ML-based SVR model was adopted to count the
    total number of bolls in each segmented cotton boll candidate identified in the
    previous step. The response variable in this SVR model was the number of cotton
    bolls in a cotton boll candidate with four predictors comprising geometric aspects
    of the cotton boll candidate, including area, perimeter, maximum length, and roundness.
    The final outcome of this process was a polygon vector of cotton boll candidates
    with information on the number of cotton bolls in each candidate. 2.3.1. Cotton
    boll candidate recognition The cotton boll candidate recognition was performed
    in four stages using a band-mean filter, the Otsu thresholding [16], a red/blue
    band ratio filter, and an area filter. The primary goal of this candidate selection
    approach was to eliminate the background pixels of the image, including plant
    shadows, foliage/residue, dark ground, and weeds. The spectral behaviors of these
    classes across the RGB bands were analyzed at all three sites (Fig. 3) by averaging
    the pixel values of 20 randomly selected samples within each class, with a minimum
    of 10 pixels per sample. Cotton boll pixels were found to be associated with the
    highest reflectance. Therefore, in the first stage, a band-mean filter was applied
    to remove dark pixels. In this process, the pixels having a lower reflectance
    value than the mean value of each band in all three RGB bands were filtered out
    (Fig. 4b and 4d). A majority of the remaining pixels after the band-mean filtering
    process belonged to cotton boll and soil classes. Download : Download high-res
    image (140KB) Download : Download full-size image Fig. 3. Spectral signatures
    of different classes across red, green, and blue bands. Download : Download high-res
    image (1MB) Download : Download full-size image Fig. 4. Cotton boll candidate
    recognition process outputs: (a) subset of orthomosaic image from experiment site
    III, (b) image after applying band-mean filter, (c) image after applying Otsu
    threshold, (d) histogram of pixel values for blue band in the orthomosaic image;
    black columns representing pixel values less than the band mean, (e) histogram
    of pixel values for blue band after the band-mean filtration process; black columns
    representing pixel values less than the Otsu threshold. One of the most challenging
    tasks in image processing is to separate pixels with identical reflectance values
    using a threshold. Selecting a fixed threshold to separate these remaining two
    major classes from different UAV images could lead to error due to varying spectral
    properties of the images depending upon UAV sensors, field conditions, and several
    other environmental and instrumental factors. The Otsu method was employed in
    this study to set an independent threshold for each scene using the spectral properties
    of the filtered pixels. The Otsu method is an automatic thresholding approach
    that determines the threshold by minimizing intra-class variation and maximizing
    inter-class variation [16]. A bimodal distribution was discovered in the reflectance
    histogram of filtered pixels in the R, G, and B channels suggesting the presence
    of two major classes i.e., soil/bright weed and cotton boll. The Otsu thresholding
    approach divided the filtered pixels into two classes: (i) low reflectance class,
    which represented bright weed and bare soil pixels, and (ii) high reflectance
    cotton boll candidate pixels (Fig. 4e). All low reflectance pixels were then masked
    out, thereby eliminating a majority of the background pixels (Fig. 4c). To ensure
    the complete removal of all background pixels, a third filter, red/blue band ratio,
    was used. This filter was used to further eliminate the non-target pixels. It
    was observed from the spectral signature analysis (Fig. 3) that cotton boll pixels
    showed similar digital number values on all three bands indicating a red/blue
    band ratio close to one. Therefore, pixels with a red/blue band ratio greater
    than 1.2 were thus filtered out. Additionally, an area filter was applied to remove
    noise from the data assuming that a single cotton boll would have a minimum of
    3 cm2 area. 2.3.2. Errors associated with recognition of cotton boll candidates
    and their removal Following the cotton boll candidate recognition process, a preliminary
    inspection of output binary images revealed few challenges or potential error
    sources that could introduce substantial errors in boll count and yield predictions.
    In this study, three error sources were recognized, including bright weed (Fig.
    5a), center pivot system parts (Fig. 5b), and cotton bolls on the ground (Fig.
    5c) pixels, which were misclassified as cotton boll candidates. These error pixels
    had similar spectral properties as those of cotton boll pixels, and thus they
    could not be removed using previously applied spectral filters. Consequently,
    two additional steps (Fig. 2) were adopted to exclude these misclassified pixels.
    First, an elevation filter was implemented to remove error pixels associated with
    cotton bolls on the ground and parts of the center pivot system since they were
    at different elevations as compared to the cotton bolls on the plants. A canopy
    height model (CHM), resampled to the spatial resolution of the orthomosaic image,
    was developed using digital surface and terrain models. Details about the CHM
    generation process using UAV imagery can be found in Chang et al. [3]. The recognized
    candidates that are either less than 5 cm or greater than 2 m elevation in CHM
    were removed. Some parts of the center pivot system were, however, present in
    between the selected threshold elevation criteria and they were therefore still
    misclassified as cotton boll candidates. A filter with a combination of roundness
    index (< 0.1) and maximum length (> 1 m) was implemented to filter these misclassified
    candidates. Here, the maximum length of the candidates was defined as the maximum
    length among all the combinations of vertex-to-vertex lengths of the candidate
    and the roundness was calculated using Eq. (1) [27]: (1) Download : Download high-res
    image (3MB) Download : Download full-size image Fig. 5. Snapshots of orthomosaiced
    UAV image from experiment site III with recognized cotton boll candidate polygons
    before (red) and after (blue) the error removal process. (Note: Green arrow is
    pointing to the falsely classified pixel of cotton on ground before and after
    the error removal process). 2.3.3. Boll counting Varying sizes and complex shapes
    of cotton bolls pose challenges in counting the number of bolls in a cluster of
    cotton bolls. In addition, the orientation and overlapping of cotton bolls make
    cotton boll counting from UAV images even more challenging. Clusters composed
    of several overlapping cotton bolls generally have a larger area and elongation
    ratio than a single cotton boll. However, these clusters may have a similar elongation
    ratio as that of a single cotton boll. In addition, a single cotton boll can have
    a round or non-round/elongated structure, depending upon the boll facing angle
    when detected by the UAV sensors. As a result, counting the number of cotton bolls
    from a cotton boll candidate using a simple threshold for area or elongation ratio
    or using a combination of these thresholds can lead to errors. Therefore, we implemented
    an SVR-based ML approach to overcome these limitations and account for all complex
    and varied sizes of cotton bolls in the images. The SVR is a regression version
    of the SVM that is used to develop the relation between a dependent variable and
    one or more independent variables. A detailed description of the SVR can be found
    in Smola and Schölkopf [21] and Tian et al. [24]. In this study, the approach
    proposed for cotton boll counting from segmented candidates includes a supervised
    SVR model with four geometric aspects of the cotton boll candidates as feature
    vectors: area, parameter, roundness, and maximum length. These feature vectors
    and their corresponding labels were used to train a non-linear kernel function
    based SVR model. Before training SVR model in R, a trainControl (Package: caret)
    function was used to generate parameters that control computational nuances of
    the train (Package: caret) method. The repeatedCV (repeated cross-validation)
    resampling method with three separate 10-fold cross-validation was used as resampling
    scheme for the trainControl function. The SVR model was trained using train function
    with the svmRadial method. The train function fits predictive models over different
    tuning parameters, evaluates the effect of tuning parameters on model performance,
    and selects an optimal model across the tuning parameters considered. A preprocessing
    of the training data using the “center” & “scale” parameters in the train function
    was also incorporated to standardize training data. The train function selected
    the optimal SVR model for the training data at C = 2 (penalty parameter of the
    error term) using the smallest RMSE values at different C values. The SVR model
    was trained using 80 training samples and evaluated over 20 test samples. The
    training and test samples for the SVR model were collected through visual inspection
    from the orthomosaic images of experiment site II. The samples were chosen in
    such a way that a wide variation in candidate area was captured. Further, the
    createDataPartition (Package: caret) function was used to create stratified random
    samples based on cotton boll count (i.e., response variable) and split the samples
    into training and test data. Three independent users counted the number of cotton
    bolls in the selected sample candidates using visual inspection of the ortho-mosaiced
    images, and the mean values from the three inspections were recorded as a response
    variable in the SVR model. The evaluated SVR model was then used to count the
    number of cotton bolls in the segmented candidates across all three experiment
    sites. 2.4. Statistical analysis The accuracy of the candidate recognition process,
    before and after the removal of error sources, was assessed using visual inspection
    of 1000 randomly selected pixels (500 cotton boll candidates and 500 background
    pixels) for each experiment site. The confusion matrix and precision-recall methods
    were implemented to evaluate the quality of the output of the candidate recognition
    process by classifying the data into four categories, namely, true positive (TP),
    true negative (TN), false positive (FP), and false negative (FN). The TP and TN
    categories represented the correctly recognized pixels whereas FN and FP represented
    the omission and commission errors, respectively. The accuracy of the candidate
    recognition process was further assessed using three statistical metrics presented
    in Eqs. (2)–(4): (2) (3) (4) While classification accuracy represents the fraction
    of correctly classified pixels among all pixels, precision represents the fraction
    of retrieved pixels that were relevant to the query, and recall represents the
    fraction of relevant pixels that were successfully retrieved. The proposed SVR-based
    cotton boll counting approach was quantitatively validated using a two-step approach.
    First, the performance of the established SVR model was evaluated for training
    and test data. Later, the estimated number of cotton bolls was compared with the
    measured/ground truth cotton boll count data for validation. Three statistical
    metrics, viz., coefficient of determination (R2), mean absolute percentage error
    (MAPE), and root mean square error (RMSE) were used to evaluate the performance
    of the SVR-based cotton boll counting approach Eqs. (5)–((7)). (5) (6) (7) where,
    n is the number of plots, and are the observed (or ground truth) and predicted
    number of cotton bolls. Additionally, observed and predicted boll count data were
    plotted and compared for the agreement with the 1:1 line. 2.5. Lint yield prediction
    and validation The number of cotton bolls estimated from the UAV images is typically
    lower than the actual number of bolls due to the covering of lower canopy cotton
    bolls by upper canopy cotton bolls while collecting UAV images with the sensor
    pointed to the nadir direction. However, we found a high correlation between the
    UAV detected boll count and actual boll count in this study with a low MAPE (discussed
    in Section 3.1.2). Therefore, we used a linear regression analysis to develop
    relations between lint yield and UAV estimated boll count and candidate area.
    Lint yield prediction relations were developed for each variety, assuming that
    the weight of each variety''s cotton boll is different. A multiple linear regression
    approach was also used while considering both boll count and candidate area as
    predictors. These yield prediction relations were developed using 75% of observed
    lint yield data as training data and the remaining 25% of the data as the testing
    data for each cotton variety. The createDataPartition (Package: caret) function
    was used to split the data into training and test data, and to create a stratified
    random sample considering observed yield data as response variable. The developed
    linear relations were compared and evaluated using four statistical metrics: R2,
    RMSE, MAPE, and Nash–Sutcliffe Efficiency (NSE; Eq. (8)). (8) 3. Results 3.1.
    Validation of cotton boll counting approach 3.1.1. Accuracy assessment of cotton
    boll candidate recognition process Fig. 6 and Table 2 show the confusion matrix
    and values of statistical metrics related to evaluation of the candidate recognition
    process. Although the evaluation statistics suggested high classification accuracy
    in extracting cotton boll candidates even before removing the errors, output images
    still contained falsely classified pixels. The greatest error was noted for the
    experiment site III, which was primarily due to the presence of a large number
    of bright weed and pivot part pixels (Fig. 5). The bright soil pixels introduced
    additional commission errors (mistakenly accepting a false observation) in the
    candidate recognition process for the experiment site III. The commission errors
    at experiment sites I and II (Fig. 6) were only due to bright soil pixels. Download
    : Download high-res image (994KB) Download : Download full-size image Fig. 6.
    Experiment-wise confusion matrix for accuracy assessment of cotton boll candidate
    recognition process. Table 2. Experiment-wise statistical metrics for accuracy
    assessment of cotton boll candidate recognition process (before and after error
    removal). Statistical metrics Site I Site II Site III Before After Before After
    Before After Classification accuracy 0.95 0.97 0.96 0.98 0.89 0.94 Precision 0.94
    0.97 0.94 0.98 0.86 0.96 Recall 0.96 0.96 0.98 0.98 0.93 0.93 The digital numbers
    for these error pixels were found to be comparable to those of cotton boll pixels
    in all three bands, implying that these pixels could not be segmented from the
    cotton boll pixels solely based on spectral properties. Therefore, mean elevation,
    max length, and roundness parameters of recognized candidates were used in the
    error removal process. This approach improved the accuracy of candidate extraction
    process (Table 2) by removing commission error pixels while it did not affect
    omission error (mistakenly rejecting a true observation; Fig. 7) pixels. High
    values for precision and recall after the error removal process indicated a low
    number of falsely identified pixels among the recognized pixels and identification
    of a high proportion of true cotton boll pixels, respectively. Overall, the observed
    statistical metrics indicated that the cotton boll candidate recognition technique
    was highly effective in segmenting cotton bolls and background pixels (Fig. 8).
    Download : Download high-res image (497KB) Download : Download full-size image
    Fig. 7. Omission error resulting from pivot shadowed pixels at experiment site
    III. 3.1.2. SVR-based cotton boll counting approach The SVR model performance
    in estimating the total number of cotton bolls was good as indicated by only 0.55%
    error during training and 3.6% error during validation (Table 3). The evaluated
    model was then used to estimate boll count from the image of the entire experimental
    area at each site. Fig. 8 depicts the outcomes of the candidate recognition and
    cotton boll counting steps for a single cotton plant. The estimated boll count
    was then validated against the observed/ground truth boll count. Fig. 9 and Table
    4 show the correlation analysis between the estimated and observed boll counts.
    The high correlation and associated low MAPE values found for all three sites
    (Fig. 9) suggested a good agreement between the observed and estimated boll counts.
    Overall, the evaluation and validation statistics suggested that the use of geometric
    features of segmented candidates as predictors in association with the SVR model
    demonstrated a good performance in estimating boll count from recognized cotton
    boll candidates. Table 3. Evaluation statistics of developed SVR model for the
    training and test datasets. Parameters Training Test Total predicted cotton bolls
    362 81 Total original cotton bolls 364 84 Difference 2 3 Percent Error 0.5 3.6
    Misclassification error from the model (%) 5 25 Download : Download high-res image
    (549KB) Download : Download full-size image Fig. 8. Cotton boll count pipeline:
    (a) UAV captured orthomosaic image, (b) cotton boll candidate recognition using
    spectral properties, and (c) cotton boll count using geometrical features of recognized
    candidates. Download : Download high-res image (413KB) Download : Download full-size
    image Fig. 9. Experiment-wise correlation analysis of estimated and ground-truth
    observations for cotton boll count [Note: Dotted black line - 1:1 line; Blue line
    - Line of best fit]. Table 4. Experiment-wise statistical metrics for correlation
    analysis of estimated and ground-truth observations for cotton boll count. Experiment
    site Statistical Metrics R2 RMSE MAPE Site I 0.91 4.14% 5.80% Site II 0.83 5.80%
    0.69% Site III 0.76 7.07% 11.46% Note: R2: Coefficient of determination; RMSE:
    Root mean square error; MAPE: Mean absolute percentage error. 3.2. Lint yield
    prediction Cotton boll count and candidate area were found to be highly correlated
    with lint yield in the linear regression analysis (Fig. 10). Multiple linear regression
    was used to test if both parameters, i.e., cotton boll count and candidate area
    predicted lint yield well. Fig. 10 shows the fitted regression models, and Table
    5 presents the information on statistical metrics and fitted model equations for
    all three cotton cultivars used in the field experiments. It was found that the
    candidate area did not contribute significantly (p > 0.05) to lint yield prediction
    under multiple linear regression models. However, in the fitted linear regression
    models, candidate area showed high correlation with lint yield (R2= 0.55–0.85;
    Table 5) for all cotton cultivars and significantly predicted lint yield (p <
    0.05). Overall, the statistical metrics indicated the boll count as a better predictor
    of lint yield than the candidate area, with higher R2 and NSE values and lower
    RMSE and MAPE values (Table 5). Although multiple linear regression models predicted
    lint yield better, the yield prediction did not improve much in comparison to
    the linear regression model using boll count alone as a predictor (Table 5). Download
    : Download high-res image (904KB) Download : Download full-size image Fig. 10.
    Variety-wise cotton yield prediction using cotton boll count (left column: parts
    a, d, and g), recognized candidate area (middle column: parts b, e, and h); and
    cotton boll count and candidate area (right column: parts c, f, and i) as predictors.
    Table 5. Variety-wise regression models for cotton yield prediction. Cotton Cultivar
    Training Test Empty Cell Empty Cell Empty Cell R2 NSE RMSE MAPE R2 NSE RMSE MAPE
    Model p-value Linear Regression- Boll Count PHY480 0.81 0.81 112.83 0 0.90 0.79
    94.65 −2.3 Y = 0.0018*X + 349.63 < 0.001 FM2484 0.77 0.77 195.23 0 0.85 0.83 181.07
    −2.6 Y = 0.0026*X - 183.42 < 0.001 FM2011 0.87 0.87 157.56 0 0.87 0.85 188.53
    −5.4 Y = 0.0035*X - 241.07 < 0.001 Linear Regression - Candidate Area PHY480 0.58
    0.58 168.05 0 0.79 0.55 140.31 7.5 Y = 1.37*X + 290.87 < 0.001 FM2484 0.76 0.76
    199.05 0 0.82 0.81 191.64 0.8 Y = 1.26*X + 250.42 < 0.001 FM2011 0.77 0.77 211.83
    0 0.85 0.84 190.98 −4.1 Y = 1.67*X + 240.74 < 0.001 Multiple Linear Regression-
    Boll count and Candidate Area Boll Count Candidate Area PHY480 0.81 0.81 112.83
    0 0.9 0.79 94.31 −2.4 Y = 0.0019*X1 −0.01*X2 +351.36 < 0.001 NS FM2484 0.79 0.79
    186.65 0 0.86 0.85 170.32 −1.1 Y = 0.0015*X1 +0.59*X2 −22.69 NS NS FM2011 0.88
    0.88 156.40 0 0.88 0.86 180.84 −5.4 Y = 0.0031*X1 +0.21*X2 −204.01 < 0.001 NS
    Note: R2: Coefficient of determination; NSE: Nash–Sutcliffe model efficiency coefficient;
    RMSE: Root mean square error; MAPE: Mean absolute percentage error; X1: Boll count;
    X2: Candidate area. 4. Discussion In this study, a simplified cotton boll candidate
    recognition algorithm was developed using simple spectral filters and an SVR-based
    machine learning tool. The evaluation and validation statistics suggested that
    this method can reliably recognize and estimate cotton boll count from RGB-based
    UAV measurements collected after defoliation at the late boll opening cotton growth
    stage. The UAV imagery collected after defoliation resulted in a relatively simple
    background with few to no green leaves covering or casting shadows on the cotton
    bolls in the lower canopy. Additionally, UAV flights during solar noon produced
    images free from shadows and minimized the omission errors. An 85% front and side
    overlap was set while taking UAV measurements to target oversampling, which helped
    in avoiding image gaps introduced during the orthomosaicking process when removing
    uncalibrated/blurred/geometrically distorted images [1]. In addition, taking UAV
    measurements at solar noon with a wind speed of less than 15 kmph resulted in
    low commission errors. Therefore, this study recommends collecting and processing
    UAV measurements in the above-mentioned manner as the first step to minimize error
    sources and obtain high accuracy with the proposed candidate recognition process.
    Although the errors in candidate recognition were small, these error pixels could
    introduce significant errors in cotton yield estimates if yield estimates are
    solely based on the area under the recognized candidates [27] or boll count [18].
    Therefore, we included an error removal step to remove major commission error
    pixels and improve the performance of the candidate recognition process. Different
    types of error sources in the UAV images from different sites, particularly error
    sources with similar spectral properties to cotton bolls in RGB bands, suggested
    a need for the consideration of non-spectral filters to minimize with these errors.
    After a preliminary evaluation of the outputs of the candidate recognition process
    and identification of the error sources, this study considered mean CHM, roundness,
    and the maximum length of recognized candidates as non-spectral filters because
    error sources were found to be elongated and at a different height as compared
    to the target cotton boll pixels. In this study, band-mean digital number values
    ranged from 121 to 147 and the Otsu threshold ranged between 163 and 186 for the
    blue band. The observed long range of band-mean and Otsu thresholding indicated
    the importance of selecting thresholds based on the spectral properties of the
    image or a subset of the image. Selecting a fixed threshold could lead to a high
    omission or commission error due to removal of lower reflectance cotton boll pixels
    or inclusion of higher reflectance background pixels, respectively. For example,
    Jung et al. [11] used a threshold value of 190 for the red band to segment cotton
    boll pixels from the background, which worked well for that study area, but using
    this threshold for UAV images in this study led to high omission error. We also
    used a fixed red/blue band ratio of 1.2 in the candidate recognition process to
    avoid high omission error, but we recommend evaluating classification accuracy
    by changing this threshold value. However, given the high classification accuracy
    obtained for complex backgrounds across seven different cover crop treatments
    in this study, a fixed threshold of 1.2 for the red/blue band ratio can provide
    satisfactory results. In this study, cotton boll pixels in the lower canopy were
    primarily responsible for the omission errors. These pixels were shadowed by the
    upper canopy, resulting in lower digital numbers, and they were eliminated during
    the background filtration processes. This study did not include steps for removal
    of omission error sources due to high complexity of removal process and presence
    of low omission error values The SVR algorithm is generally considered as a relatively
    simple method in ML, however robust in its performance, because the final decision
    function is determined by only a few support vectors [15]. Here, support vectors
    are nonlinear combinations derived from geometric aspects of the cotton boll candidate,
    namely area, perimeter, max length, and roundness. In this study, the SVR-based
    cotton boll count approach showed potential to extract information from complex
    shapes of clusters. However, this approach might not estimate the exact number
    of cotton bolls in a large cluster due to large overlapping and different orientations
    of cotton bolls. These large clusters were the key source of classification error
    for the test data. However, our approach performed very well for the small clusters
    of less than six cotton bolls. A larger training dataset with a greater number
    of large clusters could be useful to further improve this approach to reduce classification
    errors. Nonetheless, the small error found in the overall boll count during both
    training and testing demonstrated the applicability of this boll count algorithm.
    The identification of true boundaries of the individual cotton bolls within a
    cluster was not considered in this study since it could result in loss of cotton
    boll pixels and reducing the area under recognized candidates, which is a potential
    predictor of cotton yield. The linear regression analysis for predicting lint
    yield using boll count and candidate area suggested that lint yield was sensitive
    to both predictors, but boll count had a greater impact on lint yield than the
    candidate area. While the proposed boll count approach addressed some of the issues
    related to overlapping and clustering, it did not change the area under the recognized
    cotton boll candidates. In this study, the UAV imageries taken at the nadir (pitch
    = 90º) direction were used to validate the proposed approaches. However, some
    cotton bolls were present in the lower canopy and some other bolls had a downfaced
    orientation. Although these conditions were limited, they resulted in a slight
    underprediction of boll count since these bolls were not visible in the captured
    images. Future studies can explore the suitability of different pitch angles for
    UAV sensors for estimating cotton boll count. Although the methods proposed in
    this study showed high accuracy, there is still scope to improve the cotton boll
    recognition and counting processes. For, example, this study utilized a common
    red/blue band ratio threshold of 1.2 which can be changed as per the UAV measurements
    and background characteristics. Another improvement can be through the increase
    of training sample size or increasing the number of predictors in the model development
    to increase the accuracy of the SVR model. Utilizing more complex approaches such
    as artificial or convolutional neural network-based deep learning approaches could
    also improve the boll counting process. Generating 3-D data such as using LiDAR
    could also be effectively implemented for the error removal steps in the proposed
    cotton boll count approach as well as to quantify the spatial distribution of
    bolls [23]; however, that would increase the cost and complexity of the process.
    Another limitation of this study is that it considered only three cotton cultivars.
    Cotton varieties affect spatial distribution and the number of cotton bolls in
    the lower canopy, and hence affect the omission error in the UAV measurements.
    Therefore, there is a need for an omission error removal process under certain
    cotton varieties. 5. Conclusion This study proposed a simplified cotton boll candidate
    recognition process, followed by a cotton boll count method within the recognized
    candidates. The evaluation statistics of the candidate recognition process indicated
    that the spectral properties of the cotton and background pixels could be effectively
    used to segment cotton boll pixels from the UAV images. The classification accuracy
    assessment of the candidate recognition process revealed that there could be background/nontarget
    pixels that introduce commission errors (false positives) due to similar spectral
    properties to cotton boll pixels in UAV images, such as bright soil or weed pixels,
    that cannot be segmented using spectral filters. The commission error removal
    processes implemented in this study highlighted the utilization of the CHM and
    geometrical features of recognized candidates to improve classification accuracy.
    The shadowed cotton boll pixels were found to be the source of omission error
    (false negative). This study did not include the omission error removal steps
    due to presence of low omission errors and to avoid computational complexity.
    The boll count method also performed promisingly, with a slight underestimation
    of the number of cotton bolls. A potential source for the underestimation was
    the overlapping of cotton bolls in the captured 2-D UAV images, which could be
    addressed by collection of 3-D images in the future. Linear regression analysis
    results indicated that both boll count and candidate area are the potential predictors
    of the lint yield. However, boll count was found to be a better predictor of lint
    yield than the candidate area. Overall, statistical evaluation and validation
    metrics obtained in this study suggest that the proposed cotton boll candidate
    recognition and boll count methods for UAV images could potentially lead to more
    effective and efficient evaluation and management of experimental plots or fields
    by researchers and producers. Declaration of Competing Interest The authors declare
    that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgements
    We gratefully acknowledge the funding support provided by Cotton Incorporated
    for this study. Partial funding for this study was also provided by the Ogallala
    Aquifer Program, a consortium between USDA Agricultural Research Service, West
    Texas A&M University, Texas Tech University, Texas AgriLife Research, Texas AgriLife
    Extension Service and Kansas State University. Data availability Data will be
    made available on request. References [1] J.S. Aber, I. Marzolff, J. Ries Small-Format
    Aerial photography: Principles, Techniques and Geoscience Applications Elsevier
    (2010) Google Scholar [2] J.P. Bordovsky, J.T. Mustian, G.L. Ritchie, K.L. Lewis
    Cotton irrigation timing with variable seasonal irrigation capacities in the Texas
    south plains Appl. Eng. Agric., 31 (2015), pp. 883-897 View in ScopusGoogle Scholar
    [3] A. Chang, J. Jung, M.M. Maeda, J. Landivar Crop height monitoring with digital
    imagery from Unmanned Aerial System (UAS) Comput. Electron. Agric., 141 (2017),
    pp. 232-237 View PDFView articleView in ScopusGoogle Scholar [4] C. Cortes, V.
    Vapnik Support-vector networks Mach. Learn., 20 (1995), pp. 273-297 View in ScopusGoogle
    Scholar [5] P. DeLaune, P. Mubvumba Winter cover crop production and water use
    in Southern Great Plains cotton Agron. J., 112 (2020), pp. 1943-1951 CrossRefView
    in ScopusGoogle Scholar [6] P. DeLaune, P. Mubvumba, Y. Fan, S. Bevers Cover crop
    impact on irrigated cotton yield and net return in the Southern Great Plains Agron.
    J., 112 (2020), pp. 1049-1056 CrossRefView in ScopusGoogle Scholar [7] Drucker
    H., Burges C.J.C., Kaufman L., Smola A., Vapnik V. 1997. Support vector regression
    machines. In: Mozer M.C., Jordan M.I., and Petsche T. (Eds.), Advances in Neural
    Information Processing Systems 9, MIT Press, Cambridge, MA, pp. 155ȓ161. Google
    Scholar [8] J.J. Heitholt Cotton boll retention and its relationship to lint yield
    Crop Sci., 33 (1993), pp. 486-490 Google Scholar [9] S.K. Himanshu, S. Ale, J.P.
    Bordovsky, J. Kim, S. Samanta, N. Omani, E.M. Barnes Assessing the impacts of
    irrigation termination periods on cotton productivity under strategic deficit
    irrigation regimes Sci. Rep., 11 (2021), pp. 1-16 Google Scholar [10] S.K. Himanshu,
    Y. Fan, S. Ale, J. Bordovsky Simulated efficient growth-stage-based deficit irrigation
    strategies for maximizing cotton yield, crop water productivity and net returns
    Agric. Water Manag., 250 (2021), Article 106840 View PDFView articleView in ScopusGoogle
    Scholar [11] J. Jung, M. Maeda, A. Chang, J. Landivar, J. Yeom, J. McGinty Unmanned
    aerial system assisted framework for the selection of high yielding cotton genotypes
    Comput. Electron. Agric., 152 (2018), pp. 74-81 View PDFView articleView in ScopusGoogle
    Scholar [12] F. Kurtulmus, W.S. Lee, A. Vardar Immature peach detection in colour
    images acquired in natural illumination conditions using statistical classifiers
    and neural network Precis. Agric., 15 (2014), pp. 57-79 CrossRefView in ScopusGoogle
    Scholar [13] D. Li, Y. Miao, C.J. Ransom, G.M. Bean, N.R. Kitchen, F.G. Fernández,
    J.E. Sawyer, J.J. Camberato, P.R. Carter, R.B. Ferguson Corn nitrogen nutrition
    index prediction improved by integrating genetic, environmental, and management
    factors with active canopy sensing using machine learning Remote Sens., 14 (2022),
    p. 394 CrossRefView in ScopusGoogle Scholar [14] Y. Li, Z. Cao, H. Lu, Y. Xiao,
    Y. Zhu, A.B. Cremers In-field cotton detection via region-based semantic image
    segmentation Comput. Electron. Agric., 127 (2016), pp. 475-486 View PDFView articleCrossRefGoogle
    Scholar [15] G. Mountrakis, J. Im, C. Ogole Support vector machines in remote
    sensing: a review ISPRS J. Photogramm. Remote Sens., 66 (2011), pp. 247-259 View
    PDFView articleView in ScopusGoogle Scholar [16] N. Otsu A threshold selection
    method from gray-level histograms IEEE Trans. Syst. Man Cybern., 9 (1979), pp.
    62-66 CrossRefGoogle Scholar [17] L. Prado Osco, A.P. Marques Ramos, D. Roberto
    Pereira, É. Akemi Saito Moriya, N. Nobuhiro Imai, E. Takashi Matsubara, N. Estrabis,
    M. de Souza, J. Marcato Junior, W.N. Gonçalves Predicting canopy nitrogen content
    in citrus-trees using random forest algorithm associated to spectral vegetation
    indices from UAV-imagery Remote Sens., 11 (2019), p. 2925 CrossRefGoogle Scholar
    [18] G. Rouze, H. Neely, C. Morgan, W. Kustas, M. Wiethorn Evaluating unoccupied
    aerial systems (UAS) imagery as an alternative tool towards cotton-based management
    zones Precis. Agric., 22 (2021), pp. 1861-1889 CrossRefView in ScopusGoogle Scholar
    [19] S. Sankaran, L.R. Khot, A.H. Carter Field-based crop phenotyping: multispectral
    aerial imaging for evaluation of winter wheat emergence and spring stand Comput.
    Electron. Agric., 118 (2015), pp. 372-379 View PDFView articleView in ScopusGoogle
    Scholar [20] Y. Shi, J.A. Thomasson, S.C. Murray, N.A. Pugh, W.L. Rooney, S. Shafian,
    N. Rajan, G. Rouze, C.L. Morgan, H.L. Neely Unmanned aerial vehicles for high-throughput
    phenotyping and agronomic research PLoS ONE, 11 (2016), Article e0159781 CrossRefView
    in ScopusGoogle Scholar [21] A.J. Smola, B. Schölkopf A tutorial on support vector
    regression Stat. Comput., 14 (2004), pp. 199-222 View in ScopusGoogle Scholar
    [22] J.V. Stafford Implementing precision agriculture in the 21st century J. Agric.
    Eng. Res., 76 (2000), pp. 267-275 View PDFView articleView in ScopusGoogle Scholar
    [23] S. Sun, C. Li, A.H. Paterson, P.W. Chee, J.S. Robertson Image processing
    algorithms for infield single cotton boll counting and yield prediction Comput.
    Electron. Agric., 166 (2019), Article 104976 View PDFView articleView in ScopusGoogle
    Scholar [24] Y. Tian, Y.P. Xu, G. Wang Agricultural drought prediction using climate
    indices based on support vector regression in Xiangjiang River basin Sci. Total
    Environ., 622 (2018), pp. 710-720 View PDFView articleView in ScopusGoogle Scholar
    [25] Wells, R., Meredith, W., 1984. Comparative growth of obsolete and modern
    cultivars. II. Reproductive dry matter partitioning nal References. Google Scholar
    [26] K. Yamamoto, W. Guo, Y. Yoshioka, S. Ninomiya On plant detection of intact
    tomato fruits using image analysis and machine learning methods Sensors, 14 (2014),
    pp. 12191-12206 CrossRefView in ScopusGoogle Scholar [27] J. Yeom, J. Jung, A.
    Chang, M. Maeda, J. Landivar Automated open cotton boll detection for yield estimation
    using unmanned aircraft vehicle (UAV) data Remote Sens., 10 (2018), p. 1895 CrossRefView
    in ScopusGoogle Scholar [28] H. Zha, Y. Miao, T. Wang, Y. Li, J. Zhang, W. Sun,
    Z. Feng, K. Kusnierek Improving unmanned aerial vehicle remote sensing-based rice
    nitrogen nutrition index prediction with machine learning Remote Sens., 12 (2020),
    p. 215 CrossRefView in ScopusGoogle Scholar [29] C. Zhang, J.M. Kovacs The application
    of small unmanned aerial systems for precision agriculture: a review Precis. Agric.,
    13 (2012), pp. 693-712 CrossRefView in ScopusGoogle Scholar Cited by (6) Fast
    and Precise Detection of Dense Soybean Seedlings Images Based on Airborne Edge
    Device 2024, Agriculture (Switzerland) Multispectral image processing using ML
    based classification approaches in satellite images 2024, Artificial Intelligence,
    Blockchain, Computing and Security - Proceedings of the International Conference
    on Artificial Intelligence, Blockchain, Computing and Security, ICABCS 2023 YOLO-C:
    An Efficient and Robust Detection Algorithm for Mature Long Staple Cotton Targets
    with High-Resolution RGB Images 2023, Agronomy Support Vector Machine Chains with
    a Novel Tournament Voting 2023, Electronics (Switzerland) Evaluation of Field
    Germination of Soybean Breeding Crops Using Multispectral Data from UAV 2023,
    Agronomy Multispectral image processing using ML based classification approaches
    in satellite images 2023, Artificial Intelligence, Blockchain, Computing and Security:
    Volume 1 © 2022 The Author(s). Published by Elsevier B.V. Recommended articles
    A preliminary investigation into the automatic detection of diseased sheep organs
    using hyperspectral imaging sensors Smart Agricultural Technology, Volume 3, 2023,
    Article 100122 Cassius E.O. Coombs, …, Luciano A. González View PDF An adaptive
    grain-bulk aeration system for squat silos in winter: Effects on intergranular
    air properties and grain quality Smart Agricultural Technology, Volume 3, 2023,
    Article 100121 Guangbiao Gao, …, Xiaofeng Zhang View PDF Development of a method
    to assess the risk of aflatoxin contamination of corn within counties in southern
    Georgia, USA using remotely sensed data Smart Agricultural Technology, Volume
    3, 2023, Article 100124 R. Kerry, …, B. Scully View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 4 Captures Readers: 22 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Smart Agricultural Technology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A support vector machine and image processing based approach for counting
    open cotton bolls and estimating lint yield from UAV imagery
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Tschand A.
  citation_count: '5'
  description: Agricultural irrigation wastage, especially from inefficient infrastructure
    in developing nations, is among the leading causes of the global water shortage
    epidemic, which affects an estimated 2.8 billion people worldwide. In this project,
    a three-component cloudless internet-of-things (IoT)-based irrigation network
    is designed and constructed to affordably integrate into existing irrigation infrastructure
    and approach zero water wastage while maintaining crops at a perceived healthy
    threshold. A Raspberry Pi-based electrical circuit of sensors and drone are constructed
    to collect environmental data and extract brightness-adjusted HSV crop color using
    k-means clustering-based computer vision algorithms. A semi-supervised approach
    is used to train a bidirectional Recurrent Neural Network, which enables long-term
    and working memory analysis within hidden layers for time-sensitive outputs. The
    resulting predicted irrigation volume is executed by an Arduino Nano-based irrigation
    micropiece to dynamically adjust the water flow to each crop for individualized
    irrigation. Induction from water flow through irrigation pipes generates current,
    yielding complete and scalable power self-sufficiency at an excess margin of 25.8%.
    Empirical data collection reflects a 68.8% increase in irrigation effectiveness
    for crop growth. Model testing on various African nation crop yields and corresponding
    satellite images (n = 10,301) reflects an accuracy rate of 89.1% and a binary
    classification success of 96.0% after a single growth cycle. The ability to cost-effectively
    integrate into existing irrigation infrastructure and minimize irrigation resource
    wastage enables the constructed network to be a viable solution to mitigate the
    negative impacts of water scarcity on agriculture yields.
  doi: 10.1016/j.atech.2022.100116
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Keywords 3. Materials
    and methods 4. Results 5. Discussion of results and implications Declaration of
    Competing Interests Data availability References Show full outline Figures (17)
    Show 11 more figures Tables (1) Table 1 Smart Agricultural Technology Volume 3,
    February 2023, 100116 Semi-supervised machine learning analysis of crop color
    for autonomous irrigation Author links open overlay panel Arya Tschand Show more
    Add to Mendeley Share Cite https://doi.org/10.1016/j.atech.2022.100116 Get rights
    and content Under a Creative Commons license open access Highlights • The proposed
    irrigation network is designed and constructed to affordably integrate into existing
    irrigation infrastructure and approach zero water wastage while maintaining crops
    at a perceived healthy threshold. • A semi-supervised bidirectional recurrent
    neural network analyzes crop images using k-means clustering-based computer vision
    algorithms to yield an ideal predicted irrigation volume. • Model testing on various
    African nation crop yields and corresponding satellite images (n = 10,301) reflects
    an accuracy rate of 89.1% and a binary classification success of 96.0% after a
    single growth cycle. • The ability to cost-effectively integrate into existing
    irrigation infrastructure and minimize irrigation resource wastage enables the
    constructed network to be a viable solution to mitigate the negative impacts of
    water scarcity on agriculture yields. Abstract Agricultural irrigation wastage,
    especially from inefficient infrastructure in developing nations, is among the
    leading causes of the global water shortage epidemic, which affects an estimated
    2.8 billion people worldwide. In this project, a three-component cloudless internet-of-things
    (IoT)-based irrigation network is designed and constructed to affordably integrate
    into existing irrigation infrastructure and approach zero water wastage while
    maintaining crops at a perceived healthy threshold. A Raspberry Pi-based electrical
    circuit of sensors and drone are constructed to collect environmental data and
    extract brightness-adjusted HSV crop color using k-means clustering-based computer
    vision algorithms. A semi-supervised approach is used to train a bidirectional
    Recurrent Neural Network, which enables long-term and working memory analysis
    within hidden layers for time-sensitive outputs. The resulting predicted irrigation
    volume is executed by an Arduino Nano-based irrigation micropiece to dynamically
    adjust the water flow to each crop for individualized irrigation. Induction from
    water flow through irrigation pipes generates current, yielding complete and scalable
    power self-sufficiency at an excess margin of 25.8%. Empirical data collection
    reflects a 68.8% increase in irrigation effectiveness for crop growth. Model testing
    on various African nation crop yields and corresponding satellite images (n = 10,301)
    reflects an accuracy rate of 89.1% and a binary classification success of 96.0%
    after a single growth cycle. The ability to cost-effectively integrate into existing
    irrigation infrastructure and minimize irrigation resource wastage enables the
    constructed network to be a viable solution to mitigate the negative impacts of
    water scarcity on agriculture yields. Previous article in issue Next article in
    issue Keywords Machine learningInternet of thingsPrecision irrigationRecurrent
    neural network 1. Introduction 1.1. Rationale There is no necessity more valuable
    than water. However, as the global water crisis continues to spread from American
    towns like Flint, Michigan to small towns in Uganda to booming Asian cities like
    Chennai, India [1], it is increasingly evident that without a solution in sight,
    millions more across the world will lack access to clean drinking water. However,
    it is statistically apparent that the most significant water shortages are experienced
    in some of the most vulnerable developing nations where agriculture yield shortages
    can significantly affect the large lower class [2]. An ideal solution to this
    problem lies in agricultural irrigation, the single largest consumer and cause
    of water wastage across the world. The United Nations estimates that over 70%
    of global water consumption is used for agriculture. Of this majority, 60% is
    wasted through over-watering or leakages [3,4]. By combining the boundless capabilities
    of artificial intelligence with the rapidly innovating irrigation methods, a potential
    solution to affordably mitigate the global water crisis can be developed. In some
    areas, irrigation networks are composed of sets of water-carrying tubes and pipes
    that line the area to be irrigated [5]. The only control a farmer has over the
    volume of water given to each crop is by controlling the overall flow rate through
    the pipes, providing absolutely no capabilities to individually adjust each crop''s
    irrigation volume. Because there is a very limited intelligent analysis of each
    crop, vast amounts of wastage from over-watering can be rectified with affordable
    and efficient irrigation equipment [6]. As is well known and understood, there
    are varying levels of complexity tied to each form of precision irrigation to
    discern between intra-field variability and adjust irrigation, pesticide placement,
    and fertilizer use accordingly. Newer systems implement various forms of artificial
    intelligence (AI) to draw trends from data and maximize efficiency, either with
    crop yield or resource management [7,8]. Neural networks, which have gained popularity
    for their aptitude in identifying inter-variable correlations, combine multiple
    AI algorithms into individual layers and continually update cell states to improve
    outputted prediction accuracy. Utilizing a long short-term memory recurrent neural
    network enables analysis of memory within hidden layers for time-sensitive outputs.
    When created and trained successfully, it can make relevant predictions and improve
    overall system efficiency [9]. Compared to commonly utilized metrics such as temperature,
    pH, light, humidity, etc., color data provides direct insight into crop health
    and can therefore predict a crops’ water and nutrient needs with improved accuracy.
    However, there are limitations to color data collection, primarily its high cost
    that is only beginning to receive low-cost innovation [10]. Of the small percentage
    that utilizes color-input precision irrigation networks, it is primarily used
    for a benchmark value for crop irrigation with few to no changes throughout their
    growth. Because of the lack of color data usage in high volatility irrigation
    networks, artificial intelligence algorithms designed to utilize crop color to
    maximize the accuracy of resource use changes are non-existent. While the color
    data utilized is often remarkably accurate, unique changes to each crop are necessary
    to maximize crop yield and quality while minimizing resource wastage. However,
    even high-cost current precision irrigation networks utilizing crop color as their
    primary source of data cannot offer these micro-analysis capabilities of crop
    health changes. 2. Keywords 1 Precision Irrigation 2 Internet of Things 3 Recurrent
    Neural Network 3. Materials and methods 3.1. System architecture The proposed
    irrigation system was designed, constructed, and tested as a practical and implementable
    irrigation network. The constructed autonomous irrigation network comprises four
    independent components shown in Fig. 1: Irrigation Micropiece, Field Analysis
    Drone, and Data Analysis Server. A user facing iOS app was also created for feedback
    and user input, but it is not integral to overall functionality. Download : Download
    high-res image (97KB) Download : Download full-size image Fig. 1. System Architecture
    Diagram. Most developing nations lack access to widespread and reliable LTE or
    internet connection. Therefore, all data transfer is made through Local Area Network
    (LAN) internal request communication [11], which sends required requests through
    a central router and via the internet protocol (IP) address of the desired recipient.
    All requests of RNN analysis, database search queries, and image encodings are
    made via representational state transfer (REST) application programming interface
    (API) requests containing specific parameters in the required headers over the
    LAN towards all components. This allows near-instantaneous communication with
    high data capacity without internet access or cellular service. 3.2. RFID relative
    location usage Location is a requirement for the irrigation network. The data
    collection drone must know its location relative to the field to tie the collected
    data points to a region of crops and corresponding irrigation micropieces. However,
    in developing nations, GPS communication is not reliable and accurate enough to
    provide the necessary location services. Instead, RFID (Radio Frequency Identification)
    is used as an alternative to provide relative location. Ultra-high frequency waves
    are emitted by the drone and received by the RFID tag in each micropiece, returning
    its unique ID [12]. The length between wave emission and response can be corresponded
    to the distance between the drone and micropiece at relatively high accuracy.
    Furthermore, if needed, the unique ID cross-referenced with the server database
    to correspond to an absolute location. Another advantage of RFID usage is that
    the tags in each irrigation micropiece do not consume power when returning their
    ID, contributing to its individual power self-sufficiency. 3.3. Recurrent neural
    network overview The success of the statistical machine learning model was indicative
    of the potential success of a neural network, which has capabilities to recognize
    fine-grained trends. A semi-supervised recurrent neural network (RNN) component
    of the irrigation network allows the model to identify correlations between data
    points and resulting crop health without requiring extensive data sets to provide
    predictions for optimal irrigation volumes [13]. A many-to-one approach was utilized
    in the model with a sequence of data points inputted for a single output. Long
    Short-Term Memory (LSTM) units were found to have better accuracy, and its bidirectional
    analysis of past and present iterations allows better context analysis of the
    data [14]. At a high level (server usage stage), when a new data point is collected
    and inputted into the RNN, an ideal PIV is calculated based on previous data and
    iteration output recency. The yielded volume is optimized to maintain the crop
    at the minimum healthy threshold while utilizing the minimal water volume for
    irrigation. The data from the initial data collection mechanism (n = 2760) was
    analyzed to find the unique crop response to water volumes and a subjective minimum
    healthy threshold. As the data set grows and more iterations are performed, perceived
    trends and predictions become more accurate, resulting in irrigation volumes closer
    to zero error. 3.4. RNN construction and usage The RNN shown in Fig. 2 was completely
    programmed with Python 3 using the Google TensorFlow Python Machine Learning library
    as a fundamental model [15]. An authenticated MySQL database was utilized to store
    crop metrics and efficiently query data using Structure Query Language (SQL) [16].
    Mathematical formulas, computer science algorithms, and public libraries were
    used in each layer to yield the desired outcome. Download : Download high-res
    image (181KB) Download : Download full-size image Fig. 2. LSTM RNN Structure Diagram
    With Function Labels. 3.5. Input layer The input layer of each iteration of the
    RNN consists of 3 sets of data [17]. The first set of data was collected from
    the drone and contains the most recent crop image (average crop color is derived
    from the image using k-means clustering algorithms explained later), temperature,
    and photoresistance/light. The second set of data was the old cell states passed
    from previous iterations of the network. The last set of data was the previous
    outputs of each iteration of the RNN, utilizing the data set queried from the
    MySQL shell to efficiently collect large quantities of previous data inputs and
    results stored locally on the central server. 3.6. Hidden layer 1 - forget gate
    layer The first layer of the LSTM unit of the RNN shown in Equation 1 decides
    which collected crop information received from the input layer will be used towards
    updating the cell state and output. It passes the previous outputs of PIV and
    new data points through a sigmoid function that outputs a value on the interval
    [0, 1], which represents the percentage of the data that should be kept and which
    will be thrown away [18]. Eq. 1: Forget Gate Sigmoid Algorithm Based on the sigmoid
    function utilizing a weight coefficient (Wf) of the crop information points, previous
    inputs and results from past iterations (ht-1), the new input data points (xt),
    and a defined constant (bf), the percentage of information to throw away (ft)
    is defined from 0 to 1 and carried out in this layer. 3.7. Hidden layer 2 - cell
    state storage layer In this layer, shown in Equation 2, the information to be
    stored in the cell state is calculated. One step of this layer is calculating
    the magnitude of the values (it) that will be updated later. The same variables
    from the previous layer are utilized, but the weight coefficient, representing
    the correlation between the data points and future success, and constant are different
    values. Because this step requires an outputted percentage, a sigmoid function
    is used to decide which values will be updated and remain constant. Eq. 2: Update
    Percentage Sigmoid Algorithm The other step of this layer, shown in Equation 3,
    utilizes a hyperbolic tangent function to create new candidate values (Čt) that
    can be used to update the cell state for irrigation volume predictions. Again,
    the same variables from the previous layer are utilized, but the weight coefficient
    and constant are values unique to this step in the layer. This step does not consider
    the resulting value from the previous step, so every possible value will be created.
    However, these vector magnitudes will be changed in future layers. Eq. 3: Candidate
    Value Vector Hyperbolic Tangent Algorithm 3.8. Hidden layer 3 - cell state update
    layer In this layer, shown in Equation 4, the old cell state is updated with new
    values. The cell state is a key value when calculating the irrigation volume output
    of the network, so it is updated with each iteration to ensure that the most accurate
    output based on previous data and trends is calculated to minimize water wastage.
    The values from the previous layers and past cell states are utilized to update
    the new cell state values. Eq. 4: Cell State Update Algorithm The magnitude value
    calculated from the first step of the previous layer (it) is multiplied by the
    candidate values (Čt) to create final state values. These values are used to manipulate
    the old cell state (Ct-1) multiplied by the forget gate layer value (ft) into
    the new cell state (Ct). 3.9. Hidden layer 4 - output calculation layer The network
    iteration output shown in Equation 5 will be based on a filtered version of the
    current cell state. Based on a sigmoid function utilizing a weight coefficient
    (Wo), previous inputs and results (ht-1), new input data points (xt), and a defined
    constant (bo), the parts of the cell state to be outputted as the final PIV. Eq.
    5: Output Percentage Sigmoid Hyperbolic Tangent Algorithm Next, the current cell
    state value is passed through a hyperbolic tangent function outputting a value
    on the interval [−1, 1]. However, because only a portion of this output is desired,
    this value is multiplied by the sigmoid value from the previous step to calculate
    the final network iteration output (ht), shown in Equation 6.). Eq. 6: Output
    Calculation Algorithm This output represents the ideal PIV to maximize crop color
    interpreted within a healthy interval while minimizing irrigation wastage. While
    this value is inaccurate at the beginning of a growth cycle, it will reach lesser
    error in crop health and irrigation wastage as trends are clarified. The output
    is sent back to the data collection drone to instruct micropiece adjustments for
    the irrigation of a small area of crops. 3.10. Computer vision image analysis
    There is an intermediate step between receiving the drone data and passing it
    through the recurrent neural network for data analysis. Data is passed from the
    drone to the server as a local IP API call contains a base64 encoded image parameter
    string. This image has no quantitative significance without computer vision analysis
    to derive a numerical average crop color, which is interpreted as the relative
    crop health. To accomplish this, k-means clustering algorithms are utilized to
    extrapolate significant inter-pixel disparities and isolate color clusters within
    the image parameter [19,20]. When the algorithms are run on an image, it isolates
    the specified number of significant color clusters within the image that are matched
    with the HSV ranges for each color. Because it was expected that there is significant
    noise within the image that is not wanted in average color analysis, only clusters
    that conform to the general crop color range are collected and averaged by a percentage
    of pixels within the image. After isolating the aggregate desired color, it is
    adjusted to conform to a standard brightness for consistent color analysis. This
    process is shown in Fig. 3. Download : Download high-res image (155KB) Download
    : Download full-size image Fig. 3. Crop Image Computer Vision Analysis Flow. 3.11.
    Data collection drone overview A data collection drone, shown in Fig. 4 and Fig.
    5, serves as the primary data collection component of the irrigation network.
    Its primary purpose is to capture data points (crop color, temperature, photoresistance)
    of crops in the desired area. The drone sends base64 encoded images from an attached
    webcam to the server for computer vision analysis to find the HSV crop color.
    API communication via LAN is used to send collected environment data and micro-irrigation
    instructions between the irrigation network components. These irrigation instructions
    contain the PIV for each irrigation micropiece to adjust its orientation accordingly.
    Download : Download high-res image (633KB) Download : Download full-size image
    Figs. 4 and 5. Final DJI Spark Drone With Constructed Circuit, Side and Top View.
    3.12. Drone quadcopter framework To ensure the drone fulfills its desired purpose
    as part of the irrigation network and remains affordable, it was constructed from
    scratch, shown in Fig. 6, with all parts purchased separately. The parts were
    attached, screwed, soldered, etc. together by the researcher to fit its desired
    function. The brushless motor controllers were soldered onto a central microcontroller
    allowing for full motion control. This microcontroller was then attached to the
    main Raspberry Pi as a communication medium with the server for wireless instructions.
    Download : Download high-res image (124KB) Download : Download full-size image
    Fig. 6. Drone Framework Construction. 3.13. Drone circuit wiring and construction
    A Raspberry Pi 4 with 4GB RAM, WiFi connectivity, and the Linux-based Raspian
    operating system loaded onto the inserted SD card is used to control DJI Spark
    drone motion. Out of the 40 total GPIO (ground, power, input, output) pins for
    hardware control, 22 were used for various purposes. All components are connected
    to a unique GPIO pin for data collection and ground or power pins as needed. For
    power-intensive components such as the brushless motors via the microcontroller
    and liquid-crystal display, 5 V output power is supplied. For other components,
    variable power is outputted. Specific sensors are wired to various pins on the
    Raspberry Pi, shown in Fig. 7, to collect individual data points, which are collected
    and parsed in the executed code written in Python 3 before being sent to the server
    as an API call with encoded headers. A DHT11 temperature/humidity module is used
    to collect environmental temperature data. A photoresistor and 1μF capacitor is
    used to measure photoresistance. While this evidently changes by time of day,
    it is capable of informing the network of factors such as cloudiness and potential
    rain. Additional data points may be collected by the drone and processed by RNN,
    as the network is designed to quantify the magnitude of the correlation between
    infinite independent variables. Download : Download high-res image (279KB) Download
    : Download full-size image Fig. 7. Wiring Diagram of Raspberry Pi Based Circuit.
    The MFRC522 RFID reader is located on the drone''s edge and creates a listener
    for unique tags by continually sending ultra-high frequency waves and waiting
    for responses. The collected RFID tag identifier and distance are cross-referenced
    with the server through an API request to return the location and crop identifier.
    Based on this information, each captured image, photoresistance, and humidity
    can be easily linked to the crop for precise data analysis by the RNN and instructions
    can be sent to the correct micropiece with irrigation instructions. Through multi-thread
    Python implementation, analysis of RFID tags can be sped up significantly by collecting
    and cross-referencing multiple identifiers at one time [21]. 3.14. Dynamic irrigation
    micro piece overview The dynamic irrigation micro piece, shown in Fig. 8, is utilized
    to control the micro-irrigation volume given to a small crop area [22]. Each micro
    piece is designed to integrate into the irrigation pipes by drawing water out
    of a shaft injected into the tube, allowing the previous irrigation infrastructure
    to be utilized as the primary means of water transport. A turbine is also drilled
    inside the pipe to draw power from the flowing current, providing a sustainable
    power source that does not require batteries. Each piece receives commands from
    the drone detailing the required servo motor position to irrigate each crop the
    desired volume. Because each micro piece serves as an individual entity and generates
    more power from the turbine than it consumes for communication and motion, the
    irrigation network is entirely scalable for implementation. Download : Download
    high-res image (374KB) Download : Download full-size image Fig. 8. Final Constructed
    Micropiece With Circuit, 3D Shell, and Turbine. 3.15. Calculations for EMF of
    turbine from axial flux The turbine''s EMF (electromotive force) is generated
    by the rotation of the coils of metal from the constant water current through
    the irrigation pipes. Axial magnetic flux turbines, which have shorter flux paths
    with the permanent magnets farther away than radial flux turbines, allow for greater
    electricity generation efficiency in this specific instance. To calculate the
    power of the turbine, the EMF was calculated and converted to watts [23]. When
    performing the calculations, the collected values from part specifications are
    as follows: number of turns per turbine coil = 300/coil, number of turbine coils = 9
    coils, B field = 0.2 T, turbine radius = 1.2 cm, and average water flow rate through
    pipe = 5 ft/s ≈ 1.52 m/s. The EMF formula (ε) was derived by plugging in the B-field
    magnetic flux formula (ø) into the base EMF formula shown in Equation 7. Eq. 7:
    Derivation of EMF Formula Based on the turbine radius, basic circumference and
    area of circle calculations are performed to output 0.07539 m as the turbine''s
    circumference and 4.523 × 10−4 m2 as the area of the turbine. Water flow rate
    per circumference was used to calculate a turbine rotation of 20.21 rotations
    per second. Eq. 8: Calculate EMF Induced by Turbine Finally, the EMF of the turbine
    was calculated by plugging in the collected values and the calculated turbine
    rotations per second into the derived EMF formula (ε) These calculations are shown
    in Equation 8, [24]. Evaluation yields a value of 31.014 V, which represents the
    voltage generation of the turbine. However, for further analysis of turbine usage,
    it must be converted to Watts (power). To do so, the current through the turbine
    was measured with a multimeter, yielding an amperage of 220 mA. The fundamental
    power formula with voltage and amperage plugged in yields a value of 6.82 W of
    power generated by the turbine. 3.16. Power consumption of circuit components
    For the micropiece to be power self-sufficient, the generated power must be greater
    than the components’ power consumption at any time. Because there is no battery
    in the micropiece to store electricity because of its relatively high cost and
    tendency to break quickly, the generated power must always be enough to power
    micropiece functions. However, the functions performed by the micropiece, specifically
    LAN communication with the drone and servo rotation to adjust the water flow rate
    through the shaft, are systematically spread out to limit a spike in power consumption.
    The ESP8226 WiFi module requires 3.7 V and 440 mA, which equates to 1.63 W of
    power consumption. The micro DC stepper motor and potentiometer within the servo
    motor require 5 V and 550 mA, which equates to 2.75 W of power consumption. Adding
    these values up to 4.38 total Watts of power consumption at any point in time,
    which is less than the 6.82 Watts of power generation, indicates complete power
    self-sufficiency for each irrigation micropiece at any point in time. 3.17. Graphical
    depiction of power generation and consumption A multivariable graphical depiction
    of power generation and consumption was created to visualize the gap allowing
    for power self-sufficiency [25]. The 3D graph shows the relationship between 2
    independent variables and the resulting dependent variable from the EMF function.
    Isolating water flow rate as an independent variable on the x-axis, amperage of
    current as an independent variable on the y-axis, and power wattage on the z-axis
    creates the curves in Fig. 9. Download : Download high-res image (177KB) Download
    : Download full-size image Fig. 9. Multivariable Graph of Power Generation/Consumption.
    The intersection between red plane x = 1.52 m/s, teal plane y = 220 mA, and blue
    power generation graph represents the average power induced by the axial magnetic
    flux. The linear distance from this point of intersection to green plane z = 4.38
    W represents the power consumed by the micropiece. At these specifications, there
    is a 25.8% excess margin in power production versus power consumption. 3.18. Micropiece
    circuit wiring and construction An Arduino Nano circuit, shown in Fig. 10, is
    used to control the communication and motion of the irrigation micropiece. The
    ESP8226 WiFi Module is used for API requests to the drone and vice versa for data
    transfer. The servo motor rotates a 3D printed piece within the shaft to control
    the water flow rate towards the plants and will be described in further detail.
    Both circuit components are connected to respective control pins, grounded, and
    given variable voltage power. The turbine is connected to VIN input to provide
    power. Download : Download high-res image (244KB) Download : Download full-size
    image Fig. 10. Wiring Diagram of The Arduino Nano Based Circuit. A 3D printed
    shell is used to protect the components and prevent water entry into the electrical
    components for short circuits. The shell consists of a container for the electrical
    components and Arduino Nano and a cylindrical shaft with a hole through the shaft.
    The shaft is designed to be punctured into the water-carrying pipes to allow water
    through. The rotational part of the servo motor is inserted into the shaft through
    a hole and contains a semi-cylinder 3D printed attachment. 180° motion of the
    servo can adjust the flow rate between 0% (no flow) and 50% (maximum) of the potential
    irrigation volume. The motor is adjusted multiple times daily to adjust irrigation
    volume based on RNN instructions. 4. Results 4.1. Irrigation system data testing
    The initial data collection mechanism success in irrigation efficiency validates
    increasing efficiency from artificial intelligence implementation into micro-irrigation
    networks. Next, the irrigation network is designed to not only improve irrigation
    efficiency through RNN training, but also contain all required components to affordably
    integrate into existing irrigation infrastructure in developing nation farms.
    Through future collaboration with commercial companies, the irrigation network
    will soon be tested for accuracy in real-world implementation. More specifically,
    it will incorporate the proposed machine learning algorithms into current artificial
    intelligence models for crop analysis and parts of the physical system into existing
    infrastructure. However, before full implementation can be rolled out, the irrigation
    system RNN must be tested with real data for theoretical analysis of efficiency
    and accuracy. An 18 year, 393 site data set from the NASA PhenoCam Vegetation
    Phenology Imaging data set [26] was utilized to provide crop images with RGB analysis
    for South and Central Africa countries (47 year span, n = 10,301). Over the course
    of the collected time period, the studied geographical regions experienced drought,
    heavy winds, light and heavy precipitation, and other impactful weather conditions
    that had a significant effect on crop growth. Weather records from the National
    Centers for Environmental Information [27], including rainfall for drought/irrigation
    surplus, were programmatically related to each crop image to complete the desired
    input set. This additional climactic condition data ensures that the model is
    tested on practical crop growth conditions. The full input data set was cross-referenced
    with crop production data sets from the Food and Agriculture Organization of the
    United Nations [28] to provide a dependent variable against the inputted data
    to quantify model accuracy. 4.2. Irrigation system analysis results and metrics
    Accuracy is used as the primary metric when measuring the success of the model
    predictions. With each epoch iteration representing a new data point input and
    irrigation volume prediction output, accuracy was expected to increase. The absolute
    accuracy after a 200 epoch model evaluation was recorded as the overall accuracy
    of a full growth cycle. Epoch iterations are used as the independent variable
    over standard model iterations because of its ability to consider data batch size.
    Additional metrics include F1-Score and Receiver Operator Characteristic Area
    Under Curve (ROC AUC). Because both scores measure binary classification model
    success, a 5% margin or error in an irrigation adjustment volume versus ideal
    irrigation volume was considered a correct prediction. The F1-Score provides insight
    into the model''s ability to avoid false predictions in imbalanced data sets [29].
    The ROC AUC measures the integral of a true positive against false positive graph
    where a value approaching 1 represents maximum accuracy. The model results are
    shown in Table 1 [30]. Table 1. Semi-Supervised Model Results. Country Crop Accuracy
    F1-Score ROC AUC Algeria Wheat 0.913750 0.915751 0.977943 Sudan Taro 0.832500
    0.832080 0.911414 Libya Maize 0.932000 0.933464 0.985911 Burkina Faso Cassava
    0.886667 0.882960 0.964046 Average 0.891230 0.891064 0.959829 The models achieved
    an accuracy of around 0.914 for Algeria Wheat, 0.833 for Sudan Taro, 0.932 for
    Libya Maize, and 0.887 for Burkina Faso Cassava. Sudan Taro had the lowest measured
    metrics, but this was likely because of the relatively low volume of the crop
    production compared to the data set size of other crops. The average accuracy
    of the collective models was around 0.891, which was relatively low because of
    the large variability of surrounding unmeasured environment conditions but can
    be raised through additional growth cycles and larger, more personalized data
    sets. Furthermore, the average ROC AUC of around 0.960 indicates that success
    was very high when affording the model a margin of error. 4.3. Model progression
    graphs Graphs depicting the progression of loss and accuracy after each epoch
    iteration were computer generated using the Python Matlab library for each country/crop
    combination. The loss graphs shown in Fig. 11, Fig. 13, Fig. 15, and 17 depict
    the reducing net error in predicted versus ideal irrigation volume. As expected,
    the test data set flattens to a slightly higher loss percentage because of relative
    overfitting to the training data. However, as the data set and total iterations
    grow, the test and train curve will merge as they approach 0. The accuracy graphs
    shown in Fig. 12, Fig. 14, Fig. 16 and 18 depict the growing accuracy in approaching
    the ideal irrigation volume. A primary reason why accuracy is an ideal metric
    for overall model success is the minimal impact of overfitting, indicated by the
    near-identical train versus test curves. However, it does not afford a margin
    of error, which may be misleading in the context of this project. Download : Download
    high-res image (134KB) Download : Download full-size image Fig. 11. Algeria Wheat
    Model Loss. Download : Download high-res image (151KB) Download : Download full-size
    image Fig. 12. Algeria Wheat Model Accuracy. Download : Download high-res image
    (153KB) Download : Download full-size image Fig. 13. Sudan Taro Model Loss. Download
    : Download high-res image (165KB) Download : Download full-size image Fig. 14.
    Sudan Taro Model Accuracy. Download : Download high-res image (142KB) Download
    : Download full-size image Fig. 15. Libya Maize Model Loss. Download : Download
    high-res image (155KB) Download : Download full-size image Fig. 16. Libya Maize
    Model Accuracy. Download : Download high-res image (144KB) Download : Download
    full-size image Fig. 17. Burkina Faso Cassava Model Loss. Download : Download
    high-res image (160KB) Download : Download full-size image Fig. 18. Burkina Faso
    Cassava Model Accuracy. 5. Discussion of results and implications The micropiece
    component of the irrigation network functions at complete power self-sufficiency
    at an excess wattage margin of 25.8%. Empirical data found that the autonomous
    irrigation network operates at a 68.8% increase in irrigation efficiency in organic
    crop mass per mL of water. Testing of the machine learning model on African nation
    crop yields predictions based on environmental data and satellite imagery analysis
    yielded an overall success rate of 89.1% and a binary classification success of
    96.0% after a single growth cycle. The Alternate Hypothesis (H1) was supported
    by all data. The implications of the performed research and constructed prototype
    in agriculture are significant. Demographics show that water shortages particularly
    affect warm third world countries where agricultural irrigation is most primitive.
    Because the constructed irrigation network is designed to integrate into existing
    irrigation infrastructure, implementing the technology is not limited to solely
    affluent areas anymore and can potentially solve the global water crisis at the
    root of its problems. While the constructed irrigation network is primarily designed
    to conserve water, it can also be used for controlling usage volumes of other
    agricultural substances such as fertilizers and pesticides [31]. A significant
    advantage of the system''s recurrent neural network is that there are no explicitly
    required parameters in the inputted data set. Because the neural network independently
    isolates the strength of correlation between any number of independent variables
    and the dependent variable of color, it is readily capable of predicting the usage
    of substances like fertilizer and pesticides along with irrigation volume. 5.1.
    Further study The current irrigation network captures and analyzes crop images
    on the visible light spectrum. However, capturing additional images outside of
    the visible light spectrum may offer additional accuracy in irrigation volume
    predictions. Research has shown that capturing crop images in the red-edge and
    near-infrared light region can be used to detect changes in the reflectance of
    vegetation and chlorophyll deficiencies in crops [32]. Further research to find
    a wavelength of light captured by affordable camera equipment may increase the
    network''s overall effectiveness while keeping cost increases to a minimum. Furthermore,
    because the speed of training and reduction in error is so heavily based on the
    data set, it would be extremely beneficial to share data sets between nearby farms.
    However, potential issues include data security and potential corruption of data.
    Storing the data in a blockchain network instead of a shared database allows the
    data to be publicly accessible without risking data corruption or abuse. This
    allows farms to easily share data to improve the learning rate without the detriments
    of public data. Another advantage of blockchain usage over a central database
    is its decentralized nature, removing the need for a centralized data storage
    server, instead of allowing each client to store an individual copy. Declaration
    of Competing Interests The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Data availability Data will be made available on
    request. References [1] Hannah. Dormido These countries are the most at risk from
    a water crisis Bloomberg.com, Bloomberg (6 Aug. 2019) Google Scholar [2] Kevin
    Watkins Human development report 2006 - beyond scarcity: power, poverty and the
    global water crisis (November 9, 2006) UNDP Hum. Dev. Rep. (2006) Available at
    SSRN https://ssrn.com/abstract=2294691 Google Scholar [3] Agriculture at a crossroads
    Glob. Agric. (2019) Google Scholar [4] M. Kummu, et al. Lost food, wasted resources:
    global food supply chain losses and their impacts on freshwater, cropland, and
    fertiliser use Sci. Total Environ., 438 (2012), pp. 477-489, 10.1016/j.scitotenv.2012.08.092
    27 Aug. View PDFView articleView in ScopusGoogle Scholar [5] Rajan K. Sampath
    Issues in irrigation pricing in developing countries World Dev., 20 (7) (July
    1992), pp. 967-977, 10.1016/0305-750x(92)90124-e View PDFView articleView in ScopusGoogle
    Scholar [6] H. Kadiveti et al., \"Water management through integrated technologies,
    a sustainable approach for village Pandori, India,\" 2019 IEEE R10 Humanitarian
    Technology Conference (R10-HTC)(47129), 2019, pp. 180–185, doi: 10.1109/R10-HTC47129.2019.9042469.
    Google Scholar [7] Tanha Talaviya, et al. Implementation of artificial intelligence
    in agriculture for optimisation of irrigation and application of pesticides and
    herbicides Artif. Intell. Agric., 4 (2020), pp. 58-73, 10.1016/j.aiia.2020.04.002
    View PDFView articleView in ScopusGoogle Scholar [8] Anil Chougule, Maithili,
    Anil S. Mashalkar A comprehensive review of agriculture irrigation using artificial
    intelligence for crop production Comput. Intell. Manuf. (2022), pp. 187-200, 10.1016/b978-0-323-91854-1.00002-9
    Google Scholar [9] Zaremba, Wojciech, Ilya Sutskever, and Oriol Vinyals. \"Recurrent
    neural network regularization.\" arXiv preprint arXiv:1409.2329 (2014). Google
    Scholar [10] Gonzalo Cucho-Padin, et al. Development of low-cost remote sensing
    tools and methods for supporting smallholder agriculture Appl. Geomat., 12 (3)
    (2020), pp. 247-263 CrossRefView in ScopusGoogle Scholar [11] Wei Zhou, et al.
    REST API design patterns for SDN northbound API 2014 28th International Conference
    on Advanced Information Networking and Applications Workshops, IEEE (2014) Google
    Scholar [12] Chenglong Li, et al. RePos: relative position estimation of UHF-RFID
    tags for item-level localization 2019 IEEE International Conference on RFID Technology
    and Applications (RFID-TA), IEEE (2019) Google Scholar [13] Nishant Shukla, Kenneth
    Fricklas Machine Learning With TensorFlow Greenwich, Manning (2018) Google Scholar
    [14] Alex. Sherstinsky Fundamentals of recurrent neural network (RNN) and long
    short-term memory (LSTM) network Physica D, 404 (2020), Article 132306 View PDFView
    articleView in ScopusGoogle Scholar [15] Jim Melton, Alan R. Simon Understanding
    the New SQL: a Complete Guide Morgan Kaufmann (1993) Google Scholar [16] Michiel
    Hermans, Benjamin Schrauwen Training and analysing deep recurrent neural networks
    Adv. Neural Inf. Process. Syst., 26 (2013) Google Scholar [17] Jun Han, Claudio
    Moraga The influence of the sigmoid function parameters on the speed of backpropagation
    learning Int. Workshop Artif. Neural Netw., Springer, Berlin, Heidelberg (1995)
    Google Scholar [18] Duc Truong Pham, Stefan S. Dimov, Chi D. Nguyen Selection
    of K in K-means clustering Proc. Inst. Mech. Eng. Part C J. Mech. Eng. Sci., 219
    (1) (2005), pp. 103-119 View in ScopusGoogle Scholar [19] Aristidis Likas, Nikos
    Vlassis, Jakob J. Verbeek The global k-means clustering algorithm Pattern Recognit.,
    36 (2) (2003), pp. 451-461 View PDFView articleView in ScopusGoogle Scholar [20]
    Lawrence Spracklen, Santosh G. Abraham Chip multithreading: opportunities and
    challenges 11th International symposium on high-performance computer architecture,
    IEEE (2005) Google Scholar [21] Goulden, C.H. “T-Test: definition and examples.”
    Statistics How To, 6 July 2020. Google Scholar [22] Brian W. Kernighan, Dennis
    M. Ritchie The C Programming Language Prentice Hall (2010) Google Scholar [23]
    13.7: electric generators and back Emf Phys. LibreTexts (18 Nov. 2019) Google
    Scholar [24] P.J. Scanlon, R.N. Henriksen, J.R. Allen Approaches to electromagnetic
    induction Am. J. Phys., 37 (7) (1969), pp. 698-708 CrossRefView in ScopusGoogle
    Scholar [25] Paul. Dawkins Functions of several variables Calculus III (2020)
    26 May Google Scholar [26] PhenoCam Dataset v2.0: vegetation Phenology from Digital
    Camera Imagery, 2000-2018 Distrib. Active Arch. Center Biogeochem. Dyn. (2018)
    Google Scholar [27] World Weather Records (WWR) clearinghouse Natl. Centers Environ.
    Inf. (2019) Google Scholar [28] Crop Statistics for 173 Products in Africa, the
    Americas, Asia, Europe, and Oceania Food and Agriculture Organization of the United
    Nations (2020) Google Scholar [29] Lipton, Zachary Chase, Charles Elkan, and Balakrishnan
    Narayanaswamy. \"Thresholding classifiers to maximize F1 score.\" arXiv preprint
    arXiv:1402.1892 (2014). Google Scholar [30] Sarang. Narkhede Understanding auc-roc
    curve Towards Data Sci., 26 (1) (2018), pp. 220-227 Google Scholar [31] David
    W. Franzen, Ted R. Peck Field soil sampling density for variable rate fertilization
    J. Prod. Agric., 8 (4) (1995), pp. 568-574 CrossRefGoogle Scholar [32] Jaime Honrado,
    E. Luis, et al. UAV imaging with low-cost multispectral imaging system for precision
    agriculture applications 2017 IEEE Global Humanitarian Technology Conference (GHTC),
    IEEE (2017) Google Scholar Cited by (0) © 2022 The Author(s). Published by Elsevier
    B.V. Recommended articles Comparing machine-learning models of different levels
    of complexity for crop protection: A look into the complexity-accuracy tradeoff
    Smart Agricultural Technology, Volume 7, 2024, Article 100380 Olivier Gauriau,
    …, François Joudelat View PDF Multiclass insect counting through deep learning-based
    density maps estimation Smart Agricultural Technology, Volume 3, 2023, Article
    100125 Arantza Bereciartua-Pérez, …, Till Eggers View PDF Optical and portable
    equipment for characterizing soil roughness Smart Agricultural Technology, Volume
    3, 2023, Article 100062 Bianca Batista Barreto, …, Blair McKenzie View PDF Show
    3 more articles Article Metrics Citations Citation Indexes: 5 Captures Readers:
    48 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Smart Agricultural Technology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Semi-supervised machine learning analysis of crop color for autonomous irrigation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Biradar V.S.
  - Al-Jiboory A.K.
  - Sahu G.
  - Tilak Babu S.B.G.
  - Mahender K.
  - Natrayan L.
  citation_count: '0'
  description: Intelligent control systems are a game-changer for robotics and industrial
    automation. This abstract explores how computer vision (CV) and artificial neural
    network (ANN) algorithms may be used to improve automation and robotics in terms
    of efficiency, accuracy, and flexibility. The goal of industrial automation is
    to streamline processes, decrease the need for human interaction, and increase
    output, all of which have progressed dramatically over the years. Integrating
    intelligent control systems that make use of computer vision and artificial neural
    networks is crucial to this transformation. The ability of these systems to detect
    and understand their environments is made possible by computer vision (CV). In
    order to make sound judgements, CV algorithms analyse data captured by cameras
    and other sensors. CV lets robots recognise things, navigate hazardous terrain,
    and carry out precise industrial tasks. CV has become an integral part of industrial
    automation, used for anything from monitoring production quality to navigating
    warehouses autonomously. Artificial neural networks (ANN s) mimic the human brain
    in many ways, including their ability to learn and make decisions on their own.
    ANNs are built from networks of nodes (neurons) that work together to analyse
    and process information. ANN s may learn to identify patterns, refine their control
    settings, and adjust to new circumstances. Predictive maintenance, problem identification,
    and control strategy optimisation are just some of the ways in which ANN s are
    put to use in industrial automation and robotics. Combining CV with ANN algorithms
    makes for a formidable tool with many practical uses in industry. The automated
    examination of produced goods is one significant use. Cracks and imperfections
    are easy targets for CVs, while ANNs can analyse the data in real time to make
    judgements about the product's quality. As a result, we can maintain constant
    quality control, cut down on waste, and boost output. Combining CV with ANN has
    been incredibly useful for robotics in industrial automation. Robots using CV
    systems can accurately pick up and place things from their surroundings without
    human intervention. By allowing robots to learn from their environments, ANNs
    increase their flexibility and usefulness in the workplace. The combination of
    CV with ANNs has improved the viability of 'cobots' in production, in which robots
    and humans work together in harmony. Autonomous navigation is another important
    field where CVs and ANNs excel. AGVs and drones need to be able to efficiently
    handle complicated layouts in large warehouses and factories. Using CV, these
    systems are better able to perceive and map their environments, while ANNs allow
    them to plan ideal courses, avoid obstacles, and adapt to a constantly shifting
    landscape. The advantages of combining CV with ANN go well beyond those of conventional
    industrial automation. For instance, these technologies are used for precision
    farming in the agriculture industry. Increased yields and efficient use of resources
    are the consequence of the combination of CV systems for identifying crop health
    and pest infestations and ANNs for making data-driven decisions regarding irrigation,
    fertiliser application, and harvesting. In conclusion, the advent of a new era
    of intelligent control systems has been heralded by the incorporation of computer
    vision and artificial neural networks into industrial automation and robotics.
    From autonomous navigation to precision agriculture, these technologies improve
    efficiency, accuracy, and flexibility in a variety of fields. The future of industrial
    automation and robotics will be significantly influenced by the complementary
    nature of CVs and ANNs.
  doi: 10.1109/UPCON59197.2023.10434927
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 10th IEEE Uttar Pradesh ...
    Intelligent Control Systems for Industrial Automation and Robotics Publisher:
    IEEE Cite This PDF Vijaykumar S. Biradar; Ali Khudhair Al-Jiboory; Gaurav Sahu;
    S. B G Tilak Babu; Kommabatla Mahender; Natrayan L All Authors 27 Full Text Views
    Abstract Document Sections I. Introduction II. Related Study III. Methodology
    IV. Results and Discussions V. Conclusion Show Full Outline Authors Figures References
    Keywords Metrics Abstract: Intelligent control systems are a game-changer for
    robotics and industrial automation. This abstract explores how computer vision
    (CV) and artificial neural network (ANN) algorithms may be used to improve automation
    and robotics in terms of efficiency, accuracy, and flexibility. The goal of industrial
    automation is to streamline processes, decrease the need for human interaction,
    and increase output, all of which have progressed dramatically over the years.
    Integrating intelligent control systems that make use of computer vision and artificial
    neural networks is crucial to this transformation. The ability of these systems
    to detect and understand their environments is made possible by computer vision
    (CV). In order to make sound judgements, CV algorithms analyse data captured by
    cameras and other sensors. CV lets robots recognise things, navigate hazardous
    terrain, and carry out precise industrial tasks. CV has become an integral part
    of industrial automation, used for anything from monitoring production quality
    to navigating warehouses autonomously. Artificial neural networks (ANN s) mimic
    the human brain in many ways, including their ability to learn and make decisions
    on their own. ANNs are built from networks of nodes (neurons) that work together
    to analyse and process information. ANN s may learn to identify patterns, refine
    their control settings, and adjust to new circumstances. Predictive maintenance,
    problem identification, and control strategy optimisation are just some of the
    ways in which ANN s are put to use in industrial automation and robotics. Combining
    CV with ANN algorithms makes for a formidable tool with many practical uses in
    industry. The automated examination of produced goods is one significant use.
    Cracks and imperfections are easy targets for CVs, while ANNs can analyse the
    data in real time to make judgements about the product''s quality. As a result,
    we can maintain constant quality control, cut down on waste, and boost output.
    Combining C... (Show More) Published in: 2023 10th IEEE Uttar Pradesh Section
    International Conference on Electrical, Electronics and Computer Engineering (UPCON)
    Date of Conference: 01-03 December 2023 Date Added to IEEE Xplore: 26 February
    2024 ISBN Information: ISSN Information: DOI: 10.1109/UPCON59197.2023.10434927
    Publisher: IEEE Conference Location: Gautam Buddha Nagar, India SECTION I. Introduction
    Incorporating cutting-edge technologies like CV and ANN has propelled the rapid
    development of industrial automation and robotics in recent years. Intelligent
    control systems made possible by these advancements have revolutionised the industrial
    sector, vastly improving productivity, accuracy, and malleability across a wide
    range of sectors. This introductory section gives a comprehensive look at the
    significance and effects of CV and ANN algorithms in industrial automation and
    robotics. [15] A. Manufacturing and Production Processes Manufacturing and production
    processes have relied heavily on industrial automation for quite some time. Simplifying
    processes, reducing the need for human involvement, and increasing output have
    long been primary objectives. Rule-based automation systems used to rely on these
    pre-programmed instructions to carry out mundane chores. However, robotics looked
    to provide mechanical aid in completing jobs that were either too risky or too
    precise for human beings to handle alone. However, there were restrictions with
    the conventional automation methods. They were not flexible and, hence, had difficulty
    in unpredictable situations. They weren''t designed to handle shifts in production
    or to make judgements on the fly based on sensory data. The combination of CV
    and ANN has proven to be revolutionary in this regard. B. Using CV to Its Full
    Potential The goal of computer vision, a branch of AI, is to teach computers to
    recognise objects and scenes in their environment. It entails the creation of
    algorithms and methods that enable computers to analyse visual data, draw conclusions,
    and act accordingly. CV systems in the realm of industrial automation and robotics
    rely on cameras and sensors to gather information about their surroundings and
    make decisions based on that information. CV has altered several facets of manufacturing.
    It allows for very precise flaw, anomaly, and quality issue detection using automated
    inspection systems in the industrial sector. Products, their assembly, and their
    conformity to quality requirements may all be evaluated with the use of such systems.
    This has resulted in less need for human inspection of products, which in turn
    has helped keep quality stable and minimised the possibility of human error. Robots
    can already recognise and control items in dynamic, unstructured settings, thanks
    in large part to CV''s function in object identification and tracking. When robots
    are required to navigate complicated layouts and interact with a wide variety
    of things, such a skill is beneficial in the fields of logistics, storage, and
    material handling. C. Artificial Neural Networks (anns) and Their Significance
    Inspired by the intricate neural architecture of the human brain, artificial neural
    networks are a type of machine learning method. Data processing and analysis are
    performed by layers of linked nodes (neurons). ANNs may be trained to learn from
    data, making them flexible and capable of recognising a wide variety of patterns.
    The ability to learn and make decisions is essential for intelligent control systems,
    and ANN s provide this capability in the context of industrial automation and
    robotics. ANNs can analyse sensor data to foresee when machinery may break down,
    making them useful for predictive maintenance. Because of this, preventative maintenance
    may be performed, cutting down on downtime and expenses. Adaptive control techniques
    use ANNs to optimise control settings in response to both incoming data and the
    environment''s dynamic nature. This flexibility is especially useful in sectors
    with highly variable processes, such as chemical manufacturing, where external
    factors can significantly alter product quality. The combination of CV and ANN
    unites the strengths of both fields to improve perception and decision-making.
    While ANNs evaluate the visual data captured by CV systems, they may then identify
    patterns and make educated judgements based on this information. Many innovations
    in industrial automation and robotics can be attributed to the cooperation between
    these two fields. The automated examination of produced goods is one significant
    use. While ANN s analyse the data to decide if the product is up to par, CV s
    can spot flaws and outliers in real time. Integrating these processes makes it
    easier to maintain high standards of quality, cuts down on waste, and boosts output.
    Autonomous navigation is another important field where CVs and ANNs shine. CV
    systems allow robots, drones, and AGVs to see and map their environments, allowing
    them to detect hazards and adapt to changing conditions.[16]. These systems can
    use ANNs to plot out efficient routes, avoid obstacles, and adjust to changing
    conditions. This is especially helpful in big facilities, where swift movement
    around the building is crucial to maximising output. The ability of autonomous
    cars to sense their environments, make split-second judgements, and travel safely
    relies heavily on CV and ANN algorithms. Transportation might be made much more
    effective and secure with the use of this technology. In summary, the use of computer
    vision and artificial neural networks has revolutionised robotics and automation
    in the manufacturing sector. These innovations have increased accuracy, flexibility,
    and productivity across many sectors. As we learn more about CV and ANN, it becomes
    evident that these technologies will have far-reaching effects on the future of
    automation, robotics, and other areas, fostering development and innovation. SECTION
    II. Related Study This study proposes a paradigm for dealing with the difficulties
    encountered while creating automation systems that make use of collaborative robots
    and other devices with a degree of autonomy. These robots attain their extraordinary
    adaptability through the use of online algorithms for detecting and acting. Control
    systems, in order to make use of this modern equipment and algorithm, need to
    be progressively adaptable. In this study, we introduce Sequence Planner (SP),
    a framework for controlling the emerging category of intelligent automation systems
    that aids in the management of both conventional automation tools and autonomous
    machines. SP utilises auxiliary algorithms for control logic synthesis and online
    planning to facilitate the difficult process of designing automation control solutions.[17].
    The Robot Operating System (ROS) has been used to develop SP with plug-in support,
    and it has been applied to a working industrial demonstration. In this presentation,
    we discuss the results of using SP as the control system for this demonstration,
    demonstrating that this method is a suitable way to automate a very versatile
    single-station setup. We expect that our work will serve as a starting point for
    the development of intelligent automation systems, as there is currently no standardised
    approach to automating such systems [1]. Automation and robots have revolutionised
    the automobile industry over the past 50 years, boosting productivity and raising
    standards across the board. The autonomous, electrified, and networked vehicles
    of the future, however, will necessitate a new level of adaptability and intelligence
    in the manufacturing process, particularly in the final assembly phase. [18].
    The need for collaborative and intelligent automation systems during final assembly
    has arisen in response to the growing complexity of goods, industrial processes,
    and logistics networks. Together, sophisticated vision-based control, adaptive
    safety systems, online optimisation and learning algorithms, and networked and
    well-informed human operators will constitute these systems. Transforming the
    present trucking business so that it can develop, deploy, and operate large-scale
    collaborative and intelligent automated systems will be a massive job, though.
    In this article, we discuss the problems that arise throughout the current steps
    of planning and preparing for final assembly, as well as the necessity of these
    steps and some potential remedies. The suggested planning and preparation methods
    are evaluated using an industrial use case at Volvo Trucks built using Sequence
    Planner and ROS2 [2]. The challenges of adapting to digital technology in agriculture
    with the hopes of boosting productivity and competitiveness are discussed. State
    assistance for the digitization of agriculture has led to some triumphs in the
    agricultural industry, but there are still many technological challenges that
    have yet to be overcome. Automating the whole agricultural process, from planting
    seeds to gathering harvested produce, is the current fad in the usage of digital
    technology in this sector. This article summarises the findings from an investigation
    into the robotization and automation of the agro-industrial sector, outlining
    the most promising technological developments in this space. Intelligent control
    information systems'' practicality is examined. The potential use of unmanned
    aerial vehicles is given particular consideration. Despite their obvious benefits,
    agricultural robots are plagued by unrealistic expectations and a host of other
    drawbacks. Concerns about how to legislate the agro-industrial complex''s transition
    to digitalization are discussed. Key strategies for boosting agricultural productivity
    were uncovered through an examination of the agricultural market and associated
    technology. Among these are the education of new professionals in order to create
    and execute cutting-edge technology, the greening of agricultural practices, the
    design and implementation of intelligent systems, and the advancement of robots
    [3]. Qualitative criteria have been developed to map fabric qualities to the optimal
    sewing machine settings for smart sewing machines by observing how machines interact
    with cloth at various speeds. Fuzzy logic inference processes have been included
    in a neural network, enabling the optimisation of membership function outputs
    and, ultimately, self-learning. The method has been successfully applied to the
    creation of smart sewing machines and is now widely used in the garment industry.
    Using the Neuro-Fuzzy model''s feedback closed loop, a system has been proposed
    for intelligent manufacturing in which fabric characteristics can forecast the
    sewability of any fabric, ascertain the smallest change in fabric properties necessary,
    and regulate the stitching of a garment in real time. The technology has been
    put through its paces in a commercial context with positive results. Optimal settings
    were attained over the whole speed range of the sewing machine, accounting for
    the qualities of challenging materials and the operator''s mishandling [4]. SECTION
    III. Methodology Using CV and ANN algorithms, the methodology section describes
    how intelligent control systems were implemented for industrial automation and
    robotics. This section describes in depth the methods, resources, and processes
    that were employed to accomplish the aims of the study or project. A. Data Collection
    and Preprocessing Gathering and preprocessing data is the first stage in creating
    an intelligent control system. Visual data from cameras and sensors, as well as
    historical data pertaining to automated operations, may fall into this category
    in the context of industrial automation and robotics. High-resolution cameras
    and sensors are only two examples of the data-collection equipment used to collect
    data in real time. The gathered information is then “pre-processed” to eliminate
    unwanted elements, rectify any errors, and transform it into a format ready for
    analysis.[19]. B. Computer Vision (cv) Implementation The computer vision (CV)
    subsystem of an intelligent control system processes and makes sense of visual
    information. Specifically, this entails using CV algorithms to glean useful data
    from visual sources. Object identification, tracking, and picture segmentation
    are all typical CV tasks. Algorithm development typically makes use of open-source
    computer vision libraries like OpenCV or deep learning frameworks like TensorFlow
    or PyTorch. To discover and identify things of interest in visual data, object
    identification algorithms like YOLO (You Only Look Once) and Faster R-CNN are
    used. In order to detect things important to the automation job, such as product
    faults or workpiece placements, these algorithms are trained using labeled datasets.
    [20] Segmentation methods are applied to pictures in order to separate out individual
    sections for closer examination. In quality control, where flaws must be accurately
    localized, this can be very helpful. C. Artificial Neural Networks (ann) Implementation
    In order to automate tasks that involve learning and decision-making, intelligent
    control systems rely on ANN algorithms. Different forms of ANNs, such as feedforward
    neural networks, convolutional neural networks (CNNs), and recurrent neural networks
    (RNN s), may be utilized for various tasks. Artificial neural networks (ANN s)
    rely heavily on training datasets to gain the ability to draw meaningful conclusions
    from past data. To anticipate when machinery may break down, ANN s are trained
    using data collected from the equipment''s sensors in predictive maintenance.
    Adjusting network parameters iteratively during training helps reduce prediction
    mistakes. In order to optimize control parameters in real time, ANN s are frequently
    used in adaptive control techniques. Deep Q-networks (DQNs) and other reinforcement
    learning approaches can be used to fine-tune control rules in response to external
    feedback. Intelligent control systems rely heavily on the interplay between CV
    and ANN. CV algorithms analyze visual data to feed ANNs, which in turn makes it
    possible to make decisions based on empirical evidence. Integration entails setting
    up a channel of communication between the two halves, with the goal of having
    ANN s correctly comprehend and respond to CV output. For example, in automated
    inspection, CV finds product flaws and sends that data to ANNs for analysis and
    decision-making. Automatic rejection or acceptance is made possible by ANNs based
    on their assessments of defect severity and compliance with quality requirements.
    D. Testing and Validation Extensive testing and validation are performed on the
    intelligent control system to assure its dependability and accuracy. This encompasses
    both virtual and actual testing environments. It is common practice to assess
    the system''s performance in a variety of scenarios using simulation environments
    like MATLAB/Simulink or ROS (Robot Operating System).During real-world testing,
    the technology is actually implemented in a robotic or industrial environment.
    Adjustments to algorithms and solutions to real-world problems, such as varying
    illumination or noise, might be made during this stage. E. Evaluation and Metrics
    for Success Metrics for measuring the system''s performance are established. A
    few examples of often used metrics are F 1 score, response time, accuracy, and
    recall. The success of the intelligent control system in achieving its goals in
    quality control, predictive maintenance, or autonomous navigation is measured
    using these indicators. Intelligent control systems for industrial automation
    and robotics using CV and ANN algorithms necessitate a methodical approach that
    incorporates data acquisition, preprocessing, algorithm implementation, integration,
    real-time control, testing, and evaluation. The goal of this approach is to use
    CV and ANN to improve the accuracy, precision, and flexibility of manufacturing
    processes, which has far-reaching implications. Equations: Object Detection Probability
    (P _detection): CV+ANNCombination; P − detection=TruePositives/(TruePositives+
    FalseNegatives)− (1) View Source This equation provides a quantitative measure
    for the likelihood of identifying items or abnormalities in a dataset, which may
    be used for quality control purposes. Detections that are true positives are right,
    whereas detections that are false negatives are overlooked. The Reward Function
    (Reward) in Reinforcement Learning: Reward=R(s, a)− (2) View Source The reward
    function in a reinforcement learning scenario specifies the instantaneous payoff
    for an agent (robot or system) performing action an in state s. This incentive
    directs learning towards increasing long-term benefits. SECTION IV. Results and
    Discussions In-depth presentation, analysis, and discussion of the study''s findings
    all go into the “Results and Discussion” portion of a research paper or project
    report. The meat of the study may be found here, including a detailed analysis
    of the study''s data, conclusions, and implications. In this introductory section,
    we outline the aims of the study, emphasize the methods employed, and stress the
    importance of the subsequent discussion. Fig. 1. Proposed System Architecture
    Show All Fig. 2. Accuracy comparison of proposed method with existing algorithms
    Show All In the context of object identification, image classification, and other
    machine learning tasks, accuracy is a popular performance parameter used to quantify
    the overall correctness of a classification or prediction system. Accuracy is
    determined by contrasting the fraction of training data that was properly categorised
    with the total number of training data instances. As shown in figure 4.1,the graph
    shows that accuracy comparison between the proposed method with other existing
    methods. Here the accuracy rate of LSTM is 75%,Support Vetor Machine has 86% followed
    by K-Nearest neighbour records 89%.Finally our proposed method records the accuracy
    of95%. The F 1 Score takes into account both the precision and recall of a model.
    When one class greatly outnumbers the other in a dataset, this method comes in
    quite handy. The Fl Score is a numeric value between 0 and 1 that is the harmonic
    mean of the recall and accuracy scores.Here the F 1 Score is measured for LSTM
    is 65%,followed by the Support Vector Machine is 69%,KNN have recorded 74% and
    finally our proposed method has 78%. Sensitivity, also known as Recall or True
    Positive Rate, is a metric for evaluating a model''s efficacy in properly identifying
    positive events. When false negatives might have serious effects, like in medical
    diagnostics or safety-critical applications, this is of paramount importance The
    Recall value of LSTM is 73%,where Support Vector Machine is 84%,followed by the
    K-Nearest Neighbour has 87% and Finally our Proposed method is 91 %. Figure 4.1,
    shows the Response time comparison of the proposed method with other existing
    algorithms. Here our Proposed Method has more faster in responding. SECTION V.
    Conclusion Finally, a new age of intelligent control systems has begun with the
    use of Computer Vision (CV) and Artificial Neural Networks (ANN) in industrial
    automation and robotics. The convergence of perception and reasoning has resulted
    in revolutionary improvements in productivity, accuracy, and flexibility in a
    wide range of fields. Automated systems now have extraordinary perceptual and
    interpretive acuity thanks to the widespread use of CV algorithms for object identification,
    tracking, and picture processing. The learning and decision-making skills afforded
    by ANN s, on the other hand, have allowed systems to respond to changing situations,
    fine-tune their controls, and improve their overall performance. CV and ANN algorithms
    have had a significant influence in many areas, from quality control in manufacturing
    to autonomous navigation in warehouses. The use of these innovations has decreased
    the need for human involvement while simultaneously raising product quality, output,
    and security. In addition, its use may be seen in other areas, such as precision
    agriculture and healthcare, where it is transforming procedures and broadening
    the scope of automation. Industrial automation and robotics will continue to benefit
    from the synergy between CV and ANN as technology develops. It is certain that
    these intelligent control systems will continue to develop, providing new answers
    to difficult problems in industry and paving the way for more complicated uses
    down the road. The future looks bright for advancing automation''s role in increasing
    efficiency, quality, and sustainability across many different industrial areas.
    Fig. 3. Response time comparison Show All Table I. Performance metrices comparison
    SECTION VI. Future Work The goal of future research in this area should be to
    further automate and generalize the capabilities of artificial neural networks
    and computer vision. Improving human-robot cooperation involves, among other things,
    creating more reliable and real-time CV algorithms, optimizing ANN topologies
    for specific industrial applications, and so on. Improving the interpretability
    and explain ability of AI-driven judgments and investigating the possibilities
    of edge computing for on-device processing are also important. To guarantee the
    safe and responsible use of these technologies, additional study is needed into
    the cybersecurity elements of intelligent control systems as well as the ethical
    considerations associated with automation in sensitive fields. Authors Figures
    References Keywords Metrics More Like This Intelligent control of robot arm using
    artificial neural networks Proceedings of 8th Mediterranean Electrotechnical Conference
    on Industrial Applications in Power Systems, Computer Science and Telecommunications
    (MELECON 96) Published: 1996 Mobile Robot Navigation Control in Moving Obstacle
    Environment Using Genetic Algorithm, Artificial Neural Networks and A* Algorithm
    2009 WRI World Congress on Computer Science and Information Engineering Published:
    2009 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical,
    Electronics and Computer Engineering, UPCON 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Intelligent Control Systems for Industrial Automation and Robotics
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Shahzad M.
  citation_count: '0'
  description: In Pakistan, agriculture and food invention classifications are facing
    accumulative pressures from climate change affecting biotic stresses on field
    crops specifically and in general on soil health and irrigation water. Pakistan
    has its GDP based on agriculture with major crops including Wheat, Sugarcane,
    Paddy and Cotton. However, data of the crops is taken manually, which is mostly
    labor intensive, relatively ineffective and non-scientific. Therefore, technological
    innovations in artificial intelligence are the most feasible and economically
    viable and proven options than ever to sheltered adequate food for the fast-growing
    population of the country. Crop maps are frequently designed using foliage indices
    and field data. With the recent advances in remote sensing and Artificial Intelligence
    (AI) such as high resolution satellite imagery analysis, deep learning and computer
    vision, automation and improvement in precision of crop mapping can be achieved.
    Now-e-days we can enumerate field scale phenotypic data accurately and assimilate
    the big data into analytical and prescriptive management tools. The integration
    of AI with geographic information systems (GIS) provides a powerful tool for real-time
    monitoring of accurate crop classification, plant health (Biotic Stress), crop
    growth, water (quality and quantity) and harvest monitoring. These models also
    have the capacity to do image analysis for disease diagnostics and associated
    management recommendations on farmers phones. It will also help to develop future
    training methodologies and modules according to running requirements in response
    to the existing biotic stresses of major field crops in Pakistan.
  doi: 10.33866/PHYTOPATHOL.035.02.0949
  full_citation: '>'
  full_text: '>

    "USER Username Password Remember me ARTICLE TOOLS Print this article Indexing
    metadata How to cite item Finding References Review policy Email this article
    (Login required) Email the author (Login required) RELATED ITEMS Author''s work
    Related studies Government policy Book searches Relevant portals Databases Online
    forums Data sets Pay-per-view Media reports Web search NOTIFICATIONS View Subscribe
    JOURNAL CONTENT Search Search Scope      All Authors Title Abstract Index terms
    Full Text      Browse By Issue By Author By Title ABOUT THE AUTHOR Minahil Shahzad
    Lahore Grammar School, 55 Main Gulberg, Lahore Pakistan LANGUAGE Select Language
    Euskara Català Čeština Dansk Deutsch English Español (España) Français (Canada)
    Galego ελληνικά Hrvatski Bahasa Indonesia Italiano മലയാളം Nederlands Norsk فارسی
    Português (Brasil) Português (Portugal) Limba Română Русский Cрпски Svenska Tiếng
    Việt Türkçe Українська 日本語 简体中文 繁體中文 KEYWORDS Alternaria solani Biological control
    Botanicals Cultivars Eggplant FHB species Fungi Fungicides Fusarium oxysporum
    Incidence Isolation Petri-dish assay Plant extracts Rice Trichoderma spp Wheat
    disease incidence identification pathogenic variation races virulence HOME ABOUT
    LOGIN REGISTER SEARCH CURRENT ARCHIVES ANNOUNCEMENTS THESIS ABSTRACTS Home > Vol
    35, No 2 (2023) > Shahzad MAPPING BIOTIC STRESS IN FIELD CROPS: PROSPECTS OF ARTIFICIAL
    INTELLIGENCE IN PAKISTAN Minahil Shahzad  Abstract  In Pakistan, agriculture and
    food invention classifications are facing accumulative pressures from climate
    change affecting biotic stresses on field crops specifically and in general on
    soil health and irrigation water. Pakistan has its GDP based on agriculture with
    major crops including Wheat, Sugarcane, Paddy and Cotton. However, data of the
    crops is taken manually, which is mostly labor intensive, relatively ineffective
    and non-scientific. Therefore, technological innovations in artificial intelligence
    are the most feasible and economically viable and proven options than ever to
    sheltered adequate food for the fast-growing population of the country. Crop maps
    are frequently designed using foliage indices and field data. With the recent
    advances in remote sensing and Artificial Intelligence (AI) such as high resolution
    satellite imagery analysis, deep learning and computer vision, automation and
    improvement in precision of crop mapping can be achieved. Now-e-days we can enumerate
    field scale phenotypic data accurately and assimilate the big data into analytical
    and prescriptive management tools. The integration of AI with geographic information
    systems (GIS) provides a powerful tool for real-time monitoring of accurate crop
    classification, plant health (Biotic Stress), crop growth, water (quality and
    quantity) and harvest monitoring. These models also have the capacity to do image
    analysis for disease diagnostics and associated management recommendations on
    farmers phones. It will also help to develop future training methodologies and
    modules according to running requirements in response to the existing biotic stresses
    of major field crops in Pakistan.  Keywords  GIS; TensorFlow; Vegetation Index;
    Cross-Validation; Model Optimization.  Full Text: PDF References  Bock, C. H.,
    J. G. Barbedo, A. K. Mahlein and E. M. Del Ponte. 2022. A special issue on phytopathometry—visual
    assessment, remote sensing, and artificial intelligence in the twenty-first century.
    Tropical Plant Pathology, 47(1):1-4. Eckstein, D., V. Künzel and L. Schäfer. 2021.
    The global climate risk index. 2021. Bonn: Germanwatch, Recuperado de. FARMDAR.
    2022. Annual report of an Agri-Tech Company, Pakistan. Garrett, K. A., D. P. Bebber,
    B. A. Etherton, K. M. Gold, A. I. Plex Sulá and M. G. Selvaraj. 2022. Climate
    change effects on pathogen emergence: Artificial intelligence to translate big
    data for mitigation. Annual Review of Phytopathology, 60:357-378. Johnson, K.
    A., C. H. Bock and P. M. Brannen. 2021. Phony peach disease: past and present
    impact on the peach industry in the southeastern USA. CABI Agriculture and Bioscience,
    2(1):1-23. Morris, C. E., G. Géniaux, C. Nédellec, N. Sauvion and S. Soubeyrand.
    2022. One Health concepts and challenges for surveillance, forecasting, and mitigation
    of plant disease beyond the traditional scope of crop production. Plant Pathology,
    71(1):86-97.    DOI: https://doi.org/10.33866/phytopathol.035.02.0949 REFBACKS
    There are currently no refbacks.   Copyright (c) 2023 MINAHIL SHAHZAD   This work
    is licensed under a Creative Commons Attribution-NonCommercial 4.0 International
    License.                Pakistan Journal of Phytopathology ISSN: 1019-763X (Print),
    2305-0284 (Online). © 2013 Pak. J. Phytopathol. All rights reserved.    "'
  inline_citation: '>'
  journal: Pakistan Journal of Phytopathology
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'MAPPING BIOTIC STRESS IN FIELD CROPS: PROSPECTS OF ARTIFICIAL INTELLIGENCE
    IN PAKISTAN'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Moses D.
  - Kumar T.P.
  - Varalakshmi S.
  - Pamulaparty L.
  citation_count: '0'
  description: In this proposal, we study the advances of major core technologies
    and their applicability in creating an Intelligent farming System (IFS). As the
    world is trending into new technologies and implementations it is a necessary
    goal to trend up with agriculture also. Cyber Physical System (CPS) plays a very
    important role in Smart Farming. IOT sensors are capable of providing information
    about agriculture or Farming fields. We have proposed a Cyber Physical System
    (CPS) enabled smart agriculture system using different technologies like AI&ML,
    Data Science and Cloud Computing. This CPS based Intelligent Farming system makes
    use of sensor networks that collects data from different sensors which as a result
    develop an Intelligent Village Farming. Several Utilities such as Pest management,
    Crop Stress management, Nutrient management, Water management and Deep Analysis
    can be done to suggest the farmer regarding the crop and climatic conditions.
    This smart agriculture or Smart Farming using Cyber Physical System (CPS) is powered
    by advances in sensor technology, wireless communication technologies and their
    applicability to farming Chatbot, Computer vision, technology enabling farming,
    it consists of sensor followed by technological techniques.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 14th International Conference on Advances in Computing, Control, and Telecommunication
    Technologies, ACT 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Cyber Physical System Enabled Intelligent Farming System with Artificial
    Intelligence, Machine Learning and Cloud Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kalaiselvi V.K.G.
  - Gopalakrishnan J.
  - Anand S.S.
  - Hariharan S.
  - Saravanan S.
  - Annamalai H.
  citation_count: '1'
  description: India, the largest exporter of farm products, faces low agricultural
    productivity, resulting in reduced income for farmers. To boost revenue, performance
    improvement is crucial. Irrigation plays a vital role in farming, necessitating
    careful management of soil moisture levels. However, traditional irrigation methods
    often lead to water wastage. Efficient utilization of land and resources is essential
    in addressing this issue. Pest attacks pose a significant threat to crop yield
    and quality, making pest identification critical. Farmers and agronomists now
    utilize sensors and IoT technology to remotely monitor crops and manage them in
    controlled environments. However, crop productivity is still affected by weather
    conditions and disease variations. Precise detection of plant deficiencies and
    accurate fertilizer deployment is crucial for efficient farming. Modern computer
    vision techniques can be leveraged for positive financial and environmental outcomes
    in these tasks. Good crop selection significantly impacts yield, especially in
    developing countries. Additionally, growing vegetables in greenhouses or hydroponics
    offers a promising solution to discontinuous and costly supply chains. This abstract
    highlights the importance of addressing productivity challenges and implementing
    innovative farming methods to enhance agricultural outcomes.
  doi: 10.1109/ICAISS58487.2023.10250631
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 Second International Con...
    Sustainable Algorithms using Artificial Intelligence and Various Stages for Precision
    Agricultural Cultivation Publisher: IEEE Cite This PDF Kalaiselvi V.K.G; Jagadeesh
    Gopalakrishnan; Sahana Shri Anand; Shanmugasundaram Hariharan; Sivasankaran Saravanan;
    Hemalatha Annamalai All Authors 2 Cites in Papers 65 Full Text Views Abstract
    Document Sections I. Introduction II. Related Work III. Proposed System IV. CONCLUSIONS
    Authors Figures References Citations Keywords Metrics Abstract: India, the largest
    exporter of farm products, faces low agricultural productivity, resulting in reduced
    income for farmers. To boost revenue, performance improvement is crucial. Irrigation
    plays a vital role in farming, necessitating careful management of soil moisture
    levels. However, traditional irrigation methods often lead to water wastage. Efficient
    utilization of land and resources is essential in addressing this issue. Pest
    attacks pose a significant threat to crop yield and quality, making pest identification
    critical. Farmers and agronomists now utilize sensors and IoT technology to remotely
    monitor crops and manage them in controlled environments. However, crop productivity
    is still affected by weather conditions and disease variations. Precise detection
    of plant deficiencies and accurate fertilizer deployment is crucial for efficient
    farming. Modern computer vision techniques can be leveraged for positive financial
    and environmental outcomes in these tasks. Good crop selection significantly impacts
    yield, especially in developing countries. Additionally, growing vegetables in
    greenhouses or hydroponics offers a promising solution to discontinuous and costly
    supply chains. This abstract highlights the importance of addressing productivity
    challenges and implementing innovative farming methods to enhance agricultural
    outcomes. Published in: 2023 Second International Conference on Augmented Intelligence
    and Sustainable Systems (ICAISS) Date of Conference: 23-25 August 2023 Date Added
    to IEEE Xplore: 22 September 2023 ISBN Information: DOI: 10.1109/ICAISS58487.2023.10250631
    Publisher: IEEE Conference Location: Trichy, India SECTION I. Introduction Agriculture
    plays a vital role in the economy of every country. However, it faces numerous
    challenges, including efficient water management for irrigation. Traditional irrigation
    techniques have proven to be costly and labor-intensive. With the advancements
    in machine learning, there is an opportunity to enhance irrigation practices.
    Previous studies have utilized Fuzzy models, which rely on predefined rules and
    struggle with automatic data computation. This study proposes the popular application
    of algorithms for irrigation prediction. This study investigates various abundant
    grading mechanics, namely Support Vector Machine (SVM), K-Nearest Neighbor (KNN),
    Naive Bayes, Random Forest(RF), and Decision Tree, to accurately forecast irrigation
    needs. Experimental results indicate that the Decision Tree algorithm outperforms
    the other cloning precisions. Furthermore, the outcomes of this research will
    be implemented in the prediction of automatic paddy irrigation using sensor data
    based on the Internet of Things (IoT). By leveraging machine learning techniques,
    this study aims to address the challenges faced by traditional irrigation methods
    and provide more accurate predictions for irrigation needs. The results demonstrate
    the superiority of the Decision Tree algorithm in this context, offering potential
    benefits for optimizing agricultural practices [1], [22], [23]. In agro-based
    supply chain operations (ASCs), customers purchase agricultural products from
    farmers. The quality of agri food is of utmost importance to customers, while
    farmers aim for higher profits. However, effective traceability and governance
    in ASCs face significant challenges due to their size and dynamism. Existing solutions
    often fail to meet several requirements like accountability, administrations of
    ASCs. To address these issues, this study proposes a blockchain-based ASC architecture
    that enables product traceability and establishes an association unit for agri-food
    tracking data. Additionally, a Learning Based training-based ASC technology is
    employed to optimize the manufacturing and preservation processes of agri-food
    products, aiming to maximize profitability. Detailed simulation tests are conducted
    to evaluate the effectiveness of this proposed Altcoin system and the DR-SCM approach
    for various ASC contexts. The outcome indicates that the suggested ledger ASC
    architecture ensures reliable product traceability, and the DR-SCM approach outperforms
    intuitive and Q-learning methods in terms of product profitability. This research
    contributes to enhancing the efficiency and profitability of ASCs while ensuring
    trustworthiness in product traceability [2]. The agricultural sector in India
    is experiencing a steady decline, leading to concerns about food production. Numerous
    farmers in India have chosen to abandon agriculture and pursue alternative business
    opportunities. This shift can be attributed to various factors, such as a shortage
    of labour and unfavourable natural conditions. To address this issue and improve
    productivity, this paper proposes a comprehensive view of different techniques
    for crop preference, sowing, hemp detection, and system monitoring. The proposed
    approach leverages technologies such as image processing, machine learning, the
    IoT, and AI. By comparing the advanced framework with the current framework, a
    comparative research analysis is conducted to evaluate their respective effectiveness
    [3]. This study aims to tackle the primary concern of ensuring the quality and
    safety certification of agricultural products that are free from pollution. The
    paper proposes a framework that seamlessly combines the identification of pollution-free
    agricultural practices with the process of certifying the products. It employs
    a central distributed topology structure and establishes a workflow management
    system that is partially centralized and partially distributed. This system utilizes
    workflow and web services technology, with active instance objects distributed
    across two distinct workflow domains; mutual certification through web services.
    The system supports streamlined administration of certification information shared
    between declaration corporations and certification institutions, ensuring efficient
    management, facilitating cross-sectoral and cross-boundary teamwork. Implemented
    in Henan Province and the Ministry of Agriculture, the system realizes whole process
    and electronic management of the entire certification process for pollution-free
    agricultural products. Overall, the research shows promising prospects for application
    and extension [4]. Climate change is becoming increasingly evident through frequent
    weather pattern changes that result in severe droughts and floods. This poses
    a threat to food security, particularly with the reduction of cultivable land
    and the growing population. Agricultural Research Institutes are actively engaged
    in developing new cropping patterns, heat and flood-tolerant crop varieties, and
    cultivation practices. However, the lack of an effective knowledge extension system
    prevents the timely dissemination of extensive and expensive research outcomes
    to its intended beneficiaries, the farmers. This research paper discusses the
    National Agricultural Innovation Project (NAIP) pilot, conducted in vulnerable
    regions of India, aimed at enhancing adaptive capacity to climate change. The
    study employed extensive field demonstrations and data analysis to showcase the
    pivotal role of Information and Communication Technology (ICT) in establishing
    a two-way connection between research laboratories and end users. While agricultural
    research identifies current and future risks associated with climatic variability
    and develops or identifies region-specific crop varieties and practices, the collaborative
    efforts of IT and ICT can leverage the research outcomes to raise awareness and
    promote the adoption of these varieties and practices by farmers and other stakeholders
    through an ICT platform [5]. This paper emphasizes the inevitability of plant
    stress in farming and highlights the importance of early stress detection for
    the advancement of smart farming. It introduces an IoT based system designed to
    identify lettuce stresses. The system combines practical environment features
    and leaf features to effectively manage stresses like heat stress and nitrogen
    stress. A key tool employed is the chlorophyll fluorescence (ChF) technique, which
    enables the extraction of leaf stress features. The proposed IoT system enables
    real-time analysis and discussion, promoting collaboration among farm managers,
    experts, and owners [6]. SECTION II. Related Work Efficient irrigation is crucial
    in farming, but traditional methods often lead to significant water wastage. To
    overcome this challenge, farmers are increasingly embracing IoT and smart technologies
    for intelligent resource management. A recent study introduced an innovative solution:
    an Arduino-based irrigation system integrated with sensors for soil moisture,
    temperature, and humidity. By accurately assessing the moisture level in the soil''s
    root zone, the system intelligently determines whether irrigation is necessary,
    skipping cycles when the moisture surpasses a predetermined threshold. This approach
    minimizes water runoff, wind loss, and evaporation, resulting in improved water
    conservation and efficient resource utilization. The integration of IoT and smart
    irrigation technology offers significant advantages. Farmers can make informed
    decisions, utilizing water only when required and in optimal quantities. This
    not only reduces water wastage but also increases crop yield. By automating irrigation
    processes and employing advanced sensor systems, farmers can achieve precise control
    over their irrigation practices. Ultimately, this leads to more sustainable farming
    practices, conserving water resources and promoting efficient land use. The combination
    of IoT and smart irrigation technology holds immense promise for enhancing agricultural
    productivity while minimizing environmental impact [7]. The agricultural sector
    plays a vital role in meeting the global demand for nutritious food. However,
    farmers face significant challenges due to insect pest attacks during cultivation,
    leading to reduced productivity and compromised crop quality. Timely and accurate
    pest identification is crucial for effective pest management. Conventional methods
    of naked-eye perception are inefficient and protracted, particularly in huge-scale
    plant cultivation. To address that issue, this plan focuses on evaluating a self-operating
    pest classification system using deep learning techniques. The entered data is
    augmented and pre-processed using histogram equalization to enhance the accuracy
    of classification. Deep learning algorithms are employed for highlighted extraction,
    and these highlights are then fed into conventional classifiers such as SVM, Naive
    Bayes (NB), RF, and Logistic Regression (LR). Among the classifiers tested, SVM
    demonstrates the highest classification accuracy of 97%, outperforming the other
    algorithms. This indicates the effectiveness of using deep learning for pest identification
    and the potential for implementing this automated system in real-world agricultural
    practices. By enabling early recognition of insect pest attacks, farmers can take
    proactive measures to prevent further spreading and minimize crop damage. The
    integration of deep learning strategies into pest management showcases a promising
    approach for improving agricultural productivity and reducing losses caused by
    pest infestations [8]. One key aspect of an effective farming strategy involves
    implementing an automated framework to overcome the limitations of existing methods
    for detecting and analyzing plant fertilizers. By utilizing advanced computer
    vision techniques, it is feasible to achieve favorable financial and environmental
    outcomes for these tasks. The framework aims to be versatile and applicable across
    various scenarios, applicability in various field conditions and to increase the
    sophisticated tools. The main focus is on cultivation which is economically and
    globally significant with large benefits from an enhanced nutrient deficiency
    detection. Corn cultivation held significant economic importance both in the United
    States and globally in 2018, with the USA alone accounting for 81.7% of the harvested
    acres. The proposed methodology relies on image analysis as a key component collected
    from drone and also RGB sensors to detect nitrogen deficiencies in maize fields.
    It also recommends a scheme to identify the candidate plants with deficiencies
    and helps in creating a training dataset for an object detection neural network.
    Experimental result shows an assuring performance with an average precision of
    82.3% in detecting N-deficient leaves. The issue of inefficient fertilizer application
    persists in corn fields throughout the entire cultivation season by offering a
    more accurate and targeted approach to the nutrient Traditionally, vegetable cultivation
    relies on outdoor farming in specific areas and is limited by seasonal availability,
    leading to discontinuous and rural supply chains. This necessitates transportation
    and maintenance costs when transferring produce to urban centers. In recent years,
    greenhouse and hydroponic techniques have emerged as promising solutions. Soilless
    cultivation technologies enable the efficient production of high-value vegetables,
    increasing yields, ensuring safety, and extending harvest periods. However, these
    systems are still subject to seasonal limitations, making consistent product supply
    challenging. To overcome this, innovative technologies like vertical farms have
    emerged, integrating water based mineral nutrient practices to cultivate horticultural
    crops regardless of weather conditions. The key focus areas of this study include
    productivity, costs, crop cultivation, technology transfer, prototypes, safety,
    and sustainable development [10]. The significance of crop-related services and
    traditional farming methods in the development of third world economies and their
    impact on developed countries. The choice of crops plays a crucial role in determining
    the yield a farmer can achieve in a given agricultural year. Inadequate crop selection
    patterns, neglecting factors like rainfall, temperature, and humidity, can lead
    to detrimental outcomes and contribute to the mounting debts faced by Indian farmers
    in recent years. The social, economic, and mental well-being of farmers is directly
    affected by poor crop selection and low yields. Considering the heavy reliance
    of the Indian agriculture industry on climatic conditions, various Artificial
    Intelligence (AI)-based techniques have emerged to revolutionize farming practices
    [21]. Precision Agriculture, encompassing concepts like ensemble models, K-nearest
    neighbors (KNN) based models, and similarity-based frameworks, aims to mitigate
    traditional farming challenges. Artificial Neural Network (ANN)-based method that
    effectively recommends crops based on specific factors such as rainfall and temperature.
    The proposed method leverages Al technology to provide accurate and efficient
    crop recommendations, benefiting farmers by enhancing the precision of their decision-making.
    Such advancements in precision agriculture hold immense potential to alleviate
    the difficulties faced by farmers, improve agricultural productivity, and contribute
    to the overall socio-economic well-being of farming communities. By embracing
    Al-driven solutions and considering climatic factors, farmers can make informed
    choices that optimize crop yields, thereby positively impacting their livelihoods
    and the agricultural sector as a whole [11]. The objective of the study is to
    develop a recommendation system for crop cultivation using exceptional mining
    algorithms in the agricultural context. The RF algorithm demonstrates superior
    performance with the lowest error measures, including RMSE, RAE, and RRSE. This
    highlights its effectiveness in classifying agricultural text data. The integration
    of robust and complete description with crop recommendation leads to improved
    accuracy forecast, ultimately develops crop yield. Additionally, it contributes
    to the development of an environmental monitoring system based on condition-based
    awareness. The research suggests that the combination of multisensory data fusion
    and machine learning techniques can enhance crop classification accuracy and provide
    valuable insights for precision agriculture and yield optimization. The findings
    demonstrate the effectiveness of the RF algorithm in classifying agricultural
    text data and highlight the potential for improved precision and crop yield optimization
    through the proposed methodology [12]. Coastal communities and Small Island Developing
    States (SIDS) face significant food insecurity threats due to natural and economic
    components. Their farming systems are highly exposed to climate risks under rising
    sea levels. The most vulnerable groups to food instability are lower-income individuals,
    domestic populations, rural communities, standards and religious minorities, as
    well as women and children. To address these challenges, Hydroponic Crop Cultivation
    (HCC) is being explored as a potential solution. HCC involves growing crops in
    nutrient-rich solutions, reducing resource, time, and space requirements. This
    project aims to analyze the contribution of HCC in addressing global food security
    and nutrition (GFSN) risks through three distinct approaches. Firstly, the project
    will assess the diverse applications of HCC, such as in SIDS, refugee camps, areas
    with limited access to fresh food (food deserts), rooftop gardens, and apartment
    units. Secondly, HCC will be compared with other technologies for mitigating GFSN
    risks using a multi-criterion decision-making (MCDM) method. Lastly, a floating
    and storm-resilient HCC system will be specifically designed and tested to tackle
    GFSN challenges in SIDS. The system will utilize the Dutch bucket method, providing
    a practical and efficient solution, allowing for the cultivation of root crops
    and multiple crop varieties within the same system. It will be designed to float
    on water, withstand wind loads, and endure hurricanes. Solar photovoltaic power
    will be utilized, ensuring up to 72 hours of exigency power for communication
    and lighting. This system''s functionality will be assessed through calm water
    trial and wind load simulations [13]. Smart agriculture, also known as Agriculture
    4.0, integrates modern technology with traditional agricultural practices to enable
    automation and intelligence in farming. With the advancements in information technology,
    the potential security concerns in agriculture can be mitigated. This study presents
    three development modes in smart agriculture: precision agriculture, facility
    agriculture, and order agriculture. It further explores seven key technologies
    and their applications derived from these modes. The research emphasizes the significance
    of addressing security issues in smart agriculture and proposes six countermeasures
    for ensuring security and privacy. The security challenges in smart agriculture
    are analyzed, considering both agricultural production and information technology
    aspects. The study particularly highlights the need to consider agricultural equipment
    as potential security threats. Additionally, future research directions include
    exploring 5G communication, fog computing, Internet of Everything, renewable energy
    management, software-defined networks, virtual and augmented reality, and cybersecurity
    datasets for smart agriculture [14]. Fig. 1 presents a sustainable plan of action
    for agricultural cultivation. Fig 1. Sustainable plan of action for agricultural
    cultivation Show All SECTION III. Proposed System Table 1 presents a summary of
    general precision agriculture and research findings. It is clearly inferred that
    an improved system us necessary. Two different perspectives are presented in Fig.
    2 and Fig. 3 which focus on agricultural illustrations with several algorithms
    using machine learning and artificial intelligence respectively. Fig 2. Stages
    in agricultural cultivation [20] Show All TABLE I. Summary of the general precision
    agricultural research findings Fig 3. Agriculture monitoring system using Artificial
    Intelligence Show All SECTION IV. CONCLUSIONS In nutshell, the agricultural sector
    in India faces significant challenges in terms of low productivity, water wastage,
    pest attacks, and unpredictable weather conditions. However, advancements in technology
    and innovative farming methods offer promising solutions to address these issues.
    By leveraging sensors, IoT, and computer vision techniques, farmers can remotely
    monitor crops, optimize water usage, and detect plant deficiencies accurately.
    Additionally, adopting greenhouse cultivation and hydroponics can help overcome
    supply chain limitations and enhance crop selection for improved yield. These
    measures can contribute to a more productive and sustainable agricultural sector
    in India, ensuring better financial outcomes for farmers and overall economic
    growth. Embracing these advancements and implementing effective strategies will
    pave the way for a brighter future in Indian agriculture. Authors Figures References
    Citations Keywords Metrics More Like This Greenhouse Crop Monitoring with Low-Cost
    Sensors: Assessing Lettuce Production Through Air-Canopy Temperature Difference
    2023 IEEE CHILEAN Conference on Electrical, Electronics Engineering, Information
    and Communication Technologies (CHILECON) Published: 2023 Sensor based Crop Protection
    System with IOT monitored Automatic Irrigation 2020 2nd International Conference
    on Advances in Computing, Communication Control and Networking (ICACCCN) Published:
    2020 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: Proceedings of the 2023 2nd International Conference on Augmented Intelligence
    and Sustainable Systems, ICAISS 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Sustainable Algorithms using Artificial Intelligence and Various Stages for
    Precision Agricultural Cultivation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sivakumar S.A.
  - Shankar B.M.
  - Anuradha B.
  - Karan K.A.
  - Karthik A.
  - Karthik R.
  - Kumar J.R.R.
  citation_count: '0'
  description: The integration of artificial intelligence (AI) in agriculture has
    the potential to revolutionize the industry, making it more efficient, sustainable,
    and productive. AI can be used to analyze data from various sources, such as satellite
    imagery, drones, and sensors, to provide farmers with valuable insights. It helps
    in optimizing irrigation, fertilization, and pesticide application by identifying
    areas of the field that require attention. AI algorithms can also predict crop
    yield, disease outbreaks, and recommend optimal planting and harvesting times.
    AI-powered systems can monitor crops and livestock through computer vision techniques
    and sensor data analysis. This enables early detection of plant diseases, nutrient
    deficiencies, or pest infestations. Livestock monitoring can include facial recognition
    to identify individual animals, behavior analysis to detect signs of illness or
    distress, and automated feeding systems. In developing countries like India, the
    rapid spread of mobile internet technology is offering a vital role in economic
    growth, social empowerment, and grass roots creativity. Every harvested crop needs
    to be transported. Transportation is one of the important factors for a farmer’s
    success. Transport that is well-managed is effective in transporting farm resources
    and harvested products as quickly as possible. So we are going to develop an android
    application for farmers and transport service providers. By this, we can pool
    the farmers according to their requirements.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: International Journal of Intelligent Systems and Applications in Engineering
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Artificial Intelligence based Agricultural Chatbot and Virtual Assistant
    for Delivery of Harvested Crops
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Roshandel S.
  - Eslamian S.
  citation_count: '0'
  description: The ability to use water economically has been an issue that farmers
    and growers have been attempting to overcome since mankind started to till the
    earth. The advent of sophisticated technology has seen a renewed focus in securing
    effective, efficient water management techniques in agriculture amid a period
    of climatic and population concerns. Advances in Wireless Sensor Networking and
    the Internet of Things have leveraged further investment and research into effectual
    food production and supply chain management. With the application of machine learning
    (ML) to a combination of sensor data gathered in the field and historical data,
    proficient on-demand Intelligent Irrigation Systems can contribute to cost savings,
    higher crop yields and optimal water use. This chapter will look at how different
    technologies have been combined to create Smart Irrigation systems with discussion
    on how ML and Computer Vision are increasingly playing a valuable role in assuring
    food security. The chapter will include a description of Smart Manual Irrigation
    before looking at what the future holds for Smart Irrigation systems.
  doi: 10.1201/9780429290152-18
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeEngineering & TechnologyEngineering ManagementHandbook of Irrigation
    Hydrology and ManagementAutomation and Smart Irrigation Chapter Automation and
    Smart Irrigation BySajjad Roshandel, Saeid Eslamian Book Handbook of Irrigation
    Hydrology and Management Edition 1st Edition First Published 2023 Imprint CRC
    Press Pages 19 eBook ISBN 9780429290152 Share ABSTRACT The ability to use water
    economically has been an issue that farmers and growers have been attempting to
    overcome since mankind started to till the earth. The advent of sophisticated
    technology has seen a renewed focus in securing effective, efficient water management
    techniques in agriculture amid a period of climatic and population concerns. Advances
    in Wireless Sensor Networking and the Internet of Things have leveraged further
    investment and research into effectual food production and supply chain management.
    With the application of machine learning (ML) to a combination of sensor data
    gathered in the field and historical data, proficient on-demand Intelligent Irrigation
    Systems can contribute to cost savings, higher crop yields and optimal water use.
    This chapter will look at how different technologies have been combined to create
    Smart Irrigation systems with discussion on how ML and Computer Vision are increasingly
    playing a valuable role in assuring food security. The chapter will include a
    description of Smart Manual Irrigation before looking at what the future holds
    for Smart Irrigation systems. Previous Chapter Next Chapter Your institution has
    not purchased this content. Please get in touch with your librarian to recommend
    this.  To purchase a print version of this book for personal use or request an
    inspection copy  GO TO ROUTLEDGE.COM  Policies Privacy Policy Terms & Conditions
    Cookie Policy Journals Taylor & Francis Online Corporate Taylor & Francis Group
    Help & Contact Students/Researchers Librarians/Institutions Connect with us Registered
    in England & Wales No. 3099067 5 Howick Place | London | SW1P 1WG © 2024 Informa
    UK Limited"'
  inline_citation: '>'
  journal: 'Handbook of Irrigation Hydrology and Management: Irrigation Methods'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Automation and Smart Irrigation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ram P.P.V.S.
  - Yaswanth K.V.S.
  - Kamepalli S.
  - Sankar B.S.
  - Madupalli M.
  citation_count: '2'
  description: The agriculture industry has been facing a growing challenge with the
    rising cost of labor in recent years. A large portion of the expenses incurred
    in agriculture goes towards employing workers. This increase in labor expenses
    puts farmers under pressure to find ways to reduce costs while still producing
    at high levels. To tackle this issue, many farmers have turned to technology and
    automation. Using machines and automated systems to perform tasks that were previously
    done by humans can help decrease labor costs and increase efficiency, keeping
    farmers competitive in a continuously changing industry. The usage of technology
    in agriculture has rapidly increased in various forms, including precision farming,
    crop monitoring, irrigation systems, and decision-making tools based on data.
    Additionally, the growth of machine learning and AI has also played a crucial
    role in modernizing agriculture, with computer vision-based systems being developed
    for crop detection, classification, and monitoring. This research focuses on creating
    a red chili detection system using the YOLOv5 deep learning model. This project's
    objective is to accurately and successfully identify ripened red chilies in real-world
    images. The proposed approach first applies methods for preparing images to increase
    the input photos' quality, and then utilizes the YOLOv5 model for object detection.
    The model was trained using a large dataset of red chili images and evaluated
    based on performance metrics, including accuracy, speed, and robustness to variations
    in the input data. The outcomes of our tests show that the suggested technique
    is more successful than other cutting-edge deep learning models, making it a promising
    solution for applications in the fields of agricultural automation and food processing.
  doi: 10.1109/I2CT57861.2023.10126327
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 IEEE 8th International C...
    Deep Learning Model YOLOv5 for Red Chilies Detection from Chilly Crop Images Publisher:
    IEEE Cite This PDF Pusala Pawan Venkata Sai Ram; Kaza Venkata Siva Yaswanth; Sujatha
    Kamepalli; Bodapati Siva Sankar; Manpj Madupalli All Authors 3 Cites in Papers
    76 Full Text Views Abstract Document Sections I. Introduction II. Objectives III.
    Literature Review IV. Proposed Methodology V. Conclusions and Future Scope Authors
    Figures References Citations Keywords Metrics Abstract: The agriculture industry
    has been facing a growing challenge with the rising cost of labor in recent years.
    A large portion of the expenses incurred in agriculture goes towards employing
    workers. This increase in labor expenses puts farmers under pressure to find ways
    to reduce costs while still producing at high levels. To tackle this issue, many
    farmers have turned to technology and automation. Using machines and automated
    systems to perform tasks that were previously done by humans can help decrease
    labor costs and increase efficiency, keeping farmers competitive in a continuously
    changing industry. The usage of technology in agriculture has rapidly increased
    in various forms, including precision farming, crop monitoring, irrigation systems,
    and decision-making tools based on data. Additionally, the growth of machine learning
    and AI has also played a crucial role in modernizing agriculture, with computer
    vision-based systems being developed for crop detection, classification, and monitoring.
    This research focuses on creating a red chili detection system using the YOLOv5
    deep learning model. This project’s objective is to accurately and successfully
    identify ripened red chilies in real-world images. The proposed approach first
    applies methods for preparing images to increase the input photos'' quality, and
    then utilizes the YOLOv5 model for object detection. The model was trained using
    a large dataset of red chili images and evaluated based on performance metrics,
    including accuracy, speed, and robustness to variations in the input data. The
    outcomes of our tests show that the suggested technique is more successful than
    other cutting-edge deep learning models, making it a promising solution for applications
    in the fields of agricultural automation and food processing. Published in: 2023
    IEEE 8th International Conference for Convergence in Technology (I2CT) Date of
    Conference: 07-09 April 2023 Date Added to IEEE Xplore: 23 May 2023 ISBN Information:
    DOI: 10.1109/I2CT57861.2023.10126327 Publisher: IEEE Conference Location: Lonavla,
    India SECTION I. Introduction Red chili is a highly sought-after spice used in
    many cuisines worldwide and is widely grown for its pungent taste and vibrant
    colour. The detection of fully ripened red chilies is crucial in agriculture as
    it impacts the quality, quantity, and profitability of the crop. The traditional
    approach of manual inspection and picking is demanding and time-consuming, resulting
    in higher expenses and reduced productivity. With the development of technology
    (Sihombing et al., 2022)(Cruz-Domínguez et al., 2021)(Sujatha & Srinivasa Rao,
    2019), there has been a growing interest in utilizing deep learning and computer
    vision-based methods for the detection of red chilies. These methods aim to automate
    the process, increase accuracy, and reduce the time and cost associated with manual
    inspection. Some common objectives of red chilies detection include: High Accuracy:
    One of the main goals is to create a system that can precisely identify red chilies
    in images. This is important for applications such as quality control, where accurate
    detection is essential for ensuring the quality of the end product. Fast Detection
    Speed: Another crucial goal is to create a system that can quickly identify red
    chilies, as fast detection speeds are essential for applications such as agricultural
    automation and food processing. Robustness to Variations: The system should be
    robust to variations in the input data, such as different lighting conditions,
    camera angles, and the presence of complex backgrounds and other distractions.
    Easy Deployment: The system should be easy to deploy and integrate into existing
    systems and processes. This can include providing user-friendly software interfaces
    and APIs that allow users to easily access the system’s predictions. Scalability:
    The system should be scalable and able to handle large amounts of data. This is
    important for applications where the system needs to be used on a large scale,
    such as in large-scale agricultural operations or food processing plants. Deep
    Learning Models in Agriculture: Technology is transforming agriculture and providing
    new opportunities for farmers to improve productivity, increase profitability,
    and better meet the demands of an ever-growing global population. Recently, there
    has been a sharp increase in the use of technology in agriculture, bringing a
    wide range of benefits to farmers. From precision farming (Shaikh et al., 2022)
    to crop monitoring (Raj et al., 2022) and data-driven decision-making tools, technology
    has revolutionized the way farmers manage their operations. Automated irrigation
    systems, drones, and sensor-based technologies have enabled farmers to increase
    efficiency and reduce costs. The development of artificial intelligence and machine
    learning (Spanaki et al., 2022) models has also played a significant role in modernizing
    agriculture. These models are used for various purposes such as crop detection(Raja
    et al., 2019), classification (Sujatha et al., 2020)(Naik et al., 2022), plant
    disease detection(Raj et al., 2022) and monitoring, providing farmers with valuable
    insights into their operations. The use of deep learning algorithms can significantly
    improve the efficiency and accuracy of these tasks compared to traditional methods,
    enabling farmers to make data-driven decisions and increase their productivity.
    Additionally, deep learning models can also help to address some of the challenges
    faced by the agriculture industry, such as labour shortages and increasing costs
    (Sujatha et al., 2021). By automating certain processes and reducing the reliance
    on manual labour, deep learning models in agriculture can help to increase the
    efficiency, productivity, and profitability of agricultural operations. The aim
    of this work is to develop an automated red chili detecting system. The system
    takes a real-time image of a chili plant as input and combines deep learning techniques
    and computer vision to recognize and choose the ripening chilies. The proposed
    method employs YOLO version 5, a convolutional neural network, and PyTorch to
    achieve this goal in an agricultural setting. The aim is to assist farmers by
    reducing the time and effort required for manual chili detection, as well as to
    increase work speed and quality. Organization of the paper: The remaining sections
    of the paper are organized as follows: The first section provides an overview
    of the use of deep learning models in farming practices and red chili detection.
    The next section provides an update on the recent research related to chili detection
    and classification. The third section delves into the details of the proposed
    approach. The fourth section presents the results and discussions, and finally,
    the conclusion and potential avenues for future research are outlined. SECTION
    II. Objectives The main objectives of red chili detection research are to: Develop
    an automatic red chili detection system that can identify ripened chilies from
    plants without human intervention. Use image recognition techniques to locate
    matured red chilies on the plant in an agricultural setting. Implement object
    detection algorithms such as YOLO version 5, CNN, and PyTorch for red chili detection.
    Utilize a prototype camera module to capture real-time images of red chili plants.
    Improve efficiency and productivity in the agriculture industry by reducing manual
    labor and reducing the cost of labor. Enhance the quality and quantity of the
    final output by eliminating hand burns and other physical challenges faced by
    farmers. The ultimate objective of the research is to offer a workable answer
    to the problems farmers experience in the agricultural sector SECTION III. Literature
    Review A literature survey on red chilies detection using deep learning models
    would reveal that there has been growing interest in using deep learning models
    for red chilies detection in recent years. This is due to the ability of deep
    learning models to automatically learn complex features from large amounts of
    data and make highly accurate predictions. Some of the popular deep learning models
    used for red chilies detection include Convolutional Neural Networks (CNNs), RCNNs,
    YOLO models, and Single Shot Detector (SSD) models. In a study by (Zhang et al.,
    2020), a CNN-based model was used to detect red chilies in images and the results
    showed that the model was able to achieve high accuracy in detecting red chilies
    even in images with complex backgrounds. In another study by(Naranjo-Torres et
    al., 2020), a Faster R-CNN model was used for object detection and the results
    showed that the model was able to achieve high accuracy and improved detection
    speed compared to traditional object detection methods. \" A new artificial intelligence
    approach for classifying dried chili peppers\" by (Cruz-Domínguez et al., 2021),
    presents a method for classifying dried chili peppers using artificial intelligence.
    The authors aim to increase the accuracy of dried chili pepper classification
    to aid in the supply chain management of the chili pepper industry. The study
    used a CNN and image processing techniques to classify the chili peppers. The
    outcomes demonstrate that the suggested technique has a high accuracy rate for
    appropriately categorizing dry chili peppers. The authors conclude that the use
    of artificial intelligence in the classification of dried chili peppers has the
    rate to greatly increase the efficiency and accuracy of the supply chain management
    process in the chili pepper industry. (Naik et al., 2022)\" Chili leaf disease
    detection and classification with a squeeze-and-excitation-based CNN model,\"
    focuses on utilizing machine learning to identify and categorize illnesses that
    affect chili leaves. The authors use a squeeze-and-excitation-based CNN model
    to analyse the images of chili plants and identify the presence of leaf diseases.
    The study results indicate that the suggested model is able to effectively detect
    and classify different types of chili leaf diseases with high accuracy. \" Using
    image processing, classify chilies based on their shape and colour \" by (Sihombing
    et al., 2022) focuses on using image processing to classify chilies based on their
    shape and colour features. The authors aim to develop an efficient method for
    chili classification using computer vision techniques. The results of their study
    suggest that the combination of shape and colour features provide accurate and
    efficient chili classification. The authors conclude that their method has potential
    for use in the agriculture industry for efficient and automated chili sorting.
    SECTION IV. Proposed Methodology YOLO version 5 is a popular object identification
    technique that can be used to detect red chilies. YOLOv5 is a fast and accurate
    model that can detect objects in real-time using a single forward pass through
    the network (Katsamenis et al., 2023). To detect red chilies using YOLOv5, you
    would first need to train the model on a dataset of images that contains red chilies.
    Then, you would use the trained model to detect red chilies in new images by inputting
    the image into the model and using the model’s outputs to draw bounding boxes
    around the detected red chilies. The objectives of using YOLOv5 for red chilies
    detection could be: Accurate detection: To ensure that the model is able to accurately
    detect red chilies in an image. High speed: To ensure that the model is able to
    run in real-time, making it suitable for applications where detection needs to
    be performed quickly. Robustness: To ensure that the model is able to detect red
    chilies even under challenging conditions such as variations in lighting, background,
    and orientation. Scalability: To ensure that the model can be used to detect red
    chilies in a variety of different settings, such as different types of images,
    different resolutions, and different object scales. Generalization: To ensure
    that the model is able to detect red chilies in new images, even if it has only
    been trained on a limited set of images. Architecture of the proposed model can
    be represented using the following figure. Fig. 1. Overall Architecture of the
    Research Work Done Source: Proposed Research Work Flow Diagram Show All There
    are three modules in this research work, Labelling: The labelling can be done
    by giving a bounding box to the red chilies detected form the input image. Labelling
    process composed of three parts,1. giving label index, 2. (x, y) coordinates of
    the bbox, and 3. height and width of bbox. Training: This module is being used
    to build a model for red chili detection. The YOLOv5 model was customized to train
    the custom red chilies dataset. Detection: The trained model was used to detect
    red chilies. The architecture of YOLOV5 model (Xu et al., 2021) can be represented
    as follows Fig. 2. Architecture of YOLOV5 model (Xu et al., 2021) Show All The
    YOLO version 5 architecture consists of several key components: Backbone Network:
    A backbone network, such as ResNet or MobileNet, is used as the feature extractor
    for the YOLOv5 model. This network is responsible for extracting high-level features
    from the input image that can be used for object detection. Neck: The neck component
    is used to connect the backbone network to the head component. It can be used
    to add additional features to the characteristics gleaned from the backbone network.
    Head: The head component is in charge of calculating the probability for each
    class and object bounding boxes for each item in the picture. The final predictions
    are made using a number of layers, including Convolutional layers and Up sampling
    layers. Anchors: The anticipated bounding boxes are matched with the items in
    the image using anchors, which are preset bounding boxes. The YOLOv5 model’s performance
    may be adjusted by adjusting the number of anchors and their sizes, which are
    referred to as hyperparameters. Non-Maximum Suppression (NMS): NMS is used to
    remove overlapping bounding boxes and to produce a single bounding box for each
    object in the image. Loss Function: The YOLOv5 model uses a custom loss function
    that is designed to optimize the accuracy of the predictions and to ensure that
    the model is robust to variations in the input data. The YOLOv5 network uses a
    combination of convolutional neural networks (CNNs), non-max suppression, and
    anchor boxes to perform object detection. The network initially uses a number
    of convolutional and pooling layers to extract features from an input picture.
    To anticipate the bounding boxes surrounding the items of interest, these characteristics
    are subsequently input into a fully connected layer. The prediction also includes
    the object class probabilities. The prediction is made using the following equation:
    bbox(x)=sigmoid(target(x))+c_x bbox(y)=sigmoid(target(y))+c_y bbox(h)=exp(target(h))∗p_h
    bbox(w)=exp(target(w))∗p_w View Source where bbox(x), bbox(y), bbox(h), and bbox(w)
    are the predicted bounding box coordinates, sigmoid is the sigmoid activation
    function, exp is the exponential function, and target(x), target(y), target(h),
    target(w), c_x, c_y, p_h, and p_w are the network’s predicted values for the target
    coordinates, center coordinates, height and width scaling factors, respectively.
    The network also uses a loss function, typically mean squared error or cross-entropy,
    to measure the accuracy of its predictions and update its weights through backpropagation
    to improve its predictions over time. A. Experimentation A minimum of 8 GB GPU
    and 12 GB RAM are needed for this operation, which takes more time. It is thus
    recommended to use Google Collaboratory, which is accessible for free and employs
    cloud-based GPU and RAM. The YOLOV5 model trained with more than 500 epochs to
    get the best model. B. Dataset Used We have created a customized dataset which
    is a collection of all red chilies from the plants and individual chilies. The
    dataset consists of 1078 images where 75% (807 images) is used for training and
    25% (270 images) of images is used for testing Fig. 3. Sample Red Chilli’s Images
    Collected for Preparing Custom Dataset Show All C. Results and Discussions Several
    performance indicators, including efficiency, quality, recall, F1-score, including
    mean average precision, may be used to understand the outcomes of red chili identification
    using the YOLOV5 model. The accuracy of the model can be evaluated by the proportion
    of correctly classified chili images, while precision and recall can be used to
    evaluate the model’s ability to detect and classify red chilies in the image.
    The F1-score, which offers a general assessment of the model’s performance, is
    the harmonic mean of accuracy and recall. Object detection models are frequently
    evaluated using the mean average precision (MAP), which is a typical statistic.,
    which measures the average precision across all image-level predictions. Moreover,
    visualization of the bounding box predictions can also be helpful in analyzing
    the results of the model. By precisely drawing a bounding box around the red chili
    in the image and classifying it as a red chili, for instance, the model may be
    judged. Additionally, the model’s ability to handle different scales and orientations
    of red chilies can also be evaluated. The YOLOv5 model is capable of detecting
    red chilies in images, regardless of their orientation or position, whether they
    are positioned horizontally, vertically, or at an angle, and even if they are
    located on the plant. The following shows the detection of red chilies in different
    orientations. Fig. 4. Show All The results of the YOLOv5 model can be compared
    with other object detection models, such as Faster R-CNN, R-FCN, and RetinaNet,
    to determine its effectiveness in red chili detection. The following table shows
    the comparison of other object detection models with YOLOV5 model in red chili
    detection. TABLE I. Results Comparison Between Several Object Detection Models
    The following figure shows the comparison of other object detection models with
    YOLOV5 model in red chili detection. Fig. 5. Comparison of Results Among Different
    Object Detection Models Show All Multiple YOLOV5 model implementations exist;
    the following table compares the various YOLOV5 model implementations. TABLE II.
    Comparison of Different Variants of YOLOV5 Model The following graph represents
    the different varients of YOLO version 5 and the how properly then can work in
    real time applications. Fig. 6. Different Varients of YOLO Version 5 Show All
    The following figure shows the comparison of different variants of YOLOV5 model.
    Fig. 7. Comparison of Different Variants of YOLOV5 Model Show All The results
    show the YOLOv5 model’s performance and its suitability for the task of red chili
    detection in agriculture. The performance metrics, visualizations, and comparisons
    with other models can be used to further improve and optimize the model for more
    accurate and efficient red chili detection. SECTION V. Conclusions and Future
    Scope The Red Chili detection, aims to revolutionize the agricultural industry
    by providing a solution that saves time, reduces effort, and accommodates workers
    with physical limitations. The successful implementation of YOLOv5 as an object
    detection model for red chili detection is a testament to the potential of technology
    and computer science in advancing agriculture. The YOLOv5 architecture consists
    of several key components that work together to ensure accurate detection results.
    In conclusion, the Red Chili Plucking project is a step towards a more efficient
    and effective agricultural industry. In the current research, we developed a deep
    learning model for better detection of the red chilies from the chili plant images,
    provided a practical solution for some limitations such as hand burns, struggle
    with the cost of labour in agriculture etc. In future, a prototype can be specifically
    designed for plucking red chili, making the process quicker and more accurate.
    Further we plan to expand the prototype’s capabilities by incorporating features
    such as detecting plant illnesses, predicting crop yield, enabling night time
    operations, and creating similar prototypes for drone-assisted harvesting and
    various crops. By integrating technology and computer science into agriculture,
    we aim to streamline labour and bring benefits to farmers and the agricultural
    industry as a whole. Authors Figures References Citations Keywords Metrics More
    Like This Robustness of Deep Learning Methods for Occluded Object Detection -
    A Study Introducing a Novel Occlusion Dataset 2023 International Joint Conference
    on Neural Networks (IJCNN) Published: 2023 Object Detection approach for Crop
    and Weed Identification based on Deep Learning 2024 IEEE International Students''
    Conference on Electrical, Electronics and Computer Science (SCEECS) Published:
    2024 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 IEEE 8th International Conference for Convergence in Technology, I2CT
    2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Deep Learning Model YOLOv5 for Red Chilies Detection from Chilly Crop Images
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Raju R.
  - Thasleema T.M.
  citation_count: '1'
  description: Agriculture is considered to be a significant contributor to the global
    financial system and human diets. It has been recognized as the country's primary
    source of income and employment. So, it's really important to protect crops from
    various dangerous hazards, such as diseases, insects, bird and animal attacks,
    high atmospheric temperature, etc., and also from weak irrigation systems, poor
    soil quality, weeds management, etc. Specific insect attacks and diseases have
    long been a primary crop sector concern. Computer vision (CV)-based automatic
    insect and disease detection methods are used in smart farming systems because
    of their high cost-effectiveness and efficient automation. This paper gives an
    overview of the use of Machine Learning (ML), Deep Learning (DL), and the Internet
    of Things (IoT) in agriculture to protect crops from various dangerous hazards
    and proposes an automatic Animal-Repelling System (ARS). This study implements
    a system based on IoT to protect crops from animals. The proposed low-cost agricultural
    field protection system helps farmers to protect their crops and increase production
    yield and income.
  doi: 10.1109/INOCON57975.2023.10100983
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 2nd International Confer...
    An IoT Solutions for Ungulates Attacks in Farmland Publisher: IEEE Cite This PDF
    Ratheesh Raju; T M Thasleema All Authors 1 Cites in Paper 72 Full Text Views Abstract
    Document Sections I. Introduction II. Intelligent Systems On Precision Agriculture
    III. Proposed Yolov5 Based Automatic Animal Repelling System IV. Results and Discussions
    V. Conclusion Authors Figures References Citations Keywords Metrics Abstract:
    Agriculture is considered to be a significant contributor to the global financial
    system and human diets. It has been recognized as the country’s primary source
    of income and employment. So, it’s really important to protect crops from various
    dangerous hazards, such as diseases, insects, bird and animal attacks, high atmospheric
    temperature, etc., and also from weak irrigation systems, poor soil quality, weeds
    management, etc. Specific insect attacks and diseases have long been a primary
    crop sector concern. Computer vision (CV)-based automatic insect and disease detection
    methods are used in smart farming systems because of their high cost-effectiveness
    and efficient automation. This paper gives an overview of the use of Machine Learning
    (ML), Deep Learning (DL), and the Internet of Things (IoT) in agriculture to protect
    crops from various dangerous hazards and proposes an automatic Animal-Repelling
    System (ARS). This study implements a system based on IoT to protect crops from
    animals. The proposed low-cost agricultural field protection system helps farmers
    to protect their crops and increase production yield and income. Published in:
    2023 2nd International Conference for Innovation in Technology (INOCON) Date of
    Conference: 03-05 March 2023 Date Added to IEEE Xplore: 19 April 2023 ISBN Information:
    DOI: 10.1109/INOCON57975.2023.10100983 Publisher: IEEE Conference Location: Bangalore,
    India SECTION I. Introduction The world’s population is enormously increasing,
    and ensuring food security for this growing population is a serious problem. According
    to the Food and Agriculture Organization (FAO) of the United Nation, more than
    815 million people are chronically hungry, with Asia accounting for 64% of the
    total of the total [1]. However, increasing agriculture or food production rapidly
    to meet the growing food supply demands is not easy, because of several problems
    such as defunct agricultural techniques, inadequate storage, economies, and political
    crisis. The food and agricultural organizations calculate that agriculture production
    will need to increase by 70% by 2050 to feed the world’s growing population. It
    is not enough to simply feed people; we must also make sure that offer them extremely
    nutritious food without any environmental damage [2].To convey economical agrarian
    production, the agriculture sector employs cutting-edge technologies like Artificial
    Intelligence (AI) [3],Machine Learning (ML) and Internet of Things (IoT) [4].
    The automation of agricultural production has empowered the consistent observation
    of yield development and weeds management [5], as crop and plant defense against
    insects, rodents, birds, and other predators. This gives precise and effective
    solutions to agrarian activities contrasted with the customary strategies performed
    physically [6]. Precision Agriculture (PA) is a farming technique that uses Information
    and Communication Technology (ICT) to collect valuable data from many sources
    to improve crop cultivation processes. So, each farming unit’s spatial and temporal
    variation must be identified and reported [7]. It also helps all production strategies
    using ICT to optimize supply usage to obtain the desired items or to track the
    outcome. Variable Rate Technology (VRT), Yield Monitoring (YM), and numerous types
    of sensors are just a few examples of ICT for PA [8]. Managing the relationship
    with factors outside the agricultural ecosystem, such as wildlife, is a relevant
    open challenge in this dynamic situation. Protecting crops from wild animals is
    also one of the primary concerns. Indeed, in the last three decades, the amount
    of damage inflicted by predatory wild animals has grown exponentially worldwide
    [6]. PA’s core objectives include protecting crops from pests, diseases, animals,
    birds, and weeds and managing irrigation [9]. Furthermore, PA enhances crop productivity
    by minimizing complexity and production costs [10]. In addition, it has the potential
    to reduce costs by only applying fertilizer where it is needed, based on soil
    survey and yield data analysis, and to improve the management of water resources,
    optimizing performance through mechanized reaping procedures [11]. The main objective
    of this review paper is to identify the significant challenges in the agriculture
    industry and provides researchers and readers with the current advancement in
    PA. Conventional agricultural practices are facing various difficulties such as
    the excessive use of pesticides, lack of knowledge about the climate and environmental
    conditions, various animal and insect attacks, etc., This analysis covers every
    aspect of the use of technology and innovation in agriculture that is required
    to monitor crop health and productivity and various strategies to increase the
    plant life span. In the proposed study, we conducted this experiment in Pullur-Periya
    gramapanchanchayath of Kasaragod’s district, Kerala. This region is slightly Endosulfan-affected,
    and also faces various issues such as crop and plant diseases, animal, bird, and
    insect infestations, a severe water shortage, extremely poor soil quality, and
    extremely high temperatures. This paper is organized as follows: the next section
    covers recent similar research works in precision agriculture, section 3 proposes
    a new method to protect agricultural land and crops from different animal attacks
    and section 4 summarizes the findings and discussion, and section 5 concludes
    the article with future directions, and section 6 includes the references. SECTION
    II. Intelligent Systems On Precision Agriculture A. Internet of Things Smart farming
    systems based on IoT are quickly gaining popularity because they use low-cost
    sensors to provide real-time status of environmental variables relevant to the
    crop [12]. Throughout the growing and harvesting cycle, the IoT provides a platform
    for smart agriculture, wireless connection of several soil sensors and context-aware
    sensors, different hardware, and data analytical applications to enhance farmers’
    ability to resolve complicated agricultural issues such as irrigation evaluation,
    soil preparation, yield prediction, and so on [13]. 1) Irrigation System: In the
    current scenario, Irrigation is scheduled all around the world based on farmers’
    crop regular inspection, and as a result, the traditional irrigation system wastes
    nearly half of the water it uses. So, controlled irrigation methods such as Sprinkle
    irrigation, drip irrigation, and furrow irrigation are used reduce water waste
    by 30-70% [14]. In [15] proposes a deep learning neural network supported Internet
    of Things (IoT) enabled intelligent irrigation system. Smart irrigation is a farming
    strategy that combines water management into the cultivation process. Wireless
    technology and internet of things technologies are often used in the deployment
    of smart irrigation systems [16], [17]. IoT technologies can be used in a variety
    of farming sectors, including irrigation, fertilization, plant growth, weed control,
    and more, by adapting various relevant technologies such as wireless sensor networks,
    big data, communication protocols, edge computing, and so on [18]. By developing
    an in-house Wireless Sensor and Actuator Network (WSAN) design and communication
    protocol, [19] deploying WSAN for agricultural irrigation and control. Similarly,
    [20] reported an IoT-based system in which images were analysed for disease identification
    and soil moisture and humidity sensors were applied to monitor irrigation requirements.
    IoT is considered to be one of the leading technologies that will transform traditional
    farming into new aspects of precision agriculture intelligence [18], [21]. 2)
    Disease Detection: To enhance agricultural yield, early detection of crop diseases
    and deployment of management strategies are extremely desirable, Sensor-based
    innovations are vital and plays a critical role in the early stages of diagnosis
    of diseases [22]. [22] proposes an in-house IoT-enabled and connected system On
    flexible substrates, an economical digital Leaf Wetness Sensor (LWS) is employed
    for integrated plant disease control. [23] proposes a real-time data collection
    IoT-based automated system for cotton crop monitoring. The suggested system also
    includes the implementation of a wireless sensor network (WSN) in the cotton fields
    for crop health monitoring and recording. The Waspmote agriculture sensor board,
    which includes temperature and humidity sensors, soil moisture sensors, and leaf
    wetness sensors, were employed. 3) Crop yield prediction: GreenDrone, an inexpensive
    low-altitude remote sensing platform created for monitoring the maize crop, has
    mentioned in [24]. This system included a big, robust fixed-wing aircraft with
    a Canon camera and a FLIR thermal camera for calculating indices like the Normalized
    Difference Vegetation Index (NDVI) and Water Stress Index (WSI). A number of aircraft
    operations were conducted to scan the test region at various phases of crop development.
    The NDVI and NGB (Near-infrared Green Blue) images were created from the collected
    photographs, which helped to detect regions with low yield potential, and areas
    with varying plant densities, and indicate unequal nitrogen and water management
    concerns. Another study [25] combines WSN and drone technologies to construct
    a crop healthcare monitoring system. The focus of this study is the creation of
    run-time sensor clusters while taking into account aspects such as run-time data
    acquisition, the scanned area, the unavailability of a suitable number of nodes,
    and the drone’s dynamic flight track. [26], which employs a UAV and WSN integrated
    approach. Images and real-time data were collected using the drone and the WSN,
    accordingly. The focus of the work was on drone route optimization for WSN data
    gathering. WSN data is gathered and sent to the cloud, where it is analyzed by
    the end-user. Table I summarises the implementation of IoT in different agricultural
    applications SECTION III. Proposed Yolov5 Based Automatic Animal Repelling System
    An ARS system is proposed to protect crops and agricultural land from various
    animals, birds, and insect attacks. The system consists of hardware components
    such as Raspberry Pi, a PIR sensor, a low-cost Raspberry compactable camera, and
    a water pump. The proposed system identifies the presence of ungulates, birds,
    and flies near the agricultural field. When they are near or enter the into fields,
    the PIR sensors will detect the motion of that attacker, which will inform the
    Raspberry Pi to turn on the camera. The camera then detects that attacker based
    on the Object detection algorithm YOLOv5. Then Raspberry Pi will enable the buzzer
    to produce the corresponding frequency of signals which will cause the detected
    animal, bird, or fly to repel away from the fields. To categorize the animals,
    the COCO (Common Objects in Context.) dataset is used in this work. As hinted
    by the name, images in the COCO dataset are taken from everyday scenes, thus attaching
    “context” to the objects captured in the scenes. COCO was an initiative to collect
    natural images, the images that reflect everyday scenes and provide contextual
    information. In the everyday scene, multiple objects can be found in the same
    image, and each should be labeled as a different object and segmented properly.
    The COCO dataset provides the labeling and segmentation of the objects in the
    images. The dataset contains 80 classes, the targeted ten classes containing a
    variety of commonly seen animals. The proposed system makes use of a quad-core
    CortexA72 Raspberry Pi 4 B from Broadcom (ARM v8) 1GB, 2GB,4GB, or 8GB LPDDR4-3200
    SDRAM, 40-pin GPIO standard header for the Raspberry Pi (fully backward compatible
    with previous boards) Micro-HDMI ports in two (up to 4kp60 supported) MIPI DSI
    display port with two lanes 2 lanes for MIPI CSI cameras. 8 Mega Pixel resolution
    camera is used here. When the PIR sensors are triggered, this camera is used to
    record live videos of the field. Even in poor light, it can capture 1080p at 30
    FPS. PIR sensors, often known as passive infrared sensors, can detect infrared
    radiation levels. They contain two knobs; one is to delay the time of the signal
    generated and motion detected. And another knob is to set the sensitivity of the
    sensor and a buzzer is also used, an audio signaling device capable of producing
    a frequency from 64Hz to 67Khz. To monitor the moisture content of the soil, soil
    sensors are used. Table I Various Monitoring and Control Strategies Using IOT
    Object detection is the core aspect of the proposed approach. The ungulates that
    the PIR sensors detect are identified by using the YOLOv5 object detection algorithm.
    The YOLO algorithm is based on convolutional design, where a grid system is used
    to partition an input image into candidate regions for object detection. Each
    grid cell represents a candidate region of discovered items. The ability to complete
    the surveys all at once is the key innovation that YOLO brings, making it quick
    and effective. Additionally, YOLO has the advantage of being able to forecast
    a huge fixed number of items while also establishing a cutoff point to reject
    predictions with low probabilities. SECTION IV. Results and Discussions The attacks
    of ungulates in agriculture have always been a significant problem in the agriculture
    sector. This study provides an innovative and efficient solution for this problem.
    The method discussed here would not cause any harm to animals or humans but can
    be used to repel the ungulates efficiently. The YOLOv5 object detection algorithm
    proposed here is considered the State-of-the-Art Object detection algorithm and
    is so fast that it has become a standard way of detecting objects in the field
    of computer vision. All the experimental analysis has been done by using the COCO
    dataset. In this paper, a real-time preliminary analysis is carried out by capturing
    the movement of a dog near the field. This real-time ARS detects the presence
    of dogs near the agricultural field, this pre-trained model provides great accuracy
    in detecting ungulates. The real-time detection of ungulates near the field is
    depicted in Fig. 1, the detected object is bounded by a box. On the top of that
    bounding box, the category name and accuracy of that object are mentioned. Fig.
    1.(a), contains the dog image with a left-side view and the head is not visible;
    the image shows an accuracy of only 57% because of the position and invisibility
    of the head. In Fig 1. (b), the front view of the dog is detected, and the accuracy
    of the detected dog is slightly increased because the face is visible. In Fig
    1. (c), back and left side view is detected and it shows 85% accuracy, which is
    greater compared to the other two images, Fig. 1. (d), has a back view of the
    dog and, Fig. 1. (e), has a back and right-side view of the dog, which indicates
    91% and 93% accuracy respectively because of the good gestures and clear vision
    of the dog in the image. The back and left-turned view of the dog image is present
    in 1. (f), which is misclassified as a cow with 43% of accuracy because of the
    low visibility of the head, and 1. (g), provides a side view of the dog with the
    head-turned back and legs straight, due to the low visibility of the head and
    straight leg position causes the dog image to which be misclassified as sheep
    with 62% accuracy. This result indicates that the slight movement of the object
    leads to the wrong classification of the object. TableII summarizes the ungulate’s
    various positions and detection accuracy. After the detection and classification
    of objects as dogs, raspberry pi enables the buzzer, and it automatically generates
    a sound with 25KHz frequency, this high-frequency sound repels the dog from the
    agricultural field. This system provides good accuracy in repelling the animal
    from the farmland. Fig. 1: Detecting objects and classifying with accuracy. Show
    All Table II Ungulates Detection Accuracy in Various Positions SECTION V. Conclusion
    This research work provides an overview of advanced control techniques in Precision
    Agriculture that have been discussed by various researchers. The selected articles
    discuss advanced control strategies using spectral imaging, IoT sensors, and artificial
    intelligence-based methods to address issues in agriculture such as yield improvement,
    disease detection, animal and insect attacks, etc. And also helps to understand
    the current status of the traditional agricultural system. Smart agriculture is
    a farming management technique that uses information technology to boost agricultural
    yield and keeps plants healthy. IoT and AI-based systems are particularly effective
    for automatically monitoring agriculture fields for important challenges, such
    as disease identification, pesticide control, crop yield increases, and weed and
    irrigation management. The “Precision Agriculture” system recognizes the early
    indications and symptoms of various problems, which helps farmers avoid large
    financial losses. In the agricultural industry, many research projects are being
    carried out to accomplish the ultimate goals. The proposed IoT-based method to
    protect crops from various animal and insect attacks is very effective. The system
    repels the ungulates from the field without any harm to them. In the future, we
    can extend this to identify the presence of all the ungulates, birds, and quickly
    if it is far away from the field, this helps to repel the attackers from the field
    at the earliest, and also, we can use more sensors such as soil, temperature,
    etc. with the help of these sensors, we can reduce the usage of water and improve
    soil quality. Authors Figures References Citations Keywords Metrics More Like
    This Soil Macro-Nutrients Detection, Crop and Fertilizer Recommendation with Irrigation
    System 2023 International Conference on Advances in Electronics, Communication,
    Computing and Intelligent Information Systems (ICAECIS) Published: 2023 Arduino-based
    smart irrigation using water flow sensor, soil moisture sensor, temperature sensor
    and ESP8266 WiFi module 2016 IEEE Region 10 Humanitarian Technology Conference
    (R10-HTC) Published: 2016 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 2nd International Conference for Innovation in Technology, INOCON
    2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An IoT Solutions for Ungulates Attacks in Farmland
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Elbasi E.
  - Mostafa N.
  - Alarnaout Z.
  - Zreikat A.I.
  - Cina E.
  - Varghese G.
  - Shdefat A.
  - Topcu A.E.
  - Abdelbaki W.
  - Mathew S.
  - Zaki C.
  citation_count: '20'
  description: Due to the increasing global population and the growing demand for
    food worldwide as well as changes in weather conditions and the availability of
    water, artificial intelligence (AI) such as expert systems, natural language processing,
    speech recognition, and machine vision have changed not only the quantity but
    also the quality of work in the agricultural sector. Researchers and scientists
    are now moving toward the utilization of new IoT technologies in smart farming
    to help farmers use AI technology in the development of improved seeds, crop protection,
    and fertilizers. This will improve farmers' profitability and the overall economy
    of the country. AI is emerging in three major categories in agriculture, namely
    soil and crop monitoring, predictive analytics, and agricultural robotics. In
    this regard, farmers are increasingly adopting the use of sensors and soil sampling
    to gather data to be used by farm management systems for further investigations
    and analyses. This article contributes to the field by surveying AI applications
    in the agricultural sector. It starts with background information on AI, including
    a discussion of all AI methods utilized in the agricultural industry, such as
    machine learning, the IoT, expert systems, image processing, and computer vision.
    A comprehensive literature review is then provided, addressing how researchers
    have utilized AI applications effectively in data collection using sensors, smart
    robots, and monitoring systems for crops and irrigation leakage. It is also shown
    that while utilizing AI applications, quality, productivity, and sustainability
    are maintained. Finally, we explore the benefits and challenges of AI applications
    together with a comparison and discussion of several AI methodologies applied
    in smart farming, such as machine learning, expert systems, and image processing.
  doi: 10.1109/ACCESS.2022.3232485
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 11
    Artificial Intelligence Technology in the Agricultural Sector: A Systematic Literature
    Review Publisher: IEEE Cite This PDF Ersin Elbasi; Nour Mostafa; Zakwan AlArnaout;
    Aymen I. Zreikat; Elda Cina; Greeshma Varghese; Ahmed Shdefat; Ahmet E. Topcu;
    Wiem Abdelbaki All Authors 26 Cites in Papers 11104 Full Text Views Open Access
    Comment(s) Under a Creative Commons License Abstract Document Sections I. Introduction
    II. Systematic Review Methodology III. Background on Artificial Intelligence IV.
    AI in Agriculture V. Robotics in Agriculture Show Full Outline Authors Figures
    References Citations Keywords Metrics Abstract: Due to the increasing global population
    and the growing demand for food worldwide as well as changes in weather conditions
    and the availability of water, artificial intelligence (AI) such as expert systems,
    natural language processing, speech recognition, and machine vision have changed
    not only the quantity but also the quality of work in the agricultural sector.
    Researchers and scientists are now moving toward the utilization of new IoT technologies
    in smart farming to help farmers use AI technology in the development of improved
    seeds, crop protection, and fertilizers. This will improve farmers’ profitability
    and the overall economy of the country. AI is emerging in three major categories
    in agriculture, namely soil and crop monitoring, predictive analytics, and agricultural
    robotics. In this regard, farmers are increasingly adopting the use of sensors
    and soil sampling to gather data to be used by farm management systems for further
    investigations and analyses. This article contributes to the field by surveying
    AI applications in the agricultural sector. It starts with background information
    on AI, including a discussion of all AI methods utilized in the agricultural industry,
    such as machine learning, the IoT, expert systems, image processing, and computer
    vision. A comprehensive literature review is then provided, addressing how researchers
    have utilized AI applications effectively in data collection using sensors, smart
    robots, and monitoring systems for crops and irrigation leakage. It is also shown
    that while utilizing AI applications, quality, productivity, and sustainability
    are maintained. Finally, we explore the benefits and challenges of AI applications
    together with a comparison and discussion of several AI methodologies applied
    in smart farming, such as machine learning, expert systems, and image processing.
    Due to the increasing global population and the growing demand for food worldwide
    as well as changes in weather conditions and the availability of water, artificial
    intel...View more Published in: IEEE Access ( Volume: 11) Page(s): 171 - 202 Date
    of Publication: 26 December 2022 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2022.3232485
    Publisher: IEEE CCBY - IEEE is not the copyright holder of this material. Please
    follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain
    full-text articles and stipulations in the API documentation. Nomenclature Abbreviations
    Definition AI Artificial Intelligence. IoT Internet of Things. FAO The Food and
    Agriculture Organization. SVM Support Vector Machine. KNN K-Nearest Neighbor.
    UAVs An-manned Aerial Vehicles. WSN Wireless Sensor Network. LEACH Low Energy
    Adaptive Clustering Hierarchy. PEGASIS Power Efficient Gathering in Sensor Information
    Systems. Wi-Fi Wireless Fidelity. WiMAX Worldwide Interoperability for Microwave
    Access. WPAN Wireless Personal Area Network. LoRaWAN Long Range Wide Area Network.
    SigFox French global network operator. LPWA Low Power Wide Area. LTE Long Term
    Evolution. SaaS Software as Service. PaaS Platform as Service. CNNs Convolutional
    Neural Networks. UAV Unmanned Aerial Vehicle. GPS Global Positioning System. GHG
    Greenhouse Gas. ZigBee A Zonal Intercommunication Global-standard. NodeMCU Node
    Microcontroller Unit. ARM Advanced RISC Machine. SMS Short Message Service. M2M
    Machine-to-Machine Communication. ELSCP Enhanced locally selective combination
    in parallel. AUCPR Area under the precision-recall curve. WDSs Water distribution
    systems. UFMNet Ultrasonic Flow Metering Network. SECTION I. Introduction Smart
    farming applies information technologies for the optimization of complex farming
    systems. It incorporates information and communication technologies to improve
    agriculture production system. The agricultural sector is one of the most important
    production sectors. It is concerned with all aspects of agricultural activities
    and is divided into the four major subsectors of crops, forestry, livestock (production
    and animal health), and aquaculture. Artificial intelligence (AI) encompasses
    a broad range of applications in the field of computer science related to the
    possibility of building smart machines, robots, or sensors that are capable of
    simulating human actions to achieve tasks on behalf of humans to serve society
    intelligently. These actions are controlled by application programs using information
    technology devices. Combining AI approaches and traditional agricultural methods,
    smart agriculture is being used to improve national economies by monitoring crop
    growth using the principles of precision farming [1]. With these strategies, together
    with the help of machine learning, the Internet of Things (IoT), and cloud computing,
    all environmental features can be monitored to choose the best environment for
    each type of crop through the classification of the gathered data using one of
    the available classification techniques. The Internet of things (IoT) is a system
    of interrelated physical devices or objects with sensors, processing ability,
    software, and other technologies that connect and exchange data with other devices
    and systems over the Internet or other communications networks without requiring
    human-to-computer or human-to-human interaction. Smart irrigation is another new
    technique in agriculture to help farmers in automating irrigation processes by
    collecting data using smart devices such as Raspberry Pi [2]. The collected data
    are then analyzed to select the best technique for switching the flow of water
    on the farm to the ON or OFF state. Therefore, smart irrigation system provides
    the agriculture sector and farmers with many benefits such as: Cost savings due
    to minimized water waste Reduced human efforts A unified view of soil characteristics,
    including moisture and nutrient contents Smart notifications in case of abnormalities
    Better long-term landscape health IoT ecosystem for smart irrigation AI and machine
    learning can be used to monitor crops and soil health on a real-time basis, allowing
    companies to estimate crop yields and predict the best time for harvesting to
    maximize profit. Similarly, the early classification of plant diseases will help
    farmers use the best strategies to fight them. Using sensors, machine vision,
    AI models, and robots makes it possible to perform harvesting processes on behalf
    of workers with greater accuracy and speed. Moreover, it helps reduce the wastage
    of crops in the field that is experienced with the traditional method of harvesting.
    Using artificial intelligence techniques and tools, it is possible to predict
    the best time to fertilize fields and sow seeds to achieve maximum yield and better
    prices in a proper time and manner. The spraying of chemicals is considered an
    important method to control pest insects, fungi, and bacterial diseases of plants.
    Applications of artificial intelligence in agriculture are divided into different
    types of activities that can be handled using AI as follows: Crop and soil monitoring
    (monitor the crop health). For example, AI and machine learning can be used to
    monitor crops and soil health on a real-time basis, allowing companies to estimate
    crop yields and predict the best time for harvesting to maximize profit. Disease
    diagnosis (early classification of plant diseases helps to use the proper strategy
    to fight it). The early classification of plant diseases will help farmers use
    the best strategies to fight diseases. Agriculture robot (tackling the labor challenges).
    Using sensors, machine vision, AI models, and robots makes it possible to perform
    harvesting processes on behalf of workers with greater accuracy and speed. Predictive
    insights (Enables right decision-making) Crop yield prediction (predicting the
    best time to sow). Using artificial intelligence techniques and tools, it is possible
    to predict the best time to fertilize fields and sow seeds to achieve maximum
    yield and better prices in a proper time and manner. Intelligent spraying (allows
    for cost savings). Intelligent spraying helps to reduce the wastage of crops in
    the field that is experienced with the traditional method of harvesting. The spraying
    of chemicals is considered as an important method to control pest insects, fungi,
    and bacterial diseases of plants. According to United Nations reports [3], the
    world’s population will rapidly increase from 7.8 billion in 2020 to around 11
    billion in the upcoming years. As predicted by the FAO, the global population
    will reach 8 billion people by 2025 and 9.6 billion by the end of 2050 [4]. Therefore,
    global food production must be increased to satisfy the huge increase in population.
    This growing demand for food cannot be met by using traditional farming techniques,
    as farmers not only need to increase their productivity; they also need to provide
    food of better quality for their customers. Parasuraman et al. [5] emphasized
    the fact that because the total population is expanding extremely rapidly and
    the demand for food is increasing alongside the increase in population, different
    classes of IoT applications, robotization, machine learning, deep learning, and
    AI techniques should be utilized effectively to increase not only the production
    of food but also the quality of the produced food. In this regard, they proposed
    a framework for utilizing classifier models to support better detection of crop
    diseases and reported that the proposed detection algorithm achieved 99.96% accuracy.
    The contribution of this research work can be summarized as follows: The survey
    is addressing how researchers can utilize AI applications effectively in data
    collection using sensors, smart robots, and monitoring systems for crops and irrigation
    leakage. It is also shown that while utilizing AI applications, quality, productivity,
    and sustainability are maintained. This survey contributes to knowledge through
    the identification of the gaps and challenges in existing research in smart farming.
    The provided discussions and comparisons between different AI methodologies applied
    in smart farming such as machine learning, expert systems, and image processing
    will create new opportunities for researchers to conduct new research tracks in
    this area of research. In this survey, the provided information on the application
    of smart agricultural technology would help different countries especially developing
    countries to improve the quality of the agriculture sector to achieve farm sustainability
    in those countries. In this survey we listed, compared and classified the existing
    studies based on new criteria of classification: we did a comparison based on
    the type of sensors, protocols, wireless communication technologies as well as
    applications. We have presented a classification of the publications examined
    in the three dimensions (Benefits, Challenges and Methodology). We have proposed
    a categorization of each dimension according to the context and the field of application
    of the publications. In-depth classification is also introduced to distinguish
    publications according to their types (automated decision-making, farm tracking,
    workflow assistant, etc.). We investigated many different domains such as IoT,
    and sensors including networks, communication protocols, cloud computing services,
    image processing, data collection, crop, and livestock monitoring. Moreover, different
    types of challenges, for example, data accuracy, security, network, and data transmission
    may exist in the smart farms and are investigated fully to provide a sustainable
    and good productive environment for the farms. Some scientists have performed
    research based on Artificial Intelligence (AI) to resolve farmers’ problems. But
    they do not have a comprehensive review study to include many different technologies
    as discussed in the paper. To fill in this gap, this research provides a systematic
    literature review on AI in the agricultural sector based on 190 research publications
    mostly from recent years. This evaluation not only provides effective direction
    to the farmers but also shows possible models and implementations using AI techniques
    using different technologies. The remainder of this article is organized as follows.
    In Section II, a systematic review of the methodology is presented. Background
    information about AI is presented in Section III. An extensive literature review
    is offered in Section IV, and in Section V, a general discussion of various related
    topics is presented, including the benefits and challenges of several techniques,
    the classification of previous studies, and a comparison of several AI methodologies
    applied in smart farming, followed by the conclusions of this work. SECTION II.
    Systematic Review Methodology Agriculture and food industries are considered among
    the most critical fields around the world. This sector can take advantage of AI
    and its subfields such as machine learning, computer vision, and image processing
    to solve many emerging problems. In these processes, IoT equipment can be used
    to collect helpful information from raw data on farms regarding agriculture and
    irrigation, while AI techniques can be utilized during preproduction (crop yield
    and finding irrigation leaks), production (disease detection and weather prediction),
    processing (product estimation), and distribution (storage and consumer analysis).
    In a systematic literature review, authors should search, understand, and classify
    the current research works in the area of interest and then perform analysis and
    draw conclusions based on their findings. For the present literature review, we
    searched journal papers, conference papers, and book chapters addressing AI and
    IoT applications in agriculture. We focused on high-quality papers indexed by
    IEEE Xplore, Clarivate, and Scopus. Figure 1. Systematic literature review phases
    show the review process followed for this paper. FIGURE 1. Systematic literature
    review phases. Show All FIGURE 2. ML Algorithm – categorization. Show All A systematic
    review is completed with three phases: planning, execution, and reporting. In
    the planning phase, the reasons for conducting a literature review in a given
    area are considered. In the present case, several emerging applications are discussed,
    such as crop and livestock monitoring, abnormal activity detection, irrigation
    leakage detection, monitoring, remote operations, and productivity. In addition
    to these application areas, the most useful AI methodologies are discussed, such
    as machine learning, expert systems, the IoT, and image/video processing. The
    selected papers are categorized based on the dimensions of benefits, challenges,
    and methodologies. In the execution phase of a systematic review, keywords are
    selected for each subtopic and then filters are applied. In the present case,
    most of the publication searches were performed manually in databases such as
    IEEE Xplore, Scopus, and Clarivate. Papers were selected from lists based on their
    titles, years, abstracts, conclusions, and publication sources. Finally, the authors
    read the full papers, filled out related tables, and analyzed the information.
    In the reporting phase of this systematic review, papers were classified into
    the three dimensions; benefits, challenges, and methodologies. Figure 3 illustrates
    the classification of AI applications in agriculture. AI technologies have benefited
    farmers, companies, and other members of the agricultural sector. Automated decision-making,
    monitoring, observation of irrigation leakage, and remote operations are some
    examples of the benefits for farmers. AI is also beneficial for companies and
    other organizations in terms of improving performance, cost and time optimization,
    quality and productivity improvement, security, and data collection. FIGURE 3.
    Taxonomy of AI-based smart farming systems. Show All Developing and using AI technology
    for farms also has some challenges including data collection, availability, and
    integration. Weather conditions, human interventions, government regulations,
    and privacy are some of the other challenges related to data collection. Most
    of the data need preprocessing before any image processing or machine learning
    can be applied. Methodologies can be categorized into the four main groups of
    the IoT, machine learning, expert systems, and image/video processing. While other
    AI methods do exist, these four categories constitute the most recent and frequently
    used techniques. Machine learning methods can be used for classification, clustering,
    decision-making, and optimization. Expert systems are often used in recommendation
    systems, decision-making, rule-based decisions, and fuzzy problems, as shown in
    Figure 2. Taxonomy of AI-based smart farming systems. SECTION III. Background
    on Artificial Intelligence AI is an emerging topic of importance in the field
    of computer science. Computers and machines use AI methodologies to understand,
    analyze, and learn from data. There are many application areas for AI, such as
    robotics, e-commerce, social media, computer vision, face recognition, healthcare,
    agriculture, military usage, and games, and AI methodologies are also used in
    smart farming. Machine learning, smart sensors, image processing, computer vision,
    and expert systems methodologies can be used to solve problems in agriculture.
    AI information systems improve the quality, productivity, and sustainability of
    farming. A. Machine Learning in the Agricultural Sector Machine learning is a
    part of AI technology and it contributes to the agricultural sector by monitoring
    and controlling agricultural activities, thereby increasing productivity and improving
    the quality of the crops that are cultivated. Machine learning algorithms play
    essential roles in precision agriculture by detecting objects in agricultural
    fields. Treboux and Genoud [6] showed 94.27% accuracy with machine learning algorithms
    in detecting specific objects, clearly reflecting the immense impact of these
    applications in smart farming. Machine learning algorithms allow machines to learn
    about particular agricultural lands, the geographical structure of farming areas,
    and plants and crops using supervised and unsupervised learning methods. Datasets
    are organized and predefined in the former case, whereas datasets are not classified
    in the latter. Once the machine has learned about agricultural activities, it
    can perform actions such as monitoring and predicting temperature and humidity,
    soil moisture, crop yield, and plant diseases [7]. Simulatenously, machine learning
    algorithms are used to classify various agricultural datasets according to soil
    and land types. Such classifications help farmers select suitable crops. Machine
    learning algorithms such as random forest, naive Bayes, and K-means can classify
    these datasets to predict the most suitable crops for each area [8]. Applying
    these techniques will undoubtedly assist farmers in different agricultural activities
    for efficient and cost-productive crop production. Therefore, machine learning
    in the agricultural sector is applied with the aim of developing computer programs
    that can handle the input data to make predictions such as the most ideal time
    for sowing or harvesting, irrigation methods and levels, selection of soil type,
    temperature, and plants. These inputs train the machine learning model to make
    appropriate decisions in the field, thereby helping farmers identify ideal farming
    opportunities. The selection of machine learning algorithms is highly dependent
    on the availability of data, size of the training data, accuracy and/or interpretability
    of the output, speed or training time, linearity, number of features and the modeling
    process involves regression, classification, learning, and clustering. In smart
    farming, machine learning systems work with the help of computer vision techniques
    (such as Image Classification, Object Detection, Panoptic Segmentation and Keypoint
    Detection) to recognize and evaluate various objects in an agricultural field.
    The data can be acquired through different sensors to be used in modeling the
    system, including training and testing with various machine learning algorithms.
    For example, to maintain controlled water irrigation, an automatic drip irrigation
    system can be implemented and controlled based on data such as temperature, light,
    humidity, and rain captured using various sensors in the field [9]. Furthermore,
    the support vector machine (SVM) algorithm is identified as one of the best classification
    algorithms and accuracy rates of 90%-97% were found in various studies where it
    was used to detect diseases in certain plants. These studies showed that the K-nearest
    neighbor (KNN) and SVM algorithms are suitable for classifying data and producing
    excellent overall accuracy [10]. Figure 3. ML Algorithm – categorization shows
    a brief categorization of machine learning algorithms based on their behaviors
    in the machine learning modeling process. They are divided into supervised and
    unsupervised learning categories. Meanwhile, supervised algorithms produce output
    based on organized input data where the datasets are clearly labeled, classification
    algorithms can predict or classify data based on categories, such as male/female
    or spam mail/not spam, and KNN, decision tree, random forest, and SVM are examples
    of classification algorithms. Regression algorithms will predict continuous data
    or series of data such as salary and age. Simple linear regression, logistic regression,
    and multiple linear regression are examples of regression algorithms. Unsupervised
    learning algorithms are used when datasets are not labeled or organized. The machine
    learning model will learn from the dataset to identify an unknown object, such
    as identifying a person from a collection of image patterns. Clustering algorithms
    are used to form a structure for these uncategorized data. Upon identifying the
    pattern, the algorithm will group them into different clusters [11]. To illustrate
    the ML process and its features, Figure 4. ML Model for agriculture sector shows
    a machine learning model for the agricultural sector, consisting of three parts.
    The “input” part collects the required data from an agricultural field for data
    processing. Various types of IoT sensors and manually entered datasets are the
    primary resources for machine learning systems to train models. The collected
    datasets will be categorized as labeled or unlabeled in machine learning systems
    based on the data processing outcome. Some datasets will be separated for testing
    and classifications, while other sets will be used for making predictions with
    appropriate machine learning algorithms. The generated results in the “output”
    part can be analyzed further to improve system performance or for further related
    studies. FIGURE 4. ML Model for agriculture sector. Show All B. Internet of Things
    in Smart Farming In this section, we provide an overview of categories of sensors,
    IoT sensor types used in smart farming, wireless sensor networks, and IoT protocols
    used in smart farming. 1) Primitive Sensors Vs. Smart Sensors A sensor is defined
    as any device that can detect and measure different types of physical properties
    and quantities, such as wind speed and direction, air pressure, light, humidity,
    heat, and many other physical variables. The input value read by the sensor results
    in an electrical signal that is usually transmitted to a microcontroller and then
    makes its way to a network interface for further processing. The evolution from
    primitive to smart sensors allowed a leap in how data are collected from the environment,
    processed, and used in making decisions for further investigations. IoT smart
    sensors can connect huge numbers of smart systems that help us develop smart solutions
    for emerging problems [12], [13]. Figure 5. Primitive sensor shows a block diagram
    for a primitive sensor, which basically senses a physical attribute, and then
    the resulting signals are manipulated for further processing and sent out as an
    analog current. Technological advancements have improved modern sensors in terms
    of how they convert the physically sensed data; signals are conditioned and converted
    to digital format, becoming input for an algorithm for processing and then being
    sent to the transceiver unit as illustrated in Figure 6. Smart sensor. A smart
    sensor usually consists of the following: A sensing device that measures a physical
    attribute (heat, humidity, etc.). Signal conditioning to translate the sensed
    signal into data. A connected processing unit with memory and a user interface.
    This unit is loaded with an algorithm to process digital data. A transceiver unit
    to exchange information with the gateway/sink sensor node. 2) Sensor Types for
    Smart Farming Innovation is rapidly improving traditional farming practices. Technologies
    such as satellite imaging, unmanned aerial vehicles (UAVs), and sensor technologies
    are revolutionizing the agricultural industry. Smart farming applies information
    technologies for the optimization of complex farming systems. The objective of
    smart farming is to access and use data collected to solve a problem or optimize
    a solution. The main goal is to find a way to use the collected information in
    a “smart” way [15]. Smart farming embraces almost all operations of a farm [16].
    Farmers can use portable devices such as smartphones and tablets to monitor real-time
    data (irrigation, climate, fertilization, etc.) that will aid farmers in reacting
    to situations based on the collected data and making informed decisions supported
    by smart algorithms. There are many types of sensors that can be used to read
    and process agricultural data. Below, we list the most common sensors used in
    smart farming and their specifications: Water content sensor: This is used to
    measure the ratio of the amount of water in the tested soil to the total amount
    of the tested soil, which is the ability of a substance to hold an electrical
    charge. It measures changes due to the change in the dielectric permittivity of
    the soil. Values range from 0 (dry soil) to the saturation of the porosity in
    the tested soil [17] where porosity saturation is the ratio of the pore volume
    to the total volume of the soil sample. The measurements depend on the soil type;
    consequently, the sensor needs to be calibrated for different locations. Volumetric
    water content sensor: This type of sensor measures the water content of soil [18].
    It works by evaluating the water suction in the soil, reflecting plant roots’
    efforts to extract water from the soil. It provides an estimation of the amount
    of water stored or the irrigation required to ensure the needed amount of water
    in the soil. Electrical conductivity sensor: This is used to measure the saline
    content in soil by estimating the solute concentration, which can be hazardous
    for crops if the soil salinity is too high [19]. Soil salinity around the roots
    of plants is mainly caused by salt build-up from irrigation water, which can potentially
    cause long-term damage to the land itself. pH sensor: This type of sensor is used
    to measure pH values, reflecting the acidity and alkalinity of the soil. Ideally,
    soil pH values range between 6.0 and 7.0. Values outside of this range indicate
    a lack of nutrients in the soil. Farmers need to regulate the pH value by using
    alkaline or acidic fertilizers, which improves production [20]. Weed seeker sensor:
    This sensor uses advanced optics and processing power; it detects and eliminates
    resistant weeds. When it passes over a detected weed, it sends a signal to the
    attached spray nozzle to precisely deliver herbicide to the weed. The sensor consists
    of an active light source and a chlorophyll-identifying selective spray sensor.
    This allows for detecting and spraying only weeds, significantly reducing the
    amount of chemicals applied by up to 90%. As a result, optimized use of chemicals
    is achieved, which also reduces the cost [21]. Temperature sensor: This sensor
    gives an alert if the temperature deviates from the normal range. The soil temperature
    determines what types of crops can be cultivated in a field. Temperature is important
    for plant growth processes such as water absorption and transpiration by plants
    through photosynthesis. Each crop has a different temperature range for its growth.
    The enzymes necessary for growth will not be active if the temperature is outside
    of the normal range [22]. Wind speed sensor: This sensor aims to measure wind
    speed at a certain surface level. It is necessary to observe the changes in wind
    speed patterns and directions. The height at which this sensor is mounted depends
    on the crop [23]. FIGURE 5. Primitive sensor Show All FIGURE 6. Smart sensor Show
    All Table 1 lists the most commonly used sensor types, wireless communication
    protocols, and user applications. TABLE 1 Different Sensor Types, Protocols and
    Applications 3) Wireless Sensor Networks in Smart Farming A wireless sensor network
    (WSN) is a group of dedicated and spatially distributed sensors used for monitoring
    physical environmental variables. The sensed value is stored temporarily, and
    then the collected values are transmitted to a central station or sink [42]. Efficient
    WSNs for smart farming are now attracting the attention of both the research community
    and industry leaders. A WSN for smart farming consists of multiple nodes with
    wireless communication capabilities. Figure 7. A generic WSN node architecture
    illustrates a generic WSN node architecture. Each WSN node has a power source,
    sensor/actuator, microcontroller and memory, and transceiver (Tx/Rx) [43]. A node
    can support one or more sensors in measuring different values such as soil moisture/water
    content, soil temperature, soil electrical conductivity, and weather parameters.
    FIGURE 7. A generic WSN node architecture. Show All FIGURE 8. Cloud-based CPS/IoT
    architecture for monitoring livestock [88]. Show All WSNs are characterized by
    being self-organized, self-configured, and self-healing. One of their most common
    advantages is the significant reduction in wiring since they do not rely on a
    wired infrastructure; this can reduce costs by up to 80% [44]. WSNs allow for
    tracking practices in hazardous, infrastructure-less, and rural areas. This offers
    nearly limitless setup flexibility for sensors and improved network robustness.
    Moreover, this technology reduces the need for network maintenance. Another advantage
    of WSNs is their portability, allowing them to be moved around in different farming
    fields. This allows farmers to conduct measurements for multiple fields or sites
    easily. In contrast, wired sensors are more expensive and require regular maintenance,
    and ensuring their mobility is not a trivial undertaking [45]. 4) IoT Protocols
    in Smart Farming The essential components of a smart farm’s architecture are the
    deployed networking sensors, the sinks/base stations, a server, and the communication
    links of the network [46]. The sensors that are deployed in a specific farmland
    area communicate with the sink either directly or by relaying packets toward the
    sink to other sensor nodes using a common wireless routing protocol such as LEACH
    or PEGASIS. The sink then stores the data and sends them via the internet to the
    server. The server can be implemented using a cloud computing infrastructure,
    which allows for scalability and efficiency. WSN protocols are used in IoT to
    provide the PHY/MAC connectivity between IoT sensor nodes and the central gateway/sink.
    IoT encompasses different technology stacks, WSN is a subset of IoT where data
    is transmitted using several IoT devices without internet. 5) Wireless Communication
    Protocols for PHY/MAC Layers In the context of WSNs, the relevant communication
    protocols used are IEEE 802.11 (Wi-Fi), IEEE 802.16 (WiMAX), and IEEE 802.15.4
    (WPAN); the 2G, 3G, and 4G generations of cellular networks; and IEEE 802.15.1
    (Bluetooth), LoRaWAN (LoRA) [47], SigFox, and NB-IoT [48]. The choice of the communication
    protocol depends on multiple factors such as the needed data rate, the power consumption,
    the transmission range, and the cost. NB-IoT is a standards-based low-power wide-area
    (LWPA) technology developed by 3GPP that supports a wide range of new IoT devices
    and services. It uses a subset of the Long-Term Evolution (LTE) standard with
    limited bandwidth for a single narrow band of 200 kHz. LoRaWAN offers a high transmission
    range of about 32 km and very low energy consumption; it provides very limited
    data rates (maximum of 50 kbps), but they are generally enough for transmission
    of measurements from most currently available agricultural sensors. Table 2 lists
    the different wireless technologies used in WSNs. TABLE 2 Different Wireless Communication
    Technologies The characteristics provided in Table 2 for the different wireless
    communications technologies show that each technology has some advantages and
    disadvantages. Table 3 below, summarizes the advantages and disadvantages of each
    of the technologies in the context of smart farming. The summary reveals that
    the ZigBee Alliance sensors and SigFox are the best candidates for smart farming
    applications and their advantages overweigh the disadvantages. TABLE 3 Pros and
    Cons of Wireless Communications Technologies in Smart Farming 6) Cloud Computing
    Services Cloud computing services can be used in smart farming applications for
    the purpose of collecting and storing the data transmitted by remote sensors.
    On the other hand, cloud computing can be used for processing the data and generating
    results for the users. Data processing consists of data analysis, visualization,
    and decision-making. Cloud services can be classified into three connected layers,
    namely Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure
    as a Service (IaaS), which correspond to the internet applications offered for
    end users, the tools used to implement a wide range of applications, and physical
    resources, respectively [49]. C. Expert Systems in Agriculture AI is now regarded
    as a well-established and important technology that has contributed to many fields,
    such as commerce, medicine, electronics, games, manufacturing, and many more.
    In the domain of agriculture, AI technology has been used to create computer programs
    that can perform tasks that require human skills. There are a wide range of AI
    technologies that have been used successfully in agriculture, including expert
    systems and artificial neural networks. Expert systems are computer programs that
    can perform tasks that normally require the abilities of a skilled human. These
    tasks are usually decision-making tasks rather than physical activities, such
    as predicting or forecasting weather conditions. Expert systems are used in agriculture
    to change farming practices and replace human labor. In expert systems, “intelligence”
    means understanding and analyzing a pattern in the data to replicate human behavior
    for decision-making and problem-solving. The first application of AI techniques
    in the management of crops occurred in 1985 [50]. Numerous expert systems were
    developed to overcome the problems of vague, unfocused, and imperfect information
    in agricultural management, such as TEAPEST, which recommends a suitable control
    mechanism for identifying serious insect pests of tea. Rice-Crop Doctor was developed
    by MANAGE as an expert system to detect rice pests and diseases in addition to
    their cures. AMPRAPALIKA is an expert system that detects specific diseases of
    mango by using indicators and recommending treatments for mango tree diseases,
    and many other expert systems have been developed to address different aspects
    of agriculture [51]. There are many research proposals and systems with the aim
    of building expert systems that can determine land suitability for specific crops.
    One such study aimed to develop an expert system for making land suitability decisions
    for fruit crops, a process undertaken by analyzing the soil to recommend suitable
    plants [52]. In another study, new technological advancements were applied to
    model new types of diseases that can damage crops. The proposed system combined
    various factors in this application of technological advancements [53]. Some expert
    systems were also developed in the agricultural domain to evaluate the nutritional
    quality of the soil and determine whether plants absorbed sufficient, sustainable
    nutrients [52]. Image analysis using expert systems plays an important role in
    determining plant characteristics holistically. Such systems measure the heterogeneity
    of leaf pieces to determine leaf scorch, which in turn helps determine symptoms
    of leaf damage [54]. Image analysis particularly helps in two main fields of agriculture,
    namely ecological informatics and biometeorology, which require the quick interpretation
    of plant photo images to save time in the treatment process. In addition, many
    regression models were proposed using crop nutrition parameters. Such models and
    systems can decrease the risk of plant diseases for the global economy by detecting
    diseases and recommending applicable treatment to control them [55]. D. Challenges
    of Adopting AI in Agriculture AI has provided great opportunities to the agricultural
    sector; however, there are still many challenges faced by researchers in this
    area, such as collecting the required data for building the knowledge base. In
    addition to external factors, challenges from sowing to harvesting have led researchers
    to improve and create AI techniques such as artificial neural networks, fuzzy
    systems, expert systems, and agricultural robots. These systems are widely used
    in many farming applications such as crop and soil monitoring, weed management,
    pest management, disease detection, yield prediction, and general efforts to overcome
    challenges. Environmental sustainability is a key factor in farming, as climate
    change will cause decreases in water supplies and increased costs of production.
    Crop management systems provide interfaces that cover many features of the management
    of crops. This approach was first introduced by McKinion and Lemmon [50]. The
    designing of such systems is important for guarding crops from many different
    kinds of damage. Another challenge in farming is crop pests and the selection
    of measures to control them. Drone technologies were developed by different companies
    to help farmers virtually visit all their crops and provide full monitoring systems,
    which can be used to discover dead soil, diseases, irregular crops, and pests,
    in addition to recommending solutions to these issues. Plant diseases caused by
    pests have a significant effect on the global economy as 35% of crops are destroyed
    by different diseases. Thus, monitoring systems are needed to diagnose diseases
    and pests in addition to providing solutions. Such solutions can be based on past
    experiences. Soil quality is another factor to be considered for crop growth.
    It is known that many plants require specific soil characteristics to achieve
    maximum yield and profit [51]. Other challenges that farmers face in their efforts
    to successfully grow crops include climate factors such as temperature, humidity,
    sunlight, and rainfall. Machine learning techniques are being developed to forecast
    and predict the suitability of such factors [56]. It is now necessary to introduce
    modern technology that can use such data accurately to develop intelligent prediction
    systems in agriculture and yield maximum profit. Furthermore, to achieve large-scale
    planting, farmers face other challenges in trying to achieve precise planting
    and avoid the waste of agricultural resources, which can lead to unstable output.
    There are many solutions proposed by researchers and companies to overcome such
    problems by avoiding human labor and developing smart agriculture using sensor
    and IoT technologies for data collection and analysis [57]. AI technologies have
    shown very promising results in the domain of real-time monitoring of data, which
    is particularly suitable in the field of agriculture. As can be seen in the above
    proposed solutions, machines communicating with each other and using suitable
    AI technology will benefit farmers in their efforts to achieve their objectives
    with minimum waste of materials and maximum benefit. Nevertheless, smart farming
    is not as widespread as expected because farmers usually perceive AI as only belonging
    to the physical world, and cannot be applied on land. This is not due to hesitation
    or worry about taking the risk, rather, it is due to the difficulties they face
    in understanding how to use AI-based farming tools. E. Image Processing in Smart
    Farming Image processing consists of manipulating images using computer programs.
    The inputs of these programs are images, and the outputs are either altered images
    or sets of parameters related to the input images [58]. Image processing is useful
    in multiple fields, such as medicine [188], geology [189], and smart farming [184].
    It can be used to detect damaged stems, leaves, or fruit, and to measure the spread
    of disease in a field or the weights of fruits [185]. In image processing, an
    image is preprocessed by being transformed into a matrix of numbers. Then, depending
    on the objectives of the manipulation, different operations can be applied to
    the matrix. These operations will be performed in a fixed sequence for each pixel
    of the image. Image processing may involve multiple techniques, as follows: Image
    enhancement: Images can be easily subjected to distortion [59]. This may be due
    to Gaussian noise, contrast deviation, or blurring. The latter is usually seen
    in most images [60]. Image enhancement consists of processing an image in order
    to make it clearer. This can be achieved by noise reduction, image sharpening,
    brightness adjustment, and contrast increase, generating clearer images that are
    more suitable for display or further processing and analysis. Image restoration:
    Multiple factors cause corrupted or degraded images. These can vary from a non-adjusted
    camera focus to time effect. Image restoration techniques aim to create images
    with the initially targeted quality so that new images can be restored from initially
    corrupted ones by reducing their degradation. Image compression: Image compression
    entails minimizing the size of an image file while preserving its quality. This
    process leads to the optimization of storage resources and the reduction of time
    spent sending images and downloading them from webpages. Image smoothing: Images
    may contain noisy data such as dots, blurs, speckles, or stains, and image smoothing
    methods act as filters to remove noisy data. There are multiple image smoothing
    methods including anisotropic diffusion, median filters, Gaussian filters, adaptive
    median filters, conservative smoothing, and mean filters. Most of them are based
    on low-pass filters, which help in decreasing the large difference between pixel
    values by averaging nearby pixel values while considering single values calculated
    for an image such as median and average values. They remove impulse noise from
    images by reducing the high-frequency components and retaining the low-frequency
    components [61]. 1) Computer Vision Computer vision is intended to empower computers
    to perceive, recognize, and understand the real world in ways very similar to
    humans. This field is now becoming more popular with the success of mobile technology,
    which generates unlimited streams of photos and videos that cannot be analyzed
    by only human vision and thus require the intervention of computer vision. On
    the other hand, computer vision techniques are greedy in terms of computing power.
    The continuous progress and decreasing price of computing power has contributed
    to the flourishing of computer vision. Novel artificial intelligence techniques
    such as convolutional neural networks (CNNs) are being utilized for both software
    and hardware advancement [62]. Computer vision techniques are being widely used
    for smart farming goals such as optimizing the performance of automated robots
    or minimizing the losses of fruit harvesting with automatic robots [63] and post-harvesting
    fruit classification [64]. 2) Computer Vision Techniques Researchers have proposed
    different computer vision algorithms for different tasks. Some of these are as
    follows: Image classification: Image classification aims at classifying images
    into preknown classes. This task is very challenging, especially with larger numbers
    of variable items. Object detection: Object detection aims at localizing semantic
    objects into an image. This technique usually detects objects based on predefined
    categories of images by comparing and matching a set of features with the image
    database. Standard classification algorithms such as AdaBoost [65] and SVM [66]
    are usually adopted for object detection purposes. Research has been undertaken
    to detect different objects such as faces [65] and pedestrians. Image segmentation:
    Image segmentation involves partitioning images into multiple regions to be separately
    examined. It can be considered as pixel-level classification, where pixels are
    classified into specific entities [67]. Its main purpose is to change the representation
    of the images into a more meaningful one that can be analyzed more easily. F.
    Data Collection and IoT Sensors AI-enabled IoT sensors are widely used in many
    farms to collect data. These sensors can be categorized into many different groups
    based on location, optic, mechanics, airflow, electrochemical functions, etc.
    Effective usage of sensors helps farmers to have much more accurate predictions
    and good analysis while building their AI models. It is important to collect and
    manage data with smart sensors working in different IoT environments. One such
    study introduced a sensor management system to collect the data produced by each
    sensor in smart buildings and enable them to be processed and controlled by remote
    devices [68]. In another recent study [69], the data collection of IoT devices
    in a sparse network of IoT sensors was explored using a UAV and two new data collection
    solutions using an algorithm to address cases of full or incomplete data collection
    from sensors. In another relevant work [70], a data collection mechanism was proposed
    in remote areas using UAVs and building models for IoT-based smart farms to collect
    and process data using scheduling. Other scholars have devoted their attention
    to using the LoRa platform and cloud infrastructure to disseminate data for smart
    agriculture and effective irrigation. Sensor data obtained by monitoring temperature,
    humidity, and soil moisture in the LoRa network were transmitted to the cloud
    environment and collected IoT data were analyzed in the testing environment [71].
    Another important undertaking for smart farms is measuring soil and ambient parameters
    in agriculture. Placidi [72] proposed a model wherein visualization was provided
    after data collection using real-time operations. The overall reliability of the
    system was proven with a long-term experiment with two natural soils, loamy sand
    and silty loam. In another work [73], a data collection model using a ZigBee wireless
    sensor network was suggested; it covered all aspects of crops based on a sustainable
    agriculture model. The system supported the data collection and remote-control
    processes of agricultural production, and it also facilitated data analysis and
    operations using the single-point crossover multiple-generation genetic algorithm.
    The results proved that the smart agricultural model offered clear improvements
    in production. Other scholars have devoted attention to IoT-based smart sensors
    that provide important innovations in the agricultural industry to increase productivity
    and energy efficiency. Research on IoT sensor technology was also undertaken in
    consideration of solutions using specific IoT sensors and sensor technologies
    in remote sensing and agricultural applications to assess weather conditions,
    soil quality, and the development of crops using robots for harvesting and weeding
    [74]. In another relevant study [75], the applications of printed sensors in smart
    farming were investigated and the advantages and disadvantages of measurement
    and monitoring applications were weighed while noting the limitations. That study
    also included measurements of chemicals, soil monitoring, and microclimate conditions
    in greenhouses. Recently, many different IoT software has been used worldwide
    in various agricultural solutions, especially for data-driven models needed to
    improve farm production or solve insect-related diseases. Additionally, these
    solutions become more effective and powerful by using machine learning applications.
    Authors [76] discussed the issues related to the software development models for
    IoT applications. Their results indicate that adaptation of these models in IoT-based
    software solutions is more difficult than the other types of standard implementations
    since the involvement of hardware-related problems. Modern software solutions
    in IoT can be categorized into remote sensing, computer imaging, livestock monitoring,
    agricultural drones, precision farming, and so on. The work [77] reflected a solution
    for protecting crops from cattle with infrared sensors by monitoring their movement
    in the fields. Another work related to the decision support system in [78] discussed
    the AgroDSS is a cloud-based smart evaluation of agricultural data analysis. G.
    Crop and Livestock Monitoring According to the World Resources Institute, there
    will be nearly 10 billion people on earth by 2050. To feed this many people sustainably,
    it will be necessary to increase food production by 53% to handle the overall
    expansion of agriculture lands and lower emissions by 67% [79]. One way to meet
    these demands is by smart farming. Incorporating IoT devices, wireless and wired
    networks, cloud computing, artificial intelligence, and software management systems,
    we can monitor and improve farming outputs. Farming can be monitored in two main
    areas: crops and livestock monitoring. Each category has its own specifics and
    needs. 1) Crop Monitoring Crop monitoring takes into consideration one or more
    of the following points: Environmental conditions including humidity, temperature,
    solar radiation, fertilization, and pesticide application, for which data can
    be collected through WSNs and IoT sensors [80]. Crop diseases, including visual
    data that can be collected with high-resolution cameras, which may be fixed or
    mobile via UAVs [81]. In both cases, the information collected with these devices
    needs to be further processed for anomaly classification, prediction, and risk
    estimation [82]. Bauer and Aschenbruck [83] proposed an IoT-based farm monitoring
    system. Their focus of analysis was the leaf area index, which provides information
    on the photosynthetic processes and vital conditions of plants. WSN clusters of
    sensors were used to measure solar radiation (including temperature, humidity,
    and light) to calculate the photosynthetically active radiation range. Raspberry
    Pi was used at each cluster node, exchanging data with the central base unit through
    the LTE modem. The data were subsequently processed within a farm management information
    system for generating reports and making decisions. Bagheri [84] developed a remote
    sensor system with high spatial and temporal resolution to improve the monitoring
    processes for temporal changes in agriculture via a UAV. The system architecture
    consisted of a main onboard system and ground station subsystems, with multispectral
    cameras for high-precision capturing, a GPS tracking system, and a telemetry system
    to transfer data among the subsystems. This monitoring system could speed up the
    monitoring processes and increase the accuracy of crop classification. After image
    capturing, multispectral imaging classification maps were developed with a maximum
    likelihood model. The results were very promising, with accuracy of 94% and a
    kappa coefficient of 0.9. A similar study was developed in a vineyard [85]. The
    images captured were used to detect grape leaf stripe disease via the application
    of the normalized difference vegetation index, which facilitates analysis at the
    level of a single plant. This system allowed for the detection of anomalies near
    the infrared wavelength, which is not possible for the human visible spectrum.
    Thus, this study confirmed the benefits of using smart monitoring for plant protection.
    In another previous study [86], the aim was to implement an integrated plant protection
    architecture and tree protection architecture by combining UAVs, cameras, and
    a WSN. After extensive research, a system with the following components was proposed:
    Environmental data acquisition– Libeliu’s Plug and Sense kit, a robust waterproof
    enclosure with specific external sockets, and incorporated GPS. Data transmission
    was performed with the LoRaWAN Gateway protocol, which performs best compared
    to other technologies. Imagery data acquisition– An eBee X senseFly drone together
    with a Parrot Sequoia+ camera to capture ground and air images. Imagery data processing
    – Preliminary processing of the images directly in the field via Pix4D to improve
    the overall processing time. Cloud infrastructure– Data coming from both land
    and air are stored, processed, and analyzed using multiple machine learning and
    computer vision algorithms. They are managed through web and smartphone applications.
    Cloud platforms are the best choices for such storage due to the additional tools
    they provide. The proposed system aimed to provide multiple area solutions, extended
    area coverage, and macroscopic and microscopic data, portability, and adaptability.
    2) Livestock Monitoring According to the FAO, livestock contributes 40% of the
    global value of agricultural output and supports the livelihoods and food and
    nutrition security of almost 1.3 billion people [87]. Currently, the livestock
    sector emits an estimated 7.1 GT of CO2 equivalent per year, representing 14.5%
    of human-induced greenhouse gas (GHG) emissions. Increasing the efficiency of
    livestock supply chains is crucial for limiting the growth of GHG emissions in
    the future. It was previously reported that smart monitoring of animal health
    and welfare could affect the global meat supply by reducing emissions by 2.5%
    and health problems and diseases by up to 33% [87]. The precision farming approach
    focuses on increasing productivity and preventing the spread of diseases on farms.
    Through IoT infrastructure and cloud-based technologies, it is possible to monitor
    farms in real-time to support and predict animal diseases before they can spread.
    Digital devices such as wearables are being used to monitor the real-time behaviors
    of animals and thus improve their health, lactation, or reproduction. Moreover,
    livestock facility data collection is used to monitor environmental factors such
    as malodor, gas emissions, and ventilation. Smart systems consist of three main
    parts: sensing devices, communication channels, and storage and processing infrastructure.
    The sensing devices can be of several forms, such as collars, tags, actuators,
    or buzzers, and they are mostly known as IoT devices. Communication technologies
    are mostly wireless technologies such as Bluetooth, Wi-Fi, and mobile technologies
    such as 4G or LTE. The storage and processing infrastructure is usually a cloud
    platform, which gives system supervisors the opportunity to use multiple processing
    tools and help them make decisions. Moreover, the cloud infrastructure offers
    AI and machine learning-based applications to support IoT applications. Recent
    research has shown that the usage of smart monitoring has immense potential for
    improving livestock outputs. Several studies have been conducted with cattle,
    sheep, goats, poultry, and house pets, indicating positive impacts in several
    directions. Precision livestock farming consists of monitoring, controlling, tracking,
    predicting, and automating applications [89]. These monitoring systems process
    and transmit data to the concerned parties in real time. Moreover, it is important
    that the farmers need to obey the laws of governments to follow the national standards
    and guidelines for livestock which are need to be developed collaboratively by
    authority and livestock industries. Parameters observed via these monitoring systems
    include the following: Physiological data, such as body temperature, humidity,
    or heart rate [89]. Stress [90], [91]. Food intake [92], [93]. Disease [94]. Digestion
    [89], [95]. Other factors such as environmental conditions [93], [96]. The normal
    body temperature of cows should be between 38 °C and 42 °C. Lower temperatures
    indicate indigestion or milk fever and higher temperatures indicate serious health
    issues. Humidity values above 72% indicate mild stress and those above 80% are
    linked to severe stress causing the reduction of heat exchange and weakness. Normal
    heart rates are considered to be between 48 and 84 BPM. A normal healthy animal
    eats for approximately 3–4 hours per day and digests the food for 9.5-10 hours
    per day. Incorrect measurements of these values will lead to under- or overfeeding
    and subsequent sickness or food waste. The common tools used for physical measurements
    include wearable collars [97], [98], [99] and infrared sensors. Collars are mainly
    used to measure body temperature, blood pressure, and the pulse rate of the animals.
    Skin temperature is measured with infrared cameras and thermometers. The data
    are transferred to a database for further processing through wireless communication.
    Several types of microcontrollers can be used. According to a previous study [100],
    four features help in monitoring lameness, which are the daily number of steps,
    walking distance, time spent lying down, and eating. That study used three machine
    learning methods, namely the artificial neural network, SVM, and random forest
    approaches. All methods gave perfect results in distinguishing healthy and sick
    animals. Joshitha et al. [101] proposed an automated smart system for tracking
    the movements of cattle. A LoRaWan system combined with GPS, NodeMCU, temperature
    and humidity sensors, a power supply, and a Raspberry Pi module was arranged to
    collect and process data on the movements of the cattle. The system assured better
    productivity and protection than existing conventional methods. A similar proposal
    was made in another previous study [102], where the LoRa approach was implemented
    with a mobile gateway instead of a static gateway. Since LoRa utilizes the sub-1-GHz
    unlicensed spectrum, it was concluded that the static gateway was productive mostly
    for small livestock areas because of the sufficient data extraction rate and lower
    energy consumption. However, in larger livestock areas, the mobile gateway offers
    lower deployment costs and sufficient value. The system works by using sensor
    collars, which are hung around the cows’ necks. These collars consist of a heartbeat
    sensor, a temperature sensor, a respiration sensor, and a humidity sensor. The
    data are transferred to the gateway and then to the farmers or breeders. In cases
    where there are many monitoring devices, the data transfer infrastructure should
    be designed accordingly so that the system can operate most efficiently. 3) Poultry
    Farms Several studies and solutions have been proposed regarding poultry farms.
    The main issue to be considered in poultry farming is the housing environment.
    This entails odor monitoring, ventilation systems, temperature, carbon dioxide,
    and humidity. Other factors include the type of chickens, housing systems, building
    structures, ventilation systems, bedding materials, flow rates, types and amounts
    of feed and animal activity levels, manure handling systems, building management
    (cleaning and disinfection procedures), and cleaning practices [103]. Major malodor
    problems may arise on farms as a result of waste and chicken manure. Odor emission
    is influenced by various parameters such as temperature, humidity, wind speed
    and direction, season, and distances. Aunsa-Ard et al. [104] used an e-nose system
    to analyze malodor on poultry farms in Thailand. Their system consisted of eight
    metal oxide gas sensors and three major parts: a sample delivery system, detection
    unit, and signal processing system. The system provided high-precision data measurements.
    Another study used three fuzzy logic controllers to monitor the temperature, humidity,
    CO2, and NH3 on a poultry farm, applying LabView and fuzzy control to regulate
    those parameters. By using fuzzy controllers, the power consumption by the actuators
    was decreased by 42% [105]. Wu et al. [106] developed a combined system of a traditional
    henhouse with a remote environmental monitoring system using ZigBee and ARM. The
    system provided reliable and stable performance. The main considered parameters
    were temperature, humidity, and light. In the study by Li et al. [107], an online
    poultry monitoring system was proposed. The system was based on wireless sensor
    networks and wireless sensor technology. Temperature, CO2, and NH3 concentration
    measurements showed high accuracy, leading to a reliable system. In another study
    [108], the authors took into consideration power shortages and issues raised for
    ventilation systems for poultry. They proposed a smart notification system using
    an infrared sensor to detect fan malfunctions and notify users in three ways:
    phone calls, SMS, and the LINE application. The above section describes the background
    of artificial intelligence and using AI and IoT applications in smart farming.
    SECTION IV. AI in Agriculture Agriculture has a significant role in the sustained
    viability of any economy. It is significant for long-term economic growth and
    structural transformation, and it has evolved in terms of the processing, production,
    and conveyance of crops and domesticated animals. Currently, the agricultural
    sector is being influenced by new innovative IoT technologies, wireless communications,
    machine learning, and AI. Thanks to these technologies, the collection and analysis
    of data such as temperature, weather, soil properties, and historical crop performance
    provide predictive information that helps solve agricultural problems such as
    crop diseases, pesticide control, weed management, lack of irrigation, and water
    management [109]. At the same time, intelligent robots that operate in dynamic
    and unstructured situations and interact with humans have sparked increased interest
    and expanded applications in all fields, including agriculture. Significant advances
    have occurred in the field of agriculture from 1980 to the present day. For example,
    Jha et al. [110] listed more than 50 technological advances in subfields of agriculture,
    including the use of artificial neural networks and expert systems, machine learning
    and fuzzy logic systems, automation, and IoT techniques to solve agricultural
    problems. Artificial neural networks that predict and forecast based on parallel
    reasoning were incorporated into the agricultural sector by Robinson and Mort
    [111], who proposed one of the first models to be fed with raw meteorological
    data like humidity, temperature, precipitation, and wind direction to predict
    the occurrence of frost. Gliever et al. [112] used an artificial neural network
    successfully to differentiate weeds from cotton plants and soil in images collected
    from commercial cotton fields with 92% overall accuracy. Maier and Dandy [113]
    presented a literature review of the use of artificial neural networks for forecasting
    water resource variables and they outlined the steps that should be followed,
    the options available, and the issues that should be considered in the development
    of models that use artificial neural networks for the prediction of water resource
    variables. Song and He [114] used an artificial neural network and expert system
    to help farmers detect crop nutritional disorders in time. That combination led
    to diagnostic efficiency of 92% for nutritional disorders in crops. Prakash et
    al. [115] developed an expert system with a graphical interface based on fuzzy
    logic. It stored knowledge provided by agricultural experts, implemented reasoning
    algorithms to simulate human thinking, and provided a decision-making framework
    to help farmers improve their soybean planting and harvesting decisions in circumstances
    where the help of an agricultural expert is needed but not immediately accessible.
    Sannakki et al. [115] applied an image processing-based approach for the automatic
    grading of leaf diseases by utilizing fuzzy logic. The proposed system was divided
    into five steps including image acquisition, image preprocessing, color image
    segmentation, calculation of the image total leaf area and image total disease
    area, and disease grading by fuzzy logic. The system gave accurate results. Tilva
    et al. [116] developed a fuzzy inference system to forecast plant diseases on
    the basis of weather data. The framework was created to prevent diseases in plants
    using an “IF, THEN” condition that indicated diseases happening because of a particular
    range of temperature and humidity. Shahzadi et al. [117] developed a specialist
    expert system based on the IoT that gathers and sends real-time data to a server
    to make appropriate decisions to enhance productivity and limit losses due to
    diseases and insects/pests. Embedded intelligence aims to discover individual
    behaviors by mining their digital traces during interactions with the IoT. Yong
    et al. [118] applied wireless sensor networks and embedded intelligence in the
    domain of agriculture and presented a technology roadmap that explained the challenges
    and opportunities in agricultural areas in general and offered examples of IoT
    applications for smart irrigation. Patil and Thorat [119] used the IoT and machine
    learning to predict grape disease before it occurred. That involved developing
    a monitoring system for leaf temperature and a humidity sensor to identify grape
    disease risks in the early stages using a hidden Markov model that provides SMS
    alerts to farmers and experts. Several studies have presented different decision-making
    strategies to help farmers monitor their fields [120], [121], [122], [123], [124],
    [125], [126], [127]. For example, Koteish et al. [120] proposed a real-time agriculture
    monitoring mechanism based on the IoT for sensing the soil moisture of a field
    and enhancing the irrigation system. They divided the monitored field into small
    zones and studied the data collected by the sensors from each zone to allow farmers
    to make the right decisions based on a predefined decision table. These studies
    showed efficiency in terms of data reduction, energy conservation, and accurate
    decision-making. The above section describes using IoT technologies in smart farming.
    SECTION V. Robotics in Agriculture The literature has reported various ideas regarding
    the ability of robots to assist in agricultural activities. Indeed, the mechanization
    and automatization of agricultural tasks are an essential step to addressing population
    growth. Khadatkar et al. [128] emphasized the available robotic systems for various
    farm operations for field crops and horticulture and they discussed the following
    approaches and technologies presented in the literature for undertaking various
    agricultural operations: Transplanting: Robotic transplanters use computer graphics
    or machine vision systems for transplanting operations [128]. Most robotic transplanters
    consist of a robotic arm for seedling pick-up, a path manipulator, and an end-effector
    [129], [130]. Intercultural operations: Intercultural operations such as weeding
    are done to kill weeds by mechanical weeders or chemical spraying. Robotic weeders
    use vision-based systems for weed detection, guiding weeders, and uprooting weeds
    mechanically [131], [132], [133]. Gonzalez-de-Soto et al. [134] developed a robotic
    patch spraying system for the precise application of herbicides. Harvesting: Fruit
    selection and detachment are among the essential tasks for efficient harvesting.
    Most robotic harvesters have been developed for fruits and operate by grasping
    the fruit with grippers and then detaching it based on shape, size, color, and
    texture [135], [136]. Rahmadian et al. [137] explored three important developments
    of autonomous robotics in agriculture: navigation (incorporating GPS technology
    and vision-based sensor navigation to direct robots through agricultural fields),
    harvesting systems (incorporating sensors for harvesting and actuators to control
    harvesting devices), and soil analysis systems (giving information about the state
    of the farm’s soil). However, agricultural conditions present many difficulties
    for robotic navigation. One relevant study [138] presented a literature review
    of the approaches for path planning in several agricultural areas. Bochtis et
    al. [139] offered a review of advances in agricultural machinery, where one of
    the approaches involves path planning methods for area coverage on farms. Palmer
    et al. [140] addressed improving the efficiency of field operations and suggested
    that precise tracking of predetermined efficient courses could reduce both overlaps
    and misses. According to some researchers [128], [138], there are two main categories
    of path planning algorithms: Point-to-point path planning: The goal consists of
    determining a collision-free path from a starting point to a destination point,
    optimizing parameters such as time, distance, or energy. Coverage path planning:
    The aim is to determine a path that passes over all points of an area or volume
    while avoiding obstacles [141]. Cao et al. [142] defined all the requirements
    for coverage operations. Luís et al. [138] indicated that path planning has been
    successfully applied to agrarian robots for field coverage and point-to-point
    navigation, with coverage path planning being slightly more advanced. Other researchers
    [143] presented a review of case studies in which robots were applied in recent
    agricultural tasks, including multi-robot systems and ground and aerial robots.
    Gliever and Slaughter [112] demonstrated that a well-validated computer simulation
    can provide a virtual proving ground that is essential for understanding how the
    robots of the future should be designed and controlled. Another study [143] suggested
    steps for making robotic simulations helpful, such as generating large amounts
    of data for machine learning and consequently facilitating the development of
    human-robot interactions and intelligent robots. Oliviera et al. [144] reviewed
    the main existing applications of agricultural robotic systems for the execution
    of land preparation before planting, sowing, plant treatments, harvesting, yield
    estimation, or phenotyping. They evaluated robots according to the following main
    criteria: locomotion system, development stage, final application, use of sensors
    in robotic arms, and computer vision algorithms. They concluded that agricultural
    robotic systems are promoted and proposed in four main areas for future research:
    locomotion systems, sensors, computer vision algorithms, and IoT-based smart agriculture
    [144]. The above section discusses the usage of robotics in the agriculture sector
    and its start of the artwork. SECTION VI. Abnormal Activities The first step in
    anomaly detection is defining the normal patterns as a standard reference point
    for the data analysis and processing phase. In general, an anomaly is defined
    as an abnormal state of the data that does not fit with the standard normal flow
    of systematic data behavior. In a previous study [145], the occurrences of abnormal
    behaviors in the data processing phase were classified into the categories shown
    in Table 4. TABLE 4 Categories and Types of Anomalies For the performance evaluation
    of the detection model, some anomalies were inserted into the data during the
    process of data collection from the sensors. The model was able to detect anomalies
    from different sensors successfully. The implemented autoencoder model in that
    study [145] was considered as one of the most well-recognized neural network techniques
    classified as an unsupervised learning method, where the encoder has to learn
    the ways of compressing, encoding, and reconstructing the data. Basically, after
    the input data are acknowledged, the autoencoder starts the encoding process,
    utilizing the bottleneck layer in order to shrink the input data size. In the
    decoding phase, the autoencoder is trained to ignore non-vital data in the process
    of reconstructing the original data. By ignoring non-vital data such as moisture
    too low, light too high, or humidity too low in the decoding process, the autoencoder
    will be able to process large numbers of features with as little loss of normal
    data as possible and maximization of the loss of the anomalies contained within
    the testing dataset. The autoencoder produces output in the form of images and
    labels the anomalies as abnormal data attributes. The data in the training phase
    are divided into validation (25% of the original data size) and training (75%
    of the original data size) datasets. After the training phase of the model, the
    threshold is determined along with the hyperparameters (batch size, learning rate,
    number of nodes, and number of epochs) using the validation dataset. The optimal
    training parameter setup values are shown in Table 5 [145]. TABLE 5 Optimal Training
    Parameter Setup Values Park et al. [146] proposed a machine-to-machine (M2M) standard
    communication method between things within the IoT environment in order to address
    the fact that most IoT services and devices are implemented to operate in a prototyped
    zone limited to the location where the experiment is taking place. The proposed
    method paved the way toward more interactive and well-connected system sensors
    and controlled devices within smart farm zones inside IoT environments. The M2M
    method offers functions including remote configuration, operation instruction,
    connections, data collection, data storage, device management, and security. In
    the same study [146], the compiled and processed information, basically generated
    from different IoT-based devices within livestock houses as shown in Figure 9.
    Livestock houses and farm communication structure using the M2M approach [146],
    was transmitted and received according to the M2M standard method. FIGURE 9. Livestock
    houses and farm communication structure using the M2M approach [146]. Show All
    The livestock houses consisted of multiple pig barns, each of which had a set
    of IoT equipment as shown in Table 6 [146]. TABLE 6 List of IoT Equipment Within
    Livestock Houses Creating a predictive model for each device is essential in order
    to identify multidevice failure situations in livestock houses. The accumulated
    received data from livestock houses’ sensors and control equipment feed the predictive
    model in the learning process. The training process takes place in a central server
    using transmitted data in a real-time manner from the livestock farm’s devices.
    The data received at the server side are used to predict occurrences of malfunctions
    in order to inform the user so that suitable action can be taken according to
    the type of malfunction. The proposed anomaly detection mechanism suits all IoT
    device types and numbers within livestock houses or farms zones. According to
    Moso et al. [147], smart farms are producing enormous spatial, temporal, and time-series
    data streams. Analyzing these enormous volumes of streamed data will aid in better
    understanding various issues of productivity and efficiency regarding farm processes.
    Monitoring and analyzing a farm’s progress by utilizing a suitable anomaly detection
    technique will help in recognizing any behavioral deviations from the norm. In
    the work of Moso et al. [147], using the enhanced locally selective combination
    in parallel outlier ensembles (ELSCP) as an ensemble anomaly detector was proposed.
    An unsupervised data-driven methodology was defined to be applied in two case
    studies of temporal data in smart farming. The first study considered harvesting
    data along with the use of a combine-harvester Global Positioning System (GPS)
    in tracking events. The second case study addressed crop data, considering the
    link between crop status (damaged or not) and detected anomalies. Referring to
    the area under the precision-recall curve (AUCPR), Moso et al. [147] concluded
    that their proposed methodology applied to the combine-harvester dataset yielded
    a score of 0.972, and for the crop dataset, 30% of the detected anomalies could
    be directly linked to crop damage. The main focus of their work was evaluating
    anomaly detection on farms by analyzing GPS logs, along with the following contributions:
    A detailed state-of-the-art report was offered for anomaly detection techniques
    with a focus on smart agriculture. A robust ensemble-based methodology for the
    detection of anomalies from data streams in smart agriculture contexts was proposed.
    The proposed technique was implemented and applied to a data stream of combine-harvester
    GPS logs with the aim of identifying anomalies that impact the harvest efficiency
    of farm machinery. The proposed technique was also implemented and applied to
    crop data with the aim of identifying anomalies that reveal the status of crops
    during harvesting. In smart agriculture field, Catalano et al. developed a multi-layered
    architecture anomaly detection system to alleviate the infrastructure threats.
    In their work, two machine-learning algorithmic approaches’; the multivariate
    linear regression (MLR) and a long-term memory neural network algorithm (LSTM)
    were employed in the development process of the anomaly detection system. The
    system was fed by a real dataset coming from a smart agriculture system located
    in the Apulia region (Italy). In the training phase of both MLR and LSTM models,
    the datasets were obtained from Google Colab platform and the performance was
    evaluated by metrics. Right after the training process, the testing phase took
    place to generate predictions on the obtained data; therefore, the result will
    be assessed to reveal the detected anomalies [148]. The novelty of Akhter et al.
    work is that they developed an interdigital phosphate sensor for smart agriculture
    with a low-cost and low-power planar. The fabrication of the sensor is produced
    using a 3D printed template; in time, Multi-Walled Carbon Nanotubes (MWCNTs) and
    Polydimethylsiloxane (PDMS) are used to form the electrodes and substrate of the
    sensor respectively. In order to characterize the sensor for a wide range of temperature
    and phosphate detection the Electrochemical Impedance Spectroscopy (EIS) is employed.
    The system is trained with a well-recognized AI data processing algorithm which
    is K-nearest neighbor (KNN) machine learning algorithm. The obtained data from
    sensors are labeled as unclassified raw data. The principle of Euclidean distance
    used in KNN to compute the nearest distance between the training dataset matrix
    and new entry. The next step in this phase is optimizing the ‘K’s parameter from
    training dataset matrix and new entry. At this stage, shortest distance group
    of rows of the matrix is classified, the nearest possible result is computed using
    the mean deviation of those distances. The experimental outcomes were validated
    via standard UV vs. Spectrometry promotes the reliability of the sensor [149].
    The above section discusses the recognition and identification of anomalies activities
    in agriculture. SECTION VII. Irrigation Leakage Water is scarce and it is one
    of the most essential resources in the agricultural sector. A large amount of
    water is wasted as a result of the improper management of irrigation systems.
    As per the United Nation’s World Water Development Report, more than 50% of the
    world’s population will be facing high levels of water scarcity by 2050 [150].
    The main reason for water wastage in farming is leakage in water distribution
    systems (WDSs). In an unmonitored irrigation system, small leaks in the WDS often
    go unnoticed, resulting in critical problems such as ruptures or bursts in the
    pipelines. The leakages in water pipelines are mainly due to excessive pressure
    on the pipelines, which causes distortion and further leads to the bursting of
    pipes when water flows through them [151]. Leakage detection in these pipelines
    by using a proper irrigation leakage monitoring system can help reduce water wastage
    and improve the efficiency of irrigation systems. A considerable number of studies
    have been conducted on leakage detection in WDSs. Researchers have developed different
    approaches for leakage detection and localization, such as the use of flow sensors
    and methodologies to analyze inputs from the sensors. Daadoo et al. [151] proposed
    a system using wireless networks for leakage detection in WDSs for domestic environments.
    The two main phases of the system were an alarm based on GSM to send SMS information
    to the user and an Android application to control the pump. The proposed system
    used water sensors and ultrasonic sensors for water leakage detection and a microcontroller
    as a controlling unit. The results showed that the proposed system gave good responses
    to the sensor and owners could enjoy the ease of controlling the water pump through
    the mobile Android application. Odumodu et al. [152] presented UFMNet, or Ultrasonic
    Flow Metering Network, with real-time flow monitoring, providing a cost-effective
    solution for pipeline leak detection. UFMNet is composed of a set of time-synchronized
    ultrasonic sensors to measure flow data (changes in the flow rate at different
    sections of the pipeline) continuously at high frequencies and radio transceivers
    to enable data correlation and in-network processing of the data flow. The authors
    listed the main advantage of the proposed system as the cost effectiveness of
    the development and installation process. Furthermore, the system provides more
    flexibility, as it can be deployed without shutting down pipelines. Results showed
    that the system could achieve reasonable accuracy. A hybrid entropy clustering-based
    framework for the identification and placement of potential pressure sensors in
    WDSs for leakage detection was proposed by Taravatrooy et al. [153]. Minimizing
    the number of pressure sensors by reducing redundant information based on information
    theory and choosing the optimal solution based on a multicriteria decision-making
    model were the unique points of this study. The main aim of the proposed system
    was to develop an effective framework for optimizing leak detection by decreasing
    the cost of pressure sensor procurement and maximizing the coverage of the sensor
    network. Fan et al. [154] and Coelho et al. [155] used a machine learning-based
    framework for water leakage detection in their works. In the former case [149],
    the authors used a semi-supervised learning framework of clustering and then localization
    for optimal sensor placement and leakage localization. In this approach, the WDS
    is partitioned into water leakage zones using a modified K-means clustering algorithm
    and a machine learning model is trained for leakage detection. New leakage characteristics
    extracted by the unsupervised learning algorithms proposed in that study [154]
    were determined by principal component analysis and an autoencoder neural network.
    An important feature of the proposed model was that it could be trained with the
    leakage characteristics matrix of the unbalanced data to detect abnormal conditions.
    The method achieved satisfactory performance for leakage detection and leakage
    localization. On the other hand, Coelho et al. [155] used a wireless sensor network
    to monitor the WDS and a machine learning algorithm to identify the precise locations
    of water leaks. A study to identify the most suitable machine learning classification
    algorithm for leakage detection was presented in that paper. The proposed system
    was able to achieve 75% accuracy for leakage detection with the benefit of being
    a low-cost application. Figure 10. Machine learning (ML)-based framework for leakage
    detection depicts a simplified model used in machine learning-based water leakage
    detection frameworks. FIGURE 10. Machine learning (ML)-based framework for leakage
    detection. Show All Aditya et al. [150] discussed different smart techniques available
    for detecting leakages and burst events in pipeline networks along with the present
    challenges and future possibilities in their work. Their study highlighted the
    major limitations of smart water technology as false alarms and the difficulty
    in identifying exact leak locations. The calibration method proposed by Moasheri
    et al. [156] used a two-step process of identifying the zone with the most leakages
    and dividing that leaky zone into virtual zones. Calibration of the probability
    of leakage and the roughness coefficients of the pipes in the WDS were obtained
    simultaneously with the imperialist competitive algorithm. This method used the
    analysis of field pressure and flow metering results in the network, and it was
    shown that the method had no limitations on the number of leakages that could
    be evaluated simultaneously. This method helped reduce operational costs by reducing
    the number of field measurement devices. Islam et al. [157] proposed a novel methodology
    based on a fuzzy-based algorithm to analyze the uncertainties of different WDS
    parameters such as roughness, nodal demands, and water reservoir levels to detect
    leakages. An experimental case study showed that the developed model could detect
    leakages and diagnose the exact locations of leakages within a minimal amount
    of time. The limitation of this model was the higher level of computational effort
    in cases of multiple leakages with limited numbers of sensors. Moreover, the model
    did not facilitate optimization of the number of nodes and their placement. The
    above section introduced the irrigation leakage and its solutions using AI techniques.
    SECTION VIII. Quality, Productivity, and Sustainability Food quality, productivity,
    and sustainability are critical issues in all countries because of the increasing
    population, climate change, and decreased resource availability. Traditional farming
    is not sufficient for ensuring high quality and quantity in secure food production.
    Global changes including climate change, water shortages, increased labor costs,
    and security challenges are essential problems in the agricultural sector. AI,
    the IoT, and robotics are important technologies used in smart farming to increase
    quality and productivity. Information technology supported by sensors, smart cameras,
    data science, and robotics can increase crop productivity and sustainability in
    farming. It is more practical and efficient to use AI technologies for monitoring
    and automating decision-making processes in agriculture [158]. In addition, crop
    and animal management is easier and more efficient with these technologies. AI-based
    smart monitoring systems provide more profitable, secure, and efficient farming.
    They reduce the cost of resources such as water and labor and increase reliability
    and security [159]. Productivity and sustainability in farming can be increased
    by applying the following AI, IoT, and robotics-based technologies: Crop monitoring
    using the IoT Automated monitoring of information systems and decision support
    Data analysis using machine learning algorithms Yield mapping using supervised
    machine learning AI-based smart tractors, agribots, and robotic technology Supply
    chain management and tracking Price forecasting and optimization Figure 11. AI-enabled
    IoT smart farming system provides an example of an AI-enabled IoT smart farming
    system. IoT sensors (optical sensors, electrochemical sensors, mechanical soil
    sensors, location sensors, airflow sensors, etc.), drones, and Wi-Fi bots collect
    data from the fields and share the data via the cloud to the AI-based smart farming
    system. Smart farming systems apply many different machine learning, image processing,
    computer vision, remote sensing, and expert system algorithms to retrieve knowledge
    from raw data and thus support farm management and decision-making. These systems
    increase the quality, productivity, and sustainability of agriculture and supply
    chains. FIGURE 11. AI-enabled IoT smart farming system. Show All Thakor et al.
    [158] proposed an IoT-based Digi farming model to analyze the production of farms.
    IoT sensors collect data from farms and help farmers make decisions and monitor
    their crops. Mobile and web applications are very efficient in disseminating product
    information and supporting e-commerce. Thakor et al. [158] evaluated several AI-based
    and IoT-based farming methodologies and compared them to traditional farming systems.
    Their results show that AI-based farming production is more efficient and profitable
    and that the Happiness Index scores of farmers and their living standards are
    higher among those engaged in smart farming compared to traditional farmers. Suebsombut
    et al. [159] classified the current trends and future possibilities of farming
    production and sustainability considering climate change, food security, and farm
    management, which are all relevant to smart farming, information management, product
    lifecycles, supply chains, and traceability. The experimental results showed that
    one of the most important variables was soil carbon emission, which affects food
    production and sustainability. Quality, productivity, and sustainability in agriculture
    are affected by plant diseases, prices of crops, weather and climate, water availability,
    insurance, agent commissions, and potential lack of farming and management skills.
    Radu et al. [160] proposed a farm information management system with two levels,
    namely the local farm and the cloud farm. The farm management systems collected
    data from several farms and applied data preprocessing and machine learning algorithms
    to extract knowledge from the raw data. These authors compared farm management
    with local, cloud, and mobile applications. Cloud-based farm management systems
    yielded more accurate results than the others. Data collection for several crop
    species and several types of data will increase the efficiency and productivity
    of farming. Soil moisture, macronutrients, and micronutrients are also important
    parameters in agriculture for the maximum productivity and efficiency of crops.
    Production resources such as water and fertilizers should not be excessively or
    insufficiently applied. Researchers have shown that soil sensing techniques using
    real-time IoT technology can increase soil productivity with efficient water usage
    [161]. Malik et al. [162] proposed fog computing in sustainable and productive
    smart farming systems. Low-cost sensors and smart management were used to enhance
    agricultural productivity. In their paper, simulation platforms were modeled for
    data collection, sensor deployment, and data processing. These are critical parts
    of the smart farming ecosystems. The system considered sensor node placement,
    robot planning, data collection, mobile nodes, energy nodes, and coverage area.
    Sensors are especially important in smart farming for efficient monitoring and
    early warning systems. The proposed model gave promising results based on sensor
    energy, transmission delay, and packet delivery ratio. It has also been proposed
    that sensor placement and management simulation models can support sustainable
    IoT systems for farm management [163], [164]. Such systems can work with cloud
    computing in real-time applications. Probabilistic rule-based and supervised learning
    algorithms have also been proposed to enhance the productivity of crop production
    and water level arrangements [165], [166], [167], and Bayesian networks are often
    used in smart farming to monitor sensors remotely. Priyadharsini et al. [168]
    proposed a new AI- and machine learning-based information system that used deep
    learning to analyze types of seasonal agricultural products. Input, hidden, and
    output layers were used with a total of 9 neurons in training. Monitoring the
    soil nutrition and pH level increased the productivity of the crops, and deep
    learning-based classification yielded more than 90% accuracy. Machine learning
    supervised classification algorithms are also used for preprocessed raw data received
    from multiple sensors using information fusion for crop monitoring to increase
    productivity and sustainability in smart farming [169], [170], [171]. Experimental
    results have shown that probabilistic-based methods such as the naïve Bayes classifier
    and Bayes network algorithms offer high accuracy in classification. Deep learning
    is one of the machine learning algorithms which uses artificial neural network
    technique. Deep learning performs feature extraction automatically without human
    intervention which provide advantages in training of data. Deep learning is used
    several applications of the smart farming such as plant classification, behavior
    recognition, anomaly detection, pest recognition, smart irrigation and weed detection.
    Park et al. [146] collect livestock data from sensors and controllers to find
    out the anomalies in the farm. Feature vector includes temperature, humidity,
    CO2, ventilation, radiator temperature and external temperature. Anomaly detection
    accuracy in the farm is more than 93% using deep learning algorithms. Shakeel
    et al. [183] proposed deep learning algorithm-based cow behavior detection. Deep
    recurrent learning algorithm is used to identify and forecast cow behavior patterns.
    Proposed algorithm provides robust, secure and efficient computing time. Durai
    et al. [184] proposed several deep learning and other machine learning algorithms
    to predict the weather conditions, analyze the soil, recommend the crops for cultivation,
    determine the amount of fertilizers. Results in deep learning is more promising
    than the rule-based algorithms such as decision tree and random forest. SECTION
    IX. Benefits of Smart Farming Smart farming helps to determine the optimal use
    of natural resources in an economically sustainable manner. In addition, IOT based
    smart farming systems facilitates demand forecast, improves quality of supply,
    and ultimately the experience of the consumer. The objective of the extensive
    data collection and analysis approach used in smart farming is to increase the
    agricultural output while contributing to the environmental protection. Smart
    farming has proven to be beneficial to society in several regards. In this section,
    the main benefits of smart farming as seen by farmers, companies, and other members
    of the agricultural sector will be addressed. A. Farmers As was described above,
    the usage of smart devices, AI, machine learning, expert systems, and cloud computing
    can significantly improve the monitoring processes of farms and allow actions
    to be taken to resolve any abnormalities or problems that have occurred. The usage
    of smart sensors allows farmers to measure all the required parameters related
    to their specific farms, such as crops or livestock, in real time. Image processing
    and machine learning enable the early diagnosis of plant diseases, leading to
    an early identification of the best strategies to fight them. Moreover, AI provides
    predictive insights that facilitate the decision-making processes of farmers in
    several stages of the farming process. Cloud computing and web and mobile applications
    make notification processes remotely available, allowing farmers to be anywhere
    and still have real-time knowledge about the daily conditions of their farms.
    Finally, smart monitoring systems allow farmers to use fewer resources, including
    water, energy, food, fertilizers, land, and human resources, compared to traditional
    approaches. B. Companies The farming and agricultural industry can benefit from
    IoT-enabled smart farming for real-time data collection and process automation,
    which helps achieve better decision-making, reduces waste, and maximizes efficiency
    in operations. According to Hunter et al. [172], the growing demands for global
    food consumption will require an increase in agricultural production of 25%-70%
    by 2050. Meeting this demand is a serious challenge around the world. With smart
    farming, it is possible to support the production of larger quantities of food.
    Smart agriculture has the potential to help address the world’s problems with
    food security and sustainability. Smart farming enables accuracy and precision
    in agriculture, subsequently allowing for improved labor and fuel efficiency.
    Resource consumption and human errors can be reduced using IoT technologies, thus
    reducing the operational costs and enhancing the quality of the products. The
    implementation of smart farming technologies is a major factor driving the growth
    of the smart farming market. However, although smart agriculture models are beneficial,
    there are still many challenges that need to be addressed, such as the high costs
    of smart agriculture equipment and the management of huge volumes of data related
    to productive decision-making. Precision in smart farming allows us to optimize
    the use of resources such as fertilizers and irrigation water and thus improves
    food quality. Agricultural data collected using IoT devices help agricultural
    companies make the right decisions related to farming and the selling of crops
    [168]. C. Agricultural Sector Smart farming solutions have important effects on
    the agricultural sector from different perspectives. For example, observing and
    collecting data from large farms with respect to humidity, air temperature, soil
    moisture, and sunlight intensity will have positive effects on the efficiency
    of water usage. Therefore, it will affect the overall crop yield. Since the world’s
    population is increasing day by day, it is essential to use new techniques in
    the agricultural sector to increase food productivity. In this sense, smart farming
    is the best solution for increasing food production and maximizing profit. Smart
    farming solutions should be implemented effectively using IoT platforms and low-cost
    sensors while saving time, money, and resources. The outcomes of such implementations
    will benefit the agricultural sector in different ways that include increased
    production quality, the protection of water supplies, real-time data collection,
    lower operational costs, improved livestock farming, remote monitoring of fields,
    and reduced environmental footprints. In many countries, research and development
    in the field of smart farming is being promoted to maximize sustainable food production
    while ensuring better profitability for farmers. One example of the implementation
    of smart farming in the agricultural sector was offered by Collado et al. [174].
    They addressed the challenges related to the implementation of such projects,
    taking into consideration the human resources, the availability of fully equipped
    research centers, and the environmental aspects. SECTION X. Challenges A. Data
    Accuracy The success of a smart farming system is highly reliant on the accuracy
    of the data captured by IoT devices, as decisions will be made based upon the
    analysis of the data. IoT platforms, low cost sensors and data insights enables
    increase of efficiency and production in smart farming. However, in smart farming,
    data accuracy can be easily affected by multiple factors. First, IoT devices are
    generally designed for indoor environments, while in real life, smart farming
    takes place outdoors where environmental conditions can be very harsh, with snow,
    hail, floods, wind, or dust. This may lead to the rapid deterioration of IoT sensors.
    For example, sensors that contain copper might experience rapid oxidation, dust
    may easily cover several types of sensors, and some humidity sensors might be
    saturated in highly humid environments [175]. This will lead to the deterioration
    of the measurement capacity of the sensors and thus to degradation in data accuracy.
    Furthermore, electromagnetic interference caused by high-voltage grids across
    rural areas can cause data distortion or corruption due to the generation of electromagnetic
    fields [175]. Furthermore, IoT devices should not be running 24 hours a day; they
    should be switched off if no data are to be read. However, continuous data transmission
    is important for smart farming, so a serious challenge is seen in efforts to balance
    energy consumption and continuous signal transmissions. Battery depletion can
    also cause data inaccuracy since it is a gradual process that is not immediately
    detected. Until the moment of detecting a battery problem and fixing it, sensors
    may be sending data of questionable accuracy. B. Security Since smart farming
    relies on the integration of multiple technologies, networking, the IoT, and cloud
    computing, it inherits all the security issues related to those technologies.
    Different attacks can be executed against smart farming systems, and node capture
    may alter or replace devices [176]. Denial of service and sleep-deprivation attacks
    deplete the batteries of IoT devices and disrupt data transmission, which means
    that decision-making processes are also disrupted [177]. Furthermore, for technical
    reasons, IoT sensors usually cannot be placed in protective boxes [175]. The installation
    and use of advanced smart farming systems require the intervention of experts.
    The uncertainty, the technical and operational feasibilities of the system are
    critical. With the use of sensors and IoT in smart farming, some issues related
    to security and data privacy arise. Indeed, when a device is connected, it can
    be the source of an attack. In addition, sensors have batteries with a limited
    lifespan and in the majority of cases the battery is not replaceable, i.e. the
    age of a sensor is that of its battery. This is why reducing energy consumption,
    recycling or waste disposal are important challenges. Smart farming may be applied
    in huge rural areas, where controlling the security of the whole location is challenging
    and expensive. Generally, security cameras on farms will be stationed at critical
    locations and access points, but it would be very challenging to ensure the security
    of every part of a farm. Attacks with consequences like flooding, the under-watering
    or over-watering of crops, and the misuse of pesticides are often described as
    agroterrorism, and agroterrorism may also be waged against food animal populations
    [175] [178]. Such actions can create fear, financial loss, and social disturbances.
    Rettore de Araujo Zanella et al. [175] stated that with the emergence of smart
    farming, new types of agroterrorism may appear, which can also be referred to
    as cyber-agroterrorism. Cyber-agroterrorism involves actions of attacking smart
    farming systems to cause serious financial and social damage. Security is thus
    very challenging for smart farms, as they face the possibility of both local physical
    attacks and online cyber-attacks. In smart farming, sensors, actuators and other
    technologic devices are exposed to climatic and natural events such as rain, snow,
    sun and hail. Animals, human, or agricultural machinery can damage or remove them
    from the installed locations. C. Knowledge The attitudes of farmers toward smart
    farming play crucial roles in the success and democratization of smart farming,
    especially since most farmers prefer to not take risks and continue with traditional
    farming practices [179]. The required skills and knowledge might represent a barrier
    for farmers that hinders them from adopting smart farming. In particular, attaining
    new skills and knowledge consumes both extra time and extra expenses. Researchers
    [180] [181] have agreed that training farmers would be an easy matter in developed
    countries where different types of advanced technology are already available and
    adopted by farmers. However, it is more difficult in developing countries, where
    most farms are in rural areas. Charania and Li [182] stated that smart farming
    reduces the need for labor. Therefore, in countries where unemployment is a problem,
    smart farming would constitute a threat to farm laborers. This may discourage
    laborers from cooperating and contributing to the success of such technology as
    it injects insecurity in terms of employment stability. D. Network and Data Transmission
    Transmitting data continuously to the cloud is an important task for smart farming
    systems. Subsequently, high-quality internet services with reliable bandwidth
    are mandatory. However, the largest percentage of farming zones are located in
    rural areas, where internet services are weaker than in urban zones, and this
    poses a serious challenge. If the network and/or internet is extremely unreliable
    and does not meet the minimum requirements of smart farming platforms, it is recommended
    to equip the platforms with local computers for data storage and decision-making
    processes rather than losing efficiency while sending a continuous stream of data
    over a weak network. However, this will make smart farming more expensive to implement
    [175]. The costs will increase upon including computers with adequate computational
    capacity and highly skilled employees may be required to operate the systems.
    The population is estimated at 9.7 billion by 2050, which requires improving agricultural
    mechanisms to increase food production and have high yields in a limited time.
    Moreover, agriculture is not limited to food production but constitutes the necessary
    raw materials in various sectors such as poultry, medicine, industrial, etc. We
    believe that the collection of new types of data such as water quality, citizens’
    behaviors, degree of air pollution and geographical characteristics of agricultural
    areas (the spatio-temporal characteristic of the agricultural area and its topological
    relationship with water points and urban areas) combined with the knowledge of
    experts in the agricultural field constitute an essential source for understanding
    potential agricultural problems and finding potential solutions. Indeed, in the
    short term, new types of sensors will be used in agriculture with a large continuous
    automation and use of AI techniques. In the long term, the strategies for improving
    productivity and selecting agricultural products will become more widespread to
    solve global problems rather than local ones. Despite the major benefits smart
    farming offers to farmers, there is still a few things that demotivate them are
    mainly the lack of knowledge and costs associated. Some other general factors
    are as follows: Initial cost of investment into a smart environment such as hardware,
    software, configurations and training. Building a smart farm requires technical
    knowledge on choosing the appropriate smart devices, a reliable network infrastructure
    based on the characteristics of the farm, setting up and monitoring software packages
    to implement actions such as watering triggers, alarms and notifications. Missing
    this knowledge is associated with extra cost of maintenance. One of the main parts
    of the system is the smart energy. The solar agricultural market is still in the
    early stages of development and challenges related to technology costs, limited
    awareness of the benefits, lack of appropriate policy incentives and limited governmental
    subventions for farmers and suppliers who decide to use these technologies. Coping
    with climate change, soil erosion and biodiversity loss is a novel journey for
    the majority, which may lead to continuously changing on the smart farm infrastructure.
    Farmers need to meet global rising demand of higher quality food. They need to
    reduce their impact on the environment, increase the nutritional content of crops
    and minimize chemical residues, which also needs extra technologies. There is
    a trend of youth migration form rural areas into cities which makes it more difficult
    for farmers convincing and inspiring young people to stay and become future farmers.
    Encouraging and training farmers to use technologies in farming has become a policy
    priority in several counties. SECTION XI. Classification of Papers Table 7 provides
    the categorization of the reviewed publications in the three aforementioned dimensions.
    These findings show that IoT data collection, machine learning, and benefits for
    farmers are the focuses of a large percentage of the relevant publications in
    the literature. TABLE 7 Classification of Publications According to the Dimensions
    of Benefits, Challenges, and Methodologies SECTION XII. Comparison of Methodologies
    Machine learning, expert systems, and image processing methodologies are commonly
    used to solve various problems in the agricultural sector. Table 6 provides information
    about several recent applications of AI techniques for smart farming systems.
    For example, Shakeel et al. [183] proposed a deep learning-based classification
    algorithm for cow behavior recognition. Durai et al. [184] developed a system
    using the random forest classifier and deep learning algorithm to classify crops.
    They reported very promising results with accuracy of 95.45%. Decision tree, K-nearest
    neighbor, and random forest algorithms were used in mushroom classification by
    Rahman et al. [186] with accuracy of 100%. Other relevant algorithms used in recent
    works are given in Table 8. It shows that Junior et al. [185] have the best accuracy
    in the spectral, hierarchical, and DBSCAN clustering applications using decision
    tree and K-nearest neighbor algorithm compared with other machine learning algorithms.
    Sharma et al. provided a review of precision agriculture using machine learning
    algorithms to demonstrate that data-driven solutions in smart farms improve the
    productivity and quality of the products. In prediction of the crop growth K-neighbor’s
    classifier, Logistic Regression, Ensemble classifiers algorithms give very promising
    results. Linear regression algorithm is commonly used predict the production value
    for climate data such as rainfall, temperature and humidity. Deep learning algorithms
    are very successful for weed detection, image classification, image segmentation
    and object tracking in agricultural data. Neural network, k-nearest neighbors
    and Naïve Bayes classifier algorithms are used in insect recognition and classification.
    Experimental results show that accuracy is more than 90%. TABLE 8 Recent AI Techniques
    and Accuracy Rates in Smart Farming SECTION XIII. Conclusion Smart farming is
    a concept that involves handling and controlling farms using new technologies
    such as the IoT, robotics, drones, and AI to increase the quantity and quality
    of products while reducing the human labor required for production. These benefits
    will have positive effects on the profitability and the growth of the economy
    as population sizes are dramatically increasing worldwide. Therefore, researchers
    and scientists are moving toward the utilization of recently introduced IoT technologies
    in smart farming to help farmers use AI technology in the development of improved
    seeds, crop protection, and fertilizers. AI in agriculture is emerging in the
    three major areas of soil and crop monitoring, predictive analytics, and agricultural
    robotics. In this regard, farmers are rapidly beginning to use sensors and soil
    sampling to gather data to be used by farm management systems for further investigation
    and analysis. In this survey, we have studied many AI applications in the agricultural
    sector to investigate the various developments and solutions to improve the productivity
    of farms and solve some environmental problems encountered during the production
    of different types of products in agriculture. The AI models for farms help countries
    to maintain sustainability in this sector. We began with background on AI, which
    included a discussion of all AI methods utilized in the agricultural sector, such
    as machine learning, the IoT, expert systems, image processing, and computer vision.
    Second, a comprehensive literature review was presented, focusing on how researchers
    have utilized AI applications effectively in data collection by using sensors,
    utilizing smart robots, monitoring crops, and monitoring irrigation leakage. It
    was shown that quality, productivity, and sustainability are maintained while
    utilizing AI applications. Third, the benefits and challenges of AI applications
    were explored along with a comparison and discussion of several AI methodologies
    applied in smart farming. In this regard, considering the publications that were
    reviewed, it was concluded that machine learning, expert systems, and image processing
    methodologies are the most frequently used methodologies in the literature for
    solving problems in the agricultural sector. Smart farming technologies are emerging
    technologies that help countries to maintain sustainability in the agriculture
    sector, however, the research community should consider some research gaps and
    challenges that create new opportunities for researchers to conduct new research
    tracks using trusted, secure data, factors in climate changes and weather forecasting
    to improve productivity. A lot of research work effort has been conducted to use
    machine learning for the early detection of disease in farms. However, there is
    a limitation in this field due to disease infestation and therefore, new models
    should be developed for early prediction of diseases before the farm harvest is
    affected significantly. The research gaps and challenges explained above encourage
    researchers to work on these gaps and create new opportunities and direct- ions
    to conduct new research on various tracks as future work. In this survey, we have
    also discussed the most recent applications of AI methods in smart farming while
    focusing on which AI methods or algorithms are used and the accuracy rates that
    were obtained. Tables were provided to demonstrate the most recent AI techniques
    and the associated applications as well as the obtained accuracies and, researchers
    have obtained very promising results while utilizing AI methodologies effectively.
    In conclusion, this survey has provided in-depth descriptions of AI applications
    in smart farming. Therefore, due to the provided information, discussions, and
    comparisons given here, this survey will be a useful guide for researchers conducting
    research on AI applications in smart farming. Authors Figures References Citations
    Keywords Metrics More Like This Internet of Things (IoT) Assisted Smart Agriculture
    Monitoring and Summarization System using NodeMCU and Efficient Sensor Unit 2023
    9th International Conference on Smart Structures and Systems (ICSSS) Published:
    2023 Analysis of Decision Support System for Crop Health Management in Smart and
    Precision Agriculture Based on Internet of Things (IoT) and Artificial Intelligence
    (AI) 2023 1st DMIHER International Conference on Artificial Intelligence in Education
    and Industry 4.0 (IDICAIEI) Published: 2023 Show More IEEE Personal Account CHANGE
    USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile
    Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS
    Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT
    Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use |
    Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy
    A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Artificial Intelligence Technology in the Agricultural Sector: A Systematic
    Literature Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
