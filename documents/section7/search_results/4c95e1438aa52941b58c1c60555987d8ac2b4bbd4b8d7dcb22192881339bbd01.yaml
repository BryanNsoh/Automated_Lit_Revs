- analysis: '>'
  authors:
  - Cheng P.
  - Wang S.
  - Zhu Y.
  - Cui C.
  - Pan J.
  citation_count: '2'
  description: 'Three-dimensional fluorescence spectroscopy is a fast, nondestructive
    analysis method with good selectivity and high precision, which provides a foundation
    for the development of the current smart agriculture system. In modern agriculture,
    where agricultural information is fully perceived, it is still very difficult
    to quickly and destructively detect the internal chemical composition of soil,
    crops and agricultural products. Accurate determination of oil pollutants in water
    by using three-dimensional fluorescence spectroscopy technology can provide a
    basis for crop irrigation and is of great significance for improving agricultural
    benefits. The fluorescence spectrum analysis method is adopted to distinguish
    three kinds of mineral oil-gasoline, kerosene and diesel. In order to make the
    distinguishment more intuitive and convenient, a new identification method for
    mineral oil is proposed. The three-dimensional fluorescence spectra of the experimental
    dimension are reduced into two-dimensional fluorescence spectra. The concrete
    operations are as follows: adopting the method of end-to-end data matrix to constitute
    a large Ex image, and then figuring out the envelope curve, processing and analyzing
    the envelope image. Four factors, such as the ranges of excitation wavelength
    when the relative fluorescence intensity is greater than 0.5, the optimal excitation
    wavelengths, their kurtosis coefficients and skewness coefficients, are to be
    selected as the distinguishing feature parameters of mineral oil, and thus different
    kinds of mineral oil can be distinguished directly according to the feature parameters.
    The experimental results show that the proposed method has a high resolution for
    different kinds of mineral oil. Accurate and fast spectral data analysis methods
    can make up for the deficiencies of other agricultural information perception
    methods, provide a basis for the application of smart agriculture in many aspects
    and have a positive significance for promoting the comprehensive intelligent development
    of agriculture.'
  doi: 10.1142/S0218001423550042
  full_citation: '>'
  full_text: '>

    "brought to you by UNIVERSITY OF NEBRASKA-LINCOLN Search My Cart Sign in    Institutional
    Access Skip main navigation Subject Journals Books Major Reference Works Resources
    For Partners Open Access About Us Help Cookies Notification We use cookies on
    this site to enhance your user experience. By continuing to browse the site, you
    consent to the use of our cookies. Learn More ×   International Journal of Pattern
    Recognition and Artificial IntelligenceVol. 37, No. 03, 2355004 (2023) Computer
    Vision No Access Application of Three-Dimensional Fluorescence Spectroscopy in
    Smart Agriculture — Detection of Oil Pollutants in Water Pengfei Cheng , Shuchen
    Wang , Yanping Zhu , Chuanjin Cui , and Jinyan Pan https://doi.org/10.1142/S0218001423550042Cited
    by:1 (Source: Crossref) Previous Next PDF/EPUB Tools Share Cite Recommend To Library
    Abstract Three-dimensional fluorescence spectroscopy is a fast, nondestructive
    analysis method with good selectivity and high precision, which provides a foundation
    for the development of the current smart agriculture system. In modern agriculture,
    where agricultural information is fully perceived, it is still very difficult
    to quickly and destructively detect the internal chemical composition of soil,
    crops and agricultural products. Accurate determination of oil pollutants in water
    by using three-dimensional fluorescence spectroscopy technology can provide a
    basis for crop irrigation and is of great significance for improving agricultural
    benefits. The fluorescence spectrum analysis method is adopted to distinguish
    three kinds of mineral oil-gasoline, kerosene and diesel. In order to make the
    distinguishment more intuitive and convenient, a new identification method for
    mineral oil is proposed. The three-dimensional fluorescence spectra of the experimental
    dimension are reduced into two-dimensional fluorescence spectra. The concrete
    operations are as follows: adopting the method of end-to-end data matrix to constitute
    a large Ex image, and then figuring out the envelope curve, processing and analyzing
    the envelope image. Four factors, such as the ranges of excitation wavelength
    when the relative fluorescence intensity is greater than 0.5, the optimal excitation
    wavelengths, their kurtosis coefficients and skewness coefficients, are to be
    selected as the distinguishing feature parameters of mineral oil, and thus different
    kinds of mineral oil can be distinguished directly according to the feature parameters.
    The experimental results show that the proposed method has a high resolution for
    different kinds of mineral oil. Accurate and fast spectral data analysis methods
    can make up for the deficiencies of other agricultural information perception
    methods, provide a basis for the application of smart agriculture in many aspects
    and have a positive significance for promoting the comprehensive intelligent development
    of agriculture. Keywords: Three-dimensional fluorescence spectrummineral oildimension
    reductionenvelope We recommend Crop Yield Estimation Using the Internet of Things
    B. Mishachandar et al., Journal of Information & Knowledge Management, 2021 Chapter
    4: Recent Advances in Polymer Hydrogels for Agricultural Applications Roop Singh
    Lodhi et al., World Scientific Book OPTICAL FIBER SPECTROSCOPY FOR MEASURING QUALITY
    INDICATORS OF LUBRICANT OILS World Scientific Book Gram Scale and Room Temperature
    Functionalization of Boron Nitride Nanosheets for Water Treatment Shirin Daneshnia
    et al., Nano, 2019 Green Synthesis of Multifunctional Carbon Nanodots and Their
    Applications as a Smart Nanothermometer and Cr(VI) Ions Sensor Lu Li et al., Nano,
    2019 Advances in characterization and evaluation of oil shale based on terahertz
    spectroscopy by Energy Reviews, TechXplore.com, 2023 Sustainable but smartly:
    Tackling security and privacy issues in smart agriculture by Chinese Association
    of Automation, TechXplore.com, 2021 Assessment of Environmental Damage and Policy
    Actions by Using Contingent Valuation Method: An Empirical Analysis of Sago Industrial
    Pollution in Tamil Nadu, India Palani Periyasamy et al., Journal of Environmental
    Law and Policy, 2021 Facile Synthesis and Characterization of Chitosan Nanofibers
    by Oil/Water Emulsion Method Ragupathy Dhanusuraman et al., Advanced Nano Research,
    2018 Magnetically responsive nanofibrous ceramic scaffolds for on-demand motion
    and drug delivery Yonggang Zhang et al., Bioactive Materials, 2022 Powered by
    Figures References Related Details Vol. 37, No. 03 Metrics Downloaded 31 times
    1 History Received 10 October 2022 Accepted 9 December 2022 Published: 4 March
    2023 Keywords Three-dimensional fluorescence spectrum mineral oil dimension reduction
    envelope PDF download Resources For Authors For Booksellers For Librarians Copyright
    & Permissions Translation Rights How to Order Contact Us Sitemap    About Us &
    Help About Us News Author Services Help Links World Scientific Europe World Scientific
    China 世界科技 WS Education (K-12) Global Publishing 八方文化 Asia-Pacific Biotech News
    World Century Privacy policy © 2024 World Scientific Publishing Co Pte Ltd Powered
    by Atypon® Literatum"'
  inline_citation: '>'
  journal: International Journal of Pattern Recognition and Artificial Intelligence
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Application of Three-Dimensional Fluorescence Spectroscopy in Smart Agriculture
    - Detection of Oil Pollutants in Water
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Yadav R.
  - Saroj R.
  - Kumar Verma A.
  - Kumar Mishra A.
  citation_count: '0'
  description: The world is rapidly changing as a result of technological advancement.
    New techniques and frameworks are being created over the globe in many application
    fields. In recent years, there has been a lot of interest in using machine learning
    and Internet of Things (IoT) technologies in agricultural practices. Examining
    the state of research and recent developments in crop monitoring, particularly
    in the context of IoT and machine learning, is the goal of this review. The evaluation
    covers a range of topics, including the different IoT device and sensor types
    used, the machine learning techniques used for data analysis, and the use of blockchain
    technology for improved data security and traceability. This review paper investigates
    how IoT and machine learning can be used to track agricultural growth, with a
    focus on how blockchain technology can be used. The advancement in technology
    can reduce the burden of feeding such a huge population around the world. In this
    paper, monitoring using machine learning and IoT for the growth of crops using
    Blockchain Technology is being reviewed This review outlines the advantages and
    difficulties related to the IoT and machine learning-based agricultural monitoring
    systems through a thorough analysis of the available literature. It highlights
    how real-time data collecting on weather conditions, soil moisture, and other
    pertinent characteristics that affect crop growth is made possible by IoT devices,
    including sensors and actuators. Additionally, it investigates the use of machine
    learning algorithms for data analysis, crop growth pattern prediction, and resource
    allocation optimization for better agricultural practices The integration of blockchain
    technology is also highlighted in this assessment as a viable way to improve data
    security and transparency in the context of crop monitoring. It looks at the potential
    of blockchain to provide end-to-end traceability in the agricultural supply chain,
    ensure data integrity, foster stakeholder confidence, enable smart contracts for
    automated transactions, and more. The results of this review add to a thorough
    understanding of the advantages and difficulties of implementing IoT- and machine
    learning-based systems for crop monitoring. The knowledge acquired from this analysis
    will help direct future research and development projects that will use new technologies
    for productive and sustainable agricultural practices. The promise of combining
    IoT, machine learning, and blockchain technology for agricultural growth monitoring
    is highlighted by this review's findings. Developing agricultural practices and
    promoting the cause of global food security, lays the groundwork for future research
    and innovation in this field. After review, it is found that models previously
    developed need further optimization. These models can be trained faster but their
    evaluation speed is slow. The problem of overfitting and underfitting needs to
    be tackled.
  doi: 10.1109/ICICAT57735.2023.10263755
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 International Conference...
    A Survey of IoT and Machine Learning Based Monitoring of the Growth of Crops Using
    Blockchain Technology Publisher: IEEE Cite This PDF Randheer Yadav; Rahul Saroj;
    Alok Kumar Verma; Ashish Kumar Mishra All Authors 76 Full Text Views Abstract
    Document Sections I. Introduction II. Need III. Methodology IV. Discussion V.
    Conclusion Show Full Outline Authors Figures References Keywords Metrics Abstract:
    The world is rapidly changing as a result of technological advancement. New techniques
    and frameworks are being created over the globe in many application fields. In
    recent years, there has been a lot of interest in using machine learning and Internet
    of Things (IoT) technologies in agricultural practices. Examining the state of
    research and recent developments in crop monitoring, particularly in the context
    of IoT and machine learning, is the goal of this review. The evaluation covers
    a range of topics, including the different IoT device and sensor types used, the
    machine learning techniques used for data analysis, and the use of blockchain
    technology for improved data security and traceability. This review paper investigates
    how IoT and machine learning can be used to track agricultural growth, with a
    focus on how blockchain technology can be used. The advancement in technology
    can reduce the burden of feeding such a huge population around the world. In this
    paper, monitoring using machine learning and IoT for the growth of crops using
    Blockchain Technology is being reviewed This review outlines the advantages and
    difficulties related to the IoT and machine learning-based agricultural monitoring
    systems through a thorough analysis of the available literature. It highlights
    how real-time data collecting on weather conditions, soil moisture, and other
    pertinent characteristics that affect crop growth is made possible by IoT devices,
    including sensors and actuators. Additionally, it investigates the use of machine
    learning algorithms for data analysis, crop growth pattern prediction, and resource
    allocation optimization for better agricultural practices The integration of blockchain
    technology is also highlighted in this assessment as a viable way to improve data
    security and transparency in the context of crop monitoring. It looks at the potential
    of blockchain to provide end-to-end traceability in the agricultural supply chain,
    ensure data integrity, foster stakeho... (Show More) Published in: 2023 International
    Conference on IoT, Communication and Automation Technology (ICICAT) Date of Conference:
    23-24 June 2023 Date Added to IEEE Xplore: 02 October 2023 ISBN Information: DOI:
    10.1109/ICICAT57735.2023.10263755 Publisher: IEEE Conference Location: Gorakhpur,
    India SECTION I. Introduction The foundation of human existence on Earth is agriculture.
    Agriculture must be wholesome and long-lasting for any civilization to prosper.
    The world''s population is fed by agriculture. It can be considered the primary
    means of support for India''s enormous population and other developing countries.
    Additionally, it is anticipated that both population growth and a rise in earnings
    may increase the demand [1]. About 10 million individuals kicked the bucket due
    to a deficiency in the supply of nourishment. The sum of nourishment generation
    has diminished since farmers are still inclined towards their conventional way
    of farming. They are not willing to transform into recent innovations and contraptions.
    It resulted in a lot of strain on the agricultural industry to satisfy the growing
    demand for food. Agriculture innovations are increasingly required to meet the
    rising global demand for food and sustainable growth. It helps in the conservation
    and efficient use of natural resources. It also maintains the environment''s capacity
    to deliver economic, social, and environmental services to society [2]. Technology''s
    quick development has made it possible for creative solutions to be found in a
    variety of fields, including agriculture. The fusion of the Internet of Things
    (IoT) and machine learning techniques has demonstrated tremendous potential for
    revolutionizing conventional farming methods in recent years. Farmers may gather
    enormous volumes of real-time data on weather conditions, soil moisture, and other
    critical aspects that affect the growth of crops by utilizing IoT devices, such
    as sensors and actuators. This data is further analyzed by machine learning algorithms
    to produce insights, forecast growth patterns, and optimize resource allocation.
    IoT innovation aims to set up common communication between gadgets through the
    web. Thoughts about data security, openness, and trust have surfaced as agriculture
    becomes more digitized and data-driven. Blockchain technology can be used in this
    situation. Blockchain, which was once associated with cryptocurrencies, has expanded
    beyond virtual money and found use in a variety of sectors, including agriculture.
    Blockchain offers a decentralized, open, and transparent framework for validating,
    storing, and preserving the integrity of data. Crop monitoring using blockchain
    technology integration has the potential to improve data security, traceability,
    and trust within the agricultural ecosystem. It may be an endless connection that
    connects individuals and distinctive associated things to gather and share information.
    The associated gadgets have in-built sensors that are related to IoT stages to
    access the information from the gadgets. It applies distinctive analytics to attain
    and display important data from the device''s data [3]. This review paper intends
    to examine the most recent developments and research in the field of IoT and machine
    learning-based agricultural growth monitoring, with a focus on the application
    of blockchain technology. The goal of this review is to shed light on the advantages,
    difficulties, and prospective applications of this integrated approach by undertaking
    a thorough examination of the existing literature. SECTION II. Need To reduce
    farmers'' labor and increase efficiency, the Internet of Things can be implemented.
    It provides automated control and remote monitoring of agriculture. This system
    will also result in a reduction in money and time [4]. It enables remote data
    collection from several field-installed sensors. Sensors can monitor a variety
    of things like soil pH, moisture content, temperature, etc. It can also detect
    humidity, crop development, insect population, pest detection, and many others
    [5]. Machine Learning Technologies can be combined with a hyperspectral and multispectral
    picture to monitor the health of crops. It also helps in looking into prospective
    insect assaults to take necessary actions. Deep learning and machine learning
    are combined by users for auto-aware of potential pests or disease problems if
    any. SECTION III. Methodology Farming is encountering extreme changes and is confronting
    various natural and social issues. Numerous farmers are still depending on conventional
    cultivating hones. These farmers have no coordination to utilize their resources
    efficiently. This scenario has created trouble in increased production with restricted
    common assets such as land and water [6]. Maintainable nourishment and agrarian
    generation cannot be finished by traditional farming frameworks. These frameworks
    have driven considerable deforestation, water shortage, and soil disintegration.
    Hence, progressed frameworks that moderate and strengthen the premise of normal
    assets and increment generation must be utilized [7]. Shrewd horticulture is an
    approach that guides rural field supervision during times of climate alteration.
    Savvy Farming has employed the Blockchain, Machine Learning, and the Internet
    of Things to screen the field environment. It also develops and gives data to
    the agriculturist for decision-making [8]. Figure 1 depicts the process involved
    in the complete cycle of advanced agriculture. Fig. 1. Management cycle for advanced
    agriculture Show All A. Internet of Things IoT gives a wide run of applications.
    It includes soil and plant following, edit development perception and choice.
    This with water system appraisal helps in the observation of the farming environment.
    To optimize agribusiness, the usage of IoT within the field has expanded the efficiency
    and viability of agriculturists. It may offer assistance to decide field factors
    such as soil quality and plant biomass [9]. It can moreover be utilized to test
    and screen factors count temperature, soil dampness, and trim infections. Other
    than that, IoT can be utilized to track edit development and surrender impacting
    variables. Farmers can moreover figure out which crops are most suited for which
    conditions and can turn to edit in like manner [10] Table I. Machine learning
    models for crop quality a) Humidity Sensor Humidity sensors work by recognizing
    changes that are responsible for modification in temperature inside the air. The
    utility of humidity sensors runs distant and wide. Individuals with troubles influenced
    by humidity, observing and a preventive degree in homes utilize humidity sensors.
    A humidity sensor is also used as a part of domestic warming, ventilating, and
    discussing conditioning frameworks (HVAC frameworks). These sensors can be utilized
    in workplaces, exhibition halls, mechanical spaces, and nurseries. These can be
    utilized in meteorology stations to report and anticipate the climate [11]. b)
    Temperature Sensor Measuring temperature dampness can make or break our agricultural
    operation. In this way, it is imperative to urge it right. Temperature and humidity
    go hand in hand when raising crops in a nursery. In part, the awkward nature of
    either can regularly grant comparable comes about. Plants respond to their surroundings.
    Also, for plants to survive, a particular set of atmospheric conditions must exist.
    Plant breath is the process of water clearing out through dissipation. Stomata
    are modest openings, often cleared on the surface of the plant. It also permits
    the control of gas trades and dampness direction amid photosynthesis. Imbalances
    in stickiness can be influenced by this preparation. If mugginess levels are high,
    the dissipating water from the stomata has no way to go out [12]. c) Stem Sensor
    Hydration, or the sum of water, inside plants, is the basic unit for almost every
    physiological work. It includes transpiration stomatal conductance to photosynthesis
    and more. Subsequently, measuring water substances inside plants will give priceless
    experiences to an endless cluster of physiological processes [13]. d) Leaf Sensor
    A leaf sensor is a photometric gadget that measures plants'' water misfortune
    or the water shortfall stretch (WDS). It is achieved by real-time observation
    of the dampness level in plants [14]. B. Machine Learning In agriculture and related
    fields, machine learning (ML) techniques have become a viable complement to traditional
    modeling approaches. Agricultural production, yield prediction, etc. are being
    benefited from the popularity of ML algorithms. Machine learning in agriculture
    has vast applications in this sector. It is used in numerous ways today with excellent
    outcomes. They include monitoring for weeds and diseases, predicting crop output
    and quality, gathering data, etc. [15]. a) Artificial Neural Network The most
    effective learning models are artificial neural networks. They can have a wide
    range of complex functions that can represent input-output maps in multiple dimensions.
    The biological nervous system, such as the brain, serves as the inspiration for
    the information processing paradigm known as ANN. It is typically depicted as
    a system of interconnected “neurons” communicating with one another. Perceptron,
    Multi-Layer Perceptron, Recurrent Neural Networks, and Self Organizing Maps are
    a few examples of the various artificial neural networks [16]. The Multi-Layer
    perceptron technique is used to predict data in the system. Artificial neural
    networks are typically difficult to set up and take a long time to train. Once
    they are ready, they can be used [17]. b) Regression Regression is a supervised
    ML technique. It predicts continual responses like stock value, variations in
    fuel demand, and time-series-based device data. The types of regression can be
    used according to the needs of the situation. Linear models are based on the supposition
    that independent and dependent variables have a linear relationship [18]. c) Decision
    Tree This method can be used with input and output values that are both obvious
    and enduring. The data is divided into two or more equivalent sets to carry out
    the machine-learning process. Based on the strongest splitter among the autonomous
    components, it divides sets. It operates by modeling its decisions after those
    made at a tree''s root [19]. The tree also has branching circumstances where a
    weighted indicator''s value is compared to it. The number of branches and weights
    are chosen as training preparation begins. The primary aspect of the splitting
    process that distinguishes itself is the regular cost reduction [20]. d) Ensemble
    Learning Ensemble learning could be a common meta-approach to machine learning
    that looks for superior prescient execution. It works by combining the forecasts
    from different models. There are a boundless number of gatherings that can be
    created for prescient modeling issues. Three strategies overwhelm the field of
    ensemble learning. Instead of calculations per se, each becomes a field of view,
    yielding several more specialized methods [21]. The three primary classes of gathering
    learning strategies are sacking, stacking, and boosting. It is imperative for
    both to have a point-by-point understanding of each strategy and to consider them
    in your prescient modeling venture. Bagging involves averaging the predictions
    by fitting multiple decision trees to various dataset samples. Using stacking,
    we can learn to combine the predictions using various model types that are fitted
    to the same set of data. By sequentially adding ensemble members, boosting creates
    a weighted average of forecasts that corrects the predictions made by earlier
    models [22]. Fig. 2. Data from different sources Show All e) Bayesian Model A
    group of models is called Bayesian models based on probability. In this, the investigation
    is started inside the context of Bayesian induction. It could be a directed learning
    category and can be implemented to tackle problems such as classification or regression
    [23]. Figure 2 describes the models developed for crop quality using Machine Learning
    [24]. C. Blockchain Blockchain technology is a developing innovation and getting
    the consideration of many businesses. It is being considered in finance, healthcare,
    instruction, food, and administration. The main reason that blockchain technology
    is getting popular is its interesting highlights. It works only by a trusted middle
    person in a decentralized strategy, without the assistance of an authentication
    system. It can attain the same objective with the same volume of dependability.
    Blockchain technology opened new pathways and presented perishable networks. With
    blockchain technology, we''ll be able to make an exchange without believing in
    other parties. The work of arbiters has been dispensed with, and exchanges have
    ended up faster between distinctive partners. The security of the data may too
    be ensured through the utilization of cryptography. Figure 3 depicts the structure
    of the Blockchain and how individual data blocks are saved. SECTION IV. Discussion
    Despite the potential benefits, there are some challenges and limitations that
    need to be addressed. There are technical challenges in integrating various IoT
    devices and sensors, data interoperability, and standardization, and the availability
    and quality of training data for machine learning models can impact their accuracy
    and versatility. For widespread adoption, blockchain networks need to improve
    their scalability and energy efficiency. Additionally, legal and regulatory frameworks
    need to be developed to address issues related to data ownership, privacy, and
    intellectual property rights. The review recognizes that the deployment of IoT,
    machine learning, and blockchain-based crop monitoring systems faces obstacles,
    especially in resource-constrained environments. Barriers to adoption include
    high initial costs, lack of technical expertise, and limited access to connectivity
    and infrastructure. To ensure equal access to and benefits from these technologies,
    socioeconomic factors such as the digital divide and farmers'' digital literacy
    must also be considered. With the growing environmental problems that negatively
    affect this sector. It is necessary to switch to the use of modern technologies
    to face these challenges and problems with high efficiency. Smart farming is a
    broad field that encompasses various issues such as the expert system, decision-making,
    etc. The development of IoT and Machine Learning over the years has dramatically
    increased the number of projects in the agriculture sector. Analysis has shown
    that machine learning techniques outperform most related work and traditional
    methods in this industry. The report points out several future research directions,
    including developing advanced predictive models, integrating multiple data sources,
    exploring new blockchain applications, and assessing economic and environmental
    impacts. User-centered design, usability research, and real-world implementation
    and validation are also highlighted as important areas for future work. Fig. 3.
    Blockchain structure Show All Table II Different machine learning algorithms for
    soil and weather data used for crop growth monitoring SECTION V. Conclusion In
    this article, an analysis of recent research efforts related to utilizing machine
    learning techniques, IoT, and blockchain in agriculture has been done. Important
    published contributions are discussed and problems related to them are highlighted.
    During this survey, the technical details of the data sets used, and the models
    for machine learning have been thought of. The working environment, data pre-processing,
    and data augmentation techniques have also been included. The findings show that
    machine learning predictions perform better than other typical traditional techniques.
    But it can be significantly improved by considering other specific parameters
    like training and evaluation speed of the dataset, the problem of overfitting,
    underfitting, and accuracy. In addition, machine learning technologies provide
    a high level of accuracy and outperform existing and commonly used processing
    techniques. This work will be useful for researchers working in the field to solve
    various agricultural problems related to monitoring or predicting the growth of
    plants. Additionally, this technique has shown promising results when applied
    to agriculture, leading to smarter and more effective solutions to make farming
    more efficient and sustainable. In the future, the plan is to develop a machine-learning
    model through the integration of different models and evaluate its performance
    in terms of accuracy with the existing models. SECTION VI. Future Works After
    the survey following can be the future research directions for the researchers:
    Enhanced integration of IoT, machine learning, and blockchain: Future research
    should concentrate on improving crop monitoring systems'' integration of IoT devices,
    machine learning techniques, and blockchain technology as the technology develops.
    Among these include developing cutting-edge techniques for smoothly synchronizing
    and connecting data from IoT sensors, improving the scalability and interoperability
    of blockchain systems, and optimizing machine learning algorithms for in-the-moment
    decision-making. Development of advanced predictive models: We will discover how
    to apply advanced machine learning techniques such as deep learning and reinforcement
    learning to plant growth monitoring. We investigate how these techniques can improve
    forecast accuracy, disease detection, yield estimation, and resource optimization
    in agriculture. Authors Figures References Keywords Metrics More Like This Machine
    Learning based Crop Yield Prediction in Agriculture: A Survey in Vellore 2023
    Innovations in Power and Advanced Computing Technologies (i-PACT) Published: 2023
    Development of High-Quality Crops using Optimized Machine Learning in Smart Agriculture
    Environment 2023 Third International Conference on Artificial Intelligence and
    Smart Energy (ICAIS) Published: 2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 International Conference on IoT, Communication and Automation Technology,
    ICICAT 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Survey of IoT and Machine Learning Based Monitoring of the Growth of Crops
    Using Blockchain Technology
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Tian Q.L.
  - Guo B.J.
  - Ye F.W.
  - Li Y.
  - Liu P.F.
  - Chen X.J.
  citation_count: '3'
  description: The spectrum is a comprehensive reflection of the mineral's physical
    chemistry characteristics, composition and structure, which has been used in mineral
    and rock identification. The traditional classification methods of the mineral
    spectrum require complex spectral pretreatment, and then some spectral features
    are analyzed by different methods to achieve the goal of fine classification.
    However, the pretreatment may cause a partial loss of the spectral information
    and reduce the classification accuracy. Besides, the operation process is complex,
    so the efficiency is low, making it difficult to cope with the growing demand
    for big data processing. Therefore, it is important to establish an accurate,
    efficient and automatic classification model for the mineral spectrum. As one
    of the widely used deep learning models, the convolutional neural network extracts
    data features layer by layer and combines them to form higher-level semantic information.
    It has a strong capability of model formulation and great potential for the analysis
    of spectral data. This paper proposes a novel mineral spectrum classification
    method based on a one-dimensional dilated convolutional neural network (1D-DCNN).
    The DCNN is used for spectral feature extraction. The backpropagation algorithm
    combined with the random gradient descent optimizer is used to adjust the model's
    parameters, then output the classification result, which implements the end-to-end
    discrimination of mineral species. The 1D-DCNN includes one input layer, three
    dilated convolution layers, two pooling layers, two full connection layers and
    one output layer. It uses cross-entropy as the loss function, and dilated convolution
    is introduced to enlarge the receptive field of filters effectively avoid the
    loss of spectral feature details. The spectrum of four different minerals, muscovite,
    dolomite, calcite and kaolinite, are collected, and the data are augmented by
    way of adding noise to construct sufficient spectral samples, which are used for
    model training and testing. Then, we explore the impacts of different model parameters,
    such as the convolution type and the number of iterations, and then compare the
    proposed model with the traditional mineral spectrum classification methods to
    evaluate its performance. Experimental results indicate that the 1D-DCNN model
    can quickly and accurately classify mineral spectrum with the accuracy of 99.32%,
    which is superior to the backpropagation (BP) algorithm and support vector machine
    (SVM) methods, and it shows that the proposed method can fully learn mineral spectral
    features and implement a fine classification result, with good robustness and
    scalability. The proposed method can apply further to the spectra classification
    in coal, oil-gas, lunar soil and other fields.
  doi: 10.3964/j.issn.1000-0593(2022)03-0873-05
  full_citation: '>'
  full_text: '>

    "学习中心 应用 会员 登录 / 注册 简 繁 搜索 首页 > 期刊导航 > 光谱学与光谱分析 > 2022年3期 > 一维空洞卷积神经网络的矿物光谱分类
    DOI: 10.3964/j.issn.1000-0593(2022)03-0873-05 一维空洞卷积神经网络的矿物光谱分类 田青林 1 郭帮杰 1 叶发旺
    1 李瑶 2 刘鹏飞 1 陈雪娇 1 1.核工业北京地质研究院遥感信息与图像分析技术国家级重点实验室,北京 1000292.Zachry Department
    of Civil and Environmental Engineering ,Texas A&M University ,Texas 77843 ,USA
    在线阅读 下载 引用 分享 打印 摘要： 矿物光谱综合反映了岩矿的物理化学特性、组分和内部结构特征,已被应用于岩矿识别研究.传统的矿物光谱分类方法需要先对矿物光谱进行预处理,再采用不同方法分析光谱特征,从而实现分类目的.但同时也会造成部分光谱信息丢失,导致最终分类精度不高且操作过程繁琐、效率低下,难以应对日益增长的大数据处理需求.因此,建立一个准确、高效的矿物光谱自动分类模型意义重大.卷积神经网络是应用最广泛的深度学习模型之一,它通过逐层抽取数据特征并组合形成高层语义信息,具有极强的模型表达能力,在光谱数据分析方面应用潜力巨大.针对矿物光谱数据的特点,提出了基于一维空洞卷积神经网络(1D-D
    C-NN)的矿物光谱分类方法,利用空洞卷积神经网络提取光谱特征,采用反向传播算法结合随机梯度下降优化器调整模型参数,输出光谱分类结果,实现了矿物类别的端到端检测.该网络包含1...
    关键词： 矿物光谱自动分类空洞卷积深度学习 分类号： TP391(计算技术、计算机技术) 资助基金： 核能开发项目 遥感信息与图像分析技术国家级重点实验室基金项目
    ( ZJ2019-2 ) 论文发表日期： 2022-03-28 在线出版日期： 2022-03-18 （万方平台首次上网日期，不代表论文的发表时间） 页数：
    5 ( 873-877 ) 英文信息 引证文献 (2) 仅看全文 排序： 发表时间 被引频次 [1] 阮坤.基于卷积神经网络VGG的三维荧光组分预测及识别研究[D].2022.
    [2] 徐苏,鄢容,刘玉明,等.基于卷积神经网络的第一镜表面杂质沉积状态识别研究[J].西北师范大学学报（自然科学版）.2023,59(2).DOI:10.16783/j.cnki.nwnuz.2023.02.010
    . 光谱学与光谱分析 CSTPCD 北大核心 EI SCI ISSN：1000-0593 年,卷(期)：2022,42(3) 相关文献   换一换 1. 基于光谱分解和PSOBP组合模型的光谱重构研究
    胡春晖 等; 量子电子学报 ;2024 2. 利用Sentinel-2影像超分辨率重建的红树林冠层氮含量反演 甄佳宁 等; 遥感学报 ;2022 3. 支持向量机复合核函数的高光谱显微成像木材树种分类
    赵鹏 等; 光谱学与光谱分析 ;2019 4. 基于双图正则的半监督NMF混合像元解混算法 邹丽 等; 计算机科学 ;2018 5. 基于拉格朗日的高光谱解混算法研究
    刘万军 等; 计算机应用研究 ;2016 月卡 - 期刊畅读卡 - ¥68 季卡 - 期刊畅读卡 - ¥128 年卡 - 期刊畅读卡 - ¥199 年卡 -
    超级文献套餐 - ¥499 查重 - 个人文献检测 - 快速入口 开通阅读并同意 《万方数据会员(个人)服务协议》 相关主题 矿物光谱 自动分类 空洞卷积
    深度学习 红树林 冠层氮素含量 Sentin... 影像重建 SVM-RF... KRR 光谱解混 相似端元 端元提取 丰度估计 解混算法 高光谱图像 混合像元解混
    非负矩阵分解 双图正则 恒星天文光谱 C4.5 朴素贝叶斯 softma... PCA RFE 离散子波变换 遥测 污染云团 光谱特征 相关学者 赵鹏 东北林业大学
    张骏 烟台大学 王湘晖 南开大学 程辉 湖南科技大学 郭澎 南开大学 张太宁 石家庄经济学院 蒋夕平 南京农业大学 陈黎 北京师范大学 陈浩杰 曲阜师范大学
    陈桥 深圳职业技术学院 帮助 客户服务 问卷调查 关于我们 公司首页 加入我们 网站地图 官方店铺 网络出版服务许可证：(署)网出证(京)字第072号 药品医疗器械网络信息服务备案：(京)网药械信息备字（2023）第
    00470 号 信息网络传播视听节目许可证 许可证号：0108284 万方数据知识服务平台--国家科技支撑计划资助项目（编号：2006BAH03B01） 万方数据学术资源发现获取服务系统[简称：万方智搜]
    V3.0 证书号：软著登字第11363462号 京ICP证：010071 京公网安备11010802020237号 京ICP备08100800号-1 ©北京万方数据股份有限公司
    万方数据电子出版社 在线客服 客服电话：4000115888 客服邮箱：service@wanfangdata.com.cn 违法和不良信息举报电话：4000115888
    举报邮箱：problem@wanfangdata.com.cn 举报专区：https://www.12377.cn/ 个人文献 检测入口 万方检测 京东店铺
    手机版 联系 客服"'
  inline_citation: '>'
  journal: Guang Pu Xue Yu Guang Pu Fen Xi/Spectroscopy and Spectral Analysis
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Mineral Spectra Classification Based on One-Dimensional Dilated Convolutional
    Neural Network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 56 papers. The topics discussed include: control
    of environmental pollution by utilizing wastes from industry on fly ash based
    geopolymer concrete; fuzzy logic intelligent system for an automatic medical waste
    segregation; efficient identification of node failure and recovery through end
    to end probing techniques; a study on vulnerable risks in security of cloud computing
    and proposal of its remedies; agriculture cloud system based emphatic data analysis
    and crop yield prediction using hybrid artificial intelligence; runoff volume
    prediction in the Megadrigadda reservoir catchment due to past land use/land cover
    trends-a case study; design and fabrication of automatic squeezing machine for
    food industries; comparative study on soil stabilization using industrial by products
    and coconut coir; and design of hybrid deep learning approach for covid-19 infected
    lung image segmentation.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: 'Journal of Physics: Conference Series'
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: International Conference on Physics and Energy 2021, ICPAE 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Del Maestro M.
  - Rampini A.D.
  - Mauramati S.
  - Giotta Lucifero A.
  - Bertino G.
  - Occhini A.
  - Benazzo M.
  - Galzio R.
  - Luzzi S.
  citation_count: '18'
  description: 'Background: Human placenta is recognized as a valuable vascular microneurosurgery
    training model because of its abundant availability, ethical acceptance, and analogous
    vasculature with other vessels of the human body; however, human placenta laboratory
    preparation techniques are not well described in the literature. This study outlines
    a detailed and standardized laboratory protocol for preparation of a color-perfused
    human placenta model. Survey-based validation of the model is also reported herein.
    Methods: The protocol involved cleaning and cannulation of the umbilical vein
    and arteries, irrigation with heparin, and storage at 3°C or freezing at −18°C.
    Before use, arteries were perfused with carmine/cochineal, and veins were perfused
    with methylthioninium chloride. A questionnaire with 5 questions was administered
    to 40 participants among attending or resident neurosurgeons, otolaryngologists,
    and maxillofacial surgeons on 4 consecutive microsurgical courses to assess the
    reliability of the placenta model. Trainees were divided into 3 groups based on
    their experience. A χ2 test was used to identify differences between groups. Results:
    Forty-two placentas were considered appropriate for training and were successfully
    perfused with dyes. Thirty-three participants completed the questionnaire, of
    which most, especially advanced and intermediate participants, indicated the placenta
    as a valuable, accurate, and reproducible model. No differences were observed
    among the groups. Conclusions: The human placenta is an excellent tool for vascular
    microneurosurgery laboratory training. Color perfusion enhances the reliability
    of this model, which was validated by most surgeons, regardless of their experience.'
  doi: 10.1016/j.wneu.2020.11.034
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Background Methods Results Conclusions Key words Introduction Methods
    Results Discussion Conclusions CRediT authorship contribution statement Acknowledgments
    Supplementary Data References Show full outline Cited by (18) Figures (10) Show
    4 more figures Tables (2) Table 1 Table 2 Extras (1) Video 1 World Neurosurgery
    Volume 146, February 2021, Pages e854-e864 Original Article Dye-Perfused Human
    Placenta for Vascular Microneurosurgery Training: Preparation Protocol and Validation
    Testing Author links open overlay panel Mattia Del Maestro 1 4, Angela Dele Rampini
    2, Simone Mauramati 1 5, Alice Giotta Lucifero 2, Giulia Bertino 5, Antonio Occhini
    5, Marco Benazzo 3 5, Renato Galzio 6, Sabino Luzzi 2 4 Show more Share Cite https://doi.org/10.1016/j.wneu.2020.11.034
    Get rights and content Background Human placenta is recognized as a valuable vascular
    microneurosurgery training model because of its abundant availability, ethical
    acceptance, and analogous vasculature with other vessels of the human body; however,
    human placenta laboratory preparation techniques are not well described in the
    literature. This study outlines a detailed and standardized laboratory protocol
    for preparation of a color-perfused human placenta model. Survey-based validation
    of the model is also reported herein. Methods The protocol involved cleaning and
    cannulation of the umbilical vein and arteries, irrigation with heparin, and storage
    at 3°C or freezing at −18°C. Before use, arteries were perfused with carmine/cochineal,
    and veins were perfused with methylthioninium chloride. A questionnaire with 5
    questions was administered to 40 participants among attending or resident neurosurgeons,
    otolaryngologists, and maxillofacial surgeons on 4 consecutive microsurgical courses
    to assess the reliability of the placenta model. Trainees were divided into 3
    groups based on their experience. A χ2 test was used to identify differences between
    groups. Results Forty-two placentas were considered appropriate for training and
    were successfully perfused with dyes. Thirty-three participants completed the
    questionnaire, of which most, especially advanced and intermediate participants,
    indicated the placenta as a valuable, accurate, and reproducible model. No differences
    were observed among the groups. Conclusions The human placenta is an excellent
    tool for vascular microneurosurgery laboratory training. Color perfusion enhances
    the reliability of this model, which was validated by most surgeons, regardless
    of their experience. Previous article in issue Next article in issue Key words
    BypassHuman placentaLaboratory trainingMicrosurgeryVascular anastomosisVascular
    neurosurgery Introduction Vascular microneurosurgery requires specific bimanual
    abilities to perform vessel dissection and anastomosis. Dissection and anastomosis
    can be achieved by means of regular exposure and experience in surgical procedures,
    together with constant laboratory training. For laboratory training, implementation
    of reliable vascular models is pivotal. Three main classes of microvascular training
    model are classically reported, namely, synthetic, ex vivo, and in vivo. Synthetic
    models make use of latex gloves, silicon tubes, endovascular protheses, and 3-dimensional
    printed samples, whereas in vivo models make use of live rats and pigs. In the
    context of the reported poor reliability of synthetic models and the ethical limitations
    of in vivo models,1 ex vivo models, namely, chicken wings, turkey neck, bovine
    and human placenta, and human and animal specimens, achieve a perfect compromise.
    In 1979, Goldstein first reported the use of fresh human placenta for microsurgical
    training.2 Later, other authors highlighted the potential advantages of human
    placenta for laboratory use, especially because of its availability and low cost.3,
    4, 5, 6, 7, 8, 9, 10 Nevertheless, despite many now well-known advantages, research
    on the optimal laboratory preparation technique for human placenta and its validation
    as a reliable training model is scant. The present study aimed to describe a detailed
    laboratory protocol for the implementation of a color-perfused human placenta
    model for vascular microneurosurgery training. The results of specifically designed
    survey-based validation of the placenta model are also presented. Methods This
    study was performed at Fondazione I.R.C.C.S. Policlinico San Matteo Pavia, Italy
    and at the Laboratory of Experimental Medicine of the University of Pavia, Italy.
    Forty-six fresh human placentas were used. Informed consent for donation of nonpathologic
    placentas for use in surgical training was obtained from obstetrics patients who
    tested negative for common infections via blood tests. Preparation of the Color-Perfused
    Human Placenta Model After delivery, the placenta and umbilical cord were cleaned
    with warm water (36°C). Care was taken not to damage the allantoid. The umbilical
    cord was cut axially at a distance of between 7 and 10 cm from the fetal surface,
    according to the cord length. The cord was then irrigated with warm saline solution
    to identify the umbilical vein and the 2 umbilical arteries, all of which were
    encased within connective tissue. The umbilical vein was cannulated using a 14-G
    or 16-G catheter (Abbocath, Abbott, Chicago, IL), depending on the vessel caliper,
    and cut at 45°. An initial saline flush was performed to remove clots and debris
    and to check flow. A second irrigation was performed using a 5000 IU/5 mL vial
    of sodic heparin with saline (1:4). The same process was repeated for both umbilical
    arteries. Afterwards, all catheters were secured together to the umbilical cord
    using a 0–0 silk thread to avoid pull-out during later saline infusion. The infusion
    was performed with a pressure bag line at 70–90 mm Hg, where the duration varied
    according to the efficacy of clot removal. Overpressure was avoided because it
    may cause damage to the vessel wall. As a general rule, achievement of a transparent
    conduit is proof of complete clot removal. The placenta is then kept in a sterile
    plastic bag (Figure 1). The specimen is then stored at a temperature of 3°C for
    use within the following 4 days, or frozen at –18°C, in which case it can be maintained
    for up to 40 days. Before use, the frozen placenta was defrosted at 20°C–22°C
    for 15 hours, and further cleaned with running warm water. After the patency of
    the vessels was checked, the placenta was perfused with 2 different infusion sets,
    1 for the arteries and 1 for the vein, which released 2 different dyes. Carmine/cochineal
    (E120) red colorant (Bioindustria L.I.M., Fresonara, AL, Italy) (3 mLvial) and
    methylthioninium chloride blue dye (100 mg/10 mL vial) (Bioindustria L.I.M.) were
    used for the arteries and the vein, respectively. Both dyes were diluted with
    saline (1:100). Infusion was stopped when the vessels appeared completely filled
    (Video 1). Overinfusion was avoided to prevent damage to the vessel wall. The
    entire preparation process took approximately 20 minutes (Figure 2). Download
    : Download high-res image (2MB) Download : Download full-size image Figure 1.
    Stepwise description of the laboratory protocol for implementation of a color-perfused
    human placenta model. (A) Fresh human placenta with the entire umbilical cord.
    (B) Axial cut of the umbilical cord. (C) Identification of the vein and arteries.
    (D–F) Cannulation of the vein and arteries and locking of the system. (G and H)
    Infusion of saline with the pressurized bag at 70–90 mm Hg (G). (I) Storage bag.
    Download : Download high-res image (1MB) Download : Download full-size image Figure
    2. (A) Dye-perfusion system of the placenta model. (B) Arterial and venous perfusion
    completed. (C) Details of relationship between perfused arteries, veins and the
    overlying allantoid membrane. Validation of the Human Placenta Model The human
    placenta model was validated during 4 consecutive editions of a hands-on microsurgical
    training course held at the Experimental Laboratory of Microsurgery, University
    of Pavia, Italy, between July 2018 and February 2020 (Figure 3). The course objective
    was to achieve a good level of skill and dexterity in performing microdissection
    of arterial and venous vessels and also to successfully execute the 3 main types
    of microvascular anastomosis, namely, end-to-end anastomosis, end-to-side anastomosis,
    and side-to-side anastomosis (Figure 4). The course participants were young attendants
    (<3 years from the residency) and residents in neurosurgery, otolaryngology, and
    maxillofacial surgery; a total of 40 participants were enrolled in the course.
    No vascular neurosurgeons were involved. Based on their experience and regardless
    of their specialization, all attendees were assigned to 1 of 3 groups: a low-experience
    (1st to 3rd years of residency), an intermediate-experience (4th and 5th years
    of residency), and an advanced group. A single prepared placenta was assigned
    to each workstation, and participants worked on the same model for 2 consecutive
    days. Participants were introduced to each aforementioned exercise in a step-by-step
    fashion. At the end of each course, a survey-based questionnaire was administered
    to each participant. The survey specifically included 5 consecutive questions
    aimed to assess the validity and reliability of the color-perfused human placenta
    model for vascular microsurgical training. The survey questions about the reliability
    of the model and the relative possible answers are reported in Table 1. Differences
    in categorical responses between the 3 groups were analyzed using a χ2 test using
    commercially available software (STATA 16, StataCorp LLC, College Station, TX).
    Download : Download high-res image (985KB) Download : Download full-size image
    Figure 3. (A) Microsurgical training workstation for a hands-on microsurgery course.
    (B) Overview of the laboratory with participants. (C) Participant performing anastomosis
    on the perfused placenta model. Download : Download high-res image (527KB) Download
    : Download full-size image Figure 4. End-to-end (A), end-to-side (B), and side-to-side
    (C) anastomoses performed by a young attendant in neurosurgery on the color-perfused
    placenta model. Table 1. Survey Questions Question n. Question Possible Answers
    1 Do you think that the proposed training model faithfully reproduces a possible
    real microsurgical scenario? “absolutely yes” “somewhat”, “absolutely no” 2 According
    to your surgical experience, is the consistency of the placental vessels comparable
    to the one of the in-vivo tissues? “very similar,” “quite similar”, “different”
    3 Do you think that the dissection of human placenta vessels is similar to in-vivo
    vessels dissection? 4 Do you think that practice on this type of surgical training
    model can improve the surgical technique and reduce errors on the patient? “absolutely
    yes” “somewhat”, “absolutely no” 5 Do you think you will reuse or propose to use
    of this microsurgical training model? “yes”, “no” Results Qualitative Assessment
    of the Human Placenta Model The mean placental weight was 480 ± 22 g, and the
    mean placental diameter was 17.5 cm. All placentas were characterized by an approximately
    central umbilical cord attachment. Forty-two placentas were considered suitable
    for preparation and training. The remaining placentas were excluded because of
    evidence of allantoid impairment or arterial damage at the umbilical cord, which
    prevented cannulation and perfusion. Eighteen specimens were stored at 3°C, and
    the remaining placentas were stored at –18°C. Ten of the frozen placentas were
    used 19 ± 4 days after preparation. Ten placentas were prepared after 24 hours,
    but never beyond 48 hours, from delivery. Transverse umbilical cord sections showed
    presence of 2 arteries and a single vein in all cases. In every placenta, umbilical
    arteries were easily distinguished from umbilical veins. The vein and the 2 arteries
    were cannulated successfully in all specimens. Five placentas had small-sized
    umbilical arteries and, in 3 of them, arterial cannulation was performed using
    a 20-G catheter. In the remaining 2 placentas, cannulation was executed more proximally.
    In consideration of the vessel fragility of these 5 specimens, saline infusion
    was carried out at a pressure of 40–60 mm Hg instead of the pressure of 70–90
    mm Hg reported in the protocol. It is important to note that small-sized umbilical
    arteries had some dye leaks after perfusion. Demographic Data and Group Set-up
    The mean age of attendees was 31 ± 4 years (range: 27–43 years). Twenty participants
    were neurosurgeons, of which 16 were residents (5 in 1st year, 3 in 2nd year,
    5 in 3rd year, and 3 in 4th year) and 4 were specialists. Ten attendees were otolaryngologists,
    3 were residents (2 in 3rd year and 1 in 4th year), and 7 were attending physicians.
    Ten participants were maxillofacial surgeons, of which 6 were residents (3 in
    3rd year and 3 in 4th year) and 4 were attending physicians. The low-experienced
    group consisted of 14 residents, and the intermediate-experience and advanced
    groups comprised 11 residents and 15 attending physicians, respectively. Survey-Based
    Validation of the Human Placenta Model Thirty-three participants completed the
    questionnaire, of which 13 belonged to the advanced group, 9 to the intermediate
    group, and 11 to the low-experience group. A total of 59%, 89%, and 89% of participants
    in the low-experience, intermediate, and advanced groups, respectively, answered
    “absolutely yes” to question 1 (Figure 5). The difference between the groups was
    not significant (χ2 = 4.0954, Pr = 0.129). For question 2, “very similar” was
    chosen by 54% of participants in the advanced group, 44% of participants in the
    intermediate group, and 36% of participants in the low-experience group (χ2 =
    1.8650, Pr = 0.761; Figure 6). For question 3, “very similar” was chosen by 46%,
    33%, and 45% of participants in the advanced, intermediate, and low-experience
    groups, respectively (χ2 = 0.8864, Pr = 0.926; Figure 7). All participants answered
    “absolutely yes” to question 4 (Figure 8) and “yes” to question 5 (Figure 9).
    Download : Download high-res image (430KB) Download : Download full-size image
    Figure 5. Questionnaire. Q1: Do you think tha thte proposed training model faithfully
    reproduces a possible real microsurgical scenario? Download : Download high-res
    image (408KB) Download : Download full-size image Figure 6. Questionnaire. Q2:
    According to your surgical experience, is the consistency of the placental vessels
    comparable to that of in vivo tissues? Download : Download high-res image (450KB)
    Download : Download full-size image Figure 7. Questionnaire. Q3: Do you think
    that the dissection of human placenta vessels is similar to in vivo vessel dissection?
    Download : Download high-res image (389KB) Download : Download full-size image
    Figure 8. Questionnaire. Q4: Do you think that practice on this type of surgical
    training model can improve surgical technique and reduce errors on the patient?
    Download : Download high-res image (259KB) Download : Download full-size image
    Figure 9. Questionnaire. Q5: Do you think you will reuse or propose to use this
    microsurgical training model? Discussion The present study reports a detailed
    laboratory preparation protocol for human placenta for the purpose of microneurosurgical
    vascular training. The model is extensible also to other specialties that deal
    routinely with microanastomosis. In our experience, use of color perfusion in
    this training model increases the similarity of the placental vasculature with
    other vessels of the human body. In addition, survey-based validation confirms
    the validity of this model across different specialties, regardless of the experience
    of the trainees. In 1979, Goldstein published the first report about the use of
    fresh human placenta for microsurgical training.2 Later, in 1983, McGregor et
    al. examined 10 human placentas to describe the detailed histology of the vessel
    wall, which is characterized by a thick adventitia and a lacking media.11 In 1992,
    Ayoubi et al. also highlighted the close similarity between the length and diameter
    of placental vessels and cerebral vessels.4 In 2008, Romero et al. described the
    main suturing exercises that can be performed on placental vessels, stressing
    the usefulness of this model for training.12 In the following years, microvascular
    training using the placenta became widespread, especially in neurovascular surgery.
    Several studies and training programs have been developed to create increasingly
    complex training exercises, with the aim of making a model that is realistic and
    close to in vivo models.13, 14, 15, 16, 17 A great contribution to the development
    of this training tool was made by Magaldi et al., who, in 2014, reported a technique
    to create and clip cerebral aneurysms using the human placenta model.5 The same
    group proposed the placenta model to simulate removal of brain tumors.18 As is
    widely reported, human placenta is an inexpensive and readily available tissue
    that can be used for microneurosurgical training. One of the main strengths of
    this model is the lack of ethical implications compared with other types of human
    or animal models,19, 20, 21, 22, 23, 24, 25 which has justified its widespread
    use in different laboratory activities. Our protocol for preparation, storage,
    and use of the color-perfused human placenta requires no more than 20 minutes
    and does not involve use of fixatives or detrimental agents. In light of this,
    the protocol can be considered safe for both trainers and trainees. Furthermore,
    use of fresh specimens has few equivalence in terms of resemblance to living tissues.
    Experience from our laboratory training activities has allowed us to verify that
    the human placenta remains in an excellent condition, even after freezing. Thawing
    ought to begin at least 15 hours before use and should take place at a constant
    temperature of 3°C to maintain tissue consistency. In this way, it is possible
    to use the same placenta for 4 consecutive days.5 From a technical standpoint,
    natural arborization of placental vessels and their diameters, which have been
    studied in detail elsewhere,8 make this model particularly suitable for creation
    of vascular anastomoses, which are commonly utilized in neurovascular surgery
    (Video 1, Figure 10). The large number and dimensional variability of placental
    vessels lead to significantly improved dexterity in vessel dissection and bypass.
    These skills have already been reported by our group as fundamental to every type
    of intracranial vascular surgery.26, 27, 28, 29, 30, 31, 32, 33, 34 In our experience,
    use of red and blue dyes significantly enhances identification of arteries and
    veins, respectively, and increases the resemblance between the training exercise
    and the real-life operative scenario. Nevertheless, it must be stressed that our
    perfusion technique is continuous, derived by the pressure gradient created by
    the pressure bag line, which acts up to the fulfilling of the vessels. The flow
    is not peristaltic, because it involves no vascular pulsatility. A further theoretical
    evolution of the model may come in the future from the implementation of a constant
    flow involving an arterial inflow and a venous outflow. The placenta microneurosurgical
    training model falls within a multistage learning paradigm where any improvement
    in preparation technique is useful to making the model increasingly realistic
    and may lead to theoretical replacement of in vivo models. Download : Download
    high-res image (586KB) Download : Download full-size image Figure 10. Overview
    of possible exercises and anastomoses using the human placenta model. The placenta
    model has undergone validation over the years. In 2016, Belykh et al. assessed
    the usefulness of microvascular bypass simulation models on human and bovine placental
    vessels by means of the Northwestern Objective Microanastomosis Assessment Tool
    scale and post-training survey.1 With the aim of objectively assessing the validity
    of this training model, they also followed the Objective Structured Assessment
    of Aneurysm Clipping Skills, which consists of a set of specific operative parameters
    for aneurysm-clipping surgery.6 In 2019, the Skills Assessment in Microsurgery
    for Brain Aneurysms scale was created as an interface between learning and practicing,
    highlighting the importance of this microsurgical training model on outcomes in
    a real surgical environment.7,8,35,36 Table 2 summarizes the main studies on the
    use of the placenta model for microsurgical training. Survey-based assessment
    of the placenta model has highlighted its appreciation among neurosurgeons, but
    also among other specialties, namely, otolaryngology and maxillofacial surgery.
    Focusing on neurovascular surgery, especially in the current era, which is characterized
    by a severe decrease in the number of cases, development of simulation models
    for neurosurgery training programs is recognized as being of utmost importance
    worldwide. Finally, special consideration should be reserved for the need to verify
    teaching efficacy with this training model by means of reliable and reproducible
    rating scales.35,36 Table 2. Summary of Studies on the Use of Placenta Model for
    Microsurgical Training First Author, Year Aim of the Study Species Preparation
    Process Type of Exercises Goldstein, 19792 To report the overall advantages of
    the placenta model for training Human Not reported Not reported McGregor, 198311
    Human placenta anatomy description and training model Human Not reported Vessel
    dissection Ayoubi, 19924 Use of placenta for microvascular training Human Not
    reported Not reported Romero, 200812 Training model description and exercises
    Human Not reported Vessels anastomosis Oliveira, 20145 To describe model for aneurysms
    clipping Human Reported Sylvian-like dissection/aneurysms clipping Belykh, 201537
    To describe a new haptic CEA model. Bovine Not reported Simulation of arteriotomy
    Oliveira, 201618 To simulate tumor resection using placenta Human Reported Microsurgical
    tumor removal and vessels anastomosis Belykh, 20161 To describe microvascular
    bypass simulator Human and bovine Reported Vessels anastomosis Belykh, 20176 To
    validate human placenta aneurysm model Human and bovine Reported Sylvian-like
    dissection and aneurysms clipping Oliveira, 20187 To compare aneurysm clipping
    in cadaver and placenta models Human Reported Aneurysms clipping Oliveira, 20188
    To evaluate IC-IC bypass apprenticeship Human Not reported Vessels anastomosis
    Oliveira, 201936 To validate aneurysm simulator Human Not reported Aneurysm clipping
    Oliveira, 201935 To describe and validate a skills assessment instrument Human
    Not reported Aneurysm clipping CEA, carotid endoarterectomy; IC-IC, intracranial-to-intracranial.
    Limitations Despite the aforementioned advantages, there are still limitations
    that should be considered when using the placenta model. First, absence of real
    blood prevents the in vivo effects of coagulation on thrombus formation after
    execution of bypass. Second, absence of natural vessel pulsation does not allow
    one to verify the tightness of anastomoses in a realistic way. Finally, the proposed
    model only works in a superficial field; therefore, it is necessary to combine
    it with other simulation tools (e.g., 3-dimensional printed skull, glass) to generate
    a more difficult surgical scenario that encompasses deep and narrow spaces. In
    terms of study design, a non-negligible weakness lies in survey-based model validation,
    which unavoidably entails potential bias due to subjective data interpretation.
    Conclusions The human placenta model is technically excellent for vascular microneurosurgery
    laboratory training. Its low cost, availability, ease of preparation, ethical
    acceptance, and strong vascular resemblance to other anatomical structure make
    it a valuable alternative to in vivo models. Color perfusion of placental arteries
    and veins enhances the overall reliability of this model, which, with validation
    testing, has proven to be a worthwhile training tool by most attending physicians
    and residents, regardless of their experience. Efforts to implement peristaltic
    perfusion and further validation studies based on reliable rating scales are both
    necessary to increase the reliability of the placenta model. CRediT authorship
    contribution statement Mattia Del Maestro: Conceptualization, Writing - original
    draft. Angela Dele Rampini: Conceptualization, Writing - original draft. Simone
    Mauramati: Conceptualization, Resources. Alice Giotta Lucifero: Conceptualization,
    Resources. Giulia Bertino: Visualization, Methodology. Antonio Occhini: Visualization,
    Methodology. Marco Benazzo: Supervision. Renato Galzio: Supervision. Sabino Luzzi:
    Writing - review & editing, Supervision. Acknowledgments We want to thank the
    Equipe of Gynecology and Obstetrics of Fondazione I.R.C.C.S. Policlinico San Matteo
    Pavia for their precious support in collecting and storing fresh human placenta.
    Supplementary Data Play Restart Rewind Forward Slower Faster Preferences Enter
    full screen 0:00 / 1:36 Speed: 1x Stopped Download : Download video (24MB) Video
    1. Illustrative video showing the colored perfusion of the placenta model and
    the types of microvascular anastomosis that can be performed. References 1 E.
    Belykh, T. Lei, S. Safavi-Abbasi, et al. Low-flow and high-flow neurosurgical
    bypass and anastomosis training models using human and bovine placental vessels:
    a histological analysis and validation study J Neurosurg, 125 (2016), pp. 915-928
    View in ScopusGoogle Scholar 2 M. Goldstein Use of fresh human placenta for microsurgical
    training J Microsurg, 1 (1979), pp. 70-71 CrossRefView in ScopusGoogle Scholar
    3 F.C. Gallardo, J.L. Bustamante, C. Martin, et al. Novel simulation model with
    pulsatile flow system for microvascular training, research, and improving patient
    surgical outcomes World Neurosurg, 143 (2020), pp. 11-16 View PDFView articleView
    in ScopusGoogle Scholar 4 S. Ayoubi, P. Ward, S. Naik, M. Sankaran The use of
    placenta in a microvascular exercise Neurosurgery, 30 (1992), pp. 252-254 CrossRefView
    in ScopusGoogle Scholar 5 M. Oliveira Magaldi, A. Nicolato, J.V. Godinho, et al.
    Human placenta aneurysm model for training neurosurgeons in vascular microsurgery
    Neurosurgery, 10 (Suppl 4) (2014), pp. 592-600 [discussion: 600-591] Google Scholar
    6 E. Belykh, E.J. Miller, T. Lei, et al. Face, content, and construct validity
    of an aneurysm clipping model using human placenta World Neurosurg, 105 (2017),
    pp. 952-960.e952 View in ScopusGoogle Scholar 7 M.M.R. de Oliveira, C.E. Ferrarez,
    T.M. Ramos, et al. Learning brain aneurysm microsurgical skills in a human placenta
    model: predictive validity J Neurosurg, 128 (2018), pp. 846-852 CrossRefGoogle
    Scholar 8 M.M. Oliveira, L. Wendling, J.A. Malheiros, et al. Human placenta simulator
    for intracranial-intracranial bypass: vascular anatomy and 5 bypass techniques
    World Neurosurg, 119 (2018), pp. e694-e702 View PDFView articleView in ScopusGoogle
    Scholar 9 E. Trignano, N. Fallico, G. Zingone, L.A. Dessy, G.V. Campus Microsurgical
    training with the three-step approach J Reconstr Microsurg, 33 (2017), pp. 87-91
    Google Scholar 10 N. Waterhouse, A.L. Moss, P.L. Townsend The development of a
    dynamic model for microvascular research and practice using human placenta: a
    preliminary report Br J Plast Surg, 38 (1985), pp. 389-393 View PDFView articleView
    in ScopusGoogle Scholar 11 J.C. McGregor, F.J. Wyllie, K.M. Grigor Some anatomical
    observations on the human placenta as applied to microvascular surgical practice
    Br J Plast Surg, 36 (1983), pp. 387-391 View PDFView articleView in ScopusGoogle
    Scholar 12 F.R. Romero, S.T. Fernandes, F. Chaddad-Neto, J.G. Ramos, J.M. Campos,
    E. Oliveira Microsurgical techniques using human placenta Arq Neuropsiquiatr,
    66 (2008), pp. 876-878 View in ScopusGoogle Scholar 13 A. Beris, I. Kostas-Agnantis,
    I. Gkiatas, D. Gatsios, D. Fotiadis, A. Korompilias Microsurgery training: a combined
    educational program Injury (2020), 10.1016/j.injury.2020.03.016 accessed September
    19, 2020 Google Scholar 14 V.G. Ilie, V.I. Ilie, C. Dobreanu, N. Ghetu, S. Luchian,
    D. Pieptu Training of microsurgical skills on nonliving models Microsurgery, 28
    (2008), pp. 571-577 CrossRefView in ScopusGoogle Scholar 15 T. Menovsky, D. De
    Ridder Training of deep microsurgical skills Microsurgery, 28 (2008), pp. 390-391
    CrossRefView in ScopusGoogle Scholar 16 M. Takeuchi, N. Hayashi, H. Hamada, N.
    Matsumura, H. Nishijo, S. Endo A new training method to improve deep microsurgical
    skills using a mannequin head Microsurgery, 28 (2008), pp. 168-170 CrossRefView
    in ScopusGoogle Scholar 17 M. Higurashi, Y. Qian, M. Zecca, Y.K. Park, M. Umezu,
    M.K. Morgan Surgical training technology for cerebrovascular anastomosis J Clin
    Neurosci, 21 (2014), pp. 554-558 View PDFView articleView in ScopusGoogle Scholar
    18 M.M. Oliveira, A.B. Araujo, A. Nicolato, et al. Face, content, and construct
    validity of brain tumor microsurgery simulation using a human placenta model Oper
    Neurosurg (Hagerstown), 12 (2016), pp. 61-67 View in ScopusGoogle Scholar 19 A.
    Lahiri, S.S. Muttath, S.K. Yusoff, A.K. Chong Maintaining effective microsurgery
    training with reduced utilisation of live rats J Hand Surg Asian Pac Vol, 25 (2020),
    pp. 206-213 CrossRefView in ScopusGoogle Scholar 20 J.R. Rodriguez, R. Yañez,
    I. Cifuentes, J. Varas, B. Dagnino Microsurgery workout: a novel simulation training
    curriculum based on nonliving models Plast Reconstr Surg, 138 (2016), pp. 739e-747e
    Google Scholar 21 S. Schoeff, B. Hernandez, D.J. Robinson, M.J. Jameson, D.C.
    Shonka Jr. Microvascular anastomosis simulation using a chicken thigh model: Interval
    versus massed training Laryngoscope, 127 (2017), pp. 2490-2494 CrossRefView in
    ScopusGoogle Scholar 22 D. Masud, N. Haram, M. Moustaki, W. Chow, S. Saour, P.N.
    Mohanna Microsurgery simulation training system and set up: an essential system
    to complement every training programme J Plast Reconstr Aesthet Surg, 70 (2017),
    pp. 893-900 View PDFView articleView in ScopusGoogle Scholar 23 F.X. Creighton,
    A.L. Feng, N. Goyal, K. Emerick, D. Deschler Chicken thigh microvascular training
    model improves resident surgical skills Laryngoscope Investig Otolaryngol, 2 (2017),
    pp. 471-474 CrossRefView in ScopusGoogle Scholar 24 A. Hino Training in microvascular
    surgery using a chicken wing artery Neurosurgery, 52 (2003), pp. 1495-1498 View
    in ScopusGoogle Scholar 25 J.Y. Bao Rat tail: a useful model for microvascular
    training Microsurgery, 16 (1995), pp. 122-125 CrossRefView in ScopusGoogle Scholar
    26 A. Ricci, H. Di Vitantonio, D. De Paulis, et al. Cortical aneurysms of the
    middle cerebral artery: a review of the literature Surg Neurol Int, 8 (2017),
    p. 117 View in ScopusGoogle Scholar 27 M. Del Maestro, S. Luzzi, M. Gallieni,
    et al. Surgical treatment of arteriovenous malformations: role of preoperative
    staged embolization Acta Neurochir Suppl, 129 (2018), pp. 109-113 CrossRefView
    in ScopusGoogle Scholar 28 M. Gallieni, M. Del Maestro, S. Luzzi, D. Trovarelli,
    A. Ricci, R. Galzio Endoscope-assisted microneurosurgery for intracranial aneurysms:
    operative technique, reliability, and feasibility based on 14 years of personal
    experience Acta Neurochir Suppl, 129 (2018), pp. 19-24 CrossRefView in ScopusGoogle
    Scholar 29 S. Luzzi, M. Del Maestro, D. Bongetta, et al. Onyx embolization before
    the surgical treatment of grade iii spetzler-martin brain arteriovenous malformations:
    single-center experience and technical nuances World Neurosurg, 116 (2018), pp.
    e340-e353 View PDFView articleView in ScopusGoogle Scholar 30 S. Luzzi, M. Gallieni,
    M. Del Maestro, D. Trovarelli, A. Ricci, R. Galzio Giant and very large intracranial
    aneurysms: surgical strategies and special issues Acta Neurochir Suppl, 129 (2018),
    pp. 25-31 CrossRefView in ScopusGoogle Scholar 31 S. Luzzi, M. Del Maestro, R.
    Galzio Letter to the editor. Preoperative embolization of brain arteriovenous
    malformations J Neurosurg, 132 (2019), pp. 2014-2016 Google Scholar 32 S. Luzzi,
    A. Elia, M. Del Maestro, et al. Indication, timing, and surgical treatment of
    spontaneous intracerebral hemorrhage: systematic review and proposal of a management
    algorithm World Neurosurg, 124 (2019), pp. e769-e778 View PDFView articleView
    in ScopusGoogle Scholar 33 S. Luzzi, M. Del Maestro, S.K. Elbabaa, R. Galzio Letter
    to the editor regarding \"one and done: multimodal treatment of pediatric cerebral
    arteriovenous malformations in a single anesthesia event\" World Neurosurg, 134
    (2020), p. 660 View PDFView articleView in ScopusGoogle Scholar 34 S. Luzzi, C.
    Gragnaniello, A. Giotta Lucifero, M. Del Maestro, R. Galzio Surgical management
    of giant intracranial aneurysms: overall results of a large series World Neurosurg,
    144 (2020), pp. e119-e137 View PDFView articleView in ScopusGoogle Scholar 35
    M.M. Ribeiro de Oliveira, T.M. Ramos, C.E. Ferrarez, et al. Development and validation
    of the Skills Assessment in Microsurgery for Brain Aneurysms (SAMBA) instrument
    for predicting proficiency in aneurysm surgery J Neurosurg, 133 (2019), pp. 190-196
    Google Scholar 36 M.M. Oliveira, C.E. Ferrarez, R. Lovato, et al. Quality Assurance
    during brain aneurysm microsurgery-operative error teaching World Neurosurg, 130
    (2019), pp. e112-e116 View PDFView articleView in ScopusGoogle Scholar 37 E.G.
    Belykh, T. Lei, M.M. Oliveira, et al. Carotid endarterectomy surgical simulation
    model using a bovine placenta vessel Neurosurgery, 77 (2015), pp. 825-829 [discussion:
    829-830] View in ScopusGoogle Scholar Cited by (18) Topographical Systematization
    of Human Placenta Model for Training in Microneurosurgery 2024, World Neurosurgery
    Show abstract Photo-Stacking Technique for Neuroanatomical High-Definition Photography
    and 3-Dimensional Modeling 2023, World Neurosurgery Show abstract Paraclinoid
    aneurysms: Outcome analysis and technical remarks of a microsurgical series 2022,
    Interdisciplinary Neurosurgery: Advanced Techniques and Case Management Citation
    Excerpt : Our and other groups have previously demonstrated and reported that
    these valid technologies, as indocyanine green and fluorescein videoangiography,
    micro-Doppler ultrasonography, Doppler flowmetry, and intraoperative DSA, are
    important tools in neurovascular surgery armamentarium, since they contribute
    to decreasing the overall morbidity of intracranial aneurysms and arteriovenous
    malformations [117–122]. We strongly recognize in the microsurgical training and
    cooperation within a neurovascular team the most important aspect to optimize
    the patient outcome and avoid complications [123,124]. The present study has some
    important limitations which can be summarized in the retrospective nature of the
    analysis, the relatively limited number of patients, and the heterogeneity of
    the surgical and endovascular teams. Show abstract How we do it: the Zurich Microsurgery
    Lab technique for placenta preparation 2023, Acta Neurochirurgica Seven bypasses
    simulation set: description and validity assessment of novel models for microneurosurgical
    training 2023, Journal of Neurosurgery Dye-Perfused Human Placenta for Simulation
    in a Microsurgery Laboratory for Plastic Surgeons 2023, Archives of Plastic Surgery
    View all citing articles on Scopus Conflict of interest statement: The authors
    declare that the article content was composed in the absence of any commercial
    or financial relationships that could be construed as a potential conflict of
    interest. View Abstract © 2020 Elsevier Inc. All rights reserved. Recommended
    articles Low Glasgow Coma Score in Traumatic Intracranial Hemorrhage Predicts
    Development of Cerebral Vasospasm World Neurosurgery, Volume 120, 2018, pp. e68-e71
    Fawaz Al-Mufti, …, Chirag D. Gandhi View PDF Implementation of clinical references
    for undergraduates in anatomy Annals of Anatomy - Anatomischer Anzeiger, Volume
    210, 2017, pp. 164-169 André Kranz, …, Sabine Löffler View PDF An unexpected diagnosis
    causing a neck lump and fevers European Annals of Otorhinolaryngology, Head and
    Neck Diseases, Volume 138, Issue 1, 2021, p. 63 R. Chessman, …, H. Daya View PDF
    Show 3 more articles Article Metrics Citations Citation Indexes: 18 Captures Readers:
    16 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: World Neurosurgery
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Dye-Perfused Human Placenta for Vascular Microneurosurgery Training: Preparation
    Protocol and Validation Testing'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Malakhov A.V.
  - Mitrofanov I.G.
  - Litvak M.L.
  - Sanin A.B.
  - Golovin D.V.
  - Djachkova M.V.
  - Nikiforov S.Y.
  - Anikin A.A.
  - Lisov D.I.
  - Lukyanov N.V.
  - Mokrousov M.I.
  citation_count: '12'
  description: 'Abstract: We present the first results of Mars neutron sounding by
    FREND instrument onboard ESA’s Trace Gas Orbiter data analysis. Neutron flux mapping
    is performed with high spatial resolution, which allows to match epithermal neutron
    flux variation with relief’s geomorphological structures. Local regions with suppressed
    neutron flux indicate the presence of considerable amount of hydrogen atoms in
    the surface material, which is supposedly part of water molecules. Local regions
    in the equatorial latitudes of the planet with significant decrease of neutron
    flux are discovered, which points to large mass fraction of water ice in the soil
    material, ranging from tens to 100%. Considering high water content, these regions
    are named ice permafrost ‘‘oases.’’ Estimates of ice mass fraction for 7 such
    regions are obtained based on neutron measurements analysis and end-to-end numerical
    modeling of the entire physical process of neutron sounding by FREND on Mars orbit.
    Possible reasons for formation of such ‘‘oases’’ and their significance for future
    Mars exploration are discussed.'
  doi: 10.1134/S1063773720060079
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Astronomy Letters Article Ice Permafrost
    ‘‘Oases’’ Close to Martian Equator: Planet Neutron Mapping Based on Data of FREND
    Instrument Onboard TGO Orbiter of Russian-European ExoMars Mission Published:
    22 October 2020 Volume 46, pages 407–421, (2020) Cite this article Download PDF
    Access provided by University of Nebraska-Lincoln Astronomy Letters Aims and scope
    Submit manuscript A. V. Malakhov, I. G. Mitrofanov, M. L. Litvak, A. B. Sanin,
    D. V. Golovin, M. V. Djachkova, S. Yu. Nikiforov, A. A. Anikin, D. I. Lisov, N.
    V. Lukyanov & M. I. Mokrousov  263 Accesses 12 Citations Explore all metrics Abstract
    We present the first results of Mars neutron sounding by FREND instrument onboard
    ESA’s Trace Gas Orbiter data analysis. Neutron flux mapping is performed with
    high spatial resolution, which allows to match epithermal neutron flux variation
    with relief’s geomorphological structures. Local regions with suppressed neutron
    flux indicate the presence of considerable amount of hydrogen atoms in the surface
    material, which is supposedly part of water molecules. Local regions in the equatorial
    latitudes of the planet with significant decrease of neutron flux are discovered,
    which points to large mass fraction of water ice in the soil material, ranging
    from tens to 100 . Considering high water content, these regions are named ice
    permafrost ‘‘oases.’’ Estimates of ice mass fraction for 7 such regions are obtained
    based on neutron measurements analysis and end-to-end numerical modeling of the
    entire physical process of neutron sounding by FREND on Mars orbit. Possible reasons
    for formation of such ‘‘oases’’ and their significance for future Mars exploration
    are discussed. Similar content being viewed by others Geological context of potential
    landing site of the Luna-Glob mission Article 12 November 2014 Active neutron
    sensing of the Martian surface with the DAN experiment onboard the NASA “Curiosity”
    Mars rover: Two types of soil with different water content in the gale crater
    Article 01 April 2016 Water in the Moon’s polar areas: Results of LEND neutron
    telescope mapping Article 01 February 2016 INTRODUCTION The study of water abundance
    in the near-surface layer of the matter is one of the main areas of space exploration
    of Mars since water probably played a key role in its evolution. In addition,
    liquid aquatic environment is a prerequisite for the origin and development of
    life similar to that of the Earth (Grotzinger 2014; McKay and Stoker 1989), and
    if such life has ever existed on the ‘‘red planet,’’ then signs of its existence
    should be sought in the ice of the contemporary ice permafrost. Nuclear planetology
    methods have long been used in researches of the Moon, Venus, Mars, Mercury, and
    asteroids for the study of regolith’s elemental composition, including the assessment
    of hydrogen content in it, which is most likely bound in water molecules (Fialips
    et al. 2005). An obvious advantage of the nuclear-physical method of neutron sensing
    is its sensitivity to hydrogen content in the near-surface regolith layer with
    a thickness of 1–2 m (Drake et al. 1988). This circumstance is extremely important
    for Mars, where water cannot exist on the surface due to low atmospheric pressure
    (Forget et al. 2006). Neutrons with energies of several tens of MeV are produced
    in the upper layer of the Martian soil as a result of the impact of heavy energetic
    particles of galactic cosmic rays (GCR) with the nuclei of rock-forming elements.
    Diffusing through the soil, neutrons slow down due to collisions with the nuclei
    of the matter. Collisions with hydrogen nuclei (i.e., with protons equal in mass
    to the neutron) make the process of neutron deceleration most efficient—the neutron
    loses energy comparable to its energy before the collision. Thus, the higher is
    the concentration of hydrogen in the soil, the more thermal neutrons and the less
    epithermal neutrons are there in the flux radiated from Mars. The method of measuring
    hydrogen content in the soil of a celestial body, based on epithermal neutron
    flux variations measurements, is used in the present study (see Drake et al. 1988
    for details). During more than 15 years of operation of the HEND neutron detector
    aboard the NASA’s Mars-Odyssey spacecraft, a huge amount of Mars neutron sounding
    data has been accumulated, the analysis of which made it possible to construct
    a global map of hydrogen distribution in its regolith (Mitrofanov et al. 2002).
    This map’s spatial resolution is about 550 km (Maurice et al. 2011). This resolution
    is determined by the instrument’s omnidirectional field of view in orbit with
    an altitude of about 400 km. Neutron mapping in the HEND experiment revealed significant
    masses of water in the upper layers of the Martian surface (Mitrofanov 2002).
    But low spatial resolution did not allow the use of such maps in relation to relief
    features or for the landing sites analysis for future missions: the characteristic
    size of craters or other relief structures of Mars ranges from several tens to
    200–300 km. The FREND instrument (Fine Resolution Epithermal Neutron Detector)
    installed on the ESA TGO spacecraft (Trace Gas Orbiter, see Vago 2015) is designed
    to solve this problem, since its data make it possible to construct maps of epithermal
    neutron emission from Mars with a high spatial resolution of 60 to 200 km, given
    the altitude of TGO orbit of 400 km (Mitrofanov et al. 2018). METHOD OF NEUTRONS’
    COLLIMATION IN THE FREND EXPERIMENT The main feature of Mars neutron sounding
    by the FREND experiment is collimation of the instrument’s field of view. An omnidirectional
    neutron detector in orbit around the planet is sensitive to all neutrons arriving
    from any direction within the visible horizon. To increase spatial resolution,
    the instrument’s detectors should be surrounded by a collimator, which will absorb
    neutrons coming from outside of the narrow field of view with an axis directed
    to the nadir (Mitrofanov et al. 2008). In the FREND instrument, such a collimator
    consists of an outer layer of high-pressure polyethylene and an inner layer containing
    a B boron isotope, which has an exceptionally large absorption cross section for
    thermal neutrons. In the polyethylene layer, epithermal neutrons arriving from
    outside of the field of view are slowed down to thermal energies, and then absorbed
    in the inner layer (Mitrofanov et al. 2018). The main problem in the design of
    such an instrument was the mass limitation—it was this external parameter that
    determined the sensitivity of the instrument for neutrons within its field of
    view and the efficiency of their collimation for directions outside this field.
    The neutron collimation method was successfully tested in the experiment with
    the LEND instrument in the NASA LRO project (Mitrofanov et al. 2008). The FREND
    instrument collimator architecture is inherited from that experiment (see Mitrofanov
    et al. 2018). High spatial resolution allows to match spatial distribution of
    water mass fraction in the regolith with relief structures of Mars surface and
    also to take into account the presence of water when choosing landing sites for
    future landing missions. ON WATER IN THE REGOLITH OF MARS The hydrogen content
    in the near-surface regolith can be estimated based on measurements of the epithermal
    neutron flux in orbit around Mars. Traditionally, for such estimates, the mass
    fraction of water equivalent hydrogen (WEH, wt ) is used, as if all hydrogen was
    contained in H O molecules. According to generally accepted notions, water in
    the Martian regolith can be present in three forms: firstly, water molecules can
    condense from the atmosphere on the surface of regolith particles (adsorbed water).
    Its content depends on the specific surface of regolith particles and is only
    a few percent by mass. Besides, mass fraction of water can undergo seasonal changes.
    In spring and summer, the regolith heats up and water molecules partially evaporate
    into the atmosphere. In autumn and winter, the soil becomes cold, and water molecules
    from the atmosphere condense in its near-surface layer (Vincendon et al. 2010).
    Secondly, water or hydroxyl molecules can be embedded in minerals molecules. Such
    chemically bound water was included into the molecules of hydrated minerals during
    their formation in the early era of Mars evolution, when free water bodies covered
    the surface of the planet. The mass fraction of this form of water depends on
    the degree of hydration and the chemical composition of the mineral and usually
    does not exceed 15 by mass (Fialips et al. 2005). Finally, thirdly, in the near-surface
    layer of the Martian permafrost, free water molecules can be present in the form
    of water ice. It is known that water ice together with carbon dioxide ice is present
    in the polar caps of contemporary Mars (see, for example, Litvak et al. 2007).
    It is also part of the permafrost at latitudes above , where its mass fraction
    can be tens of percent (Mitrofanov et al. 2004). It is assumed that the presence
    of water in the regolith with a large mass fraction of wt cannot be explained
    by the presence of hydrated minerals. In these cases, water ice is supposed to
    be present in the ground. Fig. 1 Martian surface epithermal neutron flux counting
    rate map, represented by SEN parameter values characterizing its attenuation relative
    to the reference value in Solis Planum region. SEN values scale is compared to
    water mass fraction values (WEH parameter). The map is smoothed with a Gaussian
    filter with half-height width of . Relief is plotted according to MOLA data (Smith
    2001). Full size image The boundary of the polar permafrost on contemporary Mars
    corresponds to the condition of the surface heating at the current polar axis
    to the Sun direction inclination angle . It is known that this angle undergoes
    periodic variations, and epochs periodically occurred on Mars, when, due to the
    large angle of deviation of the polar axis of up to (Forget et al. 2006; Jakosky
    et al. 2005; Schulz and Lutz 1988), ice permafrost arose in the vicinity of its
    equator. The relict remains of such permafrost can be preserved in the modern
    era in separate near-equatorial regions with a favorable relief. Such ‘‘oases’’
    of permafrost are of particular interest since the ice that remained in them was
    liquid water in the past Martian era. Dissolved chemical compounds reflect those
    physicochemical processes that were taking place on the planet at that time. Complex
    molecular compounds can be preserved in the relict Martian ice, indicating biochemical
    processes in ancient Martian water bodies associated with the presence of the
    simplest forms of Martian life. EPITHERMAL NEUTRON EMISSION MAP OF THE MARTIAN
    SURFACE At present the main result of the FREND experiment is a map of epithermal
    neutron count rate variability measured from TGO in the near-Martian orbit. The
    orbit of the TGO satellite has an inclination of , thus FREND can map Mars neutron
    radiation only in the latitudinal range of to . The map is obtained based on the
    measurement data during 678 Martian days (sols) from April 27, 2018 to March 6,
    2020. To create maps, surface pixels with a size of are used. At the equator,
    such a pixel corresponds to a surface area of km. The initial measurement data
    are pre-processed to consider external factors affecting the instrument’s counting
    rate. These are variations of the GCR flux and the solid angle of Mars, seen from
    the orbit. In addition, excluded from the total counting rate are the contribution
    of background radiation of the spacecraft itself and also the contribution of
    neutrons coming from directions outside the field of view of the instrument through
    the collimator walls. For the convenience of working with the data, we introduced
    a dimensionless empirical parameter, which shows the epithermal neutrons count
    rate reduction relative to a certain reference value (Suppression of epithermal
    neutrons, or SEN). The average rate in the vicinity of Solis Planum region with
    coordinates of to longitude and of to latitude was used as a reference value of
    the neutron count rate for estimating the SEN parameter. The neutron radiation
    map measured in the FREND experiment is presented in Fig. 1for the SEN value.
    This map is smoothed with a Gaussian filter with a half-height width of . The
    most important basis for the analysis of neutron water mapping data is the knowledge
    of the relationship between the SEN parameter and the water content WEH in the
    Martian soil. This ratio of SEN vs. WEH (Fig. 2) was obtained based on a through
    numerical simulation of the entire physical process of neutron sounding of the
    surface by the FREND instrument in Mars orbit, starting from the process of bombarding
    the surface with GCR particles and ending with the registration of counts from
    epithermal neutrons in the instrument’s detectors. Important initial assumptions
    for modeling are the composition of the rock-forming soil elements and the structure
    of the surface layer. It was assumed that the composition of the soil does not
    change with depth and corresponds to the values characteristic of measurements
    in the APXS experiment on the Curiosity rover (Gellert and Clark 2015). A numerical
    calculation showed that the value of the SEN parameter decreases monotonically
    with increasing WEH (Fig. 2), and the dynamic range of this variability is quite
    large: with an increase of water mass fraction from 5 to 15 , the SEN parameter
    decreases by a factor of 2 (Fig. 2). Fig. 2 Dependence of the SEN parameter values
    from the soil water content (WEH). SEN parameter characterizes the attenuation
    of the Martian surface epithermal neutron flux relative to the reference value
    of 1 in Solis Planum region. Full size image The Solis Planum region is known
    to be one of the driest on Mars with an average water content of WEH (Boynton
    et al. 2007a). Areas with counting rate less than in Solis Planum (i.e., with
    SEN <1) should have water content > WEH , and vice versa - in areas with counting
    rate greater than in Solis Planum (i.e., with SEN > 1) the water mass fraction
    should be <WEH (Fig. 2). Thanks to the established relationship between the measured
    SEN parameter and the WEH value, Mars neutron emission map (Fig. 1) can be presented
    using both the empirical SEN parameter and the corresponding WEH values of water
    mass fraction in the soil. The dispersion in values of the SEN parameters is estimated
    from the statistical uncertainty of the initial measurement data at the level
    of 1 , while the errors in the estimation of the WEH values are determined based
    on the relationship between SEN and WEH. POTENTIAL DIVISION OF THE MARTIAN SURFACE
    INTO HYDROLOGICAL TYPES Large-scale global maps of hydrogen distribution in the
    near-surface regolith of Mars were constructed from measurements of the HEND,
    MONS, and GRS instruments aboard the NASA Mars Odyssey satellite (Boynton 2007a).
    Building a similar map was not a part of the experiment with the FREND instrument.
    The main interest of the FREND experiment is represented by small (with a characteristic
    size of hundreds of kilometers) local areas near the equator and at temperate
    latitudes with unexpectedly large ground water content compared to their surrounding
    areas—those with high water content, that is, permafrost ‘‘oases.’’ On the map
    of SEN parameter values smoothed with a Gaussian filter of (Fig. 1), two isolevels
    SEN and SEN were plotted, corresponding to the values of WEH 5 and 15 wt , respectively,
    which separate the range of values of the SEN neutron flux attenuation parameter
    [0, 1] into three approximately equal intervals (Fig. 3). Based on the relationship
    between SEN values and mass fraction of water WEH in the regolith (SEN vs. WEH,
    Fig. 2), the indicated intervals of the parameter are associated with three hydrological
    types of the Martian surface with a small, moderate, and high averaged mass water
    fraction in the regolith: these are, respectively, the first type with , the second
    type with SEN , and the third type with SEN (beige, blue, and dark blue on the
    map in Fig. 3). Fig. 3 Martian surface hydrological types. The areas with water
    content of 0–5 WEH (beige), 5–15 WEH (blue), and above 15 WEH (dark blue) are
    highlighted with color. Full size image Fig. 4 SEN values pixels distribution
    of the first and the third hydrological groups, presented in the form of probability
    density (dashed and solid lines, respectively). The dash-dotted vertical line
    marks the formal boundary between the distributions at SEN , at which the probability
    densities of the two distributions are equal. Full size image In this study, we
    are interested in the search for local regions of the Martian surface of the third
    type, that is, of an area with a high mass water fraction. To search for such
    local areas, the analysis of smoothed maps (Figs. 1 and 3) may turn out to be
    insufficient. The statistical certainty of measurements for each individual pixel
    of the initial map does not allow a sufficiently reliable estimation of the SEN
    parameter to reliably identify this pixel with one or another hydrological type
    of surface. However, the entire set of measurement data can confirm the physical
    validity of the proposed separation of surface pixels into hydrological types.
    Figure 4 shows statistical distributions of the SEN parameter values for pixels
    of the first and the third surface types with and , respectively. Obviously, these
    distributions are significantly different from each other. They are normalized
    to 1 and they have the physical meaning of probability density distributions for
    the values of SEN parameter. A significant contribution to the width of the distributions
    is made by statistical fluctuations of the initial measurements—negative values
    of the SEN parameter for individual pixels do not have physical meaning and arise
    due to such fluctuations. Table 1 Neutron flux parameters and estimates of water
    mass fraction for the first and the third hydrological types of the Martian surface
    Full size table The intersection point of two distributions (Fig. 4) corresponds
    to the value SEN . According to the calculated dependence of SEN vs. WEH (Fig.
    2), this value corresponds to the mass water fraction of about 9 wt . The value
    of SEN can conditionally be considered the boundary between two distributions,
    for which the probability densities for the left and right distributions coincide.
    For pixels with SEN < SEN (interval [0.0–0.5]), the probability of belonging to
    the third type of regolith is much higher than to the first type, and vice versa,
    for pixels with SEN > SEN (interval [0.5–1.0] ) belonging of the pixel to the
    first type of surface is more likely. The average values of SEN and WEH and their
    corresponding standard deviations ( ) for the distributions of the first and third
    surface types (Fig. 4) are presented in Table 1. The above noted formal boundary
    between the two distributions is in good agreement with the lower limit ( for
    the right distribution (first type of surface with low water content), as well
    as with the upper limit ( for the left distribution (third type of surface with
    a high water content). Thus, the excess of mass fraction of water WEH of 9 wt
    may indicate the third type of surface with high water content in the composition
    of the regolith. However, it should be borne in mind that the global map of Mars
    contains vast areas corresponding to the second type of surface with moderate
    averaged water content, for which the SEN value is in the range of [0] (and accordingly,
    the average value of WEH is in the range of 5–15 wt ). For such areas the water
    in the substance is present not only due to adsorption on regolith particles,
    as for the first type, or due to the presence of water ice in the regolith, as
    for the third surface type, but also due to possible presence of hydrated minerals
    in the regolith. It can be assumed that the maximum water content in hydrated
    minerals does not exceed 15 wt . In this case, the mass fraction of chemically
    bound water in the substance depends both on the number of water molecules in
    the chemical formula of the mineral, and on the mass fraction of the mineral itself
    in the regolith of the planet. Therefore, the analysis of only FREND measurement
    data does not allow us to clearly identify the surface with the second hydrological
    type. Results of other studies need to be involved. For example, the presence
    of hydrated minerals on the surface should be noticeable according to IR spectrometry.
    In addition, the mass fraction of water for the surface of the second type does
    not necessarily require the presence of hydrated minerals in the substance. A
    moderate water content in the soil can be provided by combined contribution of
    adsorbed water and a small fraction of free ice in the free volume between the
    regolith particles. Thus, the analysis of the FREND instrument data carried out
    in this study is based on the search and identification of local areas of the
    third type surface with high water content against the background of the surrounding
    surface of the first type with dry regolith. Below such areas are called SEN-areas.
    Of particular interest are areas with the surface of the third type, for which
    the mass fraction of water exceeds 20–30 wt . In this case, the free volume between
    the regolith particles may happen to be insufficient for such a large amount of
    ice. Thus, water ice becomes rock-forming component of surface material, its mass
    fraction is tens of wt and may approach 100 wt . We suggest that regions with
    such a large mass fraction of water ice are called ice permafrost by analogy with
    terrestrial analogues. Fig. 5 SEN-area no. 1 near Medusa Fossae. Colors indicate
    SEN levels and WEH values according to the scale. Dark-red hatching is a reference
    area. In the center, SEN-area no. 1 is hatched (see Table 2). The map is smoothed
    with a Gaussian filter with a half-height width of . The relief is plotted according
    to MOLA (Smith, 2001). Colors, hatching, and relief in the following Figures are
    similar to this one. Full size image SEARCH FOR LOCAL SEN-AREAS WITH POTENTIAL
    PRESENCE OF ICE PERMAFROST ON THE SURFACE OF MARS The search for local SEN-areas
    was carried out based on the analysis of epithermal neutron emission variability
    map represented by the SEN parameter (Fig. 1). Areas were selected with such a
    set of closed isolevels that with SEN decrease, the levels turned out to be embedded
    in the previous isolevel with a larger SEN value. The SEN-area is characterized
    by the maximal isolevel of SEN , which defines its boundary, and by the minimal
    isolevel in the central region of SEN , which determines the maximum epithermal
    neutron flux attenuation. Of greatest interest are ice permafrost ‘‘oases’’ in
    the vicinity of the equator. Therefore, the search was limited to a latitudinal
    belt from to . Thus, local SEN-areas were preliminarily selected, within which
    a significant decrease in SEN towards the center was observed; accordingly, such
    SEN regions should be characterized by high-water content. For each pre-selected
    SEN-area, reference and test pixel groups were selected. The group of reference
    pixels was selected in the immediate vicinity of a SEN-area outside its spatial
    boundary, for which the average SEN parameter corresponded to an extended neighborhood
    of the selected area. Based on averaging of the reference pixels, SEN value was
    estimated. As a rule, the group of reference pixels was selected based on the
    condition of their belonging to the first surface type. Table 2 Selected areas
    with high water content characteristics Full size table The group of test pixels
    of the pre-selected SEN-area was chosen inside its boundary. Test pixels were
    arranged in sequence as SEN increased from the minimum SEN value, using SEN map,
    smoothed by a Gaussian filter with a window. In the aligned pixel sequence, group
    1 was selected that satisfied the significance condition: the average value for
    group 1 has a significance of at least 3 relative to the reference value SEN .
    When evaluating average values of for the test and the reference groups of pixels,
    we used the initial data of the counting measurements with corresponding statistical
    errors. Then, in the aligned sequence of pixels groups 2, 3, etc., were selected
    by a similar criterion until the significance condition was violated. Selected
    groups of pixels for groups 1, 2, 3, etc., matched to SEN-subareas. Thus, the
    initial SEN-area was divided into subareas embedded in one another with monotonically
    increasing water content towards the center. At the same time, the statistical
    certainty of such a partition was ensured by the selection method. Based on the
    numerical simulation of the FREND experiment (see above), the average water content
    was estimated for selected SEN-areas and their subareas. The pre-selected SEN-areas
    were additionally checked according to three criteria: firstly, the length of
    the external boundary of SEN area should not exceed 2000 km—only in this case
    we can speak of a local SEN-area. Its characteristic surface area is on average
    50 000 km . Secondly, average SEN values for test and reference groups of pixels
    should have a statistically significant difference , and, as a rule, they should
    correspond to the third and the first surface types, respectively. One should
    note that when searching for a SEN-area on the map, a very large set of pixels
    is enumerated, as a result a group of pixels with low SEN values that are randomly
    grouped on the map might be selected. To exclude such a false detection, measurement
    data was divided into two equivalent sets corresponding to even and odd serial
    numbers of telemetry files. For each pre-selected SEN-area, a test was conducted
    to compare maps based on the data of even and odd file sets. Detection of a SEN-area
    was considered to be fulfilled if a large number of pixels in independently selected
    SEN-areas coincided on ‘‘even’’ and ‘‘odd’’ maps. Below, this third test for pre-selected
    SEN-areas is referred to as the Test of Odd-Even or TOE. SEN-AREAS IN THE VICINITY
    OF THE EQUATOR WITH POSSIBLE PRESENCE OF ICE PERMAFROST For a detailed analysis,
    the most interesting local SEN-areas in the equatorial latitudinal band of were
    selected, both from the point of view of low values of the SEN parameter and taking
    into account their proximity to interesting geomorphological regions on Mars (Figs.
    1 and 3, Table 2). Medusa Fossae Region SEN-Area no. 1 is near Medusa Fossae (Fig.
    5, Table 2). In this region, only one statistically significant central area (12
    pixels, area—37 000 km can be identified, which has weaker epithermal neutron
    flux relative to the nearest reference region with statistical significance of
    . It should be noted that SEN-area no. 1 passes the TOE criterion with certainties
    of and of the corresponding areas, but the number of matched pixels is 12. The
    estimate of the average water mass fraction in SEN-area no. 1 is 82.0 wt with
    a lower limit of about 30 wt (Table 2). The average water content in the adjacent
    reference region is wt . Such a high WEH content in the SEN-area indicates that
    the substance consists almost entirely of water ice. This interpretation is indirectly
    confirmed by the fact that the data of orbital observations of the OMEGA and CRISM
    experiments do not indicate the presence of hydrated minerals on the surface (Carter
    et al. 2013). Fig. 6 Area SEN no. 2 near Olympus Mons. Full size image The most
    likely process of forming the surface of the Medusa Fossae region is considered
    by the literature to be the result of volcanic activity, which threw out a massive
    layer of soft and porous material onto the surface (Bradley 2002). This layer
    could cover and preserve underneath the relict ice, remaining from the time when
    the Martian pole could be located in this area. This well explains the higher
    water content on the global scale of the Medusa Fossae formation, but additional
    local geomorphological features are likely to influence the appearance of local
    ice detected by the FREND instrument. Southwest Surroundings of Olympus Mons Southwest
    of Olympus Mons is SEN-area no. 2 (Table 2). Statistical data availability for
    this area made it possible to split the area into two concentric subareas: no.
    2a in the center and no. 2b around the center with statistical significance of
    about each (Fig. 6 and Table 2). SEN-area no. 2 has 7 matching pixels for the
    TOE test. The WEH estimate for subarea no. 2a (combines 13 pixels) with an area
    of about 40 000 km is close to 100 wt . In the external subarea no. 2b wt . Its
    area is about 61 000 km (20 pixels). The average water content in the adjacent
    reference region is wt . Obviously, in the central subarea no. 2a, a layer of
    water ice may be a possible form of water. For the external subarea no. 2b, such
    an unambiguous statement cannot be made, the mass fraction of water in hydrated
    minerals can also correspond to the lower estimate, provided that such minerals
    fill the entire near-surface volume of the indicated subarea. However, the data
    of orbital IR spectrometry with CRISM and OMEGA instruments do not confirm the
    presence of such minerals in the vicinity of SEN-area no. 2 (Carter et al. 2013).
    Therefore, in this subarea no. 2b, the most likely form of water in the regolith
    is also water ice mixed with regolith. Much attention is paid in the literature
    to hydrological processes on Mount Olympus and in its northwestern neighborhood
    (Bazilevsky et al. 2006; de Blasio 2011). The discovered SEN-area no. 2 is located
    southwest of the mountain outside this zone. Neither do the data of the surface
    height measurements with the MOLA instrument (Smith 2001) detect in the area no.
    2 a local lowland where water could have accumulated, that flew down from the
    southwestern slope of Olympus during warm periods of the planet’s hydrological
    history (Palumbo 2018). On the other hand, even small impact craters are practically
    absent on the entire surface of SEN-area no. 2—probably this fact indicates the
    icy nature of the surface layer in which the craters formed during an impact are
    immediately filled with melted water. Fig. 7 Area SEN no. 3 on the slope of Ascraeus
    Mons. Full size image Ascraeus Mons In contrast, the detection of SEN-area no.
    3 on the Ascraeus Mons slope, located in the Tharsis Valley, is quite expected.
    The reliability of this detection is confirmed by the TOE test with a good coincidence
    of statistically significant SEN-areas on maps constructed with odd and even files
    data. SEN-area no. 3 may also be divided into two concentric sub-areas with no.
    3a in the center and no. 3b (Fig. 7 and Table 2). The surface of the central subarea
    no. 3a with a relatively small area of 11 000 km (4 pixels) probably consists
    of pure ice. The surrounding area no. 3b with an area of 70 000 km (24 pixels)
    has a much lower soil water content wt . The average water content in the adjacent
    reference region is wt . The origin of SEN-area no. 3 is probably connected to
    the formation of the glacier on the northern slope of the northernmost mountain
    of the Tharsis array which was predicted by Vincendon et al. 2010. At greater
    heights, the water vapor of the atmosphere may condense into ice on the cold slopes
    of the mountain. The preference for the northern slopes in this case is explained
    by a reduced heat flux from solar radiation and the wind rose. Fig. 8 Area SEN
    no. 4 in the Xanthe Terra Valley. Green dots indicate location of hydrated minerals
    according to the CRISM and OMEGA data (Carter 2013). Full size image Fig. 9 Area
    SEN no. 5 and three subareas, near Naktong Vallis. Full size image Fig. 10 Area
    SEN no. 6, Orcus Patera. Full size image Fig. 11 Area SEN no. 7, near the Appolinaris
    Mons volcano. Full size image Xanthe Terra Area SEN-area no. 4, found in Xanthe
    Terra, is located in the channel and mouth of the ancient river (Fig. 8 and Table
    2), which indicates the presence of free water on the surface of this region in
    the past (Rodriguez et al. 2005). SEN-area no. 4 consists of five unrelated components
    (Fig. 8) no. 4a–4e, and it can be assumed that all of them are quite real. Indeed,
    comparing the results of mapping SEN-area no. 4 based on TOE test shows good pixel
    matching with very low SEN values. The number of such pixels for the entire SEN-area
    no. 4 exceeds 80. Table 2 shows the estimates of area and water content for each
    of the components of SEN-area no. 4. For components 4a, 4b, 4d, WEH values significantly
    exceed the value of 15 wt , which suggests the presence of free water ice in the
    regolith of these areas. On the other hand, WEH estimates for components 4c, 4e
    are quite consistent with the presence of hydrated minerals. The average water
    content in the adjacent reference region is wt . According to CRISM and OMEGA
    orbital spectrometric measurements, it is known that hydrated minerals are present
    on the surface of SEN-area no. 4 (Carter et al. 2013). On the other hand, a comparison
    of spatial arrangement of such minerals with neutron mapping data shows that they
    do not coincide with the detected components of SEN-area no. 4 with high water
    content (Fig. 8). This fact, however, does not exclude the identification of some
    components with a not too high water content with areas of hydrated minerals—it
    is sufficient to assume that they are covered by a thin layer of dry regolith.
    Neutron sensing makes it possible to detect hydration of the substance in the
    upper layer of about 1 m thick, while IR spectrometry data characterize the composition
    of only a thin surface layer. Thus, the FREND experiment data for SEN-area no.
    4 as a whole confirm the previously obtained data on the presence of a significant
    mass of hydrated minerals in the Xanthe Terra area. An important novelty based
    on the data obtained is the fact that in several local areas the estimate of water
    content turns out to be too large to identify substances of these areas with hydrated
    minerals. In areas such as 4a, 4b, 4d, the surface material may consist predominantly
    water ice to the depth of about 1 m. Arabia Region A vast region of Arabia is
    of particular interest for understanding the hydrological evolution of Mars. Already
    based on the very first neutron sounding data of the planet, it was discovered
    that the average water content in Arabia regolith is several times higher than
    the values for other equatorial regions of Mars and can reach values of about
    10 wt . The origin of this hydrological feature of the Arabia region has not yet
    been resolved. On global maps of Mars, it is seen that the Arabia region shows
    an increased variability in the water mass fraction in the soil: on the map of
    hydrological surface types (Fig. 2), it can be noted that the density of local
    areas of dark blue against the general background of the blue surface is much
    higher than in any other areas within the equatorial and temperate latitudes belt.
    For example, in the Arabia region’s surface segment within longitude and latitude,
    the surface marked in dark blue with an average value of wt comprises 12 of the
    total area, while for the rest of the surface within the wide latitude belt of
    the total surface area with an average value of wt is only 2.6 . In this regard
    in the Arabia region it is of great interest to search for local SEN-areas, for
    which water content can reach significantly higher values than those that characterize
    this region on average. As the most prominent example of possible ‘‘oases’’ of
    permafrost in the Arabia region, we can consider SEN-area no. 5, located in the
    eastern equatorial part of Arabia near Naktong Vallis and including the triplet
    of spatial components 5a, 5b, and 5c (Fig. 9). The reliability of the spatial
    partition of the SEN-area into the triplet is confirmed by TOE test: the number
    of matching pixels with extremely low SEN values for area no. 5 exceeds 30. The
    areas and estimates of water content for component nos. 5a–5c are presented in
    Table 2. It turned out that the statistical significance of component no. 5c is
    high enough for its spatial resolution into concentric subarea nos. 5c.1 and 5c.2
    (Fig. 9). The largest mass fraction of water was found for component no. 5a, with
    an average value of about 90 wt , while the lower limit of 25 wt at 1 is still
    too large to identify the substance with hydrated minerals. The average WEH estimates
    for other component nos. 5b, 5c.1, and 5c.2 are less than for component no. 5a,
    but for them the lower limits of about 20 wt at the level of 1 are also large
    for hydrated minerals. The average water content in the adjacent reference area
    is wt . Thus, SEN-area no. 5 of the Arabia region probably represents spatially
    separated ‘‘oases’’ of ice permafrost, and the presence of a relatively large
    number of similar local SEN-areas in this region suggests that similar ‘‘oases’’
    exist in other places of this vast region. Orcus Patera Region SEN-area no. 6
    north of the Orcus Patera region has the smallest size among those considered
    in this article, while it has a fairly high statistical significance and well
    satisfies the TOE (Fig. 10and Table 2). Despite its relatively small size, this
    SEN-area can be spatially divided into two components with different estimates
    of water content: no. 6a (5 pixels) with WEH rating of about 100 wt and no. 6b
    (6 pixels) with WEH rating of 34 wt . Obviously, such a high mass fraction cannot
    be explained by anything other than the presence of water ice in the regolith.
    The average water content in the adjacent reference region is wt . The Orcus Patera
    area has an unusual geomorphological nature (van der Kolk et al. 2001): it is
    located near a region with extensive volcanic activity in the past (Vaucher et
    al. 2009) and current geothermal activity. Studies of its relief suggest that
    relatively recently there were lakes present on the surface (Fairen et al. 2005).
    The discovery in the Orcus Patera region of a possible ‘‘oasis’’ of ice permafrost
    makes it especially interesting for further studies, taking into account the possible
    proximity of the water ice massif to the local area of geothermal activity. If
    primitive organisms exist on contemporary Mars, then the reservoir of warm water
    under the surface of the Orcus Patera region may happen to be the most favorable
    place for their habitat. Zephyria Mesnae Region SEN-area no. 7 in the Zephyria
    Mesnae region is located in the southern vicinity of the Appolinaris Mons volcano
    (Fig. 11 and Table 2) and is, like SEN no. 1, a part of the Medusa Fossae formation.
    This area has a high statistical significance of detection and its reliability
    is confirmed by the TOE test (more than 15 pixels coincide). Area no. 7 with a
    size of 93 000 km has an estimate of water content at the level of water ice with
    a lower mass fraction of about 40 wt (at level 1 . The average water content in
    the adjacent reference region is wt . It is noteworthy that the location of SEN-area
    no. 7 near Appolinaris volcano is similar to that of SEN-area no. 2 near Olympus
    Mons and SEN-area no. 3 on the slope of Ascraeus Mons volcano. One can assume
    that the formation and/or presence of ‘‘oases’’ of ice permafrost in the vicinity
    of Martian volcanoes may have a similar reason associated with the preservation
    of water layer under a dry layer of emissions resulting from volcanic activity.
    CONCLUSIONS The obtained estimates of mass fraction of water in the near-surface
    layer of Mars matter allow us to decide on the form in which water is present
    in the regolith. It was shown that at temperate and equatorial latitudes two hydrological
    types of surface prevail. Thus, in the latitudinal zone from to , the proportion
    of the first type surface with regolith up to 5 wt water is about 55 (see Fig.
    2). It is assumed that water in the regolith of this surface type is adsorbed
    on regolith particles. Water enters the regolith from the planet’s atmosphere,
    and its mass fraction is determined by the balance conditions between the processes
    of condensation and sublimation in the upper layer of the surface. It can be assumed
    that the ratio of deuterium to hydrogen in the soil of the first hydrological
    type coincides with the value of this ratio for the contemporary Martian atmosphere.
    The second type surface has regolith with mass fraction of water from 5 to 15
    wt . Its share in the latitudinal belt from to is about 42 . In the regolith of
    this hydrological type, in addition to adsorbed water, there may happen to be
    molecules in the composition of hydrated minerals and it is also impossible to
    exclude a small amount of water ice in the pores between the regolith particles.
    The ratio of deuterium to hydrogen for this form of water corresponds to the ancient
    era in which these minerals were formed. Finally, the surface of the third hydrological
    type with high water content is most likely ice permafrost. The mass fraction
    of water can vary from about 15–30 wt , when ice fills the pores between the regolith
    particles, up to about tens wt and even up to 100 wt , when ice is a rock-forming
    substance, as Boynton wrote in 2007b. At temperate latitudes of Mars, the surface
    of the third type is present only in relatively small local areas, only 2.6 of
    the total surface excluding the vast region of Arabia, where the proportion of
    this surface type rises to 12 . Regions of the third surface type are characterized
    by significant decrease in the emitted epithermal neutron flux and are therefore
    called SEN-areas. Free ice in the ‘‘oases’’ of permafrost could have accumulated
    during one of the past ice ages on Mars, when high inclination of its axis contributed
    to the formation of glaciers near the equator (Schultz and Lutz 1988). This article
    presents data on seven most interesting local SEN-areas with high concentration
    of water in the near-surface regolith in the vicinity of Martian equator, detected
    using FREND neutron telescope data due to its high spatial resolution. The list
    includes SEN-areas, detection of which against the statistical fluctuations background
    in the neutron map pixel counts is of high statistical significance. It turned
    out that almost every area is located in a region with unusual geomorphological
    properties, which probably happened to be favorable for the formation of local
    ice permafrost areas. By analogy with local water-containing areas in deserts
    on Earth, these areas are called ‘‘oases.’’ The list of such areas will be expanded
    on the basis of further analysis of measurement data and as Mars neutron radiation
    map pixel statistics are accumulated. In particular, special attention will be
    paid to Valles Marineris canyon, taking into account published arguments in favor
    of possible presence of glaciers at its bottom (Gourronc et al. 2014). It is the
    equatorial regions of the ice-containing permafrost that are of the greatest interest
    for the study of Mars. Firstly, water ice of permafrost should have dissolved
    compounds and impurities, the composition of which characterizes natural environment
    and climate of the planet in the era of permafrost formation. Secondly, an ice
    ‘‘oasis’’ may include high-molecular weight compounds or biochemical structures
    indicating the presence of primitive life forms on the planet. Thirdly, the proximity
    of near-surface glaciers and geothermal regions can lead to the formation of water
    reservoirs under the surface of modern Mars, which can be a favorable habitat
    for primitive forms of Martian life. Finally, data on ‘‘oases’’ near the equator
    with high water ice content are certainly of practical interest for planning future
    Martian manned expeditions. REFERENCES A. T. Basilevsky, S. C. Werner, and G.
    Neukum, Geophys. Res. Lett. 33, L13201 (2006). Article   ADS   Google Scholar   F.
    V. de Blasio, Earth Planet. Sci. Lett. 312, 126 (2011). Article   ADS   Google
    Scholar   W. V. Boynton, G. J. Taylor, and L. G. Evans, J. Geophys. Res. 112,
    E12S99 (2007a). ADS   Google Scholar   W. V. Boynton, W. C. Feldman, and S. W.
    Squyres, Science (Washington, DC, U. S.) 297, 81 (2007b). Article   Google Scholar   B.
    A. Bradley, S. E. H.Sakimoto, and H. Frey, J. Geophys. Res. E 107, 2-1 (2002).
    Article   Google Scholar   J. Carter, F. Poulet, and J.-P. Bibring, J. Geophys.
    Res.: Planets 118, 831 (2013). Article   ADS   Google Scholar   D. M. Drake, W.
    C. Feldman, and B. M. Jakosky, J. Geophys. Res. 93, 6353 (1988). Article   ADS   Google
    Scholar   A. G. Fairén, J. M. Dohm, and E. R. Uceda, Planet. Space Science 53,
    1355 (2005). Article   ADS   Google Scholar   C. I. Fialips, J. W. Carey, and
    D. T. Vaniman, Icarus 178, 74 (2005). Article   ADS   Google Scholar   F. Forget,
    K. M. Haberle, and F. Montmessin, Science (Washington, DC, U. S.) 311, 368 (2006).
    Article   ADS   Google Scholar   R. Gellert and B. C. Clark, Elements 11, 39 (2015).
    Article   Google Scholar   M. Gourronc, O. Bourgeois, and D. Mège, Geomorphology
    204, 235 (2014). Article   ADS   Google Scholar   J. P. Grotzinger, Science (washington,
    DC, U. S.) 343, 386 (2014). B. M. Jakosky, M. T. Mellon, and E. S. Varnes, Icarus
    175, 58 (2005). Article   ADS   Google Scholar   D. A. van der Kolk, K. L. Trib-Bett,
    and E. B. Grosfils, in Proceedings of the 32nd Annual Lunar and Planetary Scientific
    Conference (2001), p. 1085. M. L. Litvak, I. G. Mitrofanov, A. S. Kozyrev, et
    al., J. Geophys. Res. 112, E03S13 (2007). Google Scholar   S. Maurice, W. Feldman,
    and B. Diez, J. Geophys. Res., E 116, E11008 (2011). C. P. McKay and C. R. Stoker,
    Rev. Geophys. 27, 189 (1989). Article   ADS   Google Scholar   I. G. Mitrofanov,
    D. Anifimov, A. Kozyrev, et al., Science (Washington, DC, U. S.) 297, 78 (2002).
    Article   ADS   Google Scholar   I. G. Mitrofanov, M. L. Litvak, A. S. Kozyrev,
    et al., Solar Syst. Res. 38, 253 (2004). Article   ADS   Google Scholar   I. G.
    Mitrofanov, A. B. Sanin, D. V. Golovin, M. L. Litvak, A. A. Konovalov, A. S. Kozyrev,
    A. V. Malakhov, M. I. Mokrousov, et al., Astrobiology 8, 793 (2008). Article   ADS   Google
    Scholar   I. G. Mitrofanov, A. Malakhov, B. Bakhtin, D. Golovin, A. Kozyrev, M.
    Litvak, M. Mokrousov, A. Sanin, et al., Space Sci. Rev. 214, 86 (2018). Article   ADS   Google
    Scholar   A. M. Palumbo and J. W. Head, Geophys. Res. Lett. 45, 10249 (2018).
    Article   ADS   Google Scholar   J. A. P. Rodriguez, S. Sasaki, and R. O. Kuzmin,
    Icarus 175, 36 (2005). Article   ADS   Google Scholar   P. H. Schultz and A. B.
    Lutz, Icarus 73, 91 (1988). Article   ADS   Google Scholar   D. E. Smith, M. T.
    Zuber, and H. V. Frey, J. Geophys. Res. E 106, 23689 (2001). Article   ADS   Google
    Scholar   J. Vago, O. Witasse, and H. Svedhem, Solar Syst. Res. 49, 518 (2015).
    Article   ADS   Google Scholar   J. Vaucher, D. Baratoux, and N. Mangold, Icarus
    204, 418 (2009). Article   ADS   Google Scholar   M. Vincendon, F. Forget, and
    J. Mustard, J. Geophys. Res. 115, E10001 (2010). Article   ADS   Google Scholar   Download
    references Funding The work described in this article was supported by a grant
    no. 19-72-10144 from the Russian Science Foundation. Author information Authors
    and Affiliations Space Research Institute, Russian Academy of Sciences, Profsoyuznaya
    ul. 84/32, Moscow, 117997, Russia A. V. Malakhov, I. G. Mitrofanov, M. L. Litvak,
    A. B. Sanin, D. V. Golovin, M. V. Djachkova, S. Yu. Nikiforov, A. A. Anikin, D.
    I. Lisov, N. V. Lukyanov & M. I. Mokrousov Corresponding author Correspondence
    to A. V. Malakhov. Additional information Translated by the authors Rights and
    permissions Reprints and permissions About this article Cite this article Malakhov,
    A.V., Mitrofanov, I.G., Litvak, M.L. et al. Ice Permafrost ‘‘Oases’’ Close to
    Martian Equator: Planet Neutron Mapping Based on Data of FREND Instrument Onboard
    TGO Orbiter of Russian-European ExoMars Mission. Astron. Lett. 46, 407–421 (2020).
    https://doi.org/10.1134/S1063773720060079 Download citation Received 07 May 2020
    Revised 07 May 2020 Accepted 26 May 2020 Published 22 October 2020 Issue Date
    June 2020 DOI https://doi.org/10.1134/S1063773720060079 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords:
    planets Mars neutron sounding FREND hydrogen water Use our pre-submission checklist
    Avoid common mistakes on your manuscript. Sections Figures References INTRODUCTION
    METHOD OF NEUTRONS’ COLLIMATION IN THE FREND EXPERIMENT ON WATER IN THE REGOLITH
    OF MARS EPITHERMAL NEUTRON EMISSION MAP OF THE MARTIAN SURFACE POTENTIAL DIVISION
    OF THE MARTIAN SURFACE INTO HYDROLOGICAL TYPES SEARCH FOR LOCAL SEN-AREAS WITH
    POTENTIAL PRESENCE OF ICE PERMAFROST ON THE SURFACE OF MARS SEN-AREAS IN THE VICINITY
    OF THE EQUATOR WITH POSSIBLE PRESENCE OF ICE PERMAFROST CONCLUSIONS REFERENCES
    Funding Author information Additional information Rights and permissions About
    this article Advertisement Discover content Journals A-Z Books A-Z Publish with
    us Publish your research Open access publishing Products and services Our products
    Librarians Societies Partners and advertisers Our imprints Springer Nature Portfolio
    BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state
    privacy rights Accessibility statement Terms and conditions Privacy policy Help
    and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University
    of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Astronomy Letters
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Ice Permafrost ‘‘Oases’’ Close to Martian Equator: Planet Neutron Mapping
    Based on Data of FREND Instrument Onboard TGO Orbiter of Russian-European ExoMars
    Mission'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Varadharajan C.
  - Faybishenko B.
  - Henderson A.
  - Henderson M.
  - Hendrix V.C.
  - Hubbard S.S.
  - Kakalia Z.
  - Newman A.
  - Potter B.
  - Steltzer H.
  - Versteeg R.
  - Agarwal D.A.
  - Williams K.H.
  - Wilmer C.
  - Wu Y.
  - Brown W.
  - Burrus M.
  - Carroll R.W.H.
  - Christianson D.S.
  - Dafflon B.
  - Dwivedi D.
  - Enquist B.J.
  citation_count: '16'
  description: The U.S. Department of Energy's Watershed Function Scientific Focus
    Area (SFA), centered in the East River, Colorado, generates diverse datasets including
    hydrological, geological, geochemical, geophysical, ecological, microbiological
    and remote sensing data. The project has deployed extensive field infrastructure
    involving hundreds of sensors that measure highly diverse phenomena (e.g. stream
    and groundwater hydrology, water quality, soil moisture, weather) across the watershed.
    Data from the sensor network are telemetered and automatically ingested into a
    queryable database. The data are subsequently quality checked, integrated with
    the United States Geological Survey's stream monitoring network using a custom
    data integration broker, and published to a portal with interactive visualizations.
    The resulting data products are used in a variety of scientific modeling and analytical
    efforts. This paper describes the SFA's end-to-end infrastructure and services
    that support the generation of integrated datasets from a watershed sensor network.
    The development and maintenance of this infrastructure, presents a suite of challenges
    from practical field logistics to complex data processing, which are addressed
    through various solutions. In particular, the SFA adopts a holistic view for data
    collection, assessment and integration, which dramatically improves the products
    generated, and enables a co-design approach wherein data collection is informed
    by model results and vice-versa.
  doi: 10.1109/ACCESS.2019.2957793
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 7
    Challenges in Building an End-to-End System for Acquisition, Management, and Integration
    of Diverse Data From Sensor Networks in Watersheds: Lessons From a Mountainous
    Community Observatory in East River, Colorado Publisher: IEEE Cite This PDF Charuleka
    Varadharajan; Deborah A. Agarwal; Wendy Brown; Madison Burrus; Rosemary W. H.
    Carroll; Danielle S. Christianson; Baptiste Dafflon All Authors 18 Cites in Papers
    1331 Full Text Views Open Access Comment(s) Under a Creative Commons License Abstract
    Document Sections I. Introduction II. Field Infrastructure and Data Collection
    III. SFA Data Management Framework IV. Overcoming Challenges in Obtaining and
    Using Data From Watershed Sensor Networks V. Utilizing a Co-Design Approach Enables
    Better Measurements and Modeling Show Full Outline Authors Figures References
    Citations Keywords Metrics Footnotes Abstract: The U.S. Department of Energy''s
    Watershed Function Scientific Focus Area (SFA), centered in the East River, Colorado,
    generates diverse datasets including hydrological, geological, geochemical, geophysical,
    ecological, microbiological and remote sensing data. The project has deployed
    extensive field infrastructure involving hundreds of sensors that measure highly
    diverse phenomena (e.g. stream and groundwater hydrology, water quality, soil
    moisture, weather) across the watershed. Data from the sensor network are telemetered
    and automatically ingested into a queryable database. The data are subsequently
    quality checked, integrated with the United States Geological Survey''s stream
    monitoring network using a custom data integration broker, and published to a
    portal with interactive visualizations. The resulting data products are used in
    a variety of scientific modeling and analytical efforts. This paper describes
    the SFA''s end-to-end infrastructure and services that support the generation
    of integrated datasets from a watershed sensor network. The development and maintenance
    of this infrastructure, presents a suite of challenges from practical field logistics
    to complex data processing, which are addressed through various solutions. In
    particular, the SFA adopts a holistic view for data collection, assessment and
    integration, which dramatically improves the products generated, and enables a
    co-design approach wherein data collection is informed by model results and vice-versa.
    Topic: Advanced Sensor Technologies on Water Monitoring and Modeling Screenshot
    from the Field Information Portal that contains detailed site pages describing
    the location and infrastructure available at each long-term sampling location.
    Published in: IEEE Access ( Volume: 7) Page(s): 182796 - 182813 Date of Publication:
    05 December 2019 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2019.2957793 Publisher:
    IEEE Funding Agency: CCBY - IEEE is not the copyright holder of this material.
    Please follow the instructions via https://creativecommons.org/licenses/by/4.0/
    to obtain full-text articles and stipulations in the API documentation. SECTION
    I. Introduction Watersheds are fundamental units of the Earth, and are complex
    systems whose behavior and evolution are governed by coupled interactions between
    hydrological, biogeochemical, and ecological processes, as well as human factors
    such as land use [1]–[3]. It is becoming increasingly important to understand
    and predict how watersheds will respond to changing environmental conditions and
    anthropogenic perturbations to optimize management of water-energy resources.
    This requires access to a diverse set of observations at multiple spatial and
    temporal scales that mutually provide insight into the functioning of watershed
    subsystems, and guide the development and application of predictive modeling tools.
    In particular, the use of sensors and environmental observatories for measuring
    watershed processes has exploded over the past few decades. Sensors can dramatically
    enhance data collection, for example by increasing the temporal resolution to
    sub-hourly or greater timescales, enabling data to be collected from remote locations
    or harsh periods where the field site can be inaccessible, alerting users of unusual
    or otherwise diagnostic conditions warranting further examination in a rapid response
    manner, and reducing the amount of manual effort and time needed to collect measurements.
    Thousands of sensors are now routinely being used in large networks including
    the United States Geological Survey (USGS) stream network [4], the National Ecological
    Observatory Network (NEON; https://www.neonscience.org/), and the United States
    Department of Energy’s (DOE) Ameriflux network (https://ameriflux.lbl.gov/) that
    observe critical watershed parameters such as river discharge, surface and groundwater
    levels, water quality (temperature, pH, dissolved oxygen, conductivity), and fluxes
    of carbon, water and energy [5]. Emerging optical sensors for solutes such as
    nitrate, phosphate and dissolved organic matter (DOM) present an opportunity to
    measure and determine aquatic health and biogeochemical dynamics at unprecedented
    temporal and spatial resolution [6]–[8]. In turn, high-resolution data from sensors
    have led to novel discoveries in watershed hydrobiogeochemical behavior. For example,
    electrical conductivity and pH data at the Hore stream in Plynlimon, Wales, collected
    at hourly resolution revealed diurnal patterns that were not visible in the data
    collected at daily, weekly, and monthly intervals, thereby demonstrating the value
    of collecting data using automated sensors [9]. Similarly high-resolution nitrate
    and DOM concentrations in the Sleepers River in Vermont, US, were found to be
    influenced by peak snowmelt at seasonal time-scales, and in-stream biogeochemical
    processes at daily time-scales [10]. High-frequency data from monitoring networks
    have provided driving and validation data for a broad suite of watershed models
    such as the National Water Model [11], Amanzi-All Terrestrial Simulator [12],
    PFLOTRAN [13], [14], SWAT [15] and are now leading to a new research paradigm
    where data mining and machine learning algorithms can be used to gain insights
    into Earth system science [16]. A number of cyberinfrastructure systems to acquire,
    process, and publish sensor network data have been developed for water monitoring
    sensor networks. The USGS maintains the National Water Information System (NWIS;
    https://nwis.waterdata.usgs.gov/), and the National Groundwater Monitoring Network
    (NGWMN; https://cida.usgs.gov/ngwmn/) to publish data collected from its monitoring
    networks [17]. The USGS data can be explored, subsampled and retrieved through
    user portals and the NWIS web services. The Consortium of Universities for the
    Advancement of Hydrologic Science (CUAHSI) has developed the Hydrologic Information
    System (HIS; https://hiscentral.cuahsi.org). This system maintains a metadata
    catalog of approximately 100 providers, from which data can be retrieved using
    web services.. The HIS enables users to access the data via the Hydroclient interactive
    portal and the WaterOneFlow web services. Additionally, CUAHSI maintains the Hydroshare
    system to share and publish data, and obtain digital object identifiers (DOIs)
    for citations and has developed the Observation Data Model (ODM2), an information
    model to store and manage diverse Earth observations [18]–[20]. The United States
    Environmental Protection Agency (EPA) maintains the Water Quality Exchange (WQX),
    which is a framework to enable data sharing across local, state, and federal agencies
    (https://www.epa.gov/waterdata/water-quality-data-wqx), and is developing the
    Interoperable Watershed Networks to handle continuous water quality data [21].
    Collectively these systems contribute to the Open Water Data Initiative (https://acwi.gov/spatial/owdi/),
    and more recently the Internet of Water initiative [22], which seek to overcome
    challenges related to water data publication and discovery across the United States
    (US). The Ameriflux network has developed methods to standardize, QA/QC, gap fill,
    and process high-frequency eddy covariance and micrometeorological datasets [23].
    Despite the immense progress made over the last two decades, significant gaps
    and challenges still remain for scientific researchers seeking to acquire, manage,
    and use data from watershed monitoring networks (e.g. [24]). While the need for,
    and advantages of using sensors and related cyberinfrastructure to advance watershed
    science are well documented, literature on challenges associated with sensor data
    collection, curation, processing, and use in models are relatively sparse (e.g.,
    [25], [26]). There is an inherent assumption that the use of sensors for environmental
    observations will make data acquisition easier, and that data will become readily
    available for analysis once a sensor is installed in the field. In reality, the
    automation of data collection requires periodic maintenance and calibrations of
    the sensors, constant access to power, and the ability to download data at regular
    intervals, which for real-time streaming data requires internet connectivity in
    the field. The data need to be formatted, documented, and organized in a manner
    that facilitates easy retrieval of large datasets. Data quality issues of various
    types including flatlining, sensor drift, and sensor malfunction need to be handled
    prior to data use. Often, data from various types of measurements need to be integrated
    across providers, which requires implementation of metadata and data standards,
    exchange protocols, and controlled vocabularies. Finally, the data need to be
    easily accessible through technologies such as APIs and web portals to enable
    their use in subsequent analyses, visualizations, and modeling. This paper highlights
    the potential problems and solutions that can be involved in building the integrated
    field and data infrastructure necessary to make sensor data useful for scientific
    analysis and readily assimilable into predictive numerical models. First, we present
    the field- and cyber-infrastructure that we have developed to obtain an array
    of datasets across an emblematic watershed for the DOE’s Watershed Function Scientific
    Focus Area (SFA) project. This infrastructure forms an end-to-end system that
    deals with many aspects of sensor-based measurements, including data acquisition,
    curation, quality analysis and quality control (QA/QC), integration with other
    data streams, visualization, and download. We then illuminate challenges faced
    in obtaining these data from both the field and data management perspectives,
    describe our solutions to some of these issues, and identify areas for improvement.
    Finally, we describe how such an end-to-end data pipeline can enable co-design
    strategies for situations in which data are used for model development, and models
    can help optimize data collection from sensor networks. Although we provide examples
    from the Watershed Function SFA, many of the challenges and solutions discussed
    are not unique to watershed science, and are broadly applicable to studies of
    other complex systems (e.g. other natural or urban environments, transportation
    infrastructure), where sensors are used to monitor the environment, generating
    heterogenous data that are then used in predictive models. SECTION II. Field Infrastructure
    and Data Collection The Watershed Function SFA seeks to understand and predict
    the impacts of hydrologic perturbations such as drought and early snowmelt on
    the retention and release of water, carbon, nutrients, and metals in mountainous
    watersheds [27]. The SFA’s primary field site is the East River (ER), a snow-dominated,
    headwater basin located in the Upper Colorado River Basin (UCRB) in the western
    US. The ER is considered to be representative of the snow-dominated headwater
    watersheds in the UCRB and is part of several testbeds in the US supported by
    the DOE’s Biological and Environmental Research Subsurface Biogeochemistry Program
    (https://doesbr.org). The ER watershed spans 300 square km., and comprises the
    pristine East River and Washington Gulch drainages, as well as the mining-impacted
    Slate River and Coal Creek drainages. The watershed contains various life zones,
    including alpine, subalpine, montane, and floodplain ecosystems. The SFA has deployed
    extensive sensor instrumentation in the ER (see interactive map at https://wfsfa-data.lbl.gov/watershed/).
    The measurement network includes 17 stream-gauging stations paired with stream-water
    analyte measurements, 6 weather stations with soil moisture and temperature probes,
    10 instrumented groundwater wells, ~40 digital repeat photography cameras (phenocams),
    and about 30 multilevel soil instruments. The ER watershed also has significant
    infrastructure maintained by several federal, state, and local agencies that have
    different data systems. The USGS maintains gaging stations and collects water
    quality measurements in the East River, Coal Creek, Slate River and at Almont
    (confluence of the East and Taylor Rivers), and makes the data available through
    NWIS. Meteorological and snow data are available from the National Resources Conservation
    Service (NRCS) ‘Butte’ and ‘Schofield’ Snow Telemetry (SNOTEL) sites, Crested
    Butte Cooperative Observer Network (COOP) stations, and several Weather Underground
    stations. Additional water quality data are available from the EPA’s STORET database
    and the National Water Quality Portal, which includes measurements by the Colorado
    Department of Public Health and Environment (CDPHE), and other local groups such
    as the Coal Creek Watershed Coalition and the Rivers of Colorado Water Watch.
    The EPA also has a Colorado Clean Air Status and Trends Network (CASTNET) station
    at Gothic, which collects data under the National Atmospheric Deposition Program
    (NADP). The Rocky Mountain Biological Laboratory (RMBL; https://www.rmbl.org)
    located near Crested Butte is a field station that has collected biological and
    ecohydrological measurements for 90 years. In addition, an eddy flux tower is
    maintained by the National Center for Atmospheric Research (NCAR). For analysis
    and modeling efforts, the data from the SFA’s sensor network need to be integrated
    with other diverse data collected by the SFA and its collaborators at the East
    River, as well as the data from the other agencies. Examples of additional data
    available include a variety of remote sensing measurements such as Light Detection
    and Ranging (LiDAR) [28], Airborne Snow Observatory [29], Airborne Electromagnetic,
    and hyperspectral surveys ground-based observations such as electrical resistance
    tomography, seismic data, time-lapse camera images of snow distribution and phenology,
    microbial community genome (metagenomic) investigations, soil and sediment biogeochemistry
    and mineralogy, river and porewater chemistry, and extensive measurements of snow
    density, snow water equivalent, snow chemistry and stable isotopes [30], [31].
    SECTION III. SFA Data Management Framework Environmental observatories such as
    the SFA’s ER site typically require multiple components of an information system
    to curate, process, store, and publish data [26]. The SFA’s Data Management Framework
    provides infrastructure and services to support various aspects of the data lifecycle
    [32]. These include the ability to (1) manage, archive, and publicly release datasets
    as per the project’s data policy, (2) enable the SFA team and the broader community
    to discover and access relevant datasets, (3) perform Quality Assurance and Quality
    Control (QA/QC) of priority datasets; and (4) enable efficient data collection,
    data integration, and product generation. We have developed specific tools for
    data management and preservation, QA/QC, data discovery and visualization (Figure
    1), as described below. FIGURE 1. Overview of the Watershed SFA end-end Data Management
    Framework. The framework contains (a) a data collection and acquisition system
    involving a distributed sensor network across the watershed for diverse observations,
    (b) a queryable database with a workflow to telemeter and store data and associated
    data products with relevant metadata, (c) scripts for semi-automated QA/QC with
    cleaned data stored in the ERDB, (d) a data integration broker (BASIN-3D) to synthesize
    project data with external datasets for real-time queries, (e) an advanced data
    search and access portal for data discovery, exploratory analysis and download,
    and (f) periodic publication of data with DOIs in the DOE’s ESS-DIVE repository.
    Show All A. East River Database We have maintained a well-curated repository for
    the Watershed Function SFA’s data and metadata [27] called the East River database
    (ERDB). It is a SQL database, which holds the metadata (such as locations) of
    monitoring stations and observational time-series, as well as data from hundreds
    of physical sensors that comprise over 100 million time point-value pairs (Table
    1), and additional data such as water quality measurements from laboratory analysis
    of geochemical water samples. East River sensor measurements are telemetered and
    automatically ingested into the ERDB. An automated data flow maps instrument output
    to an SFA-developed controlled vocabulary for standardization of variable names
    across instruments, and cross-comparison of data. The lab measurements of field
    samples are also standardized and integrated into the database regularly via Python
    scripts. Data from NRCS SNOTEL sites and Weather Underground are harvested from
    the sources and federated into the ERDB. Data can be retrieved through queries
    to the ERDB, and are optimized through the use of caching and materialized views.
    All data stored in the ERDB are accessible to the project team through an Application
    Programming Interface (API), and web portal interfaces (Section III.D). TABLE
    1 The SFA Maintains Hundreds of Sensors of Various Types in the ER Watershed,
    Which Measure Diverse Datasets. Some Sensors are Placed at Multiple Sites or Depths,
    and Measure Multiple Variables; Hence There Typically are Many Data Streams Generated
    by a Single Instrument. The Streaming Data are Automatically Telemetered and Ingested
    into a Queryable Database B. Quality Assurance and Quality Control (QA/QC) of
    Meteorological, Hydrological, and Geochemical Data Quality Assurance(QA)/Quality
    Control(QC) is necessary to ensure the reliability of data and enable scientific
    accuracy. We perform QA/QC for the SFA’s data using R (https://www.r-project.org/)
    and Python (https://www.python.org/) scripts, and document our workflows using
    R Markdown (https://rmarkdown.rstudio.com/) and Jupyter Notebooks (https://jupyter.org/).
    The QA/QC process for time-series data involves both the identification of problems
    in the data (QA), and subsequent data cleaning or processing (QC). An important
    component of the QA/QC procedure is gap filling of missing values. Our QA/QC workflow
    includes three major steps: Step 1(QA)—Cataloging data, determining the temporal
    frequency of sampling to assess data availability, and assessing data quality;
    Step 2(QC)—Processing and cleaning raw datasets, including custom procedures for
    formatting timestamps, detecting and removing duplicates, bad data and outliers,
    gap filling, and flagging QC-ed data; and Step 3—Preparing time grids of the desired
    frequency, and exporting data into the desired format for modeling or further
    analysis. The first step involves an automated data inventory workflow to catalog
    available meteorological, hydrological, and geochemical datasets combined with
    semi-automated methods to assess their quality (Step 1). A Python script implemented
    as a Jupyter notebook queries across all monitored stations and variables, and
    retrieves data using the ERDB API. The script creates high-level lookup tables
    of available data displaying the data types, locations, and temporal frequency
    of measurements, as well as the number of data points with other descriptive statistics
    such as the mean, min, and max values for each location. Cross-comparison of the
    datasets using their range, mean, and standard deviation values, and temporal
    sampling frequency enables quick identification of potential problems. Finally,
    the scripts plot the data inventory and time-series values for visual inspection.
    Thus, we collectively use the subroutines to perform real-time quality checks
    of data and determine the datasets that require subsequent application of the
    QA/QC procedures. The next step involves processing and cleaning the raw time-series
    datasets through semi-automated scripts (Step 2). The workflow for cleaning time-series
    includes: (1) conversion of timestamps into a uniform ISO 8601 date/time format
    specified as YYYY-MM-DD HH:MM:SS [33]; (2) identification and removal of duplicates
    using the function “duplicated” in the “base” package of R; (3) removal of data
    points that are clearly outside the sensor measurement ranges; and (4) automatic
    detection and removal other bad data in time-series using the R package “tsoutliers”
    [34], which is a widely used method to detect outliers, level shifts and temporary
    changes [35]. Various criteria and metrics are applied in cleaning the datasets.
    For example, the cleaned data have to be consistent with the sampling resolutions
    and descriptive statistics from Step 1. Meteorological datasets needed as driving
    inputs to climate models require additional processing. These datasets are gap
    filled using the Singular Spectrum Analysis method in the R package “Rssa” [36].
    The final step involves aggregating the cleaned, gap-filled time-series into the
    desired temporal frequency, which is typically hourly or half-hourly time intervals
    for modeling needs, and exporting the data into files with csv or netCDF formats,
    for data analysis or input into simulation codes. This QA/QC workflow was applied
    to diverse data from the UCRB, including data from 17 meteorological stations
    located in the East River watershed, and additional meteorological data from other
    organizations, including North American Regional Reanalysis (NARR, produced by
    the National Centers for Environmental Prediction, NCEP), Western Regional Climate
    Center (WRCC) data, PRISM high-resolution spatial climate dataset (http://www.prism.oregonstate.edu/),
    NRCS SNOTEL data, and data from the USGS (Figure 2a). We also performed QA/QC
    of groundwater level data measured in monitoring wells (e.g. Figure 2b), river
    discharge measurements, and geochemical data. FIGURE 2. Examples of QA/QC for
    different types of sensor data collected at the East River, CO: (a) Time-series
    data from the Billy Barr meteorological station, with red ovals indicating the
    outliers and suspicious data; (b) Time-series plot groundwater level data obtained
    from a downhole pressure transducer in the monitoring well PLM-6; the black line
    shows the raw water level (absolute elevation in m), and the red line shows the
    QA/QC-ed, de-spiked, clean time-series of water level. Many of the spikes are
    caused due to the sensors being pulled out of the borehole for calibration or
    water sampling. Show All The QA/QC-ed data products are used for a variety of
    purposes in the SFA including data analysis [27] and numerical modeling of hydrological
    and geochemical processes, including infiltration, evapotranspiration, and evolution
    of CO2 concentrations, at the ER, using numerical codes such as the Community
    Land Model (CLM), Ecosys, PFLOTRAN, and ParFlow [37]–[41]. C. Data Integration
    The SFA needs to obtain and integrate data from various providers for scientific
    analysis and modeling. Hence, we developed a data broker, BASIN-3D (Broker for
    Assimilation, Synthesis and Integration of eNvironmental Diverse, Distributed
    Datasets) that provides a generic framework to integrate diverse, multiscale data
    across a variety of additional data sources and environmental data types [27].
    BASIN-3D is an extensible brokering software that enables creation of synthesized
    datasets for use in scientific analysis, modeling, and visualization. It is designed
    to enable researchers to search and visualize the latest data available from many
    providers, as easily as if it were an integrated dataset in local storage. BASIN-3D
    retrieves data from the providers in real-time, as-needed, which ensures that
    the latest version of the data is always served to the user. This is particularly
    useful for sensor data, where the data stream is constantly changing with additional
    measurements and QA/QC processing. BASIN-3D connects to data sources in real-time
    via web services, provides lists of the available data types and locations, retrieves
    subsets of data requested by the user, and transforms the data streams into uniform,
    abstracted formats to provide common nomenclature and formats required for an
    integrated dataset. The abstracted formats are based on the Open Geospatial Consortium
    (OGC) and ISO “Observations and Measurements” (OGC 10-004r3/ISO 19156: 2013) and
    OGC “Timeseries Profile of Observations and Measurement “(OGC 15-043r3) data standards
    [42], [43]. The current implementation of BASIN-3D integrates time-series data;
    however the OGC/ISO framework also allows for future additions of diverse data
    types such as sample-based or remotely-sensed data. Notably, BASIN-3D is able
    to handle multiscale data, through the use of the OGC/ISO framework that allows
    for specification of hierarchies of spatial features (e.g., points, plots, sites,
    watersheds, basin). Thus, users can retrieve data for a specific site or river
    basin. The BASIN-3D software is open source and available at https://github.com/Watershed-Function-SFA/BASIN-3D.
    D. Interactive Portals for Data Access and Visualization The SFA data management
    framework also includes two Javascript-based interactive web portals to disseminate
    information on field activities to the public, and provide data discovery and
    access tools to the team. The first is a Field Information Portal, which is a
    publicly-accessible, interactive map providing information on the SFA’s field
    infrastructure and research activities (https://wfsfa-data.lbl.gov/watershed/).
    Detailed location pages provide information about each sampling location such
    as the measurements and instrumentation available there (Figure 3). Users are
    able to pick measurement themes (e.g. hydrogeology, biogeochemistry) or research
    themes (e.g. soil carbon) to filter sites of interest. FIGURE 3. Screenshot from
    the Field Information Portal that contains detailed site pages describing the
    location and infrastructure available at each long-term sampling location. Show
    All The second portal that builds on the BASIN-3D data integration broker (Section
    III.C), provides users an integrated, intuitive search and download of the data
    (Figure 4). The application of BASIN-3D for this portal requires an additional
    software component (the WFSFA data broker) to connect to the specific data sources
    of interest (i.e., the SFA’s East River and Rifle field databases, USGS NWIS),
    and map the data source vocabularies to the BASIN-3D synthesis models. This enables
    users to search across the SFA database and NWIS to retrieve variables of interest
    (e.g. discharge) in a uniform format for visualization and download. FIGURE 4.
    Screenshot from the SFA’s data access portal showing the integrated visualization
    of multiple datasets provided by BASIN-3D using interactive visualizations. Here,
    discharge data from the SFA at its Pumphouse site (PH ISCO) and USGS at the downstream
    Almont gaging station (USGS ID 09112500) are shown over the same time period.
    A video providing a virtual tour of the SFA data portal is provided in the supplementary
    material. Show All The portal utilizes several third party Javascript libraries
    to enable various capabilities that make the user interface more intuitive. The
    features and associated libraries include search and selection of multiple inputs
    from dropdown lists (enabled by Bootstrap Multiselect; https://github.com/davidstutz/bootstrap-multiselect),
    slicing and dicing data (Crossfilter; https://square.github.io/crossfilter/),
    interactive maps (Google Maps; https://cloud.google.com/maps-platform/ and Leaflet;
    https://leafletjs.com/), interactive tables (DataTables; https://github.com/DataTables/DataTables),
    and interactive visualizations for exploratory data analysis (Dygraphs charting
    library; http://dygraphs.com and other libraries utilizing D3.js; https://d3js.org/).
    Thus the portal allows users to seamlessly browse, integrate, sub-sample, download,
    and visualize integrated datasets across different databases without writing any
    code (see video in Supporting Information). The portal is deployed on the Spin
    platform [44], a Docker-based system for hosting scalable web services provided
    by the DOE’s National Energy Research Scientific Computing Center (NERSC). The
    portal is deployed as a “microservices” architecture using Docker containers (https://www.docker.com/).
    Multiple instances are easy to ”spin up” using isolated containers, which enables
    easier maintenance and deployment, and allows for scaling of resources to meet
    increasing user demand. E. Data Publication The SFA sensor and other project data
    are made publicly available as per the project’s data management policy (available
    at http://watershed.lbl.gov). Data are first curated within the ERDB or an internal
    project data management portal. The data are then published periodically with
    relevant metadata through the DOE’s Environmental Systems Science Data Infrastructure
    for a Virtual Ecosystem (ESS-DIVE) data repository [45]. Sensor data are published
    in snapshots with a few years of data comprising a single data package (e.g. discharge
    data for water year 2015–2018 [46]). SECTION IV. Overcoming Challenges in Obtaining
    and Using Data From Watershed Sensor Networks Organizations seeking to deploy
    sensor networks in watersheds will face an array of challenges that span practical
    field logistics to building cyberinfrastructure, data management support services,
    and processing pipelines for complex data. The issues are compounded by the fact
    that the technologies for obtaining and managing data are continuously evolving.
    Hence, the data acquired are becoming more complex over time, as new sensors and
    data acquisition hardware become available. Similarly, software and data processing
    capabilities, as well as community data/metadata standards, are rapidly changing.
    Projects need to anticipate some of these problems at the outset and dedicate
    staff time and resources to resolving them. In this section, we discuss various
    challenges encountered, and potential current or future solutions for building
    an end-to-end sensor data system, illustrated with practical examples from our
    sensor network in the ER. A. Data Acquistion Many factors need to be considered
    for the deployment, maintenance, and curation of sensor measurements [47]. First,
    the network design needs to capture the spatial and temporal heterogeneity of
    the system given the research questions posed, stakeholder needs, and available
    resources. Secondly, data acquisition and maintentance protocols must be designed
    to obtain as many high-quality, continuous observations as possible, given the
    environmental conditions and logsitics at the field site. Contingency plans must
    be made to handle the inevitable, unexpected events that can lead to missing data
    or problems with data quality. Finally, all field installations, maintenance/calibrations,
    and other events must be well documented and made available for future QA/QC and
    data interpretation. The challenges inherent to establishing and maintaining viable
    sensor networks in the field are compounded in snow-dominated, mountainous watersheds
    due to complex topography, animal/rodent activities, access, and safety considerations.
    In addition, harsh environmental conditions in the winter test power supplies,
    sensor applicability and survival, as well as limited telemetry options. Furthermore,
    site visits may not be possible during critical times of change in the environmental
    system. Data quality issues arise at all points in the data collection process.
    As an example, the ER stream discharge network [31], [46] embodies many of these
    challenges. Discharge measurements require two types of data. First, one needs
    to measure river stage. In the ER, this is accomplished using a pressure transducer
    installed in the stream and corrected for barometric pressure. Second, instantaneous
    discharge observations are measured. For this, we rely on wading the stream with
    an Acoustic Doppler Velocimeter (ADV). From these data, one develops a depth-to-discharge
    relationship (rating curve) to establish near-continuous discharge values. However,
    discharge data quality is hampered in steep channels by turbulent conditions,
    which can result in high-frequency temporal variations that need to be smoothed
    in post-processing (Figure 5). Rating curves depend on high-flow observations
    when stream wading is often unsafe, and are invalidated by changes in channel
    cross-sectional geometry that can potentially occur during peak discharge. High-flows
    and woody debris in the streams can lead to loss of instrumentation and missing
    data. Winter iced-conditions can produce anomalously high pressure readings that
    need correction [48]. The spring freshette is a critical period of solute export
    that requires sensors be installed in the fall and survive the winter. However,
    sensors will freeze if temperatures drop below 0°C and can be swept away during
    peak runoff. With lack of telemetry, invalid or lost data are often not discovered
    until the summer when access to the transducers is safe. Subsequently, a critical
    annual event is missed. FIGURE 5. Discharge measurements collected at two sites
    in the East River to illustrate several challenges in data acquisition in mountainous
    watersheds. The upstream East Below Copper (EBC) location sees considerable turbulence
    during spring snowmelt in comparison to the downstream Pumphouse (PH) location.
    It is notable that the EBC transducer was washed away during peak discharge in
    2016, 2017 and 2018. (a) A comparison of EBC and PH 10-minute hydrograph for runoff
    in Spring 2019 (b) the detail shows the 10-min data for June 27 to July 4, 2019.
    Wave-induced turbulence at EBC increases variance in the hydrograph to falsely
    produce more estimated streamflow. In contrast, the hydrograph for PH is smooth,
    indicating that no turbulence is observed at that location. The high-frequency
    noise from turbulence at EBC is removed using a simple 3-hour moving average (c)
    installation of EBC in October 23, 2018 at low discharge appears ideal with a
    large eddy and bedrock controls constraining the cross section, (d) EBC in June
    21, 2019 experiences standing waves and turbulent conditions imposing on the transducer
    (inside the stilling well) and un-wadable conditions. Show All Sensor networks
    deployed on land, such as along the ER hillslopes, face a unique set of challenges
    as well. A specific example is sensor and cable damage due to rodents or the grazing
    activities of cows. Temporal fencing during the active season, and other protective
    measures can mitigate damages to a certain degree, but complete damage avoidance
    is rare. Another problem that can occur are potential power outages during the
    winter season, when site access for battery replacement might be challenging or
    when the solar panel becomes buried under the snow. Even if the system’s power
    consumption is configured to last through the winter, data transmission under
    harsh winter conditions can consume an abnormally high amount of power, resulting
    in faster drainage of the onboard batteries. Finally, unexpected events in the
    field can lead to missing or erroneous data. As a classic example, the solar panels
    used for CO2 flux instrumentation at the East River were completely buried under
    a deep snowpack in the spring of 2019. This caused the top panel to later be damaged
    by snow grooming for a ski race near our research site. To address some of these
    challenges, we have designed our network to be robust by installing redundant
    sensors at all our stream sites. The network sustainability is reviewed annually
    to assess human resources, safety concerns, which data are critical to research
    goals, and if sensors should be removed, added, re-located or replaced to meet
    data quality standards. In addition, we are continually exploring new technologies
    and their applicability across a range of stream characteristics and ability to
    survive extreme environmental conditions. Specifically, we now use stand-alone
    bluetooth sensors to measure water levels (HOBO MX2001), which are proving capable
    of surviving winter conditions, and additionally are cost-effective for safe and
    rapid download of data without the need for a large power source, cell service,
    or satellite communication. We retrieve data weekly, which limits the potential
    for lost data even if the sensor is swept away during subsequent flows. We are
    also incorporating non-wading tracer techniques, and are investigating non-contact
    options such as hand-held microwave radar, scanning LiDAR, airborne GPR, and Interferometric
    stream radar (ISRad) through institutional support as well as collaborations with
    federal and university partners. Finally, we document details about our sensor
    installs (Section IV.B), and address some of the quality issues through post-processing
    and QA/QC (Section IV.C). In the future, advances in low-cost/low-power sensors,
    wireless sensor networks, and related hardware (e.g. Arduino or Raspberry Pi boards)
    can make it possible to overcome many of these challenges, and particularly enable
    installation of several redundant sensors. Currently, low-cost sensors are not
    available for many of the measurements needed within the ER (e.g. water chemistry,
    soil moisture). Similarly, off-the-shelf options for ultra-low-power or energy
    harvesting devices are limited, particularly for remote monitoring in rugged,
    outdoor environments. Although, wireless sensor technologies are rapidly evolving
    with more developed instrumentation becoming commercially available, these still
    pose challenges (e.g. limited range, durability, cost) for practical field deployment
    at scale. Significant improvements in low-cost/ultra-low-power wireless sensor
    hardware and communication protocols (e.g. bluetooth, wireless mesh networks,
    LoRa) are needed to revolutionize monitoring of natural water systems. B. Data
    Curation and Storage Considerations for storage and management of sensor data
    include the volume of data generated, heterogeneity of instrumentation, diversity
    of data types and formats, the need to federate external data or be interoperable
    with other databases, and requirements for data usage. Data curation can be especially
    difficult when researchers from multiple institutions and scientific backgrounds
    collect data through a variety of sensors and data acquisition infrastructure,
    which typically occurs in many research projects including at our ER field site.
    The heterogeneity in personnel, organizations, sampling preferences, equipment,
    and data types poses a number of challenges for data curation and archival. First,
    it becomes difficult to track additions or modifications to the sensor network,
    which occur for several reasons such as new sensor installations or instrument
    relocation. Second, maintaining adequate documentation and metadata about sensor
    characteristics (e.g. instrument types, observational footprints), maintenance
    (e.g. calibration times and procedures, replacements), and event details (e.g.,
    install specifications, failure modes) becomes a time- and resource-intensive
    task. Moreover, the use of different sensor instrumentation and data acquisition
    hardware systems across research groups makes it challenging to create a common
    workflow for transmitting data into a database and comparing data across sensor
    types. For example, soil temperature at the ER is monitored at various locations
    with three types of sensors (RT1, 5TE, and MPS-6) that have inherently different
    characteristics such as support volume, orientation, and sensitivity (Table 1).
    Even for sensors from the same vendor, differences in installation (e.g., orientation
    angle for phenocams) can result in different responses. A variety of data acquisition
    systems are used in the ER, requiring data to be downloaded by exchanging a Secure
    Digital (SD) card, through a serial cable, over bluetooth, and through satellite
    or cellular modems. Each of these are subject to unique failure modes such as
    loss of power to a data logger or loss of wireless connectivity that can lead
    to gaps in data acquisition or transmission (Section IV.A). Additional challenges
    occur since data from other organizations need to be federated into the database.
    This requires that differences in metadata reporting, variable names, time zones,
    and data formats between different providers are reconciled. We approach these
    challenges through a multi-pronged effort. First, we curate and store detailed
    metadata information on permanent infrastructure including sensor placement, sensor
    type and other details about the installation. In some cases, this information
    is not readily available, especially for public data obtained from external sources.
    Second, we identify locations using unique project-specific identifiers for each
    sampling site, and homogenize variable names by mapping to a controlled vocabulary
    that covers the range of data types used in the SFA. This helps mitigate issues
    that might arise due to inconsistent use of location or variable names. Third,
    we use Quantities, Units, Dimensions, and Types (QUDT), a NASA developed architecture
    to standardized unit conversion (http://www.qudt.org/). Then all data are stored
    in a queryable database following a modified version of the Observation Data Model
    [19], which supports storage of heterogeneous environmental data types. Finally,
    all data are made available through an API with token-based authorization. Based
    on our experience, we recommend the use of community-accepted templates for metadata
    reporting and adoption of data standards as a solution to many of these challenges.
    Metadata templates would describe aspects such as sensor and acquisition systems,
    sensor location and placement, calibration procedures, sample collection metadata,
    reporting units, timezone, owner and use restrictions, and sensor events such
    as installs/failures. Some of this metadata are typically not recorded, but provide
    valuable information needed for data interpretation to users. Examples of metadata
    templates that are relevant to watershed monitoring include the EPA’s WQX continuous
    monitoring template (https://www.epa.gov/waterdata/water-quality-exchange-web-template-files)
    [7], FLUXNET’s Biological, Ancillary, Disturbance and Metadata (BADM; https://ameriflux.lbl.gov/data/badm-data-templates/),
    and the FRAMES reporting templates for ecohydrological measurements [49]. In addition,
    adoption of standardized data reporting and formats, will greatly reduce the effort
    required to harmonize and store data, and enable interoperability of the database
    with other systems. Several standards are relevant for water data including the
    OGC/ISO Observations and Measurements [43] and related standards such as SensorML
    [50] and WaterML (https://www.opengeospatial.org/standards/waterml). In large
    sensor networks, the data volumes generated from thousands of raw data streams,
    camera imagery, and all the different postprocessing versions will add up over
    time. In the future, solutions for compression, storage (e.g. fog and edge computing),
    and retrieval of data efficiently will be needed [51]–[53]. C. Data Assessement
    The accuracy and uncertainty in climate, hydrological, and biogeochemical predictions
    depend on the representativeness and quality of the data collected in the field
    and laboratory. Commonly, a QA/QC process starts with a manual (i.e., subjective)
    inspection of data, followed by selection of a statistical algorithm to perform
    cleaning and gap filling that is most suitable for the particular dataset and
    investigation objectives. QA/QC can pose several challenges in detection and cleaning
    of missing or erroneous data, corrections of sensor drift and periodic malfunctioning
    due to intermittent current surges and temperature shifts, as well as accounting
    for the effects of periodic calibration of sensors. First, QA/QC procedures are
    not generalizable, especially when the data have diverse structures, formats,
    and patterns. As an example, data cleaning requires the identification and removal
    of erroneous or inconsistent entries through statistical methods for anomaly/outlier
    detection. However, anomalies or outliers are not necessarily the outcomes of
    incorrect or inconsistent entries - they can infrequently result from extreme
    events, such as flood or drought periods in the ER. Therefore, any anomaly or
    outlier should be examined through the application of domain knowledge (e.g.,
    in terms of ranges and other attributes, and with consideration of complementary
    datasets) before discarding the data. Similarly, effective handling of missing
    data, including gap filling, requires domain-specific knowledge for different
    types of meteorological, hydrological, and geochemical variables. Although a variety
    of statistical methods and tools exist for estimating missing values or correcting
    inconsistencies, gap filling is not easy and can lead to erroneous data. Several
    sources can give rise to missing or inconsistent entries such as periodic malfunctioning
    of sensors, loss of power, sensor drift, technological glitches (i.e., hardware
    failure), signals that exceed the sensor measurement or calibration ranges, and
    human errors. It may be easy to fix technological or sensor related errors but
    not manual errors. Rigorous documentation of all actions conducted on sensors,
    particularly reinstallations and recalibrations, is required to correct for some
    of the manual errors in post-processing (Section IV.B). To address some of these
    challenges, we created interactive scripts to allow users to access data using
    APIs and run cataloging, QA/QC or visualization subroutines in Jupyter Notebooks.
    Users can query for the data available at different stations, and group stations
    or datasets based on designated attributes (e.g., types of measurements). The
    modular workflow helps determine the datasets that need QA/QC, and additionally
    enables exploratory data analysis by helping researchers understand the data availability
    across the watershed to refine their science questions and determine data needs
    for modeling. Users can then run the subroutines that are most appropriate for
    the quality problems in the raw data streams. We find that the use of interactive
    workflow tools such as Jupyter notebooks to be an effective method to provide
    transparency for data processing. The Jupyter tools also help address some of
    the common challenges in QA/QC [54] by generalizing across computing platforms
    and processing software (e.g., R and Python), and enabling users to tweak QA/QC
    procedures and parameters for their custom scientific purpose. Further advancements
    in QA/QC can be achieved by using models (mechanistic or data driven), to identify
    and correct erroneous values, using a co-design approach (Section V), For example,
    we are working to implement mechanistic models (e.g., reactive transport models,
    such as PFLOTRAN and Crunch) in our QA/QC framework to provide sanity checks.
    Examples include flowing the data through models to confirm the charge balance
    in geochemical samples. In addition, the use of machine learning for anomaly detection,
    sequence predictions and signal reconstruction, also holds promise for fully automating
    a real-time QA/QC process. In the ideal situation, QA/QC algorithms are performed
    on real-time streaming data, with automated notifications being sent to the field
    team in the event of a sensor failure or suspicious data. D. Data Integration
    Bringing data from a variety of sources into a unified framework is a difficult
    task but is increasingly needed to quantify and predict complex watershed behavior.
    Integrating data requires re-organizing data from their native formats to a generalized,
    abstracted format to create a synthesized product. The primary challenge with
    integrating heterogenous data is that they can have a variety of native formats
    or structures. Science domains have different norms, and even within a domain,
    common data ontologies or organizational standards have often not been adopted.
    Therefore, data are organized and named in very different ways among data sources,
    requiring significant investment to learn the native format of each data source.
    Even then, the mapping to a generalized format may not be obvious. For example,
    the same term “site” can be used by different providers to represent a region
    containing multiple sampling activities or an individual sampling station. Differences
    in units also require additional processing, and in some cases may need domain
    experts to write conversion routines. Although unit ontologies and conversion
    libraries such as QUDT, UDUNITS (https://www.unidata.ucar.edu/software/udunits/),
    and Pint (https://pint.readthedocs.io/en/0.9/) now exist, no single library covers
    the entire range of physical, chemical, and biological data types that are collected
    at the ER site. Additionally, there is no consistency in the type of metadata
    provided by different sources. Some data sources include detailed metadata (e.g.,
    methods, units, aggregation type), and others provide none. For example, data
    catalog and availability information, and sensor characteristics can be missing,
    making it difficult to create efficient queries to retrieve data. One possible
    solution is for providers or data brokers to adopt internationally recognized
    data standards. For example, in the BASIN-3D data broker, we map data source terminologies
    onto the OGC/ISO data standards [43]. While data standards, such as those from
    the OGC/ISO and Federal Geospatial Data Committee (FGDC) are helpful in tackling
    some of the data diversity issues, they are not easy for domain scientists to
    adopt. This is because the standards are typically described at a high level and
    do not provide implementation guidance beyond simple examples for a few science
    domains. The standards also tend to contain specialized informatics terminology
    that is not easily interpretable by domain scientists. To overcome this problem
    in BASIN-3D, we used a hybrid approach for terminology choices by following the
    standards in most cases, yet modifying some opaque terminology to more common
    terms that can be easily understood by domain scientists (e.g., using utc_offset
    instead of ZoneOffset, units_of_measurement instead of uom). Although BASIN-3D
    is the outcome of our initial efforts at synthesizing diverse multiscale data,
    much work remains to be done in the field of data integration for complex watershed
    datasets. For example, there is no single standard that encompasses all scientific
    data types that we use in the SFA, thus requiring the use of multiple standards.
    Moreover, it is difficult to track the various standards available, and their
    evolution over time. Besides the heterogeneity in metadata, data sources also
    differ in their QA/QC objectives and procedures, and thus comparing the quality
    of data available from different sources is an unresolved problem. Advances in
    data integration technologies are especially needed to achieve the vision of next-generation
    global sensor networks, wherein data from different sites can be synthesized to
    identify universal patterns of watershed behavior [47]. E. Data Access and Visualization
    Tools to discover and explore data must be intuitive and easily usable, to encourage
    adoption by domain scientists [47]. There are many challenges associated with
    designing an intuitive data retrieval and visualization platform for diverse,
    complex data types, and particularly for streaming sensor data. Search interfaces
    need to enable users to easily discover the parameters of interest from amongst
    hundreds of variables. High-resolution sensor data can impact the performance
    of data retrieval and visualization algorithms, and often require some type of
    aggregation or dimension reduction for display on small screens. Multiparameter
    charts can get complex very quickly, particularly when needing to display more
    than two co-located parameters with different units. Display of data from different
    time-zones requires conversion to a uniform time format. Heterogeneous data may
    need different representations - e.g. line plots and bar charts to display on
    the same chart, and there are an increasing number of visualization techniques
    and tools to choose from [55]. Users also need to view annotations on additional
    information such as QA/QC flags (e.g., duplicated data, outliers, corrected or
    gap-filled data), and sensor metadata for data interpretation. While there is
    abundant literature on effective user interfaces for big and streaming sensor
    data, we have found two approaches to be particularly useful in designing such
    systems. The first is to employ user experience (UX) research methods in the design
    of web portal interfaces. UX is a well-established field of human-computer interaction
    research, and has been used effectively for about 30 years in designing products
    and web interfaces. This method develops an understanding of user needs through
    well-defined processes, such as interviews, development of use cases, and guidelines
    for developers, and can be an effective tool for improving usability. For the
    SFA, we employed UX research methodologies (e.g., interviews, use cases, mock-ups,
    and work process observation) that have been shown to work effectively for scientific
    data management and analyses [56]. Both the SFA’s web portals were built after
    following a thorough UX process, and the time spent upfront in designing these
    interfaces resulted in products that have needed minimal modifications since their
    first release. The second solution that has proved to be particularly effective
    for scientific data discovery is the use of interactive widgets and visualizations.
    We used a number of Javascript libraries to create intuitive search and display
    interfaces by customizing text boxes, mapping interfaces, tabular displays, and
    plots. Based on the feedback obtained through the UX approach, we eventually chose
    to create an interface that supported multiparameter charts to deal with the problem
    of displaying various types of data with different units. These charts are synchronized
    between all the variables and allow users to zoom in and out for exploratory data
    analysis (see video in Supporting Information). However, a potential pitfall of
    using third party libraries is their long-term maintenance or lack thereof, which
    can lead to incompatibilities between software versions or security concerns.
    Thus the libraries need to be tracked over time to incorporate their modifications
    into the codebase. Additionally, these tools may not be performant with large
    datasets leading to increased latency in data retrieval or visualizations. We
    chose to aggregate sensor data to daily intervals, with the option of obtaining
    higher-resolution data for smaller periods to overcome the data-size limitations.
    For future improvements, parallelization of the data retrieval/visualization tasks
    can also help with performance [55]. F. Data Publication and Ownership Federally-funded
    research and monitoring programs are now required to publish their data in repositories
    to make their data findable, accessible, interoperable and reusable (FAIR) [57].
    For most long-term sensor deployments, this entails periodic data archival with
    the appropriate metadata and usage policies that allow the public to understand
    and use the data. A challenge faced by many sensor data collectors is the most
    effective means by which to distribute data publicly, given how repositories currently
    operate. Most data repositories, including ESS-DIVE, accept and publish “data
    packages”, which are bundles of data files and metadata. The current recommendation
    from the scientific data community is that sensor data be published in snapshots,
    as periodic data collections with persistent identifiers in recognized repositories
    (http://wiki.esipfed.org/index.php/Sensor_Data_Archiving). This is the approach
    that we have chosen for publicly releasing data from the SFA’s sensor network.
    However this format for archiving and publishing data is not well-suited for streaming
    sensor data, and many challenges remain. For one, the dynamic nature of sensor
    data requires an ability to archive incremental changes (such as new data additions)
    and track different versions of the data, without necessarily assigning a new
    DOI. Data products from QA/QC or processing need to be linked to the original
    raw data streams. Data ownership tends to get complex, and may change for many
    reasons such as the addition of new technical staff or processing by a different
    person on the team. Thus assigning ownership on a constantly evolving data stream
    can be difficult, and currently requires the creation of many different data packages
    that credit the appropriate authors. However, this becomes confusing for data
    users, placing the burden on them to keep track of and cite all the different
    versions of the data accurately. The data package format also doesn’t allow users
    to retrieve subsets of the data easily, and do not require users to adhere to
    data standards. Better solutions for publishing streaming sensor data are urgently
    needed, with increasing amounts of data becoming publicly available due to requirements
    by funding agencies. An ideal approach for data providers would be the ability
    to publish a continuous data stream with persistent identifiers that allow identification
    of different versions of the data. From a user perspective, it’d be preferable
    to have the data organized such that it would be possible to retrieve and visualize
    relevant subsets of the data using custom queries, and with the usage policies
    and citations clearly defined. SECTION V. Utilizing a Co-Design Approach Enables
    Better Measurements and Modeling Developing a predictive understanding of complex
    systems such as watersheds requires models to account for non-linear interactions
    between the intrinsic processes involved across various temporal and spatial scales,
    based on observations. However, the uncertainty associated with modeling and predicting
    watershed behavior has remained high despite the advent of advanced sensor networks
    and availability of high-resolution data for several reasons. First, even with
    sensors, data collection is expensive, and it is practically impossible to collect
    fine-resolution point measurements (e.g., soil moisture, groundwater level) everywhere
    at the watershed scale. Second, processes are linked across the range of scales
    and compartments (e.g., from the vegetative canopy to bedrock, from terrestrial
    hillslopes through aquatic environments); therefore, intelligent networked sensing
    strategies are required for capturing the most valuable information needed to
    predict watershed function. One solution to reduce uncertainties in model predictions,
    and simultaneously optimize sensor systems is to adopt the concept of co-design.
    Co-design refers to the intentional co-development of observation and modeling
    strategies across scales to enhance the two-way exchange of information. While
    developing observation strategies to inform models has been common in recent years,
    in practice there are often gaps in the datasets needed to inform the models,
    and large time delays between data collection and simulation. Few models currently
    take advantage of the increasingly frequent autonomous data streams available
    in near real-time. Strategies to systematically incorporate machine learning or
    other analytical tools to enhance the rapid exchange between observations and
    model predictions and to reduce computational efficiency are lacking. Models are
    not used routinely to optimize observation strategies such as the measurement
    type, frequency and placement. Co-design strategies emphasize the seamless and
    rapid exchange of information and data analytics to address science questions
    or hypotheses about watershed behavior (Figure 6). Thus, co-design enables assimilation
    of integrated data streams into models to improve predictions and computational
    efficiency. It also enables models to be used to determine measurement approaches
    that reduce the uncertainty of system response to perturbation. FIGURE 6. Schematic
    illustration of co-design concepts that judiciously link models (both mechanistic
    and data-driven) and data across scales by involving assimilation, uncertainty
    quantification, and inverse strategies. Co-design strategies should be able to
    capture the highest uncertainty, capture outsized impacts of different processes,
    and predict larger scale system behavior. It should also be able to take advantage
    of increasingly-frequent autonomous datasets, seamlessly incorporate machine learning
    and other analytical tools for near real time exchange of information, and provide
    enhanced computational efficiency. Show All We are developing and testing formal
    co-design strategies within the SFA, to both improve our model predictions as
    well as our observational networks. As an example, we developed a three-dimensional
    flow and reactive transport model using PFLOTRAN [38], [58], which provided information
    about the system behavior before acquiring any field observations at two active
    meanders of the East River watershed. Subsequently, this model-derived understanding
    was used to frame science questions and collect geochemical samples along a two-dimensional
    transect for quantifying subsurface geochemical exports to the river, thereby
    demonstrating one aspect of the co-design strategy [13]. Adopting co-design strategies
    for development of field- and cyber-infrastructure can help address many of the
    challenges presented earlier. For example, sensor network configurations can be
    altered in response to model predictions, after accounting for practical logistical
    issues such as availability of power or site access. As model-informed autonomous
    data acquisition becomes more developed, cyber-infrastructure could play a role
    in translating model output to data acquisition control systems, and documenting
    the rationale for the adjustment in the data collection strategy in the database.
    Additionally, the use of co-design will trigger the collection of sufficient metadata
    about the sensor installation and maintenance to enable use of the data by models,
    including metadata that may not be traditionally recorded as part of sensor deployment
    [49]. This can include more detailed information on sensor placement and orientation,
    or other ancillary data needed to use the sensor outputs (e.g. intermediate manual
    measurements for calibration that can be used to validate gap filling procedures).
    Co-design will also offer an opportunity to coordinate and standardize data collection
    and reporting protocols, to minimize subsequent efforts needed to harmonize and
    convert diverse data into the formats required for use in the models. Finally,
    the incorporation of continual, real-time data QA/QC and processing in the co-design
    workflow will result in better products for modeling and analysis, as well as
    improved data collection. For example, we prepare some model input data by gap-filling
    at hourly (or higher) resolutions with all outliers removed, and transforming
    the data using detrending or normalization. The resulting products are not only
    used in models but are also used to improve and optimize data collection and sensor
    maintenance. Similarly, efforts are underway in the SFA to use models inform QA/QC
    and provide sanity checks (Section IV. C). Thus, the use of co-design requires
    one to view the field data collection and subsequent treatment of the data as
    a continuous, iterative process involving all aspects of the end-to-end field
    to data framework. SECTION VI. Summary The Watershed Function SFA project generates
    and utilizes diverse datasets at its UCRB field site in East River, Colorado,
    which include hydrological, geochemical, geological, geophysical, ecological,
    microbiological, and remote sensing data. The project has developed extensive
    infrastructure and services to: (1) acquire data in the field, (2) manage the
    data with appropriate metadata; (3) enable the team and the broader community
    to discover, visualize and access relevant datasets; (4) perform Quality Assurance
    and Quality Control (QA/QC) of priority datasets; (5) enable efficient data integration,
    and product generation; and (6) publish data for use by the broader scientific
    community and public. Notably, this infrastructure includes tools for end-to-end
    data management and preservation, QA/QC, data discovery and download, and interactive
    visualizations. Such an extensive data pipeline is required to provide quality
    data products for scientific use and modeling. However, the development and maintenance
    of such infrastructure and services is a complex endeavor, which requires addressing
    various problems that can arise with data acquisition, storage, curation, QA/QC,
    integration, and publication. Scientific research projects that deploy and utilize
    sensor networks in the natural environment need to anticipate and plan to overcome
    some of these challenges with an integrated view of the field-data system. Adopting
    a holistic, co-design approach will enable optimization of resources needed for
    solutions that include improving field logistics and sampling procedures, creating
    data processing and software tools, and ultimately methods to better integrate
    data with models. Efforts to simplify and scale such systems across observational
    platforms are needed to make it easier to use data from environmental sensor networks.
    ACKNOWLEDGMENT We also acknowledge the support of the Watershed SFA team, including
    project members who assisted with other aspects of field sampling and laboratory
    analysis, and those who were interviewed for the SFA data portal design. We thank
    Emily Robles for her assistance in preparing the video of the SFA data portal.
    Appendix A video of the SFA data portal is included as Supporting Information.
    Authors Figures References Citations Keywords Metrics Footnotes More Like This
    Monitoring snow depth and its change using repeat-pass interferometric SAR in
    Manas River Basin 2016 IEEE International Geoscience and Remote Sensing Symposium
    (IGARSS) Published: 2016 Using a Ground-Based SAR Interferometer and a Terrestrial
    Laser Scanner to Monitor a Snow-Covered Slope: Results From an Experimental Data
    Collection in Tyrol (Austria) IEEE Transactions on Geoscience and Remote Sensing
    Published: 2009 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Access
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Challenges in Building an End-to-End System for Acquisition, Management,
    and Integration of Diverse Data from Sensor Networks in Watersheds: Lessons from
    a Mountainous Community Observatory in East River, Colorado'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Jayasuriya D.
  - Tuteja N.K.
  - Perkins J.
  citation_count: '2'
  description: Australia has experienced marked climate extremes over the first decade
    of the 21st century. Its streamflow regime can go through prolonged periods of
    droughts such as the "Millennium drought" that occurred between 1997 and 2009
    across eastern Australia. This extreme dry period was followed by back-to-back
    La Niña years during 2010-11 and 2011-12, when Australia experienced severe flood
    events. This variability in extremes has a profound impact on the management of
    water resources in Australia, key drivers being managing community safety from
    floods on the one hand and managing water scarcity from droughts on the other
    hand to minimise the risks related to water supply for urban, irrigation and environmental
    needs. The Bureau is working actively and cooperatively with all key stakeholders
    and end users to develop, implement and deliver end-to-end seamless water forecasting
    services to minimise the impacts of climate variability. The Bureau's Flood Forecasting
    and Warning service provides forecasts of expected river heights across Australia.
    The service spans the full spectrum of activities from the commissioning of networks
    of physical sensors delivering real-time observations, through data analysis and
    modelling, to warning products, delivered to emergency response agencies and the
    Australian public. Under the Water Act 2007 and the Water Regulations 2008, the
    Bureau is working with water managers across Australia to deliver timely, accurate
    and reliable seamless water availability forecasts across Australia at a range
    of time scales - 7 day, monthly, seasonal (3 month) streamflow forecasting services
    and long-term trends in water availability outlooks. The Bureau is actively leveraging
    necessary research and development collaborations - with Commonwealth Scientific
    and Industrial Research Organisation (CSIRO) through the Water Information Research
    and Development Alliance (WIRADA), the Research and Development Branch (Bureau),
    and the University sector. In this paper, we describe the rationale, progress-to-date
    and challenges involved in developing and delivering operational water forecasts
    for Australia.
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: The Art and Science of Water - 36th Hydrology and Water Resources Symposium,
    HWRS 2015
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Seamless water forecasting for Australia - Vision, progress-to-date and challenges
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hamlet A.F.
  - Elsner M.M.G.
  - Mauger G.S.
  - Lee S.Y.
  - Tohver I.
  - Norheim R.A.
  citation_count: '119'
  description: The Columbia Basin Climate Change Scenarios Project (CBCCSP) was conceived
    as a comprehensive hydrologic database to support climate change planning, impacts
    assessment, and adaptation in the Pacific Northwest (PNW) by a diverse user community
    with varying technical capacity over a wide range of spatial scales. The study
    has constructed a state-of-the-art, end-to-end data processing sequence from "raw"
    climate model output to a suite of hydrologic modelling products that are served
    to the user community from a web-accessible database. A calibrated 1/16 degree
    latitude-longitude resolution implementation of the VIC hydrologic model over
    the Columbia River basin was used to produce historical simulations and 77 future
    hydrologic projections associated with three different statistical downscaling
    methods and three future time periods (2020s, 2040s, and 2080s). Key products
    from the study include summary data for about 300 river locations in the PNW and
    monthly Geographic Information System products for 21 hydrologic variables over
    the entire study domain. Results from the study show profound changes in spring
    snowpack and fundamental shifts from snow and mixed-rain-and-snow to rain-dominant
    behaviour across most of the domain. Associated shifts in streamflow timing from
    spring and summer to winter are also evident in basins with significant snow accumulation
    in winter (for the current climate). Potential evapotranspiration increases over
    most of the PNW in summer because of rising temperatures; however, actual evapotranspiration
    is reduced in all but a few areas of the domain because evapotranspiration is
    mostly water limited in summer, and summer precipitation decreases in the simulations.
    Simulated widespread increases in soil moisture recharge in fall and winter in
    areas with significant snow accumulation in winter (for the current climate) support
    hypotheses of increased landslide risk and sediment transport in winter in the
    future. Simulations of floods and extreme low flows increase in intensity for
    most of the river sites included in the study. The largest increases in flooding
    are in mixed-rain-and-snow basins whose current mid-winter temperatures are within
    a few degrees of freezing. The CBCCSP database has been a valuable public resource
    that has dramatically reduced costs in a number of high-visibility studies in
    the PNW and western United States focused on technical coordination and planning.
    © 2013 Copyright Taylor and Francis Group, LLC.
  doi: 10.1080/07055900.2013.819555
  full_citation: '>'
  full_text: '>

    "Access provided by University of Nebraska, Lincoln Log in  |  Register Cart Home
    All Journals Atmosphere-Ocean List of Issues Volume 51, Issue 4 An Overview of
    the Columbia Basin Climat .... Search in:                                        This
    Journal                                                                                Anywhere                                                                  Advanced
    search Atmosphere-Ocean Volume 51, 2013 - Issue 4: From Icefield to Estuary: The
    Columbia Basin / Du champ de glace à l''estuaire : Le bassin du Columbia Submit
    an article Journal homepage Full access 4,085 Views 109 CrossRef citations to
    date 8 Altmetric Listen Original Articles An Overview of the Columbia Basin Climate
    Change Scenarios Project: Approach, Methods, and Summary of Key Results Alan F.
    Hamlet , Marketa McGuire Elsner, Guillaume S. Mauger, Se-Yeun Lee, Ingrid Tohver
    & Robert A. Norheim Pages 392-415 | Received 04 Mar 2012, Accepted 05 Apr 2013,
    Published online: 26 Jul 2013 Cite this article https://doi.org/10.1080/07055900.2013.819555
    In this article 1 Introduction and background 2 Study motivation 3 Overview of
    approach and methods 4 Overview of key products 5 Summary of key results 6 Use
    of products and information by stakeholders, water professionals, and researchers
    7 Future Work 8 Summary and conclusions Acknowledgements References Appendixes
    Full Article Figures & data References Citations Metrics Reprints & Permissions
    View PDF Abstract The Columbia Basin Climate Change Scenarios Project (CBCCSP)
    was conceived as a comprehensive hydrologic database to support climate change
    planning, impacts assessment, and adaptation in the Pacific Northwest (PNW) by
    a diverse user community with varying technical capacity over a wide range of
    spatial scales. The study has constructed a state-of-the-art, end-to-end data
    processing sequence from “raw” climate model output to a suite of hydrologic modelling
    products that are served to the user community from a web-accessible database.
    A calibrated 1/16 degree latitude-longitude resolution implementation of the VIC
    hydrologic model over the Columbia River basin was used to produce historical
    simulations and 77 future hydrologic projections associated with three different
    statistical downscaling methods and three future time periods (2020s, 2040s, and
    2080s). Key products from the study include summary data for about 300 river locations
    in the PNW and monthly Geographic Information System products for 21 hydrologic
    variables over the entire study domain. Results from the study show profound changes
    in spring snowpack and fundamental shifts from snow and mixed-rain-and-snow to
    rain-dominant behaviour across most of the domain. Associated shifts in streamflow
    timing from spring and summer to winter are also evident in basins with significant
    snow accumulation in winter (for the current climate). Potential evapotranspiration
    increases over most of the PNW in summer because of rising temperatures; however,
    actual evapotranspiration is reduced in all but a few areas of the domain because
    evapotranspiration is mostly water limited in summer, and summer precipitation
    decreases in the simulations. Simulated widespread increases in soil moisture
    recharge in fall and winter in areas with significant snow accumulation in winter
    (for the current climate) support hypotheses of increased landslide risk and sediment
    transport in winter in the future. Simulations of floods and extreme low flows
    increase in intensity for most of the river sites included in the study. The largest
    increases in flooding are in mixed-rain-and-snow basins whose current mid-winter
    temperatures are within a few degrees of freezing. The CBCCSP database has been
    a valuable public resource that has dramatically reduced costs in a number of
    high-visibility studies in the PNW and western United States focused on technical
    coordination and planning. RÉSUMÉ [Traduit par la rédaction] Le projet de scénarios
    de changement climatique du bassin du Columbia (CBCCSP) a été conçu comme une
    base de données hydrologiques complète pour appuyer les activités de planification,
    d’évaluation des répercussions et d''adaptation dans la région pacifique nord–ouest
    menées par une communauté d''utilisateurs diversifiée disposant de capacités techniques
    variées dans une large gamme d’échelles spatiales. L’étude a produit une séquence
    de traitements de données de bout en bout, à la fine pointe, partant d''une sortie
    « brute » de modèle climatique pour aboutir à une série de produits de modélisation
    hydrologique, qui sont offerts à la communauté d''utilisateurs via une base de
    données Web. Nous avons implémenté une résolution latitude–longitude calibrée
    à 1/16 de degré dans le modèle à capacité d''infiltration variable (VIC) et avons
    appliqué dans le modèle bassin du fleuve Columbia pour produire des simulations
    historiques et 77 projections hydrologiques futures correspondant à trois méthodes
    de réduction d’échelle statistique et trois périodes futures (les décennies 2020,
    2040 et 2080). Les principaux produits de l’étude comprennent des données sommaires
    pour environ 300 sites fluviaux dans la région pacifique nord–ouest et des produits
    mensuels de Système d''information géographique pour 21 variables hydrologiques
    couvrant tout le domaine à l’étude. Les résultats de l’étude montrent de profonds
    changements dans l''accumulation de neige au printemps et des déplacements radicaux
    de « neige ou pluie et neige mêlées » vers « principalement pluie » dans presque
    tout le domaine. Des déplacements correspondants des caractéristiques d’écoulement
    fluvial du printemps et de l’été vers l''hiver sont également évidents dans les
    bassins où l''accumulation de neige est importante en hiver (sous le climat actuel).
    L’évapotranspiration potentielle augmente dans la majeure partie de la région
    du Pacifique et du Nord–Ouest en été à cause des températures plus élevées; cependant,
    l’évaporation réelle est réduite dans presque tous les secteurs du domaine parce
    que l’évapotranspiration est principalement limitée par l''eau en été et les précipitations
    estivales diminuent dans les simulations. Des accroissements généralisés simulés
    de la réhumidification du sol en automne et en hiver dans les secteurs où l''accumulation
    de neige en hiver est importante (sous le climat actuel) appuient les hypothèses
    de risque accru de glissement de terrain et de transport de sédiments durant l''hiver
    dans le futur. Les simulations d’écoulements de crue et d’étiage augmentent en
    intensité pour la plupart des sites fluviaux compris dans cette étude. Les plus
    fortes augmentations dans les crues sont dans les bassins de pluie et neige mêlées
    dont les températures actuelles au milieu de l''hiver sont à quelques degrés du
    point de congélation. La base de données du CBCCSP s''est avérée une ressource
    publique précieuse qui a permis de réduire énormément les coûts liés à un certain
    nombre d’études de haute visibilité dans la région pacifique nord–ouest et dans
    l''ouest des États–Unis axées sur la coordination technique et la planification.
    Keywords: Columbia River basinclimate change scenariosdownscalinghydrologic modellingwater
    resources impactshydrologic extremesdecision support 1 Introduction and background
    The CIG (http://cses.washington.edu/cig/; see the Table of Acronyms in the Appendix)
    is an interdisciplinary research group at the UW focused on climate-related research
    in five major sectors: atmospheric sciences, hydrology and water resources, aquatic
    ecosystems, forests, and coasts. Since its inception in 1995, CIG has been extensively
    involved in climate-related research focused on the CRB and its water resources
    management systems. In the first five years (1995–2000) of operation, the research
    efforts of CIG were primarily directed towards the assessment of the impacts of
    interannual and interdecadal climate variability associated with ENSO (Battisti
    & Sarachik, 1995; Trenberth, 1997) and the PDO (Gershunov & Barnett, 1998; Mantua,
    Hare, Zhang, Wallace, & Francis, 1997). Hydrology and water resources research
    at CIG was particularly focused on the use of experimental climate and hydrologic
    forecasts for the CRB (Hamlet & Lettenmaier, 1999a, 2000; Lettenmaier & Hamlet,
    2003; Leung, Hamlet, Lettenmaier, & Kumar, 1999) in the context of decision support
    for various water management applications (Hamlet, Huppert, & Lettenmaier, 2002;
    Lee, Hamlet, Fitzgerald, & Burges, 2011; Voisin et al., 2006). Although studies
    addressing natural climate variability remain an important research focus for
    the group, over time research assessing the impacts of anthropogenic climate change
    has become an increasingly important need. One of the first major efforts of CIG
    in this area was focused on the preparation of a detailed and comprehensive regional
    assessment report for the PNW for the 1999 National Assessment of the Impacts
    of Climate Variability and Change in the United States (ultimately published as
    Mote et al., 2003). The regional report for the National Assessment was supported
    by two detailed water management studies focused on the CRB by Hamlet and Lettenmaier
    ( 1999b) and Miles, Snover, Hamlet, Callahan and Fluharty ( 2000). Other climate
    change studies on the Columbia River and its sub-basins followed (Cohen, Miller,
    Hamlet, & Avis, 2000; Elsner et al., 2010; Hamlet, 2003, 2011; Hamlet, Lee, Mickelson,
    & Elsner, 2010b; Lee, Fitzgerald, Hamlet, & Burges, 2011; Lee, Hamlet, Fitzgerald,
    & Burges, 2009; NWPCC, 2005; Payne, Wood, Hamlet, Palmer, & Lettenmaier, 2004;
    Snover, Hamlet, & Lettenmaier, 2003; Vano et al., 2010). These extensive and ongoing
    research activities have also been materially supported by the long-term outreach
    and education programs of CIG, which have, from the outset, fully recognized the
    transboundary nature of the CRB (Hamlet, 2003; Miles et al., 2000) and responded
    by promoting sustained, long-term interaction with CRB researchers and stakeholders
    in the United States and Canada (Hamlet, 2011). An understanding of the basin''s
    transboundary nature has also informed CIG''s hydrologic modelling studies, which
    have consistently provided complete coverage of the Canadian and US portions of
    the basin. Although these ongoing research and outreach efforts had already laid
    an extensive foundation in support of pilot climate services in the PNW, starting
    in 2006–2007 it was realized that a much more comprehensive and focused effort
    to provide hydrologic climate change scenarios was needed if stakeholders and
    water professionals in the region were to take the next steps in preparing for
    climate change. These emerging needs ultimately led to the CBCCSP, and similar
    efforts in BC led by PCIC (Werner, Schnorbus, Shrestha, & Eckstrand, 2013). In
    the remainder of this paper, we present an overview of the development of the
    CBCCSP, a description of the primary methods used to produce the study databases,
    an overview of the products and information the study databases provide, and some
    high-visibility examples of the use of these products and services in regional
    planning. 2 Study motivation As mentioned above, 2006–2007 was something of a
    turning point for regional stakeholders considering future actions to prepare
    for climate change. The sweeping statements in the 2007 IPCC AR4 (Solomon et al.,
    2007) regarding the scientific consensus on observed warming (“unequivocal”) and
    the direct human role in the alteration of the climate system (“90% confidence”)
    made it clear to many management professionals that the “waiting game” for climate
    change planning was nearing an end. Also, at about this time, successful lawsuits
    challenging NEPA studies because they had not addressed climate change effects
    began to appear (Hamlet, 2011). In 2008, many of the financial and institutional
    barriers to climate change assessment and adaptation that had been erected over
    the preceding eight years by the Bush Administration were substantially reduced
    by the incoming Obama Administration. By that same year, a large number of natural
    resources management agencies in the US federal system (e.g., the USFS, USNPS,
    USBR, USFWS, FERC, FEMA, NMFS) were actively engaged in educating and training
    their upper-level leadership and staff about climate change and were attempting
    to acquire appropriate data and information to support long-term planning and
    develop long-term climate change adaptation strategies. Agencies at the state
    and local levels were similarly engaged, two notable examples in the PNW being
    King County, Washington (Casola et al., 2005), and the WDOE, which manages (among
    many other water-related issues) the state''s water resources and water quality
    permitting programs. In 2006, The Act relating to Water Resource Management in
    the Columbia River Basin [hereinafter HB2860] (2006) directed the WDOE to study
    water resources systems in Washington and identify specific projects in which
    to invest up to US$200 million provided by the bill to improve water resources
    infrastructure or management systems. The WDOE was also directed to incorporate
    climate change explicitly in these comprehensive assessment efforts. One of the
    fundamental difficulties with this task was that there was not, at the time, an
    available database of hydrologic projections that could support such planning,
    and regulatory agencies such as WDOE did not have the capacity or expertise to
    produce these resources themselves. Prior climate change datasets for the CRB
    produced by CIG (using CMIP2/TAR projections) only included about 20 river sites
    (e.g., NWPCC, 2005; Snover et al., 2003) and provided very limited support for
    planning efforts at smaller spatial scales. It was also well understood by practitioners
    at CIG that aquatic and terrestrial ecosystem researchers, managers, and stakeholders
    needed a similar, but more comprehensive, data resource to support long-term planning
    and the development of climate change adaptation strategies at the landscape scale.
    The CBCCSP was developed to address these diverse needs. The scope of work for
    the project called for hydrologic modellers at CIG to produce the following results:
    • A suite of up-to-date hydrologic projections for the entire CRB (including portions
    of the basin in Canada) based on the CMIP3/AR4 (Meehl et al., 2007) GCM projections.
    • Detailed water balance summaries and streamflow data for up to 300 river locations
    to be specified by WDOE and other stakeholders in the region. • A comprehensive
    assessment of hydrologic extremes such as Q100 and 7Q10. • Gridded databases providing
    full GIS coverage of important hydrometeorological variables in support of a wide
    range of research applications, including ecosystem research. A comprehensive
    website was to be developed to serve all the data produced by the study, at no
    cost, to the general public, management professionals agency staff members, scientific
    researchers, private sector consultants, etc. To support ecosystem research and
    impacts assessment, CIG extended the project to include specific meteorological
    and hydrological variables needed to support ecological studies (see discussion
    in Section 3). These approaches were further developed and refined during the
    WACCIA in 2009 (Miles, Elsner, Littell, Binder, & Lettenmaier, 2010), which included
    assessments of aquatic and forest ecosystem impacts (Littell et al., 2010; Mantua,
    Tohver, & Hamlet, 2010). 3 Overview of approach and methods In this section we
    provide an overview of the methods associated with the primary elements of the
    CBCCSP. Additional details on the approach and methods are available in the CBCCSP
    study report (Hamlet, Carrasco, et al., 2010a). a Gridded Historical Meteorological
    Datasets Gridded meteorological datasets (daily total precipitation and maximum
    and minimum daily temperature) at 1/16 degree latitude-longitude resolution (approximately
    7 km by 5 km) were constructed for the study from observed station records for
    the period 1915 to 2006. The first nine months of the dataset were used for hydrologic
    model spin-up, resulting in 91 water years (October–September) of usable historical
    data from the hydrologic model simulations. The approach used a refined version
    of the methods established by Hamlet and Lettenmaier ( 2005), which created a
    hybrid historical meteorological dataset based on three primary data resources:
    a) monthly HCN data in the United States and the similar AHCCD datasets in Canada,
    b) daily data from the cooperative station network in the United States (co-op
    stations) and similar data from Environment Canada, and c) a monthly climatology
    (1971–2000) for precipitation and daily minimum and maximum temperatures at 30
    arc-second resolution produced using the PRISM (Daly, Gibson, Taylor, Johnson,
    & Pasteris, 2002; Daly, Neilson, & Phillips, 1994; DiLuzio, Johnson, Daly, Eischeid,
    & Arnold, 2008). The PRISM data for Canada were interpolated to 30 arc-second
    resolution data from a 2.5 arc-minute (approximately 4 km) product and were statistically
    adjusted to remove the bias associated with the different time period (1961–1990
    means for the 4 km product). The approach and methods are more fully described
    in the study report (Hamlet, Carrasco, et al., 2010a, Chapter 3) and also by Elsner
    et al. ( 2010), but the essential idea behind the methods is that monthly gridded
    data are based only on serially complete and quality-controlled HCN and AHCCD
    stations (thus ensuring self-consistent long-term trends based on the same group
    of stations), but daily variations within the month come from re-gridded daily
    co-op station data, which add additional spatial detail on an event basis at daily
    time scales (Hamlet & Lettenmaier, 2005). Topographic corrections for precipitation
    and temperature are carried out by rescaling the data by a fixed factor for each
    calendar month so that the mean values from 1971 to 2000 match the PRISM climatology
    for the same period. Wind speed data are based on interpolated NCEP reanalysis
    data (Kalnay et al., 1996) using methods described by Elsner et al. ( 2010). Additional
    meteorological forcings needed for hydrologic model simulations (e.g., net incoming
    long- and shortwave radiation, dew point temperature, etc.) are estimated by the
    VIC hydrologic model (discussed below) using empirical methods described by Kimball,
    Running and Nemani ( 1997) and Thornton and Running ( 1999). All of the meteorological
    forcing data, except wind speed, are reproduced in the output files produced by
    the hydrologic model. These extended meteorological data have proved particularly
    useful in supporting ecological studies (e.g., Littell et al., 2010). b Selection
    of Streamflow Locations A primary motivation for the study was to support planning
    at geographic scales ranging from relatively small river basins (e.g., planning
    studies in individual sub-basins, such as the Yakima or Okanagan basins) to main-stem
    planning studies for the CRB as a whole (e.g., planning studies for the Columbia
    River hydro system). To select an appropriate group of specific streamflow locations
    to include in the study to meet these diverse needs, the primary funding agencies
    for the study and several other key water management agencies in the region (MDNR,
    IDWR, USBR, and USACE) were asked to submit prioritized lists of streamflow locations.
    Many river locations that were submitted for consideration were at gauging locations
    supported by the USGS and ECAN, or at locations associated with important water
    resources monitoring needs (e.g., checkpoints for flood control, water supply,
    or environmental flows) or infrastructure (e.g., dams and diversion points). Figure
    1 shows a map of the approximately 300 streamflow locations that were ultimately
    compiled from these lists for inclusion in the study (a spreadsheet listing these
    sites is available on the CBCCSP website (CIG, 2013a)). The smallest of these
    sub-basins is about 500 km2 (approximately fifteen 1/16 degree VIC cells), and
    the largest encompasses most of the CRB (approximately 620,000 km2 or about 18,800
    VIC cells). About 15 sites in western Washington, outside the CRB, were also included
    in support of the 2009 WACCIA. Fig. 1 Map of the selected streamflow locations
    supported by the CBCCSP. Red dots indicate sites that are essentially unimpaired
    by human use or for which there is estimated modified* or naturalized flow. Sites
    without modified or natural flow estimates are shown in yellow. (*Modified flows
    are essentially naturalized flows with a consistent level of consumptive demand
    for water supply subtracted for the entire time series.) Display full size c Climate
    Change Scenarios, Selection of GCM Projections, and Downscaling Procedures The
    SRES A1B and B1 GHG emissions scenarios (Nakićenović et al. 2000) were selected
    as the basis for the study because they provide a) a wide range of plausible outcomes
    while also reflecting some potential GHG mitigation by the end of the twenty-first
    century, and b) most of the approximately 20 GCM projections associated with the
    CMIP3 archived runs from both emission scenarios (Mote & Salathé, 2010). During
    the study (and afterwards), some stakeholders expressed interest in including
    less optimistic emissions scenarios (such as SRES A1FI), in order to better understand
    the implications of a potential “worst case” scenario. This extreme scenario,
    however, was only run by a few GCMs, which ultimately limits the ability to show
    consistent ranges of outcomes for each emissions scenario. Although not as extreme
    as A1FI, the high-end A2 scenario was archived by most GCMs and could have been
    used in the CBCCSP in place of the A1B scenario. The choice of the A1B scenario,
    however, was informed by the authors’ viewpoint that this scenario is an instructive
    and plausible scenario reflecting relatively little GHG mitigation until mid-century
    (similar to A2 until about 2050), followed by more effective GHG mitigation efforts
    in the second half of the twenty-first century as impacts intensify. Three statistical
    downscaling approaches were selected or developed for the study: • Composite Delta
    (CD): regional average projections compiled from 10 GCMs (Elsner et al., 2010)
    • Bias Correction and Spatial Downscaling (BCSD): (Salathé, 2005; Salathé, Mote,
    & Wiley, 2007; Wood, Leung, Sridhar, & Lettenmaier, 2004; Wood, Maurer, Kumar,
    & Lettenmaier, 2002) • Hybrid Delta (HD): (Hamlet et al., 2010a) Each of these
    methods has its specific advantages and limitations (as discussed in detail in
    Hamlet et al., 2010a); however, the HD method combines several important strengths
    of the CD and BCSD methods and was developed specifically to support the prediction
    of daily hydrologic extremes (Hamlet et al., 2010a). The HD method was also selected
    as the basis for the main summary products derived for each river location (see
    description below), primarily because it was capable of providing good performance
    over the complete range of products produced by the CBCCSP (Hamlet et al., 2010a).
    Ten GCM projections for the A1B scenario and nine projections for the B1 scenario
    (Table 1) were selected based on a ranking of the GCMs reflecting the combined
    ability of each GCM to reproduce key features of PNW climate variability, including
    the seasonal cycle of precipitation, observed trends in temperature in the late
    twentieth century, bias in reproducing historical temperature and precipitation,
    and ability to capture key features of observed climate variability (spatial patterns
    of temperature, pressure, and precipitation) over the North Pacific (Mote & Salathé,
    2010). For the BCSD runs (for which the ability to capture key elements of the
    region''s climate variability is arguably even more important to the outcomes)
    the projections based on the seven highest ranked GCMs (Table 1) were selected
    for each emissions scenario. For the CD and HD downscaling methods, which construct
    a 91-year time series for both historical and future time periods, three future
    time periods were selected: 2020s (2010–2039), 2040s (2030–2059), and 2080s (2070–2099).
    The BCSD runs are transient runs from 1950 to 2098 or 1950 to 2099 (depending
    on the GCM). Table 1 summarizes the 77 future meteorological forcing datasets
    that were prepared for the study. Table 1. Matrix of climate change projections
    included in the study. Numbers in the table show the number of GCM projections
    used for each downscaling approach and/or time period. (Note that data for the
    B1 emissions scenario were not available from one of the ten GCMs used as input
    to the HD and CD approaches.) Download CSVDisplay Table It is important to acknowledge
    that opinions differ on the utility or even possibility of improving ensembles
    of future projections based on the ability to simulate the past climate (e.g.,
    Gleckler, Taylor, & Doutriaux, 2008). However, our goal in this case was not to
    reduce the range of uncertainty by selecting a smaller group of GCMs. Instead
    our primary goal was to encompass the approximate range of all available scenarios
    while reducing costs by downscaling projections from a subset of the larger group
    of 20 GCMs (Hamlet et al., 2010a). Thus, by selecting 10 GCM scenarios with good
    historical performance that also spanned the range of impacts, we effectively
    reduced the computational and storage requirements of the CBCCSP by approximately
    a factor of two. d Macroscale Hydrologic Model Implementation and Calibration
    The macroscale hydrologic model used in the CBCCSP is the VIC model (Cherkauer
    & Lettenmaier, 2003; Liang, Lettenmaier, Wood, & Burges, 1994) implemented at
    1/16 degree resolution. The VIC model has been widely applied in climate change
    studies at both the regional scale (e.g., Christensen & Lettenmaier, 2007; Lettenmaier,
    Wood, Palmer, Wood, & Stakhiv, 1999; Maurer, 2007; Maurer & Duffy, 2005; Payne
    et al., 2004; Van Rheenen, Wood, Palmer, & Lettenmaier, 2004) and global scale
    (e.g., Adam, Hamlet, & Lettenmaier, 2009; Nijssen, O''Donnell, Hamlet, & Lettenmaier,
    2001). Among its most useful features is the predominantly physical basis of the
    model, which largely avoids concerns about parameter stationarity in a changing
    climate. For climate change studies in the western United States where snow is
    an important element of the hydrologic cycle, the model''s use of a sophisticated
    energy-balance snow model, which incorporates important effects on snow accumulation
    and melt associated with vegetation canopy (Andreadis, Storck, & Lettenmaier,
    2009) has been a notable advantage. Using the VIC model, Mote, Hamlet, Clark,
    and Lettenmaier ( 2005), Mote, Hamlet, and Salathé ( 2008), for example, showed
    excellent reproduction of observed trends in 1 April SWE over both the PNW as
    a whole and over the Cascade Range in Oregon and Washington. Most of the fundamental
    details regarding the VIC implementation used in this study are covered by Elsner
    et al. ( 2010). Here we will review a few important aspects of the basic implementation
    to help orient the reader and will then focus most of our attention on the additional
    implementation and calibration tasks carried out during the CBCCSP. The VIC model
    (version 4.0.7) was implemented at 1/16 degree resolution, with three active soil
    layers and up to five elevation bands with an approximate spacing of 500 m. The
    model was run in water balance mode with a snow model time step of 1 h and a water
    balance time step of 24 h. The model was coupled to a simple daily-time-step routing
    model (Lohmann, Raschke, Nijssen, & Lettenmaier, 1998), which was used to produce
    daily flow estimates at each of the approximately 300 streamflow locations included
    in the study. The VIC implementations make use of preprocessed soil and vegetation
    databases for the basin of interest. During initial model development steps, Elsner
    et al. ( 2010) interpolated existing 1/8 degree model parameters to 1/16 degree
    and also included previously calibrated soil parameters for the Yakima sub-basin
    (please see acknowledgements). Elsner et al. ( 2010) also updated the soil depth
    map using a more sophisticated approach developed for the DHSVM (Wigmosta, Nijssen,
    Storck, & Lettenmaier, 2002; Wigmosta, Vail, & Lettenmaier, 1994) that varies
    soil depth with elevation. Starting from these parameter sets, an additional large-scale
    calibration was performed during the CBCCSP to improve model performance in reproducing
    historical streamflow. The strategy for model calibration used in the CBCCSP was
    to calibrate 11 relatively large sub-basins within the domain (Fig. 2), for which
    overall errors in meteorological driving data were assumed to be relatively small;
    then, using these model parameters, to check the results in smaller sub-catchments.
    This approach was partly based on practical limitations on time and computational
    resources but was also informed by previous experience using the VIC model at
    finer spatial scales. In relatively small basins (approximately 500–1500 km2),
    of which there are a substantial number included in the study, errors in meteorological
    driving data are often a strong determinant of simulation errors. Fine-scale calibration
    of the model to compensate for such errors, although technically feasible, is
    of questionable utility, because it essentially ensures that the model is getting
    something closer to the right answer for the wrong reasons, which in turn has
    the potential to distort the sensitivity of the model to changing future conditions
    (Bennett, Werner, & Schnorbus, 2012). Fig. 2 Eleven sub-basins in the CRB used
    for large scale calibration (left panel). REVEL = Columbia River at Revelstoke
    Dam, CORRA = Kootenay River at Corra Linn Dam, WANET = Pend Oreille River at Waneta
    Dam, LIBBY = Kootenai (Kootenay) River at Libby Dam, DWORS = N. Fork Clearwater
    River at Dworshak Dam, MILNE = Snake River at Milner, ICEHA = Snake River at Ice
    Harbor Dam, PRIRA = Columbia River at Priest Rapids Dam, YAPAR = Yakima River
    at Parker, DALLE = Columbia River at The Dalles, OR, WILFA = Willamette River
    above falls at Oregon City. A number of sub-basins are nested within each other,
    as shown in the right panel along with their relative sizes. (For example, Dworshak
    and Milner are nested within the larger Ice Harbor sub-basin.). Display full size
    Because of a general lack of observed naturalized daily time-step flow for most
    streamflow sites, daily time step calibration using additional parameters such
    as the infiltration parameter (bi ) in VIC (Liang et al., 1994), or routing parameters
    (such as the unit hydrograph for each cell) were not attempted during the study.
    A subsequent study in the Skagit River basin (Lee and Hamlet, unpublished manuscript)
    has demonstrated that substantial improvements in the simulation of high flow
    extremes can be achieved by calibrating the routing model, but it is not yet clear
    whether these conclusions can be generalized to other areas of the domain. Three
    base flow parameters (Ds max, Ds, Ws) associated with the non-linear baseflow
    curve from the third soil layer (Liang et al., 1994) were used to calibrate the
    model. The highest value of baseflow is Ds max (in millimetres of runoff per time
    step) for a saturated soil layer; Ws represents the soil moisture threshold below
    which the baseflow curve is linear; and Ds is the baseflow value (in millimetres)
    at this breakpoint. These parameters were chosen because they strongly affect
    the timing and volume of runoff production in the model simulations and are, in
    general, not available from observed data. Naturalized or modified flow data were
    available at a number of locations in the PNW. These data were compiled from naturalization
    studies prepared for the BPA (Crook, 1993), WDOE (Flightner, 2008), OWRD (Cooper,
    2002), IDWR, and the USBR. In addition, some observed streamflow data are suitable
    for use as “natural” data if the effects of storage and diversions are relatively
    small (e.g., for the USGS Hydro-Climatic Data Network streamflow sites). Naturalized
    streamflow data were used exclusively in the CBCCSP to calibrate the hydrologic
    model. Naturalized and modified streamflow data were used to produce bias-corrected
    streamflow realizations (see below). Calibration of the VIC model was carried
    out using an automated calibration tool called MOCOM-UA developed by the Land
    Surface Hydrology group at the UW, following the approach described by Yapo, Gupta,
    and Sorooshian ( 1998). The process was also significantly improved by researchers
    at PCIC who reconfigured and optimized the code to run more efficiently on a Linux
    cluster (Schnorbus, Bennett, Werner, & Berland, 2011). The MOCOM-UA tool uses
    an objective function (defined by the user) and a shuffle complex evolution procedure
    to optimize model calibration parameters to create a set of Pareto (equally) optimal
    calibration parameters. We used 50 parameter sets to define the initial optimization
    parameter space, of which the 25 best parameter sets advance in each evolution
    of the optimization. Three VIC model calibration parameters described above were
    varied in the optimization process, and six error metrics were used to define
    the objective function: squared correlation coefficient (R 2), NSE, the NSE of
    log-transformed data, annual volume error, mean hydrograph peak difference, RMSE,
    and number of sign changes in the simulated streamflow errors. The objective function
    for the optimization process in this case was: where Q is the monthly streamflow;
    NSE(Q) is the NSE (monthly flow), which varies between [−inf, 1] and typically
    between [0,1]; NSE(log(Q + 1)) is the NSE (monthly flow), which varies between
    [−inf, 1] (this metric places less emphasis on high flow errors in calculating
    NSE); Vol_Err(Q) is the annual volume error (in 1000 acre feet); R 2(Q) = R 2
    (squared correlation coefficient between simulated and observed Q), which varies
    between [0,1]; Peak_Diff(Q) is the mean hydrograph peak difference—the absolute
    value varies for different sites; RMSE(Q) is the root mean square error, whose
    absolute value varies for different sites; and NumSC(Q) is the number of sign
    changes in the errors (this metric penalizes simulations with too much month-to-month
    variability in comparison with observations). Model calibration and validation
    used a split sample approach in which calibration was performed for each of the
    11 primary watersheds over a 15-year period (typically water years 1975–1989)
    and model validation was performed over a separate 15-year period (typically water
    years 1960–1974). Both calibration and validation periods were chosen to test
    model performance over a wide range of climate and streamflow conditions. The
    two time periods also represent very different patterns of decadal climate variability
    in the historical record, providing a useful test in the context of simulating
    a changing climate. Final calibration results for the model are shown in Fig.
    3. The model shows reasonably good calibration statistics for the majority of
    the sites, and the calibration is robust (showing equally good or better statistics
    in the validation period when compared with the calibration period). About 50
    of the 80 sites evaluated show monthly NSE scores greater than or equal to approximately
    0.7 (good to excellent fit). The NSE scores for about 20 sites are marginal (between
    0.3 and 0.7). About 10 locations show negative NSE scores, which usually occurs
    when the simulations are strongly biased in comparison with observations. This
    diagnosis is confirmed by the R 2 values for the same sites, which are generally
    higher and more consistent with neighbouring values over the entire domain. In
    other words, although at these sites the model results do not match the observations
    in the absolute sense (large bias), the relative changes follow the observed variations
    quite well (high R 2). Strong bias in the simulations is commonly caused by precipitation
    errors (too much or too little annual precipitation), or in some cases by substantial
    errors in base flows because of contributions from groundwater in the actual system,
    which are not simulated by the VIC model (Wenger, Luce, Hamlet, Isaak, & Neville,
    2010). Fig. 3 Summary map of 80 streamflow locations (out of a total of 297) for
    which error statistics between simulated and naturalized flows were computed.
    The top two panels show the Nash Sutcliffe Efficiency (NSE) (left) and R 2 (right)
    for the calibration period, while the two lower panels show NSE (left) and R 2
    (right) for the validation period. Small black dots indicate streamflow sites
    where naturalized flows were not available. Display full size To explore how much
    the model simulations might be improved by additional fine-scale calibration,
    we also recalibrated three additional smaller sites within the Pend Oreille River
    basin. The results were only slightly better than those achieved in the large-scale
    calibration. Although results could potentially vary in different areas of the
    model domain, these results support the hypothesis that only modest improvements
    in validation statistics would result from individual calibration of additional
    streamflow sites within each sub-basin. e Fine-Scale Hydrologic Modelling The
    CBCCSP also included fine-scale hydrologic modelling using DHSVM (Wigmosta et
    al., 1994, 2002) in four pilot watersheds in the PNW. Because of space limitations,
    we will not be able to cover these alternative modelling efforts in this paper.
    The interested reader is directed to the CBCCSP report (Hamlet, Carrasco, et al.,
    2010a, Chapter 6) for a detailed discussion of methods and results. f Model Output
    Variables Twenty-one daily time-step output variables were archived for the VIC
    simulations (Table 2). These include the full meteorological forcings for the
    model (variables 1–8), a suite of water balance variables simulated by the model
    (variables 9–16), and five different PET metrics (variables 17–21) (Elsner et
    al., 2010). Table 2. Archived daily VIC hydrologic model output variables. The
    variables are displayed in the order that they are archived in the VIC output
    files. The far right column shows the method used to aggregate daily output to
    monthly values for each variable. Download CSVDisplay Table g Model Post Processing
    Post-processing of the primary VIC model output (see Table 2) was carried out
    to produce a number of specific products discussed in the following sections.
    Figure 4 shows a flow chart of these post-processing steps. Fig. 4 Flow chart
    illustrating the post-processing steps used to produce the various hydrologic
    products served on the study website. Display full size Daily and monthly streamflow
    for each streamflow site are provided in two formats: a) raw VIC simulations,
    and b) bias-adjusted simulations. To produce the bias-adjusted flows, a bias correction
    procedure using quantile mapping techniques is applied (Elsner et al., 2010; Snover
    et al., 2003; Vano et al., 2010). These techniques remove systematic biases in
    the simulations of routed streamflow to produce products that closely match the
    long-term statistics of a natural or modified flow dataset for a particular site.
    The bias-corrected monthly values are then used to rescale the simulated daily
    flow sequences produced by the hydrologic model to produce bias-corrected daily
    streamflows. This daily disaggregation technique sometimes introduces an undesirable
    discontinuity in the bias-corrected daily values at the beginning and end of months.
    To minimize this data processing artifact, boundaries between months were smoothed
    while keeping the sum of daily streamflows equal to the original monthly values
    in the final product. Bias-corrected streamflow values are useful in water planning
    studies, especially for providing inputs to reservoir operations models that are
    calibrated on a particular naturalized or modified flow dataset (e.g., Hamlet,
    Lee, et al., 2010b; NWPCC, 2005; Vano et al., 2010). These approaches are also
    useful for avoiding biases in the streamflow simulations that result from systematic
    errors in gridded precipitation or temperature data. As noted above, such errors
    are commonly encountered at relatively small spatial scales, particularly when
    meteorological stations are sparse, and often cannot be resolved using conventional
    hydrologic model calibration strategies. Bias-correction procedures provide an
    alternative statistical approach that effectively avoids these difficulties (Shi,
    Wood, & Lettenmaier, 2008; Snover et al., 2003). Either naturalized or modified
    flows (Crook, 1993) are used for bias correction of data provided in the site-specific
    products discussed below, with naturalized data taking precedence if available.
    Thus, each site is bias-corrected using either naturalized or modified flow but
    not both. If neither naturalized nor modified flow is available, no bias-adjusted
    data were provided. Bias adjustment was also used in preparing the specific reservoir
    modelling support products discussed below. In this case only modified flows (2000-level
    modified flows obtained from the BPA (Crook, 1993)) were used to train the bias-correction
    procedure, even if naturalized flows were also available. h Calculation of Extreme
    Event Statistics Extreme event statistics are calculated directly from the raw
    (i.e., not bias-adjusted) daily streamflows at each streamflow site, applying
    methods developed by Hamlet and Lettenmaier ( 2007) and Mantua et al. ( 2010)
    over the entire PNW (Tohver et al. unpublished manuscript). The methods are discussed
    in more detail in these references, but a brief description of the procedure is
    given here to help orient the reader. Daily streamflow data from the CD and HD
    downscaling methods are first processed to extract the peak daily flow in each
    water year of the simulations (91 years). These annual peak daily flows are then
    ranked and assigned a quantile value using an unbiased quantile estimator based
    on the method of Cunane (Stedinger, Vogel, & Foufoula-Georgiou, 1993). After fitting
    three-parameter generalized extreme value probability distributions to the annual
    peak flow data, the daily 20-, 50-, and 100-year floods (under natural flow conditions)
    are estimated for both historical and future periods. The same procedure is followed
    for estimating extreme low flows, except the lowest 7-day flow is extracted for
    each water year, and 7Q10 (the extreme 7-day low flow with a return interval of
    10 years) is estimated. These data are summarized in figures and tables prepared
    for each streamflow site discussed in Section 4. Regional summaries were prepared
    by Tohver et al. (unpublished manuscript). 4 Overview of key products a Site Specific
    Data For each streamflow location (and its associated contributing basin area),
    a set of identical products is available on the study web site (CIG, 2013b). •
    Basin topographic map and smoothed basin boundary at 1/16 degree resolution. •
    Summary information and statistics: geographic location, basin area, calibration
    statistics (if available), links to the USGS or ECAN websites, and so forth. •
    Daily and monthly average streamflow for all projections listed in Table 1 and,
    where possible, daily and monthly bias-corrected streamflow values. • Figures
    and summary tables for long-term average monthly precipitation, monthly average
    temperature, evapotranspiration, PET4 (Table 2), PET5 (Table 2), total column
    soil moisture, SWE, combined flow (runoff + baseflow). These products are based
    solely on the HD projections listed in Table 1. • Figures and summary tables for
    flood statistics and low-flow statistics. These products are based solely on the
    CD and HD projections listed in Table 1. Note that unlike the raw VIC flux files
    discussed above (Table 2), imperial units are used for these products on the study
    website (cubic feet per second, inches, degrees Fahrenheit). This choice was imposed
    by WDOE. Here, however, we show the same figures in metric units. b Site Specific
    Summary Figures The summary figures for water balance variables at each site have
    the same format, two examples of which are shown in Fig. 5. Each of the six panels
    in the figure shows the long-term monthly mean for the 10 (9 for B1) HD GCM scenarios
    (red lines) and the historical simulations (blue lines). For the future scenarios,
    the range of the projections (pink shading) is plotted. The six panels display
    results from the combination of three time periods (rows) and two emissions scenarios
    (columns) used in the HD downscaling process. Text files (six per figure) providing
    all the ensemble data used to construct each panel in the figure are also provided
    on the CBCCSP website. Statistics for hydrologic extreme events (as discussed
    above) are presented in a different format, shown in Fig. 6. Fig. 5 Examples of
    summary plots for monthly snow water equivalent (SWE) (mm) (averaged over the
    upstream basin area) and raw streamflow not adjusted for bias (cubic metres per
    second) for the Skagit River at Mount Vernon. Blue traces show monthly averages
    for historical conditions; the pink bands show the range of projected change associated
    with each scenario and future time period; the red lines show the average of the
    future ensemble. Display full size Fig. 6 Example of a summary plot for extreme
    high flows (Q20, Q50, Q100, left panels) and extreme low flows (7Q10, right panels)
    for the Skagit River at Mount Vernon for two emissions scenarios (A1B, B1) and
    three future time periods (2020s, 2040s, 2080s). Blue dots represent the historical
    values; the red dots show the range of values from the HD ensemble (10 or 9 values);
    black dashes show the mean of the HD ensemble, and the orange dots show the single
    value calculated for the CD projections. Display full size c Gridded Datasets
    Gridded datasets provide full spatial coverage (i.e., all grid cells in the model
    domain) at monthly time scales, of the key hydroclimatic variables listed in Table
    2. Each product is provided as a gridded file (one file for each variable and
    calendar month) in ASCII format. In addition to the time series gridded data,
    the long-term monthly mean data for each hydrologic variable, for each scenario,
    is provided in GridASCII format, compatible with ArcGIS. These products are available
    for all 77 climate scenarios listed in Table 1, as well as for the historical
    simulation. d Reservoir Model Inputs Bias corrected inflows were produced to support
    the GENESYS and HYDSIM reservoir operations models, which are used by the NWPCC
    and BPA, respectively, for main-stem studies in the CRB (e.g., NWPCC, 2005). Naturalized
    flow products from specific sites were also used to provide naturalized inflows
    at model nodes needed to run the USBR MODSIM (Labadie, 2007) reservoir model for
    the Snake River basin. 5 Summary of key results Changes in snowpack are a key
    driver of hydrologic impacts in the PNW (Elsner et al., 2010; Hamlet & Lettenmaier,
    1999b). Confirming the sensitivity to warming demonstrated in earlier studies,
    the CBCCSP results show widespread reductions in the 1 April snowpack, and systematic
    reductions in the long-term average SWE2PR, a measure of the importance of snow
    in the hydrologic cycle (Fig. 7). Changes in the 1 April snowpack have been shown
    to depend strongly on winter temperature regimes (Hamlet, Mote, Clark, & Lettenmaier,
    2005; Mote, 2006; Mote et al., 2005). As a result the largest changes in snowpack
    are apparent in the simulations for relatively warm coastal mountain ranges, such
    as the Cascade Range, and at moderate elevation in the Rockies, where snowpack
    is most sensitive to changes in temperature of a few degrees Celsius. Note the
    relative insensitivity of SWE to warming in the coldest, and most heavily snowmelt-dominant,
    areas in the northern tip of the CRB in British Columbia in comparison with the
    rest of the domain. These areas are so cold in winter (DJF average on the order
    of -10°C temperature) that a change in temperatures of 2°–3°C has relatively little
    effect on seasonal snow accumulation in the 2020s and 2040s. Instead, these areas
    respond primarily to projected changes in precipitation until late in the twenty-first
    century, and in fact some of these areas show modest increases in SWE (about +5%)
    until the middle of the twenty-first century under the combined effects of warming
    and increasing cool season precipitation. The same basic effects are seen in the
    SWE2PR maps, where snowmelt remains dominant in the northern tip of the CRB even
    at the end of the twenty-first century, whereas in the US portions of the domain
    there are widespread transformations of mixed-rain-and-snow river basins to rain-dominant
    basins and snowmelt-dominant basins to mixed-rain-and-snow basins. The changes
    in the importance of snow in some areas of the United States are particularly
    striking. The state of Oregon, for example, is classified as about 75% mixed-rain-and-snow
    for the twentieth century climate. By the end of the twenty-first century, essentially
    the entire state is classified as rain-dominant for the A1B scenario, and the
    B1 scenario is only slightly different (Fig. 7). These results point to extensive,
    landscape-scale transformations in hydrologic behaviour associated with climate
    change. Fig. 7 Left panel: Simulated historical 1 April snow water equivalent
    (SWE) (upper right) and percentage changes in 1 April SWE for two emissions scenarios
    and three future time periods extracted from the CD VIC scenarios. Inset numbers
    at the upper left in the future projections are the percentage changes in 1 April
    SWE averaged over each grid cell in the entire domain. Right panel: Historical
    and projected future watershed classification (rain-dominant, transitional (mixed-rain-and-snow),
    snow-dominant) for 10-digit Hydrologic Unit Code watersheds, based on the long-term
    mean of the SWE2PR for each watershed. (1 April SWE and SWE2PR values were calculated
    using the CD VIC scenarios.) Display full size Monthly hydrographs in different
    portions of the domain primarily reflect changes in snow accumulation and melt
    processes and seasonal changes in precipitation (generally wetter falls, winters,
    and springs and drier summers). Figure 8 shows hydrographs from selected basins
    with different hydrologic classifications (snowmelt-dominant, mixed-rain-and-snow,
    and rain-dominant) in the United States and Canada. The site at Corra Linn Dam
    on the Kootenay River is representative of changing hydrographs in many locations
    in the northern tip of the CRB in British Columbia. Peak flows actually increase
    at many sites in Canada because of increasing fall, winter, and spring precipitation
    in this part of the domain, although the peak flow also occurs about a month earlier.
    Snowmelt-dominant basins in the United States, which are somewhat warmer and do
    not experience as much precipitation change in the scenarios, show increases in
    winter flow, earlier and reduced peak flow in spring, followed by an earlier streamflow
    recession and lower flows in late summer (e.g., Columbia River at The Dalles in
    Fig. 8). The most sensitive basins are mixed-rain-and-snow basins. These basins
    experience dramatic losses of snowpack and substantial changes in seasonal flow
    timing (Fig. 5; Yakima River at Parker in Fig. 8). Rain-dominant basins in the
    United States (e.g., Chehalis River at Grand Mound in Fig. 8) experience little
    change in the shape of the monthly hydrograph because there is only occasional
    low-elevation snow in mid-winter in the twentieth century base case; therefore,
    there is relatively little sensitivity of monthly runoff timing to warming. Fig.
    8 Monthly mean hydrographs not adjusted for bias (water year: October–September)
    for four representative river sites in the PNW: Kootenay River at Corra Linn Dam
    (upper left), Columbia River at The Dalles, Oregon (upper right), Yakima River
    at Parker (lower left), and the Chehalis River at Grand Mound (lower right). Blue
    lines show the average historical values (1916–2006) (repeated in each panel).
    Pink bands show the range of nine or ten HD climate change scenarios for B1 and
    A1B emissions scenarios for three future time periods. Dark red lines show the
    average of the climate change ensemble. Display full size Changes in PET (PET3,
    see Table 2) and AET (see Table 2) are shown in Fig. 9. The PET is shown to increase
    dramatically over most of the domain (primarily because of warming in the scenarios),
    whereas AET shows widespread declines east of the Cascade Range. However, some
    areas west of the Cascade Range and in the northern Rockies show increasing AET.
    These spatial variations in the change in AET are broadly reflective of the dominant
    drivers of AET in each case. In relatively wet and cool areas along the coast
    and at high elevation in the northern parts of the basin, summer AET is energy
    limited; therefore, rising temperatures result in increases in AET. In much of
    the CRB, however, summer AET is water limited (i.e., there is abundant surface
    energy to evaporate whatever water is available), and changes in AET are dominated
    by decreasing summer precipitation in the scenarios, which effectively decreases
    summer AET in most cases. Fig. 9 Left panel: Historical estimates of summer (JJA)
    potential evapotranspiration (PET) (based on PET3, see Table 2) (upper right)
    compared with percentage changes in PET for two emissions scenarios and three
    future time periods from the CD scenarios. Right panel: Historical estimates of
    summer AET (upper right) compared with percentage changes for the same CD scenarios.
    Display full size The magnitude of hydrologic extremes such as Q100 and 7Q10 are
    expected to shift markedly in some basins in response to cool season warming,
    increasing cool season precipitation, and warmer, drier summers. Figure 10 shows
    a map of Q100 ratios (future Q100 to historical Q100) for 297 river locations
    and a scatterplot of the Q100 ratio as a function of winter temperature regime
    in each basin. Rain dominant basins (DJF temperatures greater than 2°C) show moderate
    increases in flood risk (primarily reflecting increasing storm intensity in the
    simulations), whereas snowmelt-dominant basins that currently flood in June show
    relatively little change in flood risk. The largest changes in flood risk are
    simulated in mixed-rain-and-snow basins. Flooding in these basins is sensitive
    to both warming (which raises snow lines and effectively enlarges the contributing
    basin area during most flood events) and increasing winter precipitation. Because
    both these effects increase flood risk in the simulations, the effects are unusually
    large in these basins (Fig. 6). Increasing low flow risks (declining 7Q10 values)
    are widespread across the domain as a result of the combined effects of declining
    snowpack (which tends to result in earlier streamflow recession and lower flows
    in late summer, see Fig. 8) and warmer and drier summers (which increase PET)
    (Fig. 11). The largest reductions in low flows occur west of the Cascade Range
    in the simulations. This is likely because soil moisture is higher in summer west
    of the Cascade Range and evapotranspiration is mostly energy limited, whereas
    east of the mountains the late summer soil moisture is already very low in the
    current climate and increasing evapotranspiration does not result in much additional
    soil moisture stress. In other words, dry areas east of the Cascade Range have
    less base-flow potential to lose with increasing evapotranspiration and loss of
    summer precipitation because the soil moisture is already at very low levels in
    late summer. Tague, Grant, Farrell, Choate and Jefferson ( 2008) showed analogous
    differences between watersheds in the PNW based on the relative contribution of
    groundwater to base flows. We should note that glaciers and “deep” groundwater
    (e.g., contributions to streamflow from large confined aquifers) are not simulated
    by the VIC model, and impacts in areas profoundly influenced by these hydrologic
    features may not be well characterized in the simulations (Wenger et al., 2010).
    Broad changes in “shallow” groundwater (e.g., localized contributions to streamflow
    from smaller unconfined aquifers), however, are likely well captured by the VIC
    model based on a strong correlation between VIC-simulated base flows and observations
    in many basins examined by Wenger et al. ( 2010). Fig. 10 Left panel: Changes
    in Q100 for 297 streamflow locations expressed as a ratio of Q100 for the future
    period to Q100 for the historical period based on the average of the nine or ten
    HD scenarios for the B1 and A1B emissions scenarios for three future time periods.
    Right panel: Same data shown as a scatter plot of the average ratio of Q100 for
    the 2040s A1B scenarios to Q100 for the historical period versus historical basin-average
    mid-winter (DJF) temperature in each case. Typical month of historical flooding
    events is shown by the colour of the dots in the scatter plot (legend inset in
    the upper right corner), by permission of I. Tohver, A.F. Hamlet, and S.-Y. Lee.
    Display full size Fig. 11 Changes in 7Q10 for 297 river locations expressed as
    a ratio of 7Q10 for the future period to 7Q10 for the historical period based
    on the average of the nine or ten HD scenarios for the B1 and A1B emissions scenarios
    for three future time periods, by permission of I. Tohver, A.F. Hamlet, and S.-Y.
    Lee. Display full size Other impacts, such as changes in soil moisture dynamics
    are also apparent in the simulations. In most basins with substantial snowpack,
    elevated soil moisture in winter accompanies warming in the simulations resulting
    from more infiltration from rain in the fall and winter months (Fig. 12). These
    results support the hypothesis that widespread increases in winter landslide risks
    and sediment transport in rivers are likely to accompany increased winter precipitation
    and loss of interannual snowpack in mountain watersheds. Fig. 12 Changes in monthly
    mean total column soil moisture (October–September) for three representative river
    sites in the PNW: Kootenay River at Corra Linn Dam (left), Columbia River at The
    Dalles, Oregon (centre), and Yakima River at Parker (right). Blue lines show average
    historical values (1916–2006) (repeated in each panel). Pink bands show the range
    of nine or ten HD climate change scenarios for B1 and A1B emissions scenarios
    for three future time periods. The dark red lines show the average of the climate
    change ensemble. Display full size Differences in the impacts in the US and Canadian
    portions of the basin are striking, confirming results reported in two previous
    studies (Hamlet, 2003; Hamlet & Lettenmaier, 1999b). Reductions in spring snowpack
    and summer streamflow, for example, are relatively modest in the Canadian portions
    of the basin because of cold winter temperatures that delay warming-related impacts
    to seasonal snowpack (Elsner et al., 2010). Corresponding shifts in the seasonal
    timing of streamflow are also relatively small in the Canadian CRB until late
    in the twenty-first century. Based on these substantial differences in hydrologic
    impacts it is difficult to escape the conclusion that management of water resources
    in the Canadian portions of the basin will play a crucial role in the ability
    of US water managers to adapt to more substantial changes in streamflow timing
    and summer low flows in the United States. In particular, it is clear that Canada
    will have not only about 50% of the reservoir storage in the CRB (Hamlet, 2003)
    but also an increasingly dominant portion of the natural water storage as snowpack
    in the future. The presence of glaciers in Canada (not included in the CBCCSP
    simulations) may further exacerbate the discrepancies between impacts to summer
    flows in the United States and Canada in late summer (Werner et al., 2013). 6
    Use of products and information by stakeholders, water professionals, and researchers
    The CBCCSP was designed from the outset to support users with a very wide range
    of technical sophistication and capacity. For example, to support academic or
    agency researchers with their own hydrologic modelling capability, the study provides
    projections of meteorological drivers such as temperature, precipitation, solar
    radiation, and humidity and a calibrated VIC hydrologic model implementation.
    Using these resources, other modelling groups can carry out their own investigations
    of hydrologic impacts using either their own hydrologic model (just using the
    driving data) or the VIC implementation from the CBCCSP. At the other extreme,
    for those without any hydrologic modelling or post-processing capability the study
    provides a wide range of hydrologic products that can be used without any expertise
    in the preceding steps. Intermediate products are available as well, which can
    be used by people with GIS capabilities, but with little or no knowledge of climate
    projections and hydrologic modelling. Thus, depending on their needs and level
    of technical sophistication, stakeholders can make the best use of the study products
    by extracting information at different points in the data processing sequence,
    all of which are available on the study web site. Similarly, three different downscaling
    approaches were used in the study, each with its own advantages and limitations
    in the context of different natural resources management applications (Hamlet
    et al., 2010a). Thus, stakeholders can select different products, using different
    downscaling approaches that are appropriate to their needs. As noted above, the
    study also supports planning efforts over a wide range of geographic scales. A
    number of high-visibility studies have made use of the CBCCSP database to date,
    a few of which are summarized below. a Columbia River Management Joint Operating
    Committee (RMJOC) Studies The primary activities and objectives of the RMJOC studies
    are described in the Executive Summary of the project report (US Department of
    the Interior, 2012): The Bonneville Power Administration (BPA), U.S. Army Corps
    of Engineers (USACE), and U.S. Bureau of Reclamation (Reclamation) collaborated
    to adopt climate change and hydrology datasets for their longer-term planning
    activities in the Columbia–Snake River Basin (CSRB). This was coordinated through
    the River Management Joint Operating Committee (RMJOC), a sub-committee of the
    Joint Operating Committee which was established through direct funding Memorandum
    of Agreements between BPA, Reclamation, and the USACE. The RMJOC is specifically
    dedicated to reviewing the practices, procedures, and processes of each agency
    to identify changes that could improve the overall efficiency of the operation
    and management of the Federal Columbia River Power System projects. In addition
    to creating these datasets, the RMJOC agencies worked together to adopt a set
    of methods for incorporating these data into those longer-term planning activities.
    One important application of these efforts relates to the potential renegotiation
    of the Treaty Relating to Cooperative Development of the Water Resources of the
    Columbia River Basin [hereinafter CRT] ( 1961–1964) between the United States
    and Canada. The CRT was ratified by the United States in 1961 and subsequently
    ratified by Canada and enacted into law in 1964. According to the terms of the
    treaty, the CRT can be cancelled by either party with ten year''s notice starting
    in 2024 (60 years after the CRT''s inception). Thus, 2014 would be the first year
    when either party could initiate an end to the treaty by giving the stipulated
    ten-year notice of that intent. To support US decision-making in this arena, the
    RMJOC for the CRB charged the BPA, the USBR, and the USACE to carry out a series
    of planning studies for the CRB to help support the consideration of various alternatives
    associated with the CRT. The consideration of alternatives encompasses complex
    tradeoffs between hydropower production (and hydropower revenues), flood control,
    water supply, navigation, and environmental services, all of which are likely
    to be substantially affected by climate change (Hamlet, 2011). The CBCCSP provided
    bias-adjusted streamflow data to support several reservoir operations models and
    temperature data to support electrical load forecasting applications. To date,
    reservoir operation studies by the BPA and USBR have been produced for the Columbia
    main stem and Snake River basin, respectively, and a series of reports describing
    the study methods and current results have been released to the general public
    (US Department of the Interior, 2013). The RMJOC projections have also fed into
    formal planning exercises such as the WaterSMART Yakima River Basin Study (US
    Department of the Interior, 2012). Although a number of pilot climate change studies
    have been carried out in the CRB in collaboration with various water management
    agencies in the past (e.g., NWPCC, 2005), the RMJOC study was something of a landmark
    in that it was the first time that the BPA, USBR, and USACE used climate change
    information in coordinated interagency planning exercises in the CRB. The study
    was also unique in that this was the first time that these agencies had run their
    own reservoir operation models to assess climate change impacts in the CRB, an
    element of the study design which greatly increased the impact of the study conclusions
    in the agencies involved. b Washington State University (WSU) Crop Water Demand
    and Water Supply Studies under HB2860 As mentioned in the introduction, WA HB2860,
    which provided the bulk of the funding for the CBCCSP, also charged WDOE with
    identifying where US$200 million earmarked for water resources infrastructure
    improvements should be spent. Funding was received by WSU to carry out research
    quantifying crop water demand, water resources system performance, and economic
    impacts under current climate conditions and a range of future climate scenarios.
    The CBCCSP provided climate change projections of meteorological drivers and a
    calibrated VIC implementation in support of the study. The calibrated CBCCSP VIC
    model was modified by WSU by integrating it with a sophisticated crop model (CropSyst;
    Stöckle, Donatelli, & Nelson, 2003) that, among other functions, estimates crop
    water demand. The study also used the ColSim reservoir operations model (Hamlet
    & Lettenmaier, 1999b) to estimate water deliveries to the Columbia Basin Project
    (the primary irrigation project supplied by water from Grand Coulee Dam) under
    future climate scenarios using streamflows generated by the integrated VIC/CropSyst
    model. A set of simpler, lumped-storage reservoir operations models was used to
    quantify impacts in a number of smaller water supply systems. These hydrologic
    studies support detailed assessment of the economic impacts of climate change
    on irrigation and important crops in WA (Yorgey et al., 2011), which in turn will
    inform decisions regarding best use of funding to improve water supply benefits
    in WA under climate change. This study represents one of the first attempts to
    dynamically couple a sophisticated, physically based hydrologic model with a detailed
    crop model to estimate the integrated impacts on water supply and crop viability
    at a range of spatial scales. c Washington State (WA) Integrated Climate Change
    Response Strategy Following the WACCIA in 2009, the WA Legislature, via the Act
    relating to State Agency Climate Leadership (2009), charged WDOE and other state
    agencies with preparing a first climate change adaptation plan for WA. Impacts
    assessments from the WACCIA played a central role in these planning activities,
    but updated and extended data from the CBCCSP also materially supported these
    efforts. The CBCCSP, in particular, provided access to additional scenarios and
    downscaling methods that provided a range of hydrologic outcomes associated with
    uncertainty in the climate projections, which the WACCIA assessments largely did
    not. The CBCCSP also provided a more thorough assessment of hydrologic extremes
    via the HD scenarios, providing ranges of these values that were more geographically
    specific as opposed to an estimate of the central tendency from the CD approaches
    used in the WACCIA. Although results from the WACCIA would arguably have been
    adequate to support WDOE''s adaptation planning, the CBCCSP provided additional
    foundation support for these efforts, and helped improve confidence in the outcomes
    of the adaptation strategies identified by better quantifying a range of outcomes.
    d West-Wide Extensions to Support USFS and USFWS Needs Over the last several years,
    the USFS and USFWS have engaged with the CIG to produce a set of initial climate
    change hydrologic scenarios over much of the west using a common methodology intended
    to support landscape-scale assessment of climate change impacts (Littell et al.,
    2011). Using the VIC model, the study has projected impacts for the Great Busin
    and the Columbia River, Missouri River, and Colorado River basins, and assessment
    of impacts in California is underway at the time of writing. Although this project
    consists of an abbreviated set of scenarios and products using three scenarios
    (high, medium, and low impact) for one downscaling method, the CBCCSP played an
    important role in providing an established set of methods for developing historical
    driving datasets and implementing and running the hydrologic models. The CBCCSP
    also provided a complete and well-tested data processing sequence for post-processing
    and summarizing the hydrologic data to provide figures and analysis efficiently.
    The CBCCSP had a budget of about US$500 thousand (in 2010 dollars) over two years.
    West-wide studies to support USFS and USFWS needs (including current efforts to
    include California) will have a budget of less than half this amount, a level
    of efficiency that would not have been achievable without the CBCCSP pilot effort.
    The USFS and USFWS studies have supported a number of high-visibility ecosystem
    studies, including assessment of the impacts of changing snowpack on wolverine
    populations (McKelvey et al., 2011) and subsequent proposed ESA listing of wolverine
    populations, and comprehensive assessment of climate change impacts to trout species
    over the west (Wenger et al., 2011). e Studies in Support of the Department of
    the Interior (DOI) Landscape Conservation Cooperatives and Regional Climate Science
    Centers The DOI via the USFWS has recently established a set of LCCs across the
    United States (USFWS, 2013) and has generated additional funding to support a
    group of regional CSCs, one of which was recently established in the PNW (PNWCSC),
    combining the efforts of about 15 PNW research universities, jointly led by the
    USGS, Oregon State University, the UW, and the University of Idaho. Data from
    the CBCCSP are currently supporting two CIG studies funded by the LCCs and the
    CSC, including a study of impacts to wetlands in the PNW (funded by the North
    Pacific LCC and the PNWCSC) and a study assessing climatic and hydrologic extremes
    and their effects on ecosystems over the western United States (funded by the
    PNWCSC). The CBCCSP (and extensions over the west) has greatly reduced the costs
    of these types of studies by supplying (at essentially no cost) a wide range of
    high-quality hydrologic scenarios as a foundation for further work. f Seattle
    City Light Case Study A detailed climate change assessment report was prepared
    by the CIG for Seattle City Light (Snover et al., 2010) based primarily on the
    CBCCSP database. Additional streamflow sites were routed from the primary VIC
    data, and water temperature simulations for a number of additional sites were
    based on temperature projections from the study. This is a good example of the
    use of the study data to support relatively fine-scale planning needs. 7 Future
    Work A key design element of the CBCCSP from the outset of the project was to
    produce a well-organized and well-documented “end-to-end” (i.e., GCM to hydrologic
    products) data processing sequence and a web-accessible data archive that would
    greatly reduce the cost of producing updates in response to each subsequent CMIP/IPCC
    cycle. Future work on the project will likely focus on expanding the number of
    streamflow sites for which products are available (e.g., inclusion of additional
    sites in coastal Oregon in the site-specific products) and providing a comprehensive
    suite of products associated with CMIP5 results (Taylor, Stouffer, & Meehl, 2012)
    associated with IPCC AR5. 8 Summary and conclusions The CBCCSP was founded on
    the basis of regional partnerships to support a shared need for climate change
    scenarios and directly encompassed the transboundary nature of the CRB''s management
    framework by including both US and Canadian interests in an integrated and comprehensive
    data resource. The study is intended to support research, vulnerability assessment,
    long-term planning, and climate change adaptation by the natural resources management
    community at a range of spatial scales, by users with varying levels of technical
    expertise. The study employs a state-of-the-art, end-to-end data processing sequence
    that moves from raw GCM output to a set of final hydrological products that can
    be accessed by the user community from a web-accessible database. Key products
    from the study include detailed summary data for about 300 river sites in the
    PNW and monthly GIS products for 21 hydrologic variables over the entire study
    domain. Additional products such as bias-adjusted inflow sequences for specific
    reservoir operations models are also included. Depending on their needs and/or
    level of technical sophistication, users can tap into the study databases at a
    number of different levels. For example, researchers who wish to run their own
    hydrologic models can do so by downloading the statistically downscaled meteorological
    forcings from the study. Those who lack their own hydrologic model, but wish to
    make additional runs themselves, can obtain the calibrated VIC model implementation.
    Those who wish to do additional post-processing of the existing VIC data can access
    the archived model output. Full knowledge of the preceding steps is not required
    to use the products obtained at any level of the study, which increases the utility
    of the products. Results from the study show profound changes in spring snowpack
    and fundamental shifts from snow to rain-dominant behaviour across most of the
    domain. Associated shifts in streamflow timing from spring and summer to winter
    are also evident in basins that currently have significant snow accumulation in
    winter, whereas rain-dominant basins show minimal shifts in streamflow timing.
    The PET increases over most of the PNW in summer as a result of rising temperatures;
    however, AET is reduced in all but a few areas of the domain because AET is water
    limited and summer precipitation decreases in the simulations. Both floods (Q100)
    and extreme 7-day low flows (7Q10) increase in intensity for most of the river
    sites simulated. The largest increases in flooding are in mixed-rain-and-snow
    basins whose mid-winter temperatures are presently within a few degrees of freezing.
    Simulated widespread increases in soil moisture recharge in fall and winter in
    areas with significant snow accumulation in winter (for the current climate) support
    hypotheses of increased landslide risk and sediment transport in winter in the
    future. The CBCCSP database has been a valuable resource which has dramatically
    reduced the cost of a number of high-visibility planning studies in the PNW, including
    the RMJOC water resources planning studies conducted by the BPA, USBR, and USACE,
    WSU integrated crop modelling and irrigation water demand studies under HB2860,
    the WA Integrated Climate Change Response Strategy, and west-wide extensions of
    the CBCCSP supported by the USFS and USFWS. Acknowledgements Primary support for
    the project was provided by WDOE, with additional major support provided by the
    BPA, NWPCC, BCME, OWRD, and CTED via the 2009 WACCIA (http://cses.washington.edu/cig/res/ia/waccia.shtml)
    (Miles et al., 2010). Special thanks to Kurt Unger and Ken Slattery, who were
    the primary architects of the CBCCSP at WDOE. Thanks also to Nancy Stephan (BPA),
    John Fazio and Jim Ruff (NWPCC), Allan Chapman and Ben Kagasniemi (BCME), and
    Barry Norris (OWRD) for their contributions to the initial study design. Dave
    Rodenhuis, Markus Schnorbus, Arelia Werner, and Katrina Bennett at PCIC at the
    University of Victoria, British Columbia, also provided in-kind support and funding
    for collaborative research which contributed materially to this project. A number
    of regional partners provided recommendations for streamflow locations and other
    in-kind support including Jesse Abner (MDNR), Steve Running (University of Montana),
    Hal Anderson (IDWR) Levi Brekke, Pat McGrane and John Roache (USBR), Carolyn Fitzgerald
    (Seattle District USACE), Seshu Vaddey and Randy Wortman (Portland District USACE).
    Thanks to Shrad Shukla, at the UW for 1/16 degree VIC model calibration over the
    Yakima basin. The authors would like to acknowledge the contributions of other
    members of the CBCCSP research team at the UW including Lara Whitely Binder, Pablo
    Carrasco, Jeffrey Deems, Carrie Lee, Dennis P. Lettenmaier, Tyler Kamstra, Jeremy
    Littell, Nathan Mantua, Edward Miles, Kristian Mickelson, Philip W. Mote, Erin
    Rogers, Eric Salathé, Amy Snover, and Andrew Wood. Thanks to Sean Fleming (Environment
    Canada) for spearheading this contribution to Atmosphere-Ocean. The authors would
    like to acknowledge the contributions of two anonymous reviewers and the lead
    and associate editors for Atmosphere-Ocean, whose constructive suggestions substantially
    improved the paper during the review process. Special thanks also to Dennis P.
    Lettenmaier, head of the Land Surface Hydrology Group at the UW, for providing
    access to computer resources and system administration support for the CBCCSP.
    References Act relating to water resource management in the Columbia river basin,
    H.R. 2860, 59th Legislature (WA 2006).  Google Scholar Act relating to state agency
    climate leadership, S. 5560, 61st Legislature (WA 2009).  Google Scholar Adam
    , J. C. , Hamlet , A. F. and Lettenmaier , D. P. 2009 . Implications of global
    climate change for snowmelt hydrology in the 21st century . Hydrological Processes
    , 23 ( 7 ) : 962 – 972 . (doi:10.1002/hyp.7201)  Web of Science ®Google Scholar
    Andreadis , K. , Storck , P. and Lettenmaier , D. P. 2009 . Modeling snow accumulation
    and ablation processes in forested environments . Water Resources Research , 45
    : W05429 doi:10.1029/2008WR007042 (doi:10.1029/2008WR007042)  Google Scholar Battisti
    , D. S. and Sarachik , E. 1995 . Understanding and predicting ENSO . Reviews of
    Geophysics , 33 : 1367 – 1376 . (doi:10.1029/95RG00933)  Web of Science ®Google
    Scholar Bennett , K. E. , Werner , A. T. and Schnorbus , M. 2012 . Uncertainties
    in hydrologic and climate change impact analyses in headwater basins of British
    Columbia . Journal of Climate , 25 : 5711 – 5730 . doi:http://dx.doi.org/10.1175/JCLI-D-11-00417.1
    (doi:10.1175/JCLI-D-11-00417.1)  Web of Science ®Google Scholar Casola, J. H.,
    Kay, J. E., Snover, A. K., Norheim, R. A., Binder, L. C. W., & the Climate Impacts
    Group. (2005). Climate impacts on Washington''s hydropower, water supply, forests,
    fish, and agriculture. A report prepared for King County (Washington). Climate
    Impacts Group (Center for Science in the Earth System, Joint Institute for the
    Study of the Atmosphere and Ocean, University of Washington, Seattle).  Google
    Scholar Cherkauer , K. A. and Lettenmaier , D. P. 2003 . Simulation of spatial
    variability in snow and frozen soil . Journal of Geophysical Research , 108 (
    D22 ) : 8858 doi:10.1029/2003JD003575 (doi:10.1029/2003JD003575)  Google Scholar
    Christensen , N. and Lettenmaier , D. P. 2007 . A multi-model ensemble approach
    to assessment of climate change impacts on the hydrology and water resources of
    the Colorado River basin . Hydrology and Earth System Sciences , 11 : 1417 – 1434
    . (doi:10.5194/hess-11-1417-2007)  Web of Science ®Google Scholar CIG (Climate
    Impacts Group). 2013a. Site specific data [Data]. Retrieved from http://warm.atmos.washington.edu/2860/products/sites/  Google
    Scholar CIG (Climate Impacts Group). 2013b. Hydrologic climate change scenarios
    for the Pacific Northwest Columbia River basin and coastal drainages. Retrieved
    from http://warm.atmos.washington.edu/2860/  Google Scholar Cohen , S. J. , Miller
    , K. , Hamlet , A. and Avis , W. 2000 . Climate change and resource management
    in the Columbia River Basin . Water International , 25 ( 2 ) : 253 – 272 . (doi:10.1080/02508060008686827)  Web
    of Science ®Google Scholar Cooper, R. M. (2002). Determining surface water availability
    in Oregon. Open File Report SW 02-002. Oregon Water Resources Department. Salem,
    Oregon.  Google Scholar Crook, A. G. (1993). 1990 level modified streamflow 1928–1989.
    Contract # DE-AC79-92BP21985. Report of A.G. Crook Company to Bonneville Power
    Administration, Portland, OR.  Google Scholar Daly , C. , Gibson , W. P. , Taylor
    , G. , Johnson , G. L. and Pasteris , P. 2002 . A knowledge-based approach to
    the statistical mapping of climate . Climate Research , 22 : 99 – 113 . (doi:10.3354/cr022099)  Web
    of Science ®Google Scholar Daly , C. , Neilson , R. P. and Phillips , D. L. 1994
    . A statistical-topographic model for mapping climatological precipitation over
    mountainous terrain . Journal of Applied Meteorology , 33 : 140 – 158 . (doi:10.1175/1520-0450(1994)033<0140:ASTMFM>2.0.CO;2)  Google
    Scholar DiLuzio , M. , Johnson , G. L. , Daly , C. , Eischeid , J. K. and Arnold
    , J. G. 2008 . Constructing retrospective gridded daily precipitation and temperature
    datasets for the conterminous United States . Journal of Applied Meteorology and
    Climatology , 47 : 475 – 497 . (doi:10.1175/2007JAMC1356.1)  Web of Science ®Google
    Scholar Elsner , M. M. , Cuo , L. , Voisin , N. , Deems , J. S. , Hamlet , A.
    F. , Vano , J. A. , Mickelson , K. E. B. , Lee , S. Y. and Lettenmaier , D. P.
    2010 . Implications of 21st century climate change for the hydrology of Washington
    State . Climatic Change , 102 ( 1–2 ) : 225 – 260 . doi:10.1007/s10584-010-9855
    (doi:10.1007/s10584-010-9855-0)  Web of Science ®Google Scholar Flightner, G.
    (2008). Natural streamflows: Columbia River tributaries in Washington, 1928–2006.
    Contract # C0700334, 28 March 2008. Report of Gary Flightner Engineering to Washington
    State Dept. of Ecology, Olympia WA.  Google Scholar Gershunov , A. and Barnett
    , T. P. 1998 . Interdecadal modulation of ENSO teleconnections . Bulletin of the
    American Meteorological Society , 79 ( 12 ) : 2715 – 2725 . (doi:10.1175/1520-0477(1998)079<2715:IMOET>2.0.CO;2)  Web
    of Science ®Google Scholar Gleckler , P. J. , Taylor , K. E. and Doutriaux , C.
    2008 . Performance metrics for climate models . Journal of Geophysical Research-Atmospheres
    , 113 ( D6 ) D06104. doi:10.1029/2007JD008972  PubMed Web of Science ®Google Scholar
    Hamlet , A. F. 2003 . “ The role of transboundary agreements in the Columbia River
    basin: An integrated assessment in the context of historic development, climate,
    and evolving water policy ” . In Climate and water: Transboundary challenges in
    the Americas , Edited by: Diaz , H. and Morehouse , B. 263 – 289 . Dordrecht/Boston/London
    : Kluwer Press .  Google Scholar Hamlet , A. F. 2011 . Assessing water resources
    adaptive capacity to climate change impacts in the Pacific Northwest region of
    North America . Hydrology and Earth System Sciences , 15 : 1427 – 1443 . doi:10.5194/hess-15-1427-2011
    (doi:10.5194/hess-15-1427-2011)  Web of Science ®Google Scholar Hamlet, A. F.,
    Carrasco, P., Deems, J., Elsner, M. M., Kamstra, T., Lee, C., Lee, S-Y, Mauger,
    G., Salathe, E. P., Tohver, I., & Binder, L. W. (2010a). Final project report
    for the Columbia Basin Climate change scenarios project. Retrieved from http://warm.atmos.washington.edu/2860/report/  Google
    Scholar Hamlet , A. F. , Huppert , D. and Lettenmaier , D. P. 2002 . Economic
    value of long-lead streamflow forecasts for Columbia River hydropower . ASCE Journal
    of Water Resources Planning and Management , 128 ( 2 ) : 91 – 101 . (doi:10.1061/(ASCE)0733-9496(2002)128:2(91))  Web
    of Science ®Google Scholar Hamlet , A. F. , Lee , S. Y. , Mickelson , K. E. B.
    and Elsner , M. M. 2010b . Effects of projected climate change on energy supply
    and demand in the Pacific Northwest and Washington State . Climatic Change , 102:
    doi:10.1007/s10584-010-9857-y  Web of Science ®Google Scholar Hamlet , A. F. and
    Lettenmaier , D. P. 1999a . Columbia River streamflow forecasting based on ENSO
    and PDO climate signals . ASCE Journal of Water Resources Planning and Management
    , 125 ( 6 ) : 333 – 341 . (doi:10.1061/(ASCE)0733-9496(1999)125:6(333))  Web of
    Science ®Google Scholar Hamlet , A. F. and Lettenmaier , D. P. 1999b . Effects
    of climate change on hydrology and water resources in the Columbia River basin
    . Journal of the American Water Resources Association , 35 ( 6 ) : 1597 – 1623
    . (doi:10.1111/j.1752-1688.1999.tb04240.x)  Web of Science ®Google Scholar Hamlet
    , A. F. and Lettenmaier , D. P. 2000 . Long-range climate forecasting and its
    use for water management in the Pacific Northwest Region of North America . Journal
    of Hydroinformatics , 2 ( 3 ) : 163 – 182 .  Google Scholar Hamlet , A. F. and
    Lettenmaier , D. P. 2005 . Production of temporally consistent gridded precipitation
    and temperature fields for the continental U.S . Journal of Hydrometeorology ,
    6 ( 3 ) : 330 – 336 . (doi:10.1175/JHM420.1)  Web of Science ®Google Scholar Hamlet
    , A. F. and Lettenmaier , D. P. 2007 . Effects of 20th century warming and climate
    variability on flood risk in the western U.S . Water Resources Research , 43 :
    W06427 doi:10.1029/2006WR005099 (doi:10.1029/2006WR005099)  Web of Science ®Google
    Scholar Hamlet , A. F. , Mote , P. W. , Clark , M. P. and Lettenmaier , D. P.
    2005 . Effects of temperature and precipitation variability on snowpack trends
    in the western U.S . Journal of Climate , 18 ( 21 ) : 4545 – 4561 . (doi:10.1175/JCLI3538.1)  Web
    of Science ®Google Scholar Kalnay , E. , Kanamitsu , M. , Kistler , R. , Collins
    , W. , Deaven , D. , Gandin , L. , Iredell , M. , Saha , S. , White , G. , Woollen
    , J. , Zhu , Y. , Leetmaa , A. , Reynolds , R. , Chelliah , M. , Ebisuzaki , W.
    , Higgins , W. , Janowiak , J. , Mo , K. C. , Ropelewski , C. , Wang , J. , Jenne
    , R. and Joseph , D. 1996 . The NCEP/NCAR 40-year reanalysis project . Bulletin
    of the American Meteorological Society , 77 : 437 – 470 . (doi:10.1175/1520-0477(1996)077<0437:TNYRP>2.0.CO;2)  Web
    of Science ®Google Scholar Kimball , J. S. , Running , S. W. and Nemani , R. 1997
    . An improved method for estimating surface humidity from daily minimum temperature
    . Agricultural and Forest Meteorology , 85 : 87 – 98 . (doi:10.1016/S0168-1923(96)02366-0)  Web
    of Science ®Google Scholar Labadie, J. W. (2007). MODSIM-DSS: River basin management
    Decision Support System. Retrieved from http://modsim.engr.colostate.edu/  Google
    Scholar Lee , S.-Y. , Hamlet , A. F. , Fitzgerald , C. J. and Burges , S. J. 2009
    . Optimized flood control in the Columbia River basin for a global warming scenario
    . ASCE Journal of Water Resources Planning and Management , 135 ( 6 ) : 440 –
    450 . doi:10.1061/(ASCE)0733-9496 (doi:10.1061/(ASCE)0733-9496(2009)135:6(440))  Web
    of Science ®Google Scholar Lee , S.-Y. , Hamlet , A. F. , Fitzgerald , C. J. and
    Burges , S. J. 2011 . Methodology for developing flood rule curves conditioned
    on El Niño-Southern Oscillation classification . Journal of the American Water
    Resources Association , 47 ( 1 ) : 81 – 92 . doi:10.1111/j.1752-1688.2010. 00490.x
    (doi:10.1111/j.1752-1688.2010.00490.x)  Web of Science ®Google Scholar Lee , S.-Y.
    , Fitzgerald , C. J. , Hamlet , A. F. and Burges , S. J. 2011 . Daily time step
    refinement of optimized flood control rule curves for a global warming scenario
    . ASCE Journal of Water Resources Planning and Management , 137 ( 4 ) : 309 –
    317 . doi:10.1061/(ASCE)WR.1943-5452.0000125 (doi:10.1061/(ASCE)WR.1943-5452.0000125)  Web
    of Science ®Google Scholar Lettenmaier , D. P. and Hamlet , A. F. 2003 . “ Improving
    water resources system performance through long-range climate forecasts: The Pacific
    Northwest experience (Chapter 7) ” . In Water and climate in the western United
    States , Edited by: Lewis , W. M. Jr. 107 – 122 . Boulder : University Press of
    Colorado .  Google Scholar Lettenmaier , D. P. , Wood , A. W. , Palmer , R. N.
    , Wood , E. F. and Stakhiv , E. Z. 1999 . Water resources implications of global
    warming, a U.S. regional perspective . Climatic Change , 43 ( 3 ) : 537 – 579
    . (doi:10.1023/A:1005448007910)  Web of Science ®Google Scholar Leung , L. R.
    , Hamlet , A. F. , Lettenmaier , D. P. and Kumar , A. 1999 . Simulations of the
    ENSO hydroclimate signals in the Pacific Northwest Columbia River basin . Bulletin
    of the American Meteorological Society , 80 ( 11 ) : 2313 – 2329 . (doi:10.1175/1520-0477(1999)080<2313:SOTEHS>2.0.CO;2)  Web
    of Science ®Google Scholar Liang , X. , Lettenmaier , D. P. , Wood , E. F. and
    Burges , S. J. 1994 . A simple hydrologically based model of land surface water
    and energy fluxes for general circulation models . Journal of Geophysical Research
    , 99 ( D7 ) : 14415 – 14428 . (doi:10.1029/94JD00483)  Web of Science ®Google
    Scholar Littell, J. S., Elsner, M. M., Mauger, G. S., Lutz, E. R., Hamlet, A.
    F., & Salathé, E. P. (2011). Regional climate and hydrologic change in the northern
    U.S. Rockies and Pacific Northwest: Internally consistent projections of future
    climate for resource management. Preliminary project report, USFS JVA 09-JV-11015600-039.
    Prepared by the Climate Impacts Group, University of Washington, Seattle. Retrieved
    from http://cses.washington.edu/picea/USFS/pub/Littell_etal_2010/Littell_etal._2011_Regional_Climatic_And_Hydrologic_Change_USFS_USFWS_JVA_17Apr11.pdf  Google
    Scholar Littell , J. S. , Oneil , E. E. , McKenzie , D. , Hicke , J. A. , Lutz
    , J. A. , Norheim , R. A. and Elsner , M. M. 2010 . Forest ecosystems, disturbance,
    and climatic change in Washington State, USA . Climatic Change , 102 ( 1–2 ) :
    129 – 158 . doi:10.1007/s10584-010-9858-x (doi:10.1007/s10584-010-9858-x)  Web
    of Science ®Google Scholar Lohmann , D. , Raschke , E. , Nijssen , B. and Lettenmaier
    , D. P. 1998 . Regional scale hydrology: I. Formulation of the VIC-2L model coupled
    to a routing model . Hydrological Sciences Journal , 43 : 131 – 141 . (doi:10.1080/02626669809492107)  Web
    of Science ®Google Scholar Mantua , N. , Hare , S. , Zhang , Y. , Wallace , J.
    M. and Francis , R. 1997 . A Pacific interdecadal climate oscillation with impacts
    on salmon production . Bulletin of the American Meteorological Society , 78 :
    1069 – 1079 . (doi:10.1175/1520-0477(1997)078<1069:APICOW>2.0.CO;2)  Web of Science
    ®Google Scholar Mantua , N. , Tohver , I. and Hamlet , A. F. 2010 . Climate change
    impacts on streamflow extremes and summertime stream temperature and their possible
    consequences for freshwater salmon habitat in Washington State . Climatic Change
    , 102 ( 1–2 ) doi:10.1007/s10584-010-9845-2 (doi:10.1007/s10584-010-9845-2)  Web
    of Science ®Google Scholar Maurer , E. P. 2007 . Uncertainty in hydrologic impacts
    of climate change in the Sierra Nevada, California under two emissions scenarios
    . Climatic Change , 82 ( 3–4 ) : 309 – 325 . (doi:10.1007/s10584-006-9180-9)  Web
    of Science ®Google Scholar Maurer , E. P. and Duffy , P. B. 2005 . Uncertainty
    in projections of streamflow changes due to climate change in California . Geophysical
    Research Letters , 32 ( 3 ) : L03704 doi:10.1029/2004GL021462 (doi:10.1029/2004GL021462)  Google
    Scholar McKelvey , K. S. , Copeland , J. P. , Schwartz , M. K. , Littell , J.
    S. , Aubry , K. B. , Squires , J. R. , Parks , S. A. , Elsner , M. M. and Mauger
    , G. S. 2011 . Climate change predicted to shift wolverine distributions, connectivity,
    and dispersal corridors . Ecological Applications , 21 ( 8 ) : 2882 – 2897 . doi:10.1890/10-2206.1
    (doi:10.1890/10-2206.1)  Web of Science ®Google Scholar Meehl , G. A. , Covey
    , C. , Delworth , T. , Latif , M. , McAvaney , B. , Mitchell , J. F. B. , Stouffer
    , R. J. and Taylor , K. E. 2007 . The WCRP CMIP3 multi-model dataset: A new era
    in climate change research . Bulletin of the American Meteorological Society ,
    88 : 1383 – 1394 . (doi:10.1175/BAMS-88-9-1383)  Web of Science ®Google Scholar
    Miles , E. L. , Elsner , M. M. , Littell , J. S. , Binder , L. C. W. and Lettenmaier
    , D. P. 2010 . Assessing regional impacts and adaptation strategies for climate
    change: The Washington Climate Change Impacts Assessment as a case study . Climatic
    Change , 102 ( 1–2 ) : 9 – 27 . doi:10.1007/s10584-010-9853-2 (doi:10.1007/s10584-010-9853-2)  Web
    of Science ®Google Scholar Miles , E. L. , Snover , A. K. , Hamlet , A. F. , Callahan
    , B. and Fluharty , D. 2000 . Pacific Northwest regional assessment: The impacts
    of climate variability and climate change on the water resources of the Columbia
    River basin . Journal of the American Water Resources Association , 36 ( 2 ) :
    399 – 420 . (doi:10.1111/j.1752-1688.2000.tb04277.x)  Web of Science ®Google Scholar
    Mote , P. W. 2006 . Climate-driven variability and trends in mountain snowpack
    in western North America . Journal of Climate , 19 ( 23 ) : 6209 – 6220 . (doi:10.1175/JCLI3971.1)  Web
    of Science ®Google Scholar Mote , P. W. , Hamlet , A. F. , Clark , M. P. and Lettenmaier
    , D. P. 2005 . Declining mountain snowpack in western North America . Bulletin
    of the American Meteorological Society , 86 ( 1 ) : 39 – 49 . (doi:10.1175/BAMS-86-1-39)  Web
    of Science ®Google Scholar Mote , P. W. , Hamlet , A. F. and Salathé , E. P. 2008
    . Has spring snowpack declined in the Washington Cascades? . Hydrology and Earth
    System Sciences , 12 : 193 – 206 . (doi:10.5194/hess-12-193-2008)  Web of Science
    ®Google Scholar Mote , P. W. , Parson , E. A. , Hamlet , A. F. , Ideker , K. G.
    , Keeton , W. S. , Lettenmaier , D. P. , Mantua , N. J. , Miles , E. L. , Peterson
    , D. W. , Peterson , D. L. , Slaughter , R. and Snover , A. K. 2003 . Preparing
    for climatic change: The water, salmon, and forests of the Pacific Northwest .
    Climatic Change , 61 : 45 – 88 . (doi:10.1023/A:1026302914358)  Web of Science
    ®Google Scholar Mote , P. W. and Salathé , E. P. 2010 . Future climate in the
    Pacific Northwest . Climatic Change , 102 ( 1–2 ) : 29 – 50 . doi:10.1007/s10584-010-9848-z
    (doi:10.1007/s10584-010-9848-z)  Google Scholar Nakićenović, N., Alcamo, J., Davis,
    G., De Vries, B., Fenhann, J., Gaffin, S., … Dadi, Z. (2000). Special Report on
    Emissions Scenarios. A special report of Working Group III of the Intergovernmental
    Panel on Climate Change. Cambridge University Press, Cambridge, United Kingdom
    and New York, NY, USA.  Google Scholar Nijssen , B. , O''Donnell , G. M. , Hamlet
    , A. F. and Lettenmaier , D. P. 2001 . Hydrologic sensitivity of global rivers
    to climate change . Climatic Change , 50 : 143 – 145 . (doi:10.1023/A:1010616428763)  Web
    of Science ®Google Scholar NWPCC (Northwest Power and Conservation Council). (2005).
    Fifth Northwest Electric Power and Conservation Plan, Appendix N: Effects of climate
    change on the hydroelectric system. Retrieved from http://www.nwcouncil.org/energy/powerplan/5/  Google
    Scholar Payne , J. T. , Wood , A. W. , Hamlet , A. F. , Palmer , R. N. and Lettenmaier
    , D. P. 2004 . Mitigating the effects of climate change on the water resources
    of the Columbia River basin . Climatic Change , 62 ( 1–3 ) : 233 – 256 . (doi:10.1023/B:CLIM.0000013694.18154.d6)  Web
    of Science ®Google Scholar Salathé , E. P. 2005 . Downscaling simulations of future
    global climate with application to hydrologic modeling . International Journal
    of Climatology , 25 : 419 – 436 . (doi:10.1002/joc.1125)  Web of Science ®Google
    Scholar Salathé , E. P. , Mote , P. W. and Wiley , M. W. 2007 . Review of scenario
    selection and downscaling methods for the assessment of climate change impacts
    on hydrology in the United States Pacific Northwest . International Journal of
    Climatology , 27 ( 12 ) : 1611 – 1621 . doi:10.1002/joc.1540 (doi:10.1002/joc.1540)  Web
    of Science ®Google Scholar Schnorbus, M. A., Bennett, K. E., Werner, A. T., &
    Berland, A. J. (2011). Hydrologic impacts of climate change in the Peace, Campbell
    and Columbia watersheds, British Columbia, Canada. Report prepared by the Pacific
    Climate Impacts Consortium, University of Victoria, Victoria, BC.  Google Scholar
    Shi , X. , Wood , A. W. and Lettenmaier , D. P. 2008 . How essential is hydrologic
    model calibration to seasonal streamflow forecasting? . Journal of Hydrometeorology
    , 9 ( 6 ) : 1350 – 1363 . (doi:10.1175/2008JHM1001.1)  Web of Science ®Google
    Scholar Snover, A. K., Hamlet, A. F., Lee, S.-Y., Mantua, N. J., Salathé, E. P.
    Jr, Steed, R., & Tohver, I. (2010). Seattle City Light climate change analysis:
    Climate change impacts on regional climate, climate extremes, streamflow, water
    temperature, and hydrologic extremes. Prepared for The City of Seattle, Seattle
    City Light by The Climate Impacts Group, Center for Science in the Earth System,
    Joint Institute for the Study of the Atmosphere and Ocean, University of Washington,
    Seattle.  Google Scholar Snover , A. K. , Hamlet , A. F. and Lettenmaier , D.
    P. 2003 . Climate change scenarios for water planning studies . Bulletin of the
    American Meteorological Society , 84 ( 11 ) : 1513 – 1518 . (doi:10.1175/BAMS-84-11-1513)  Web
    of Science ®Google Scholar Solomon , S. , Qin , D. , Manning , M. , Chen , Z.
    , Marquis , M. , Averyt , K. B. , Tignor , M. and Miller , H. L. 2007 . Contribution
    of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel
    on Climate Change, 2007 , Edited by: Solomon , S. , Qin , D. , Manning , M. ,
    Chen , Z. , Marquis , M. , Averyt , K. B. , Tignor , M. and Miller , H. L. Cambridge,
    United Kingdom and New York, NY, USA : Cambridge University Press .  Google Scholar
    Stedinger , J. R. , Vogel , R. M. and Foufoula-Georgiou , E. 1993 . “ Frequency
    analysis of extreme events ” . In Handbook of hydrology , Edited by: Maidment
    , D. R. 18.1 – 18.66 . New York : McGraw-Hill, Inc .  Google Scholar Stöckle ,
    C. O. , Donatelli , M. and Nelson , R. 2003 . CropSyst, a cropping systems simulation
    model . European Journal of Agronomy , 18 : 289 – 307 . (doi:10.1016/S1161-0301(02)00109-0)  Web
    of Science ®Google Scholar Tague , C. , Grant , G. , Farrell , M. , Choate , J.
    and Jefferson , A. 2008 . Deep groundwater mediates streamflow response to climate
    warming in the Oregon Cascades . Climatic Change , 86 : 189 – 210 . doi:10.1007/s10584-007-9294-8
    (doi:10.1007/s10584-007-9294-8)  Web of Science ®Google Scholar Taylor , K. E.
    , Stouffer , R. J. and Meehl , G. A. 2012 . An overview of CMIP5 and the experiment
    design . Bulletin of the American Meteorological Society , 93 : 485 – 498 . doi:10.1175/BAMS-D-11-00094.1
    (doi:10.1175/BAMS-D-11-00094.1)  Web of Science ®Google Scholar Thornton , P.
    E. and Running , S. W. 1999 . An improved algorithm for estimating incident daily
    solar radiation from measurements of temperature, humidity, and precipitation
    . Agricultural and Forest Meteorology , 93 ( 4 ) : 211 – 228 . (doi:10.1016/S0168-1923(98)00126-9)  Web
    of Science ®Google Scholar Treaty Relating to Cooperative Development of the Water
    Resources of the Columbia River Basin, United States of America–Canada, January
    17, 1961–September 16, 1964, 15 U.S.T. 1555.  Google Scholar Trenberth , K. E.
    1997 . The definition of El Niño . Bulletin of the American Meteorological Society
    , 78 : 2771 – 2777 . (doi:10.1175/1520-0477(1997)078<2771:TDOENO>2.0.CO;2)  Web
    of Science ®Google Scholar US Department of the Interior. (2012). Yakima River
    Basin study, first one completed under Reclamation''s WaterSMART Program. Retrieved
    from http://www.usbr.gov/newsroom/newsrelease/detail.cfm?RecordID=39123  Google
    Scholar US Department of the Interior. (2013). Technical report to the RMJOC climate
    change study. U.S. Bureau of Reclamation, Pacific Northwest Region, Retrieved
    from http://www.usbr.gov/pn/programs/climatechange/reports/index.html  Google
    Scholar USFWS (US Fish & Wildlife Service). 2013. Landscape Conservation Cooperatives.
    Retrieved from http://www.fws.gov/landscape-conservation/lcc.html  Google Scholar
    Vano , J. A. , Scott , M. , Voisin , N. , Stöckle , C. O. , Hamlet , A. F. , Mickelson
    , K. E. B. , Elsner , M. M. and Lettenmaier , D. P. 2010 . Climate change impacts
    on water management and irrigated agriculture in the Yakima River basin, Washington,
    USA . Climatic Change , 102 ( 1–2 ) doi:10.1007/s10584-010-9856-z  Web of Science
    ®Google Scholar Van Rheenen , N. T. , Wood , A. W. , Palmer , R. N. and Lettenmaier
    , D. P. 2004 . Potential implications of PCM climate change scenarios for California
    hydrology and water resources . Climatic Change , 62 : 257 – 281 . (doi:10.1023/B:CLIM.0000013686.97342.55)  Web
    of Science ®Google Scholar Voisin , N. , Hamlet , A. F. , Graham , L. P. , Pierce
    , D. W. , Barnett , T. P. and Lettenmaier , D. P. 2006 . The role of climate forecasts
    in western U.S. power planning . Journal of Applied Meteorology , 45 ( 5 ) : 653
    – 673 . (doi:10.1175/JAM2361.1)  Google Scholar Wenger , S. J. , Isaak , D. J.
    , Luce , C. H. , Neville , H. M. , Fausch , K. D. , Dunham , J. B. , Dauwalter
    , D. C. , Young , M. K. , Elsner , M. M. , Rieman , B. E. , Hamlet , A. F. and
    Williams , J. E. 2011 . Flow regime, temperature and biotic interactions determine
    winners and losers among trout species under climate change . Proceedings of the
    National Academy of Sciences , 108 ( 34 ) : 14175 – 14180 . doi:10.1073/pnas.1103097108
    (doi:10.1073/pnas.1103097108)  PubMed Web of Science ®Google Scholar Wenger ,
    S. J. , Luce , C. H. , Hamlet , A. F. , Isaak , D. J. and Neville , H. M. 2010
    . Macroscale hydrologic modeling of ecologically relevant flow metrics . Water
    Resources Research , 46 : W09513 doi:10.1029/2009WR008839 (doi:10.1029/2009WR008839)  Web
    of Science ®Google Scholar Werner , A. , Schnorbus , M. , Shrestha , R. and Eckstrand
    , H. 2013 . Projected changes in climate, snowpack, evapotranspiration and streamflow
    in the Canadian portion of the Columbia River basin . Atmosphere-Ocean , 51 doi:
    10.1080/07055900.2013.821400  Google Scholar Wigmosta , M. S. , Nijssen , B. ,
    Storck , P. and Lettenmaier , D. P. 2002 . “ The distributed hydrology soil vegetation
    model ” . In Mathematical models of small watershed hydrology and applications
    , Edited by: Singh , V. P. and Frevert , D. K. 7 – 42 . Littleton , CO : Water
    Resource Publications .  Google Scholar Wigmosta , M. S. , Vail , L. W. and Lettenmaier
    , D. P. 1994 . A distributed hydrology-vegetation model for complex terrain .
    Water Resources Research , 30 : 1665 – 1679 . (doi:10.1029/94WR00436)  Web of
    Science ®Google Scholar Wood , A. W. , Leung , L. R. , Sridhar , V. and Lettenmaier
    , D. P. 2004 . Hydrologic implications of dynamical and statistical approaches
    to downscaling climate model outputs . Climatic Change , 62 ( 1–3 ) : 189 – 216
    . (doi:10.1023/B:CLIM.0000013685.99609.9e)  Web of Science ®Google Scholar Wood
    , A. W. , Maurer , E. P. , Kumar , A. and Lettenmaier , D. P. 2002 . Long range
    experimental hydrologic forecasting for the eastern U.S . Journal of Geophysical
    Research , 107 ( D20 ) : 4429 doi:10.1029/2001JD000659 (doi:10.1029/2001JD000659)  Web
    of Science ®Google Scholar Yapo , P. O. , Gupta , H. V. and Sorooshian , S. 1998
    . Multi-objective global optimization for hydrological models . Journal of Hydrology
    , 204 : 83 – 97 . (doi:10.1016/S0022-1694(97)00107-8)  Web of Science ®Google
    Scholar Yorgey, G. G., Rajagopalan, K., Chinnayakanahalli, K., Brady, M., Barber,
    M. E., Nelson, R., Stockle, C. S., Kruger, C. E., Dinesh, S., Malek, K., Yoder,
    J., & Adam, J. C. (2011). Columbia River basin long-term water supply and demand
    forecast (11-12-011), Washington State Legislative Report, Retrieved from http://www.ecy.wa.gov/biblio/1112011.html  Google
    Scholar Appendix: Table of Acronyms Download CSVDisplay Table Download PDF X Facebook
    LinkedIn Email Share Related research  People also read Recommended articles Cited
    by 109 Climate Change and Resource Management in the Columbia River Basin Stewart
    J. Cohen et al. Water International Published online: 22 Jan 2009 Climate and
    Streamflow Trends in the Columbia River Basin: Evidence for Ecological and Engineering
    Resilience to Climate Change Kendra L. Hatcher et al. Atmosphere-Ocean Published
    online: 20 Jun 2013 Spatial and Temporal Change in the Hydro-Climatology of the
    Canadian Portion of the Columbia River Basin under Multiple Emissions Scenarios
    A. T. Werner et al. Atmosphere-Ocean Published online: 2 Aug 2013 Information
    for Authors R&D professionals Editors Librarians Societies Open access Overview
    Open journals Open Select Dove Medical Press F1000Research Opportunities Reprints
    and e-prints Advertising solutions Accelerated publication Corporate access solutions
    Help and information Help and contact Newsroom All journals Books Keep up to date
    Register to receive personalised research and resources by email Sign me up Copyright
    © 2024Informa UK Limited Privacy policy Cookies Terms & conditions Accessibility
    Registered in England & Wales No. 3099067 5 Howick Place | London | SW1P 1WG     Cookies
    Button About Cookies On This Site We and our partners use cookies to enhance your
    website experience, learn how our site is used, offer personalised features, measure
    the effectiveness of our services, and tailor content and ads to your interests
    while you navigate on the web or interact with us across devices. By clicking
    \"Continue\" or continuing to browse our site you are agreeing to our and our
    partners use of cookies. For more information seePrivacy Policy CONTINUE"'
  inline_citation: '>'
  journal: Atmosphere - Ocean
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'An overview of the columbia basin climate change scenarios project: Approach,
    methods, and summary of key results'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Choi J.
  - Sanders A.
  citation_count: '3'
  description: Soil Moisture Active Passive (SMAP) is an Earth-orbiting, remote-sensing
    NASA mission slated for launch in 2014.‡ The ground data system (GDS) being developed
    for SMAP is composed of many heterogeneous subsystems, ranging from those that
    support planning and sequencing to those used for real-time operations, and even
    further to those that enable science data exchange. A full end-to-end automation
    of the GDS may result in cost savings during mission operations, but it would
    require a significant upfront investment to develop such a comprehensive automation.
    As demonstrated by the Jason-1 and Wide-field Infrared Survey Explorer (WISE)
    missions, a measure of "lights-out" automation for routine, orbital pass, ground
    operations can still reduce mission costs through smaller staffing of operators
    and limiting their working hours. The challenge, then, for the SMAP GDS engineering
    team, is to formulate an automated operations strategy-and corresponding system
    architecture- to minimize operator intervention during routine operations, while
    balancing the development costs associated with the scope and complexity of automation.
    This paper discusses the automated operations approach being developed for the
    SMAP GDS. The focus is on automating the activities involved in routine passes,
    which limits the scope to real-time operations. A key subsystem of the SMAP GDS-NASA's
    AMMOS Mission Data Processing and Control System (AMPCS)-provides a set of capabilities
    that enable such automation. Also discussed are the lights-out pass automations
    of the Jason-1 and WISE missions and how they informed the automation strategy
    for SMAP. The paper aims to provide insights into what is necessary in automating
    the GDS operations for Earth satellite missions. © 2012 by the American Institute
    of Aeronautics and Astronautics, Inc.
  doi: 10.2514/6.2012-1275978
  full_citation: '>'
  full_text: '>

    "Skip to main content Brought to you by UNIVERSITY OF NEBRASKA ANYWHERE Quick
    Search anywhere Enter words / phrases / DOI / ISBN / keywords / authors / etc
    Search Advanced search Cart Join AIAA Institution Login Log In Skip main navigation
    Home Journals Books Meeting Papers Standards Other Publications AIAA.org  Video
    Library   Meeting Paper HomeFor Authors SpaceOps Conferences Home Skip to article
    control options Free Access Cost-Effective Telemetry and Command Ground Systems
    Automation Strategy for the Soil Moisture Active Passive (SMAP) Mission Joshua
    Choi Session: general session Published Online:27 Mar 2013https://doi.org/10.2514/6.2012-1275978
    Read Now Tools Share Previous Chapter Next Chapter Figures References Related
    Details SpaceOps 2012 Conference 11 June 2012 - 15 June 2012 Stockholm, Sweden
    https://doi.org/10.2514/6.2012-1275978 Crossmark Topics Artificial Intelligence
    Avionics Software Communication System Communication Technology and Equipment
    Computer Programming and Language Computing and Informatics Computing, Information,
    and Communication Data Processing Data Science Deep Space Network Human-Computer
    Interaction Human-Machine Interaction Operating Systems Software Systems Software
    Testing Spacecraft Communication Systems Software Telemetry Keywords Telemetry
    SystemWide Field Infrared Survey ExplorerEarthAdvanced Multi Mission Operations
    SystemData ProcessingEarth SatellitesControl SystemsRemote SensingApplication
    Programming InterfaceGraphical User Interface Digital Topics Space Operations
    Publications Journals Books Meeting Papers Standards Resources For Authors Booksellers
    Companies Educators Librarians Researchers Standards Contributors Students Information
    How to Order How to Videos About Publications License Agreement FAQs Publish with
    Us Rights & Permissions Send Us Your Feedback Connect Announcements Contact Us
    Join AIAA © 2024 American Institute of Aeronautics and Astronautics American Institute
    of Aeronautics and Astronautics 12700 Sunrise Valley Drive, Suite 200 Reston,
    VA 20191-5807 703.264.7500 Privacy Policy Terms of Use"'
  inline_citation: '>'
  journal: SpaceOps 2012 Conference
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Cost-Effective telemetry and command ground systems automation strategy for
    the Soil Moisture Active Passive (SMAP) mission
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Talone M.
  - Camps A.
  - Mourre B.
  - Sabia R.
  - Vall-Llossera M.
  - Gourrion J.
  - Gabarró C.
  - Font J.
  citation_count: '21'
  description: 'The Soil Moisture and Ocean Salinity (SMOS) Mission is the second
    of the European Space Agency''s Living Planet Program Earth Explorer Opportunity
    Missions, and it is scheduled for launch in July 2009. Its objective is to provide
    global and frequent soil-moisture and sea-surface-salinity (SSS) maps. SMOS''
    single payload is the Microwave Imaging Radiometer by Aperture Synthesis (MIRAS)
    sensor, an L-band 2-D aperture-synthesis interferometric radiometer. For the SSS,
    the output products of SMOS, at Level 3, will have global coverage and an accuracy
    of 0.10.4 psu (practical salinity units) over & 100 × 100- 200 × 200 km2 in 1030
    days. During the last few years, several studies have pointed out the necessity
    of combining auxiliary data with the MIRAS-measured brightness temperature to
    provide the required accuracy. In this paper, we propose and test two techniques
    to include auxiliary data in the SMOS SSS retrieval algorithm. Aiming at this,
    pseudo-SMOS Level-3 products have been generated according to the following steps:
    1) A North Atlantic configuration of the NEMO-OPA ocean model has been run to
    provide consistent geophysical parameters; 2) the SMOS end-to-end processor simulator
    has been used to compute the brightness temperatures as measured by the MIRAS;
    3) the SMOS Level-2 processor simulator has been applied to retrieve SSS values
    for each point and overpass; and 4) Level-2 data have been temporally and spatially
    averaged to synthesize Level-3 products. In order to assess the impact of the
    proximity to the coast at Level 3, and the effect of these techniques on it, two
    different zones have been simulated: the first one in open ocean and the second
    one in a coastal region, near the Canary Islands (Spain) where SMOS and Aquarius
    CAL/VAL activities are foreseen. Performance exhibits a clear improvement at Level
    2 using the techniques proposed; at Level 3, a smaller effect has been recorded.
    Coastal proximity has been found to affect the retrieval of up to 150 and 300
    km from the coast, at Levels 2 and 3, respectively. Results for both scenarios
    are presented and discussed. © 2006 IEEE.'
  doi: 10.1109/TGRS.2008.2011618
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Geoscien...
    >Volume: 47 Issue: 9 Simulated SMOS Levels 2 and 3 Products: The Effect of Introducing
    ARGO Data in the Processing Chain and Its Impact on the Error Induced by the Vicinity
    of the Coast Publisher: IEEE Cite This PDF Marco Talone; Adriano Camps; Baptiste
    Mourre; Roberto Sabia; MercÈ Vall-llossera; JÉrÔme Gourrion; Carolina Gabarro;
    Jordi Font All Authors 19 Cites in Papers 207 Full Text Views Abstract Document
    Sections I. Introduction II. Methodology III. Results and Discussion IV. Conclusion
    Authors Figures References Citations Keywords Metrics Abstract: The Soil Moisture
    and Ocean Salinity (SMOS) Mission is the second of the European Space Agency''s
    Living Planet Program Earth Explorer Opportunity Missions, and it is scheduled
    for launch in July 2009. Its objective is to provide global and frequent soil-moisture
    and sea-surface-salinity (SSS) maps. SMOS'' single payload is the Microwave Imaging
    Radiometer by Aperture Synthesis (MIRAS) sensor, an L-band 2-D aperture-synthesis
    interferometric radiometer. For the SSS, the output products of SMOS, at Level
    3, will have global coverage and an accuracy of 0.1-0.4 psu (practical salinity
    units) over 100 times 100-200 times 200 km 2 in 10-30 days. During the last few
    years, several studies have pointed out the necessity of combining auxiliary data
    with the MIRAS-measured brightness temperature to provide the required accuracy.
    In this paper, we propose and test two techniques to include auxiliary data in
    the SMOS SSS retrieval algorithm. Aiming at this, pseudo-SMOS Level-3 products
    have been generated according to the following steps: 1) A North Atlantic configuration
    of the NEMO-OPA ocean model has been run to provide consistent geophysical parameters;
    2) the SMOS end-to-end processor simulator has been used to compute the brightness
    temperatures as measured by the MIRAS; 3) the SMOS Level-2 processor simulator
    has been applied to retrieve SSS values for each point and overpass; and 4) Level-2
    data have been temporally and spatially averaged to synthesize Level-3 products.
    In order to assess the impact of the proximity to the coast at Level 3, and the
    effect of these techniques on it, two different zones have been simulated: the
    first one in open ocean and the second one in a coastal region, near the Canary
    Islands (Spain) where SMOS and Aquarius CAL/VAL activities are foreseen. Performance
    exhibits a clear improvement at Level 2 using the techniques proposed; at Level
    3, a smaller effect has been recorded. Coastal proximity has been found to affect
    the retrieval of up to ... (Show More) Published in: IEEE Transactions on Geoscience
    and Remote Sensing ( Volume: 47, Issue: 9, September 2009) Page(s): 3041 - 3050
    Date of Publication: 17 April 2009 ISSN Information: DOI: 10.1109/TGRS.2008.2011618
    Publisher: IEEE SECTION I. Introduction Sea-Surface salinity (SSS) is one of the
    most important variables to achieve an adequate characterization of the water
    cycle and, thus, of the global-climate system of our planet. Jointly with sea-surface
    temperature (SST), SSS determines the water density and regulate the global ocean
    circulation currents, permitting the large-scale heat transportation, crucial
    for moderating the Earth''s climate. Despite the importance of measuring SSS,
    up to now, very few data are available: in situ measurements are very scarce and
    not uniformly distributed, and no data from remote sensors exist due to the complexity
    to achieve the necessary resolution and accuracy. A big step forward in the characterization
    of the global climate will be given by the Soil Moisture and Ocean Salinity (SMOS)
    mission, that aims at providing SSS remote measurements with global coverage and
    an accuracy of 0.1–0.4 psu (practical salinity units) over 100×100−200×200  km
    2 in 10–30 days [1]. The innovation of SMOS resides in its payload, the Microwave
    Imaging Radiometer by Aperture Synthesis (MIRAS) sensor, an L-band 2-D aperture-synthesis
    interferometric radiometer, that will provide brightness-temperature measurements
    at different incidence angles with a ground resolution varying between 30 and
    100 km depending on the incidence angle [2], [3]. Even with this new powerful
    instrument, the measurement of SSS requires special care: Even in the ideal case
    (flat sea surface), the sensitivity of brightness temperature to SSS is low [4].
    Due to this low sensitivity, auxiliary data will be used in combination with the
    brightness temperatures to help the retrieval. In particular, in the case of retrieving
    SSS, these auxiliary data will be the sea state, the SST, and the SSS, even at
    low-density sampling (few measurements per degree per month). One of the obvious
    candidates of auxiliary data is the ARGO buoys array. The importance of the auxiliary
    parameters on the retrieval performance has already been pointed out in [5], and
    the improvement at Level 2 of introducing ARGO data in the processing chain has
    been proved in [6] for an academic case. The aim of this paper is to test it in
    a realistic scenario and to bring this improvement up to Level 3, as well as to
    assess whether, and, if so, how much, including auxiliary data can mitigate the
    coastal-proximity effect. It is well known, in fact, that the coast vicinity induces
    errors in the retrieved SSS [7] due to the particular image processing applied
    in SMOS. SMOS Level 2 and both 10- and 30-day Level-3 products have been simulated
    using a North Atlantic configuration of the NEMO-OPA ocean model [8], [9] as a
    provider for geophysical parameters, an SMOS End-to-end Processor Simulator (SEPS)
    (in its full mode: considering all the 69 different antennas, using the G-matrix
    image reconstruction, etc. [2], [3]) as a source of SMOS-like brightness temperatures,
    and the Level-2 Processor Simulator (L2PS). The procedure followed is the same
    as described in [5], where the external brightness-temperature calibration and
    the external salinity calibration have been tested simultaneously for the first
    time considering ad hoc geophysical parameter and using the SEPS in its light
    mode. Two different scenarios have been taken into account: one in open ocean
    and another in a coastal zone. SECTION II. Methodology A. SMOS Characteristics
    SMOS'' payload (MIRAS) is an innovative L-band interferometric radiometer that
    will provide 50-km-resolution (on average) brightness-temperature measurements
    from space with an accuracy of 1.2 K [1]. It embodies 69 antennas uniformly distributed
    in a Y-shaped array; each of the three arms of the array having 21 antennas plus
    two redundant ones. Every 1.2 s, the interferometric radiometer synthesizes a
    full image from the cross correlation of the simultaneous measurements of the
    single antenna elements [2]. Due to the noncompliance of the Nyquist sampling
    criterion in the array design, there will be zones in the reconstructed brightness-temperature
    image affected by aliasing. The resulting alias-free field of view (FOV) has the
    form of a kind of distorted hexagon [12] and covers a large range of different
    incidence angles ( 0 ∘ − 65 ∘ ) . In a series of consecutive snapshots, the same
    pixel is observed under different incidence angles, with varying spatial resolution
    (from 30 to 100 km depending on the incidence angle) and with a different radiometric
    accuracy and sensitivity (1.2 K at boresight and from 2.6 to 5 K [13], [14], depending
    on the position of the pixel in the FOV). For this paper, SEPS [2] in its full-mode
    (including measured antenna patterns for each antenna, all instrument errors,
    G-matrix image reconstruction, and so on…) has been used to generate SMOS-like
    brightness temperatures. B. Geophysical Data Features In this paper, three databases
    are defined. 1. Original Data Daily outputs of a 1/2° configuration of the NEMO-OPA
    ocean model [8], [9] are used as original SSS and SST data, 10-m-height wind-speed
    ( U 10 ) fields come from the European Centre for Medium-Range Weather Forecast
    (ECMWF) ERA40 reanalysis [10]. Fig. 1 shows the Google Earth view, SSS, SST, and
    U 10 fields for the first day of simulation (March 1) for both the open-ocean
    [Fig. 1(a)] and the coastal-zone [Fig. 1(b)] scenarios. Fig. 1. (a) Open ocean
    and (b) coastal simulated scenario: Google Earth view, SSS (OPA model), SST (OPA
    model), and U 10 (ECMWF) fields for the first day of simulation (March 1), respectively.
    Show All 2. Auxiliary Data Due to the very restrictive requirements of the mission,
    the use of auxiliary data is mandatory. In this case, auxiliary SSS and SST come
    from Levitus climatology, and U 10 are extracted from the National Centers for
    Environmental Predictions (NCEP) NCAR reanalysis [11]. 3. Instantaneous Data One
    of the most obvious candidates of auxiliary data providers for SMOS is the ARGO
    buoys array. It consists of almost 3000 floats and provides 100 000 temperature/salinity
    profiles and velocity measurements per year, distributed over the global oceans
    at an average 3° spacing [15]. The data come from battery-powered autonomous floats
    that spend most of their life drifting at a depth called “parking depth,” where
    they are stable by having a density equal to the ambient pressure and a compressibility
    that is less than that of sea water. Floats cycle to 2000-m depth every ten days,
    with four- to five-year lifetimes for individual instruments. All ARGO data are
    publicly available in near real time via the Global Data Assembly Centers (GDACs)
    in Brest (France) [16] and Monterey, CA (U.S.) [15] after an automated quality
    control and in scientifically quality-controlled form (delayed-mode data) via
    the GDACs within six months of collection. Due to the lack of measurements in
    the chosen zones (Fig. 2), volunteer observing ships (VOS) have been added to
    the database. VOS data are available at the CORIOLIS FTP server [16]. Fig. 2.
    Total ARGO buoys observations between 2001 and June 2007 in a uniform 2 ∘ × 2
    ∘ grid. Show All A preselection of the ARGO data has been made before performing
    the retrieval, and data collected between the sea surface and the maximum depth
    of 10 m have been taken into account. In order to ensure consistency between original
    and instantaneous data, for both ARGO and VOS, only temporal and spatial information
    are kept, whereas salinity measurements have been overwritten by the OPA-output
    SSS (original data); no error has been added. A kriging interpolation [18], based
    on the modified ARGO and VOS measurements, provides the complete instantaneous
    SSS field. C. Retrieval Algorithm As explained, the aim of this paper is to assess
    the impact of including ARGO data in the SMOS SSS retrieval processing chain at
    both Levels 2 and 3. To perform the Level-2 SSS retrieval, L2PS has been used.
    The program, using as source of L1C data the output product of SEPS, and using
    a Bayesian approach, proceeds to minimize, according to the Levenberg–Marquardt
    algorithm [19], the following cost function: χ 2 = 1 N obs ∑ N obs n=1 ∥ ∥ F meas
    n − F model n ∥ ∥ 2 σ 2 F n +  (SST−SS T aux ) 2 σ 2 SST + ( U 10 − U 10aux )
    2 σ 2 U 10 (1) View Source where N obs is the number of observations of the same
    point in a satellite overpass and F meas n and F model n are the brightness temperatures,
    respectively, measured by MIRAS and obtained using the models, for the n th observation.
    In particular, F n can be defined as in the following conditions. F n =[ T v ¯
    ¯ ¯ ¯ ¯ , T h ¯ ¯ ¯ ¯ ¯ ] T in case of using the vertical (v) and horizontal (h)
    polarization of the measured brightness temperatures T ; F n =[ I ¯ ¯ ¯ ]=[ T
    x ¯ ¯ ¯ ¯ ¯ + T y ¯ ¯ ¯ ¯ ¯ ] in case of using the first parameter of Stokes i.
    In both formulations, the line above the letters T and i stands for vector; they
    are, in fact, vector of length N obs , in which each element corresponds to one
    incidence angle. Brightness temperatures both at the antenna and Earth frame can
    be used in the cost function, even if the use of the first one is preferred due
    to the singularities induced by the inversion of the geometric and Faraday rotations
    while passing the measured T B from antenna to Earth frame [20]. The cost function
    χ 2 is composed by two contributions: respectively, the MIRAS measurements, weighted
    by the radiometric accuracy for the n th observation, and the constraints for
    the SST and U 10 as sea-roughness descriptor, weighted by the inverse of the variance
    of the misfit considering the corresponding auxiliary field with respect to the
    original one, as expressed in σ 2 p = 1 N ∑ i=1 N ( p mis i −( 1 N ∑ j=1 N p mis
    j )) (2) View Source where n is the total number of points in the zone of interest
    and p mis = p aux − p orig , where p stands for SST and U 10 . As suggested in
    [21], no constraints are imposed to the SSS value. The geophysical models used
    in the L2PS are the same as in SEPS ([22] for the dielectric constant and [23]
    for the sea-roughness correction). The two methods used are as follows. The external
    brightness-temperature calibration [24]: For each snapshot and for all the points
    in the extended alias-free FOV, using the auxiliary data (Levitus + NCEP), pseudo-brightness
    temperatures are calculated through the same models used by SEPS [2], [3]. The
    calculated pseudo-brightness temperatures are subtracted to the measured ones,
    and the mean of this subtraction is considered as the mean bias introduced by
    the instrument errors. It is then defined the “corrected brightness temperature”
    as the measured brightness temperature minus the mean bias. The algorithm can
    be summarized by the expressions as follows, where ⟨⟩ stands for mean value Δ
    T B = T corrected B = ⟨ T meas B (SS S or ,SS T or , U 10 or ,θ) − T mod B (SS
    S aux ,SS T aux , U 10 aux ,θ)⟩ T meas B −Δ T B . (3) (4) View Source The external
    salinity calibration [6]: Once the SSS is retrieved, for each overpass, the algorithm
    performs a calibration to correct for the errors introduced by the forward models''
    inaccuracies, implementing a technique similar to the one used in rain-radar calibration
    [25]. The calibration is done by calculating the so-called calibration factor
    (CF) as the ratio between the instantaneous (ARGO + VOS) SSS mean value and the
    retrieved SSS one. In the case of the instantaneous measurements, all the data
    within the zone covered by the overpass during a temporal window of one month
    centered on the overpass day are taken into account to compute the mean value.
    Whereas the mean value of the retrieved SSS is calculated considering only the
    points of the overpass observed more than 40 times because of the large instrument
    error for points in that part of the FOV (more than 3.5 K [1]). The corrected
    retrieved salinity is given by the product between CF and the retrieved salinity.
    The algorithm is simply implemented through the following expressions: CF= SS
    S corr = ⟨SS S inst ⟩ ⟨SS S retr ⟩ CF⋅SS S retr . (5) (6) View Source Fig. 3 shows
    a summary of the setup of this paper. The original (“True”) data feed SEPS to
    synthesize brightness temperatures; the auxiliary data are used to perform the
    external brightness-temperature calibration, that can be considered a sort of
    preprocessing of the brightness temperature before they enter in the core of the
    L2PS, which is the minimization of the cost function. Once the SSS has been retrieved,
    the instantaneous data are used for the external salinity calibration. The final
    product is a calibrated Level-2 SSS map. Fig. 3. Schematic view of the procedure
    followed for the study. Show All Level-2 products have been defined as a weighted
    mean of the Level-2-retrieved SSS. The weights have been calculated as inversely
    proportional to the standard deviation of the error in the retrieval, sorted as
    a function of the number of observations, as expressed in (7) and (8), where the
    subindex i stands for the number of measurements at Level 2 used to synthesize
    the correspondent one at Level 3 SS S L3 = w( n obs )= ∑ w i ⋅SS S L2 i ∑ w i
    (std(SS S retr ( n obs )−SS S orig )) −1 . (7) (8) View Source D. Simulation To
    test the impact of the proposed algorithms on the SMOS Levels 2 and 3 products,
    using SEPS and L2PS, the whole month of March has been simulated for two scenarios:
    one in open ocean (a rectangular box in the northern Atlantic Ocean bounded by
    41 ∘  W , 27 ∘  W longitude and 9 ∘  N , 27 ∘  N latitude) and another one in
    a coastal zone (centered on Canary Islands and bounded by 20 ∘  W , 5 ∘  W longitude
    and 20 ∘  N , 40 ∘  N latitude). More than 3000 snapshots have been simulated,
    and SSS has been retrieved using only the external temperature calibration and
    using both the external brightness-temperature and the salinity calibrations.
    Performance at Levels 2 and 3 has been analyzed as well as the impact of the external
    salinity calibration on the coastal-proximity effect. The performance of the described
    techniques is evaluated according to their effect on the retrieved SSS error (SS
    S err ) , defined as the difference between the retrieved SSS and the original
    one (SS S err =SS S ret −SS S orig ) , both at Levels 2 and 3. SECTION III. Results
    and Discussion At Level 2, the external salinity calibration corrects almost perfectly
    the mean value of SS S err , while its standard deviation does not change, preserving
    the local SSS variations within the same overpass. In Figs. 4 and 5, the change
    in the SS S err mean value (top) and standard deviation (bottom), for all the
    overpasses of the (Fig. 4) open-ocean and (Fig. 5) coastal-zone cases, are shown
    for the case of applying only the brightness-temperature calibration [Figs. 4(a)
    and 5(a)] or both calibrations [Figs. 4(b) and 5(b)]. The SS S err mean value
    passes from having a peak-to-peak amplitude of more than 0.7–0.2 psu after the
    external salinity calibration in the case of open ocean (Fig. 4) and from having
    the 20% of overpasses with a mean SS S err larger than 1 psu to having just 8%
    of the overpasses in that condition after the external salinity calibration in
    the case of coastal region (Fig. 5). Comparing Figs. 4 and 5, it is evident that
    the effect of the vicinity to the coast is a degradation of the retrieval, increasing
    the standard deviation of the SS S err by a factor 1.5, or even 2 (from 0.8–1.2
    to 1–2 psu). Fig. 4. Retrieval result at Level 2 for the open-ocean scenario using
    (a) only the brightness-temperature calibration or (b) both calibrations. Show
    All Fig. 5. Retrieval result at Level 2 for the coastal-zone scenario using (a)
    only the brightness-temperature calibration or (b) both calibrations. Show All
    The trend of the error in the retrieved SSS, as a function of the distance from
    the ground track, in terms of its mean value and standard deviation, has been
    calculated and is shown in Fig. 6(a) and (b), respectively. Both external brightness-temperature
    and salinity calibrations are used, and all the pixels retrieved in the whole
    month for the open-ocean scenario are considered [masking the transition areas
    at the beginning and at the end of the sequence, as shown in Fig. 6(c)]. The bias
    is almost equal to zero, with fluctuations on the order of 0.5 psu in both the
    so-called “Q-Swath” (631 km) [26] and “Narrow Swath” (640 km) [27]. A similar
    trend is followed by the standard deviation of the retrieved SSS error, which
    is approximately constant and lower than 1.5 psu, in the same zone. Fig. 6. (a)
    Mean value and (b) standard deviation of the error in the retrieved SSS as a function
    of the distance from the ground track using both external brightness-temperature
    and salinity calibrations, (c) the transition areas at the beginning and end of
    the sequence have not been considered. The dash-dot line remarks the so-called
    “Q-swath.” Show All In Fig. 7, the mean [Fig. 7(a) and (b)] and the rms [Fig.
    7(c) and (d)] values of SS S err are shown as a function of the distance from
    the coast and of the number of observations. On the left panel [Fig. 7(a) and
    (c)], only brightness-temperature calibration has been used, whereas on the left
    one [Fig. 7(b) and (d)], the results of using both calibrations are presented.
    Large errors (3–4 psu) are found up to 150 km from the coast even for points observed
    more than 60 times. The external salinity calibration improves the quality of
    the retrieval: mean SS S err reduced by 25% (0.5 over 2 psu) and rms by 10% (0.5
    over 5 psu), particularly in the zones with largest errors (low number of observations).
    Fig. 7. Mean and rms values of the retrieved SSS error as a function of the distance
    from the coast and of the number of observations, using (a) and (c) only the brightness-temperature
    calibration or (b) and (d) both calibrations. Show All In Fig. 8, SS S err at
    Level 2 as a function of the number of observations (dots) and its standard deviation
    (solid line) are shown. Very large error is found for points observed less than
    20 times (from 5 to 22 psu); it continues being larger than 4 psu up to 40 observations,
    when it becomes almost constant and bound between 0 and 3 psu. Fig. 8. Retrieved
    SSS error at Level 2 as a function of the number of observations. (Solid line)
    Standard deviation of the error has been used in the calculation of the Level-3
    product. Show All As explained in Section II, the inverse of the SS S err standard
    deviation is used to ponder the SSS retrieved at Level 2 and construct the Level-3
    products; only points observed more than 40 times are considered. Two Level-3
    products have been generated: the 10-day product ( 2 ∘ × 2 ∘ averaged) and the
    30-day product ( 1 ∘ × 1 ∘ averaged). In both cases, performance improves after
    using the external salinity calibration. The SS S err mean value (μ) , standard
    deviation (σ) , and rms are reported in Table I. A decrease of 1.5%–7.7% and a
    15.7%–21.3% of the SS S err rms is found for the 30- and 10-day product, respectively,
    larger in the open-ocean case. Results indicate that SS S err at Level 3 is dominated
    by the standard deviation of the error at Level 2, limiting the effects of the
    external salinity calibration. Table I Retrieval Performance at Level 3 In Figs.
    9 and 10, the resulting Level 3 10-day [Figs. 9(a) and 10(a)] and 30-day [Figs.
    9(c) and 10(c)] products and the corresponding error when applying both external
    brightness-temperature and salinity calibrations for the open ocean and coastal
    zone [Figs. 9(b) and (d) and 10(b) and (d)] are shown, respectively. Applying
    both external calibrations, for the open-ocean zone, error at Level 3 is almost
    included between −0.4 and 0.4 psu for the 10-day product and −0.2 and 0.3 psu
    for the 30-day product; rms is, respectively, 0.20 and 0.15 psu (Table I), fulfilling
    the SMOS mission requirements in the first case and very close to that in the
    second one. Performance is, instead, very different in the coastal region, where
    rms is far away from the requirements; as expected, larger error is found near
    to the coast, where it exceeds the 2 psu. The very low retrieved SSS in Fig. 10
    at ( 28 ∘ − 29 ∘  N , 14 ∘ − 16 ∘  W ) are caused by the presence of the Canary
    Archipelago, that, even if masked, strongly affects the neighbor pixels. Fig.
    9. Level-3 (a) 10-day and (c) 30-day products and (b) and (d) corresponding error
    when applying both the external brightness-temperature and salinity calibrations—open
    ocean. Show All Fig. 10. Level-3 (a) 10-day and (c) 30-day products and (b) and
    (d) corresponding error when applying both the external brightness-temperature
    and salinity calibrations—coastal zone. Show All Coast-proximity dependence of
    the error at Level 3 is more apparent from Fig. 11, where retrieved SSS error
    as a function of the distance from the coast is presented for the 10-day [Fig.
    11(a) and (b)] and 30-day [Fig. 11(c) and (d)] products, applying only the brightness-temperature
    calibration [Fig. 11(a) and (c)], or both [Fig. 11(b) and (d)]. The triangle-,
    square-, and circle-marked lines stand for the mean value, rms, and standard deviation
    of the error, respectively. Fig. 11. Retrieved SSS error as a function of the
    distance from the coast for (a) and (b) the 10-day and (c) and (d) 30-day Level-3
    product, using (a) and (c) only the brightness-temperature calibration or (b)
    and (d) both calibrations. Show All Large errors up to 1 psu at ∼ 300 km from
    the coast are found; further away ( >400 km ), the rmse remains approximately
    constant at 0.5 psu, for both 10- and 30-day products. Applying the external salinity
    calibration partially corrects for the coastal-vicinity effect at Level 3, particularly
    in the zone nearer to the coast, helping in the discrimination of shallow-water
    processes. Comparing these results with previous works appeared in the literature,
    it is evident the increase in the estimation of the error due to coast proximity.
    Particularly, in [6], the SS S err rms is estimated to rapidly decrease in the
    first 50 km from the coast, remaining constant at 0.5 psu beyond that threshold,
    differently from our case, in which the coast seriously affects the retrieval
    of up to 300 km. To properly relate one result to the other, the differences between
    the two studies have to be recalled. In [6], for the semirealistic case, real
    coast line, SSS, and SST have been used, wind speed has been set constant at 10
    m/s, as well as σ SST and σ U 10 at 1 °C and 1.5 m/s, respectively. Radiometric
    noise has been assumed to have amplitude included between 2 and 4 K and to be
    Gaussian within the FOV. Moreover, due to the demanding computational time, only
    points at less than 600 km from the center of the swath have been simulated, and
    the simulations of monthly and 10-day products have been achieved, assuming Gaussian
    errors and scaling the σ p accordingly to the number of overpasses. In this paper,
    realistic SSS, SST, and U 10 have been used for both original and auxiliary fields,
    and the uncertainty associated to this estimation has been calculated as explained
    in Section II. Radiometric noise has been simulated as realistically as possible
    using the SEPS in its full mode, including all the non-Gaussianities related to
    the image-reconstruction algorithm. Finally, for the monthly and 10-day products,
    all the overpasses have been computed individually, averaging afterward the resulting
    retrieved SSS. All the non-Gaussianities taken into account in the simulation
    make that the SS S err at Level 3 does not decrease as expected using a Gaussian
    approximation. SECTION IV. Conclusion Two algorithms to include SSS auxiliary
    data in the SSS retrieval procedure for SMOS (external brightness-temperature
    calibrations [6], [23] and external salinity calibration [6]) have been tested
    and their impact assessed on the Levels 2 and 3 products, as well as the coast-proximity
    effect. A complete month has been simulated, using the SEPS [2], [3] in its full
    mode, whereas the L2PS, for two different scenarios: one in open ocean and another
    one in a coastal zone. The SSS has been retrieved, applying only the external
    brightness-temperature calibration or both the brightness-temperature and the
    salinity calibrations, and the performance of the retrieval has been analyzed
    and compared to previous studies. According to the results, it can be concluded
    as in the following conclusions. 1. At Level 2 The external salinity calibration
    remarkably reduces the retrieved SSS mean error, while keeping unchanged the standard
    deviation, preserving in this way the local variation of salinity within the same
    snapshot. The mean value of the retrieved SSS error is approximately equal to
    zero, with fluctuations on the order of 0.5 psu, while its standard deviation
    is almost constant and lower that 1.5 psu within both the so-called “Q-swath”
    and “Narrow Swath.” The proximity of the coast degrades the performance of the
    SSS retrieval, increasing the standard deviation of the error by a factor of between
    1.5 and 2. Large errors (3–4 psu) are found up to 150 km from the coast and ∼
    1 psu up to 300 km. External salinity calibration slightly decreases the error
    induced by the coastal proximity: the mean error by 25% (0.5 over 2 psu) and the
    rms by 10% (0.5 over 5 psu), particularly in the zones with largest errors (low
    number of observations). 2. At Level 3 The mean error is reduced by more than
    15% for 10-day product and by 5% in the 30-day one. The external salinity calibration
    only partially mitigates the error caused by the coast proximity for both for
    the 10- and the 30-day products. Level-3 mean error is dominated by the standard
    deviation of the Level-2 error, and therefore, averaging does not significantly
    reduce the SSS retrieval error. Authors Figures References Citations Keywords
    Metrics More Like This The WISE 2000 and 2001 field experiments in support of
    the SMOS mission: sea surface L-band brightness temperature observations and their
    application to sea surface salinity retrie... IEEE Transactions on Geoscience
    and Remote Sensing Published: 2004 An updated analysis of the ocean surface wind
    direction signal in passive microwave brightness temperatures IEEE Transactions
    on Geoscience and Remote Sensing Published: 2002 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Geoscience and Remote Sensing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Simulated SMOS levels 2 and 3 products: The effect of introducing argo data
    in the processing chain and its impact on the error induced by the vicinity of
    the coast'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Talone M.
  - Camps A.
  - Mourre B.
  - Sabia R.
  - Vall-llossera M.
  - Gabarró C.
  - Font J.
  citation_count: '0'
  description: 'The SMOS (Soil Moisture and Ocean Salinity) Mission is the second
    of the ESA''s Living Planet Programme Earth Explorer Opportunity Missions and
    it is scheduled for launch on November 2008. Its objective is to provide global
    and frequent Soil Moisture (SM) and Sea Surface Salinity (SSS) maps. SMOS'' single
    payload is the Microwave Imaging Radiometer by Aperture Synthesis (MIRAS) sensor,
    an L-band two-dimensional aperture synthesis interferometric radiometer. To help
    in the retrieval process, auxiliary data must be used in combination with the
    brightness temperatures measured by MIRAS. The output products of SMOS at Level
    3 will be SSS remote measurements with global coverage and an accuracy of 0.1-0.4
    psu (practical salinity units) over 100 x 100 - 200 x 200 km 2in 30 - 10 days.
    In this study pseudo SMOS Level 3 Products have been obtained in order to test
    the impact at Level 3 of introducing ARGO salinity measurements in the SMOS data
    processing chain. To do so: 1) The Océan Parallélisé (OPA) Model has been run
    to provide geophysical parameters; 2) The SMOS End-to-end Processor Simulator
    (SEPS) has been used to compute the brightness temperatures as measured by the
    MIRAS; 3) The SMOS Level 2 Processor Simulator (SMOS-L2PS) has been applied to
    retrieve SSS values for each point and overpass. To asses also the possible impact
    of the coastal vicinity effect, two different zones have been simulated; the first
    one in open ocean and the second one in a coastal region, near the Canary islands
    (Spain) where SMOS and Aquarius CAL/VAL activities are foreseen. The results for
    both simulation scenarios are presented and discussed. ©2008 IEEE.'
  doi: 10.1109/MICRAD.2008.4579463
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2008 Microwave Radiometry and...
    The impact of combining SMOS and ARGO data on the SMOS Level 2 and 3 products
    and effect of the vicinity of the coast Publisher: IEEE Cite This PDF M. Talone;
    A. Camps; B. Mourre; R. Sabia; M. Vall-llossera; C. Gabarro; J. Font All Authors
    76 Full Text Views Abstract Document Sections I. Introduction II. Methodology
    III. Simulations IV. Results and discussion V. Conclusions Authors Figures References
    Keywords Metrics Abstract: The SMOS (Soil Moisture and Ocean Salinity) Mission
    is the second of the ESApsilas Living Planet Programme Earth Explorer Opportunity
    Missions and it is scheduled for launch on November 2008. Its objective is to
    provide global and frequent Soil Moisture (SM) and Sea Surface Salinity (SSS)
    maps. SMOSpsila single payload is the Microwave Imaging Radiometer by Aperture
    Synthesis (MIRAS) sensor, an L-band two-dimensional aperture synthesis interferometric
    radiometer. To help in the retrieval process, auxiliary data must be used in combination
    with the brightness temperatures measured by MIRAS. The output products of SMOS
    at Level 3 will be SSS remote measurements with global coverage and an accuracy
    of 0.1-0.4 psu (practical salinity units) over 100 x 100 - 200 times 200 km 2
    in 30 - 10 days. In this study pseudo SMOS Level 3 Products have been obtained
    in order to test the impact at Level 3 of introducing ARGO salinity measurements
    in the SMOS data processing chain. To do so: 1) The Ocean Parallelise (OPA) Model
    has been run to provide geophysical parameters; 2) The SMOS End-to-end Processor
    Simulator (SEPS) has been used to compute the brightness temperatures as measured
    by the MIRAS; 3) The SMOS Level 2 Processor Simulator (SMOS-L2PS) has been applied
    to retrieve SSS values for each point and overpass. To asses also the possible
    impact of the coastal vicinity effect, two different zones have been simulated;
    the first one in open ocean and the second one in a coastal region, near the Canary
    islands (Spain) where SMOS and Aquarius CAL/VAL activities are foreseen. The results
    for both simulation scenarios are presented and discussed. Published in: 2008
    Microwave Radiometry and Remote Sensing of the Environment Date of Conference:
    11-14 March 2008 Date Added to IEEE Xplore: 25 July 2008 ISBN Information: DOI:
    10.1109/MICRAD.2008.4579463 Publisher: IEEE Conference Location: Florence, Italy
    SECTION I. Introduction Sea Surface Salinity (SSS) is one of the most important
    variables to achieve an adequate characterization of the water cycle and thus
    of the global climate system of our planet. Despite the importance of measuring
    SSS, up to now very few data are available: in-situ measurements are very scarce
    and not uniformly distributed and no data from remote sensors exist due to the
    complexity to achieve the necessary resolution and accuracy from such a distance.
    A big step toward the characterization of the global climate will be provided
    by the SMOS mission, that will allow SSS remote measurements with global coverage
    and an accuracy of 0.1 −0.4 psu (practical salinity units) over 100×100−200×200  km
    2 in 30–10 days [1]. The innovation of SMOS resides in its payload, the MIRAS
    (Microwave Imaging Radiometer by Aperture Synthesis) sensor, an L-band two-dimensional
    aperture synthesis interferometric radiometer, that will provide brightness temperature
    measurements at different incidence angles with a ground resolution varying between
    30 and 100 km depending on the incidence angle [2]. Even with this new, powerful
    instrument, the measurement of SSS requires special care: even in the ideal case
    (smooth surface), the sensitivity of brightness temperature to SSS is low [3].
    Due to this low sensitivity, auxiliary data will be used in combination with the
    brightness temperatures, to help the retrieval. In particular, in the case of
    retrieving SSS, these auxiliary data will be the sea state, the sea surface temperature
    (SST), and the sea surface salinity (even at low density sampling or from numerical
    models). One of the obvious candidate providers of auxiliary data is the ARGO
    buoys array. The importance of the auxiliary parameters on the retrieval performance
    has been pointed out [4]. The improvement at level 2 of introducing Argo data
    in the processing chain has been proved in [5] for an academic case. The aim of
    this study is to test it in a realistic scenario and to bring this improvement
    up to level 3, as well as to assess whether and how much including auxiliary data
    can mitigate the coastal proximity effect. It is well-known, in fact, that the
    coast vicinity induces errors in the retrieved SSS [6] due to the particular image
    processing applied in SMOS. Using the OPA model [7] as a source of geophysical
    parameters, SEPS as a source of SMOS-like brightness temperatures, and the L2PS
    , following the procedure described in [5] SMOS Level 2 and both 10-day and 30-day
    Level 3 products have been simulated. Two different scenarios have been taken
    into account: “Open-Ocean” (a rectangular box in the Northern Atlantic Ocean bounded
    by 41 ∘ W,  27 ∘ W longitude and 9 ∘ N , 27 ∘ N latitude) and “Coastal-Zone” (centered
    on Canary Islands and bounded by 20 ∘ W,  5 ∘ W longitude and 20 ∘ N,  40 ∘ N
    latitude). SECTION II. Methodology A. SMOS Characteristics SMOS’ payload (MIRAS)
    is an innovative L-band interferometric radiometer that will allow good resolution
    and accuracy brightness temperature measurements from space. Every 1.2 s the interferometric
    radiometer synthesizes a full image from the cross-correlation of simultaneous
    measurements of the 69 single antenna elements [2]. Due to the non-compliance
    of the Nyquist sampling criterion in the array design, there are zones in the
    reconstructed brightness temperatures ( T B ) image affected by alias. The resulting
    alias-free Field Of View (FOV) has the form of a kind of distorted hexagon [8]
    and covers a large range of different incidence angles ( 0 ∘ − 65 ∘ ) . In a series
    of consecutive snapshots, the same pixel is observed under different incidence
    angles, with varying spatial resolution (from 30 to 100 km depending on the incidence
    angle) and with a different radiometric accuracy and sensitivity (also depending
    on the position of the pixel in the FOV) [9]. B. Geophysical Data Features In
    this study three databases are defined: Original Data (used to feed SEPS and generate
    TB): As original SSS and SST the OPA model output is used, for U10 ECMWF prediction
    has been chosen; Auxiliary Data (used for the external brightness temperature
    calibration [5] and the level 2 cost function): Due to the very restrictive requirements
    of the mission the use of auxiliary data is mandatory. In this case auxiliary
    SSS and SST are from Levitus climatology and as auxiliary U10 NCEP prediction
    is used; Instantaneous Data (used for the external salinity calibration [5]):
    One of the most obvious candidates auxiliary data providers for SMOS is the Argo
    buoys array, an array of almost 3000 floats that provides 100,000 temperature/salinity
    (T/S) profiles and velocity measurements per year distributed over the global
    oceans at an average 3-degree spacing [10]. Due to the lack of measurements in
    the chosen zones (Figure 1), Volunteer Observing Ships (VOS) have been added to
    the database. Figure 1. Total Argo buoys observations between 2001 and June 2007
    in a uniform 2 ∘ × 2 ∘ grid. Show All A pre-selection of the ARGO data has been
    made before performing the retrieval and data collected between the sea surface
    and the maximum depth of 10 m have been taken into account [11]. In order to ensure
    consistency between original and instantaneous data both Argo and VOS salinity
    measurements have been overwritten by the OPA output (original data) SSS . A kriging
    interpolation [12], based on the modified Argo and VOS measurements, provides
    the complete instantaneous SSS field. C. Retrieval Algorithm As explained, the
    aim of this study is to assess the impact of including Argo data in the SMOS SSS
    retrieval processing chain at both Level 2 and 3. To perform the Level 2 SSS retrieval
    L2PS has been used. The program, using as source of L1C data the output product
    of SEPS, proceeds to minimize, according to the Levenberg-Marquardt algorithm
    [13], the following cost function: χ 2 = 1 N obs ∑ n=1 N obs ∥ F meas n − F model
    n ∥ 2 σ 2 F n + (SST−SS T aux ) 2 σ 2 SST + ( U 10 − U 10aux ) 2 σ 2 U 10 , (1)
    View Source where N obs is the number of observations of the same point in a satellite
    overpass, F meas n , F model n are the brightness temperatures, respectively measured
    by MIRAS and obtained using the models, for the n th observation. In particular
    F n can take the values: F n = [ T ¯ ¯ ¯ ¯ v , T ¯ ¯ ¯ ¯ h ] T , in case of using
    the two polarizations of the measured brightness temperature. F n =[ I ¯ ¯ ¯ ]=[
    T ¯ ¯ ¯ ¯ x + T ¯ ¯ ¯ ¯ y ] , in case of using the first parameter of Stokcs I
    . The cost function χ 2 is composed by two contributions: the MIRAS measurements,
    weighted by the radiometric accuracy for the n th observation, and the constraints
    for the Sea Surface Temperature (SST) and 10-meters height wind speed ( U 10 )
    as sea roughness descriptor, weighted by the inverse variance of the error considering
    the corresponding auxiliary field with respect to the original one. No constraints
    are imposed for SSS value, as suggested in [14]. The geophysical models used in
    the L2PS are the same as in SEPS ([15] for the dielectric constant and [16] for
    the sea roughness correction). The two methods used are: The external brightness
    temperature calibration [5], [16]: Using the Auxiliary Data (Levitus + NCEP),
    a pseudo-brightness temperature is calculated through the same models used by
    SEPS [2] for all the points in the FOV. The calculated pseudo-brightness temperatures
    are subtracted to the measured ones and the mean of this subtraction is considered
    as the mean bias introduced by the instrument errors. Is then defined the “corrected
    brightness temperature” as the measured brightness temperature minus the mean
    bias. The algorithm can be summarized by the expressions below: Δ T B =⟨ T meas
    B (SS S or ,SS T or , U 10or ,θ)− T mod B (SS S aux ,SS T aux , U 10aux ,θ)⟩,
    T corrected B = T meas B −Δ T B . (2) (3) View Source The external salinity calibration
    [5]: Once retrieved the SSS , the algorithm performs a sort of calibration trying
    to correct the errors introduced by the forward models inaccuracies, implementing
    a technique similar to the one used in rain radar calibration [18]. The calibration
    is done calculating the so-called Calibration Factor (CF) : the ratio between
    the instantaneous (Argo + VOS) SSS mean and the retrieved SSS one. Computing the
    CF the pixels observed less than 40 times are not taken into account because of
    the huge instrument error in that case. The corrected retrieved salinity is given
    by the product between CF and the retrieved salinity. The algorithm is simply
    implemented through the following expressions: CF=⟨SS S inst ⟩/⟨SS S ret ⟩, SS
    S corr =CF⋅SS S retr . (4) View Source SECTION III. Simulations To test the impact
    of the proposed algorithms on the SMOS Level 2 and 3 product, using SEPS and L2PS
    , the whole month of March has been simulated for two scenarios: one in Open-Ocean
    (a rectangular box in the Northern Atlantic Ocean bounded by 41 ∘ W,  27 ∘ W longitude
    and 9 ∘ N,  27 ∘ N latitude) and another one in a Coastal Zone (centered on Canary
    Islands and bounded by 20 ∘ W,  5 ∘ W longitude and 20 ∘ N,  40 ∘ N latitude).
    A total of more than 3000 snapshots have been simulated and retrieved using only
    the external temperature calibration and using both external brightness temperature
    and salinity calibration. Performance at level 2 and 3 has been analyzed as well
    as the impact of the external salinity calibration on the coastal proximity effect.
    SECTION IV. Results and discussion At Level 2, applying the external salinity
    calibration almost perfectly corrects the retrieved SSS mean error, on the other
    hand the retrieved SSS error standard deviation doesn''t change, preserving the
    local SSS variations within the same overpass. In Figure 2 the change in mean
    value and standard deviation of the error in retrieved SSS for all the overpasses
    of the Open-Ocean case are shown, results are very similar for the Coastal Zone.
    Figure 2. Retrieval result at Level 2 using (a) only the brightness temperature
    calibration or (b) both calibrations. Show All The vicinity to the coast heavily
    degrades the retrieval performance. The retrieved SSS error standard deviation
    is 1.5, or even 2 times the one in Open-Ocean. Large errors (2–3 psu) are found
    up to 150 km from the coast even for points observed more than 60 times. The external
    salinity calibration only slightly improves the quality of the retrieval (10%
    for mean error and 5% for r.m.s.). In Figure 3 the r.m.s. of the retrieved SSS
    error as a function of the distance from the coast and of the number of observations,
    using (a) only the brightness temperature calibration, or (b) both calibrations,
    is shown. Figure 3. R.m.s. of the retrieved SSS error as a function of the distance
    from the coast and of the number of observations, using (a) only the brightness
    temperature calibration or (b) both calibrations. Show All At Level 3, two products
    have been simulated: the 10-day product (2° × 2° averaged), and the 30-day product
    (1° × 1° averaged). Error Statistics are shown in Figs 4 (Open Ocean) and 5 (Coastal
    Zone) in case of using: (a−b) only the external brightness temperature calibration,
    or (c−d) both calibrations. Figure 4. Retrieval error statistics for the Open-Ocean
    (a) 10-day and (b) 30-day products in case of using (a-b) only the external brightness
    temperature calibration. or (c-d) both calibrations. Show All Figure 5. Retrieval
    error statistics for the Coastal Zone (a) 10-day and (b) 30-day products in case
    of using (a-b) only the external brightness temperature calibration, or (c-d)
    both calibrations. Show All Using the external salinity calibration corrects approximately
    the 25% and the 10% of the retrieved SSS r.m.s error of the 10-day and 30-day
    product, respectively. At Level 3 the vicinity to the coast induces large errors
    up to 80 km from the coast; Further away the error remains approximately constant
    at ~0.5 psu, for both 10–day and 30–day products. Figure 6. Retrieved SSS error
    as a function of the distance from the coast for the 10-day and 30-day level 3
    product, using (a-b) only the brightness temperature calibration or (c-d) both
    calibrations. Show All SECTION V. Conclusions Two algorithms to include SSS auxiliary
    data in the SSS retrieval procedure for SMOS have been tested and their impact
    has been assessed on the Level 2 and 3 products, as well as the coast proximity
    effect. It can be concluded that: At Level 2: The external salinity calibration
    remarkably reduces the retrieved SSS mean error, while the standard deviation
    remains constant. Coast proximity degrades the performance of the SSS retrieval,
    large errors are found up to 150 km from the coast. External salinity calibration
    slightly decreases the error induced by the coastal proximity: 10% for mean error
    and 5% for rms. At Level 3: Mean error is reduced by 25% for 10-day product and
    10% in 30-day one. The impact of the external salinity calibration on the coast
    proximity effect is negligible (2–4 %). Level 3 mean error dominated by std error
    at Level 2. As a final remark it has to be reminded that the averaging techniques
    used in this study have to be optimized by including physical constraints and
    discarding pixels contaminated. ACKNOWLEDGMENT This study has been founded by
    the Spanish National Program on Space through the project ESP2005-06823-C05. Authors
    Figures References Keywords Metrics More Like This Ocean surface wind direction
    measurement by scanning polarimetric microwave radiometry IGARSS ''98. Sensing
    and Managing the Environment. 1998 IEEE International Geoscience and Remote Sensing.
    Symposium Proceedings. (Cat. No.98CH36174) Published: 1998 Compensation of elevation
    angle variations in polarimetric brightness temperature measurements from airborne
    microwave radiometers IEEE Transactions on Geoscience and Remote Sensing Published:
    2001 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2008 Microwave Radiometry and Remote Sensing of the Environment - 10th
    Specialist Meeting, Proceedings, MICRORAD
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: The impact of combining SMOS and ARGO data on the SMOS level 2 and 3 products
    and effect of the vicinity of the coast
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Corbella I.
  - Torres F.
  - Camps A.
  - Colliander A.
  - Martín-Neira M.
  - Ribó S.
  - Rautiainen K.
  - Duffo N.
  - Vall-Llossera M.
  citation_count: '106'
  description: End-to-end calibration of the Microwave Imaging Radiometer by Aperture
    Synthesis (MIRAS) radiometer refers to processing the measured raw data up to
    dual-polarization brightness temperature maps over the earth's surface, which
    is the level 1 product of the Soil Moisture and Ocean Salinity (SMOS) mission.
    The process starts with a self-correction of comparators offset and quadrature
    error and is followed by the calibration procedure itself. This one is based on
    periodically injecting correlated and uncorrelated noise to all receivers in order
    to measure their relevant parameters, which are then used to correct the raw data.
    This can deal with most of the errors associated with the receivers but does not
    correct for antenna errors, which must be included in the image reconstruction
    algorithm. Relative S-parameters of the noise injection network and of the input
    switch are needed as additional data, whereas the whole process is independent
    of the exact value of the noise source power and of the distribution network physical
    temperature. On the other hand, the approach relies on having at least one very
    well-calibrated reference receiver, which is implemented as a noise injection
    radiometer. The result is the calibrated visibility function, which is inverted
    by the image reconstruction algorithm to get the brightness temperature as a function
    of the director cosines at the antenna reference plane. The final step is a coordinate
    rotation to obtain the horizontal and vertical brightness temperature maps over
    the earth. The procedures presented are validated using a complete SMOS simulator
    previously developed by the authors. © 2005 IEEE.
  doi: 10.1109/TGRS.2004.840458
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Geoscien...
    >Volume: 43 Issue: 5 MIRAS end-to-end calibration: application to SMOS L1 processor
    Publisher: IEEE Cite This PDF I. Corbella; F. Torres; A. Camps; A. Colliander;
    M. Martin-Neira; S. Ribo; K. Rautiainen; N. Duffo; M. Vall-llossera All Authors
    88 Cites in Papers 2 Cites in Patents 556 Full Text Views Abstract Document Sections
    I. Introduction II. Data Levels Zero and One III. Instrument Raw Data IV. Visibility
    Function V. Distributed Noise Injection Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: End-to-end calibration of the Microwave Imaging
    Radiometer by Aperture Synthesis (MIRAS) radiometer refers to processing the measured
    raw data up to dual-polarization brightness temperature maps over the earth''s
    surface, which is the level 1 product of the Soil Moisture and Ocean Salinity
    (SMOS) mission. The process starts with a self-correction of comparators offset
    and quadrature error and is followed by the calibration procedure itself. This
    one is based on periodically injecting correlated and uncorrelated noise to all
    receivers in order to measure their relevant parameters, which are then used to
    correct the raw data. This can deal with most of the errors associated with the
    receivers but does not correct for antenna errors, which must be included in the
    image reconstruction algorithm. Relative S-parameters of the noise injection network
    and of the input switch are needed as additional data, whereas the whole process
    is independent of the exact value of the noise source power and of the distribution
    network physical temperature. On the other hand, the approach relies on having
    at least one very well-calibrated reference receiver, which is implemented as
    a noise injection radiometer. The result is the calibrated visibility function,
    which is inverted by the image reconstruction algorithm to get the brightness
    temperature as a function of the director cosines at the antenna reference plane.
    The final step is a coordinate rotation to obtain the horizontal and vertical
    brightness temperature maps over the earth. The procedures presented are validated
    using a complete SMOS simulator previously developed by the authors. Published
    in: IEEE Transactions on Geoscience and Remote Sensing ( Volume: 43, Issue: 5,
    May 2005) Page(s): 1126 - 1134 Date of Publication: 25 April 2005 ISSN Information:
    DOI: 10.1109/TGRS.2004.840458 Publisher: IEEE SECTION I. Introduction The Soil
    Moisture and Ocean Salinity (SMOS) mission is the second selected opportunity
    mission on the Earth Observation Program of the European Space Agency (ESA) [1].
    Over the ocean, its objective is to Fig. 1. Mechanical configuration of MIRAS.
    Courtesy of EADS-CASA espacio. Show All produce sea surface salinity global maps
    with 200-km spatial resolution and accuracy of 0.1psu every 10–30 days. Over land,
    it will provide soil moisture global maps at 0.04 m 3 / m 3 accuracy and vegetation
    water content at 0.2 kg/m 2 every three days with a spatial resolution of 50 km.
    Finally, over the cryosphere it will contribute to the improvement of the monitoring
    of snow layer and multilayer structure of ice. The mission is currently in its
    phase C, and it is scheduled to be launched in early 2007. The single payload
    of the mission is the Microwave Imaging Radiometer by Aperture Synthesis (MIRAS)
    [2]. It will provide brightness temperature images at L-band after level 1 processing
    of the raw data. This radiometer will be the first spaceborne instrument to use
    interferometric aperture synthesis, a technique suggested back in the 1980''s
    as an alternative to real aperture radiometry for earth observation [3]. The whole
    instrument includes 69 L-band state-of-the-art high-sensitivity receivers, three
    of them capable to operate as accurate, highly stable noise injection radiometers
    (NIR). Each receiver has a wide beamwidth antenna and can operate alternatively
    in two orthogonal polarizations, except the NIRs that are fully polarimetric.
    The antennas are evenly distributed along a “Y-shape” structure (Fig. 1) providing
    a spatial resolution similar to that of a real aperture antenna having the same
    overall dimension as the whole instrument. This paper presents a description of
    the processing needed to pass from the MIRAS level 0 data to the level 1 product,
    this one consisting of horizontal and vertical brightness temperatures on the
    earth''s surface. The second section gives a description of the different processing
    levels, while the remaining ones are devoted to giving an insight on the different
    processing steps. These are grouped into two main procedures named calibration
    and inversion. For the first one a new approach based on two-level noise injection
    is presented and formulas are given. For the second one, a comprehensive description
    of the required processing is done while specific algorithms are properly referenced
    to open literature. SECTION II. Data Levels Zero and One A general overview of
    the SMOS data levels can be found in the mission web site at http://www.esa.int/export/esaLP/smos.html.
    Although the specific products that each level will contain are still being discussed
    in detail at the date of writing this paper, the main product definitions are
    already established and are summarized below. Level zero is basically the instrument''s
    output received by the ground station. Apart from housekeeping and ancillary data,
    such as readouts of thermal sensors at selected locations on the instrument and
    UTC time, it includes the following (see Section III): digital correlator counts;
    Power Measurement System (PMS) output voltages; NIR outputs. Scientific SMOS level
    1A products are the so-called “Calibrated Visibilities” that will be properly
    defined in Section IV. Within each level 1A product, vectors of calibrated visibilities
    are arranged as snapshots, i.e., referred to a single integration time. Level
    1A products are physically consolidated in pole-to-pole time-based segments (i.e.,
    using time-based segments instead of geographic segments). These time-based segments
    are thus related to the location of the spacecraft rather than the location of
    the observation data. The level 1B products are the output of the image reconstruction
    of the SMOS observation measurements and consist of geolocated vectors of brightness
    temperatures in the antenna polarization reference frame. Level 1C are geolocated
    vectors of brightness temperatures on the ground pixel polarization reference
    frame. Faraday rotation must be taken into account, and a land–water mask must
    be used in order to have the two areas computed separately. The input data needed
    for this level are the following: satellite orbit and attitude; data needed to
    correct the Faraday rotation; land/sea mask to separately process brightness temperatures.
    Fig. 2. Schematic block diagram of a receiver. Show All Level 1C products are
    geographically sorted as a continuous strip and constitute the necessary input
    to level 2 processing. The products defined so far must be complemented with ancillary
    data, assumed to be available from on-ground measurements performed prior to the
    instrument launching. In particular, antenna ohmic losses and relative S-parameters
    of the noise distribution network are needed for level 1A computation. Also, receiver
    nonlinearity and cross-coupling of noise injection between channels is needed
    as auxiliary data for the NIR. SECTION III. Instrument Raw Data The mechanical
    layout of MIRAS consists of a hexagonal hub and three arms forming a Y-shape (Fig.
    1). The hub has 12 standard receivers and three NIRs. To allow in-orbit deployment,
    each arm has three segments with six antennas each, so that the total number of
    receivers in the hub and the arms is 69. Fig. 2 shows a simplified block diagram
    of a given receiver along with part of the calibration hardware. The four-way
    input switch selects one of two antenna outputs (X or Y), uncorrelated noise from
    a resistor (U), or correlated noise from the noise distribution network (C). The
    radio-frequency (RF) part amplifies and filters the input signal to 1404–1423
    MHz. The mixer shifts this band to an intermediate frequency at 8–27 MHz using
    a local oscillator common to all receivers. Two intermediate frequency (IF) output
    signals are produced, namely the in-phase (i) and quadrature (q) components. One
    of them is sent to the PMS consisting of a diode detector and an integrator providing
    a low-frequency voltage proportional to the system noise temperature, which includes
    the antenna temperature and the receiver noise temperature. Simultaneously, both
    IF outputs are clipped using zero-voltage comparators to get digital signals which
    are in turn sent to a centralized matrix of one-bit two-level correlators. In
    addition to the receivers'' output signals, fixed values (digital 1 and 0) are
    also present in the correlator matrix. In this way, the following five different
    correlation products are available: between i channels of different receivers;
    between q and i channels of different receivers; between q and i channels of same
    receivers; between “0” and i and q channels; between “1” and i and q channels.
    Each individual cell in the correlator matrix is an exclusive NOR gate, so its
    output is 1 only if the two inputs are identical. The digital correlation is measured
    by accumulating the output during the integration time at a rate given by the
    clock frequency f s =55.84 MHz . At the end of the integration time, the counts
    are read and the accumulator reset for the following period. The correlator output
    is thus the absolute number of times (counts) that the two input signals coincide
    during the integration time. The NIR consists of two independent receivers connected
    to a dual-polarization antenna and to an internal noise injection circuitry, so
    that all four Stokes parameters can be measured using the correlator matrix [4].
    Its operation is based on an internal noise source which is used to balance the
    input signal to the receivers. Due to this, the effect of the receiver gain and
    offset variations are removed and thus the inherent stability of a noise injection
    radiometer is high [5]. The NIR is designed to measure the fully polarimetric
    antenna temperature and also, with the appropriate selection of a switch, the
    power coming from the noise distribution network (NDN) (Fig. 2). The specific
    NIR raw data are the noise injection pulse length and the physical temperature
    of the receivers. The correlator counts, all PMS voltages and NIR outputs are
    the raw data sent to ground by MIRAS. SECTION IV. Visibility Function To compute
    the visibility function, the correlator counts are first preprocessed to eliminate
    comparators offsets and quadrature errors. Actual calibration is performed afterward
    by injecting correlated and uncorrelated noise at receivers'' inputs. This is
    used to estimate the system temperatures needed to denormalize the visibilities
    and also to correct the in-phase and amplitude errors in the correlations due
    to receivers different frequency responses. The procedure is based on using the
    NIRs as reference receivers, so the stability of the calibration will ultimately
    depend on the stability of these radiometers. In orbit the variations of the physical
    temperature of the NIR are in order of some degrees. Although the NIR is inherently
    stable, the effect of these changes is significant, and software correction must
    be applied in the data processing. Required parameters for the correction are
    solved on-ground. With a prototype of the NIR, it has been demonstrated that the
    adequate stability can be achieved with this kind of method. The process cannot
    deal with antenna imperfections, since the noise is injected through a switch
    located after the antenna. Therefore, antenna pattern errors must be on-ground
    characterized and taken into account in the image reconstruction process. A. Correlator
    Offset and Quadrature Corrections The correlator counts N c are first converted
    to digital correlation using Z=2 N c / N c max −1 , where N c max is the number
    of counts obtained when two identical signals are applied to the correlator cell
    inputs. The digital correlation Z is a real number ranging from −1 to +1. Theoretically,
    the normalized correlation of the corresponding analog Gaussian signals is related
    to this digital correlation by [6] μ=sin( π 2 Z) (1) View Source which is only
    valid for comparators having zero offset. If this offset is small enough, it can
    be corrected if μ is computed by solving the following nonlinear equation [7]
    in which (1) is used as initial solution: Z= 2 π arcsinμ− 2 1− μ 2 − − − − − √
    (μ X 2 01 +μ Y 2 01 −2 X 01 Y 01 ) (2) View Source where for one of the two input
    signals X 01 =1/4( Z 0 − Z 1 ) , in which Z 0 and Z 1 are the digital correlations
    of the input signal with all ones and all zeroes, respectively. The same applies
    to Y 01 for the other signal. Cross-correlations are measured for a large number
    of signal pairs corresponding to the in-phase and quadrature outputs of the receivers.
    After performing the processing described so far, the following real normalized
    correlations are obtained: μ ii kj , μ qi kj and μ qi kk where k and j (k<j) are
    the receiver numbers and i or q denotes the in-phase or quadrature channel of
    the corresponding receiver. Note that the first two correlations apply to different
    receivers, while the last one to the i and q outputs of a the same receiver. Denoting
    by b i k (t) and b q k (t) the analytic signals of the two outputs of receiver
    k , taking into account the phase shift introduced by the mixer in the q branch,
    any of the above normalized correlations can be written as [8] μ αβ kj = Re[ e
    jΔ ϕ LO ⟨ b α k (t) b β j (t ) ∗ ⟩] ⟨ ∣ ∣ b α k (t) ∣ ∣ 2 ⟩⟨ ∣ ∣ b β j (t) ∣ ∣
    2 ⟩ − − − − − − − − − − − − − − − − √ (3) View Source where α and β should be
    replaced by i or q depending on the receiver output being considered and Δ ϕ LO
    accounts for the local oscillator nominal phase difference between the in-phase
    and quadrature outputs: Δ ϕ LO =0 if α=β and Δ ϕ LO =−π/2 if αβ=qi . Now, using
    the results of [9], (3) can be expressed as μ αβ kj = Re[ e jΔ ϕ LO r ~ αβ kj
    (0) V kj ] T sys k T sys j − − − − − − − √ (4) View Source where the system temperatures
    in the denominator are assumed equal for both IF branches and are given by T sys
    k = T a k + T R k in which T a k is the antenna temperature and T R k the equivalent
    noise temperature of receiver k . The fringe washing function at zero lag r ~
    αβ kj (0) is related to the frequency domain complex transfer functions of the
    corresponding channels H(f) by r ~ αβ kj (0)= 1 B α k B β j − − − − − √ G α k
    G β j − − − − − √ ∫ 0 ∞ H α k (f) H β j (f ) ∗ df (5) View Source where G stands
    for the power gain and B for noise-equivalent bandwidth. Finally, V kj in (4)
    is the so-called calibrated visibility, the level 1A product of SMOS and directly
    related to the brightness temperature of the scene by a Fourier-like integral
    transform [see (22) in Section VI]. It should be noted that r ~ αβ kj (0) carries
    the information of the in-phase and quadrature errors, as well as the nonseparable
    amplitude and phase errors. Given two receivers k and j the fringe washing functions
    of the (ii) and (qi) signals can be written approximately as [8] r ~ ii kj (0)=
    G kj e − j( θ qj − θ qk ) 2 r ~ qi kj (0)= G kj e − j( θ qj + θ qk ) 2 (6) View
    Source where G kj is the common part of the fringe washing function to both i
    and q branches and θ qk is the quadrature error of receiver k . For the special
    case of k=j and αβ=qi , (4) and (6) reduce to μ qi kk = = r ~ qi kk (0)= Re[ e
    − jπ 2 r ~ qi kk (0)] Im[ r ~ qi kk (0)] e −j θ qk (7) (8) View Source from which
    the quadrature error of any receiver is easily retrieved θ qk =−arcsin( μ qi kk
    ) (9) View Source Once the quadrature error is known, V kj comes after solving
    (4) V kj = T sys k T sys j − − − − − − − √ G kj M kj (10) View Source where M
    kj is the so-called quadrature-corrected normalized correlation M kj = 1 cos θ
    qk (Re[ M 1 μ kj ]+jIm[ M 2 μ kj ]) (11) View Source where μ kj = μ ii kj +j μ
    qi kj is the normalized complex correlation and M 1 and M 2 depend exclusively
    on the quadrature error of receivers k and j M 1 =cos Θ ′ kj +jsin Θ kj M 2 =cos
    Θ kj +jsin Θ ′ kj (12) View Source where Θ kj = θ qj 2 − θ qk 2 Θ ′ kj = θ qj
    2 + θ qk 2 (13) View Source and the quadrature error is given by (9). The quadrature
    corrected normalized correlation M kj is continuously computed from the measurements
    μ ii kj , μ qi kj , and μ qi kk . According to (10), to get the calibrated visibility
    the system temperatures and the fringe-washing factor G kj must be measured. This
    is done by periodically injecting noise to the receivers'' inputs and measuring
    both the total power and the quadrature corrected normalized correlation. B. System
    Temperatures System temperatures are measured using the PMS available in all receivers.
    In order to calibrate them, two different levels of noise are sequentially injected.
    For one of such levels, the PMS reading is v k = v off k + G k T C sys k (14)
    View Source where v off is the detector offset, G k the power gain, and T C sys
    k the total system temperature referred to the switch “C” input plane T C sys
    k = T S | S k0 | 2 + T n + T Rk (15) View Source where T Rk is the receiver noise
    temperature, T n the noise distribution network contribution, T S the source noise
    temperature, and S k0 the S -parameter of the distribution network from the input
    to receiver k (the source port is numbered as 0). The contributions of T n and
    T Rk are cancelled out by injecting two levels of correlated noise and subtracting
    the resulting equations (14) [10]. The next step consists of referring the system
    temperatures of all receivers to the NIR measurements, since this is used as a
    well-calibrated reference receiver. By combining (14) and (15), the system temperature
    of any receiver can be written as T C sys k = v k − v off k v k (2)− v k (1) |
    S k0 | 2 | S N0 | 2 [ T C sys N (2)− T C sys N (1)] (16) View Source where the
    subscript N refers to the port number of the noise distribution network to which
    the NIR is connected (Fig. 2), and (1) and (2) refer to both levels of correlated
    noise. Since the NIR is independently calibrated using an internal load and cold
    space views (see Section VI), the term T C sys N (2)− T C sys N (1) becomes directly
    known from the calibrated NIR measurements during noise injection at both levels.
    Finally, v off is independently estimated using additional detected power levels
    by means of the variable attenuator inserted in the receiver chain (Fig. 2) [11].
    The system temperature at the antenna reference plane is finally computed as T
    sys k = T C sys k | S LC k | 2 | S LA k | 2 η k (17) View Source where S LC k
    and S LA k are S parameters of the switch from the C and antenna ports to the
    output, respectively, and η k is the antenna efficiency. This equation should
    be applied for both antenna polarization outputs (X and Y). C. Baseline Phase
    and Amplitude Calibration The parameter G kj in (10) is estimated from the measured
    cross-correlations while injecting the two levels of noise described in Section
    IV-B. For each level, the correlation temperature T C kj of a given baseline kj
    at the receivers input plane depends, in general, on the power of the noise source,
    the distribution network physical temperature and its S-parameters [12]. If two
    levels are injected, the difference of the corresponding correlation temperatures
    is directly written in terms of the difference between the system temperatures
    of the reference receiver (NIR) T C kj (2)− T C kj (1)= S k0 S ∗ j0 | S N0 | 2
    [ T C sys N (2)− T C sys N (1)]. (18) View Source On the other hand, the correlation
    temperature can be computed by an equation similar to (10) T C kj (x)= T C sys
    k (x) T C sys j (x) − − − − − − − − − − − − √ G C kj M C kj (x) (19) View Source
    where x=1 or 2 and M c kj is the quadrature corrected normalized correlation measured
    while injecting correlated noise. Combining (18) and (19), G C kj can be written
    in terms of the system temperatures of the receivers, which are computed using
    (16). The final expression for G C kj shows that it only depends on the quadrature-corrected
    normalized correlation at both levels of injection, the corresponding PMS voltages,
    and the relative phases of the NDN S-parameters. To convert G C kj to the antenna
    reference plane, the following equation is used: G kj = G C kj S ¯ LA k S ¯ ∗
    LA j S ¯ LC k S ¯ ∗ LC j (20) View Source where the S parameters are the ones
    of the switch defined in the previous section and the overbar means normalized
    to unit amplitude, that is S ¯ =S/|S| . D. Residual Offset Thermal noise generated
    by the common local oscillator and distributed simultaneously to different receivers
    produces nonzero correlation. This offset is corrected by applying the whole procedure
    to the data measured while the input switch is in the “U” position, so injecting
    uncorrelated noise to all receivers. The calibrated visibility measured in this
    situation is then directly subtracted to the one measured for the switch in antenna
    positions (“X” or “Y”) V kj = V kj | X or Y position − V kj | U position . (21)
    View Source SECTION V. Distributed Noise Injection Noise injection is accomplished
    by two kinds of distribution networks. In the central part of the instrument (the
    hub) there is a 1-to-15 power divider for distributing the noise generated by
    a single source simultaneously to all hub receivers. As explained in the previous
    sections, three of them can operate as accurate, highly stable NIR and are used
    as reference receivers, so the procedure described in the previous section can
    be directly applied for the hub receivers. On the other hand, each of the three
    arms is formed by separate sections having six receivers each (Fig. 1) and there
    is a different noise source at each section. In the present configuration, each
    source drives a 1-to-12 network for distributing noise to the receivers of the
    same segment and of the adjacent one. A set of switches allows injecting noise
    in overlapping sets of receivers to apply the distributed noise injection concept
    proposed in [13] with the algorithms modified according to the methodology outlined
    in this paper. In particular, at each segment the reference receiver is chosen
    as one previously calibrated using another source. The first section of each arm
    is overlapped with the receivers in the hub that are in line with the arm, so
    that the noise injection of the first section includes a NIR, which is used to
    calibrate the rest of receivers in the section. And this process continues for
    the other sections. SECTION VI. Brightness Temperature in Antenna Reference Planes
    The brightness temperature map at a given polarization is computed from the calibrated
    visibility function by inverting the following Fourier-like integral [9] V kj
    =∫ ∫ ξ 2 + η 2 ≤1 T ′ kj (ξ,η) r ~ ¯ ¯ ¯ kj (− uξ+vη f 0 ) e −j2π(uξ+vη) dξdη
    (22) View Source where r ~ ¯ ¯ ¯ kj () is the fringe washing function normalized
    to its value at the origin (since r ~ kj (0) has been corrected in the calibration
    process) and T ′ kj the modified brightness temperature T ′ kj (ξ,η)= D k D j
    − − − − − √ 4π T B (ξ,η)− T r 1− ξ 2 − η 2 − − − − − − − − − √ F n k (ξ,η) F ∗
    n j (ξ,η) (23) View Source where T B is the brightness temperature defined for
    a field polarization that is selected by the input switch position [14], T r the
    physical temperature of the receivers, F n the normalized voltage antenna patterns
    for the corresponding polarization, and D the maximum directivity of the antennas.
    These ones are assumed at coordinates ( x k , y k ) and the following definitions
    apply: u=( x j − x k )/ λ 0 and v=( y j − y k )/ λ 0 where λ 0 is the wavelength
    at the center frequency f 0 . Finally, ξ and η are the director cosines with respect
    to the X and Y axes, respectively. For a given point in the (u,v) plane except
    the (0,0), the left-hand side of (22) is the calibrated visibility V kj measured
    by two receivers having relative position given (u,v) . In general, for a Y-shape
    instrument, the redundancy is small and limited only to antennas of the same arm.
    This means that almost all of the visibility samples are unique. For those samples
    that are measured simultaneously by different pairs of receivers their average
    is used, although other approaches are possible. In this way, a function V(u,v)
    having a unique value for a given pair (u,v) is computed. Fig. 3. Field of view
    of SMOS showing the alias. Show All The zero baseline V(0,0) is the antenna temperature
    and is obtained from the calibrated NIR data. Usually radiometers are calibrated
    using two known loads. However, in the NIR the so-called one-point calibration
    is applied. This means that the level of the noise injection is determined with
    one target, after which the antenna temperature can be determined using the physical
    temperature of the NIR and the ancillary data described in Section II [4]. In
    orbit, the space background radiation is foreseen to be used as the calibration
    target for NIR. In consequence the instrument will have to periodically perform
    a maneuver to point to the cold space. Since three different NIR are available
    in the hub, the average of their three calibrated measurements is used as antenna
    temperature. Furthermore, note that when the NIRs are measuring the fully polarimetric
    scene brightness temperature, the data can also be used to retrieve visibilities
    of baselines consisting of the NIR and other receivers. In this case, however,
    the noise injection mechanism and the Dicke switching of NIR degrade the corresponding
    visibility samples. For the third and fourth Stokes parameters the correlations
    measured between H- and V-channel need to be calibrated. This can be done as described
    in Section IV and adding the phase difference of the antenna connecting cables,
    which is measured on-ground, to the in-phase error. The first order solution for
    the inversion of (22) and (23) is an inverse Fourier transform followed by a compensation
    of the average antenna pattern and the obliquity factor 1− ξ 2 − η 2 − − − − −
    − − − − √ . Due to the Y-shape of the instrument, the visibility sampling grid
    is hexagonal so an hexagonal fast Fourier transform (HFFT) must be used [15].
    The whole space maps into the unit circle ( ξ 2 + η 2 <=1) so T ′ kj (ξ,η) is
    a function bounded by this region. Furthermore, since the antenna separation is
    larger than the Nyquist sampling spacing, there is some amount of aliasing. Fig.
    3 shows the location of the alias of the unit circle and of the earth for a given
    configuration of platform. In principle, the useful data is the one that is inside
    the region delimited by the alias of the unit circle. However this one can be
    extended to the earth alias by appropriate algorithms, as will be explained hereafter.
    Improved inversion techniques require the knowledge of r ~ ¯ ¯ ¯ (τ) and of the
    voltage patterns and maximum directivity of all antennas. The normalized fringe
    washing function is measured on board during the noise injection mode of operation
    by correlating the receivers'' outputs at three different time delays and approximating
    the amplitude by a sinc function and the phase by a quadratic polynomial [16].
    Due to the distributed noise injection scheme, all receivers are not simultaneously
    driven by the same noise source and the fringe-washing functions of baselines
    formed by receivers not sharing a noise source are computed using the approach
    of [17]. On the other hand, all antenna parameters are assumed to have been measured
    on ground and that they do not change during the mission lifetime. Measured antenna
    patterns are used for the inversion of (23). The proposed image reconstruction
    algorithm allows to extend the alias-free field of view up to the earth replicas
    by subtracting to the calibrated visibility function V(u,v) the contributions
    from 1) the physical temperature of the receivers; 2) the sky; 3) a constant brightness
    temperature in the region occupied by the earth; and 4) the direct and reflected
    contributions from the sun and moon which can be determined as in [18]. The differential
    visibility is then conveniently defined as ΔV(u,v)= V(u,v)− V R − V sky − T Earth
    V Earth − T Sun,dir V Sun,dir − T Sun,ref V Sun,ref − T Moon,dir V Moon,dir −
    T Moon,ref V Moon,ref (24) View Source where the different visibilities are computed
    by using (22) and (23) with the measured antenna patterns and fringe washing function
    but substituting ( T B (ξ,η)− T r ) by the corresponding contribution: the negative
    of the physical temperature of the receivers (− T r ) for V R , the computed brightness
    temperature of the Sky ( T Bsky ) for V sky , a constant 1 for V Earth and unit
    amplitude point sources at the directions of the direct and reflected sun and
    moon for the rest of terms. The value of T Earth is computed from (24) particularized
    at the origin u=v=0 and by forcing ΔV(0,0)=0 . The inversion algorithm is then
    applied to the differential visibility, thus obtaining the differential brightness
    temperature Δ T B (ξ,η)= T B (ξ,η)− T Earth that is, the deviations of the brightness
    temperature over the mean value in the alias-free field of view. The solution
    can be reached by discretizing all the visibility samples for all (ξ,η) points,
    and then solving the set of linear equations of the form ΔV=GΔ T B where G is
    the so-called G-matrix, using the following: an extension of the CLEAN iterative
    algorithm, as described in [19]; pseudoinverse matrix as in [3] in the one-dimensional
    case; conjugate-gradient algorithm as in [20] in the two-dimensional case. SECTION
    VII. Conversion to Ground Reference Frame The brightness temperature recovered
    by visibility inversion is defined at the antenna polarization reference frame.
    The next step in the level 1 processor consists of applying a rotation matrix
    to convert the measured brightness temperature to the standard horizontal and
    vertical frame over the earth''s surface. Due to Faraday rotation effects and
    to the relative orientation between the pixels reference frame over the earth''s
    surface and the antennas, the electric fields collected E x , E y are rotated
    an angle φ with respect to the field in the h–v frame on the earth''s surface
    E h , E v [ E x E y ]=[ cosφ −sinφ sinφ cosφ ][ E h E v ]. (25) View Source Moreover,
    depending on the input switch positions, three different brightness temperature
    maps are obtained after image reconstruction: T xx (ξ,η) proportional to ⟨ E x
    E ∗ x ⟩ , T yx (ξ,η) proportional to ⟨ E y E ∗ x ⟩ , and T yy (ξ,η) proportional
    to ⟨ E y E ∗ y ⟩ . The corresponding rotation matrix between them is readily obtained
    from (25) [14] ⎡ ⎣ ⎢ T xx T yx T yy ⎤ ⎦ ⎥ = ⎡ ⎣ ⎢ cos 2 φ −sin2φ sin 2 φ sin 2
    φ sin2φ cos 2 φ ⎤ ⎦ ⎥ [ T hh T vv ] (26) View Source where T hh and T vv are the
    standard horizontal and vertical brightness temperature of the earth and it has
    been used the experimental fact that the horizontal and vertical components of
    the field emitted by the earth at L-band are uncorrelated to each other, resulting
    that T vh has negligible values and so it has set equal to 0. The pass from the
    antenna to the earth reference frame consists of inverting (26) in the alias-free
    field of view pixel to pixel. Note that the antenna reference frame (x−y) is defined
    by the co-polar and cross-polar directions according to the Ludwig third definition
    of polarization [21]. This means that the angle φ depends on the direction of
    observation and it is different for each antenna. MIRAS supports two modes of
    operation: dual polarization, in which only T xx (ξ,η) and T yy (ξ,η) are measured,
    and full-polarimetric mode where T yx (ξ,η) is also measured. For the dual-polarization
    mode, instead of inverting (26), the following reduced equation must be used:
    [ T xx T yy ]=[ cos 2 φ sin 2 φ sin 2 φ cos 2 φ ][ T hh T vv ]. (27) View Source
    In this case, as already reported previously in [22], the matrix is singular for
    φ= 45 ∘ and some pixels are lost at every snapshot. On the contrary, in the full-polarimetric
    mode there are no singularities and the whole alias-free field of view can be
    imaged. Fig. 4 shows a snapshot retrieval for both cases obtained with the SMOS
    end-to-end performance simulator developed by the authors [23], [24]. The singularities
    at φ= 45 ∘ are clearly seen in the dual-pol case. Fig. 4. Images of a single snapshot
    retrieved with the SMOS End to End Simulator (SEPS) for full-pol (left) and dual-pol
    (right) modes of operation. Note the singularities at 45° for the dual-pol mode.
    Show All On the other hand, in the full-polarimetric mode the integration time
    per polarization is smaller (2.4 s/3=0.8 s) than in dual-polarization mode (2.4
    s/2=1.2 s) due to the time sharing between polarization-switch positions. This
    leads to a radiometric sensitivity increase (i.e., larger noise) by a factor of
    3/2 − − − √ =1.22 for the full-polarimetric case. Geophysical parameter retrievals
    in the earth reference frame require a precise knowledge of the observation geometry
    and of the Faraday rotation, both contributors to the overall rotation angle φ
    . However, some parameter retrievals (sea surface salinity and soil moisture over
    bare or low-vegetated soils) [25] can also be performed in the antenna reference
    frame, and even working with the first Stokes parameter I= T xx + T yy = T hh
    + T vv , which does not require any geometrical or Faraday rotation correction.
    Faraday rotation using the full-polarimetric mode of MIRAS is studied also in
    [26]. SECTION VIII. Conclusion The specificities of a two-dimensional interferometric
    aperture synthesis radiometer, such as MIRAS, makes the process of getting the
    brightness temperature map on the earth''s surface from the instrumental raw data
    very complex. In particular, the data processing must strongly rely on the hardware
    solutions chosen to correct for non ideal behavior of the electronic components.
    Also a complete understanding of the theory behind the interferometric aperture
    synthesis technique, including the simplifications and approximations carried
    out, is essential to achieve accurate image reconstruction algorithms, especially
    due to the aliasing condition produced by the sparse sampling in the (u,v) domain
    and the important role that the antenna patterns and fringe washing functions
    play. Polarization issues must also be taken into account due to the large field
    of view of the instrument. All processing algorithms are being tested using the
    end-toend simulator developed by the authors in the frame of the SMOS project
    and the results show that the spaceborne instrument will be able to produce brightness
    temperature maps with the required accuracy and resolution both from dual-pol
    or full-polarimetric measurements. The operating mode of MIRAS will be in the
    end dependent on the type of scene being imaged: over the ocean for example, simulation
    and experimental results show that the sea surface salinity can be retrieved effectively
    by using the first Stokes parameter and this can be obtained with only the dual-polarization
    mode. On the other hand, soil moisture over high vegetated soils may require the
    full-polarimetric data. Authors Figures References Citations Keywords Metrics
    More Like This Calibration of RapidScat Brightness Temperature 2018 IEEE 15th
    Specialist Meeting on Microwave Radiometry and Remote Sensing of the Environment
    (MicroRad) Published: 2018 Seawinds radiometer (SRad) on ADEOS-II brightness temperature
    calibration/validation Proceedings. 2005 IEEE International Geoscience and Remote
    Sensing Symposium, 2005. IGARSS ''05. Published: 2005 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Geoscience and Remote Sensing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'MIRAS end-to-end calibration: Application to SMOS LI processor'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
