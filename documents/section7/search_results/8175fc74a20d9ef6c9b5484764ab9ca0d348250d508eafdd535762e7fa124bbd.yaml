- analysis: '>'
  authors:
  - Zhao S.
  - Xu S.
  - Han S.
  - Ren S.
  - Wang Y.
  - Chen Z.
  - Chen X.
  - Lin J.
  - Liu W.
  citation_count: '0'
  description: The smart grid (SG) is a new type of grid that integrates traditional
    power grid with the Internet of Things (IoT) to make the entire grid system more
    compatible, controllable, and self-healing. However, the flourishing of SG still
    faces some challenges in term of privacy-preserving data aggregation. Previous
    multidimensional data aggregation schemes need heavy computation operations, cannot
    support multisubset data aggregation, and resist neither collusion attack among
    the gateway (GW) and control center (CC) nor differential attack. To solve these
    issues, we propose a privacy-preserving data aggregation scheme for fog-based
    SGs to achieve multidimensional and multisubset data aggregation. The parallel
    composability of differential privacy is used to reasonably allocate the privacy
    budget, which can provide higher data utility in multidimensional data aggregation.
    In addition, each user's multidimensional power consumption data will be structured
    as a composite data by utilizing the Chinese Remainder Theorem (CRT), which will
    further reduce the computational overhead. Security analysis shows that our scheme
    can resist differential attack, eavesdropping attack, collusion attack, and active
    attack. Evaluation of the performance also demonstrates that our scheme is more
    efficient in terms of computational overhead and communication overhead.
  doi: 10.1109/JIOT.2023.3309132
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things
    Journal >Volume: 11 Issue: 4 PPMM-DA: Privacy-Preserving Multidimensional and
    Multisubset Data Aggregation With Differential Privacy for Fog-Based Smart Grids
    Publisher: IEEE Cite This PDF Shuai Zhao; Shuhua Xu; Song Han; Siqi Ren; Ye Wang;
    Zhixian Chen; Xiaoli Chen; Jianhong Lin; Weinan Liu All Authors 491 Full Text
    Views Open Access Under a Creative Commons License Abstract Document Sections
    I. Introduction II. Related Work III. Problem Formalization IV. Preliminaries
    V. PPMM-DA Scheme Show Full Outline Authors Figures References Keywords Metrics
    Abstract: The smart grid (SG) is a new type of grid that integrates traditional
    power grid with the Internet of Things (IoT) to make the entire grid system more
    compatible, controllable, and self-healing. However, the flourishing of SG still
    faces some challenges in term of privacy-preserving data aggregation. Previous
    multidimensional data aggregation schemes need heavy computation operations, cannot
    support multisubset data aggregation, and resist neither collusion attack among
    the gateway (GW) and control center (CC) nor differential attack. To solve these
    issues, we propose a privacy-preserving data aggregation scheme for fog-based
    SGs to achieve multidimensional and multisubset data aggregation. The parallel
    composability of differential privacy is used to reasonably allocate the privacy
    budget, which can provide higher data utility in multidimensional data aggregation.
    In addition, each user’s multidimensional power consumption data will be structured
    as a composite data by utilizing the Chinese Remainder Theorem (CRT), which will
    further reduce the computational overhead. Security analysis shows that our scheme
    can resist differential attack, eavesdropping attack, collusion attack, and active
    attack. Evaluation of the performance also demonstrates that our scheme is more
    efficient in terms of computational overhead and communication overhead. Published
    in: IEEE Internet of Things Journal ( Volume: 11, Issue: 4, 15 February 2024)
    Page(s): 6096 - 6110 Date of Publication: 28 August 2023 ISSN Information: DOI:
    10.1109/JIOT.2023.3309132 Publisher: IEEE Funding Agency: SECTION I. Introduction
    The smart grid (SG) is an advanced digital two-way flow power system, especially
    in Industrial Internet of Things (IoT), with self-healing, adaptive, and resilient
    [1], [2], [3]. In order to achieve the real-time monitoring and optimal control
    of the power grid, the SG needs to collect a large number of power consumption
    data from users via IoT devices such as smart meters (SMs). The collection and
    transmission of power consumption data for SMs bring risks of individual user’s
    privacy leakage [4]. How to protect the security and privacy of power consumption
    data has become a major concern [5]. Hence, the collaborative interaction and
    cooperation among users, devices, and environments are critical [6]. We need to
    ensure the security of data exchange and the privacy of the computation, etc.
    [7], [8]. Existing works [9], [10], [11], [12], [13], [14], [15] allow SMs to
    report one type of data, and the control center (CC) can only obtain the total
    power consumption data for all users. However, the fog-based SG system has different
    types of devices, such as refrigerator, microwave, oven, TV, etc. In order to
    analyze the data more deeply and optimize the data more finely, the CC needs more
    detailed power consumption data for each device. Some solutions [16], [17], [18],
    [19], [20], [21], [22] are dedicated to solving the fine-grained problem of multidimensional
    data aggregation. However, these schemes use heavy computation operations to encrypt
    the multidimensional data, and the collusion attack is not explicitly addressed.
    In addition, one of the key challenges for data aggregation is how to achieve
    multisubset aggregation of multidimensional data in the SG, which is expected
    to reduce the peak-to-average ratio, balance energy supply, and demand. Another
    challenging issue for data aggregation is differential attack [23], [24], [25],
    [26]. Specifically, even if an aggregation scheme is secure, once adversary obtains
    the sum of n and n−1 users’ power consumption data, the private data of different
    user will inevitably be obtained. This issue has been studied in [27], [28], [29],
    [30], [31], [32], [33], and [34], but they only achieve differential privacy for
    1-D data. Therefore, it remains a critical challenge for multidimensional data
    aggregation to resist differential attack, offer provable differential privacy
    guarantees on the aggregated statistic, and improve the utility of data. Futhermore,
    the above literatures adopt the traditional SG model, that is, the gateway (GW)
    aggregates the data reports from SMs and transmits them to CC for further analysis.
    However, these SMs generate a large amount of data every 15 min, which will put
    a heavy burden on the cloud, cause huge network delay, and endanger the privacy
    and security of personal data [37], [38]. Actually, the fog computing model can
    extend cloud computing functions from the network center to the network edge,
    optimize local computing capacity, and make the collection, transmission, and
    processing of big data more efficient [39], [40], [41]. Using the fog computing
    model in data aggregation schemes can improve computational efficiency and reduce
    transmission delay [42], [57], [58]. In sum, in this article, we present a privacy-preserving
    multidimensional and multisubset data aggregation (PPMM-DA) scheme with differential
    privacy for fog-based SGs. Our major contributions are summarized below. In order
    to analyze the power consumption data more deeply and implement effective grid
    monitoring and management, our scheme achieves multidimensional and multisubset
    data aggregation by employing the Chinese Remainder Theorem (CRT), super-increasing
    sequence, and Paillier cryptosystem. Each user’s multidimensional power consumption
    data will be structured as one composite data by CRT. We can extract aggregated
    results of power consumption for each dimension from the aggregated ciphertext,
    which makes computational overhead be independent of the dimension of power consumption
    data, results in significant saving in computational overhead. Besides, our scheme
    also can resist collusion attack among the GW and CC. In order to realize privacy-enhanced
    multidimensional data aggregation and offer provable differential privacy guarantees
    on the aggregated statistic, we propose two methods to achieve differential privacy,
    which extracted noise from the Geometric distribution and Laplace distribution,
    respectively. Meanwhile, the parallel composability of differential privacy is
    used to reasonably allocate the privacy budget, which can provide higher data
    utility. Then, the data utility of these two methods is compared and analyzed.
    The roadmap of this article is as follows. The related works are given in Section
    II. The problem formalization is introduced in Section III. Then we recall the
    Paillier cryptosystem, differential privacy, CRT as the preliminaries in Section
    IV. In Section V, we introduce our PPMM-DA scheme. To achieve privacy-enhanced
    multidimensional data aggregation, we propose the scheme with differential privacy
    in Section VI. In Section VII, the correctness and security of our scheme are
    analyzed. Section VIII presents performance comparison, experimental results,
    and differential privacy comparison. Finally, we conclude this article in Section
    IX. SECTION II. Related Work Research on privacy preserving is conducted on all
    phases of the information life cycle, including information collection, storage,
    processing, publication, and destruction [9]. As a key phase of data collection
    and processing, data aggregation has become one of the research hotspots, and
    many methods have arisen. Lu et al. [16] presented the first multidimensional
    data aggregation scheme utilizing super-incremental sequence and Paillier cryptosystem.
    Chen et al. [17] proposed a data aggregation scheme that enables the SM to report
    more than one type power consumption data, and allows the CC to perform variance
    analysis and one-way analysis of the variance on the power consumption data. However,
    they can only provide the result of global aggregation for the CC, and can not
    satisfy more fine-grained requirements. At the same time, they do not solve the
    collusion attack between the GW and CC. Lu et al. [23] first tried to divide users
    into two subsets based on power consumption according to an additive homomorphism
    of composite order cipher set, but it does not support multisubset data aggregation.
    From the perspective of multisubset data aggregation, Li et al. [24] proposed
    a PPMA scheme, where the CC can obtain the power consumption data of users in
    different ranges, but the approximate result is obtained instead of accurate aggregated
    result and it does not support multisubset data aggregation of multidimensional
    data. Furthermore, data integrity verification [56] and fault tolerance are not
    considered. Chien and Su [25] improved the PPMA scheme to support fault tolerance,
    but it still obtains approximate results and the computational overhead is expensive.
    In addition, the collusion attack is not explicitly addressed. Zuo et al. [55]
    proposed a scheme for addressing the collusion attack, but it can not resist differential
    attack. Recently, many researchers have worked with differential privacy technology
    to resist differential attack. Bao and Lu [30] proposed a data aggregation scheme
    using differential privacy, where aggregated data is interfered by random noise
    from the Geometric distribution to resist differential attack. They design a novel
    data aggregation scheme that can flexibly support fault tolerance of malfunctioning
    SMs. However, their scheme require assigning random values via bi-directional
    interaction. Ni et al. [31] proposed a new privacy-preserving smart metering scheme
    for SGs, which supports fault tolerance, differential privacy, and range-based
    filtering simultaneously. Lu et al. [36] used CRT, the Paillier encryption and
    one-way hash chain technique to present a lightweight privacy-preserving data
    aggregation scheme. Shi et al. [32] presented a diverse grouping-based aggregation
    scheme to achieve grouping-based private stream aggregation and utilized differential
    privacy technique to resist differential attack. Through normalizing and modifying
    the confidence score vectors with a differential privacy mechanism, Ye et al.
    [52] proposed a one-parameter defense method to against both model inversion and
    membership inference attacks. Based on randomized responses, Gai et al. [29] presented
    an efficient data aggregation scheme satisfying local differential privacy with
    privacy preserving for SGs. However, they only achieve differential privacy for
    1-D data. With the development of SG, we need to focus on how multidimensional
    data aggregation can resist differential attack and improve the utility of data.
    Lately, some new results in [51], [53], and [54] achieved privacy-preserving multidimensional
    data aggregation via differential privacy and resisted differential attack. However,
    they do not take into consideration other attacks, such as eavesdropping attack,
    collusion attack, and active attack. The above schemes solve the problems for
    SGs in different aspects, but there are still many weaknesses. Motivated by the
    above-mentioned, we are aiming to design an effective privacy-preserving data
    aggregation scheme for fog-based SGs, which can achieve multisubset aggregation
    of multidimensional data, fault tolerance, resist differential attack, and collusion
    attack, and minimize privacy leakage, network latency and loss of data availability.
    SECTION III. Problem Formalization A. System Model We consider a fog-based SG
    system that consists of a trusted authority (TA), a CC, some fog nodes (FNs) and
    SMs. The architecture of our system model is shown in Fig. 1. TA: In our system
    model, the TA is a only globally-trusted entity, whose duty is to bootstrap the
    whole system, manage, and distribute key materials to other entities. After that,
    the TA will turn to offline. CC: The CC performs Paillier decryption, utilizes
    CRT and super-increasing sequence to analyze the data more deeply based on the
    power consumption data of each device and the corresponding number of users within
    a specific power consumption range. FN: The FN is located in the middle layer
    between the Cloud layer and the IoT layer, and transfers the computing and storage
    functions of cloud to the edge of terminal equipment. In our system model, the
    FN acts as an aggregator to aggregate encrypted data from users within its coverage
    area. User: Each user equips with an SM (i.e., IoT node) which encrypts the collected
    power consumption data and transmits the encrypted report to the nearby FN. Fig.
    1. System model. Show All B. Threat Model In our threat model, we assume that
    the TA is trustable and other entities are honest-but-curious [35], which means
    that the CC, FN, and users will strictly execute the protocol, but remain curious
    about other users private information (such as individual users private key and
    power consumption data). In addition, an external adversary may be lurking in
    the residential area, eavesdropping on the communication links between various
    entities or invading the database of the FN to obtain private information. In
    this article, we mainly consider the following four types of security threats,
    i.e., the differential attack, eavesdropping attack, collusion attack, and active
    attack, and other threats are beyond the scope of this article. Differential Attack:
    The adversary can analyze the aggregated difference between each multidimensional
    data set to infer device-specific power consumption data for an individual user
    in the fog-based SG. Eavesdropping Attack: The adversary can eavesdrop on the
    communication links from the IoT layer to the Fog layer or from the Fog layer
    to the Cloud layer to obtain the transmitted power consumption data, and try to
    compromise the individual users private power consumption information. Collusion
    Attack: The FN is allowed to collude with some users and the CC is able to collude
    with the FN or some users by sharing and analyzing their private information,
    which consists of private key, public parameters, and ciphertext of power consumption
    data, to obtain an individual users private information, i.e., individual power
    consumption data. Active Attack: The adversary may invade into the FN to steal
    users private information. By intercepting messages, the adversary can also forge
    the identities of trusted users to transmit false power consumption data to compromise
    the integrity of data in the fog-based SG. C. Design Goal To realize a privacy-preserving
    multidimentional and multisubset data aggregation with differential privacy for
    fog-based smart grids, the design goal of the proposed scheme includes: Multidimensional
    and Multisubset: To conduct fine-grained analysis and achieve effective grid monitoring
    and management, the CC should obtain power consumption data of each dimension
    and the corresponding number of users within a specific power consumption range.
    Privacy and Security: The power consumption data of individual user should be
    protected in the system. The proposed data aggregation scheme should meet the
    security requirements and can resist differential attack, eavesdropping attack,
    collusion attack, and active attack. Efficiency: Due to the limited resource of
    terminals, the proposed data aggregation scheme should be extremely efficient
    at each phase of the system. SECTION IV. Preliminaries In this section, we give
    an overview of Paillier cryptosystem [43], differential privacy [44], and CRT
    [49], all of which serve as the basis of our proposed scheme. The main notations
    of this article and their definitions are summarized in Table I. TABLE I Main
    Notations A. Paillier Cryptosystem Paillier cryptosystem is composed of three
    algorithms: 1) key generation; 2) encryption; and 3) decryption. Key Generation:
    Given the security parameter κ , two large and independent prime numbers p and
    q are first chosen, where |p|=|q|=κ . Then, the RSA modulus N=pq and λ=lcm(p−1,q−1)
    are computed. Define a function L(φ)=([φ−1]/N) , after choosing a generator g=(1+N),φ=(L(
    g λ mod N 2 ) ) −1 modN is further calculated. The public key is (N,g) , and the
    private key is (λ,μ) . Encryption: Given a message m∈ Z N , first, we choose a
    random number r∈ Z ∗ N , and the ciphertext can be calculated as follows: c=E(m)=
    g m ⋅ r N mod N 2 . (1) View Source Decryption: Given the ciphertext c∈ Z ∗ N
    2 , the corresponding plaintext message can be computed as follows: m=D(c)=L(
    c λmod N 2 )⋅μmodN. (2) View Source In our proposed scheme, we utilized another
    form of Paillier cryptosystem. As the generator g=(1+N) and ϕ( N 2 )=N⋅ϕ(N)=N⋅λ
    , we can conclude the following equations according to the Euler theorem: c= =
    = (1+N ) m ⋅ r N⋅λ mod N 2 (1+N ) m ⋅ r ϕ( N 2 ) mod N 2 (1+N ) m mod N 2 . (3)
    View Source We can expend the power (1+N ) m with the Binomial theorem (1+N )
    m = = ∑ i=1 m ( m i ) N i 1+mNmod N 2 (4) View Source as all items with i≥2 turn
    to zero. B. Differential Privacy Dwork [44] first proposed a differential privacy
    model, which can achieve the privacy-preserving effectively by adding appropriate
    noise to the query or analysis result. In addition, parallel composability was
    proposed in [45]. Definition 1 (Differential Privacy):The aggregation function
    A gives ϵ -differential privacy if for any data sets D 1 and D 2 differing by
    at most one element, and for any O∈Range(A) , have Pr[A( D 1 )∈O]≤exp(ϵ)⋅Pr[A(
    D 2 )∈O] (5) View Source where the probability is taken over the randomness of
    A . The privacy parameter ϵ called the privacy budget, represents the privacy
    degree offered by the mechanism. In general, a larger perturbation noise is required
    for a smaller ϵ , which implies stronger privacy guarantee but worse utility of
    data set. Definition 2 (Parallel Composability):Generally, when the data set is
    relatively complex, the parallel composability of differential privacy will be
    used to reasonably allocate the privacy protection budget to the algorithm, whose
    goal is to keep the level of privacy protection within the privacy budget and
    ensure the security of data. Assuming that D is a privacy database, which can
    be divided into n disjoint subsets { D 1 , D 2 ,…, D n } . Let { A 1 , A 2 ,…,
    A n } be a series of mutually independent differential privacy algorithms, and
    the corresponding privacy protection budgets of these algorithms are { ϵ 1 , ϵ
    2 ,…, ϵ n } . Then, the combined algorithm A( A 1 ( D 1 ), A 2 ( D 2 ),…, A n
    ( D n )) will satisfy max{ ϵ 1 , ϵ 2 ,…, ϵ n } -differential privacy. C. Chinese
    Remainder Theorem The CRT can uniquely solve any pair of congruences that have
    relatively prime moduli [36], [49]. In our PPMM-DA scheme, we can use CRT to integrate
    each user’s dimension of power consumption data into one composite data, which
    is described in Section V-B. Definition 3 (Chinese Remainder Theorem):Assume that
    { d 1 , d 2 ,…, d l } are l integers and { q 1 , q 2 ,…, q l } are pairwise relatively
    prime positive integers. Then, the system of congruences m≡ d k mod q k for 1≤k≤l
    , has a unique solution m≡ d 1 Q 1 y 1 + d 2 Q 2 y 2 +⋯+ d l Q l y l modQ (6)
    View Source where Q= ∏ l k=1 q k , Q k =(Q/ q k ) , and y k ⋅(Q/ q k )=1mod q
    k for 1≤k≤l . Therefore, l integers { d 1 , d 2 ,…, d l } can be integrated into
    one composite integer m . SECTION V. PPMM-DA Scheme In this section, we expound
    the PPMM-DA scheme, which includes the following phases: system initialization,
    user report generation, privacy-preserving report aggregation, and secure report
    reading, the flowchart of the PPMM-DA scheme is shown in Fig. 2. Then, we apply
    differential privacy to enhanced this scheme to against differential attack in
    the next section. Fig. 2. Flowchart of the PPMM-DA scheme. Show All A. System
    Initialization Similar to [55], the power consumption range is classified into
    s continuous subsets [ R 1 , R 2 ),[ R 2 , R 3 ),…,[ R s , R s+1 ), where R 1
    =0 , R s+1 =E , and m i ∈[ R j , R j+1 )(j∈[1,s]) indicates that m i is equal
    or greater than R j but less than R j+1 , where E denotes the maximum power consumption.
    1) Step 1: The TA selects system parameter κ and two large primes p , q that satisfy
    |p|=|q|=κ , and gets a bilinear mapping tuple ( G 1 , G 2 ,g,N=pq,e) by running
    the algorithm Gen(κ) . The public key of the Paillier encryption system is (N,g)
    and the corresponding private key is (λ,μ) . 2) Step 2: The TA generates a random
    number θ∈ Z N , and calculates θ+ x fn + x 0 =0 mod λ. (7) View Source To share
    the secret key θ with n users under the coverage area of FN, the TA distributes
    subsecret keys to all users with a polynomial function of degree d as follows:
    G(x)=θ+ α 1 x+ α 2 x 2 +⋯+ α d x d (8) View Source through employing Shamir’s
    Secret Sharing [46]. Then, the TA computes G(i) , distributes G(i) to user i as
    his private key and computes user i ’s public key Y i = g G(i) . In the secure
    report reading phase, θ can be recovered when the message is held by d+1(d+1≤n)
    or more participants. Finally, the TA forwards x 0 to CC, sends x fn to FN through
    a secure channel, and computes the public key of FN as Y fn = g x fn . 3) Step
    3: Considering that there are l different kinds of devices in the fog-based SG
    system, which means that user i ’s power consumption data is l dimension { d i1
    , d i2 ,…, d il } , where each dimensional data d il ≤X . To be noted, X denotes
    the maximum value of power consumption data of each dimension. Then, the TA chooses
    l prime numbers { q 1 , q 2 ,…, q l } and computes ⎧ ⎩ ⎨ ⎪ ⎪ ⎪ ⎪ Q= ∏ l k=1 q
    k Q k = Q q k , y k ⋅( Q q k )=1 mod  q k ν k = Q k ⋅ y k . (9) View Source 4)
    Step 4: The TA generates a set of super-increasing sequence { b 1 , b 2 ,…, b
    s } , which satisfies { b i > b 0 + b i−1 ⋅n b 0 >n⋅X⋅ ∑ l k=1 ν k . (10) View
    Source In addition, the TA selects two hash functions H:{0,1 } ∗ → Z ∗ N and H
    1 :{0,1 } ∗ → G 1 , and sends { b 1 , b 2 ,…, b s } to CC. 5) Step 5: For k∈[1,l]
    , the TA publishes parameters { G 1 , G 2 ,e,g,N,H, H 1 , q k , ν k ,( R 1 ,…,
    R s+1 ),( g b 1 ,…, g b s )} . B. User Report Generation In this phase, assuming
    that there are n users in an FN covered area, each user i (i∈[1,n]) collects and
    encrypts l dimensional power consumption data { d i1 , d i2 ,…, d il } , and reports
    them to the corresponding FN periodically, e.g., every 15 min. The specific steps
    are as follows. 1) Step 1: user i periodically collects power consumption data
    { d i1 , d i2 ,…, d il } , and computes m ′ i = d i1 + d i2 +⋯+ d il . (11) View
    Source If user i ’s total power consumption data m ′ i ∈[ R j , R j+1 ), user
    i is denoted to lie in the subset U j . Then, user i integrates these power consumption
    data { d i1 , d i2 ,…, d il } by computing m i = ∑ k=1 l d ik ⋅ ν k . (12) View
    Source Next, user i first computes ϱ i = ∏ n t=1,t≠i (t/[t−i]) and the hash value
    H(T) , where T denotes the current timestamp, then utilizes the private key G(i)
    and g b j to compute ciphertext C i = g m i ⋅ g b j ⋅H(T ) G(i)⋅ ϱ i ⋅N  mod  N
    2 . (13) View Source 2) Step 2: user i generates a signature σ i with G(i) as
    follows: σ i =H ( C i ||RA||T) G(i) (14) View Source where RA represents the residential
    area. 3) Step 3: Finally, user i reports < C i ||RA||T|| σ i > to the corresponding
    FN. C. Privacy-Preserving Report Aggregation After FN receiving n ′ reports <
    C i ||RA||T|| σ i > from user i (i∈[1, n ′ ]) , where n ′ represents the number
    of users working normally. Then, the FN performs the following steps. 1) Step
    1: The FN first checks the timestamp T , then verifies n ′ signatures by e(g,
    ∏ i=1 n ′ σ i )= ∏ i=1 n ′ e( Y i ,H( C i ||RA||T)). (15) View Source 2) Step
    2: After successfully verifying users’ signatures, the FN aggregates all ciphertexts
    to obtain the aggregated ciphertext. If d+1≤ n ′ ≤n , the FN computes C= = ∏ i=1
    n ′ C i ⋅H(T ) x fn N g ∑ n ′ i=1 m i ⋅ g ∑ s j=1 b j | U j | ⋅H(T ) ( ∑ n ′ i=1
    G(i)⋅ ϱ i + x fn )⋅N  mod  N 2 . (16) View Source Note that, if n ′ <d+1 , the
    FN requests that TA randomly selects the private key G(i) of d+1− n ′ users U
    ^ and sends them via a secure channel. Subsequently, the FN calculates ∑ i∈ U
    ^ G(i)⋅ ϱ i and C= = ∏ i=1 n ′ C i ⋅H(T ) ( x fn + ∑ i∈ U ^ G(i)⋅ ϱ i )N g ∑ n
    ′ i=1 m i ⋅ g ∑ s j=1 b j | U j | ⋅H(T ) ( ∑ n ′ i=1 G(i)⋅ ϱ i + x fn + ∑ i∈ U
    ^ G(i)⋅ ϱ i )N  mod  N 2 . (17) View Source 3) Step 3: The FN generates signature
    σ fn with the private key x fn as follows: σ fn =H (C||RA||T) x fn . (18) View
    Source Finally, the FN sends report <C||RA||T|| σ fn > to CC. D. Secure Report
    Reading Upon receiving <C||RA||T|| σ fn > from FN, the CC verifies the integrity
    and source authentication of aggregated ciphertext, uses the Paillier decryption
    algorithm and CRT to process C . The detailed process is as follows: 1) Step 1:
    The CC first examines timestamp T , then computes and verifies signature by e(g,
    σ fn )=e( Y fn ,H(C||RA||T)). (19) View Source 2) Step 2: After successfully verifying
    the signature of FN, the CC uses its private key x 0 to compute C ′ = = = = =
    C⋅H(T ) x 0 ⋅N  mod  N 2 g ∑ n ′ i=1 m i ⋅ g ∑ s j=1 b j | U j | ⋅ ⋅H(T ) ( ∑
    n ′ i=1 G(i)⋅ ϱ i + x fn + x 0 )⋅N  mod  N 2 − → − − − − − − − − − − − − − − −
    − ∑ n ′ i=1 G(i)⋅ ϱ i + x fn + x 0 =0 mod λ g ∑ n ′ i=1 m i ⋅ g ∑ s j=1 b j |
    U j |  mod  N 2 g ∑ l k=1 ( ∑ n ′ i=1 d ik ⋅ ν k ) ⋅ g ∑ s j=1 b j | U j |  mod  N
    2 g V  mod  N 2 (20) View Source where V= ∑ l k=1 ( ∑ n ′ i=1 d ik ⋅ ν k )+( ∑
    s j=1 b j | U j |) . The CC can recover V as follows: V= C ′ −1 N  mod  N 2 .
    (21) View Source 3) Step 3: By executing Algorithm 1, the CC can recover | U j
    |(j∈[1,s]) and M ′ from V , where | U j | denotes the number of users in the j
    th subset, and M ′ = ∑ i=1 n ′ m i  mod Q= ∑ k=1 l ( ∑ i=1 n ′ d ik ⋅ ν k ) mod
    Q. (22) View Source Algorithm 1: Algorithm to Recover the Number of Users in Each
    Subset and M ′ Show All 4) Step 4: Based on the CRT, the CC can obtain M k = =
    = M ′  mod  q k ∑ k=1 l ( ∑ i=1 n ′ d ik ⋅ ν k ) mod  q k ∑ i=1 n ′ d ik ,(k∈[1,l])
    (23) View Source where M k is the sum of power consumption data of the k th dimension
    in the area covered by FN. SECTION VI. Privacy-Enhanced Scheme With Differential
    Privacy Technique In the PPMM-DA scheme, although users’ multidimensional power
    consumption data are encrypted by the Paillier cryptosystem, the adversary still
    can launch differential attack to threaten their privacy. For example, the adversary
    launches two queries on two data sets D k and D ′ k , which represent the k th
    dimensional data sets and these two data sets differing on user i ’s data. Let
    A be a sum aggregation query operation, the corresponding results are A( D k )
    and A( D ′ k )=A( D k )+ d ik , and it is possible for the adversary to gain user
    i ’s k th dimensional power consumption data d ik by computing A( D k )−A( D ′
    k ) . To avoid device-specific power consumption data of individual user leaking,
    we use a differential privacy technique [44] to enhance privacy. Moreover, each
    dimension of power consumption data under the FN coverage area is regarded as
    an independent set, and the power consumption data in different sets have no intersection,
    which satisfies the parallel composability of differential privacy mentioned in
    Section IV-B. In order to realize privacy-enhanced multidimensional data aggregation,
    we propose two methods to achieve differential privacy, which extracted noises
    from the Geometric distribution and Laplace distribution, respectively. A. Geometric
    Distribution Applying the Geometric distribution to generate noises was first
    put forward by Ghosh et al. [50]. Specifically, noises are chosen from a symmetric
    Geometric distribution Geom(α) with 0<α<1 , where α can be seen as a discrete
    approximation of the Laplace distribution Lap( λ ~ ) , i.e., α≈exp(−[1/ λ ~ ])
    . The probability density function of the Geometric distribution Geom(α) is Pr[x]=
    1−α 1+α α |x| . (24) View Source Formally, the sensitivity of aggregation function
    A is △A= max D, D ′ ||A(D)−A( D ′ )| | 1 (25) View Source for all data sets D
    and D ′ differing on at most one element, then by adding geometric noises r 1
    and r 2 , which are randomly chosen from Geom(exp(−ϵ/△A)) , to the original aggregated
    result, the perturbed aggregated result can achieve ϵ -differential privacy, i.e.,
    for any integer O∈Range(A) Pr[A(D)+ r 1 =O]≤exp(ϵ)⋅Pr[A( D ′ )+ r 2 =O]. (26)
    View Source The specific steps, which extracted noise from the Geometric distribution
    to achieve differential privacy, are as follows. 1) Step 1: As shown in the user
    report generation phase, each user i computes C i and σ i . Then, user i sends
    report < C i ||RA||T|| σ i > to FN. 2) Step 2: The FN calculates the aggregated
    data C ~ as follows. Due to A( D k )= ∑ n ′ i=1 d ik , which D k represents the
    k th dimensional data set in the area covered by FN, then |A( D k )−A( D ′ k )|≤X
    holds for any two data sets D k and D ′ k differing on at most one element. Therefore,
    we can set △ A k =X . The FN chooses random noises d ~ k (k∈[1,l]) from Geom(exp(−
    ϵ k /△ A k )) to implicitly add them to C C ~ = = = = C⋅ g ∑ l k=1 ( d ~ k ⋅ ν
    k ) g ∑ n ′ i=1 ( ∑ l k=1 d ik ⋅ ν k )+ ∑ l k=1 ( d ~ k ⋅ ν k ) ⋅ g ∑ s j=1 b
    j | U j | ⋅H(T ) ( ∑ n ′ i=1 G(i) ϱ i + x fn )N  mod  N 2 g ∑ l k=1 (( ∑ n ′ i=1
    d ik + d ~ k )⋅ ν k ) ⋅ g ∑ s j=1 b j | U j | ⋅H(T ) ( ∑ n ′ i=1 G(i) ϱ i + x
    fn )N  mod  N 2 g ∑ l k=1 ( M ~ k ⋅ ν k ) ⋅ g ∑ s j=1 b j | U j | ⋅H(T ) ( ∑ n
    ′ i=1 G(i) ϱ i + x fn )N  mod  N 2 . (27) View Source 3) Step 3: The FN generates
    signature σ fn and returns report < C ~ ||RA||T|| σ fn > to CC. 4) Step 4: Upon
    receiving the report < C ~ ||RA||T|| σ fn > from FN, the CC verifies the signature
    of FN and decrypts the aggregated data C ~ as shown in the secure report reading
    phase. Finally, the CC obtains the sum of power consumption data of each dimension
    after perturbing M ~ k = ∑ n ′ i=1 d ik + d ~ k (k∈[1,l]) and the number of users
    | U 1 |,| U 2 |,…,| U s | in each subset, where M ~ k is the sum of power consumption
    data of k th dimension after perturbing. B. Laplace Distribution The Laplace distribution
    has been widely used to achieve ϵ -differential privacy by adding the Laplace
    noise Lap ( λ ~ ) to the output of a query. The noise Lap ( λ ~ ) is sampled from
    the Laplace distribution, whose probability density function is Pr[x]= 1 2 λ ~
    exp(− |x| λ ~ ). (28) View Source We assume that the differential privacy aggregation
    function A answers a query on two datesets D and D ′ , which are different on
    one single element. Then, we have Pr[A(D)=O] Pr[A( D ′ )=O] = ≤ 1 2 λ ~ exp(−
    |O−A(D)| λ ~ ) 1 2 λ ~ exp(− |O−A( D ′ )| λ ~ ) exp( |A(D)−A( D ′ )| λ ~ )≤exp(
    △A λ ~ ). (29) View Source Here ΔA denotes the global sensitivity of A , which
    is the maximum change of A between two neighboring data sets D and D ′ , that
    is, ΔA= max D≃ D ′ |A(D)−A( D ′ )| , where D≃ D ′ denotes that D and D ′ are neighboring.
    Let ϵ=(ΔA/ λ ~ ) , we have Pr[A(D)∈O]≤exp(ϵ)⋅Pr[A( D ′ )∈O] , i.e., adding the
    Laplace noise Lap( λ ~ ) to a query result for achieving ϵ -differential privacy,
    where λ ~ denotes noise scale and λ ~ =(ΔA/ϵ) . Furthermore, the distribution
    of Lap( λ ~ ) is infinitely divisible. Specially, for every integer ξ≥1 Lap( λ
    ~ )= ∑ i=1 ξ G i (ξ, λ ~ )− ∑ i=1 ξ G ′ i (ξ, λ ~ ) (30) View Source where G i
    (ξ, λ ~ ) and G ′ i (ξ, λ ~ ) are two random variables having a Gamma distribution
    with the probability density function Pr(x,ξ, λ ~ )= 1 λ ~ 1/ξ Γ( 1 ξ ) x 1 ξ
    −1 exp(− x λ ~ ) (31) View Source where x>0 and Γ(1/ξ) is the Gamma function evaluated
    at 1/ξ . In our scheme, A k is an aggregation function which calculates the sum
    of power consumption of k th dimension in the area covered by FN, and Δ A k is
    the maximum power consumption change of A k in the k th dimension. If the number
    of users is n , for each user i , we can add G ik (n, λ ~ k )− G ′ ik (n, λ ~
    k ) to its measurement d ik before reporting, where λ ~ k =(Δ A k / ϵ k ) . Thus,
    the sum of power consumption of k th dimension is ∑ i=1 n d ik + ∑ i=1 n ( G ik
    (n, λ ~ k )− G ′ ik (n, λ ~ k )) = ∑ i=1 n d ik +Lap( λ ~ k )(k∈[1,l]). (32) View
    Source In this way, ϵ k -differential privacy is satisfied. The process of extracting
    noise from the Laplace distribution to achieve differential privacy is as follows.
    1) Step 1: Each user i generates the Laplace noise d ~ ik = G ik ( n ′ , λ ~ k
    )− G ′ ik ( n ′ , λ ~ k )(i∈[1, n ′ ],k∈[1,l]) , which is random value independently
    sampled from the Gamma distribution, and adds this noise to d ik . The perturbed
    power consumption data can be expressed as m ~ i = ∑ l k=1 ( d ik + d ~ ik )⋅
    ν k . Next, the user i computes C i ~ = g m ~ i ⋅ g b j ⋅H(T ) G(i)⋅ ϱ i ⋅N  mod  N
    2 and σ i following the same processes as shown in the user report generation
    phase. Then, the user i reports < C i ~ ||RA||T|| σ i > to FN. 2) Step 2: The
    FN verifies signatures of n ′ users following the same processes as shown in the
    privacy-preserving report aggregation phase. After verifying users’ signatures,
    the FN computes the aggregated data C ~ . Especially, we let d ~ k = ∑ n ′ i=1
    d ~ ik and the FN aggregates all the received ciphertexts C i ~  (i∈[1, n ′ ])
    as follows: C ~ = = = = ∏ i=1 n ′ C ~ i ⋅H(T ) x fn N g ∑ n ′ i=1 ∑ l k=1 ( d
    ik + d ~ ik )⋅ ν k ⋅ g ∑ s j=1 b j | U j | ⋅H(T ) ( ∑ n ′ i=1 G(i)⋅ ϱ i + x fn
    )⋅N  mod  N 2 g ∑ l k=1 (( ∑ n ′ i=1 d ik + d ~ k )⋅ ν k ) ⋅ g ∑ s j=1 b j | U
    j | ⋅H(T ) ( ∑ n ′ i=1 G(i)⋅ ϱ i + x fn )⋅N  mod  N 2 g ∑ l k=1 M k ~ ⋅ ν k ⋅
    g ∑ s j=1 b j | U j | ⋅H(T ) ( ∑ n ′ i=1 G(i)⋅ ϱ i + x fn )⋅N  mod  N 2 . (33)
    View Source 3) Step 3: The FN generates signature σ fn and returns report < C
    ~ ||RA||T|| σ fn > to CC following the same processes as shown in the privacy-preserving
    report aggregation phase. 4) Step 4: Upon receiving the report < C ~ ||RA||T||
    σ fn > from FN, the CC verifies the signature of FN and decrypts the aggregated
    data C ~ following the same processes as shown in the secure report reading phase.
    Finally, the CC obtains the sum of power consumption of each dimension after perturbing
    M k ~ = ∑ n ′ i=1 d ik + d ~ k and the number of users | U 1 |,| U 2 |,…,| U s
    | in each subset. SECTION VII. Correctness and Security Analysis A. Proof of Correctness
    1) Formula (20): With the CC’s private key x 0 as well as the Lagrange interpolation
    polynomial [46], this formula eliminates the term containing H(T) in C . We assume
    that V= ∑ i=1 n ′ m i + ∑ j=1 s b j | U j | (34) View Source and obtain C= g V
    ⋅H(T ) ( ∑ n ′ i=1 G(i)⋅ ϱ i + x fn )⋅N  mod  N 2 . (35) View Source According
    to the Lagrange interpolation polynomial, we have G(x)= ∑ i=1 d+1 ( ∏ t=1,t≠i
    d+1 t−x t−i )G(i). (36) View Source Therefore, when d+1≤ n ′ ≤n ∑ i=1 n ′ G(i)
    ϱ i = ∑ i=1 d+1 ( ∏ t=1,t≠i d+1 t−0 t−i )G(i)=G(0)=θ ∑ i=1 n ′ G(i) ϱ i + x fn
    + x 0 =0 mod λ. (37) (38) View Source We assume θ+ x fn + x 0 =β⋅λ , and then
    we can get C ′ =C⋅H(T ) x 0 ⋅N = = = g V ⋅H(T ) β⋅λ⋅N  mod  N 2 g V ⋅ (H(T ) β
    ) N⋅λ  mod  N 2 − → − − − − r=H(T ) β g V ⋅ r N⋅λ  mod  N 2 . (39) View Source
    Therefore, we have C ′ = g V  mod  N 2 . 2) Formula (21): From the description
    in our preliminaries, we can get C ′ = = = ⟹V= g V  mod  N 2 (1+N ) V  mod  N
    2 1+N⋅V mod  N 2 C ′ −1 N  mod  N 2 . (40) View Source 3) Algorithm 1: This algorithm
    has one for loop, which recovers | U j | and M ′ . Since ∑ i=1 n ′ m i = ∑ i=1
    n ′ ∑ k=1 l d ik ⋅ ν k < ∑ i=1 n ′ ∑ k=1 l X⋅ ν k < b 0 (41) View Source and b
    1 ⋅| U 1 |+ b 2 ⋅| U 2 |+⋯+ b s−1 ⋅| U s−1 | < b s−1 ⋅| U 1 |+ b s−1 ⋅| U 2 |+⋯+
    b s−1 ⋅| U s−1 | ≤ b s−1 ⋅n (42) View Source we can get ∑ i=1 n ′ m i + b 1 ⋅|
    U 1 |+ b 2 ⋅| U 2 |+⋯+ b s−1 ⋅| U s−1 | < b 0 + b s−1 ⋅n< b s . (43) View Source
    Therefore, we can obtain (V−V mod  b s )/ b s =( b s ⋅| U s |)/ b s =| U s | ,
    and by using the same method, we can recover all | U 1 |,| U 2 |,…,| U s | . Then,
    we can obtain M ′ = M k = ∑ i=1 n ′ m i  mod Q= ∑ k=1 l ( ∑ i=1 n ′ d ik ⋅ ν k
    ) mod Q M ′  mod  q k = ∑ i=1 n ′ d ik (k∈[1,l]). (44) (45) View Source B. Security
    Analysis In this section, we analyze and prove that our proposed scheme can achieve
    all security goals, that is, our scheme is secure against the differential attack,
    eavesdropping attack, collusion attack, and active attack. 1) Privacy of Individual
    User’s Power Consumption Data is Protected From Differential Attack: According
    to the threat model mentioned above, the adversary may intend to learn the private
    power consumption data of individual user through differential attack. Our proposed
    scheme can guarantee differential privacy for avoiding leakage of fine-grained
    private power consumption data. On the one hand, with the differential privacy
    technique, we demonstrate that the aggregated data for k th dimension, which adds
    noise extracted from the Geometric distribution, can achieve ϵ k -differential
    privacy. For example, assuming the adversary obtains two perturbed aggregated
    data A( D k )=v+ d ~ k and A( D ′ k )=w+ d ~ ′ k , where v and w are two adjacent
    aggregations, d ~ k and d ~ ′ k are the corresponding geometric noises from Geom(exp(−
    ϵ k /△ A k )) . Since |v−w|≤X , for integer O , we have η= = Pr[v+ d ~ k =O] Pr[w+
    d ~ ′ k =O] = Pr[ d ~ k =O−v] Pr[ d ~ ′ k =O−w] 1−α 1+α ⋅ α |O−v| 1−α 1+α ⋅ α
    |O−w| = α |O−v|−|O−w| . (46) View Source Because −|v−w|≤|O−v|−|O−w|≤|v−w| (47)
    View Source and 0<α<1 , α≈exp(− ϵ k /X) , we can obtain α X ≤ α |v−w| ≤η≤ (exp(−
    ϵ k X )) X ≤η≤ exp(− ϵ k )≤η≤ α −|v−w| ≤ α −X (exp(− ϵ k X )) −X exp( ϵ k ). (48)
    View Source On the other hand, we demonstrate that the aggregated data for k th
    dimension, which adds noise extracted from the Laplace distribution, can also
    achieve ϵ k -differential privacy. For example, assuming the adversary obtains
    two perturbed aggregated data A( D k )=v+ d ~ k and A( D ′ k )=w+ d ~ ′ k , where
    v and w are two adjacent aggregations, d ~ k and d ~ ′ k are the corresponding
    noises from the Laplace distribution with the scale of λ ~ k =(Δ A k / ϵ k ) .
    Therefore, we have Pr[A( D k )=O] Pr[A( D ′ k )=O] = = ≤ 1 2 λ ~ k exp(− |O−A(
    D k )| λ ~ k ) 1 2 λ ~ k exp(− |O−A( D ′ k )| λ ~ k ) exp( |A( D k )−A( D ′ k
    )| λ ~ k ) exp( Δ A k λ ~ k )=exp( ϵ k ). (49) View Source In both methods, converting
    the perturbed power consumption data into ciphertext C i ~ can also satisfy ϵ
    k -differential privacy. According to the differential privacy axiom proposed
    by Kifer and Lin in [47], differential invariance is defined: transformation invariance,
    which shows data set that satisfies ϵ -differential privacy can satisfy ϵ -differential
    privacy after encryption. Therefore, M ~ k achieves ϵ k -differential privacy
    in these two methods. Additionally, we regard each dimension of power consumption
    data as an independent set, and the power consumption data in different sets have
    no intersection, which satisfies the parallel composability of differential privacy
    mentioned in Section IV. Consequently, the perturbed aggregated result of multidimensional
    data after adding noise satisfies max{ ϵ 1 , ϵ 2 ,…, ϵ l } -differential privacy.
    2) Privacy of Individual User’s Power Consumption Data is Protected From Eavesdropping
    Attack: The adversary may obtain the private power consumption data of individual
    user by eavesdropping on communication links, such as the communication links
    from users to FN or from FN to CC. Due to users’ private power consumption data
    are transmitted on each communication link in ciphertext, even if the adversary
    obtains C i or C , he cannot obtain private power consumption data of individual
    user. On the one hand, we consider an adversary who has obtained a report by eavesdropping
    on the communication between users and FN and obtains < C i ||RA||T|| σ i > ,
    where C i = g m i ⋅ g b j ⋅H(T ) G(i)⋅ ϱ i ⋅N  mod  N 2 . Let m= m i + b j and
    r i =H(T ) G( x i )⋅ ϱ i , then ciphertext C i = g m ⋅ r N i  mod  N 2 is still
    a legal ciphertext of the Paillier cryptosystem. Since Paillier cryptosystem is
    indistinguishable under the chosen plaintext attack (IND-CPA) secure, the adversary
    cannot decrypt user i ’s ciphertext C i and get his private data m i , let alone
    obtain user i ’s fine-grained data, such as the power consumption data of k th
    dimension d ik . On the other hand, we consider that the adversary has obtained
    the report by eavesdropping on the communication from FN to CC. However, he can
    only get the aggregated data and ciphertexts of all users’ power consumption data.
    Similarly, the FN aggregates these ciphertexts and computes the aggregated ciphertext
    C , which has the same form as individual report C i . Since C is still a valid
    ciphertext of the Paillier cryptosystem, and ∑ n ′ i=1 m i and d ik are transparent
    to the adversary. Therefore, our proposed scheme can protect the private power
    consumption data of individual user from eavesdropping attack. 3) Privacy of Individual
    User’s Power Consumption Data is Protected From Collusion Attack: On the one hand,
    if the FN colludes with some users, they can obtain their secret parameters {G(i),
    ϱ i } . In our scheme, the TA employs Shamir’s Secret Sharing and computes a polynomial
    function of degree d , which is G(x)=θ+ α 1 x+ α 2 x 2 +⋯+ α d x d (50) View Source
    and then divides θ into n shares G(i) to n users as their private keys, and any
    d users or fewer than d users cannot obtain θ . Supposing an extreme situation
    occurs, the adversary successfully compromises d+1 users and obtains their corresponding
    private keys {G(1),G(2),…,G(d+1)} . However, due to θ+ x fn + x 0 =0 mod λ (51)
    View Source and private key x 0 is kept secretly by CC, even if the FN colludes
    with d+1 users, x 0 cannot be recovered. Therefore, we can conclude that no matter
    how many users collude with FN, they cannot obtain the private data of other users.
    On the other hand, the private key of the Paillier cryptosystem λ is kept secretly
    by TA, and even if the CC colludes with FN or some users by sharing and analyzing
    their information (such as ciphertexts, private keys, and public information),
    they cannot obtain the private data of other users. Hence, we can conclude that
    our proposed scheme can protect the privacy of individual user’s power consumption
    data from collusion attack. 4) Privacy of Individual User’s Power Consumption
    Data is Protected From Active Attack: It is important to protect users’ private
    data from active attack, such as message forgery and replay attack. We will elaborate
    that the report can be authenticated by FN and CC, which is indeed sent by a legitimate
    entity and cannot be changed during transmission. Source Authentication: The FN
    needs to verify the identity of user before aggregating the data to ensure that
    the report was sent by the legitimate user who has not been tampered with. Similarly,
    the CC must perform identity verification before decrypting data. Therefore, our
    scheme can guarantee that the received message actually come from the legitimate
    entity and resist message forgery attack. Data Integrity and Replay Attack. On
    the one hand, we consider the communication link from IoT layer to Fog layer.
    When user i sends a report < C i ||RA||T|| σ i > to nearby FN, the FN verifies
    if e(g, σ i )=e( Y i ,H( C i ||RA||T)) holds. Only legitimate user i can generate
    valid ciphertext C i = g m i ⋅ g b j ⋅H(T ) G(i)⋅ ϱ i ⋅N  mod  N 2 and signature
    σ i =H( C i ||RA||T ) G(i) by G(i) and timestamp T . At the same time, an external
    adversary cannot make any modifications on the encrypted data C i and report <
    C i ||RA||T|| σ i > , which can be guaranteed by FN to detect whether the report
    has been tampered during transmission or not based on the equation e(g, σ i )=e(
    Y i ,H( C i ||RA||T)) . On the other hand, the data integrity of the report from
    FN to CC can also be achieved. The FN generates signature σ fn =H(C||RA||T ) x
    fn with x fn and timestamp T . In addition, the T is used in the report <C||RA||T||
    σ fn > . After the FN and CC receive the report, they check T to detect replay
    attack. Therefore, our scheme can satisfy data integrity and resist message replay
    attack. SECTION VIII. Performance Evaluation In this section, we compare our proposed
    scheme with [13], [17], [24], [25], [29], [31], [51], [52], [53], [54], and [55]
    from functions, computational and communication overheads, and errors in the fog-based
    SG system. A. Functional Comparison The functional comparison is shown in Table
    II. As shown in Table II, our proposed scheme achieves multidimensional and multisubset
    data aggregation, supports fault tolerance, resists differential attack, eavesdropping
    attack, collusion attack, and active attack. The scheme [13] can against eavesdropping
    attack and active attack using the Boneh–Goh–Nissim public key cryptography. In
    the scheme [17], the CC can only obtain aggregated results of multidimensional
    data. The scheme [31] combines differential privacy technology to resist differential
    attack for 1-D data, realizes fault tolerance, and resists eavesdropping attack.
    The scheme [52] can against both differential and active attacks with a differential
    privacy mechanism. The scheme [29] achieves fault tolerance and resists differential
    attack via randomized responses. However, these schemes [29], [31], [52] can only
    obtain aggregated result of 1-D data and cannot perform multisubset data aggregation.
    Some schemes [51], [53], [54] realize privacy-preserving multidimensional data
    aggregation via differential privacy and resisted differential attack. However,
    these schemes [51], [53], [54] cannot resist eavesdropping attack, collusion attack,
    and active attack. In schemes [24], [25], the CC can perform the multisubset data
    aggregation, but SMs only report one type of data to CC. Also, they cannot resist
    differential attack. Zuo et al. [55] came up with a privacy-preserving multidimensional
    and multisubset data aggregation for addressing the collusion attack. However,
    this scheme also cannot resist differential attack. TABLE II Functional Comparison
    B. Comparison of Computational Overhead In this section, we compare the computational
    overhead of our proposed scheme with these of [17], [24], [25], and [55] in terms
    of each user, GW, CC, and the total. As authentication is not considered in schemes
    [24], [25], we will not discuss the computational overhead of signature here.
    Let T e be the time of exponential operation in Z N 2 , and T mul be the time
    of multiplication operation in Z N 2 . Our experiment is conducted on a laptop
    with 64-bits Windows 10 Enterprise operating system, the Intel Core i7-4510U CPU
    and 8-GB memory. The experiment results show that T e =1.7 ms and T mul =0.16
    ms. In addition, we assume that there are n users and each user’s power comsumption
    data is l dimension. The comparison of computational overhead for [17], [24],
    [25], and [55] and ours is depicted in Table III. TABLE III Comparison of Computational
    Overhead Chen et al.’s Scheme [17]: In the user report generation phase, each
    user requires (l+1) T e +l T mul to generate the ciphertext. In the privacy-preserving
    report aggregation phase, GW requires (n−1) T mul to generate the aggregated ciphertext.
    In the secure report reading phase, the CC requires T e to decrypt the aggregated
    data. Thus, the computational overhead of scheme [17] is (nl+n+1) T e +(nl+n−1)
    T mul in total. Li et al.’s Scheme [24]: In the user report generation phase,
    each user requires 2 T e + T mul to generate the ciphertext. In the privacy-preserving
    report aggregation phase, the GW requires (n−1) T mul to generate the aggregated
    ciphertext. In the secure report reading phase, the CC requires T e + T mul to
    decrypt the aggregated data. Therefore, the computational overhead of scheme [24]
    is (2n+1) T e +2n T mul in total. Chien et al.’s Scheme [25]: In the user report
    generation phase, each user requires 4 T e +2 T mul to generate the ciphertext.
    In the privacy-preserving report aggregation phase, the GW requires 2(n−1) T mul
    to generate the aggregated ciphertext. In the secure report reading phase, the
    CC requires 2 T e to decrypt the aggregated data. Therefore, the computational
    overhead of scheme [25] is (4n+2) T e +(4n−2) T mul in total. Zuo et al.’s Scheme
    [55]: In the user report generation phase, each user requires 3 T e + T mul to
    generate the ciphertext. In the privacy-preserving report aggregation phase, the
    GW requires 2(n−1) T mul to generate the aggregated ciphertext. In the secure
    report reading phase, the CC requires n T e + T mul to decrypt the aggregated
    data. Therefore, the computational overhead of scheme [55] is 4n T e +(3n−1) T
    mul in total. Our Proposed Scheme: In the user report generation phase, each user
    requires 2 T e + T mul to generate the ciphertext. Particularly, we structure
    multidimensional data as a composite data, which can reduce the computational
    overhead of encryption significantly. In the privacy-preserving report aggregation
    phase, the GW requires (n−1) T mul to generate the aggregated ciphertext. In the
    secure report reading phase, the CC requires T e + T mul to decrypt the aggregated
    data. Therefore, the computational overhead of our proposed scheme is (2n+1) T
    e +2n T mul in total. On the one hand, we compare our proposed scheme with [17],
    [24], [25], and [55] for different numbers of dimensions l in Fig. 3. Specifically,
    we vary the dimension of power consumption data from {5,10,15,20,25,30,35,40,45,50}
    and assume there are 100 users in the system. From Fig. 3, we can find that the
    computational overhead of scheme [17] increases linearly with l increases, while
    that of our proposed scheme is independent of l in the user report generation
    phase. Fig. 3. Variation of computational overheads for different number of dimensions
    when n=100 . (a) Each user. (b) GW. (c) CC. (d) Total. Show All On the other hand,
    we compare our proposed scheme with [17], [24], [25], and [55] for different user
    numbers n in Fig. 4. Specifically, we vary the number of users from {100,200,300,400,500,600,700,800,900,1000}
    and assume each user’s power consumption data is ten dimensions. Fig. 4 demonstrates
    that the computational overhead of our proposed scheme is lower than these of
    [17], [25], and [55]. Fig. 4. Variation of computational overheads for different
    numbers of users when l=10 . (a) Each user. (b) GW. (c) CC. (d) Total. Show All
    C. Comparison of Communication Overhead The communication overhead is closely
    related to the size of messages transmitted between entities. We utilize the Paillier
    encryption algorithm to encrypt users’ power consumption data and use the bilinear
    aggregate signature [48] to realize authentication between entities. Consequently,
    the size of a ciphertext is 2048 bits, if we choose the security parameter κ=1024
    bits. Chen et al.’s Scheme [17]: In the user report generation phase, user i generates
    a ciphertext C i and sends it to FN, where C i ∈ Z N 2 . Therefore, the communication
    overhead from user i to FN is S i =2048 bits, and the communication overhead from
    FN to CC is S FN =2048 bits in the privacy-preserving report aggregation phase.
    Li et al.’s Scheme [24]: In the user report generation phase, user i generates
    a ciphertext C i and sends it to FN, where C i ∈ Z N 2 . Therefore, the communication
    overhead from user i to FN is S i =2048 bits, and the communication overhead from
    FN to CC is also S FN =2048 bits in the privacy-preserving report aggregation
    phase. Chien et al.’s Scheme [25]: user i generates two ciphertexts C 1 i ,C 2
    i ∈ Z N 2 and sends them to FN in the user report generation phase. Therefore,
    the communication overhead from user i to FN is S i =4096 bits, and the communication
    overhead from FN to CC is also S FN =4096 bits in the privacy-preserving report
    aggregation phase. Zuo et al.’s Scheme [55]: user i generates two ciphertexts
    C a i , C b i ∈ Z N 2 and sends them to FN in the user report generation phase.
    Therefore, the communication overhead from user i to FN is S i =4096 bits, and
    the communication overhead from FN to CC is also S FN =4096 bits in the privacy-preserving
    report aggregation phase. Our Proposed Scheme: user i generates a ciphertext C
    i and sends it to nearby FN in the user report generation phase, where C i ∈ Z
    N 2 . Therefore, the size of user i ’s report is calculated as S i =2048 bits.
    Subsequently, in the privacy-preserving report aggregation phase, the aggregated
    result C is sent to CC. As a result, the size of FN’s report is calculated as
    S FN =2048 bits. From the above comparison, we can draw the conclusion that our
    proposed scheme achieves PPMM-DA in lower communication overhead than [25], [55].
    D. Comparison of Error We use the relative error to analyze the error caused by
    extracting noise from the Geometric distribution or Laplace distribution. This
    error can be measured by comparing the difference between the original aggregated
    data M k and the perturbed aggregated data M ~ k . The mathematical expectation
    of relative error is calculated as E(error)=([E| M ~ k − M k |]/ M k ) . We take
    the k th dimensional power consumption of a user as an example, and set the failure
    rate of SMs is 0.8%. The error analysis is as follows. 1) Geometric Distribution
    Error Analysis: In the privacy-enhanced scheme, we add noise d ~ k extracted from
    the Geometric distribution to the original aggregated result M k in the k th dimension.
    Then, the perturbed aggregated data after adding noise is M k + d ~ k , denoted
    as M ~ k . According to the mathematical expectation of relative error, we can
    obtain E ˜ ( ζ k )= E| M ~ k − M k | M k = E| d ~ k | M k (52) View Source where
    d ~ k ∼Geom(exp(− ϵ k /Δ A k )) and E| d ~ k |= = = = = = ∑ d ~ k =−∞ ∞ | d ~
    k |⋅Pr[ d ~ k ] ∑ d ~ k =−∞ ∞ | d ~ k |⋅ 1−α 1+α α d ~ k 2 1+α ⋅ ∑ d ~ k =1 ∞
    d ~ k (1−α)⋅ α d ~ k 2 1+α ⋅ ⎛ ⎝ ∑ d ~ k =1 ∞ d ~ k ⋅ α d ~ k − ∑ d ~ k =1 ∞ d
    ~ k ⋅ α d ~ k +1 ⎞ ⎠ 2 1+α ⋅ ∑ d ~ k =1 ∞ α d ~ k = 2 1+α ⋅ α 1−α 2α 1− α 2 .(∵0<α<1).
    (53) View Source As α=exp(− ϵ k /Δ A k ) , therefore, the mathematical expectation
    of relative error of the Geometric distribution is E ˜ ( ζ k )= E| d ~ k | M k
    = 2exp(− ϵ k Δ A k ) M k (1−exp(− 2 ϵ k Δ A k )) . (54) View Source 2) Laplace
    Distribution Error Analysis: In the privacy-enhanced scheme, we add noise d ~
    ik extracted from the Laplace distribution to d ik . Then, the perturbed aggregated
    data after adding noise is M k + d ~ k , denoted as M ~ k . According to the mathematical
    expectation of relative error, we can obtain E ˜ ( ζ k )= E| M ~ k − M k | M k
    = E| d ~ k | M k (55) View Source where d ~ k ∼Lap(Δ A k / ϵ k ) and E| d ~ k
    |= = = 2 ∫ +∞ 0 d ~ k ⋅Lap( Δ A k ϵ k )d( d ~ k ) 2 ∫ +∞ 0 d ~ k ⋅ ϵ k 2Δ A k
    ⋅ e − d ~ k ⋅ ϵ k Δ A k d( d ~ k ) Δ A k ϵ k . (56) View Source Therefore, the
    mathematical expectation of relative error of the Laplace distribution is E ˜
    ( ζ k )= E| d ~ k | M k = Δ A k ϵ k ⋅ M k . (57) View Source 3) Comparison of
    Utility: The utility of the privacy-preserving data aggregation scheme with differential
    privacy is mainly affected by the relative error of the aggregated result. Therefore,
    from the perspective of utility of the privacy-preserving data aggregation scheme,
    we conduct experiment to evaluate and compare the utility here. On the one hand,
    we compare relative errors of the Geometric distribution with these of Laplace
    distribution under different ϵ k . We extract noises from the Geometric distribution
    and Laplace distribution, respectively, and add them to the real power consumption
    data in the fog-based SG. Specifically, we vary the privacy budgets of the k th
    dimension ϵ k from {0.1,0.2,0.3} , and vary the number of users n from {1000,
    2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000} . The comparison of relative
    errors on the Geometric distribution and Laplace distribution under different
    ϵ k are depicted in Fig. 5. As shown in Fig. 5, when the number of users is n=5000
    , the privacy budget is ϵ k =0.2 , the relative error of privacy-preserving data
    aggregation scheme which extracted noise from the Geometric distribution is 0.192307564%,
    and the relative error of privacy-preserving data aggregation scheme which extracted
    noise from the Laplace distribution is 0.192307692%. The relative errors of these
    two methods are very close and are difficult to distinguish. At the same time,
    we can find that a higher level of privacy needs sacrifice more data utility as
    a cost. That is, for the same number of users n , the increase of privacy-preserving
    intensity will introduce more noise resulting in a greater loss of data utility.
    Fig. 5. Comparison of relative errors on Geometric distribution and Laplace distribution
    under different ϵ k . Show All On the other hand, we compare our proposed scheme
    with the state-of-the-art schemes [29], [51], [52], [53], [54] in term of relative
    error. We set ϵ k =0.2 and vary the number of users n from {1000, 2000, 3000,
    4000, 5000, 6000, 7000, 8000, 9000, 10000} . The comparison of relative errors
    for schemes [29], [51], [52], [53], [54] and ours when ϵ k =0.2 are depicted in
    Fig. 6. According to Fig. 6, when the number of users n increases, the relative
    error will decrease while the data utility will grow, and the relative error is
    kept within 0.2% when n≥5000 . Therefore, we can draw the conclusion that our
    proposed scheme can maintain higher data utility under the same privacy budget,
    which means that our proposed scheme introduces lower noises while resisting differential
    attack. Fig. 6. Comparison of relative errors for schemes [29], [51], [52], [53],
    [54] and ours when ϵ k =0.2 . Show All SECTION IX. Conclusion In this article,
    for fog-based SGs, we proposed a PPMM-DA scheme with differential privacy. Even
    if the user’s power consumption data is multidimensional, users can also be divided
    into different subsets according to their power consumption. As a result, the
    CC can perform fine-grained analysis on user’s power consumption data. Security
    analysis demonstrates that our proposed scheme can resist differential attack,
    eavesdropping attack, collusion attack, and active attack. The experiment results
    show that our proposed scheme is more efficient at computational overhead and
    communication overhead. In future, we will study tariff problem [59], general
    transactive energy (TE) retailing problem [60], user access control using lightweight
    face verification [61], trusted entities problem by leveraging the trusted execution
    environment (TEE) [62] to meet the requirements of fair pricing, intelligent electronic
    devices, and user trust. ACKNOWLEDGMENT The authors sincerely thank the editors
    and all the anonymous reviewers for their valuable comments which have helped
    to enhance the quality of this article. This research was supported in part by
    the advanced computing resources provided by the Supercomputing Center of Hangzhou
    City University. Authors Figures References Keywords Metrics More Like This Lightweight
    Multidimensional Encrypted Data Aggregation Scheme With Fault Tolerance for Fog-Assisted
    Smart Grids IEEE Systems Journal Published: 2022 A New Differentially Private
    Data Aggregation With Fault Tolerance for Smart Grid Communications IEEE Internet
    of Things Journal Published: 2015 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'PPMM-DA: Privacy-Preserving Multidimensional and Multisubset Data Aggregation
    With Differential Privacy for Fog-Based Smart Grids'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Goyal S.B.
  - Rajawat A.S.
  - Kumar M.
  - Agarwal P.
  citation_count: '0'
  description: 'INTRODUCTION: Cloud computing''s offshoot, fog computing, moves crucial
    data storage, processing, and networking capabilities closer to the people who
    need them. There are certain advantages, such improved efficiency and lower latency,
    but there are also some major privacy and security concerns. For these reasons,
    this article presents a new paradigm for fog computing that makes use of blockchain
    and Artificial Intelligence (AI). OBJECTIVES: The main goal of this research is
    to create and assess a thorough framework for fog computing that incorporates
    AI and blockchain technology. With an emphasis on protecting the privacy and integrity
    of data transactions and streamlining the management of massive amounts of data,
    this project seeks to improve the security and privacy of Industrial Internet
    of Things (IIoT) systems that are cloud-based. METHODS: Social network analysis
    methods are utilised in this study. The efficiency and accuracy of data processing
    in fog computing are guaranteed by the application of artificial intelligence,
    most especially Support Vector Machine (SVM), due to its resilience in classification
    and regression tasks. The network''s security and reliability are enhanced by
    incorporating blockchain technology, which creates a decentralised system that
    is tamper resistant. To make users'' data more private, zero-knowledge proof techniques
    are used to confirm ownership of data without actually disclosing it. RESULTS:
    When applied to fog computing data, the suggested approach achieves a remarkable
    classification accuracy of 99.8 percent. While the consensus decision-making process
    of the blockchain guarantees trustworthy and secure operations, the support vector
    machine (SVM) efficiently handles massive data analyses. Even in delicate situations,
    the zero-knowledge proof techniques manage to keep data private. When these technologies
    are integrated into the fog computing ecosystem, the chances of data breaches
    and illegal access are greatly reduced. CONCLUSION: Fog computing, which combines
    AI with blockchain, offers a powerful answer to the privacy and security issues
    with cloud centric IIoT systems. Combining SVM with AI makes data processing more
    efficient, while blockchain''s decentralised and immutable properties make it
    a strong security measure. Additional security for user privacy is provided via
    zero-knowledge proofs. Improving the privacy and security of fog computing networks
    has never been easier than with this novel method.'
  doi: 10.4108/eetiot.5555
  full_citation: '>'
  full_text: '>

    "EAI Endorsed Transactions on Internet of Things Home About Current Archives Special
    Issues Publication Ethics Announcements Search Register Login Home / Archives
    / Vol. 10 (2024): EAI Endorsed Transactions on Internet of Things / Research article
    Leveraging AI and Blockchain for Privacy Preservation and Security in Fog Computing
    S B Goyal City University image/svg+xml Anand Singh Rajawat Sandip University
    Manoj Kumar University of Wollongong in Dubai image/svg+xml Prerna Agarwal Bennett
    University image/svg+xml DOI: https://doi.org/10.4108/eetiot.5555 Keywords: Artificial
    Intelligence, Fog computing, Privacy Preservation Model, Cloud Computing Abstract
    INTRODUCTION: Cloud computing''s offshoot, fog computing, moves crucial data storage,
    processing, and networking capabilities closer to the people who need them. There
    are certain advantages, such improved efficiency and lower latency, but there
    are also some major privacy and security concerns. For these reasons, this article
    presents a new paradigm for fog computing that makes use of blockchain and Artificial
    Intelligence (AI). OBJECTIVES: The main goal of this research is to create and
    assess a thorough framework for fog computing that incorporates AI and blockchain
    technology. With an emphasis on protecting the privacy and integrity of data transactions
    and streamlining the management of massive amounts of data, this project seeks
    to improve the security and privacy of Industrial Internet of Things (IIoT) systems
    that are cloud-based. METHODS: Social network analysis methods are utilised in
    this study. The efficiency and accuracy of data processing in fog computing are
    guaranteed by the application of artificial intelligence, most especially Support
    Vector Machine (SVM), due to its resilience in classification and regression tasks.
    The network''s security and reliability are enhanced by incorporating blockchain
    technology, which creates a decentralised system that is tamper resistant. To
    make users'' data more private, zero-knowledge proof techniques are used to confirm
    ownership of data without actually disclosing it.  RESULTS: When applied to fog
    computing data, the suggested approach achieves a remarkable classification accuracy
    of 99.8 percent. While the consensus decision-making process of the blockchain
    guarantees trustworthy and secure operations, the support vector machine (SVM)
    efficiently handles massive data analyses. Even in delicate situations, the zero-knowledge
    proof techniques manage to keep data private. When these technologies are integrated
    into the fog computing ecosystem, the chances of data breaches and illegal access
    are greatly reduced. CONCLUSION: Fog computing, which combines AI with blockchain,
    offers a powerful answer to the privacy and security issues with cloud centric
    IIoT systems. Combining SVM with AI makes data processing more efficient, while
    blockchain''s decentralised and immutable properties make it a strong security
    measure. Additional security for user privacy is provided via zero-knowledge proofs.
    Improving the privacy and security of fog computing networks has never been easier
    than with this novel method. Downloads <br data-mce-bogus=\"1\"> <br data-mce-bogus=\"1\">
    References Ferrag, M.A., Derhab, A., Maglaras, L., Mukherjee, M., Janicke, H.:
    Privacy-preserving Schemes for Fog-based IoT Applications: Threat models, Solutions,
    and Challenges. 2018 International Conference on Smart Communications in Network
    Technologies (SaCoNeT) pp. 37–42 (2018) Gowda, N.C., Manvi, S.S., M, B.: Blockchain-based
    Access Control Model with Privacy preservation in a Fog Computing Environment.
    2022 IEEE International Conference on Elec- tronics, Computing and Communication
    Technologies (CONECCT) pp. 1–6 (2022) Chen, S., Zhu, X., Zhang, H., Zhao, C.,
    Yang, G., Wang, K.: Efficient Privacy Preserving Data Collection and Computation
    Offloading for Fog-Assisted IoT. IEEE Transactions on Sustainable Computing 5,
    526–540 (2020) Lai, C., Li, Q., Zhou, H., Zheng, D.: SRSP: A Secure and Reliable
    Smart Parking Scheme With Dual Privacy Preservation. IEEE Internet of Things Journal
    8(13), 10619–10630 (2021) Zhonghua, C., Goyal, S.B., Rajawat, A.S.: Smart contracts
    attribute-based access control model for security & privacy of IoT system using
    blockchain and edge computing. J Super- comput (2023) Huynh-The, T., Gadekallu,
    T.R., Wang, W., Yenduri, G., Ranaweera, P., Pham, Q.V., Costa, D.B.D., Liyanage,
    M.: Blockchain for the metaverse: A Review. Future Generation Com- puter Systems
    143, 401–419 (2023) Huynh-The, T., Gadekallu, T.R., Wang, W., Yenduri, G., Ranaweera,
    P., Pham, Q.V., Costa, D., Liyanage, M.: Blockchain for the metaverse: A Review.
    Future Generation Com- puter Systems 143, 401–419 (2023) Rajawat, A.S.: Blockchain-based
    Security Framework for Metaverse: A Decentralized Ap- proach. In: 2023 15th International
    Conference on Electronics, Computers and Artificial Intelligence (ECAI). pp. 1–06
    (2023) Pundir, S., Wazid, M., Singh, D.P., Das, A.K., Rodrigues, J.J.P.C., Park,
    Y.: Intrusion De- tection Protocols in Wireless Sensor Networks Integrated to
    Internet of Things Deployment: Survey and Future Challenges. IEEE Access 8, 3343–3363
    (2020) Luong, T.D.: FedChain: A Collaborative Framework for Building Artificial
    Intelligence Models using Blockchain and Federated Learning. 2021 8th NAFOSTED
    Conference on Information and Computer Science (NICS) pp. 149–154 (2021) Rajawat,
    A.S., Goyal, S.B., Bedi, P., Verma, C., Ionete, E.I., Raboaca, M.: https://doi.org/10.
    3390/math11030679 Zerka, F.: Blockchain for Privacy Preserving and Trustworthy
    Distributed Machine Learning in Multicentric Medical Imaging (C-DistriM). IEEE
    Access 8, 183939–183951 (2020) Dave, M., Rastogi, V., Miglani, M.: Smart Fog-Based
    Video Surveillance with Privacy Preservation based on Blockchain. Wireless Pers
    Commun 124, 1677–1694 (2022) Alzoubi, Y.I., Gill, A., Mishra, A.: A systematic
    review of the purposes of Blockchain and fog computing integration: classification
    and open issues. J Cloud Comp 11, 80–80 (2022) Shah, K., Chadotra, S., Tanwar,
    S.: Blockchain for IoV in 6G environment: review solutions and challenges. Cluster
    Comput 25 (1927) Li, W., Wu, J., Cao, J.: Blockchain-based trust management in
    cloud computing systems: a taxonomy, review and future directions. J Cloud Comp
    10, 35–35 (2021) Krishnamoorthy, S., Dua, A., Gupta, S.: Role of emerging technologies
    in future IoT-driven Healthcare 4.0 technologies: a survey, current challenges
    and future directions. J Ambient Intell Human Comput (2021) 18. Amiri, Z., Heidari,
    A., Navimipour, N.J.: (2022), https://doi.org/10.1007/s10586-022- 03738-5 Singh,
    A., Satapathy, S.C., Roy, A.: AI-Based Mobile Edge Computing for IoT: Applications,
    Challenges, and Future Scope. Arab J Sci Eng 47, 9801–9831 (2022) Bagga, P., Das,
    A.K., Chamola, V.: Blockchain-envisioned access control for internet of things
    applications: a comprehensive survey and future directions. Telecommun Syst 81,
    125–173 (2022) Alfa, A.A., Alhassan, J.K., Olaniyi, O.M.: Blockchain technology
    in IoT systems: current trends, methodology, problems, applications, and future
    directions. J Reliable Intell Environ 7, 115–143 (2021) Bhushan, B., Sahoo, C.,
    Sinha, P.: Unification of Blockchain and Internet of Things (BIoT): requirements,
    working model, challenges and future directions. Wireless Netw 27, 55–90 (2021)
    Alagheband, M.R., Mashatan, A.: Advanced encryption schemes in multi-tier heterogeneous
    internet of things: taxonomy, capabilities, and objectives. J Supercomput 78,
    18777–18824 (2022) Rejeb, A., Rejeb, K., Simske, S.J.: Blockchain technology in
    the smart city: a bibliometric review. Qual Quant 56, 2875–2906 (2022) Wang, C.,
    Cheng, X., Li, J.: A survey: applications of blockchain in the Internet of Vehicles.
    J Wireless Com Network 2021, 77–77 (2021) Shafay, M., Ahmad, R.W., Salah, K.:
    Blockchain for deep learning: review and open chal- lenges. Cluster Comput (2022)
    Himeur, Y., Elnour, M., Fadli, F.: AI-big data analytics for building automation
    and manage- ment systems: a survey, actual challenges and future perspectives.
    Artif Intell Rev (2022) Li, D., Han, D., Weng, T.H.: Blockchain for federated
    learning toward secure distributed machine learning systems: a systemic survey.
    Soft Comput 26, 4423–4440 (2022) Elrahman, S.A., Alluhaidan, A.S.: Blockchain
    technology and IoT-edge framework for shar- ing healthcare services. Soft Comput
    25, 13753–13777 (2021) Jiang, M., Qin, X.: Distributed ledger technologies in
    vehicular mobile edge computing: a survey. Complex Intell. Syst 8, 4403–4419 (2022)
    Su, W., Li, L., Liu, F.: AI on the edge: a comprehensive review. Artif Intell
    Rev 55, 6125– 6183 (2022) Selvarajan, S., Srivastava, G., Khadidos, A.O.: An artificial
    intelligence lightweight blockchain security model for security and privacy in
    IIoT systems. J Cloud Comp 12, 38–38 (2023) Zubaydi, H.D., Varga, P., Molnár,
    S.: Leveraging Blockchain Technology for Ensuring Se- curity and Privacy Aspects
    in Internet of Things: A Systematic Literature Review. Sensors 23, 788–788 (2023)
    Sameera, K.M., Vinod, P., Rehiman, K.A.R., Jifhna, P., Sebastian, S.: Blockchain
    Feder- ated Learning Framework for Privacy-Preservation. In: Rajagopal, S., Faruki,
    P., Popat, K. (eds.) Advancements in Smart Computing and Information Security.
    ASCIS 2022. vol. 1760. Springer (2022) PDF Published 26-03-2024 How to Cite S.
    B. Goyal, A. S. Rajawat, M. Kumar, and P. Agarwal, “Leveraging AI and Blockchain
    for Privacy Preservation and Security in Fog Computing”, EAI Endorsed Trans IoT,
    vol. 10, Mar. 2024. More Citation Formats Issue Vol. 10 (2024): EAI Endorsed Transactions
    on Internet of Things Section Research article License Copyright (c) 2024 EAI
    Endorsed Transactions on Internet of Things This work is licensed under a Creative
    Commons Attribution 3.0 Unported License. This is an open-access article distributed
    under the terms of the Creative Commons Attribution CC BY 3.0 license, which permits
    unlimited use, distribution, and reproduction in any medium so long as the original
    work is properly cited. Most read articles by the same author(s) Anand Singh Rajawat,
    S B Goyal, Manoj Kumar, Thipendra P Singh, An AI-Enabled Blockchain Algorithm:
    A Novel Approach to Counteract Blockchain Network Security Attacks , EAI Endorsed
    Transactions on Internet of Things: Vol. 10 (2024): EAI Endorsed Transactions
    on Internet of Things Preeti Sharma, Manoj Kumar, Hitesh Kumar Sharma, Robust
    GAN-Based CNN Model as Generative AI Application for Deepfake Detection , EAI
    Endorsed Transactions on Internet of Things: Vol. 10 (2024): EAI Endorsed Transactions
    on Internet of Things Make a Submission Current Issue Keywords deep learning wireless
    sensor network network on chip real-time simulation publish/subscribe vehicular
    platform wearables parkinson eeg emotiv early warning score sip dc rejection vehicular
    networks safety beaconing ieee 802.11p wnsns nanosensors yarns European Alliance
    for Innovation (EAI) is a non-profit organization and a professional community
    empowering global research and innovation, promoting cooperation between European
    and International ICT communities around the globe."'
  inline_citation: '>'
  journal: EAI Endorsed Transactions on Internet of Things
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Leveraging AI and Blockchain for Privacy Preservation and Security in Fog
    Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gao L.
  - Xia X.
  - Zheng Z.
  - Xiang H.
  - Meng Z.
  - Han X.
  - Zhou Z.
  - He Y.
  - Wang Y.
  - Li Z.
  - Zhang Y.
  - Ma J.
  citation_count: '0'
  description: In the era of future mobility within Transportation 5.0, autonomy and
    cooperation across all road users and smart infrastructure stand as the key features
    to enhance transportation safety, efficiency, and sustainability, supported by
    cooperative perception, decision-making and planning, and control. An accurate
    and robust localization system plays a vital role in enabling these modules for
    future mobility and is constrained by environmental uncertainties and sensing
    limitations. To achieve precise and resilient localization in this new era, this
    paper introduces emerging technologies including edge computing, hybrid data-driven
    and physical model approaches, foundation models as well as parallel intelligence,
    that are beneficial for next-generation localization systems. On top of these
    key technologies, by integrating real-world testing and digital twin technology,
    we further put forward a Decentralized Autonomous Service (DAS)-based cooperative
    localization framework for future mobility systems to enhance the resilience,
    robustness, and safety of transportation systems.
  doi: 10.1109/TIV.2024.3377163
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Intellig...
    >Early Access Cooperative Localization in Transportation 5.0 Publisher: IEEE Cite
    This PDF Letian Gao; Xin Xia; Zhaoliang Zheng; Hao Xiang; Zonglin Meng; Xu Han;
    Zewei Zhou; Yi He; Yutong Wang; Zhaojian Li; Yubiao Zhang; Jiaqi Ma All Authors
    35 Full Text Views Abstract Authors Keywords Metrics Abstract: In the era of future
    mobility within Transportation 5.0, autonomy and cooperation across all road users
    and smart infrastructure stand as the key features to enhance transportation safety,
    efficiency, and sustainability, supported by cooperative perception, decision-making
    and planning, and control. An accurate and robust localization system plays a
    vital role in enabling these modules for future mobility and is constrained by
    environmental uncertainties and sensing limitations. To achieve precise and resilient
    localization in this new era, this paper introduces emerging technologies including
    edge computing, hybrid data-driven and physical model approaches, foundation models
    as well as parallel intelligence, that are beneficial for next-generation localization
    systems. On top of these key technologies, by integrating real-world testing and
    digital twin technology, we further put forward a Decentralized Autonomous Service
    (DAS)-based cooperative localization framework for future mobility systems to
    enhance the resilience, robustness, and safety of transportation systems. Published
    in: IEEE Transactions on Intelligent Vehicles ( Early Access ) Page(s): 1 - 6
    Date of Publication: 18 March 2024 ISSN Information: DOI: 10.1109/TIV.2024.3377163
    Publisher: IEEE Funding Agency: Authors Keywords Metrics More Like This A New
    Estimation of Road Safety Index in Transportation Systems with Fuzzy-DEA Method:
    A Case Study on Roads of East Azarbaijan Province in Iran Fuzzy Information and
    Engineering Published: 2020 The Safety Evaluation on Road Passenger Transportation
    Enterprises Based on Modified AHP method 2009 Sixth International Conference on
    Fuzzy Systems and Knowledge Discovery Published: 2009 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Transactions on Intelligent Vehicles
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Cooperative Localization in Transportation 5.0
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Patidar S.
  - Jindal R.
  - Kumar N.
  citation_count: '0'
  description: IoT devices generate a massive amount of sensitive data, which is transferred
    tremendously to the cloud for processing and decision-making. The most significant
    issues that need to be solved for IoT devices are improving energy efficiency
    and guaranteeing data security while several devices are connected. In this paper,
    for edge computing, a hybrid algorithm is proposed that uses compression and encryption
    in the same manner to improve efficiency in terms of energy and data security
    in IoT devices. The authenticated encryption with associated data (AEAD) ChaCha12-Poly1305
    algorithm and improved SZ 2.1 compression are used in this hybrid architecture.
    While sending the data to the edge, confidentiality, integrity, and authentication
    are maintained. Several experiments were conducted considering the driver stress
    detection dataset (Bernstein DJ. Lecture Notes in Computer Science including subseries
    Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics 4986:84–97
    (2008); Lakshminarasimhan S et al. LNCS 6852(1):366–379 (2011)) and gas-sensor
    dataset (Ibarria L et al. Comput Graph Forum 22(3):343–348 (2003)) for monitoring
    the home activity to validate this approach. The performance of the proposed improved
    SZ 2.1 compression algorithm is compared with the five baseline algorithms including
    SZ 1.4, original SZ 2.1, selective compression, algorithm-based fault tolerance
    (ABFT), and digit rounding algorithms. The key parameters used in the experiment
    to measure the performance of the proposed algorithm are data reduction, compression
    ratio, power consumption, encryption time, total processing time, and error. Using
    the improved SZ 2.1 compression algorithm in conjunction with the ChaCha12-Poly1305
    AEAD algorithm, the device’s battery life is also enhanced by about 10% while
    ensuring the security of data disseminated to the Edge. The use of the proposed
    secure hybrid model reduces both encryption and overall processing time by 95%
    and 98% respectively for both datasets.
  doi: 10.1007/s11042-024-18765-0
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Multimedia Tools and Applications
    Article A secure and energy-efficient edge computing improved SZ 2.1 hybrid algorithm
    for handling iot data stream Published: 19 March 2024 (2024) Cite this article
    Download PDF Access provided by University of Nebraska-Lincoln Multimedia Tools
    and Applications Aims and scope Submit manuscript Sanjay Patidar, Rajni Jindal
    & Neetesh Kumar  34 Accesses 1 Altmetric Explore all metrics Abstract IoT devices
    generate a massive amount of sensitive data, which is transferred tremendously
    to the cloud for processing and decision-making. The most significant issues that
    need to be solved for IoT devices are improving energy efficiency and guaranteeing
    data security while several devices are connected. In this paper, for edge computing,
    a hybrid algorithm is proposed that uses compression and encryption in the same
    manner to improve efficiency in terms of energy and data security in IoT devices.
    The authenticated encryption with associated data (AEAD) ChaCha12-Poly1305 algorithm
    and improved SZ 2.1 compression are used in this hybrid architecture. While sending
    the data to the edge, confidentiality, integrity, and authentication are maintained.
    Several experiments were conducted considering the driver stress detection dataset
    (Bernstein DJ. Lecture Notes in Computer Science including subseries Lecture Notes
    in Artificial Intelligence and Lecture Notes in Bioinformatics 4986:84–97 (2008);
    Lakshminarasimhan S et al. LNCS 6852(1):366–379 (2011)) and gas-sensor dataset
    (Ibarria L et al. Comput Graph Forum 22(3):343–348 (2003)) for monitoring the
    home activity to validate this approach. The performance of the proposed improved
    SZ 2.1 compression algorithm is compared with the five baseline algorithms including
    SZ 1.4, original SZ 2.1, selective compression, algorithm-based fault tolerance
    (ABFT), and digit rounding algorithms. The key parameters used in the experiment
    to measure the performance of the proposed algorithm are data reduction, compression
    ratio, power consumption, encryption time, total processing time, and error. Using
    the improved SZ 2.1 compression algorithm in conjunction with the ChaCha12-Poly1305
    AEAD algorithm, the device’s battery life is also enhanced by about 10% while
    ensuring the security of data disseminated to the Edge. The use of the proposed
    secure hybrid model reduces both encryption and overall processing time by 95%
    and 98% respectively for both datasets. Similar content being viewed by others
    A context-aware encryption protocol suite for edge computing-based IoT devices
    Article 14 October 2019 Requirements for Energy Efficient Edge Computing: A Survey
    Chapter © 2018 Edge computing based secure health monitoring framework for electronic
    healthcare system Article 02 September 2022 1 Introduction IoT is a network of
    computational devices or entities with distinct identities that can automatically
    exchange data over the internet without the intervention of humans. The accessibility
    of cloud computing, which is essential for processing IoT data, has allowed IoT
    to grow to its current size and complexity on a worldwide scale [1]. However,
    there are several drawbacks to this approach, including bandwidth issues, network
    latency in transmission, high energy consumption, and security threats. These
    factors reduce the longevity and reliability of IoT systems. The most significant
    challenge that IoT devices must address is improving energy efficiency while still
    maintaining data security [2]. In IoT applications, sensors capture constant real-time
    streaming of data that must be cloud based processed. Because of the huge amount
    of streaming data, transmission is energy consuming in IoT devices, reducing their
    efficiency and lifetime. A mobile phone, a local PC, or an IoT gateway are examples
    of edge devices. By bringing computing capacity closer to IoT devices, the edge
    computing architecture, depicted in Fig. 1, has developed as a viable solution
    for this. The data that is initially gathered by smart items such as sensors and
    wearables is used here. The edge node may then process the data locally before
    transmitting it to the cloud. Moving cloud services to the edge of the network
    is the practice of edge computing. Effectively, it can be compared to a decentralized
    cloud that places computing power near the data source to support local decision-making.
    SZ 2.1[3] is an enhanced version of the fast error bounded lossy method. The proposed
    hybrid SZ compression method is used in conjunction with an algorithm namely,
    ChaCha12-Poly1305 AEAD. Use of the proposed hybrid SZ 2.1 compression approach
    reduces the quantity of data delivered from the IoT device to the edge device.
    As a consequence, the results reached by processing this data do not vary much
    from those acquired by analyzing the original bulk of data. The ChaCha12-Poly1305
    algorithm is a hybrid of the ChaCha encryption method [4] and the Poly1305 message
    authentication technique [5]. The combination of these two methods offers data
    secrecy, authentication, and integrity at the same time and can guarantee efficiency
    with security in IoT devices. On devices with little power, these light-weight
    algorithms are simple to use. In this paper, the effects of applying these algorithms
    on IoT devices are studied and the performance of the proposed hybrid improved
    SZ 2.1 compression method is compared to SZ 1.4 [6], SZ 2.1 [6], selective compression
    [39], algorithm-based fault tolerance (ABFT) [40], and digit rounding algorithms
    [41]. Additionally, compared the three AEAD algorithms ChaCha8-Poly1305, ChaCha12-Poly1305,
    and ChaCha20-Poly1305 [7, 8]. Fig. 1 - Edge computing architecture  Full size
    image The motivation of this study is that the IoT devices have not yet become
    commonplace in everyday life due to their inherent challenges in battery life
    and efficient data management when transmitting real-time data streams. These
    challenges necessitate a trade-off between data usage and battery consumption
    on IoT devices. In response to this problem statement, the proposed enhanced SZ2.1
    algorithm makes a significant contribution in addressing these specific issues.
    One of the key aspects we address is authenticated encryption with associated
    data (AEAD), which demands an intermediate level of security. To achieve this,
    we employ the ChaCha12-Poly1305 encryption algorithm. It’s worth noting that the
    number of rounds in the ChaCha algorithm directly impacts the time required for
    execution. Consequently, the ChaCha8-Poly1305 algorithm boasts the shortest execution
    time, while the ChaCha20-Poly1305 algorithm requires the longest. The ChaCha12-Poly1305
    algorithm strikes a balance, necessitating an intermediate amount of time for
    execution. However, it’s essential to emphasize that the greater the number of
    rounds, the stronger the security provided by the algorithm. Therefore, our selection
    of the 12-round variant reflects a deliberate trade-off between security and performance—a
    distinctive feature of our AEAD approach. On the basis of the above motivation
    and novelty proposed in improved SZ2.1, the study have following contributions
    to the proposed work: 1. Balancing Security and Performance: Our proposed approach
    revolves around the strategic use of security and encryption algorithms to strike
    a delicate equilibrium between the security and performance aspects inherent to
    IoT-based systems. The novelty of our work lies in effectively managing both these
    critical elements to facilitate seamless data transfer within diverse IoT-based
    systems. 2. Exploring Data Security Mechanisms: Our research endeavors delve deep
    into the realm of data security mechanisms. To achieve an intermediate level of
    security, we adopt the ChaCha12-Poly1305 encryption algorithm for authenticated
    encryption with associated data (AEAD). The ChaCha algorithm’s execution time
    is intrinsically linked to the number of rounds employed, with the ChaCha8-Poly1305
    algorithm demonstrating the swiftest execution, while the ChaCha20-Poly1305 algorithm
    necessitates the maximum time investment. In contrast, the ChaCha12-Poly1305 algorithm
    strikes a well-calibrated balance by requiring an intermediate amount of time.
    It is imperative to highlight that the algorithm’s security strengthens with an
    increased number of rounds. Our deliberate selection of the 12-round variant underscores
    our commitment to preserving an equilibrium between security and performance.
    3. Enhancing the SZ Compression Algorithm: We have introduced specific enhancements
    to the SZ compression algorithm, culminating in the creation of the improved SZ2.1
    version. This enhanced iteration seamlessly integrates with the selected encryption
    algorithm, effectively harmonizing time efficiency, security, and power consumption
    within IoT devices. Our work significantly contributes to optimizing the interplay
    between power efficiency and security measures inherent in the management of data
    transfers for IoT devices. 4. Data Reduction through Compression and Encryption:
    Our approach ingeniously combines compression and encryption algorithms to achieve
    a substantial reduction in data volume. This reduction indirectly translates into
    reduced transmission times and enhanced power efficiency, ultimately leading to
    prolonged battery life. Conversely, employing fewer encryption rounds expedites
    the encryption process, directly benefiting battery power consumption due to the
    reduced algorithmic complexity. 5. Enhancing Energy Efficiency: We further bolster
    the energy efficiency of IoT systems by reducing the number of rounds within the
    ChaCha encryption algorithm. This strategic reduction serves to conserve the battery
    life of IoT devices. This optimization is complemented by the use of the Improved
    SZ2.1 compression algorithm, which simultaneously reduces data transfer times,
    thus further extending battery longevity. 6. Impact Assessment: To comprehensively
    assess the collective impact of compression and encryption algorithms on device
    energy consumption and network latency, we conduct an extensive comparative analysis
    of performance parameters. These parameters encompass data reduction, power consumption,
    encryption time, total processing time, and error metrics across various versions
    of existing compression methods. 7. Impact Analysis: An analysis is conducted
    on the speed of the proposed compression method for various input sizes. It investigates
    current data compression methods that may be implemented on IoT devices to decrease
    the quantity of data transmitted to the edge device, thus decreasing the consumption
    of energy in IoT devices. The study investigates an improved and more efficient
    data compression method than the existing state of art methods to improve the
    compression ratio while maintaining data quality and reducing network load. Nomenclature   AAD
    Additional Associated Data(optional) CD Compressed Data obtained after compression
    Ciphertext Encrypted data obtained from ChaCha12 Data Data collected through sensors
    E Error bound for compression Energy Energy level of device Hash Message Authentication
    Code (MAC) obtained from Poly1305 IoT Internet of Things K Key for encryption
    M Ciphertext is combined with additional related data to create MAC data N Nonce
    in ChaCha12-Poly1305 OTK One Time Key for Poly1305 algorithm P period to collect
    data before transmitting S Sampled Dataset Sensor status Status or mode of the
    device X Number of data points in data Y Number of points sampled from data The
    arrangement of paper is as follows: the literature review discussed in Section
    2; the basic terminologies related to architecture, domain protocols, and coding
    described in Section 3; the proposed methodology and experimental analysis using
    two different datasets i.e., drivers stress detection dataset and gas sensors
    for home activity monitoring Data set illustrated in Section 4; the result analysis
    using the improved SZ 2.1 compression algorithm in combination with the ChaCha12-Poly1305
    algorithm for making sure the security of IoT data in Section 5. Subsequently,
    the conclusion in Section 6. 2 Literature review Several approaches have been
    put up in relation to data minimization and data security enhancement in IoT applications.
    S. Lakshminarasimhan et al. [9] proposed an algorithm ISABELA for compression
    that uses B-spline interpolation after sorting the data. The reallocation of each
    point must, however, be stored separately because sorting destroys this data,
    necessitating additional storage. Thus, when there is a lot of data, this approach
    generates low compression. Furthermore, this technique depends heavily on the
    smoothness of the local region data considered as a drawback because spikey and
    abrupt data changes frequently occur in simulated data. This results in low prediction
    accuracy and degraded quality of compression. Z. Chen et al. [10] proposed a compression
    algorithm called NUMARCK. It uses vector quantization while compressing data.
    It stores two successive iterated data. N. Sasaki et al. [11], just like NUMARCK,
    also quantizes the data distribution. This algorithm effectively overcomes the
    limitation of ISABELA by alleviating the dependence on the efficacy of data. Although,
    this algorithm is not error bound. Additionally, the compression ratio obtained
    is low as well. A lossy compressor developed by P. Lindstrom et al. [12] is based
    on lifted orthogonal data block transform and can give error bounded compression
    as an option. It doesn’t rely upon the smoothness of data. However, using this
    method there is no way to guarantee that the decompressed results will satisfy
    the error bound. SZ 1.1 deals with a considerable amount of data that is produced
    while HPC applications are running. The difference between the input data and
    the decompressed data is designed to stay within the error bound. On the driver
    stress detection dataset, it has been experimentally demonstrated that an altered
    version of this technique can achieve a maximum compression ratio of 103. Using
    physiological signals, Healey et al. [13] suggested a method for measuring stress.
    These signals can be used for gathering the continuous driving task performance
    as well as providing feedback related to the driver stress. In [14], Goldberger
    et al. explained different components of research resources related to complex
    physiologic signals. In [15] for high-performance computing applications (HPC),
    Azar et al. suggested a fast error-bounded lossy compression scheme that deals
    with the wide range of HPC applications generated data. The authors in M. Burtscher
    et al. [16] describe a lossless data compression technique called FPC. It is fabricated
    to effectively compress and simultaneously meet scientific computing environments’
    i.e., high throughput requirements. Although it is a fast-lossless compression
    technique, which is not capable of achieving the large compression ratios required
    for IoT applications. P. Lindstrom et al. [17] proposed a data compression technique
    that, unlike FPC, works well with integer data and variable-precision floating
    points. When compared to its predecessors, this compression method achieves a
    higher throughput. However, it doesn’t attain compression rates appropriate for
    IoT devices that produce a lot of data. K. L. Tsai et al. [18] proposed the strategy
    of secure low power communication. This approach uses incredibly little power
    and is safe. By using fewer AES encryption cycles, it lowers energy usage. Additionally,
    the authors’ suggested key management and the D-Box upgrade process. LPADA is
    an architecture for the AES cipher proposed by K. L. Tsai et al. [19]. This architecture
    has been designed to lower the energy consumption of AES by making use of energy-efficient
    S-Box, power gating strategies, and other energy management methods. The outcomes
    the writers were able to achieve were a dynamic power reduction of 62.0% in contrast
    to the traditional AES data encryption. S. Roy et al. [20] proposed another encryption
    technique that has been proposed for IoT systems. It is essentially a lightweight
    cipher based on cellular automata. While the edge node performs the decryption,
    the IoT node performs the encryption. It can achieve better results than DES and
    3-DES and is shown to be advantageous for resource-constrained IoT devices. SAFER
    (secure and fast encryption routine) proposed by X. Guo et al. [21], is a complexity-reduced
    format block encryption method. It takes into account the diffusion and confusion
    theories. In essence, the confusion layer is made up of the SAFER algorithm, and
    the diffusion layer is composed of the fermat number theory transform. Due to
    the lack of multipliers and the focus on integer operations alone, this method
    has a low level of complexity. This method is suitable for Internet of Things
    devices with minimal power. S. Singh et al. [22] proposed a suite of cryptographic
    algorithms comprising encryption algorithms and hashes to design an efficient
    system consisting of resource-constrained IoT devices. They also analyzed a variety
    of lightweight ciphers based on certain characteristics viz. the number of rounds,
    size of the key, and block. D. A. F. Saraiva et al. [23] talk about how AES, ChaCha20-Poly1305,
    and a few more algorithms perform in IoT applications. The operating times, throughputs,
    and energy requirements of these algorithms were contrasted. They discovered that
    the lightweight block ciphers (such as SPECK and LEA) and the ChaCha20-Poly1305
    stream cipher are suitable alternatives for devices with limited resources. B.
    J. Mohd et al. [24] offered a straightforward approach for measuring the effectiveness
    of light cyphers. The model is used to look for areas where the encryption structure
    can be more effective. In order to ensure high throughput and minimal energy consumption,
    it is utilized to forecast the ideal block size and the number of rounds. They
    also provided an enhanced energy management algorithm that enables a device to
    handle critical data even when there is a low energy level. A. A. Diro et al.
    [25] uses Fog computing architecture to ensure distribution and scalability and
    offload the security functions from IoT devices to Fog nodes to lower the burden
    on resource-constrained IoT devices. They use elgamal based elliptic curve cryptography
    for data security while ensuring simplicity and energy efficiency. A. O. Akmandor
    et al. [26] proposed several architectures for different IoT applications. In
    order to boost security and energy efficiency while maintaining the devices’ intelligence,
    this article combined decision-making utilizing machine learning and cryptography
    algorithms on IoT devices. On the IoT device, they applied compressive sensing,
    compressed signal processing, and machine learning inference to achieve high classification
    accuracy while drastically reducing energy usage. A. Fragkiadakis et al. [27]
    presented the use of Adaptive Compressive Sensing. Compressive sensing is utilized
    to deliver compression and encryption at the same time. In order to assure the
    effectiveness of the system, the compression rate in this study is chosen based
    on the projected data scarcity. Y. Zhang et al. [28] appraised Compressive Sensing’s
    security features. The ChaCha20-Poly1305 AEAD algorithm was described by the authors
    as a small and quick implementation for ARM Cortex-M4 processors. J. Qi et al.
    [29] used a hybrid securing and CS-based scheme for ensuring energy efficiency
    and security in IoT devices. They used compressive sensing for compressing signals,
    chaotic block encryption of 8-bit for privacy, and message authentication codes
    for authentication. F. de Santis et al. [30] presented a compact and fast implementation
    of the ChaCha20-Poly1305 AEAD algorithm for ARM Cortex-M4 processors. The results
    show that ChaCha20- Poly1305 is a reliable technique to ensure security in IoT
    devices. To secure data packets in lossy and low-powered networks, S. Luangoudom
    et al. [31] proposed using an Authenticated Encryption method based on XSalsa20
    and Poly1305. Till now different researchers focused on the security and the energy
    efficiency of IoT devices as a separate subject, because these two are competing
    ideas [32, 39,40,41]. Currently, Compressive Sensing is being used for simultaneously
    providing security and energy efficiency to IoT systems. Compressive sensing involves
    efficiently reconstructing a signal by solving undermined linear systems. However,
    compressive sensing has certain limitations. The first one is sparsity and the
    second one is incoherence. The signal to be compressed must be sparse in some
    domain to allow compression. Hence this approach is not applied in cases when
    the signal is not sparse. Moreover, the compression ratio obtained is limited
    and much less than what can be obtained from the recent state-of-the-art lossy
    compression algorithms. Hence, this work is focused on reducing the power consumption
    in IoT devices by using a lossy error bounded compression algorithm that can be
    applied irrespective of the characteristics of the input data. At the same time,
    it is aimed to increase the security of IoT devices by employing the trusted ChaCha20-Poly1305
    AEAD algorithm and adapting it for IoT devices. 3 Background This section covers
    the background knowledge of the work including basic terminologies, coding and
    domain protocols. 3.1 Compression and decompression Compression is a method for
    encoding input data with fewer bits than the original representation requires
    to decrease its size. Decompression is the method used to recover the original
    data representation from compressed data [33]. It is the process of reversing
    compression. Lossy and lossless compression, these are two classification of data
    compression methods. Lossy compression methods decrease the data size by eliminating
    extraneous or unimportant information [42, 43]. This creates some data distortion.
    These methods are irreversible and use some inexact approximations to compress
    the data. The data’s quality is compromised and merely allowed for the reconstruction
    of an approximation of the original data. Data loss enables these methods to achieve
    high compression ratios. On other hand, lossless compression methods decrease
    the data size without distorting the data. The quality of the data is not affected
    in lossless compression. These techniques make it possible to fully recreate the
    original data from the compressed data. They only achieve a limited degree of
    compression since they are lossless. 3.1.1 Fast error bounded lossy compression
    The SZ compression algorithm described in [1, 4], and [7] is used in this work.
    They are lossy error-bounded compression algorithms. It has been suggested as
    a method of compressing massive amounts of data produced by simulation software.
    It can compress all data types, including integers, floating-point numbers, and
    doubles. However, in this study, floating-point data is taken into account. For
    data compression, the SZ method relies on a prediction model. It looks for patterns
    and then utilizes those patterns to anticipate previously not known data. It predicts
    new data points based on previously observed data points. The algorithm’s design
    enables the error to stay precisely within the defined range. This approach also
    enables a maximum compression ratio while keeping error bounds depending on the
    application. The absolute error bound value of 0.1 is considered in this research
    [7]. The absolute error is computed u the Eq. (1), where \\({d}_{o}\\) represents
    the actual value of data and \\({d}_{c}\\) represents the decompressed data value.
    As a result, any data value X after decompression should lie between X—0.1 and
    X + 0.1.SZ algorithms are lossy error bounded compression techniques. They use
    a prediction model to forecast new data based on previously observed data. As
    a result, when data is compressed, instead of retaining the original data, it
    saves specific information about the prediction model. Using the prediction model,
    the algorithm can recreate an approximation to the initial data during decompression.
    The SZ compression method consists of three varieties: SZ 1.1 [7], SZ 1.4 [4],
    and SZ 2.1 [1]. In the proposed work, the SZ 2.1 compression method is modified
    to produce better compression. $$Error= \\left|{d}_{c}-{d}_{o}\\right|$$ (1) Variable-length
    encoding—The quantization codes generated after quantization were discovered to
    have an uneven distribution. To compress this non-uniform distribution, variable-length
    encoding is employed. More frequent symbols were given a smaller code than less
    frequent symbols in variable-length encoding. The overall length of the data is
    successfully decreased in this manner. It’s a lossless data compression method.
    For variable-length encoding, all three methods use Huffman encoding. 3.1.2 SZ
    1.1 Compression SZ 1.1tries to predict new data based on previously observed data
    by using several curve fittings models. The multi-dimensional data array is initially
    converted to a single dimensional array using the compression technique. Then,
    for each data point, on the basis of previous three data points, it uses the best
    fit curve fitting model to forecast the following data point. The Preceding Neighbor
    Fitting Model, the Linear Curve Fitting Model, and the Quadratic Curve Fitting
    Model are the three models used by the method. The predictable data is replaced
    with the model’s two-digit code. The Preceding Neighbor Curve Fitting Model is
    represented by Eq. (2). Here, \\({\\widehat{p}}_{i}^{P}\\) denotes the \\({i}^{th}\\)
    predicted point, and \\({\\widehat{p}}_{i-1}\\) denotes the final expected data
    point. The Eq. (3) represents the linear curve fitting model, where \\({\\widehat{p}}_{i}^{L}\\)
    represents the \\({i}^{th}\\) predicted point and \\({\\widehat{p}}_{i-1}\\) and
    \\({\\widehat{p}}_{i-2}\\) represents the last and the second last predicted data
    points and Eq. (4) represents the quadratic curve fitting model, where \\({\\widehat{p}}_{i}^{Q}\\)
    represents the \\({i}^{th}\\) predicted point and \\({\\widehat{p}}_{i-1}\\),
    \\({\\widehat{p}}_{i-2}\\) and \\({\\widehat{p}}_{i-3}\\) represents the last,
    the second last and the third last predicted data points. $${\\widehat{p}}_{i}^{P}={\\widehat{p}}_{i-1}$$
    (2) $${\\widehat{ p}}_{i}^{L}={2\\times \\widehat{p}}_{i-1}-{\\widehat{p}}_{i-2}$$
    (3) $${\\widehat{p}}_{i}^{Q}={3\\times \\widehat{p}}_{i-1}-3\\times {\\widehat{p}}_{i-2}+{\\widehat{p}}_{i-3}$$
    (4) 3.1.3 SZ 1.4 Compression SZ 1.4 [4] uses a multidimensional prediction model
    to predict new data based on previously observed data. If a data point is predictable,
    some metadata that can be used to recover the point after decompression is saved
    instead of the data point itself. The general formula [4] of the n layer prediction
    model, as described in [4] is given by Eq. (5). Where, d denotes the number of
    dimensions, g(i,j) the initial value of the data point, and f(i,j) the value predicted
    for the point. Finally, variable length encoding is applied to further enhance
    compression. SZ 1.4 additionally employs adaptive error-controlled quantization
    to hold back the predicted points under a defined error limit, allowing to achieve
    the highest compression ratio while ensuring an error bound depending on the application.
    A higher n may lead to a more précised prediction, which leads to greater compression
    quality. A good n choice will result in a good compression ratio, reduced compression
    error, and faster compression. $$f\\left({a}_{1},\\dots ,{a}_{d}\\right)= {\\sum
    }_{0 \\le {b}_{1,..,{b}_{d}\\le n}}^{\\left({b}_{1},\\dots ,{b}_{d}\\right)\\ne
    \\left(0,\\dots ,0\\right)}-{\\prod }_{j=1}^{d}{\\left(-1\\right)}^{{b}_{j}}\\left(n{b}_{j}\\right).g\\left({a}_{1}-{b}_{1},\\dots
    ,{a}_{d}-{b}_{d}\\right)$$ (5) 3.1.4 SZ 2.1 Compression SZ 2.1 [1] uses three
    different prediction models to predict the data points namely the Lorenzo Prediction
    Model [34], the Mean Integrated Lorenzo Prediction Model [1] and the Linear Regression
    Model. In SZ 2.1 the algorithm divides the input data into blocks of equal size.
    Then for each block, it tries to find out the best model for predicting the data.
    The best model for prediction is decided dynamically for each data block. Then
    the prediction model is used to get predicted values after which linear-scaling
    quantization and variable length encoding are performed. For blocks that use the
    Linear Regression Model, the regression coefficients are compressed using IEEE
    754 binary representation analysis [7]. In the end the algorithm compresses the
    resulting stream using the zstd lossless compression algorithm [35]. 3.1.5 Compression
    using 754 binary representation analysis The unpredictable data is compressed
    using IEEE 754 binary representation analysis. This is a three-step process. First,
    all the unpredictable data is mapped to a smaller range by subtracting the median
    value of the range from all the values. This data is now referred to as normalized
    data. The data will be closer to 0, and it requires fewer mantissa bits to meet
    specified precision. Second, the value is curtailed by ignoring the insignificant
    mantissa part which is in accordance with the user-specified error bounds. Third,
    to further minimize the storage space, the leading zero-based floating-point compression
    is utilized. In essence, each point is compressed by making use of a leading zero
    count followed by the left over significant bits following the XORed of the subsequent
    normalized values. With the aid of three separate prediction models, SZ 2.1 forecasts
    data points: Lorenzo prediction model [34], Mean integrated Lorenzo prediction
    model, and linear regression model. The SZ 2.1 method splits the incoming data
    into equal-sized blocks. Then it attempts to identify the best model for predicting
    the data for each block. For each data block, the optimal model for prediction
    is determined dynamically. The predicted values are then obtained using the prediction
    model, which is followed by linear-scaling quantization and variable length encoding.
    The regression coefficients for blocks that utilize the Linear Regression Model
    are compressed using IEEE 754 binary representation analysis. Finally, the method
    uses the zstd lossless compression algorithm [35] to compress the output stream.
    The Lorenzo model is a prediction model that uses the values of adjacent points
    to predict the value of a point. It is a special case of the multidimensional
    prediction model represented by Eq. (5) with the value of n set to 1. For each
    block, it selects the best prediction model between the best Lorenzo predictor
    L-predictor and linear regression predictor dynamically based on the cost function.
    The cost function for linear regression model, Lorenzo model, and mean integrated
    Lorenzo model is given by Eqs. (6), (7) and (8) respectively, where \\({p}_{i}\\)
    is the predicted value, \\({o}_{i}\\) is the original value, Y is the sample size
    and E is the error bound, and mean is the mean calculated for the Mean Integrated
    Lorenzo model. $${E}_{r}= \\sum \\limits_{i \\in S}\\left|{p}_{i}-{o}_{i}\\right|$$
    (6) $${E}_{r}= {\\sum }_{i \\in S}\\left|{p}_{i}-{o}_{i}\\right|+Y*1.22*E$$ (7)
    $${E}_{r}={\\sum }_{i \\in S}\\left(\\left|{p}_{i}-{o}_{i}\\right|+Y*1.22*E,\\left|mean-{o}_{i}\\right|\\right)$$
    (8) 3.2 Encryption and decryption Data must be encoded through the process of
    encryption so that anyone other than the intended recipient cannot access it.
    Decryption is the process of breaking the encryption for the sake of recovering
    the original data. The plaintext is the original data, whereas the ciphertext
    is the encrypted data. For encrypting and decrypting data, a pseudo-random key
    is required called the encryption key. To access the plaintext, only the authorized
    entity with a valid key may decode the ciphertext. In this study, the following
    methods are utilized to manage security. 3.2.1 MAC (Messages authentication code)
    By using MAC, the recipient can confirm that the message was sent by the designated
    sender and was not altered. The MAC code is produced using both the message and
    a shared key of the sender and the receiver. The receiver then receives this code
    along with the message. After that, the receiver recreates the code and compares
    it with the code sent by the sender. If the generated code matches the received
    code, the authenticity and integrity of the received message are verified. Otherwise,
    the communication is invalid, and the receiver should reject it and take the necessary
    action. 3.2.2 ChaCha20 encryption algorithm This algorithm belongs to the family
    of stream ciphers that are specially designed for the software platform. To support
    divers’ performance versus security trade-offs, it permits for different rounds,
    nonce, key, and counter length. This stream cipher was designed as a concentration
    of Salsa20 stream cipher, as it provides security as well as high throughput without
    degrading performance on different software platforms. 3.2.3 Poly1305 Message
    authentication algorithm Daniel J. Bernstein developed the message authentication
    code Poly1305. It is used to validate the message’s validity and integrity. From
    the message, the algorithm generates a MAC. For key expansion, the Poly1305 architecture
    uses the AES encryption. Poly1305-AES uses two 128-bit keys and a 128-bit nonce
    to calculate a 128-bit MAC from a variable length message. The message is processed
    by the algorithm in 16-byte chunks. 3.2.4 ChaCha20-Poly1305 for IoT data eEncryption
    To authenticate encrypted communications, the encryption algorithmChaCha20 is
    often coupled with the Poly1305 authentication algorithm. The ChaCha20 algorithm
    is used to encrypt communications and generate keys for the Poly1305 method. The
    message authentication code is generated using the Poly1305 algorithm. The ChaCha20-Poly1305
    authenticated encryption with associated data, or AEAD algorithm, is formed by
    their combination. In this work, the Poly1305 message authentication technique
    and the ChaCha symmetric key stream cipher with 12 rounds are combined. The integrity,
    validity, and confidentiality of data exchanged over the network are all guaranteed
    by this architecture. 4 Proposed approach & experimental analysis The goal of
    this study is to enhance battery life as well as the security of IoT devices.
    This objective is to be achieved by implementing appropriate compression and encryption
    algorithms in IoT devices. Because data transmission consumes energy and significantly
    reduces the life duration of IoT devices. It is aimed that by using data compression,
    the amount of data to be sent would be reduced, resulting in lower energy usage.
    Therefore, the advantage of this study are lesser time for encryption, lesser
    complex computations for decryption stage as well, middle level compression, relatively
    lower bandwidth for data transmission, significant reduction in battery consumed
    by IoT system involved proposed work. 4.1 Different proposed model to enhance
    battery life as well as the security of IoT devices IoT devices consume the energy
    while data transmission, which also degrades the battery’s life. Therefore, the
    following models are proposed to enhance battery life as well as the security
    of IoT devices. 4.1.1 Proposed hybrid model A hybrid SZ and ChaCha12-Poly1305
    based secure energy-efficient model is presented in this study. The work is compared
    to current SZ 1.4, original SZ 2.1, selective compression, algorithm-based fault
    tolerance (ABFT) and digit rounding compression algorithms, as well as the version
    of the ChaCha-Poly1305 algorithm, to enhance the security and energy of IoT devices.
    The performance of the three compression algorithms is compared in terms of compression
    and speed. Similarly, the ChaCha12-Poly1305 technique is compared to the inferior
    ChaCha8-Poly1305 and ChaCha20-Poly1305 algorithm which are used more frequently.
    The block diagram of the procedure on the Client device is shown in Fig. 2. The
    data is first gathered by the client device and then compressed using the SZ compression
    method. The compressed data, along with any additional associated data and a nonce
    N, are used in the ChaCha12-Poly1305 algorithm. It generates both the Ciphertext
    and the hash. The message payload consists of the ciphertext, hash, nonce, and
    any other related data that is transmitted to the edge device. While Fig. 3 depicts
    the process block diagram on the Edge device. The hash is computed after extracting
    the ciphertext, additional associated data, and nonce N from the message payload.
    If the generated hash matches the hash in the message payload, the message is
    authenticated and proceeds. In the case of a mismatch, the message may be denied,
    and precautionary actions will be taken. If the communication is authenticated,
    the ChaCha12 algorithm is used to decode the ciphertext into plaintext. To retrieve
    the original data, the encrypted data is decompressed using the SZ method. Fig.
    2 - Client (IoT device) Full size image Fig. 3 –Server (IoT edge device) Full
    size image Algorithm 1 describes the proposed hybrid algorithm. The error bound
    E, key K, time period P, and SZ compression algorithm version V are all inputs
    to the algorithm. Initially, P days are needed to gather data utilizing sensors.
    The gathered data is then leveled into a one-dimensional array and compressed
    using the SZ compression method. Any extra associated data that must be sent in
    addition to the gathered data does not need to be protected or encrypted. In addition,
    a nonce N is produced. It must be distinct for each invocation of a given key.
    Finally, the ChaCha12-Poly1305 algorithm, which created the Ciphertext and Hash,
    was applied to the compressed data CD, key K, nonce N, and additional related
    data AAD. However, as the SZ technique uses lossy compression, the decompressed
    data will contain mistakes up to the set error limit. The decompressed data could
    be examined at the edge device before being transmitted to the cloud for archival
    and further processing. Algorithm 1 Proposed Hybrid Algorithm Full size image
    It is believed that the IoT device is constantly collecting data. It transmits
    the data to the edge node after P seconds of waiting. The gathered data is leveled
    before being compressed using the SZ compression method. The ChaCha12-Poly1305
    technique is used to protect the compressed data once it has been compressed.
    The ChaCha12 encryption algorithm is used to encrypt the data first. The Poly1305
    authentication technique, which creates the Hash for the input, sends the encrypted
    data together with any optional related data. The hash, encrypted data, and accompanying
    information was then delivered to the edge node. The data may be authenticated,
    decrypted, and decompressed at the edge node in order to restore the original
    data. 4.1.2 Proposed SZ compression SZ 2.1 compresses the data after lossy compression
    using the zstd lossless compression algorithm to achieve even more compression.
    The Burrows-Wheeler transform (BWT) compression algorithm SZ 2.1 is replaced within
    the proposed SZ. The BWT technique is used by bzip2 [36], a free and open-source
    lossless compression software. When compared to the LZW and Deflate algorithms,
    bzip2 achieves a significantly greater compression ratio. To achieve high compression,
    bzip2 uses the Burrows-Wheeler transform, run-length encoding, move to front transform,
    and Huffman encoding techniques. By sorting the data, the Burrows-Wheeler transform
    helps in data compression. This produces a series of symbols that can be easily
    compressed using run-length encoding and Huffman encoding. The improved SZ compression
    algorithm is represented by Algorithm 2. The densest location v and frequency
    f 1 are calculated by sampling X points, where X is the number of points in D.
    The densest frequency f2 is determined by sampling 1% of the data. If f 1 > f
    2, the mean integrated Lorenzo predictor is chosen as the best Lorenzo predictor
    for the data. Otherwise, the original Lorenzo predictor is used to compute the
    mean value. Following that, the regression coefficients for all blocks are computed.
    This method eventually divides the data into chunks. To generate a sample dataset
    S for the block, Y points are sampled. The cost of both prediction models is determined
    using S. Based on the cost function, it dynamically chooses the best prediction
    model among the best Lorenzo predictor, L-Predictor, and linear regression predictor.
    To create predictions, the best-fit prediction model is utilized. This is followed
    by quantization based on linear scaling and variable-length encoding. Finally,
    the data is compressed further using bzip2. Algorithm 3 represents the bzip2 compression.
    It begins by dividing the data into blocks. Then it conducts a series of actions
    on each block to compress the data. It encodes the data using run length. This
    is followed by the most crucial step in bzip2 compression, the Burrows-Wheeler
    transformation. Following this, apply run-length encoding, Huffman, and delta
    encoding to generate compressed data CD. Algorithm 2 SZ_Compression Full size
    image Algorithm 3 bzip2_compression Full size image 4.1.3 Chacha12-Poly1305 Encryption
    In this section the ChaCha12 encryption algorithm and the Poly1305 message authentication
    code are described. ChaCha12 is the twelve rounds variant of the popular ChaCha20
    stream cipher presented by Daniel J. Bernstein. Instead of 20 rounds of computation,
    it takes 12 rounds. It is like the 20 rounds variant in all other aspects. The
    method becomes faster when the number of rounds is reduced. However, a smaller
    number of rounds implies degradation in terms of security. Algorithm 4 ChaCha12-Poly1305-AEAD-Algorithm
    Full size image Algorithm 5 ChaCha12-Encrypt Full size image Algorithm 6 Poly1305
    Algorithm Full size image In this work, the ChaCha12-Poly1305-AEAD algorithm is
    proposed. The Poly1305 message authentication code and the ChaCha12 encryption
    algorithm are combined to create this algorithm. The ChaCha12-Poly1305 algorithm
    is represented by Algorithm 4.Here, the key K and the nonce N are first combined
    to create a one-time key. Before concatenation, AAD and Ciphertext were padded
    until they were a multiple of 16. Finally, the Poly1305 Algorithm is used to calculate
    the hash value. These two algorithms were selected because they are portable and
    may be used on IoT devices. Particularly, the CPU-friendly Add-Rotate-XOR (ARX)
    operations used by the ChaCha12 encryption algorithm make it simple to operate
    on devices with limited resources. Algorithm 5 represents the ChaCha12 encryption.
    The ChaCha12 algorithm accepts the plaintext P, key K, nonce N, and a counter
    with the default value of 1 as input. It begins by initializing some constants
    then divides the input plaintext P into blocks of 64 bytes. The size of the last
    block can be less than 64 bytes. For each block, it creates a state variable from
    the constants, the key K, the counter C, and the nonce N, then creates a copy
    of the state variable and performs three double rounds. In each double round,
    it performs some Add-Rotate-XOR operations. Finally, it constructs a key-stream
    by combining the working state variable with the state variable. To get the cipher
    text, this key-stream is XORed with the block. Daniel J. Bernstein developed the
    message authentication code Poly 1305, represented by algorithm 6. It is used
    to validate the message’s validity and integrity. As input, it takes the message
    M and the key K. It begins by initializing certain constants. The key is then
    used to generate two variables, r, and s, then it adds b ‘ × 01’ to each block
    of 16 bytes in M and reads it as a little-endian integer. This value is appended
    to the accumulator variable, following that, the value of the accumulator is updated
    using r and s. After all of the blocks have been processed, the hash value is
    calculated by reading the accumulator’s 16 least significant bytes as a little-endian
    integer. 4.1.4 Example of chacha12-poly1305 algorithm The ChaCha12-Poly1305 algorithm
    is implemented using the technique described in [5, 6], except that instead of
    20 rounds, the ChaCha encryption algorithm is performed using just 12 rounds.
    Although a larger number of rounds indicated better cryptographic security. The
    research provided in [37] demonstrates an attack against seven-round versions
    of the ChaCha algorithm. Even though there is no known attack on the ChaCha algorithm’s
    8 rounds version, the 12 rounds variant was selected since it offers a larger
    security gap. The 20-round version would improve security while also increasing
    computing complexity. Table 1 shows the results achieved while using the ChaCha12-Poly1305
    method as an example. Table 1 An example of the ChaCha12-Poly1305 algorithm Full
    size table 4.2 Experimental analysis We have utilized two datasets for our experimental
    analysis, namely, the Drivers Stress Detection Dataset [13, 14], and the Gas Sensors
    for Home Activity Monitoring Dataset [38]. The first dataset contains a variety
    of signals collected from individuals while doing real-world driving activities.
    The sample of sample were at a rate of 496 Hz. They were captured as vehicles
    in the Greater Boston region followed a predetermined path. Three segments—rest,
    city, and highway—each driving job lasts at least 50 min. Rest refers to the intervals
    of rest at the start and end of a driving task that is classified as \"Low Stress”.
    City is the period when the driving job is carried out inside the city and the
    driver may have encountered a range of traffic conditions. These were referred
    to as \"High Stress”. Highway periods include the time spent traveling on the
    highway. These were designated as \"Moderate Stress\" periods. This data labeling
    is verified by the drivers themselves using self-reporting questionnaires. Five
    psychological signals were used to identify stress. ECG (electrocardiogram), HR
    (Heart Rate), GSR (Galvanic skin response) of the hand, GSR (Galvanic skin response)
    of the foot, RR (respiration rate). The WFDB (waveform database) format is used
    to store the data, which is preprocessed to convert into a 32-bit binary file.
    The second dataset of gas sensor home activity monitoring consists of 100 recordings
    which is an array of eight MOX gas sensors, a humidity sensor, and a temperature
    sensor. The sensor array is exposed to two different stimuli: wine and banana.
    The duration of each stimulation ranges from seven minutes to two hours, with
    an average of 42 min. There are 36 wine readings, 33 banana readings, and 31 background
    activity readings. Readings are taken at one sample per second, with minor variations
    due to wireless communication issues. From this dataset, only the temperature
    sensor readings are used. Three experiments were performed on these datasets to
    study the Application of the suggested protocol and its effects on IoT devices,
    Effect of larger input data size on SZ compression techniques and Effect of increase
    in the size of input data on ChaCha8-Poly1305, ChaCha12-Poly1305, and ChaCha20-Poly1305
    algorithms. 4.2.1 Effect of applying the proposed protocol on IoT Devices This
    section consists experiments that were conducted for verifying the effectiveness
    of the SZ 1.4, original SZ 2.1, selective compression, algorithm-based fault tolerance
    (ABFT), digit rounding algorithms and the improved SZ 2.1 compression algorithms
    in combination with the ChaCha12-Poly1305 algorithm have been explained. A client
    and server program for this purpose were implemented. Those experiments were conducted
    as mentioned in [15]. Client IoT Program: This program was deployed on Raspberry
    pi. Figure 2 represents the IoT Client Program written in Python programming language.
    The existing compression algorithms, and the improved SZ 2.1 with AEAD algorithm
    ChaCha12-Poly1305 were deployed on the client IoT device. Server IoT Program:
    This program was deployed on the Asus-X541UA laptop running on the Ubuntu 18.04.03
    LTS. The existing compression algorithms, and the improved SZ 2.1 as well as the
    AEAD_algorithm ChaCha12Poly1305 were deployed on the edge device. Table 2 lists
    the two devices’ specifications. Table 2 Device Specification  Full size table
    The Raspberry Pi 4B with power 5 V|USB-C having processor ARM-CORTEX-A72 BCM2711
    was used for the experiment as a client device. It is assumed that the client
    and server have previously used a secure key exchange mechanism to establish the
    shared secret key for the encryption algorithm. The data is broken up into blocks
    that are 2,976,000 bytes in size. A data block is sent to the edge device, where
    the received data is processed, after one minute. If the data is encrypted at
    the client device, the edge device must first verify its authenticity using the
    tag or hash that was delivered with the data. Data is decrypted after it has been
    authenticated. If the client device compresses the data, the edge device will
    decompress it. The processing of that data would follow. After that, the data
    might be processed again or uploaded to the cloud. Four hours, or 241 intervals,
    of this process are repeated. 4.2.2 Impact of increase in the size of input Data
    on SZ compression algorithms In this section, the experiments performed for studying
    the effect of size of input data on the existing algorithms have been explained.
    The experiments were conducted using data blocks of size 100 KB, 200 KB, 400 KB,
    800 KB, 1200 KB, 1600 KB, 3200 KB, 6400 KB, 12,800 KB, 25,600 KB, and 51,200 KB.
    The data blocks were created from the Driver Stress Detection Dataset and the
    Gas Sensors for Home Activity Monitoring Dataset. The experiments were conducted
    on the Asus X541UA whose specifications are mentioned in Table 2. 4.2.3 Impact
    of increase in the size of input data on CHACHA8-POLY1305, CHACHA12-POLY1305,
    and CHACHA20-POLY1305 algorithms In this section, the experiments performed for
    studying the effect of size of input data on the ChaCha8-Poly1305, ChaCha12-Poly1305,
    and the ChaCha20-Poly1305 algorithms has been explained. The experiments were
    conducted using data blocks of size 100 KB, 200 KB, 400 KB, 800 KB, 1200 KB, 1600
    KB, 3200 KB, 6400 KB, 12,800 KB, 25,600 KB, and 51,200 KB. The data blocks were
    created from the Driver Stress Detection Dataset and The Gas Sensors for Home
    Activity Monitoring Dataset. The experiments were conducted on the Dell Inspiron
    15R 5537 whose specifications are mentioned in Table 2. 5 Result and discussion
    In this section, the results obtained from these experiments are presented. The
    observations were made during experimental analysis by applying the proposed protocol
    i.e., Data reduction, Power consumption, Encryption time, Total processing time,
    and Error on IoT devices which are explained as follows: 5.1 Data reduction This
    subsection includes the results of data reduction i.e., the amount of data reduced
    due to compression and its effect on both the datasets. Figure 4(a) shows the
    variation in data transmission to the edge device during each period when no compression
    algorithms are utilized. Figure 4(b) shows the same when the existing compression
    algorithms are used. The data transmitted is obtained from the driver stress detection
    and home activity monitoring dataset. The vertical axis on a logarithmic scale
    shows the data delivered in bytes, while the horizontal axis represents the periods.
    Without compression, 2,976,000 bytes of data are transmitted. When the SZ 1.4
    compression algorithm is used, the transmitted data varies between 19,171 bytes
    to 44,588 bytes and 11,388 bytes to 15,926 bytes. Also, the maximum compression
    ratio obtained is 155, and 261 times while the average compression ratio is 100,
    and 214 times respectively for driver stress detection and home activity monitoring
    dataset. Similarly the compression ratio of existing algorithms is also presented
    in Fig. 4a and b. When the SZ 2.1 compression algorithm is used, the amount of
    data transmitted varies between 16,444 bytes and 40,821 bytes and 6723 bytes and
    8291 bytes. Also, the maximum data reduction achieved is 180 times and 442 times.
    The average compression ratio achieved is 116 times, and 392 times respectively
    for driver stress detection and home activity monitoring dataset. When the improved
    SZ 2.1 compression algorithm is used, the amount of data transmitted varies between
    12,296 to 29,408 bytes and 6723 to 8291 bytes. Also, a maximum data reduction
    of up to 242 and 442 times can be achieved while the average compression ratio
    achieved is 154 and 392 times respectively for driver stress detection and home
    activity monitoring dataset. Fig. 4 Data transmitted during each period. a. Driver
    stress data detection dataset. b. Gas sensor data dataset Full size image Variations
    in the amount of data transmitted are seen when these compression algorithms were
    used. This is because some blocks are easy to compress while some are harder to
    compress. The results showed that using the improved SZ 2.1, the data to be transmitted
    was significantly able to be reduced. It also showed that the improved SZ 2.1
    compression algorithm performed much better than the existing compression algorithms
    for driver stress detection and home activity monitoring dataset. Figure 5 presented
    the comparison of both the datasets using the existing compression algorithms
    with the improved SZ 2.1 and also out of all the algorithms, the improved SZ 2.1
    compression algorithm has the highest average compression ratio. Fig. 5 Comparison
    of both the datasets using the compression algorithms SZ 1.4, original SZ 2.1,
    selective compression, algorithm-based fault tolerance (ABFT), digit rounding
    algorithms, and the improved SZ 2.1 Full size image 5.2 Power consumption This
    subsection includes the details of the power consumption factor on two different
    datasets, that is, the power consumed during each experiment. That experiment
    is conducted in different scenarios such as: An inactive client, The client is
    utilizing sensors to collect data, The client collects data and sends it to the
    edge at the end of each period, After each period, the client gathers data and
    encrypts it, After each period, the client collects data, encrypts it, and sends
    it to the edge, After each period, the client gathers data, compresses it, and
    encrypts it, After each period, the client gathers data, compresses it, encrypts
    it, and then sends it to the edge. For the measurement of power consumption during
    data transmission and compression, we employed a power meter, as described by
    Eq. 9. During data stream transmission, the power consumption was recorded at
    1.253 W, and it increased to 1.44 W during the compression process. $$Power\\;Consumption\\;(W)
    = Transmission\\;Time\\;*\\;1.253 + Compression\\;Time\\;*\\; 1.44$$ (9) 5.2.1
    Driver stress detection dataset These power consumption results shown by Fig.
    6a, b,c, d, e and f present the level of battery of the client IoT device over
    4 h during various scenarios when it is sending the data of the stress detection
    dataset and uses with versions of the SZ compression algorithms namely, (SZ1.4,
    SZ2.1), and selective compression, ABFT, digit rounding algorithm and the proposed
    hybrid model improved SZ2.1 algorithm and ChaCha12-Poly1305 encryption algorithm
    over 241 periods in different scenarios as follows: Blue line: Reduction of battery
    level when the client is inactive. The level of battery reduces to 51% using the
    proposed hybrid model. Redline: Reduction of battery level when a client is sensing
    data. The battery level reduces to 46% in this case. Orange Line: Data is being
    sensed by the client and sent to the edge. In this instance, the battery level
    drops to 37%. Black Line: Using the ChaCha12-Poly1305 algorithm, the client senses
    the data and encrypts it. In this instance, the battery level drops to 37%. Purple
    Line: The client detects the data, compresses it with the existing and proposed
    algorithms and encrypts it with the ChaCha12-Poly1305 technique. In this instance,
    the battery level drops to 45%. Green Line: Data is being sensed by the client,
    encrypted with the ChaCha12-Poly1305 algorithm, and sent to the edge. In this
    instance, the battery level drops to 37%. Yellow Line: The client is detecting
    the data, encrypting it with the ChaCha12-Poly1305 algorithm, compressing it with
    the existing and proposed algorithm, and sending it to the edge. In this instance,
    the battery level drops to 45%. It shows the in green line that the data is being
    sensed by the client, encrypted with the ChaCha12-Poly1305 method, and sent to
    the edge. In this instance, the battery level drops to 35%. Fig. 6 Battery level
    of the client for Driver Stress Detection Dataset during 241 periods Full size
    image 5.2.2 Gas Sensors for home activity monitoring dataset These power consumption
    results are shown by Fig. 7a, b, c, d, e and f depict the level of battery of
    the client over 4 h during different scenarios when it is sending the data from
    the Gas Sensors for Home Activity Monitoring Dataset and by using using SZ 1.4,
    original SZ 2.1, selective compression, algorithm-based fault tolerance (ABFT),
    digit rounding algorithms and the proposed hybrid model improved SZ 2.1 algorithm
    and ChaCha12-Poly1305 encryption algorithm over 241 periods. It shows the following
    different cases which are explained as follows: blue line: An inactive client.
    In this case, the level of the battery reduces to 51%. Redline: Client is gathering
    information. In this instance, the battery level drops to 49%. Pink Line: The
    data is being sensed by the client and sent to the edge. In this instance, the
    battery level drops to 36%. Black Line: The data is being sensed by the client,
    who is then utilizing the ChaCha12-Poly1305 method to encrypt it. In this instance,
    the battery level drops to 45%. Purple Line: Data is being sensed by the client,
    which then uses the SZ 1.4 method to compress it and the ChaCha12-Poly1305 algorithm
    to encrypt it. In this instance, the battery level drops to 46%. Green Line: The
    data is being sensed by the client, encrypted with the ChaCha12-Poly1305 method,
    and sent to the edge. In this instance, the battery level drops to 35%. Yellow
    Line: Data is being sensed by the client, which then compresses it with the existing
    and proposed algorithm, encrypts it with the ChaCha12-Poly1305 method, and sends
    it to the edge. In this instance, the battery level drops to 43%. Figure 7a represents
    the battery level of the client with SZ 1.4. Similarly, the battery level of the
    client for other algorithms selective compression, ABFT, digit rounding algorithm
    and the proposed hybrid model improved SZ2.1 algorithm are shown in the Fig. 7b,
    c, d, e and f. Fig. 7 Battery level of the client with home activity monitoring
    dataset during 241 periods Full size image All of these findings demonstrated
    that when the client is idle, the battery is used the least. When detecting data,
    the client device used more battery. The battery was further depleted by using
    the encryption technique. The maximum amount of energy is consumed when the device
    is encrypting data and then transmits it directly. However, using compression
    considerably lowers energy use. 5.2.3 Percentage of battery saved Tables 3 and
    4 shows the amount of battery saved when the compression is used before encryption
    and transmission as compared to the case when the data is just encrypted and then
    transmitted. Using SZ1.4, 7.41% of battery in the case of the driver stress detection
    dataset and 7.42% of battery in case of gas sensors for home activity monitoring
    dataset were able to be saved. Through the use of compression algorithm SZ 2.1,
    battery life by up to 10.02% could be increased in the case of driver stress detection
    dataset and by 9.74% in the case of gas sensors for home activity monitoring dataset.
    By using Selective Compression algorithm, the percentage of battery saved in the
    case of the driver stress detection dataset was 8.89 and 8.98 in the case of gas
    sensor dataset. The ABFT algorithm was able to save the battery by 9.57% for the
    driver stress detection dataset and by 8.99% for the gas sensor dataset. Next
    the table shows the result for the digit rounding algorithm where 9.98% of battery
    in the case of the driver stress detection dataset and 9.45% of battery in case
    of gas sensors for home activity monitoring dataset were able to be saved. Lastly,
    we have the Improved SZ 2.1 algorithm with the highest percentage of battery saved
    in comparison to the previously mentioned algorithms by producing 10.12% and 9.75%
    of battery saved for the driver detection dataset and gas sensor dataset, respectively
    because original SZ2.1 use zstd lossless compression and improved SZ2.1 uses bzip2_compression
    [39]. To achieve high compression, bzip2 uses the Burrows-Wheeler transform, run-length
    encoding, move to front transform, and Huffman encoding techniques. By sorting
    the data, the Burrows-Wheeler transform helps in data compression. This produces
    a series of symbols that can be easily compressed using run-length encoding and
    Huffman encoding. The improved SZ compression algorithm is represented by Algorithm
    2. Table 3 Percentage of Battery Saved for Drivers Stress Detection Dataset Full
    size table Table 4 Percentage of Battery Saved for Gas Sensors Dataset Full size
    table Through the use of proposed improved SZ 2.1, battery life by up to 10.12%
    could be increased in the case of Driver Stress Detection Dataset and by 9.75%
    in the case of Gas Sensors for Home Activity Monitoring Dataset. Hence, the battery
    life of the device is improved even after using the encryption algorithm. The
    results also showed that more energy is saved when using the improved SZ 2.1 algorithm
    as compared to other state of art techniques. This is because for both datasets,
    the improved SZ 2.1 compression algorithm has a higher compression ratio as compared
    to others. A higher compression ratio implies lower encryption and transmission
    time which leads to lower energy consumption. The amount of energy spent in compressing
    the data is lower than the amount of energy saved during transmission. 5.2.4 Encryption
    time Encryption plays an important role in the security aspect. This subsection
    shows the result of the time taken to perform encryption. The time required by
    the ChaCha12-Poly1305 algorithm to process every block of data from the Stress
    Detection Dataset over the course of 241 periods is displayed in Fig. 8a. The
    vertical axis shows the time required on a logarithmic scale, while the horizontal
    axis shows the periods. The time needed by the ChaCha12-Poly1305 algorithm was
    greatly reduced by the use of the compression algorithm improved SZ 2.1. The average
    encryption time is 0.026 for improved SZ 2.1 and it is lower than remaining SZ
    1.4,the SZ 2.4, selective compression, ABFT, digit rounding algorithm. Hence the
    encryption time is lowest for the improved SZ 2.1 algorithm. Figure 8b shows the
    data from the gas sensors for the home activity monitoring dataset during 241
    periods. The average encryption time is 0.010 for improved SZ 2.1, 0.014 for SZ
    2.1, and 0.019 for the SZ 1.4 compression algorithm and its also lower than other
    algorithms selective compression, ABFT, and digit rounding algorithm. Hence once
    again, encryption time is the lowest for the improved SZ 2.1 algorithm. Equation
    (10) defines the formula for calculating encryption time, with the following parameters:
    \"size of data\" representing the total data volume intended for encryption, and
    \"Encryption speed\" denoting the rate at which the encryption algorithm handles
    data. Fig. 8 Encryption Time Full size image $$\\mathrm{Encryption\\;Time}= \\frac{Size\\;of\\;Data\\;(in\\;bytes)}{Encryption\\;Speed\\;(\\;bytes/
    sec)}$$ (10) 5.2.5 Total processing time This subsection includes the details
    of processing time that is, the time taken to process a data block. The overall
    processing time for each data block over 241 different periods is shown in Fig.
    9a. The Driver Stress Detection Dataset is where the information came from. On
    a logarithmic scale, the horizontal axis depicts the periods and the vertical
    axis the time. When the ChaCha12-Poly1305 technique is employed to secure data
    before transmission, the total time needed is at its greatest. The processing
    time is shortened by using the SZ 1.4, SZ 2.1, and improved SZ 2.1 compression
    algorithms. According to the Figure, the enhanced SZ 2.1 algorithm requires the
    least amount of processing time overall among the three compression techniques.
    Figure 9b shows data obtained from the gas sensors for the home activity monitoring
    dataset. The use of compression algorithms SZ 1.4, SZ 2.1, and the improved SZ
    2.1 reduces the time required for processing. For the Driver Stress Detection
    Dataset and the Gas Sensors for Home Activity Monitoring Dataset, on average,
    the total processing time required when using improved SZ 2.1 compression as well
    as encryption before transmitting is significantly lower than the processing time
    required when data is just transmitted without compression and encryption. It
    is clear from this that using compression and encryption together is possible
    without having a negative impact on the system. Even better, it reduces latency,
    which enhances the system. Fig. 9 Total Processing Time Full size image 5.2.6
    Error This subsection includes the results of error parameters generated while
    working on driver stress detection and gas sensor dataset, that is, the error
    produced in the decompressed data due to lossy compression. Figure 10a shows the
    error produced due to lossy compression in each data block during 241 periods.
    The data is obtained from the driver stress detection dataset. The absolute inaccuracy
    is shown by the vertical axis, while the horizontal axis indicates the periods.
    The error bound is set as 0.1. However, for all three algorithms the error remains
    around 0.05 well below the error bound. Figure 10b shows the data is obtained
    from the gas sensors for home activity monitoring dataset. Fig. 10 Error produced
    due to lossy compression Full size image 5.3 Impact of increase in the size of
    input data on SZ compression algorithm Figure 11a and b show that the time required
    for compression or decompression by all three algorithms increases linearly with
    size. Also, the time taken for decompression is always smaller than the time required
    for compression. This is because compression involves more complex calculations
    than decompression. Although the improved SZ 2.1 compression algorithm takes a
    longer time than the original SZ 2.1 algorithm for compression in the case of
    the gas sensors for home activity monitoring dataset, the algorithm can achieve
    much higher compression as compared to the original version. Therefore, the time
    required for encrypting and transmitting the data is less for the improved SZ
    2.1 compression algorithm and it can effectively reduce the total processing time
    required. Fig. 11 Compression and Decompression Time Full size image 5.4 Impact
    of increase in the size of input data on CHACHA8-POLY1305, CHACHA12-POLY1305,
    and CHACHA20-POLY1305 algorithms Figure 12a depicts the time required by the ChaCha12-Poly1305
    algorithm for processing blocks of different sizes. The encryption and decryption
    time required is approximately the same for all the data sizes. Hence the points
    in the Figure are overlapping. Additionally, the it shows that the time complexity
    of both encryption and decryption is linear. The time utilized by the algorithm
    increases linearly as the data size increases. Figure 12b depicts the time taken
    by the three AEAD algorithms viz. ChaCha8-Poly1305, ChaCha12-Poly1305 and ChaCha20-Poly1305
    algorithms for different data sizes. It shows that the time required by each algorithm
    increases as the size of the data block increases. Fig. 12 Encryption and Decryption
    Time for ChaCha12-Poly1305 algorithm.a. ChaCha12-Poly1305 algorithm. b. Three
    AEAD algorithms for blocks of different sizes Full size image Also, as the number
    of rounds in the ChaCha algorithm increases, so does the time required by the
    algorithm. Thus the ChaCha8-Poly1305 algorithm requires the least amount of time
    and the ChaCha20-Poly1305 requires the maximum amount of time while ChaCha12-
    Poly1305 requires an intermediate amount of time. However, the greater the number
    of rounds, the greater is the security of the algorithm. By choosing the 12 rounds
    variant we maintain a trade-off between security and performance. 6 Conclusion
    and future work In this paper, proposed algorithm (improved sz2.1) is compared
    with existing five state of art methodology. The usage of the improved SZ 2.1
    (compression algorithm) in combination with the ChaCha12-Poly1305 algorithm is
    presented for ensuring the data security in IoT devices and it increases the energy
    efficiency of the edge computing architecture system. Both the ChaCha12-Poly1305
    technique and the modified SZ 2.1 algorithm can be simply deployed for IoT devices
    on a real time solution because of their simplicity. The results of the experimental
    research demonstrate that this combination boosts the battery life of IoT devices
    while also securing data. This combination improves security while preserving
    about 10% of the device’s energy. Additionally, the latency is decreased, improving
    system performance. The improved SZ 2.1 compression algorithm achieves such high
    compression because of is a lossy compression algorithm. It also reduces the encryption
    time and the total processing time required by 95% and 98% for both the datasets.
    In this paper, the impact of the increase in input data size for the ChaCha12-Poly1305
    algorithm is also analyzed. The time taken by the ChaCha12-poly1305 algorithm
    increases linearly with size. Based on the performance of the ChaCha12-Poly1305
    algorithm, ChaCha8-Poly1305 algorithm, and ChaCha20-Poly1305 algorithm it is observed
    that ChaCha8-Poly1305 is faster than the ChaCha12-Poly1305 algorithm, as it provides
    very little security margin. On the other hand, the ChaCha20-Poly1305 algorithm
    is very slow and computationally expensive. Hence, by choosing the ChaCha12-Poly1305
    algorithm, a balance between the security-energy tradeoff is achieved. In order
    to strike a balance between power efficiency and security, our study primarily
    emphasizes energy conservation. We opt for a moderate level of security by implementing
    data compression during data transmission among IoT devices, as higher security
    levels tend to entail greater computational demands and costs. Consequently, for
    future work, we aimed at elevating security, we envision the potential for reducing
    the complexity of encryption algorithms. Our forthcoming work will target the
    reduction of computational time by exploring encryption algorithms with 20 rounds
    or more within the ChaCha Poly1305 encryption algorithm. Data availability Availability
    of data as per request. References Shi W, Cao J, Zhang Q, Li Y, Xu L (2016) Edge
    computing: Vision and challenges. IEEE Int Things J 3(5):637–646. https://doi.org/10.1109/JIOT.2016.2579198
    Article   Google Scholar   Cui L et al (2022) Security and Privacy-Enhanced Federated
    Learning for Anomaly Detection in IoT Infrastructures. IEEE Trans Indust Inform
    18(5):3492–3500. https://doi.org/10.1109/TII.2021.3107783 Article   Google Scholar   Liang
    X et al (2019) “Error-Controlled Lossy Compression Optimized for High Compression
    Ratios of Scientific Datasets,” in Proceedings. IEEE Int Conf on Big Data (Big
    Data) 438–447. https://doi.org/10.1109/BigData.2018.8622520 Patidar S, Panwar
    A, Garg A, KTV (2021) Secure and Energy-Efficient IoT Systems Using SZ 1.4 and
    ChaCha12-Poly1305. In: Computing and Signal Processing. Reddy, V.S., Prasad, V.K.,
    Wang, J., Reddy (eds). Advances in Intelligent Systems and Computing, vol 1325.
    Springer Singapore. https://doi.org/10.1007/978-981-33-6912-2_10 Bernstein DJ
    (2005) The poly1305-AES message-authentication code. Lecture Notes Comp Sci 3557:32–49.
    https://doi.org/10.1007/11502760_3 Article   Google Scholar   Underwood R, Calhoun
    JC, Di S, Apon A, Cappello F (2022) OptZConfig: Efficient Parallel Optimization
    of Lossy Compression Configuration. IEEE Trans Parallel Distributed Syst 33(12):3505–3519.
    https://doi.org/10.1109/TPDS.2022.3154096 Article   Google Scholar   Huerta R,
    Mosqueiro TS, Fonollosa J, Rulkov NF, Rodriguez-Lujan I (2016) Online Decorrelation
    of Humidity and Temperature in Chemical Sensors for Continuous Monitoring. Chemom
    Intell Lab Syst 157:169–176. https://doi.org/10.1016/j.chemolab.2016.07.004 Article   CAS   Google
    Scholar   “UCI Machine Learning Repository: Gas sensors for home activity monitoring
    Data Set.” [Online]. Available: http://archive.ics.uci.edu/ml/datasets/Gas+sensors+for+home+activity+monitoring.
    [Accessed: 25-Apr-2020]. Niwa N, Amano H, Koibuchi M (2022) Boosting the performance
    of interconnection networks by selective data compression. IEICE Trans Inf Syst
    105(12):2057–2065 Article   Google Scholar   Nir Y, Langley A (2015) RFC 7539:
    ChaCha20 and Poly1305 for IETF Protocols. RFC Editor, USA Nir Y, Langley A (2018)
    ChaCha20 and Poly1305 for IETF Protocols. RFC 7539. https://doi.org/10.17487/RFC7539
    Di S and Cappello F (2016) “Fast Error-Bounded Lossy HPC Data Compression with
    SZ,” in Proceedings - IEEE 30th International Parallel and Distributed Processing
    Symposium, IPDPS 2016 730–739. https://doi.org/10.1109/IPDPS.2016.11 Bernstein
    DJ (2008) The salsa20 family of stream ciphers. Lecture Notes in Computer Science
    including subseries Lecture Notes in Artificial Intelligence and Lecture Notes
    in Bioinformatics 4986:84–97. https://doi.org/10.1007/978-3-540-68351-3_8. (LNCS)
    Article   Google Scholar   S Lakshminarasimhan et al (2011) Compressing the incompressible
    with ISABELA: In-situ reduction of spatio-temporal data, in Lecture Notes in Computer
    Science including subseries Lecture Notes in Artificial Intelligence and Lecture
    Notes in Bioinformatics. LNCS 6852 (1): 366–379. https://doi.org/10.1007/978-3-642-23400-2_34
    Z Chen, SW Son, W Hendrix, A Agrawal, W K Liao, and A Choudhary (2015) “NUMARCK:
    Machine Learning Algorithm for Resiliency and Checkpointing,” in International
    Conference for High Performance Computing, Networking, Storage and Analysis, SC,
    2014, vol. -Janua 733–744. https://doi.org/10.1109/SC.2014.65. N Sasaki, K Sato,
    T Endo, S Matsuoka (2015) “Exploration of Lossy Compression for Application-Level
    Checkpoint/Restart,” in Proceedings - 2015 IEEE 29th International Parallel and
    Distributed Processing Symposium, IPDPS 2015. 914–922. https://doi.org/10.1109/IPDPS.2015.67
    Lindstrom P (2014) Fixed-rate compressed floating-point arrays. IEEE Trans Visual
    Comput Graphics. https://doi.org/10.1109/TVCG.2014.2346458,20(12),pp.2674-2683
    Article   Google Scholar   Healey JA, Picard RW (2005) Detecting stress during
    real-world driving tasks using physiological sensors. IEEE Trans Intell Transp
    Syst 6(2):156–166. https://doi.org/10.1109/TITS.2005.848368 Article   Google Scholar   Goldberger
    AL, Amaral LA, Glass L, Hausdorff JM, Ivanov PC, Mark RG, Mietus JE, Moody GB,
    Peng CK, Stanley HE (2000) PhysioBank, PhysioToolkit, and PhysioNet: components
    of a new research resource for complex physiologic signals. Circulation 101(23):E215–220.
    https://doi.org/10.1161/01.cir.101.23.e215 Article   CAS   PubMed   Google Scholar   Azar
    J, Makhoul A, Barhamgi M, Couturier R (2019) An energy efficient IoT data compression
    approach for edge machine learning. Futur Gener Comput Syst. https://doi.org/10.1016/j.future.2019.02.005,vol.96,pp.168-175
    Article   Google Scholar   M. Burtscher and P. Ratnaprabha (2007) “High throughput
    compression of double-precision floating-point data,” in Data Compression Conference
    Proceedings 293–302. https://doi.org/10.1109/DCC.2007.44 Lindstrom P, Isenburg
    M (2006) Fast and efficient compression of floating-point data. IEEE Trans Visual
    Comput Graphics 12(5):1245–1250. https://doi.org/10.1109/TVCG.2006.143 Article   Google
    Scholar   Tsai KL, Huang YL, Leu FY, You I, Huang YL, Tsai CH (2018) AES-128 based
    secure low power communication for LoRaWAN IoT environments. IEEE Access 6:45325–45334.
    https://doi.org/10.1109/ACCESS.2018.2852563 Article   Google Scholar   Tsai KL,
    Leu FY, You I, Chang SW, Hu SJ, Park H (2019) Low-Power AES Data Encryption Architecture
    for a LoRaWAN. IEEE Access 7:146348–146357. https://doi.org/10.1109/ACCESS.2019.2941972
    Article   Google Scholar   Roy S, Rawat U, Karjee J (2019) A Lightweight Cellular
    Automata Based Encryption Technique for IoT Applications. IEEE Access 7:39782–39793.
    https://doi.org/10.1109/ACCESS.2019.2906326 Article   Google Scholar   Guo X,
    Hua J, Zhang Y, Wang D (2019) A complexity-reduced block encryption algorithm
    suitable for internet of things. IEEE Access 7:54760–54769. https://doi.org/10.1109/ACCESS.2019.2912929
    Article   Google Scholar   Singh S, Sharma PK, Moon SY, Park JH (2017) Advanced
    lightweight encryption algorithms for IoT devices: survey, challenges and solutions.
    J Ambient Intell Humaniz Comput. https://doi.org/10.1007/s12652-017-0494-4,pp.1-1
    Article   Google Scholar   Saraiva DAF, Leithardt VRQ, de Paula D, Mendes AS,
    González GV, Crocker P (2019) PRISEC: Comparison of symmetric key algorithms for
    IoT devices. Sensors (Switzerland). https://doi.org/10.3390/s19194312,vol.19,no.19
    Article   Google Scholar   Mohd BJ, Hayajneh T (2018) Lightweight block ciphers
    for IoT: Energy optimization and survivability techniques. IEEE Access 6:35966–35978.
    https://doi.org/10.1109/ACCESS.2018.2848586 Article   Google Scholar   Diro AA,
    Chilamkurti N, Nam Y (2018) Analysis of Lightweight Encryption Scheme for Fog-to-Things
    Communication. IEEE Access 6:26820–26830. https://doi.org/10.1109/ACCESS.2018.2822822
    Article   Google Scholar   AO Akmandor, H Yin, NK Jha (2018) “Simultaneously ensuring
    smartness, security, and energy efficiency in Internet-of-Things sensors,” in
    2018 IEEE Custom Integrated Circuits Conference, CICC 1–8. https://doi.org/10.1109/CICC.2018.8357069
    A Fragkiadakis, E Tragos, L Kovacevic, P Charalampidis (2016) A practical implementation
    of an adaptive Compressive Sensing encryption scheme,” in WoWMoM 2016 - 17th International
    Symposium on a World of Wireless, Mobile and Multimedia Networks, 2016. https://doi.org/10.1109/WoWMoM.2016.7523561
    Zhang Y, Xiang Y, Zhang LY, Rong Y, Guo S (2019) Secure Wireless Communications
    Based on Compressive Sensing: A Survey. IEEE Commun Surv Tutor 21(2):1093–1111.
    https://doi.org/10.1109/COMST.2018.2878943 Article   Google Scholar   Qi J, Hu
    X, Ma Y, Sun Y (2015) A hybrid security and compressive sensing-based sensor data
    gathering scheme. IEEE Access 3:718–724. https://doi.org/10.1109/ACCESS.2015.2439034
    Article   Google Scholar   F de Santis, A Schauer, G Sigl (2017) ChaCha20-Poly1305
    authenticated encryption for high-speed embedded IoT applications,” in Proceedings
    of the 2017 Design, Automation and Test in Europe, DATE 2017 692–697. https://doi.org/10.23919/DATE.2017.7927078
    Luangoudom S, Nguyen T, Tran D and Nguyen LG (2019) “End to end message encryption
    using Poly1305 and XSalsa20 in Low power and Lossy Networks,” in Proceedings of
    2019 11th International Conference on Knowledge and Systems Engineering, KSE 2019.
    https://doi.org/10.1109/KSE.2019.8919479 Levine J (2012) “The ‘application/zlib’
    and ‘application/gzip’ Media Types,” no. 6713. RFC Editor 1–4. https://doi.org/10.17487/RFC6713
    Ibarria L, Lindstrom P, Rossignac J, Szymczak A (2003) Out-of-core compression
    and decompression of large n-dimensional scalar fields. Computer Graphics Forum
    22(3):343–348. https://doi.org/10.1111/1467-8659.00681 Article   Google Scholar   Collet
    Y, Kucherawy M (2018) RFC 8478: Zstandard compression and the application/zstd
    media type. RFC Editor, USA. https://doi.org/10.17487/RFC8478 “bzip2: Home.” [Online].
    Available: https://www.sourceware.org/bzip2/. [Accessed: 21-May-2020]. Aumasson
    JP, Fischer S, Khazaei S, Meier W and Rechberger C (2008) “New features of Latin
    dances: Analysis of Salsa, ChaCha, and Rumba,” in Lecture Notes in Computer Science
    (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes
    in Bioinformatics) , vol. 5086 LNCS, pp. 470–488. https://doi.org/10.1007/978-3-540-71039-4_30
    Li S et al (2021) Resilient error-bounded lossy compressor for data transfer.
    In: Proceedings of the international conference for high performance computing,
    networking, storage and analysis (2021, St. Louis, MO), association for computing
    machinery (ACM). https://doi.org/10.1145/3458817.3476195 Delaunay X, Courtois
    A, Gouillon F (2019) Evaluation of lossless and lossy algorithms for the compression
    of scientific datasets in netCDF-4 or HDF5 files. Geoscientific Model Development
    12(9):4099–4113 Article   ADS   Google Scholar   Download references Acknowledgements
    Abidance with ethical standards. Author information Authors and Affiliations Delhi
    Technological University, Delhi, India Sanjay Patidar & Rajni Jindal Indian Institute
    of Technology, Roorkee, Haridwar, India Neetesh Kumar Corresponding author Correspondence
    to Sanjay Patidar. Ethics declarations Ethical approval This article does not
    contain any studies on human participants or animals performed by any of the authors.
    Informed consent Informed consent was obtained from all individual participants
    included in the study. Conflict of interest The authors declare that they have
    no conflict of interest. Additional information Publisher’s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society
    or other partner) holds exclusive rights to this article under a publishing agreement
    with the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Patidar, S., Jindal, R. & Kumar, N. A secure and energy-efficient
    edge computing improved SZ 2.1 hybrid algorithm for handling iot data stream.
    Multimed Tools Appl (2024). https://doi.org/10.1007/s11042-024-18765-0 Download
    citation Received 15 September 2022 Revised 30 September 2023 Accepted 24 February
    2024 Published 19 March 2024 DOI https://doi.org/10.1007/s11042-024-18765-0 Share
    this article Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords IoT Power consumption Security Encryption Edge computing Use
    our pre-submission checklist Avoid common mistakes on your manuscript. Sections
    Figures References Abstract Introduction Literature review Background Proposed
    approach & experimental analysis Result and discussion Conclusion and future work
    Data availability References Acknowledgements Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Multimedia Tools and Applications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A secure and energy-efficient edge computing improved SZ 2.1 hybrid algorithm
    for handling iot data stream
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Karnehm D.
  - Samanta A.
  - Neve A.
  - Williamson S.
  citation_count: '0'
  description: 'It is needless to mention that the proven capability of cloud computing
    and digital twin-based monitoring and control systems will play a major role in
    the implementation of digital twin-based lithium-ion battery management systems
    (BMS) for e-transportation. In this context, the combination of the Internet of
    Things (IoT) and fog computing emerges as an enabling technology that facilitates
    bidirectional data communication and processing. This paper proposes a five-layer
    IoT framework for the implementation of cloud-based battery monitoring and BMS.
    Additionally, it critically analyzes the existing IoT frameworks applied to BMS
    and justifies the suitability of the five-layer architecture. Furthermore, the
    advantages of fog computing, such as low latency, fault tolerance, limitless systems,
    multiple applications, and shared responsibility, are discussed. The paper also
    explores four previously proposed computing-enabled architectures: centralized,
    hybrid, distributed, and hybrid-distributed computing. Moreover, the potential
    of utilizing hybrid computing architectures, incorporating fog computing as a
    supplement to cloud computing, is highlighted to address fault, privacy, or latency
    issues in cloud-based BMS.'
  doi: 10.1109/SGRE59715.2024.10428786
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2024 4th International Confer...
    Five-layer IoT and Fog Computing Framework Towards Digital Twinning of Battery
    Management Systems for e-Transportation Publisher: IEEE Cite This PDF Dominic
    Karnehm; Akash Samanta; Antje Neve; Sheldon Williamson All Authors 31 Full Text
    Views Abstract Document Sections I. Introduction II. Internet of Things in BMS
    III. Fog Computing for BMS IV. Experimental Validation V. Potentials and Challenges
    Show Full Outline Authors Figures References Keywords Metrics Abstract: It is
    needless to mention that the proven capability of cloud computing and digital
    twin-based monitoring and control systems will play a major role in the implementation
    of digital twin-based lithium-ion battery management systems (BMS) for e-transportation.
    In this context, the combination of the Internet of Things (IoT) and fog computing
    emerges as an enabling technology that facilitates bidirectional data communication
    and processing. This paper proposes a five-layer IoT framework for the implementation
    of cloud-based battery monitoring and BMS. Additionally, it critically analyzes
    the existing IoT frameworks applied to BMS and justifies the suitability of the
    five-layer architecture. Furthermore, the advantages of fog computing, such as
    low latency, fault tolerance, limitless systems, multiple applications, and shared
    responsibility, are discussed. The paper also explores four previously proposed
    computing-enabled architectures: centralized, hybrid, distributed, and hybrid-distributed
    computing. Moreover, the potential of utilizing hybrid computing architectures,
    incorporating fog computing as a supplement to cloud computing, is highlighted
    to address fault, privacy, or latency issues in cloud-based BMS. Published in:
    2024 4th International Conference on Smart Grid and Renewable Energy (SGRE) Date
    of Conference: 08-10 January 2024 Date Added to IEEE Xplore: 15 February 2024
    ISBN Information: DOI: 10.1109/SGRE59715.2024.10428786 Publisher: IEEE Conference
    Location: Doha, Qatar SECTION I. Introduction Effective battery management system
    (BMS) plays a vital role in enhancing battery performance, ensuring safety, and
    prolonging cell life [1]. In addition to the battery pack control, the estimation
    of battery states is an important part of modern BMS [2]. These states are, such
    as state of charge (SOC), state of health (SOH), and the core temperature. Over
    the years, data-driven methods and digital twins have been proposed to estimate
    battery states [3], [4]. NASA defined a digital twin as a simulation of the physical
    object based on historical data and current sensor measurements, to estimate and
    forecast the state of the physical object [5]. In addition to the digital twin,
    the digital model and the digital shadow must also be defined for clear definition
    and differentiation [6]. A digital model is defined as a static representation
    of the physical object. A digital shadow includes a one-way data flow from the
    physical object to the digital representation of the object to update the state
    of the shadow. Such a digital shadow can be used to monitor the physical object.
    A digital twin contains a fully interconnected automated data flow with the physical
    object. This enables automated control of the physical object based on the knowledge
    created by the digital twin. Assume that a BMS using a digital twin for state
    estimation and control requires bidirectional data communication between the physical
    object and the digital representation. Furthermore, larger data storage and higher
    processing power compared to the classical approach of BMS. Especially to enable
    highly accurate machine learning methods as digital representations to estimate
    the state of the battery cell and the battery pack. To use such proposed methods,
    the problem of necessarily increasing computational resources and access through
    historical data, adopting the models, can be faced from two sites. On the one
    hand, the installed computing power and storage on the device can be increased.
    On the other hand, the scalability and flexibility of cloud computing technologies
    can be utilized. This paper is concerned with the second method of using the possibilities
    of cloud computing and its technologies. For this purpose, Internet of things
    (IoT) and fog computing are some of the primary enabling technologies for cloud
    computing. A. Internet of Things Internet of things (IoT) connects physical devices
    to the internet. Connected devices share data measured by sensors and data through
    operation tasks [8]. These data are sent to cloud or on-premise nodes to analyze
    and operate the data. Depending on the specific application, different connectivity,
    networking, communication technologies, and protocols are used [9]. There are
    a wide variety of networking technologies, such as 5G, LTE, LoRa, ZigBee, Wi-Fi,
    Bluetooth 5, and other cellular networks. The different cellular networks can
    be utilized in different areas of application depending on the required bandwidth,
    data range, and power usage [7]. A comparison of these technologies is shown in
    Table I. In the case of public e-transportation, primary LTE and 5G as the next
    generation of broadband cellular networks must focus on research and development.
    This is because existing infrastructures can be used and the available bandwidth.
    Other technologies, such as Wi-Fi, LoRa, and ZigBee, may be suitable for localized
    applications, such as buses in amusement parks and cargo vehicles at factory sites.
    In addition to communication technologies, another important decision to make
    during the conception of a IoT application is the choice of the application layer
    protocol. Multiple application layer protocols exist, such as HTTP REST, MQTT,
    AMQP, CoAP, XMPP, DDS, and WebSocket. Table II shows a comparison between the
    protocols listed. The table distinguishes between the two transmission patterns,
    publish/subscriber, and request/responce. The publisher/subscriber pattern is
    a messaging pattern based on the principle of loose coupling, where publishers
    generate messages and send them to a message broker, which distributes them to
    interested subscribers. The process of the request/response pattern is that a
    client sends a request to a server and waits for the corresponding response. The
    server processes the request and sends back the requested information as a response.
    The MQTT and HTTP REST protocols are the main protocols used today in the field
    of IoT. Not only the analysis conducted by Bayilmis et al. [10] shows this by
    the use in research studies nowadays. Furthermore, the fact that cloud service
    providers, such as Amazon Web Services (AWS), Google Clouds, and Microsoft Azure,
    supply mainly these protocols highlights the importance of these two protocols
    [11–13]. B. Fog Computing Fog Computing is a computational paradigm that acts
    as a bridge between the device and the cloud [14], [15]. The fog is highly distributed
    and highly integrated with the cloud to bring the possibilities of cloud services
    close to the device and reduce the communication latency between the device and
    the cloud services. In Fig. 1 the schema of cloud computing and fog computing
    is shown. In Fig. 1a the devices are connected directly to the cloud. This increases
    the complexity of such systems. Furthermore, Fig. 1b shows the layer structure
    including the fog layer. The distributed fog nodes facilitate local buffering
    of data and execution of time-critical calculations. Habibi [16] clarifies that
    there is no consensus definition of fog computing, and some authors use it as
    a synonym of edge computing and others as an extension of cloud computing. In
    this paper, the concept of fog computing is defined as the delivery of services
    and storage on the network edge, with the objective of reducing latency and improving
    Quality of Service (QoS) [17]. Fig. 1. Cloud computing and Fog computing models
    Show All SECTION II. Internet of Things in BMS Sivaraman et al. [18] applied the
    three-layer IoT framework proposed by the IEEE Internet Initiative [19] through
    the application of BMS. The concept behind proposing such a general framework
    is to generalize the structure, define the required levels and connections between
    different components, and set the data flow and dependencies between them. Fig.
    2a shows the framework proposed by Sivaraman et al. [18]. The first layer, the
    perception layer includes the physical components, thus the sensors and the battery
    pack. The network layer connects the BMS with cloud services. It is also used
    to process and transfer. Specific applications, including storage, are provided
    by the application layer. As the arrows in the figure indicate, a two-way data
    flow is not scheduled in this framework. Only data exchange from the device to
    the cloud is foreseen. Furthermore, the cloud service provider AWS introduced
    at the end of 2021 a service called AWS IoT FleetWise [20]. The architectural
    layers of the platform are shown as proposed in Fig. 2b [21]. The approach of
    this service is to provide a highly scalable platform for collecting, transforming,
    and transferring vehicle data to the cloud in close to real-time. These data can
    be used to analyze the state of the fleet in a highly automated manner. By gathering
    information for crucial use cases from customer fleets, Continental expects to
    save up to 50% on field operation test (FoT) costs, as well as lower the cost
    of routine software upgrades by up to 25% and provide software updates to customers
    faster [21]. TABLE I Comparison of Wireless Communication Technologies and their
    Parameters [7] TABLE II Comparison of Different Application Layer Protocols [10]
    These proposed frameworks cannot provide input to control the battery pack. Toward
    the cloud-based BMS and a real digital twin of the battery pack as defined previously,
    it is necessary to enable a bidirectional data flow between the physical device
    and the digital representation. Therefore, in Fig. 3 a IoT framework based on
    the five-layer framework by Guth et al. [22] applied through BMS is proposed,
    which enables a fully automated bidirectional data flow. Also, this framework
    includes a gateway. This is responsible for the communication technologies and
    protocols between the local device and the cloud and also converts the data into
    the necessary format, such as between binary Controller Area Network (CAN) and
    JavaScript Object Notation (JSON). CAN is a standard that is extensively utilized
    in the automotive sector to enable communication between microcontrollers and
    devices. On the other hand, JSON is a format that uses text to represent structured
    data and is widely used for data transfer in web applications. It offers a lightweight
    solution for data storage and transmission and is frequently employed to transport
    data between servers and clients over the Internet. SECTION III. Fog Computing
    for BMS Beside the IoT framework, also cloud computing in combination with fog
    computing will also have a huge impact in cloud-based BMS developments. To decrease
    the delay between the physical BMS, and its digital twin, fog computing will be
    an essential component. Yang et al. [23] proposes a framework for cloud-based
    BMS that includes four subsystems: end, edge, cloud, and knowledge. To enable
    time critical functionalities, the local edge device is used. Such an edge device
    reduces the communication between the device and the cloud. The cloud component
    can be used to run not as time critical functions such as SOH estimation, adaptive
    control, fallout prediction, and early warning of safety. The study by Yang et
    al. expected, a research gap can be identified in the field of edge and fog computing
    for cloud-based BMS. The results in the field of industrial internet of things
    (IIoT), and smart cities can be partly adopted for the field of e-transportation.
    Due to the assumed infrastructure in both fields, issues like rural areas, scaling
    for mobile devices around the globe, and network connection losses are only minor
    factors. Nevertheless, concepts and frameworks how to use fog computing in these
    fields can be analyzed and adopted for e-transportation and cloud-based BMS. Fig.
    2. Existing IoT frameworks for BMS and vehicles. The applied three-layer IoT framework
    (a) and the Aws IoT Fleetwise Connected vehicle platform (b) [18], [21] Show All
    Fig. 3. Five layer IoT framework proposed Guth [22] appplied for BMS Show All
    Basir et al. [24] highlight fog computing as a major component to enable more
    effective, efficient and manageable communication in the case of a large number
    of IoT devices. To allow low latency, high security, location awareness, and real-time
    connectivity for IIoT. With an increase in the number of connected vehicles to
    enable intelligent vehicles, the communication bandwidth to central data centers
    can become a bottleneck. This will increase the round-trip time in communication
    between vehicles and the cloud infrastructure [25]. Therefore, edge computing
    extends the computing and caching capabilities to network edges. Maheswaran et
    al. [26] describe five main challenges to overcome to achieve the goal of fog
    computing for autonomous driving. The expected data rate and computing power for
    autonomous driving is even higher compared to the requirements of online BMS,
    but the challenges are similar. The authors highlighted the following points:
    Low Latency: To enable the main control components in the fog, the latency between
    the fog component and the device is a primary object. Fault Tolerance: The challenge
    is to find mechanisms to ensure the accessibility of the vehicle service, even
    if a fog component fails. This is necessary to prevent the failure of local vehicle
    groups. Limitless System: The architecture of the fog must be designed in such
    a way that the architecture scales with the territorial size. So, the request
    time must be independent by the area covered and thus with the quantity of nodes.
    Multiple Applications: The combination of different applications, such as autonomous
    driving, optimal routing of individual vehicles, or battery management to increase
    vehicle range. Shared Responsibility: Actions to control the vehicle have local
    inputs directly from the device and the fog. Fig. 4. Centralized (a), hybrid (b),
    distributed (c), and hybrid-distributed (d) computing-enabled architectures [27]
    Show All To enable cloud-based BMS many smart city approaches can be applied,
    especially vehicle-to-everything (V2X). V2X is an emerging technology that improves
    the mobility and efficiency of urban traffic operations and driver safety. V2X
    includes vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), vehicle-to-network
    (V2N) and vehicle-to-pedestrian (V2P) communications [28]. In [27] the authors
    describe four different methods of data storage and computing architectures for
    connected vehicles proposed and discussed in the literature. Fig. 4 illustrates
    these four different architectures: centralized (4a), hybrid (4b), distributed
    (4c), and hybrid distributed (4d). They differ in the location of computing and
    storing: Centralized: The computing tasks are executed in distributed computing
    units, such as fog nodes and cloud computing nodes. The connected vehicle only
    generates computing tasks, uploads them and process the result. Local execution
    of computing tasks is not scheduled. Hybrid: The vehicle generates computing tasks
    and computes them locally or uploads them to distributed computing units and processes
    the result. Distributed: A cluster of connected vehicles shares computational
    and storage resources. Here, a vehicle generates a computing task, which is executed
    by one or more of the connected vehicles in the cluster. Hybrid-Distributed: A
    cluster of connected vehicles is also connected to distributed computing units,
    such as fog nodes or cloud computing nodes. Thus, a generated computing task can
    be executed locally, by other connected vehicles, or by unit of distributed computing
    units. In the case of centralized computing, as shown in Fig. 4a, the on-board
    BMS would be completely replaced by the cloud-based BMS, and all control would
    take place in fog, or cloud nodes. In the case of network loss, no control would
    be possible anymore. This would greatly increase the probability of errors due
    to faulty data packets, temporary or permanent network loss. For scenarios, like
    in fabric properties, where network coverage can be ensured and temporary network
    losses can be excluded or rapid recovery is ensured, this could be a chosen architecture.
    A distributed network scenario, as shown in Fig. 4c, would be the case if the
    cars shared computational power to calculate computationally expensive tasks.
    One scenario for this would be a highly accurate SOC, SOH, and core temperature
    estimation. Therefore, a cluster of vehicles could share data and computational
    power as a computing cluster. To enable such an architecture, multiple issues
    must be resolved, such as interoperability between different resellers, fault
    tolerance, privacy, and security. In addition, the control of computing and networking
    increases in this architecture. The same issues would arise with the hybrid-distributed
    architecture, as shown in Fig. 4d. In this architecture, the resources are shared
    between the vehicles in a cluster, also with fog nodes, and cloud computing resources.
    To share data, like in the concept of vehicle-to-vehicle (V2V), for automotive
    driving, such a concept could be helpful, especially for safety issues like accident
    warning or the early indication of a traffic jam, or swarm-based fleet charge
    control. Hybrid computing, as seen in Fig. 4d, enables computing on the vehicle
    and on fog notes. This gives the possibility to use adaptive resources of the
    fog to use computing and storage resources for not as time-dependent, or for computational
    intensive calculations. This architecture gives huge potential by using additional
    services in the fog and cloud, but the car is still not totally dependent on the
    network connection. Therefore, a connection failure will not result in an inoperable
    state of the vehicle. Fig. 5. Experimental test-bed for the real-time battery
    monitoring and state estimation in cloud Show All Fig. 6. Real-time visualiation
    of measured and estimated battery states in cloud Show All SECTION IV. Experimental
    Validation To validate the effectiveness of the proposed five-layer IoT framework
    for the BMS application, an experimental setup is developed for real-time battery
    monitoring and control. The components of the experimental setup conjugated to
    the layers of the proposed framework in Fig. 3 can be seen in Fig. 5. The experimental
    setup consists of a single cell with temperature, current, and voltage sensors,
    a BMS to control the discharge, and a Raspberry Pi with a CAN shield. Cumulatively,
    the experimental setup represents the physical device in this study. Apache Kafka,
    an open-source event-stream platform, is used as IoT integration middleware. The
    application level is capable of providing multiple services for a cloud-based
    BMS which includes SOC estimation, long-term storage for future analytics, and
    real-time monitoring and visualization of the system parameters. Real-time visualization
    of the measured and estimated battery parameters SOC can be seen in Fig. 6. SECTION
    V. Potentials and Challenges In the field of IoT and fog computing for BMS mainly
    for e-transportations applications multiple questions are already been towards
    implementing cloud-based BMS. Nevertheless, many challenges still need to be addressed.
    There is a lack of research on how to deal with loss of connectivity issues and
    the ways to distribute and manage fog computing nodes, especially in rural areas.
    The global urban region has a coverage of 97% with 4G connectivity. In rural areas,
    it covers only 76% of the population [29]. To the best of our knowledge, the details
    of the coverage of the road network have not been published. To enable smart cloud-based
    BMS, it is an open question how to enable fog computing in rural areas and how
    to handle areas without network connectivity. Fallback methods must be proposed
    to ensure vehicle usability in the case of network loss. In addition, control
    mechanisms must be developed at higher latencies due to the longer distances between
    the fog nodes and the electrical vehicles in rural areas. If it comes to real-world
    implementations, it remains to be seen by whom these fog nodes will be operated,
    by automakers like Tesla, BMW, and BYD. For example, additional hardware can be
    implemented with more than 45,000 superchargers worldwide operated by Tesla [30].
    Furthermore, company consortiums of different electric vehicle companies can also
    collaboratively install common systems to provide cloud computing service providers
    in collaboration with cloud service providers like AWS, Microsoft Azure, Google
    Cloud, and so on. In [27] different kinds of computing-enabled architectures were
    discussed. Among them the distributed and hybrid distributed architectures, as
    shown in Fig. 4c and 4d, are highlighted in this study. The concept of sharing
    computational resources over a cluster of vehicles is an interesting topic of
    research especially the ways to orchestrate computational tasks while ensuring
    privacy and security. In addition, handling the leaf of a single vehicle from
    a cluster by identifying the driving pattern would also be an interesting topic.
    For the implementation in commercial fleets, questions such as interoperability
    between car manufacturers, privacy, and benefits of such an architecture for customers,
    and manufacturers by reducing the total ownership cost by providing financial
    incentives can be explored. In the case of bus fleets or vehicles on a company
    campus, such an architecture could be useful to utilize existing capacities more
    efficiently. In a more applied manner, the hybrid architecture seen in Fig. 4b
    araises high opportunities. The potential to utilize additional computational
    and storage resources in addition to the hardware installed in the vehicle allows
    flexibility in the development of methods and services used for electrical vehicles.
    Furthermore, cloud computing is seen as an enabler for digital twinning-based
    BMS to improve on-board SOC estimation by adopting model parameters, or cloud-based
    SOH and core temperature estimation. The benefit of the scalable resources of
    the cloud can also be used on a limited level in the fog, but with the benefit
    of low latencies compared to cloud computing due to the closer physical location
    of the computing resources from the end user. In addition, the fog layer can be
    used as a data aggregator and the filter layer can be used to reduce the amount
    of data needed to transfer to the cloud. SECTION VI. Conclusion A detailed discussion
    on the three-layer, four-layer, and five-layer IoT framework, and the AWS IoT
    Fleetwise platform architectural layers towards developing digital twin enabled
    BMS is presented in this paper. The three-layer framework is only suitable for
    monitoring whereas the four-layer IoT framework is suitable for both monitoring
    and controlling functionality of a BMS. The five-layer framework includes Sensor
    and actor, device, gateway, IoT integration middleware, and application found
    to be the best suitable for the digital twinning of BMS for e-mobility applications.
    Furthermore, fog computing has been shown as an enabler for time-dependent cloud-based
    BMS functionalities. The scalability and nevertheless local alignment of fog computing
    in combination with cloud computing infrastructure for computationally intensive
    but less time-dependent tasks. A significant research and development potential
    is also noticed in the field of digital twinning and cloud-based BMS and the application
    of IoT and fog computing in addition to cloud computing. Authors Figures References
    Keywords Metrics More Like This Integration of Cloud Computing with Internet of
    Things for Network Management and Performance Monitoring 2020 18th International
    Conference on ICT and Knowledge Engineering (ICT&KE) Published: 2020 A Universal
    Complex Event Processing Mechanism Based on Edge Computing for Internet of Things
    Real-Time Monitoring IEEE Access Published: 2019 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 4th International Conference on Smart Grid and Renewable Energy, SGRE 2024
    - Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Five-layer IoT and Fog Computing Framework Towards Digital Twinning of Battery
    Management Systems for e-Transportation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ovesen A.B.
  - Nordmo T.A.S.
  - Riegler M.A.
  - Halvorsen P.
  - Johansen D.
  citation_count: '0'
  description: Uncontrolled over-fishing has been exemplified by the UN as a serious
    ecological challenge and a major threat to sustainable food supplies. Emerging
    trends within governing bodies point towards digital solutions by deploying CCTV-based
    video monitoring systems on a large scale. We conjecture that such systems are
    not feasible when reliant on satellite broadband in remote areas, and expose workers
    aboard fishing vessels to unneeded manual surveillance. To facilitate this, we
    propose Dorvu, a AI-based multimedia distributed storage system designed for edge
    environments, with a specific focus on commercial fishery monitoring. Dorvu addresses
    the challenges of secure data storage, fault tolerance, availability, and remote
    access in hostile edge environments. The system employs a novel data distribution
    scheme involving sensor readings and AI video content extraction to ensure the
    preservation of forensic evidence even in unstable conditions. Experimental evaluations
    demonstrate the feasibility of real-time multimedia data collection, analysis,
    and distribution in networks of edge devices on-board active fishing vessels.
    Dorvu offers a practical alternative to current governmental surveillance trends
    that compromise data security and privacy, and we propose it as a solution for
    edge-based forensic data management in commercial fisheries and similar applications.
  doi: 10.1007/978-3-031-53311-2_24
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home MultiMedia Modeling Conference
    paper Sustainable Commercial Fishery Control Using Multimedia Forensics Data from
    Non-trusted, Mobile Edge Nodes Conference paper First Online: 28 January 2024
    pp 327–340 Cite this conference paper Access provided by University of Nebraska-Lincoln
    Download book PDF Download book EPUB MultiMedia Modeling (MMM 2024) Aril Bernhard
    Ovesen, Tor-Arne Schmidt Nordmo, Michael Alexander Riegler, Pål Halvorsen & Dag
    Johansen  Part of the book series: Lecture Notes in Computer Science ((LNCS,volume
    14556)) Included in the following conference series: International Conference
    on Multimedia Modeling 331 Accesses Abstract Uncontrolled over-fishing has been
    exemplified by the UN as a serious ecological challenge and a major threat to
    sustainable food supplies. Emerging trends within governing bodies point towards
    digital solutions by deploying CCTV-based video monitoring systems on a large
    scale. We conjecture that such systems are not feasible when reliant on satellite
    broadband in remote areas, and expose workers aboard fishing vessels to unneeded
    manual surveillance. To facilitate this, we propose Dorvu, a AI-based multimedia
    distributed storage system designed for edge environments, with a specific focus
    on commercial fishery monitoring. Dorvu addresses the challenges of secure data
    storage, fault tolerance, availability, and remote access in hostile edge environments.
    The system employs a novel data distribution scheme involving sensor readings
    and AI video content extraction to ensure the preservation of forensic evidence
    even in unstable conditions. Experimental evaluations demonstrate the feasibility
    of real-time multimedia data collection, analysis, and distribution in networks
    of edge devices on-board active fishing vessels. Dorvu offers a practical alternative
    to current governmental surveillance trends that compromise data security and
    privacy, and we propose it as a solution for edge-based forensic data management
    in commercial fisheries and similar applications. Keywords multimedia storage
    edge computing privacy digital forensics file systems Access provided by University
    of Nebraska-Lincoln. Download conference paper PDF Similar content being viewed
    by others Performance, Resilience, and Security in Moving Data from the Fog to
    the Cloud: The DYNAMO Transfer Framework Approach Chapter © 2018 Framework for
    Intelligent Wildlife Monitoring Chapter © 2020 IoT with Multimedia Investigation:
    A Secure Process of Digital Forensics Chain-of-Custody using Blockchain Hyperledger
    Sawtooth Article 24 December 2022 1 Introduction Sustainability issues resulting
    from criminal over-fishing has resulted in several governments around the world
    proposing and implementing remote video surveillance as a form of continuous monitoring
    of fishing trawlers. Fishery video monitoring is for example mandated by law in
    Denmark, and it is expected that the majority of Danish trawlers will be equipped
    with CCTV monitoring systems during 2023 [28]. Similar policies are expected to
    be implemented in New Zealand [22] and has been proposed in Norway [15]. In the
    European Union, compulsive video surveillance has been mandated in cases where
    trawlers are known to catch beyond their assigned quota [17]. These policies have
    been characterized as antagonizing by some of those working aboard fishing vessels
    [14, 25]. At the same time, the dependence on food from the ocean is growing worldwide
    [5], while over-fishing is increasingly becoming a threat to global resources
    [29]. The combination of criminal over-fishing, increased global reliance on fish,
    and effects of climate change, can have significant consequences both for life
    in the ocean and the global population [26]. Our work focuses on providing an
    alternative to proposed solutions for commercial fishery surveillance programs
    in Norway, but this foundation does not limit our work from being deployed in
    other areas or scenarios. Ongoing going governmental proposals for deployments
    in fishing vessels include video streaming, manual surveillance, and automatic
    logging of catch [15]. We conjecture that edge systems for surveillance in the
    commercial fishery domain should attempt to (i) minimize the negative impact from
    introducing workplace surveillance [25], (ii) provide the ability to automatically
    process video information without human input [1] (iii) provide mechanisms to
    limit the consumption of video data, even for privileged users (i.e. federal fishery
    authorities) [21], and (iv) be specifically designed for the difficult edge environments
    found onboard active fishing vessels [18] with regards to limited connectivity,
    risk of system faults, and tampering from malicious actors. To that end, we have
    developed the Dorvu 1 multimedia distributed storage system, designed for robust
    and secure data collection, analysis and storage, in hostile edge environments.
    Dorvu’s primary objective is 24/7 reliable data collection, real-time analysis,
    and protection in non-trusted off-shore edge environments where adversaries might
    be local to the data monitored, analyzed, and stored. Due to very limited connectivity
    from the isolated and remote Arctic ocean, a secondary objective is to support
    reliable satellite transmission of locally analyzed event data of potential relevance
    to cloud-connected mainland governmental surveillance nodes. The threat model
    includes physical attacks and environmental instability, emphasizing system robustness.
    We propose an architecture containing a distributed storage layer that is configured
    in real-time by automatic multimedia processing and data extraction, in addition
    to input from privileged remote clients. The Dorvu system is designed for deployment
    and data collection on-board active fishing vessels, while clients can access
    the system remotely from mainland nodes through satellite connections. Clients
    are able to issue system configuration patches in the form of data policies, denoting
    filters for data that is important for the remote operation, for example as forensic
    evidence of fishery activities. 1.1 Related Work The storage and analysis of visual
    data on specialized systems at edge as been implemented in previous works. Winkler
    et al. [32] define a class of visual sensor networks that enable reactive monitoring
    for enforcement, for example traffic monitoring. While reactive networks trigger
    data recording after certain events, Bischof et al. [3] note that an additional
    solution is to analyze a continuous stream of data to detect these events as part
    of the same on-board data pipeline. Privacy-preservation is a topic in certain
    visual sensor networks; Fitwi et al. [6] define that privacy in video surveillance
    is achieved when people cannot be identified by human observers without their
    knowledge and consent. Collection and storage of sensitive data at the edge have
    been explored in [2, 32]. While traditional secure storage systems rely on keys
    for static confidentiality and authorization, when producing potentially privacy-sensitive
    information, the actual contents of data or meta-data can be important to enforce
    a principle of least privilege while maintaining the ability to extract information
    [9, 33]. Traditional distributed storage systems typically do not optimize for
    low bandwidth and fluctuating computation capabilities, while dedicated low bandwidth
    systems are aimed at compression or minimizing the transfer of redundant data
    [16], i.e., data that already exists at the recipient, likely already delivered
    in a different context. The contents of certain data objects at clients operating
    at the recipient is of no concern to the underlying file system, because generalized
    distributed storage systems are designed to provide a storage layer for numerous
    applications whose critical operations may rely on insertion and retrieval of
    arbitrary data objects [8]. Distributed fault tolerance schemes aim to allow systems
    to continue all of these operations despite failures of certain components [13].
    A domain-specific system may utilize domain knowledge to make intelligent decisions
    in the face of different faults. They can evaluate failures and allow them to
    persist in order to prioritize other components, or dynamically schedule operations
    based on their usefulness to specialized clients, sacrificing the general usefulness
    of the system to retain critical operations. 2 System Overview Dorvu is a distributed
    multimedia storage system for the edge, that is designed as a supplement to ongoing
    efforts of digitization and remote surveillance of commercial fishery operations
    in the European Economic Area (EEA). It is developed as a middleware system that
    can be deployed on numerous compute nodes, and provides robust, privacy-preserving
    multimedia storage and access for data collection in low-bandwidth and remote
    areas. The system is deployed with a pre-configured number of compute nodes, each
    of which with its own storage device attached. Nodes can be connected to their
    own video input devices as their main source of data input. These nodes are deployed
    in a network onboard a fishing vessel, connected to each other through either
    Ethernet of Wi-Fi, with power being supplied from the vessel. We assume that each
    node is reasonably physically sealed and tamper-proofed, but also that adversaries
    with physical possession of a node may be able to compromise it given enough time.
    Clients that want to access the system during the vessel’s voyage may do so through
    satellite broadband communication, which is present on the vessel as part of the
    system deployment. Client access involves querying data and computation results,
    and communicating new policies to configure system behaviour during run time.
    Additional sensor input can be used to annotate video data; for the remote fishery
    monitoring scenario described in this paper, we utilize a data stream from an
    Automatic Identification System (AIS). This is an automated tracking system present
    on virtually all commercial vessels in the EEA [7], and can be used to provide
    geo-location annotations for video data. Fig. 1. An overview of the system architecture
    of Dorvu. The colored area indicate components that are internal to the system.
    Full size image Figure 1 shows an overview of the data flow across several layers
    in Dorvu, which will be further elaborated in the following sections. The figure
    shows that (a) video data is generated at active fishing vessels, where it is
    (b) sent to a node running Dorvu and stored (Sect. 2.1), logged (Sect. 2.3), and
    (c) processed (Sect. 2.4). This processing involves AI inference to extract information
    about ongoing events on the fishing vessel. The output of this layer represents
    the semantic transformation of the input video file, which can be (e) requested
    and accessed by remote clients. The purpose of this transformation is to reduce
    the bandwidth cost of transferring information, and eliminate information that
    is irrelevant for the fishery operation, with respect to the privacy of fishery
    workers [21]. After data is processed, it is (d) distributed among nodes in the
    edge system (Sect. 2.2). Finally, clients can update the configuration of the
    system in real-time by (f) patching in new policies that determine how newly generated
    data is handled (Sect. 2.5). Dorvu is implemented in 3648 lines of Rust code,
    covering the File System in Userspace (FUSE) overlay file system [27] and encryption
    scheme, and 2105 lines of C# code, covering most of the remaining components,
    while AI inference modules are implemented in Python. A FUSE file system API is
    used to access data in Dorvu. This is used to implement a POSIX-like interface,
    in order to provide interoperability with most tools a client would use to consume
    data from the storage system. 2.1 Secure Storage Platform A key component of Dorvu
    is the ability to secure data stored locally at the edge. The off-shore edge environment
    introduces particular needs for strict security properties, as adversaries may
    gain physical possession of parts of the system. The purpose of providing local
    storage is to generate a plausible view of events occurring in proximity to the
    system, i.e. observed by any of its multiple sensors. Usually, distributed storage
    systems can rely on several techniques to ensure that all data is captured, to
    give a view of events that is as close to the reality as the system can possibly
    provide. To prevent data loss, cloud-connected systems can elastically scale out
    their capacity to increase storage space and redundancy, while edge systems may
    deploy data sinks to secure data and free up local storage devices [23]. If real-time
    observation is required, data can be processed on-the-fly, and be streamed to
    remote users [3, 32]. These traditional techniques are not available in the deployment
    environment of Dorvu; bandwidth limitations and unstable networks prevent access
    to cloud resources and most data streaming, while the remoteness of the system
    deployment prevents any manual intervention and reliance on data sinks. This means
    that the system must continuously adapt to observed faults, and allow degradation,
    in terms of storage space, compute capability and availability, to occur gracefully.
    In addition to this, a question of access control and privacy rights arises in
    the context of remote surveillance. Data must be handled with fine granularity,
    and access to the system does not necessarily imply access to all its stored and
    continuously generated data, even in cases of fishing vessel inspections conducted
    by federal authorities. Thus the system must provide real-time unsupervised management
    of access rights and encryption keys, to prevent unauthorized access and privacy
    infringement. Because of the requirements outlined above, data should be written
    to Dorvu in a way where the following guarantees are upheld: First, newly created
    data must be annotated with access policies based on their contents before files
    are persisted to disk, where these policies prevent unauthorized access, even
    for privileged users and adversaries obtaining physical control of the system.
    This is elaborated in Sect. 2.6. Second, once written, any modifications to the
    data will compromise its integrity and must be transparent to data consumers.
    As physical control of the system will imply the ability to destroy existing data
    and prevent the creation of new data, the goal of the storage platform is to guarantee
    the integrity of existing data. Fig. 2. An overview of three layers of data movement
    internal to the Dorvu system. Full size image 2.2 Data Distribution Layer Data
    is stored locally at the receiving node, and distributed across the edge network
    to achieve fault tolerance through replication. The aim is to ensure that the
    failure of a single component does not compromise the integrity of the remaining
    components. Figure 2 illustrates the data flow between edge nodes in Dorvu, with
    the leftmost part of the illustration showing replication between nodes through
    the data distribution layer. Distribution of data in Dorvu is deterministic based
    on each individual node’s view on the network. Nodes transmit challenges to others
    to prove that data replicated from themselves exists and is unmodified on other
    nodes in the network. Cryptographic and algebraic challenges has been explored
    as a means to prove the existence of files in previous works, most notably related
    to provable data possession [30] and provable data retrievability [11]. In Dorvu,
    due to fear of tampering, nodes have limited trust in other nodes, and data movement
    is independent of decisions made by other nodes. Node A will reduce its trust
    in Node B if it observes Node B failing a challenge on data that originated in
    A. A challenge for file F at B is initiated an arbitrary amount of time after
    F was transmitted from A, where A requests a cryptographic hash of an arbitrary
    range of the encrypted F. The use of trust to manage the network is further detailed
    in Sect. 2.5. 2.3 Distributed Log Layer Figure 2 (right) illustrates logging mechanisms
    in Dorvu. Names of newly created files along with hash digests of file contents
    are appended to a sequential log stored at each node. Storing digests or signatures
    alongside file meta-data to detect changes is a common approach for file system
    integrity checking [12]. Nodes in Dorvu distribute this local log to the rest
    of the network on fixed intervals. The number of files included in each interval
    depends on the frequency of file generation, and in the case of remote surveillance,
    the length of individual video chunks. Files within the same interval can be appended
    to calculate a common hash digest, as an optimization, because integrity checking
    can only be performed at the granularity of which the data is transmitted from
    their origin nodes. The purpose of this logging mechanism is to store and provide
    forensic evidence of data tampering in cases where some (but not all) nodes are
    byzantine or compromised by an adversary. Other approaches to similar problems
    of nodes with limited trust in a remote distributed storage system include implementing
    a blockchain within the network [4]. Our approach is simpler, as nodes does not
    represent individual stakeholders; data distribution is used as a fault tolerance
    mechanism to ensure data survival. 2.4 Inference Layer Video data that is input
    to Dorvu is automatically processed through inference in order to extract information
    about its contents. Details regarding AI inference and training are out of scope
    of this paper, but is further detailed in [19]. In short, we have previously developed
    a dataset from fishery surveillance videos from an active voyage of a collaborating
    fishing trawler [20] that provides us with annotations to detect people, fishing
    equipment, and various events of interest. This procedure is conducted on each
    node for its individual data input, independent of other nodes. A checkpointing
    scheme is utilized in order to resume processing from replicas, in case of node
    failure [1]. This is illustrated in Fig. 2 (middle), in which the topmost node
    has failed at the third and final step of computation, and the right node resumes
    the work. For a file f originating at node A, replicated at node B, B will poll
    A for checkpoints of the computation on f, and resume data processing in cases
    where A stops responding. 2.5 Policies and Trust Data distribution, data processing,
    and remote client access, are tied together with data policies, which are used
    by privileged clients to modify the behaviour of the system during run-time in
    remote environments. Data policies are assigned to features that can be found
    in data, in addition to sensor readings. For our use-case, we focus on people,
    fishery equipment, fish, and special events [20]. For sensor input, we use AIS
    data points annotated in their corresponding video files. Remote clients can configure
    network behaviour by providing nodes with policies related to which files are
    of highest importance to retain. Challenges, as described in Sect. 2.2, are used
    to confirm that files are adequately replicated in the network. Classification
    of file importance through challenges is performed by providing relevant video
    features (output from the data processing layer) and sensor readings. This is
    useful data input for fishery surveillance, because AIS data points can be used
    to limit data to a certain geographical area. An additional benefit of this approach
    is that a sequence of AIS readings can be used to calculate vessel movement. Data
    policies are thus provided by remote clients during the Dorvu run-time to determine
    to what degree of robustness video data should be secured, based on their calculated
    content, and their context provided from sensor readings. Robustness within the
    network is determined by trust, as described in Sect. 2.2. trust in the network
    is implemented in relation to data policies and video features, so that each node
    contains a nxm matrix of 32 bit floating point values between 0 and 1, with n
    being the number of nodes in the network and m being the features extracted from
    video data. Additionally, each node maintains a list of active policies p that
    inherits trust from one or more of the m features. This means that nodes maintain
    a degree of trust in that other nodes are able to satisfy their active policies.
    Each node maintains their list of policies independently of other nodes, and must
    therefore calculate trust to identify nodes most likely to retain their files.
    This approach allows users to customize the inherent data distribution mechanisms
    in Dorvu to maximize the availability and survivability of the video data that
    is most relevant as forensic evidence. 2.6 Encryption Data stored locally on disk
    in edge environments are subject to strict requirements of confidentiality, due
    to the risk of data leakage. Bhatnagar et al. [2] define an encryption scheme
    for storage in edge nodes in wireless sensor networks. This scheme has the aim
    of minimizing data leakage in case an adversary gains physical possession of a
    storage node. This is achieved by generating new keys for every data object, with
    a one-way function, such as a cryptographic hash function, applied on the old
    key. A trusted party has control of the initial key, and every subsequent key
    can as such be re-calculated when the node is in a trusted environment. Dorvu
    utilizes a modified version of this. For our scenario, the goal of minimizing
    data leakage is the same, but using the same one-way key for every file is infeasible
    if remote data access should be possible for certain, but not all, files. The
    scheme is modified to better fit this specific scenario, by adding a second initial
    seed used to generate not only keys, but also subsequent seeds. An initial seed
    (\\(S_1\\)), as well as two cryptographic one-way functions (\\(F_1\\) and \\(F_2\\)),
    are initialized on both an edge node, and a trusted main-land node. A key is generated
    by applying the function \\(F_1\\) on the seed \\(S_1\\), generating \\(K_1\\).
    A second seed is generated by applying function \\(F_2\\) on \\(S_1\\), generating
    \\(S_2\\). \\(S_1\\) is subsequently deleted from the disk on the edge node. \\(K_1\\)
    can now be used to encrypt data with a synchronous encryption scheme, without
    the possibility of this key being re-calculated by information present in the
    edge environment. After a file has been encrypted with \\(K_1\\), a new key \\(K_2\\)
    is generated by applying \\(F_1\\) on \\(K_1\\), and \\(K_1\\) is deleted from
    the disk. This continues by generating \\(K_n\\) for the nth file written to disk.
    After a pre-configured time interval, the key on the edge node is discarded, and
    a new key is generated by applying \\(F_1\\) on \\(S_2\\), and deleting \\(S_2\\)
    after generating \\(S_3\\). As a result of this scheme, the trusted party that
    initialized the seed can grant access to an authorized third-party for a given
    time-interval, by calculating the key for that time period by applying \\(F_2\\)
    on the initial seed \\(S_1\\). At the same time, any adversaries that gain possession
    of the device, and any keys stored on it, will be unable to read any data written
    prior to the possession. This achieves the functionality of file retrieval for
    authorized users under certain conditions, while retaining the guarantees from
    physical attacks that were described in the original scheme [2]. The time interval
    defined in our encryption scheme determines how much data a user can access at
    minimum when granted an access key. This can be reduced to a single file, by never
    applying \\(F_1\\) on a key K. As this scheme will be used to encrypt video data,
    it is assumed that files need a minimum length to be useful. There is a trade-off
    between encrypting the smallest units possible, each individual frame, with unique
    keys, and granting a potentially too large window of access because the time-interval
    is configured to be too long, opening up for data leaks. 3 Experiments and Results
    In evaluating this system, we investigate individual components of the proposed
    data pipeline setup. The purpose of these experiments is to evaluate the feasibility
    of the system in an edge environment. As such, we investigate the capabilities
    of a satellite link and the embedded computing boards that we have used to deploy
    our prototype. For these experiments, we have utilized a Iridium Certus 200 broadband
    satellite service, an L-band non-geostationary satellite network, connected to
    a Thales VesselLink 200 maritime antenna. Service providers claim that the Certus
    network provides global coverage through a network containing 66 satellites, with
    transmission speeds up to 176 kilobits per second (kBps) [10]. The VesselLink
    router is connected over Ethernet to a Nvidia Jetson Xavier NX embedded computing
    board (NVIDIA Carmel ARMv8.2 CPU @ 1400MHz, 384-core Volta GPU @ 1100MHz), which
    was used as a test system for the following experiments. For many of these experiments,
    test data is collected from the Njord dataset [20], which contains 29 h of video
    footage recorded from cameras installed in an operational fishing trawler. 3.1
    File Storage The time to encrypt or digest a video file with additional metadata
    and write it to persistent storage was measured. This experiment encompasses our
    encryption scheme (Sect. 2.6), and hash functions. The video file is a 10 s snippet
    from the Njord dataset. A sample AIS signal indicating the location where the
    video originated from is included as metadata. Various encryption and hashing
    functions were utilized, and most of these OpenSSL implementations were selected
    because the hardware configuration on the Nvidia Jetson Xavier NX board denoted
    support for them through a specialized instruction set. This experiment included
    reading the video file from persistent storage, producing an encrypted file or
    digest, and writing the result to persistent storage again. The results from this
    experiment are shown in Fig. 3. 3.2 Policy-Based Data Replication Data replication
    based on data policies in Dorvu is evaluated in unstable environments with comparison
    to hash-based replication schemes [31] and round robin schemes. An unstable environment
    is emulated by injecting probabilistic crashes in the network. The system is deployed
    with 8 nodes, with 100 video files streamed to each node. Each node is given a
    random failure rate \\(\\lambda \\in (0.0, 1.0)\\) which is used to generate a
    timeline of failures under an exponential distribution. The system runs for 30
    s, each data access increments the timeline, and each failure prevents the next
    data access. A separate monitor program continuously queries all files and tracks
    their availability across all nodes. If all replicas that contain a given file
    is unresponsive, the file is considered unavailable. For this experiment, Dorvu
    runs with an active policy that detects persons in video files. Of the 100 files
    in the test set, 33 contains detectable persons. The number of nodes that each
    file is replicated to is varied from 1 to 5 for this experiment. Results are shown
    in Fig. 4, where the availability of files that match the active policy, and the
    files that do not, are shown as separate data points. Fig. 3. Encryption and digest
    time of a tuple containing a video file and location metadata. Error bars represent
    standard deviation after 10 repetitions. (*) denotes hardware support. Full size
    image Fig. 4. Availability of files in an unstable network. Results show average
    availability after 10 runs, where error bars represent standard deviation. Full
    size image 3.3 Satellite Network Connection The potential for video streaming
    or transfer over satellite broadband, as an alternative to local storage, is evaluated.
    Live video transfer from active vessels to mainland nodes could reduce risk of
    data loss and increase response time for system observers. For this experiment,
    a test video file is hosted on the test system, connected to the internet through
    satellite broadband. The video is served through the Python ffmpeg library implementation
    of Dynamic Adaptive Streaming over HTTP (DASH) [24]. The test system serves the
    test client a manifest (.mpd) file containing an index of all available video
    chunks (.m4s). The video is separated into both 2 and 10 s snippets from the original
    video file in various resolutions and bitrates. This allows a viewer of the DASH
    stream to select segments in different sizes depending on current network quality.
    For this experiment, the client used VLC media player 3.0.11 to receive the stream.
    This software was responsible for adapting its file requests according to the
    observed bandwidth. File system access was observed to indicate which files were
    read at a given time, and the total length of each segment transferred over the
    network was summed to get an indication of expected quality for DASH streaming
    under the network conditions in the testing environment. The results from this
    experiment are shown in Fig. 5 (left). Additionally, the files were downloaded
    by the client using the Linux wget utility. This utility allows for the documentation
    of both the total transfer time of a file, in addition to a running estimated
    transfer time, given the most recently observed network conditions extrapolated
    to the entire file size. The results from this experiment are shown in Fig. 5
    (right). Fig. 5. Left: Time spent sending videos of different bitrates during
    DASH streaming over satellite broadband. Right: Time taken to download a 10 s
    video file of different qualities over satellite broadband. Grey markers indicate
    the estimated time to completion after every 50 kilobits of download. Both: Results
    show average results after ten repetitions, with error bars representing the standard
    deviation of observations. Full size image 4 Discussion The solution presented
    in this article is intended for a specific and rapidly emerging domain of trusted
    edge computing where the edge nodes are mobile, potentially malicious, and error-prone.
    Because of this, direct points of comparisons to existing systems were hard to
    find. Therefore, through our experiments we aim to evaluate two main aspects.
    First, evaluate if the proposed solution is feasible given the constraints in
    a hostile edge environment. Second, compare the proposed solution to the alternative
    of full off-shore surveillance through CCTV video streaming described in some
    governmental propositions [15]. The first component we evaluate is the storage
    for locally collected video and sensor data. The experiment described in Sect.
    3.1 investigates the time to encrypt and store data. The results show that, while
    speed-up can be achieved by using ciphers or functions with explicit hardware
    instruction support, the test system is capable of encrypting or digesting a file
    and writing it to disk in shorter than 10 s time using our scheme, with every
    function in the experiment. This experiment demonstrates that encryption and storage
    at edge devices in real time are possible to achieve, and that the choice of encryption
    scheme could affect performance if multiple video inputs were to be handled in
    parallel. The experiment described in Sect. 3.2 shows that the policy-based distribution
    in Dorvu achieves favorable performance compared to traditional replication techniques
    with regards to file availability. An important detail is that the availability
    of non-policied files is not significantly worse than the alternative methods.
    Systems deployed in environments where elastic storage scaling or manual intervention
    and repairs are possible, would likely replace or ignore nodes that are demonstratively
    faulty or malicious, but in off-shore edge deployments, systems may have to rely
    on all available storage space, despite risks of data losses. In those scenarios,
    the system should be able to adapt to maximize the survival of important information.
    Policy-based data distribution is introduced as a means towards the goal of securing
    forensic evidence in unstable environments. Further evaluation was performed with
    satellite connectivity to evaluate the capabilities of data transfer in an off-shore
    edge scenario. First, a video streaming experiment was performed, to evaluate
    the possibility of continuous CCTV monitoring. The streaming experiment described
    in Sect. 3.3 demonstrates that the bandwidth available through the test satellite
    network is insufficient to provide a continuous streaming service at a bit-rate
    of 17.6 kBps or higher. While the results show that the lowest sized variant of
    the video was most suitable for the network conditions, the experiment does not
    indicate anything of the quality of the service provided when streaming that particular
    video. The following bandwidth experiment indicate that the bandwidth supplied
    by the satellite connection is in most cases not sufficient to transfer entire
    files of any of these bit-rates in real-time, although it was occasionally observed
    for the smallest file sizes. 5 Conclusion We have designed and implemented a multimedia
    distributed storage system for video and sensor data captured in edge environments,
    with specific focus on the emerging field of commercial fishery monitoring. Conflicting
    properties like security, fault tolerance, availability and remote access has
    been taken into account so that the system can be put to good practical use for
    securing forensic evidence in a compliant manner. This solution is opposed to
    the current governmental trend that mandates deployment of surveillance systems
    that violate some of these properties. Our experimental evaluation shows that
    traditional video streaming solutions are not always viable in off-shore edge
    environments, and that our proposed system makes desirable trade-offs for the
    targeted domains. This is accomplished by introducing AI inference into the storage
    system, combined with a novel data distribution scheme where individual node trust
    in the network, combined with user input over satellite links, determines data
    distribution behaviour. Notes 1. Dorvu is a Northern Sámi term, meaning security
    or trust. References Alsile, J.A., et al.: Áika: a distributed edge system for
    AI inference. Big Data Cogn. Comput. 6(2), 68 (2022) Article   Google Scholar   Bhatnagar,
    N., Miller, E.L.: Designing a secure reliable file system for sensor networks.
    In: Proceedings of the 2007 ACM workshop on Storage security and survivability,
    pp. 19–24 (2007) Google Scholar   Bischof, H., Godec, M., Leistner, C., Rinner,
    B., Starzacher, A.: Autonomous audio-supported learning of visual classifiers
    for traffic monitoring. IEEE Intell. Syst. 25(3), 15–23 (2010) Article   Google
    Scholar   Conoscenti, M., Vetro, A., De Martin, J.C.: Blockchain for the internet
    of things: A systematic literature review. In: 2016 IEEE/ACS 13th International
    Conference of Computer Systems and Applications (AICCSA), pp. 1–6. IEEE (2016)
    Google Scholar   Costello, C.e.a.: The future of food from the sea. Nature (London)
    588(7836), 95 (2020) Google Scholar   Fitwi, A., Chen, Y., Zhu, S.: Lightweight
    frame scrambling mechanisms for end-to-end privacy in edge smart surveillance.
    IET Smart Cities 4(1), 17–35 (2022) Article   Google Scholar   Fournier, M., Casey
    Hilliard, R., Rezaee, S., Pelot, R.: Past, present, and future of the satellite-based
    automatic identification system: Areas of applications (2004–2016). WMU J. Marit.
    Aff. 17, 311–345 (2018) Article   Google Scholar   Ghemawat, S., Gobioff, H.,
    Leung, S.T.: The google file system. In: Proceedings of the 19th ACM symposium
    on Operating systems principles, pp. 29–43 (2003) Google Scholar   Gu, B., Wang,
    X., Qu, Y., Jin, J., Xiang, Y., Gao, L.: Context-aware privacy preservation in
    a hierarchical fog computing system. In: ICC 2019–2019 IEEE International Conference
    on Communications (ICC), pp. 1–6. IEEE (2019) Google Scholar   Inc, I.C.: Iridium
    certus 200 datasheet. https://www.iridium.com/services/iridium-certus-200/ (2023).
    Accessed 8 Sept 2021 Juels, A., Kaliski Jr, B.S.: Pors: Proofs of retrievability
    for large files. In: Proceedings of the 14th ACM Conference on Computer and Communications
    Security, pp. 584–597 (2007) Google Scholar   Kim, G.H., Spafford, E.H.: The design
    and implementation of tripwire: a file system integrity checker. In: Proceedings
    of the 2nd ACM Conference on Computer and Communications Security, pp. 18–29 (1994)
    Google Scholar   Kuhl, J.G., Reddy, S.M.: Distributed fault-tolerance for large
    multiprocessor systems. In: Proceedings of the 7th Annual Symposium on Computer
    Architecture, pp. 23–30 (1980) Google Scholar   Martinussen, T.M.: Danske fiskere
    samler seg mot kameraovervåkning. Fiskeribladet (2020), https://www.fiskeribladet.no/nyheter/danske-fiskere-samler-seg-mot-kamera-overvakning-i-fiskeriene/2-1-839478.
    Accessed 5 Dec 2022 Ministry of Trade: Industry and Fisheries: Framtidens fiskerikontroll.
    NOU 19, 21 (2019) Google Scholar   Muthitacharoen, A.: A low-bandwidth network
    file system. In: Proceedings of the 18th ACM Symposium on Operating Systems Principles,
    pp. 174–187 (2001) Google Scholar   Márcia Bizzotto: Fishing rules: Compulsory
    cctv for certain vessels to counter infractions. European Parliament Press Release
    (2021). https://www.europarl.europa.eu/news/en/press-room/20210304IPR99227/fishing-rules-compulsory-cctv-for-certain-vessels-to-counter-infractions
    Nordmo, T.A.S., Ovesen, A.B., Johansen, H.D., Riegler, M.A., Halvorsen, P., Johansen,
    D.: Dutkat: A multimedia system for catching illegal catchers in a privacy-preserving
    manner. In: Proceedings of the 2021 Workshop on Intelligent Cross-Data Analysis
    and Retrieval, pp. 57–61 (2021) Google Scholar   Nordmo, T.A.S.: Dutkat: a privacy-preserving
    system for automatic catch documentation and illegal activity detection in the
    fishing industry (2023) Google Scholar   Nordmo, T.A.S., et al.: Njord: a fishing
    trawler dataset. In: Proceedings of the 13th ACM Multimedia Systems Conference,
    pp. 197–202 (2022) Google Scholar   Ovesen, A.B., Nordmo, T.A.S., Johansen, H.D.,
    Riegler, M.A., Halvorsen, P., Johansen, D.: File system support for privacy-preserving
    analysis and forensics in low-bandwidth edge environments. Information 12(10),
    430 (2021) Article   Google Scholar   for Primary Industries, M.: On-board cameras
    for commercial fishing vessels (2023), https://www.mpi.govt.nz/fishing-aquaculture/commercial-fishing/fisheries-change-programme/on-board-cameras-for-commercial-fishing-vessels/
    Accessed 20 Nov 2022 Rajagopalan, R., Varshney, P.K.: Data aggregation techniques
    in sensor networks: a survey (2006) Google Scholar   Sodagar, I.: The mpeg-dash
    standard for multimedia streaming over the internet. IEEE Multimedia 18(4), 62–67
    (2011) Article   Google Scholar   Solberg, R.R.: Bærekraftig fiskeri, governance
    og tid. Master’s thesis, The University of Bergen (2022) Google Scholar   Sumaila,
    U.R., Tai, T.C.: End overfishing and increase the resilience of the ocean to climate
    change. Front. Mar. Sci. 7, 523 (2020) Article   Google Scholar   Tarasov, V.,
    Gupta, A., Sourav, K., Trehan, S., Zadok, E.: Terra incognita: on the practicality
    of user-space file systems. In: 7th \\(\\{\\)USENIX\\(\\}\\) Workshop on Hot Topics
    in Storage and File Systems (HotStorage 15) (2015) Google Scholar   Tornsberg,
    L.: Danske pelagiske fiskere indfører 100% dokumenteret fiskeri. Fiskerforum (2022),
    https://fiskerforum.dk/danske-pelagiske-fiskere-indfoerer-100-dokumenteret-fiskeri-%E2%80%A8/
    Accessed 15 Jan 2023 UNODC: Fisheries crime: transnational organized criminal
    activities in the context of the fisheries sector (2016) Google Scholar   Wang,
    Q., Ren, K., Yu, S., Lou, W.: Dependable and secure sensor data storage with dynamic
    integrity assurance. ACM Trans. Sensor Netw. (TOSN) 8(1), 1–24 (2011) Google Scholar   Weil,
    S.A., Brandt, S.A., Miller, E.L., Maltzahn, C.: Crush: Controlled, scalable, decentralized
    placement of replicated data. In: Proceedings of the 2006 ACM/IEEE conference
    on Supercomputing, pp. 122-es (2006) Google Scholar   Winkler, T., Rinner, B.:
    Security and privacy protection in visual sensor networks: a survey. ACM Comput.
    Surv. (CSUR) 47(1), 1–42 (2014) Article   Google Scholar   Xu, M., Xu, W., O’Kane,
    J.: Content-aware data dissemination for enhancing privacy and availability in
    wireless sensor networks. In: 2011 IEEE Eighth International Conference on Mobile
    Ad-Hoc and Sensor Systems, pp. 361–370. IEEE (2011) Google Scholar   Download
    references Author information Authors and Affiliations UiT The Arctic University
    of Norway, Tromsø, Norway Aril Bernhard Ovesen, Tor-Arne Schmidt Nordmo, Michael
    Alexander Riegler & Dag Johansen Simula Metropolitan Center for Digital Engineering,
    Oslo, Norway Michael Alexander Riegler & Pål Halvorsen Oslo Metropolitan University,
    Oslo, Norway Michael Alexander Riegler & Pål Halvorsen Corresponding author Correspondence
    to Aril Bernhard Ovesen . Editor information Editors and Affiliations University
    of Amsterdam, Amsterdam, The Netherlands Stevan Rudinac Delft University of Technology,
    Delft, The Netherlands Alan Hanjalic Delft University of Technology, Delft, The
    Netherlands Cynthia Liem University of Amsterdam, Amsterdam, The Netherlands Marcel
    Worring Reykjavik University, Reykjavik, Iceland Björn Þór Jónsson Microsoft Research
    Lab – Asia, Beijing, China Bei Liu The University of Tokyo, Tokyo, Japan Yoko
    Yamakata Rights and permissions Reprints and permissions Copyright information
    © 2024 The Author(s), under exclusive license to Springer Nature Switzerland AG
    About this paper Cite this paper Ovesen, A.B., Nordmo, TA.S., Riegler, M.A., Halvorsen,
    P., Johansen, D. (2024). Sustainable Commercial Fishery Control Using Multimedia
    Forensics Data from Non-trusted, Mobile Edge Nodes. In: Rudinac, S., et al. MultiMedia
    Modeling. MMM 2024. Lecture Notes in Computer Science, vol 14556. Springer, Cham.
    https://doi.org/10.1007/978-3-031-53311-2_24 Download citation .RIS.ENW.BIB DOI
    https://doi.org/10.1007/978-3-031-53311-2_24 Published 28 January 2024 Publisher
    Name Springer, Cham Print ISBN 978-3-031-53310-5 Online ISBN 978-3-031-53311-2
    eBook Packages Computer Science Computer Science (R0) Share this paper Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Publish
    with us Policies and ethics Sections Figures References Abstract Introduction
    System Overview Experiments and Results Discussion Conclusion Notes References
    Author information Editor information Rights and permissions Copyright information
    About this paper Publish with us Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Sustainable Commercial Fishery Control Using Multimedia Forensics Data from Non-trusted,
    Mobile Edge Nodes
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ahmad I.
  - Rodriguez F.
  - Kumar T.
  - Suomalainen J.
  - Jagatheesaperumal S.K.
  - Walter S.
  - Asghar M.Z.
  - Li G.
  - Papakonstantinou N.
  - Ylianttila M.
  - Huusko J.
  - Sauter T.
  - Harjula E.
  citation_count: '0'
  description: Industry 4.0 is moving towards deployment using 5G as one of the main
    underlying communication infrastructures. Thus, the vision of the Industry of
    the future is getting more attention in research. Industry X (InX) is a significant
    thrust beyond the state-of-the-art of current Industry 4.0, towards a mix of cyber
    and physical systems through novel technological developments. In this survey,
    we define InX as the combination of Industry 4.0 and 5.0 paradigms. Most of the
    novel technologies, such as cyber-physical systems, industrial Internet of things,
    machine learning, advances in cloud computing, such as edge and fog computing,
    and blockchain, to name a few, are converged through advanced communication networks.
    Since communication networks are usually targeted for security attacks, these
    new technologies upon which InX relies must be secured to avoid security vulnerabilities
    propagating into InX and its components. Therefore, in this article, we break
    down the security concerns of the converged InX-communication networks into the
    core technologies that tie these, once considered distinct, fields together. The
    security challenges of each technology are highlighted and potential solutions
    are discussed. The existing vulnerabilities or research gaps are brought forth
    to stir further research in this direction. New emerging visions in the context
    of InX are provided towards the end of the article to provoke further curiosity
    of researchers.
  doi: 10.1109/OJCOMS.2024.3356076
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Open Journal of the
    Comm... >Volume: 5 Communications Security in Industry X: A Survey Publisher:
    IEEE Cite This PDF Ijaz Ahmad; Felipe Rodriguez; Tanesh Kumar; Jani Suomalainen;
    Senthil Kumar Jagatheesaperumal; Stefan Walter; Muhammad Zeeshan Asghar All Authors
    489 Full Text Views Open Access Under a Creative Commons License Abstract Document
    Sections I. Introduction II. Related Work III. A Brief Introduction to Key Enabling
    Technologies IV. Security of Communications in InX V. Security of Key Industry
    Infrastructure Technologies Show Full Outline Authors Figures References Keywords
    Metrics Abstract: Industry 4.0 is moving towards deployment using 5G as one of
    the main underlying communication infrastructures. Thus, the vision of the Industry
    of the future is getting more attention in research. Industry X (InX) is a significant
    thrust beyond the state-of-the-art of current Industry 4.0, towards a mix of cyber
    and physical systems through novel technological developments. In this survey,
    we define InX as the combination of Industry 4.0 and 5.0 paradigms. Most of the
    novel technologies, such as cyber-physical systems, industrial Internet of things,
    machine learning, advances in cloud computing, such as edge and fog computing,
    and blockchain, to name a few, are converged through advanced communication networks.
    Since communication networks are usually targeted for security attacks, these
    new technologies upon which InX relies must be secured to avoid security vulnerabilities
    propagating into InX and its components. Therefore, in this article, we break
    down the security concerns of the converged InX-communication networks into the
    core technologies that tie these, once considered distinct, fields together. The
    security challenges of each technology are highlighted and potential solutions
    are discussed. The existing vulnerabilities or research gaps are brought forth
    to stir further research in this direction. New emerging visions in the context
    of InX are provided towards the end of the article to provoke further curiosity
    of researchers. Published in: IEEE Open Journal of the Communications Society
    ( Volume: 5) Page(s): 982 - 1025 Date of Publication: 19 January 2024 Electronic
    ISSN: 2644-125X DOI: 10.1109/OJCOMS.2024.3356076 Publisher: IEEE Funding Agency:
    CCBY - IEEE is not the copyright holder of this material. Please follow the instructions
    via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles
    and stipulations in the API documentation. SECTION I. Introduction Moving through
    the ladder of the industrial revolution on its logical path [1], the industry
    of the future or Industry X (InX) couples the digital and physical world through
    novel scientific and technological transformations, beyond Industry 4.0. Industry
    4.0 [2], is the present big industrial transformation after mechanization, electrification,
    and information were introduced, and it is considered a key step in the advancement
    of the industry to its state-of-the-art [3]. Industry 5.0 is often seen as the
    extension of Industry 4.0 (focusing on data-driven applications and connectivity)
    towards the adoption of advanced artificial intelligence (AI) technologies for
    industrial automation and human-robot collaboration [4]. The European Union (EU)
    commission envisions Industry 5.0 as an extension that will focus on and be an
    enabler for advanced R&D, investment for up- and re-skilling of workers, circular
    economy, and human-centric adoption of digital technologies and AI [5]. InX is
    a combination of technology paradigms in Industry 4.0 and 5.0, as well as drivers
    and processes enabling the continuous evolution of industry beyond Industry 5.0.
    The foundations of Industry 4.0, i.e., connectivity of industrial systems, processes,
    and services through novel communication networks, have become pivotal to the
    success of InX [6]. The fifth-generation wireless networking, known as 5G, facilitates
    the envisioned humongous growth of the Industrial Internet of Things (IIoT), and
    cyber-physical systems (CPS), providing extremely low latency connectivity for
    critical functions of InX. However, advanced communications networks have their
    security challenges and require novel solutions for mitigating these challenges
    [7], [8]. The key technologies of Industry 4.0 include mobile Internet, IoT, CPS,
    cloud computing, big data, and advanced analytics techniques [3]. From the communications
    perspective, each of these has its security weaknesses and vulnerabilities and
    when combined into an ecosystem, the emerging complexity due to mixed criticality
    [9] can further exacerbate the challenges. Therefore, a thorough analysis of the
    security of the underlying communication systems and technologies is necessary
    from many perspectives, yet the main one is the improvement of the overall resilience
    of critical InX applications enabling their uninterrupted role in our societies.
    The security weaknesses in the enabling technologies of InX, which can be used
    by malicious internal and external actors, must be properly studied. For instance,
    weaknesses of most IoT devices in using proper encryption techniques must also
    be brought forth to avoid sending or redirecting sensitive information through
    such devices. Similarly, if physical access to CPS systems cannot be restricted,
    proper security mechanisms must be in place to avoid tampering even with physical
    access. Furthermore, communication networks, such as 5G, have loopholes in terms
    of security as clearly elaborated and outlined in [7], [10]. InX relies on such
    communication networks to connect critical infrastructure and its elements, such
    as IIoT and CPS [11]. The EU Commission recognizes these emerging challenges and
    places resilience as one of the three main pillars of Industry 5.0. Therefore,
    security concerns related to the enabling technologies of InX, such as 5G, CPS,
    IIoT, etc., must be considered at all levels and resolved to avoid possible cascading
    effects due to the reliance of technologies on each other. A. Roadmap to InX:
    Motivation The industrial revolutions have transformed the quality of life of
    most of the world by changing the means of production from human-intensive to
    machine-intensive. Scientific and technological innovations brought the revolutions
    that changed the ways of working, living, using resources, and human experiences.
    The first industrial revolution (Industry 1.0) (1760–1840) was mainly characterized
    by the mechanization of production and steam power. The second industrial revolution
    (Industry 2.0) (1870–1920) was mainly driven by electricity and the development
    of the internal combustion engine. The third industrial revolution (Industry 3.0)
    (1960–2000) was characterized by the development of electronics, computers, and
    telecommunications. The fourth industrial revolution (Industry 4.0) (2000–2025)
    is attributed to the development of Internet technologies, whereas the fifth industrial
    revolution (Industry 5.0) (beyond 2025) revolves around the role of AI. Industry
    X, the term used in this paper, engulfs Industry 5.0 and beyond and is described
    in the rest of this subsection. These revolutions are also depicted in Fig. 1.
    It is important to note that there are differences in definitions and variations
    in the time spans of the revolutions. However, the aim is to provide a brief generic
    background for the rest of the study of this paper. FIGURE 1. Continuum from Industry
    1.0 to Industry 5.0, and then converging to InX. Show All Industry 4.0 has become
    a center of attraction in developed countries and a strong strategic or even political
    goal in the first place [12]. Even though it is considered a fundamental paradigm
    shift in industrial production with great expectations for innovation, the concept
    of Industry 4.0 is only loosely defined and heavily linked to the technological
    developments in the last decade [13]. Considered the driving force behind innovation
    in many fields and dimensions of social development, Industry 4.0 is not a singleton
    technological development, but rather an ecosystem that provides an umbrella of
    distinct technological developments under the guise of industry of the future
    [14]. The main features of Industry 4.0 include i) horizontal integration through
    value networks to facilitate collaboration among corporate sectors, ii) vertical
    integration of hierarchical subsystems in a factory to create a flexible and re-configurable
    manufacturing system, and (iii) end-to-end engineering integration across entire
    value chains to support customization of products [2], [11], [15]. These features
    indicate a strong need for the integration of various systems and services that
    may comprise different combinations of the above stages and can be in different
    geographical locations. Therefore, the security of the communication systems that
    facilitate integrating the systems of Industry 4.0 is of paramount importance
    due to the critical nature of the infrastructure. The basic design principles
    of Industry 4.0 are i) interconnection, ii) information transparency, iii) decentralized
    decisions, and iv) technical assistance [16]. Communication is a core requirement
    to implement these principles. Therefore, beyond traditional industrial communication,
    the major enabler of Industry 4.0 has been the introduction of Internet technologies
    to achieve the required massive interconnection on and across all levels [6].
    This includes modern cloud concepts as well as the IoT and goes far beyond individual
    remote connections to production facilities that have been discussed already 25
    years ago. On the downside, the strong reliance on Internet technologies and the
    increasing use of IT devices on the factory floor has also brought cyber-threats
    closer to the industrial environments where they have an impact on the safety
    and stability of production systems [17]. The European Commission formally introduced
    the term “Fifth Industrial Revolution (Industry 5.0)” in 2021 through the Directorate-General
    for Research and Innovation [18]. The main aim as discussed in [18] was to initiate
    a wider debate on shaping Industry 5.0 in the European context. Industry 5.0 revolves
    around three main drivers, i.e., i) Sustainability, ii) Resilience, and iii) Human-centricity,
    as defined by the European Commission [18]. Even though the roots of Industry
    5.0 are in the concepts of Industry 4.0, the focus of Industry 5.0 remains on
    long-term service to humanity within our planetary boundaries, highlights the
    European Commission. The concepts of Industry 5.0 and Society 5.0, a term coined
    by the Japanese, are related in the sense that Society 5.0 represents a society
    after the dominance of “information” and ripe with the use of IT technologies,
    IoT, AI, robots, and AR, all serving humanity in everyday life and every sphere
    including industries [18]. Even though Industry 5.0 is human-centric, its emphasis
    on advanced digitization, big data, and AI in the digital sphere will meet new
    and emerging requirements of the future industrial landscape, such as InX. Being
    one of the three main visions, resilience is the key pillar to develop a higher
    degree of robustness in industrial production to work normally during disruptions,
    and even provide support to critical infrastructure during times of crisis. Even
    though resilience can be defined according to a specific context, generally resilience
    is “the capacity of a system to absorb disturbances while responding to an ongoing
    change so that the system can sustain its function, structure, and output levels”
    [19], whereas, technological resilience allows industries to adopt to and respond
    to crisis [20]. Industry 5.0 must have resilient strategic value chains, adaptable
    production capacity, as well as enough flexibility in the business processes.
    Furthermore, resilience enables the industry to cope flexibly with disruptive
    changes, and vulnerabilities on many levels including the factory floor, supply
    networks, and industrial systems. However, the general trend in innovation focuses
    on efficiency, whereas resilience is mostly overlooked. Since the main differentiating
    pillars of InX from the earlier industrial revolutions are emerging technologies,
    cybersecurity becomes the main pillar to make InX as resilient as required. Therefore,
    the security of the most important enabling technologies needs a thorough investigation.
    Since communication technologies connect the vital components of the industrial
    ecosystem, the security of communication technologies comes to the forefront for
    securing the overall ecosystem. It is important to note that the main driving
    force for evolution from Industry 4.0 to Industry 5.0 is remote production with
    distributed value chain [21] that requires fool-proof security of enabling connectivity
    technologies. Therefore, in this article, we focus on all technological enablers
    of InX that are used either as communication media or require digital communication
    for its very functionality. Fig. 2 highlights the scope of our paper, where each
    underlying enabling technology converges for enabling InX. We are looking at the
    security aspects of the most relevant technological enablers, as well as the impacts
    arising from their integration into Industry 4.0 and 5.0 concepts. Furthermore,
    we are looking at the security enablers and challenges of future technologies
    adopted by InX. FIGURE 2. Scope of the survey: security implications and impacts
    inherited from different InX technologies, and new security implications arising
    from paradigm integration and from the technology evolution. Show All This work
    is motivated by the increasing cyber security challenges related to the technological
    enablers of InX. As depicted in Fig. 2, many technologies will converge in InX,
    and each enabling technology, such as 5G, IIoT, AI, and AR, to name a few, has
    its security challenges. With its integration into InX, the overall security threat
    landscape will hugely increase. For example, the security threats in 5G as explained
    in [22] can expose production lines to security threats if insecure 5G base stations
    provide the connectivity between different assembly lines. Similarly, AI has many
    security challenges as explained in [23], which can cause serious harm in several
    operations, such as during monitoring and maintenance, in the InX ecosystem. IIoT
    makes the foundation of many systems and services, however, it has been revealed
    in many studies that the firmware of the majority of IIoT devices has inherent
    weaknesses that can be exploited for security attacks, as discussed in Section
    IV. Hence, the applications of these technologies in InX necessitate a thorough
    investigation of the security threat landscape of InX. Therefore, in this article,
    our main motivation is to investigate the consequential security challenges InX
    will face due to the integration of such novel technologies, study the potential
    security solutions, and find the existing security gaps for future research. B.
    Contributions of this Article In the evolution towards InX, what are the most
    important technological enablers of InX that rely on communications networks and
    technologies? Since communications technology will make the backbone of most enabling
    or supporting technologies of InX, what will be the most important security challenges
    (weaknesses and threats) to those technologies and the overall InX ecosystem?
    What are the potential security solutions for those security challenges in each
    of the enabling technologies? What are the main standardization efforts in the
    realm of security of those enabling and supporting technologies of InX? What are
    the main existing security gaps that require further research? In this article,
    the emerging security challenges in communications of technologies of InX are
    identified, discussed, and evaluated to motivate future research in this direction.
    First, the main technologies used to enable InX are highlighted. Then their security
    weaknesses are discussed based on recent state-of-the-art research work. Furthermore,
    the potential security solutions and technological concepts are presented. Future
    research directions are drawn to grasp the attention of researchers to the existing
    security challenges. This article is organized as follows: Section II presents
    the related work, focusing on the existing survey and review articles that either
    focus on the security of Industry 4.0, Industry 5.0, or the important technological
    enablers of InX. Section III provides a brief introduction to the key enabling
    technologies of InX. The section also gives a glimpse of the importance of security
    of the overall ecosystem, as well as the key enabling technologies. Section IV
    focuses on the security of communications networks and technologies in InX. Section
    V discusses the security of the selected technologies in the industrial infrastructure
    technologies, such as IIoT, CPS, and robots, etc., and Section VI details the
    security of industrial applications with examples of augmented reality and blockchain.
    Risk management and standardization efforts are elaborated in Section VII. Important
    insights and lessons learned are discussed in Section VIII. Future research directions
    are presented in Section IX, and the article is concluded in Section X. For smooth
    readability, the organization of the survey is depicted in Fig. 3, and the most
    used acronyms are presented in full form in Table 1. TABLE 1 List of Most Common
    Abbreviations FIGURE 3. Organization of the article. Show All SECTION II. Related
    Work Due to the increasingly critical nature of operations of future industrial
    systems, huge research efforts are underway on various aspects of its security.
    Most research efforts focus on specific themes that can be counted within the
    boundaries of InX. However, there are limited efforts that present security challenges
    and possible solutions in communications of industrial systems as a whole. Since,
    InX uses several technologies that rely on communications, such as IIoT, CPS,
    machine learning, big data, and unmanned aerial vehicle (UAV), to name a few,
    its security has become highly complex. This complexity can be the main reason
    for limited efforts in presenting security challenges and possible solutions for
    the whole InX ecosystem. In this section, we provide a brief literature review
    of existing surveys and review articles on the security of InX, and/or technologies
    that are highly related to InX from the communications perspective. The most relevant
    recent articles are highlighted in Table 2. The main theme of the article is tick-marked
    (✓) concerning the relevant technology. As an example, the first article in Table
    2, Overview of Industry 4.0, focuses on CPS, and thus there is (✓) under CPS.
    TABLE 2 Existing Survey and Literature Review Articles With Main Focus Highlighted
    and Compared to Our Article A survey on opportunities and challenges existing
    in Industry 4.0 is presented in [3]. The authors emphasize on mobile Internet,
    IoT, CPS, cloud computing, and big data as the most important enabling technologies.
    Among the vital challenges are the development of smart devices, the construction
    of a network environment for CPPS, the integration models for CPS and CPPS into
    a homogeneous environment, and the lack of verification and testing platforms
    for CPS. A survey on the security of Industry 4.0 from the aspects of edge computing
    and blockchain, mainly to secure IIoT-based critical infrastructure, is presented
    in [58]. The main focus of the article is on the convergence of edge computing
    and blockchain for scalable security of critical infrastructure. A detailed account
    of security challenges in Industry 4.0 is presented in [43]. The main design principles
    pivotal to Industry 4.0 are interoperability, information transparency, technical
    assistance, and decentralized decisions. Each of these design principles will
    attract new security challenges when converged to practicality in future industries
    since new technologies attract new types of attacks. Security attacks can include
    simple that can be mitigated with simple techniques as well as complex attacks
    that can circumvent the functionality of the whole system. Various attacks are
    highlighted on different enabling components of Industry 4.0 such as CPS, IoT,
    cloud infrastructures, Industrial Control Systems (ICS), and the flow of goods
    and information. The authors also outline security design principles that are
    relevant to each underlying enabling technology. Conti et al. [59] provided a
    review of ICS designs, devices, and security protocols, and evaluated their robustness
    over existing ICS testbeds and datasets. It also offers recommendations for their
    design and reports on the top-performing algorithms. The survey in [60] examines
    how ICS has developed from standalone setups to cloud-based settings, emphasizing
    how these technologies are convergent with the Internet. The study places particular
    emphasis on the application of machine learning techniques to improve cybersecurity
    in the context of cloud-based industrial process migration. The recently published
    survey in [55] highlights the growing necessity for customized cybersecurity measures
    in ICSs by highlighting complex and individualized attacks on key infrastructures.
    Here, the authors examined the benefits of Software-Defined Networking (SDN) in
    creating coordinated intrusion response plans for ICS and provided a taxonomy
    of intrusion response plans. The adaptation of threat modeling for ICSs is summarized
    by Khalil et al. [56] through comprehensive literature evaluation, provided their
    vulnerability to cyberattacks that can have severe consequences. The study emphasizes
    the significance of strategic frameworks covering security, privacy, and improved
    validation metrics in ICS threat modeling approaches. Supervisory control and
    data acquisition (SCADA) systems have become an integral part of modern ICSs.
    SCADA [61], [62] are ICS used to monitor and control critical distributed systems
    that span large geographic areas. Examples of such systems include electric power
    transmission and water distribution systems, and facilities in single sites such
    as manufacturing industries. A survey on the security of SCADA systems is presented
    in [47]. The survey presents protocols, security threats, and possible solutions
    to those threats. Another survey on the security of SCADA systems [63] discusses
    various attacks and countermeasures. However, there is no survey, at the time
    of writing this article, that directly addresses different aspects of security
    in InX. Therefore, we also present survey articles that cover the security of
    each of the most important technologies to communications in InX. Since 5G is
    considered one of the main technological enablers of reliable communications in
    InX, the security of 5G will have strong implications for InX. The security challenges
    in 5G with possible solutions and future research directions are presented in
    [22]. Since 5G is a conglomeration of several technologies, including 4G technologies,
    the security of 5G is highly dependent on those technologies. For example, network
    function virtualization (NFV) [64] and the concepts of software-defined networking
    (SDN) [65] have their own security implications [8], [66], and thus these technologies
    must be properly secured to ensure the security of 5G. Furthermore, due to the
    conglomeration of new devices (e.g., IoT) and services (5G verticals), security
    monitoring must be automated due to the resulting humongous growth in network
    traffic. Therefore, authors in [22] discuss the need for machine learning-based
    automated security systems that can also predict outage or failure of different
    technologies and segments of the network. However, there is no visible work, at
    the time of writing this article, on reviewing the security implications of 5G
    networks on InX or even Industry 4.0. A detailed discussion on the enabling technologies,
    applications, and challenges of the industrial Internet is presented in [28].
    The article discusses the security of the industrial Internet from the perspectives
    of industrial terminal security, industrial data security, industrial communication
    security, and industrial management security. Communication authorization and
    data encryption have been considered to be the most important security concerns.
    A survey on Information and Communication Technologies (ICT) for Industry 4.0
    is presented in [40]. The article highlights the security challenges that can
    arise from the integration of different technologies in Industry 4.0 such as IIoT
    and cloud computing. A detailed survey on IoT-induced security vulnerabilities
    in critical infrastructures is presented in [34]. The authors discuss how malicious
    actors exploit weak IoT technologies as a first step toward compromising critical
    systems connected to those IoT devices. The article [34] further explains the
    security challenges caused to other industrial systems including smart grids,
    smart homes, and building automation systems, and also highlights the possible
    mitigation techniques. In [45] the authors focus on the security of several IoT/IIoT
    applications by classifying threats according to the object of vulnerability,
    either software, network, or data. Also, the role and importance of blockchain
    (as well as its limitations) in IoT/IIoT security are highlighted, and use cases
    such as E-Healthcare, VANET, supply chain, and smart grids are discussed. Issues
    like reducing blockchain feedback latency and computation overhead, and the need
    to develop application-specific security approaches are mentioned as open research
    areas. A survey of IIoT is presented in [67]. The article provides a state-of-the-art
    study on IoT and its relation to industry. Challenges, opportunities, and future
    research directions in IIoT are presented in [33]. A survey on threats to IoT
    is presented in [38], where besides threats to IoT on the general level, a comprehensive
    attack methodology for malware attacks is presented. Persistent attacks include
    node compromise and malware attacks which are attributed to weaknesses in communication
    protocols. Similarly, centralized control architectures can be detrimental because
    of a single point of failure. Furthermore, a systematic survey on the security
    of IIoT with requirements and opportunities presented by fog computing is provided
    in [44]. IoT security is one of the biggest weak points that holds back the adoption
    of IIoT, mainly because of poor security resulting in globally known compromises
    in industrial systems [44]. A survey of practical security vulnerabilities in
    IoT is presented in [37] and [57]. The latter article also addresses the general
    challenges the IoT philosophy creates in structures automation systems. The CPS
    security is often seen as overlapping with IoT security [68]. Therefore, threats
    and defenses for wireless connectivity for IoT and CPS have been surveyed together,
    e.g., in [27]. The differences in the concepts [68] lie in the emphasis: while
    IoT emphasizes identification and Internet connectivity for all kinds of devices,
    the CPS concept emphasizes monitoring, control, and automation of physical processes
    without referred connectivity protocols. Security threats and solutions for CPS
    have been surveyed in [26]. Similarly, a survey on security control and attack
    detection in industrial CPS is presented in [36]. The work presents a security
    overview, keeping in view the limitations of resources of CPS for security, from
    control theory perspectives. The main challenges, such as DoS, replay, and deception
    attacks, are discussed from the engineering perspective. Furthermore, the approaches
    of using honeypots and honeynets for the security of IIoT and CPS are presented
    in [49]. A comparative examination of protocols and architectures of industrial
    wireless sensor networks (WSNs) from the perspectives of existing standards is
    presented in [24]. A survey on data management in Industry 4.0 is presented in
    [39], where the article discusses security laps in the technological enablers
    of assembly lines and industrial robots. Here, the distributed systems to avoid
    sending data over insecure channels and single points of failure, are suggested
    to be adopted. Furthermore, the article [39] highlights that real-time security
    systems are required that detect abnormal behaviors early on to avoid the cascade
    of failures throughout the whole system. Authors in [69] present the applications
    of digital twins and big data in the smart manufacturing process, for carrying
    out predictive maintenance, design of products, and planning during the production
    process. Müller et al. [70] illustrated the relationship of an industry encountered
    with big data analytics, in which the economic study helps to analyze the magnitude,
    direction, and impact of their relation. It helps to provide robust business value
    by marking out vital boundaries by providing empirical evidence. CPS research
    trends related to big data in industry 4.0 along with cloud computing are investigated
    in [71]. In a smart manufacturing process, profit per hour is assessed in production
    processes as a control parameter [72]. It helps to achieve better throughput,
    yield, and optimal decisions and provides good benefits using advanced algorithms
    on industrial big data. There are several survey articles on the security of cloud
    computing [73], [74]. Related to the security of cloud platforms and the security
    of information or data on the cloud platform, authors in [75] survey blockchain
    technologies to improve the privacy and security of cloud platforms. A survey
    on isolation techniques in cloud data centers that can be crucial to InX is presented
    in [76]. A systematic survey on the opportunities that fog computing brings to
    secure industrial systems is presented in [44]. Fog nodes can be used to effectively
    isolate infected nodes, whereas the rest of the industry can perform normally.
    Similarly, fog nodes can perform localized monitoring processes, provide on-premises
    authentication and access control, and perform time-sensitive tasks. Therefore,
    fog computing can improve the resilience of industrial systems [44]. A survey
    on edge computing in IIoT is presented in [48]. The article elaborates on the
    motivation for using edge computing for IIoT, the research progress in this direction,
    and then highlights the potential challenges. The main benefits, outlined in the
    article, include improving the system performance, protecting data security and
    privacy, and reducing operational costs in IIoT environments. A survey on machine
    learning methods for the security of industrial protocols is presented in [77].
    Since the main focus of the article [77] is on the protocols, the security weaknesses
    in many protocols are exposed. The authors provide methods of machine learning
    that are most helpful in analyzing the security of protocols in ICS. A survey
    of machine learning techniques used in the analysis of security and stability
    of power control systems is available in [46]. The article highlights studies
    on various types of machine learning techniques in this regard and discusses their
    strengths and limitations. The security challenges and possible solutions for
    machine learning in communication networks are presented in [78]. Big data analytics,
    machine learning, and the applications of artificial intelligence in wireless
    networks are discussed in [79]. Security issues in cloud robotics environments
    are surveyed in [41]. The authors discuss cryptographic algorithms such as Rives
    Shamir Adleman (RSA), Advanced Encryption Standard (AES), or Elliptic Curve Cryptography
    (ECC), as options for enhancing security against threats such as network or data
    storage attacks. Research work in authentication is identified as the starter
    point of extended research toward the next security phases. In [35], the authors
    perform a study of the most common middleware used by robotics frameworks, their
    cybersecurity capabilities, and the impact of security on communications performance.
    Results show there is no significant effectuation in terms of latency and packet
    loss. Security of UAVs is studied in [80], the authors cover security threats
    such as jamming or spoofing as potential threats. Also, basic use cases related
    to physical layer security (PLS) are mentioned. The authors in [42] focus on PLS
    as an approach for avoiding eavesdropping attacks and thus enhancing security
    on UAVs. Technologies such as multiple input multiple outputs (MIMO) antenna and
    mmWaves are also considered to improve security alongside spectral efficiency.
    The work in [81] focuses on the weaknesses of UAVs for civilian and military use
    cases, as well as countermeasures for efficiently avoiding their exploitation.
    Among the vulnerabilities discussed, we find user-level vulnerabilities, drone
    vulnerabilities, and wireless vulnerabilities. In [82], the authors focus on the
    lack of security mechanisms in widely used UAV and ground control stations (GCSs)
    communication protocols. Different vulnerabilities are identified, among them
    integrity attacks, availability attacks, as well as authenticity attacks. The
    authors in [83] scanned the whole IPV4 address space looking for visible ROS services,
    they were able to obtain readings and manipulate a robot located in a remote laboratory
    as proof of the vulnerabilities of robot systems. Also, some recommendations regarding
    the use of firewalls, VPNs, and exposure limitations are provided. The work in
    [84] focuses on describing the different vulnerabilities present in robot systems,
    from physical vulnerabilities to communication and even software vulnerabilities.
    Also, the authors propose solutions to mitigate possible attacks, including designing
    for security, the use of encryption for secure communications, and the detection
    of security breaches. A review of AR systems in Industry 4.0 with a use-case of
    the shipyard is given in [85]. The principles of Industry 4.0 are discussed to
    pave the way for future digital shipyards, termed Shipyard 4.0. Cloudlets and
    fog computing nodes are suggested for use in the shipyard, similar to Industry
    4.0, to minimize the latency and accelerate rendering tasks while offloading heavy
    computation tasks from cloud platforms. The security of IAR is considered to be
    important, however, not discussed. Several security risks, potential solutions,
    critical assets and goals, and sensitive IAR applications are discussed in [29].
    Auto-Identification (Auto-ID) and traceability technologies for Industry 5.0 are
    discussed in [86]. The main focus of the article is on Different surveys discussing
    various security services offered by blockchain technology are covered in [87],
    [88]. Blockchain-based security in the domain of industry 4.0 applications (e.g.,
    smart manufacturing, smart grids, smart vehicles) are presented in presented in
    [89], [58], [90], [91]. However, despite their growing popularity due to several
    key features such as decentralization, immutability, and transparency, there are
    still several security threats in blockchain that must be resolved before its
    complete adoption in various industrial and manufacturing applications [92], [93].
    In this context, the authors in [94], [95], [96] explored various attacks on the
    blockchain network and possible countermeasures from various perspectives, i.e.,
    threats to the network, attacks on consensus mechanism, and smart contract vulnerabilities.
    A review of blockchain-based solutions for industry 4.0 is presented in [97],
    which also provides an overview of using blockchain to provide security solutions.
    On a holistic level, the security trends and advances in manufacturing systems
    in Industry 4.0 are described in [30]. The three main security requirements based
    on which various solutions and proposals are evaluated are confidentiality, integrity,
    and availability. The article discusses the security implications on a general
    or high level, without going into details about the security of each enabling
    technology. Furthermore, the 5G infrastructure which is considered one of the
    main enabling technologies of future industries, as elaborated in [11], is not
    discussed in depth to understand its security implications. For example, 5G can
    expose factory information through shared cloud environments or provide means
    for the propagation of security vulnerabilities into the industry. SECTION III.
    A Brief Introduction to Key Enabling Technologies InX represents a highly complex
    environment due to the amalgamation of huge number of diverse sets of technologies
    with unique requirements. For example, massive numbers of IIoT will be used and,
    generally, IoT applications have different requirements. Some IIoT applications
    require high reliability and availability, whereas, some applications require
    high throughput and low latency [98]. The priorities of throughput or latency
    might even change over time. Therefore, AI with its disciplines will be a major
    enabler of the applications and technologies of IIoT [99], [100]. Since IIoT will
    generate massive data, big data analytics [101] and AI will play a major role
    in learning the behavior and needs of IIoT and allocate the resources accordingly.
    For connectivity, different communication technologies will be used, leveraging
    the latest developments in networking such as SDN, NFV, and MEC [102], [103],
    for instance, to allocate the necessary resources dynamically. All of the integrated
    technologies of InX will have very distinct and unique, security requirements,
    challenges, and solutions. In this section, we aim to provide a high level overview
    of the security posture of InX. The InX ecosystem is presented in Fig. 4 that
    comprises i) secure communications, that connects all the components within InX,
    ii) secure factory environment, that is composed of different enabling technologies
    that empower the very functionalities of industrial systems, and iii) industrial
    applications, that are used between different industrial systems and external
    stake holders with two specific examples of Augmented Reality (AR) and blockchain.
    In the following subsections, we briefly discuss these as an introduction for
    the security analysis. FIGURE 4. Simplified visualization of InX integrating factory
    and external environments. Show All A. Security of Communications in InX The backbone
    of InX is communication technologies that enable the diverse set of industrial
    systems to work in unison. The main differentiating factor in InX and other industrial
    revolutions is the capabilities of devices to interact with each other through
    communication technologies. Therefore, the security of communications technologies
    is extremely important. The important technologies that are covered under secure
    communications include 5G wireless networks, since 5G is poised to connect future
    industrial systems. We also discuss cloud computing under communications security
    mainly due to its vital role in bringing communications-specific technologies,
    such as core network functions, into critical infrastructures to meet its unique
    demands. The extension of clouds, such as edge computing, may well be part of
    the critical factory environment due to its role in data analytics and machine
    learning, however, we discuss all these concepts together in cloud computing for
    smooth readability. Furthermore, UAV-based communications and standard industrial
    communications technologies are also discussed under secure communications. 1)
    5G Wireless Networks The existing wired systems, even including the latest technologies,
    cannot meet the requirements of InX due to the mobile, remote, and dynamic nature
    of functions and services of InX. Wireless control of industrial processes, on
    the other hand, requires an ultra-fast, secure, and always-available (five-nines
    availability) underlying communication system. Since InX requires connectivity
    of its systems beyond the traditional restricted short-range communications, for
    example in the supply and demand chain, as highlighted in Fig. 4, cellular networks
    have become a necessity for industrial technologies [104], [105]. 5G, in this
    sense, is becoming one of the main enablers of industrial automation [6], [106]
    with new disruptive technologies that fulfill the requirements of InX. For instance,
    5G can serve InX components and services that need extremely low-latency communication,
    such as the operation of robotic arms, using Ultra-reliable low latency communication
    (URLLC) [107], [108], and through the migration of critical services or control
    functions to the vicinity of InX, to edge and fog nodes. Cloud-based systems can
    also help in separating automation functions from the traditional specialized
    physical equipment to help increase flexibility and agility [11]. Since cloud
    computing has become increasingly important in enabling latency-critical services,
    cloud computing is also discussed in the realm of secure communications, even
    though cloud computing is also critical to the factory environment for various
    functions such as data analysis. 2) Cloud Computing Benefiting from higher computing
    and storage resources, cloud computing and its extension in the form of edge and
    fog computing, can bring elasticity to InX. Multi-access Edge Computing (MEC)
    is a standard solution by the European Telecommunications Standards Institute
    (ETSI) for edge computing [109]. The concept of edge computing has been further
    pushed towards the local environment to meet even stricter latency requirements,
    for example, to facilitate lightweight microservices, i.e., nanoservices [110].
    Cloud computing is pivotal to InX because of its main role in other enabling technologies
    of InX such as machine learning and big data analytics, CPS and IoT, and linking
    other physical objects or systems to services [12]. The main delivery mechanism
    of data between the local, edge, and remote clouds, as well as between IIoT or
    CPS and clouds, is considered to be advanced wideband cellular networks [39],
    such as 5G. 3) UAV-Based Communications With the adoption of smart factories (Industry
    4.0) and the envisioning of the InX paradigm, alongside the vast deployment of
    sensors and actuators, production systems need an efficient optimization of data
    transmission, low-latency computation, and dynamic decision-making. InX use case
    scenarios will rely on UAVs for providing ubiquitous wireless connectivity, and
    efficient in-network computation capabilities that allow the processing of sensory
    data on time. UAVs will be mainly deployed as aerial base stations or relay nodes
    for enhancing coverage, capacity, and reliability. Furthermore, UAVs can be deployed
    as flying mobile terminals for enabling real-time video streaming for generating
    situational awareness, item delivery or infrastructure inspections, and sensor
    monitoring [111]. UAVs will also have an important role in supply-chain management
    in InX. 4) Standard Industrial Communication Technologies There is a wide array
    of standardized industrial communication technologies. For example, there are
    different standards for communication between different IIoT devices, communication
    technologies for industrial automation, wired communications, and time-sensitive
    communications. Examples of these standard communication technologies include
    Message Queuing Telemetry Transport (MQTT), Advance Message Queuing Protocol (AMQP),
    Constrained Application Protocol (COAP), and ISA100 Wireless, to name a few. These
    technologies fall in the realm of a secure factory environment, for instance between
    IIoT devices and production lines. However, for smooth readability, we briefly
    discuss these technologies in the section on secure communications. Below we describe
    the most important communication-relying technologies in the secure factory environment.
    B. Security of Key Industry Infrastructure Technologies The factory environment
    is a main industrial site where the actual industry-related functions, such as
    production, happen. Various technologies are highly intertwined with each other.
    For example, IIoT and CPS-based production systems, AI-based automation and monitoring,
    and advanced robots that collaborate for production, as shown in Fig. 4. Below,
    we briefly outline the importance of these technologies in InX along with their
    security. 1) IIoT Beginning with an abstract idea of cost-efficient tagging and
    tracking of “things”, which we use daily, IoT started its movement of towards
    its current use and future visions [112]. Currently, IoT technology has become
    such an important aspect of future societies, that the success of future connectivity
    infrastructures, such as 5G, is tied to the widespread use of IoT, resulting in
    the enormous growth of the IoT landscape [102]. The industry of the future is
    no different, where devices and equipment ranging from tiny to powerful industrial
    systems and applications will rely on IoT [67]. IoT will facilitate pervasive
    or ubiquitous computing by bridging the gap between digital and physical existence
    through low-cost, low-power, and easy-to-deploy digital devices [113], [114],
    [115]. IoT in the sense of InX is the collection of sensors, actuators, robotic
    arms, and other mechanical components having the capability to send or receive
    data over the network or Internet, making also Industrial IoT or IIoT. IIoT has
    to be defined in [116], from which we take the bottom line: IIoT works to optimize
    the overall production value of industries. IIoT has, thus, become a backbone
    of InX, and several proposals exist for improving the performance of IIoT in Inx
    [117]. 2) CPS CPS bridges the gap between physical computing and communication
    infrastructures. The gap is already shrinking as envisioned by smart and gadget-free
    computing environments [118], and research has been initiated on the security
    of such environments [119]. The concept of CPS is an enabler for automation and
    it emphasizes control and sensing technologies as well as machine-to-machine communication.
    CPS enables close interactions of computation and physical processes, typically
    with a feedback loop and often without direct human involvement. Examples of industrial
    processes, where physical processes are sensed and actuated by controller software,
    include, e.g., smart grids, autonomous vehicles, robotic systems, nuclear power
    plants, as well as control systems for dams, oil, and gas industries. CPS, thus,
    exposes expensive and critical physical assets, processes, as well as sensitive
    information to the vulnerabilities and threats coming from the cyber-world. 3)
    Advance Robotics Robots have been an important part of automation systems, often
    synonymous with automation, and constitute the key building blocks of future industrial
    systems [120]. Smart robots designed for performing complex tasks can sense, process,
    and interact with their environment, improving the state of industries by bringing
    extreme precision into play [121]. Robot systems are present in a wide variety
    of use cases, from automotive, aerospace, and defense, to pharmaceutical, and
    distribution centers, as well as food and beverages industries. Moreover, as connectivity
    increases, the originally isolated robot systems are being exposed to either corporate
    networks or the Internet, making them valuable for collecting data and performing
    analysis on quality, reliability, and productivity. 4) Big Data Analytics Big
    data is one of the pillars of InX, with smart connected machines playing a predominant
    role in big data generation [122]. Due to the increasing number of smart devices
    in InX, some produce bursts of data while others sporadically few bytes, the data
    will be big, and therefore analytics for such big data will be inevitable to learn
    and act intelligently in the future. The paradigm shift of industrial transformation
    towards InX, aided by industrial big data is gaining momentum at a different pace
    in different parts of the industry. Big data, mostly the combination of structured,
    semi-structured, or unstructured data is collected from organizations for carrying
    out predictive modeling and analytics. Big data could be acquired from business
    transactions, customer databases, social networks, industrial data, and many other
    sources. There are no definite numerical standards to define the term big, but
    big data is often characterized by 8 Vs: Volume, Velocity, Variety, Veracity,
    Value, Variability, Validity, and Visualization as shown in Fig. 5, typically
    referring to terabytes, petabytes, and exabytes of data. Big data is used in real
    life to create new value for the industries and the customers utilizing the products
    from those smart industries. The trustworthiness of the data, its business value,
    and the variability of ways the business can use and format the data tailored
    for end applications play a crucial role in big data. When big data is deployed
    correctly with appropriate models, it helps industries to improve operations,
    enhance customer service, create personalized campaigns for marketing products,
    and improve the profitability of their ventures. FIGURE 5. Characteristics of
    Big Data with 8Vs that support InX big data management. Show All 5) Machine Learning
    Machine learning has become a critical technology with its tools to predict the
    future course of actions based on current and past states of systems, as well
    as, involved human intervention [123]. Since machine-human interaction is central
    in InX, machine learning is poised to be one of the main enabling technologies,
    as discussed in [124]. With 5G providing computing and storage capabilities in
    the vicinity of InX, machine learning in the network edge [125] in InX will be
    facilitated by 5G [126]. Furthermore, the tools and techniques of machine learning
    will use the (big) data generated from the components of InX such as CPS and IIoT,
    the network (5G), and the diverse services, to name a few. The outcome of the
    machine learning tools and techniques, along with the necessary feedback, will
    be to enable and improve real-time decision-making, resource and risk management,
    systems’ functions and security monitoring, prediction of workload and manpower,
    and improving maintenance and supply chain. One of the most important uses of
    machine learning in InX is related to the prediction of occurrences of events
    in the future [127]. C. Security of Industrial Applications There can be a huge
    number of industrial applications in InX. For example, modern healthcare, public
    transport, and other public infrastructures can be considered as part of InX,
    as discussed in [128]. However, we focus on two distinct application areas, such
    as augmented reality and blockchain, that can become integral to any future industrial
    system or ecosystem when deployed securely. 1) Augmented Reality In InX, augmented
    reality (AR) is a game-changing technology that is transforming maintenance, safety,
    operations, and training. AR provides realistic environments and scenarios much
    more than traditional simulation environments. AR projects digital data into the
    actual world, giving employees real-time insights and direction. With applications
    ranging from product design, warehouse logistics, immersive training, remote support,
    quality control, safety enhancement, data visualization, collaborative work, and
    enhanced consumer experiences, it plays a critical part in InX. Streamlined to
    the unique requirements of future industrial systems, Industrial Augmented Reality
    (IAR) streamlines industrial procedures with AR to lower mistakes and boost the
    output [85]. IAR applications in InX are broad, ranging from manufacturing to
    assembly operations, an online guidance system for training the operators, and
    maintenance to human-robot collaboration [129], [130]. Security in data connectivity,
    device compatibility, and smooth integration with industrial infrastructure are
    essential for the success of IAR in InX for more effective and efficient operations.
    2) Blockchain The InX environment will be a collaborative, intelligent, and connected
    ecosystem comprising humans, machines, and other stakeholders, such as, service
    providers, as well as AI-enabled automation. One of the prime requirements will
    be to establish distributed trust among various entities throughout the whole
    ecosystem [52]. In this regard, blockchain as a distributed technology can offer
    immense added value to InX applications by providing a tamper-proof, decentralized,
    and trustworthy computing platform for multiple involved entities to securely
    exchange their transnational data and/or share/sell/rent the available resources
    among each other without the intervention of any trusted third party or intermediaries
    [131] [132]. Strong defense against cyber threats is facilitated by fundamental
    features of the blockchain, which include decentralization, data privacy measures,
    smart contracts for automatic security enforcement, secure identity verification,
    auditability, and immutable transaction records. Blockchain holds a distributed
    and shared digital ledger, where information is encrypted and validated by all
    participants of the network. Hence, blockchain contributes to improved security
    throughout InX by ensuring supply chain transparency, offering audibility, and
    traceability, and fostering interoperability among various devices that are logically
    and geographically widely distributed [133]. D. A Brief Outline of the Security
    Posture of InX The InX ecosystem is an amalgamation of a large set of technologies.
    The technologies discussed in the prior subsections rely on communications infrastructure,
    such as 5G, as their underlying enabling infrastructure. Since communication technologies
    can be prone to security weaknesses and vulnerabilities, those technologies will
    likely face security threats. Therefore, a thorough security analysis of the InX
    ecosystem is needed, which is the main focus of this article. The security analysis
    is performed under three main parts of the InX ecosystem. These include the i)
    security of communications technologies, ii) the security of different technologies
    used in the factory environment, and iii) the security of critical industrial
    applications. First, the main security challenges for each technology under each
    category are brought forth, and then potential security solutions for those challenges
    are researched. For smooth readability, the most important challenges and their
    respective solutions are highlighted in Table 3. The left column in Table 3 presents
    the enabling technologies of InX, the middle column represents the most important
    challenges, and the last column highlights the solutions with references to articles
    that provide details about the specific solutions. In the following sections we
    discuss these security challenges and solutions, beginning from the security of
    communications in InX. TABLE 3 Summary of Security Challenges and Potential Solutions
    in Enabling Technologies SECTION IV. Security of Communications in InX In this
    section we introduce the main communication technologies along with their security
    footprint. Since, the fifth generation of wireless networks, also known as 5G,
    is poised to connect and combine most of the industrial systems either through
    standalone 5G network or non-standalone 5G network, below we discuss the potential
    of 5G in InX and then discuss the related security landscape. A. 5G Wireless Networks
    Since 5G is crucial for InX, its security is even more important. Furthermore,
    even if there are security challenges within 5G systems, vulnerabilities must
    not propagate to InX. Therefore, proper measures should be in place to not only
    stop security threats in 5G and its technological enablers but also mitigate the
    risks involved with such vulnerabilities. 5G and its key new technologies including
    cloud platforms (MEC and fog nodes), softwarized and virtual networks, and the
    techniques of enhanced mobile broadband (eMBB), and URLLC, etc., do have security
    challenges as discussed in [7], [8], [10], [54]. Therefore, in the following subsections,
    we bring forth the main security challenges and vulnerabilities in 5G, and the
    possible solutions for those challenges and vulnerabilities that are most important
    to InX. 1) Security Challenges The security challenges in 5G related to InX are
    multi-dimensional, from threats to traffic flowing through the network to the
    network entities and components of InX. Industrial traffic can be categorized
    into two types, i.e., cyclic and acyclic, generated by different sources and with
    different time requirements [192]. Cyclic traffic, typically, includes fast data
    exchange between controllers and field devices and the amount of data is usually
    a few bytes. The data can be sensing values and measurements with stringent latency
    requirements such as a few hundred microseconds. The acyclic traffic, comprising
    limited amounts of data, is triggered by unpredictable events such as process
    alarms. Communication networks introduce delay into the system, as discussed in
    [193], and can be struck on the delay constraints, as discussed in [194]. One
    of the main challenges related to meeting the real-time requirements of InX while
    using 5G will be the delay introduced in the backhaul networks, as highlighted
    in [194] for routers and switches. Therefore, any security vulnerability that
    can increase the latency at any intermediate points within 5G will cause availability
    challenges in InX. Industrial communication systems and their challenges with
    future research directions covering the need for 5G-based wireless networks are
    discussed in [192]. An example of industrial network performance is given, which
    outlines an approximate packet delivery time for wireless networks to be in the
    range of a few hundred microseconds. With such stringent requirements, any security
    threat that could exhaust the resources of intermediate nodes, or congest the
    communication link for a millisecond, will be considered successful. The very
    enabling technologies of 5G, such as Software Defined Networking (SDN), NFV, cloud
    computing, and massive MIMO, for example, have their own security challenges.
    Pertinent to InX, technologies centralizing control decisions, such as SDN, will
    cause most challenges in terms of increasing risks related to availability due
    to security vulnerabilities. For example, SDN centralizes the network control
    decisions to (even though logically) centralized control planes, called SDN controllers.
    These controllers have been demonstrated to increase the visibility of network
    control points, even if physically distributed, due to the very nature of their
    operation of installing flow rules in the underlying packet forwarding infrastructure
    [66]. As a result, there can be clear points of interest to be targeted for security
    attacks, such as denial of service (DoS) or resource exhaustion attacks. In the
    case of NFV, hypervisors can be targeted for attacks due to being central to the
    process of virtualization. Other technologically enabling components of 5G such
    as massive MIMO can be targeted for different types of attacks such as active
    and passive eavesdropping [195]. 5G is also becoming the de-facto standard in
    terms of enabling connectivity of many other technologies used in InX that have
    their own security procedures and protocols for connectivity [54]. Examples of
    such technologies include low-power wide area networks (LPWANs) that enable massive
    machine-to-machine (M2M) communications for diverse types of IoT. The security
    challenges in LPWAN are related to the interfaces, air and wired, and the most
    pertinent one is the air interface between end-user devices and the gateways or
    eNBs, as discussed in [196]. Since devices in LPWANs have low capabilities, encryption
    if not provided by the network (5G) will be left to an optional on-demand basis,
    which can result in security breaches. Furthermore, the challenges are related
    to the inherent weaknesses of devices making LPWANs, such as devices in the IoT
    domain, discussed in the IoT Section V-A. 2) Potential Security Solutions The
    network that serves or connects IIoT devices and networks needs to understand
    their unique requirements [197], to adjust or configure itself autonomously to
    fulfill the service requirements. Therefore, the disciplines of AI such as machine
    learning can be used to enable the network to learn the requirements of IIoT autonomously
    and adjust itself accordingly. AI and machine learning algorithms in the edge
    will facilitate quick network response to the needs of IIoT, as described in [198].
    The concepts of cloud computing (e.g., MEC) already facilitate the communication
    networks to fulfill the service requirements of IIoT in terms of providing computing
    and storage resources near mitigate its challenges of resource constraints. The
    extreme densification in future wireless networks (e.g., in 5G) [199], with a
    variety of heterogeneous access networks [200], using new technologies such as
    massive MIMO antennas [201], millimeter Wave (mmWave) [202], aims to cope with
    the challenges of the availability in access networks. SDN and MEC are the key
    technologies to meet the network resource requirements of IIoT [136]. For example,
    the global visibility of the status and stats of network resources coupled with
    programmable control provided by SDN enables run-time service migration from clouds
    to MEC servers or nodes in the environment. One of the naturally secured approaches
    taken in 5G, which is highly important for the security of industrial systems
    and services, is the 5G verticals as outlined in [6]. Using the concepts of virtualization,
    strengthened by the concepts of NFV [203], the verticals isolate traffic generated
    from different sources and thus have the capability to ensure end-to-end security
    of the different industrial processes. Therefore, huge research is going on in
    this direction, mainly from the perspectives of its use cases in InX, as discussed
    in [134]. Multi-access Edge Computing (MEC) has been proposed and used in 5G [204]
    to meet the latency requirements [135], where the different services can be isolated
    through the concepts of verticals as discussed in [134]. Such solutions along
    with URLLC systems [107] effectively address the challenges of latency-critical
    services. The inherent limitations in the technologies of 5G such as SDN, NFV,
    and MIMO need to be addressed first in an isolated fashion followed by security
    hardening of the integrated 5G system [22]. Solutions to the important security
    challenges of the main enabling technologies of 5G such as SDN, NFV, cloud platforms,
    massive MIMO, etc., are discussed in [7], [22]. The security of SDN and NFV in
    the context of IoT is discussed in [205]. The authors outline how the joint use
    of NFV and SDN complements the existing security approaches of IoT. For example,
    how a slice (isolated set of programmable resources) can effectively isolate traffic
    at run time using the programmable nature of the network enabled by SDN. Security
    challenges related to the centralized control points can be mitigated by devolving
    the local decision-making to data plane or localized control point elements, as
    evaluated in [137] for SDN. In terms of security of the radio devices, the nature
    of massive MIMO, for instance, being used in a vicinity provides enough opportunity
    to secure it from passive and active eavesdropping and jamming as discussed in
    [206], [207]. The tunneling beyond the vicinity using IPSec, for instance, can
    also provide the required security. The solutions for maintaining critical communication
    between InX and remote cloud platforms include maintaining redundant links and
    prioritizing traffic according to the critical nature of the traffic, as discussed
    in [208]. In the case of network exposures, some of the devices also have their
    security procedures if the network exposes its traffic for instance in the case
    of exposed air interfaces for IoT as discussed in the challenges above. For example,
    Sigfox devices increase the confidentiality of the data through end-to-end encryption
    as discussed in [138]. However, all the optional choices of security procedures
    such as security configurations and encryption technologies need to be mandated
    and brought into use. B. Cloud Computing Cloud computing will empower data-based
    real-time decisions in InX. Cloud platforms can be either centralized or distributed
    using platforms such as MEC or fog nodes, each having their own benefits and consequences
    in terms of security. Similarly, cloud platforms can be shared among multiple
    users, operators, or services. Such sharing will require the concepts of virtualization
    to be used. Virtualization will also have its security challenges. Furthermore,
    remote cloud systems, including centralized and distributed, will also require
    the underlying communication systems to be secure enough to avoid misadventures
    in terms of security. Therefore, the security of cloud systems is multi-pronged
    and has unique challenges and solutions as described below. 1) Security Challenges
    Cloud computing has its challenges of availability and security. The connectivity
    to local (MECs, fog nodes) or centralized cloud platforms will be mainly provided
    by the latest cellular technologies that have limited coverage areas, whereas
    routes to data centers may be long, exposing connections to congestion or many
    other network problems. Cellular networks also have their challenges of security
    which can expose the systems to further security challenges as discussed in Section
    IV-A. The main challenges that exist in the cloud, such as weaknesses in isolation
    and improper management of virtual machines, will also open InX systems to security
    vulnerabilities, such as DoS, man-in-the-middle attacks, and availability challenges
    that can disrupt the flow of the InX process. The Federal Office for Information
    Security of Germany considered virtual machine manipulation and lack of control
    of user data in the cloud systems among the topmost ten threats to ICSs in 2016
    [209]. The requirements for offering services in industry 4.0 as cloud applications
    are discussed in [210]. The authors outline the requirement of communication links
    from a smart grid system to a cloud-based monitoring system, highlighting the
    possibility of link congestion. The data transfer rate needed in smart grids for
    monitoring the infrastructure, for instance, is at least 500 kbps per node [211].
    Such monitoring relies on communication link providers, who may face congestion
    in their infrastructure without knowing the critical nature of the communication.
    Therefore, link congestion can, inadvertently, become an availability challenge
    for security monitoring of critical functions in cloud systems in InX. IT assets
    that the extension of cloud computing platforms, such as MEC and fog nodes, need
    to manage in the context of InX contain not only data, metadata, and software,
    but also computing, caching, and networking applications. Due to the physical
    exposure, boundary openness, weak computational capacity, device heterogeneity,
    and coarse-grained access control of such IT assets, they are threatened by physical
    security, computing security, communication security, etc. [212]. Compared with
    cloud computing, edge and fog computing are composed of computation-limited hardware
    and heterogeneous firmware. Since distributed dge and fog servers are mainly used
    for processing delay-sensitive and mobile IoT services, most of the computing,
    caching, and networking resources in distributed edge/fog servers are used for
    supporting real-time demand response. However, distributed edge/fog servers do
    not have additional resources to run complex security protection measures, and
    thus, simple physical attacks [212], [213], [214] can compromise a lot of edge/fog
    IT assets. Having noticed this, the adversary favors first capturing several edge/fog
    IT assets and turning them into weapons against upstreaming fog servers [215],
    [216]. Caching data at distributed fog servers is one of the most popular services
    in future industries relying on information-centric networking (ICN) [217]. Considering
    the remoteness and virtual nature of the Internet, the caching strategies and
    cached data will suffer from various cache poisoning attacks such as cache pollution
    attacks, cache side-channel attacks, and cache deception attacks [218]. These
    cache poisoning attacks will result in huge concerns about privacy, security,
    and trust in content placement, content delivery, and content usage for mobile
    users, respectively [219]. Another important trend is to deploy SDN on the edge
    and fog platforms to manage heterogeneous networks and schedule massive traffic
    more efficiently [220], [221]. In such frameworks, the data plane only needs to
    transmit data packets and the control plane focuses on generating/selecting reasonable
    routing paths for each data packet. The attacks on the control and data planes
    of SDN, as discussed in [66], pose significant threats to such frameworks. By
    forging some LLDP data packets in the data plane, an attacker can build fake communication
    links to fool routing algorithms in the control plane to forward packets in the
    data packets to fake communication links or interrupt fog services [222]. 2) Potential
    Security Solutions Cloud platforms, as in the case of InX, can be used to increase
    the security of InX beyond the traditional approaches. Coupled with the latest
    technologies, for instance, virtualization, cloud platforms can be used to separate
    different services within InX based on the criticality of the service. To deal
    with security challenges related to the use of cloud computing platforms in future
    industries, there are many available solutions, which can be divided into three
    aspects: first, edge data encryption and key management; second, security situation
    awareness; and third, certified adversarial defense. To guarantee edge/fog computing
    security, all data on edge devices must be encrypted [153]. However, resource-limited
    edge devices usually cannot support long-term protection over periods of ten or
    more years. Liu et al. [150] proposed the use of lattice-based cryptography to
    design efficient data encryption solutions for edge computing devices in the post-quantum
    IoT. However, most of the data on edge devices will not be stored for a long time
    and a user device often needs to configure multiple security keys or passwords
    for different applications. To simplify the complex key management scheme, a reconfigurable
    edge/fog computing security scheme is proposed, which treats edge servers as a
    new security agent (SA) to execute security authentication and access control
    [151]. Security situation awareness is proposed to construct a security state
    map of the atomized IT assets deployed in different edge computing application
    scenarios. With such a given security state map, security operators can know the
    attack behaviors timely and configure security strategy flexibly [223]. The key
    enabling technology of security situation awareness is network traffic analysis.
    The most popular network traffic analysis technology is deep learning. Known attacks
    are easy to detect by extracting features and configuring rules. The challenging
    and hot task is unknown attack detection. Since the unknown attacks have no accurate
    labels, a semi-supervised learning algorithm is an appreciable method to actively
    detect unknown attacks [152]. In such a scheme, the deep learning model actively
    requests annotations for the newly-arrived network traffic. Combined with the
    decision-making theory, the deep learning unknown attack detection method has
    good interpretability. The security risks brought by artificial intelligence should
    be circumvented through certified adversarial defense. For adversarial attacks,
    adversarial training is an active defense technique, which requires feeding adversarial
    examples into the model training procedure. When the new model learns all permutations
    of the adversary, adversarial attacks cannot fool such a new learning model [145],
    [224]. Recently, differential privacy technology has been perceived to have the
    potential to improve the model robustness and prevent deep gradient leakage [149].
    The essential of differential privacy is adding Gauss noises or Laplace noises
    into the inputs, gradients, or weights of the learning model. By assigning different
    privacy budgets, the trainer can achieve multiple learning models with different
    robustness levels. For backdoor defense, the most effective method is gradient
    pruning, whose performance can also be certified by adjusting the number of pruned
    gradients [148]. Furthermore, machine learning is discussed below. C. UAV-based
    Communications UAVs are pivotal in an increasing number of use cases within Industry
    4.0 and InX (as well as for military and civil operations), mainly due to their
    high mobility in 3D spaces. As UAVs are capable of either autonomous or semi-autonomous
    operations, they require reliable navigation in the form of control and GPS communications.
    This characteristic of UAVs makes them the target of attackers trying to hinder
    their communication links using either simple or well-designed hacks to get their
    control. Furthermore, UAVs are generally computation and energy-constrained devices,
    with limitations that hinder the deployment of complex, and upper-layer-based
    security solutions, which are deemed as computation and energy costly. In the
    same manner, their high mobility combined with their physical fragility paves
    the way for new security challenges. Therefore, we discuss the main security challenges
    and possible solutions for those challenges in the context of InX below. 1) Security
    Challenges Jamming is one of the main threats against UAVs as they provide a strong
    line-of-sight (LoS) in use cases where they act as either a base station, relay
    node, or flying mobile terminal. Strong air-to-ground (A2G) and ground-to-air
    (G2A) communication links improve the reception of malicious eavesdroppers as
    well as ground or aerial jammers, affecting the communications and control channels
    of the UAV [225]. Jamming uses radio interference to degrade wireless communications
    by keeping the channel busy, corrupting the signal at the receiver, and causing
    the transmitter to retreat when sensing the medium is busy. Although jamming attacks
    mostly target the physical layer, cross-layer attacks are also possible as a jammer
    can have similar capabilities to the legitimate nodes in the network [189]. By
    jamming the communications and control channels of a UAV, an attacker would hinder
    communication with other UAVs and with its remote controller. Jamming the GPS
    receiver will block the autonomous flight of a UAV [226]. Another important threat
    is GPS spoofing. In spoofing attacks, signals identical to those of valid satellites
    are generated by the attacker, the receiver cannot identify the real signal and
    chooses the counterfeit as valid based on its power [227]. There are two different
    methods for an attacker to take over a GPS, overt capture, and covert capture.
    In overt capture, a combination of jamming and spoofing attacks is used, whereas
    in covert capture the attacker assumes the target possesses spoofing detection
    measures that must be avoided. The covert nature of GPS spoofing attacks makes
    them difficult to identify (in comparison with the more obvious jamming attacks)
    as the UAV cannot verify whether or not the ground station has been compromised.
    In the same manner, the unencrypted, unauthenticated, and open structure of GPS
    signals alongside their data bit predictability, facilitates the job of the attacker.
    When successful, a GPS spoofing attack can grant the attacker total control over
    the UAV position, velocity, and time [228]. Malware infection is also possible
    as attackers can exploit the vulnerability of embedded communication protocols
    through a reverse shell payload that is injected into a UAV’s memory and installs
    malware on the systems running the ground stations. A reverse shell attack consists
    of a shell session that is initiated from a remote node towards the local machine,
    they are used by attackers that performed a remote command execution attack as
    it is the only way to gain remote shell access through NAT or a firewall. This
    threat is worsened by the applications used for allowing users to pilot UAVs using
    their tablets or mobile phones as wireless remote controls [80]. A combination
    of the aforementioned attacks is used to physically affect the UAVs either by
    capturing, replacement of its cargo, or controlling the drone with the sole purpose
    of crashing it. These physical vulnerabilities are relevant as drones can also
    play a logistic role in InX. 2) Potential Security Solutions An interesting approach
    for jamming protection is PLS, which efficiently protects transmissions between
    network nodes, hindering the efforts of malicious eavesdroppers. Cryptographic
    techniques are widely used for protecting data transmission of the UAVs in the
    upper layers [186], [187]. For protecting the A2G links, some of the techniques
    used are beamforming, trajectory and communications design, and UAV cooperation.
    3D beamforming offers a more refined beam resolution in both elevation and azimuth
    plane (especially effective when used alongside a noise signal), making it an
    attractive option for 5G applications, and nulling the user’s signal in the directions
    of eavesdroppers [207]. Efficient trajectory and communications design is aimed
    at helping the UAV move more freely in the 3D space, avoiding blockage with users
    and incurring blockage with malicious eavesdroppers, thus improving communications
    and secrecy rate [229], [230]. UAV cooperation expects to improve the maneuvering
    limitations of UAVs to increase security performance by deploying multiple collaborative
    UAVs. In this scenario, some of the UAVs might act as jammers being deployed close
    to ground eavesdroppers, and degrading their signal quality by sending noise signals
    [188], [189]. Protection of G2A can be achieved by using the aforementioned techniques,
    as well as implementing device-to-device (D2D) communications. Frequency-hoping
    spread spectrum (FHSS) and direct-sequence spread spectrum (DSSS) is some widely
    applied anti-jamming techniques, although their application is limited due to
    the strong LoS and a spectrum-efficiency trade-off [231]. There are several effective
    countermeasures against GPS spoofing and their application depends on the nature
    of the attack. Techniques useful against basic attacks include the observation
    and comparison of the received signal strength over time [190], and the monitoring
    of the identification codes of GPS satellites to check whether they are constant
    or not [191]. Nevertheless, more experienced attackers can avoid these protective
    measures as they tend to use sophisticated and more complex attacks. Better planned
    attacks can be detected by equipping a UAV with two GPS receivers and checking
    their cross-correlation, however, this method was not efficient against attacks
    sending weak spoofing signals [228], [232]. A technique proposed in [233] can
    detect spoofing attacks via a ground infrastructure that checks real-time information
    regarding the time of arrivals to the expected UAV positions over time, this technique
    has been quite effective in detecting spoofing attacks within two seconds, and
    the attacker’s location within fifteen minutes of monitoring. In [234], the authors
    introduce a system dynamics-based framework that includes a cooperative localization-based
    anti-spoofing mechanism that can determine the real location of an attacked UAV
    based on the location of neighboring UAVs. Finally, malware infection can be avoided
    by using secure communication protocols such as eCLSC-TKEM. Also, on the ground
    station side, privileged access needs to be tightly controlled, avoiding the execution
    of files from the /temp directory, setting up deep packet inspection solutions
    intercepting SSL and TLS connection, alongside a continuous update of the firmware
    to help reduce the possibility of suffering reverse shell attacks [80]. D. Security
    Overview of Standard Industrial Communication Technologies In this subsection,
    we briefly discuss the most important standards for industrial communications
    with their security features. The range of industrial communication systems is
    very wide and spans almost four decades of evolution [6]. Accordingly, the availability
    of security features is diverse. Most older field-level communication systems
    do not provide security at all, which led to the development of defense-in-depth
    concepts [235]. Modern industrial communication systems based on Ethernet and/or
    IP lend themselves to the application of security layers known from the IT world.
    1) MQTT MQTT (Message Queuing Telemetry Transport) is a widely used standard for
    IoT and IIoT (Industrial IoT). MQTT is based on the publish-subscribe model, providing
    an indirect route, via a broker, between publishers and subscribers [236]. The
    presence of MQTT is not limited solely to IoT or IIoT, the standard MQTT-SN (MQTT-Sensor
    Network) offers resource optimization for running on processing and memory-constrained
    devices by using simpler header and payload structures than regular MQTT [237].
    Regardless of its ubiquitous nature, MQTT is vulnerable to security threats as
    its only security feature is unilateral authentication. It lacks security functionalities
    such as access control, or control message security. To secure the communications
    channel, current MQTT implementations make use of TLS (Transport Layer Security)
    between devices and the broker [238]. 2) AMQP AMQP (Advance Message Queuing Protocol)
    is a standard for asynchronous message queuing that facilitates the exchange of
    messages between components of a system, independently of their underlying implementation.
    The AMQP model is capable of emulating store-and-forward queues, as well as topic
    subscriptions, or even content-based routing [239], [240]. Although conceived
    in the financial sector, AMQP is used in a range of challenging applications that
    include autonomous computing, cloud computing, and IoT. Unlike MQTT which is intended
    for telemetry transmissions and aims at constrained devices, AMQP can work with
    both constrained and unconstrained nodes. AMQP implements TLS and SASL (Simple
    Authentication and Security Layer), including modern SASL mechanisms like GS2
    and SCRAM-SHA (Salted Challenge Response Authentication Mechanism). Furthermore,
    AMQP’s design allows for the use of alternative security mechanisms as they are
    developed [241]. 3) CoAP CoAP (Constrained Application Protocol) is a Web transfer
    protocol that provides a client-server (URI-based) model for connecting constrained
    application nodes and easily interfacing with HTTP. CoAP is mainly deployed in
    environments such as smart energy and building automation, since its standardization
    in 2014 research has shown it is an efficient option for low signal strength environments
    [242]. Being UDP-based, the networking overhead associated with TCP is avoided,
    although a UDP-based confirmation and retry model is included to facilitate message
    delivery. CoAP makes use of DTLS (Datagram Transport Layer Security) to secure
    the communications channel, it is based on and provides a similar level of security
    as TLS [243]. 4) ISA 100 Wireless The ISA100 Wireless standards aim to be the
    universal solution for industrial wireless networks. Developed by the ISA100 committee,
    the standards focus on addressing the requirements of the emerging Industry 4.0,
    make use of 6LoWPAN (Low-power Wireless Personal Area Network), include specifications
    regarding protocol stack, system administration, security for low data rate wireless
    devices, among others. It is also fully compatible with smartphones, as well as
    IEEE 802.15x, IEEE 802.11x, and IEEE 802.16x devices [244]. In ISA100 Wireless,
    a security manager entity is in charge of authenticating, storing, and distributing
    end-to-end security keys. Security options are optional and can be deactivated
    in scenarios where end devices are constrained, however, this flexibility poses
    a security threat. One of the standards, the ISA100.11a, uses AES symmetric encryption
    and provides direct messages in a peer-to-peer fashion, the latest version of
    the standard provides security spoofing and reply attacks [245]. 5) 6TiSCH The
    Timeslotted Channel Hopping (TSCH) mode was introduced to the Medium Access Control
    (MAC) portion of the IEEE802.15.4 standard. The TSCH is the standard for industrial
    automation and process control. The IPv6 over TSCH (6TiSCH) is aimed to enable
    the adoption of IPv6 in industrial standards. Details about the security of the
    IETF 6TiSCH are presented in the survey paper [246], which outlines different
    standards for lightweight industrial communications. The security of 6TiSCH is
    still under research, where issues such as sharing secret keys among the network
    nodes are an open question. However, the 6TiSCH architecture defines static scheduling,
    hop-by-hop scheduling, neighbor-to-neighbor scheduling as well as remote monitoring
    and scheduling management, where the security demands are high. Their engagement
    in track forwarding, fragment forwarding, and IPv6 forwarding is highly recommended
    for low-power industrial communication. 6) EtherCAT EtherCAT is an ethernet-based
    control solution for industrial automation sectors. It is capable of addressing
    specific concerns in industries such as rapid response times, minimal data requirement
    for the devices engaged in communication, and efficient cost of implementation.
    With EtherCAT, the master sends data possibly only a single frame for the entire
    node network that will pass through each node [247]. However, the EtherCAT protocol
    lacks connection-based security and flow issues for recognizing the masters and
    slaves in the network, which may lead to vulnerability in the MAC layers, DoS,
    and man-in-the-middle attacks. 7) Profibus The Profibus is one of the most common
    networks used in the industrial automation process. Such a process field bus,
    which is meant for interfacing decentralized peripherals, where can drastically
    reduce the wiring costs. However, one of the serious concerns in the Profibus
    is the authentication issues among the master and slave nodes in the network [248].
    Moreover, they are also susceptible to DoS threats, which need isolation from
    the other devices in the network. SECTION V. Security of Key Industry Infrastructure
    Technologies In this section the security of key technologies in industrial environments
    that rely on communication technologies as their backbone are discussed. For example,
    IIoT and CPS systems are already integrated into industrial systems for a range
    of purposes, ranging from controlling large production and assembly lines to actuators
    in individual components. Similarly, collaborative robots relying on fast communication
    infrastructure co-work to create or assemble different products. Different industrial
    processes are monitored through machine learning techniques using huge amounts
    of data (big data) generated by sensors, IIoT or CPS systems, and communicated
    to nearby edge clouds or centralized clouds for processing. In the following subsections,
    we discuss the security of these technologies that make the factory environment
    and are dependent on communication technologies. A. Industrial Internet of Things
    The use of IIoT is extremely diverse, ranging from nano-chips in healthcare to
    precision agriculture and monitoring oil pipelines over long distances. In InX
    IIoT will be used in massive numbers and will be connected through communication
    networks to enable new services needed by companies such as predictive maintenance
    of industrial equipment, surveillance, remote control, consumption metering, asset
    tracking, transport, etc. Since IoT usually have low capabilities in terms of
    memory and processing [117], the environment in which they operate must provide
    sufficient security. IIoT devices themselves can have security weaknesses either
    inherently or can be compromised due to low resources onboard, as discussed below.
    1) Security Challenges There are several challenges to the smooth operation of
    IoT from various perspectives. For example, the challenges due to the limitations
    of IoT devices in terms of computing, storage, and communication capabilities
    [67], [249], and the challenges imposed by the operating environments, such as
    communication networks [250], [251], that include interference [252], security
    [253], [254], [255], and availability of network resources. One direct consequence
    of low resources, discussed in [33], is that IIoT will mostly be not capable to
    run resource-demanding cryptographic protocols, for instance, based on public-key
    cryptography. An availability challenge in network access is caused by the higher
    number of concurrent access to the network, large overhead during synchronization
    among the devices, and the lack of support for bursty or sporadic arrival of the
    data from IIoT devices and networks [256]. Security challenges of IoT with some
    case studies and their potential solutions are discussed in [257]. The article
    elaborates on attacks due to software failures and vulnerabilities, such as buffer
    overflows in firmware, or through physical tampering in electronic circuits or
    memory of physically captured IoT devices, for instance, copying or changing the
    identifying and authenticating information of devices. Furthermore, the article
    [257] discusses the possibility of eavesdropping and man-in-the-middle (MITM)
    attacks to sniff data traffic and extract critical network information in case
    of the communication lacks encryption. The article also discusses malicious code
    injection with physical access to IoT devices, for example, in a very simple way
    by pressing the hard-reset button. Some of the devices have limitations in terms
    of bandwidth and thus there is an upper bound on the packet header size leaving
    little room for additional security-related information [138]. The low header
    space, low memory, and low processing capabilities make the conventional elliptic-curve
    or asymmetric cryptography not suitable for such devices. For example, asymmetric
    cryptography works with keys of bigger lengths than the payload of sigfox [258]
    of 12 bytes. Similarly, the limited number of message transmissions in Sigfox
    does not allow the parametric exchanges of the elliptic curve algorithms [105].
    The lack of encryption is a major challenge in IIoT communication. For example,
    most of the control components in field bus communication communicate with plain
    text, allowing attackers to compromise the systems with little effort, issue control
    commands, or at least read information [209]. Furthermore, covert channel attacks,
    exploiting traditional client-server communication approach, over Modbus/TCPIP
    communication channels is demonstrated in [259]. It has been shown that signaling
    and man-in-the-middle attacks can be pretty straightforward if some basic information
    or a few nodes of the system are exposed. Industrial wireless sensor networks
    (WSNs) [260] have been considered as one of the pillars of enabling the transition
    from the old-fashioned wired industrial systems toward self-healing and controlling,
    flexible, and intelligent wireless control systems. Several standardized techniques
    for enabling industrial WSNs are discussed in [260]. These include ZigBee, Wireless
    HART, ultra-wideband (UWB), 6LoWPAN, ISA100, and blacktooth and blacktooth Low
    Energy (BLE) techniques. However, ISA 100 is the most commonly accepted standardized
    technique. In the ISA 100 standard, most of the security functions are optional,
    leaving room for security vulnerabilities. On the challenges of Wireless HART
    [261], authors in [262] explain that implementing security in the software of
    embedded devices will consume its processing, so much so that the devices will
    not be able to meet the 10 ms time-slots requirements of Wireless HART. Hardware
    accelerators are proposed, in such cases, to meet the processing requirements
    of security functions such as encryption techniques. 2) Potential Security Solutions
    Due to its massive role in InX, the security of IIoT requires several layers of
    security from strengthening the security of the device through software and hardware-based
    security hardening to operations security and minimizing its impact in case of
    breaches. Hence, the first step is to minimize inhere vulnerability levels of
    IIoT devices and then enforce and increase access control security on physical
    and logical levels on the critical infrastructures. Means to improve the security
    of the overall InX eco-system include [34]: hardening IIoT devices against physical
    and tampering attacks, addressing communication-related weaknesses such as a lack
    of cryptographic techniques, and reducing the potential impacts of such weaknesses
    by identifying dependencies and increasing segmentation. Since IIoT will leverage
    5G technologies for connectivity the security solutions used for 5G will provide
    a good level of security for IIoT communications as discussed in [22]. Furthermore,
    authors in [263] discuss the use of blockchain [264], [265] and edge computing
    to secure the use of IoT in Industry 4.0. Blockchain, as discussed in [180], can
    enable trusted data sharing in a decentralized system comprising many edge nodes.
    Such frameworks, on the one hand, can meet the requirements of latency using MECs,
    and ensure security using blockchain, on the other hand. Security solutions for
    IoT from the perspectives of physical, medium access control, network, and applications
    layers are surveyed in [266]. Different architectural alternatives have been considered
    [267] for securing IoT, including distributed security with blockchains, as well
    as the use of fog and edge computing for data analysis, response, and secure storage-
    The inherent limitations of IIoT that cause major challenges, i.e., resource limitations,
    can be overcome by utilizing the latest developments in other technologies, such
    as the extension of cloud platforms into MECs. The main purpose of MEC is to bring
    computation and storage resources into environments that need them, and SDN can
    be used to program the network at run-time to redirect traffic to such resources.
    Decentralized fog-based secure approaches [268] that use localized processing
    are proposed for the security of IoT in critical environments. Virtualization
    technologies can be used to slice resources into isolated domains even in smaller
    platforms such as MEC for isolation-based security. Furthermore, security systems
    that can be implemented with a low budget in terms of resources are highly important
    for IIoT. Therefore, methodologies such as header compression to enable encryption
    techniques on IoT devices that are capable only of low packet sizes or data rates
    can be useful [34]. One of the important methods of ensuring the timely availability
    of data in critical systems, specifically in industrial mixed-criticality, is
    to prioritize and de-prioritize traffic according to the delay sensitivity, reliability,
    or the criticality of the system, and service or the data. Data can be generally
    classified into safety, monitoring, and control [24]. Ensuring data delivery in
    industrial WSNs for critical systems with strict latency requirements through
    novel priority-aware data flow mechanisms is demonstrated in [9]. A plastic extrusion-based
    process monitoring scenario is used to define the protocol requirements and working
    principle of the proposed method. The protocol schedules access to channels for
    each data flow using a distributed prioritized medium access mechanism to guarantee
    channel access for the most critical traffic over others. B. Cyber-Physical Systems
    CPS can be realized with alternative connectivity mechanisms and support of different
    application protocols. CPS connectivity can be based on 5G and cellular networks,
    IP-based connectivity, or other wireless communication means, including, e.g.,
    WiFi, blacktooth, ZigBee, as well as in distributed cases satellite, and LoRaWAN.
    On top of wired and wireless connectivity alternatives, lay different application-specific
    protocols, such as Modbus and Distributed Network Protocol (DNP3) for ICS as SCADA;
    IEC 61850 for smart grids; as well as controller area network (CAN), vehicle-to-vehicle
    (V2V) and vehicle-to-infrastructure (V2I) protocols for vehicles. Therefore, CPSs
    also have security challenges and threats that migrate from these technologies,
    as described below. 1) Security Challenges The main threats and attack vectors
    both from the cyber and physical world against CPSs are illustrated in Fig. 6.
    Central CPS elements – control system, sensors, and actuators, as well as physical
    processes - are organized into a feedback control loop and presented in Fig. 6
    with brown rectangles. Physical, cyber, and cyber-physical attack paths are illustrated
    with red, yellow, and yellow-red ovals. Remote cyber adversaries may reach CPS
    systems, e.g., through management and control interfaces or software updates.
    CPS with Internet connections will face the remote cyber-threats of Industrial
    IoT as discussed before, in Section V-A. All CPS systems face threats originating
    from local connectivity, insiders, and the physical environment. Threats trying
    to compromise - tamper or disclose - sensing and control interactions. Physical
    threats against sensors and actuators as well as indirectly against the whole
    cyber-physical system are consequences of harsh environmental conditions or hostile
    adversaries within the weakly guarded industrial sites. The integrity of the control,
    sensor, and actuator platforms and software is threatened both by the cyber and
    physical world. Integrity and accuracy of the information collected from the physical
    world affect to the situational awareness and decisions made in the cyber-world.
    Control systems are increasingly utilizing big data and machine learning technologies,
    and are thus vulnerable to malicious or tampered feedback, adversarial learning
    [269], and other challenges, as discussed in Sections V-D and V-E. Consequently,
    physical-world attacks can escalate to malicious or misguided actions in the cyber-world,
    which then may cause even more damages – sabotage, denial of operation, destruction
    of physical devices, and thefts of service – in the physical processes. FIGURE
    6. Security threats in cyber-physical systems. Show All The convergence of critical
    infrastructure cybersecurity and ICS takes vital significance in the context of
    InX. Critical process monitoring and control are crucial to InX’s transformation
    of industrial operations through data-driven processes and innovative technologies.
    To this end, ICS plays a major role. On the other hand, due to the increased digitalization
    and interconnection that come with InX, cyber attacks might interfere with the
    operation of critical infrastructure by exposing ICS components [270]. This demands
    a thorough strategy to protect ICS and, consequently, the larger critical infrastructure
    that underpins InX. A foundational layer of security for InX is formed by safeguarding
    ICS against vulnerabilities, cyberattacks, and nation-state threats, as well as
    by strictly adhering to cybersecurity regulations. This ensures InX’s resilience,
    incident response readiness, and public-private collaboration in the face of constantly
    changing cyber threats. Current challenges include heterogeneity of devices and
    solutions, trust issues, as well as a lack of technical capabilities of devices
    used in industrial domains. The heterogeneity – different applications, various
    types of devices, and protocols – means that the security standards and solutions
    are fragmented. This causes technical interoperability issues and increases the
    complexity of the security architecture. CPS can be based on different connectivity
    and application alternatives. These alternatives have their security protocols
    for assuring the confidentiality and authenticity of the communication. Application-specific
    protocols either integrate their security approaches or rely on the underlying
    communication security. A challenge in the past has been an assumption that CPS
    are closed systems and operated in a trusted physical environment [26]. This leads
    to solutions that are non-secure-by-design. Securing the whole life cycle of CPS
    components is also a challenge. In addition to technical protection during the
    operational time, supply chains must be verified. Components should be assured
    or trusted not to contain hidden vulnerabilities and to provide the required security
    level. In complex industrial settings, the security of supply chain management
    depends on several factors and suppliers, and the amount of involved persons increases
    the risk of insider attacks; the supply chains are dynamic and constantly changing,
    and the liabilities may also be unclear due to lacking legislation. Managing trust
    as well as finding supplier-specific problems and vulnerabilities can become a
    complex challenge [271]. 2) Potential Security Solutions CPS security relies on
    confidentiality, authenticity, and access control functions that are provided
    by a) connectivity mechanisms [27], b) physical layer [170], c) end-to-end protocols
    for CPS applications [171], as well as d) platform and interface controls of controllers,
    sensors, and actuators [163]. Security architecture for addressing known threats
    against CPS systems is facilitated by recommendations and best practice documentation
    that have been produced by industrial cooperation. For instance, ENISA has produced
    guidelines for securing software and development life cycles [272]. Further, cyber
    ranges [179] are emerging to facilitate isolated security testing of CPS. Security
    metrics have been defined [178] to facilitate holistic security analysis and design
    of industrial CPS. CPS is characterized by feedback loops. While these loops for
    control are applied for various industrial applications, they can also provide
    reactive security protection for CPS. Different anomaly detection and machine
    learning approaches [172], [173], [174], [175] have been proposed for CPS to enable
    detection and reactions to intrusions, malware, anomalies, and other threats.
    Solutions for making the control systems robust against malicious or tampered
    feedback data include teaching machine learning to be resistant or to detect adversarial
    samples [273], [274], [275], [276]. Solutions addressing the security challenges
    arising from heterogeneity [176] in the application or the connectivity layer
    can be divided into two main categories: through a common language, which is achieved
    with standards [277] or semantic approaches [278], or through mediating middleboxes,
    such as gateways or proxies. Solutions for industrial communication network security
    are in general applicable to industrial applications of CPS. Technology and process-related
    aspects and requirements for industrial cybersecurity have been specified and
    standardized, e.g., in IEC 62443 [164], [277], which provides a risk-based framework
    for managing the security of industrial actors. In ICS, the problem of sophisticated
    industrial attacks is addressed by NeuPot [279], an ICS honeypot technique based
    on neural networks. Using a time-series forecast model and a Modbus honeypot framework,
    it improves security through better honeypot interaction and cyber threat detection,
    showcasing exceptional efficacy in both areas. The objective of the research presented
    in [280] is to identify off-path false-data-injection attackers in ICS while they
    are in their hiding phase. Using secret keys, the defense approach described in
    [280] continually introduces tiny distortions into sensor data to ensure accurate
    and timely identification of hidden intruders without interfering with regular
    ICS operations. A compromise between control performance and detection efficacy
    is taken into account by the ideal design of ICS watermarking, which was implemented
    in [281]. Here, the technique uses an optimization strategy to estimate the watermark
    strength, and updates detection metrics to lessen the impact of noise. Both theoretical
    analysis and real-world trials show the method’s higher performance. Heterogeneity
    introduces the need for additional solutions and hence complicates systems, which
    in turn may enable new vulnerabilities. These vulnerabilities from complexity
    can be managed by isolating different applications and technologies from each
    other. Due to the existence of different kinds of systems and devices with different
    security capabilities and risks, systems are commonly [277], [282] divided into
    zones or segments with different security levels to isolate security breaches
    and attacks. Network solutions for segregating different CPS processes have leveraged
    learning-assisted network slicing [165] where different applications are automatically
    recognized and isolated. However, in the end, complexity and security challenges
    must be solved separately using approaches that are suitable for the applied technologies
    and physical process, e.g., with power-grid [166], power-plant [167], charging
    station [168], or autonomous vehicle [169] – specific cyber-controls. C. Advance
    Robotics Robots have unique characteristics regarding data collection, learning,
    mobility, and decision-making, they are mainly built through the interconnection
    of a wide variety of components such as sensors, communication devices, and actuators,
    mostly interconnected by a wireless network. Since robots were originally designed
    to be part of isolated systems, security was not an integral part of their design,
    resulting in trivial OS-related, protocol-related, as well as hardware-related
    threats. With the advent of Industry 4.0, paradigms like cloud robotics, and the
    almost ubiquitous presence of robot systems, copious amounts of data produced
    by plants need to be analyzed and sent over communication networks to remote servers
    for further processing. Given the pivotal role of robot systems in InX, security
    in robotics has a top priority due to the impact of their vulnerabilities in the
    chain of production [283]. 1) Security Challenges Software found on robot systems
    is usually outdated and relies on weak or even obsolete cryptographic packages.
    This issue is as relevant for robots as it is for computers, software will no
    longer receive security updates which increases the possibility that vulnerabilities
    become popular among attackers. Since novel security mechanisms are not present,
    the impact of software vulnerabilities radically increases, improving the success
    probability of an attacker, and hindering any detection efforts [181]. Another
    important threat is the lack of security mechanisms in the protocols used for
    robotic systems, as they do not integrate authentication or integrity methods
    to detect suspicious behaviors. The Robot Operating System (ROS) is a popular
    development platform for robotics that uses a publish/subscribe model, from a
    security point of view this model is insecure as publishers cannot verify their
    data, and subscribers can’t verify the data received. The lack of encryption,
    and therefore privacy, increases the risk of attacks like man-in-the-middle as
    well as hijacking. Man-in-the-middle refers to an attack in which a malicious
    node acts as a relay and can alter the communications between two parties who
    are unaware of the situation [284]. A hijacking attack occurs when a malicious
    node assumes control of a session between a server and a client and replaces the
    incoming packets with new packets that are sent toward the destination [285].
    In the same manner, the use of outdated cryptographic libraries is not beneficial,
    as is misconfigured cryptographic software such as shared, symmetric keys for
    virtual private networks (VPNs) [286], [287]. Without proper measures for confidentiality,
    integrity, and privacy, attackers can eavesdrop on published data and modify messages,
    altering the robot’s behavior. More specifically, an attacker can access and modify
    the configuration parameters of robots, alter the logic of the program being executed,
    change the commands being sent by a remote operator, or inject false information
    regarding the robot’s status. Damages caused by the mentioned attacks vary from
    defective products to operator injuries [182]. ROS architectures allow clients
    to initiate remote communication with a robot via its IP address, this is necessary
    for use cases such as remote operation, or video streaming from a robot’s camera.
    Such exposure causes a massive vulnerability as found in [83], where a considerable
    amount of master ROS nodes were listening on port number 11311, leaving the robot
    systems behind them vulnerable to malicious users. Robots are also susceptible
    to physical attacks, like the use of their USB port for executing malicious commands,
    or the connection to a robot’s controller using the RJ-45 port from which the
    attacker can access other system components [288]. 2) Potential Security Solutions
    It is of vital importance to avoid robot systems running on outdated software,
    the best method to achieve this is by regular updates and upgrades. While some
    software often updates in the background, this is not always the case, the principal
    practice is to look for available updates and if available, install them. The
    purpose of updates is to provide general maintenance to software, as well as install
    patches against vulnerabilities and improve threat protection [183]. Similarly,
    upgrades are needed to keep software healthy, they usually introduce considerable
    changes and might not be needed right away. Nevertheless, vendors eventually stop
    supplying updates to old software, and in such a case upgrades are necessary to
    avoid running outdated software [289]. Protocol security can be improved by adopting
    one of the available robot application frameworks, although the security level
    offered varies depending on their popularity and scope. Data Distribution Service
    (DDS) is a connectivity framework for distributed systems capable of performing
    authentication and encryption for remote client discovery via Real-Time Publish-Subscribe
    protocol (RTPS) packets that run over any transport [290]. DDS also offers support
    for Transport Layer Security (TLS) and Datagram Transport Layer Security (DTLS),
    besides authentication and encryption DDS is also able to implement access control,
    data tagging, and security events logging [291]. The Internet Communications Engine
    (ICE) is an object-oriented framework that provides encrypted bidirectional connection
    and supports SSL at the transport layer. Although SSL has been rendered as too
    heavy for constraint devices, there are lightweight implementations available
    for embedded applications [292]. The most popular solution is the already introduced
    ROS, which is developed under a publish-subscribe approach. ROS includes its communication
    middleware, but unfortunately, it does not provide security features by default,
    except for client isolation in Virtual Private Networks (VPNs). However, due to
    its wide adoption, ROS has been enriched with several extra features that significantly
    improve its security capabilities. Research has contributed by adding security
    features to ROS, such as the use of Web tokens for secure authentication of remote
    clients [291], [292], [293], and the use of cryptographic methods to ensure data
    confidentiality and integrity as well as the use of an authentication server to
    certify only valid clients form part of the developed application [185]. SROS
    is another extension aimed at providing ROS with modern cryptography and security
    capabilities, enhancing security at transport encryption, access control, and
    process profiles [184], [294]. D. Big Data Analytics Security in the realm of
    big data will be multi-pronged. For example, avoiding errors in the data and its
    analytics will be of paramount importance due to the criticality of the decisions
    based on the data or analytics. Thus the data must be protected from errors as
    well. For example, in InX, there will be large, diverse, structured, or unstructured
    data produced by smart sensors, devices, log files, and video and audio in real-time.
    There will also be decisions based on the data. Security verification before making
    critical decisions based on such data will be extremely important. Security of
    hidden patterns and unknown information extracted from big data is essential for
    better decision-making, preventing situations of uncertainty, and mitigating the
    possibilities of malicious activities. Below, we discuss the most important security
    challenges that are linked to big data in the realm of InX. 1) Security Challenges
    The developments in the management of high-stream industrial data in InX ecosystems
    bring numerous technical challenges. Data will be generated by an overwhelming
    amount in InX. Technology has advanced to the extent that hackers can access the
    data to extract vital information. For example, unauthorized data transmission
    across different unauthenticated groups in InX can lead to data leakages and it
    imparts potential risks to industrial data that need care and security [295],
    [296]. In addition, the lack of proper authorization techniques can result in
    data breaches, which can be an extremely serious concern in InX, resulting in
    access and stealth of sensitive data [297]. False data injection, as discussed
    in [298] is another challenge, through which most of the functional, as well as
    non-functional requirements of data-driven applications in InX, can be in jeopardy.
    The reliability in the transmission and reception of IIoT data is also a serious
    concern. Industrial data acquired from IIoT devices in InX applications, as discussed
    in Section V-A, is prone to corruption attacks unless treated with robust big
    data analytic engines and appropriate encryption schemes [101]. Learning-based
    frameworks used for InX applications, associated with industrial big data, demand
    rigorous training of the data. Such frameworks consume huge computational resources,
    as well as require robust learning models. Furthermore, the training models used
    for data analytics are also subject to security threats and the data could be
    anonymized by hackers [299]. Such learning-based frameworks in InX applications
    are prone to cyber-attacks. The ultimate aim of such threats includes false decision-making.
    Moreover, data spaces in the context of InX are virtual environments that are
    essential to securely organizing and sharing data. Centralized data spaces can
    be prone to DoS or similar attacks, resulting in vulnerabilities like single-point-of-failures
    or restricting availability. Such threat necessitate serious efforts in terms
    of solutions, which are described below. 2) Potential Security Solutions There
    are many use cases of InX in which big-data analytics have already been used.
    The most promising solution for avoiding security breaches and data leakages is
    developed by Xu et al. [139] using a blockchain-based framework integrated with
    watermarks. Here, InX use cases can be used to selectively exchange data with
    other blockchains, which are accountable for resisting information leakage. Injection
    of hardware trojans on industrial data also suppresses the data breach threat
    and avoids data leakages [300]. Such techniques implement Trojan triggers using
    capacitors and are tested under different operating conditions. Trojan trigger
    accounts for securing the big data in InX applications from leakage vulnerabilities.
    Integration of IoT-based frameworks could help monitor and control the cyber security
    attacks on industrial data [140]. Security aspects of IIoT systems in InX applications
    could be carried out by incorporating appropriate security and encryption with
    the support of blockchains [301], which provide secure and trustworthy services.
    Also, the usage of dual dynamic key [302], and lightweight searchable encryption
    protocols [303] enhances the reliability in the transmission of IIoT data. The
    role of learning-based techniques is crucial in industrial big data analytics,
    particularly for anomaly detection on data [142]. Some of the machine learning
    techniques such as SVM and Random forests are used to provide anomaly detection
    [143] using real-time industrial data [141]. Wang et al. [304], propose an approach
    of using feedback on big data and coordinating the behavior of intelligent agents
    for secure decision-making in smart industries. This approach self-organizes the
    agents driven through big data for autonomous decision-making and provides strategies
    for deadlock prevention and intruder avoidance through proper negotiation mechanisms.
    Autonomous inventory management is one of the crucial operational tasks in smart
    industries. The authors in [305], deployed UAVs as autonomous navigating agents
    for automating inventory tasks by processing the big data collected by UAVs. It
    was also integrated with a blockchain architecture for ensuring security and transparency.
    The system is also capable of managing external audits using big data analytics.
    Since cloud systems and extensions of clouds such as edge and fog computing will
    be crucial for InX, and the security of most of the big data will be directly
    affected by the security of cloud platforms, in the following subsection, the
    security of cloud computing platforms in the context of InX is discussed. In the
    context of data spaces, decentralized data sharing, in which data is dispersed
    throughout a network of nodes, rather than kept in a single repository, is essential
    [306]. This decentralization lowers the possibility of a single point of failure
    while improving data security. Interoperability is given top priority in dataspace
    technology, enabling smooth communication across various data sources, formats,
    and protocols. It preserves data sovereignty, guaranteeing that businesses maintain
    authority over their data and abide by privacy laws. Robust security protocols,
    data governance, scalability, and real-time access are essential components that
    facilitate data-driven decision-making and streamline operations. E. Machine Learning
    Machine learning has been widely researched and used for improving the security
    of communication systems [123]. Due to the increasing volumes of data traffic,
    machine learning has been an important field of research specifically in terms
    of security, since human monitoring is rendered useless in traffic analysis. Since
    the learning systems are external to the systems of InX, such as CPS or IIoT systems,
    there are chances of security lapses and vulnerabilities without having the devices
    compromised. This is important in cases of strong isolated industrial domains
    that use machine learning. Therefore, the security of machine learning in the
    context of InX is even more important due to the critical nature of the infrastructure,
    as well as the dependence of many systems of InX on machine learning. Below, we
    discuss some of the most pertinent challenges and potential solutions. 1) Security
    Challenges Even though it is security-hardened, some of the properties of machine
    learning can induce basic vulnerabilities in the systems machine learning operates.
    Several security challenges of machine learning are described in [78], mainly
    concerning 5G. However, the threats can persist in InX. For example, one of the
    main threats that machine learning can induce in the systems is the denial of
    detection (DoD). The DoD can prevent machine learning from generating signals,
    for instance, from events, failures, and even cyber-attacks using adversarial
    examples [307] and data poisoning [308]. Another threat that machine learning
    can induce is leaking sensitive information from the company or factory. These
    attacks will be very critical in InX. The components of InX need to be constantly
    monitored and numerous signals for a vast number of functions and services will
    be created. The blocking of such signals, for instance with DoD, can have serious
    consequences in many stages such as processing and specific maintenance. Similarly,
    if a machine learning algorithm shares data with a malignant entity, the security
    of InX can be compromised. On top of such weaknesses within machine learning systems,
    the concepts of adversarial machine learning [309] that attempt to fool machine
    learning models are worrying. For example, the model poisoning attack shown in
    [310] for federated learning can have huge consequences in InX. By leveraging
    distributed learning, traditional fog computing is evolving toward edge intelligence.
    The security challenges of introducing deep learning in fog computing mainly include
    model fairness [311], adversarial robustness [307], [312] and privacy-preserving
    [146], [313]. Attacks on such edge intelligence frameworks refer to those that
    mislead the deep learning models using poisoned data (e.g., adversarial examples)
    and those that compromise the original inputs of pre-trained learning models using
    any publicly accessible information (e.g., gradients, open datasets, and development
    tools) that is not very privacy-sensitive. Meanwhile, with the rapid deployment
    and increment of deep learning-based intelligent infrastructures, users can have
    the possibility to join/access the edge intelligence as a service (EIaaS) platform
    and share their learning services. In such cases, attacks can happen in edge intelligence
    architecture by providing uncertified data and learning parameters. 2) Potential
    Security Solutions To deal with these challenges of machine learning in InX, trustworthy
    machine learning techniques have drawn much attention [314]. Different from environmental
    modelling [315], such as reinforcement learning, supervised learning, and unsupervised
    learning, trustworthy machine learning is investigated to improve AI’s privacy,
    security, and interpreter-ability. For InX, potential application scenarios of
    machine learning include industrial unmanned systems, industrial data analysis,
    quality detection, etc. Due to its importance in the critical infrastructures
    of InX or industrial society, adversarial threats on machine learning should be
    studied first to identify hidden attack surfaces. Nowadays, known threats including
    adversarial examples as highlighted in [316], such as data poisoning, backdoor,
    and membership attacks have been widely studied, and many defence strategies have
    been implemented. According to the types of adversarial threats, promising defences
    can be divided into four parts: 1) defending against adversarial examples; 2)
    defending against data construction, and 3) defending against backdoor attacks.
    Each part also contains several sub-branches. For adversarial examples, the most
    popular defence methods are adversarial training [317], and differential privacy
    [318]. However, adversarial training often needs more data samples and the added
    noises of differential privacy are harmful to model accuracy. To enable black-box
    defence against the adversarial example of industrial malware classifiers, authors
    in [319] designed a stateful query analysis method and a novel distance metric
    to improve the threat hunting rate. Besides, a conditional generative adversarial
    network is proposed in [307], which also can be used to identify the adversarial
    example of industrial vision applications in a black-box way without reducing
    model accuracy. For data poisoning attacks, there are three different defence
    parameters, including poisoned data detection [320], abnormal feature detection
    [321], and back door model parameter detection [322]. The challenges of preserving
    data privacy in machine learning to maintain company information or factory floor
    plans can be addressed with privacy-preserving federated learning approaches,
    such as discussed in [147]. SECTION VI. Security of Industrial Applications InX
    will have an enormous number of applications, mostly of critical nature dealing
    with critical infrastructure and information. Therefore, its security will be
    extremely important. In this section, the security of two main application areas
    in the realm of InX is discussed. A. Industrial Augmented Reality Industrial augmented
    reality (IAR) applications incorporate tele-presence systems in which a person
    can guide an operator, a person or a robot, remotely to reduce the need of physical
    movements in factory environments [129], [323]. IAR has been used in extremely
    sensitive operations to help operators in complex environments, for example, inspection
    in the aviation industry, as discussed in [324]. IAR also plays an important role
    in mirroring the physical world, such as the factory environment, in a digital
    one, which will be vital for InX [325]. Therefore, its security is also very sensitive
    and must be ensured. In a conventional AR architecture, an AR handheld mobile
    or head-mounted device is the main entity, which can be controlled by smartphones,
    tablets, or special AR glasses like Microsoft HoloLens. An AR application takes
    input data from the camera of the device, stores it, and/or sends it to a remote
    server. This data is then transformed into virtual objects, which renders the
    data and overlay output directly on the user’s perception in the real world [29],
    [326]. Since IAR systems require tactile interaction with users, the IAR system
    need to exchange and manage content as fast as possible and needs to manage a
    large amount of data. The communication between IAR devices is wireless and expected
    to enable dynamic on-demand information sharing, which requires a fast response
    from the remote servers [327]. Modern communication architecture/technologies,
    such as fog edge computing, and cloudlets, extend support to IAR applications.
    Edge computing helps meet the real-time requirements of AR and reduces the dependence
    on uninterrupted high-performance communication channels to the computing servers
    [85]. The advent of 5G brings high bandwidth and low latency to enable users to
    achieve high-fidelity telepresence systems and collaborative augmented reality
    applications [40]. Since IAR involves many 5G-based technologies and comprises
    IoT devices (head-mounted displays [328]), it will incorporate the security challenges
    of these technologies, as well as have its security challenges, as discussed below.
    1) Security Challenges The challenges of IAR are multi-dimensional, including
    those existing in IoT devices (IAR devices), those arising from the communication
    infrastructure (e.g., 5G), and those related to storage of the sensitive data.
    Many risks are associated with the input data, as data is coming from various
    sensors which are always on such as cameras, GPS data, temperature, accelerometer
    readings, and more. The confidentiality, integrity, and availability of this data
    need to be ensured because an attacker can distill sensitive data like passwords,
    and secret formulas, among other private matters from the visual information.
    Continuous sensing and video streaming may not be sensitive to the user but may
    be used by others, such as bystanders resulting in bystander privacy leakage [29],
    [329]. There are also risks involved with the output of AR, such as the capability
    to modify a user’s view of the environment. AR content may include static data
    that consists of non-sensitive data like product images, and tutorials, and sensitive
    data, such as computer-aided design (CAD) models which must be protected. A malicious
    or buggy application may potentially obscure the real-world information or occlude
    virtual content of other applications and may cause other attacks like clickjacking
    [29], [329]. One result of such a security attack can be to show the wrong speed
    limit instead of a real speed limit. Another case can be to cause a sensory overload
    of users by flashing bright lights on the display or delivering intense haptic
    feedback [29], [329]. As AR applications process and access data from various
    sensors, a big risk is involved in stealing the data or misusing that access.
    An attacker has a high interest in retrieving the processing/processed data, to
    try to manipulate the data to lead the machine operator to take wrong measures.
    Overall, this can cause process disruption or even technical and health damage,
    as discussed in [29], [85]. In IAR systems, a lot of collaboration is carried
    out using audio-video teleconferencing and computer-supported collaborative work
    (or CSCW). This enables the live sharing of information among multiple users,
    where interaction takes place in the same shared space physically or virtually,
    using shared space technologies. Using these shared spaces a component vendor
    can help a plant/machine operator to fix an error in a particular machine by embedding
    the instructions into the video stream without visiting the site/location [330].
    Various threats arise in such shared spaces/technologies that include spoofing,
    and unauthorized access from personal area networks (PANs), such as in ZigBee
    or blacktooth PANs [329]. 2) Potential Security Solutions The security assets
    of an IAR architecture need to have adequate mechanisms to protect the input data
    against eavesdropping, voice-spoofing, shoulder-surfing attacks, and manipulation
    [329]. Authorized and authenticated users should be able to access static and
    process data, and read access shall be possible [29]. Biometric authentication,
    such as voice recognition or facial recognition, provides attractive solutions
    for secure authentication and authorization. Khamis et al. [331], [332] proposed
    two multimodal schemes, called GazeTouchPass and GazeTouchPIN, that combine gaze
    and touch for shoulder-surfing resistant user authentication on mobile devices.
    These models require an attacker to simultaneously observe the device screen and
    the user’s eyes to find a password, for example. Looks Good To Me (LGTM) is an
    authentication protocol that uses a combination of facial recognition and wireless
    localization information to cross-authenticate users. In simple words, users can
    authenticate and initiate sharing using an AR head-mounted display (HMD) with
    a wireless connection [156]. HoloPair, however, avoids the use of wireless localization,
    which may be unavailable and inefficient in devices, and instead utilizes the
    exchange of visual cues between users to confirm the shared secret [154]. Lebeck
    et al. [333] has laid the foundation for the security of AR visual output and
    designed a prototype platform called Arya that implements the application output
    control based on the context-specific policies, and evaluated Arya on various
    simulated scenarios [329]. Ahn et al. [334] build upon Arya, a novel system for
    dynamic and complex environments to ensure integrity, availability, and confidentiality
    using reinforcement learning automatically [329]. Anonymization techniques, to
    obfuscate the location of users, can be used to secure location-based services
    in industrial contexts [335]. Biometrics is one of the ways of authenticating
    cloud computing architecture and has potential benefits. Benefits such as scalability,
    cost-effectiveness, reliability, hardware agnostic, and allowing ubiquitous access
    to private data and services. Biometric credentials have the advantage of not
    relying on the user’s memory [336]. Another approach is using the local computing
    and storage enabled by Edge computing. Edge computing helps meet the real-time
    requirements of AR and reduces the dependence on uninterrupted high-performance
    communication channels to the computing servers. One approach for such services,
    in which a sensing device gathers sensitive data in an environment, is moving
    the service or techniques that use that sensitive data into the environment generating
    the sensitive data as discussed in [110]. B. Blockchain Blockchain improves the
    transparency of the overall processes, and therefore generates trust by revealing
    the potential flaws and misbehavior in the operation of different components and
    stakeholders, by keeping track/record of each phase in a particular industrial
    application. Moreover, blockchain would allow a zero-trust management mechanism
    [337], [338] for the InX applications that will regularly ensure each operation
    is carried out in a trustworthy manner. Zero-trust is a security model that assumes
    any person or device attempting to access a network is already compromised, which
    must be verified before access is granted. Blockchain can be used as an enabler
    for zero-trust by, e.g., eliminating the need for a central trusted authority,
    ensuring that data cannot be altered and that all nodes on the network agree on
    the validity of transactions, and providing a transparent and auditable ledger
    that can be used to track user or device activities in a zero-trust systems. Therefore,
    blockchain can fulfill the InX requirements by providing decentralized secure,
    trusted, and optimized solutions [339]. Blockchain technology provides a zero-trust
    computing environment for industrial applications through a shared distributed
    ledger that possesses all the transactions and each of the involved participants
    can monitor these transactions. Thus blockchain further improves the security
    of the whole value chain by ensuring data integrity, transparency, and trust.
    However, the current blockchain systems still suffer from some security threats,
    i.e., at the network level, in the smart contracts/agreements, and during transactions.
    In the following part, we discuss security challenges in blockchain in the context
    of InX. 1) Security Challenges Generally, blockchain technology improves overall
    security and data breaches as it provides key features such as decentralization,
    distributed trust, immutability, and better data access control mechanisms. However,
    there are open challenges for data privacy, for example, because of the openness
    and transparency of transactions among various involved entities of the system.
    The work in [340] presents the need for careful assessment of the transparency
    and privacy of transactions through blockchain-based multi-hop tracking and tracing
    mechanisms. It also imposes a strong emphasis on information accountability, privacy
    in a dynamic environment, and real-world evaluation of blockchain frameworks for
    privacy preservation in industrial supply chains. The use of blockchain technology
    for communication networks raises numerous security and privacy concerns in various
    smart applications. For example, potential threats from network perspectives of
    blockchain may include eclipse attacks, DDoS attacks, Sybil attacks, time-jacking
    attacks, and transaction malleability attacks, among others [94], [341]. The eclipse
    attack in the blockchain network can occur when an adversary wants to take control
    of incoming and outgoing traffic by isolating the IP addresses of the other/legitimate
    nodes through a victim node [162]. Though the blockchain network works/follows
    similarly to the peer-to-peer network, it still suffers from DDoS attacks which
    make the desirable resources unavailable [342]. The Sybil attack allows the hostile
    peer to dominate the whole network by creating several fake identities [158],
    [343]. In a time-jacking attack, the adversary tries to interrupt the mining process
    by inserting inaccurate timestamps [344]. Transaction malleability threats can
    result in an inconsistent state of blockchain and open doors for further attacks
    [345]. One of the popular threats known for the blockchain is the ’51% attack’,
    where a miner node or a group of miner nodes take control over more than 50% of
    the hashing rate/computing power of the network, which results will prevent the
    other miners to mine a new computing block [346]. In this case, the double-spending
    attack is quite certain as the transaction/data can be altered easily and that
    may lead to further challenges in the verification of new transactions [347].
    In a selfish mining attack, a group of miners either want to increase revenue/reward
    by dominating the majority of the network or try to waste the resources for legitimate
    miners [348]. Furthermore, all the transactions in the blockchain systems are
    shared and traceable, which raises privacy risks as the adversaries can easily
    track the real identities of the involved entities [349]. Anonymity is required
    in the case when the sensitive data is shared over the network and any of such
    involved entities/stakeholders can track the traffic of the network. The consensus
    algorithm in the blockchain is dedicated to verifying/validating the authenticity
    of each transaction, but it is still possible to target the authenticity of the
    transactions. The transaction authenticity in the blockchain is highly dependent
    on the cryptographic operations, i.e., each new transaction is connected with
    the previous one using digital signatures/cryptographic schemes [350]. The attacker
    can perform double-spending by delaying or denying the delivery message of the
    new transaction. Blockchain technology also faces several obstacles due to the
    vulnerabilities in smart contracts. For example, there are about 12 different
    kinds of vulnerabilities in the smart contract identified in [351]. Some of the
    most common attacks include re-entrance vulnerability, coding errors, and timestamp
    dependence [263]. These types of threats are likely to occur both in the Ethereum
    Virtual Machine (EVM) and Solidity (programming language). 2) Potential Security
    Solutions There are different solutions for addressing different types of security
    challenges in blockchain. For example, in addressing the network-related threats
    of blockchain, specific approaches are proposed in the scientific literature.
    The challenge of eclipse attack can be countered by proposing an anomaly detection
    system (ADS), and by introducing randomness [162], [160], [161]. Distributed IDS
    mechanisms, game-theory approaches, and proof of activity protocols can be considered
    to address DDoS challenges in the blockchain [352]. Sybil attacks can be resolved
    by developing secure consensus mechanisms [157], and by distributed behavior monitoring
    of miner nodes [158]. To overcome the time-jacking threats, synchronized clocking
    techniques must be placed during the blockchain transactions [353]. Transaction
    malleability attacks can be eliminated using the provenance-based scheme, i.e.,
    provide an extra layer of the provenance [354]. Threats, such as ’51% attack’,
    double-spending, and selfish mining are not very straightforward to launch because
    they require higher computing power. The ’51% related attacks’ can be countered
    by two-phase proof-of-work” (2P-PoW) [355], Random mining group selection approach
    [356], and Proof of Activity protocol [357], [358]. The potential countermeasures
    to the double-spending attacks can be the non-interactive zero-knowledge (NIZK)
    proof, increasing confirmation, and deep inspection/listening/observing [359].
    Several approaches such as the “truth state” strategy [360], the Freshness Preferred
    (FP) strategy [361], and ZeroBlock [362] scheme can be practiced to avoid any
    of such selfish mining threats, [363]. To ensure privacy protection in the blockchain
    systems, some of the potential solutions such as homomorphic encryption technology
    and zero-knowledge proof can be adopted [159]. Furthermore, the concept of off-chains
    (which was originally proposed to improve the scalability of the blockchain systems)
    can play a key role in the confidentiality of the information. Blockchain-based
    decentralized data integrity, security, and trust schemes for Industry 4.0 have
    been proposed in [180]. The proposed framework, called BlockEdge (integration
    of the blockchain and edge computing), provides the necessary levels of security
    within the resource constraints and latency limitations. Also, the research work
    in [263] identified potential security challenges and solutions for blockchain-edge
    integrated communication networks. Various solutions addressing the smart contract-related
    vulnerabilities are presented in [364], [365]. Moreover, authors in [366] classified
    the smart contract attacks into four categories (i.e., malicious acts, weak protocol,
    defraud, and application bugs), and also presented the attack techniques as well
    as the relevant security approaches. SECTION VII. Risk Management and Security
    Standardization InX requires proper risk management to assess the security of
    the overall ecosystem and related consequences. The security of the InX ecosystem
    also needs agreements between different stakeholders to maintain the best security
    policies and approaches. Risk management and standardization play crucial roles
    in this regard. There are also challenges, such as fragmentation in standardization
    related to IIoT [367], which need to be solved through proper security policies
    on the organizational level if standardization fails or introduces delays in applying
    the best practices. Evaluations from other than standardization bodies can also
    be followed. For example, security recommendations for threats and vulnerabilities
    in ICSs, including automation, process control, and I&C systems, are published
    regularly by the German Federal Office for information security [368]. These include
    the latest top threats, countermeasures, or solutions for those threats and the
    existing gaps. Similarly, the National Institute of Standards and Technology (NIST)
    [369] provides a framework for improving the security of critical infrastructure
    [370]. The framework applies to ICSs, CPS, and the IoT, which deploys a risk-based
    approach for managing cyber security risks. Such recommendations must be followed
    besides the specific efforts from standardization bodies. Below we discuss risk
    management and standardization efforts in this regard. A. Risk Management The
    diverse technological issues of InX emphasize the heterogeneous and dynamic nature
    of contemporary cybersecurity. Cybernetics, as a discipline of control and communication
    structures in technical and social systems, helps in approaching cybersecurity
    risk management. Accordingly, when managers organize factories or supply chains,
    they face increasingly complex situations and problems of how to make optimal
    decisions [371], [372], [373]. The diverse approaches in cybersecurity risk management
    include incident response and proactive approaches to preventing and preparing.
    However, whatever organizations’ actions in terms of technical progress, contribute
    to the growth of complexity, making any future response more demanding and urgent.
    To keep up with the development of possibilities, resources, technologies, etc.,
    we can talk of an arms race [374]. In the specific context of industrial environments,
    cybersecurity also has an impact on system safety. It is a relatively recent observation
    that the two aspects, though traditionally treated separately, are interdependent
    and must be considered jointly [17]. An additional implication for cybersecurity
    risk management is that any technical system is only temporarily secure and that
    cybersecurity should be seen as a continuous activity [374]. The way forward will
    be about building resilience in production systems and supply chains. This includes
    considering resilience already in the design phase of new structures, developing
    effective metrics that can help evaluate vulnerability and resilience, and simulating
    complex industrial systems to understand vulnerability issues better [375], [376].
    An important aspect will be to automate safety and security risk assessment and
    extend it from design and engineering time to the regular operation of production
    systems [377]. There should be clear organizational policies regarding security
    policies, methods for implementing those policies, and training of the staff to
    work securely and maintain the security of the systems and components of InX.
    Insufficient policies and lack of knowledge of the staff result not only in direct
    security threats but also in the propagation of security threats through unintended
    facilitation for subsequent attacks. This underscores the need for zero-trust-type
    system approaches. Lack of sufficient security knowledge of the staff can impede
    the detection of threats, recovery from threats, and sanitizing processes. One
    of the most prominent security policies concerns the use of external applications,
    as discussed in the recommendation by the Federal Office for Information Security
    of Germany [378]. Proper monitoring for external applications and internal applications
    with write capabilities must be ensured. Such applications operating in insecure
    environments, for instance, can induce security vulnerabilities. Policies for
    lost devices, passwords, the use of personal/private devices, trust establishment
    techniques, as well as methods for stopping insider attacks, must be devised at
    the organizational level. B. Standardization Efforts Security of industrial systems
    has been the focus of several standardization organizations that are either positioned
    at generic information technology - computer science level or are domain specific,
    as summarized in Table 4. NIST has produced a series of information security guidelines
    and standards, where the flagship document is a collection of special publications
    on managing information security risks [379]. These publications present the basic
    principles at an organizational level for assessing, responding to, and monitoring
    risk. IEC has published the IEC 62443 series of standards on the security of industrial
    networks and communication systems [380]. The IEC approach focuses on the prevention
    and management of security risks. These standards introduce some fundamental concepts
    like process maturity levels, security levels for systems, defense in depth, and
    the division of the system into zones and conduits. The standard offers architecture
    reference models, system partition models, as well as relationships among models
    for security management. Also, IEC 62443 recommends requirements for security
    such as access control, data confidentiality, limited data flow, resource availability,
    identity identification, and authorization, among others. These security requirements
    enable three different security levels, target security levels (SL-T), achieved
    security levels (SL-A), and capability security levels (SL-C). TABLE 4 Relevant
    Standardization Bodies and Their Activities The ISO/IEC 15408 is a three-part
    standard [381] that defines a set of requirements for designing security functions,
    as well as for security assurance and evaluation. ISO/IEC has also produced the
    27000 series of standards (27001,2,3,4,5) [382] on information technology security
    techniques with a broad scope covering technical cybersecurity, as well as privacy
    and confidentiality topics. Apart from generic standards, domain-specific standards
    provide more detailed and focused guidance. As an example in the heavily regulated
    nuclear energy domain, IAEA has published a technical guidance reference manual
    within the nuclear security series [383]. Similarly, IEC 62645 [384] presents
    nuclear Instrumentation and Control cybersecurity requirements and IEC 63096 [385]
    includes security controls that are applicable in the nuclear domain. There are
    other standardization efforts related to individual technologies that are used
    in InX. For example, the 3GPP has set requirements for 5G systems used in industrial
    environments, such as service-level specifications (SLCs) for 5G technology-enabled
    connected industries, and enablers for industrial automation. Similarly, the 3GPP
    has also set technical specification groups (TSGs) to develop new standards for
    relevant technologies such as URLLC, and non-public networks. The TSG-SA working
    group (WG) 2 is responsible for specifications related to industries. The 5G Alliance
    for Connected Industries and Automation (ACIA) is meant to ensure the best possible
    applicability of 5G technology and networks for connected industries. Similarly,
    the 3GPP has also standardization activities related to CPS, IIoT, and machine
    learning, mainly to protect industrial data and systems from manipulation and
    security threats during communication. The specific output of the standardization
    organizations with a short description is presented in Table 4. Below, we discuss
    security features of the most important communication standards developed for
    industrial systems. SECTION VIII. Lessons Learned This study sheds light on the
    revolutionary possibilities of new technologies in the development of InX. These
    technologies serve as the innovation accelerators for InX. The overall InX ecosystem
    is divided into three main parts, i.e., i) secure communications, ii) secure factory
    environment, and iii) industrial applications. The enabling technological components
    of each part are first introduced, followed by a detailed analysis of the security
    landscape of each technology. It is important to mention that the technological
    landscape in InX is huge, which cannot be covered in a single article. Therefore,
    the focus has been laid on selected crucial technologies that heavily rely on
    communications networks and technologies. Overall, the study emphasizes how crucial
    it is to investigate certain use cases to enforce security features related to
    these technologies in InX. Each of these enabling technologies brings with it
    specific security issues that need customized solutions. Examples of crucial factors
    to take into account are safeguarding IIoT and CPS devices against cyberattacks,
    organizing the enormous volumes of data in big data applications leveraging clouds,
    and maintaining the integrity of information through blockchain transactions.
    These lessons highlight the importance of having a thorough security plan that
    takes into account the unique characteristics of each developing technology and
    uses it to propel advancement toward InX. Below, we provide a summary of lessons
    learned in secure communications, secure factory environments, and industrial
    applications. A. Security of Communications Since communications technologies
    make the backbone of InX, the security of communications technologies, such as
    5G, and technologies used for communications-related computations including cloud
    and edge computing are highly important. For example, 5G wireless networks can
    expose industrial systems through the air interface to external threats. Similarly,
    important industrial information stored in cloud servers can expose sensitive
    critical information to third-party vendors, result in information leakage, or
    create bottlenecks and deadlocks during run-time due to congestion or DoS attacks.
    UAV-based communications are considered extremely important in the realm of InX.
    However, due to limited computation capabilities, UAV-based communications will
    bring unique security challenges. Since industrial communications technologies
    were focused mainly during the fourth industrial revolution, various standard
    technologies will evolve for their use in InX. However, the security of those
    standards, such as MQTT, AMQP, and CoAP, to name a few, must also be improved
    to meet the needs of InX, where these technologies will be integrated with novel
    communications technologies, such as 5G, 5G advanced, and eventually 6G. Moreover,
    this study is limited to selected communications technologies, whereas the range
    of communication techniques can be huge in InX. In principle, however, security
    challenges must be first addressed independently within each technology and then
    within the integrated InX ecosystem. B. Security of Factory Environment The security
    of the factory environment is the most crucial and complicated one due to the
    amalgamation of a huge number of devices and technologies. Most technologies relying
    on IIoT and CPS, for instance, will also be vulnerable to security threats due
    to the inherent weaknesses of these technologies. For example, fingerprinting
    the firmware weaknesses in IIoT and CPS systems is an extremely daunting task
    on the one hand, and deploying strong security techniques is infeasible due to
    resource (e.g., computation) limitations, on the other hand. Similarly, collaborative
    robots will require extremely fast communication links, and a delay due to a security
    lapse, such as a man-in-the-middle attack, can cause huge damage. Therefore, specific
    research efforts are needed to strengthen the security of these technologies in
    InX. The study provides important insights into the best practices for overseeing
    and controlling the enormous amounts of data in InX that are foundational for
    big data analytics and machine learning. A well-defined classification is essential
    to efficient data management since it keeps confidential data safe and well-organized.
    Further, a key component of data protection is the standardization of security
    mechanisms like encryption and access controls. A crucial factor to take into
    account is alignment with changing data privacy laws and compliance standards,
    which calls for constant watchfulness to make sure that data activities continue
    to comply with the law. The study highlights the importance of keeping abreast
    of emerging privacy regulations and cultivating an organizational culture of data
    responsibility. This will help InX navigate the complex data governance landscape
    while maintaining security and compliance standards. However, the study is limited
    to few enabling technologies which rely heavily on communications networks and
    technologies. C. Security of Industrial Applications The study provides important
    insights into the security of industrial applications. For example, blockchain
    technology can improve data-sharing security in InX. The decentralized and tamper-proof
    nature of blockchain technology promises to improve security by guaranteeing data
    integrity and lowering the possibility of unwanted changes. Auditability, being
    a crucial component of regulatory compliance inside InX, could be improved by
    the transparency and traceability of blockchain technology. However, the review
    also identifies important challenges. Processing requirements blockchains demand
    reliable infrastructure and energy-related concerns. To further optimize interoperability,
    security, governance, and consensus processes need to be in line with particular
    InX specifications. These lessons highlight the need for a well-bound strategy
    that highlights the security benefits of blockchain technology as well as the
    complex issues associated with its successful integration into InX’s decentralized
    data-sharing system. Since the list of industrial applications can be extremely
    large, we have focused on two application scenarios as examples. D. Overall Security
    Posture of InX The thorough analysis in this review highlights the complex environment
    of InX, where the intersection of different technologies is crucial. As a result
    of the integration of cutting-edge technologies, InX depends on strong security
    policies, procedures, and techniques to safeguard and manage vital infrastructure
    and operations. The dynamic nature of threats demands that different security
    vulnerabilities must be addressed from the overall InX ecosystem perspective.
    InX can effectively manage the intricate cybersecurity problems it faces by promoting
    collaboration between different enabling technologies, guaranteeing the resilience
    of key infrastructures, and improving incident response capabilities. This synthesis
    highlights how crucial it is to take preventative measures to safeguard the transformational
    potential of InX and support its ongoing development in a society that is becoming
    more digitally linked and interconnected. The study reveals that various security
    challenges are common to most enabling technologies of InX, irrespective of whether
    the technology belongs to communications, factory environments, or applications.
    For example, DoS attacks can happen on most centralized control entities in 5G,
    IIoT, and application servers in centralized clouds. Furthermore, a huge number
    of different kinds of IoT, CPS, and UAVs have been proposed and used for monitoring
    the conditions of systems in InX. Those systems rely on the communication infrastructure
    and monitoring tools that use the sensed data/information for further actions.
    Therefore, besides the inherent security challenges of each technology, such as
    IoT and CPS, the security challenges related to the communication infrastructure,
    data analytics, and machine learning, for instance, will have strong implications
    on the security of each technology using them. Therefore, it is important to investigate
    the security of each technology individually, as well as the whole end-to-end
    InX ecosystem in unison to ensure a secure ecosystem. Furthermore, non-conventional
    security approaches, appearing in the form of edge and fog computing to limit
    the computation of sensitive processes to local environments, must also be considered
    for improving the security of the whole InX ecosystem. Limiting the information
    flow to local industrial environments will surely increase the privacy of information
    compared to information flow over the Internet. Moreover, different attack models
    from different technologies can be used together to compromise the security of
    the integrated system. Overcoming such challenges will require strong defense
    techniques also working in unison to counter the combined attack force. Such a
    secure combination of different technologies will require further research from
    different aspects, as described in the following section. SECTION IX. Future Research
    Directions InX will be a shared and connected ecosystem driven by communication
    networks and technologies, mainly 5G and beyond (6G) wireless communication networks.
    In such a shared ecosystem, several relevant enabling technologies are required
    to have intelligent collaboration among each other to fulfill the dynamic needs
    of the InX applications. On the one hand, such integration of various key technologies
    may provide the needed flexibility and opportunities to build the desired network
    architecture and infrastructure for the applications of InX. On the other hand,
    the overall network will be highly complex which can lead to several challenges.
    One of the major challenges for such future networks will be to ensure the required
    degree of security and privacy and enable intelligent security services and trust
    among various involved entities/actors. Hence, this section is dedicated to putting
    some light on the potential future research directions in terms of securing various
    enabling technologies for communication in InX. A. Pervasive AI The evolution
    of telecom infrastructures towards 6G will include highly distributed AI, moving
    the intelligence from the central cloud closer to end nodes in the form of edge
    computing [386]. Distributed AI, aided by distributed edge and fog nodes and omnipresent
    radio technologies connecting those nodes, will complement the industrial process
    ahead of what has been envisioned by InX in many aspects. In addition to existing
    MEC-based solutions, where edge computing is managed at highly-capable server
    nodes integrated into the access network architecture, edge computing is envisioned
    to be extended towards local edge computing, where local nodes provide the needed
    computational capacity with collaborative effort [110]. The resulting three-tier
    computational architecture improves, e.g., resource efficiency by enabling the
    reduction of sensor data through local data analysis, reliability by ensuring
    the operation of critical services during network problems, and privacy by making
    it possible to process private and business-confidential data locally. The complexity
    of the resulting architecture, however, requires an increasing level of distributed
    intelligence at all levels to guarantee efficient, safe, secure, robust, and resilient
    services [386]. The majority of mission-critical and privacy-concerned applications
    of InX demand online distributed learning and training algorithms that can be
    employed at the edge devices [386]. Federated learning (FL) [387] is a promising
    paradigm for privacy-preserving distributed data training, enabling original datasets
    to be kept local while only the edge AI model parameters are shared [386]. Furthermore,
    DRL has shown good performance in various complicated EC scenarios [388]. Combining
    these two is an interesting research direction for InX. The combination of FL
    and DRL has already been studied by Shan et al. [389], where the FL framework
    was integrated with the mobile edge system to train DRL agents in a distributed
    way. From the viewpoint of security in InX, studying novel secure routing schemes
    and trust network topologies for edge intelligence service delivery while considering
    the coexistence of trusted edge nodes with malicious ones [386], would be an interesting
    research direction as well. B. Data spaces The concept of data space has gained
    prominence with European initiatives to develop a reference architecture for secure
    data exchange and data sovereignty. In the frame of InX, we can expect greater
    decentralization and higher complexity. Thus, for future operations, management
    and intelligent decision-making more interfaces need to be integrated. This integration
    should happen based on data space concepts as represented by the Industrial Data
    Space and Gaia-X [390]. With a corresponding reference architecture for secure
    data exchange and trustworthy data sharing, IDS and Gaia-X contribute to the digitization
    of industry and its further evolution. One goal is to accommodate the decentralization
    of industrial architectures, as is the case in supply chains, for example, and
    to bridge the limitations of top-down approaches, both in technological terms
    and concerning the needs of industry, politics, and standardization [391]. Through
    the architecture, different cloud platforms can be connected without losing or
    compromising secure data exchange or control over the data. The mechanisms of
    the architecture place the principle of data sovereignty at the center. Arguably
    the most important component is the connector, which links enterprise architectures
    or even individual, networked devices to data space, and ensures the identity
    and integrity of the connected software systems and components [392]. The result
    is a federated system characterized by trustworthiness, transparency, and interoperability,
    relying on existing and evolving standards [393]. Decentralized data sharing promises
    to enhance data fluidity by facilitating smooth data sharing and cooperation between
    various InX components. System reliability is improved by the inherent resilience
    of decentralized networks, which reduces the possibility of single points of failure.
    The evolution towards InX will depend on the ability of the industry to exploit
    data and become part of the data economy. Therefore, it will be crucial to understand
    the impact of the data space concept on industrial operations and future business
    models and to create the data spaces needed for industrial development [394].
    C. Augmented Reality So far, the focus has been to deliver technologies that make
    IAR applications a possibility to support various industrial processes. These
    applications require different mobile devices including smartphones, tablets,
    PCs, Google glasses, or Microsoft HoleLens. These devices require different types
    of security systems and procedures. Among the most pressing challenges that need
    further research is the security and privacy of transmitted video and audio between
    a remote location and InX facilities. Furthermore, multi-modal authentication
    on IAR devices [332] is needed. Protecting collaborative interactions among parties
    providing live remote support and local operators needs further investigation.
    Since the domain of IAR is still not widely adopted, there is a high possibility
    that new security challenges will arise with the wide adoption of the technology.
    Hence, more research on proactively investigating the potential exposure from
    IAR is needed. The principle of security-by-design must be adopted in designing
    new IAR applications, services, and devices due to the extremely serious nature
    of the involved resources. D. Advanced Robotics As robot systems rise in importance
    for both industry and consumers, also the risk of security threats exploiting
    vulnerabilities from either hardware or software. Security by design is an approach
    that requires the consideration of security requirements for robotics applications
    starting in early development phases and the whole life cycle [395], thus increasing
    trustworthiness. In [396], the authors state how the monitoring and tracking of
    privileged accounts can help to estimate and mitigate the impact of a security
    breach. Anomaly detection and robot behavior fingerprinting are promising research
    directions that will help with controlling data usage and robotic systems identification.
    Finally, improving authentication, authorization, and encryption in robotics frameworks
    is a must, and ROS has the upper hand in this aspect. With the advent of ROS2
    (a merging of DDS and ROS), research toward the next security phases is possible.
    E. Visible Light Communications Extremely high data rates with extremely low latency
    can be provided by Visible Light Communications (VLC) technologies [397]. Factory
    floors lit by VLC, providing super-fast connectivity, will extend sustainable
    communications to actuators and robotic arms, mainly because the existing challenges
    of VLC such as distance and shadow effects will not exist on factory floors. Therefore,
    VLC makes one of the best high-data rate dual-function data delivery technology.
    However, more research is needed on the integration aspects of VLC into equipment
    that may not look suitable for VLC, for instance, due to its fragile nature. The
    security aspects of VLC in InX are more from the physical layer perspective, as
    discussed in [398], due to the nature of the technology needing line-of-sight,
    and use cases of InX related to indoor environments and components. F. Data Sovereignty
    Data sovereignty [399], i.e., self-authority on the control of data including
    its use and dissemination, is very important. Data sovereignty enables managing
    information in a way that is consistent with the laws, practices, and customs
    of the state where the data is located [399]. There are various approaches to
    ensuring data sovereignty including technical and legislative methods. Among the
    latter, various organizations have been formed such as the International Data
    Spaces (IDSs) [400], which has also developed a reference architecture that ensures
    data sovereignty besides the security and privacy of data. The IDS also enables
    the sharing of data in a contract-binding and safe methodology among the corporate
    sectors while storing the data in virtual spaces [401]. G. Automation of Everything
    for Security Industrial automation from the connectivity perspective is a high
    research topic, as discussed in [244]. The automation of networked systems in
    InX will be inevitable. Machine execution of complex functions or in other words,
    automation is used for i) information acquisition, ii) information analysis, iii)
    decision and action selection, and iv) action implementation for accuracy and
    reliability [402]. The complexity in communication networks due to heterogeneity
    in networks, devices, applications, and services along with its criticality in
    InX forces us to automate network operations [403], [404]. Network management
    becomes complicated as the network grows, and security policy enforcement with
    adjusting increasing numbers of parameters further complicates the whole management.
    Since, human-machine interaction has been a major reason for the network downtime
    [405] with security lapses as a consequence [209], [406], due to manual configuration
    of network security technologies [407], [408], automation of security of InX becomes
    an eminent research area. One interesting aspect related to automation that needs
    multi-disciplinary research is the right balance between human and machine control,
    as discussed in [79]. H. Software-Defined Machines Software-defined machines (SDMs)
    bring new opportunities to InX, may that be manufacturing, assembly lines, or
    simply factory floor mobility. The basic concept behind SDM is that machines can
    be configured at run-time for different functionalities by externalizing the control
    and processing functions [409]. Such externalizing would require efficient communications
    technologies with robust security in place. Since 6G aims to provide ubiquitous
    connectivity, securing SDMs will be extremely important. To understand the importance
    of the security of SDMs, consider the case of successful rogue attempts that can
    enable, for instance, robotic arms to cause damage on the factory floor. Therefore,
    the security of SDMs in InX in the era of 6G makes an interesting research area.
    SECTION X. Conclusion In this article, we highlighted the security landscape of
    communications in InX. The main security challenges that can arise from using
    the most enabling technologies of InX are elaborated followed by potential solutions.
    Since InX will use novel technologies that will share, send or receive information
    over communication networks, the security challenges that exist in communications
    networks will have serious consequences on the security of those technologies,
    and as a result on InX. For example, CPS, IoT, and machine learning, to name a
    few, will need to send or receive data. Hence, the security of the communication
    media or network and computational architecture will have direct implications
    on the working of CPS, IoT, and machine learning algorithms. Since this area has
    not been previously explored from the communications security perspective, it
    is highly important to shed light on security concerns, possible solutions, and
    existing gaps to stir further research in this direction. This article also provides
    important insights into future research directions in the domain of InX, to motivate
    research beyond the current state-of-the-art into the 6G era for InX. Authors
    Figures References Keywords Metrics More Like This A Survey of Network Automation
    for Industrial Internet-of-Things Toward Industry 5.0 IEEE Transactions on Industrial
    Informatics Published: 2023 End-to-End Transmission Control for Cross-Regional
    Industrial Internet of Things in Industry 5.0 IEEE Transactions on Industrial
    Informatics Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Open Journal of the Communications Society
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Communications Security in Industry X: A Survey'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Chen J.
  - Pu C.
  - Wang P.
  - Huang X.
  - Liu Y.
  citation_count: '0'
  description: Edge–edge collaboration is the main collaboration mode for edge computing
    (EC) in time-sensitive networking (TSN). This distributed collaboration mode,
    in which the computing work is implemented collaboratively by the edge computing
    nodes (ECNs) without the involvement of cloud servers, poses challenges to collaboration
    management. However, most existing methods of ECN collaboration only focus on
    the scheduling of computing resources rather than management process design and
    rarely consider the transparency, security, and fair distribution guarantee mechanism
    of collaboration benefits. To address this issue, we propose a blockchain-based
    and decentralized management scheme for ECNs autonomous collaboration in TSN.
    First, we design an edge–edge collaboration management framework and its workflow
    based on blockchain for EC in TSN (TECChain), which combines blockchain with TSN
    technology and EC paradigm. To solve the consensus problem in TECChain, the proof
    of diligence (PoD) and delegated PoD (DPoD) consensus mechanisms are designed.
    Furthermore, by leveraging the high-precision synchronous clock foundation of
    TSN, we propose two time-slot-driven consensus algorithms, named sequential decision-making
    based on DPoD (S-DPoD) and Byzantine fault tolerance based on DPoD (BFT-DPoD)
    respectively. Theoretical analysis results show that the proposed algorithms perform
    better in terms of security, transactions per second (TPS), energy saving, etc.,
    compared to other algorithms. Comparison experiments reveal that S-DPoD consumes
    fewer interactive messages and has higher Byzantine fault tolerance and success
    rate of consensus than BFT-DPoD, but BFT-DPoD has lower transaction verification
    latency and larger TPS.
  doi: 10.1016/j.jksuci.2023.101902
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related works 3. The proposed scheme
    4. Experiment 5. Conclusion Declaration of competing interest Acknowledgments
    References Show full outline Figures (17) Show 11 more figures Tables (3) Table
    1 Table 2 Table 3 Journal of King Saud University - Computer and Information Sciences
    Volume 36, Issue 1, January 2024, 101902 Full length article A blockchain-based
    scheme for edge–edge collaboration management in time-sensitive networking Author
    links open overlay panel Junhua Chen a b, Chenggen Pu b, Ping Wang b, Xueda Huang
    b, Yanfei Liu c Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.jksuci.2023.101902
    Get rights and content Under a Creative Commons license open access Abstract Edge–edge
    collaboration is the main collaboration mode for edge computing (EC) in time-sensitive
    networking (TSN). This distributed collaboration mode, in which the computing
    work is implemented collaboratively by the edge computing nodes (ECNs) without
    the involvement of cloud servers, poses challenges to collaboration management.
    However, most existing methods of ECN collaboration only focus on the scheduling
    of computing resources rather than management process design and rarely consider
    the transparency, security, and fair distribution guarantee mechanism of collaboration
    benefits. To address this issue, we propose a blockchain-based and decentralized
    management scheme for ECNs autonomous collaboration in TSN. First, we design an
    edge–edge collaboration management framework and its workflow based on blockchain
    for EC in TSN (TECChain), which combines blockchain with TSN technology and EC
    paradigm. To solve the consensus problem in TECChain, the proof of diligence (PoD)
    and delegated PoD (DPoD) consensus mechanisms are designed. Furthermore, by leveraging
    the high-precision synchronous clock foundation of TSN, we propose two time-slot-driven
    consensus algorithms, named sequential decision-making based on DPoD (S-DPoD)
    and Byzantine fault tolerance based on DPoD (BFT-DPoD) respectively. Theoretical
    analysis results show that the proposed algorithms perform better in terms of
    security, transactions per second (TPS), energy saving, etc., compared to other
    algorithms. Comparison experiments reveal that S-DPoD consumes fewer interactive
    messages and has higher Byzantine fault tolerance and success rate of consensus
    than BFT-DPoD, but BFT-DPoD has lower transaction verification latency and larger
    TPS. Previous article in issue Next article in issue Keywords BlockchainCollaboration
    managementEdge computingTime-sensitive networking 1. Introduction The rapid development
    of technologies such as artificial intelligence, cyber–physical system, and industrial
    Internet of Things has brought about the Industry 4.0 era (Lo Bello and Steiner,
    2019; Wollschlaeger et al., 2017; Aceto et al., 2019). The basis of the Industry
    4.0 revolution is the deep integration of Information Technology (IT) and Operational
    Technology (OT). However, it was found that the integration has to confront a
    problem of mixed transmission of periodic data and real-time data. To address
    this challenge, the IEEE proposed a new deterministic communication protocol standard,
    the IEEE802.1 time-sensitive network (TSN) (Finn, 2022, IEEE, 2017), which can
    meet the requirements of mixed transmission of time-sensitive streams and non-time-sensitive
    streams in IT and OT integrated systems. Meanwhile, the traditional centralized
    cloud-computing model could no longer meet the requirements of real-time computing
    in the Industry 4.0 field. A new computing paradigm called edge computing (EC)
    was proposed, which deploys edge computing node (ECN) or edge computing server
    to the edge network and migrates the computing power from the remote cloud server
    to the area near the data source (Cao et al., 2021, Shi et al., 2016, Donno et
    al., 2019, Xia et al., 2021). It reduces the computational data transmission distance
    and time delay to improve the system response speed. TSN can reduce data transmission
    delay and jitter in the network, and EC can improve the data processing and response
    speed of the computing systems. Therefore, the combination of TSN and EC technology
    can improve industrial systems’ response speed and overall reliability, which
    has important research significance and application prospects. In the classic
    EC model, the computing architecture is organized in a cloud–edge-end hierarchy
    (Kai et al., 2021, Qiu et al., 2021), with computing resources flowing among the
    three layers. In the typical application of edge computing, the ECNs are responsible
    for data collection and urgent task computation; the cloud server obtains materials
    from ECNs and provides computing resources to ECNs for complex tasks, such as
    big data processing. ECNs and the cloud server work together, called cloud–edge
    collaboration, to complete heavy computing works. However, in the cloud–edge collaboration
    mode, the data between the cloud servers and the ECNs would flow across many heterogeneous
    networks with different characteristics, leading to response delay. In TSN, the
    timeliness of computing is an important indicator for the whole network system;
    therefore, traditional cloud–edge collaboration is not applicable in TSNs. The
    new edge–edge collaboration mechanism (i.e., ECN with heavy workload cooperating
    with other ECNs that provide computing resources (data and CPU)) is being adopted
    in more and more edge computing application cases, and some literature has proposed
    many collaboration approaches for edge computing. However, most of these approaches
    only focus on the scheduling and management of computing resources in the collaboration
    process, and rarely consider the openness, security, and guarantee mechanism for
    collaboration benefit distribution. Blockchain (Huo et al., 2022), which is a
    distributed ledger management technology that features the advantages of decentralization,
    high security, transparency, immutability, and traceability, can provide fundamental
    technical support for ECNs autonomous collaboration management. To this end, we
    propose a blockchain for EC in TSN (TECChain), which combines blockchain technology
    with TSN and the EC paradigm and designs an edge–edge collaboration management
    scheme based on TECChain. By using this scheme, all ECNs in TSN can participate
    the edge–edge collaboration management in a distributed way, and make the management
    process safe, transparent, traceable, and fair. Furthermore, the existing mainstream
    blockchain consensus algorithms have their shortcomings. Proof of work (PoW) (Kumar
    et al., 2019), the consensus algorithm initially adopted by Bitcoin, consumes
    a large amount of energy with computing power competitions for blocks generating
    right. Proof of stake (PoS) (Nguyen et al., 2019, Deirmentzoglou et al., 2019)
    avoids the energy waste by pledging tokens, but it may lead to the rich getting
    richer and further centralization. Neither of these consensus algorithms is suitable
    for edge–edge collaboration management on the TECChain in TSN. Given this, considering
    the characteristics of the synchronous clock mechanism of TSN, we design the proof
    of diligence (PoD) and delegated PoD (DPoD) consensus mechanisms especially suitable
    for TSN, which regard the diligence degree of TECChain nodes providing clock synchronization
    service in TSN as the stake for leading the consensus process. Then, utilizing
    the high-precision synchronous clock foundation of TSN, two different time-slot-driven
    consensus algorithms, named sequential decision-making based on DPoD (S-DPoD)
    and Byzantine fault tolerance based on DPoD (BFT-DPoD) respectively, are proposed
    to efficiently solve the consensus conflict problem of the delegated group in
    the consensus process. The main contributions of this work are summarized as follows:
    (a) To make the edge–edge collaboration management process safe, transparent,
    traceable, and fair in TSN, we propose a blockchain named TECChain, which combines
    blockchain with EC paradigm and TSN technology as the foundation for edge–edge
    collaboration management, and design the management framework and unified workflow
    based on TECChain. (b) Considering the shortcomings of the existing blockchain
    consensus methods, we innovatively propose the PoD and DPoD consensus mechanisms
    based on the strategy of determining the right in the consensus process according
    to the diligence degree of each node providing clock synchronization service in
    TSN. (c) Utilizing the high-precision synchronous clock foundation of TSN, the
    time-slot-driven algorithms, i.e., S-DPoD and BFT-DPoD, are proposed to solve
    the consensus conflict problem in the DPoD consensus process. The rest of this
    paper is organized as follows. Section 2 introduces the related work. Section
    3 presents the proposed scheme, including the edge–edge collaboration management
    framework and workflow, consensus mechanisms, and consensus algorithms. The experiment
    results and analyses are given in Section 4, and the paper is concluded in Section
    5. 2. Related works Motivated by current collaboration management and blockchain
    application methods in EC, in this paper, we propose the TECChain blockchain including
    its consensus mechanism, and construct a blockchain-based edge–edge collaboration
    management framework for TSN. Consequently, in this section, we briefly review
    the literature related to collaboration management and blockchain applied in EC.
    2.1. Collaboration management for edge computing Several studies have demonstrated
    the importance of collaboration for EC and have proposed various approaches to
    address the challenges associated with managing collaboration in EC environments.
    Liu et al. (2020) introduced a cooperative UAV-enabled edge computing (UEC) network
    and designed a collaborative computation offloading scheme for unmanned aerial
    vehicles with limited computation capabilities. Tun et al. (2022) proposed a framework
    for efficient task offloading and resource allocation in a collaborative mobile
    edge computing (MEC) system. In Plachy et al. (2021), the authors introduced a
    framework for the efficient sharing of communication and computation resources
    in a MEC system. Based on the framework, appropriate communication channels between
    the base station and users are appropriately chosen and the base station’s computation
    resources can be preallocated by using a probabilistic model of user mobility.
    To enable better training and inference of the AI model, Wang et al. (2019) developed
    an Edge AI framework to enable intelligent collaboration for the exchange of data
    and model parameters between edge nodes and devices. To address the resource allocation
    problem in cooperation between multi-level edge computing nodes, the study (Ren
    et al., 2020) proposed a method in which edge nodes employ deep reinforcement
    learning agents to obtain the strategy of computing resource allocation for maximizing
    long-term benefits. In the study (Zhao et al., 2019), the authors presented a
    collaborative computing offloading problem while cloud and MEC computing coexist
    in vehicular networks. They formulated the problem as a constrained optimization
    problem and gave a solution jointly optimizing computing resource allocation and
    maximizing system availability. Xie et al. (2019) proposed a novel collaborative
    vehicular edge computing network (CVECN) architecture and introduced the corresponding
    functional modules, communication process, as well as installation and deployment
    ideas. Mao et al. (2017) showed a collaborative EC system that leverages edge
    devices and cloud servers to jointly perform computation and data storage tasks
    using a task offloading approach. Fan et al. (2023) introduced a joint task offloading
    and resource allocation for vehicle EC with edge–edge cooperation, in which the
    tasks offloaded to a high-load edge server can be further offloaded to the low-load
    edge servers. 2.2. Blockchain applied in edge computing Studies on the application
    of blockchain technology in EC have been attracting attention in recent years.
    Several works have looked at the potential benefits and challenges of combining
    blockchain and EC to improve the efficiency, security, and privacy of EC systems
    (Guo et al., 2020a, Guo et al., 2020c). For example, Xu et al. (2020) proposed
    Edgence (EDGe+intelligENCE) for decentralized Internet of Things (IoT) applications
    management. It leverages edge clouds to access IoT devices and users, and employs
    an in-built blockchain to support self-governance and self-supervision of the
    edge clouds. The study (Yuan et al., 2023) introduced CoopEdge+, a new blockchain-based
    decentralized platform designed to drive and support cooperative multi-access
    edge computing systematically. On CoopEdge+, edge servers compete for tasks published
    by a certain edge server. Nguyen et al. (2023) presented an innovative approach
    to cooperative task offloading and block mining (TOBM) in a blockchain-based MEC
    system. It also introduced a new consensus mechanism called Proof-of-Reputation,
    which utilizes a lightweight strategy for block verification. The authors in Qiu
    et al., 2019, Guo et al., 2020b and Liu et al. (2018) investigated edge offloading
    schemes for blockchain mining tasks using edge clouds. Their goal was to improve
    block mining efficiency and enhance the quality of service (QoS). There is a lot
    of exploration of collaboration management and blockchain in EC in the literature.
    However, the application of these technologies in TSN scenarios has not been considered.
    As far as we know, this paper is the first study focused on EC collaboration management
    in TSN. 3. The proposed scheme In this section, we describe the overview of the
    proposed edge–edge collaboration management framework and workflow based on TECChain,
    detail the proposed consensus mechanisms and algorithms, and discuss the algorithm
    analysis. 3.1. Edge–edge collaboration management To make the edge–edge collaboration
    management process secure, autonomous, and orderly, and to guarantee the fair
    and just distribution of benefits among ECNs, we propose the TECChain blockchain
    and then design a blockchain-based collaboration management framework shown in
    Fig. 1. The TECChain is constructed on a TSN, which may be composed of many subnetworks
    connected to each other through other kinds of networks (e.g., 5G). There are
    six entities in the TECChain: centralized user configuration (CUC), central network
    controller (CNC), edge device (ED), ECN as collaboration service requester (RECN),
    ECN as collaboration service provider (PECN), and ECN other than RECN and PECN
    (OECN), where ED, PECN, RECN, and OECN also are blockchain nodes, and CUC and
    CNC are responsible for TSN management. The description of each entity is as follows:
    Download : Download high-res image (287KB) Download : Download full-size image
    Fig. 1. Framework of edge–edge collaboration management based on TECChain. • CUC:
    The CUC is an application responsible for the configuration of user resources
    in TSN end stations. It is used to discover TSN listeners and talkers, collect
    the capability set and user requirements, and interact with the CNC through a
    user network interface. • CNC: The CNC is responsible for TSN device management,
    network topology discovery, traffic monitoring and optimization, service modeling,
    and schedule model dispatch within the same TSN domain. It also manages the authentication
    and public keys for TECChain nodes. • ED: An ED is normal equipment with poor
    computing capability in an edge network, such as the sensor node and mechanical
    arm. It is usually the source of edge data. • RECN: The RECN as a collaboration
    service requester desires for collaboration services such as data sharing or collaborative
    computing from service providers. • PECN: The PECN as a collaboration service
    provider provides collaboration services to service requesters. It can process
    its edge data in real-time while providing collaboration support to other ECNs.
    • OECN: An OECN does not require or provide collaboration service at the moment.
    In other words, it is an ECN, but not a RECN or PECN right now. In TECChain, ECNs
    can be divided into PECNs, RECNs, and OECNs as well as simple ECN (SECN) and CECN.
    SECN is a normal ECN and also a common TSN node. CECN is a compound equipment,
    which is not only an ECN but also a device that provides a TSN synchronous clock
    (such as the TSN gateway with EC function). As a TSN master clock device, CECN
    provides time synchronization services to the slave clock devices. To further
    illustrate the various roles in Fig. 1, suppose that there are EDs, ECNs including
    PECNs and RECNs, and several pure TSN devices, that is, facilities that only provide
    communication functions for TSN. Let denote the EDs set; denote the RECN set;
    represent the PECN set; { , ,…, } denote the SECN set; , represent the CECN set;
    denote the whole ECN set; and represent the blockchain nodes on TECChain. Obviously,
    , , , , and . In TSN, the edge–edge collaboration between RECNs and PECNs is carried
    out under the TECChain-based decentralized architecture proposed in this paper.
    It is necessary to standardize the procedures and interfaces for service requests,
    service response, cooperation profit distribution, and other processes among RECNs,
    PECNs, and other TECChain blockchain members, that is, to design the collaboration
    management workflow. The proposed edge–edge collaboration management workflow
    in TSN is shown in Fig. 2 and elaborated as follows: Download : Download high-res
    image (346KB) Download : Download full-size image Fig. 2. Edge–edge collaboration
    management workflow in TSN. (1) Step 1: The RECN node selects one or more PECN
    nodes to collaboratively accomplish computation work. Here, we suppose that selects
    for collaboration. (2) Step 2: negotiates with , and an agreement including the
    interest distribution scheme is achieved. drafts a smart contract and obtains
    the ciphertext by using its private key and encrypted function Encrypt . Then,
    obtains the digital signature of the smart contract . Finally, sends the smart
    contract message to , where the and represent the identifications of and , respectively,
    and the is the timestamp. obtains ’s public key from the CNC, and confirms the
    smart contract content and the signature. If all the verification results are
    correct, gives its own digital signature of the smart contract, that is, , using
    the encrypted function Encrypt , and then broadcasts the transaction message The
    transaction (smart contract) is stored in the transaction pool, or immediately
    initiates a new block to TECChain. If the verification fails, renegotiates with
    again. During the transmission, the smart contract is protected by encryption.
    Only the nodes that are identified by the CNC identity authentication center can
    obtain the public key and decrypt the smart contract, which guarantees the security
    of the contract between EC nodes. Meanwhile, all the digital signatures in the
    transaction message prove that the two nodes have reached a consensus on the smart
    contract, which cannot be denied and tampered with. (3) Step 3: Blockchain members
    reach a consensus on the new block containing the transactions (smart contracts),
    which is added to the blockchain. (4) Step 4: After and complete this collaboration
    work, sends a task completion event and confirms the event, which triggers the
    automatic execution of a smart contract to implement the benefit delivery. 3.2.
    Proposed consensus mechanism The main problem, that should be solved in applying
    TECChain in edge–edge collaboration management, is the consensus conflict between
    blockchain nodes, i.e., the process of step 3 presented in Fig. 2. Since, as mentioned
    above, the existing mainstream blockchain consensus algorithms have their shortcomings.
    We need to consider the characteristics of the TSN to design an efficient blockchain
    consensus mechanism especially suitable for TSN. In this section, we first introduce
    the clock synchronization mechanism of TSN and then propose the consensus strategy
    and algorithms based on the clock synchronization in TSN. Download : Download
    high-res image (226KB) Download : Download full-size image Fig. 3. Time synchronization
    network architecture in TSN. The most important feature of a TSN is that each
    device in the network supports the time synchronization protocol IEEE 802.1AS
    (IEEE, 2011). The time synchronization network architecture in TSN is presented
    in Fig. 3. A time synchronization network domain is made up of a grand master
    and several clock masters and clock slaves. The grand master provides a benchmark
    clock to the entire domain. The TSN bridge (gateway) provides two working modes
    to support time synchronization: first, as a repeater equipment for clock synchronization
    message transmission; and second, as a slave clock device to synchronize the clock
    with the upper master clock device and as a master device to provide master clock
    service to the subordinate slave device. All the leaf nodes of the network act
    as slave clock devices to synchronize with the upper master clocks. All members
    on TECChain including ECNs and EDs support the IEEE 802.1AS time synchronization
    function. The SECN, which is an independent computing device, is located at the
    leaf node of the TSN. The CECN plays a compound role in the network (such as a
    bridge or gateway with edge computing capability) and provides clock synchronization
    services to subordinate devices as a master clock. In the actual engineering applications
    of TSNs, ECNs are mostly deployed as CECNs. The IEEE 802.1AS standard does not
    specify the clock synchronization frequency, but theoretically, the higher the
    synchronization frequency in a certain period, the higher the precision of synchronization
    between the master clock and the slave clocks. In a TSN, a master clock device
    provides clock synchronization service to the slave clock, and the maintenance
    of a high-precision synchronous clock of the network requires its active dedication.
    The service frequency of the master clock device reflects its diligence and dedication
    degree. Inspired by this clock synchronization mechanism and considering the characteristics
    of TECChain members, we designed a novel consensus mechanism called PoD for TECChain.
    The core principle of PoD is that the more diligent a TECChain node is, the more
    decision rights it owns, that is, the higher probability it is chosen for generating
    a new blockchain block. Moreover, considering that an ED node with poor computing
    capability is not competent for complex work in blockchain consensus, we propose
    a DPoD mechanism, with which the delegated group composed of CECNs is responsible
    for consensus processing within the blockchain. Download : Download high-res image
    (268KB) Download : Download full-size image In this work, blockchain nodes on
    TECChain are categorized into three: voting nodes, candidate nodes, and delegated
    nodes. The voting nodes, which have the voting right for the delegated group,
    include EDs and ECNs, that is, set . The candidate nodes, which may be elected
    as delegated nodes, include all the CECNs, that is, set . The delegated nodes
    group, that is, the consensus node group presented with a set, which consists
    of a producer node responsible for generating new block and many validator nodes
    for new block verification. The proposed DPoD mechanism has three processes: delegated
    node election, consensus achievement, and producer node evaluation. The roles
    of the nodes may be constantly migrating during the three-step process of DPoD,
    as shown in Fig. 4. The workflow of DPoD is described as follows: Download : Download
    high-res image (287KB) Download : Download full-size image Fig. 4. Roles migration
    of nodes in the three-step process of DPoD. (a) Delegated nodes election In DPoD,
    each member of has the right to vote for the CECN providing clock synchronization
    service to it. Suppose that, in a certain period, voting node , as a slave clock
    device, obtains clock synchronization service from CECN node , which is the master
    clock device, then the voting score from to can be represented as (1) Here, returns
    the clock synchronization frequency in a certain period, which represents the
    diligence degree of node ; is the reputation function for to , and returns 1 or
    −1 according the status value, , stored in . is presented as (2) Notice that the
    judgment of each node about may be different, and the return values of may be
    opposite for two different voting nodes. In Eq. (1), is the diligence score weight
    and is the credibility weight used for adjusting the voting score of abnormal
    nodes. can prevent malicious nodes from deliberately raising fake clock synchronization
    frequency to pre-empt the delegate group seats. The total voting score of CECN
    is (3) When voting scores are obtained, let all CECN nodes arrange from high to
    low according to the scores, and then the top Z nodes are selected as delegated
    node group denoted by . The delegated nodes election algorithm in DPoD is shown
    in Algorithm 1. (b) Consensus achievement Considering that all nodes on TECChain
    work with a TSN high-precision synchronous clock, this paper proposes two kinds
    of time-slot-driven consensus methods of DPoD: S-DPoD and BFT-DPoD, which are
    described in detail in the following sections. (c) Producer node evaluation After
    the consensus process, all nodes on TECChain evaluate the producer node whether
    it is normal or abnormal. The result corresponds to the return value of the reputation
    function P for the producer node, which would impact its voting score in the next
    round. 3.2.1. S-DPoD algorithm Download : Download high-res image (506KB) Download
    : Download full-size image Download : Download high-res image (419KB) Download
    : Download full-size image Fig. 5. S-DPoD consensus process. Download : Download
    high-res image (541KB) Download : Download full-size image Fig. 6. Flowchart of
    the S-DPoD algorithm. To resolve the conflict in the process of distributed consensus,
    the S-DPoD algorithm adopts the sequential decision-making method to reach the
    consensus of the entire network. The S-DPoD consensus process is shown in Fig.
    5. Suppose all the new transaction records (smart contracts) are stored in the
    transaction pool, TECChain uses the S-DPoD algorithm to create a new block containing
    new transactions and archive consensus with the duration of time slots per round.
    The pseudocode and flowchart of the S-DPoD algorithm are shown in Algorithm 2
    and Fig. 6, respectively. In the first time slot, the Z delegated nodes are selected
    and listed in order from high to low according to the voting score. In the second
    time slot, the first one in the list, picked as a producer node, generates and
    broadcasts a new block, and then broadcasts a consensus message , where the is
    the message sender identification, is the number of this round, is the consensus
    achievement count initialized as 0 by the producer, is the signature of the producer
    for this message, and is the timestamp of the current time slot. From slot 2 to
    slot , all validator nodes receive a broadcasting consensus message from one node
    in each time slot and check its legitimacy, such as whether the timestamp is within
    the current time slot range, whether the consensus achievement count is within
    a reasonable range, and so on. If the message is found to be abnormal, it is abandoned.
    The validators verify the new block and send the decision in turn in their own
    time slot. If the validator approves the new block, it sends the consensus message
    , where is the maximum consensus count in consensus messages received from other
    nodes, and sets the status value of the producer as normal. If the validator finds
    the new block to be illegal, it sets the status value of the producer as abnormal
    and does not send any consensus message. In the last time slot, all nodes check
    the parameter of the last received consensus message. If , the consensus is achieved
    in this round; otherwise, it failed. All nodes except delegated nodes regard the
    producer as normal or abnormal according to the consensus result and set the status
    value of the producer stored in themselves. Finally, each node on TECChain updates
    according to . During the consensus process, some invalid nodes cannot verify
    consensus within their own time slots, as shown in Fig. 5. However, this will
    not affect the final decision of the consensus algorithm, because as long as more
    than half of the validators approve the new block generated by the producer, the
    consensus is reached. Download : Download high-res image (400KB) Download : Download
    full-size image Fig. 7. BFT-DPoD consensus process. Download : Download high-res
    image (538KB) Download : Download full-size image Fig. 8. Flowchart of BFT-DPoD.
    3.2.2. BFT-DPoD algorithm The main principle of S-DPoD is that the consensus process
    is split into several time slots for each validator verifying the new block. It
    prevents the consensus messages from arriving at the same time to simplify the
    process but consumes more time slots. To overcome this problem, we designed the
    BFT-DPoD algorithm based on the Byzantine fault-tolerant method. It is also driven
    by time slots based on the TSN high-precious synchronous clock. Fig. 7 describes
    the BFT-DPoD consensus process. BFT-DPoD is composed of six stages (time slots):
    REQUEST, DELEGATES ELECTION, PROPOSE, PRE-VERIFICATION, COMMIT, and REPLY, depicted
    as follows: • REQUEST: After PECN has negotiated with RECN and confirmed the smart
    contract, would broadcast the REQUEST message,¡REQUEST, , , , ¿ ,carrying the
    new transaction (smart contract), where k is the identification of this consensus
    round, is the encrypted smart contract message containing the signatures of and
    , is the timestamp, and is the identification of the message sender. • DELEGATES
    ELECTION: All nodes on TECChain elect validator nodes and a producer node using
    Algorithm 1. • PROPOSE: The producer node generates a new block and broadcasts
    the PROPOSE message, ¡ ¡PROPOSE, , , , ¿, ¿, where is the encrypted data of the
    message’s digest with the producer private key, that is, a digital signature,
    which can be used for integrity checking in the receiving node. • PRE-VERIFICATION:
    After receiving the PROPOSE message, all validators verify the digital signature
    with , the integrity of this message, the rationality of the timestamp, and so
    on, then check the new block and compare the smart contract with the one in the
    REQUEST message. If all these verifications have been successful, each validator
    broadcasts the PRE-VERIFICATION message, , and sets the status value of the producer
    node as normal; otherwise, the validator labels the PROPOSE message is illegal
    and sets the status value of the producer node as abnormal, then the message is
    discarded. The invalid nodes cannot respond to the PROPOSE message and send the
    PRE-VERIFICATION message properly. • COMMIT: After receiving the PRE-VERIFICATION
    message, validators will verify it. The message will be discarded when it is found
    to be illegal. The delegated node that receives PRE-VERIFICATION messages from
    more than 2/3 other validators, will think that the delegated group has achieved
    a consensus, and then broadcasts the COMMIT message, . • REPLY: All the nodes
    on TECChain receive the COMMIT message and discard it if it is found to be illegal.
    If more than 2/3 of the delegated nodes have sent COMMIT messages, it means that
    the consensus is achieved in the whole network, and then the producer and validators
    send the REPLY message to PECN, and the nodes not in the delegated group set the
    status value of the producer node as normal; otherwise, these nodes set it as
    abnormal. If the PECN receives more than 2(Z-1)/3 REPLY messages, it indicates
    that the new transaction in the new block has been recorded on TECChain; otherwise,
    the PECN would request again in the next round. At the end of the time slot, each
    node evaluates the producer according to its status value, which can affect the
    return value of the function . The pseudocode of the BFT-DPoD algorithm is shown
    in Algorithm 3, and the flowchart is presented in Fig. 8. Download : Download
    high-res image (738KB) Download : Download full-size image 3.2.3. Algorithm analysis
    This section analyzes the S-DPoD and BFT-DPoD algorithms, including the number
    of interactive messages, Byzantine fault tolerance, transaction throughput, security
    capability, and so on. (a) Number of interactive messages In the DPoD mechanism,
    the consensus process is confined within the delegated group, so the number of
    interactive messages is less than that of the whole network consensus mechanism.
    Let the number of the TECChain nodes and the delegated nodes be and , respectively.
    We can figure out that the interactive message amount is , that is, in and , that
    is, in BFT-DPoD, where the delegate election stage demands messages for both algorithms.
    (b) Byzantine fault tolerance As we all know, the fault tolerance ability for
    Byzantine faults is an important index of consensus algorithm security. In the
    proposed S-DPoD, the final consensus conclusion depends on the decision of most
    of the validator nodes, that is, the fault tolerance is , which is equivalent
    to PoW, PoS, and DPoS. The consensus process of BFT-DPoD is essentially consistent
    with the classic BFT mechanism; thus, its Byzantine fault tolerance is similar
    to that of PBFT. (c) Transactions per second The scalability of the blockchain
    is one of the key factors to be considered in the design of a consensus algorithm,
    which can usually be presented by the throughput, that is, transactions per second
    (TPS). In the blockchain system, the faster the block generation speed, the higher
    the transaction throughput, which also indicates the higher performance of the
    consensus algorithm. The TPS is calculated by (4) where is the number of successfully
    generated blocks within the period , and denotes the number of transactions contained
    in each block. Our proposed S-DPoD and BFT-DPoD are driven by the high-precious
    clock time slots. In each round of generating a new block and achieving consensus,
    S-DPoD and BFT-DPoD need Z 2 and 6 time slots, respectively. Assuming that the
    duration of each time slot is seconds, then S-DPoD and BFT-DPoD can carry out
    and rounds of consensus per second, respectively. Thus, the TPS of S-DPoD can
    be expressed as (5) and the TPS of BFT-DPoD can be expressed as (6) Here, the
    and are the probabilities that S-DPoD and BFT-DPoD would successfully generate
    a new block, that is, consensus is achieved in each round, respectively. Obviously,
    the TPS of the two algorithms increases with the increase in the probability and
    the number of transactions in each block, and decreases as the time slot duration
    increases. For a certain blockchain, the maximum value of is a specific number.
    For example, a Bitcoin block can contain up to 3000 transactions, and an Ethereum
    block can contain 380 transactions. To obtain a higher TPS of our algorithms,
    we can reduce the as much as possible. However, it is limited by the processing
    speed of the delegated nodes on TECChain; therefore it should be set within a
    reasonable interval according to the node computing performance. Eq. (5) shows
    that the TPS of S-DPoD is related to the number of delegated nodes and is inversely
    proportional to the number of time slots in each consensus round. However, BFT-DPoD
    consumes a fixed number of time slots in every round, and its TPS is independent
    of the delegated group size. (d) Security capability Table 1. Comparison of consensus
    algorithms. Key features PoW (Conti et al., 2018) PoS (Vitalik Buterin, 2023)
    DPoS (Larimer, 2019) PBFT (Castro and Liskov, 1999) BFT-PoS (EoS) (Block.one,
    2018) S-DPoD BFT-DPoD Communication complexity Energy saving No Partial Partial
    Yes Yes Yes Yes Byzantine fault tolerance Block fork Yes Yes No Yes Yes No No
    Transaction verifying delay High High High Low Low Medial Low TPS 10 2800 Here,
    we will analyze the security capability of the proposed consensus algorithms under
    three common blockchain attack scenarios. • Double spending The double spending
    attack occurs on the blockchain when a node tries to spend money twice. The attacker
    usually creates an ordinary transaction in the main chain first, and then creates
    and posts a fraudulent transaction in the fork chain after a period. The attacker
    continues to mine until the length of the fork chain exceeds that of the main
    chain. At this point, once other nodes find a longer chain in the network, they
    all switch to this fork chain, and the fork chain becomes the main chain. Thus,
    the previous normal transaction is rolled back, and the double spending attack
    succeeds. In our proposed S-DPoD and BFT-DPoD, the time-slot-driven consensus
    mechanism guarantees that no fork chain exists, which also means it is not vulnerable
    to double spending attack. • Collusion attack Blockchain collusion attacks often
    refer to malicious nodes colluding with others to create illegal blocks or affect
    the consensus result. Based on this analysis, we know that the final consensus
    result will not be affected as long as the number of malicious nodes in the delegated
    group is less than 50% and 33.3% for S-DPoD and BFT-DPoD, respectively. • Sybil
    attack The sybil attack is a typical security threat in P2P networks, which is
    common in public blockchains without authority management. Attackers create a
    large number of nodes to obtain plenty of verification capabilities, and even
    broadcast fake messages in the blockchain to affect the consensus results. Our
    S-DPoD and BFT-DPoD algorithms are based on the TECChain architecture, whose infrastructure
    network is TSN. The TSN security protocol has designed authentication mechanism
    of network nodes, which reduces the possibility of sybil attacks to a certain
    extent. In addition, the DPoD has a punishment strategy for the producer node
    that generates illegal blocks, which can prevent it from continuously getting
    into the delegated group, so as to resist the sybil attack. (e) Comparisons with
    other consensus algorithms To further analyze the advantages and disadvantages
    of the S-DPoD and BFT-DPoD algorithms, we compare their key features with those
    of other mainstream consensus algorithms, and the results are presented in Table
    1. Since DPoS, S-DPoD, BFT-DPoD, and BFT-PoS(EoS) all need to undergo the delegated
    group election stage, there is an part in their communication complexity; BFT-DPoD
    and BFT-PoS (EoS) have a similar workflow of verification process for a new block,
    and there are similar parts of the two algorithms too. Because the consensus algorithms
    except PoW do not have the “mining” requirement, they consume less energy. The
    Byzantine fault tolerance of the algorithms in the table is divided into two categories.
    The Byzantine fault tolerance of the algorithms without the BFT mechanism is 1/2
    of consensus nodes. The algorithms using the BFT mechanism improve the confirmation
    speed of the distributed consensus process, but reduce the Byzantine fault tolerance,
    which is 1/3 of consensus nodes. Download : Download high-res image (259KB) Download
    : Download full-size image Fig. 9. S-DPoD vs. BFT-DPoD about the number of interactive
    messages in each consensus round. Download : Download high-res image (439KB) Download
    : Download full-size image Fig. 10. S-DPoD vs. BFT-DPoD on TPS in each consensus
    round. Download : Download high-res image (742KB) Download : Download full-size
    image Fig. 11. Number of times that CECNs are selected as delegated nodes and
    producer nodes with , when in (a), in (b), in (c), and in (d), respectively. The
    area in the red dotted box is the data of the Class B malicious nodes. Download
    : Download high-res image (746KB) Download : Download full-size image Fig. 12.
    Number of times that each CECN is elected as delegated nodes and producer nodes
    with , when in (a), in (b), in (c), and in (d), respectively. The area in the
    red dotted box is the data of the Class B malicious nodes. The BFT-PoS consensus
    mechanism, used in the enterprise operation system (EoS) blockchain, is similar
    to our proposed BFT-DPoD algorithm. However, because of the lack of high-precision
    synchronous clock in an EoS network, the BFT verification process is carried out
    at a rough time in each round. To improve the verification efficiency, EoS allows
    many new blocks to be created by multiple nodes within a certain period, which
    may lead to a block fork. Then, the longest chain selection method is used to
    solve the bifurcation problem. In our BFT-DPoD, each validator node synchronously
    verifies the new block strictly according to the precise time slot. Thus, there
    is no block fork, and the transaction verification delay is lower and the security
    is higher. Similarly, S-DPoD does not generate blockchain forks, but due to the
    large number of time slots required for sequential verification, the transaction
    verification delay is generally higher than that of BFT-PoS and BFT-DPoD but lower
    than that of PoW, PoS, and DPoS. 4. Experiment In this section, a simulation experiment
    platform is built in a Linux 64-bit system using multithreads to simulate multiple
    blockchain nodes with different attributes. Considering the small size of each
    interactive message in the consensus stage and the nanosecond transmission delay
    in TSN, this experiment uses the inter-thread messaging mechanism to simulate
    the communication between nodes in the TSN. The edge–edge management procedure
    proposed in this paper can be divided into three: contract negotiation between
    RECN and PECN, new block with smart contract consensus, and smart contract execution.
    The first and third parts are all fixed step-by-step interaction processes between
    RECN and PECN, while the second part is the most complicated and important procedure
    for the whole management scheme. Therefore, the experiments mainly focus on evaluating
    our proposed consensus algorithms. 4.1. Interactive messages First, we evaluate
    the volume of interactive messages in S-DPoD and BFT-DPoD. Assume that there is
    no abnormal node in the network, we construct simulation experiments with the
    following parameter settings: the number of EDs is , the numbers of ECNs and CECNs
    are 200 and 150 respectively, the number of delegated nodes is Z , the sizes of
    REQUEST message and PROPOSE message are 100 and 1000 bytes respectively, and the
    sizes of the voting message, PRE-VERIFICATION message, COMMIT message, and RELAY
    message are all 8 bytes. The experiment results about the number of interactive
    messages of S-DPoD and BFT-DPoD in each consensus round are listed in Table 2,
    Table 3 respectively. We can draw two conclusions from the two tables: the interactive
    message volume increases with the increase of the number of TECChain nodes, i.e.,
    , both in S-DPoD and BFT-DPoD; the larger size of the delegated group elevates
    the interactive message volume in BFT-DPoD but has little influence in S-DPoD.
    This is because the communication complexity of S-DPoD is , while is little (about
    in this experiment), the increase of Z has a little effect on the number of total
    messages. However, the communication complexity of BFT-DPoD is . Hence, increasing
    Z will significantly increase the number of interactive message in BFT-DPoD. Fig.
    9 shows the comparison of the number of interactive messages in each consensus
    round of S-DPoD and BFT-DPoD. This demonstrates that in the scenario with the
    same number of nodes and the same size of the delegated group, the interactive
    messages in BFT-DPoD are significantly more than that in S-DPoD, and the gap in
    number increases with the increase in Z. This is because in the Pre-verification
    and Commit stages of BFT-DPoD, the validators send messages to each other, and
    the larger Z is, the larger the number of messages will be. Table 2. Number of
    interactive messages of S-DPoD in each consensus round. Number of TECChain nodes
    Z 21 Z 51 Z 81 Z 111 400 159 640 159 700 159 760 159 820 450 202 090 202 150 202
    210 202 270 500 249 540 249 600 249 660 249 720 550 301 990 302 050 302 110 302
    170 600 359 440 359 500 359 560 359 620 650 421 890 421 950 422 010 422 070 Table
    3. Number of interactive messages of BFT-DPoD in each consensus round. Number
    of TECChain nodes Z 21 Z 51 Z 81 Z 111 400 160 881 165 201 173 121 184 641 450
    203 381 207 701 215 621 227 141 500 250 881 255 201 263 121 274 641 550 303 381
    307 701 315 621 327 141 600 360 881 365 201 373 121 384 641 650 423 381 427 701
    435 621 447 141 4.2. TPS Then, we construct an experiment for comparison and contrast
    of S-DPoD and BFT-DPoD on TPS. Let the time slot duration of S-DPoD and BFT-DPoD
    be , number of delegated nodes, that is, , be in the interval [1, 30], each block
    contains 20 transactions, that is, 20; the number of EDs be , and the number of
    ECNs and CECNs be 200 and 150, respectively. Assume that there are no abnormal
    nodes on TECChain, in other words, the success rate of consensus is 100%. The
    comparison and contrast of S-DPoD and BFT-DPoD on TPS in each consensus round
    are shown in Fig. 10. This illustrates that the TPS of the two algorithms decreases
    with the increase in the time slot duration , and the TPS of S-DPoD decreases
    with the increase in , but the TPS of BFT-DPoD is independent of , which is consistent
    with the analysis in Section 3. Meanwhile, it was found that while the success
    probability, the number of transactions in a block, and the duration of the time
    slot are fixed, the TPS of S-DPoD and BFT-DPoD are the same if the size of the
    delegated group , the TPS of S-DPoD is more than BFT-DPoD when , and that of BFT-DPoD
    is more than S-DPoD when . In an actual TECChain application, generally is much
    larger than 4, that is, BFT-DPoD can provide more TPS. Download : Download high-res
    image (645KB) Download : Download full-size image Fig. 13. Number of all the malicious
    nodes being elected as delegated nodes or even producer nodes when Z 21 or Z 51
    with when in (a), in (b), in (c), and in (d). 4.3. Validators and producer election
    In the previous experiment, an ideal environment is assumed, that is, there are
    no abnormal nodes and the consensus success rate is 100%, which is difficult to
    achieve in practical applications. To further analyze the performance of the two
    algorithms in the case of abnormal nodes existing on TECChain, several malicious
    nodes are added to the network. The malicious nodes can be categorized into two:
    malicious EDs and ECNs, which forge fake diligence values for other nodes in the
    election stage, are denoted by Class A malicious nodes, and their quantity is
    marked with ; malicious CECNs, which can not only forge the diligence values for
    other nodes but also participate in the creation and verification of new blocks
    as delegated nodes, are denoted by Class B malicious nodes, and their quantity
    is marked with . The behaviors of malicious nodes are defined as follows: Class
    A and B malicious nodes collude with each other and declare the diligence value
    of class B nodes as a fake and maximal number, while declaring the diligence value
    of normal CECNs as zero. By increasing the voting score of malicious partners
    and reducing that of the normal node, malicious nodes expect more Class B nodes
    to join the delegated group. If Class B nodes are elected as producers, they would
    construct a false new block; if Class B nodes are elected as validators, they
    would make a reverse verification statement, that is, approve the false new blocks
    but decline the correct new blocks, so as to achieve the purpose of interfering
    with the final consensus results. The S-DPoD and BPT-DPoD mainly consist of the
    election and verification stages. Here, the influence of malicious nodes in the
    election stage is firstly analyzed experimentally. Since the election process
    of the two algorithms is basically the same, the experimental design and the results
    analysis are unified for them. Let the number of EDs be 400, the number of ECNs
    be 200 including 150 CECNs and 50 SECNs, the size of the delegated group be Z
    21, the weight parameters are and , the frequency of clock synchronization service
    provided by normal CECNs follow the average distribution in the range of [0,100],
    the fake diligence value of Class B malicious nodes declared by other malicious
    nodes also follow the average distribution in the range of [80,100], and the number
    of Class A and B malicious nodes be and respectively. Here we implement 100 times
    experiments. Fig. 11, Fig. 12 show that the times of each CECN are elected as
    delegated nodes and producer nodes with and , where the area in the red dotted
    box is the data of the Class B nodes. The red dotted box parts in the two figures
    show that the number of times each Class B malicious node joining the delegated
    group increases when the number of malicious nodes increases. Especially in Fig.
    11(d), it reaches over 80 (relative to the total 100 times experiments) when and
    . By comparing Fig. 11 with Fig. 12, we find that, when the and are the same,
    the frequency of each Class B malicious node being elected as a delegated node
    is relatively smaller for than . This indicates that the weight parameters and
    affect the experimental results. To further analyze the relationship between the
    values of and and the possibility of malicious nodes being elected as delegated
    nodes or even producers, we conducted the previous experiment again with instead
    of , and then combined the data of the two experiments to draw Fig. 13, which
    shows the relationship between / and the probability of malicious nodes being
    elected as members of the delegate group or as the producer when is 21 and 51,
    respectively. Fig. 13 shows that, in the scenario with the same delegated group
    size and a different number of malicious nodes, the number of all the malicious
    nodes being selected as delegated nodes and producers remains stable when / is
    below a certain threshold, and increases when / is above the threshold. This is
    because and , as the weights of diligence and penalty, also represent the weights
    of “gain” and “loss” in the vote score of a candidate node. In the process of
    consensus, when a node is identified as an abnormal node, the punishment intensity
    largely determines whether it can be elected as a delegated node or even a producer
    node in the next round. The smaller / means the heavier punishment, and vice versa.
    Obviously, when the penalty is high enough, the malicious node may get a high
    vote score and join the delegated group only in the first round, but from the
    second round, even if it gets a fake and high diligence value, it would have little
    chance to join the delegated group again as its status stored in each normal node.
    Fig. 13 also shows that as / gradually increases, that is, the punishment gradually
    decreases, malicious nodes can be elected into the delegated group and even get
    the opportunity to be producer nodes again when / is greater than the threshold.
    This threshold decreases as the number of abnormal nodes increases. In other words,
    to prevent more malicious nodes from entering the delegated group, the / needs
    to be reduced. At the same time, we investigate the influence of Z on the probability
    of malicious nodes being elected as delegated nodes and find that under the same
    conditions, the larger the Z is, the lower the probability of a malicious node
    joining the delegated group. However, the impact of Z on the probability of a
    malicious node becoming a producer is not significant. Download : Download high-res
    image (417KB) Download : Download full-size image Fig. 14. SoC with different
    numbers of Class B malicious nodes. 4.4. Success rate of consensus Next, we focus
    on comparative experiments on the success rate in S-DPoD and BFT-DPoD algorithms.
    Here, the success rate of consensus (SoC) is used to denote the probability of
    the consensus algorithm successfully generating a new block. SoC is defined as
    (7) where the and represent, respectively, the number of total consensus rounds
    and the number of times consensus is finally achieved within a certain period
    in TECChain. Since the key factor of whether a new block can be successfully created
    is the malicious nodes joining the delegated group, we set up an experiment with
    , , and to analyze the relationship between SoC and the number of Class B malicious
    nodes. We implement the experiment 100 times for various parameter combinations
    and obtain the relationship as shown in Fig. 14. It can be seen that when is small,
    the SoC of the two algorithms is almost stable at 100%. This is because even if
    all the Class B malicious nodes are elected as delegated nodes, the Byzantine
    fault tolerance threshold, which is for S-DPoD or for BFT-DPoD, cannot be reached,
    while is much smaller than Z. However, as increases beyond a certain threshold,
    each SoC of the two algorithms begins to decline to a low value at an approximately
    linear rate. Fig. 14 shows that the threshold of S-DPoD is higher than that of
    BFT-DPoD. Meanwhile, from the four comparative graphs in Fig. 14, it is found
    that the larger Z is, the larger SoC will be because more normal delegated nodes
    can provide more resistance to a specific number of Class B malicious nodes. In
    summary, under the same condition, the SoC of BFT-DPoD is generally lower than
    that of S-DPoD, but if the number of Class B malicious nodes is not large, the
    ones of the two algorithms are equivalent. In other words, increasing the size
    of the delegated group can also improve the SoC. Overall, in general, S-DPoD consumes
    fewer interactive messages and has a higher Byzantine fault tolerance and success
    rate of consensus than BFT-DPoD, but BFT-DPoD has lower transaction verification
    latency and larger TPS. Therefore, we should choose the appropriate one according
    to the actual application requirements, while deploying the proposed edge–edge
    collaboration management scheme. 5. Conclusion In this paper, we propose a blockchain-based
    scheme that ensures the safety, autonomy, and orderliness of the edge–edge collaboration
    management process in TSN, and guarantees the fair and equitable distribution
    of collaboration benefits among nodes. There are two main novelties of our method:
    the design of the edge–edge management framework and the unified workflow based
    on the proposed TECChain blockchain, and the proposed consensus algorithms that
    are tailored for TECChain. We analyze the proposed consensus algorithms both theoretically
    and through simulation in this paper. The results demonstrate that our algorithms
    outperform other algorithms in various aspects, such as security, TPS, and energy
    saving. Based on the algorithm comparison results, we recommend S-DPoD for scenarios
    that require high Byzantine fault tolerance and success rate of consensus, and
    BFT-DPoD for scenarios that demand low transaction verification latency and higher
    TPS. It is assumed that all nodes voluntarily and actively participate in the
    collaboration management process in our method, but in practical application,
    each node is an independent and selfish agent, and its enthusiasm to participate
    in collaboration management may not be high. Therefore, in the future, we plan
    to incorporate a benefit-driving mechanism into our scheme to enhance the motivation
    of the blockchain participants in edge–edge collaboration management for edge
    computing in TSN. Declaration of competing interest The authors declare that they
    have no known competing financial interests or personal relationships that could
    have appeared to influence the work reported in this paper. Acknowledgments This
    work was supported by the National Key R&D Program of China under Grant 2022YFE0204500,
    the Natural Science Foundation of Chongqing, China under Grant CSTB2022NSCQ-MSX0996,
    and the key project of science and technology research program of Chongqing Education
    Commission of China under Grant KJZD-K202301102. References Aceto et al., 2019
    Aceto G., Persico V., Pescapé A. A survey on information and communication technologies
    for industry 4.0: State-of-the-art, taxonomies, perspectives, and challenges IEEE
    Commun. Surv. Tutor., 21 (4) (2019), pp. 3467-3501 CrossRefView in ScopusGoogle
    Scholar Block.one, 2018 Block.one Technical white paper v2 (2018) https://github.com/EOSIO/Documentation/blob/master/TechnicalWhitePaper.md#consensus-algorithm-bft-dpos
    Google Scholar Cao et al., 2021 Cao K., Hu S., Shi Y., Colombo A.W., Karnouskos
    S., Li X. A survey on edge and edge-cloud computing assisted cyber-physical systems
    IEEE Trans. Ind. Inform., 17 (11) (2021), pp. 7806-7819 CrossRefView in ScopusGoogle
    Scholar Castro and Liskov, 1999 Castro, M., Liskov, B., 1999. Practical byzantine
    fault tolerance. In: Proceedings of the 13rd Symposium on Operating Systems Design
    and Implementation, Vol. 99. pp. 173–186. Google Scholar Conti et al., 2018 Conti
    M., Kumar E.S., Lal C., Ruj S. A survey on security and privacy issues of bitcoin
    IEEE Commun. Surv. Tutor., 20 (4) (2018), pp. 3416-3452 CrossRefView in ScopusGoogle
    Scholar Deirmentzoglou et al., 2019 Deirmentzoglou E., Papakyriakopoulos G., Patsakis
    C. A survey on long-range attacks for proof of stake protocols IEEE Access, 7
    (2019), pp. 28712-28725 CrossRefView in ScopusGoogle Scholar Donno et al., 2019
    Donno M.D., Tange K., Dragoni N. Foundations and evolution of modern computing
    paradigms: Cloud, IoT, edge, and fog IEEE Access, 7 (2019), Article 150936-150948
    Google Scholar Fan et al., 2023 Fan W., et al. Game-based task offloading and
    resource allocation for vehicular edge computing with edge-edge cooperation IEEE
    Trans. Veh. Technol. (2023), pp. 1-16, 10.1109/TVT.2023.3241286 View in ScopusGoogle
    Scholar Finn, 2022 Finn N. Introduction to time-sensitive networking IEEE Commun.
    Stand. Mag., 6 (4) (2022), pp. 8-13 CrossRefView in ScopusGoogle Scholar Guo et
    al., 2020a Guo S., Dai Y., Guo S., Qiu X., Qi F. Blockchain meets edge computing:
    Stackelberg game and double auction based task offloading for mobile blockchain
    IEEE Trans. Veh. Technol., 69 (5) (2020), pp. 5549-5561 CrossRefView in ScopusGoogle
    Scholar Guo et al., 2020b Guo S., Dai Y., Guo S., Qiu X., Qi F. Blockchain meets
    edge computing: Stackelberg game and double auction based task offloading for
    mobile blockchain IEEE Trans. Veh. Technol., 69 (5) (2020), pp. 5549-5561 CrossRefView
    in ScopusGoogle Scholar Guo et al., 2020c Guo S., Hu X., Guo S., Qiu X., Qi F.
    Blockchain meets edge computing: A distributed and trusted authentication system
    IEEE Trans. Ind. Inform., 16 (3) (2020), pp. 1972-1983 CrossRefView in ScopusGoogle
    Scholar Huo et al., 2022 Huo R., et al. A comprehensive survey on blockchain in
    industrial internet of things: Motivations, research progresses, and future challenges
    IEEE Commun. Surv. Tutor., 24 (1) (2022), pp. 88-122 CrossRefView in ScopusGoogle
    Scholar IEEE, 2011 IEEE IEEE 802.1as-2011 - IEEE standard for local and metropolitan
    area networks - timing and synchronization for time-sensitive applications in
    Bridged Local Area networks (2011) Retrieved December 10 2020, from https://standards.ieee.org/standard/802_1AS-2011.html
    Google Scholar IEEE, 2017 IEEE Time-sensitive networking task group (2017) http://www.ieee802.org/1/pages/tsn.html
    Google Scholar Kai et al., 2021 Kai C., Zhou H., Yi Y., Huang W. Collaborative
    cloud-edge-end task offloading in mobile-edge computing networks with limited
    communication capability IEEE Trans. Cogn. Commun. Netw., 7 (2) (2021), pp. 624-634
    CrossRefView in ScopusGoogle Scholar Kumar et al., 2019 Kumar G., Saha R., Rai
    M.K., Thomas R., Kim T.-H. Proof-of-work consensus approach in blockchain technology
    for cloud and fog computing using maximization-factorization statistics IEEE Internet
    Things J., 6 (4) (2019), pp. 6835-6842 CrossRefView in ScopusGoogle Scholar Larimer,
    2019 Larimer D. Delegated proof-of-stake (DPOS) (2019) https://www.geeksforgeeks.org/delegated-proof-of-stake/
    Google Scholar Liu et al., 2020 Liu Y., Xie S., Zhang Y. Cooperative offloading
    and resource management for UAV-enabled mobile edge computing in power IoT system
    IEEE Trans. Veh. Technol., 69 (10) (2020), pp. 12229-12239 CrossRefView in ScopusGoogle
    Scholar Liu et al., 2018 Liu M., Yu F.R., Teng Y., Leung V.C.M., Song M. Computation
    offloading and content caching in wireless blockchain networks with mobile edge
    computing IEEE Trans. Veh. Technol., 67 (11) (2018), pp. 11008-11021 CrossRefView
    in ScopusGoogle Scholar Lo Bello and Steiner, 2019 Lo Bello L., Steiner W. A perspective
    on IEEE time sensitive networking for industrial communication and automation
    systems Proc. IEEE, 107 (6) (2019), pp. 1094-1120 CrossRefView in ScopusGoogle
    Scholar Mao et al., 2017 Mao Y., You C., Zhang J., Huang Z., Letaief K.B., Li
    Y. A survey on mobile edge computing: The communication perspective IEEE Commun.
    Surv. Tutor., 19 (4) (2017), pp. 2322-2358 View in ScopusGoogle Scholar Nguyen
    et al., 2023 Nguyen D.C., Ding M., Pathirana P.N., Seneviratne A., Li J., Poor
    H.V. Cooperative task offloading and block mining in blockchain-based edge computing
    with multi-agent deep reinforcement learning IEEE Trans. Mob. Comput., 22 (4)
    (2023), pp. 2021-2037 CrossRefView in ScopusGoogle Scholar Nguyen et al., 2019
    Nguyen C.T., Hoang D.T., Nguyen D.N., Niyato D., Nguyen H.T., Dutkiewicz E. Proof-of-stake
    consensus mechanisms for future blockchain networks: Fundamentals, applications
    and opportunities IEEE Access, 7 (2019), pp. 85727-85745 CrossRefView in ScopusGoogle
    Scholar Plachy et al., 2021 Plachy J., Becvar Z., Strinati E.C., di Pietro N.
    Dynamic allocation of computing and communication resources in multi-access edge
    computing for mobile users IEEE Trans. Netw. Serv. Manag., 18 (2) (2021), pp.
    2089-2106 CrossRefView in ScopusGoogle Scholar Qiu et al., 2019 Qiu X., Liu L.,
    Chen W., Hong Z., Zheng Z. Online deep reinforcement learning for computation
    offloading in blockchain-empowered mobile edge computing IEEE Trans. Veh. Technol.,
    68 (8) (2019), pp. 8050-8062 CrossRefView in ScopusGoogle Scholar Qiu et al.,
    2021 Qiu C., Wang X., Yao H., Du J., Yu F.R., Guo S. Networking integrated cloud–edge–end
    in IoT: A blockchain-assisted collective Q-learning approach IEEE Internet Things
    J., 8 (16) (2021), pp. 12694-12704 CrossRefView in ScopusGoogle Scholar Ren et
    al., 2020 Ren J., Wang H., Hou T., Zheng S., Tang C. Collaborative edge computing
    and caching with deep reinforcement learning decision agents IEEE Access, 8 (2020),
    Article 120604-120612 Google Scholar Shi et al., 2016 Shi W., Cao J., Zhang Q.,
    Li Y., Xu L. Edge computing: Vision and challenges IEEE Internet Things J., 3
    (5) (2016), pp. 637-646 View in ScopusGoogle Scholar Tun et al., 2022 Tun Y.K.,
    Dang T.N., Kim K., Alsenwi M., Saad W., Hong C.S. Collaboration in the Sky: A
    distributed framework for task offloading and resource allocation in multi-access
    edge computing IEEE Internet Things J., 9 (23) (2022), pp. 24221-24235 CrossRefView
    in ScopusGoogle Scholar Vitalik Buterin, 2023 Vitalik Buterin Ethereum whitepaper:
    a next-generation smart contract and decentralized application platform (2023)
    https://ethereum.org/en/whitepaper/ Google Scholar Wang et al., 2019 Wang X.,
    Han Y., Wang C., Zhao Q., Chen X., Chen M. In-edge AI: Intelligentizing mobile
    edge computing, caching and communication by federated learning IEEE Netw., 33
    (5) (2019), pp. 156-165 CrossRefView in ScopusGoogle Scholar Wollschlaeger et
    al., 2017 Wollschlaeger M., Sauter T., Jasperneite J. The future of industrial
    communication: automation networks in the era of the Internet of Things and industry
    4.0 IEEE Ind. Electron. Mag., 11 (1) (2017), pp. 17-27 View in ScopusGoogle Scholar
    Xia et al., 2021 Xia X., Chen F., He Q., Grundy J.C., Abdelrazek M., Jin H. Cost-effective
    app data distribution in edge computing IEEE Trans. Parallel Distrib. Syst., 32
    (1) (2021), pp. 31-44 CrossRefView in ScopusGoogle Scholar Xie et al., 2019 Xie
    R., Tang Q., Wang Q., Liu X., Yu F.R., Huang T. Collaborative vehicular edge computing
    networks: Architecture design and research challenges IEEE Access, 7 (2019), Article
    178942-178952 Google Scholar Xu et al., 2020 Xu J., Wang S., Zhou A., Yang F.
    Edgence: A blockchain-enabled edge-computing platform for intelligent IoT-based
    dapps China Commun., 17 (4) (2020), pp. 78-87 CrossRefView in ScopusGoogle Scholar
    Yuan et al., 2023 Yuan L., et al. CoopEdge+: Enabling decentralized, secure and
    cooperative multi-access edge computing based on blockchain IEEE Trans. Parallel
    Distrib. Syst., 34 (3) (2023), pp. 894-908 CrossRefView in ScopusGoogle Scholar
    Zhao et al., 2019 Zhao J., Li Q., Gong Y., Zhang K. Computation offloading and
    resource allocation for cloud assisted mobile edge computing in vehicular networks
    IEEE Trans. Veh. Technol., 68 (8) (2019), pp. 7944-7956 CrossRefView in ScopusGoogle
    Scholar Cited by (0) Peer review under responsibility of King Saud University.
    © 2023 The Author(s). Published by Elsevier B.V. on behalf of King Saud University.
    Recommended articles A secure and lightweight container migration technique in
    cloud computing Journal of King Saud University - Computer and Information Sciences,
    Volume 36, Issue 1, 2024, Article 101887 Gursharan Singh, …, Mustapha Hedabou
    View PDF Designing a heuristic computing structure to solve the human balancing
    model Journal of King Saud University - Computer and Information Sciences, Volume
    36, Issue 1, 2024, Article 101890 Najah AbuAli, Zulqurnain Sabir View PDF Personalized
    neural network-based aggregation function in multi-criteria collaborative filtering
    Journal of King Saud University - Computer and Information Sciences, Volume 36,
    Issue 1, 2024, Article 101922 Rita Rismala, …, Kridanto Surendro View PDF Show
    3 more articles Article Metrics Captures Readers: 4 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Journal of King Saud University - Computer and Information Sciences
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A blockchain-based scheme for edge–edge collaboration management in time-sensitive
    networking
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Thejasree P.
  - Manikandan N.
  - Vimal K.E.K.
  - Sivakumar K.
  - Krishnamachary P.C.
  citation_count: '0'
  description: Machine learning (ML) has emerged as a powerful tool in supply chain
    management (SCM), enabling organizations to accomplish valuable insights from
    numerous data and attain informed decisions. This paper presents an inclusive
    review of the recent advancements and applications of ML in SCM. The objective
    is to provide a holistic understanding of how ML techniques are being utilized
    to enhance various aspects of supply chain operations. The review begins by outlining
    the fundamental concepts of ML and its relevance to SCM. It then discusses the
    key challenges faced by supply chain professionals and how ML can address these
    challenges. The paper presents an overview of different ML techniques, including
    regression analysis, clustering, classification, time series analysis, neural
    networks, genetic algorithms, reinforcement learning, and ensemble methods, highlighting
    their specific applications in SCM. Furthermore, the review discusses the recent
    research trends and developments in the field, focusing on demand forecasting,
    inventory optimization, supplier selection and risk assessment, logistics optimization,
    supply chain risk management, and sustainability initiatives. The paper also explores
    the integration of ML with emergent technologies such as blockchain, IoT, and
    edge computing in the context of SCM. The findings of the review indicate that
    ML has demonstrated significant potential in improving decision-making, optimizing
    operations, enhancing supply chain resilience, and addressing sustainability challenges.
  doi: 10.1007/978-981-99-4819-2_6
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Industry 4.0 Technologies:
    Sustainable Manufacturing Supply Chains Chapter Applications of Machine Learning
    in Supply Chain Management—A Review Chapter First Online: 01 October 2023 pp 73–82
    Cite this chapter Access provided by University of Nebraska-Lincoln Download book
    PDF Download book EPUB Industry 4.0 Technologies: Sustainable Manufacturing Supply
    Chains P. Thejasree, N. Manikandan, K E K Vimal, K. Sivakumar & P. C. Krishnamachary  Part
    of the book series: Environmental Footprints and Eco-design of Products and Processes
    ((EFEPP)) 258 Accesses Abstract Machine learning (ML) has emerged as a powerful
    tool in supply chain management (SCM), enabling organizations to accomplish valuable
    insights from numerous data and attain informed decisions. This paper presents
    an inclusive review of the recent advancements and applications of ML in SCM.
    The objective is to provide a holistic understanding of how ML techniques are
    being utilized to enhance various aspects of supply chain operations. The review
    begins by outlining the fundamental concepts of ML and its relevance to SCM. It
    then discusses the key challenges faced by supply chain professionals and how
    ML can address these challenges. The paper presents an overview of different ML
    techniques, including regression analysis, clustering, classification, time series
    analysis, neural networks, genetic algorithms, reinforcement learning, and ensemble
    methods, highlighting their specific applications in SCM. Furthermore, the review
    discusses the recent research trends and developments in the field, focusing on
    demand forecasting, inventory optimization, supplier selection and risk assessment,
    logistics optimization, supply chain risk management, and sustainability initiatives.
    The paper also explores the integration of ML with emergent technologies such
    as blockchain, IoT, and edge computing in the context of SCM. The findings of
    the review indicate that ML has demonstrated significant potential in improving
    decision-making, optimizing operations, enhancing supply chain resilience, and
    addressing sustainability challenges. Access provided by University of Nebraska-Lincoln.
    Download chapter PDF Similar content being viewed by others Machine Learning and
    Supply Chain Management Chapter © 2023 Machine Learning and Supply Chain Management
    Chapter © 2024 Artificial Intelligence in Supply Chain Management: A Systematic
    Literature Review and Guidelines for Future Research Chapter © 2023 1 Introduction
    1.1 Machine Learning (ML) Techniques Machine learning techniques have gained significant
    attention and popularity in various fields because of their capability to analyze
    and interpret large volumes of intricate data [1, 2]. In recent years, these techniques
    have found widespread applications in supply chain management, revolutionizing
    traditional approaches to decision-making and optimization. Machine learning refers
    to the usage of algorithms which may absorb data and offer forecasts or decisions
    without being clearly programmed [3,4,5]. By leveraging statistical models and
    computational power, machine learning algorithms can recognize patterns, extract
    insights, and make accurate forecasts based on past and real-time data. The overview
    of machine learning methods has been depicted in Fig. 1 and the general system
    configuration of machine learning is illustrated in Fig. 2. Fig. 1 Overview of
    machine learning methods Full size image Fig. 2 Machine learning system configuration
    Full size image 1.2 Supply Chain Management (SCM) In the background of SCM, machine
    learning techniques offer immense potential for improving various aspects of operations,
    including demand forecasting, inventory management, logistics optimization, and
    risk assessment. These techniques enable supply chain professionals to analyze
    vast amounts of data from diverse sources, uncover hidden relationships, and make
    data-driven decisions [3]. One popular machine learning technique used in supply
    chain management is regression analysis, which aims to establish the correlation
    among dependent and independent parameters. Regression models can be adopted for
    sales forecasting, predicting demand patterns, and identifying influential factors
    that impact supply chain performance. Another widely used technique is clustering,
    which groups same kind of information composed based on their characteristics
    or attributes. Clustering algorithms have been adopted to segment customers, products,
    or suppliers, facilitating targeted marketing strategies, personalized recommendations,
    and efficient supplier management. Classification algorithms are also valuable
    in supply chain management, as they help categorize data into distinct classes
    or categories. For example, these algorithms can be used to classify products
    based on demand patterns, identify high-risk suppliers, or predict the likelihood
    of quality defects. Furthermore, time series analysis techniques, such as autoregressive
    integrated moving average (ARIMA) and long short-term memory (LSTM) models, are
    commonly used for forecasting future values based on past observations. These
    techniques are particularly effective in capturing temporal dependencies and seasonality
    patterns in supply chain data. Additionally, optimization algorithms, such as
    linear programming and genetic algorithms, are employed to solve complex supply
    chain optimization problems, such as production scheduling, inventory optimization,
    and route optimization [3,4,5,6]. The advent of big data and advancements in computing
    power has further fueled the usage of “ML” techniques in supply chain management.
    With the ability to process and analyze vast amounts of structured and unstructured
    data, machine learning algorithms can generate actionable insights, enhance decision-making
    processes, and enhance overall supply chain performance. Hence, machine learning
    techniques have emerged as powerful tools in supply chain management, enabling
    organizations to extract valuable insights from data, optimize operations, and
    gain a competitive edge. By leveraging these techniques, supply chain professionals
    can make more informed decisions, enhance efficiency, and adapt to dynamic market
    conditions. As the field of machine learning continues to advance, its applications
    in supply chain management are expected to evolve and drive further innovation
    in the industry [5,6,7,8]. 2 Challenges in SCM Supply chain professionals face
    various challenges in managing complex and dynamic supply chains such as demand
    volatility in which fluctuations in customer demand make it challenging to accurately
    forecast and plan inventory levels, leading to either excess inventory or stockouts
    [9]. Also, the lack of visibility across the supply chain hampers the ability
    to track inventory, monitor supplier performance, and identify bottlenecks or
    disruptions. Modern supply chains involve multiple tiers of suppliers, diverse
    product portfolios, global networks, and intricate logistics. Managing this complexity
    requires effective coordination and collaboration [10,11,12]. Balancing the level
    of inventories to fulfill the need of customers when minimalizing the cost of
    holding and stockouts is becoming one of the complex optimization problems during
    SCM activities. Identifying reliable suppliers, managing relationships, and assessing
    supplier performance is crucial for ensuring quality, timely delivery, and cost-effectiveness
    [13, 14]. Efficiently managing transportation, optimizing routes, and coordinating
    deliveries across various modes of transport can be challenging, especially with
    changing customer expectations and dynamic market conditions. In addition to natural
    disasters and political and geopolitical factors, supply chains are also vulnerable
    to other risks, such as supplier disruptions and quality issues. To ensure continuity,
    proper management and mitigation of these risks is essential. Supply chains generate
    massive amounts of data, including sales data, production data, logistics data,
    and external data. Extracting meaningful insights from this data requires robust
    data management and advanced analytics capabilities. Increasingly, supply chain
    professionals need to address sustainability goals, ethical sourcing, and environmental
    impacts. Balancing economic objectives with social and environmental responsibilities
    is a challenge. Emergent techniques such as AI, ML, blockchain, and IoT offer
    opportunities for innovation but require a deep understanding and strategic adoption
    to fully leverage their potential. Addressing these challenges requires supply
    chain professionals to embrace digital transformation, adopt advanced analytics
    and decision-support tools, foster collaboration with partners, and continuously
    adapt strategies to align with changing market dynamics [11,12,13,14,15]. 3 Machine
    Leaning and Supply Chain Management 3.1 Need of ML in SCM Machine learning (ML)
    plays a crucial role in addressing the complex challenges faced by SCM in the
    present days dynamic business environment [16,17,18]. Some key reasons highlighting
    the need for ML in SCM are: Handling Big Data: SCM generates vast amounts of information
    from different bases such as sales transactions, production records, level of
    inventories, client feedback, and external factors. ML techniques are capable
    of processing and analyzing this big data, uncovering patterns, and extracting
    actionable insights for better decision-making. Demand Forecasting: Demand forecasting
    is an essential part of any organization’s supply chain management strategy to
    ensure that it is able to achieve optimal inventory levels and production planning.
    ML algorithms can analyze historical sales data and identify trends, seasonality,
    and other factors influencing demand patterns, leading to more accurate forecasts
    and reduced forecasting errors. Inventory Optimization: Efficient inventory management
    is critical to balancing costs and customer satisfaction. ML techniques can analyze
    historical data, customer behavior, market trends, and other factors to optimize
    inventory levels, reducing stockouts and excess inventory while maximizing service
    levels. Supply Chain Risk Management: Various risks can affect the operations
    of a supply chain, such as natural calamities, supplier disagreements, and market
    shifts. ML models can assess historical risk data, external data sources, and
    real-time information to identify potential risks, predict their impact, and enable
    proactive risk mitigation strategies. Enhancing Efficiency: ML algorithms can
    automate and streamline routine SCM tasks, such as demand planning, order fulfillment,
    and logistics optimization. By reducing manual effort and human error, ML improves
    process efficiency, reduces lead times, and enhances overall supply chain performance.
    Real-time Decision-Making: SCM requires real-time decision-making to respond to
    changing market conditions, customer demands, and unforeseen events. ML techniques
    enable organizations to process and analyze real-time data streams, providing
    actionable insights and supporting agile decision-making for effective supply
    chain management. Customer Relationship Management: ML algorithms can analyze
    customer data, including preferences, buying behavior, and sentiment analysis
    from social media, to enhance customer relationship management. This enables targeted
    marketing campaigns, personalized recommendations, and improved customer satisfaction.
    Supply Chain Network Optimization: ML algorithms can optimize the design and configuration
    of the supply chain network by considering factors such as customer demand, transportation
    costs, lead times, and supplier capabilities. This leads to optimized sourcing,
    distribution, and production strategies, improving overall supply chain efficiency.
    Continuous Improvement: ML models can continuously learn and adapt to changing
    patterns and conditions in the supply chain. By analyzing feedback, monitoring
    performance, and identifying areas of improvement, ML enables continuous optimization
    and drives supply chain innovation. The need for ML in SCM arises from the complexity,
    scale, and dynamic nature of supply chain operations. ML techniques enable organizations
    to leverage data-driven insights, optimize processes, mitigate risks, and enhance
    customer satisfaction, ultimately consequences to competent benefits in the marketplace.
    3.2 Various ML Techniques Available for SCM There are several ML techniques that
    can be applied to SCM to improve decision-making, optimize operations, and enhance
    overall performance [19,20,21,22]. Here are some commonly used ML techniques in
    SCM: Regression Analysis: Regression models, such as linear regression and polynomial
    regression, are used for predicting and modeling relationships between dependent
    and independent variables. In SCM, regression analysis can be applied to forecast
    demand, examine the influence of factors on supply chain performance, and optimize
    inventory levels. Clustering: The algorithms, such as k-means clustering and hierarchical
    clustering, cluster same kind of data points composed as per their characteristics
    or attributes. In SCM, clustering can be used for customer segmentation, product
    categorization, and supplier segmentation, enabling targeted strategies and personalized
    approaches. Classification: It is including decision trees, random forests, and
    support vector machines (SVM), which are adopted to classify data into distinct
    classes. In SCM, classification can be applied to supplier evaluation and selection,
    quality control, and risk assessment, enabling effective decision-making and risk
    mitigation. Time Series Analysis: Techniques such as autoregressive integrated
    moving average (ARIMA) and exponential smoothing, are used for forecasting future
    values based on past observations. In SCM, time series analysis is valuable for
    demand forecasting, inventory optimization, and production planning. Neural Networks:
    Neural networks, including feedforward neural networks, recurrent neural networks
    (RNN), and long short-term memory (LSTM) networks, are used for complex pattern
    recognition and sequence modeling. In SCM, neural networks can be applied to demand
    forecasting, anomaly detection, and optimization problems, capturing nonlinear
    relationships and temporal dependencies in the data. Genetic Algorithms: Genetic
    algorithms are optimization techniques enthused by the process of natural selection.
    These algorithms have been adopted to solve complex optimization problems in SCM,
    such as facility location, vehicle routing, and production scheduling, considering
    multiple constraints and objectives. Reinforcement Learning: Reinforcement learning
    algorithms learn optimal decision-making policies by interacting with the environment
    and receiving rewards or penalties. In SCM, reinforcement learning can be applied
    to optimize supply chain operations, such as inventory control, order fulfillment,
    and routing, by dynamically adapting to changing conditions. Support Vector Machines
    (SVM): This algorithm is supervised and can perform regression analysis and classification.
    In SCM, SVM can be applied to demand forecasting, anomaly detection, and supplier
    evaluation, providing accurate predictions and insights based on historical and
    real-time data. Ensemble Methods: Gradient boosting and random forests are examples
    of ensembles that combine multiple ML models for improved accuracy and reduced
    overfitting. In SCM, ensemble methods can be used for demand forecasting, inventory
    optimization, and risk analysis, leveraging the strengths of multiple models.
    These are just a few examples of ML techniques that can be applied to SCM. The
    selection of technique is contingent on the specific problem, available data,
    and desired outcomes. Often, a combination of multiple techniques and algorithms
    is employed to tackle the complexities and challenges of supply chain management
    effectively. 4 Recent Applications of ML in SCM Recent uses of ML in SCM have
    shown promising results in various areas. Here are some recent applications of
    ML and notable research in the field of SCM [22,23,24,25,26]: Demand Forecasting:
    ML techniques, namely neural networks and deep learning models, were applied to
    improve demand forecasting accuracy. Researchers have explored the use of recurrent
    neural networks (RNNs) and LSTM models to record temporal dependencies and seasonality
    patterns in demand data. Inventory Optimization: ML algorithms have been employed
    to optimize inventory levels by considering demand patterns, lead times, and other
    factors. Reinforcement learning approaches have been used to dynamically adjust
    inventory policies based on real-time demand and supply information. Supplier
    Selection and Risk Assessment: ML techniques have been utilized for supplier evaluation
    and risk assessment. Researchers have developed models that integrate various
    data sources, including supplier performance data, financial indicators, and external
    data, to identify reliable suppliers and assess their risk levels. Logistics Optimization:
    ML algorithms were adopted to optimize logistics operations, such as route planning,
    vehicle scheduling, and load balancing. Techniques like genetic algorithms and
    ant colony optimization were used to find optimal solutions considering multiple
    constraints and objectives. Predictive Maintenance: ML models have been utilized
    for predictive maintenance in SCM to improve asset reliability and decrease downtime.
    With the aid of analysis of sensor data, past records, and remaining information,
    predictive models can detect equipment failures in advance and schedule maintenance
    activities accordingly. Supply Chain Risk Management: ML techniques have been
    used for risk identification, assessment, and mitigation in supply chains. Researchers
    have explored anomaly detection algorithms, such as LSTM autoencoders and support
    vector machines, to identify abnormal patterns in supply chain data and detect
    potential risks. Blockchain and ML Integration: The integration of blockchain
    technology with ML has been studied for enhancing supply chain transparency, traceability,
    and security. Researchers have explored the use of ML algorithms for data analysis
    in blockchain-enabled supply chains, enabling efficient data management and decision-making.
    Sustainability and Green SCM: ML techniques have been applied to support sustainability
    initiatives in SCM. Researchers have developed models to analyze data related
    to carbon emissions, energy consumption, and environmental impact, enabling organizations
    to optimize their supply chain operations while minimizing their environmental
    footprint. Notable exploration in the field of ML applications in SCM includes
    studies on ensemble models for demand forecasting, hybrid optimization algorithms
    for supply chain network design, explainable AI for decision support in SCM, and
    the integration of ML with emerging technologies like Internet of Things (IoT)
    and edge computing for real-time analytics in SCM. Overall, the recent research
    in SCM focuses on leveraging ML techniques to improve decision-making, optimize
    operations, enhance supply chain resilience, and address sustainability challenges.
    These advancements contribute to the development of more intelligent, data-driven,
    and efficient supply chain management practices. 5 Summary Machine learning (ML)
    has attained significant consideration and application in supply chain management
    (SCM) in recent days. It has proved to be an appreciated tool in addressing the
    complexities and challenges of SCM by providing data-driven insights, optimizing
    operations, and enhancing decision-making processes. Recent applications of ML
    in SCM have focused on various areas. Demand forecasting has been improved through
    the use of neural networks and deep learning models that can capture temporal
    dependencies and seasonality patterns in demand data. Inventory optimization has
    benefited from ML algorithms that consider demand patterns, lead times, and other
    factors to optimize inventory levels. ML techniques have also been applied to
    supplier selection and risk assessment, logistics optimization, predictive maintenance,
    supply chain risk management, blockchain integration, and sustainability initiatives
    in SCM. Notable research in the field has explored ensemble models for demand
    forecasting, hybrid optimization algorithms for supply chain network design, explainable
    AI for decision support, and the integration of ML with emerging technologies
    like IoT and edge computing. These research efforts aim to improve decision-making,
    optimize operations, enhance supply chain resilience, and address sustainability
    challenges in SCM. Overall, the recent advancements in ML applications in SCM
    hold great potential for transforming supply chain operations, improving efficiency,
    reducing costs, mitigating risks, and driving sustainable practices. The integration
    of ML techniques with SCM practices is expected to continue growing, leading to
    further innovations and advancements in the field. References Ni D, Xiao Z, Lim
    MK (2020) A systematic review of the research trends of machine learning in supply
    chain management. Int J Mach Learn Cybern 11:1463–1482 Article   Google Scholar   Bousqaoui
    H, Achchab S, Tikito K (2017, October) Machine learning applications in supply
    chains: an emphasis on neural network applications. In: 2017 3rd international
    conference of cloud computing technologies and applications (CloudTech). IEEE,
    pp 1–7 Google Scholar   Seif G (2018) The 5 clustering algorithms data scientists
    need to know. Towards Data Science Google Scholar   Du CJ, Sun DW (2006) Learning
    techniques used in computer vision for food quality evaluation: a review. J Food
    Eng 72(1):39–55 Article   Google Scholar   Wenzel H, Smit D, Sardesai S (2019)
    A literature review on machine learning in supply chain management. In: Artificial
    intelligence and digital transformation in supply chain management: innovative
    approaches for supply chains. Proceedings of the Hamburg international conference
    of logistics (HICL), vol 27. Epubli GmbH, Berlin, pp 413–441 Google Scholar   Akbari
    M, Do TNA (2021) A systematic review of machine learning in logistics and supply
    chain management: current trends and future directions. Benchmarking: Int J 28(10):2977–3005
    Google Scholar   Hu H, Xu J, Liu M, Lim MK (2023) Vaccine supply chain management:
    an intelligent system utilizing blockchain, IoT and machine learning. J Bus Res
    156:113480 Article   Google Scholar   Lin H, Lin J, Wang F (2022) An innovative
    machine learning model for supply chain management. J Innov Knowl 7(4):100276
    Article   MathSciNet   Google Scholar   Tirkolaee EB, Sadeghi S, Mooseloo FM,
    Vandchali HR, Aeini S (2021) Application of machine learning in supply chain management:
    a comprehensive overview of the main areas. Math Probl Eng 2021:1–14 Article   Google
    Scholar   Ghazal TM, Alzoubi HM (2022) Fusion-based supply chain collaboration
    using machine learning techniques. Intell Autom Soft Comput 31(3):1671–1687 Article   Google
    Scholar   Kohli S, Godwin GT, Urolagin S (2021) Sales prediction using linear
    and KNN regression. In: Advances in machine learning and computational intelligence:
    proceedings of ICMLCI 2019. Springer Singapore, pp 321–329 Google Scholar   Shilong
    Z (2021, January) Machine learning model for sales forecasting by using XGBoost.
    In: 2021 IEEE international conference on consumer electronics and computer engineering
    (ICCECE). IEEE, pp 480–483 Google Scholar   Park KJ (2021) Determining the tiers
    of a supply chain using machine learning algorithms. Symmetry 13(10):1934 Article   Google
    Scholar   Islam S, Amin SH (2020) Prediction of probable backorder scenarios in
    the supply chain using distributed random forest and gradient boosting machine
    learning techniques. J Big Data 7:1–22 Article   Google Scholar   Vairagade N,
    Logofatu D, Leon F, Muharemi F (2019) Demand forecasting using random forest and
    artificial neural network for supply chain management. In: Computational collective
    intelligence: 11th international conference, ICCCI 2019, Hendaye, France, September
    4–6, 2019, Proceedings, Part I, vol 11. Springer International Publishing, pp
    328–339 Google Scholar   Ali MR, Nipu SMA, Khan SA (2023) A decision support system
    for classifying supplier selection criteria using machine learning and random
    forest approach. Decis. Anal. J 100238 Google Scholar   Raza SA, Govindaluri SM,
    Bhutta MK (2023) Research themes in machine learning applications in supply chain
    management using bibliometric analysis tools. Benchmarking: Int J 30(3):834–867
    Google Scholar   Ding S, Cui T, Wu X, Du M (2022) Supply chain management based
    on volatility clustering: the effect of CBDC volatility. Res Int Bus Financ 62:101690
    Article   Google Scholar   De Lucia C, Pazienza P, Bartlett M (2020) Does good
    ESG lead to better financial performances by firms? Machine learning and logistic
    regression models of public enterprises in Europe. Sustainability 12(13):5317
    Article   Google Scholar   Li L (2022) Predicting the investment risk in supply
    chain management using BPNN and machine learning. Wirel Commun Mob Comput 2022
    Google Scholar   Sinha GK (2022) Relationship between sustainable logistics practices
    and the organization’s performance in automobile industry—an empirical study with
    logistic regression machine learning. Int J Mech Eng 7(1) Google Scholar   Nguyen
    HD, Tran KP, Thomassey S, Hamad M (2021) Forecasting and anomaly detection approaches
    using LSTM and LSTM Autoencoder techniques with the applications in supply chain
    management. Int J Inf Manag 57:102282 Article   Google Scholar   Weng T, Liu W,
    Xiao J (2020) Supply chain sales forecasting based on lightGBM and LSTM combination
    model. Ind Manag Data Syst 120(2):265–279 Article   Google Scholar   Bousqaoui
    H, Achchab S, Tikito K (2019) Machine learning applications in supply chains:
    Long short-term memory for demand forecasting. In: Cloud computing and big data:
    technologies, applications and security, vol 3. Springer International Publishing,
    pp 301–317 Google Scholar   Yani LPE, Priyatna IMA, Aamer AM (2019) Exploring
    machine learning applications in supply chain management. In: 9th international
    conference on operations and supply chain management, pp 161–169 Google Scholar   Carbonneau
    R, Vahidov R, Laframboise K (2007) Machine learning-Based demand forecasting in
    supply chains. Int J Intell Inf Technol (IJIIT) 3(4):40–57 Article   MATH   Google
    Scholar   Download references Author information Authors and Affiliations Department
    of Mechanical Engineering, School of Engineering and Technology, Mohan Babu University
    (MBU), Tirupati, Andhra Pradesh, India P. Thejasree & N. Manikandan Department
    of Production Engineering, National Institute of Technology, Tiruchirappalli,
    Tamil Nadu, India K E K Vimal Centre for Logistics and Supply Chain Management,
    Loyola Institute of Business Administration, Loyola College Campus, Chennai, TN,
    India K. Sivakumar J.B. Institute of Engineering and Technology, Hyderabad, Telangana,
    India P. C. Krishnamachary Corresponding author Correspondence to N. Manikandan
    . Editor information Editors and Affiliations Department of Production Engineering,
    National Institute of Technology Tiruchirapalli, Trichy, India K E K Vimal Department
    of Mechanical Engineering, National Institute of Technology Patna, Patna, India
    Sonu Rajak Faculty of Business, Law and Social Sciences, Birmingham City University,
    Birmingham, UK Vikas Kumar Business Systems and Operations, Faculty of Business
    and Law, University of Northampton, Northampton, UK Rahul S. Mor Water, Environment
    and Climate Change Centre, Royal Scientific Society, Amman, Jordan Almoayied Assayed
    Rights and permissions Reprints and permissions Copyright information © 2024 The
    Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. About
    this chapter Cite this chapter Thejasree, P., Manikandan, N., Vimal, K.E.K., Sivakumar,
    K., Krishnamachary, P.C. (2024). Applications of Machine Learning in Supply Chain
    Management—A Review. In: Vimal, K.E.K., Rajak, S., Kumar, V., Mor, R.S., Assayed,
    A. (eds) Industry 4.0 Technologies: Sustainable Manufacturing Supply Chains. Environmental
    Footprints and Eco-design of Products and Processes. Springer, Singapore. https://doi.org/10.1007/978-981-99-4819-2_6
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-981-99-4819-2_6
    Published 01 October 2023 Publisher Name Springer, Singapore Print ISBN 978-981-99-4818-5
    Online ISBN 978-981-99-4819-2 eBook Packages Engineering Engineering (R0) Share
    this chapter Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Sections Figures References Abstract
    Introduction Challenges in SCM Machine Leaning and Supply Chain Management Recent
    Applications of ML in SCM Summary References Author information Editor information
    Rights and permissions Copyright information About this chapter Publish with us
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Environmental Footprints and Eco-Design of Products and Processes
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Applications of Machine Learning in Supply Chain Management—A Review
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ali J.
  - Shan G.
  - Gul N.
  - Roh B.h.
  citation_count: '2'
  description: The frequency of link failures in Internet-of-Things (IoT) network
    are more than the node failures. Hence, effective link recovery schemes are required
    for a seamless communication in the IoT. In contrast to traditional networking,
    the IoT is configured through software-defined networking premise due to its intelligent
    programmable controller, and orchestration features. SD-IoT with edge computing
    can help organizations to create a highly flexible and agile IT infrastructure
    that can adapt quickly to changing business requirements. By automating network
    operations, optimizing network performance, and improving application availability,
    SD-IoT can help to improve the efficiency and effectiveness of edge computing
    environments. Moreover, the security is also a prime issue in the IoT, and DDos
    attacks are typical threats concerning security in IoT. To combat with these challenges,
    in this paper we propose a reactive recovery strategy for link failures by proposing
    a TOPSIS module employing multi-objective-decision making in the intelligent SDN
    controller, which selects the alternative path for the failed link on the end-to-end
    (E2E) path in a SD-IoT considering multiple criteria instead of the shortest path
    only as proposed by the conventional methods for failed link recovery. Moreover,
    DDoS attacks detection as well as mitigation mechanism based on blockchain in
    SD-IoT with machine learning is proposed. The blockchain guarantee the security
    for IoT system. The results demonstrated and validated in Mininet with real SDN
    controller as well as proof-of-concept experiments in real topologies for IoT
    shows the effectiveness of the proposed method. Our proposed strategy surpass
    the traditional schemes in terms of network throughput, recovery delay, and packet
    delivery ratio.
  doi: 10.1007/s10723-023-09693-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Grid Computing Article
    An Intelligent Blockchain-based Secure Link Failure Recovery Framework for Software-defined
    Internet-of-Things Research Published: 20 October 2023 Volume 21, article number
    57, (2023) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Journal of Grid Computing Aims and scope Submit manuscript Jehad Ali, Gaoyang
    Shan, Noor Gul & Byeong-hee Roh  224 Accesses 3 Citations Explore all metrics
    Abstract The frequency of link failures in Internet-of-Things (IoT) network are
    more than the node failures. Hence, effective link recovery schemes are required
    for a seamless communication in the IoT. In contrast to traditional networking,
    the IoT is configured through software-defined networking premise due to its intelligent
    programmable controller, and orchestration features. SD-IoT with edge computing
    can help organizations to create a highly flexible and agile IT infrastructure
    that can adapt quickly to changing business requirements. By automating network
    operations, optimizing network performance, and improving application availability,
    SD-IoT can help to improve the efficiency and effectiveness of edge computing
    environments. Moreover, the security is also a prime issue in the IoT, and DDos
    attacks are typical threats concerning security in IoT. To combat with these challenges,
    in this paper we propose a reactive recovery strategy for link failures by proposing
    a TOPSIS module employing multi-objective-decision making in the intelligent SDN
    controller, which selects the alternative path for the failed link on the end-to-end
    (E2E) path in a SD-IoT considering multiple criteria instead of the shortest path
    only as proposed by the conventional methods for failed link recovery. Moreover,
    DDoS attacks detection as well as mitigation mechanism based on blockchain in
    SD-IoT with machine learning is proposed. The blockchain guarantee the security
    for IoT system. The results demonstrated and validated in Mininet with real SDN
    controller as well as proof-of-concept experiments in real topologies for IoT
    shows the effectiveness of the proposed method. Our proposed strategy surpass
    the traditional schemes in terms of network throughput, recovery delay, and packet
    delivery ratio. Article PDF Similar content being viewed by others A Blockchain-Based
    Security Traffic Measurement Approach to Software Defined Networking Article 15
    January 2020 A Security Traffic Measurement Approach in SDN-Based Internet of
    Things Chapter © 2019 Securing of Software-Defined Networking (SDN) from DDoS
    Attack Using a Blockchain Chapter © 2021 References Kianpisheh, S., Taleb, T.:
    A survey on in-network computing: Programmable data plane And technology specific
    applications. IEEE Communications Surveys & Tutorials (2022) Anerousis, N., Chemouil,
    P., Lazar, A.A., Mihai, N., Weinstein, S.B.: The origin and evolution of open
    programmable networks and SDN. IEEE Commun. Surv. Tutor. 23(3), 1956–1971 (2021)
    Article   Google Scholar   Sarmiento, D., Lebre, A., Nussbaum, L., Chari, A.:
    Decentralized SDN Control Plane for a Distributed Cloud-Edge Infrastructure: A
    Survey. IEEE Commun. Surv. Tutor. 1–1 (2021). https://doi.org/10.1109/COMST.2021.3050297
    Dubey, K., Sharma, S.C., Kumar, M.: A secure IoT applications allocation framework
    for integrated fog-cloud environment. J. Grid. Comput. 20, 5 (2022). https://doi.org/10.1007/s10723-021-09591-x
    Article   Google Scholar   Snehi, Jyoti, Snehi, Manish, Prasad, Devendra, Simaiya,
    Sarita, Kansal, Isha, Baggan, Vidhu: SDN-Based Cloud Combining Edge Computing
    for IoT Infrastructure, pp. 497–540. Architecture and Applications, Software Defined
    Networks (2022) Ali, J., Lee, G.M., Roh, B.H., Ryu, D.K., Park, G.: Software-defined
    networking approaches for link failure recovery: A survey. Sustainability 12(10),
    4255 (2020) Article   Google Scholar   Ali, J., Jhaveri, R.H., Alswailim, M.,
    Roh, B.H.: ESCALB: An effective slave controller allocation-based load balancing
    scheme for multi-domain SDN-enabled-IoT networks. J. King Saud Univ. Comput. Inf.
    Sci. 35(6), 101566 (2023) Ren, Q., et al.: SDN-ESRC: A secure and resilient control
    plane for software-defined networks. IEEE Trans. Netw. Serv. Manage. 19(3), 2366–2381
    (2022). https://doi.org/10.1109/TNSM.2022.3163198 Article   Google Scholar   Yinbo,
    Yu., Li, Xing, Leng, Xue, Song, Libin, Kai, Bu., Chen, Yan, Yang, Jianfeng, Zhang,
    Liang, Cheng, Kang, Xiao, Xin: Fault management in software-defined networking:
    a survey. IEEE Commun. Surv. Tutor. 21(1), 349–392 (2019) Gill, P., Jain, N.,
    Nagappan, J.: Understanding Network Failures in Data Centers: Measurement, Analysis,
    and Implications. In ACM SIGCOMM Computer Communication Review; ACM: Toronto,
    ON, Canada, Volume 41, pp. 350–361 (2011) Heidari, A., Jabraeil Jamali, M.A.:
    Internet of Things intrusion detection systems: A comprehensive review and future
    directions. Clust. Comput. 1–28 (2022) Makhdoom, I., Abolhasan, M., Lipman, J.,
    Liu, R.P., Ni, W.: Anatomy of threats to the internet of things. IEEE Commun.
    Surv. Tutor. 21, 1636–1675 (2018) Article   Google Scholar   Al-Hadhrami, Y.,
    Hussain, F.K.: DDoS attacks in IoT networks: A comprehensive systematic literature
    review. World Wide Web 24, 971–1001 (2021) Article   Google Scholar   Dubey, K.,
    Sharma, S.C.: An extended intelligent water drop approach for efficient VM allocation
    in secure cloud computing framework. J. King Saud Univ. Comput. Inf. Sci. 34(7),
    3948–3958 (2022) Jmal, R., Ghabri, W., Guesmi, R., Alshammari, B.M., Alshammari,
    A.S., Alsaif, H.: Distributed blockchain-SDN secure IoT system based on ANN to
    mitigate DDoS attacks. Appl. Sci. 13(8), 4953 (2023) Article   Google Scholar   Bakhshi
    Kiadehi, K., Rahmani, A.M., Sabbagh Molahosseini, A.: A fault-tolerant architecture
    for internet-of-things based on software-defined networks. Telecommun. Syst. 77(1),
    155–169 (2021) Article   Google Scholar   Hu, T., Yi, P., Lan, J., Hu, Y., Sun,
    P.: FTLink: Efficient and flexible link fault tolerance scheme for data plane
    in Software-Defined Networking. Futur. Gener. Comput. Syst. 111, 381–400 (2020)
    Article   Google Scholar   Liang, D., Liu, Q., Yan, B., Hu, Y., Zhao, B., Hu,
    T.: Low interruption ratio link fault recovery scheme for data plane in software-defined
    networks. Peer-to-Peer Netw. Appl. 14(6), 3806–3819 (2021) Article   Google Scholar   Thorat,
    P., Challa, R., Raza, S.M., Kim, D.S., Choo, H.: Proactive failure recovery scheme
    for data traffic in software defined networks. IEEE NetSoft Conf. Workshops (NetSoft)
    2016, 219–225 (2016). https://doi.org/10.1109/NETSOFT.2016.7502416 Article   Google
    Scholar   Yan, B., Liu, Q., Shen, J., Liang, D.: BatchUp: Achieve fast TCAM update
    with batch processing optimization in SDN. Futur. Gener. Comput. Syst. 134, 93–106
    (2022) Article   Google Scholar   Ali, J., Roh, B.H.: An effective hierarchical
    control plane for software-defined networks leveraging TOPSIS for end-to-end QoS
    class-mapping. IEEE Access 8, 88990–89006 (2020) Article   Google Scholar   Malik,
    A., Aziz, B., Adda, M., Ke, C.H.: Optimisation methods for fast restoration of
    software-defined networks. IEEE Access 5, 16111–16123 (2017) Article   Google
    Scholar   Zhang, X., Hou, W., Guo, L., Wang, S., Sun, Y., Yang, X.: Failure recovery
    solutions using cognitive mechanisms for Software-defned optical Networks. In
    15th International Conference on Optical Communications and Networks (ICOCN) (2016)
    Zheng, L., Xu, H., Chen S., Huang, L.: Performance guaranteed single link failure
    recovery in SDN overlay networks. In 2020 IEEE 26th International Conference on
    Parallel and Distributed Systems (ICPADS), Hong Kong, pp. 703–708 (2020 ) Yamansavascilar,
    B., Baktir, A.C., Ozgovde, A., Ersoy, C.: Fault tolerance in SDN data plane considering
    network and application based metrics. J. Netw. Comput. Appl. 170, 102780 (2020)
    Article   Google Scholar   Li, Z., Hu, Y., Wu, J., Lu, J.: P4Resilience: Scalable
    resilience for multi-failure recovery in SDN with programmable data plane. Comput.
    Netw. 208, 108896 (2022) Article   Google Scholar   Miura, H., Hirata, K., Tachibana,
    T.: P4-based design of fast failure recovery for software-defined networks. Comput.
    Netw. 216, 109274 (2022) Article   Google Scholar   Li, Q., Liu, Y., Zhu, Z.,
    Li, H., Jiang, Y.: BOND: Flexible failure recovery in software defined networks.
    Comput. Netw. 149, 1–12 (2019) Article   Google Scholar   Komajwar, S., Korkmaz,
    T.: SPRM: Source path routing model and link failure handling in Ssoftware-defined
    networks. IEEE Trans. Netw. Serv. Manage. 18(3), 2873–2887 (2021) Article   Google
    Scholar   Dubey, K., Sharma, S.C.: A novel multi-objective CR-PSO task scheduling
    algorithm with deadline constraint in cloud computing. Sustain. Comput. Inform.
    Syst. 32, 100605 (2021) Google Scholar   Cascone, C., Sanvito, D., Pollini, L.,
    Capone, A., Sanso, B.: Fast failure detection and recovery in SDN with stateful
    data plane. Int. J. Network Manage 27(2), e1957 (2017) Article   Google Scholar   Vasan,
    K.K., Surendiran, B.: Dimensionality reduction using Principal Component Analysis
    for network intrusion detection. Perspect. Sci. 8, 510–512 (2016) Article   Google
    Scholar   de Oliveira, R.L.S., Schweitzer, C.M., Shinoda, A.A., Prete, L.R.: Using
    mininet for emulation and prototyping software-defined networks. In Proc. IEEE
    Colombian Conf. Commun. Comput. (COLCOM), pp. 1–6 (2014) Ali, J., Roh, B.H.: A
    novel scheme for controller selection in software-defined internet-of-things (SD-IoT).
    Sensors 22(9), 3591 (2022) Article   Google Scholar   Sarica, A.K., Angin, P.:
    A novel sdn dataset for intrusion detection in iot networks. In Proceedings of
    the 2020 16th International Conference on Network and Service Management (CNSM),
    Izmir, Turkey, 2-6 November 2020; pp. 1–5 Download references Acknowledgements
    This work was supported partially by the MSIT (Ministry of Science and ICT), Korea,
    under the ITRC (Information Technology Research Center) support program (IITP-2023-2018-0-01431)
    supervised by the IITP (Institute for Information & Communications Technology
    Planning & Evaluation). Author information Authors and Affiliations Department
    of AI Convergence Network, Ajou University, Suwon, 16499, South Korea Jehad Ali
    & Byeong-hee Roh Dept. Software and Computer Eng., Ajou University, Suwon, 16499,
    South Korea Gaoyang Shan Department of electronics, University of Peshawar, Peshawar,
    KPK, 25120, Pakistan Noor Gul Corresponding author Correspondence to Byeong-hee
    Roh. Ethics declarations Conflict of Interest The authors declare that they have
    no conflict of interest. Additional information Publisher''s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society
    or other partner) holds exclusive rights to this article under a publishing agreement
    with the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Ali, J., Shan, G., Gul, N. et al. An Intelligent Blockchain-based
    Secure Link Failure Recovery Framework for Software-defined Internet-of-Things.
    J Grid Computing 21, 57 (2023). https://doi.org/10.1007/s10723-023-09693-8 Download
    citation Received 13 July 2023 Accepted 26 September 2023 Published 20 October
    2023 DOI https://doi.org/10.1007/s10723-023-09693-8 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Software-defined networking Intelligent networking Blockchain ANN multi-criteria-decision-making
    fault tolerance edge computing IoT network PCA Use our pre-submission checklist
    Avoid common mistakes on your manuscript. Associated Content Part of a collection:
    Artificial Intelligence (AI)-enabled Blockchain for the Edge of Things (EoT) Sections
    References Abstract Article PDF References Acknowledgements Author information
    Ethics declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Grid Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An Intelligent Blockchain-based Secure Link Failure Recovery Framework for
    Software-defined Internet-of-Things
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Bakhshi Z.
  - Rodriguez-Navas G.
  - Hansson H.
  citation_count: '0'
  description: In this paper, we analyze the scalability and performance of a persistent,
    fault-tolerant storage approach that provides data availability and consistency
    in a distributed container-based architecture with intended use in industrial
    control applications. We use simulation to evaluate the performance of this storage
    system in terms of scalability and failures. As the industrial applications considered
    have timing constraints, the simulation results show that for certain failure
    patterns, it is possible to determine whether the storage solution can meet critical
    deadlines. The presented approach is applicable for evaluating timing constraints
    also of other container-based critical applications that require persistent storage.
  doi: 10.1016/j.sysarc.2023.103004
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Persistent fault-tolerant storage
    for container-based fog architectures 3. Identification of response-time 4. Our
    evaluation strategy 5. Simulator 6. Evaluation 7. Discussion 8. Related work 9.
    Conclusion Declaration of competing interest Acknowledgments References Vitae
    Show full outline Cited by (1) Figures (14) Show 8 more figures Tables (4) Table
    1 Table 2 Table 3 Table 4 Journal of Systems Architecture Volume 144, November
    2023, 103004 Analyzing the performance of persistent storage for fault-tolerant
    stateful fog applications Author links open overlay panel Zeinab Bakhshi a, Guillermo
    Rodriguez-Navas b, Hans Hansson a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.sysarc.2023.103004
    Get rights and content Under a Creative Commons license open access Abstract In
    this paper, we analyze the scalability and performance of a persistent, fault-tolerant
    storage approach that provides data availability and consistency in a distributed
    container-based architecture with intended use in industrial control applications.
    We use simulation to evaluate the performance of this storage system in terms
    of scalability and failures. As the industrial applications considered have timing
    constraints, the simulation results show that for certain failure patterns, it
    is possible to determine whether the storage solution can meet critical deadlines.
    The presented approach is applicable for evaluating timing constraints also of
    other container-based critical applications that require persistent storage. Previous
    article in issue Next article in issue Keywords Container-based architecturesFault-tolerant
    designFog computingIndustrial automationPersistent storageStatful applications
    1. Introduction To deploy stateful applications with cloud-native and container-based
    solutions, a persistent storage system is required [1], [2], [3], [4]. For industrial
    control applications that have safety-critical timing requirements, a cloud-native
    solution may not provide sufficient predictability. As a remedy, the storage solution
    could be implemented at the lower system layers, such as the fog and edge layers
    to ensure that applications can access data within guaranteed time. We recently
    published such a solution, with proper support for data consistency [5]. This
    solution provides a distributed data storage called Replicated Data Structure
    (RDS), which is instantiated locally in each node of a cluster of nodes, and a
    dedicated storage application called Storage Container (SC) that implements data
    synchronization and consistency services over the RDSs (See Fig. 1). The coordinated
    operation of the SCs using the RAFT consistency protocol [6] has been formally
    verified to enforce eventual consistency over the RDS even in the presence of
    failures [7]. Download : Download high-res image (158KB) Download : Download full-size
    image Fig. 1. High-level architecture of the persistent storage solution. Our
    goal in this paper is to determine if this solution, considering that it adds
    a new layer to the data transfer and storage system, can scale and meet the strict
    timing requirements of robotics and other automation applications. Specifically,
    since this solution supports auto-recovery, it is vital to evaluate the timing
    aspects upon failures that trigger restarting and re-integration of components,
    as they incur additional delays on the synchronization services managed by the
    SCs. An important part of assessing the temporal performance of this solution
    is to understand the relation between the various system design parameters and
    the delay caused by the synchronization and error-recovery operations. Intuitively,
    an increase of size and complexity will be detrimental to responsiveness. Thus,
    our objective in this article is to quantitatively establish suitable configuration
    parameters that will ensure that fault-tolerant permanent storage can be provided
    with delays that are tolerable by the application, i.e., delays that will not
    lead to the violation of critical deadlines for applications. For this purpose
    we compute the application response-time including the overhead due to data synchronization.
    We do this assessment by means of simulation of the fault-tolerant storage solution,
    applying software-based fault injection. Our evaluation will answer the following
    questions: RQ1 What is the impact of system size (number of nodes and applications)
    on the response-time? RQ2 How to partition nodes and applications to a cluster
    to achieve maximum utilization of resources, as well as tolerable response-time?
    RQ3 What is the total outage time of each component (Nodes, SCs, Apps) during
    system operation? RQ4 What failure scenarios have most effect on increasing the
    response-times? We answer these research questions by presentation of contributions.
    Our main contribution is that we show that the fulfillment of specified timing
    requirements can be evaluated for a given system configuration and under given
    failure scenarios. The specific contributions of this paper are as follows: •
    We provide quantitative design configuration parameters to ensure fault-tolerant
    permanent storage at the fog layer with tolerable delays for applications, specifically
    considering industrial use-cases. • We simulate a cluster of nodes at the fog
    layer to evaluate and analyze the advantages of the persistent storage for containerized
    applications, including scalability, software reallocation, and self-recovery.
    • We provide insights on how to apply container-based storage for the implementation
    of cyber–physical systems on fog platforms while ensuring dependability and real-time
    performance. This specifically includes critical factors such as resource allocation,
    fault tolerance, and data consistency. The rest of this article is organized as
    follows. We describe the design of our system and the logic of the storage solution
    in Section 2. In Section 3, we formulate the response time and calculate the tolerable
    response time of an example use-case. In Section 4, we explain our evaluation
    method. We describe our simulator in Section 5. In Section 6, we present the simulation
    results and answer our research questions. In Section 7, we discuss the applicability
    of our solution to real systems and to other applications. We review related work
    in Section 8, and in Section 9 we summarize and conclude by outlining possible
    future work. 2. Persistent fault-tolerant storage for container-based fog architectures
    2.1. System model For the sake of clarity, Table 1 summarizes the notation used
    throughout this paper. A system consists of multiple clusters. Each cluster has
    Nodes (fog nodes). Each node can host one or more applications as per application
    resource demands and node resource capacity. A node is characterized by its computing
    capacities , memory , and storage . Nodes can communicate directly with each other.
    The communication link delay between nodes is denoted (assuming symmetric links).
    Due to the small data volume and size of each data transition through the network,
    we make the simplifying assumption that varies within a limited range depending
    on the cluster size. We recognize that studying network congestion caused by the
    routing protocol is beyond the scope of this work. However, the data generated
    by the studied use-case are relatively small in comparison to the bandwidth of
    the simulated communication channels which indicates the impact of network congestion
    on the performance of the system is likely to be minimal. Each cluster has an
    administrator node ( ) which works as an orchestrator in the cluster. The purpose
    of is to (1) monitor the whole cluster, (2) apply the configuration rules to the
    cluster entities i.e. nodes, applications, etc., and (3) orchestrate and manage
    workload run-times. An application consists of several tasks which are encapsulated
    in a single container. Applications interact as per their service descriptions,
    which describe the interaction between applications and their dependencies. Let
    be a set of applications to be deployed in a cluster. Each application runs for
    a number of iterations as described in a service file. For an application the
    resource requirements are, CPU demand , memory , and storage . The timing of the
    application is given by its deployment time, , and worst-case execution time,
    . In addition, an application has a unique and a storage tag which are used in
    the service file and other configuration files related to the application. Applications
    will be automatically recovered when they fail. This means that applications upon
    failure are automatically restarted on the same node or, on a different node if
    that node is not available for instance, when application failure is due to a
    permanent node failure. The applications are stateful, meaning that their behaviors
    are dependent on their execution history, as explained in [7], [8]. Table 1. System
    notations. Type Parameter Description Empty Cell Fault-free application response-time
    Empty Cell Application response-time Empty Cell Application write time Empty Cell
    Application read time Empty Cell SC deployment latency Empty Cell SC data fetch
    delay (from RDS) Empty Cell SC write delay (to RDS) Empty Cell RAFT protocol delay
    Empty Cell Application deployment latency Empty Cell Application execution time
    Empty Cell Node startup time Empty Cell Fault-free data exchange delay Empty Cell
    Worst-case data exchange delay Time Link delay Empty Cell Set of applications
    Empty Cell Application Empty Cell CPU demand of an application Empty Cell Memory
    demand of an application Empty Cell 1 if fails, 0 otherwise Application Storage
    demand of an application Empty Cell Set of Nodes Empty Cell Administrator Node
    Empty Cell Fog node Empty Cell CPU capacity of a fog node Empty Cell Memory capacity
    of a fog node Empty Cell 1 if fails, 0 otherwise Node Storage capacity of a fog
    node Empty Cell CPU demand of a SC Empty Cell Memory demand of a SC Empty Cell
    1 if fails, 0 otherwise SC Storage demand of a SC 2.2. Persistent storage system
    Applications write the value of state variables in a local storage on the node,
    named Replicated Data Structure (RDS). There is an RDS in each node. The RDSs
    store the state variables of all applications in the cluster in each node and
    the replicas need to be kept consistent. This requires that whenever a state is
    updated, this update must propagate to all the nodes in the cluster so that other
    applications can also access it. For this reason, there are storage applications/containers
    (SC) equipped with a data consistency protocol (RAFT [6] is used) and work along
    with the applications in nodes. SCs are automatically restarted upon failure (as
    are applications). SCs take responsibility for propagating state updates in the
    cluster. Still, applications are not blocked from reading and executing while
    data propagates through the nodes. One of the SCs is elected to be the SC leader.
    As defined in RAFT, the leader takes responsibility for data consistency in a
    cluster of nodes. This means that for any state update on a node, the corresponding
    SC sends the updates to the leader and it is the role of the leader to broadcast
    the updates to the other SCs. By this, any changes in a state variable is propagated
    to the whole cluster in bounded time. The functionalities of different components
    are shown in Fig. 2. Arrows indicate relations between components, and numbers
    indicate the order of actions in different stages of application execution and
    data synchronization. Actions that can occur simultaneously and do not have precedence,
    e.g., deployment of applications and SC, are indicated with the same numbers.
    Fig. 3 illustrates the progress of system components and their interactions in
    different phases of system operation. In this figure, we can see when each component
    interacts with the RDS and other components. Application execution and SC synchronization
    occur repeatedly in the system in between read and write actions. The application
    is launched in the deployment phase, . It then goes to execution mode, , before
    it writes its state value to the RDS, . Write and read actions by application
    occur before and after each iteration of execution. Download : Download high-res
    image (310KB) Download : Download full-size image Fig. 2. System functionalities.
    Download : Download high-res image (372KB) Download : Download full-size image
    Fig. 3. System component interactions in different phases of system operation.
    Meanwhile, the SC starts its process after deployment, . The SCs, including the
    leader, manage the states in RDS. Initially when a SC is deployed its role is
    as a follower in the network of SCs. The SC fetches the latest state written by
    applications to the RDS, , and sends it to the leader. The leader broadcasts the
    updated states to all SCs. indicates any delay derived from the RAFT protocol,
    including sending/receiving states, leader election, state replication, etc. The
    SCs write the updated states to their local RDS after receiving a state update
    from the leader. The SCs and leader continue synchronizing. For instance, when
    writes its state in the state location , this state is propagated to the cluster
    when the related SC sends it to the leader and then the leader sends it to other
    SCs. When needs to read the state produced by , this is the SC which receives
    the related state from the leader and then writes it to its local RDS. The goal
    of this setup is to make sure that the applications have access to the required
    states for execution in presence of failures of any system component. Therefore,
    the states are stored in the local RDS as well as being accessible throughout
    the whole cluster using SCs and the data consistency algorithm. This introduces
    delay in the data exchange for data synchronization in the cluster. To measure
    the timing performance of this setup, we define the data exchange delay under
    fault-free conditions as: (1) Fig. 4 shows the sources of the data exchange delay.
    Download : Download high-res image (142KB) Download : Download full-size image
    Fig. 4. Data exchange illustration. As shown in Fig. 4, the data exchange process
    starts right after an application writes its state value in its local RDS. When
    a SC or node fail during the data exchange process, startup latencies will be
    added to data exchange delay. We formulate the worst-case data exchange delay
    as follow (with in case of failure of the related component, otherwise ) (2) In
    Eq. (2), and are the maximum number of SC failures and node failures that can
    occur on a node during the data exchange delay. 3. Identification of response-time
    Real-time systems should provide their outputs within certain temporal boundaries,
    typically derived from the dynamics of the physical system they interact with.
    In this section, we restrict ourselves to the domain of mobile robotics, for which
    we give a definition of tolerable response-time based on some fundamental physical
    properties of robot mobility, which is general enough for a wide range of applications.
    For simplicity, and given that the initial robotic application used in these studies
    [5] was implemented in ROS [9], we consider that applications encapsulated in
    containers follow the ROS methods for managing concurrency and delay in message
    exchanges. As shown in Fig. 5, if the application execution and data exchange
    time is short enough, the application will meet its deadline before its dependent
    applications start. Based on the defined deadline, this might also be the case
    when an application or SC fail and recover (see Fig. 6). However, when application
    or SC failure and recovery time increases, the applications might miss their defined
    deadline. The application response-time includes application writing, reading,
    deployment, execution, failure, and recovery time for re-execution. is characterized
    by two time points: and , where is the time when an application is deployed and
    is the time when the application state is propagated to the cluster. We define
    application response-time in fault-free conditions as: (3) And the worst-case
    response-time in case of faults as: (4) In Eq. (4), is the number of application
    failures and is the number of node failures both occurring at the worst possible
    time, which is immediately before the completion of the respective task. However,
    is calculated based on the failure time of the application (see Fig. 6). We define
    the tolerable response-time to be less than the defined deadline time for any
    application. Download : Download high-res image (101KB) Download : Download full-size
    image Fig. 5. Application deadline. Download : Download high-res image (100KB)
    Download : Download full-size image Fig. 6. Application deadline with failure.
    3.1. Tolerable response-time calculation There are many factors involved in defining
    a tolerable response-time which all depend on the specifics of the use-case, e.g.
    the robot speed, acceleration, sensing range, etc. These parameters are all defined
    individually for any application based on the desired performance, vicinity of
    obstacles, actuation and sensing intervals, etc. [10]. To study the involved parameters
    that impact the tolerable response-time, we will use a robotic use-case, including
    the use of a formula given by Liu et al. [10], which explains how different parameters
    are involved in calculation of the tolerable response-time: (5) is the maximum
    allowed velocity of the robot, is the maximum acceleration allowed, is the sensing
    range, and is the sense to act time for the robot. Considering a use-case where
    the maximum velocity of a robot system is set to and maximum acceleration , and
    sensing range to , the sense to act time is equal to, . Therefore, the tolerable
    response-time in this use-case becomes: (6) For our evaluation in this work, we
    use this range as the tolerable response-time. 4. Our evaluation strategy To evaluate
    our solution, we use a combination of two approaches: model-based verification
    and simulation. Fig. 7 illustrates the evaluation process. As shown in this figure,
    the process begins with the conceptual architecture proposed in [5]. We then proceed
    with model-based evaluation to verify the key properties of our solution, followed
    by simulation to evaluate our solution in larger scale clusters. The following
    subsections provide more detailed explanations of these steps. Download : Download
    high-res image (175KB) Download : Download full-size image Fig. 7. Evaluation
    Method. 4.1. Verification-based evaluation We present a formal model of our solution
    using the model-checking verification tool UPPAAL [11] to formally verify the
    fault-tolerance, data consistency mechanisms, and temporal aspects of our solution
    in [7], [8]. There are four types of properties for validating our solution: (1)
    Model properties that are part of validating the underlying infrastructure of
    the system; (2) Fault-tolerance properties that are related to the behavior of
    the designed model in the presence of faults; (3) Data consistency properties
    that verify eventual data consistency in distributed replicated data structures;
    and (4) Temporal properties that refer to the impact of adding temporal delays
    on data synchronization, data consistency, and data accessibility after failures
    and recovery. The limitations of model checking include state-space explosion,
    where abstraction must be applied, resulting in a loss of model granularity and
    the practical impossibility to verify a system consisting of a large number of
    nodes. For this reason, we also use simulation-based evaluation to complement
    model checking verification. 4.2. Simulation-based evaluation We need to assess
    if the states which are assumed to be consistent throughout the cluster will be
    available within tolerable time when the number of nodes and applications increase.
    To evaluate this, we have developed a simulator that implements our solution in
    [5]. We start with a system description based on the conceptual architecture and
    the results of the formal verification [7], [8]. Using this system description,
    we identify the parameters to be collected for further evaluation, including hardware
    and software specifications of nodes and applications from different use-cases.
    We perform our experiments in three different conditions, (1) fault-free, (2)
    adding random failures, and (3) adding specific failures in critical points of
    the system during operation. This provides us with a fault-free baseline to which
    the fault scenarios can be compared. If in the fault-free condition, the performance
    of our solution does not meet the timing requirements, we understand that this
    solution is not suitable, and that we need to improve it. After examining the
    behavior of the system in fault-free conditions, we want to evaluate the system
    in cases when random failures occur. These random failures are going to be injected
    to the system at random times. The random variables in this experiment are normally
    distributed with an upper limit of 10% of cluster components. This means that
    the maximum number of components that can fail during one operation time in random
    failure scenarios is 10% of the total number of system components. In this case,
    we can evaluate the behavior of the system when it is prone to failures and determine
    if the response-time is still within the tolerable range. In the third condition,
    specific failures, we want to inject failures into the system at different points
    that we know are critical. We identify these critical points based on the results
    of the verification in [7], [8]. For instance, recovery after failure is going
    to take longer time when a leader fails and a leader-election is required. We
    evaluate the system by examining if the response-time is going to be tolerable
    even in those situations. This increases the likelihood that no significant point
    of failure will be missed in random failure scenarios and that our system can
    tolerate failures at any point. Based on the obtained results we will adjust the
    detailed design of our solution. 5. Simulator Our simulator [12]1 is a discrete
    event simulator developed in Python. In the following we explain the simulator
    design, and the consensus algorithm used in simulating our system. In Section
    8, we explain why we decided to develop an in-house simulator instead of using
    already available ones. Table 2. Applications details. App types App1 App2 App3
    App4 App5 App6 App7 App8 App9 App10 CPU Usage (GHz) 1 2 1.1 3.2 1 1.4 1.2 2 1.3
    1.6 Memory Usage (GB) 2 3 3 4 1 4 2 4 3 4 Storage (GB) 2 4 3 3 800(MB) 2 3 3 1
    2 Execution time (in ms) 150 185 190 255 210 260 180 230 200 250 Deployment time
    (in ms) 10 12 10 20 14 20 10 15 13 18 Number of iterations 2 3 4 2 4 4 5 3 4 4
    Table 3. Resource specifications. Node Types CPU Memory Storage RPi 1.6 GHz (4
    cores) 4 GB 32 GB Fog Node type1 4.40 GHz (5 cores) 8 GB 64 GB Fog Node type2
    5 GHz (7 cores) 12 GB 128 GB 5.1. Simulator design The simulator is designed based
    on the system model described in Section 2. We define three types of nodes, RasberryPis
    (RPis), Fog-nodes type1, and Fog-nodes type2. The specification of these resources
    is given in Table 3. The applications are also defined based on the specification
    of the robotic applications implemented in ROS [13]. A summary of the application
    resource and timing parameters is provided in Table 2. Resource allocation to
    applications are based on the application resource demand and available resources
    in the cluster. A node can host an application if the remaining resources available
    on it exceeds the application resource demands. We assume applications do not
    exceed their resource request and that worst-case execution time (WCET) is calculated
    based on the resource demands. Also, we use the WCET as the actual execution time
    in the simulations. However, we define a safety margin to the resource consumption
    demand when an application deployment request is received, to make sure if applications
    anyhow exceed the requested resource demands they will still continue to perform.
    The RDS is defined as an array that stores the latest state of the application.
    When a node fails the array representing the RDS is reset to empty. To reduce
    the complexity of the experiment we assign a specific node to host the SC Leader.
    The response-time is calculated as defined in Eqs. (2), (4). Parameters , , and
    in these equations are considered to be at most 1 in each iteration of execution.2
    We capture all response-times (including best case, worst-case and average response-time).
    However, in our evaluation we only consider the worst-case response-time. 5.2.
    Consensus algorithm We implement a simplified version of RAFT to ensure data consistency
    and correctness throughout the cluster. Our implementation has two parts. The
    Storage container classes of roles follower, candidate and leader (explained in
    detail in [6]). The second part of the implementation is the data exchange system
    where we implement a modified version of the Gossip protocol [14] to broadcast
    states, using reliable broadcast protocol which is a protocol that guarantees
    the delivery of a message, to all peers in the cluster simultaneously when they
    are available. Other properties such as retrieving data after recovery of a failed
    peer and updating state when a new peer joins the network are implemented as the
    primary functionalities of the Gossip protocol. 6. Evaluation In the evaluation,
    we measure: response-time, operation time, and outage time of each component in
    the cluster under the conditions mentioned in Section 4-A. We answer the research
    questions introduced in Section 1 based on our experiments. RQ1: What is the impact
    of system size (number of nodes and applications) on the response-time? Experiments:
    We conduct the experiment for the baseline scenario to measure the operation time
    and response-time. The operation time is captured by the total time from when
    the first application is deployed until the time the last application runs for
    its last iteration. We increase the number of nodes and applications for each
    experiment to observe the impact of scaling the cluster size on the response-time.
    We aim at finding the upper limit for the cluster size in fault-free conditions.
    Results and analysis:The results of this experiment are shown in Fig. 8. The operation
    time and response-time both increase when the cluster size grows. When the cluster
    size is scaled to 20 times the initial cluster size (10 nodes, 15 applications),
    the response-time and operation time increase to 2.5 and 5.7 times the value,
    respectively. When the cluster consist of 200 nodes hosting a total of 370 applications,
    the response-time is 0.890 which is close to the threshold of tolerable response-time.
    Therefore, we consider this cluster setup to be the upper limit of a cluster size
    for the experiment with random failure and specific failure conditions. We also
    observe that the increase in cluster size considering that cluster components
    are prone to failures increases the response-time as shown in Fig. 9, Fig. 10.
    Compared to the failure free scenario, the response-time increases 1.5 and 2 times
    in presence of random failure and specific failures, respectively. RQ2: How to
    partition nodes and applications to a cluster to achieve maximum utilization of
    resources, and tolerable response-time? Experiments: To answer this research question,
    we need to measure the impact of resource capacity and demand on the response-time.
    We conduct two experiments: (1) to examine if less resource consumption decreases
    the response-time; and (2) to evaluate the impact on resource utilization of node
    types hosting applications. For (1), we changed the resource allocation strategy
    to place a maximum of one application on each node. Then, we apply the resource
    allocation algorithm to optimize resource consumption and compare the response-time
    for each experiment. The number of nodes and applications, in scenarios when we
    use the resource allocation algorithm to utilize resource consumption, must be
    defined in a way to meet the maximum allowed response-time (based on the use-case).
    For (2), we changed the cluster configuration by changing the type of nodes while
    keeping the number of applications and nodes fixed. We set the number of RPis
    to 50% of the total number of nodes, 25% to be Fog node type 1, and Fog node type
    2. In a second setup we set the number of RPis to 70% of the total number of nodes
    and 20% Fog node type 1 and 10% Fog node type 2. By allocating a different percentage
    of RPis, we can investigate the impact of having fewer and more high-performance
    nodes on the response-time. The setup with more RPis can be useful in scenarios
    where the deployment of edge devices with limited capabilities is preferred due
    to cost or power constraints. Results and analysis: The results of the first experiment
    show that placing one application per node does not decrease the response-time
    when the cluster size grows. Based on the setup we defined for measuring the response-time
    by applying the resource allocation algorithm, we observe that, application placement
    considering margin increases the resource utilization and results in reasonable
    response-times in larger scale clusters. The results for the second experiment
    show that a lower response-time is achieved in setups where there are 70% RPis.
    We believe the reason is that, although the total amount of resource capacity
    is less in this case (cluster consisting of 70% of RPis), the resource utilization
    is higher which affects the response-time. However, we also note that we abstracted
    away the impact of different communication media that might change the response-time
    when using different types of nodes in the cluster. The reported results are based
    on the second experiment. To partition nodes and applications to a cluster to
    achieve maximum utilization of resources, and tolerable response-time, we can
    design cluster partitions based on the point where the response-time is intolerable.
    Any cluster size with response-time below the intolerable point can be considered
    as an acceptable cluster partition. As shown in Fig. 9, for a cluster with 10%
    failure of the components, the acceptable cluster size that achieves tolerable
    response-time is 100 nodes hosting 120 applications. The acceptable cluster size
    to achieve tolerable response-time for specific failure scenarios, in which critical
    components are prone to failure at critical times, is 70 nodes hosting 95 applications
    as shown in Fig. 10. RQ3: What is the total outage time of each component (Nodes,
    SCs, Apps) during the system operation? Download : Download high-res image (215KB)
    Download : Download full-size image Fig. 8. Response-time in Failure Free Conditions.
    Download : Download high-res image (221KB) Download : Download full-size image
    Fig. 9. Response-time in presence of random failures. Download : Download high-res
    image (225KB) Download : Download full-size image Fig. 10. Response-time in presence
    of specific failures. Download : Download high-res image (196KB) Download : Download
    full-size image Fig. 11. Outage time in presence of random failures. Download
    : Download high-res image (197KB) Download : Download full-size image Fig. 12.
    Outage time in presence of specific failures. Experiments: Outage time for each
    components is defined as the duration from when a component fails until it is
    back to service again. To measure the outage time of applications in the cluster
    we need to measure the total time each application is not available in each iteration
    of execution. Application outage time is calculated based on the recovery time
    which is the time from failure to completed redeployment, and the reintegration,
    which starts from redeployment to the time the application is back to the state
    it had before failure. Fig. 13 illustrates the sequence of these actions in time.
    We measure the outage time of SCs, based on the total time it takes for SCs to
    recover from failure. The outage time of nodes is calculated based on the total
    time it takes for resource re-allocation to applications and SCs failed due to
    node failure. (see Fig. 14-A and 14-B). We measure the outage time of each component
    under both random and specific failure scenarios. Results and analysis: The results
    of this experiment are shown in Fig. 11, Fig. 12. The outage time of applications,
    SCs, and nodes in presence of random failure varies in experimenting with different
    cluster size. This is due to the distribution of component failures randomly in
    each experiment. In scenarios with specific failures, we observe that the outage
    time of SC and applications increases by an average of 7% and 20% respectively
    comparing to the average outage of these components in random failure scenarios.
    Download : Download high-res image (80KB) Download : Download full-size image
    Fig. 13. Application outage. Download : Download high-res image (81KB) Download
    : Download full-size image Fig. 14. SC and node outage. These findings highlight
    the importance of considering specific failure scenarios when evaluating system
    outage times, as they can significantly impact the performance of different system
    components. For example, in the event of a failure of an SC leader, the outage
    time encompasses not only the failure and redeployment but also the leader election
    process, which is enforced across the SC network. Furthermore, if the node hosting
    the leader fails, the outage time is further increased, as a reallocation of resources
    to applications hosted by the failed node is enforced within the cluster. Therefore,
    it is important to consider the impact of such scenarios on the overall system
    performance and to devise appropriate measures to mitigate the associated outage
    times. RQ4: What failure scenarios have the most effect on increasing the response-times?
    Experiments: To answer this research question, we need to compare the response-time
    values under random and specific failure. We also need to evaluate how component
    failures impact the response-time in different failure scenarios and cluster setup.
    Results and analysis: The measurements of the response-time in presence of random
    and specific failure are shown in Fig. 9, Fig. 10. As can be seen, the impact
    of specific failures on the response-time is higher than the impact of random
    failure. The response-time in presence of specific failure is increased to 2 and
    1.4 times its value comparing to failure free and random failure scenarios, respectively.
    The results show that cluster size to fulfill tolerable response-time in presence
    of specific failures decreases by 30% comparing to its value in random failure
    scenario, which indicates that considering random failures alone could give optimistic
    results, i.e., estimations within the tolerable response-time, while in the worst-case
    the tolerable time is exceeded. Hence, it is strongly recommended to consider
    specific failures in the analysis. We also observed that application and SC outage
    time increase when we force critical components to fail at critical times based
    on the formal verification [7]. This results in higher component outage time that
    leads to an increase in response-time. 7. Discussion In this section, we discuss
    the applicability of our solution to real systems. In addition, we explain how
    our solution can be applied to applications other than the use-case we study in
    this work. 7.1. Applicability to real systems The simulator’s applicability to
    real systems depends on the similarity between the simulated system and the real
    system. The specification of resources in the simulated cluster in this work are
    based on the real testbed that we have in our lab. The system model is designed
    based on the specification of the resources used in the system, and the applications
    are defined based on the resource demands of the robotic applications implemented
    in ROS. This testbed is used for our initial experiments with containerizing the
    ROS application and deploying the containers of the resources in that testbed
    [15]. The simulator is designed to be used to evaluate different resources and
    applications with capacities and demands varying from what is used in our experiments.
    These evaluations can be conducted based on different fault conditions as well.
    It is possible to change the rate of failures and inject faults to any components
    of the system. The simulated system reflects the real system, making the simulation
    results more reliable and applicable to real-world scenarios. However, it is important
    to note that we make some simplifying assumptions that may not be realistic in
    the real-world, such as assuming that (1) applications do not exceed their resource
    request, and (2) RDS is an array that stores the latest state of the application.
    To address these two limitations, we (1) add a safety margin to the resource consumption
    demand when an application deployment request is received, and (2) when a node
    fails, the array representing the RDS is reset to empty which is similar to the
    behavior of what we observed in our testbed environment when injecting a node
    failure. 7.2. Applicability to other use-cases The focus of our approach is to
    assess the overhead caused by our solution on the application response-time, and
    compare it with the application timing requirements. Specifically, we evaluate
    scalability and timing performance using a robotic use-case. Our approach can
    be applied in similar contexts with shared timing requirements. Thus, we provide
    a general framework for evaluating response-time that can be utilized in a broader
    range of industrial automation and robotic applications, irrespective of the specific
    timing requirements they may have. The timing limitations that we considered in
    our evaluation, specifically the need to meet response time deadlines, are common
    requirements in many different domains within this field. For example, in the
    context of industrial automation, it is important to ensure that sensors are read
    and actuators are controlled within a certain time frame. Similarly, in robotics
    applications, timely processing of sensor data is critical to ensure accurate
    and responsive movement of the robot. By conducting our evaluation using a robotic
    use-case, we were able to demonstrate the scalability of our persistent storage
    solution under a specific set of fault conditions. However, the simulation methodology
    that we used can be extended to other industrial automation and robotic applications
    as well. 8. Related work In this section, we briefly review existing works in
    two different areas. In Section 8.1, we review storage solutions for stateful
    applications in container-based architectures, and in Section 8.2, we review current
    simulation tools for cloud native and network solutions. 8.1. Storage system for
    container-based architectures Designing a distributed, fault-tolerant storage
    system at the fog, edge, and cloud layers requires two main considerations: providing
    fault-tolerant, permanent data storage and achieving distributed data consistency
    [5]. The first consideration is to provide distributed data storage systems that
    support reintegration and recovery after failure while optimizing the allocation
    of redundancy in resource-constrained networks. The second consideration focuses
    on using consensus protocols that are appropriate for the system requirements
    and the complexity of the protocol. To address the need for fault-tolerant, consistent,
    and persistent data storage in container-based fog or close-to-edge architectures,
    a solution that is paired with a consensus protocol that aligns with the built-in
    fault tolerance mechanism and lightweight nature of container-based solutions
    is necessary. The issue of volatile storage for stateful applications deployed
    in a container-based architecture has been addressed by several studies. For example,
    Sharma et al. [16] propose a distributed storage system using storage application
    deployment on Kubernetes [1]. In another study, Kristiani et al. [17] propose
    a persistent volume solution for containerized applications using Openstack and
    Kubernetes. However, these works do not consider data consistency in a distributed
    network as part of the solution. Additionally, the persistent storage system in
    these studies, as well as other proposed solutions [2], [18], [19], is located
    at the cloud layer, which increases the response time which could be a critical
    issue in time-sensitive and safety-critical applications like control applications
    that is the target of our research. To provide data storage for edge applications,
    Ismail et al. [20] evaluate Docker Swarm for edge computing. They consider four
    criteria: deployment/termination, resource/service management, fault tolerance,
    and caching. However, they did not propose a unified fault tolerance mechanism
    and ignored application re-integration. Integration of consensus protocols to
    address the volatile storage issue of container-based solutions has also been
    explored in a number of studies. For example, Netto et al. in two different works
    address the volatile storage issue by proposing the use of state-machine replication
    in containers [4], incorporating the RAFT consensus protocol in Kubernetes to
    provide a container orchestration solution [21]. Although the level of data protection
    against container failure compared to other studies in the literature is high,
    the overhead of these solutions increases the container footprints and changes
    their lightweight characteristics to be heavier than expected. This is in contrary
    to one of the main reasons for using container-based solutions at resource-constrained
    fog and edge layers: their lightweight nature which reduces resource consumption
    and response time. The need for a persistent storage system that can manage stateful
    applications, ensure timely data access, and function at the fog layer arises
    from our robotic use-case [5]. To fulfill these requirements, we propose utilizing
    container-based storage applications that offer a distributed storage system and
    recovers from failure. In terms of consensus protocol, RAFT has been chosen as
    it fulfills correct data delivery, safety, liveliness, and fault-tolerance properties
    [22]. In [23], different solutions are compared to our solution. In this work,
    we investigate the suitability of leveraging our solution in larger-scale industrial
    setup through simulation in Section 5. Table 4 is the updated version of the comparison
    between different solutions provided in [23]. Table 4. Summary of most related
    works on container-based architecture storage systems [23]. Aspects Our solution
    [2] [4] [18] [19] [20] [21] Persistent storage ✓ ✓ – ✓ – – – Stateful application
    ✓ ✓ ✓ ✓ ✓ ✓ ✓ Data consistency ✓ – ✓ – ✓ – ✓ Fault-tolerant storage ✓ – – – –
    Flocker – Node failure recovery ✓ – – – – ✓ ✓ Self-healing ✓ – – – – – – Timing
    requirements ✓ ✓ ✓ ✓ ✓ – ✓ Network level Fog Cloud Cloud Cloud Cloud Edge Cloud
    Application Robotic Video streaming Log producer Not specified Log producer Hadoop
    Log producer Scalabilty up to 70 nodes 2 nodes 4 nodes 3 nodes 4 nodes 3 nodes
    4 nodes 8.2. Other simulation tools Cloud native solutions, such as containerization
    and container orchestration, handle the difficult task of provisioning and deploying
    resources. However, due to the complexity of these requirements, these solutions
    tend to be complex in design, involving various interactions and communications
    between different systems. As a result, assessing the services and performance
    of these complex systems is not simple. To evaluate the performance of these systems,
    understand their behavior, make predictions and comprehend the interactions between
    them, more in-depth examination is required, such as our proposed storage application
    as a persistent storage solution. One effective way of gaining a deeper understanding
    of the complexity and behavior of these systems is through real-world implementation.
    This allows for measuring the performance of the system and closely studying the
    interactions between these. However, creating a real-world environment and evaluating
    such behaviors on a large scale for research purposes require significant resources
    and incurs high costs. Many researchers and industry professionals believe that
    simulating computer systems is a powerful way to evaluate and experiment with
    complex scenarios. Simulation also enables testing of various configurations and
    setups that may have unintended consequences in a real-world environment [24],
    [25], [26], [27]. There are many tools and simulations available for measuring
    the performance and behavior of cloud systems. Some of these tools provide measurements
    at different levels, taking into account complex policies and configurations,
    while others focus on evaluating system attributes and testing system requirements
    for design purposes. One of the most powerful and widely used simulators in this
    field is CloudSim [25], which belongs to the first category and provides suitable
    performance measuring tools. CloudSim is an infrastructure simulator for cloud
    environments that enables modeling of data centers, workload scheduling, and allocation
    policies. By using CloudSim, it is possible to test a variety of scenarios without
    deploying test applications in production environments. There are various other
    simulators that have been built based on CloudSim, such as the iFogSim toolkit
    [28] which is a simulator for modeling fog computing, edge and IoT environments
    and resources. iFogSim inherits many of its resource and communication features
    from CloudSim and is extended for modeling edge and fog infrastructures. Additionally,
    other tools have been developed based on CloudSim, such as EdgeCloudSim proposed
    by Sonmez et al. [29] which simulates the specifications of edge and fog computing
    to support the functionalities defined in the edge and fog layers of a network.
    In other studies [30], [31], authors have extended CloudSim to study different
    scenarios at various layers of the network, such as cloud, fog, edge, and their
    applications. All these tools and simulators based on CloudSim have been developed
    to provide powerful and easy-to-use tools for researching various infrastructures
    for research and industrial purposes that cannot be tested except in production
    environments such as real data centers, scheduling applications, deployment of
    complex services, data brokers, etc. However, these simulators and extended tools
    are not specifically designed for simulating rigorous performance testing of container
    orchestration solutions under highly granular resource allocation and placement
    policies that are common in containerized services in cloud native applications.
    Furthermore, CloudSim and other simulators developed based on CloudSim are not
    designed to handle the specific configurations involved in containerization and
    container orchestration solutions, for example, specifying the communication links
    between different containers within the defined services and deployment setup
    is not supported in CloudSim. Other than CloudSim, there are also simulators for
    cloud and fog infrastructure such as Yet Another Fog Simulator (YAFS) [32] which
    is a discrete event simulator developed in Python and using the Simpy package
    to provide various customizable strategies to simulate and evaluate the impact
    of application deployment at the edge and fog layer. YAFS offers network throughput
    prediction, network latency prediction, dynamic routing and service scheduling
    by modeling various infrastructure configurations. Despite its powerful capabilities
    to simulate large scale and complex networks with various topologies, YAFS lacks
    the adequate configurations to simulate cloud-native solutions such as container-based
    architectures. In a study by Khan et al. [33], the authors propose PerfSim, a
    performance simulator for cloud-native microservice chains. PerfSim offers systematic
    methods for modeling and simulating large-scale services for cloud-native applications
    and provides monitoring mechanisms to trace the performance, resource management,
    and timing measurements. While PerfSim is entirely based on container orchestration
    tools (in this case, Kubernetes), it does not include the evaluation of stateful
    applications that require persistent storage. Besides simulators that are focused
    on modeling network and resource of cloud, fog and edge infrastructures, there
    are also simulators such as GreenCloud [24]. The primary goal of such simulators
    is to provide measurements to design energy-efficient data centers [34], [35].
    Other types of simulators such as NS-3 [27], OMNet++ [36] and NetSim [37] also
    exist but are mainly designed for simulating different network topologies that
    can be used when simulating network-related aspects of cloud, fog, and edge computing.
    When it comes to measuring performance and service evaluation, designing resources
    and application deployments, they cannot provide the necessary tools for simulating
    container-based architectures. Thus, simulating and evaluating the performance
    of complex services offered in container-based solutions is still an ongoing task
    for research purposes. Furthermore, our proposed solution includes an enhancement
    of the cloud-native architecture to support persistent storage which cannot conveniently
    be modeled in the currently available simulation tools. Therefore, we developed
    a bespoke simulator in Python that models resources, applications, resource provisioning,
    services, dependencies, etc. which are the de facto requirements of container-based
    solutions. We also included the persistent storage solution in our simulator to
    allow us to perform large-scale experiments using our solution. 9. Conclusion
    In this work, we provide a simulation-based evaluation of a proposed solution
    for close to edge persistent fault-tolerant storage for container-based architectures.
    We evaluated the behavior of the system while scaling up the cluster size. Our
    main goal was to evaluate if this solution can be applied in industrial setups
    with a large number of deployed nodes and applications. The result of our simulations
    shows that for the studied use-case, we can define system designs that complete
    within an application dependent tolerable response-time, . We provide an approach
    to evaluate the response-time for different use-cases as per the essential parameters
    that impact the tolerable response-time, including random and specific failures.
    An interesting continuation of this work would be to extend our simulator to compare
    the performance of the solution when the storage system is located in the cloud
    instead of at the fog or edge. This would helps us to compare the scalability
    of our solution with the scalability of existing solutions. In future work, we
    will also consider the communication aspect of the network by considering different
    data size that impact communication delay based on network bandwidth and latency
    of different layers. Declaration of competing interest The authors declare that
    they have no known competing financial interests or personal relationships that
    could have appeared to influence the work reported in this paper. Acknowledgments
    This research has received funding from the European Union’s Horizon 2020 research
    and innovation programme under the Marie Skłodowska-Curie grant agreement No 764785
    and also from the VINNOVA project 2018-02437. References [1] Kubernetes Foundation,
    Kubernetes Documentation, https://kubernetes.io/. Google Scholar [2] L. Abdollahi
    Vayghan, M.A. Saied, M. Toeroe, F. Khendek, Microservice Based Architecture: Towards
    High-Availability for Stateful Applications with Kubernetes, in: IEEE 19th International
    Conference on Software Quality, Reliability and Security, QRS, 2019, pp. 176–185.
    Google Scholar [3] H.V. Netto, A.F. Luiz, M. Correia, L. de Oliveira Rech, C.P.
    Oliveira, Koordinator: A Service Approach for Replicating Docker Containers in
    Kubernetes, in: IEEE Symposium on Computers and Communications, ISCC, 2018, pp.
    58–63. Google Scholar [4] Netto H.V., Lung L.C., Correia M., Luiz A.F., de Souza
    L.M.S. State machine replication in containers managed by Kubernetes J. Syst.
    Archit., 73 (2017), pp. 53-59 View PDFView articleView in ScopusGoogle Scholar
    [5] Z. Bakhshi, G. Rodriguez-Navas, H. Hansson, Fault-tolerant Permanent Storage
    for Container-based Fog Architectures, in: 22nd IEEE International Conference
    on Industrial Technology, Vol. 1, ICIT, 2021, pp. 722–729. Google Scholar [6]
    D. Ongaro, J. Ousterhout, In search of an understandable consensus algorithm,
    in: {USENIX} Annual Technical Conference, {USENIX}{ATC} 14, 2014, pp. 305–319.
    Google Scholar [7] Z. Bakhshi, G. Rodriguez-Navas, H. Hansson, Using UPPAAL to
    Verify Recovery in a Fault-tolerant Mechanism Providing Persistent State at the
    Edge, in: 26th IEEE International Conference on Emerging Technologies and Factory
    Automation, ETFA, 2021, pp. 1–6. Google Scholar [8] Z. Bakhshi, G. Rodriguez-Navas,
    H. Hansson, Verifying the timing of a persistent storage for stateful fog applications,
    in: 6th International Conference on Computer, Software and Modeling, ICCSM, 2022,
    pp. 1–8. Google Scholar [9] O’Kane J.M. A gentle introduction to ROS University
    of South Carolina (2014) Independently published, available at http://www.cse.sc.edu/~jokane/agitr/
    Google Scholar [10] S. Liu, M. Watterson, S. Tang, V. Kumar, High speed navigation
    for quadrotors with limited onboard sensing, in: IEEE International Conference
    on Robotics and Automation, ICRA, 2016, pp. 1484–1491. Google Scholar [11] UPPAAL
    model checker, UPPAAL official website, https://https://uppaal.org/. Google Scholar
    [12] Bakhshi Z. Persistent fault tolerant distributed storage for fog applications
    (2023), 10.5281/zenodo.8052558 Zenodo Google Scholar [13] Y. Saito, F. Sato, T.
    Azumi, S. Kato, N. Nishio, ROSCH:Real-Time Scheduling Framework for ROS, in: IEEE
    24th International Conference on Embedded and Real-Time Computing Systems and
    Applications, RTCSA, 2018, pp. 52–58. Google Scholar [14] Kwiatkowska M., Norman
    G., Parker D. Analysis of a gossip protocol in PRISM SIGMETRICS Perform. Eval.
    Rev., 36 (3) (2008), pp. 17-22 CrossRefGoogle Scholar [15] Z. Bakhshi Valojerdi,
    G. Rodriguez-Navas, H. Hansson, Fault-tolerant Permanent Storage for Container-based
    Fog Architectures, in: Proceedings of the 22nd IEEE International Conference on
    Industrial Technology, ICIT, 2021. Google Scholar [16] Sharma A., Yadav S., Gupta
    N., Dhall S., Rastogi S. Proposed model for distributed storage automation system
    using Kubernetes operators Advances in Data Sciences, Security and Applications,
    Springer (2020), pp. 341-351 CrossRefView in ScopusGoogle Scholar [17] Kristiani
    E., Yang C.-T., Wang Y.T., Huang C.-Y. Implementation of an edge computing architecture
    using openstack and Kubernetes International Conference on Information Science
    and Applications, Springer (2018), pp. 675-685 Google Scholar [18] Mercl L., Pavlik
    J. Public cloud Kubernetes storage performance analysis International Conference
    on Computational Collective Intelligence, Springer (2019), pp. 649-660 CrossRefView
    in ScopusGoogle Scholar [19] Oliveira C., Lung L.C., Netto H., Rech L. Evaluating
    raft in docker on Kubernetes International Conference on Systems Science, Springer
    (2016), pp. 123-130 Google Scholar [20] B.I. Ismail, E. Mostajeran Goortani, M.B.
    Ab Karim, W. Ming Tat, S. Setapa, J.Y. Luke, O. Hong Hoe, Evaluation of Docker
    as Edge computing platform, in: 2015 IEEE Conference on Open Systems, ICOS, 2015,
    pp. 130–135. Google Scholar [21] Netto H., Pereira Oliveira C., Rech L.d.O., Alchieri
    E. Incorporating the Raft consensus protocol in containers managed by Kubernetes:
    An evaluation Int. J. Parallel Emergent Distrib. Syst., 35 (4) (2020), pp. 433-453
    CrossRefView in ScopusGoogle Scholar [22] Shahaab A., Lidgey B., Hewage C., Khan
    I. Applicability and appropriateness of distributed ledgers consensus protocols
    in public and private sectors: A systematic review IEEE Access, 7 (2019), pp.
    43622-43636 CrossRefView in ScopusGoogle Scholar [23] Bakhshi Z. Persistent Fault-tolerant
    Storage at the Fog Layer (Licentiate Thesis) Mälardalen University (2021) Google
    Scholar [24] Kliazovich D., Bouvry P., Khan S.U. GreenCloud: A packet-level simulator
    of energy-aware cloud computing data centers J. Supercomput., 62 (3) (2012), pp.
    1263-1283 CrossRefView in ScopusGoogle Scholar [25] Calheiros R.N., Ranjan R.,
    Beloglazov A., De Rose C.A., Buyya R. CloudSim: A toolkit for modeling and simulation
    of cloud computing environments and evaluation of resource provisioning algorithms
    Softw. - Pract. Exp., 41 (1) (2011), pp. 23-50 CrossRefGoogle Scholar [26] Puliafito
    C., Gonçalves D.M., Lopes M.M., Martins L.L., Madeira E., Mingozzi E., Rana O.,
    Bittencourt L.F. MobFogSim: Simulation of mobility and migration for fog computing
    Simul. Model. Pract. Theory, 101 (2020), Article 102062 View PDFView articleView
    in ScopusGoogle Scholar [27] Riley G.F., Henderson T.R. The ns-3 network simulator
    Modeling and Tools for Network Simulation, Springer (2010), pp. 15-34 CrossRefView
    in ScopusGoogle Scholar [28] Gupta H., Vahid Dastjerdi A., Ghosh S.K., Buyya R.
    IFogSim: A toolkit for modeling and simulation of resource management techniques
    in the Internet of Things, edge and fog computing environments Softw. - Pract.
    Exp., 47 (9) (2017), pp. 1275-1296 CrossRefView in ScopusGoogle Scholar [29] Sonmez
    C., Ozgovde A., Ersoy C. Edgecloudsim: An environment for performance evaluation
    of edge computing systems Trans. Emerg. Telecommun. Technol., 29 (11) (2018),
    Article e3493 View in ScopusGoogle Scholar [30] Silva Filho M.C., Oliveira R.L.,
    Monteiro C.C., Inácio P.R., Freire M.M. CloudSim plus: A cloud computing simulation
    framework pursuing software engineering principles for improved modularity, extensibility
    and correctness IFIP/IEEE Symposium on Integrated Network and Service Management,
    IM, IEEE (2017), pp. 400-406 CrossRefGoogle Scholar [31] Byrne J., Svorobej S.,
    Gourinovitch A., Elango D.M., Liston P., Byrne P.J., Lynn T. Recap simulator:
    Simulation of cloud/edge/fog computing scenarios Winter Simulation Conference,
    WSC, IEEE (2017), pp. 4568-4569 CrossRefView in ScopusGoogle Scholar [32] Lera
    I., Guerrero C., Juiz C. YAFS: A simulator for IoT scenarios in fog computing
    IEEE Access, 7 (2019), pp. 91745-91758 CrossRefView in ScopusGoogle Scholar [33]
    Khan M.G., Taheri J., Al-dulaimy A., Kassler A. PerfSim: A performance simulator
    for cloud native microservice chains IEEE Trans. Cloud Comput. (2021) Google Scholar
    [34] Fiandrino C., Kliazovich D., Bouvry P., Zomaya A.Y. Performance and energy
    efficiency metrics for communication systems of cloud computing data centers IEEE
    Trans. Cloud Comput., 5 (4) (2015), pp. 738-750 Google Scholar [35] Boru D., Kliazovich
    D., Granelli F., Bouvry P., Zomaya A.Y. Energy-efficient data replication in cloud
    computing datacenters Clust. Comput., 18 (1) (2015), pp. 385-402 CrossRefView
    in ScopusGoogle Scholar [36] Varga A. A practical introduction to the OMNeT++
    simulation framework Recent Advances in Network Simulation, Springer (2019), pp.
    3-51 CrossRefGoogle Scholar [37] Wahid-Ul-Ashraf A., Budka M., Musial K. NetSim–The
    framework for complex network generator Procedia Comput. Sci., 126 (2018), pp.
    547-556 View PDFView articleView in ScopusGoogle Scholar Cited by (1) Storage
    Placement in Computing Continuum for a Robotic Application 2023, Research Square
    Zeinab Bakhshi is a Ph.D. student at Mälardalen University (MDU) in Sweden since
    2018, researching on the fault-tolerance and dependability aspects of Fog computing
    and container-based solutions. She received her Master’s degree and Bachelor’s
    degree from Islamic Azad University Science and Research in 2013 and Naragh in
    2010, respectively. Zeinab has previously worked as a University Lecturer in Informatics
    University in Iran and a security engineer in three different companies in the
    UAE and Iran. She has also gained experience in the R&D department of Rightel
    telecommunication company. Guillermo Rodrigues-Navas received the telecommunication
    engineer degree from the University of Vigo, Spain, in 2001, and the doctorate
    in informatics from the University of the Balearic Islands (UIB), Palma de Mallorca,
    Spain, in 2010. He is currently a lead data scientist at Nokia, as part of the
    Cloud and Network Services (CNS) Business Applications Analytics group. Previous
    appointments include researcher at Nokia Bell Labs and Senior Lecturer at the
    Mälardalen University (MDU). His research interests comprise fault tolerance,
    dependability and real-time communication for cyber–physical systems, as well
    as application of novel data analytics algorithms for 5G network management and
    automation. Hans Hansson is professor of Computer Engineering at Mälardalen University
    (MDU) since 1997 and since 2012 part-time Scientific Leader at RISE Smart Industrial
    Systems in Västerås. He is one of the founders of MDU’s largest and most successful
    research environments, Embedded Systems, where he has a history of leading large
    scientific and educational initiatives. Hans is author/co-author of more than
    200 scientific publications cited 7000+ times (according to GS). In recent years,
    Hans’ research is focused on functional safety and cybersecurity, but he has a
    track record of research contributions also in component-based design of safety-critical
    real-time embedded systems, modeling, and analysis of real-time communication,
    testing and debugging, development of automotive control software, and formal
    modeling of timing and probability. Hans has been, and is, active in scientific
    committees both nationally and internationally. Five of his former PhD-students
    have been appointed university professors. He has a broad national and international
    scientific and business sector network. In recent years, Hans has been a driving
    force in efforts at MDU into online education for professionals (aka lifelong
    learning). Hans has a strong belief that collaboration both internally and externally
    with society, business, and academia is key in addressing societal challenges.
    1 Simulation files are also available on a GitHub repository: https://github.com/ZeinabBa/Persistent-Storage-Simulation.
    2 This is a simplifying assumption that in some scenarios can be justified by
    the low probability of repetitive failures. © 2023 The Author(s). Published by
    Elsevier B.V. Part of special issue Special Issue of International Conference
    on Reliable Software Technologies AEiC 2023 Edited by Elena Troubitsyna View special
    issue Recommended articles Impact of priority assignment on schedule-based attacks
    in real-time embedded systems Journal of Systems Architecture, Volume 145, 2023,
    Article 103021 Sina Yari Karin, …, Matthew Anderson View PDF Compositional verification
    of embedded real-time systems Journal of Systems Architecture, Volume 142, 2023,
    Article 102928 Mohammed Foughali, …, Alexander Zuepke View PDF Energy-efficient
    scheduling of imprecise mixed-criticality real-time tasks based on genetic algorithm
    Journal of Systems Architecture, Volume 143, 2023, Article 102980 Yi-Wen Zhang,
    Rong-Kun Chen View PDF Show 3 more articles Article Metrics Captures Readers:
    6 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Journal of Systems Architecture
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Analyzing the performance of persistent storage for fault-tolerant stateful
    fog applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Fernandes D.
  - Douglas L.L.M.
  - Santos G.
  - Geymerson S.R.
  - Queiroz F.
  - Andre L.L.A.
  citation_count: '0'
  description: 'The rapid urbanization growth has underscored the need for innovative
    solutions to enhance transportation efficiency and safety. Intelligent Transportation
    Systems (ITS) have emerged as a promising solution in this context. However, analyzing
    and processing the massive and intricate data generated by ITS presents significant
    challenges for traditional data processing systems. This work proposes an Edge-based
    Data Lake Architecture to integrate and analyze the complex data from ITS efficiently.
    The architecture offers scalability, fault tolerance, and performance, improving
    decision-making and enhancing innovative services for a more intelligent transportation
    ecosystem. We demonstrate the effectiveness of the architecture through an analysis
    of three different use cases: (i) Vehicular Sensor Network, (ii) Mobile Network,
    and (iii) Driver Identification applications.'
  doi: 10.1145/3616394.3618270
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesMSWIMProceedingsPE-WASUN
    ''23Towards Edge-Based Data Lake Architecture for Intelligent Transportation System
    RESEARCH-ARTICLE SHARE ON Towards Edge-Based Data Lake Architecture for Intelligent
    Transportation System Authors: Danilo Fernandes , Douglas L. L. Moura , Gean Santos
    , + 3 Authors Info & Claims PE-WASUN ''23: Proceedings of the Int''l ACM Symposium
    on Performance Evaluation of Wireless Ad Hoc, Sensor, & Ubiquitous NetworksOctober
    2023Pages 1–8https://doi.org/10.1145/3616394.3618270 Published:30 October 2023Publication
    History 0 citation 58 Downloads eReaderPDF PE-WASUN ''23: Proceedings of the Int''l
    ACM Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, & Ubiquitous
    Networks Towards Edge-Based Data Lake Architecture for Intelligent Transportation
    System Pages 1–8 Previous Next ABSTRACT References Index Terms Recommendations
    Comments ABSTRACT The rapid urbanization growth has underscored the need for innovative
    solutions to enhance transportation efficiency and safety. Intelligent Transportation
    Systems (ITS) have emerged as a promising solution in this context. However, analyzing
    and processing the massive and intricate data generated by ITS presents significant
    challenges for traditional data processing systems. This work proposes an Edge-based
    Data Lake Architecture to integrate and analyze the complex data from ITS efficiently.
    The architecture offers scalability, fault tolerance, and performance, improving
    decision-making and enhancing innovative services for a more intelligent transportation
    ecosystem. We demonstrate the effectiveness of the architecture through an analysis
    of three different use cases: (i) Vehicular Sensor Network, (ii) Mobile Network,
    and (iii) Driver Identification applications. References Kaveh Ahmadi, S Pourya
    Miralavy, and Mona Ghassemian. 2020. Software-defined networking to improve handover
    in mobile edge networks. International Journal of Communication Systems, Vol.
    33, 14 (2020), e4510. Estefan''ia Coronado, Rasoul Behravesh, Tejas Subramanya,
    Adriana Fernández-Fernández, Shuaib Siddiqui, Xavier Costa-Pérez, and Roberto
    Riggio. 2022. Zero Touch Management: A Survey of Network Automation Solutions
    for 5G and 6G Networks. IEEE Communications Surveys & Tutorials , Vol. 24, 4 (2022),
    2535--2578. Tasneem S. J. Darwish and Kamalrulnizam Abu Bakar. 2018. Fog Based
    Intelligent Transportation Big Data Analytics in The Internet of Vehicles Environment:
    Motivations, Architecture, Challenges, and Critical Issues. IEEE Access , Vol.
    6 (2018), 15679--15701. Show All References Index Terms Towards Edge-Based Data
    Lake Architecture for Intelligent Transportation System Computer systems organization
    Dependable and fault-tolerant systems and networks Redundancy Embedded and cyber-physical
    systems Embedded systems Robotics Networks Network properties Network reliability
    Recommendations On the Integration of Ledger Technology and Edge Computing for
    Intelligent Transportation Systems PE-WASUN ''23: Proceedings of the Int''l ACM
    Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, & Ubiquitous Networks
    Intelligent Transportation Systems (ITS) involve integrating information and communication
    technologies with traffic infrastructure and vehicles to support the development
    of more sustainable transportation systems. However, ITS face security, ... Read
    More Research and Development of Intelligent Transportation Systems DCABES ''12:
    Proceedings of the 2012 11th International Symposium on Distributed Computing
    and Applications to Business, Engineering & Science Intelligent Transportation
    Systems (ITS) have been developed for more than ten years in China. Furthermore,
    a new generation Intelligent Transportation Systems should be launched to meet
    the requirement of rapid development of transportation in China. ... Read More
    Edge Intelligence in Intelligent Transportation Systems: A Survey Edge intelligence
    (EI) is becoming one of the research hotspots among researchers, which is believed
    to help empower intelligent transportation systems (ITS). ITS generates a large
    amount of data at the network edge by millions of devices and sensors. ... Read
    More Comments 30 References View Table Of Contents Footer Categories Journals
    Magazines Books Proceedings SIGs Conferences Collections People About About ACM
    Digital Library ACM Digital Library Board Subscription Information Author Guidelines
    Using ACM Digital Library All Holdings within the ACM Digital Library ACM Computing
    Classification System Digital Library Accessibility Join Join ACM Join SIGs Subscribe
    to Publications Institutions and Libraries Connect Contact Facebook Twitter Linkedin
    Feedback Bug Report The ACM Digital Library is published by the Association for
    Computing Machinery. Copyright © 2024 ACM, Inc. Terms of Usage Privacy Policy
    Code of Ethics Feedback"'
  inline_citation: '>'
  journal: PE-WASUN 2023 - Proceedings of the International ACM Symposium on Performance
    Evaluation of Wireless Ad Hoc, Sensor, and Ubiquitous Networks
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Towards Edge-Based Data Lake Architecture for Intelligent Transportation
    System
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ebrahim M.
  - Hafid A.
  citation_count: '2'
  description: The advent of Cloud Computing enabled the proliferation of Internet-of-Things
    (IoT) applications for smart environments. However, the distance of these resources
    makes them unsuitable for delay-sensitive applications. Hence, Fog Computing has
    emerged to provide such capabilities in proximity to end devices through distributed
    resources. These limited resources can collaborate to serve distributed IoT application
    workflows using the concept of stateless micro Fog service replicas, which provides
    resiliency and maintains service availability in the face of failures. Load balancing
    supports this collaboration by optimally assigning workloads to appropriate services,
    i.e., distributing the load among Fog nodes to fairly utilize compute and network
    resources and minimize execution delays. In this paper, we propose using ELECTRE,
    a Multi-Criteria Decision Analysis (MCDA) approach, to efficiently balance the
    load in Fog environments. We considered multiple objectives to make service selection
    decisions, including compute and network load information. We evaluate our approach
    in a realistic unbalanced topological setup with heterogeneous workload requirements.
    To the best of our knowledge, this is the first time ELECTRE-based methods are
    used to balance the load in Fog environments. Through simulations, we compared
    the performance of our proposed approach with traditional baseline methods that
    are commonly used in practice, namely random, Round-Robin, nearest node, and fastest
    service selection algorithms. In terms of the overall system performance, our
    approach outperforms these methods with up to 67% improvement.
  doi: 10.1016/j.micpro.2023.104893
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract MSC Keywords 1. Introduction 2. Related work 3. ELECTRE 4. System
    design 5. Evaluation of ELECTRE method 6. Conclusion Declaration of Competing
    Interest Acknowledgment References Vitae Show full outline Cited by (3) Figures
    (22) Show 16 more figures Tables (4) Table 1 Table 2 Table 3 Table 4 Microprocessors
    and Microsystems Volume 101, September 2023, 104893 Resilience and load balancing
    in Fog networks: A Multi-Criteria Decision Analysis approach Author links open
    overlay panel Maad Ebrahim, Abdelhakim Hafid Show more Add to Mendeley Share Cite
    https://doi.org/10.1016/j.micpro.2023.104893 Get rights and content Abstract The
    advent of Cloud Computing enabled the proliferation of Internet-of-Things (IoT)
    applications for smart environments. However, the distance of these resources
    makes them unsuitable for delay-sensitive applications. Hence, Fog Computing has
    emerged to provide such capabilities in proximity to end devices through distributed
    resources. These limited resources can collaborate to serve distributed IoT application
    workflows using the concept of stateless micro Fog service replicas, which provides
    resiliency and maintains service availability in the face of failures. Load balancing
    supports this collaboration by optimally assigning workloads to appropriate services,
    i.e., distributing the load among Fog nodes to fairly utilize compute and network
    resources and minimize execution delays. In this paper, we propose using ELECTRE,
    a Multi-Criteria Decision Analysis (MCDA) approach, to efficiently balance the
    load in Fog environments. We considered multiple objectives to make service selection
    decisions, including compute and network load information. We evaluate our approach
    in a realistic unbalanced topological setup with heterogeneous workload requirements.
    To the best of our knowledge, this is the first time ELECTRE-based methods are
    used to balance the load in Fog environments. Through simulations, we compared
    the performance of our proposed approach with traditional baseline methods that
    are commonly used in practice, namely random, Round-Robin, nearest node, and fastest
    service selection algorithms. In terms of the overall system performance, our
    approach outperforms these methods with up to 67% improvement. Previous article
    in issue Next article in issue MSC 00001111 Keywords Fog ComputingLoad balancingIoTMCDAELECTRE
    1. Introduction Fog Computing complements Cloud Computing to support delay-sensitive
    Internet-of-Things (IoT) applications and to support mobility, geo-distribution,
    and location awareness for these applications [1]. It saves the network bandwidth
    by reducing the traffic between end devices and the Cloud. In addition, it increases
    the security and privacy of IoT applications by pre-processing and encrypting
    data closer to its source [2]. Fog resources extend from the edge of the network
    to the Cloud, i.e., cloud-to-thing continuum, while Edge Computing limits these
    resources within one-hop distance from those things [3] (see Fig. 1). Hence, the
    Fog is more complex than the Edge; indeed, the Edge can be viewed as a subcategory
    of the Fog as shown in Fig. 1. Only modular applications, like workflow and bag-of-tasks
    [4], allow for distributed Fog deployments as pipelined workflows [5]. In contrast,
    monolithic applications cannot be divided into multiple logical modules, and hence
    each shall run as a single module in a single computing entity. Many applications
    benefit from Fog and Edge Computing to support different types of IoT and mobile
    applications. Fig. 2, for example, presents a Fog-based video surveillance system,
    where the overall system performance is increased using pipelined application
    workflows. This is essential for the success of real-time IoT systems like those
    proposed in [6], [7]. The first module, i.e., on the camera, performs simple subtractions
    between subsequent frames to prevent sending identical frames of stationary views.
    Edge and Fog modules perform object recognition and face detection, respectively;
    this allows to only send and process frames with humans and faces, respectively.
    Face recognition will run on the Cloud, where faces are matched against a privately-owned,
    or government-owned, database to track people in restricted areas, suspects, or
    criminals. Immediate feedback can be sent from each module in the application
    workflow to change frame resolution and video Bitrate in the sensing device when
    needed. For example, if a face is detected, the camera is notified to send higher-resolution
    frames to increase the face recognition accuracy. However, the camera is notified
    to decrease its resolution and Bitrate to save resources when no objects are detected.
    Estimating the volume of IoT workloads in advance can help build Fog environments,
    from scratch or by extending existing infrastructures, to support IoT frameworks
    and their applications [8]. However, it is almost impossible to accurately estimate
    the expected volume of IoT workloads, a problem that is imposed by the ever-growing
    number of IoT devices and their applications. Therefore, optimal resource management
    becomes critical to cope with these increasing demands, which is done through
    optimal resource provisioning and resource allocation schemes. In addition, optimally
    balancing IoT workload among Fog resources helps achieve optimal utilization of
    existing resources without requiring additional hardware resources. Although,
    optimal load balancing is not sufficient when the total available hardware resources
    cannot provide enough computational power to serve all generated IoT workloads
    at a given point in time. Download : Download high-res image (206KB) Download
    : Download full-size image Fig. 1. Cloud, Fog, and Edge computing for IoT networks.
    Download : Download high-res image (200KB) Download : Download full-size image
    Fig. 2. Fog-based video surveillance system. Time-dependent data rate fluctuations,
    uneven sensor distribution, and sensor mobility are usually the main reasons for
    dynamic workloads in realistic IoT networks. Hence, efficient management of the
    network’s scarce resources is essential to improve the performance of these systems.
    In mobile networks, for example, this requires understanding the behavior of network
    traffic [9]. These dynamic environment changes increase the complexity of efficient
    workload distribution across Fog nodes [1]. This led researchers to oversimplify
    their experiments using many pre-assumptions, which makes their proposed solutions
    far from being applicable to realistic environments [10]. For example, Kashani
    et al. [11] proposed to use a three-layer hierarchical architecture, where the
    Cloud resides at the top, Fog nodes in the middle, and IoT devices at the bottom
    of the hierarchy. However, flat mesh-like architectures and semi-hierarchical
    architectures are more realistic, where Fog nodes can communicate with adjacent
    Fog nodes without the concept of layers [10]. This can be achieved by interconnecting
    Fog nodes with each other to allow offloading workloads and traffic among themselves,
    if needed, without necessarily going through the Cloud. Load balancing becomes
    critical to achieving resource efficiency by avoiding bottlenecks, underload,
    and overload situations [11]. For this to work, the total available resources
    should be equal to or greater than the requirements of all incoming requests at
    a given time [12]. Hence, load balancing methods try to maximize resource utilization
    while minimizing the execution delay to satisfy the deadline requirements for
    delay-sensitive tasks. In addition, to distribute workloads among interconnected
    Fog nodes without migrating data and services, these nodes must have redundant
    service replicas of the requested modules [13]. Using redundant microservices
    for Fog-based applications is essential, especially for resource-hungry real-time
    IoT applications that are used globally, such as trending online games, Internet
    of Vehicles (IoV), and health monitoring systems. Balancing the load of such heavy
    applications is critical, and avoiding migration while serving the global population
    can be only achieved through service replication. These Fog-based microservices
    can be deployed globally as background services, and only loaded when triggered,
    i.e., when requested by an IoT application. Hence, the compute resources of those
    Fog nodes will only be consumed while serving these workloads. If workloads of
    some applications are less frequent in a specific region, their corresponding
    micro-services that are deployed in the Fog nodes in that region will be idle
    most of the time, hence saving their compute resources. Considering redundant
    micro Fog services with resource-demanding real-time applications saves compute
    and network resources from the overhead of service and data migration. Migration
    overhead is a heavy burden in Fog networks, especially the overhead of transmission,
    allocation, and security of the migrated services and their associated data [14].
    In addition, redundancy allows performing load balancing through Fog service selection
    decisions, where a Fog service in a given Fog node is selected to serve a given
    request. Hence, an up-to-date directory of available services in available Fog
    nodes must be presented for the node that performs load balancing. Such a directory
    represents a virtual sub-network, i.e., a local view of the network, for every
    workload, where multiple Fog nodes host the required service. Hence, the goal
    of the load balancing algorithm is to select one of these Fog nodes to serve this
    workload. Service selection decisions depend on whether Fog modules need cached
    data to process IoT requests, a concept referred to as the state of the flow of
    requests [15]. For stateless requests, where cached data is not needed, incoming
    requests can be offloaded to other service replicas without migration [15]. However,
    migration is needed for stateful, i.e., not stateless, requests during the offloading
    process. Hence, to be able to use service selection decisions to balance the load
    between redundant micro Fog services while avoiding migration, workloads/requests
    must be assumed stateless, i.e., serverless. All these assumptions are common
    in Fog environments, in fact, considering stateless requests with redundant microservices
    provides resilience for the system by maintaining service availability in case
    of the failure of one or more Fog nodes in the system [16]. Service selection
    decisions range from sophisticated load balancing algorithms to simple random
    node selection [17]. Round-Robin (RR), nearest node, and fastest service selection
    algorithms are other simple service selection methods. Using the nearest node
    minimizes communication delay and energy consumption in the network while using
    the fastest service increases resource utilization and minimizes task processing
    time. But, smarter algorithms simultaneously include workload requirements, resource
    capabilities of computing nodes and network links, and their current load information.
    Search-based optimization algorithms can be used to balance the load in such fully
    observable environments, where the network load information is accessible to the
    load balancing algorithm. Even though search-based algorithms may require higher
    processing power than simple traditional approaches, they can provide optimal,
    or near-optimal, results that can be used as a baseline for other algorithms.
    Such algorithms can be deployed in resource-rich network controllers, like Software-Defined
    Network (SDN) Controllers, to provide service selection decisions for every generated
    workload in the network. Therefore, we propose, in this paper, a multi-objective
    search-based optimization approach using an outranking Multi-Criteria Decision
    Analysis (MCDA) algorithm called ELECTRE [18]. To evaluate the effectiveness of
    our approach, we compare it against common traditional service selection algorithms
    that do not require storing or training a model. These methods include random,
    RR, nearest node, and fastest service selection algorithm. Like ELECTRE, such
    methods provide spontaneous decision-making in dynamically changing environments.
    On the other hand, algorithms that require training, i.e., machine-learning-based
    methods, will always need to retrain to adapt to the continuous changes in the
    environment. The simplicity and spontaneous adaptation to dynamically changing
    environments are the main reasons for considering traditional methods in practical
    Fog deployment. Nonetheless, these baseline algorithms are inadequate for achieving
    good performance in any Fog architecture, particularly in unbalanced architectures.
    We considered using an MCDA-based approach to balance the load in Fog networks
    as it recently showed good performances in similar tasks. For instance, these
    methods achieved satisfactory performance in Mobile Crowd Computing (MCC) systems
    by making optimal resource selection decisions [19]. They were also used to solve
    the service selection problem in Cloud Computing environments [20]. In addition,
    these algorithms were used to solve the service placement problem in Fog Computing
    systems [21]. However, this is the first time an MCDA-based method is used to
    balance the load in Fog systems to increase the overall Fog system performance.
    We also provide in this work a generic network architecture with heterogeneous
    and unbalanced resources, unbalanced load distribution, heterogeneous workload
    requirements, and a semi-hierarchical topology. It demonstrates the need for load
    balancing in unbalanced Fog environments with bottlenecks in computational and
    communication resources. It is used to evaluate and compare the performance of
    our approach with other service selection methods. This architecture avoids common
    simplifications in the literature, like using hierarchical topologies with homogeneous
    resource and workload requirements [10]. We then evaluate our approach in a more
    realistic experimentation setup, which consists of multiple Fog nodes that are
    randomly configured to simulate unbalanced resource and load distribution. This
    larger setup confirms both the performance of our proposed approach, as well as
    the effectiveness of our generic architecture in evaluating load balancing algorithms
    in a simplified setup. In addition, we consider pipelined IoT application workflows
    with computational modules that span from the edge of the network to the Cloud
    (see Fig. 3). This consideration avoids simplifying Fog problems into Edge problems,
    where atomic IoT workloads are assigned to a single computing entity that then
    sends feedback to the source IoT device in response [22]. Hence, our main contributions
    can be summarized as the: Download : Download high-res image (213KB) Download
    : Download full-size image Fig. 3. A simple distributed IoT application workflow
    for the generic Fog architecture, with a single application loop. 1. Design and
    implementation of a generic Fog architecture that can be used to evaluate load
    balancing algorithms away from the common simplifications in the literature. 2.
    Design and implementation of ELECTRE MCDA-based load balancing algorithm that
    simultaneously considers five selection criteria for its decision-making process.
    3. Comparison of our proposed approach with traditional methods that are commonly
    used in practice, which are wrongly assumed to behave well in Fog environments.
    4. Evaluation of our proposed load balancing solution in a larger and more realistic
    setup to confirm our findings. The rest of the paper is organized as follows.
    Section 2 presents a comparison between this work and existing work in the literature.
    We introduce the ELECTRE algorithm in Section 3. Section 4 presents the generic
    architecture used to evaluate the proposed solution. Section 5 discusses the results
    and the superior performance of our approach. Finally, Section 6 concludes the
    paper and presents future directions for the work. 2. Related work Increasing
    the performance of IoT applications using Fog resources can be done in two ways:
    (1) Constructing Fog infrastructures that match the expected IoT traffic in a
    system [23]. (2) Allocating the resource of existing Fog infrastructures to run
    IoT applications, and optimally distributing applications workloads to those resources
    in real-time. The first approach works for geographical regions that do not have
    existing Fog infrastructures. The goal here is to find the optimal geographical
    locations of Fog nodes and to determine their optimal resource requirements, i.e.,
    processing, memory, and storage. In contrast, the second approach is more realistic
    as we simply manage the resources of existing hardware in the network using optimal
    resource allocation and load distribution. Different approaches were used to manage
    these distributed resources (see Fig. 4). Most of these approaches target load
    balancing problems; However, some of them focus only on resource allocation (also
    called service placement) and task scheduling as marked in Fig. 4 by and § , respectively.
    Before balancing the load in Fog environments, we need to allocate Fog resources
    to distributed application modules, i.e., where to deploy service instances of
    IoT application modules. The allocation of resources can be static or dynamic
    [24]. With static allocation, services do not migrate after their initial deployments,
    while they can migrate during their lifetime when dynamic allocation is used,
    which is more realistic in dynamic mobile environments. In this case, efficient
    mobility-aware bandwidth reservation schemes are required to support real-time
    IoT applications to preserve valuable and scarce bandwidth resources [25]. In
    addition, the overhead of migrating services and their associated data can be
    unacceptable in realistic deployments, which is often underestimated in the literature
    [26]. It can be ignored only when migration is done on micro Fog services that
    are not associated with user data. Download : Download high-res image (595KB)
    Download : Download full-size image Fig. 4. Resource management approaches for
    distributed computing systems. Velasquez et al. [27], for example, reduced network
    latency in Fog environments by optimizing service placement using Integer Linear
    Programming (ILP). The optimization is based on the popularity of each application,
    i.e., the number of requests for each application. Then, they proposed a faster,
    but near-optimal, heuristic solution based on the PageRank algorithm. The results
    show that the heuristic approach was better in balancing the load between Fog
    nodes with latency values close to those of the ILP optimal solution. They used
    a simulator called YAFS (Yet Another Fog Simulator) [28] to test their solution
    and compared their results with the First Fit (FF) placement algorithm. However,
    they only solved the initial placement problem, i.e., static resource allocation,
    in Fog environments, which cannot adapt to dynamic environments with mobile nodes.
    The Fog placement problem was also addressed using other optimization algorithms,
    including Genetic Algorithms (GA) [29], Particle Swarm Optimization (PSO) [30],
    Gravitational Search Algorithm (GSA) [31], Monte Carlo simulations [32], and Linear
    Programming (LP) [33]. However, solving the Fog placement problem does not necessarily
    provide resiliency in the system and it might require service and data migration
    to support mobile networks, which introduces a huge overhead in dynamic systems.
    Ni et al. [34] compared static allocation strategies with dynamic resource allocation
    in the Fog, where they filtered, classified, and grouped Fog nodes based on their
    computing capabilities and their credibility. They considered the price and time
    costs needed to complete the tasks along with the credibility of end-users and
    Fog resources. However, users or resources can have similar credibility, which
    can cause sub-optimal behavior when breaking ties. In addition, the dynamic nature
    of users’ and resources’ credibility can cause deviations when calculating these
    values. Téllez et al. [35] balanced the load between Fog nodes and the Cloud by
    finding the optimal task allocation using an ILP-based Tabu search method. They
    transformed multi-objective optimization into a single-objective problem using
    the Pareto Frontier method. They used a Task Scheduling Coordinator to receive
    the tasks, schedule them, and assign them to suitable computing nodes. Puthal
    et al. [36] used Breadth-First Search (BFS) to choose the best computing node
    in the network to improve resource utilization and job response time in the system.
    Xu et al. [37] balanced the load between Fog nodes based on processing, memory,
    and bandwidth resources using static resource allocation and dynamic service migration.
    Services were partitioned to Fog nodes based on their type and predefined request
    generation rates. They compared their approach with FF, Best-Fit (BF), FF-Decreasing
    (FFD), and BF-Decreasing (BFD). Pereira et al. [38] proposed a priority-based
    load balancing algorithm with two predefined priority levels. They used a centralized
    controller with a global knowledge of system resources and workload requirements.
    The controller uses a search table to select the best available computing node
    and can create more nodes if needed. High-priority tasks are sent to the node
    with the lowest load and latency and the highest number of cores and available
    memory. While low priority tasks wait for nodes with low load or will be sent
    to the Cloud. Pinto Neto et al. [13] compared their multi-tenant load distribution
    algorithm with a delay-driven load distribution strategy. They considered homogeneous
    Fog environments with redundant services replicas. In their hierarchical architecture,
    a management node in the upper layer manages all Fog nodes in the lower layer.
    It keeps a table for the resource utilization in each Fog node as well as the
    communication delay between adjacent nodes. When a Fog node receives a task, it
    consults the management node to select the best node to run this task based on
    its priority and delay requirements. If all nodes are fully loaded, tasks are
    kept in a queue in the management node. When a node becomes available, the management
    node sends to it the task with the highest priority. Despite their near-optimality,
    machine-learning-based solutions have been also considered to balance the load
    in Fog systems. Mseddi et al. [39], for example, used Deep Q-networks (DQN) to
    maximize user satisfaction with mobile Fog nodes that are free to join or leave
    the network. It runs in multiple controllers, where each controller is responsible
    for a group of Fog nodes in its region. To build the state of the environment,
    they included the location of Fog nodes and IoT devices and the requirements of
    computational tasks. Each controller selects the best Fog node in its region that
    can perform the task within a predefined delay threshold. They compared their
    results with ILP, random, and nearest node selection algorithms. However, they
    assumed identical independent tasks with homogeneous workloads, violating the
    properties of distributed applications. Talaat et al. [40] used a Master controller
    for each isolated Fog region, which continuously monitors traffic, collects resource
    and load information, and fairly distributes requests to Fog nodes in their regions.
    They used Reinforcement Learning (RL) and GA to allocate and migrate tasks using
    Adaptive Weighted-RR (AWRR), where the decisions are based on the processing,
    memory, and Cache values of each node and other predefined conditions. They compared
    their approach with RR, Weighted-RR (WRR), and Least-Connection (LC) selection
    methods. Baek et al. [41] used SDN controllers to lower the overload probability
    of Fog nodes while reducing job latency. That was done by offloading the optimal
    number of tasks to the neighboring Fog node. They calculated a load index value
    using Q-Learning to represent the load on a given node based on a global system
    average. They compared their work with random, nearest, and Least-Queue (LQ) node
    selection schemes. Wang and Varghese [4] used Cloud-based DQN agents to decide
    on the number of services that should be offloaded from the Cloud to the Fog layer,
    and their optimal placement, considering Quality-of-Service (QoS) and running
    costs. They formulated Fog nodes as the environment and nodes characteristics
    as the state of the agent. They compared their approach with a static predetermined
    service distribution. However, they did not consider dynamic environments, where
    the number of users and workload generation rates can change over time. Also,
    they did not consider multi-tenancy, where multiple Fog applications are deployed
    on a single Fog node simultaneously. Approximation-based approaches provide semi-optimal
    results through function approximation. Conversely, random-based load balancing
    approaches provided good performances with simple implementations and minimum
    resource requirements while avoiding complex coordination between Fog nodes. For
    instance, Beraldi and Alnuweiri [42] proposed a simple load balancing algorithm
    that leverages the power of random choice property. In their solution, every time
    a node receives a job, it sends a request to a randomly selected node to take
    this task. The selected node informs the task initiator in case it accepts it.
    Otherwise, it repeats the process sequentially by sending another request to another
    randomly selected node. This sequential solution is equal to their parallel solution,
    proposed in [43], with a fan-out value equal to 1, which means sending the request
    to a single random node. In another work, Fog nodes probe random nodes to offload
    tasks if their current load exceeds a predefined threshold [44]. This is similar
    to the work in [45] except for probing a predefined number of random nodes, then
    selecting the least loaded. Similarly, Beraldi et al. [46] proposed sequential
    and adaptive random forwarding algorithms. Here, if the job is already offloaded
    more than a predefined threshold, it is executed locally or dropped based on the
    available resources. Otherwise, the node decides between processing the task locally
    and offloading it to a random node based on its current load. The sequential algorithm
    uses a predefined load threshold for each node, while the adaptive algorithm tunes
    this threshold based on the number of times the job was already forwarded. There
    are many limitations in existing load balancing approaches (summarized in Table
    1). That motivated us to propose a solution that mitigates these limitations.
    Our proposed load balancing solution optimizes multiple objectives simultaneously
    using an MCDA-based method to improve the overall system performance. MCDA-based
    methods were used to make resource selection decisions in different domains, including
    MCC systems, where smart mobile devices are utilized as computing resources [19].
    These methods achieved satisfactory performance and provided quality of service
    by selecting the most suitable resources. Furthermore, MCDA methods were also
    used to efficiently solve the service selection problem in Cloud Computing environments
    [20]. The varying significance of the selection criteria makes MCDA methods suitable
    for load balancing problems in the Fog. ELECTRE, for example, is an effective
    outranking MCDA method that was used to solve the Cloud service selection [20]
    and the Fog service placement [21] problems. Fog service selection is a more complex
    task when compared to Cloud service selection due to the distributed and decentralized
    nature of Fog computing over sparse networks with relatively slow network links.
    Therefore, the algorithms for Fog service selection need to prioritize reducing
    latency and enhancing network efficiency. In addition, the application of ELECTRE
    for Fog load balancing is still relatively new and challenging; it involves balancing
    the requests of multiple users across multiple fog nodes in real-time. Unlike
    Fog service placement, which involves choosing the most appropriate service based
    on predetermined criteria, load balancing requires dynamic decision-making based
    on real-time data (e.g., load on each fog node). This requires a more sophisticated
    and flexible approach than previous uses of ELECTRE for Fog service placement.
    Table 1. Comparison between existing load balancing approaches for Fog networks
    . Approach Optimization objectives Evaluated against Limitations How we address
    these limitations Random offloading [42], [43] Random probing [44], [45] Random
    forwarding [46] – No load balancing Can unnecessarily waste resources as it does
    not guarantee optimality Provide optimal/near-optimal solution Popularity Ranked
    Placement [27] Minimize network latency ILP and FF Initial/static service placement
    only Consider dynamic service selection to adjust to system changes Credibility-based
    Grouping [34] Minimize task completion time and processing price Predetermined
    static allocation Credibility deviation and ties Address ties by selecting the
    nearest node to save network resources ILP-based Tabu search [35] Minimize computational
    cost using Fog or Cloud – Price-based task assignment only Consider improving
    service utilization and execution delay BFS [36] Improve resource utilization
    and job response time Static, proportional, and random Uses a balanced Fog architecture
    Consider unbalanced Fog topology with unbalanced load distribution Static allocation
    with migration [37] Minimize the load-balance variance FF, BF, FFD, and BFD Migration
    cost is not considered Avoid migration, which depends on network resources and
    its traffic Priority-based search [38] Homogeneous tasks distribution No load
    balancing Predefined priorities and thresholds that must be determined by experts
    Avoid predefined parameters to dynamically adjust to system changes Multi-tenant
    load distribution [13] Priority and communication delay Delay-driven load distribution
    Homogeneous Fog architecture Consider heterogeneous resources with heterogeneous
    workloads DQN allocation and migration [39] QoS satisfaction within predefined
    delay threshold ILP, random, and nearest Considers identical atomic tasks Avoid
    pre-training/retraining and consider heterogeneous tasks RL with GA [40] Reduce
    allocation cost and response time LC, RR, WRR Migration cost is not considered
    Avoid pre-training/retraining and avoid migration Q-Learning [41] Lower overloading
    probability and job latency Random, nearest, and LQ Do not consider unbalanced
    scenarios Avoid pre-training/retraining and consider unbalanced scenarios Dynamic
    redeployment with DQN [4] Optimal placement considering QoS & running cost Static
    service distribution Single Application per Fog node only Avoid pre-training/retraining
    and consider multiple applications To the best of our knowledge, MCDA-based methods
    have never been used to balance the load in Fog environments using service selection
    decisions of stateless micro Fog service replicas. To fill this gap, we propose
    in this work an MCDA-based load balancing solution using the ELECTRE algorithm,
    where we simultaneously minimize the hop count, propagation delay, processing
    delay, execution delay, and waiting delay to improve the overall system performance.
    We compare the performance of our approach against traditional service selection
    methods, i.e., random, RR, nearest node, and fastest service selection. Evaluating
    load balancing algorithms against such baselines is a common practice in the literature
    (see Table 1). These baselines, like ELECTRE, do not require the storage or training
    of a model, enabling them to seamlessly work in constantly changing environments.
    In contrast, machine learning algorithms will always require training and retraining
    to adapt to ongoing environmental changes. Because of their simplicity and instantaneous
    decisions, traditional methods are often favored for practical Fog deployments,
    even though they are not necessarily effective in all Fog architectures. To advance
    the state of the art, our proposed solution addresses a number of limitations
    in existing work in the literature (see Table 1). For example, our solution provides
    a near-optimal solution for the load balancing problem in Fog environments. Unlike
    randomized methods (e.g., [42], [43], [44], [45], [46]), which do not guarantee
    optimal load distribution, and can unnecessarily waste compute and network resources.
    In addition, our proposed approach can easily adapt to dynamic changes in the
    topology of the system, availability of compute and network resources, as well
    as the changes in load distribution and its generation rate. In contrast to static
    allocation solutions (e.g., [27], [37]), with initial service placement of Fog
    services, which can only perform well in unrealistic static environments that
    never change over time. Our proposed solution also avoids predefined priorities
    and thresholds that are common in existing solutions (e.g., [38]), which are hard
    to be estimated even by experts. Avoiding such predetermined parameters allows
    for dynamic load balancing solutions that can easily adjust to system changes.
    We also carefully address the problem of having ties between alternatives, which
    is a problem in credibility-based grouping approaches (e.g., [34]). Hence, in
    case of a tie, i.e., more than one alternative is identified as top-ranked, the
    nearest Fog node is selected to save network resources. Additionally, our approach
    aims to improve service utilization in computing nodes while minimizing execution
    delay; this is different from price-based task assignment solutions (e.g., [35])
    that only focus on minimizing the offloading monetary cost without considering
    system performance. Moreover, we evaluate our approach in unbalanced Fog architectures
    with unbalanced load distribution, heterogeneous resources and workloads, and
    multiple simultaneous applications. In the literature, however, the evaluation
    is often simplified by considering balanced and homogeneous system architectures
    (e.g., [13], [36], [41]), identical or homogeneous workloads (e.g., [39]), and
    even considering a single application service in each Fog node (e.g., [4]). Considering
    redundant stateless micro Fog services provide resiliency in the system by maintaining
    service availability in case of possible failures in Fog nodes or network links.
    In addition, this also avoids the need for service and data migration during the
    load balancing and workload offloading process. Avoiding migration also means
    avoiding its induced cost and overhead on the system, which is often ignored for
    simplicity in existing contributions in the literature (e.g., [37], [40]). Furthermore,
    the cost of training for RL-based algorithms, and retraining in case of dynamic
    environments, is also often ignored in the literature (e.g., [4], [39], [40],
    [41]). This cost is usually huge since RL agents need to train for hundreds of
    thousands of training steps using powerful Graphics Processing Units (GPUs) and/or
    Central Processing Units (CPUs) before reaching an optimal solution. With our
    approach, no training or retraining is needed as it simply builds a number of
    matrices to make a service selection decision; this enables our approach to work
    with a small overhead, especially in dynamically changing environments. 3. ELECTRE
    Balancing the load in Fog networks includes minimizing network congestion, network
    latency, and application execution delay while maximizing the resource utilization
    of all computing nodes in the system. These objectives can be optimized simultaneously
    using a multi-objective optimization approach, a subcategory of MCDA methods.
    MCDA methods also referred to as Multi-Criteria Decision-Making (MCDM) methods,
    aim to explicitly evaluate multiple conflicting criteria in a decision-making
    process. Having MCDA methods solve the Cloud service selection problem, the Fog
    placement problem, and the MCC resource selection problem, motivated us to use
    it to efficiently distribute the load in Fog networks. ELECTRE, for instance,
    is a greedy outranking MCDA method that is based on pairwise comparisons between
    multiple criteria [18]. This category of search-based optimization methods can
    optimize Fog service selection decisions by removing outranked alternatives. It
    is a search-based method because it works by checking whether one alternative
    is better or worse than the other using pairwise comparisons between all possible
    alternatives in the solution space. Search-based methods quickly adapt to dynamically
    changing environments since they do not require training or predefined parameters.
    ELECTRE, which is a French acronym for ELimination and Choice Expressing the REality,
    was first proposed in 1965 by Bernard Roy. Since then, different versions of ELECTRE
    methods have emerged, including ELECTRE III [47]. ELECTRE III solves some limitations
    in ELECTRE II, like dealing with data inaccuracies, imprecision, and uncertainties.
    In this method, a binary outranking relationship between two alternatives and
    is represented as , which means that is at least as good as . This relationship
    is not symmetric, and hence four different scenarios exist: Alternatives are evaluated
    by problem-related criteria that are weighted by importance factors , where .
    For every criterion ; when alternative is preferred to according to that criterion.
    The novelty of ELECTRE III is the introduction of the concept of pseudo-criteria
    using discriminating thresholds for every criterion, i.e., indifference and preference
    thresholds. These thresholds can be constant or can dynamically vary along the
    scale of the values of each criterion, such that for every criterion . ELECTRE
    III defines a credibility matrix for the outranking relation using a concordance
    matrix and a discordance matrix . This credibility matrix is given as follows:
    , where: is defined as , where: while the discordance matrix is defined as: The
    concordance represents the majority among all different criteria that favor an
    alternative to another one. While the discordance represents, when the concordance
    condition holds, the minority criteria that strongly oppose that assertion. Also,
    ELECTRE III uses the concept of veto thresholds for each criterion, i.e., , which
    is a variable threshold such that for every criterion . This threshold represents
    the power of a given criterion to be against that assertion when the performance
    difference between the two alternatives for that criterion is greater than this
    threshold. In ELECTRE III, veto thresholds are used to define the discordance
    matrix, hence, they can be set large enough to make the discordance values equal
    to zero, hence, making . At least three criteria are needed to build an ELECTRE-based
    decision model. However, ELECTRE methods are more adequate when there are between
    five and thirteen criteria. Also, the criteria should be heterogeneous with ordinal,
    or weakly interval, scales, where the loss of one criterion does not benefit another.
    In addition, small differences are eliminated with the help of indifference and
    preference thresholds that dynamically scale to the data. 4. System design 4.1.
    Problem formulation To formulate our Fog environment, we use capital letters to
    refer to the set of items, i.e., nodes, links, modules, or messages. To refer
    to a single item in that set we use the lowercase of that letter with a subscript.
    Fog environments are composed of a set of nodes, i.e., computing nodes ( Cloud
    nodes and Fog nodes) and non-computing nodes ( IoT nodes and other nodes), where
    node is defined by its compute ( ) and memory ( ) resources, as shown below: ≔
    represents sensors and actuators with pure source and sink nodes, respectively.
    represents routers, switches, proxy servers, and firewalls. For load balancing
    algorithms, nodes must be characterized by identifiers (IDs), the number of instructions
    performed per unit of time (IPT), and their memory capacity (RAM). These nodes
    are connected through wired/wireless bidirectional links, where each link is characterized
    by the pair ( , ) that it connects, its bandwidth , and its propagation delay
    : ≔ IoT distributed applications are represented using distributed data flow (DDF)
    models [48]. There can be several distributed applications simultaneously running
    in the system; let be the set of these applications. The workflow of each application
    is represented by a set of modules and a set of dependencies between these modules
    , as shown here: ≔ Application modules can be categorized into three main classes,
    i.e., computing modules ( ), pure source modules ( ), and pure sink modules (
    ): ≔ Pure source/sink modules are implemented in IoT nodes as they do not perform
    computations on the generated/consumed data. Computing modules, in contrast, are
    deployed in computing nodes, i.e., Cloud, Fog, and Edge nodes. They serve incoming
    workloads and often generate a new workload for each processed request. They can
    also act as non-pure source and sink modules by periodically emitting aggregate
    or sync information based on the data they receive over a certain period. In addition,
    they can collect data without emitting messages in response, i.e., storing data
    for future analysis. Application workflows are represented as a Directed Acyclic
    Graph (DAG), where nodes represent the modules and edges represent the dependencies
    between those modules. Message transfer between module and module represents the
    dependency, denoted by , between these modules. Each message, i.e., IoT workload,
    is characterized by the number of instructions ( ) required by the workload from
    the computing entity and the size of the message in bytes ( ): ≔ Now that we formally
    formulated our nodes, links, and applications, we can define our environment (
    ) by the set of nodes , the set of links , and the set of applications , as follows:
    ≔ 4.2. Simple unbalanced environment For a realistic evaluation of our proposed
    approach, we provide a generic Fog architecture (see Fig. 5) to demonstrate the
    need for load balancing through network bottlenecks, unbalanced Fog resources,
    and unbalanced workload distribution. Then, we use a more realistic Fog topology
    to evaluate our proposed approach, with more Fog nodes that are randomly assigned
    resources under the constraint of creating unbalanced architectures. In both those
    evaluation scenarios, we consider heterogeneous computation and communication
    resources with heterogeneous workload requirements to mimic realistic Fog environments.
    We also consider non-hierarchical architectures to mimic flat Fog systems instead
    of layered ones. Starting with the simple generic architecture in Fig. 5, acts
    as a computation bottleneck as it has the least amount of computational resources;
    as represented by the smallest square in Fig. 5; while being directly connected
    to the largest number of IoT devices. In addition, the longest communication link,
    i.e., higher propagation delay, is the one that connects the fastest Fog node
    ( ) with the Cloud. This link has the lower bandwidth, i.e., represented by a
    thinner line, compared to the links that connect the Cloud with the other two
    Fog nodes. Hence, this link is a communication bottleneck when is used to process
    IoT workloads. Download : Download high-res image (131KB) Download : Download
    full-size image Fig. 5. Generic Fog topology that emphasizes the need for load
    balancing. Fig. 6 shows a simple implementation of this generic architecture using
    YAFS [28], a Discrete-event Simulator (DES) that mimics realistic deployment of
    Fog-based IoT applications. In Fig. 6, the Cloud is connected to three interconnected
    Fog nodes with heterogeneous resources. We simulate the Cloud with one order of
    magnitude more resources than , which has one order of magnitude more resources
    than , which also has an order of magnitude more resources than . A communication
    bottleneck exists between the fastest Fog node, i.e., , and the Cloud. In addition,
    the slowest Fog node, i.e., , is connected to an order of magnitude more IoT devices
    than . While the fastest Fog node only accepts offloaded workloads from the other
    two Fog nodes since it is not connected to any IoT devices. This unbalanced resource
    distribution creates a computation bottleneck, which, alongside the communication
    bottleneck, emphasizes the need for load balancing in this architecture. Download
    : Download high-res image (194KB) Download : Download full-size image Fig. 6.
    A simple Fog architecture with three interconnected Fog nodes. The generic architecture
    has three deployed applications with heterogeneous workload requirements. It has
    four modules with three dependencies, i.e., messages, requests, or workloads,
    between them. Service replicas for Sensor and Actuator modules are deployed in
    IoT devices. A single service replica for the Cloud module is deployed in the
    Cloud, and each Fog node has a stateless service replica of the Fog module. For
    the load balancer, this is a local view of the network with the available Fog
    nodes that can serve a given workload. Each IoT device has instances of the three
    Sensor and three Actuator modules, one for each application. Fog nodes and the
    Cloud have instances of the three Fog and Cloud modules, respectively, one for
    each application. Workload requirements were chosen relative to the resources
    of computing nodes to simulate resource-demanding, moderate, and light workloads.
    This was done by defining the workload requirements of with one order of magnitude
    more compute instructions than to simulate resource-demanding and moderate workloads,
    respectively. In addition, was defined with one order of magnitude fewer workload
    requirements than to represent light workloads. Sensor messages are generated
    by IoT devices using a Poisson Point Process. These messages wait for their turn
    to access network links since nodes and links are modeled in YAFS as M/M/1 queuing
    models. Modeling resources with infinite queue sizes allows studying network saturation
    as a performance measure instead of workload-dropping probability. The next group
    of messages in the workflow, i.e., Fog messages, are triggered after Fog nodes
    are done serving Sensor messages. The same happens when the Cloud finishes processing
    Fog messages, when it triggers Cloud messages as a response. Cloud messages are
    finally consumed by the Actuator module in the IoT device that initiated that
    particular loop. 4.3. Load balancing algorithms Load balancing takes place by
    selecting the proper Fog node to serve an incoming IoT workload. Selecting the
    nearest Fog node to the source IoT device creates a computation bottleneck on
    , which has the least amount of resources while being connected to around 91%
    of the system’s IoT devices. In contrast, selecting the fastest node creates communication
    bottlenecks on the links connecting , because they have smaller bandwidths and
    larger propagation delays. Another simple approach is for each IoT device to select
    between the three Fog nodes in a Round-Robin manner, which we called Distributed
    Round-Robin (DRR). We also included a random service selection approach, which
    leverages the power of random-choice-property to randomly select a Fog node for
    every newly generated workload. This approach is similar to the sequential randomization
    load balancing solution proposed in [42] with the difference that the selected
    node must accept the assigned request. Hence, the process of informing the task
    initiator about accepting or rejecting the request, and the process of repeating
    the selection process in case of rejection are no longer needed. This helps us
    study workload saturation rather than studying the dropping probability of workloads.
    Our proposed ELECTRE-based method (see Algorithm 1), however, considers five criteria
    (see Table 2) to select an optimal Fog node, i.e., an optimal alternative, to
    process each generated workload based on the outranking relation . Each criteria
    for each two alternatives, i.e., , is represented by the concordance and discordance
    matrices and , respectively. The objective is to find alternatives that minimize
    these criteria to determine their outranking relationships. Alternatives that
    are dominated by others are then identified and eliminated based on these relationships;
    this results in a smaller set of alternatives. In an iterative procedure, this
    results in an ordering/ranking of alternatives with the possibility of having
    ties in the ranks. Download : Download high-res image (318KB) Download : Download
    full-size image The shortest path between the source and destination is used for
    calculating the hop count, propagation time, and execution delay. If there are
    multiple shortest paths, i.e., with the same number of links, the path with the
    smallest propagation time is selected. Given a request to travel links between
    its source and a destination candidate Fog node , along with tasks that are currently
    waiting to be processed in this candidate Fog node, we can formally define these
    five criteria as follows: Table 2. The criteria used in our ELECTRE-based load
    balancing algorithm for each candidate Fog node . Criteria Definition Hop count
    The number of links on the path between the node that generates the load and the
    candidate Fog node. Propagation delay The accumulative propagation time of all
    the links that connect the workload source and the candidate Fog node. Processing
    delay The time a candidate Fog node takes to process a single computational instruction.
    Execution delay The total execution delay to process the given workload on the
    candidate Fog node. Waiting delay The current load on the candidate Fog node.
    The link propagation delay is a constant value for each link representing the
    time required to transmit a single bit from one end of the link to the other.
    It represents the physical link proprieties and the distance between the pair
    of nodes it connects, i.e., material and length, respectively. The link communication
    delay is determined by the transmission delay ( ) of request , which is given
    by dividing the message size in bytes ( ) by the link bandwidth in Bps ( ). This
    represents the time required to push the whole message into the communication
    link. Both propagation and transmission delays represent the time required for
    the whole message to reach the other end of the link. The execution delay is calculated
    based on the network communication delay and the processing time ( ) of request
    on the candidate Fog node . The network communication delay is given by adding
    propagation and transmission delays for every link between the source and the
    destination. The service time, i.e., processing delay ( ), is calculated by dividing
    the workload’s required instructions ( ) by the computational capabilities of
    the candidate Fog node, i.e., . Finally, the expected waiting delay ( ) in a computing
    node ( ) represents the load information for that node. It is calculated by adding
    the processing delay ( ) of every task that is currently waiting to be served
    by node ( ). Except for the expected waiting delay, all other criteria need information
    about the resource capabilities of candidate Fog nodes, which can be collected
    using triggered updates only, i.e., during upgrades or downgrades of these resources.
    However, the current load in Fog nodes, i.e., their resource availability, requires
    an active monitoring system that collects this information from candidate Fog
    nodes. This real-time information allows adapting to recurring topological changes
    and fluctuations in workload generation rates. The criteria, in our simulations,
    have identical weights for every criterion , where is the number of criteria used
    in the algorithm. This gives equal importance to every criterion since, in ELECTRE,
    the weights are a measure of the relative importance of the criteria [49], [50].
    These weights depend neither on the ranges nor the scales of the values in each
    criterion [51]. In this case, weights can be viewed as the number of votes given
    to a criterion in a voting procedure, which indicates the relative importance
    of each criterion [52], [53]. The algorithm avoids alternatives, i.e., Fog nodes,
    that maximize these criteria, which means that; indeed, it chooses alternatives
    that minimize all criteria for every incoming workload. Therefore, ELECTRE can
    simultaneously optimize these criteria, given their equal importance, to optimize
    the overall system performance. The algorithm dynamically calculates the preference
    and indifference thresholds, in every decision epoch, using percentile ranking.
    The veto threshold, for example, was set to the 100th percentile rank of every
    criterion to avoid discarding alternatives that surpass that threshold. On the
    other hand, we use an indifference threshold of one-third of the 10th percentile
    rank while having the preference threshold equal to the 20th percentile rank.
    This was done to avoid considering small differences in preferences as significant.
    To rank Fog nodes from best to worst, ELECTRE outranks the worst alternatives
    until the best alternative is identified, according to the thresholds discussed
    above. However, because of indifference thresholds, the possibility of having
    ties among alternatives exists, and hence a set of alternatives can be identified
    as top-ranked instead of a single one. In case of a tie, we choose the nearest
    Fog node to the source device to avoid unnecessary use of network links. In 30
    experiment runs, the average number of ties in the generic architecture was ,
    and the nearest Fog node to the workload source was selected instead. 4.4. Realistic
    unbalanced environment To validate our findings from the simple generic Fog architecture,
    a larger and more complex Fog architecture is used. Fig. 7 shows a YAFS implementation
    of such architecture, which is produced from a randomized graph generator that
    simulates the Internet Autonomous System (AS) network [54]. To create a Fog system
    out of this graph, we compute the shortest-path betweenness centrality for each
    node; it is a measure of centrality in the graph based on shortest paths. In other
    words, for each node, the betweenness centrality is the number of the shortest
    paths that pass through this node [55]. To add a Cloud in the center of the architecture,
    we create a Cloud node and connect it to the two nodes that have the highest betweenness
    centrality in this randomized graph. Nodes with zero centrality, i.e., edge nodes
    in the graph, are identified as IoT devices, while the rest of the nodes are identified
    as Fog nodes. Fog nodes are then assigned their compute resources based on their
    betweenness centrality to resemble unbalanced resources with unbalanced load distribution.
    To do this, Fog nodes with high centrality get smaller IPT values compared to
    Fog nodes with smaller centrality. Download : Download high-res image (208KB)
    Download : Download full-size image Fig. 7. AS-inspired randomized unbalanced
    Fog environment with 21 IoT devices (purple), 11 Fog nodes (orange), and a Cloud
    (green). The size of the node represents its computing capabilities. (For interpretation
    of the references to color in this figure legend, the reader is referred to the
    web version of this article.) In addition to the use of a more complex Fog architecture
    to validate the performance of our approach, we also use a more complex application
    workflow to execute in these complex Fog environments. Instead of using a single
    application loop for each application workflow, we now consider two loops for
    each application workflow (see Fig. 8). Loop 1 represents the immediate feedback,
    through what we called Fog Down messages. These messages travel from Fog nodes
    to IoT devices after processing Sensor workloads, where 100% of Sensor workloads
    trigger this feedback. On the other hand, Loop 2 represents data aggregation and
    Cloud feedback through Cloud messages, where only 10% of Sensor workloads will
    trigger messages to the Cloud (Fog Up messages) and only 50% of those Fog Up messages
    will trigger Cloud feedback to the initiating IoT device. This way we mimic the
    workflow of many realistic applications, where immediate feedback is needed from
    Fog nodes for every processed Sensor workload. While the Cloud is only involved
    in a portion of those processed messages to perform data aggregation and/or provide
    feedback based on the aggregated data. This distributed application workflow reasonably
    represents existing IoT applications, such as online games, IoV applications,
    and health monitoring systems. Download : Download high-res image (230KB) Download
    : Download full-size image Fig. 8. A complex distributed application workflow
    for the AS-inspired Fog architecture, with two application loops. 5. Evaluation
    of ELECTRE method 5.1. Simulation parameters In Table 3, we show the system’s
    parameters used to perform our experiments on the generic Fog architecture. In
    these experiments, IoT workloads, i.e., Sensor messages, are generated as a Poisson
    Point Process using an exponential distribution with a scale parameter of 100,
    which is common for IoT workload generation rates [21]. The scale parameter is
    the inverse of the rate parameter , which is another widely used parameter for
    the exponential distribution that represents the rate of occurrences in a Poisson
    process. The exponential distribution is the probability distribution of the time
    between events in a Poisson Point Process (see Fig. 9), i.e., a process in which
    events occur continuously and independently at a constant average rate [56]. With
    exponential distribution, the average time between events, i.e., average inter-arrival
    time, is constant and known through its scale parameter (100 time-steps in our
    simulations). This distribution is supported on the interval , but 0 is replaced
    by 1 in our simulations to avoid generating simultaneous tasks in the DES environment,
    i.e., to create gap intervals between events. The exponential distribution is
    selected because it easily simulates, using a DES environment, the expected time
    interval between generated workloads in a Poisson process. Unlike the Poisson
    distribution, which simulates the expected number of workloads that are generated
    between two time-steps in that process (see Fig. 9). Fig. 9 shows that smaller
    intervals are generated with a higher probability; this means that most workloads
    are generated with small inter-arrival intervals. This is why exponential distribution
    realistically mimics real-time IoT applications with heterogeneous, but frequent,
    generation rates. Table 3. Simulation system parameters for the generic architecture.
    Parameters Details Algorithms Service selection Random, DRR, nearest, fastest,
    & ELECTRE Simulation Number of iterations 10 experiment runs each with a different
    random seed Simulation duration 10 000, 100 000, & 100,000 Time-steps Applications
    Number of apps 3 Heterogeneous apps with three modules in each app, i.e., Source,
    Fog, & Cloud modules Workload instructions 100, 1000, & 10 000 IPT for each App
    Workload sizes 10, 100, & 1000 Bytes for each App Nodes IoT 22 devices with CPU
    power of 10 IPT each Fog 3 nodes with CPU power of 1000, 10 000, & 100 000 IPT
    for , , & , respectively Cloud 1 Cloud with CPU power of 1 000 000 IPT Links IoT-Fog
    =1000 & =1 =1000 & =2 Fog-Cloud -Cloud & -Cloud ( =100 000 & =10), & -Cloud (
    =1000 & =20) Download : Download high-res image (319KB) Download : Download full-size
    image Fig. 9. Poisson vs. Exponential distributions to represent a Poisson-point-process
    over time-steps. In our experiments, simulation time-steps refer to seconds for
    easier analogy and they will be used interchangeably throughout the rest of the
    text. For instance, the processing power is measured in Instructions Per Time-step
    (IPT) or Instructions per Second (IPS). Similarly, the bandwidth ( ) and the propagation
    delay ( ) are defined in our simulations in Bytes per second (Bps) and seconds,
    respectively. The values for the bandwidth and the propagation delay in our simulations
    were chosen to simulate available link resources in shared, i.e., public, network
    environments, especially at peak hours. In the AS-inspired evaluation environment,
    workloads are generated using the same exponential distribution. However, a different
    network topology is generated, in each of the 10 experiment runs, using a random
    AS graph generator. PR and BW resources, for every link in the network, are uniformly
    chosen from the range of values listed in Table 4. These network parameters are
    inspired from [21], and they were chosen relative to workload requirements to
    simulate resource-demanding workloads in networks with scarce resources. Considering
    scarce resources helps demonstrate the effectiveness of load balancing algorithms
    in extreme conditions. Likewise, the resource capabilities of computing nodes
    and the resource requirements of application workloads were chosen relative to
    each other to simulate high-demand workloads that require large computing power
    from limited computing resources. Table 4. Network parameters for the AS-inspired
    architectures . Parameter Values IoT devices Nodes with 0 betweenness centrality,
    each with CPU power of 10 IPT Fog nodes IPT values are evenly spaced integers
    over the interval , values are then assigned to Fog nodes inversely proportional
    to their betweenness centrality The Cloud Connected to the two Fog nodes with
    the highest betweenness centrality, and assigned CPU power of IPT IoT-Fog links
    BW and PR values are drawn from a uniform distribution over the intervals and
    , respectively Fog-Fog links BW and PR values are drawn from a uniform distribution
    over the intervals and , respectively Fog-Cloud links BW and PR values are drawn
    from a uniform distribution over the intervals and , respectively 5.2. Performance
    evaluation in a simple environment The implementations of the random offloading
    and the DRR selection algorithms are straightforward, where in the former, IoT
    devices randomly select one of the three Fog nodes to serve Sensor workloads.
    For DRR, the three Fog nodes are alternatively selected by each IoT device in
    a Round-Robin manner. With the nearest node selection algorithm, IoT devices select
    directly connected Fog nodes to serve Sensor workloads, which means that is never
    selected using this method. receives around 91% of load as it is directly connected
    to around 91% of IoT devices, while receives workloads from only two IoT devices.
    This approach, which is common in practice in Edge Computing environments, wastes
    computing and communication resources that shall be utilized to increase the overall
    system performance. Using the fastest service selection algorithm, the Fog node
    with the smallest total execution delay is always selected, which is calculated
    by adding the service time to the network latency as we have discussed in the
    previous section. In the simple generic architecture, the fastest service for
    IoT devices 4 & 5 is always their directly connected Fog node ( ) (see Fig. 10(a)).
    However, for the rest of the IoT devices, only less demanding workloads are sent
    to the directly connected Fog node, i.e., , because of its scarce resources. While
    high-demanding workloads, i.e., from , are sent to as it has the fastest service
    for such workload requirements. Our proposed approach, however, has different
    behavior for IoT devices that are directly connected to (see Fig. 10(b)). ELECTRE
    distributes high-demand workloads, i.e., requests, between and to avoid the communication
    bottleneck between and the Cloud. This is because of the transmission of large
    workload data over limited network resources, which significantly affects the
    performance of the application loop as a whole. However, ELECTRE chooses to send
    smaller workloads to the fastest Fog node as they have less impact on network
    transmission between and the Cloud. Fig. 11 confirms these findings, where is
    always selected for IoT devices 4 & 5 using the nearest node, the fastest service,
    and the ELECTRE algorithms. While for the rest of the IoT devices, , , and the
    three Fog nodes are chosen by these three algorithms, respectively. Download :
    Download high-res image (354KB) Download : Download full-size image Fig. 10. The
    distribution of workloads in the generic architecture. Since the network is modeled
    as an M/M/1 queue model, we can calculate the number of messages that are, by
    the end of the simulation, still waiting to access a network link. Typically,
    these messages will have to stay in a network buffer during the simulation until
    they get their turn to access the network link, which is done in a First-Come-First-Served
    (FCFS) manner. Fig. 12 shows that the nearest node selection algorithm produces
    the fewest number of waiting messages as it pumps the least number of messages
    into the network. The reason for this is choosing the slowest Fog node ( ) 91%
    of the time because it is directly connected to the majority of IoT devices in
    our unbalanced generic architecture. Hence, will act as a computation bottleneck,
    emitting a smaller number of messages through the application workflow. Download
    : Download high-res image (426KB) Download : Download full-size image Fig. 11.
    Load of Fog nodes from each Sensor (over time-steps). Fig. 13 supports this analogy
    by showing the number of sent messages, i.e., messages that traveled a communication
    link. It confirms that the nearest node selection algorithm sends the least number
    of messages to the system for the reason discussed above. Fig. 14 shows the number
    of messages that were served by each service, which also supports the same fact
    regarding acting as the bottleneck of the system due to its poor computing resources.
    The number of workloads served by the Cloud module using the nearest service selection
    algorithm is less than all other algorithms. That is due to the number of workloads
    served by the system computation bottleneck ( ). Download : Download high-res
    image (133KB) Download : Download full-size image Fig. 12. Network saturation
    (over the simulation duration of time-steps). Our proposed ELECTRE algorithm smartly
    distributes workloads based on the workload’s resource requirements and the current
    load of Fog nodes. This allows more workloads to be served in the Cloud, especially
    high-demanding workloads, i.e., from . The reason for this is the intelligent
    workload assignment to the resource-rich Fog node, i.e., (see Fig. 14). ELECTRE
    selects for workloads from , and distributes resource-demanding workloads (from
    ) between . Download : Download high-res image (241KB) Download : Download full-size
    image Fig. 13. Number of transmitted messages (over time-steps). Download : Download
    high-res image (424KB) Download : Download full-size image Fig. 14. Number of
    served messages (over time-steps). Fig. 15 shows how the nearest node selection
    algorithm produces the minimum network latency values compared to the rest of
    the algorithms. However, this is a bit optimistic since it is calculated for all
    transmitted messages in the network. Hence, latency values for this algorithm
    must be smaller as it sends the smallest number of messages over the network compared
    to all other algorithms. Moreover, spending more time processing workloads in
    the system bottleneck ( ) causes a small number of Fog messages to be sent over
    network links, and hence a small number of Cloud messages will be sent accordingly.
    Because a small number of messages compete for links resources using this algorithm,
    the waiting time for these messages to access network links will be smaller. In
    addition, Sensor messages only travel a single link from source to destination
    using the nearest node selection algorithm. Unlike the rest of the algorithms,
    where they might travel multiple links to reach their destination. This adds to
    why latency values are smaller than other algorithms using the nearest node selection
    algorithm. ELECTRE achieved the second-best performance in terms of latency even
    with Sensor messages traveling multiple links to reach their destination Fog node.
    However, latency here is more realistic as more messages are generated using ELECTRE,
    compared to the nearest node selection algorithm. Download : Download high-res
    image (246KB) Download : Download full-size image Fig. 15. Network latency of
    transmitted messages (resampled every time-steps over the simulation duration
    of time-steps). After evaluating the performance of these algorithms in terms
    of individual messages, we now evaluate their performance in terms of the whole
    application loop by accumulating the execution delay of each message in that loop
    (see Fig. 16). The nearest node selection algorithm achieved the minimum mean
    loop execution delay. This is too optimistic; as stated earlier; because this
    method sends the least number of messages along each application loop. In contrast,
    ELECTRE achieved the second-best mean loop execution delay while sending more
    messages along that loop. To provide a fair comparison between the performance
    of each algorithm in our generic architecture, we evaluate the loop execution
    delay in terms of the number of messages, or bytes, transmitted along that loop.
    This is done using what we call the mean loop transfer rate (see Fig. 17), which
    is calculated by dividing the number of transmitted bytes along an application
    loop by the mean execution delay of that loop. The mean loop transfer rate, measured
    in Bytes per Second (BPS), can be viewed as the overall performance of that loop.
    This performance measure provides fairness by considering a good algorithm to
    be the one that transmits as many messages as possible while maintaining a small
    loop execution delay. Using this metric, in Fig. 17, DRR and nearest node selection
    algorithms achieve the worst performance compared to the rest of the algorithms.
    The random-based approach achieves slightly better performance by increasing the
    system’s resource utilization using random load distribution between Fog nodes.
    Download : Download high-res image (241KB) Download : Download full-size image
    Fig. 16. Mean execution delay for the three distributed application loops. Our
    proposed ELECTRE-based method achieved the best overall system performance, i.e.,
    the highest loop transfer rate, compared to other methods in this study. Over
    simulation time-steps, our approach achieves a 1% improvement over the second-best
    approach in our study, i.e., the fastest service selection. However, the performance
    of our approach significantly increases when running the simulation over longer
    time-steps, i.e., and , where it achieves 20% and 23% improvement over the second-best
    approach, respectively. Thus, over time-steps, our approach achieved an improvement
    between 23% and 67% over the other approaches used in this study. Download : Download
    high-res image (378KB) Download : Download full-size image Fig. 17. Mean loop
    transfer rate (overall system performance). Fig. 18 shows the module utilization
    that was achieved by each one of these algorithms. The fastest Fog node ( ) is
    never utilized using the nearest node selection algorithm, while it is used the
    most using the fastest service selection algorithm. In contrast, the slowest Fog
    node ( ) was used the most using the nearest node selection algorithm, while it
    was used the least using the fastest service selection algorithm. A better utilization
    for both and is very important since they are directly connected to IoT devices.
    Unlike , which must be carefully used since it requires traveling more network
    links to be reached from IoT devices. In our generic architecture, we see this
    behavior using ELECTRE, where it utilizes for lighter workloads as discussed earlier.
    On the other hand, it distributes resource-demanding workloads that are initiated
    from the majority of IoT devices, i.e., those connected to the slowest Fog node,
    between . This is why is utilized the most using ELECTRE, while it uses more than
    all other algorithms except the nearest node method. Considering workload requirements,
    resource capabilities of Fog nodes, and their resource availability allows our
    proposed method to outperform other traditional methods with better load balancing
    in this generic unbalanced Fog architecture. Download : Download high-res image
    (170KB) Download : Download full-size image Fig. 18. Module utilization for each
    message type (over time-steps). 5.3. Performance evaluation in a realistic environment
    To confirm our findings in the generic architecture, we evaluate these algorithms
    on our AS-inspired Fog environment. Fig. 19 shows the performance of the five
    service selection algorithms in the AS-inspired topology averaged over the two
    application loops of each application. It shows how the mean loop execution delay
    and the number of served requests increase by increasing the simulation duration
    in our experiments. Fig. 19 shows how the nearest node selection algorithm performs
    the worst in this environment, i.e., with the largest mean loop execution delay
    and the smallest number of requests served along the application loops. The fastest
    service selection algorithm has the second worst performance in terms of the mean
    loop execution delay. However, the largest number of served requests was achieved
    using this method. Although, the number of served requests along application loops
    using our proposed method is very close to that achieved by the fastest service
    selection algorithm. Nevertheless, our proposed method achieved the best mean
    loop execution delay in this complex environment. To check why the nearest node
    and the fastest service selection algorithms performed the worst in the AS-inspired
    architecture, we examine the distribution of workloads in this environment (see
    Fig. 20). Download : Download high-res image (228KB) Download : Download full-size
    image Fig. 19. The growth of the mean loop execution delay and the number of served
    requests in the AS-inspired topology over three simulation durations. Download
    : Download high-res image (499KB) Download : Download full-size image Fig. 20.
    The distribution of workloads in the AS-inspired Fog topology. In Figs. 20(a)
    & 20(b), we see the distribution of workloads from IoT devices to Fog nodes in
    the AS-inspired architecture using the fastest service and the nearest node selection
    algorithms, respectively. Fog nodes 2 & 3 were never selected by any IoT device
    using the nearest node selection algorithm since they were not directly connected
    to any IoT device. Using the fastest service selection algorithm, Fog node 3 was
    never used while Fog node 2 is used for most resource-demanding workloads. The
    reason for choosing Fog node 2 for these types of workloads is having more resources
    compared to the resources of directly connected Fog nodes of most IoT devices.
    Interestingly, our proposed ELECTRE-based method smartly distributes the load
    between all Fog nodes in the system (see Fig. 20(c)). ELECTRE distributes applications
    workloads among a number of Fog nodes, which increases their resource utilization
    while minimizing the number of waiting requests. This allows ELECTRE to achieve
    the smallest average waiting delay in the AS-inspired environment (see Fig. 21(b)).
    Although, the nearest node and fastest service selection algorithms achieved the
    smallest average latency and service time (Figs. 21(a) and 21(c)), respectively.
    However, the effect of reducing the waiting delay is significant, which is why
    ELECTRE has achieved the best response time (see Fig. 21(d)), and the best execution
    delay accordingly (total response time in Fig. 21(e)). The results on the randomized
    AS-inspired Fog architectures confirm our findings on the generic Fog architecture.
    These results help us understand the reasons for outperforming traditional baseline
    approaches using our proposed method. However, this performance gain does not
    come without a cost, which is in our case the computational complexity of the
    ELECTRE algorithm compared to these simple traditional methods. Still, this complexity
    can be acceptable when the performance gain is significant for the whole system.
    Moreover, the algorithm can run in resource-rich controllers that act as load
    balancers for the system to mitigate its complexity. This can be done through
    SDN-based Fog systems, where SDN controllers make offloading decisions to distribute
    the load in the system. In addition, ELECTRE can be used as a baseline to evaluate
    new load balancing algorithms, besides traditional baseline approaches. Download
    : Download high-res image (690KB) Download : Download full-size image Fig. 21.
    Average delays in the AS-inspired complex Fog topology. 6. Conclusion Fog Computing
    is one of the main building blocks that enable the development of distributed,
    real-time, and delay-sensitive IoT applications. IoT devices are often resource-limited
    and battery-powered, which calls for the exploitation of Internet-based computing
    and storage resources. The Cloud was the first source of such resources, but Fog
    Computing quickly emerged to provide these resources in proximity to IoT devices.
    However, the vast number of IoT devices requires efficient utilization of such
    distributed resources, which can be only done through efficient load distribution.
    But load balancing in the Fog is very complex due to various factors, including
    but not limited to: • Heterogeneity of Fog, IoT, and network resources as well
    as IoT application modules. • Using undedicated, public, and slow network links.
    • Fluctuations in workload generation rates. • Physical distribution of IoT devices
    and Fog nodes, i.e., IoT and Fog communities. • Proximity of Fog resources to
    IoT devices, i.e., number of network links and communication latency. • Fog/IoT
    mobility and their re-association process, i.e., dynamically changing environments.
    In this work, we provide an efficient load balancing algorithm for Fog environments
    through efficient task assignment decisions, i.e., by selecting optimal Fog service
    replicas to serve IoT workloads. Considering stateless micro Fog service replicas
    provides network resilience in such environments by maintaining service availability
    in the face of faults and challenges to normal operation. To demonstrate the effectiveness
    of our approach in a realistic setup, we evaluated our approach in two Fog architectures
    with unbalanced resource and load distribution, heterogeneous resources, heterogeneous
    workload requirements, and a semi-hierarchical topology. We start evaluating our
    approach on a simple and generic Fog architecture, which can be used to evaluate,
    evolve, and advance load balancing algorithms. Then, we evaluate our proposed
    solution in a randomized, complex, and realistic Fog setup that is inspired by
    the architecture of AS networks. Since our proposed ELECTRE algorithm does not
    require storing a trained model, we only compare it against simple service selection
    methods that automatically adapt to environmental changes. On the other hand,
    sophisticated machine learning methods will require retraining their model for
    ongoing environmental changes. The first simple selection method is a random-based
    algorithm inspired by a sequential randomization load balancing solution from
    the literature. This simple randomized approach exploits the power of random choice
    property to achieve good results with minimal overhead. In addition, we also implemented
    DRR, nearest node, and fastest service selection algorithms to be compared against
    our proposed approach. The results of this study help us understand the importance
    of efficient load distribution in unbalanced Fog networks, especially with resource-demanding
    heterogeneous workload requirements. Efficient load distribution is achieved by
    increasing Fog resource utilization while minimizing the average loop execution
    delay of distributed IoT applications. Our proposed ELECTRE-based method outperformed
    the other traditional baseline methods used in this study with improvements of
    up to 67% in the generic architecture. Our proposed method has also outperformed
    the other methods in the AS-inspired randomized architecture over 10 randomized
    experiment trials. Using our proposed approach, more messages were sent over the
    network while achieving a better loop transfer rate, mean execution delay, mean
    network latency, and utilization of computing resources in Fog nodes. However,
    this performance gain comes with the cost of requiring more storage and processing
    power when compared to simple traditional methods. However, this overhead can
    be acceptable when there is a significant gain in the overall system performance.
    In addition, the overhead of running these algorithms can be minimized when load
    balancing is done through resource-rich network controllers, like SDN Controllers.
    In future work, we want to evaluate approximation-based RL algorithms in large-scale
    Fog environments. The use of Deep Neural Networks (DNN) in approximation-based
    solutions makes them suitable to run in resource-limited devices, like IoT, Edge,
    and Fog devices. In addition, RL agents can learn in partially observable environments,
    which will allow them to perform without the need for collecting real-time load
    information from every computing node in the system. Instead, approximating load
    information can be enough to provide efficient load distribution between Fog nodes
    while minimizing the overhead of transmitting real-time load information over
    the network. Hence, using such lightweight solutions will allow the implementation
    of efficient distributed load balancing algorithms that minimize compute, storage,
    and network overhead. Declaration of Competing Interest The authors declare that
    they have no known competing financial interests or personal relationships that
    could have appeared to influence the work reported in this paper. Acknowledgment
    The research was supported by FQRNT and NSERC . References [1] Yi S., Li C., Li
    Q. A survey of fog computing: Concepts, applications and issues Proceedings of
    the 2015 Workshop on Mobile Big Data (Mobidata ’15), Association for Computing
    Machinery, New York, NY, USA (2015), pp. 37-42, 10.1145/2757384.2757397 View in
    ScopusGoogle Scholar [2] Guan Y., Shao J., Wei G., Xie M. Data security and privacy
    in fog computing IEEE Netw., 32 (5) (2018), pp. 106-111, 10.1109/MNET.2018.1700250
    View in ScopusGoogle Scholar [3] Antonini M., Vecchio M., Antonelli F. Fog computing
    architectures: A reference for practitioners IEEE Internet Things Mag., 2 (3)
    (2019), pp. 19-25, 10.1109/IOTM.0001.1900029 Google Scholar [4] Wang N., Varghese
    B. Context-aware distribution of fog applications using deep reinforcement learning
    (2020) arXiv preprint arXiv:2001.09228. URL https://arxiv.org/abs/2001.09228 Google
    Scholar [5] Rana O., Shaikh M., Ali M., Anjum A., Bittencourt L. Vertical workflows:
    Service orchestration across cloud & edge resources Proceedings of the 6th International
    Conference on Future Internet of Things and Cloud (FiCloud), IEEE (2018), pp.
    355-362, 10.1109/FiCloud.2018.00058 View in ScopusGoogle Scholar [6] T. S., S.
    C.S., S. S. The IoT-based real-time image processing for animal recognition and
    classification using deep convolutional neural network (DCNN) Microprocess. Microsyst.,
    95 (2022), Article 104693, 10.1016/j.micpro.2022.104693 URL https://www.sciencedirect.com/science/article/pii/S014193312200223X
    View PDFView articleView in ScopusGoogle Scholar [7] Karakaya A., Ulu A., Akleylek
    S. GOALALERT: A novel real-time technical team alert approach using machine learning
    on an IoT-based system in sports Microprocess. Microsyst., 93 (2022), Article
    104606, 10.1016/j.micpro.2022.104606 URL https://www.sciencedirect.com/science/article/pii/S0141933122001508
    View PDFView articleView in ScopusGoogle Scholar [8] Martinez I., Hafid A.S.,
    Jarray A. Design, resource management, and evaluation of fog computing systems:
    A survey IEEE Internet Things J., 8 (4) (2021), pp. 2494-2516, 10.1109/JIOT.2020.3022699
    View in ScopusGoogle Scholar [9] Nadembega A., Hafid A., Taleb T. A destination
    and mobility path prediction scheme for mobile networks IEEE Trans. Veh. Technol.,
    64 (6) (2015), pp. 2577-2590, 10.1109/TVT.2014.2345263 View in ScopusGoogle Scholar
    [10] Karagiannis V., Schulte S. Comparison of alternative architectures in fog
    computing Proceedings of the 4th International Conference on Fog and Edge Computing,
    ICFEC, IEEE (2020), pp. 19-28, 10.1109/ICFEC50348.2020.00010 View in ScopusGoogle
    Scholar [11] Kashani M.H., Ahmadzadeh A., Mahdipour E. Load balancing mechanisms
    in fog computing: A systematic review (2020) arXiv preprint arXiv:2011.14706.
    URL https://arxiv.org/abs/2011.14706 Google Scholar [12] Sthapit S., Thompson
    J., Robertson N.M., Hopgood J.R. Computational load balancing on the edge in absence
    of cloud and fog IEEE Trans. Mob. Comput., 18 (7) (2019), pp. 1499-1512, 10.1109/TMC.2018.2863301
    View in ScopusGoogle Scholar [13] Pinto Neto E.C., Callou G., Aires F. An algorithm
    to optimise the load distribution of fog environments Proceedings of the 2017
    International Conference on Systems, Man, and Cybernetics, SMC, IEEE (2017), pp.
    1292-1297, 10.1109/SMC.2017.8122791 Google Scholar [14] C. Puliafito, E. Mingozzi,
    G. Anastasi, Fog Computing for the Internet of Mobile Things: Issues and Challenges,
    in: 2017 IEEE International Conference on Smart Computing, SMARTCOMP, 2017, pp.
    1–6, http://dx.doi.org/10.1109/SMARTCOMP.2017.7947010. Google Scholar [15] Yu
    Y., Li X., Qian C. SDLB: A scalable and dynamic software load balancer for fog
    and mobile edge computing Proceedings of the Workshop on Mobile Edge Communications,
    MECOMM ’17, Association for Computing Machinery, New York, NY, USA (2017), pp.
    55-60, 10.1145/3098208.3098218 Google Scholar [16] B. Wagner, A. Sood, Economics
    of Resilient Cloud Services, in: 2016 IEEE International Conference on Software
    Quality, Reliability and Security Companion (QRS-C), 2016, pp. 368–374, http://dx.doi.org/10.1109/QRS-C.2016.56.
    Google Scholar [17] Beraldi R., Canali C., Lancellotti R., Mattia G.P. Distributed
    load balancing for heterogeneous fog computing infrastructures in smart cities
    Pervasive Mob. Comput., 67 (2020), Article 101221, 10.1016/j.pmcj.2020.101221
    URL https://www.sciencedirect.com/science/article/pii/S1574119220300791 View PDFView
    articleView in ScopusGoogle Scholar [18] Govindan K., Jepsen M.B. ELECTRE: A comprehensive
    literature review on methodologies and applications European J. Oper. Res., 250
    (1) (2016), pp. 1-29, 10.1016/j.ejor.2015.07.019 URL https://www.sciencedirect.com/science/article/pii/S0377221715006529
    View PDFView articleView in ScopusGoogle Scholar [19] Pramanik P.K.D., Biswas
    S., Pal S., Marinković D., Choudhury P. A comparative analysis of multi-criteria
    decision-making methods for resource selection in mobile crowd computing Symmetry,
    13 (9) (2021), 10.3390/sym13091713 URL https://www.mdpi.com/2073-8994/13/9/1713
    Google Scholar [20] Whaiduzzaman M., Gani A., Anuar N.B., Shiraz M., Haque M.N.,
    Haque I.T. Cloud service selection using multicriteria decision analysis Sci.
    World J., 2014 (2014), pp. 1-10, 10.1155/2014/459375 Google Scholar [21] Lera
    I., Guerrero C., Juiz C. Analyzing the applicability of a multi-criteria decision
    method in fog computing placement problem Proceedings of the Fourth International
    Conference on Fog and Mobile Edge Computing, FMEC, IEEE (2019), pp. 13-20, 10.1109/FMEC.2019.8795361
    View in ScopusGoogle Scholar [22] Fan Q., Ansari N. Towards workload balancing
    in fog computing empowered IoT IEEE Trans. Netw. Sci. Eng., 7 (1) (2020), pp.
    253-262, 10.1109/TNSE.2018.2852762 View in ScopusGoogle Scholar [23] Martinez
    I., Jarray A., Hafid A.S. Scalable design and dimensioning of fog-computing infrastructure
    to support latency-sensitive IoT applications IEEE Internet Things J., 7 (6) (2020),
    pp. 5504-5520, 10.1109/JIOT.2020.2979705 View in ScopusGoogle Scholar [24] Chandak
    A., Ray N.K. A review of load balancing in fog computing Proceedings of the 2019
    International Conference on Information Technology, ICIT, IEEE (2019), pp. 460-465,
    10.1109/ICIT48102.2019.00087 View in ScopusGoogle Scholar [25] Nadembega A., Hafid
    A., Taleb T. Mobility-prediction-aware bandwidth reservation scheme for mobile
    networks IEEE Trans. Veh. Technol., 64 (6) (2015), pp. 2561-2576, 10.1109/TVT.2014.2345255
    View in ScopusGoogle Scholar [26] Puliafito C., Gonçalves D.M., Lopes M.M., Martins
    L.L., Madeira E., Mingozzi E., Rana O., Bittencourt L.F. MobFogSim: Simulation
    of mobility and migration for fog computing Simul. Model. Pract. Theory, 101 (2020),
    Article 102062, 10.1016/j.simpat.2019.102062 Modeling and Simulation of Fog Computing.
    URL https://www.sciencedirect.com/science/article/pii/S1569190X19301935 View PDFView
    articleView in ScopusGoogle Scholar [27] Velasquez K., Abreu D.P., Paquete L.,
    Curado M., Monteiro E. A rank-based mechanism for service placement in the fog
    Proceedings of the 2020 IFIP Networking Conference (Networking), IEEE (2020),
    pp. 64-72 View in ScopusGoogle Scholar [28] Lera I., Guerrero C., Juiz C. YAFS:
    A simulator for IoT scenarios in fog computing IEEE Access, 7 (2019), pp. 91745-91758,
    10.1109/ACCESS.2019.2927895 View in ScopusGoogle Scholar [29] Guerrero C., Lera
    I., Juiz C. Genetic algorithm for multi-objective optimization of container allocation
    in cloud architecture J. Grid Comput., 16 (1) (2018), pp. 113-135, 10.1007/s10723-017-9419-x
    View in ScopusGoogle Scholar [30] He X., Ren Z., Shi C., Fang J. A novel load
    balancing strategy of software-defined cloud/fog networking in the internet of
    vehicles China Commun., 13 (Supplement2) (2016), pp. 140-149, 10.1109/CC.2016.7833468
    View in ScopusGoogle Scholar [31] Karamoozian A., Hafid A., Aboulhamid E.M. On
    the fog-cloud cooperation: How fog computing can address latency concerns of IoT
    applications Proceedings of the Fourth International Conference on Fog and Mobile
    Edge Computing, FMEC, IEEE (2019), pp. 166-172, 10.1109/FMEC.2019.8795320 View
    in ScopusGoogle Scholar [32] Brogi A., Forti S. QoS-aware deployment of IoT applications
    through the fog IEEE Internet Things J., 4 (5) (2017), pp. 1185-1192, 10.1109/JIOT.2017.2701408
    View in ScopusGoogle Scholar [33] Velasquez K., Abreu D.P., Curado M., Monteiro
    E. Service placement for latency reduction in the internet of things Ann. Telecommun.,
    72 (1) (2017), pp. 105-115, 10.1007/s12243-016-0524-9 View in ScopusGoogle Scholar
    [34] Ni L., Zhang J., Jiang C., Yan C., Yu K. Resource allocation strategy in
    fog computing based on priced timed Petri nets IEEE Internet Things J., 4 (5)
    (2017), pp. 1216-1228, 10.1109/JIOT.2017.2709814 View in ScopusGoogle Scholar
    [35] Téllez N., Jimeno M., Salazar A., Nino-Ruiz E. A tabu search method for load
    balancing in fog computing Int. J. Artif. Intell., 16 (2018), pp. 78-105 Google
    Scholar [36] Puthal D., Obaidat M.S., Nanda P., Prasad M., Mohanty S.P., Zomaya
    A.Y. Secure and sustainable load balancing of edge data centers in fog computing
    IEEE Commun. Mag., 56 (5) (2018), pp. 60-65, 10.1109/MCOM.2018.1700795 View in
    ScopusGoogle Scholar [37] Xu X., Fu S., Cai Q., Tian W., Liu W., Dou W., Sun X.,
    Liu A.X. Dynamic resource allocation for load balancing in fog environment Wirel.
    Commun. Mob. Comput., 2018 (2018), Article 6421607, 10.1155/2018/6421607 View
    in ScopusGoogle Scholar [38] Pereira E., Fischer I.A., Medina R.D., Carreno E.D.,
    Padoin E.L. A load balancing algorithm for fog computing environments Crespo-Mariño
    J.L., Meneses-Rojas E. (Eds.), Proceedings of the Latin American High Performance
    Computing Conference, Springer International Publishing, Cham (2020), pp. 65-77
    CrossRefView in ScopusGoogle Scholar [39] Mseddi A., Jaafar W., Elbiaze H., Ajib
    W. Intelligent resource allocation in dynamic fog computing environments Proceedings
    of the 8th International Conference on Cloud Networking (CloudNet), IEEE (2019),
    pp. 1-7, 10.1109/CloudNet47604.2019.9064110 Google Scholar [40] Talaat F.M., Saraya
    M.S., Saleh A.I., Ali H.A., Ali S.H. A load balancing and optimization strategy
    (LBOS) using reinforcement learning in fog computing environment J. Ambient Intell.
    Humaniz. Comput., 11 (11) (2020), pp. 4951-4966, 10.1007/s12652-020-01768-8 View
    in ScopusGoogle Scholar [41] Baek J., Kaddoum G., Garg S., Kaur K., Gravel V.
    Managing fog networks using reinforcement learning based load balancing algorithm
    Proceedings of the 2019 Wireless Communications and Networking Conference, WCNC,
    IEEE (2019), pp. 1-7, 10.1109/WCNC.2019.8885745 Google Scholar [42] Beraldi R.,
    Alnuweiri H. Sequential randomization load balancing for fog computing Proceedings
    of the 26th International Conference on Software, Telecommunications and Computer
    Networks (SoftCOM), IEEE (2018), pp. 1-6, 10.23919/SOFTCOM.2018.8555797 Google
    Scholar [43] Beraldi R., Canali C., Lancellotti R., Proietti Mattia G. Randomized
    load balancing under loosely correlated state information in fog computing Proceedings
    of the 23rd International ACM Conference on Modeling, Analysis and Simulation
    of Wireless and Mobile Systems, MSWiM ’20, Association for Computing Machinery,
    New York, NY, USA (2020), pp. 123-127, 10.1145/3416010.3423244 View in ScopusGoogle
    Scholar [44] Beraldi R., Alnuweiri H. Exploiting power-of-choices for load balancing
    in fog computing Proceedings of the 2019 International Conference on Fog Computing,
    ICFC, IEEE (2019), pp. 80-86, 10.1109/ICFC.2019.00019 View in ScopusGoogle Scholar
    [45] Beraldi R., Proietti Mattia G. Power of random choices made efficient for
    fog computing IEEE Trans. Cloud Comput. (2020), p. 1, 10.1109/TCC.2020.2968443
    Google Scholar [46] Beraldi R., Canali C., Lancellotti R., Mattia G.P. A random
    walk based load balancing algorithm for fog computing Proceedings of the Fifth
    International Conference on Fog and Mobile Edge Computing, FMEC, IEEE (2020),
    pp. 46-53, 10.1109/FMEC49853.2020.9144962 View in ScopusGoogle Scholar [47] Roy
    B. ELECTRE III: Un algorithme de classements fondé sur une représentation floue
    des préférences en présence de criteres multiples Cah. CERO, 20 (1) (1978), pp.
    3-24 Google Scholar [48] Giang N.K., Blackstock M., Lea R., Leung V.C. Developing
    IoT applications in the fog: A distributed dataflow approach Proceedings of the
    5th International Conference on the Internet of Things, IOT, IEEE (2015), pp.
    155-162, 10.1109/IOT.2015.7356560 View in ScopusGoogle Scholar [49] Rogers M.,
    Bruen M., Maystre L.-Y. Weighting Criteria for Use within ELECTRE Springer US,
    Boston, MA (2000), pp. 87-113 Ch. 4 CrossRefGoogle Scholar [50] RogerS M., Bruen
    M. A new system for weighting environmental criteria for use within ELECTRE III
    European J. Oper. Res., 107 (3) (1998), pp. 552-563, 10.1016/S0377-2217(97)00154-9
    URL https://www.sciencedirect.com/science/article/pii/S0377221797001549 View PDFView
    articleView in ScopusGoogle Scholar [51] Greco S., Figueira J., Ehrgott M. Multiple
    Criteria Decision Analysis, Vol. 37 Springer (2016) Google Scholar [52] Vincke
    P. Multicriteria Decision-Aid John Wiley & Sons (1992) Google Scholar [53] Figueira
    J.R., Greco S., Roy B., Słowiński R. ELECTRE Methods: Main Features and Recent
    Developments Springer Berlin Heidelberg, Berlin, Heidelberg (2010), pp. 51-89
    Ch. 3 CrossRefGoogle Scholar [54] Elmokashfi A., Kvalbein A., Dovrolis C. On the
    scalability of BGP: The role of topology growth IEEE J. Sel. Areas Commun., 28
    (8) (2010), pp. 1250-1261, 10.1109/JSAC.2010.101003 View in ScopusGoogle Scholar
    [55] Brandes U. A faster algorithm for betweenness centrality J. Math. Sociol.,
    25 (2) (2001), pp. 163-177, 10.1080/0022250X.2001.9990249 arXiv:https://doi.org/10.1080/0022250X.2001.9990249
    View in ScopusGoogle Scholar [56] Peebles P.Z. Jr. Probability, Random Variables,
    and Random Signal Principles McGraw-Hill (2001) Google Scholar Cited by (3) Privacy-aware
    load balancing in fog networks: A reinforcement learning approach 2023, Computer
    Networks Show abstract Lifelong Learning for Fog Load Balancing: A Transfer Learning
    Approach 2023, arXiv Load-Aware Resource Scheduling in Fog Computing Based Delay-Sensitive
    IoT Networks 2023, 2023 International Conference on Smart Applications, Communications
    and Networking, SmartNets 2023 Maad Ebrahim is currently a Ph.D. candidate at
    the Department of Computer Science and Operations Research (DIRO), University
    of Montreal, Canada. He received his M.Sc. degree in 2019 from the Computer Science
    Department, Faculty of Computer and Information Technology, Jordan University
    of Science and Technology, Jordan. His B.Sc. degree in Computer Science and Engineering
    has been received from the University of Aden, Yemen, in 2013. His research experience
    includes Computer Vision, Artificial Intelligence, Machine learning, Deep Learning,
    Data Mining, and Data Analysis. His current research interests include Fog and
    Edge Computing technologies, Internet of Things, Reinforcement Learning, and Blockchains.
    Abdelhakim Hafid spent several years as the Senior Research Scientist with Bell
    Communications Research (Bellcore), NJ, USA, working in the context of major research
    projects on the management of next generation networks. He was also an Assistant
    Professor with Western University (WU), Canada, the Research Director of Advance
    Communication Engineering Center (venture established by WU, Bell Canada, and
    Bay Networks), Canada, a Researcher with CRIM, Canada, the Visiting Scientist
    with GMDFokus, Germany, and a Visiting Professor with the University of Evry,
    France. He is currently a Full Professor with the University of Montreal. He is
    also the Founding Director of the Network Research Laboratory and Montreal Blockchain
    Laboratory. He is a Research Fellow with CIRRELT, Montreal, Canada. He has extensive
    academic and industrial research experience in the area of the management and
    design of next generation networks. His current research interests include the
    IoT, fog/edge computing, blockchain, and intelligent transport systems. View Abstract
    © 2023 Elsevier B.V. All rights reserved. Recommended articles Implementation
    of polyphase digital down converter for wireless applications Microprocessors
    and Microsystems, Volume 100, 2023, Article 104850 Debarshi Datta, Himadri Sekhar
    Dutta View PDF An improved residue multiplier using Montgomery modular multiplication
    Microprocessors and Microsystems, Volume 100, 2023, Article 104853 Zabihollah
    Ahmadpour View PDF Hardware design and implementation of high-efficiency cube-root
    of complex numbers Microprocessors and Microsystems, Volume 100, 2023, Article
    104847 Elias Rajaby, …, Ehsan Yazdian View PDF Show 3 more articles Article Metrics
    Citations Citation Indexes: 2 Captures Readers: 11 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: Microprocessors and Microsystems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Resilience and load balancing in Fog networks: A Multi-Criteria Decision
    Analysis approach'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Shao S.
  - Gong W.
  - Yang H.
  - Guo S.
  - Chen L.
  - Xiong A.
  citation_count: '3'
  description: The 6G wireless network aims to forge a new spectrum, high technical
    standards of high time and phase synchronization accuracy, and 100% geographic
    coverage to connect trillions of devices flexibly and efficiently in the future.
    However, as connectivity increases and applications become novel, it is a challenge
    to ensure the privacy and security of networks and applications. Blockchain is
    seen as a promising technology that can improve efficiency, reduce costs, mitigate
    security, and privacy threats, and establish a trusted data-sharing environment.
    This article presents a trusted framework based on blockchain technology from
    the perspective of how to build a trusted software-defined content delivery network.
    As the peer node of the blockchain, the software-defined network (SDN) controller
    establishes trust between different regions and a wide range of participants,
    realizing peer autonomy and flexible business orchestration. The two main purposes
    of the architecture are to enhance the security of network communications and
    establish trust relationships between entities in different domains. It includes
    trusted communication based on routing sandbox, service choreography based on
    blockchain, proxy server selection strategy based on model predictive control
    (MPC), and optimization consensus based on practical Byzantine fault tolerance.
    Some simulation experiments verify the effectiveness of the theoretical method.
  doi: 10.1109/JIOT.2021.3124091
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things
    Journal >Volume: 10 Issue: 14 Data Trusted Sharing Delivery: A Blockchain-Assisted
    Software-Defined Content Delivery Network Publisher: IEEE Cite This PDF Sujie
    Shao; Weichao Gong; Huifeng Yang; Shaoyong Guo; Liandong Chen; Ao Xiong All Authors
    2 Cites in Papers 776 Full Text Views Abstract Document Sections I. Introduction
    II. Related Work III. Blockchain-Assisted Software-Defined Content Delivery Network
    Architecture IV. Blockchain Consensus Mechanism V. Path Compute Element to the
    Optimal CDN Surrogate Show Full Outline Authors Figures References Citations Keywords
    Metrics Abstract: The 6G wireless network aims to forge a new spectrum, high technical
    standards of high time and phase synchronization accuracy, and 100% geographic
    coverage to connect trillions of devices flexibly and efficiently in the future.
    However, as connectivity increases and applications become novel, it is a challenge
    to ensure the privacy and security of networks and applications. Blockchain is
    seen as a promising technology that can improve efficiency, reduce costs, mitigate
    security, and privacy threats, and establish a trusted data-sharing environment.
    This article presents a trusted framework based on blockchain technology from
    the perspective of how to build a trusted software-defined content delivery network.
    As the peer node of the blockchain, the software-defined network (SDN) controller
    establishes trust between different regions and a wide range of participants,
    realizing peer autonomy and flexible business orchestration. The two main purposes
    of the architecture are to enhance the security of network communications and
    establish trust relationships between entities in different domains. It includes
    trusted communication based on routing sandbox, service choreography based on
    blockchain, proxy server selection strategy based on model predictive control
    (MPC), and optimization consensus based on practical Byzantine fault tolerance.
    Some simulation experiments verify the effectiveness of the theoretical method.
    Published in: IEEE Internet of Things Journal ( Volume: 10, Issue: 14, 15 July
    2023) Page(s): 11949 - 11959 Date of Publication: 29 October 2021 ISSN Information:
    DOI: 10.1109/JIOT.2021.3124091 Publisher: IEEE Funding Agency: SECTION I. Introduction
    As the standardization of 5G communication has been completed, the vision and
    planning for sixth-generation (6G) communication have begun. 6G aims to cast a
    new spectrum, high technical standards with high time and phase synchronization
    accuracy, and 100% geographic coverage to connect trillions of devices flexibly
    and efficiently in the future [1]. This will involve a large number of heterogeneous
    devices with limited resources interacting with our physical world. As a result,
    security and privacy issues stand in the way of further deployment and adoption
    of 5/6G [2]. From the perspective of establishing trusted software-defined content
    delivery, we will try to provide blockchain-based solutions to trust and privacy
    issues encountered during the transition from 4G to 5/6G networks. With the development
    of 5/6G, content delivery networks (CDNs) have become an important way to alleviate
    Internet congestion and improve users’ service experience [3]. CDN improves users’
    Quality of Service (QoS) by backing up the content of the origin server to a replica
    server that is closer to users. Thus, the user’s requests are redirected to the
    replica server according to the user’s geographical location [4]. However, CDN
    cannot perceive network topology and cannot obtain global network status information,
    so it cannot select the optimal server for users from a global perspective. Specifically,
    the replica server with the nearest geographic location is not the best choice
    for redirection from a global perspective [5]. To cope with this problem, a software-defined
    network (SDN) with more advantages in security, reliability, and maintainability
    is a promising solution. In the SDN architecture, network control is separated
    from the data plane. Usually, there is a controller (or cluster) which is responsible
    for collecting the topology and flow of the entire network, calculating the forwarding
    path, and sending the flow table to the switch. The switch only performs forwarding
    actions according to the flow table rules [6]. In addition, SDN has programmability.
    It can dynamically allocate underlying network resources through programming.
    By assuming a part of the CDN’s function of obtaining network information, it
    not only reduces the CDN load but also strengthens the control of the underlying
    network [7], [8]. Therefore, the software-defined CDN can make up for the shortcomings
    of CDN that cannot obtain complete and accurate network status information. However,
    software-defined CDNs still face many security challenges in practice. When SDN
    controllers are deployed in a centralized manner, it is easy to cause single points
    of failure, such as controller hijacking, DDoS attacks, etc., while distributed
    deployment controllers are more likely to encounter Internal attacks, such as
    malicious node intrusion [9] and consensus issues [10]. The application of blockchain
    technology to ensure the security of the data plane and control plane in SDN has
    received great attention from the academic community, because the blockchain can
    provide secure and trusted interactions in distributed peer-to-peer networks [11].
    The blockchain uses asymmetric encryption technology to verify every new transaction
    in the control plane. In this case, the blockchain allows unknown entities to
    exchange data with each other without a trusted third party [12]. Therefore, blockchain
    not only enhances the security of distributed controllers and various forwarding
    devices but also improves the robustness of the software-defined CDN. This article
    mainly focuses on the application of SDN to provide a global view for the selection
    of the optimal replica server in the CDN and further build a consortium blockchain
    on the SDN control plane to respond to security threats. The specific contributions
    of this article are summarized as follows. A software-defined CDN architecture
    based on blockchain is proposed. On the one hand, it perceives the network topology
    through the SDN control plane and then provides a global view for the optimal
    selection of CDN replica servers. On the other hand, the consortium blockchain
    is built on the SDN control plane where distributed controllers participate in
    the consensus process in order to deal with single points of failure, malicious
    node intrusion, and consensus issues. Model predictive control (MPC) is used to
    solve the problems of joint optimization delay and bandwidth satisfaction in the
    process of selecting the optimal replica server in the software-defined CDN. The
    importance of the two optimization goals of response time and bandwidth satisfaction
    can be balanced by adjusting the weight value according to actual network requirements.
    In order to resist malicious nodes intrusion and improve reliability and security,
    not only the consortium blockchain is used to verify the identity of the controller
    but also a fuzzy synthetic evaluation model is established to evaluate the trust
    of the SDN controller to prevent untrusted nodes from joining. In order to overcome
    the shortcomings of low consensus efficiency and poor self-healing when the practical
    Byzantine fault tolerance (PBFT) consensus algorithm is directly applied in the
    SDN scenario, we propose a simplified and PBFT consensus mechanism based on trust
    evaluation, which is referred to as trusted simplified PBFT (TS-PBFT). The rest
    of the organization structure is as follows. Section II reviews the related work.
    Section III presents the blockchain-assisted software-defined CDN architecture.
    The blockchain consensus mechanism is introduced in Section IV. Section V describes
    the path compute element to optimal CDN surrogate. The experimental results and
    corresponding discussions are described in Section VI. Section VII gives the conclusion
    and some problems to be solved in the future. SECTION II. Related Work In this
    section, we review some interesting research on software-defined CDNs, blockchain,
    5/6G, and Internet of Things (IoT) security. A. Software-Defined Content Delivery
    Network Tran et al. [13] proposed an SDN-based intelligent CDN architecture, where
    the controller makes cross-domain decisions in a centralized manner, and the MABRESE
    algorithm is applied to solve the server selection problem. Fu et al. [14] used
    the programmability and virtuality of SDN/NFV to design a scalable CDN architecture,
    where load-based redirection algorithms are applied to accurately and fine-grained
    control of load management traffic routing methods. In [15], CDN providers use
    OpenFlow switches to manage the underlying infrastructure. It also uses SDN controllers
    to achieve a global view of CDNs, thereby improving traditional load-balancing
    algorithms to make better decisions. A simple and effective CDN request rerouting
    architecture based on SDN is proposed in [16], which can achieve user–server allocation
    with higher flexibility and requires fewer additional components to achieve the
    same rerouting performance. Mowla et al. [17] proposed a cognitive detection and
    defense mechanism in the SDN-based CDNi architecture, which can more accurately
    detect and defend DDoS attacks. Sharma et al. [18] proposed a distributed and
    secure SDN architecture (DistBlockNet) using blockchain technology. In this network,
    unconfident members can interact with each other without a trusted third party.
    The blockchain-based SDN architecture can inherit many advantages of SDN and blockchain,
    including scalability, reliability, and security [19]. In order to ensure the
    security of SDN on different planes and related interconnections, Aujla et al.
    [20] proposed BlockSDN, a framework that uses blockchain as an SDN network service,
    which can deal with malware intrusion on the data plane and DDoS attacks on the
    control plane. Aujla et al. [20] discussed the feasibility of integrating blockchain
    and SDN in IoT and proposed a blockchain-based SDN controller architecture. This
    architecture uses public and private blockchains for P2P communication between
    IoT devices and SDN controllers, with higher throughput, lower latency, and lower
    energy consumption. B. Blockchain-Enabled Privacy Protection in IoT As mentioned
    in [21], “Blockchain has been regarded as a promising technology for IoT, since
    it provides significant solutions for decentralized networks that can address
    trust and security concerns, high maintenance cost problems, and so on.” The blockchain
    consensus mechanism allows peer-to-peer transactions to take place in a distributed
    manner without third parties. By integrating a public chain running the delegated
    PoS (DPOS) consensus and subchains running the PBFT consensus, [22] achieve higher
    transaction throughput, less execution time, diverse privacy protection, and access
    control compared with traditional POW/POS-based blockchain. Furthermore, Zou et
    al. [23] considered the interference behavior of Byzantine elastic protocol in
    wireless networks, which is a very critical and realistic behavior that must be
    considered in modern wireless networks. Compared with PoW and Proof of Stake (PoS)
    that have been widely used in blockchain, direct acyclic graph (DAG) consensus
    proposed in [24] and [25] can overcome some weaknesses, such as high resource
    consumption, high transaction fee, low transaction throughput, and long confirmation
    delay. In order to better combine blockchain and IoT, Zheng and Cai [26] investigated
    the problem of data sharing for IoT systems, where multiple data consumers exist
    in different stages. In the network topology, Yu et al. [27] defined the dynamicity
    in terms of localized topological changes in the vicinity of each node, rather
    than a global view of the whole network. A local dynamic model suits distributed
    algorithms better than a global one. In terms of computing power, Cao et al. [28]
    proposed multiaccess edge computing as a supplement to the traditional remote
    cloud center, which is deployed in the area adjacent to the device side of the
    IoT and is also considered as a promising 5G heterogeneous network technology.
    In terms of wireless communication, Xu et al. [29] presented wChain, a blockchain
    protocol specifically designed for multihop wireless networks that deeply integrates
    wireless communication properties and blockchain technologies under the realistic
    SINR model. In terms of physical, the concept of physical-layer anonymity is introduced
    in [30], which reveals that the receiver can reveal the identity of the sender
    only by analyzing the physical-layer information, namely, signaling mode and channel
    characteristics. Furthermore, Wei et al. [31] overviewed a distributed approach
    that utilizes interference in power-constrained IoT devices and applications to
    provide efficient, hardware efficient secure transmission for IoT downlinks. Cai
    and Zheng [32] considered conservation of energy and privacy protection to implement
    an efficient data uplinks scheme by introducing an acceptable amount of additional
    content. There is also some promising research on data trust access and AI. In
    [33], a blockchain-enhanced security access control scheme that supports traceability
    and revocability has been proposed in IoT for smart factories, which achieved
    secure storage, access control, information update, and deletion for smart factory
    data. Tan et al. [34] proposed a HoneyNet approach that includes both threat detection
    and situational awareness to enhance the security and resilience of Artificial
    Intelligence of Things (AIoT). It is more suitable for intelligent IoT than traditional
    network security methods. C. Blockchain Meets 5/6G As a result in [35], 6G wireless
    networks improve on 5G by further increasing reliability, speeding up the networks
    and increasing the available bandwidth. However, along with the enhanced connectivity
    and novel applications, privacy and security of the networks and the applications
    must be ensured. For example, blockchain has the transparent nature of disclosing
    potentially private information to all participants. There are several solutions
    to address this risk, including ring signatures, zero-knowledge parameters, and
    coin mixing [36]. Meanwhile, 6G will dramatically improve the performance and
    service of wireless networks, leading to a dramatic increase in ubiquitous computing
    resources. Blockchain’s network layer will benefit from improved performance,
    as well as the expected increased configurability of wireless networks [37]. For
    example, Feng et al. [38] proposed an efficient and secure 5G data-sharing model
    based on blockchain to deal with the problems that an open and untrusted environment
    may bring to authentication and data sharing. The model uses blockchain and attribute-based
    encryption (ABE) to ensure the security of instruction problems and data sharing.
    The authentication mechanism in the model adopts a smart contract for authentication
    and access control. SECTION III. Blockchain-Assisted Software-Defined Content
    Delivery Network Architecture There are still many issues to be resolved in the
    future of software-defined CDNs, one of the most important of which is cross-domain
    trust. More and more CDN services begin to be under the business scenario of multiparty
    participation, and the transaction mode will support the dynamic switch of user
    networks among many operators. However, the uncontrollable factors in the business
    scenario of multiparty participation increase, and it is difficult to guarantee
    the credibility of each participant and transaction. For example, users use different
    operators’ network services in a settlement period, and a clear bill is needed
    for the specific use of network traffic. If the bill is only generated by the
    operator, users will not be convinced, so a reliable bill generation mechanism
    is needed to support it. In addition, the relative equality among operators does
    not allow one party to control the trading rules and process, that is, the traditional
    architecture is no longer applicable to the application scenario of multiparty
    participation and equal autonomy. A. System Model Overview Blockchain can build
    trust between different nodes and has the characteristics of peer-to-peer autonomy,
    data maintenance, and tamper-proof. Therefore, it can be introduced into the software-defined
    CDN and establish a trusted scheduling mechanism and trust trading patterns with
    the help of a smart contract. 1) Credible Scheduling Mechanism: Traditional SDN
    controller face credibility is difficult to ensure that the risk of a single point
    of failure, instructions, and can use blockchain technology, the scheduling strategy
    into smart contracts, deployment in the blockchain system, from blockchain when
    SDN controller platform after listening to the needs of users, the user needs
    with operators network parameters as the trigger scheduling policy contract, blockchain
    directly generated from the decision-making results, to avoid the problem of single-point
    controller scheduling is not credible. 2) Trust Trading Patterns: The trading
    mechanism based on the blockchain can be written by the trading rules smart contracts
    through the access control mechanism to trigger the execution of the contract,
    so as to control the whole process of trading, the operators to participate in
    the scene, the scheduling results generated, users and operators directly through
    the blockchain signed a contract to establish trade relations, when the network
    operators to provide quality no longer meet the demand of users, a new scheduling
    results generated, users will be signed directly by blockchain system with new
    operators to establish trade relations. 3) Routing Sanbox: To ensure trusted data
    delivery, a routing sandbox based on state channel is established. Routing sandboxes
    can set up private communication paths for single-point communication. Unlike
    virtual private network mechanisms, verification data for private channels is
    stored on the blockchain. After the channel is established, packets are transmitted
    through the intelligent contract-based network management portal, and the credibility
    check is performed at the exit according to the verification data on the chain.
    In addition, blockchain nodes jointly maintain the data stored in the blockchain,
    which has the advantage of preventing tampering. Once the data goes through the
    chain of consensus, even if the consensus node exits, it will not cause data loss
    or failure. Under the content distribution scenario, the decision basis and decision
    instructions of the control layer will be broadcast and stored in the blockchain
    system in the form of transactions. The network traffic provided by the operator
    can be distributed collected through the blockchain as the basis for settlement,
    and the billing process will also be recorded through the blockchain system. When
    the two sides of a transaction disagree, the data obtained from the blockchain
    can be held accountable as evidence. B. System Components Based on the credibility
    analysis in the multiparticipant scenario described earlier, we found that the
    existing software-defined CDN architecture does not meet the need to ensure that
    it performs trusted data delivery. At the same time, some existing studies have
    confirmed that blockchain technology can build trust between different nodes and
    has the characteristics of peer-to-peer autonomy, data maintenance, and tamper-proof.
    Further, we propose a blockchain-assisted software-defined CDN. As shown in Fig.
    1, the trusted data delivery process is implemented using a software-defined CDN
    architecture with blockchain help. The SDN controller acts as a peer node of the
    blockchain in order to establish trust between different regions and participants
    and achieve peer autonomy. The SDN controller controls CDN cache devices located
    in the data plane to achieve flexible service choreography. The two main purposes
    of this architecture are to enhance the security of network communication and
    to establish trust relationships for entities in different domains. Two main components
    of the blockchain-assisted software-defined CDN are described as follows. Fig.
    1. Blockchain-assisted software-defined CDN architecture. Show All 1) Blockchain
    Module for Connected Cross-Domain SDN Controller: The blockchain module is mainly
    responsible for ensuring the credibility of cross-domain services. It consists
    of four assembly components: 1) consensus management; 2) cryptography; 3) ledger
    storage; and 4) smart contract. They are described in detail as follows. Consensus
    Management: The consensus mechanism is a blockchain governance system, a set of
    methods designed by combining economics, game theory, and other disciplines to
    ensure that each node in the blockchain can actively maintain the blockchain system.
    It was first proposed by Satoshi Nakamoto in the White paper of Bitcoin and gradually
    developed into an important mechanism to maintain the multicentralization of distributed
    ledger, which is the core to maintain the safe and stable operation of the blockchain.
    To be specific, the so-called “consensus mechanism” is to complete the verification
    and confirmation of transactions within a very short period of time through the
    voting of special nodes. For a transaction, if several nodes with unrelated interests
    can reach a consensus. Considering the performance and security issues, we proposed
    an architecture compatible with mainstream consensus mechanisms, such as PoW,
    PoS, DPoS, PBFT, and Raft to meet different service requirements. In this article,
    we adopt the optimized PBFT algorithm, which is introduced in Section IV in detail.
    Cryptography: The encryption mechanism is the foundation of the whole blockchain.
    We will only briefly introduce the encryption mechanism used in the blockchain.
    The hash function is a kind of mathematical function, which can compress a message
    of arbitrary length into a binary string of fixed length within a reasonable time,
    and its output value becomes a hash value, also known as a hash value. The hash
    algorithm is often used to realize data integrity (message digest) and entity
    authentication (signature), and it also constitutes a security guarantee for various
    cryptographic systems and protocols. The elliptic curve cryptography (ECC) algorithm
    is an asymmetric encryption algorithm based on elliptic curve mathematics. Its
    security depends on the difficulty of offline logarithm of elliptic curve. Zero-Knowledge
    Proof: The verifier can make the verifier believe that they have the knowledge
    without telling the verifier what the knowledge is. Ledger Storage: In our proposed
    architecture, network state information is contained in a public ledger in the
    form of blocks, as shown in Fig. 1 of the node behavior data block structure,
    with each block anchored to the blockchain. The ledger is made up of blocks linked
    together by a block head and a block body, which mainly stores transaction information.
    The block head is used to connect the blocks and form a chain. Data are stored
    as a Merkle tree, and its root is generated by a hash algorithm. Any modification
    behavior can change the Merkle root value, which means that node behavior can
    be tracked and not tampered with. The node behavior data block contains important
    information that can be analyzed, such as node number, node digital signature,
    a large amount of node behavior data, and timestamp. Smart Contracts: Smart contracts
    are event-driven, multirecognized programs that run on the blockchain and automatically
    process services based on preconditions. Smart contracts, once written, can be
    trusted by users and cannot be changed. Therefore, smart contracts can enable
    blockchain technology to be applied to the practical application plane. 2) SDN
    Controller Structure for CDN Point of Presence: Fig. 1 also shows the SDN controller
    structure in the blockchain-assisted software-defined CDN architecture. The SDN
    controller consists of Parser, View Builder, Verifier, Path Compute Element, Data
    Set Storage, Controller Module, and Blockchain Module. Four of the components
    are described in detail as follows. Parser: The switch sends Packet_In, Flow_Mod,
    Stats_Reply, and Features_Reply information to the controller through the southbound
    interface. Packet Parser parses the data packet sent by the switch. Packet_In
    is the signaling for the switch to request the flow table from the controller;
    Flow_Mod is the signaling for the controller to send the flow table after processing
    the request; Stats_Reply contains the status information of the switch; and Features_Reply
    is the signal for the switch Function characteristic information, including path
    identifier (path_id) and flow table information (such as Flow Entry), etc. Packet
    Parser dynamically monitors and parses the data packets passed into the controller
    and extracts important metadata sets. View Builder: Responsible for analyzing
    metadata sets, extracting switch network topology and status information, and
    playing an important role in locating network faults and optimizing the network
    reasonably. Verifier: From the perspective of system security, in order to prevent
    attackers from modifying packet data or injecting additional packets to damage
    the system security, it is necessary to verify the legitimacy of the node. Path
    Compute Element: The path compute module in the SDN controller is used to calculate
    the optimal path according to the relevant configuration information of the service.
    The calculation of the optimal path needs to consider some factors, such as delay,
    packet loss rate, bandwidth, jitter, and cost, as well as the weight of the above
    factors. The routing path from the user source to the optimal CDN surrogate can
    be calculated based on the composite measurement of each link, and then sent to
    the SDN switch through the flow table. In this article, we focus on bandwidth
    and delay and achieve dynamic adjustment of weights according to actual network
    requirements through MPC. This will be described in detail in Section V. SECTION
    IV. Blockchain Consensus Mechanism In this section, the blockchain consensus mechanism
    is introduced, and we propose an optimized BPFT consensus process. A. Blockchain
    Consensus In the blockchain asynchronous network, state replication between hosts
    is necessary to ensure a consistent state for each host. The consensus algorithm
    ensures consistency between hosts after state replication, which is the most fundamental
    and important part of the blockchain. Based on the consortium blockchain, a variety
    of consensus algorithms have been designed as shown in Table I, such as proof
    of work, PoS, DPoS, Casper, Proof of Elapsed Time (PoET), and PBFT. Later, we
    will detail the PBFT consensus process and the enhancements we have made to it.
    TABLE I Comparison of Consensus Mechanisms B. Optimized PBFT Consensus Mechanism:
    TS-PBFT Aiming at the shortcomings of the PBFT algorithm directly applied to the
    SDN control plane, such as low consensus efficiency and poor self-healing, this
    article proposes a simplified trust-based PBFT algorithm, introduces a trust evaluation
    mechanism, uses node reputation as an election proof, and combines it with the
    simplified PBFT algorithm. Achieve a virtuous circle of the system, reduce communication
    costs, and improve efficiency. At the same time, this article also enhances system
    security by limiting communication time and adding intrusion detection mechanisms.
    There are primary nodes (Primary) and regular nodes (follower) in the protocol.
    The primary node mainly has the following three functions. When working normally,
    receive the transaction request from the client, set the number for the request
    after verifying the identity of the request, and broadcast the preprepare message.
    When the new primary is elected, it sends View-News messages according to the
    View-Change messages collected by itself to let other nodes synchronize data.
    Primary maintains a heartbeat with other nodes. It should be noted that the status
    of the primary node is the same as that of the follower node, and it has no privileges.
    If it is down, no messages occur, messages with incorrect numbers or tampered
    messages will be sensed by other nodes and trigger view-change. 1) Traditional
    PBFT: In an SDN control plane consensus process, there is only one master node,
    which is responsible for verifying the transactions received within a period of
    time, and the verified transactions will be packaged into the block. The nodes
    participating in the consensus will have different numbers from 0 to R=3f+1 ,
    and the main node selection formula is, where p is the selected node number; v
    is the view number gradually increasing from zero; and |R| is the total number
    of nodes participating in the consensus. As shown in Fig. 2, a complete and detailed
    consensus process includes the following steps. The system elects a primary in
    turn according to the number, and the primary initiates a view-new message to
    synchronize the data of all nodes; The client initiates a request and forwards
    it to the primary. After the primary is verified, it broadcasts the request, initiates
    a preprepare message to all follower nodes, and saves the request itself. After
    all followers receive the preprepare message, the first step is to verify, including
    data sequence, and transaction signatures (to prevent client fraud or primary
    node tampering). After the follower verifies correctly, write it to its own disk,
    then broadcast the Prepare message, and enter the Prepare stage by itself. All
    nodes count the prepare message for a certain Request. When the statistics result
    exceeds 2f nodes, it indicates that most of the nodes have completed the persistence,
    and then enter the commit phase. The node broadcasts the commit message and counts
    the number of received commit messages. When all the nodes that exceed 2f send
    commit messages, the node completes the commit phase, writes data, and updates
    the controller table data. Fig. 2. Schematic of PBFT consensus process. Show All
    2) TS-PBFT: Because the execution priority mechanism in the SDN network does not
    require strict ordering of requests, in this article, the PBFT consensus algorithm
    is simplified, the trust evaluation mechanism is introduced, and the controller
    with the highest trust is selected as the primary node to lead the invasion. For
    detection and security policies, other nodes still act as followers. The simplified
    consensus process cancels the preprepare process. As shown in Fig. 3, a consensus
    process includes the following steps. The system elects the primary node according
    to the trust evaluation mechanism. Any controller node (A) that wants to request
    broadcasts a request < REQUEST, o , t , c> to other controllers, o : the specific
    operation requested, operation; t : the timestamp added to the requested information;
    and c : the controller Node ID REQUEST contains message content and message digest
    (used for signature verification). Other controllers (follower) verify the source
    of the request, and if the verification is successful, broadcast a prepare message
    and enter the prepare phase. If the verification fails, the log information is
    reported to the primary node to perform intrusion detection and security policies.
    Fig. 3. Schematic of TS-PBFT consensus process. Show All The following steps are
    the same as steps 5 and 6 of PBFT. The controller waits for the reply result <REPLY,
    v , t , c , i> , enters the Commit phase and completes the consensus process,
    where v : current view; t : timestamp; c : the number of controllers that initiated
    the request; i : the number of current reply controllers; and REPLY: the operation
    result, which must have the signature of the controller i to prevent network interception
    and fraud. C. Evaluation of the Reliability of the Controller This article uses
    the fuzzy comprehensive evaluation model to analyze the trust attributes of the
    SDN controller. It mainly includes the following steps. 1) Establish Node Trust
    Evaluation Factor Set: The factor set is an ordinary set composed of various factors
    that affect the trust degree of the evaluation node, U=( u 1 , u 2 ,…, u m ) ,
    where u i is the i th factor that affects the trust degree of the evaluation node.
    These factors usually have varying degrees of ambiguity. This article chooses
    the trust evaluation attributes from the perspective of reliability and security.
    In terms of security, in order to prevent attackers from modifying data packets
    or injecting additional data packets to damage the system security, it is necessary
    to ensure that the received data is reliable, so it is necessary to determine
    whether the node is legitimate. This article uses blockchain to achieve node identity
    authentication as shown in Fig. 1. The blockchain uses a public key–private key
    pair to identify the node’s identity. Node identification key pair (PKi, SKi):
    The public key–private key pair is the identity certificate of node i , SKi is
    used to sign data packets, and PKi is used for verification. Before forwarding
    tasks, first, verify the node’s identity information. If the verification is successful,
    the received data will be forwarded. If the verification fails, it is directly
    determined that the trust level of the node is 0, the node is considered untrusted,
    and the SDN network is rejected. After the node is successfully authenticated,
    analyze the running status of the node from the perspective of reliability, including
    the following attributes. In terms of reliability, the above factors are combined
    with the state and behavior of the node. While ensuring that the node is not malicious,
    the node must complete the forwarding task quickly and efficiently. The confidentiality
    and integrity of data cannot guarantee the freshness of data in the network. Incorporating
    reliability into the trust evaluation system can evaluate node conditions more
    comprehensively. 2) Establish Evaluation Set for Trust Evaluation: The evaluation
    set is a set of various results that the evaluator may make to the evaluation
    object, usually represented by V , V=( v 1 , v 2 ,…, v n ) , where the element
    v j represents the j th evaluation result in Table II. To simplify the process,
    set judgment V={0,0.2,0.4,0.6,0.8,1} is used in this article. TABLE II Fuzzy Evaluation
    Factor Set 3) Carry Out Single-Factor Fuzzy Evaluation and Obtain Evaluation Matrix:
    If the membership degree of the i th element in the factor set U to the first
    element in the evaluation set V is r i1 , then the result of the single-factor
    evaluation of the i th element is expressed as a fuzzy set R i =( r i1 , r i2
    ,…, r in ) , taking m single-factor evaluation sets R 1 , R 2 ,…, R m as the row.
    The matrix is called the fuzzy comprehensive evaluation matrix R m∗n = ⎛ ⎝ ⎜ ⎜
    r 11 ⋮ r m1 … ⋱ ⋯ r 1n ⋮ r mn ⎞ ⎠ ⎟ ⎟ . (1) View Source 4) Determine the Full
    Vector of Factors: In trust evaluation work, the importance of each factor is
    different. For this reason, give each factor u i a weight a i . The fuzzy set
    of the weight set of each factor is represented by A=( a 1 , a 2 ,…, a m ) , which
    can be constructed by the pairwise comparison matrix of AHP Weight vector. 5)
    Establish Trust Evaluation Model: After determining the single-factor evaluation
    matrix R and the factor weight vector A , the fuzzy vector A on U is changed to
    the fuzzy vector B on V through fuzzy change B= = A 1∗m ∘ R m∗n =( a 1 , a 2 ,…
    a m )∘ ⎛ ⎝ ⎜ ⎜ r 11 ⋮ r m1 … ⋱ ⋯ r 1n ⋮ r mn ⎞ ⎠ ⎟ ⎟ ( b 1 , b 2 ,…, b n ). (2)
    View Source It is worth noting that ∘ is called the comprehensive evaluation synthesis
    operator. In order to simplify the calculation, general matrix multiplication
    is used in this article. 6) Determine the System Score: In order to facilitate
    the Consortium blockchain to evaluate the trust of nodes in the consensus process,
    S is defined as the mapping of the response factor scores in V , so the total
    score of node trust evaluation F can be obtained as F= B 1∗n ∗ S T 1∗n . (3) View
    Source SECTION V. Path Compute Element to the Optimal CDN Surrogate This article
    uses MPC to solve the problem of simultaneous optimization of server and path
    selection in software-defined content distribution networks. Define the system
    state at time t as Z t , N t is the total number of requests in the system at
    time t, the system input of MPC is p l,s (t) , CDN proxy server set S={1,2,…s,…}
    , and path set L={1,2,…,l,…} . Optimization goal: the user’s average response
    time is the smallest; the user’s bandwidth satisfaction conversion is the smallest.
    The average user response time is the round-trip delay of the user, including
    the round-trip link delay and the processing delay of the server. Regarding the
    server as an M/M/1 queuing model, the processing time of the server can be expressed
    as d s (t)= ∑ s∈S υ t p s (t) υ s − υ t p s (t) . (4) View Source υ t represent
    the rate at which requests arrive at time t , υ s is the processing speed of the
    server, and p s (t) is the probability of the task being processed by the proxy
    server s . In the SDN network, the controller can obtain the global status information
    of the network, including the link delay. The link delay detected by the controller
    is expressed by d l , then the average round-trip delay requested by the user
    is d(t)= ∑ s∈S d s (t) p s (t)+ ∑ l∈L d l p l,s (t). (5) View Source Among them
    p l,s (t) is the probability that the task will be processed by the proxy server
    through the path. Therefore, the delay optimization function J a is defined as
    J a = ∑ k=t+1 t+T d(k). (6) View Source Then consider the user’s bandwidth satisfaction,
    the ratio of the bandwidth that the network can actually provide to the bandwidth
    requested by the user, so the user bandwidth satisfaction offset on the link l
    is represented by B l (t) , which is defined as follows: B l (t)= [ ∑ l∈L p l,s
    (t)− C l ∑ l∈L p l,s (t)R N t ] + . (7) View Source R is the average size of user
    requests, [x ] + represents when x>0 , the value is itself, when x≤0 , the value
    is 0, therefore, the optimization function J b that defines the bandwidth satisfaction
    offset is J b = ∑ k=t+1 t+T p l,s (t) B l (t). (8) View Source Finally, consider
    reducing the change value of the system input P l.s (t) to maintain the stability
    of the system and define the third optimization function J c as J c = ∑ k=t+1
    t+T ∑ s∈S ∑ l∈L | p l,s (k)− p l,s (k−1)| (9) View Source In order to consider
    three optimization functions at the same time, define the joint optimization function
    as J a + ω b J b + ω c J c , where ω b and ω c are the weight values of and, respectively,
    relative to J a . By solving the following nonlinear programming problem, all
    the values in each time slot t+1≤k≤t+T can be calculated as min s.t. J a + ω b
    J b + ω c J c ⎧ ⎩ ⎨ ⎪ ⎪ p s (t)= ∑ l∈L p l,s (t)  ∑ l∈L ∑ s∈S p l,s (t)=1 0≤ p
    l,s (t)≤1∀l∀s. (10) View Source In the simulation part, the performance change
    of the algorithm is analyzed by changing the weight value, and the importance
    of the two optimization goals of response time and bandwidth satisfaction is balanced
    by adjusting the weight value according to the actual network demand. SECTION
    VI. Experiment and Analysis In this part, we introduce our simulation results
    and analysis in detail. First, we introduce the open-source simulation tools we
    use, including SDN virtualization simulation platform Mininet, SDN open-source
    controller floodlight, global network topology emulator simple Global Network
    Topology Emulator, and Fast linearization detector Porcupine. From Fig. 4, we
    can see that when selecting replica servers in the same network state, our CPU
    utilization rate is lower than that of round-robin and random, which are 18.25%,
    23.59%, and 26.72%, respectively. This indicates that our strategy consumes fewer
    computing resources while performing the same task. This makes sense to cope with
    the ever-increasing demand for computing resources created by 6G. Fig. 4. Comparison
    of CUP utilization. Show All From Fig. 5, we can see that as the user’s demand
    per minute increases, bandwidth satisfaction will decrease, but under the same
    user request, our bandwidth satisfaction will be higher. However, it should be
    noted that when the number of requests per minute is less than 300, our performance
    is more obvious than that of round-robin and random. However, with the further
    increase of requests, the downward trend of bandwidth satisfaction in the three
    methods is similar. The reason is that the importance of response time and bandwidth
    to meet these two optimization objectives can be adjusted according to the actual
    network needs to balance the weight values. Fig. 5. Comparison of bandwidth satisfaction.
    Show All The comparison of response time is shown in Fig. 6. As user demand increases
    per minute, the average response time gradually increases. When the requests per
    minute are low, our method is more round-robin, and the random advantage is not
    obvious, but as the requests per minute increase, our method can significantly
    reduce the average response time. Fig. 6. Response time comparison. Show All We
    compare TS-PBFT and PBFT in terms of consensus spending time, signaling overhead,
    and resistance in the best (no malicious nodes) and worst (one-third of malicious
    nodes) network environment. From Fig. 7, we can see that under the same network
    environment, TS-PBFT spends less time in the consensus process than PBFT; from
    Fig. 8, it is clear that TS-PBFT has lower signaling overhead than PBFT. The reason
    is that TS-PBFT reduces a consensus process in the SDN network, and while simplifying
    PBFT, it can still reach a consensus smoothly. In Fig. 9, we can see that TS-PBFT
    is not as resistant to defamation as PBFT, but considering the high data volume
    requirements of software-defined CDNs, TS-PBFT can effectively solve the problem
    by simplifying the consensus process and reducing signaling overhead. PBFT is
    directly applied to the problem of low consensus efficiency in SDN scenarios.
    Considering that TS-PBFT has better performance than PBFT. Fig. 7. Comparison
    of time spent on consensus. Show All Fig. 8. Comparison of signaling overhead.
    Show All Fig. 9. Comparison of invulnerability. Show All We compared the trust
    evaluation model for normal nodes and two types of malicious nodes. Type A means
    that malicious nodes directly discard data packets after receiving data; Type
    B means that malicious nodes repeatedly send the same data packet after receiving
    data. The result is shown in Fig. 10. Starting from the 12th cycle, the model
    can distinguish between normal nodes and malicious nodes. Moreover, as the cycle
    increases, the evaluation of normal nodes will increase and the evaluation of
    malicious nodes will decrease. Type A nodes have lower scores than type B nodes.
    Fig. 10. Trust evaluation in different scenarios. Show All SECTION VII. Conclusion
    and Future Work On the one hand, we use the SDN technology to perceive the network
    topology and provide a global view for the optimal selection of CDN replica servers;
    on the other hand, the consortium blockchain is built on the SDN control plane
    where the distributed deployment controllers participate in the consensus process
    to deal with a single point of failure, malicious node intrusion, and consensus
    issues. In addition, we use MPC to solve the joint optimization of delay and bandwidth
    satisfaction in the process of selecting the optimal replica server; establish
    a fuzzy comprehensive evaluation model to evaluate the trust of the SDN controller
    to prevent untrusted nodes from joining the SDN network; TS-PBFT is proposed to
    overcome the shortcomings of low consensus efficiency and poor self-healing when
    the PBFT consensus algorithm is directly applied in the SDN scenario. Simulation
    experiments have been done to prove it. While blockchain technology holds the
    promise of providing a credible solution in the business scenario of software-defined
    content distribution networks, there are still some constraints. The constraints
    are mainly reflected in system stability, business model supportability, application
    security, and other aspects. Most blockchains cannot meet the requirements of
    “high efficiency,” “decentralization,” and “security” at the same time, that is,
    impossible triangulation. Presently, blockchain is restricted to different degrees
    in terms of transaction throughput, block capacity, trading capacity, etc., which
    leads to network congestion and high-frequency business needs that are difficult
    to be met. Furthermore, problems, such as privacy protection, smart contract vulnerability,
    consensus mechanism, private key protection, computational force attack, cryptography
    algorithm security, etc., make the blockchain still face certain challenges in
    terms of low-level security and application security. Authors Figures References
    Citations Keywords Metrics More Like This A Blockchain-Based Internet of Things
    (IoT) Network for Security-Enhanced Wireless Battery Management Systems 2019 IEEE
    Industry Applications Society Annual Meeting Published: 2019 A Comparitive Study
    of Blockchain Applications for Enhancing Internet of Things Security 2019 10th
    International Conference on Computing, Communication and Networking Technologies
    (ICCCNT) Published: 2019 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Data Trusted Sharing Delivery: A Blockchain-Assisted Software-Defined Content
    Delivery Network'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 36 papers. The topics discussed include: open
    distance pattern uniform self-complementary graphs; rough sets induced by spherical
    fuzzy ideals in near rings; transient behavior of an M/M/2 queue subject to disasters
    and repairs - a new approach; a load balancing strategy using Adam optimizer in
    fog computing environment; behavioral finance, financial literacy and its impact
    on financial decision making, economic well-being and happiness; influence of
    psychological empowerment of women on their job satisfaction across India; predicting
    fake news :analyzing the reference network structure of news articles; resilience
    measures in Christmas trees; the metric dimension of double oxide networks; numerical
    study of free convectional heat transfer within the triangular cavity; and computation
    of sixteen topological indices of certain conjugated polymers.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: AIP Conference Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: AIP Conference Proceedings
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors: []
  citation_count: '0'
  description: 'The proceedings contain 15 papers. The topics discussed include: extensible
    embedded hardware description languages with compilation, simulation and verification;
    breaking boundaries: optimizing FPGA CAD with flexible and multi-threaded re-clustering;
    efficient FPGA implementation of amoeba-inspired SAT solver with feedback and
    bounceback control: harnessing variable-level parallelism for large-scale problem
    solving in edge computing; quantitative study of floating-point precision on modern
    FPGAs; resource-efficient RISC-V vector extension architecture for FPGA-based
    accelerators; CSA based Radix-4 Gemmini systolic array for machine learning applications;
    and noise resilience of reduced precision neural networks.'
  doi: null
  full_citation: '>'
  full_text: '>'
  inline_citation: '>'
  journal: ACM International Conference Proceeding Series
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Proceedings of the 13th International Symposium on Highly Efficient Accelerators
    and Reconfigurable Technologies, HEART 2023
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hu B.
  - Zhang X.
  - Li Y.
  - Lai R.
  citation_count: '1'
  description: In view of the problem that the functions of the current privacy protection
    data aggregation scheme were insufficient to meet the increasingly rich application
    requirements, a multi-function supported privacy protection data aggregation (MFPDA)
    scheme for V2G network was proposed. By using cryptographic algorithms such as
    BGN, BLS, and Shamir’s secret sharing, as well as fog computing and consortium
    blockchain technology, multiple security functions like fault tolerance, resistance
    to internal attacks, batch signature verification, no need for trusted third parties,
    and multiple aggregation functions were integrated into one privacy protection
    data aggregation scheme. Security analysis shows that the proposed scheme can
    protect data aggregation’s security, privacy and reliability. The performance
    evaluation shows that the introduction of fog computing can significantly reduce
    the computing overhead of the control center, and the reduction rate can be as
    high as 66.6%; the improvement of the consortium blockchain can effectively reduce
    the communication and storage overhead of the system, and the reduction rate can
    reach 16.7% and 24.9% respectively.
  doi: 10.11959/j.issn.1000-436x.2023081
  full_citation: '>'
  full_text: '>

    "VISIT DOI.ORG DOI NOT FOUND 10.11959/j.issn.1000-436x.2023081 This DOI cannot
    be found in the DOI System. Possible reasons are: The DOI is incorrect in your
    source. Search for the item by name, title, or other metadata using a search engine.
    The DOI was copied incorrectly. Check to see that the string includes all the
    characters before and after the slash and no sentence punctuation marks. The DOI
    has not been activated yet. Please try again later, and report the problem if
    the error continues. WHAT CAN I DO NEXT? If you believe this DOI is valid, you
    may report this error to the responsible DOI Registration Agency using the form
    here. You can try to search again from DOI.ORG homepage REPORT AN ERROR DOI: URL
    of Web Page Listing the DOI: Your Email Address: Additional Information About
    the Error: More information on DOI resolution: DOI Resolution Factsheet The DOI
    Handbook Privacy Policy Copyright © 2023 DOI Foundation. The content of this site
    is licensed under a Creative Commons Attribution 4.0 International License. DOI®,
    DOI.ORG®, and shortDOI® are trademarks of the DOI Foundation."'
  inline_citation: '>'
  journal: Tongxin Xuebao/Journal on Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Multi-function supported privacy protection data aggregation scheme for V2G
    network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Pérez R.
  - Rivera M.
  - Salgueiro Y.
  - Baier C.R.
  - Wheeler P.
  citation_count: '4'
  description: Software Defined Networking (SDN) is a communication alternative to
    increase the scalability and resilience of microgrid hierarchical control. The
    common architecture has a centralized and monolithic topology, where the controller
    is highly susceptible to latency problems, resiliency, and scalability issues.
    This paper proposes a novel and intelligent control network to improve the performance
    of microgrid communications, solving the typical drawback of monolithic SDN controllers.
    The SDN controller’s functionalities are segregated into microservices groups
    and distributed through a bare-metal Kubernetes cluster. Results are presented
    from PLECS hardware in the loop simulation to validate the seamless transition
    between standard hierarchical control to the SDN networked microgrid. The microservices
    significantly impact the performance of the SDN controller, decreasing the latency
    by 10.76% compared with a monolithic architecture. Furthermore, the proposed approach
    demonstrates a 42.23% decrease in packet loss versus monolithic topologies and
    a 53.41% reduction in recovery time during failures. Combining Kubernetes with
    SDN microservices can eliminate the single point of failure in hierarchical control,
    improve application recovery time, and enhance containerization benefits, including
    security and portability. This proposal represents a reference framework for future
    edge computing and intelligent control approaches in networked microgrids.
  doi: 10.3390/s23073395
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all    Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 23 Issue 7 10.3390/s23073395 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Academic Editor Chun Sing Lai
    Subscribe SciFeed Recommended Articles Related Info Links More by Authors Links
    Article Views 2065 Citations 5 Table of Contents Abstract Introduction Main Disadvantage
    of an SDN Controller Hierarchical Control Approach Disaggregating Functionalities
    and Migrating SDN as Microservices Implementation of μONOS SDN Controller Experimental
    Scenarios and Results Communication Failure and Recovery Test Conclusions Author
    Contributions Funding Data Availability Statement Acknowledgments Conflicts of
    Interest Nomenclature Appendix A Appendix B References share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Moving Microgrid Hierarchical Control to an SDN-Based Kubernetes Cluster: A Framework
    for Reliable and Flexible Energy Distribution by Ricardo Pérez 1, Marco Rivera
    2,3,*, Yamisleydi Salgueiro 4, Carlos R. Baier 2 and Patrick Wheeler 3 1 Department
    of Computer Science, Faculty of Engineering, Universidad de Talca, Curicó 3341717,
    Chile 2 Department of Electrical Engineering, Faculty of Engineering, Universidad
    de Talca, Curicó 3341717, Chile 3 Department of Electrical and Electronic Engineering,
    Faculty of Engineering, University of Nottingham, Nottingham, NG7 2GT, UK 4 Department
    of Industrial Engineering, Faculty of Engineering, Universidad de Talca, Curicó
    3341717, Chile * Author to whom correspondence should be addressed. Sensors 2023,
    23(7), 3395; https://doi.org/10.3390/s23073395 Submission received: 13 February
    2023 / Revised: 13 March 2023 / Accepted: 18 March 2023 / Published: 23 March
    2023 (This article belongs to the Special Issue New Algorithms of Sensing/Metering
    and Optimal Control in Electrical Networks and Grid-Connected Converters) Download
    keyboard_arrow_down     Browse Figures Versions Notes Abstract Software Defined
    Networking (SDN) is a communication alternative to increase the scalability and
    resilience of microgrid hierarchical control. The common architecture has a centralized
    and monolithic topology, where the controller is highly susceptible to latency
    problems, resiliency, and scalability issues. This paper proposes a novel and
    intelligent control network to improve the performance of microgrid communications,
    solving the typical drawback of monolithic SDN controllers. The SDN controller’s
    functionalities are segregated into microservices groups and distributed through
    a bare-metal Kubernetes cluster. Results are presented from PLECS hardware in
    the loop simulation to validate the seamless transition between standard hierarchical
    control to the SDN networked microgrid. The microservices significantly impact
    the performance of the SDN controller, decreasing the latency by 10.76% compared
    with a monolithic architecture. Furthermore, the proposed approach demonstrates
    a 42.23% decrease in packet loss versus monolithic topologies and a 53.41% reduction
    in recovery time during failures. Combining Kubernetes with SDN microservices
    can eliminate the single point of failure in hierarchical control, improve application
    recovery time, and enhance containerization benefits, including security and portability.
    This proposal represents a reference framework for future edge computing and intelligent
    control approaches in networked microgrids. Keywords: kubernetes; hierarchical
    control; microgrids; microservices; software defined networking 1. Introduction
    Microgrid hierarchical control aims to regulate the network frequency and voltage
    through collaborative work between distributed energy sources. This strategy has
    changed the impact of distributed generation. However, it has imposed several
    challenges in power control systems, especially those related to integrating power
    electronics, telecommunications, fault monitoring, and security issues [1]. The
    trend of this strategy is to use three levels of hierarchical control to standardize
    the microgrid’s operation and increase its resilience. The primary level regulates
    the network’s frequency and voltage, ensuring power-sharing between the distributed
    generators (DGs) [2]. The most important approach is the droop control [3], which
    is based on the droop features of a conventional generator. Droop control ensures
    stability between frequency or active power ( 𝑓−𝑃 ) and voltage or reactive power
    ( 𝑉−𝑄 ). The secondary level stabilizes the voltage and frequency deviations due
    to the output impedance decoupling, failures in power-sharing or the presence
    of circulating currents [4]. At this level, sharing information in real time between
    the DGs and supporting the stability of the most critical control variables within
    their nominal values is essential. The literature provides evidence that integrates
    the power system with communications architectures in networked microgrids (NMG).
    For example, refs. [5,6,7,8] proposes a two-level hierarchical control capable
    of regulating the active and reactive power sharing. However, despite using a
    low-bandwidth communication system, the topology needed to be more scalable due
    to the recent development of complex control algorithms. Previously reported research
    modified the communications architecture and control approach to add new facility
    units and control algorithms [9]. Furthermore, conventional communications and
    routing protocols cannot handle multiple topology events and lack sufficient intelligence
    to make appropriate control decisions [10]. Software Defined Networking (SDN)
    is a modern approach to networking that allows for controlling and managing network
    resources and traffic flow through software abstractions [11]. SDN provides greater
    flexibility and scalability by decoupling the control and data planes and enabling
    network administrators to configure and manage the topology behavior. This leads
    to faster deployment times, improved network visibility, and easier troubleshooting
    [12]. Additionally, it provides new opportunities for network automation, real-time
    analytics, and increased security through granular policy enforcement. SDN also
    promotes an open and interoperable network environment, enabling organizations
    to utilize various vendor offerings and technology advancements. The main limitation
    of SDN technology is the use of a centralized control plane and the inconveniences
    associated with a single point of failure of this critical node. The centralized
    control plane can potentially lead to network downtime if not properly managed.
    In [13,14,15], the authors propose a set of systems that aim to solve the drawbacks
    of conventional SDN technology. However, using a reliable communications strategy
    is still necessary to improve the heterogeneous communication between different
    DGs according to the restrictions defined by the architecture and control algorithms.
    On the other hand, the complexity associated with multiple programming APIs within
    a monolithic controller requires a high level of technical expertise and represents
    a significant challenge to network operations. Despite these limitations, different
    researchers and organizations are allocating resources toward modernizing their
    power systems and communication networks. One promising approach is to distribute
    the functions of the SDN controller into multiple microservices [16,17]. The goal
    is to implement these functions as distributed units with a replication factor
    for redundancy. By doing so, the workload and resources of the SDN controller
    are spread among different worker nodes. In the event of a failure, the data remains
    accessible through the microservice controller without requiring manual intervention.
    There is a wide variety of SDN controllers developed using different programming
    languages [18,19], among which Ryu [20], Opendaylight [21], and ONOS [22] being
    the most significant due to their capabilities, ease of deployment, and reliability.
    A study in [17] explored the development of a microservices-based system for the
    Ryu controller within the OpenStack virtualized network infrastructure [23]. However,
    despite dividing the main functionalities of the Ryu controller into Docker containers,
    these microservices cannot automatically scale in case of failure. In the case
    of a worker node failure, there is no automatic method to restore the functions
    assigned to that element, impacting the communication system and the microgrid’s
    performance. Although this approach offers a fresh perspective on deploying SDN
    controllers, it must be noted that Ryu was not originally designed to support
    microservices [24]. The previous drawback requires careful testing of the architecture,
    services, and communication interfaces before implementation in real-world environments.
    𝜇𝑂𝑁𝑂𝑆 is an SDN controller that uses microservices in large-scale communication
    networks [25]. The ONOS Project presents 𝜇𝑂𝑁𝑂𝑆 as a solution for disaggregating
    the functionalities of the SDN controller into microservices. This project aims
    to enhance the versatility of the SDN architecture by targeting cloud computing
    platforms, data centers, and bare-metal deployments. A bare-metal cluster removes
    the hypervisor overhead and puts the Kubernetes installation directly on the host
    server’s operating system. An orchestrator that automates network management,
    load balancing, and new instances provides an intelligent topology that uses microgrid
    resources efficiently. Combining SDN with Kubernetes provides an intelligent system
    that analyzes traffic and application requirements in real-time to adjust network
    configuration and application deployment. Recent literature [26,27] suggests that
    although there are differences between this proposal and artificial intelligence
    (AI), the communication alternative can also be classified as intelligent because
    of SDN’s programmability and the system’s capacity to optimize network performance.
    Kubernetes manages containerized applications’ orchestration and deployment, while
    SDN manages the network infrastructure supporting those applications. Together,
    they can dynamically respond to changes in traffic, adjust network policies, and
    optimize network performance. For instance, when traffic surges, SDN can automatically
    increase network resources by adding nodes or expanding bandwidth. Likewise, if
    an application needs to be migrated, SDN can adapt the network routing to reduce
    recovery time. Compared to traditional networking, which can be time-consuming
    and error-prone, our proposal offers automated configuration and management that
    is not restricted to specific conditions [28]. According to literature [29,30],
    the restrictions in the communications systems imposed by the penetration of more
    distributed generation sources force the use of distributed, autonomous and efficient
    communications strategies to control the power system efficiently. Microgrids
    need bare-metal Kubernetes to ensure high reliability, low latency, and optimal
    resource utilization [31], which are critical requirements for a microgrid’s efficient
    and safe operation. Standby servers may provide low reliability and performance
    than a Kubernetes-based infrastructure, especially in dynamic and changing load
    conditions. There are several reasons to propose bare-metal Kubernetes for microgrids:
    Reliability [32]: Bare-metal Kubernetes provides a highly reliable infrastructure
    for microgrids by distributing the workloads across multiple nodes and ensuring
    the high availability of resources. Computational cost [33]: Low costs because
    virtualization software is no longer necessary. Cluster automation and microservices
    deployment are straightforward because there is no hypervisor. Low latency [34]:
    Microgrids require low latency and high-speed communication between the devices
    to ensure safe and efficient operations. Bare-metal Kubernetes provides low-latency
    network connectivity and efficient data communication. Optimal resource utilization
    [33,35]: Microgrids require optimal resource utilization to ensure energy efficiency
    and reduce operational costs. Bare-metal Kubernetes provides efficient resource
    allocation and utilization, which can help optimize energy consumption and reduce
    costs. Scalability [36]: Network configuration is more straightforward on the
    bare-metal cluster and troubleshooting. Microgrids require the ability to scale
    up or down depending on the demand. Bare-metal Kubernetes provides automatic scaling
    and load balancing, which can help ensure optimal performance under varying load
    conditions. Despite the previous comments and the benefits of Kubernetes, there
    is no evidence of SDN microservices being used to solve the disadvantages of microgrid
    hierarchical control. Decoupling the applications from the monolithic controller
    into a series of sub-functions enables the deployment of a highly flexible SDN
    architecture. The most critical impact of this research is the ability to coordinate
    different applications as microservices and provide the guidelines for programming
    APIs in microgrid hierarchical control. This paper proposes a novel hierarchical
    control architecture based on microservices to address the limitations of SDN
    controllers in networked microgrids. Implementing a set of controllers as a distributed
    system in the Kubernetes bare-metal cluster increases the redundancy and resilience
    of the communication system. The load is distributed among various devices based
    on communication and power system restrictions. Rather than using a monolithic
    controller, the controller functions are divided into a group of microservices.
    The key contributions of this research are presented as follows. A new architecture,
    based on microservices, as a solution to the centralized SDN controller problem
    regarding load balancing, scalability, and low latency. The proposed methods improve
    the global resilience of the system and allow the integration of SDN controllers
    as pod services in distributed Kubernetes platforms. The proposed approach allows
    the deployment of bare-metal Kubernetes cluster parameters and can be applied
    to multiple configurations of AC/DC microgrids. A new SDN communication architecture
    has been developed for hardware-in-the-loop platforms connected to Raspberry pi,
    serving as both a Kubernetes worker and an OpenFlow communication device. Furthermore,
    this paper analyzes the most significant drawbacks of the SDN control plane in
    networked microgrids. Provides a proof of concept to apply 𝜇𝑂𝑁𝑂𝑆 for segregating
    and orchestrating services in bare-metal Kubernetes cluster. The proposed method
    decreases the data flow traffic through the SDN infrastructure, setting the most
    appropriate route between the DGs. The distributed communication system is capable
    of managing real-time energy data. This implementation can be replicated and modified
    through our project’s GitHub repository. Furthermore, it designs a monitoring
    tool integration that allows visualization of logs and measures metrics to carry
    out a complete analysis of the networked microgrid. The rest of this paper is
    organized as follows. Section 2 reviews the main limitations of SDN controllers
    according to physical architecture, interfaces, reliability, and scalability.
    Section 3 describes the development of hierarchical control of microgrids and
    the control method applied. Section 4 provides the microservices benefits of SDN
    controller functionalities disaggregation and our methodology. The implementation
    of the SDN controller as a group of microservices is presented in detail in Section
    5. The results and discussion of the significance of our proposal are given in
    Section 6, according to different metrics. On the other hand, Section 7 presents
    different communication failures and compares the performance of the communication
    systems. Section 8 concludes the work and provides an overview of future research
    topics. 2. Main Disadvantage of an SDN Controller This section includes the fundamental
    disadvantages of SDN controllers, comparing monolithic and centralized architectures
    versus microservices-based architectures. The contrasted elements allow determining
    the most relevant metrics that mark the performance of the SDN protocol. The scalability
    of the communications system proposed by SDN integrated with Kubernetes allows
    the deployment of an intelligent DG architecture at a reduced cost and increases
    the overall security through SDN controller functions. Although Software Defined
    Networking has several benefits, it has certain limitations that must be considered.
    Additionally, the security elements of SDN solutions are still a concern, as the
    centralized control plane can be a target for cyberattacks. The most significant
    drawbacks of SDN technology are summarized below. 2.1. Centralized Controller
    The most widely used architecture for microgrid control uses a centralized SDN
    controller as an intelligent element within the topology. However, critical aspects
    such as latency, network convergence (less than 100 ms), reliability (close to
    99% ), and packet losses must be managed carefully [37,38]. Achieving these properties
    is difficult in a centralized control scheme, especially for a large topology,
    due to communication devices’ propagation latency and processing time. In this
    way, centralized SDN controllers have significant challenges, especially regarding
    scalability and reliability. Consequently, the controller node becomes a key target
    for cyberattacks. If the information stored in the controller is compromised,
    there is no way to recover it, resulting in a negative impact on the network and
    economic losses. A possible alternative is to use distributed SDN controllers
    as stated in [39,40]. 2.2. Monolithic Controller Multiple SDN implementations
    adopt a centralized controller that relies on a monolithic control plane architecture.
    All the possible functionalities are included in an extensive control program,
    which needs to be flexible for changing topology conditions [17]. Controllers
    often use a set of services from a pool, which can result in some services being
    unused or restricted by the controller. This feature forces the replication of
    the entire control implementation in the distributed architecture, which limits
    controller portability. As a result, developers must carefully prepare the functionalities
    and ensure integration between the modules. This behavior poses a significant
    challenge for users who need to implement new services and require fast controller
    deployment. Monolithic and microservice architectures are two approaches to building
    Software-Defined Networking (SDN) solutions. Table 1 compares monolithic centralized
    SDN controller with microservices controller. A monolithic SDN is simple to implement
    and deploy but has scalability, flexibility, and fault tolerance limitations.
    In contrast, microservice SDN is designed to decompose the control plane into
    smaller, independent services that communicate with each other through APIs. Each
    service is responsible for a specific control function and can be developed, deployed,
    and scaled independently. Table 1. Comparison of monolithic centralized SDN controller
    vs. microservices controller. 2.3. Variability in Programming Interfaces Some
    of the most popular SDN controllers (such as OpendayLight, Ryu, or ONOS [18,41])
    use REST API interfaces as a communication mechanism. The RESTful API is a device
    communication interface to secure information exchange over the hypertext transfer
    protocol. However, each controller exposes its APIs in a different way, forcing
    users to modify the request syntax or the programming language according to the
    specific conditions [16]. This lack of standardization turns the applications
    into systems that depend on a particular SDN controller. 2.4. Dependencies between
    Applications and Controllers The close relationship between control applications
    and the controller type presents a challenge that restricts the ability to reuse
    applications and module configurations. In several situations, modifying the programming
    language and readjusting critical plugins becomes necessary due to the interdependence
    between the modules and the controller’s central component. 2.5. Lack of Reliability
    and Scalability of SDN Controller The principal reason for the lack of reliability
    and scalability is the high dependency between the SDN controller and the communication
    events handled by the control plane. Furthermore, a monolithic architecture is
    complex to scale into a standalone system due to the relationship between the
    application of the SDN controller and the communication device. Any failure in
    one component will result in a cascade failure of the entire system. 3. Hierarchical
    Control Approach Hierarchical control is a practical approach to managing power
    sharing in a Microgrid. The primary control level determines the amount of power
    to be generated by each DG based on factors such as power demand, availability
    of energy, and system constraints, as Figure 1 shows. The secondary control level
    manages the power-sharing between the Microgrid and the utility grid, ensuring
    that the Microgrid operates within acceptable limits. System optimization and
    long-term planning are carried out at the tertiary control level to ensure optimal
    power sharing among the energy sources, energy storage systems, and loads. Figure
    1. Hierarchical control levels and communication restrictions for networked microgrids.
    Power sharing between DGs is usually carried out by parallel inverters connected
    to a common AC bus with multiple loads [42]. The proposed controller in this paper
    considers the microgrid as three Voltage Source Inverters (VSI) feeding an RL
    load at the point of common coupling. An RL load was chosen to study the microgrid’s
    active and reactive power sharing. Each DG represents a power source implemented
    in The Simulation Platform for Power Electronic Systems (PLECS) [43,44]. Droop
    control in parallel inverters is a widely used strategy to regulate MG power sharing.
    The main goal of primary droop control is to set a proportional load sharing among
    DGs, based on the well-known (P-Q) droop method [5]. Each inverter has an external
    droop control loop to improve performance and provide a decentralized control
    method. Figure 2 shows the strategy applied to regulate frequency and voltage
    for one VSI. The details of hierarchical control can be found in Appendix A and
    Appendix B. Figure 2. General structure and hierarchical control of one VSI. The
    acronym SC represents secondary control. The control strategy is evaluated according
    to different events within the microgrid. The droop control is activated during
    the first second until it reaches the steady state condition. As shown in Figure
    3 and Figure 4, there is a deviation in voltage and frequency that needs to be
    solved by the secondary control. For that reason, it is necessary to perform secondary
    control to reach the stability of the MG. The secondary control is activated after
    10 seconds and remains until the end of the simulation. Figure 3. Voltage end
    frequency regulation with SDN microservice controller of this proposal. Figure
    4. Line to the neutral voltage and phase current for SDN microservice proposal.
    Research studies have used a hierarchical control approach with an SDN-based communication
    architecture to enhance overall system intelligence [13,37,45]. However, while
    this approach addresses some aspects of power sharing in networked microgrids,
    the communication system remains a critical factor that impacts the power system
    and hinders overall recovery. The monolithic architecture of the SDN controller,
    the integration of new control functionalities, and the excessive workload can
    be improved through a microservices-based architecture and automatic orchestration.
    4. Disaggregating Functionalities and Migrating SDN as Microservices SDN controllers
    implement different modules to handle the most relevant aspect of the communication
    devices. These modules afford key functionalities, such as selecting the best
    route for message forwarding, monitoring topology changes, and enhancing security
    [46,47], as outlined in Figure 5. The process starts with discovering and managing
    the nodes, finding the active communication links, and updating functions accordingly.
    Next, optimal packet flow and routing management are established by creating new
    flows and forwarding packets. The controller must activate the functions to monitor
    the traffic and guarantee the quality of service (QoS) according to predefined
    metrics. Finally, several options are applied to ensure network management and
    security, avoid latency issues, and improve restrictions imposed by the power
    system. Figure 5. Main functionalities of the SDN controller. Previous functionalities
    are the components that support segmentation in microservices. Components and
    Interfaces as Microservices In this proposal, the SDN controller deploys a set
    of microservices in several Docker containers, as shown in Figure 6. The upper
    layer corresponds to the SDN controller, the middle layer represents the communication
    topology, and the bottom layer is the electrical system. The container’s functionality
    includes traffic routing, topology management, and event handling. The ability
    to segregate the controller functions as microservices allows a distributed system
    to scale according to topology demand. If the number of containers increases,
    a platform for orchestrating multiple pods is required. Figure 6. Communication
    framework for microservices implementation. Figure 6 presents the proposed topology
    and considers the aggregation of microservices deployed in three Raspberry pi
    4 (with ARM architecture). Given the increased computational capacity of the latest
    versions of Raspberry and its lower cost compared to other computing devices,
    this platform is selected to operate as a worker in the Kubernetes cluster and
    an OpenFlow communication device. OpenFlow is a communication protocol SDN uses
    to standardize the interfaces between the control and data planes [48]. This research
    uses OpenFlow because it is a standard supported by multiple communication devices,
    adding great flexibility and programmability to the network. The flexibility of
    OpenFlow minimizes the effort and resources necessary to manage complex networks,
    including those in microgrid hierarchical control. The Kubernetes cluster is deployed
    in K3s [49] to obtain the SDN controller’s global functionality. K3s is a lightweight
    Kubernetes distribution built for IoT and Edge computing. The three Raspberrys
    are physically independent and configured as a high-availability cluster to improve
    the controller operation and provide good resilience through microservices. The
    synchronization of each container and the global administration of the SDN controller
    is orchestrated by Rancher [50]. Rancher is a Kubernetes-based orchestration software
    integrating container monitoring and management tools through a simple graphical
    interface. The northbound interface (NB) allows the interaction between the SDN
    controller and external applications. On the other hand, the southbound interface
    (SB) standardizes the operation of the communication protocols, as in the case
    of OpenFlow [51]. REST APIs are communication channels that use the HTTP protocol
    to carry out operations such as GET, POST, or DELETE on data. It has high scalability,
    good performance, and the ability to decouple its functions, making it an ideal
    candidate for developing microservices. 5. Implementation of μONOS SDN Controller
    Microgrids are changing into more complex and extensive networks in which applications,
    service virtualization, and edge computing are highly related to control strategies
    [29,52]. Micro ONOS (μONOS) is a new version of the SDN controller, developed
    by the Open Network Foundation (ONF [53]), which uses microservices to deploy
    a scalable infrastructure with excellent throughput and low latency. It is based
    on Docker containers deployed in a cloud-based infrastructure or local data centers.
    Unlike monolithic controllers that integrate multiple APIs, it comprises a few
    interfaces such as Google Remote Procedure Call (gRPC), gRPC Network Management
    Interface (gNMI) and P4Runtime [25]. The 𝜇𝑂𝑁𝑂𝑆 infrastructure, network functions
    and monitoring services only support Kubernetes applications through Helmcharts
    implementation [54]. To install, manage and delete the device configurations and
    their performance, 𝜇𝑂𝑁𝑂𝑆 configures the gNMI as an open-source data management
    protocol for network devices. The functionalities provided by gNMI can be modeled
    using YANG (Yet Another Next Generation data modeling language) [25]. The network
    manager interacts with the SDN controller through the gRPC interface, accessing
    both the onos-cli and onos-gui services for network management, network modifications
    and reverting changes. According to ONF [53], the onos-config service offers a
    gNMI endpoint for various functions, including reading states, configuring settings,
    and subscribing to specific features. This interface can also prevent invalid
    values and enable the operational state. The 𝜇𝑂𝑁𝑂𝑆 identification component facilitates
    the controller’s management from external applications, as depicted in the top
    layer of Figure 7. A proposed middleware enables information exchange between
    external applications (power-sharing information from networked MG) and the SDN
    control plane. As shown in Figure 7, the core of the SDN controller is separated
    from the event handler within the middleware, allowing communication between the
    REST API and the controller microservices. This service, named onos-config, is
    responsible for linking the module’s functionality to the specific microservices
    that are executed in a distributed manner. Figure 7. High-level design of the
    proposed 𝜇𝑂𝑁𝑂𝑆 controller (modified from [25]). The essential microservice of
    Figure 7 is the block onos-config, as it manages the configuration of devices
    through gNMI interfaces and registers all events to send them to the Atomix driver.
    The Atomix driver implements an API to scale the 𝜇 𝑂𝑁𝑂𝑆 Kubernetes resources.
    This property of scaling resources adds greater redundancy to the controller.
    It provides a system with distributed additional resources and is responsible
    for maintaining and managing the µ𝑂𝑁𝑂𝑆 service. Atomix controller details can
    be found in the GitHub repo of the Open Networking Foundation [55]. 5.1. Functionalities
    of onos-config Module The onos-config module strives to handle network and device
    alterations by invoking the NetworkChange and DeviceChange services, respectively.
    These services keep records of all change logs and pass them to the Atomix drivers
    via the gRPC interface. For connecting devices through the southbound interface,
    onos-config only supports it through gNMI. Furthermore, the YANG models set the
    configuration and topology architecture into onos-config service. The deployment
    of this module requires a Kubernetes cluster capable of running Helmcharts as
    detailed in its deployment page [56]. The interaction between onos-config and
    onos-cli allows the user to define rules or route paths through the gNMI interface.
    To execute it, access the onos-cli pod and run the plugins from there (you can
    view the list of plugins by running onos-config get plugins). Figure 8 shows the
    deployment of the pods during the execution of the 𝜇𝑂𝑁𝑂𝑆 in the topology. Five
    pods of the onos-config service are deployed, and six pods of Atomix increase
    the availability and distribute the services across the nodes. All steps to deploy
    this proposal are available in the GitHub repository [57]. Figure 8. Number of
    pods of the SDN controller distributed in the worker’s nodes. 5.2. Network Interface
    Cluster Implementation This paper implements two network interfaces for each Raspberry,
    as shown in Figure 9. The eth0 is the default ethernet interface of the Raspberry
    pi 4. It serves as the Linux bridge and is used for configuring Kubernetes clusters,
    exchanging pods control information, accessing Rancher and the load balancer,
    and connecting to the Internet gateway. It means that eth0 acts as an overlay
    interface for external communications. On the other hand, eth1 is used by the
    OpenFlow protocol to communicate between SDN service pods. Conversely, the eth1
    interface, connected via a USB to Ethernet adapter, enables the substitution of
    the veth interfaces of the pods with kbr-int-ex. Each pod in the cluster will
    use the kbr interface instead of its veth to avoid routing and forwarding issues.
    Figure 9. Kubernetes management and overlay tunneling networking with Calico-CNI.
    Calico CNI (Kubernetes Container Network Interface (CNI)) deploys a Daemonset
    on each node, ensuring communication between gRPC and the 𝜇 𝑂𝑁𝑂𝑆 controller. In
    other words, this plugin provides networking for the containers and pods within
    the Kubernetes cluster. The cluster formed by the Raspberry nodes can operate
    as conventional Ethernet devices (via TCP and Unix domain socket) or as an OpenFlow
    switch to handle SDN traffic. 5.3. Create the Kubernetes Cluster on Raspberry
    K3s distribution [49] allows deploying of the bare-metal cluster and is an excellent
    choice for IoT devices, particularly Raspberry pi. From Rancher’s official documentation
    [58], a high-availability cluster with an embedded database is implemented. However,
    using a single load balancer as implemented in the default configuration brings
    back the drawbacks of the centralized system. Integrating the K3s cluster with
    Keepalived and HAproxy as described in [59], solves the previous disadvantages
    in a distributed way. To set up an HA Kubernetes cluster using Keepalived and
    HAproxy, you need to install and configure Keepalived and HAproxy on each node
    in the cluster. Keepalived is used to manage the virtual IP address that clients
    use to access the Kubernetes API. In contrast, HAproxy is used to load balance
    incoming traffic across the Kubernetes API server nodes. This alternative is superior
    to the external database implementation because the storage is distributed in
    each etcd node’s services [58], increasing the system’s availability and removing
    the single point of failure through distributed storage. Ansible automates node
    creation, storage configuration, network interfaces, services, and deployment
    processes. This tool allows (in a simple way) the cluster to be deployed through
    the execution of a series of scripts developed in Ansible. All the manifest and
    Ansible inventory files are in the shared GitHub repo. 5.4. Connection to PLECS
    RT Box Possible communication alternatives between Raspberry and PLECS are SPI,
    I2C, and CAN protocols. However, the PLECS RT Box only has two SPI communication
    modules (SPI1 and SPI2), leaving one of our worker nodes unconnected. In [60],
    the average data rates of the three technologies are compared. According to the
    PLECS manual, SPI has the best data rate, followed by the CAN bus and I2C. We
    decided to connect the third node of the cluster to the same SPI port of node
    2’s PLECS server for these reasons. Although this is not the best communication
    alternative, as the frequency and voltage values on each Raspberry are averaged,
    this will not affect the overall performance of the MG. For more complex microgrid
    implementations, it is recommended to use the SFP transceiver modules, available
    from PLECS, to achieve speeds of up to 10Gbps. Each Raspberry will serve as an
    OpenFlow communication device that ensures the exchange of secondary control information.
    Figure 10 shows the practical implementation of this proposal. There are three
    Raspberrys connected through Ethernet and USB. The first connection allows external
    access, and the second provides SDN functionalities. Additionally, the PLECS platform
    is wired to the cluster through the SPI bus of the Launchpad F28069M. Figure 10.
    Experimental setup for Kubernetes cluster with Raspberry pi: (a) PLECS output,
    (b) ONOS command line interface, (c) SDN flows during communication, (d) Raspberry
    pi Kubernetes cluster, (e) USB network adapter, (f) overlay interface, (g) Cisco
    router and 52switch, (h) SPI connection, (i) MG output current. According to the
    price of this proposal, Table 2 shows that it is possible to create a bare-metal
    cluster with less than 1300 USD. The Cisco router can be replaced by other cheaper
    alternatives, such as the ZodiacFX [61]. Table 2. Total cost of bare-metal cluster.
    5.5. Monitoring Platform Different factors, such as hardware resources, load,
    or communication architecture, limit the number of pods executed on each device.
    However, with Rancher, these pods can be scaled automatically without compromising
    the cluster’s overall structure. To configure automatic scaling in Rancher is
    necessary to select the deployment. From there, the “Scaling” tab specifies the
    minimum, maximum, and desired number of replicas for the deployment and also sets
    the scaling policy based on CPU or memory usage. This proposal shows the number
    of replicas for each service in Figure 8, with the boundary for scaling new pods
    set at 80% of CPU usage. It’s important to note that automatic scaling requires
    a monitoring and metrics system to track your deployment’s resource utilization.
    Rancher integrates with Prometheus and Grafana to trigger alerts according to
    the threshold values configured. Finally, the Grafana graphical user interface
    allows monitoring the deployment’s status and the number of replicas scaled. The
    solution to monitoring the communication devices’ status and the tools’ interaction
    is presented in Figure 11. Our proposal uses a Rancher Helmchart to develop a
    Java application (Prometheus Exporter) to obtain information about network metrics.
    Furthermore, it exports the data to Prometheus, which is responsible for monitoring
    events and triggering alerts according to standard conditions. Figure 11. Proposed
    architecture for monitoring and integration of the electrical system, SDN topology,
    and Cloud Computing environment. Prometheus sends the information to the local
    storage to collect operating system metrics. We use the 𝜇𝑂𝑁𝑂𝑆 interface to obtain
    information and network metrics. Different notifications can be triggered through
    the Prometheus configuration file to perform fast reactions without downtime.
    Finally, Grafana allows importing a series of dashboards with information on the
    DG flows. This tool obtains the knowledge of the packet flows that pass through
    the USB ethernet adapter to the SDN controller. Figure 12 demonstrate the correct
    integration of Prometheus and Grafana within the Kubernetes cluster. Using Rancher
    in this proposal simplifies the deployment and configuration of the services.
    Figure 12. Types of events in Grafana tool. Packets out to PLECS with destination
    microservices in K3S cluster. Grafana registers data. An essential element to
    consider when implementing a high-availability Kubernetes cluster on Raspberry
    is the performance of the computational resources. As the cluster scales, the
    computational resources become more limited. Increasing the number of pods in
    Kubernetes can be done to minimize the impact on service performance as follows.
    Ensure that the nodes in your cluster have enough resources (CPU, memory, storage)
    to support the increased number of pods. The notification system and the alerts
    configured in Grafana allow the monitoring of resource usage and global capacity.
    Using horizontal pod autoscaling (HPA) automatically adjusts the number of pods
    based on resource usage and demand. HPA can be configured based on CPU usage,
    memory usage, or custom metrics. To prevent resource contention and performance
    issues, pod anti-affinity rules ensure that pods are not placed on the same node.
    This method avoids the scheduling of pods on the same node. Optimize pod resource
    requests and limits to function correctly. Use pod disruption budgets (PDB) to
    ensure that a minimum number of pods are available during node maintenance or
    failures. By setting a PDB, you can guarantee that the service is unaffected by
    removing pods from the cluster. Monitoring the service’s performance following
    the previous points and adjusting the settings as necessary to optimize resource
    utilization and performance is crucial. 6. Experimental Scenarios and Results
    To analyze the results of this proposal, a series of critical elements were tested,
    which imposed restrictions on the communications and power systems. Figure 13
    present the topology used to test the microservice (left side) and monolithic/OSPF
    (to the right side). OSPF and monolithic only differ in the setup of Raspberry
    configuration. Each Raspberry was configured as a router for OSPF, while the monolithic
    was configured as an OpenFlow switch. Table 3 shows the main parameter settings
    of the experiments. The first scenario tested is latency, introduced by decoupling
    the controller functionalities into distributed microservices. Figure 13. Proposed
    architecture for testing the metric performance. Table 3. Electrical and control
    parameters of the MG. 6.1. Latency The evaluation tests of this proposal consider
    the measurement of two types of latency. The first column in Figure 14 considers
    the response time when sending the first message from the MG architecture. This
    latency gives us a measure of the initial cost of the distributed controller as
    microservices and the time it takes to obtain a valid route. Once the entry in
    the flow table is updated, the rest of the packets do not need to go through the
    SDN controller, so the rest of the flows are expected to have lower latency. The
    columns update1 and update2 in Figure 14 represent this type of test. Figure 14.
    Comparison of latency between OSPF, SDN with a monolithic controller and SDN with
    the microservice controller. “First” columns represent the latency for the first
    message and “update” columns represent the latency when the route is established.
    The second experiment considers the average latency for the rest of the packets
    using OSPF routing and SDN technology. As shown in Figure 14, the performance
    of the OSPF-based strategy is slightly better. This is due to the additional processing
    overhead added by the distribution of microservices in three different Raspberry
    nodes to the processing time of the applications. The complexity of the network
    topology, as determined by the number of OpenFlow switches, notably impacts the
    number of controller connections and the processing capacity of Docker containers.
    The connections between two DGs are denoted by first1 and update1, while the connections
    between DG1 and the SDN controller are represented by first2 and update2. In OSPF,
    the connection is established between the two nearest DGs. As can be seen, the
    highest latency is originated when the first flow is sent between two nodes. Power
    data (frequency and voltage) shared by hierarchical control in PLECS are shipped
    using the SPI and CAN protocol, and the results are summarized as average round
    trip time. However, even though OSPF and SDN with centralized control have slightly
    higher performance, the need to use a load balancer is evident, especially in
    the topology with a monolithic SDN controller and multiple DGs. Furthermore, when
    the traffic increases, its performance starts to degrade. For the OSPF routing
    strategy, it can be seen that the sending of the first packet of the topology
    is slightly higher than its competitors. This is mainly due to the neighbor discovery
    algorithm and the SPF algorithm needing to know the entire topology for proper
    performance. To avoid a single point of failure, the proposal of this paper incorporates
    a distributed load balancer, following the recommendation in [59]. The researchers
    performed three communication tests to evaluate the response of the proposed alternatives
    as a solution to the hierarchical control problem in electrical microgrids. Figure
    15 illustrates the percentages of packets successfully delivered on the first
    transmission, those requiring single retransmission, those requiring multiple
    retransmissions, and those experiencing packet loss for each scenario. Furthermore,
    Figure 15 presents a trade-off between monolithic and microservices architectures
    regarding latency and throughput. Ten simulations were conducted for each of the
    three tests, and the resulting averages were analyzed to investigate the aforementioned
    relationship across different data rates. Specifically, low data rates ( 0<𝑇ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡≤299
    Mbps), medium data rates ( 300<𝑇ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡≤599 Mbps), and high data rates ( 𝑇ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡≤600
    Mbps) were covered in the analysis. The message throughput is modified from the
    Plecs simulation, increasing the sample period of secondary control. This graph
    shows the average loss rate for the centralized communication strategies is similar
    (with a difference of about 5%). In comparison, the distributed systems improve
    these results by 25 to 60% for the tests performed. The average latency was 287
    ms for the global test. The latency increases as the throughput increase due to
    the closeness to the speed limit supported by the Raspberries. Figure 15. The
    trade-off between latency and throughput varies across different test scenarios.
    Columns 𝜇𝑆 refers to this proposal, 𝑀𝑙 means Monilithics architecture and OSPF
    is O. The findings reveal that monolithic architectures can offer lower latency
    since all the system components are closely integrated and communicate directly,
    reducing network communication overhead. However, this close coupling can also
    limit the system’s horizontal scalability and ability to handle high throughput
    requirements. On the other hand, microservices architectures can provide higher
    throughput as the system can be scaled horizontally by adding more instances of
    individual services as needed. However, the added network communication between
    services can increase latency and reduce the system’s response time. Typically,
    the Raspberry Pi 4 (ARM64) can achieve consistent transfer rates of 600–700 Mbps
    with appropriate network configuration and optimization. Therefore, the choice
    between monolithic and microservices architectures depends on the system’s requirements.
    A monolithic architecture may be the better choice if low latency is critical
    and high throughput requirements are manageable with a tightly-coupled system.
    A microservices architecture may be more appropriate if high throughput is critical
    and latency can be tolerated with loosely-coupled services. Our proposal handles
    the surge in demand by increasing the transmitted packets. In contrast, the monolithic
    architecture and OSPF demonstrate reduced resilience, scalability, and multiple
    retransmissions. In this way, it is evident that the three strategies have an
    acceptable behavior according to Table 4. However, OSPF and SDN microservices
    are two different approaches to network management, and they have distinct characteristics
    and features. The results of Table 4 can be summarized as follows. Table 4. Summary
    of communication results for test scenarios. Latency: Regarding latency, OSPF
    is a distributed protocol that relies on exchanging routing information between
    devices. It’s designed to find the shortest path between two points, which can
    help to minimize latency. In general, monolithic architectures can offer lower
    latency since all the system components are closely integrated and communicate
    directly, reducing network communication overhead. However, this close coupling
    can also limit the system’s horizontal scalability and ability to handle high
    throughput requirements. On the other hand, SDN microservices rely on a central
    controller that manages the network, and the latency can be affected by the communication
    between the controller and the devices. Throughput: OSPF is a protocol that supports
    link-state routing and can quickly adapt to network topology changes. As a result,
    it can provide high throughput in a stable network environment. In contrast, SDN
    microservices can provide higher throughput as the system can be scaled horizontally
    by adding more instances of individual services as needed. Recovery time: OSPF
    is designed to support fast convergence and can quickly recover from a link or
    device failure. However, the convergence time can depend on the size and complexity
    of the network. SDN microservices can also provide fast recovery times, but it
    depends on the specific implementation and configuration. Link failure: OSPF can
    detect a link failure and reroute traffic along an alternate path, which helps
    to maintain connectivity. SDN microservices can also detect link failures and
    potentially provide more granular control over how traffic is rerouted. Device
    failure: In OSPF, if a device fails, the routing tables are recalculated, and
    the network can continue to operate. In SDN microservices, the central controller
    can detect a device failure and reconfigure the network accordingly. Controller
    failure: In SDN microservices, the central controller is a single point of failure.
    If the controller fails, the network may not be able to operate correctly. However,
    many SDN solutions provide redundancy and failover mechanisms to minimize the
    impact of controller failure. Overall, OSPF and SDN microservices have different
    strengths and weaknesses, and the choice of which one to use depends on the specific
    network requirements and goals. All of these values are within the ranges defined
    by the IEEE 61850 standard [37] for safe microgrid operation. Nevertheless, this
    microservice implementation has the best portability (by using a Docker container),
    resiliency (provided by Rancher orchestrator), and scalability results (demonstrated
    by the recovery time presented in Table 5). Table 5. Recovery time of different
    protocols when a failure occurs. 6.2. Throughput Throughput is generally used
    to determine how well SDN routers and controllers can handle traffic and how efficient
    they are at this task. This test measures the number of packets sent per second
    in the case of OSPF, while SDN measures the number of flows installed on the devices.
    Equation (1) presents a simple way to calculate the throughput. 𝑇ℎ𝑟𝑜𝑢𝑔ℎ𝑝𝑢𝑡=𝑚𝑎𝑥𝑖𝑚𝑢𝑚_𝑟𝑒𝑐𝑒𝑖𝑣𝑒𝑟_𝑏𝑎𝑛𝑑𝑤𝑖𝑑𝑡ℎ/𝑟𝑜𝑢𝑛𝑑−𝑡𝑟𝑖𝑝_𝑡𝑖𝑚𝑒
    (1) The iperf3 tool obtains the maximum receiver bandwidth, while a simple ping
    returns the round-trip time. Figure 15 shows the results of this test. As is evident,
    the results show a better performance in the case of conventional strategies.
    However, this advantage may be compromised in more extensive networks where the
    OSPF protocol needs to describe all router neighbors. 7. Communication Failure
    and Recovery Test Combining Kubernetes with a set of SDN microservices can improve
    application recovery time by eliminating the single point of failure in hierarchical
    control. In a traditional network architecture, the control plane and the data
    plane are tightly coupled, which means that a failure in the control plane can
    lead to significant disruptions in the network’s operation. This hierarchical
    control model has a single point of failure, which can be a bottleneck for recovery
    time. However, with Kubernetes and SDN microservices, the control plane is decoupled
    from the data plane, and the control functions are distributed across the network.
    If a failure occurs in one part of the network, the rest can continue normally,
    and recovery time can be significantly reduced. Moreover, the combination of Kubernetes
    and SDN microservices offers a significantly automated and customizable network
    setting that enables quick and flexible network topology adjustments in response
    to any modifications in the infrastructure or application. This attribute further
    reduces the network reconfiguration time, hence enhancing the recovery period,
    which would otherwise require manual intervention. Combining Kubernetes with SDN
    microservices can improve application recovery time by eliminating the hierarchical
    control’s single point of failure and providing a highly automated and programmable
    network environment. This can help ensure that applications and services are always
    available and performing optimally, even during network failures. A communication
    system failure (closer to the distributed generation sources) is simulated in
    this scenario. The objective is to verify which strategy has better performance
    from the point of view of communications without degrading the power-sharing between
    the local controllers of the MG. The failure is generated, for instance, containing
    the SDN monolithic, and they are compared with the losses produced in one of the
    instances that include the microservices. The orchestrator is expected to be able
    to instantiate a new subsystem instance without degrading the performance of the
    MG. Kubernetes has been configured to be scaled automatically according to the
    research shown in [62]. Furthermore, in Figure 16, the packet loss percentage
    shows that microservices perform well due to distributed services above the nodes.
    Figure 16. Number of packet loss during convergence time. Figure 17 and Figure
    18 show the results obtained during power-sharing with and without hierarchical
    control, respectively. At one second, the droop control is started to distribute
    the active and reactive power-sharing. At 10 s, the hierarchical control is enabled
    to regulate voltage and frequency deviations. Since there is no message loss between
    controllers (minor glitches only in the average message delay), it proves the
    system’s robustness. However, the secondary control with a monolithic controller
    is highly susceptible to small latencies, CPU burden, and propagation delay. In
    this scenario, one of the OpenFlow switches was removed to determine the effects
    of the monolithic control strategy. This experiment should be understood as a
    way to highlight the robustness achieved by the SDN system in its deployment based
    on microservices. Figure 17. Active and reactive power sharing with SDN distributed
    controller in this proposal. Figure 18. Active and reactive power sharing with
    SDN monolithic controller. Our proposal uses a proactive approach to latency testing
    and a reactive approach to manage topology changes. The architecture will react
    immediately if a new distributed generation source is added, allowing proper power
    sharing, as Figure 3 shows. On the other hand, the orchestrator can add intelligence
    to the topology according to its appropriate programming. For example, our monitoring
    system architecture can detect latency increasing or node congestion and take
    the necessary actions to reduce the impact and the consequences. For example,
    it can scale a more significant number of instances as microservices and thus
    simultaneously serve more communication requests. 8. Conclusions Monolithic controllers
    have several drawbacks concerning scalability and reliability. In most cases,
    these architectures do not meet the requirements of fault tolerance and rapid
    adaptability, which are imposed by networked microgrids. This proposal’s most
    significant contribution was the property to dynamically reconfigure control flows
    based on a microservices architecture and the automatic deployment of microservices
    instances. Using a bare-metal Kubernetes cluster on a Raspberry pi and deploying
    distributed microservices allowed us to improve the reality of a distributed energy
    control system. Microservices testbeds demonstrated the SDN controllers’ rapid
    deployment, portability, high availability and resiliency to application failures.
    The Kubernetes orchestrator provided good scalability of the communication system,
    as well as improved the fault tolerance and replication capacity. This is due
    to the high fault tolerance, capable of managing and distributing the load between
    microservices. From a comparative perspective, this proposal significantly improves
    failure recovery time and resilience concerning communications devices. The API
    REST microservice topology allowed the splitting of the SDN controller’s core
    functionalities into small, well-defined functions. In all test case scenarios,
    reliability showed excellent behavior. Furthermore, the portability of all the
    nodes in the topology is possible due to the Docker containers. The ability to
    exchange control information between DGs over an SDN network allows them to regulate
    the system’s response and reach a steady state more quickly. The results show
    that to increase the resilience of the network, more sophisticated control strategies
    and highly available programmable communications networks are required. Finally,
    the monitoring architecture allows the export of logs in real-time and detects
    failures through notification software. Author Contributions Conceptualization,
    R.P.; Investigation, R.P., M.R. and Y.S.; Methodology, C.R.B. and P.W.; Project
    administration, M.R.; Software, R.P., M.R. and Y.S.; Supervision, C.R.B. and P.W.;
    Validation, R.P., M.R., C.R.B. and P.W.; visualization, R.P. and Y.S.; Writing—original
    draft, R.P., M.R. and C.R.B. All authors have read and agreed to the published
    version of the manuscript. Funding This research was funded by the National Doctorate
    Scholarship CONICYT 2019; ANID/ATE220023 Project; FONDECYT Regular Research Project
    1220556; CLIMAT AMSUD 21001 and FONDAP SERC Chile 15110019. Data Availability
    Statement Not applicable. Acknowledgments The authors express their gratitude
    to the University of Talca and the University of Nottingham for their support
    during this research period. Furthermore, we would like to thank the μONOS Team
    for the guidelines provided to complete this manuscript. Conflicts of Interest
    The authors declare that they have no known competing financial interests or personal
    relationships that could have appeared to influence the work reported in this
    paper. Nomenclature The following abbreviations are used in this manuscript: API
    Application Programming Interface SDN Software-defined networking DGs Distributed
    generators NMG Networked microgrids PLECS The Simulation Platform for Power Electronic
    Systems 𝜇 𝑂𝑁𝑂𝑆 Microservices Open Network Operating System Bare-metal Physical
    device designed to run dedicated services AC/DC Alternating current/direct current
    REST API Representational state transfer for application programming interface
    VSI Voltage source inverter RL Resistive-inductive load P, Q Active and reactive
    power MG Microgrid 𝜔 Frequency 𝜔 𝑟𝑒𝑓 , 𝜐 𝑟𝑒𝑓 Nominal frequency and voltage Δ𝑃
    , Δ𝑄 Power input error for droop control 𝑚 𝑝 , 𝑛 𝑞 Constant to handle maximum
    deviation of the microgrid 𝑃 𝑟𝑒𝑓 , 𝑄 𝑟𝑒𝑓 Nominal frequency and voltage 𝜐 𝑑𝑟𝑒𝑓
    Droop control voltage 𝜐 𝑎𝑏𝑐 Voltage across the filter 𝛼 , 𝛽 𝛼 , 𝛽 constant frame
    𝑖 𝑓 , 𝑖 𝑜 Filter and output currents 𝛿 𝜔 𝐷 𝐺 𝑘 Frequency obtained by secondary
    control 𝛿 𝜐 𝐷 𝐺 𝑘 Voltage obtained by secondary control 𝑘 𝑝 𝜔 , 𝑘 𝑖 𝜔 Controller
    parameters of PI 𝜔 𝐷 𝐺 𝑘 , 𝜐 𝐷 𝐺 𝑘 Average frequency and voltage broadcasted by
    each DG QoS Quality of service K3s Lightweight Kubernetes ONF Open Network Foundation
    gRPC Remote Procedure Calls gNMI gRPC Network Management Interface gNOI gRPC Network
    Operations Interface NB, SB North bound and south bound interfaces P4Runtime Control
    plane specification for controlling the data plane elements YANG model Yet another
    next-generation data modeling language CNI Kubernetes container network interface
    CAN Controller area network protocol SPI Serial peripheral interface I2C Inter-Integrated
    Circuit communication protocol OSPF Open shortest path first communication protocol
    Appendix A Primary Control The primary control includes outer control loops (for
    voltage regulation), inner control loops (for current regulation), and droop control
    loops to share the power between inverters. Grid-forming (comprised of the inner
    and outer loops) includes a proportional-integral PI controller to regulate the
    output voltage and frequency of the MG. Furthermore, the frequency and voltage
    amplitude can be addressed by droop control as shown in the Equations (A1) and
    (A2), [1]: 𝜔= 𝜔 𝑟𝑒𝑓 − 𝑚 𝑝 (𝑃− 𝑃 𝑟𝑒𝑓 )= 𝜔 𝑟𝑒𝑓 − 𝑚 𝑝 Δ𝑃 (A1) 𝜐 𝑑𝑟𝑒𝑓 = 𝜐 𝑟𝑒𝑓 − 𝑛
    𝑞 (𝑄− 𝑄 𝑟𝑒𝑓 )= 𝜐 𝑟𝑒𝑓 − 𝑛 𝑞 Δ𝑄 (A2) where 𝜔 𝑟𝑒𝑓 and 𝜐 𝑟𝑒𝑓 are the nominal frequency
    and voltage, P and Q are the measured active and reactive power injection, and
    Δ𝑃 and Δ𝑄 are the corresponding power input errors for the droop controller. Coefficients
    𝑚 𝑝 and 𝑛 𝑞 regulate maximum deviations allowed in the MG [42,63]. Different lengths
    of transmission lines make the VSI output impedance different, causing unequal
    power sharing in droop control. Virtual impedance is an essential concept [64]
    to fix the output impedance value and decouple the control of active and reactive
    powers. The virtual impedance can help to keep the voltage within certain limits
    and is used for applications such as harmonic voltage compensation and improved
    stability. Refs. [37,65] show the implementation of virtual impedance. However,
    it is out of the scope of this research. The voltage across the capacitor 𝜐 𝑎𝑏𝑐
    can be calculated using Equation (A3): 𝜐 𝑎𝑏𝑐 (𝑡)= 𝜐 𝑟𝑒𝑓 𝑠𝑖𝑛( 𝜔 𝑟𝑒𝑓 𝑡)    
       𝜐 ∗ 𝛼 (𝑡) + 𝑗 𝜐 𝑟𝑒𝑓 𝑐𝑜𝑠( 𝜔 𝑟𝑒𝑓 𝑡)        𝜐 ∗ 𝛽 (𝑡) (A3) where 𝜐 𝑟𝑒𝑓
    and 𝜔 𝑟𝑒𝑓 are the voltage amplitude and angular frequency 𝜔 𝑟𝑒𝑓 =2𝜋 𝑓 𝑟𝑒𝑓 of the
    reference signal in time t. The voltage derivative reference is obtained from
    Equation (A4): 𝑑 𝜐 𝑎𝑏𝑐 (𝑡) 𝑑𝑡 = 𝜔 𝑟𝑒𝑓 𝜐 𝑟𝑒𝑓 𝑐𝑜𝑠( 𝜔 𝑟𝑒𝑓 𝑡)          𝜔
    𝑟𝑒𝑓 𝜐 ∗ 𝛽 (𝑡) + 𝑗 𝜔 𝑟𝑒𝑓 𝜐 𝑟𝑒𝑓 𝑠𝑖𝑛( 𝜔 𝑟𝑒𝑓 𝑡)          𝜔 𝑟𝑒𝑓 𝜐 ∗ 𝛼 (𝑡)
    (A4) To track 𝜐 𝑟𝑒𝑓 it is necessary to expand the last results to 𝛼,𝛽 frame. The
    predicted currents 𝑖 𝑓 and measured currents 𝑖 𝑜 are calculated as [64], to predict
    the capacitor derivative voltage as follows: 𝑑 𝜐 𝑎𝑏𝑐 (𝑡) 𝑑𝑡 = 𝑖 𝑓𝛼 (𝑡)− 𝑖 𝑜𝛼 (𝑡)
    𝐶 𝑓        𝑑 𝜐 𝑎𝑏𝑐 𝛼(𝑡) 𝑑𝑡 +𝑗 𝑖 𝑓𝛽 (𝑡)− 𝑖 𝑜𝛽 (𝑡) 𝐶 𝑓        𝑑 𝜐
    𝑎𝑏𝑐 𝛽(𝑡) 𝑑𝑡 (A5) From Equation (A5), it can be seen that the derivative path of
    the voltage can be tracked if the error between the first term and the second
    (A4) and (A5) is minimized. Appendix B Secondary Control Secondary control allows
    the regulation of voltage and frequency, which are not adjusted by the primary
    control loop. Implementing a local secondary controller at each distributed generator
    (DG) results in improved output quality and offers a significant advantage compared
    to centralized controller approaches [10]. Each DG measures its frequency at each
    sampling time, averaging the received information from other units and then broadcasting
    its average version ( 𝜔 𝐷𝐺 ) to the other DGs. Then, the consensus data is compared
    with the nominal frequency of the MG ( 𝜔 𝑟𝑒𝑓 ) and sent to the secondary controller
    of 𝐷 𝐺 𝑖 to restore the frequency using: 𝛿 𝜔 𝐷 𝐺 𝑘 =𝑘 𝑝 𝜔 ( 𝜔 𝑟𝑒𝑓 − 𝜔 𝐷 𝐺 𝑘 )+𝑘
    𝑖 𝜔 ∫( 𝜔 𝑟𝑒𝑓 − 𝜔 𝐷 𝐺 𝑘 )𝑑𝑡 (A6) 𝜔        𝐷 𝐺 𝑘 = ∑ 𝑁 𝑖=1 𝜔 𝐷 𝐺 𝑖 𝑁 (A7)
    where 𝑘 𝑝 𝜔 and 𝑘 𝑖 𝜔 are the controller parameters of PI and 𝛿 𝜔 𝐷 𝐺 𝑘 is the
    compensator signal for primary control (refer to Secondary Control block shown
    in Figure 6). 𝜔 𝐷 𝐺 𝑘 and 𝜔 𝑟𝑒𝑓 are the averages of frequency for all DGs and
    reference frequency of the MG, respectively. After calculating the average voltage
    received from the communication network ( 𝜐 𝑀𝐺 ), the local controller determines
    the error between this value and the voltage reference 𝜐 𝑟𝑒𝑓 as shown in Equation
    (A8). Finally, 𝛿 𝜐 𝐷 𝐺 𝑘 is sent to the primary control to compensate for the
    voltage deviation. The strategy is shown in the Power System layer of Figure 6.
    𝛿 𝜐 𝐷 𝐺 𝑘 =𝑘 𝑝 𝑣 ( 𝜐 𝑎𝑏𝑐 − 𝜐 𝐷 𝐺 𝑘 )+𝑘 𝑖 𝑣 ∫( 𝜐 𝑎𝑏𝑐 − 𝜐 𝐷 𝐺 𝑘 )𝑑𝑡 (A8) 𝜐 𝐷 𝐺 𝑘
    = ∑ 𝑁 𝑖=1 𝜐 𝐷 𝐺 𝑖 𝑁 (A9) where 𝑣 𝐷 𝐺 𝑘 refers to average voltages broadcasted
    from each DG at the sampling time. Small signal representations of the frequency
    and voltage for secondary control are detailed in [63]. References Tinajero, G.D.A.;
    Nasir, M.; Vasquez, J.C.; Guerrero, J.M. Comprehensive power flow modelling of
    hierarchically controlled AC/DC hybrid islanded microgrids. Int. J. Electr. Power
    Energy Syst. 2021, 127, 106629. [Google Scholar] [CrossRef] Han, Y.; Li, H.; Shen,
    P.; Coelho, E.A.A.; Guerrero, J.M. Review of active and reactive power sharing
    strategies in hierarchical controlled microgrids. IEEE Trans. Power Electron.
    2016, 32, 2427–2451. [Google Scholar] [CrossRef] [Green Version] Kulkarni, S.V.;
    Gaonkar, D.N. Improved droop control strategy for parallel connected power electronic
    converter based distributed generation sources in an Islanded Microgrid. Electr.
    Power Syst. Res. 2021, 201, 107531. [Google Scholar] [CrossRef] Pérez-Guzmán,
    R.E.; Salgueiro-Sicilia, Y.; Rivera, M. Communications in smart grids. In Proceedings
    of the 2017 CHILEAN Conference on Electrical, Electronics Engineering, Information
    and Communication Technologies (CHILECON), Pucon, Chile, 18–20 October 2017; pp.
    1–7. [Google Scholar] Simpson-Porco, J.W.; Shafiee, Q.; Dörfler, F.; Vasquez,
    J.C.; Guerrero, J.M.; Bullo, F. Secondary frequency and voltage control of islanded
    microgrids via distributed averaging. IEEE Trans. Ind. Electron. 2015, 62, 7025–7038.
    [Google Scholar] [CrossRef] Khayat, Y.; Shafiee, Q.; Heydari, R.; Naderi, M.;
    Dragičević, T.; Simpson-Porco, J.W.; Dörfler, F.; Fathi, M.; Blaabjerg, F.; Guerrero,
    J.M.; et al. On the secondary control architectures of AC microgrids: An overview.
    IEEE Trans. Power Electron. 2019, 35, 6482–6500. [Google Scholar] [CrossRef] Ferreira,
    D.; Silva, S.; Silva, W.; Brandao, D.; Bergna, G.; Tedeschi, E. Overview of Consensus
    Protocol and Its Application to Microgrid Control. Energies 2022, 15, 8536. [Google
    Scholar] [CrossRef] Shan, Y.; Pan, A.; Liu, H. A switching event-triggered resilient
    control scheme for primary and secondary levels in AC microgrids. ISA Trans. 2022,
    127, 216–228. [Google Scholar] [CrossRef] Shafiee, Q.; Dragičević, T.; Vasquez,
    J.C.; Guerrero, J.M. Hierarchical control for multiple DC-microgrids clusters.
    IEEE Trans. Energy Convers. 2014, 29, 922–933. [Google Scholar] [CrossRef] [Green
    Version] Zhou, Q.; Shahidehpour, M.; Paaso, A.; Bahramirad, S.; Alabdulwahab,
    A.; Abusorrah, A. Distributed control and communication strategies in networked
    microgrids. IEEE Commun. Surv. Tutor. 2020, 22, 2586–2633. [Google Scholar] [CrossRef]
    Yang, L.; Ng, B.; Seah, W.K.; Groves, L.; Singh, D. A survey on network forwarding
    in Software-Defined Networking. J. Netw. Comput. Appl. 2021, 176, 102947. [Google
    Scholar] [CrossRef] Ndiaye, M.; Hancke, G.P.; Abu-Mahfouz, A.M.; Zhang, H. Software-defined
    power grids: A survey on opportunities and taxonomy for microgrids. IEEE Access
    2021, 9, 98973–98991. [Google Scholar] [CrossRef] Ren, L.; Qin, Y.; Li, Y.; Zhang,
    P.; Wang, B.; Luh, P.B.; Han, S.; Orekan, T.; Gong, T. Enabling resilient distributed
    power sharing in networked microgrids through software defined networking. Appl.
    Energy 2018, 210, 1251–1265. [Google Scholar] [CrossRef] Ren, L.; Qin, Y.; Wang,
    B.; Zhang, P.; Luh, P.B.; Jin, R. Enabling Resilient Microgrid Through Programmable
    Network. IEEE Trans. Smart Grid 2017, 8, 2826–2836. [Google Scholar] [CrossRef]
    Danzi, P.; Angjelichinoski, M.; Stefanovic, C.; Dragicevic, T.; Popovski, P. Software-Defined
    Microgrid Control for Resilience Against Denial-of-Service Attacks. IEEE Trans.
    Smart Grid 2019, 10, 5258–5268. [Google Scholar] [CrossRef] Comer, D.; Rastegarnia,
    A. Toward disaggregating the SDN control plane. IEEE Commun. Mag. 2019, 57, 70–75.
    [Google Scholar] [CrossRef] Arzo, S.T.; Scotece, D.; Bassoli, R.; Barattini, D.;
    Granelli, F.; Foschini, L.; Fitzek, F.H. MSN: A Playground Framework for Design
    and Evaluation of MicroServices-Based sdN Controller. J. Netw. Syst. Manag. 2022,
    30, 1–31. [Google Scholar] [CrossRef] Siddiqui, S.; Hameed, S.; Shah, S.A.; Ahmad,
    I.; Aneiba, A.; Draheim, D.; Dustdar, S. Towards Software-Defined Networking-based
    IoT Frameworks: A Systematic Literature Review, Taxonomy, Open Challenges and
    Prospects. IEEE Access 2022, 10, 70850–70901. [Google Scholar] [CrossRef] Isong,
    B.; Molose, R.R.S.; Abu-Mahfouz, A.M.; Dladlu, N. Comprehensive review of SDN
    controller placement strategies. IEEE Access 2020, 8, 170070–170092. [Google Scholar]
    [CrossRef] Nippon Telegraph and Telephone Corporation (NTT). Ryu SDN Controller.
    Available online: https://ryu-sdn.org/ (accessed on 22 December 2022). OpenDaylight
    (ODL) Controller. Available online: https://www.opendaylight.org/ (accessed on
    3 June 2022). ONOS Project Community. Open Network Operating System (ONOS). Available
    online: https://opennetworking.org/onos/ (accessed on 18 January 2023). Markelov,
    A. OpenStack Networking. In Certified OpenStack Administrator Study Guide; Springer:
    Berlin/Heidelberg, Germany, 2022; pp. 77–121. [Google Scholar] Hölscher, A.; Asplund,
    M.; Boeira, F. Evaluation of an SDN-based Microservice Architecture. In Proceedings
    of the 2022 IEEE 8th International Conference on Network Softwarization (NetSoft),
    Milan, Italy, 27 June–1 July 2022; pp. 151–156. [Google Scholar] Open Network
    Foundation. Open Network Operating System (ONOS). Available online: https://docs.onosproject.org/
    (accessed on 22 October 2022). Ray, P.P.; Kumar, N. SDN/NFV architectures for
    edge-cloud oriented IoT: A systematic review. Comput. Commun. 2021, 169, 129–153.
    [Google Scholar] [CrossRef] Okwuibe, J.; Haavisto, J.; Harjula, E.; Ahmad, I.;
    Ylianttila, M. SDN enhanced resource orchestration of containerized edge applications
    for industrial IoT. IEEE Access 2020, 8, 229117–229131. [Google Scholar] [CrossRef]
    Nsafoa-Yeboah, K.; Tchao, E.T.; Yeboah-Akowuah, B.; Kommey, B.; Agbemenu, A.S.;
    Keelson, E.; Monirujjaman Khan, M. Software-Defined Networks for Optical Networks
    Using Flexible Orchestration: Advances, Challenges, and Opportunities. J. Comput.
    Netw. Commun. 2022, 2022, 5037702. [Google Scholar] [CrossRef] Marzal, S.; Salas,
    R.; González-Medina, R.; Garcerá, G.; Figueres, E. Current challenges and future
    trends in the field of communication architectures for microgrids. Renew. Sustain.
    Energy Rev. 2018, 82, 3610–3622. [Google Scholar] [CrossRef] [Green Version] Abbasi,
    M.; Abbasi, E.; Li, L.; Aguilera, R.P.; Lu, D.; Wang, F. Review on the Microgrid
    Concept, Structures, Components, Communication Systems, and Control Methods. Energies
    2023, 16, 484. [Google Scholar] [CrossRef] Lévy, L.N.; Bosom, J.; Guerard, G.;
    Amor, S.B.; Bui, M.; Tran, H. DevOps Model Appproach for Monitoring Smart Energy
    Systems. Energies 2022, 15, 5516. [Google Scholar] [CrossRef] Johansson, B.; Rågberger,
    M.; Nolte, T.; Papadopoulos, A.V. Kubernetes orchestration of high availability
    distributed control systems. In Proceedings of the 2022 IEEE International Conference
    on Industrial Technology (ICIT), Shanghai, China, 22–25 August 2022; pp. 1–8.
    [Google Scholar] Zhu, C.; Han, B.; Zhao, Y. A Comparative Study of Spark on the
    bare metal and Kubernetes. In Proceedings of the 2020 6th International Conference
    on Big Data and Information Analytics (BigDIA), Shenzhen, China, 4–6 December
    2020; pp. 117–124. [Google Scholar] Huedo, E.; Montero, R.S.; Moreno-Vozmediano,
    R.; Vázquez, C.; Holer, V.; Llorente, I.M. Opportunistic deployment of distributed
    edge clouds for latency-critical applications. J. Grid Comput. 2021, 19, 1–16.
    [Google Scholar] [CrossRef] Tonini, F.; Natalino, C.; Temesgene, D.A.; Ghebretensaé,
    Z.; Wosinska, L.; Monti, P. Benefits of Pod dimensioning with best-effort resources
    in bare metal cloud native deployments. IEEE Netw. Lett. 2023, 5, 41–45. [Google
    Scholar] [CrossRef] Klos, A.; Rosenbaum, M.; Schiffmann, W. Scalable and highly
    available multi-objective neural architecture search in bare metal kubernetes
    cluster. In Proceedings of the 2021 IEEE International Parallel and Distributed
    Processing Symposium Workshops (IPDPSW), Portland, OR, USA, 17–21 June 2021; pp.
    605–610. [Google Scholar] Guzmán, R.E.P.; Rivera, M.; Wheeler, P.W.; Mirzaeva,
    G.; Espinosa, E.E.; Rohten, J.A. Microgrid Power Sharing Framework for Software
    Defined Networking and Cybersecurity Analysis. IEEE Access 2022, 10, 111389–111405.
    [Google Scholar] [CrossRef] Yadav, G.; Joshi, D.; Gopinath, L.; Soni, M.K. Reliability
    and Availability Optimization of Smart Microgrid Using Specific Configuration
    of Renewable Resources and Considering Subcomponent Faults. Energies 2022, 15,
    5994. [Google Scholar] [CrossRef] Ahmad, S.; Mir, A.H. Scalability, Consistency,
    Reliability and Security in SDN Controllers: A Survey of Diverse SDN Controllers.
    J. Netw. Syst. Manag. 2021, 29, 9. [Google Scholar] [CrossRef] Mokhtar, H.; Di,
    X.; Zhou, Y.; Hassan, A.; Ma, Z.; Musa, S. Multiple-level threshold load balancing
    in distributed SDN controllers. Comput. Netw. 2021, 198, 108369. [Google Scholar]
    [CrossRef] Gupta, N.; Maashi, M.S.; Tanwar, S.; Badotra, S.; Aljebreen, M.; Bharany,
    S. A Comparative Study of Software Defined Networking Controllers Using Mininet.
    Electronics 2022, 11, 2715. [Google Scholar] [CrossRef] Guerrero, J.M.; Vasquez,
    J.C.; Matas, J.; De Vicu na, L.G.; Castilla, M. Hierarchical control of droop-controlled
    AC and DC microgrids—A general approach toward standardization. IEEE Trans. Ind.
    Electron. 2010, 58, 158–172. [Google Scholar] [CrossRef] Garces, L. Microgrid
    in Island Operation. 2022. Available online: https://www.plexim.com/support/application-examples/1259
    (accessed on 3 December 2022). Garces, L.J.; Liu, Y.; Bose, S. System and Method
    for Integrating Wind and Hydroelectric Generation and Pumped Hydro Energy Storage
    Systems. U.S. Patent 7,239,035, 3 July 2007. [Google Scholar] Zargar, R.H.M.;
    Yaghmaee, M.H. Energy exchange cooperative model in SDN-based interconnected multi-microgrids.
    Sustain. Energy Grids Netw. 2021, 27, 100491. [Google Scholar] [CrossRef] Khorsandroo,
    S.; Gallego Sanchez, A.; Tosun, A.S.; Arco, J.; Doriguzzi-Corin, R. Hybrid SDN
    evolution: A comprehensive survey of the state-of-the-art. Comput. Netw. 2021,
    192, 107981. [Google Scholar] [CrossRef] Biswas, R.; Wu, J. Traffic Engineering
    to Minimize the Number of Rules in SDN Datacenters. IEEE Trans. Netw. Sci. Eng.
    2021, 8, 1467–1477. [Google Scholar] [CrossRef] Miguel-Alonso, J. A Research Review
    of OpenFlow for Datacenter Networking. IEEE Access 2022, 11, 770–786. [Google
    Scholar] [CrossRef] CNC Foundations. Available online: https://k3s.io/ (accessed
    on 25 November 2022). Labs, R. Rancher: Enterprise Kubernetes Management. Available
    online: https://www.rancher.com/ (accessed on 25 November 2022). Wazirali, R.;
    Ahmad, R.; Alhiyari, S. SDN-OpenFlow Topology Discovery: An Overview of Performance
    Issues. Appl. Sci.-Basel 2021, 11, 6999. [Google Scholar] [CrossRef] Yan, L.;
    Sheikholeslami, M.; Gong, W.; Shahidehpour, M.; Li, Z. Architecture, Control,
    and Implementation of Networked Microgrids for Future Distribution Systems. J.
    Mod. Power Syst. Clean Energy 2022, 10, 286–299. [Google Scholar] [CrossRef] Siva
    Ananmalay, J.A.; Barton, D. Open Networking Foundation. 2022. Available online:
    https://opennetworking.org/ (accessed on 16 July 2022). Vachuska, T. ONOS Helm
    Charts. 2023. Available online: https://github.com/onosproject/onos-helm-charts
    (accessed on 3 December 2022). Vachuska, T.; Halterman, J. Atomix-Controller:
    Kubernetes Controller for Atomix 4. Available online: https://github.com/atomix/atomix-controller
    (accessed on 16 July 2022). Open Network Foundation. Deploying Onos-Config. Available
    online: https://docs.onosproject.org/onos-config/docs/deployment/ (accessed on
    7 December 2022). Pérez, R. Deploy HA Kubernetes Cluster for SDN Microgrid Hierarchical
    Control. 2023. Available online: https://github.com/ricardopg1987/kubernetes-rpi
    (accessed on 3 December 2022). CNC Foundations. Available online: https://docs.k3s.io/installation/ha-embedded
    (accessed on 25 September 2022). KubeSphere. Set up an HA Kubernetes Cluster Using
    Keepalived and HAproxy. 2023. Available online: https://kubesphere.io/docs/v3.3/installing-on-linux/high-availability-configurations/set-up-ha-cluster-using-keepalived-haproxy/
    (accessed on 25 September 2022). Zhang, Z. A comparison of low-speed communication
    modes. In Proceedings of the International Conference on Network Communication
    and Information Security (ICNCIS 2021), Qingdao, China, 19–21 August 2022; Volume
    12175, pp. 38–43. [Google Scholar] ZodiacFX Communication Device. Available online:
    https://www.cryptomuseum.com/radio/zodiac/ (accessed on 17 March 2023). Muhammad,
    A.; Saqib, M.; Song, W.C. Sensor Virtualization and Data Orchestration in Internet
    of Vehicles (IoV). In Proceedings of the 2021 IFIP/IEEE International Symposium
    on Integrated Network Management (IM), Bordeaux, France, 18–20 May 2021; pp. 998–1003.
    [Google Scholar] Heydari, R.; Dragicevic, T.; Blaabjerg, F. High-bandwidth secondary
    voltage and frequency control of vsc-based ac microgrid. IEEE Trans. Power Electron.
    2019, 34, 11320–11331. [Google Scholar] [CrossRef] [Green Version] Dragičević,
    T. Model predictive control of power converters for robust and fast operation
    of AC microgrids. IEEE Trans. Power Electron. 2017, 33, 6304–6317. [Google Scholar]
    [CrossRef] Villalón, A.; Rivera, M.; Salgueiro, Y.; Mu noz, J.; Dragičević, T.;
    Blaabjerg, F. Predictive control for microgrid applications: A review study. Energies
    2020, 13, 2454. [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements,
    opinions and data contained in all publications are solely those of the individual
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or
    the editor(s) disclaim responsibility for any injury to people or property resulting
    from any ideas, methods, instructions or products referred to in the content.  ©
    2023 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open
    access article distributed under the terms and conditions of the Creative Commons
    Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share
    and Cite MDPI and ACS Style Pérez, R.; Rivera, M.; Salgueiro, Y.; Baier, C.R.;
    Wheeler, P. Moving Microgrid Hierarchical Control to an SDN-Based Kubernetes Cluster:
    A Framework for Reliable and Flexible Energy Distribution. Sensors 2023, 23, 3395.
    https://doi.org/10.3390/s23073395 AMA Style Pérez R, Rivera M, Salgueiro Y, Baier
    CR, Wheeler P. Moving Microgrid Hierarchical Control to an SDN-Based Kubernetes
    Cluster: A Framework for Reliable and Flexible Energy Distribution. Sensors. 2023;
    23(7):3395. https://doi.org/10.3390/s23073395 Chicago/Turabian Style Pérez, Ricardo,
    Marco Rivera, Yamisleydi Salgueiro, Carlos R. Baier, and Patrick Wheeler. 2023.
    \"Moving Microgrid Hierarchical Control to an SDN-Based Kubernetes Cluster: A
    Framework for Reliable and Flexible Energy Distribution\" Sensors 23, no. 7: 3395.
    https://doi.org/10.3390/s23073395 Note that from the first issue of 2016, this
    journal uses article numbers instead of page numbers. See further details here.
    Article Metrics Citations Web of Science   3 Crossref   5 ads   2 Scopus   4 PubMed   2
    PMC   2 Google Scholar   [click to view] Article Access Statistics Article access
    statistics Article Views 10. Jan 20. Jan 30. Jan 9. Feb 19. Feb 29. Feb 10. Mar
    20. Mar 30. Mar 0 500 1000 1500 2000 2500 For more information on the journal
    statistics, click here. Multiple requests from the same IP address are counted
    as one view.   Sensors, EISSN 1424-8220, Published by MDPI RSS Content Alert Further
    Information Article Processing Charges Pay an Invoice Open Access Policy Contact
    MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians
    For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum
    MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series
    Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications
    and newsletters from MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel,
    Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Sensors
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Moving Microgrid Hierarchical Control to an SDN-Based Kubernetes Cluster:
    A Framework for Reliable and Flexible Energy Distribution'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Fernández Blanco D.
  - Le Mouël F.
  - Lin T.
  - Ponge J.
  citation_count: '2'
  description: With the increasing computing needs of the new systems and applications,
    cloud offloading has become a popular choice for constructors to keep the prices
    of their devices affordable. However, this solution only shifts the scaling problem
    from the end devices to the cloud, increasingly enhancing the capacities of cloud
    infrastructures. As a way to reinforce the cloud capabilities on the edge without
    needing to add extra computing resources, we propose PyCloudIoT, a collaborative
    energy-efficient Function-as-a-Service (FaaS) computing platform (pltf.) with
    low-to-medium availability targeting the execution of punctual stateless functions
    over the already deployed IoTs and gateways. As these resources are extremely
    dynamic, with intermittent availability, heterogeneity and faultiness, the addition
    of strong control mechanisms is key to efficient operation. In this paper, we
    discuss the PyCloudIoT Consensus Model (PCM), which enables the coordination and
    orchestration of resources dynamically and compensates for the faults of the IoT
    computing farm. Compared to SOTA, PCM shows promising results with a performance
    and energy consumption improvement of 20% and 66% and 37% and 65% respectively
    compared to the best configurations of Raft and Pirogue (4+1 quorum), achieving
    at the same time a slightly stronger fault tolerance level.
  doi: 10.1145/3555776.3577598
  full_citation: '>'
  full_text: '>

    "This website uses cookies We occasionally run membership recruitment campaigns
    on social media channels and use cookies to track post-clicks. We also share information
    about your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Use the check boxes below to
    choose the types of cookies you consent to have stored on your device. Use necessary
    cookies only Allow selected cookies Allow all cookies Necessary Preferences Statistics
    Marketing Show details       skip to main content University of Nebraska Lincoln
    Browse About Sign in Register Journals Magazines Proceedings Books SIGs Conferences
    People Search ACM Digital Library Advanced Search Conference Proceedings Upcoming
    Events Authors Affiliations Award Winners HomeConferencesSACProceedingsSAC ''23An
    Energy-efficient FaaS Edge Computing platform over IoT Nodes: Focus on Consensus
    Algorithm RESEARCH-ARTICLE SHARE ON An Energy-efficient FaaS Edge Computing platform
    over IoT Nodes: Focus on Consensus Algorithm Authors: David Fernandez Blanco ,
    Frederic Le Mouel , Trista Lin , + 1 Authors Info & Claims SAC ''23: Proceedings
    of the 38th ACM/SIGAPP Symposium on Applied ComputingMarch 2023Pages 661–670https://doi.org/10.1145/3555776.3577598
    Published:07 June 2023Publication History 0 citation 47 Downloads eReaderPDF SAC
    ''23: Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing An Energy-efficient
    FaaS Edge Computing platform over IoT Nodes: Focus on Consensus Algorithm Pages
    661–670 Previous Next ABSTRACT References Index Terms Recommendations Comments
    ABSTRACT With the increasing computing needs of the new systems and applications,
    cloud offloading has become a popular choice for constructors to keep the prices
    of their devices affordable. However, this solution only shifts the scaling problem
    from the end devices to the cloud, increasingly enhancing the capacities of cloud
    infrastructures. As a way to reinforce the cloud capabilities on the edge without
    needing to add extra computing resources, we propose PyCloudIoT, a collaborative
    energy-efficient Function-as-a-Service (FaaS) computing platform (pltf.) with
    low-to-medium availability targeting the execution of punctual stateless functions
    over the already deployed IoTs and gateways. As these resources are extremely
    dynamic, with intermittent availability, heterogeneity and faultiness, the addition
    of strong control mechanisms is key to efficient operation. In this paper, we
    discuss the PyCloudIoT Consensus Model (PCM), which enables the coordination and
    orchestration of resources dynamically and compensates for the faults of the IoT
    computing farm. Compared to SOTA, PCM shows promising results with a performance
    and energy consumption improvement of 20% and 66% and 37% and 65% respectively
    compared to the best configurations of Raft and Pirogue (4+1 quorum), achieving
    at the same time a slightly stronger fault tolerance level. References A.Yousefpour.
    2019. All one needs to know about fog computing and related edge computing paradigms:
    A complete survey. In J. of Syst. Architecture. Elsevier. D.Ongaro. 2014. In search
    of an understandable consensus algorithm. In 2014 USENIX Annual Technical Conference.
    H.Atlam. 2018. Fog computing and the internet of things: A review. In Big Data
    and Cognitive Computing Journal. Multidisciplinary Digital Publishing Institute.
    Show All References Index Terms An Energy-efficient FaaS Edge Computing platform
    over IoT Nodes: Focus on Consensus Algorithm Computer systems organization Architectures
    Distributed architectures Cloud computing Dependable and fault-tolerant systems
    and networks Hardware Power and energy Impact on the environment Recommendations
    FLY: A Domain-Specific Language for Scientific Computing on FaaS Euro-Par 2019:
    Parallel Processing Workshops Abstract Cloud Computing is widely recognized as
    distributed computing paradigm for the next generation of dynamically scalable
    applications. Recently a novel service model, called Function-as-a-Service (FaaS),
    has been proposed, that enables users to ... Read More Right-Provisioned IoT Edge
    Computing: An Overview GLSVLSI ''19: Proceedings of the 2019 on Great Lakes Symposium
    on VLSI Edge computing on the Internet of Things (IoT) is an increasingly popular
    paradigm in which computation is moved closer to the data source (i.e., edge devices).
    Edge computing mitigates the overheads of cloud-based computing arising from increased
    ... Read More Efficient and dynamic scaling of fog nodes for IoT devices It is
    predicted by the year 2020, more than 50 billion devices will be connected to
    the Internet. Traditionally, cloud computing has been used as the preferred platform
    for aggregating, processing, and analyzing IoT traffic. However, the cloud may
    not ... Read More Comments 21 References View Table Of Contents Footer Categories
    Journals Magazines Books Proceedings SIGs Conferences Collections People About
    About ACM Digital Library ACM Digital Library Board Subscription Information Author
    Guidelines Using ACM Digital Library All Holdings within the ACM Digital Library
    ACM Computing Classification System Digital Library Accessibility Join Join ACM
    Join SIGs Subscribe to Publications Institutions and Libraries Connect Contact
    Facebook Twitter Linkedin Feedback Bug Report The ACM Digital Library is published
    by the Association for Computing Machinery. Copyright © 2024 ACM, Inc. Terms of
    Usage Privacy Policy Code of Ethics Feedback"'
  inline_citation: '>'
  journal: Proceedings of the ACM Symposium on Applied Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'An Energy-efficient FaaS Edge Computing platform over IoT Nodes: Focus on
    Consensus Algorithm'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sarabia P.
  - Araujo A.
  - Sarabia L.A.
  - Ortiz M.d.l.C.
  citation_count: '0'
  description: Surface electromyography (sEMG) plays a crucial role in several applications,
    such as for prosthetic controls, human–machine interfaces (HMI), rehabilitation,
    and disease diagnosis. These applications are usually occurring in real-time,
    so the classifier tends to run on a wearable device. This edge processing paradigm
    imposes strict requirements on the complexity classifier. To date, research on
    hand gesture recognition (GR) based on sEMG uses discriminant classifiers, such
    as support vector machines and neural networks. These classifiers can achieve
    good precision; they cannot detect when an error in classification has happened.
    This paper proposes a novel hand gesture multiclass model based on partial least
    square (PLS) class modelling that uses an encoding matrix called error correcting
    output codes (ECOC). A dataset of eight different gestures was classified using
    this method where all errors were detected, proving the feasibility of PLS-ECOC
    as a fault-tolerant classifier. Considering the PLS-ECOC model as a classifier,
    its accuracy, precision, and F1 are 87.5, 91.87, and 86.34%, respectively, similar
    to those obtained by other authors. The strength of our work lies in the extra
    information provided by the PLS-ECOC that allows the application to be fault tolerant
    while keeping a small-size model and low complexity, making it suitable for embedded
    real-time classification.
  doi: 10.3390/a16030149
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Algorithms All Article Types Advanced   Journals
    Algorithms Volume 16 Issue 3 10.3390/a16030149 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Academic Editor Maryam Ravan
    Subscribe SciFeed Recommended Articles Related Info Link More by Authors Links
    Article Views 1638 Table of Contents Abstract Introduction State-of-the-Art Approaches
    Materials and Methods Results Discussion Conclusions Author Contributions Funding
    Data Availability Statement Conflicts of Interest References share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Electromyography Gesture Model Classifier for Fault-Tolerant-Embedded Devices
    by Means of Partial Least Square Class Modelling Error Correcting Output Codes
    (PLS-ECOC) by Pablo Sarabia 1, Alvaro Araujo 1, Luis Antonio Sarabia 2 and María
    de la Cruz Ortiz 3,* 1 B105 Electronic Systems Lab, Universidad Politécnica de
    Madrid, 28040 Madrid, Spain 2 Department Mathematics and Computation, Faculty
    Sciences, University of Burgos, Pza. Misael Bañuelos s/n, 09001 Burgos, Spain
    3 Department Chemistry, Faculty Sciences, University of Burgos, Pza. Misael Bañuelos
    s/n, 09001 Burgos, Spain * Author to whom correspondence should be addressed.
    Algorithms 2023, 16(3), 149; https://doi.org/10.3390/a16030149 Submission received:
    30 January 2023 / Revised: 2 March 2023 / Accepted: 5 March 2023 / Published:
    7 March 2023 (This article belongs to the Special Issue Machine Learning in Medical
    Signal and Image Processing) Download keyboard_arrow_down     Browse Figures Versions
    Notes Abstract Surface electromyography (sEMG) plays a crucial role in several
    applications, such as for prosthetic controls, human–machine interfaces (HMI),
    rehabilitation, and disease diagnosis. These applications are usually occurring
    in real-time, so the classifier tends to run on a wearable device. This edge processing
    paradigm imposes strict requirements on the complexity classifier. To date, research
    on hand gesture recognition (GR) based on sEMG uses discriminant classifiers,
    such as support vector machines and neural networks. These classifiers can achieve
    good precision; they cannot detect when an error in classification has happened.
    This paper proposes a novel hand gesture multiclass model based on partial least
    square (PLS) class modelling that uses an encoding matrix called error correcting
    output codes (ECOC). A dataset of eight different gestures was classified using
    this method where all errors were detected, proving the feasibility of PLS-ECOC
    as a fault-tolerant classifier. Considering the PLS-ECOC model as a classifier,
    its accuracy, precision, and F1 are 87.5, 91.87, and 86.34%, respectively, similar
    to those obtained by other authors. The strength of our work lies in the extra
    information provided by the PLS-ECOC that allows the application to be fault tolerant
    while keeping a small-size model and low complexity, making it suitable for embedded
    real-time classification. Keywords: electromyography; fault tolerant; gesture
    recognition; wearable; edge computing; sensitivity; specificity; class modelling;
    partial least squares; error correcting output; diagonal modified confusion entropy
    1. Introduction The growing interest in the application of gesture recognition
    (GR) using surface electromyography (sEMG) signals has been evident in recent
    years. A search on the SCOPUS database using the keywords (“Electromyography”
    or “EMG”) and “Gesture Recognition” yielded 884 references dating back to 1993.
    This is a highly interdisciplinary field, with the majority of papers falling
    under the categories of “Computer Science” and “Engineering” with 637 and 557
    references, respectively, with 351 being common to both. Other areas with a significant
    number of references include “Mathematics,” “Physics and Astronomy,” and “Medicine”
    with 169, 144, and 124 references, respectively. Of those, none, 18, and 9 do
    not belong to either “Computer Science” or “Engineering”. There are numerous medical
    applications for this technology for both people with disabilities and healthy
    individuals, as discussed in [1]. EMG signals have also been used in the design
    [2] and control of prostheses [3]. Applications in robot therapy were presented
    in [4], and [5] describes an elbow rehabilitation system using a remote-controlled
    intelligent robot. The use of EMG signals for disease diagnosis is an area with
    many applications, including the progression of primary lateral sclerosis [6],
    dysphagia [7], patellofemoral pain syndrome [8], neuromuscular disorders [9],
    and the evaluation of mobility loss in patients with musculoskeletal disorders
    [10]. Some of the uses of sEMG-based GR mentioned before can be grouped in human–machine
    interface (HMI). sEMG HMI allows the user to control or command other systems,
    such as wheelchairs, prosthetics, drones, robot arms, or exoskeletons. This use
    has been extensively studied by specific reviews on the topic. These reviews showed
    the necessity of low-complexity classifiers [11,12,13,14], which translates into
    reduced preprocessing, lower latency, and lower power consumption. These characteristics
    are critical when talking about wearable devices, as shown in review [15]. One
    of the great challenges facing GR systems is their use in wearable devices. A
    wearable device is a type of embedded system that is designed to be worn on the
    body, such as smartwatches, fitness trackers, and clothing embedded with sensors.
    These devices have several advantages, including convenience, portability, and
    personalization, but also have some limitations, such as limited battery life,
    privacy, security concerns, and limited computational power. The processing that
    takes place at the end devices is what is known as edge computing. Edge computing
    has several advantages, such as privacy, given that no data is uploaded to a third-party
    cloud server; lower and reliable latency, as data are not required to be uploaded,
    and the latency does not depend on the network availability; and lower cost or
    simplicity because the system does not require a modem or connection to a network.
    On the other hand, the resources are limited by the embedded systems, and the
    use of batteries limits the use of more complex algorithms, such as complex neural
    networks (NNs) [16]. Therefore, this leaves room for simpler lineal models, such
    as the one presented in this work. To ensure continuity of operation, a fault-tolerant
    system can be implemented, which allows the system to continue functioning correctly
    in the presence of hardware or software faults by detecting, diagnosing, and recovering
    from errors in real-time. This is achieved by using techniques such as redundancy,
    error detection and correction, diversity, and isolation. They can be combined
    to accomplish different levels of fault tolerance depending on the specific requirements
    of the system and application. As wearables are becoming more and more important
    in different fields, their fault-tolerance feature is increasingly important in
    many critical systems, such as aerospace, military, medical, and industrial systems,
    where a failure could have serious consequences. In these applications, the lack
    of robustness could lead to material or even personal damage. The contribution
    of the proposed method is its capability of detecting errors by using a gesture
    multiclass model based on partial least square (PLS) class modelling with the
    use of an encoding matrix called error correcting output codes (ECOC) while keeping
    a small size model and low complexity, therefore, maintaining its suitability
    for real-time classification on a wearable device. To achieve fault tolerance,
    error detection was implemented. An error is defined as a deviation from the expected
    or correct behaviour of the system, in this case, the difference between the gesture
    made and the obtained label. In the context of HMI, an error can lead to a wheelchair
    driving into an obstacle because of a gesture misclassification or a drone crashing
    because the GR classifier confused left withright. In the proposed class-modelling
    approach, if an error is detected, it can take action, such as asking the user
    to reposition the electrodes or to repeat the gesture, preventing the system from
    executing any incorrect actions based on the sEMG input signal. Latency and robustness
    are critical in real-time classification [17]. This work also addresses the unresolved
    issue of automating the obtention of useful features because there are dozens
    of them that are usually chosen by hand during the design of the classifier and
    have a great impact on the outcome of the classifier. This paper is organized
    as follows: Following this introduction, state-of-the-art approaches are discussed
    in Section 2, and the elements of the problem are described in Section 3. This
    includes the notation and experimental procedure to obtain the EMG signals (Section
    3.1) and a description of the structural elements of the PLS-ECOC procedure (Section
    3.2, Section 3.3, Section 3.4, Section 3.5, Section 3.6, Section 3.7 and Section
    3.8). The results are shown in Section 4, and a discussion of them in Section
    5 shows the advantages of modelling the gesture and the accuracy of gesture classification,
    comparing it with those found in several papers. This paper ends with conclusions
    and several references. 2. State-of-the-Art Approaches In gesture recognition
    using sEMG signals, computational techniques from machine learning (ML) are employed
    to assign a recorded signal to a gesture class. Specifically, a matrix 𝐗 of sEMG
    signals with N rows corresponds to N gestures grouped into K classes ( 𝐶 1 , …
    ,  𝐶 𝐾 ). There are two general approaches to addressing this issue: discriminant
    techniques and class modelling. The first one is a purely discriminative approach.
    In the training phase, the ML technique constructs a mathematical model and decision
    rule to assign each gesture class a subset ℘ 𝑘 , 𝑘=1,…,𝐾 of the signal space in
    such a way that they are disjoint, and their union is the total. As a result,
    in the prediction phase, a new signal will necessarily be assigned to one of the
    classes and only one. These types of techniques are commonly known in pattern
    recognition as discriminant or classification techniques. The performance of a
    discriminant model is measured with the expected classification accuracy, namely,
    the percentage of correct decisions in prediction or variations of this metric.
    Examples of common purely discriminant methods include linear or quadratic discriminant
    analysis (LDA, QDA [18]), regularized discriminant analysis (RDA) [19], partial
    least squares discriminant analysis (PLS-DA) [20], classification and regression
    trees (CART) [21], or support vector machines (SVM) originally developed for two-class
    classification [22], generalized to the multiclass situation [23], and the case
    of unlabelled data [24]. Neural networks (NNs) have also been widely developed
    and improved since their inception [25], with the backpropagation algorithm [26]
    allowing for computational feasibility. NNs offer flexibility, allowing them to
    adapt their structure to improve classification accuracy. For example, the convolutional
    neural networks (CNNs) proposed in reference [27] have been applied to handwritten
    digit classification. The second approach for GR using EMG signals is known as
    the class-modelling technique. It also builds K subsets, named K-class models,
    which are represented by ℘ 𝑘 , 𝑘=1,…,𝐾 , but they are not necessarily disjoints,
    and their union may not cover the entire signal space. As a result, an object(in
    this case an sEMG signal) can belong to one or more classes (gestures) or none
    at all. To evaluate the performance of the class-modelling technique, the sensitivity
    and specificity of each class model are measured. In this context, the sensitivity
    of a class model refers to its ability to correctly identify its corresponding
    gesture (usually measured as the rate of correctly classified objects within the
    class model, ℘ 𝐾 ), while its specificity refers to its ability to correctly reject
    objects that do not belong to the class model (measured as the rate of correctly
    classified objects outside the class model). Among the class-modelling methods,
    some stand out: Soft independent models of class analogy (SIMCA) [28] have been
    widely used for product authentication, quality assurance, fraud detection, unequal
    class models or unequal dispersed classes (UNEQ) [29], or an adaptation of SVM
    [30]. Two-class modelling using PLS can be seen in reference [31] and for K-classes
    in [32]. The advantage over the usual class discriminant classifiers used in sEMG
    gesture recognition problems and the proposed class modelling approach is that
    it enables the system to be fault tolerant. This is because the class modelling
    approach does not limit the answer to a unique label. Another advantage is the
    extra information provided by the sensitivity and specificity of each class model
    that allows the engineer to adapt the design by including extra channels to detect
    the difference between the confused gestures or to remove certain gestures if
    they are being problematic prior to the deployment of the system. This possibility
    in discriminant classifiers would not be available, and the only possible action
    would be to add more data to the dataset with the objective of improving its accuracy.
    In the field of gesture recognition using EMG signals, class modelling has never
    been used. In the literature reviewed, only gesture classification has been performed.
    Publications powered by pattern recognition methods have been reviewed in references
    [33,34,35]. The most commonly used are SVM [36,37] and NNs [38]. After the signal
    acquisition, among all the modules that compose a gesture recognition system,
    perhaps the most important and critical ones are feature extraction and the classification
    algorithm. In this regard, the algorithms and results of these references are
    summarized below. The review by Nazmi et al. [35] of works from 1999 to 2013 shows
    that the classifiers used are linear discriminant analysis, SVM, adaptive network-based
    fuzzy inference system, NNs, and fuzzy logic. The range of accuracy is [73.00,
    98.87] with a mean of 90.60% and a standard deviation of 7.06%. Moreover, Jaramillo-Yáñez
    et al.’s review [33] from 2013 to 2019 showed that the classifiers used are support
    vector machines, feedforward neural networks, linear discriminant analysis, convolutional
    neural networks with several variants, k-nearest neighbours coupled with several
    other procedures binary tree-support vector machines, vector autoregressive hierarchical
    hidden Markov models, Gaussian mixture models and hidden Markov models, quadratic
    discriminant analysis, fuzzy logic, recurrent neural networks, generalized regression
    neural networks, and one vs. one classifier. Among them, the most commonly used
    machine learning algorithms are support vector machines, feedforward neural networks,
    and linear discriminant analysis. The accuracy of the reviewed publications ranges
    from 71.00% to 99.78% with a mean of 91.57% and a standard deviation of 6.35%.
    Similar accuracy is reported (63.74% to 99.23% with a mean of 89% and a standard
    deviation of 10.11%) by Dhumal and Sharma’s [34] revision between 2017 and 2021.
    A more recent comparison evaluated five machine learning techniques in classifying
    daily gestures [10]. It reported an accuracy ranging from 47.7% to 83.6% with
    a mean value of 66.54% and a standard deviation of 14.95%. This revision included
    SVN, a random forest, a decision tree-based algorithm, a convolutional neural
    network, and a recurrent neural network. The analysis of the abovementioned reviews
    also shows that in order to capture and describe the complexity and variability
    of sEMG signals, it is considered necessary to handle “deep learning” methods.
    This trend can be observed in related fields such as gesture recognition based
    on electroencephalogram (EEG) signals. In Zhang et al.’s revision [38], a taxonomy
    of deep learning models was made according to their functionality in different
    scenarios of brain activity. After a very extensive description of applications,
    the authors indicated that it is still very challenging to produce classification
    results in real-time. In [39], GR performance was improved by using a graph sequence
    neural network applied to an HMI problem. In [40], six features were obtained
    using the correlation and canonical correlation analysis between filtered signals;
    after that, a selection was conducted. However, these advances have not yet been
    translated into their equivalent sEMG signals. Phinyomark and Scheme [41] made
    a very systematic analysis of the use of deep learning for classifying sEMG signals
    and define two categories of computational procedures: Feature engineering. Under
    this heading, the methods to find the best combination of features in a specific
    problem are grouped. They are parallel computing procedures, either native or
    classics parallelized. Then, conventional classifiers, such as support vector
    machines, linear discriminant analysis, k-nearest neighbours, random forests,
    multilayer perceptron neural networks, etc., are applied. Feature learning. This
    heading includes methods with a special emphasis on “deep learning”. In general,
    though, deep learning models can be roughly grouped into three main categories:
    unsupervised pretrained networks, convolutional neural networks, and recurrent
    neural networks. These three categories of models have already been used to analyse
    sEMG signals, as shown in the reviews of the preceding paragraphs. The authors
    concluded their extensive review by saying: “A key challenge and impediment to
    the clinical deployment of deep learning methods is their high computational cost
    (i.e., long training times and high computational complexity). Because of the
    stringent power and size restrictions of prosthetic components, most devices are
    built using embedded systems”. On the other hand, the use of an SVM classifier
    for gestures in EEG and sEMG signals is still interesting, as shown by Quitadamo
    et al.’s review [14]. Up to nine different variants of SVM classifiers were reviewed
    in several applications. The review showed that the accuracy of the 76 publications
    revised ranged from 18.83% to 100% with a mean of 80.8% and a standard deviation
    of 13.32%. Taking into account only the sEMG GR publications, the accuracy increases
    to a range from 73% to 100% with a mean of 91.1% and a standard deviation of 6.38%.
    The review concludes that “SVMs result to be among the most versatile classifiers
    for pattern recognition… and furthermore resulted to be particularly suitable
    for online implementations”. Therefore, along with the trend to use deep learning,
    there is still interest in exploring the feasibility of class modelling with PLS-ECOC
    that simultaneously reduces the complexity associated with feature engineering
    and that associated with the deep learning approach. In addition, PLS-ECOC distinguishes
    between failure and error. However, in all cases, the methods used are independent
    of the structure of 𝐗 , the sEMG signals, and the encoding used to describe the
    classes, which is denoted by 𝐘 . In engineering, particularly in the field of
    signal transmission, redundancy has been used to distinguish, at the receiver,
    two signals despite having lost information during transmission. This idea has
    been used in this work to encode the gesture classes and construct the matrix
    𝐘 by assigning each class a code (a vector of ones and minus ones) using an ECOC
    matrix. Little attention has been paid to the codification of classes in GR sEMG
    publications. Only ref. [14] presents the effect of two codifications (one vs.
    one and one vs. all) which have been proven to be inefficient [32]. The proposal
    in this work is to use PLS to find the linear relationship between the signals
    and the encoded classes, explaining the maximum variance in 𝐗 , in 𝐘 , and the
    maximum correlation between them. PLS constructs nvl latent variables, linear
    combinations of 𝐗 variables and 𝐘 variables under the condition of having the
    maximum correlation. In this way, PLS applies a reduction of the signal space
    but links to the classes of the gestures to be distinguished. This characteristic
    makes PLS an advance in the treatment of EMG signals. Since ancient times [42],
    the need to reduce the dimension of the signal space has been recognized, in most
    cases, as a second stage after obtaining features [43]. In [44], using several
    classifiers, a comparison has been made between feature selection and dimensionality
    reduction, including PCA. However, in general, PCA is performed independently
    of the classifier and, therefore, of the classes to be modelled. In a review made,
    only one PLS application with sEMG signals has been found. It is devoted to model
    concurrent EEG and EMG data collected in a Parkinson’s disease study [45], but
    it was not a classification of gestures. The method, PLS-ECOC [32], provides the
    sensitivity and specificity matrix of the constructed model. This matrix is evaluated
    using diagonal modified confusion entropy (DMCEN) [46]. According to the classification
    of methods to preprocess sEMG signals, PLS-ECOC can be considered an embedded
    one because the construction of latent variables and the model of classes are
    built simultaneously in the same model [47]. It is also noteworthy that PLS-ECOC
    is robust to sEMG signals that differ from those used to train the class model
    since, in addition to the equations for class prediction, the model provides a
    closed enclosure in the sEMG signal space, the PLS-box, so that a signal outside
    of it will be declared an outlier. 3. Materials and Methods This section presents
    the methodology followed to develop the hand gesture model-ling system (HGM) based
    on surface electromyography signals (sEMG) and partial least squares multiresponse.
    It has been divided into eight subsections, each of them providing enough information
    to understand the method and the results section. A complete schematic of the
    operations followed to achieve HGM can be seen in Figure 1. Figure 1. Schematic
    of the hand gesture modelling method based on sEMG and PLS-ECOC. Steps 3.1 to
    3.8 are addressed by the subsections of this section. The arrows box indicates
    iteration of the included steps in it. “c” stands for the number of binary learners
    in class codification and changes in each iteration. See the text above for an
    explanation of the other details. This figure is meant to be a guide to the reader
    showing the processing and flow of data, and it follows the same notation and
    subsection names and numbering used in the rest of this paper. In it, each rectangle
    includes the numerical matrix resulting from applying the operation indicated
    in the access arrow and describes in detail in the subsection that bears the name
    and numbering indicated. The second subscript of the matrices indicates the dimension
    of each of them. For example, the vector of the class labels, L, and the signals
    sEMG, X, are divided according to the procedure described in Section 3.2 into
    the data to be used for training, LTR and XTR, and for prediction, LTS and XTS,
    respectively. Applying the coding procedure explained in Section 3.3 returns YTR,
    the matrix of the encoded response, which, together with XTR, allows the coefficients
    of the PLS regression, 𝐁 ̂ , to be obtained. With this model, the calculated code
    matrix, 𝐘 ̂ 𝑇𝑅 , is obtained, which is decoded into 𝐋 ̂ 𝑇𝑅 to compare it with
    the vector of the labels LTR. Afterwards, the matrix of sensitivities and specificities,
    STR, is obtained in training. This process is iterated according to the procedure
    described in Section 3.7 to obtain the optimal PLS model, 𝐁 ̂ 𝑂𝑃 . This model
    is applied to the test set, XTS, and the vector of labels in prediction, 𝐋 ̂ 𝑇𝑆
    , is obtained. More details on the method can be consulted in [32]. The K-class
    model consists of K models, one for each class modelled. Class models are built
    and validated as a whole. Consequently, if a new class was added, the model would
    need to be built and validated again, obtaining a new (K + 1)-class model. I sEMG
    signals were recorded into K class, C1, C2, …, CK, each of them associated with
    each gesture type. The number of signals recorded for the j-th class is denoted
    by Ij, so 𝐼= ∑ 𝐾 𝑗=1 𝐼 𝑗 . The result of applying the K-class model is summarized
    in the confusion matrix N 𝐍= ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 𝑛 11 𝑛 21
    ⋮ 𝑛 𝑗1 ⋮ 𝑛 𝐾1 𝑛 12 𝑛 22 ⋮ 𝑛 𝑗2 ⋮ 𝑛 𝐾2 ⋯ ⋯ ⋱ ⋯ ⋱ ⋯ 𝑛 1𝑚 𝑛 2𝑚 ⋮ 𝑛 𝑗𝑚 ⋮ 𝑛 𝐾𝑚 ⋯ ⋯
    ⋱ ⋯ ⋱ ⋯ 𝑛 1𝐾 𝑛 2𝐾 ⋮ 𝑛 𝑗𝐾 ⋮ 𝑛 𝐾𝐾 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ , (1)
    where usual gesture classification algorithms verify the following: njm is the
    number of gestures belonging to class Cj which are inside the class model built
    for class Cm. 𝐼 𝑗 = ∑ 𝐾 𝑚=1 𝑛 𝑗𝑚 , which means that the sum of the j-row elements
    of matrix N, Equation (1), is equal to the number of elements in the j-th class.
    𝐼= ∑ 𝐾 𝑗=1,𝑚=1 𝑛 𝑗𝑚 , which means that the sum of all the elements of matrix N,
    Equation (1), is equal to the total I signals recorded. However, when using class
    modelling, each signal can be assigned to no class or more than one class. So,
    one or both of the above equations might not be met. This distinction between
    usual classifiers and the class modelling presented here will prove to be critical.
    The relative frequency matrix, F, Equation (2), can be obtained from Equation
    (1) as follows: 𝐅=( 𝑓 𝑗𝑚 )=( 𝑛 𝑗𝑚 / 𝐼 𝑗 ), 𝑗=1, …, 𝐾, 𝑚=1,…,𝐾. (2) As a consequence
    of the matrix N structure and the use of the class modelling algorithm, each of
    the rows of the F matrix, Equation (2), may not add 1, unlike a classification
    method. F matrix has a relevant statistical meaning. The assignation of an sEMG
    signal to a class is applying a hypothesis test. Given K classes, this decision
    is a family/set of K(K − 1) hypothesis tests such as those in Equation (3). It
    is composed of K − 1 tests for each of the K classes. The null Hypothesis H0 is
    the same for each j = 1, …, K, but H1, the alternative hypothesis, is different
    for each one. H0: EMG signal belongs to gesture Cj.                                    H1:
    EMG signal belongs to gesture Cm, m = 1, …, K, m ≠ j. (3) TEST matrix, Equation
    (4), symbolically summarizes these tests. Each diagonal term contains the null
    hypothesis (H0), and the rest of the terms of the matrix contain the corresponding
    alternative hypothesis (H1). 𝐓𝐄𝐒𝐓= ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ H 0
    : 𝐶 1 H 1 : 𝐶 2 ⋮ H 1 : 𝐶 𝑗 ⋮ H 1 : 𝐶 𝐾 H 1 : 𝐶 1 H 0 : 𝐶 2 ⋮ H 1 : 𝐶 𝑗 ⋮ H 1
    : 𝐶 𝐾 ⋯ ⋯ ⋱ ⋯ ⋯ H 1 : 𝐶 1 H 1 : 𝐶 2 ⋮ H 0 : 𝐶 𝑗 ⋮ H 1 : 𝐶 𝐾 ⋯ ⋯ ⋯ ⋱ ⋯ H 1 : 𝐶
    1 H 1 : 𝐶 2 ⋮ H 1 : 𝐶 𝑗 ⋮ H 0 : 𝐶 𝐾 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ .
    (4) Hypothesis test theory defines 𝛼 𝑗 as the significance level and 1 − βmj as
    the power of each test. These parameters are related to sensitivity and specificity
    of the test. Note that notation on sensitivity and power of the test vary and
    can be confusing, see references [35,36,48]; that is why they are defined below.
    The column j-th of TEST, Equation (4), corresponds to the K − 1 hypothesis tests
    of 𝐶 𝑗 class. Using the F matrix, it is possible to calculate the significance
    level and power of the test and to define the sensitivity and specificity: Sensitivity,
    (1− 𝛼 𝑗 ) = 𝑓 𝑗𝑗 of F, Equation (2). This means the probability of correctly assigning
    an sEMG signal of class 𝐶 𝑗 to the class model of 𝐶 𝑗   annotated as 𝑠 𝑗𝑗 in Equation
    (5). Specificity, (1− 𝛽 𝑚𝑗 )=(1− 𝑓 𝑚𝑗 ) 𝑤ℎ𝑒𝑛 𝑚≠𝑗 corresponds to the rest of the
    elements of the columns of F, Equation (2). This means the probability of correctly
    not assigning an sEMG signal of class 𝐶 𝑚 to the class model of 𝐶 𝑗   annotated
    as 𝑠 𝑗𝑚 in Equation (5). To sum up, sensitivity is the proportion of gestures
    that are assigned correctly to the modelled class, and specificity is the proportion
    of gestures correctly rejected from the modelled class. With this notation, matrix
    F is transformed into matrix S of sensitivities and specificities, Equation (5),
    that characterizes the performance of the K-class model. 𝐒= ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜
    ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 𝑠 11 𝑠 21 ⋮ 𝑠 𝑗1 ⋮ 𝑠 𝐾1 𝑠 12 𝑠 22 ⋮ 𝑠 𝑗2 ⋮ 𝑠 𝐾2 ⋯ ⋯ ⋯
    ⋯ 𝑠 1𝑗 𝑠 2𝑗 ⋮ 𝑠 𝑗𝑗 ⋮ 𝑠 𝐾𝑗 ⋯ ⋯ ⋯ ⋯ 𝑠 1𝐾 𝑠 2𝐾 ⋮ 𝑠 𝑗𝐾 ⋮ 𝑠 𝐾𝐾 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟
    ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ = ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ 𝑓 11 1− 𝑓 21
    ⋮ 1− 𝑓 𝑗1 ⋮ 1− 𝑓 𝐾1 1− 𝑓 12 𝑓 22 ⋮ 1− 𝑓 𝑗2 ⋮ 1− 𝑓 𝐾2 ⋯ ⋯ ⋯ ⋯ 1− 𝑓 1𝑗 1− 𝑓 2𝑗 ⋮
    𝑓 𝑗𝑗 ⋮ 1− 𝑓 𝐾𝑗 ⋯ ⋯ ⋯ ⋯ 1− 𝑓 1𝐾 1− 𝑓 2𝐾 ⋮ 1− 𝑓 𝑗𝐾 ⋮ 𝑓 𝐾𝐾 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟
    ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ . (5) 3.1. Acquisition of EMG Data The acquisition setup is
    composed of an embedded device (Section 3.1.1) and a computer. The methodology
    for the acquisition is described in Section 3.1.2. Data description is included
    in Section 3.1.3. A detailed description, including code, can be found in the
    previous work [37] of the authors. 3.1.1. Hardware Description The data were acquired
    using an embedded system, Figure 2, based on a 4-channel 24-bit ADC (AD7124-4)
    connected by serial peripheral interface (SPI) to a microcontroller (MCU) STM32L486RG
    microcontroller and running YetiOS [49] as the operating system. The sampling
    rate was set to 500 Hz with a hardware lowpass filter with a cutoff frequency
    of 130 Hz. The signals were transmitted via serial to the computer where they
    were processed using MATLAB [50]. Figure 2. Hardware acquisition system, left
    block design, and right picture of the system. 3.1.2. Methodology for Acquisition
    The study conducted within this work does not require ethical approval since no
    part of the experiments has any possible effect on the subjects’ bodies, harmful
    or otherwise. The system developed only gathers user motion data through EMG measurements.
    Moreover, the data were recorded and used in this publication under informed consent
    of the subject. The subject was a 23-year-old male. Figure 3 shows the 8 gestures
    recorded that emulate representative prosthetic hand movements. These particular
    movements are the same as those used in one of the most employed databases in
    the field, Ninapro DB6 [51]. Figure 3. Eight different gestures recorded. The
    classes C1, C2, …, C8 of the text are the pictures (a–h), respectively. Electrodes
    are placed on the skin of the forearm of the subject after applying conductive
    gel. Then, the subject was asked to repeat the gestures shown in Figure 3 10 times,
    each of them for 3 s, and they were subsequently stored in the computer, processed,
    and saved in matfiles using MATLAB [50]. 3.1.3. Data Description The result of
    the acquisition is a vector of around 1620 voltage measurements for each channel.
    Channel 3 was faulty during the acquisition, so, it was removed. Each of these
    vectors is concatenated, leading to a vector of 4860 coordinates. A total of 80
    gestures (10 repetitions of each class) were acquired, producing a matrix 𝐗 of
    size 80 × 4860 that will be used in this paper. This dataset is fully available
    in [52]. 3.2. Training and Test Sets to Evaluate the Prediction Abilitty of the
    PLS2-CM Model The evaluation of prediction capability can be done through a test
    set independent of the training set or by cross-validation (CV), also named k-fold
    procedure, that includes leave-one-out cross-validation. Despite its popularity,
    CV use has been criticized [53], and it has recently been formally proved that
    it is not a correct procedure to estimate prediction capability [54]. It has therefore
    been decided to divide dataset 𝐗 into two subsets: 𝐗 𝑇𝑅  for training and 𝐗 𝑇𝑆  ,
    the test set, for the evaluation of predictive capability. The Kennard-Stone algorithm
    [55], the most relevant for this task, was used by means of ‘kennardstone’ function
    of PLStoolbox [56]. The method initially selects the pair of sEMG signals with
    the largest distance and ranked as most representative. Then, in each following
    step, the remaining samples with the greatest distance from the already selected
    samples are chosen and added to the bottom of the previous rank list. This procedure
    is repeated until a predefined number of gestures had been chosen and ranked in
    each class. The method assures a uniform distribution of the sEMG signals selected,
    and it also includes boundary gestures of the class. Remaining gestures are assigned
    to test subset. In each class, 70% has been assigned for training and 30% for
    test subset. Resultantly, training matrix 𝐗 𝑇𝑅 has a size of 56 × 4860 and 24
    × 4860 for prediction 𝐗 𝑇𝑆  . 3.3. Class Encoding Error correcting code (ECOC)
    is the method of choice to encode the K classes by means of c binary learners.
    This codification is defined by the matrix 𝐌 of K rows and c columns whose elements
    are +1 or −1 . A simple example, Equation (6), of 𝐌 matrix was built to encode
    4 classes using a code of length 𝑐=7 . 𝐌= ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ +1 +1 +1 −1 −1
    −1 −1 −1 −1 +1 +1 −1 +1 +1 +1 −1 +1 +1 +1 +1 −1 −1 −1 +1 +1 +1 +1 −1 ⎞ ⎠ ⎟ ⎟ ⎟
    ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ . (6) Each row of 𝐌 represents the class code, so in Equation (6),
    the objects of the third class will be assigned the code or vector ( −1, −1, +1,
    +1, −1, −1, +1 ) . The same can be performed for the other three classes. Using
    these codes, it is possible to obtain the matrix 𝐘 formed by the class codes of
    all the objects. Each column of 𝐌 is a binary learner that can compare between
    two groups of classes or superclasses ℕ and ℝ . Continuing with the example of
    Equation (6), the first column (binary learner) divides the classes into two superclasses:
    the first one comprises the class ℕ= 𝐶 1 and left the rest in the second, ℝ= 𝐶
    2 ∪ 𝐶 3 ∪ 𝐶 4 . Another example could be the second binary learner that groups
    𝐶 1 and 𝐶 4 in the first superclass, ℕ= 𝐶 1 ∪ 𝐶 4 , and 𝐶 2 and 𝐶 3 in the second,
    ℝ= 𝐶 2 ∪ 𝐶 3 . A series of changes that do not affect the codification can be
    deduced from the aforementioned description of 𝐌 : permutations of columns, sign
    change in the whole column, and whole columns of −1 or +1 can be discarded. Therefore,
    the maximum length to encode K classes is 𝑐 𝑚𝑎𝑥 = 2 𝐾−1 −1 . For example, in the
    case of Equation (6), with K = 4 classes, 𝑐 𝑚𝑎𝑥 =7 . Selecting the adequate binary
    learners and their number c for a specific problem is challenging. As evaluated
    with several datasets in reference [46] and given that our dataset has 𝐾=8 classes,
    the proposed approach was to use function ‘designecoc’ (Statistics and Machine
    Learning Toolbox of MATLAB [50]). This function assigns positive or negative elements
    with equal probability for each element in M and uses a random generator to obtain
    the encoding, so the size of c is a random variable. However, its mean can be
    estimated by 10 log 2 (𝐾) , which in our case, means 𝑐≅30 . Applying this codification
    to the training label matrix 𝐋 𝑇𝑅 with dimensions (56×1) will result in matrix
    𝐘 𝑇𝑅 with dimensions (56×𝑐)≅(56×30) containing the encoding for each gesture,
    see Figure 1. 3.4. Partial Least Squares Regression Partial least squares regression
    (PLS) is a statistical tool that is very extended at the moment. It was introduced
    by Wold [57] in the 1970s and was successively developed [58,59]. This tool was
    used to relate 𝐗 𝑇𝑅 with 𝐘 𝑇𝑅 , see Figure 1. PLS sequentially builds pairwise
    lineal combinations of variables, one from X and one from Y, defined by the r
    and q vectors, respectively, that solve 𝑚𝑎 𝑥 𝐫,𝐪 {𝑣𝑎𝑟( 𝐗 𝑇𝑅 𝐫) [𝑐𝑜𝑟𝑟( 𝐗 𝑇𝑅 𝐫,
    𝐘 𝑇𝑅 𝐪)] 2 𝑣𝑎𝑟( 𝐘 𝑇𝑅 𝐪)}=𝑚𝑎 𝑥 𝐫,𝐪 { [ ( 𝐗 𝑇𝑅 𝐫) T 𝐘 TR 𝐪] 2 } subject to ‖𝐫‖=‖𝐪‖=1,
    (7) where var and corr stand for variance and correlation, respectively, of the
    new variables 𝐗 𝑇𝑅 𝐫 and 𝐘 𝑇𝑅 𝐪 that are named latent variables of PLS model.
    Maximization of the product 𝑣𝑎𝑟 ( 𝐗 𝑇𝑅 𝐫) [𝑐𝑜𝑟𝑟( 𝐗 𝑇𝑅 𝐫, 𝐘 𝑇𝑅 𝐪)] 2 𝑣𝑎𝑟( 𝐘 𝑇𝑅
    𝐪) tends to look for directions of large variance in both X- and Y-spaces (with
    more information), avoiding those of small variance (probably noise). In addition,
    the criterion includes the term 𝑐𝑜𝑟𝑟 ( 𝐗 𝑇𝑅 𝐫, 𝐘 𝑇𝑅 𝐪) that helps in avoiding
    directions in the signal space with small correlation with the class codification.
    PLS is particularly useful when the variables 𝐗 𝑇𝑅 and/or 𝐘 𝑇𝑅 are very correlated
    or are colinear. In our case, 𝐗 𝑇𝑅 , the sEMG signals, present both characteristics:
    They are not independent, and they do not define 4860 dimensions, and neither
    do 𝐘 𝑇𝑅 , the gesture codes, define approximately 30 dimensions. PLS is very efficient
    in finding the subjacent structure in the data by using a reduced number of latent
    variables, nvl. These latent variables define a subspace of X of nvl dimensions
    that explains the variability in the object codes, 𝐘 𝑇𝑅 , that, in our problem,
    is the variability between gesture classes. The value of nvl is adjusted between
    one and K to obtain the best response using the method described in Section 3.6.
    This step of procedure returns a linear regression model described by 4860×𝑐 coefficients
    along with the PLS-box [60], which is a closed region in the space of the sEMG
    signals. The complement set to the PLS-box identifies signals to which the model
    cannot be applied because they are significantly different from the training matrix.
    PLS-box is defined by two critical values fixed in this work to a confidence level
    of 0.95. As all the responses are fitted together, PLS provides a common PLS-box
    for the c binary learners. The result of applying the model to sEMG signals (
    𝐗 𝑇𝑅 or 𝐗 𝑇𝑆 ) will return, respectively, the predicted code ( 𝐘 ̂ 𝑇𝑅 and 𝐘 ̂
    𝑇𝑆 , respectively), see Figure 1. For this task, the function ‘pls’ of PLSToolbox
    [56] has been used. 3.5. Decoding, Class Assignation Based on the encoding section,
    Section 3.3, for each binary learner, 𝑔 𝑖 , 𝑖=1,…,𝑐 and its two superclasses ℝ
    y ℕ , there are two sets of predicted values, 𝑔 𝑖 (ℝ) and 𝑔 𝑖 (ℕ) , that should
    be close to −1 and 1, respectively. The distribution of these predicted values
    is calculated using the univariate kernel density [61]. Fixing the probabilities
    of 𝛾 𝑖 and 𝛿 𝑖 (that can be different for each 𝑔 𝑖 ), the distributions previously
    fitted are used to compute critical values 𝐶 𝑉 𝑖 (ℝ) and 𝐶 𝑉 𝑖 (ℕ) and with the
    conditions established in Equation (8). 𝑃{ 𝑦 ̂ 𝑖 ∈  𝑔 𝑖 (ℝ) | 𝑦 ̂ 𝑖 ≤ 𝐶 𝑉 𝑖 (ℝ)}=
    𝛾 𝑖 𝑃{ 𝑦 ̂ 𝑖 ∈  𝑓 𝑖 (ℕ) | 𝑦 ̂ 𝑖 ≤ 𝐶 𝑉 𝑖 (ℕ)}= 𝛿 𝑖 , (8) where P stands for probability.
    Notice that the definitions in Equation (8) imply that 𝛾 𝑖 would be a large value
    close to one, whereas 𝛿 𝑖 would be close to zero. In this application, 𝛾 𝑖 =0.99
    and 𝛿 𝑖 =0.01 for all 𝑔 𝑖 , 𝑖=1,…𝑐, have been fixed. For each EMG signal in the
    PLS-box, if 𝑦 ̂ 𝑖 ≤ 𝐶 𝑉 𝑖 (ℝ) , then the i-th coordinate on the decoding vector
    will be −1, and if 𝑦 ̂ 𝑖 >𝐶 𝑉 𝑖 (ℕ) , the coordinate will be +1. For each binary
    learner, the objects whose predicted values are between the corresponding critical
    values present two possibilities: (i) if 𝐶 𝑉 𝑖 (ℝ)<𝐶 𝑉 𝑖 (ℕ) , the signal will
    not be assigned, neither +1 nor −1, and (ii) when 𝐶 𝑉 𝑖 (ℝ)>𝐶 𝑉 𝑖 (ℕ) , the signal
    is assigned to both +1 and −1 (intersection). Finally, the signal is inside the
    i-th class model if the codeword of the i-th class in M is one of the decoding
    vectors related to it. To summarize, the sEMG signal could be assigned to none,
    one, or more classes or gestures. In our example, see Figure 1, the purpose is
    to obtain the 𝐋 ̂ 𝑇𝑅 and 𝐋 ̂ 𝑇𝑆 label matrix from the 𝐘 ̂ 𝑇𝑅 and 𝐘 ̂ 𝑇𝑆 predicted
    codes. However, based on the premise that any sample measured can only belong
    to one class, because it is not possible to perform two gestures at the same time,
    it is possible to obtain not valid results and errors when one sample belongs
    to none, two, or more classes. For this reason, predicted labels 𝐋 ̂ 𝑇𝑅 and 𝐋
    ̂ 𝑇𝑆 will have one more possible class (K + 1) than the original (K) classes in
    label matrix 𝐋 . 3.6. K-Class Model Evaluation Evaluation of a K-class model implies
    studying 𝐾×𝐾 values of its corresponding sensitivity 𝐒 matrix. Any given problem
    with more than 𝐾>2 makes it practically impossible to work with. In our case,
    it would be 64 values of sensitivity and specificity. Several metrics have been
    proposed to address this problem [62,63]. These metrics, in most of the cases,
    are obtained from the sensitivity of every individual class model, pair-wise specificities,
    efficiency or the total sensitivity, total specificity, and total efficiency of
    all class models, including convex combination of individual sensitivities and
    specificities. In particular, the mean of both is named accuracy in reference
    [48], but in [35], accuracy is the proportion of true assignations (positives
    or negatives). In general, these indexes are less sensible to changes in 𝐒 matrix.
    For this reason, they are not well suited for a systematic comparison of class
    models, for example, in an optimization. One alternative is to apply entropy concept
    as a measure of the order/information within the states of a system. A K-class
    model could be seen as a system whose states are the classes, and the information
    is provided by the sensitivities and specificities for each class model (K models
    in total). One K-class model will be better if it has less entropy according to
    Shannon’s entropy definition. In other words, a better class model will provide
    more information and better sensitivities and specificities if it successfully
    assigns sEMG signals to the model of each class and rejects the ones that do not
    belong to it, thus reducing the uncertainty or entropy of the system. In Shannon’s
    notation, followed in this paper, the entropy value will be 0 if all elements
    of 𝐒 are ones, and it will be perfectly precise, leaving no room for uncertainty.
    The development of this idea for confusion matrix can be found in references [64,65].
    The first one proposes, for the first time, a measure of the order/information
    generated by a classification method, called confusion entropy (CEN), inspired
    by Shannon’s entropy. The second solves a deficiency of CEN by proposing modified
    confusion entropy (MCEN). MCEN was evaluated and generalized in [46], defining
    a new index, diagonal modified confusion entropy (DMCEN), for sensitivity and
    specificity to evaluate K-class models. It also introduces one modification to
    solve MCEN’s almost unresponsiveness to differences in sensitivity by explicitly
    accounting for the contribution of sensitivity against specificity in the parameter
    w. This parameter can take values from zero to one; in our work, has was chosen
    w = 0.5. A detailed description of DMCEN is out of the scope of this paper. Qualitatively,
    the idea is that for the computation of Shannon’s entropy of a 𝐶 𝑗 class, is necessary
    to define the probability that the system is in class 𝐶 𝑗 in relation to another
    class 𝐶 𝑚 , for this it is considered as reference for all the decisions that
    involve both classes. In other words, to evaluate one K-class model, is necessary
    to take into account its whole 𝐒 matrix. In this work, DMCEN was calculated using
    an ad-hoc MATLAB code, available in reference [66], that calculates global DMCEN
    and for each class given an S matrix. DMCEN can take values from zero to one,
    and the lower DMCEN, the better the K-class model is. 3.7. PLS-ECOC Model Optimization
    ECOC matrix, M, has a random origin, as explained in Section 3.3, and is not related
    to 𝐗 𝑇𝑅 , but with M, the response 𝐘 𝑇𝑅 is built. Therefore, the PLS model that
    relates 𝐗 𝑇𝑅 and 𝐘 𝑇𝑅 depends on M. To select the best M matrix, the steps from
    Section 3.3 to Section 3.6 were repeated 280 times, aiming to obtain the best
    S matrix. The PLS model associated with it is called 𝑃𝐿 𝑆 𝑂𝑃 and is the one that
    will be used in Section 3.8 for the evaluation with the test subset, 𝐗 𝑇𝑆 . This
    process is illustrated in Figure 1 as the left grey rectangle. 3.8. Model Validation
    To validate the model, the calculated label matrix 𝐋 ̂ 𝑇𝑆 , DMCEN and 𝐒 𝑇𝑆 was
    obtained using the model 𝑃𝐿 𝑆 𝑂𝑃 and test data 𝐗 𝑇𝑆 as its input. Another interesting
    metric is the use of 𝐋 ̂ 𝑇𝑆 , extra class, the one assigned to errors to obtain
    the overall error and failure rate of the system. Error rate in a system is defined
    as a known misclassification per total signals predicted, and that would be the
    sEMG labelled as error in 𝐋 ̂ 𝑇𝑆 . Failure rate is the wrongly labelled sEMG signals
    in 𝐋 ̂ 𝑇𝑆 compared to 𝐋 𝑇𝑆 minus the number of errors over the total of signals
    predicted. This process is illustrated in Figure 1 as the right grey rectangle.
    4. Results PLS stands out when it comes to model data that have high collinearity
    because it uses latent variables. To evaluate the collinearity of the data, principal
    component analysis (PCA) was carried out with 𝐗 𝑇𝑅 and 𝐘 𝑇𝑅 of 𝑃𝐿 𝑆 𝑂𝑃 . The result
    of this analysis shows that 4860 dimensions of 𝐗 𝑇𝑅 can be represented with 5
    to 8 components, explaining 88.2% to 93.3% of the variance, respectively. Similar
    results were obtained when analyzing 𝐘 𝑇𝑅 , where the number of principal components
    is 7, much smaller than the original dimension, 35, and explaining all the variance.
    In addition, in this case, 15.2% of the pairwise correlation coefficients between
    𝐘 𝑇𝑅 columns are, in absolute value, greater than 0.5. These results, specific
    to our problem, indicates the need to use a regression on latent variables, such
    as PLS. Optimization of the PLS model, Section 3.8, generates 280 ECOC matrices;
    therefore, 280 matrices 𝐘 𝑇𝑅 result in 280 PLS models. Each of these iterations
    may have a different c, codeword length, of the ECOC matrix and a different DMCEN
    index. The optimization looks to minimize the DMCEN index in the training subset,
    𝐗 𝑇𝑅 , obtaining a 𝑃𝐿 𝑆 𝑂𝑃 optimum model with seven latent variables. DMCEN =0.031
    , and codeword length 𝑐=35 . All codeword lengths and DMCEN values obtained are
    depicted in Figure 4. Codeword length varies from 30 to 39, greater than the expected
    30 (Section 3.3), with a median value of 36. DMCEN varies from 0.031 to 0.217
    with a median and mean value of 0.12 with a very symmetric distribution. Figure
    4. Some characteristics of the 280 repetitions made in the optimisation step.
    (a) Codeword length of ECOC matrices. (b) DMCEN values for PLS models built with
    these ECOC matrices. Sensitivity and specificity in the training matrix 𝐒 𝑇𝑅 are
    shown in Table 1. The meaning of this matrix was thoroughly discussed at the beginning
    of Section 2. There are two elements in Table 1 that differ from one (the best
    possible result); both cases will be commented. First, the element marked with
    (*), 0.86, means that one 𝐶 3 sample was wrongly assigned to the 𝐶 1 class, so
    the 𝐶 1 class model is not completely specific against 𝐶 3 . Second, the element
    marked with (**), 0.71, means that two 𝐶 6 samples were assigned to 𝐶 5 , so the
    𝐶 5 class model is not completely specific against 𝐶 6 . From this analysis, we
    can say that the 𝐶 1 and 𝐶 5 class models are unable to fully reject signals from
    𝐶 3 and 𝐶 6 , respectively. Table 1. Sensitivity and specificity matrix STR for
    the test dataset XTR. Applying the 𝑃𝐿 𝑆 𝑂𝑃 model to the test matrix 𝐗 𝑇𝑆 , the
    sensitivity and specificity matrix 𝐒 𝑇𝑆 is obtained, Table 2, and DMCEN associated
    with 𝐗 𝑇𝑆 is 0.209. Again, we are going to comment on the three elements of 𝐒
    𝑇𝑆 that differ from the one in Table 2. First, the element marked with (*), 0.67,
    means that one out of the three signals from the 𝐶 3 class was wrongly rejected
    from the 𝐶 3 class model. Therefore, the class 𝐶 3 model sensitivity lowered to
    0.67. The 𝐶 3 class model also wrongly accepted two signals belonging to 𝐶 8 ,
    thus reducing its specificity against 𝐶 8 to 0.33, marked as (**). Lastly, the
    element marked as (***), 0.33, means that the 𝐶 5 class model failed to reject
    two signals from the 𝐶 6 class, so its specificity against 𝐶 6 reduced to 0.33.
    From these results, we can say the 𝐶 3 class model is having problems to model
    the 𝐶 3 signals like its own and is failing to completely reject all signals from
    𝐶 8 . The 𝐶 5 class model is presenting the same flaw as in the training set wrongly
    assigning 𝐶 6 signals to itself. Table 2. Sensitivity and specificity matrix STS
    for the test dataset XTS. To understand the benefits of modelling sEMG signals
    as an alternative to usual classifiers, the assignation matrix, Table 3, of the
    test dataset 𝐗 𝑇𝑆 is shown. This matrix was obtained by applying the 𝑃𝐿 𝑆 𝑂𝑃 model
    and decoding it. Each row of the matrix represents one sEMG signal from 𝐗 𝑇𝑆 and
    is composed of a vector of zeros and ones indicating the class assignment made
    by the model. Each signal sEMG belonging to test dataset ( 𝐗 𝑇𝑆 ) is a row of
    Table 3, and each 1 means that the class model, the column, accepted the signal.
    Table 3. Assignation matrix for the sEMG classes of the test dataset XTS. A 1
    means assigned class and 0 not assigned. In this analysis, an error is defined
    as a signal that belongs to a number of gestures different from one, a failure
    is defined as a signal that is assigned solely to the wrong class, and a success
    is defined as a signal that is correctly assigned to its class. For example, Row
    7 of Table 3 shows an sEMG signal from the 𝐶 3 gesture class that was not assigned
    to any class, resulting in an error and reducing the sensitivity of the 𝐶 3 class
    model, as indicated by the (*) label in Table 2 Rows 17 and 18 show two signals
    from the 𝐶 6 class that have been assigned to two classes, the correct one ( 𝐶
    6 ) and an incorrect one ( 𝐶 5 ). This results in two errors, but the 𝐶 6 class
    model sensitivity is not affected. However, incorrect assignment of the signals
    to 𝐶 5 reduces the specificity of the 𝐶 5 class model against the 𝐶 6 class, as
    indicated by the (***) label in Table 2. Similarly, rows 23 and 24 show two signals
    that are correctly assigned to the 𝐶 8 class but are also incorrectly assigned
    to the 𝐶 3 class, which reduces the specificity of the 𝐶 3 class model against
    the 𝐶 8 class and adding two errors to the overall results. To sum up, the assignation
    matrix, Table 3, shows that there were 5 errors where a signal was assigned to
    a number of gestures different than one, 19 successes where a signal was correctly
    assigned to its class, and 0 failures. It also highlights the origin of the changes
    in specificity and sensitivity in the S matrix, Table 2. 5. Discussion Differences
    between a classifier and a class-modelling classifier such as PLS-ECOC were stated
    in the Introduction and State-of-the-Art Approaches Sections. To summarize, the
    main difference is that a class-modelling classifier can assign one gesture to
    none or more than one class. This feature combined with the PLS-box was used in
    this work to implement a fault-tolerant classifier that can detect errors, increasing
    the robustness of the whole system. This feature could not be achieved with previous
    classifiers used in sEMG GR applications. To fully evaluate the extra information
    provided by a class-modelling classifier, the DMCEN [46] metric was used as described
    in Section 3.6. The usual metrics (accuracy, precision, or F1 score) will fall
    short because they assume that each object can belong to only one class. However,
    for the sake of comparison with other classifiers in the sEMG GR field, it seems
    reasonable to express the results also in the usual metric format: accuracy, precision,
    and F1 score. To this purpose, the errors (gestures assigned to none or two or
    more classes) were assigned to their most probable class. Note that with this
    transformation, the class-modelling algorithm loses its fault-tolerant capability.
    Considered PLS-ECOC as a classification method, the confusion matrix in training
    NTR = (nij), is diagonal, with nii = 7 and nij = 0 if i ≠ j. Therefore, accuracy,
    precision, and F1 are all equal to 100%. In prediction, with the test set, the
    confusion matrix is shown in Table 4. The accuracy, precision, and F1 values are
    87.5, 91.87, and 86.34%, respectively. They have been calculated in accordance
    with ref. [63] to maintain consistency with the definitions of sensitivity and
    specificity given in the introduction of Section 3. Table 4. Confusion matrix
    NTS for the test dataset XTS. However, in the field of sEMGs, there is no agreement
    on the definition of accuracy, and this ambiguity makes it difficult to compare
    results. Section 3.5 of ref. [33] is devoted to this question. The authors find
    that one work of the 56 studied defines “the recognition accuracy” (a gesture
    is considered a true positive, i.e., the gesture is recognized correctly when
    the model determines what gesture was performed and when this gesture was performed
    by a person). Eleven other papers use the concept of “classification accuracy”
    because they only took into consideration what gesture was performed by a person
    as a true positive, and the remaining papers do not show what they consider a
    true positive. This ambiguity is transferred to the calculation of accuracy. In
    our case, both definitions coincide with each other and with our calculation because
    the rest position (no gesture, in other papers) has been considered as a class
    in the model. Other definitions of accuracy used in the analysis of sEMG signals
    are the ratio of correctly predicted gestures (true positives plus true negatives)
    to the total gestures [67,68], obtaining 98.44% with our data. In [33], accuracy
    is also defined as the arithmetic mean of sensitivity and specificity, which,
    in our case, would be 87.5%. In any case, the accuracy of PLS-ECOC transformed
    into a classifier is in the range of values found in the literature described
    in Section 2. Specifically, using the means and standard deviations obtained from
    the accuracies reported in references [14,33,34,35] and assuming that the data
    come from a normal distribution, the tolerance limits state that we can be 95.0%
    confident that 99.0% of the distribution lies above 73.62, 52.76, 63.40, and 71.92%,
    respectively. The value obtained, 87.5%, with PLS-ECOC transformed into a classifier
    is inside all four tolerance intervals. Another important aspect of the proposed
    class-model is that it does not require human intervention in adjusting it. Most
    approaches in the sEMG GR field require the selection of features to make the
    classifier work or to adjust the parameters of the classifier. By contrast, the
    proposed method is totally automated, simultaneously adjusting feature extraction,
    dimensionality reduction in the nvl latent variables, and classification performance
    using the DMCEN index. It only needs a labelled dataset to work with. This approach
    should provide better flexibility, reducing design time. Another advantage from
    the designer’s point of view is the information that the PLS-ECOC provides; the
    sensitivity and specificity matrix S can give important insights on which gestures
    are being confused, providing a useful debug tool to fix or improve the overall
    system design. For example, it can highlight which electrodes should be placed
    differently or that some gestures cannot be used as commands for the system. The
    last part of this discussion will focus on the applicability of the presented
    method for a wearable, real-time application. Three metrics are evaluated: the
    size of the model, latency, and scalability. The size of the 𝑃𝐿 𝑆 𝑂𝑃  model is
    extremely important for wearable devices, as it directly impacts performance and
    limits usability. Any lineal model has a parameter size of 𝑛𝑣𝑐×𝑛𝑐×𝑐 where nvc
    is the number of voltage values recorded, nc is the number of channels, and c
    is the codeword length. The computation required is also proportional to the size
    of the model because obtaining the classification value will require one multiplication
    and one addition per parameter. This size is comparable with the SVM algorithm.
    Translated into memory parameters, it will occupy 664 kB, a size that could not
    be fitted into the ram memory of a low-power MCU. However, the current implementation
    uses a 3 s gesture as an input. So, the size will fold by 30 in a real case scenario
    where windowing of the order of the hundredth of milliseconds is used. Latency,
    another key parameter, is composed of the time to acquire one gesture to be evaluated
    (3 s) and the time to process it. Being a linear model, the processing time in
    an MCU will be low because it is only composed of a multiplication and an addition
    per parameter. The last parameter is scalability. PLS-ECOC codification makes
    the size of the model increase logarithmically ( 𝑐≅10 log 2 𝐾 ) compared to SVM
    usual encoding, one vs. one, which increases quadratically ( 𝑐=𝐾(𝐾−1)/2 ), being,
    in both cases, K, the number of classes, providing a good base for more complex
    HMI systems that will require a large number of gestures to be functional. All
    these characteristics should be tested by implementing PLS-ECOC in a real embedded
    device and evaluating its performance. Another hint that there is room for improvement
    in the reduction of the codeword, c, length is the fact that the observed correlation
    in Figure 4 is 0.2 between DMCEN and c. This result implies that a larger codeword
    (and model) does not translate into better overall classification characteristics
    of the model. 6. Conclusions The proposed PLS-ECOC model for gesture recognition
    using sEMG signals can provide a failure-free result in the prediction phase.
    While keeping comparable results in accuracy, precision, and F1 of 87.5, 91.87
    y 86.34%, respectively, to the current state-of-the-art classifiers. This highlights
    the effectiveness of the proposed method as a fault-tolerant classification system.
    To the best knowledge of the authors, this type of fault-tolerant design has not
    been studied before, and it opens up new possibilities for developing classification
    applications in critical systems where safety is a key requirement, such as controlling
    a robotic arm. The proposed method also offers great flexibility for designers
    because it does not require any exploratory analysis to obtain features, which
    can be time-consuming and will be specific to each dataset. The PLS-ECOC method
    simultaneously adjusts feature extraction, dimensionality reduction in the nvl
    latent variables, and classification performance using the DMCEN index. Dimensionality
    reduction performed simultaneously with the adjustment of the model provides a
    great base for future developments. It is more robust and adapts better to differences
    between subjects, electrode placement, or acquisition systems. However, this paper
    was only evaluated on one subject, and further studies with more subjects, electrode
    positions, and acquisition systems should be carried out. The sensitivity and
    specificity matrix S provides a powerful tool for system designers to better understand
    where gestures are confused and to make decisions to improve the design. DMCEN
    is as simple as the accuracy metric used in discriminant classifiers. However,
    it extends its validity to class modelling classifiers, proving its utility for
    optimizing the PLS model. The PLS-ECOC lineal model provides good characteristics
    for further development because of its computational simplicity. However, in its
    current state, it is too big to fit in a low-power MCU that could be used in a
    wearable design. To address this issue, a couple of future work lines are proposed:
    reduction of the codeword length by reducing the ECOC output matrix and windowing
    the sEMG signals, which would be a required step to implement the method in a
    real-time application. Another improvement that should be taken into account is
    to use DMCEN optimization for the other two criteria (minimizing errors and minimizing
    the length of the codeword) either through a desirability function that introduces
    “a priori” weight to the criteria [69] or by applying a multicriteria optimization
    and using the Pareto front of the optimal solutions to obtain the optimal solution
    “a posteriori” [70]. As the relationship between the three criteria and the experimental
    data is not described by any deterministic function, a heuristic method, such
    as a genetic algorithm [71], could be used in the optimization process. Author
    Contributions Conceptualization, P.S., A.A. and L.A.S.; methodology, P.S. and
    L.A.S.; software, P.S., M.d.l.C.O. and L.A.S.; validation, P.S., L.A.S. and M.d.l.C.O.;
    formal analysis, P.S. and L.A.S.; investigation, P.S.; data curation, P.S., M.d.l.C.O.
    and L.A.S.; writing—review and editing, P.S., A.A., M.d.l.C.O. and L.A.S.; funding
    acquisition, P.S., M.d.l.C.O. and A.A. All authors have read and agreed to the
    published version of the manuscript. Funding This research was funded by Consejería
    JCyL number BU052P20 and cofinanced with European funds. This research was partially
    funded by the Spanish Ministry of Science and Innovation and the European Union
    under Grant PID2020-116011RB-C22 MyGait and Comunidad de Madrid under grant P022/BMD-7236
    MINA-CM. Projects). Data Availability Statement Sarabia Ortiz, Pablo (2023), “sEMG
    4 channel 8 gestures dataset”, Mendeley Data, v1 http://dx.doi.org/10.17632/2f982h6vg3.1
    (accessed on 4 March 2023). Conflicts of Interest The authors declare no conflict
    of interest. References Hakonen, M.; Piitulainen, H.; Visala, A. Current state
    of digital signal processing in myoelectric interfaces and related applications.
    Biomed. Signal Process. Control 2015, 18, 334–359. [Google Scholar] [CrossRef]
    [Green Version] Dunai, L.; Novak, M.; García Espert, C. Human Hand Anatomy-Based
    Prosthetic Hand. Sensors 2021, 21, 137. [Google Scholar] [CrossRef] Khushaba,
    R.N.; Kodagoda, S.; Takruri, M.; Dissanayake, G. Toward improved control of prosthetic
    fingers using surface electromyogram (EMG) signals. Expert Syst. Appl. 2012, 39,
    10731–10738. [Google Scholar] [CrossRef] Krebs, H.; Palazzol, J.; Hogan, N. Rehabilitation
    robotics: Performance-based progressive robot-assisted therapy. Auton. Robot.
    2003, 15, 7–20. [Google Scholar] [CrossRef] Bouteraa, Y.; Ben Abdallah, I.; Ibrahim,
    A.; Ahanger, T.A. Development of an IoT-Based Solution Incorporating Biofeedback
    and Fuzzy Logic Control for Elbow Rehabilitation. Appl. Sci. 2020, 10, 7793. [Google
    Scholar] [CrossRef] Santos Silva, C.; Oliveira Santos, M.; Gromicho, M.; Pinto,
    S.; Swash, M.; de Carvalho, M. Electromyographic findings in primary lateral sclerosis
    during disease progression. Clin. Neurophysiol. 2021, 132, 2996–3001. [Google
    Scholar] [CrossRef] Roldan-Vasco, S.; Orozco-Duque, A.; Orozco-Arroyave, J.R.
    Swallowing disorders analysis using surface EMG biomarkers and classification
    models. Digit. Signal Process. 2023, 133, 103815. [Google Scholar] [CrossRef]
    Catelli, D.S.; Kuriki, H.U.; Polito, L.F.; Azevedo, F.M.; Negrão Filho, R.F.;
    Alves, N. Patellofemoral pain syndrome: Electromyography in a frequency domain
    analysis. J. Phys. Conf. Ser. 2011, 313, 012004. [Google Scholar] [CrossRef] Subasi,
    A. Classification of EMG signals using PSO optimized SVM for diagnosis of neuromuscular
    disorders. Comput. Biol. Med. 2013, 43, 576–586. [Google Scholar] [CrossRef] Salinas,
    S.A.; Elgalhud, M.A.T.A.; Tambakis, L.; Salunke, S.V.; Patel, K.; Ghenniwa, H.;
    Ouda, A.; McIsaac, K.; Grolinger, K.; Trejos, A.L. Comparison of Machine Learning
    Techniques for Activities of Daily Living Classification with Electromyographic
    Data. In Proceedings of the 2022 International Conference on Rehabilitation Robotics
    (ICORR), Rotterdam, The Netherlands, 25–29 July 2022. [Google Scholar] [CrossRef]
    Dillen, A.; Steckelmacher, D.; Langlois, K.; De Beir, A.; Marusic, U.; Vanderborght,
    B.; Nowé, A.; Meeusen Romain, F.; Ghaffari, O.; Romain, K.D.P. Deep learning for
    biosignal control: Insights from basic to real-time methods with recommendations.
    J. Neural Eng. 2022, 19, 011003. [Google Scholar] [CrossRef] Zheng, M.; Crouch,
    M.S.; Eggleston, M.S. Surface Electromyography as a Natural Human–Machine Interface:
    A Review. IEEE Sens. J. 2022, 22, 9198–9214. [Google Scholar] [CrossRef] Xiong,
    D.; Zhang, D.; Zhao, X.; Zhao, Y. Deep Learning for EMG-based Human-Machine Interaction:
    A Review, IEEE/CAA. J. Autom. Sin. 2021, 8, 512–533. [Google Scholar] [CrossRef]
    Quitadamo, L.R.; Cavrini, F.; Sbernini, L.; Riillo, F.; Bianchi, L.; Seri, S.;
    Saggio, G. Support vector machines to detect physiological patterns for EEG and
    EMG-based human–computer interaction: A review. J. Neural Eng. 2017, 14, 011001.
    [Google Scholar] [CrossRef] Diab, M.S.; Rodriguez-Villegas, E. Embedded Machine
    Learning Using Microcontrollers in Wearable and Ambulatory Systems for Health
    and Care Applications: A Review. IEEE Access 2022, 10, 98450–98474. [Google Scholar]
    [CrossRef] Lu, Y.; Le, V.L.; Kim, T.T.H. A 184-μW Error-Tolerant Real-Time Hand
    Gesture Recognition System With Hybrid Tiny Classifiers Utilizing Edge CNN. IEEE
    J. Solid-State Circuits 2023, 58, 530–542. [Google Scholar] [CrossRef] Jiang,
    Y.; Song, L.; Zhang, J.; Song, Y.; Yan, M. Multi-Category Gesture Recognition
    Modeling Based on sEMG and IMU Signals. Sensors 2022, 22, 5855. [Google Scholar]
    [CrossRef] Morrison, D.F. Multivariate Statistical Methods, 3rd ed.; McGraw-Hill:
    New York, NY, USA, 1990. [Google Scholar] Friedman, J.H. Regularized discriminant
    analysis. J. Am. Stat. Assoc. 1989, 84, 165–175. [Google Scholar] [CrossRef] Barker,
    M.; Rayens, W. Partial Least Squares for discrimination. J. Chemom. 2003, 17,
    166–173. [Google Scholar] [CrossRef] Breiman, L.; Friedman, J.H.; Olshen, R.A.;
    Stone, C.J. Classification and Regression Trees; CRC Press: Boca Raton, FL, USA,
    1984. [Google Scholar] Vapnik, V.N. Statistical Learning Theory; Wiley: New York,
    NY, USA, 1998. [Google Scholar] Sun, M. A multi-class support vector machine:
    Theory and model. Int. J. Inf. Technol. Decis. Mak. 2013, 12, 1175–1199. [Google
    Scholar] [CrossRef] Schölkopf, B.; Williamson, R.; Smola, A.; Shawe-Taylor, J.;
    Platt, J. Support Vector Method for Novelty Detection. In Advances in Neural Information
    Processing Systems 12, Proceedings of the 29 December 1999 Conference; Solla,
    S., Leen, T., Müller, K.-R., Eds.; MIT Press: Cambridge, MA, USA, 2000; pp. 582–588.
    [Google Scholar] McCulloch, W.; Pitts, W. A logical calculus of the ideas imminent
    in nervous activity. Bull. Math. Biophys. 1943, 5, 115–133. [Google Scholar] [CrossRef]
    Werbos, P. Beyond Regression. Ph.D. Thesis, Harvard University, Cambridge, MA,
    USA, 1974. Available online: https://www.researchgate.net/publication/35657389
    (accessed on 4 March 2023). Le Cun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. Gradient-based
    learning applied to document recognition. Proc. IEEE 1998, 86, 2278–2324. [Google
    Scholar] [CrossRef] [Green Version] Wold, S. Pattern recognition by means of disjoint
    principal components models. Pattern Recognit. 1996, 8, 127–139. [Google Scholar]
    [CrossRef] Derde, M.P.; Massart, D.L. UNEQ: A disjoint modelling technique for
    pattern recognition based on normal distribution. Anal. Chim. Acta 1986, 184,
    33–51. [Google Scholar] [CrossRef] Tax, D.M.; Duin, R.M. Support Vector Data Description.
    Mach. Learn. 2004, 54, 45–66. [Google Scholar] [CrossRef] [Green Version] Ortiz,
    M.C.; Sarabia, L.A.; García-Rey, R.; Luque de Castro, M.D. Sensitivity and specificity
    of PLS-class modelling for five sensory characteristics of dry-cured ham using
    visible and near infrared spectroscopy. Anal. Chim. Acta 2006, 558, 125–131. [Google
    Scholar] [CrossRef] Valencia, O.; Ortiz, M.C.; Sánchez, M.S.; Sarabia, L.A. Simultaneous
    class-modelling in chemometrics: A generalization of Partial Least Squares class
    modelling for more than two classes by using error correcting output code matrices.
    Chemom. Intell. Lab. Syst. 2022, 227, 104614. [Google Scholar] [CrossRef] Jaramillo-Yánez,
    A.; Benalcázar, M.E.; Mena-Maldonado, E. Real-Time Hand Gesture Recognition Using
    Surface Electromyography and Machine Learning: A Systematic Literature Review.
    Sensors 2020, 20, 2467. [Google Scholar] [CrossRef] Dhumal, S.; Sharma, P. EMG
    Pattern Recognition: A Systematic Review. In Lecture Notes in Networks and Systems
    ISMS 2021; Garg, L., Sisodia, D.S., Kesswani, N., Vella, J.G., Brigui, I., Xuereb,
    P., Misra, S., Singh, D., Eds.; Springer International Publishing: Cham, Switzerland,
    2023; Volume 521, pp. 120–130. [Google Scholar] [CrossRef] Nazmi, N.; Rahman,
    M.A.A.; Yamamoto, S.I.; Ahmad, S.A.; Zamzuri, H.; Mazlan, S.A. A Review of Classification
    Techniques of EMG Signals during Isotonic and Isometric Contractions. Sensors
    2016, 16, 1304. [Google Scholar] [CrossRef] [Green Version] Aviles, M.; Sánchez-Reyes,
    L.M.; Fuentes-Aguilar, R.Q.; Toledo-Pérez, D.C.; Rodríguez-Reséndiz, J. A Novel
    Methodology for Classifying EMG Movements Based on SVM and Genetic Algorithms.
    Micromachines 2022, 13, 2108. [Google Scholar] [CrossRef] Sarabia Ortiz, P. Design
    and Evaluation of Electromyography Signal Processing Techniques Using Resource-Constrained
    Devices. Master’s Thesis, E.T.S.I. Telecomunicación, UPM, Universidad Politécnica
    de Madrid, Madrid, Spain, 2020. Available online: https://oa.upm.es/72388/ (accessed
    on 4 March 2023). Zhang, X.; Yao, L.; Wang, X.; Monaghan, J.; McAlpine, D.; Zhang,
    Y. A Survey on Deep Learning-based Non-Invasive Brain Signals: Recent Advances
    and New Frontiers. J. Neural Eng. 2021, 18, 1002. [Google Scholar] [CrossRef]
    Cai, S.; Li, H.; Wu, Q.; Liu, J.; Zhang, Y. Motor Imagery Decoding in the Presence
    of Distraction Using Graph Sequence Neural Networks. IEEE Trans. Neural Syst.
    Rehabil. Eng. 2022, 30, 1716–1726. [Google Scholar] [CrossRef] [PubMed] Jia, H.;
    Su, Z.; Dua, F.; Zhan, Y.; Caiafa, C.F.; Solé-Casals, J. Improving Pre-movement
    Pattern Detection with Filter Bank Selection. J. Neural Eng. 2022, 19, 066012.
    [Google Scholar] [CrossRef] [PubMed] Phinyomark, A.; Scheme, E. EMG Pattern Recognition
    in the Era of Big Data and Deep Learning. Big Data Cogn. Comput. 2018, 2, 21.
    [Google Scholar] [CrossRef] [Green Version] Englehart, K.; Hudgins, B.; Parker,
    P.A.; Stevenson, M. Classification of the myoelectric signal using time-frequency
    based representations. Med. Eng. Phys. 1999, 21, 431–438. [Google Scholar] [CrossRef]
    Arozi, M.; Caesarendra, W.; Ariyanto, M.; Munadi, M.; Setiawan, J.D.; Glowacz,
    A. Pattern Recognition of Single-Channel sEMG Signal Using PCA and ANN Method
    to Classify Nine Hand Movements. Symmetry 2020, 12, 541. [Google Scholar] [CrossRef]
    [Green Version] Mendes Junior, J.J.A.; Freitasa, M.L.B.; Siqueira, H.V.; Lazzarettia,
    A.E.; Pichorima, S.F.; Stevan, S.L., Jr. Feature selection and dimensionality
    reduction: An extensive comparison in hand gesture classification by sEMG in eight
    channels armband approach. Biomed. Signal Process. Control. 2020, 59, 101920.
    [Google Scholar] [CrossRef] Chen, X.; Liu, A.; Wang, Z.J.; Peng, H. Corticomuscular
    Activity Modeling by Combining Partial Least Squares and Canonical Correlation
    Analysis. J. Appl. Math. 2013, 401976. [Google Scholar] [CrossRef] Valencia, O.;
    Ortiz, M.C.; Sánchez, M.S.; Sarabia, L.A. A modified entropy-based performance
    criterion for class-modelling with multiple classes. Chemom. Intell. Lab. Syst.
    2021, 217, 104423. [Google Scholar] [CrossRef] Baldomero-Naranjo, M.; Martínez-Merino,
    L.I.; Rodríguez-Chía, A.M. A robust SVM-based approach with feature selection
    and outliers detection for classification problems. Expert Syst. Appl. 2021, 178,
    115017. [Google Scholar] [CrossRef] Tannemaat, M.R.; Kefalas, M.; Geraedts, V.J.;
    Remijn-Nelissen, L.; Verschuuren, A.J.M.; Koch, M.; Kononova, A.V.; Wang, H.;
    Bäck, T.H.W. Distinguishing normal, neuropathic and myopathic EMG with an automated
    machine learning approach. Clin. Neurophysiol. 2023, 146, 49–54. [Google Scholar]
    [CrossRef] Rodriguez-Zurrunero, R.; Tirado-Andrés, F.; Araujo, A. YetiOS: An Adaptive
    Operating System for Wireless Sensor Networks. In Proceedings of the 2018 IEEE
    43rd Conference on Local Computer Networks Workshops (LCN Workshops), Chicago,
    IL, USA, 1–4 October 2018; pp. 16–22. [Google Scholar] [CrossRef] MATLAB, version
    9.9.0.2037887 (R2020b); The Mathworks, Inc.: Natick, MA, USA, 2022. Palermo, F.;
    Cognolato, M.; Gijsberts, A.; Müller, H.; Caputo, B.; Atzori, M. Repeatability
    of grasp recognition for robotic hand prosthesis control based on sEMG data. In
    Proceedings of the 2017 International Conference on Rehabilitation Robotics (ICORR),
    London, UK, 17–20 July 2017; pp. 1154–1159. [Google Scholar] [CrossRef] Sarabia
    Ortiz, P. “sEMG 4 Channel 8 Gestures Dataset”, Mendeley Data, version 1. 2023.
    [CrossRef] Esbensen, K.H.; Geladi, P. Principles of Proper Validation: Use and
    abuse of re-sampling for validation. J. Chem. 2010, 24, 168–187. [Google Scholar]
    [CrossRef] Bates, S.; Hastie, T.; Tibshirani, R. Cross-validation: What does it
    estimate and how well does it do it? arXiv 2022, arXiv:2104.00673v4. [Google Scholar]
    [CrossRef] Kennard, R.W.; Stone, L.A. Computer aided design of experiments. Technometrics
    1969, 11, 137–148. [Google Scholar] [CrossRef] PLSToolbox, Version 8.9.2 R23162;
    Eigenvector Research, Inc.: Manson, WA, USA, 2021. Wold, S. Chemometrics and Bruce:
    Some Fond Memories; Lavine, B.K., Brown, S.D.D., Booksh, K.S., Eds.; 40 Years
    of Chemometrics—From Bruce Kowalski to the Future; ACS Symposium Series; American
    Chemical Society: Washington, DC, USA, 2015. [Google Scholar] [CrossRef] [Green
    Version] de Jong, S. SIMPLS: An alternative approach to partial least squares
    regression. Chemom. Intell. Lab. Syst. 1993, 18, 251–263. [Google Scholar] [CrossRef]
    Rosipal, R.; Krämer, N. Overview and Recent Advances in Partial Least Squares
    in Subspace, Latent Structure and Feature Selection; Saunders, C., Grobelnik,
    M., Gunn, S., Shawe-Taylor, J., Eds.; Statistical and Optimization Perspectives
    Workshop, SLSFS; Springer: Berlin/Heidelberg, Germany, 2005; pp. 34–51. Available
    online: https://link.springer.com/chapter/10.1007/11752790_2 (accessed on 4 March
    2023). Ruiz, S.; Ortiz, M.C.; Sarabia, L.A.; Sánchez, M.S. A computational approach
    to partial least squares model inversion in the framework of the process analytical
    technology and quality by design initiatives. Chemom. Intell. Lab. Syst. 2018,
    182, 70–78. [Google Scholar] [CrossRef] Wand, M.P.; Jones, M.C. Kernel Smoothing,
    Monographs on Statistical an Applied Probability, 60; Springer-Science-Business
    Media: New York, NY, USA, 1995. [Google Scholar] Cuadros-Rodríguez, L.; Pérez-Castaño,
    E.; Ruiz-Samblás, C. Quality performance metrics in multivariate classification
    methods for qualitative analysis. Trends Anal. Chem. 2016, 80, 12–624. [Google
    Scholar] [CrossRef] Ballabio, D.; Grisoni, F.; Todeschini, R. Multivariate comparison
    of classification performance measures. Chemom. Intell. Lab. Syst. 2018, 174,
    33–44. [Google Scholar] [CrossRef] Wei, M.; Yuan, X.Y.; Hu, Q.H.; Wang, S.Q. A
    novel measure for evaluating classifiers. Expert Syst. Appl. 2010, 37, 3799–3809.
    [Google Scholar] [CrossRef] Delgado, R.; Núñez-González, J.D. Enhancing Confusion
    Entropy (CEN) for binary and multiclass classification. PLoS ONE 2019, 14, e0210264.
    [Google Scholar] [CrossRef] [Green Version] Sánchez, M.S.; Valencia, O.; Ruiz,
    S.; Ortiz, M.C.; Sarabia, L.A. DMCEN a MATLAB Function to Evaluate the Entropy
    Improvement Provided by a Multivariate K-Class-Model. Available online: https://www.mathworks.com/matlabcentral/fileexchange/112175-dmcen?s_tid=srchtitle
    (accessed on 4 January 2023). Tuncer, S.A.; Alkan, A. Classification of EMG signals
    taken from arm with hybrid CNN-SVM architecture. Concurr. Computat. Pract. Exper.
    2022, 34, e6746. [Google Scholar] [CrossRef] Gopal, P.; Gesta, A.; Mohebbi, A.
    A Systematic Study on Electromyography-Based Hand Gesture Recognition for Assistive
    Robots Using Deep Learning and Machine Learning Models. Sensors 2022, 22, 3650.
    [Google Scholar] [CrossRef] [PubMed] Derringer, G.C.; Suich, R. Simultaneous optimization
    of several response variables. J. Qual. Technol. 1980, 12, 214–219. [Google Scholar]
    [CrossRef] Ortiz, M.C.; Sarabia, L.A.; Herrero, A.; Sánchez, M.S. Vectorial optimization
    as a methodological alternative to desirability function. Chemometr. Intell. Lab.
    Syst. 2006, 83, 157–168. [Google Scholar] [CrossRef] Deb, K. Multi-Objective Optimization
    Using Evolutionary Algorithms; Wiley: Hoboken, NJ, USA, 2001. [Google Scholar]
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all
    publications are solely those of the individual author(s) and contributor(s) and
    not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility
    for any injury to people or property resulting from any ideas, methods, instructions
    or products referred to in the content.  © 2023 by the authors. Licensee MDPI,
    Basel, Switzerland. This article is an open access article distributed under the
    terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Sarabia, P.; Araujo, A.; Sarabia, L.A.; Ortiz,
    M.d.l.C. Electromyography Gesture Model Classifier for Fault-Tolerant-Embedded
    Devices by Means of Partial Least Square Class Modelling Error Correcting Output
    Codes (PLS-ECOC). Algorithms 2023, 16, 149. https://doi.org/10.3390/a16030149
    AMA Style Sarabia P, Araujo A, Sarabia LA, Ortiz MdlC. Electromyography Gesture
    Model Classifier for Fault-Tolerant-Embedded Devices by Means of Partial Least
    Square Class Modelling Error Correcting Output Codes (PLS-ECOC). Algorithms. 2023;
    16(3):149. https://doi.org/10.3390/a16030149 Chicago/Turabian Style Sarabia, Pablo,
    Alvaro Araujo, Luis Antonio Sarabia, and María de la Cruz Ortiz. 2023. \"Electromyography
    Gesture Model Classifier for Fault-Tolerant-Embedded Devices by Means of Partial
    Least Square Class Modelling Error Correcting Output Codes (PLS-ECOC)\" Algorithms
    16, no. 3: 149. https://doi.org/10.3390/a16030149 Note that from the first issue
    of 2016, this journal uses article numbers instead of page numbers. See further
    details here. Article Metrics Citations No citations were found for this article,
    but you may check on Google Scholar Article Access Statistics Article access statistics
    Article Views 10. Jan 20. Jan 30. Jan 9. Feb 19. Feb 29. Feb 10. Mar 20. Mar 30.
    Mar 0 500 1000 1500 2000 For more information on the journal statistics, click
    here. Multiple requests from the same IP address are counted as one view.   Algorithms,
    EISSN 1999-4893, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Algorithms
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Electromyography Gesture Model Classifier for Fault-Tolerant-Embedded Devices
    by Means of Partial Least Square Class Modelling Error Correcting Output Codes
    (PLS-ECOC)
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liu X.
  - Derakhshani M.
  - Mihaylova L.
  - Lambotharan S.
  citation_count: '2'
  description: This paper proposes an edge-assisted crowdsourced live video transcoding
    approach where the transcoding capabilities of the edge transcoders are unknown
    and dynamic. The resilience and trustworthiness of highly unstable transcoders
    in decision making are characterized with mean-variance-based measures to avoid
    making highly risky decisions. The risk level of each device's situation is assessed
    and two upper confidence bounds of the variance of transcoding performance are
    presented. Based on the derived bounds and by leveraging the contextual information
    of devices, two risk-aware contextual learning schemes are developed to efficiently
    estimate the transcoding capabilities of the edge devices. Combining context awareness
    and risk sensitivity, a novel transcoding task assignment and viewer association
    algorithm is proposed. Simulation results demonstrate that the proposed algorithm
    achieves robust task offloading with superior network utility performance as compared
    to the linear upper confidence bound and the risk-aware mean-variance upper confidence
    bound-based algorithms. In particular, an epoch-based task assignment strategy
    is designed to reduce the task switching costs incurred in assigning the same
    transcoding task to different transcoders over time. This strategy also reduces
    the computational time needed. Numerical results confirm that this strategy achieves
    up to 86.8% switching costs reduction and 92.3% computational time reduction.
  doi: 10.1109/JSAC.2022.3229423
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Journal on Selected
    Area... >Volume: 41 Issue: 3 Risk-Aware Contextual Learning for Edge-Assisted
    Crowdsourced Live Streaming Publisher: IEEE Cite This PDF Xingchi Liu; Mahsa Derakhshani;
    Lyudmila Mihaylova; Sangarapillai Lambotharan All Authors 2 Cites in Papers 576
    Full Text Views Abstract Document Sections I. Introduction II. Related Work III.
    System Model IV. Optimization Problem: Edge Transcoding and Viewer Association
    V. Risk-Aware Contextual Learning for Edge Transcoding Show Full Outline Authors
    Figures References Citations Keywords Metrics Abstract: This paper proposes an
    edge-assisted crowdsourced live video transcoding approach where the transcoding
    capabilities of the edge transcoders are unknown and dynamic. The resilience and
    trustworthiness of highly unstable transcoders in decision making are characterized
    with mean-variance-based measures to avoid making highly risky decisions. The
    risk level of each device’s situation is assessed and two upper confidence bounds
    of the variance of transcoding performance are presented. Based on the derived
    bounds and by leveraging the contextual information of devices, two risk-aware
    contextual learning schemes are developed to efficiently estimate the transcoding
    capabilities of the edge devices. Combining context awareness and risk sensitivity,
    a novel transcoding task assignment and viewer association algorithm is proposed.
    Simulation results demonstrate that the proposed algorithm achieves robust task
    offloading with superior network utility performance as compared to the linear
    upper confidence bound and the risk-aware mean-variance upper confidence bound-based
    algorithms. In particular, an epoch-based task assignment strategy is designed
    to reduce the task switching costs incurred in assigning the same transcoding
    task to different transcoders over time. This strategy also reduces the computational
    time needed. Numerical results confirm that this strategy achieves up to 86.8%
    switching costs reduction and 92.3% computational time reduction. Published in:
    IEEE Journal on Selected Areas in Communications ( Volume: 41, Issue: 3, March
    2023) Page(s): 740 - 754 Date of Publication: 21 December 2022 ISSN Information:
    DOI: 10.1109/JSAC.2022.3229423 Publisher: IEEE Funding Agency: SECTION I. Introduction
    The revolution of the mobile Internet driven by the powerful mobile devices and
    social networks has greatly enriched the sources of video platforms. As an outcome
    of the revolution, crowdsourced live streaming platforms (CLSP) such as Twitch,
    TikTok, and Periscope have emerged as a new type of video platforms, that not
    only serve tremendous viewers all over the world but also receive live videos
    from various sources in the crowd [1], allowing a growing number of people to
    broadcast their live videos over the Internet. However, due to the heterogeneity
    of broadcasters’ devices, different quality versions of live videos are created
    [2]. As a result, there is a strong need to transcode the original live videos
    into several industrial standard representations and to serve viewers with a set
    of proper versions of representations. Providing the adaptive bit rate (ABR) service
    [3] can bring massive computational demands due to real-time processing requirements.
    For instance, until 2022, there are about 9.2 million broadcasters and 140 million
    viewers, which are active on Twitch monthly, and there are an average of more
    than 100,000 live channels concurrently at any point in time [4]. Therefore, instead
    of building private data centres to facilitate ABR, cloud computing has become
    a natural solution to perform transcoding because of its powerful computing ability
    and the ‘pay as you go’ feature. Furthermore, the emergence of cloud computing
    releases CLSP from building large, expensive private data centres. In such a system,
    the CLSP controller will decide the number of representations that need to be
    transcoded for each broadcaster based on parameters such as viewer capacity, playback
    delay, bandwidth consumption etc. The original live videos will be directly transmitted
    to the cloud data centre for transcoding. When multiple versions are generated
    in the cloud, content delivery networks will be utilized to deliver proper versions
    of live videos to the corresponding viewers. On the downside, in current CLSPs,
    the cloud transcoding is not able to provide the ABR service to most of the broadcasters.
    For instance, in Twitch.TV, only the premium broadcasters have access to the ABR
    service, and for the rest of the broadcasters, only the original versions are
    available for their viewers [1]. The reason behind is that a general cloud instance
    can only deal with at most two transcoding tasks simultaneously. Therefore, an
    enormous cost will be incurred when a large number of original live videos are
    scheduled for transcoding. Moreover, in cloud transcoding systems, the cloud data
    centre can be far from the viewers or the broadcasters, which can cause high latency.
    This problem can be further magnified considering the fact that most of CLSPs
    enable an interactive live chat service which is also latency-sensitive as compared
    to the traditional live streaming platforms. The development of edge computing
    (also know as fog computing) has brought a potential transcoding solution for
    CLSP. Since edge computing [5] is more suitable for real-time processing and low-latency
    applications, it can be treated as a viable replacement [6], [7] to address the
    shortcomings of the cloud transcoding. Moreover, edge-assisted transcoding can
    lead to lower latency and avoid the network traffic traversing through the core
    network since different versions of videos can be created at the network edge.
    Edge transcoding systems have been proposed to leverage the computational resources
    at the user end [8], [9], [10], [11], [12]. However, most of the works relies
    on solving optimization problems in the presence of perfect knowledge of the system
    parameters and the performance of edge devices. However, acquiring such knowledge
    might not be practically feasible in the live streaming systems. Particularly,
    existing works do not consider the risk of assigning a transcoding task to a device
    with highly unstable transcoding performance. This paper proposes a novel concept
    to quantify the edge devices’ transcoding capabilities leveraging their contextual
    information. This concept considers not only the average performance of the transcoders
    but also the risk of performance variations, which account for the design of a
    joint transcoding task assignment and viewer association algorithm. To the best
    of authors’ knowledge, this paper is the first work to consider both risk sensitivity
    and context awareness in an online task offloading problem. The contributions
    of this paper are summarised below: The joint transcoding task assignment and
    viewer association problem is formulated to maximize the long-term network utility
    considering the cost and average latency. The transcoding capability is proposed
    to identify proper transcoders with both high transcoding quality and low transcoding
    performance variation. The transcoding quality is modelled as a linear function
    of transcoder’s context information and the performance variation is represented
    as the variance of transcoding outcomes. To estimate and learn the unknown transcoding
    capability, two upper confidence bounds (UCBs) of the transcoding performance
    variation are derived. Applying the confidence bounds and by involving a contextual
    UCB of the transcoding quality, a risk-aware contextual online learning scheme
    is designed to learn the transcoding capabilities of edge devices. The idea of
    considering both the context and risk awareness helps designing efficient and
    robust task offloading schemes and reduce the risk of assigning tasks to devices
    that cannot ensure stable performance. Based on the learnt transcoding capabilities,
    a novel joint transcoding task assignment and viewer association algorithm is
    designed. Particularly, an epoch-based strategy is designed to reduce the switching
    costs of task assignments. Numerical results based on various settings demonstrate
    that the proposed algorithm achieves significant network utility improvement while
    keeping the switching costs of transcoding task assignments competitively low.
    The remainder of this paper is organised as follows. Section II discusses the
    related works. Section III describes the model of the edge transcoding system.
    In Section IV, an optimization problem is formulated to maximize the overall network
    utility. The designed risk-aware contextual learning for edge transcoding algorithm
    is described in Section V, followed by the numerical results in Section VI. The
    conclusions are drawn in Section VII. SECTION II. Related Work In this section,
    we review the works in both cloud and edge-based live video transcoding systems
    and discuss the challenges in designing the edge-based transcoding schemes. A.
    Cloud Transcoding Due to the powerful computing ability and the ‘pay as you go’
    feature of cloud computing, previous works tended to implement the transcoding
    system with the help of the cloud resources and designed various quality of experience
    (QoE) metrics for cloud transcoding systems [13], [14], [15], [16], [17], [18],
    [19], [20]. For instance, [13] designed a cloud-based scheme to transcode crowdsourced
    video contents. The QoE is a function of the bit rate of the received live stream
    and the broadcaster’s popularity. Reference [14] proposed a cloud transcoding
    scheme considering delay constraints. In this work, the QoE was defined as a non-decreasing
    concave function of the received bit rate. In [16], a new live streaming framework
    was designed to minimize the content delivery delay with cloud transcoding. Reference
    [17] proposed a cloud transcoding scheme for both delay-tolerant and delay-sensitive
    videos with different priorities. In [19], a scheme was designed to limit the
    peak power consumption while maximizing the total processing capacity in a server
    with heterogeneous processors using dynamic programming. Reference [20] designed
    recurrent network and convolutional network based approaches to forecast the approximate
    transcoding resources which is reserved for transcoding and to maximize the quality
    of service (QoS). B. Edge and Crowdsourced Transcoding Due to the abundance of
    concurrent live broadcasters and the heterogeneity of source contents, a substantial
    amount of transcoding tasks are generated which are delay-sensitive and computationally
    intense. As a result, even cloud transcoding cannot meet these requirements with
    affordable cost [21]. Therefore, edge computing has been considered as a viable
    replacement because of its fast processing and quick application response time
    [22]. However, it is highly challenging to achieve optimal transcoding task assignment
    and viewer association due to the massive heterogeneous video contents and diversified
    QoE demands [23]. In [24], a collaborative joint caching and transcoding scheme
    was proposed to reduce the backhaul link usage and the viewer perceived delay.
    In [9], a reinforcement learning (RL)-based scheme was designed to solve the edge
    transcoding decision-making problems. To better schedule edge transcoding under
    large state space, deep RL was used to explicitly accommodate personalized QoE
    optimization for CLSP services [20], [23], [25], [26], [27]. In addition, [21]
    combined both the cloud and the edge resources to collaboratively transcode live
    videos from multiple broadcasters. In [6], a case study was presented for Twitch
    demonstrating that with the advance of personal computing devices, a significant
    fraction of CLSP viewers’ devices potentially have appropriate computing resources
    for real-time transcoding. In addition, the viewers have already expressed the
    willingness to support the broadcasters and the CLSPs in terms of donation and
    subscription [21]. Thus, the cost by involving them into transcoding can be much
    lower as compared to general edge computing. These studies demonstrate the potential
    of incentivizing the viewer devices to do transcoding. However, since the viewer
    devices are not professional and can be highly heterogeneous, their performances
    may be unknown and unstable. In [7], the transcoder selection relies on the prior
    data collection and analysis, which may become inaccurate over time. Therefore,
    optimal online decision-making strategies which can learn devices’ performance
    and select devices which are more capable for transcoding are highly desirable.
    Such edge-assisted crowdsourced transcoding systems are similar with the crowdsourcing
    systems which exploit the collective intelligence of crowd, provide an effective
    paradigm for large-scale data acquisition and distributed computing [28], [29],
    [30]. In the edge-assisted transcoding system, the viewer devices assigned with
    transcoding tasks can be treated as the crowd workers for computing. The crowdsourcing
    system has been introduced into many areas such as text translation, consumer
    research, and hiring workers for software development. There have been extensive
    works on task assignment problems in the crowdsourcing systems [31]. As a classical
    decision-making model, multi-armed bandit (MAB) has been used to model the task
    assignment problems. For instance [32] proposed a UCB-based task assignment algorithm
    with a limited budget for crowdsensing. Reference [33] modelled the crowdsourcing
    system as an MAB and proposed a bounded ε -first algorithm to maximize the overall
    utility of completing a number of tasks. [34] proposed a budget-limited UCB-based
    greedy approach to learn the worker performance of a crowdsourcing system and
    to select workers with high performance to maximize the long-term utility. In
    [35], a hierarchical context-aware learning algorithm is proposed to learn and
    estimate the worker’s context-specific performance in mobile crowdsourcing. We
    have reviewed some recent papers in Table I on live video transcoding offloading
    with edge computing. Most works either consider the context information of the
    system and edge devices when making decisions [31], [36], or directly formulate
    the feedback of the decision as a function of context information. In [38], the
    risk of performance fluctuation is considered in the MAB problem. However, this
    work only studied a standard MAB that at each time only one arm will be played
    and the arm selection is not aware of the context information of each arm. In
    addition, there are limited works studying to consider the risk of decisions which
    can lead to high performance fluctuation. However, these works only consider known
    problem-specific risk such as the edge devices’ online stability and the probability
    of failure [7], [11], [39]. None of these works directly models feedback of decision
    as a function of risk to make the task offloading risk-aware. TABLE I Summary
    of Recent Works on Edge-Assisted Task Assignment and Live Video Transcoding Overall,
    although the task assignment problem in edge-assisted computing systems has been
    studied in recent years, which can be used in edge-assisted crowdsourced transcoding
    systems, several technical problems have not yet been addressed. First, the existing
    task assignment decision-making models are not practical enough since the tasks
    were assumed to arrive sequentially and the number of tasks need to be assigned
    per time slot is fixed. In addition, most of existing works only focus on identifying
    the device with the highest average performance for task offloading. Although
    some works considered risk in the edge computing framework, the risk is not integrated
    into the feedback of the decision. Moreover, the switching costs of assigning
    a task to different edge devices over time have not been considered yet. In particular,
    to the best of our knowledge, there is no work studying to combine risk and context
    awareness in the edge-assisted task assignment problem. SECTION III. System Model
    We consider an edge-assisted CLSP as illustrated in Figure 1. The raw live videos
    from the broadcasters are first uploaded to the regional data centers which are
    responsible for video transmission and transcoding task assignment. The pre-processed
    videos (e.g. segmented video chunks) with the original bit rates are then forwarded
    to edge transcoders and the transcoded videos will be transmitted to the end viewers.
    Table II summarizes the symbols used in this article. TABLE II Symbols and Notations
    Fig. 1. System model of the edge-assisted CLSP. Show All Define a set of broadcasters,
    i.e., I={1,2,⋯,I} . The bit rate of the original live video of broadcaster i∈I
    in time slot t is defined as B t i . Moreover, consider there are J t viewers
    in time slot t and V t i,j is a binary parameter indicating whether viewer j∈
    J t ={1,2,⋯, J t } chooses to watch broadcaster i ’s live stream in time slot
    t or not. In addition, denote y∈Y={1,2,⋯,Y} as a video representation which is
    one of Y possible standard quality levels of a transcoded video and the bit rate
    of representation y is defined as b y . When an original live video is uploaded
    by a broadcaster i to the regional data centers, the scheduler of CLSP will decide
    which representation should be transcoded by which edge device based on viewer
    requirements, the performance of the edge devices, and system constraints such
    as the experienced delay by users as well as the cost for transcoding. In the
    transcoding process, each edge device f∈F={1,2,⋯,F} is able to transcode the broadcasters’
    live videos into standard video versions where F represents the set of all available
    edge devices. After the transcoding process, all of the standard transcoded live
    videos will be transmitted from the edge devices to the associated viewers. In
    addition, it is assumed that a viewer can only watch one transcoded video from
    a broadcaster in the same time slot. To describe the transcoding task assignment
    and the viewer association, a binary variables is defined as I t i,y,f,j which
    takes 1 when edge device f is selected to transcode the original video from broadcaster
    i requested by viewer j into representation y in time slot t , and 0 otherwise.
    A. Cost Model To incentivize an edge device to participate in transcoding, we
    define the cost of transcoding one live video from broadcaster i to representation
    y at edge device f , which is paid by the CLSP for the edge device, as c t i,y,f
    , which is written as c t i,y,f = ∑ j∈J I t i,y,f,j ⋅ Φ y ⋅ ω t f , (1) View Source
    where Φ y is a non-decreasing concave function of representation y and ω t f is
    defined as the transcoding capability of device f in time slot t . A higher value
    of ω t f means the edge device is more reliable and it can transcode a live stream
    with higher quality and less delay. To encourage edge devices with higher transcoding
    capability to join the transcoding candidate pool, c t i,y,f is assumed to be
    linearly increasing with ω t f . The transcoding capability plays an important
    role which is not only related to the transcoding quality but also the transcoding
    performance uncertainty. In a nut shell, an edge device with relatively higher
    average performance and lower performance fluctuation is more capable for transcoding.
    The exact definition of transcoding capability will be presented in Section V.
    Based on the definition of c t i,y,f , the total cost related to broadcaster i
    (denoted by c t i ) can be defined as c t i = ∑ y∈Y ∑ f∈F c t i,y,f . (2) View
    Source B. QoE Model From the perspective of a viewer, the quality of the received
    video, namely, the received bit rate can greatly determine the viewer’s experience
    [14]. Therefore, we use the term QoE to denote how good the received video is.
    The QoE is determined by two factors. First, the acceptable quality levels of
    the received live videos of different broadcasters vary in terms of their genres
    (e.g., card game, pixel art game, first shooter game, etc). By categorizing the
    live videos into a set of genres denoted by G={1,2,…,K} and defining g t i ∈G
    as the genre of video from broadcaster i in time slot t , we can define s g t
    i as the suggested basic bit rate, according to the genre of broadcaster i , for
    viewer to watch the live video in time slot t . Second, it is vital to consider
    the network capacity of each viewer. Let u t j be the highest bit rate that viewer
    j can receive, which varies due to the viewer network condition, the QoE model
    can be expressed as q t i,j,y =log( b y u t j + b y s g t i ), (3) View Source
    where b y represents the bit rate of representation y . In (3), the QoE model
    is a non-decreasing concave function of two ratios. The first ratio quantifies
    the effect of the network condition of viewer j . The higher this ratio is, the
    better QoE can be achieved. However, this ratio should not exceed one, and a constraint
    is added to the optimization problem; Otherwise, the viewer capacity is smaller
    than the bit rate of the transcoded representation and this transcoded representation
    cannot be smoothly played at the viewer end. The second ratio quantifies how better
    the received video quality is compared with the basic genre rate of broadcaster
    i considering the fact that same representation from different genres of broadcasters
    can lead to different QoE levels. The QoE of a viewer j watching a transcoded
    video from broadcaster i can be calculated as Q t i,j = ∑ y∈Y ∑ f∈F I t i,y,f,j
    ω t f q t i,j,y . (4) View Source C. Delay Model Delay is another performance
    measure that should be taken into account for the optimal transcoding task assignment
    and viewer association for real-time processing and delay-sensitive applications
    in the edge-assisted CLSPs. The latency experienced by the viewers can be categorized
    into three types, i.e., transmission delay, transcoding delay, and playout delay.
    The transmission delay is referred to as the round trip time, including the broadcaster-transcoder
    delay and the transcoder-viewer delay. In the traditional cloud transcoding system,
    both the broadcasters and the viewers can be far from the cloud data center, which
    brings non-negligible latency. The transcoding delay is the processing delay of
    transcoding a live video to a different quality version. Normally, the transcoding
    delay can be calculated as the time difference between the input of original video
    and the output of the transcoded representation. The playout delay is determined
    by the viewer devices and their decoding time. Thus, it would not affect the the
    transcoding task assignment and the viewer association and hence not involved
    in this paper. Let define ξ t j and δ t j as the transmission delay and the transcoding
    delay experienced by viewer j in time slot t , respectively. The transmission
    delay can be expressed as ξ t j = ∑ i∈I ∑ y∈Y ∑ f∈F τ ~ t i,f,j I t i,y,f,j V
    t i,j , (5) View Source where τ ~ t i,f,j denotes the network delay from broadcaster
    i to viewer j via edge device f . Next, the transcoding delay can be represented
    as δ t j = ∑ i∈I ∑ y∈Y ∑ f∈F δ ~ i,y,f I t i,y,f,j V t i,j , (6) View Source where
    δ ~ i,y,f represents the transcoding delay for edge device f to transcode an original
    live video with bit rate B t i to a representation with bit rate b y . Therefore,
    the overall latency in the proposed transcoding system in time slot t experienced
    by the viewer j can be represented as D t j = ξ t j + δ t j . (7) View Source
    SECTION IV. Optimization Problem: Edge Transcoding and Viewer Association According
    to the system model formulated in the previous section, there is a tradeoff between
    QoE maximization and cost minimization imposed on CLSP. On one hand, CLSP prefers
    to incentivize more edge devices to participate in transcoding and provide ABR
    service to more viewers. The more I t i,y,f,j is set to one (i.e., a larger number
    of edge devices is selected for transcoding), the higher QoE can be gained. On
    the other hand, this will lead to higher cost based on (1). Therefore, the binary
    indicators must be optimized carefully to balance the tradeoff between the QoS
    and the cost as two components of the network utility. To formalize such a tradeoff,
    we define the weighted-difference between the QoE and cost (which is referred
    to as the network utility) related to a broadcaster as U t i = ∑ j∈ J t V t i,j
    Q t i,j −λ⋅ c t i , (8) View Source where the parameter λ is used to tune the
    tradeoff between the two components. Aiming to jointly optimize the transcoding
    task assignment and viewer association by maximizing the total network utility
    in each time slot over the whole transcoding system. We therefore, formulate an
    optimization problem as (P)  max I t i,y,f,j  ∑ t=1 T ∑ i∈I U t i , s.t. C1: V
    t i,j I t i,y,f,j b y ≤min{ u t j , B t i } , C2: ∑ y∈Y ∑ f∈F I t i,y,f,j ≤1,
    ∀i∈I, ∀j∈ J t , C3: ∑ y∈Y ∑ i∈I ∑ j∈ J t I t i,y,f,j ≤M,∀f∈F, C4: I t i,y,f,j
    ∈{0,1},∀y∈Y, ∀i∈I,∀f∈F, ∀j∈ J t , C5: D t j ≤ D th ,∀j∈J, (9a) (9b) (9c) (9d)
    (9e) (9f) View Source where C1 makes sure the received bit rate is lower than
    both the original video bit rate ( B t i ) and the viewer capacity. C2 ensures
    that a viewer can only play one representation from one broadcaster in each time
    slot. C3 guarantees that each transcoder can only serve M viewers at most due
    to the limited bandwidth resource. C4 guarantees that variable I t i,y,f,j is
    binary. C5 ensures that for every viewer, the experienced delay of each viewer
    is lower than a predefined threshold D th . The formulated problem is a linear
    integer programming problem which can be efficiently solved by an optimization
    toolbox called Mosek [40]. However, the transcoding capabilities of transcoders
    are required to solve P , that are unknown in real live streaming systems. Therefore,
    an online learning scheme is highly demanded. SECTION V. Risk-Aware Contextual
    Learning for Edge Transcoding In light of the proposed system model in Section
    III, in this section, a novel risk-aware learning algorithm is designed to learn
    the transcoding capabilities (i.e., ω t f ) online leveraging contextual information.
    Then, a novel transcoding task assignment and viewer association algorithm is
    designed to maximize the network utility of the transcoding system. A. Risk-Aware
    Contextual MAB Since the aim of the edge-assisted transcoding system is to select
    a number of edge transcoders per time slot to maximize the cumulative network
    utility, this problem can be modelled using a bandit framework, where the arms
    are the edge devices and the rewards are the transcoding outcomes of selected
    edge devices. We model the transcoding outcome of a task assigned to edge device
    f in time slot t as a random variable, denoted by r t f , for which the statistical
    properties are unknown. Suppose γ t f =E[ r t f ] represents the expected performance
    of transcoding for edge device f , where E[.] is the mathematical expectation.
    Define γ t f as ‘transcoding quality’ of device f . Moreover, σ 2 f =Var[ r t
    f ] is the variance of the transcoding outcome. When the variation of transcoding
    outcome of a transcoder ( σ 2 f ) is large, the transcoder can still perform poorly
    even with high transcoding quality. This is unaffordable and risky. The risk is
    defined related to the performance fluctuations of the transcoder devices. In
    particular, the high risk represents the case when the transcoder has a large
    performance variation (i.e., transcoding outcomes with high variance). For instance,
    choosing a transcoder with high uncertainty can lead to unacceptable transcoding
    delay and severely deteriorate the viewer experience. Besides, choosing a more
    risky transcoder can lead to frequent transcoding task switches, which means the
    same transcoding task will be assigned to different edge transcoders and results
    in high communication overhead and playback latency. These problems reflect the
    importance of considering the performance uncertainty of transcoders. Such a risk
    can occur due to the unexpected unavailability of transcoders. This happens in
    edge-assisted crowdsourced live streaming since transcoder devices considered
    in our work are assumed to be edge viewers’ devices, which are not specifically
    employed for live video transcoding. The risk can also originate from the unstable
    computational and transmission resources of the edge transcoders. Particularly,
    a sudden high transmission error or a low transmission rate can aggravate the
    riskiness of edge transcoders as well. Therefore, we model the transcoder selection
    problem as a risk-aware MAB for which the objective is to balance the tradeoff
    between maximizing the expected value of returned transcoding outcomes and minimizing
    the variance of the transcoding outcomes. In particular, we define the transcoding
    capability as the mean-variance measure of each transcoder, which can be written
    as ω t f =ρ γ t f − σ 2 f , (10) View Source where ρ>0 is the risk-tolerance factor
    introduced to balance the tradeoff between a high reward and a low risk. This
    linear combination of the transcoding quality and the variance of the transcoding
    outcome in fact defines the transcoding capability of edge device f . To learn
    the transcoding capabilities of each edge transcoder online, in the following,
    we propose an index-based MAB algorithm by analytically driving the UCB of γ t
    f and σ 2 f . 1) Contextual UCB for Transcoding Quality: The transcoding quality
    ( γ t f ) of an edge device is dependant on various factors such as the device
    computational power, network conditions, online stability etc. Such factors will
    form the contextual information of a device as a transcoder. For example, since
    the viewer devices are not specifically implemented for video transcoding and
    can switch offline during transcoding [6], online stability of a device would
    affect the transcoding quality. Besides, the computational power and network condition
    of an edge device can also affect the transcoding outcome by incurring varying
    latency which can be experienced at the viewer end. Therefore, we model the transcoding
    quality of a transcoder as the linear combination of its contextual information
    and a vector of unknown coefficients θ ∗ . Consequently, the transcoding quality
    can be represented as γ t f =E[ r t f | x t f ]=( x t f ) ⊤ θ ∗ , (11) View Source
    where x t f represents the z -dimensional contextual information of edge transcoder
    f , and θ ∗ denotes the z -dimensional unknown coefficients which can be treated
    as the weight of each contextual information. In addition, the number of the contextual
    information types is defined as z . Collecting samples of the transcoding outcomes
    and the contextual information through task assignments over time, the unknown
    coefficients θ ∗ can be learned. Learning the coefficients belong to a linear
    regression problem and it can be solved by ridge regression [41], which adds L2
    regularization to the lost function. Ridge regression is a suitable technique
    to solve the linear regression problem when the number of samples is highly limited,
    which fits the situation of the crowdtranscoding system, since the samples are
    collected in an online form and there are only limited samples in the early stage.
    Define R t as the set of transcoding outcomes till time slot t , with the number
    of transcoding outcomes as m t . Let W t be a design matrix of dimension m t ×z
    whose rows correspond to the observations of contextual information of m t transcoding
    outcomes till time slot t and columns correspond to the z types of the contextual
    information. According to [42], we can acquire the estimated coefficients θ ~
    t by ridge regression as θ ~ t =(( W t ) ⊤ W t + I z ) −1 ( W t ) ⊤ R t , (12)
    View Source where I z is the z -dimensional identity matrix. Let define the estimated
    transcoding quality as γ ~ t f . Based on the learned knowledge of the unknown
    coefficients, we can update the estimated transcoding quality using the contextual
    information as γ ~ t f =( x t f ) ⊤ θ ~ t . (13) View Source For the UCB of the
    transcoding quality ( γ t f ), according to [43], for any κ>0 , with the probability
    of at least 1−κ/T , the deviation between the estimated transcoding quality and
    the real transcoding quality can be upper bounded by |( x t f ) ⊤ θ ~ t −( x t
    f ) ⊤ θ ∗ |≤(ϕ+1) ( x t f ) ⊤ A t −1 x t f − − − − − − − − − − − √ , (14) View
    Source where A t = I z +( W t ) ⊤ W t and ϕ= 1 2 ln 2TF κ − − − − − − − √ . This
    UCB can help to estimate the real transcoding quality of each transcoder, which
    holds with a high probability. 2) UCB for Variance: In order to estimate the UCB
    of the transcoding outcome’s variance ( σ 2 f ), we first define the empirical
    variance ( s t f ) 2 of the transcoding outcome of edge device f until time slot
    t as ( s t f ) 2 = 1 τ t f −1 ∑ d=1 τ t f ( r t f (d) f − r ¯ t f ) 2 , (15) View
    Source where r ¯ t f represents the empirical mean of transcoding outcome until
    t , t f (d) represents the time slot when the d th transcoding outcome of device
    f is observed, and τ t f denotes the number of times that transcoder f has been
    chosen till t . Fact 1: Let X be a Gaussian random variable with variance σ 2
    . Define the empirical variance over n samples as s 2 n , based on [44] we have
    Pr{ σ 2 ≤ v n }=1−α, (16) View Source where v n = (n−1) s 2 n χ 2 1−α,n−1 and
    χ 2 1−α,n−1 is the upper 100α percentage points of the chi-square distribution
    with (n−1) degrees of freedom. Fact 1 gives a definition of the confidence interval
    of the variance of a random variable when the variable follows the Gaussian distribution.
    It implies that there is a probability of 100(1−α)% that the constructed confidence
    interval based on the sample variance will contain the true value of σ 2 . According
    to Fact 1, a UCB of the variance of a random variable is proposed when the variable
    follows the Gaussian distribution. Therefore, based on Fact 1, we can derive the
    UCB of σ 2 f under assumption that r t f is normally distributed. Lemma 1:Given
    the UCBs of both the mean and the variance of the transcoding outcomes based on
    (14) and (16), the contextual Gaussian risk-aware UCB (CGRA-UCB) of transcoding
    capability of the transcoder f can be written as ω ~ t f =ρ( γ ~ t f +(ϕ+1) (
    x t f ) ⊤ A t −1 x t f − − − − − − − − − − − √ )− v τ t f , (17) View Source where
    v τ t f = ( τ t f −1)( s t f ) 2 χ 2 1−a, τ t f −1 . The first term in the RHS
    of (17) represents the UCB of the transcoding quality and the second term reflects
    the variance of the transcoding outcome. In (17), τ t f represents the number
    of transcoding tasks which is assigned to transcoder f till time slot t and (
    s t f ) 2 denotes the empirical variance of the transcoding outcome of transcoder
    f at time slot t . The bound in (17) is designed under the assumption that r t
    f follows an independent Gaussian distribution. However, when the reward distribution
    is unknown, the confidence interval presented in (1) is not pertinent. To overcome
    this limitation, we utilize the asymptotic distribution of the empirical variance
    to drive a confidence interval without any prior assumption of the reward distribution.
    Fact 2: Let X be a continuous random variable with mean μ , variance σ 2 , and
    μ 4 =E[(X−μ ) 4 ] . According to [45] and [38], the asymptotic distribution of
    the empirical variance is n − − √ ( s 2 n − σ 2 )→N(0, μ 4 − σ 4 ). (18) View
    Source Based on Fact 2, in the following lemma, we develop an asymptotic UCB on
    the variance. Lemma 2:Applying Fact 2, define the UCB of the reward variance as
    v upper n , for a sufficiently large n , an asymptotic confidence interval of
    the variance can be derived as Pr{ σ 2 ≥ v upper n }≤α, (19) View Source where
    v upper n = n s 2 n + n χ 2 α,1 ( μ 4 − s 4 n )+ ( χ 2 α,1 ) 2 μ 4 √ n+ χ 2 α,1
    and s 4 n is the empirical estimate of σ 4 . Proof:See Appendix A. Since μ 4 is
    unknown, an estimate of μ 4 is required. We approximate μ 4 as μ ~ n 4 = 1 n ∑
    n d=1 ( r d − μ ¯ ) 4 , where r d represents the random reward. When the distribution
    of r t f is unknown, by setting n= τ t f , we can estimate the UCB of σ 2 f according
    to Lemma 2, which can be calculated as v upper τ t f = τ t f s 2 τ t f + τ t f
    χ 2 α,1 ( μ ~ f 4 − s 4 τ t f )+ ( χ 2 α,1 ) 2 μ ~ f 4 − − − − − − − − − − − −
    − − − − − − − − − − − − √ τ t f + χ 2 α,1 , (20) View Source where μ ~ f 4 =1/
    τ t f ∑ τ t f d=1 ( r t f (d) f − r ¯ t f ) 4 . Lemma 3:Given the UCBs of both
    the mean and the variance of the transcoding outcomes based on (14) and Lemma
    2, we can build a new contextual asymptotic risk-aware UCB (CARA-UCB) of transcoding
    capability, which can be written as ω ~ t f =ρ( γ ~ t f +(ϕ+1) ( x t f ) ⊤ A t
    −1 x t f − − − − − − − − − − − √ )− v upper τ t f . (21) View Source B. Transcoder
    Selection Algorithm With the learnt transcoding capability and based on either
    (17) or (21), to assign the transcoding tasks to the edge devices which are expected
    to return relatively high reward and are less risky, we need to solve an instantaneous
    version of the optimization problem P in (9). The instantaneous optimization problem
    at time-slot t can be formulated as ( P ^ )  max I t i,y,f,j  ∑ i∈I U t i ,  s.t.  C1−C5.
    (22a) (22b) View Source The instantaneous optimization problem in (22) will be
    solved whenever new bounds of transcoding capabilities of edge devices are available.
    After the task assignment, the UCB estimations can be updated based on the observed
    transcoding outcomes. However, in order to learn the transcoding capability, simply
    assigning one transcoding task to different edge transcoders in different time
    slots is not efficient, because assigning a transcoding task to different transcoders
    frequently can lead to unaffordable task-switching costs and further increase
    the communication overheads. To deal with this problem, according to (12), we
    noticed that whichever transcoder is selected can contribute in collecting information
    about the coefficients vector θ ∗ , thus can further guide the learning process
    of the transcoding capability. Therefore, instead of determining the task assignment
    and viewer association in each time slot, we investigate an epoch-based sampling
    strategy which means a transcoding is consistently assigned to the same edge device
    for a finite number of time slots (which is referred as an epoch) and the task
    reassignments are only proceeded once at the beginning of each epoch. With this
    strategy, we can greatly reduce the switching costs while keep learning the transcoding
    capability. The effectiveness of the epoch-based sampling depends on a well-designed
    epoch length [46], which is supposed to increase as time continues. Given τ t
    f as the task assignment counter of a edge device f , define F t as the set of
    edge devices whose assigned task numbers are more than a certain threshold till
    time slot t , which can be written as F t ={f: τ t f ≥ t ζ logt}, (23) View Source
    with ζ>0 is the threshold factor. Define the smallest counter as τ t min = min
    f∈ F t τ t f , (24) View Source the length of an epoch till time slot t can be
    calculated as E t =⌈(1+ϵ ) τ t min ⌉, (25) View Source where ϵ>0 . The detailed
    risk-aware contextual transcoding task assignment and viewer association algorithm
    is described in Algorithm 1. Since the time slots have been divided into epochs,
    we only need to solve the optimization problem per epoch. Thus, the computational
    cost can be greatly reduced. Algorithm 1 Risk-Aware Contextual Transcoding Task
    Assignment and Viewer Association Algorithm Require: ζ , ρ , x t f , ϕ for t←1
    to T do if Current epoch ends then Update τ t f and τ t min Calculate the epoch
    length E t based on (23), (24), and (25) for f←1 to F do Calculate the estimated
    transcoding capability ω ~ t f based on (17) or (21) end for Solve optimization
    problem P ^ to get I t i,y,f,j Execute self-inspection and modify I t i,y,f,j
    accordingly else Maintain the same assignment, I t i,y,f,j ← I t−1 i,y,f,j end
    if Observe the transcoding outcomes Update θ ~ t based on (12) end for In addition,
    according to the optimization problem P ^ , multiple transcoding tasks can be
    assigned to the same transcoder. However, the computational resources of the transcoders
    are limited and performing excessive transcoding tasks on one transcoder concurrently
    can lead to soaring transcoding delay and exhaust the bandwidth resources. Therefore,
    to avoid overwhelming the transcoders, every time the optimization variable I
    t i,y,f,j is calculated, a self-inspection process is executed at every selected
    transcoder and any transcoder assigned with excessive tasks will offload these
    tasks to the cloud data center for transcoding. C. Computational Complexity Analysis
    The computational complexity of the proposed algorithm in each epoch consists
    of two parts. The first part of complexity originates from the ridge regression
    where matrix inversion and multiplication are introduced. The computational complexity
    of this method scales as O( z 2 m t ) , where z is the dimension of context space
    and m t is the number of transcoding outcomes till time slot t . Since the dimensionality
    of the context information is assumed to be fixed, m t will dominate the computational
    complexity and the complexity only grows linearly in terms of the number of transcoding
    outcomes. The second part of complexity comes from solving the instantaneous optimization
    problem P ^ (22) which is a 0–1 linear integer programming problem. According
    to [47], the computational complexity of such a problem is O( 2 L kL) where L
    is the number of optimization variables and k is the number of constraints. In
    our problem, we have L t =IFY J t and k= J t +F+2I J t where J t , Y , F , and
    I represent the numbers of viewers, representations, edge devices, and broadcasters,
    respectively. Combing both parts, the computational complexity at the time slot
    t is O( z 2 m t + 2 L t k L t ) . SECTION VI. Simulation Results A. Simulation
    Setup We test the proposed algorithm with a synthetic data set, which is based
    on the real-world settings. We assume a live stream transcoding system with 4
    broadcasters, 50 viewers, 15 edge devices and 4 representations. The viewer count
    of each broadcaster live stream is decided by its popularity. The popularity is
    modelled by Zipf distribution which is normally used for video content popularity
    modelling (e.g., [48]). We set the original live video rate and the representation
    rates according to the twitch broadcaster settings [49]. The original rates for
    four broadcasters are set as 4000kbps, 2500kbps, 1500kbps and 500kbps. The specific
    bit rates of the four representations are set to be 400kbps (240P), 1200kbps (480P),
    2000kbps (720P), and 3500kbps (1080P). Moreover, we randomly set the viewer capacity
    in the range of [500,4000] kbps. Based on the system model, a transcoding task
    can be assigned to multiple edge devices to serve different viewers. Since the
    transcoders are at network edge which is close to the viewers, the edge devices
    and the viewers are assumed to be distributed in a 1000 meters × 1000 meters region,
    and their locations are randomly determined following a uniform distribution.
    According to [6], the transcoding capability can be affected by transcoder’s computational
    power. Besides, since the candidate viewers can also undertake transcoding tasks
    and the viewers with low stability can be offline during transcoding, the online
    stability of the transcoders should be considered as a factor of transcoding capability.
    Based on [7], the online stability is generated by sampling the Pareto distribution.
    As a result, we choose the CPU mark, the average CPU usage, the average RAM usage
    and the online stability as the contextual information of edge transcoders. As
    discussed in [9], the transcoding delay is calculated based on the required computational
    resources of a task and the available CPU cycles (determined by the CPU mark and
    usage) of the transcoder. For the transmission delay, it can be divided into two
    parts as discussed in Section III-C. The broadcaster-transcoder delay is randomly
    set in the range of [200,300] ms according to [14], and the transcoder-viewer
    delay is set in the range of [0,100] ms depending on the distance between a viewer
    and a edge device. To be more specific, this delay can be calculated as the distance
    between transcoder and viewer times 100 ms. In other words, this setting implicitly
    considers the average channel gain and transmission rate which are functions of
    the distance between a viewer and an edge device. To demonstrate the risk-awareness
    of the designed algorithm, the variance of an edge transcoder follows a uniform
    distribution within the range of [0,1] . Finally, to test the performance of the
    designed algorithm, both Gaussian and Gamma distributions are simulated to generate
    the transcoding outcomes. The parameters are uniformly selected. The LinUCB algorithm
    [50] which utilizes the contextual information to estimate the reward is simulated
    as the benchmark. In addition, the MV-UCB algorithm [51] which is a risk-aware
    MAB algorithm, is also simulated for comparison. Table III describes the features
    of the proposed algorithm and the benchmarks. TABLE III Comparison With Benchmarks
    on Risk Sensitivityand Context Awareness TABLE IV Simulation Parameters Figure
    2a presents the initial transcoding quality and the variance of 15 transcoders.
    By sorting the transcoders in the descending order in terms of the transcoding
    capability with varying risk-tolerance factor ρ , Figures 2b–2d are generated.
    The risk-tolerance factor is set to decrease from 3 to 0.1. In Figure 2b, a larger
    risk-tolerance factor makes the transcoding quality dominate the transcoding capability,
    which represents a risk-neutral setting of transcoders. In Figure 2d, the risk-tolerance
    factor is set to be close to 0, which leads to a pure-risk setting. We can observe
    that the transcoding capability of each transcoder can be quite different. In
    Figure 2c, we set ρ=1 , which leads to another order of transcoders. In this figure,
    both transcoding quality and the variance can contribute to the capability, and
    the transcoders which are relatively more capable are quite different from both
    previous settings ( ρ=0.1 or 3). This setting highlights the importance of considering
    impacts from both the transcoding quality and the variance. We set ρ=1 in the
    following simulations, and the results are collected from 30 Monte Carlo (MC)
    runs. Fig. 2. Transcoding quality and variation given varying risk-tolerance factor
    ρ . Show All B. Numerical Results We first evaluate the proposed algorithm under
    the Gaussian distribution scenario as compared to the benchmarks. In Figure 3,
    the network utilities per time slot achieved by all three algorithms are presented.
    It is shown that the proposed algorithm using CGRA-UCB outperforms the LinUCB
    and MV-UCB because it not only utilizes the contextual information to learn the
    transcoding quality but also considers the uncertainty of the transcoder’s transcoding
    outcome. In addition, the cumulative network utilities are depicted in Figure
    4. This also confirms the superiority of the proposed algorithm in comparison
    with the benchmarks. Moreover, in Figure 5, the cumulative switching costs are
    presented and the proposed algorithm achieves up to 85.1% cumulative switching
    costs as compared to the benchmarks. This is because the proposed algorithm does
    not solve the optimization problem P ^ per time slot so that the task assignment
    and viewer association will not change frequently. Fig. 3. Network utility versus
    time: Gaussian reward case. Show All Fig. 4. Cumulative utility versus time: Gaussian
    reward case. Show All Fig. 5. Switching costs versus time: Gaussian reward case.
    Show All In Figures 6, 7, and 8, Gamma distribution is used to generate the transcoding
    outcomes. In this case, we simulate for 200 time slots since the used bound (derived
    in Lemma 3) is an asymptotic bound and its accuracy increases as more transcoding
    outcomes are collected, which takes longer time to converge. From the results,
    we can find that after 150 time slots, the proposed algorithm with the CARA-UCB
    tends to converge and shows good performance. The results demonstrate that the
    proposed algorithm using CARA-UCB achieves a higher network utility while reducing
    up to 86.8% switching costs as compared to the benchmarks. Fig. 6. Network utility
    versus time: Gamma reward case. Show All Fig. 7. Cumulative utility versus time:
    Gamma reward case. Show All Fig. 8. Switching costs versus time: Gamma reward
    case. Show All The experienced average latency per viewer of each algorithm is
    presented in Table V. According to the results, we can find that the average delays
    of the proposed algorithm are slightly higher in both scenarios, although still
    within the delay threshold. The delay thresholds in both simulations are set to
    1.3 seconds, which demonstrates that the proposed algorithm can improve the network
    utility of the transcoding system while satisfying the delay constraint. TABLE
    V Averaged Delay in Second The running time of the proposed algorithm with and
    without the designed epoch-based strategy is presented in Table VI. Here without
    the epoch-based strategy means the optimization is solved in every time slot.
    The simulation is based on Gaussian reward setting with 100 time slots and the
    results are averaged over 30 MC runs. The results demonstrate that the epoch-based
    strategy can greatly reduce the running time by 92.3% since the optimization problem
    is solved much less frequently and the number of reassignment needed is much smaller.
    TABLE VI Running Time of the Proposed Algorithm In order to study the impact of
    the epoch-based sampling strategy, ζ is changed to generate different thresholds
    based on (23), which will help to determine the length of epoch according to (24)
    and (25). In Figure 9, both the cumulative switching costs and the total network
    utilities versus ζ are presented. We can observe that by decreasing ζ , the network
    utility can be increased at the expense of higher switching costs, since when
    ζ is decreased, the epoch length will increase more slowly and the optimization
    problem P in (9) will be solved more frequently. This figure demonstrates the
    importance of selecting a proper ζ to balance the tradeoff between the switching
    costs and the network utility. Fig. 9. Transcoding performance versus ζ . Show
    All In Figures 10, 11, and 12, three different epoch length determination strategies
    are evaluated in the Gaussian reward setting. The proposed transcoder selection
    algorithm calculates the epoch length based on the smallest task assignment counter
    of the edge transcoder via (23)–(25). As benchmarks, two more cases are simulated
    using the average of counters and the largest counter to calculate the epoch length,
    respectively. The results reveal that all three strategies perform well and show
    fast convergence thanks to the proposed refined UCBs of the transcoding capability.
    Particularly, the proposed strategy based on the minimum counter achieves the
    highest network utility. This confirms that the designed strategy can efficiently
    identify suitable transcoders to maximize the network utility, since it can offer
    more chances for exploring the transcoding capability of each transcoder and help
    to refine the bounds more frequently, which helps to identify the most suitable
    transcoders efficiently. In addition, using the maximum counter to calculate the
    epoch length achieves competitively low switching costs since this strategy tends
    to increase the epoch length faster than others, which can reduce the number of
    task reassignments. Fig. 10. Network utility versus time. Show All Fig. 11. Cumulative
    network utility versus time. Show All Fig. 12. Switching costs versus time. Show
    All To further demonstrate the performance of the proposed algorithm, we present
    the transcoding performances with and without the knowledge of the transcoding
    capabilities of edge transcoders under the Gaussian distribution scenario. In
    Figure 13, the cumulative network utilities and switching costs are presented.
    This result shows that the proposed scheme can learn the transcoding capability
    quickly and achieve a highly competitively network utility as compared to the
    case when the transcoding capability is known. In addition, with known transcoding
    capabilities, a lower switching costs can be achieved since the suitable transcoders
    can be quickly identified and the transcoding task assignment will not be changed
    frequently. Fig. 13. Transcoding performance versus ζ . Show All Finally, we have
    compared the proposed edge-assisted transcoding algorithm with the Top-N scheme
    which is a currently-running cloud transcoding scheme in Twitch.TV. Top-N offers
    N premium broadcasters with the ABR service but only the basic representation
    rate is available for the rest of the broadcasters. Normally N is determined based
    on the broadcaster’s popularity. The network utility and cumulative network utility
    of both the proposed algorithm and the Top-N scheme are presented in Figures 14
    and 15. Fig. 14. Network utility versus time. Show All Fig. 15. Cumulative network
    utility versus time. Show All Since Top-N is based on the cloud transcoding, we
    assume it can ensure highly stable viewer QoE and we set the transcoding capability
    to be 1 which is higher than the most capable edge-transcoder (whose transcoding
    capability is 0.496). In addition, we set the unit cost of transcoding to be 10
    times the cost of edge transcoding. Based on the results, we can find that as
    N increases, the utility of cloud transcoding increases. In particular, the proposed
    edge-assisted transcoding algorithm can utilize edge computing resources efficiently
    and achieve highly competitive network utility. SECTION VII. Conclusion In this
    paper, we proposed an edge-assisted transcoding task offloading algorithm for
    CLSP, considering the contextual information and the risk of performance variations
    of the edge devices. First, an optimization problem was formulated to solve the
    transcoding task assignment and viewer association problem under the assumption
    of known transcoding capabilities of edge devices. Then, two risk-sensitive bandit
    algorithms are developed to deal with the exploration-exploitation dilemma and
    to learn the transcoding capabilities. An epoch-based assignment strategy was
    introduced to reduce the switching costs of transcoding task assignment. Numerical
    results based on various settings confirm that the proposed risk-aware contextual
    algorithms can achieve superior performances as compared to different benchmark
    schemes that are either contextual or risk-sensitive. In a nutshell, leveraging
    both contextual awareness and risk sensitivity can improve resilience and robustness
    of an online task offloading scheme. In future, we will extend the proposed algorithm
    by considering a larger scale problem with extremely high live video quality (such
    as 4K and 8K) and with different behaviors of edge devices. ACKNOWLEDGMENT For
    the purpose of open access, the author has applied a Creative Commons Attribution
    (CC BY) licence to any Author Accepted Manuscript version arising. Appendix The
    distribution presented in Fact 2 can be transformed into a standard Gaussian distribution
    as n − − √ ( s 2 n − σ 2 ) μ 4 − σ 4 − − − − − − √ →N(0,1). (26) View Source Therefore
    we have n ( s 2 n − σ 2 ) 2 μ 4 − σ 4 → χ 2 1 . Consequently, the one-sided confidence
    interval is defined as Pr{ n ( s 2 n − σ 2 ) 2 μ 4 − σ 4 ≤ χ 2 α,1 }=1−α. (27)
    View Source As a result, the (1−α) asymptotic confidence interval of the variance
    σ 2 is established as Pr{ v lower n ≤ σ 2 ≤ v upper n }=1−α, (28) View Source
    where v lower n = n s 2 n − n χ 2 α,1 ( μ 4 − s 4 n )+ ( χ 2 α,1 ) 2 μ 4 √ n+
    χ 2 α,1 . From (28), obviously we have Pr{ σ 2 ≥ v upper n }≤α , which completes
    the proof. Authors Figures References Citations Keywords Metrics More Like This
    Quality of Experience Framework for Cloud Computing (QoC) IEEE Access Published:
    2018 Collaborative Mobile Edge and Cloud Computing: Tasks Unloading for Improving
    Users’ Quality of Experience in Resource-Intensive Mobile Applications 2019 IEEE
    4th International Conference on Computer and Communication Systems (ICCCS) Published:
    2019 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Journal on Selected Areas in Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Risk-Aware Contextual Learning for Edge-Assisted Crowdsourced Live Streaming
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Alshehri S.
  - Bamasaq O.
  - Alghazzawi D.
  - Jamjoom A.
  citation_count: '4'
  description: The Internet of Things (IoT) is vulnerable to leakage of private information
    during data sharing. To avoid this problem, access control and secure data sharing
    have been introduced in IoT; however, many challenges are faced because of centralized
    access control and single delegator selection. Additionally, blockchain is integrated
    into IoT to enhance the security of the environment. For that purpose, this research
    proposes dynamic secure access control using the blockchain (DSA-Block) model,
    which performs secure access control and data sharing. Initially, the IoT device
    attributes and user attributes are registered at a local domain authority (LDA)
    for generating private and public keys using the hyperelliptic curve cryptography
    (HECC) algorithm, which ensures the legitimacy of the users and devices. Then,
    the IoT devices send a request message to the edge nodes (ENs) via a gateway,
    which performs request filtration by validating the user's authenticity. The filtered
    requests are sent to the edge server to perform access delegation using rock hyraxes
    swarm optimization (RHSO), which selects a set of delegator nodes. The access
    control decision is made by using the Trusted practical Byzantine fault tolerance
    (PBFT) consensus algorithm. The IoT data are stored in the cloud server for secure
    storage, in which the data are secured using a differential privacy mechanism.
    Finally, dual revocations, such as user attribute revocation and user revocation,
    are used to maintain security. The performance of DSA-Block is evaluated and the
    results demonstrate that the proposed DSA-Block model achieves superior performance
    compared to previous works.
  doi: 10.1109/JIOT.2022.3217087
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Internet of Things
    Journal >Volume: 10 Issue: 5 Dynamic Secure Access Control and Data Sharing Through
    Trusted Delegation and Revocation in a Blockchain-Enabled Cloud-IoT Environment
    Publisher: IEEE Cite This PDF Suhair Alshehri; Omaimah Bamasaq; Daniyal Alghazzawi;
    Arwa Jamjoom All Authors 5 Cites in Papers 2246 Full Text Views Open Access Under
    a Creative Commons License Abstract Document Sections I. Introduction II. Literature
    Survey III. Preliminaries IV. Problem Statement V. System Model Show Full Outline
    Authors Figures References Citations Keywords Metrics Abstract: The Internet of
    Things (IoT) is vulnerable to leakage of private information during data sharing.
    To avoid this problem, access control and secure data sharing have been introduced
    in IoT; however, many challenges are faced because of centralized access control
    and single delegator selection. Additionally, blockchain is integrated into IoT
    to enhance the security of the environment. For that purpose, this research proposes
    dynamic secure access control using the blockchain (DSA-Block) model, which performs
    secure access control and data sharing. Initially, the IoT device attributes and
    user attributes are registered at a local domain authority (LDA) for generating
    private and public keys using the hyperelliptic curve cryptography (HECC) algorithm,
    which ensures the legitimacy of the users and devices. Then, the IoT devices send
    a request message to the edge nodes (ENs) via a gateway, which performs request
    filtration by validating the user’s authenticity. The filtered requests are sent
    to the edge server to perform access delegation using rock hyraxes swarm optimization
    (RHSO), which selects a set of delegator nodes. The access control decision is
    made by using the Trusted practical Byzantine fault tolerance (PBFT) consensus
    algorithm. The IoT data are stored in the cloud server for secure storage, in
    which the data are secured using a differential privacy mechanism. Finally, dual
    revocations, such as user attribute revocation and user revocation, are used to
    maintain security. The performance of DSA-Block is evaluated and the results demonstrate
    that the proposed DSA-Block model achieves superior performance compared to previous
    works. Published in: IEEE Internet of Things Journal ( Volume: 10, Issue: 5, 01
    March 2023) Page(s): 4239 - 4256 Date of Publication: 26 October 2022 ISSN Information:
    DOI: 10.1109/JIOT.2022.3217087 Publisher: IEEE Funding Agency: CCBY - IEEE is
    not the copyright holder of this material. Please follow the instructions via
    https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and
    stipulations in the API documentation. SECTION I. Introduction The emergence of
    the Internet of Things (IoT) technology in recent times has provided connectivity
    between several devices to facilitate various applications, such as smart homes,
    smart manufacturing, and smart transportation [1], [2]. The privacy and security
    issues associated with this development necessitate a mechanism for controlling
    these devices [3]. Access control is found to be the most significant mechanism
    to control the interventions of illegitimate and unauthorized users [4], [5].
    Several access control schemes have been proposed to control the access to resources,
    but these approaches rely on a centralized entity, which results in a single point
    of failure [6]. The adversaries in the network compromise the centralized server
    to redefine the access policies in their favor. Moreover, the data stored in the
    compromised centralized entity represent serious privacy threats, as the personal
    information of the IoT users, such as location and surveillance data are exposed
    to the malicious adversary [7]. The decentralized access control in the IoT network
    is provided by the blockchain technology which mitigates the limitations of centralized
    architectures and provides services for large-scale scenarios [8], [9]. Multiauthority-based
    access control is applied in several existing works to improve secure data sharing,
    however, it leads to significant privacy issues [10], [11], [12]. Several existing
    approaches integrate the blockchain to provide decentralized access controls by
    storing the access policies in the blockchain node and making decisions based
    on the access policies [13]. In these approaches, the blockchain node acts as
    a delegator and is used to process the overall resource requirements of the IoT
    entities [14]. The selection of a single blockchain node to provide access control
    to devices in the IoT network represents various threats, as follows. Security
    Threats: The single blockchain node that is responsible for validation of all
    user requests is vulnerable to various Distributed Denial-of-Service (DDoS) attacks
    such as when the malicious adversary produces a large volume of access requests
    in a periodical manner to waste the resources of the blockchain node. Privacy
    Threats: The access control is carried out by the single public blockchain node
    by verifying the access policies and attributes stored in the blockchain, which
    poses privacy issues for the attribute owners [15], [16]. The privacy issues of
    the public blockchain have been overcome by the deployment of Hyperledger Fabric,
    which is an open-source platform that possesses several significant advantages,
    such as an effective consensus mechanism, decentralized ledgers to protect privacy,
    and increased throughput [17]. Edge-based blockchain is implemented to perform
    access control with low latency and computational complexity using Hyperledger
    Fabric [18]. However, the proper usage of Hyperledger Fabric for effective access
    control has not yet been efficiently performed [19]. The revocation process is
    implemented to revoke malicious users or attributes, to increase security efficiency.
    However, this can lead to various attacks due to a lack of time-based revocation
    [20]. In this article, secure access control-based sharing of data is carried
    out in a decentralized manner. A. Motivation and Objectives The major aim of this
    research work is to perform dynamic access control and data sharing in a decentralized
    manner. The trust-based selection of delegator nodes is performed to achieve effective
    consensus. The delegation and revocation of access rights are executed in a periodic
    way to maintain security. Various issues in previous works have motivated this
    research and are addressed within it, as follows. Inefficient Access Management:
    The existing approaches have integrated the access control with the blockchain
    technology to give it a decentralized nature, but the selection of a single delegator
    (e.g., cloud) increases the vulnerability of users and risks of leakage of private
    data. Single Point of Failure: Blockchain-based access control has been utilized
    to provide decentralized computation, but several existing approaches have used
    gateway (GW)-based access control, which results in a single point of failure
    when the number of requests or users in the network increases. Complex Cryptography:
    The existing approaches have ensured the security of IoT nodes by utilizing cryptographic
    algorithms such as elliptic curve cryptography (ECC), which possess increased
    message size and computational complexity that are not suitable for resource-constrained
    IoT devices. The major objective of this approach is to ensure security and privacy
    in the IoT network. This objective is achieved by satisfying a set of subobjectives,
    as follows. To minimize the effect of adversarial access requests in overloading
    the entity by performing authentication-based request filtration. To maximize
    the scalability and performance of blockchain by achieving increased transactional
    throughput and reduced block validation time. To maximize the reputation of the
    consensus by considering the trust value in selecting the consensus node. To maximize
    the security by performing revocation of the attributes and users based on time
    and behavior. B. Research Contributions The proposed dynamic secure access control
    using the blockchain (DSA-Block) model focuses on secure access control and secure
    data sharing using blockchain. The major contributions of this research are listed
    as follows. Authentication of nodes and users is carried out via hyperelliptic
    curve cryptography (HECC) and the entities are stored in the private local ledger
    (LL) to enhance the security, which helps to mitigate external attacks. Authentication-based
    request filtering is performed through a GW by verifying the legitimacy with a
    timestamp and freshness of the requests, which increases throughput and reduces
    latency. Access delegation is achieved through the edge server using rock hyraxes
    swarm optimization (RHSO) by considering trust, energy, load, and resource availability
    (RA), in which the trust value is evaluated using blockchain, which reduces the
    block validation time, response time, and consensus time. The data are shared
    securely manner by uploading the data to the cloud server with the help of a differential
    privacy mechanism, which increases the attack detection rate. Finally, revocation
    is performed for both user attributes and users to enhance security. The user
    attribute revocation is performed by considering expiry time and attributes updating,
    and user revocation is performed based on the trust value, which also increases
    the attack detection rate. The performance of the proposed work is evaluated in
    terms of consensus time, throughput, block validation time, transaction time,
    latency, response time, and attack detection accuracy. C. Article Organization
    This Section outlines the structure of this article that follows. Section II summarizes
    the literature review of previous works. Section III explains the preliminaries
    of this research which is followed by the major problem statement in Section IV.
    Section V presents the system model of the proposed DSA-Block approach and its
    experimental results are described in Section VI. Finally, Section VII concludes
    the research and recommends future work. SECTION II. Literature Survey This Section
    summarizes the literature review on the existing methods, where different approaches
    have been used to perform secure access control based on attributes. It also outlines
    the key research gaps in these works. A. Access Control Schemes The decentralized
    authorization of IoT devices was proposed by [21]. The necessity of updating policies
    was addressed by implementing a collaborative-based access control approach in
    which the new access policy collaborated with the old one using a collaborative
    node. The system model is comprised of entities, such as authority nodes, IoT
    nodes, and blockchain nodes. Initially, the IoT devices were authenticated by
    the hash ID using the ECC algorithm. The authorization of nodes was carried out
    using the access tree, in which the construction and reconstruction of a collaborative
    node were carried out by verifying the blockchain ledger. The generation of private
    and public keys was carried out based on the ECC algorithm, which provided increased
    security due to its asymmetric nature. However, the increased size of the encryption
    key affects the complexity of these approaches. The privacy of IoT data stored
    in the cloud was ensured by proposing attribute-based encryption (ABE) in [22].
    The vulnerabilities inherent to data being kept in cloud storage, such as privacy
    leakage, data tampering, and illegal access were considered, and an effective
    approach was introduced. The centralized server utilized in the conventional approaches
    was replaced with the decentralized blockchain approach. The identity of the users
    in the IoT network was preserved by using the asymmetric encryption algorithm
    and the keys were generated by the consensus nodes. Revocation of users in the
    network was carried out periodically and the key updating of the revoked users
    was executed. When the user requests data kept in the cloud, the cloud performs
    predecryption by retrieving the keys from the blockchain, which reduces the computational
    cost of the users. The consensus nodes were responsible for generation of keys
    and revocation of users in the network, but the selection of consensus nodes in
    the blockchain is carried out without considering the reputation of the nodes,
    which affects the credibility of the approach. Symmetric key encryption was used
    to ensure the authenticity of the IoT users, but the usage of a single key by
    the IoT nodes makes it vulnerable to being compromised. A flexible access control
    mechanism using ABE was proposed in [23]. The limitations of existing nonrevocable
    access control approaches were addressed by introducing a blockchain-based revocable
    approach. This system model was comprised of entities, such as trusted authority
    (TA), publisher, user, and miners. Initially, the registration of users along
    with their attributes was performed, after which the chameleon hash algorithm
    was used to generate the hash function for verification. The revocation of attributes
    was performed by the TA if the user attributes were found to be changed. Based
    on the new set of attributes, the access control for the respective user was performed.
    The authors stated that the decentralization of the approach can be achieved by
    deploying multiple Tas, but the matching of users to a respective TA was not performed,
    which increases the overload of a particular TA in the network. The authors performed
    multifactor-based access control of IoT users in [24]. The limitations of existing
    single-factor access control mechanisms were overcome by utilizing three factors:
    1) user password; 2) biometrics; and 3) the keys provided by the TA for access
    control. Initially, the key generation for TA was computed based on the elliptical
    curve discrete logarithm, and the registration and enrolment of users and GW nodes
    were carried out to perform access control. The data sharing was performed after
    the establishment and verification of the session key through mutual authentication
    of the GW node and IoT users. Finally, the updating of security parameters was
    performed in the user’s favor. The generation of private and public keys was carried
    out based on the ECC algorithm, which provided increased security due to its asymmetric
    nature. However, the increased size of the encryption key increases the complexity
    of this approach. A simplified access control approach for an IoT environment
    was presented in [25]. The limitations of traditional centralized access control
    approaches, such as single point of failure and data tampering, were overcome
    by a distributed approach. In this approach, the consortium blockchain was utilized
    in which the identity of the user is denoted as the address. The generation of
    public and private keys was carried out via the ECC algorithm. The practical Byzantine
    fault tolerance (PBFT) consensus was utilized for the verification of access requests
    and revocation of users was carried out to ensure the security of the approach.
    Fog computing-based distributed management of keys was carried out in [26]. The
    limitations of cloud-based access control, such as unreliability, were overcome
    by this approach. The system model is comprised of hierarchical deployment of
    entities, such as IoT devices, edge nodes (ENs), a security access manager (SAM),
    and cloud-based blockchain nodes, respectively. The initialization of the users
    was performed based on several parameters. The accessed query and access response
    was performed by the SAM presented in the fog layer. To ensure the security of
    the IoT environment, periodic updating of keys, and revocation of users was carried
    out. The generation of blocks was achieved by using proof of work consensus; however,
    the time required for generation of blocks and reduced throughput provided by
    this approach restricts the usage of this consensus. Zhang et al. [27] proposed
    the key-policy ABE (KP-ABE) scheme using the decisional learning with error (DLWE)
    problem and combined the KP-ABE with blockchain for secure access management.
    In KP-ABE, the researchers used probabilistic polynomial time (PPT) algorithms,
    including setup, keygen, encryption, and decryption. Through the combination of
    blockchain and KP-ABE scheme, access management was facilitated between the owner
    and their device and then the user. In the access management scheme, first initialization
    takes place for the registration process and then the transaction process occurs
    when a transaction request is received. By using miners, the transaction is verified
    by a PoS consensus mechanism before the block is added to the blockchain. The
    blockchain gives access permission to users. Finally, the DLWA method was used
    to prove the secure access management using the KP-ABE scheme. Here, the authors
    proposed a KP-ABE scheme for secure access management by using blockchain. The
    efficiency of the access management was considered, but not the security issues
    in the ABE scheme, which reduces the degree of security for IoT. Ali et al. [28]
    proposed an ABE scheme with a lightweight revoke method. By using a central authority
    (CA), domain authorities (DAs), cloud service provider (CSP), data owner (DO),
    and data user (DU), five processes were implemented with PPT algorithms. First,
    using a cryptographical background, the authors initialized the CA and DAs. In
    the key delegation process, key generation for the DO and DU was achieved using
    the access tree. In the data outsourcing process, the partially encrypted data
    from the DO was encrypted in CSP. Then, the encrypted data was partially decrypted
    in CSP and fully decrypted in DU during the decryption process. In the user revocation
    process, the DAs updated the parameters and the re-encrypted secret key was updated
    for the users to allow or revoke access. Finally, for the proofing process, the
    Decisional Bilinear Diffe–Hellman problem was used, with the hardness assumption.
    Here, the authors proposed a hierarchical ABE scheme for secure access control
    for IoT. However, there is a probability of data tampering due to low data integrity
    and user privacy. B. Blockchain-Based Security Schemes secure sharing of data
    in the IoT environment using identity-based encryption was proposed in [29]. The
    limitation of an ABE process, such as high time consumption, was addressed by
    implementing a proxy-based re-encryption approach in which the EN acts as a proxy
    server to perform re-encryption of the data. The DO is responsible for the generation
    of keys for each user based on their identity. The users submit their keys along
    with a request, based on which the user verification was performed by the blockchain-assisted
    TA. The PBFT consensus was utilized to achieve consensus for block generation
    in order to reduce the time required for the encryption and decryption processes.
    However, the generation of keys for each user in the network becomes complex when
    the number of users in the network increases. The PBFT consensus was utilized
    in this approach to achieve a consensus about the final decision, but the proposed
    consensus model was found to have scalability issues and increased block validation
    time. Kumar et al. [30] proposed a secure machine learning-based framework for
    ensuring trustworthiness in an IoT environment. The smart city application was
    adopted, in which three modules, namely trustworthiness, privacy, and intrusion
    detection, were presented. In the first module, the computation of a reputation
    score for the IoT devices was carried out and, based on that, the nodes were categorized
    into three clusters. The reputation score and transaction of the nodes were further
    stored in a blockchain. The off-chain storage of raw data was carried out in an
    interplanetary file system (IPFS), in which the privacy of the data was ensured
    by storing its hash values in the blockchain. The blocks were generated using
    enhanced proof of work consensus. Finally, intrusion detection was performed by
    utilizing the XGBoost algorithm. The generation of blocks was carried out using
    enhanced proof of work consensus, but the time required for block generation and
    the reduced throughput provided by this approach restricts the usage of this consensus.
    A trust-based provision of access to IoT services was presented in [31]. The limitations
    of combining blockchain technology and trust management were overcome through
    this approach. The trust values were computed based on processes such as service
    testing, service monitoring, and service rating. The trust computation was carried
    out between the IoT users for provision of services. This is facilitated by the
    trust consensus protocol, in which the trust metric is computed for processes,
    such as leader selection, block generation, and validation of blocks. The trust
    computation was based on the service testing, service monitoring, and service
    rating parameters, but the individual nodal trust of the IoT devices was not considered,
    which limits the effectiveness of the trust-based consensus protocol. Ali et al.
    [32] proposed an approach named xDBAuth for permission delegation and access control
    in a cross-domain context, by using four operations. Four transactions, such as
    T.register, T.access, T.delegate, and T.revoke, were used for the following operations.
    The first operation was domain registration in a global smart contract. In these
    various domains, the admin sent registration requests to the blockchain manager
    through the T.register transaction, followed by user or IoT device registration
    in a local smart contract in the same T.register transaction. In this operation,
    they assumed that the user had installed TPM and registered the user or IoT device
    in a local smart contract through the transaction. The third process was to publish
    the delegation policy through the T.delegate transaction. The last operation was
    resource access, which was done through the T.access transaction, where the cross-domain
    authentication was performed by a PoAI mechanism. Here, the authors proposed a
    PoAI mechanism approach for cross-domain authentication and performing the hash
    function using a sha-256 algorithm. However, when using this algorithm for hash
    generation, there is a difficult to find value, which was computed as the liable
    hash value. Local and global smart contracts were used for blockchain to carry
    out the transactions, but the delegate policies were stored in the off-chain due
    to storage limitations in the blockchain. This reduces the storage complexity,
    but leads to privacy issues and there is a probability of changing or rewriting
    the delegate policies. Qashlan et al. [33] proposed an approach to preserve privacy
    by using blockchain. Register contract and access contract were used in a smart
    contract. Initially, the end-user sends an access request to the edge server;
    the server then redirects the user to the smart contract and the user calls the
    smart contract to check its validity. If valid, the user policies are then checked
    using ERC20 for attributes and the generating token if its attributes satisfy
    the policy. By using the token, the user sends the access request to the smart
    contract via the edge server and gets access to the IoT devices. Finally, the
    authors used a differential privacy enhancement mechanism with stochastic gradient
    descent to ensure safety of the data by adding noise samples without considering
    privacy. Here, the authors used the PoW mechanism in blockchain for consensus;
    however, this leads to a Sybil attack and other attacks. Wang et al. [34] proposed
    a Proof of X-Repute (PoXR) mechanism for the consensus process in blockchain for
    IoT. For rapid and safe consensus, they used two methods: 1) repute rewards and
    2) repute punishments. Initially, the block production method was used to create
    a new block by considering the initial reputes, miner ID, and other related parameters.
    Then, the generated block undergoes a verification process by using the fork selection
    method and reputation module with parameter characteristics of the consensus algorithm.
    Finally, using the incentive method, reputation status was identified by giving
    the respective parameters to improve the consensus process in the blockchain.
    In this work, the PoXR mechanism was used to select the node for consensus to
    update the blockchain ledger. However, in PoXR, if more nodes are able to perform
    consensus, this leads to a computational overhead. Putra et al. [35] proposed
    an authorization approach for IoT in blockchain by using trust scores and reputation
    values. Here, two blockchains were used, public and private blockchains. Initially,
    service providers (SPs) and service consumers (SC) were registered in an attribute
    authority (AA) based on their attributes and received key pairs by using an asymmetric
    cryptography mechanism. In the public blockchain, the trust score was found by
    considering the previous interactions between the SP and SC, and the reputation
    value was found by using the trust score. Whenever an access request arrived from
    SC, the request was validated in the public blockchain based on the trust value
    and its corresponding reputation. If the request was valid, the attributes were
    retrieved from the private blockchain a token was created for SC, and the data
    storage system validated the token and allowed access to the data. Here, the authors
    proposed an authorization in blockchain approach for IoT using a trust score to
    achieve privacy. However, at the time of registering, the SC and SP gave the attributes
    to the AA in an off-chain scenario, which leads to a lack of privacy. Table I
    provides a summary of the related works indicating the algorithms/methods used,
    advantages and disadvantages. TABLE I Summary of the Related Work SECTION III.
    Preliminaries This Section describes the preliminaries of this research, which
    consist of the blockchain technology, Hyperledger Fabric, and PBFT consensus.
    A. Blockchain Technology The security of IoT entities is provided by the decentralized
    nature of the blockchain technology. The data stored within the blockchain are
    stored in a hashed manner, as blocks. The communications between each block are
    stored as transactions. The blockchain integrity is ensured by running consensus
    for block creation, with the help of miners. The miners validate the transactions
    and create the block. Once a new block has been created, it is broadcast to other
    blocks. The generated block contains a header and a body. The header contains
    versions of the block, the hashed value of the transaction using the Merkle hash
    root tree, the new block threshold value, nonce, and previous block information.
    The body of the block contains a transaction counter. The transaction size limits
    the size of the block. Some of the features of the blockchain are listed as follows.
    Faster execution. Immutability. Consensus. Distributed ledgers. Decentralized.
    Highly secure. Based on the size of the user data, they can be stored either in
    the blockchain or the off-chain. The nontransactional data are stored in the off-chain,
    being too large to store in the blockchain. Off-chain storage is lower cost, and
    ensures better privacy than the blockchain. The off-chain distributed system for
    storing off-chain data is IPFS. B. Hyperledger Fabric Hyperledger Fabric is the
    prefabricated architecture in blockchain that permits the IoT nodes to run consensus
    based on a plug-play mechanism for preserving privacy. The Hyperledger Fabric
    compromises the following components: chain code, peer node, channel, membership
    SP, devoted assembling service, and ledger. The three main components of Hyperledger
    Fabric are the devoted ordering service, membership SP, and peer nodes. Devoted
    Assembling Service: The assembler in the network is responsible for effective
    communication which periodically checks the state of the ledger. Any of the consensuses
    in the Hyperledger Fabric are proven by the assembler to sustain the order of
    transactions. Membership Service Provider: Membership SPs are responsible for
    ensuring the authenticity of the authorized blockchain network, which also generates
    private channels for communication among blockchain nodes. Peer Nodes: Peer nodes
    are responsible for upholding the ledgers. Generally, the Hyperledger Fabric contains
    broadcaster peer nodes and supporter peer nodes. The broadcaster peers are distributed
    in the network and broadcast the block to other peers, whereas supporter peers
    are used to validate user requests. The Hyperledger Fabric contains policies to
    maintain the nodes in a decentralized manner to reduce the scalability and mitigate
    security threats. Two types of policies, namely, implicit meta-policy and signature
    policy, provide effective access control via consortium. C. Practical Byzantine
    Fault Tolerance Consensus The PBFT is a consensus algorithm based on a voting
    procedure; it can allow unauthorized nodes, i.e., mischievous nodes to take part
    in the consensus algorithm. The PBFT consensus maintains safeness and liveliness
    among the blockchain nodes. Safeness refers to the case where, if any of the services
    are repeated by some nodes, it must satisfy the correctness condition, and liveliness
    refers to nodes’ behavior based on the request. Every node in the PBFT network
    communicates through cryptographic methods to prove its integrity, and also to
    reach the state of consensus. The steps involved in PBFT consensus are listed
    as follows. Demand. Preprepare. Prepare. Obligate. Block formation. Table II presents
    a comparison of conventional consensus and PBFT consensus. Fig. 1 presents the
    block formation in the blockchain-based on the PBFT consensus mechanisms in which
    information in the workings of the PBFT consensus is also provided. TABLE II Comparison
    of Existing Consensus Versus PBFT Fig. 1. Blockchain-based PBFT consensus. Show
    All SECTION IV. Problem Statement The security and privacy of the IoT users are
    a major concern, and several existing approaches have executed various schemes
    to achieve the requirements. However, they possess several limitations that affect
    the achievement of security and privacy in the IoT environment. The major problems
    with these approaches are described as follows. The scalable access control mechanism
    based on blockchain for a resource-constraint IoT environment was proposed in
    [36]. The limitations in the selection of a single delegation node for access
    delegation were addressed by implementing a lightweight blockchain scheme in several
    IoT nodes. The major problems with this research are defined as follows. The identity-based
    authentication of devices, users, and ENs was carried out, but the parameters
    considered for authentication possess a low degree of security, which affects
    the secure authentication of nodes and users. The selection of policy decision
    points was performed based only on the parameters such as task execution and use
    of resources, but the lack of consideration of trust-based parameters affects
    the integrity of the final access decision. The management of access control in
    an IoT environment based on Hyperledger Fabric was presented in this article [37].
    The limitations of the centralized access control mechanisms were addressed by
    this approach. The hierarchical structure of access control in the IoT environment
    was proposed in this article [38]. The limitations of traditional access control
    mechanisms in providing flexible access to IoT resources were addressed by utilizing
    an ABE mechanism. The major problems with these researches are explained as follows.
    The Hyperledger Fabric utilized Kafka consensus for block generation due to its
    performance, but the Kafka consensus does not provide resistance against the malicious
    nodes present in the network. In the Fabric-IoT approach, the access policy of
    the resources was delegated by the devices, but the revocation and updating of
    users were not performed, which affects the adoption of this approach. The data
    consumer sends the access requests directly to the centralized blockchain node,
    but the increased number of access requests generated by the malicious users will
    cause an overload of the blockchain, which affects the proper provision of access.
    The GW-based architecture for providing access control in the IoT environment
    was proposed in this article [39]. The limitations of the traditional access control
    approaches were analyzed and a decentralized approach named BorderChain for management
    of access to the IoT domains was proposed. The major problems with this research
    are listed as follows. The proposed GW-based architecture performed well in ensuring
    the security of the IoT users and devices within a domain, due to the trust provided
    by vendors and ISPs, but the single point of failure occurring in these entities
    affects the usefulness of this approach. In addition, the IoT GW was utilized
    as the blockchain node, but the presence of malicious users in the network creates
    an increased number of access requests, which wastes the resources of the blockchain
    nodes. The blockchain-based delegation of access for the IoT users was carried
    out in this article [40]. The major problems with this research are defined as
    follows. The delegator node, which is considered the broker, was responsible for
    delegation of access to the users, but this node possesses a high risk of a single
    point of failure, as attackers in the network use it to affect the proper delegation
    of access. The provision of access based on user attributes was carried out in
    this approach, but the changing nature of user attributes was not considered which
    affects the access provision process. Access provision was performed based on
    the smart contract, but the process involved in access provision and consensus
    of both the public and private blockchain was not mentioned. A. Research Solutions
    The aforementioned problems are addressed by providing the following solutions.
    The authentication of devices, users, GWs, and ENs was carried out based on several
    significant user attributes and devices attributes that increase the degree of
    security of the nodes and users. Private and public keys are generated by using
    the HECC algorithm, which enables reduced key size without any compromise on security.
    This algorithm is suitable for resource-constraint IoT environments. hierarchical
    architecture-based approach is implemented, in which the decentralized management
    of authorization is performed by using both a global domain authority (GDA) and
    a local domain authority (LDA). delegator nodes are selected by using the RHSO
    algorithm for the trust value, energy level, traffic load, and RA; this contributes
    to the effective selection of nodes. Access control is executed by the consensus
    operation using the selected delegator nodes, which increases the network scalability.
    The burden of the GW is reduced by initially filtering the requests. The Trusted
    PBFT is utilized, in which trusted nodes are selected for consensus. The number
    of nodes participating in the consensus is restricted to a particular value based
    on the number of nodes. This provides resistance to malicious nodes and also reduces
    the block validation time. The dual revocation is executed, in which the revocation
    of attributes is carried out based on the attribute expiry time, and the revocation
    of users is performed based on the trust threshold value. The overloading and
    resource wastage of blockchain nodes is mitigated by filtrating incoming requests.
    The authenticity and timestamps of the requests are validated to ignore the malicious
    requests. SECTION V. System Model This article focuses on providing security and
    privacy of the data and devices in the IoT environment. The proposed DSA-Block
    model comprises the following entities: cloud node (CN), EN, GWs, IoT users (
    U I ) , and IoT devices ( D I ) . The Hyperledger Fabric is utilized in this approach
    due to its private and permissioned nature. Blockchain is used to select the multiple
    IoT delegator nodes by running the PBFT consensus mechanism. The IoT nodes are
    categorized into domains and each domain possesses an individual LL. The LDA is
    placed in each domain and the GDA is placed in the cloud. The entities are used
    in the DSA-Block model are summarized as follows. IoT Devices ( D I ): The D I
    are the DOs, in which they are classified into LDA or GDA. The LDA is used for
    providing authenticity to the IoT device and IoT users. The GDA is for storing
    the LDA transactions and providing delegation or revocation to the IoT users.
    IoT Users ( U I ): All the U I are data customers, they are requesting access
    to D I . Based on their respective requests, access is provided to U I . Gateway:
    GWs are located between U I , D I , and ENs, which ensure the legitimacy of the
    U I by storing the U I and D I attributes in the LDA thereby mitigating DDoS attacks.
    Edge Nodes: The ENs provide access delegation and revocation to the U I by running
    PBFT consensus based on trust; they also manage the network. Cloud Node: The CNs
    are used for storing the data of D I , which contains GDA for managing the D I
    data. Fig. 2 presents the architecture of the proposed DSA-Block model, which
    shows all the processes of the proposed work in a detailed manner. Fig. 2. Architecture
    of the proposed DSA-block model. Show All A. System Initialization The entities
    in the IoT network are initialized to perform secure data sharing. There are several
    entities in the network, including IoT users ( U I ) , IoT devices ( D I ) , GW
    nodes, and ENs inside the domains, which are initialized by submitting their parameters
    to the LDA. The user attributes ( U Ia ) include user ID ( U ID ) , position (pos)
    , role (role), etc. The device attributes ( D Ia ) include Device ID ( D ID )
    , MAC address (MAC) , IMEI number (IMEI) , etc., GW nodes, and ENs are submitted
    to the respective LDA. The LDA stores these attributes in the private LL and generates
    the public and private keys for the users and devices using HECC. Initially, consider
    Ĝ as the field and Ģ as algebraic closure for Ĝ with genus υ , and it is ≥1 for
    the hyperelliptic curve. The polynomial k(m)∈ Ĝ [m] of a degree greater than or
    equal to υ , and monic polynomial f(m)∈ Ĝ [m] of degree 2υ + 1. The hyperelliptic
    curve (∂) is formulated as follows: ∂: Φ 2 +k(m)Φ=f(m). (1) View Source Solution
    set generation (b,d)∈ Ģ ∗ Ģ is performed using the curve points ∂ known as Jacobian
    with the quotient λ= ´ υ/Φ , where´ υ denotes the divisor. It consists of a summation
    of points ∈∂ . The curve is nonsingular if it does not have many pairs of (b,d)∈
    Ģ ∗ Ģ. In addition, it satisfies the curve equation with a Partial Differential
    Equation, which is expressed as follows: 2Φ+k(m)= k ′ (m)Φ− f ′ (m)= 0 0. (2)
    (3) View Source The LDA further generates the attribute code for the attributes
    along with the expiry time for respective users based on the private and public
    keys of the users. The system initialization is briefly explained by the pseudocode,
    which is described in Pseudocode 1. Pseudocode 1 Pseudocode of the System Initialization
    Begin Initialize U I i , D i i , GW i , EN i ,LL,LDA Where i=1,2,3…n Initialize
    ( U Ia ={ U ID , pos,role }, D Ia ={ D ID ,MAC,IMEI} For all U I i , D i i , GW
    i , EN i do Register Attributes to LDA (); Store Attributes in LL; // Key Generation
    Select and υ ′ // prime and divisor I R ∈S ; K S ←[ I R ] υ ′ ; Return I R and
    κ //private and public key Generate attribute code using I R and κ End For End
    B. Authentication-Based Request Filtration Users requesting real-time access to
    the IoT devices send their request message to the EN through the GW. The GW performs
    the request filtration process to mitigate DDoS attacks caused by illegitimate
    user requests. Initially, the GW verifies the authenticity of the users by which
    the requests have been generated using filters. At first, the filter starts with
    an empty array with m bits. It returns false when the element is verified for
    membership in the filter. To add the element to the filter, the element is first
    accepted through the hash function. Here, the SHA-256 algorithm is used to perform
    hashing. The hash function result is used to take a position in the array of the
    filter. The position of the bit is assigned to one. To verify the element in the
    filter, the hash functions are used to create the positions in the filter. If
    the entire position in the array is one, the filter shows that the request is
    legitimate, otherwise, it is illegitimate. Hence, the hash function is used to
    verify the entries in the array. In addition, it is also used to ensure the legitimacy
    of the request. For huge values of m , the false positive rate (μ) of the filter
    is defined as follows: μ= (1− e −pq/m ) p (4) View Source where q represents the
    number of elements in the filter, m and p represent the false positive rate, m
    represents the nearest value of power 2, and p is rounded to the nearest integer
    value. Based on the filter, we filter only legitimate requests. If the user is
    found to be an authenticated user, the timestamp and freshness of the incoming
    packets are verified. By doing so, the illegitimate access requests causing DDoS
    attacks will be filtered and ignored. Only the legitimate access requests are
    processed, which reduces the burden on access delegation. C. Trust-Based Access
    Delegation The filtered legitimate requests are forwarded to the edge server,
    which is responsible for access delegation. The disadvantage of a single delegator
    node is overcome by selecting the multiple delegators using blockchain to ensure
    security. The selection of delegator nodes is carried out by using the RHSO algorithm
    based on the parameters, such as trust value (T) , energy level (E) , load (L)
    , and RA. The trust value of a node is provided by other nodes in the domain.
    First, the population is initialized for leaders and members. Hence, the leader
    selects the best place for observing the remainder of the group. Here, the best
    nodes among all nodes are known as the leader, which is used to select the optimal
    delegator. Then, the leader updates the current location based on its previous
    location, which is defined as follows: loc= R 1 ∗lpp(lcp,j) (5) View Source where
    R 1 represents a random value between [0,1] , lpp represents the leader’s previous
    position, lcp represents the current position of the leader, and j represents
    the reduction. After updating the position of the leader, all the members update
    their current position based on their previous position. The fitness value of
    the new position is calculated as follows: F=w(T,E,L,RA) (6) View Source where
    F represents the fitness function and w represents the weight values of the parameters.
    If the new position is better than the leader, then the member changes and updates
    its position lpp(i,j)=(x(i,j)−Cir∗lpp(i,j)+loc) (7) View Source where Cir represents
    the motion that imitates the circle system, which is defined as follows: ε 1 =
    ε 2 = Cir= R 2 ∗cos(ϑ) R 2 ∗sin(ϑ) SQRT( ε 2 1 + ε 2 2 )) (8) (9) (10) View Source
    where R 2 represents the radius and it is a random value between [0,1] , ϑ is
    also a random value between [0,360] and it represents the movement angle, which
    is updated in every iteration. The updating of the angle is based on the upper
    bound ( u b ) and lower bound ( l b ) of the variables, which are defined as follows:
    Dalta= ϑ= Random[ l b , u b ) ϑ+Dalta. (11) (12) View Source After completed the
    updating process, they continue searching the food in the locality, otherwise,
    the current new position is discarded. In this way, optimal delegators are selected
    by ENs. The energy levels of the IoT nodes are considered to address its energy-constraint
    nature. The number of selected delegator nodes is restricted to a limited number
    based on the total number of nodes in the domain. The Trusted PBFT consensus is
    utilized to achieve consensus between the nodes to provide the final access decision.
    The selection of more trusted nodes from the set of trusted nodes reduces the
    block validation time, which is an advantage of Trusted PBFT. The access decision
    is carried out based on user attributes, resource attributes, permission attributes,
    and environmental attributes. If the user requests resource in another domain,
    then the request along with the respective user attributes are shared to the EN
    in the respective domain via a secure channel from which the delegation of access
    will be performed. Trust Value (T) : It is calculated for every delegator to increase
    security. Here, trust values are given by neighboring nodes in the network. The
    calculation of the trust value is defined as follows: T= ∑ i=1 n F n (13) View
    Source where F n represents the feedback of the delegator, which is provided by
    the neighbor nodes. Energy Level (E) : It is used to determine the current energy
    level of the delegators. A high-energy level delegator is selected as the optimal
    delegator to perform access delegation. The calculation of energy level is defined
    as follows: E=ξ−£ (14) View Source where ξ represents the total energy of the
    node and £ represents the utilized energy of the node. Load (L) : It used to evaluate
    the load of the node. If the node has a lower load, then it is selected as the
    optimal delegator for performing access delegation. The evaluation of load is
    defined as follows: L= ∑ i=1 n w n (15) View Source where w n represents the amount
    of work performed by the delegator node. If the node has less work then it also
    has a lower load level. Resource Availability: It is used to calculate the available
    resources of the node. If the node has high RA, then it is selected as the optimal
    delegator for access delegation. The calculation of RA is defined as follows:
    RA=η−ζ (16) View Source where η represents the total resources of the node and
    ζ represents the resources utilized by the node. Fig. 3 represents the trust-based
    access delegation mechanism using RHSO and PBFT. Fig. 3. Trust-based access delegation
    mechanism using RHSO and PBFT. Show All D. Privacy-Aware Data Sharing The IoT
    devices upload their data to the cloud server for secure storage. In this scenario,
    the IoT devices act as DOs, and the users requesting access to the data are considered
    data customers. The DOs upload their data to the cloud storage by providing a
    random key, its public key, and the access attributes. The privacy of the data
    is further improved by implementing the differential privacy mechanism, in which
    the noisy data is encoded with the original data by the DOs. The Laplace method
    is used in this work for adding noisy data to the original data shared by the
    DOs. The data shared by the D I =( D I1 , D I2 ,…, D In ) contains privacy information
    D I pv =( D 1 pv , D 2 pv ,…, D n pv , the Laplace function with mean and variance
    is given as follows: { Variance, Mean, ▽ D I pv ∃ 0. (17) View Source The probability
    distribution function is expressed as follows: PDF(z∀)= 1 2∀ exp(− |z| ∀ ) (18)
    View Source where z denotes the explicit variable and ∀=([▽ D I1 ]/[∃]) , respectively.
    If the data D I1 is proportional to the exp(([∃∀( D I pv ,CN)]/[2▽∀]) , ( D I
    pv ,CN) denotes the privacy data exported from D I to the CN, the ∃ indicating
    that differential privacy protection is applied, which can be formulated as follows:
    B( D I pv ∀)={CN:PDF( D I ∈CN)∝exp( ∃∀( D I pv ,CN) 2▽∀ )+Lap(∂( ∃ 1 ))} (19)
    View Source where Lap(∂( ∃ 1 )) denotes the noise by the Laplace function. The
    above equation is the differential scoring function, in which a higher score provides
    greater privacy. Any adversary who tries to compromise the message, will not be
    able to do so. The uploaded data are stored off-chain and their respective hash
    values are stored in the blockchain. The access of data by the users is performed
    by submitting a request to the cloud server through the EN. The CN requests access
    delegation to the respective EN, which provides access decisions through a consensus
    mechanism. Once the cloud receives the access decision, it provides the access
    accordingly. Fig. 4 presents the differential privacy mechanism-based data sharing
    from DOs to cloud. Fig. 4. Privacy data sharing using differential privacy mechanism.
    Show All E. Dual Revocation The dynamic access control is provided by timely revocation
    of the user attributes and on-demand revocation of users. The user attributes
    are revoked based on the expiry time and the updating of attributes is carried
    out. The revoked users cannot upload the data to the cloud server. If the attributes
    are revoked, then a request message is sent to the private LL to get a new key
    based on attributes for further processes. The ledger verifies the attributes
    using blockchain for new key generation, if it is correct, then it generates a
    new key, otherwise the request is rejected. This type of revocation process increases
    security. The revocation of users is used to maintain the security of the domain.
    The trust values of the IoT users vary depending on the behavior of the users.
    At the time of attribute revocation, the malicious users have an opportunity to
    compromise the users. If a user is compromised by a malicious user its trust value
    will be reduced. Once the trust value is reduced below the threshold value the
    revocation of users from the domain is carried out. Here, the threshold value
    is generated based on the Shannon entropy, which is defined as follows: H(T)=−
    ∑ i=1 n P( T i )logP( T i ) (20) View Source where H represents the threshold
    and T represents the trust value of the user. If the threshold is less than 0.5
    then user revocation is performed, otherwise, revocation is not performed. SECTION
    VI. Experimental Results This Section describes the experimentation carried out
    with the proposed DSA-Block method to analyze its performance. The experimental
    results show that the proposed DSA-Block method achieves high security and privacy
    based on the efficient access control method. This Section consists of four subsections,
    such as the simulation setup, application scenario, comparative analysis, research
    summary, and security analysis. A. Simulation Setup This Section illustrates the
    simulation of the proposed DSA-Block method. The performance analysis of the proposed
    DSA-Block method was simulated by using the Network Simulator version 3.26 (NS-3)
    simulation tool. The specifications of the NS-3 tool are closely relevant to the
    proposed DSA-Block method. The proposed DSA-Block method was simulated in an 1000
    m ×1000 m environment. The system specifications for performing simulation are
    represented in Table III and the simulation parameters are denoted in Table IV.
    TABLE III System Specifications TABLE IV Simulation Parameters The simulation
    environment of the proposed DSA-Block method is illustrated in Fig. 5. It consists
    of numerous IoT devices with several GWs, ENs, and LDA. In addition, the GDA,
    cloud layer, and blockchain are also included. The IoT devices gather data from
    the environment and transmit it to the LDA through GW and ENs. All the LDAs transfer
    the aggregated data to the GDA for further access. The ENs act as mediators for
    the IoT users and the cloud layer. Data security is ensured by hashing the data
    and storing it in the blockchain. Fig. 5. Simulation environment. Show All B.
    Application Scenario Recently, IoT has been used for numerous real-time applications,
    such as smart homes, smart hospitals, smart schools, etc., with rapidly emerging
    technologies, especially in smart hospitals (i.e., health care). Fig. 6 provides
    a diagrammatic representation of the smart hospital application flow. The personal
    details of the patients, such as name, age, sex, previous medical records, emergency
    details, such as contact details, allergies, blood group, etc., are maintained
    in a secure manner for further purposes and access is provided to authorized persons
    (i.e., doctors and healthcare workers). After consulting the doctor, the private
    notes (i.e., personal reports) of the patients are managed securely without any
    duplication. Fig. 6. Use case (smart hospital). Show All Entry access to the restricted
    area and to visit resident patients is provided only for managers, doctors, and
    healthcare workers. The medication reports of the patients are given high privacy
    and access is provided for doctors and pharmacies only. Access is provided to
    visitors only during visiting hours, otherwise, access is restricted. Resident
    patients should not have any access to the personal information of other residents.
    Similarly, based on the identity of the doctors, some access is restricted for
    certain actions. The doctors’ post-visits (i.e., when doctors attend patients
    homes to treat them) are recorded and stored on the server securely. The doctors
    have access that enables them to retrieve the emergency details of the resident
    patients for further treatments and medications. If any unauthorized persons try
    to perform a specific operation for which they do not have permission, this is
    referred to the managers for further actions. Fig. 6 represents the use case of
    the proposed work in a smart hospital scenario. C. Comparative Analysis In this
    section, the proposed DSA-Block framework is compared with existing works, such
    as TLC-Block [36], Fabric-IoT [37], and Borderchain [39] in terms of the consensus
    time consumption, throughput, block validation time, transaction time, latency,
    response time, and attack detection rate. 1) Consensus Time Consumption Comparison:
    The consensus algorithm ensures the integrity of the node. The time consumption
    for consensus is defined as the amount of time consumed to ensure the integrity
    of the node by running the consensus algorithm. Fig. 7 shows the comparison of
    consensus time against the number of selected delegator nodes for the proposed
    DSA-Block with existing works. From the figure, when the number of delegator nodes
    increases, the time consumption for the consensus also increases, though our proposed
    DSA-Block achieves less time consumption for the consensus algorithm due to the
    utilization of the PBFT consensus algorithm, in which a limited number of nodes
    are running consensus, which reduces the time consumption for a consensus algorithm.
    The existing work Fabric-IoT framework performs access delegation in a GW and
    Kafka consensus is utilized, which leads to a single point of failure and security
    threats, respectively. Fig. 7. Time consumption versus number of selected delegated
    nodes. Show All In the proposed work, consensus takes 73-min when the number of
    delegator nodes is 15 while the existing works TLC-Block, Fabric-IoT, and Borderchain
    require more time for consensus as 78, 88, and 98 min, respectively. The average
    time consumption for consensus when the number of delegator nodes increases to
    15 is 69 min, whereas in the existing works, it takes 74, 84, and 94 min, respectively.
    2) Throughput Comparison: Throughput ( T p ) is defined as the amount of data
    or number of requests by the users to the size of the data or request by the nodes
    and users, respectively, which can be formulated as follows: T p = Amountofdata
    Sizeofdata . (21) View Source Fig. 8 represents the comparison of throughput versus
    the number of illegitimate requests for the proposed DSA-Block and existing works.
    From the figure, when the number of illegitimate requests increases, throughput
    also increases. The proposed DSA-Block achieves high throughput. This is due to
    the authentication-based request filtration process and privacy-aware data-sharing
    process. In authentication-based request filtration, the legitimate user request
    is filtered out by filters in the GW for further processes, and in the privacy-aware
    data-sharing process, the data of the trusted nodes are sent to the cloud via
    the differential privacy mechanism method. In addition, the initialization and
    trust-based access delegation process also increases throughput, thereby improving
    the overall system performance. The existing works Fabric-IoT and Borderchain
    are limited with scalability issues, which lead to a single point of failure,
    thereby reducing throughput. Fig. 8. Throughput versus number of illegitimate
    requests. Show All The proposed work achieves a throughput of 90 kb/s when the
    number of legitimate user requests is 5, while the existing works TLC-Block, Fabric-IoT,
    and Borderchain achieve lower throughputs of 80, 75, and 70 kb/s, respectively.
    The average throughput when the number of legitimate user requests is 5 is 80
    kb/s, whereas in the existing works it is 71, 65, and 60.4 kb/s, respectively.
    3) Block Validation Time Comparison: The amount of time taken to validate a block
    (deciding whether to accept or revoke the block) is known as the block validation
    time. Fig. 9 presents a comparison of block validation versus the number of attributes
    for the proposed DSA-Block and existing works. From the figure, when the number
    of attributes increases, block validation time also increases. The proposed DSA-Block,
    however, achieves a low block validation time. This is due to the trust-based
    access delegation process, in which the trusted delegator nodes are selected based
    on several metrics using the RHSO algorithm through, which a set of trusted nodes
    is obtained. From the obtained set of trusted nodes, the PBFT consensus is utilized
    for access delegation, which reduces the block validation time, whereas in existing
    works TLC-Block, Fabric-IoT, and Borderchain, this is limited to a single delegator
    node section, which means block validation takes more time. Fig. 9. Block validation
    time versus number of attributes. Show All The proposed work achieves a block
    validation time of 3.5-s when the number of attributes is 50, while the existing
    works TLC-Block, Fabric-IoT, and Borderchain have higher block validation times
    of 4.5, 5.5, and 6.5 s, respectively. The average block validation time of the
    proposed work is 2.5-s when the number of attributes is 50, whereas the existing
    works have average block validation times of 3.5, 4.5, and 5.5 s, respectively.
    4) Transaction Time Comparison: The time taken to complete the verification of
    transaction time through the consensus algorithm is known as the transaction time.
    Fig. 10 presents a comparison of the transaction time of the proposed DSA-Block
    with existing works in which, as the number of transactions increases, transaction
    time also increases. The proposed work achieves a shorter transaction time due
    to faster verification of node transactions by using the PBFT consensus algorithm,
    which takes less time for verification thereby increasing the block creation speed
    and reducing the transaction time. In addition, the RHSO algorithm also contributes
    to reducing the transaction time by optimally selecting delegator nodes, whereas
    Fabric-IoT utilizes the Kafka consensus algorithm, which takes more time for verification
    and leads to increased block creation and transaction time. Fig. 10. Transaction
    time versus number of transactions. Show All The proposed work achieves a transaction
    time of 1500-ms when the number of transactions increases to 100, while the existing
    works TLC-Block, Fabric-IoT, and Borderchain have high transaction times of 2500,
    3500, and 4200 ms, respectively. The average transaction time achieved by the
    proposed DSA-Block when the number of transactions increases to 100 is 920 ms,
    whereas the existing works show high average transaction times of 1760, 2760,
    and 3700 ms, respectively. 5) Latency Comparison: Latency (Lat) is defined as
    the amount of time taken to access the user request from the total time, which
    can be formulated as follows: Lat= tot T − R A (22) View Source where tot T denotes
    the total time and R A denotes the accessed request, respectively. Figs. 11 and
    12 present the latency comparison of the proposed DSA-Block work with existing
    works in terms of number of requests and transaction rate, respectively. Fig.
    11. Latency versus number of requests. Show All Fig. 12. Latency versus transaction
    rate. Show All From Fig. 11, when the number of user requests increases, latency
    also increases, and our proposed work achieves lower latency due to the authentication-based
    request filtration. In the authentication-based filtration process, the authenticated
    users are further filtered out based on the time stamp and freshness of the data,
    which allows only filtered trusted requests for trust-based delegation thereby
    reducing latency. By contrast, the existing works TLC-Block, Fabric-IoT, and Borderchain
    perform access delegation for all requests, which leads to increased security
    threats and increased latency. The proposed work achieves a latency of 18-s when
    the number of requests increases to 150, while the existing works have higher
    latencies of 30, 40, and 50 s, respectively. The average latency achieved by the
    proposed work, when the number of requests is increased to 150 was 15 s, whereas
    for the existing works, this was 20, 30, and 40, respectively. Fig. 12 shows that
    low latency was achieved when the rate of transaction increased, due to the authentication-based
    request filtration and trust-based access delegation. The authentication-based
    request filtration process filters the legitimate users based on the time stamp
    and freshness of data and the trust-based access delegation process utilizes the
    RHSO algorithm to perform access delegation based on trust, energy level, and
    load. The PBFT consensus is used to reduce the block validation time and check
    the integrity of the requests. This process reduces latency by providing the access
    delegation to only legitimate filtered users even though the transaction rate
    is high. The proposed work achieves latency of 20-s when the transaction rate
    increases to 100, while the existing works achieve high latencies of 35, 45, and
    55 s, respectively. The average latency achieved by the proposed work when the
    transaction rate increased to 100 was 15.4 s, whereas for existing works this
    was 25, 35, and 45 s, respectively. 6) Response Time Comparison: The amount of
    time taken to transmit a user request, process the request, and return a response
    is known as the response time rp , which can be formulated as follows: rp= E R
    − S R (23) View Source where S R and E R denote the start and end of user requests,
    respectively. Fig. 13 presents a comparison of the response time of the proposed
    DSA-Block with existing works, in which when the number of user request increases,
    response time also increases. Our proposed work achieves a low response time due
    to system initialization, authentication-based filtration, and trust-based access
    delegation. The system initialization process utilizes the HECC algorithm to ensure
    the legitimacy of the user request, authentication-based request filtration further
    filters the legitimate user requests to reduce the latency of the network, and
    the trust-based access delegation process utilizes the RHSO algorithm to provide
    access delegation to only requests with high trust, a high energy level, and low
    load, respectively. The PBFT is used to reduce the block validation time and ensure
    the integrity of the user, thereby reducing the response time of the user request.
    By comparison, the existing works TLC-Block, Fabric-IoT, and Borderchain allow
    the entire request and also user-centralized methods, which leads to a high response
    time thereby affecting the performance of the system. Fig. 13. Response time versus
    number of requests. Show All The proposed work achieves response time of 50-ms
    when the number of requests increases to 150, while the existing works achieve
    high response times of 100, 150, and 180 ms, respectively. The average response
    time achieved by the proposed work when user requests increased to 150 was 40
    ms, whereas the average response times in existing works were 90, 140, and 171
    ms, respectively. 7) Attack Detection Accuracy Comparison: The measure of attack
    detection as the number of IoT nodes increases is known as the attack detection
    rate, ( D a r ) which can be formulated as follows: D a r = detectedattacks IoTnodes
    . (24) View Source Fig. 14 presents a comparison of the attack detection accuracy
    for the proposed DSA-Block with existing works. From the figure, it is shown that
    when the number of nodes increases, the attack detection rate also increases.
    From that, our proposed work achieves a high attack detection rate due to authentication-based
    request filtration, privacy-aware data sharing, and the dual revocation process.
    In authentication-based request filtration, the DDoS attacks are mitigated by
    filtering the legitimate node/user requests based on the time stamp and freshness,
    the privacy-aware data-sharing method utilizes the differential privacy mechanism,
    which reduces the man-in-the-middle attacks, and dual revocation uses the Shannon
    entropy method based on trust, which further ensures the security through the
    revocation mechanism. The existing work TLC-Block performs authentication without
    considering proper authentication metrics, which leads to an increased attack
    detection rate. Fig. 14. Attack detection accuracy versus number of nodes. Show
    All The proposed work achieves a high attack detection rate 98% when the number
    of nodes increases to 150, while the existing works achieve 80%, 75%, and 70%,
    respectively. The average attack detection rate achieved by the proposed work
    when nodes are increased to 150 is 94%, whereas for existing works this is 78%,
    73%, and 68%, respectively. 8) Computation Overhead Comparison: The amount of
    time taken by the entity to complete a process of input data is known as the computation
    overhead (C o mp oh ) , which can be formulated as follows: C o mp oh = receivingtime
    processingtime . (25) View Source Fig. 15 presents the computation overhead comparison
    for the proposed and existing works, in which computation overhead increases as
    the number of delegator nodes increases. The proposed work achieves a lower computation
    overhead than the existing works. This is due to the adoption of the RHSO algorithm
    for delegator selection. The RHSO-based trusted delegator is selected based on
    the increased energy level, the less load, and the high trust. The adoption of
    an optimization algorithm for the delegator selection reduces the time processing
    time of the request, thereby reducing the computation overhead. By contrast, the
    existing works TLC-Block, Fabric-IoT, and Borderchain lack the intelligent packet
    processing methods. Fig. 15. Computation overhead versus delegator nodes. Show
    All Fig. 15 shows that when the number of delegator node increases to 25, the
    proposed work achieves a lower computation overhead of 79%, whereas the existing
    works Fabric IoT, Borderchain, and TLC-Block achieve high computation overhead
    of 96%, 90%, and 88%, respectively. D. Research Summary The proposed DSA-Block
    was evaluated using several performance metrics, which are consensus time consumption
    (69 min), throughput (80 kb/s), block validation time (2.5 sec), transaction time
    (920 ms), latency (15 s for the number of requests) and (15.4 s for transaction
    rate), response time (40 ms), and attack detection accuracy (94%), these results
    are shown in Figs. 7–14. Initially, the proposed DSA-Block system stores both
    user and device attributes to the LDA for key generation, which increases the
    security of the environment and results in high throughput and low latency during
    attack detection. Authentication-based request filtration is used to increase
    attack detection accuracy. Trust-based access delegation increases the security
    and throughput, and reduces response time due to considering only filtered requests.
    Privacy-aware data sharing also increases the throughput and reduces latency by
    introducing a differential privacy mechanism for data sharing, which increases
    data security. Dual revocation increases the security of the environment by performing
    both attribute and user revocation. Here, the blockchain used the PBFT consensus,
    which reduces block validation time, transaction time, and consensus time. Table
    V presents the results of the proposed and existing works in terms of comparison
    metrics. The highlights of the proposed DSA-Block results are defined as follows.
    authentication of the entities in the network is carried out by utilizing the
    HECC algorithm, which has reduced key size and increased security, thereby mitigating
    the complexity of conventional cryptographic algorithms. The authentication-based
    filtration of requests mitigates the DDoS attacks and reduces the burden of the
    IoT GW. The timestamp and freshness of the requests are also validated. The selection
    of delegator nodes in the domain is performed by using RHSO based on several significant
    parameters, through which consensus on access delegation is achieved through Trusted
    PBFT. Privacy-aware data sharing is performed, in which the encrypted data to
    be uploaded to the cloud server is encoded with noise via the differential privacy
    mechanism, which ensures data privacy. The dual revocation of attributes and users
    is executed to ensure the security of the network, in which the attribute revocation
    is performed based on expiry time and user revocation is performed. TABLE V Numerical
    Results E. Security Analysis The security analysis of the proposed DSA-Block method
    is described in this section. Providing access control and data sharing in the
    IoT environment addresses critical security and privacy threats represented by
    various forms of attacks, hence, we proposed the DSA-Block method to improve the
    security of the IoT environment. In doing so, we mitigate four types of attacks,
    which are listed as follows. DDoS Attacks: This attack sends out duplicate requests
    to create high Internet traffic for the server, which increases the risk of a
    single point of failure. In the proposed DSA-Block method, this is mitigated by
    performing authentication based on request filtration to mitigate this attack,
    which reduces the risk of the single point of failure. Sybil Attacks: This type
    of attack, reduces the reputation value of the IoT nodes by generating a huge
    number of fake identities. This attack is mitigated by performing the PBFT consensus
    based on user, resource, permission, and environment attributes in the proposed
    DSA-Block method. MITM Attacks: A man-in-the-middle attack occurs between the
    IoT device and cloud storage during data sharing by deleting or modifying the
    data, which increases both security and privacy threats. To overcome these threats,
    a differential privacy mechanism was implemented in this research prior to data
    sharing, which mitigates this kind of attack. Phishing Attacks: This type of attack
    appears as reputed nodes that send fake requests to perform malicious actions,
    which reduces the security. In this research, this kind of attack is mitigated
    by performing dual revocation based on the expiry time and behavior of the users.
    SECTION VII. Conclusion Dynamic secure access and data sharing are achieved in
    the proposed DSA-Block model, which improves the security and privacy of the IoT
    environment. All the user and device attributes are registered in the LDA for
    key generation, which increases the legitimacy of both users and devices. The
    IoT devices and users send an access request message to the edge server using
    a GW that only allows legitimate requests for access delegation. Here, access
    delegation is performed by the edge server by selecting a set of access delegators
    using RHSO, which optimally selects access delegators. This increases the security
    of the environment, which also increases the throughput and attack detection rate.
    Then, legitimate data are stored in the cloud server using blockchain via a differential
    privacy mechanism, which encodes the noise to the original data to increase privacy.
    The legitimate data are stored in the off-chain to enhance security; the hash
    values of the off-chain data are stored in the blockchain. The blockchain uses
    the PBFT consensus algorithm to create and add new blocks, which reduces transaction
    time, block validation time, and consensus time. Finally, revocation is performed
    for both user attributes and users in order to maintain the security of the environment.
    The simulation results showed that the proposed DSA-Block model achieves superior
    performance compared to other state-of-the-art works. In future, we plan to mitigate
    various types of attacks by using a modified blockchain that increases security
    and processing speed and reduces energy consumption during data sharing. In addition,
    the deduction of data availability for data customers via a differential privacy
    mechanism will be further investigated. Authors Figures References Citations Keywords
    Metrics More Like This The Performance Evaluation of Blockchain-Based Security
    and Privacy Systems for the Internet of Things: A Tutorial IEEE Internet of Things
    Journal Published: 2021 A privacy data security scheme based on homomorphic encryption
    and consortium blockchain for the Internet of Things 2022 IEEE 10th Joint International
    Information Technology and Artificial Intelligence Conference (ITAIC) Published:
    2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: IEEE Internet of Things Journal
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Dynamic Secure Access Control and Data Sharing Through Trusted Delegation
    and Revocation in a Blockchain-Enabled Cloud-IoT Environment
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Cárdenas R.
  - Arroba P.
  - Risco-Martín J.L.
  - Moya J.M.
  citation_count: '3'
  description: Compute-intensive Internet of Things (IoTs) applications have led to
    the edge computing paradigm. Edge computing decentralizes the IT infrastructure
    in multiple edge data centers (EDCs) across the access networks to reduce latency
    and network congestion. Edge computing can benefit significantly from different
    aspects of smart grids to achieve lower energy consumption and greater resilience
    to electricity price fluctuations. This paper presents a modeling, simulation,
    and optimization (M&S&O) framework for analyzing and dimensioning smart grid-aware
    edge computing federations. This tool integrates aspects of a consumer-centric
    smart grid model to the resource management policies of the EDCs. To illustrate
    the benefits of this tool, we show a realistic case study for optimizing the energy
    consumption and operational expenses of an edge computing federation that provides
    service to a driver assistance IoT application. Results show that this approach
    can reduce the daily energy consumption by 20.3% and the electricity budget by
    30.3%.
  doi: 10.1007/s10586-022-03797-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Cluster Computing Article
    Modeling and simulation of smart grid-aware edge computing federations Open access
    Published: 11 November 2022 Volume 26, pages 719–743, (2023) Cite this article
    Download PDF You have full access to this open access article Cluster Computing
    Aims and scope Submit manuscript Román Cárdenas , Patricia Arroba, José L. Risco-Martín
    & José M. Moya  1942 Accesses 2 Citations Explore all metrics Abstract Compute-intensive
    Internet of Things (IoTs) applications have led to the edge computing paradigm.
    Edge computing decentralizes the IT infrastructure in multiple edge data centers
    (EDCs) across the access networks to reduce latency and network congestion. Edge
    computing can benefit significantly from different aspects of smart grids to achieve
    lower energy consumption and greater resilience to electricity price fluctuations.
    This paper presents a modeling, simulation, and optimization (M&S&O) framework
    for analyzing and dimensioning smart grid-aware edge computing federations. This
    tool integrates aspects of a consumer-centric smart grid model to the resource
    management policies of the EDCs. To illustrate the benefits of this tool, we show
    a realistic case study for optimizing the energy consumption and operational expenses
    of an edge computing federation that provides service to a driver assistance IoT
    application. Results show that this approach can reduce the daily energy consumption
    by 20.3% and the electricity budget by 30.3%. Similar content being viewed by
    others Energy-focused simulation of edge computing architectures in 5G networks
    Article Open access 17 February 2024 Internet of Things (IoTs) Evolutionary Computation,
    Enterprise Modelling and Simulation Chapter © 2020 A Multi-joint Optimisation
    Method for Distributed Edge Computing Resources in IoT-Based Smart Cities Article
    26 October 2023 1 Introduction The Internet of Things (IoTs) is leading to a disruption
    in multiple business sectors. IoT technologies capture and process the actions
    of users and infrastructures to improve quality of life, safety, and resource
    management (e.g., autonomous driving or smart cities). These technologies are
    growing, and 47% of organizations plan to increase their investments in this field
    within the next few years [1]. IoT applications progressively increased the complexity
    of their ecosystems, sharing and self-managing their resources autonomously to
    achieve a common goal. However, IoT devices typically present limited storage
    and processing capabilities due to intrinsic limitations (e.g., battery lifetime
    and production cost). IoT services that require intensive computation and mass
    data warehousing may need additional infrastructure for effective real-time processing
    of a large volume of information from geographically distributed sources. Integrating
    cloud technologies with IoT has succeeded in overcoming these limitations [2].
    IoT applications use these resources to improve their services (e.g., advanced
    visualization tools or over-the-air firmware updates). However, some IoT applications
    present strict Quality of Service (QoS) requirements (e.g., low latency and rapid
    mobility). For those applications where response time is critical, the centralized
    approach of cloud data centers presents some limitations. For example, IoT applications
    using cloud services may experience significant communication delays. Furthermore,
    if IoT applications send heavy data streams to clouds from multiple devices, the
    core network of the Internet service provider (ISP) would be congested, degrading
    the overall QoS and bandwidth usage of any service that requires a connection
    to the Internet [3]. The edge computing paradigm arises as a solution for these
    limitations by extending the concept of cloud computing to the network edge. Edge
    computing decentralizes the computing resources and distributes them geographically
    closer to IoT devices to significantly reduce latency and network costs [4]. There
    is no consensus on where edge computing is. Usually, this depends on the vendor’s
    view. For instance, some IoT manufacturers claim that edge computing starts with
    the device itself. We follow the ISP perspective, which devises edge computing
    as a continuum of edge data centers (EDCs) (also considered micro data centers,
    MDCs) distributed from the ISP access networks to the public cloud to provide
    a drastic latency and network congestion reduction [5]. The assets of each EDC
    are sized to meet the demand in a given small area. This characteristic makes
    it possible to define more efficient resource utilization techniques because the
    system load corresponding to each service area is more predictable than in a centralized
    solution such as cloud computing [6]. Regarding energy consumption, edge computing
    infrastructures can significantly benefit from different aspects of smart grids.
    A smart grid is an electricity network that employs information and communications
    technologies (ICTs) to monitor and manage electricity usage to optimize production,
    transmission, and distribution [7]. The EDCs can work as a federation to reduce
    energy costs and improve resource management strategies [8]. Furthermore, EDCs
    can adopt more efficient self-consumption policies to transition to low-carbon
    systems [9]. Additionally, the edge infrastructure would benefit from other advantages
    of smart grids, such as greater resilience against energy price fluctuations [10].
    Complex systems such as smart grid-aware edge computing federations require multi-domain
    solutions. In these scenarios, one principal source of failure is communication
    between development teams. Model-based systems engineering (MBSE) methodologies
    overcome this vulnerability by establishing modeling as the central entity for
    information exchange [11]. MBSE eases the integration of modeling and simulation
    (M&S) tools into the system development process to explore and validate the complex
    system under study, providing information about potential technical risks and
    functionality of the solution while saving expenses [12]. Here we present a modeling,
    simulation, and optimization (M&S&O) framework for the analysis and dimensioning
    of edge computing infrastructures connected to smart grids. This tool integrates
    smart grids into the resource management process of the EDCs that comprise the
    edge computing federation. This integration can help to reduce the overall energy
    consumption and operational expenses of edge computing infrastructures, enabling
    the deployment of IoT applications with a smaller carbon footprint and increasing
    the viability of investments in future edge computing infrastructures. We have
    developed the proposed framework relying on the Discrete EVent System specification
    (DEVS) formalism [13] and the principles of MBSE that ensure a logical, robust,
    and reliable incremental design. In particular, the contributions of our research
    are the following: We extend our edge computing model presented in previous publications
    [14, 15] to integrate cooling infrastructures and support dynamic and more advanced
    resource management policies. We also expand the model to support a decentralized
    load balancing protocol to map new loads to one of the EDCs comprising the edge
    computing federation. This new approach includes cloud and edge computing infrastructures
    working together to improve their overall QoS while responding to unusually high
    demand. We introduce a consumer-centric smart grid model and combine it with the
    proposed edge computing model. EDCs incorporate current electricity pricing, on-site
    renewable energy generation, and energy storage to define more efficient federated
    resource management strategies. We develop an M&S&O framework that follows the
    presented models to assist in dimensioning edge computing federations connected
    to smart grids. We provide a realistic case study to illustrate how this M&S&O
    framework can assess the efficiency of smart grid-aware edge computing federations
    for an advanced driver assistance system (ADAS) application. We show that it is
    possible to reduce the daily energy consumption and cost by 20.3% and 30.3%, respectively.
    The rest of the paper is organized as follows. First, we discuss related work
    in Sect. 2. Section 3 describes the extended edge computing model proposed in
    this research. In Sect. 4. We also illustrate how we integrate it into the edge
    computing model. Section 5 presents a use case scenario to demonstrate how the
    proposed M&S&O framework can assist in dimensioning different aspects of smart
    grid-aware edge computing federations. Finally, we conclude in Sect. 6. 2 Related
    work We provide an overview of works focused on modeling and simulating edge computing
    infrastructure for computation offloading tasks and smart grids. We also discuss
    approaches for integrating both fields. 2.1 Edge computing infrastructures Edge
    computing for IoT applications is a research topic with several publications focused
    on improving the performance and energy efficiency of the infrastructure [16].
    Some of these publications propose new resource management techniques to reduce
    energy consumption while still meeting the required QoS using different techniques
    (e.g., mathematical models [17], deep learning-based approaches [18], or M&S methods
    [19]). For example, Zhang et al. present ApproxECIoT [20], a novel edge computing
    architecture to process real-time data streams of IoT applications. The IoT nodes
    comprising a wireless sensor network (WSN) send data streams to EDCs. As the computing
    and memory resources of the EDCs are limited, they sample the incoming data streams
    and apply approximate numeric methods to compute partial results. Then, they forward
    these partial results to a cloud data center for further processing. The cloud
    data center aggregates all the partial outcomes and checks if the accuracy of
    the final result complies with the expected QoS. If not, the cloud service readjusts
    the sampling rate of the EDCs. The proposal of Dong et al. [21] is of particular
    interest to this paper. The authors use Digital Twins (DTs) to optimize user association
    and EDC resource allocation depending on the QoS requirements. DTs are virtual
    replicas of a physical entity that capture the entities and dynamics of the system
    under study [22]. DTs are extensively used together with simulation and data analytics
    tools to integrate physical and virtual data. They enable the exploration of multiple
    plausible scenarios for optimizing the system’s performance [23]. Other works
    consider cooling energy to study the energy efficiency of edge computing infrastructures
    [24]. The energy efficiency of data centers is typically measured with the power
    usage effectiveness (PUE) metric [25]: $$\\begin{aligned} \\text {PUE}=\\frac{\\text
    {Total facility power}}{\\text {IT power}}. \\end{aligned}$$ (1) Energy consumed
    by the information technology (IT) components of the EDCs eventually becomes heat.
    The cooling infrastructure dissipates this heat to avoid potential damage to the
    IT elements. Depending on the cooling system, the cooling power may suppose a
    significant portion of the total facility power, degrading the EDC’s PUE. Hyperscale
    operators can place their data centers in cool climates to use outside air with
    advanced cooling technologies to obtain PUEs near 1.1 [26]. However, cooling supposes
    up to 40% of the total power consumption in average air-cooled data centers, leading
    to a mean PUE of 1.67 [27]. As EDCs are in access networks, they may present additional
    limitations compared to cloud systems (e.g., limited space and warmer climates).
    Thus, using efficient cooling systems is a significant concern. Two-phase immersion
    cooling technologies can mitigate these limitations of edge computing scenarios
    [28]. Two-phase immersion cooling systems immerse the IT equipment in a close
    bath full of a coolant fluid with very high heat flux. The heat produced by the
    IT infrastructure evaporates the coolant. Then, a condenser sets the coolant’s
    vapor to liquid phase again. A secondary working fluid (usually water) captures
    the heat from the condenser. Usually, the setpoint temperature of the coolant
    fluid is around 60 \\(^\\circ\\)C, enabling a cooling power reduction of up to
    95% regardless of the climate. This allows us to increase the power density and
    reduce the physical footprint up to 10 times [29]. M&S tools are necessary to
    define effective deployments of edge computing architectures, as they provide
    in-depth virtual analysis of such complex systems without incurring high costs.
    Multiple software tools focus on modeling and simulating IoT and edge computing
    solutions. Thus, they allow us to explore numerous scenarios to design more efficient
    architectures. We describe next some of the most popular simulators for studying
    edge computing infrastructures. FogNetSim++ [30] studies connectivity features
    of edge computing infrastructures. YAFS focuses on the analysis of dependent applications
    and their relationships [31]. Alternatively, FogTorchPi [32] is a simulator based
    on the Monte Carlo method to optimize the QoS of IoT applications. The iFogSim
    simulator [33] considers sensors and actuators to model IoT devices. EdgeCloudSim
    [34] integrates a simple mobility model for end-users. Finally, IOTSim [35] optimizes
    IoT services using the MapReduce algorithm. Table 1 shows a brief comparison of
    the edge computing simulators previously mentioned. Mercury [14] corresponds to
    the M&S&O framework presented in this paper. As our research focuses on edge computing
    infrastructures, Mercury had not previously considered cloud computing facilities.
    However, we integrate a simplified cloud computing model to Mercury for this work.
    Section 3 describes the proposed simplified cloud model. Table 1 Edge computing
    simulators comparison Full size table 2.2 Smart grids The smart grid paradigm
    embraces multiple research fields. The literature divides smart grid conceptual
    models into seven domains: customer, markets, service providers, operations, generation,
    transmission, and distribution [36]. While we must evaluate each field separately,
    they have shared requirements (e.g., communication protocols and security) and
    often interact to enable smart grid functionalities [37]. The customer domain
    embraces the end-users of electricity. In smart grids, customers may also generate
    and store energy for better energy management (e.g., demand peak and overall cost
    reduction [38]). In this context, state-of-the-art works propose different methods
    to describe and optimize smart grid customers’ energy management (e.g., mathematical
    models for optimal load shifting [39] or machine learning behavioral models [40]).
    These models integrate pricing data to enhance the consumers’ energy consumption.
    Electricity price changes depend significantly on customer base, country, or locality
    within a given country [41]. Therefore, dynamic pricing schemes present higher
    elasticity and better resource utilization than static approaches. However, they
    are exposed to market volatility, which may affect the end-user cost negatively.
    M&S tools are extensively used for designing and validating the components that
    comprise smart grids. Vaubourg et al. [42] present a co-simulation approach to
    model the operations, generation, transmission, and distribution domains of smart
    grids. GECO [43] is an event-driven simulator with configurable time precision
    that provides good scalability for large systems. Alternatively, Mosaik [44] is
    an M&S framework that offers a flexible, composable interface for adapting custom
    consumer/producer models. However, the discrete-time nature of Mosaik leads to
    a substantial performance penalty when time accuracy is required. Table 2 compares
    these simulators with the new version of Mercury [14] presented in this paper.
    Note that the smart grid model implemented by Mercury is focused on the customer
    side and does not implement aspects that belong to other domains. Table 2 Smart
    grid simulators comparison Full size table 2.3 Edge computing and smart grids
    Some related works study edge computing infrastructures and smart grids together.
    However, these studies focus on smart grid scenarios with edge computing as a
    service for the communication, monitoring, and analysis of the infrastructure
    of these smart grids [45]. To the best of our knowledge, no previous works analyze
    generic edge computing scenarios that integrate smart grids as a service for optimizing
    the energy consumption of the edge computing infrastructure while providing service
    to different IoT applications. Huang et al. [46] propose a framework for real-time
    monitoring of smart grids using edge computing facilities, increasing the monitoring
    rate by 10 times and achieving 85% less communication delay than centralized cloud-based
    systems. Liu et al. [47] present an IoT-based solution with edge computing that
    improves the resource management of smart grids in smart cities with deep reinforcement
    learning. Also, Gai et al. [48] propose an edge computing system for securing
    smart grids using blockchain technology. Here we present a formal model for smart
    grid-aware edge computing federations. The model integrates cooling systems and
    DTs of the edge computing facilities. It also considers aspects of the smart grid
    consumer domain (e.g., energy generation and storage). The presented model describes
    edge computing resource management-related features in detail. In contrast, other
    parts of the scenario (e.g., physical interfaces, cloud facilities, or energy
    distribution infrastructures) are defined with less detail. In this way, the proposed
    model presents a balance between model complexity and simulation performance.
    The hierarchical and modular approach of the DEVS formalism eases the integration
    of models with various degrees of detail to capture the interrelationships between
    different complex systems. Mercury, our M&S&O framework [14], implements the presented
    model as an extension. This new version enables the optimization of edge computing
    facilities connected to smart grids, reducing the carbon footprint of IoT services
    that benefit from this infrastructure. Mercury incorporates a multi-faceted modeling
    approach that allows users to select different degrees of complexity in the computational
    model [49]. Multi-faceted models are particularly useful for optimizing parts
    of the system under study. When optimizing a given feature, we run several simulations
    using a model with a fine-grained level of detail in the elements of the system
    that affect this feature the most but a high-level model of the rest of the system.
    Once optimized, the scenario is extensively validated using the detailed model
    of the whole system with less effort since the design space has already been explored
    and exploited. This tool is publicly available in Mercury’s GitHub repository
    [50]. 3 Edge computing model The proposed model is an extension of our previous
    work [15]. Specifically, it includes cooling systems for EDCs and DTs of EDCs’
    components to implement more sophisticated resource management policies. It also
    adds support for decentralized dynamic resource management policies depending
    on the current state of the scenario. Furthermore, it captures the behavior of
    cloud services to support demand peaks in case the edge computing infrastructure
    runs out of resources. This model, depicted in Fig. 1, has been implemented in
    the Mercury M&S&O framework [14]. Fig. 1 Proposed edge/cloud model Full size image
    Most of the elements of the model are located in a 5G radio access network (RAN)
    of an ISP. IoT devices correspond to 5G user equipment (UE, e.g., smartphones
    or connected vehicles). These devices run one or more services that monitor the
    users and their environment. We refer to the set of all the UE in the scenario
    as \\(\\mathbf {UE}\\). UE nodes offload the computation of the data to nearby
    EDCs comprising specialized hardware resources for providing computing to the
    users. Alternatively, access points (APs) establish radio communication links
    between UE devices and the rest of the scenario. All the APs of the model comprise
    the set \\(\\mathbf {AP}\\). At time t, a given \\(\\text {UE}\\in \\mathbf {UE}\\)
    is connected to the AP with the best signal quality in the location of the UE,
    \\(\\text {ap}_{\\text {UE}}(t)\\in \\mathbf {AP}\\). This AP allows the UE to
    use less energy for transmitting data to the network. Therefore, handovers from
    one AP to another may occur as UE nodes move across the scenario. APs re-direct
    service-related messages from UE to the EDCs via the crosshaul network. This network
    comprises optical fiber communication links that interconnect EDCs, APs, and the
    core network of the ISP for providing Internet connectivity to all the elements
    connected to the RAN. Computation offloading follows a Function-as-a-Service (FaaS)
    fashion in this model. When UE nodes request to open a new service session, EDCs
    inspect their available resources and reserve those required by the service for
    granting computation offloading while the session is active. Once UE nodes close
    the session, held resources are freed and available for new eventual UE service
    session requests. All the EDCs in a RAN work in a federated manner and share their
    current state. Usually, UE requests are processed by the closest EDC in the scenario.
    However, if one of the EDCs runs out of resources due to an abnormal demand peak,
    it will forward incoming requests to the second-best EDC of the federation. Furthermore,
    EDCs can forward requests to the public cloud in the event of congestion in all
    the EDCs. While clients whose requests are sent to the cloud experience higher
    delays than usual, they will not have to wait until one or more EDCs become available.
    3.1 Edge data centers This section presents the proposed model for describing
    the behavior of EDCs. Figure 2 represents a schematic of the proposed model. Fig.
    2 Edge data center model Full size image First, the EDC interface hides the complexity
    of the EDC and shares with the rest of the scenario an alternative version of
    the EDC’s current state with less detail. It also receives new computation offloading
    requests and decides which EDC of the federation is in charge of processing it.
    If all the EDCs are busy, it forwards the request to the cloud. Alternatively,
    processing units (PUs) are the IT resources of the EDC, and the cooler dissipates
    the heat produced by the PUs to avoid any potential damage. The resource manager
    is in charge of matching new session requests to one of the PUs in the EDC. The
    resource manager incorporates a DT of the IT and cooling resources to make more
    accurate predictions of the EDC’s next state and optimize its performance. The
    demand estimator monitors incoming requests and profiles the current service demand.
    It may incorporate additional information to estimate the future need for EDC
    computing resources. Finally, the policy manager considers the current state of
    the EDC, the current demand, and the future demand estimation to change dynamically
    the policies used by the resource manager to dispatch new sessions to PUs. Next,
    we describe the behavior of each of the subcomponents comprising an EDC. 3.1.1
    Processing units PUs are the IT resources of the EDC. A PU can start a new IoT
    service session, process requests of active sessions, or close already active
    sessions. When an IoT service \\(\\text {SRV}\\) requests to open a new session,
    PUs reserve a fraction of their computing resources, \\(U_{\\text {PU}}^{\\text
    {SRV}}\\), for this session. Thus, IoT services with active sessions are guaranteed
    to have the needed resources as long as their session is still active. When a
    session is closed, these resources become available again for any eventual new
    session. PUs can only open new sessions if they have enough free resources, as
    the utilization factor of a PU at time t, \\(u_{\\text {PU}}(t)\\), cannot be
    greater than 1: $$\\begin{aligned} u_{\\text {PU}}(t)=\\sum _{\\text {SRV}\\in
    \\mathbf {srv}_{\\text {PU}}(t)}U_{\\text {PU}}^{\\text {SRV}} \\le 1, \\end{aligned}$$
    (2) where \\(\\mathbf{srv} _{\\text {PU}}(t)\\) is the set of active sessions
    on the PU at time t. The power consumption (in W) of a PU at time t, \\(p_{\\text
    {PU}}(t)\\), depends on its current utilization factor, architecture, and hardware
    specifications. When a PU is not hosting any service session [i.e., \\(\\mathbf
    {srv}_{\\text {PU}}(t)=\\emptyset\\)], it can be turned off to save energy. As
    soon as a service requests to start a new session, the PU switches on and creates
    the requested session. However, switching on and off a PU introduces additional
    delays (\\(T_{\\text {PU}}^{\\text {on}}\\) and \\(T_{\\text {PU}}^{\\text {off}}\\),
    respectively). These delays may impact negatively on the latency. To avoid poor
    QoS, a PU can remain on hot standby. Hot standby PUs persist idle even when they
    do not host any session. While their power consumption is higher, these PUs are
    ready to open new sessions, reducing the response time significantly. The model
    implemented in Mercury allows you to define custom power consumption functions
    for each PU. This feature enables the heterogeneous IT equipment in each EDC.
    The IT power consumption of an EDC at time t, \\(p_{\\text {EDC}}^{\\text {IT}}(t)\\),
    corresponds to the sum of the power consumption of all its PUs: $$\\begin{aligned}
    p_{\\text {EDC}}^{\\text {IT}}(t)=\\sum _{\\text {PU}\\in \\mathbf {PU}_{\\text
    {EDC}}}p_{\\text {PU}}(t), \\end{aligned}$$ (3) where \\(\\mathbf {PU}_{\\text
    {EDC}}\\) is the set of all the PUs within an EDC. 3.1.2 Cooler The cooler dissipates
    the heat produced by the PUs to avoid any potential damage. The cooling power
    at time t, \\(p_{\\text {EDC}}^{\\text {cool}}(t)\\), depends on \\(p_{\\text
    {EDC}}^{\\text {IT}}(t)\\) and the cooling technology used. Even though Mercury
    supports different cooling technologies, we model \\(p_{\\text {EDC}}^{\\text
    {cool}}(t)\\) as a pump-driven two-phase immersion cooler in this research. Figure
    3 shows a schematic of the proposed cooling system. Fig. 3 Schematic of pump-driven
    two phase cooling system Full size image The pump that drives the secondary working
    fluid is the only element of this cooling system that requires electric power
    to function. The pump power consumption depends on the flow rate of the secondary
    working fluid. Equation (4) shows how to obtain the flow rate: $$\\begin{aligned}
    \\varPhi _\\text {EDC}(t)=\\frac{1}{277.78}\\cdot \\frac{p_{\\text {EDC}}^{\\text
    {IT}}(t)}{\\rho \\cdot C_p \\cdot \\varDelta T} \\;\\left( \\hbox {m}^{3}\\,\\hbox
    {h}^{-1}\\right) , \\end{aligned}$$ (4) where \\(\\rho\\) is the density of the
    secondary working liquid (in \\(\\hbox {g}\\,\\hbox {cm}^{-3}\\)), \\(C_p\\) is
    its specific heat capacity (in J g\\(^{-1}\\) K\\(^{-1}\\)), and \\(\\varDelta
    T\\) corresponds to the difference between the outlet and inlet temperature of
    the fluid (in K). We divide by 277.78 to transform the flow rate units from \\(\\hbox
    {cm}^{3}\\,\\hbox {s}^{-1}\\) to \\(\\hbox {m}^{3}\\,\\hbox {h}^{-1}\\). The relation
    between \\(p_{\\text {EDC}}^{\\text {cool}}(t)\\) and \\(\\varPhi _\\text {EDC}(t)\\)
    depends on the characteristics of the pump used. 3.1.3 EDC Digital Twin Every
    EDC has a DT of its IT and cooling infrastructure. Resource management tasks can
    use this DT to explore the search space and evaluate the effect on the EDC state
    for optimizing its performance. For example, we could forecast the impact of starting
    a new service session in a given PU on its power consumption. First, the PU’s
    DT would predict the resulting PU’s utilization factor: $$\\begin{aligned} u''_{\\text
    {PU}}(\\text {SRV},t)=u_{\\text {PU}}(t)+U_{\\text {PU}}^{\\text {SRV}}. \\end{aligned}$$
    (5) Then, the DT of the PU would also evaluate the resulting power consumption
    if the PU started this new service session: $$\\begin{aligned} p''_{\\text {PU}}(\\text
    {SRV}, t) = p_{\\text {PU}}(t)|_{u_{\\text {PU}}(t)=u''_{\\text {PU}}(\\text {SRV},t)}.
    \\end{aligned}$$ (6) 3.1.4 Resource manager The main functionality of this module
    is matching new session requests to one of the PUs in the EDC. When a new service
    \\(\\text {SRV}\\) requests to open a session in the EDC, the resource manager
    forwards this request to the PU selected by the EDC dispatching function, \\(\\text
    {disp}_{\\text {EDC}}(\\text {SRV}, t)\\). Mercury allows defining multiple dispatching
    functions depending on the use case (e.g., minimizing the response time or dividing
    the PUs resource utilization evenly). In this research, we use the current state
    of the EDC and its DT for mapping new sessions to the PU that would experience
    the minimum IT power consumption increase: $$\\begin{aligned} \\begin{aligned}
    \\text {disp}_{\\text {EDC}}(\\text {SRV},t)&=\\underset{\\text {PU}\\in \\mathbf{PU}
    _{\\text {EDC}}}{\\text {arg min}}\\,p''_{\\text {PU}}(\\text {SRV}, t)-p_{\\text
    {PU}}(t)\\\\&\\text {s.t. }\\,u''_{\\text {PU}}(\\text {SRV},t)\\le 1, \\end{aligned}
    \\end{aligned}$$ (7) where \\(p_{\\text {PU}}(t)\\) is the power of the PU, and
    \\(p''_{\\text {PU}}(\\text {SRV}, t)\\) and \\(u''_{\\text {PU}}(\\text {SRV},t)\\)
    are the power consumption and utilization factor predicted by the DT. Additionally,
    the resource manager determines which PUs shall remain on hot standby depending
    on the hot standby function, \\(\\text {stdby}_{\\text {EDC}}(t)\\subseteq \\mathbf
    {PU}_{\\text {EDC}}\\). Note that the policy manager can change \\(\\text {disp}_{\\text
    {EDC}}(\\text {SRV}, t)\\) and \\(\\text {stdby}_{\\text {EDC}}(t)\\) depending
    on the state of the scenario. 3.1.5 Demand estimator The demand estimator module
    continuously monitors the incoming service requests to profile the current service
    demand, \\(\\text {prof}_{\\text {EDC,SRV}}(t)\\). It also implements a demand
    estimation model for every service, \\(\\text {estim}_{\\text {EDC,SRV}}(t)\\),
    to forecast the future need for the resources of the EDC. The demand profiling
    and estimation are forwarded to the EDC policy manager for further evaluation.
    3.1.6 Policy manager The edge computing infrastructure can adapt its resource
    management policies depending on the current status of the scenario. Service demand
    on edge computing varies significantly over time in response to the user activity
    in the region. For instance, service demand may be lower at 3:00 AM, whereas a
    demand peak may be reported at 7:00 AM. EDCs resource management policies must
    change over time to provide an efficient solution to these variations. The policy
    manager keeps track of the current state of the EDC and the future demand estimation.
    From this information, the policy manager can apply reactive, proactive, or hybrid
    techniques to automatically change the dispatching and hot standby policies and
    adjust them according to the scenario state. 3.1.7 EDC interface The EDC interface
    hides the complexity of the EDC and shares with the rest of the EDCs an alternative
    version of the EDC’s current state with less detail. It represents all the active
    service sessions on an EDC at time t, \\(\\mathbf {srv}_{\\text {EDC}}(t)\\),
    as the union of all the active service sessions of all the EDC’s PUs: $$\\begin{aligned}
    \\mathbf {srv}_{\\text {EDC}}(t)=\\bigcup _{\\text {PU}\\in \\mathbf {PU}_{\\text
    {EDC}}}\\mathbf {srv}_{\\text {PU}}(t). \\end{aligned}$$ (8) The overall resource
    utilization of the EDC corresponds to the mean resource utilization of its PUs:
    $$\\begin{aligned} u_{\\text {EDC}}(t)=\\frac{\\sum \\nolimits _{\\text {PU}\\in
    \\mathbf {PU}_{\\text {EDC}}}u_{\\text {PU}}(t)}{|\\mathbf {PU}_{\\text {EDC}}|}\\le
    1. \\end{aligned}$$ (9) It also computes the hot standby utilization of the EDC,
    \\(u_{\\text {EDC}}^{\\text {stdby}}(t)\\). The hot standby utilization estimates
    how many resources are currently available for computation offloading services:
    $$\\begin{aligned} u_{\\text {EDC}}^{\\text {stdby}}(t)=\\frac{\\sum \\nolimits
    _{\\text {PU}\\in \\text {stdby}_{\\text {EDC}}(t)}u_{\\text {PU}}(t)}{|\\text
    {stdby}_{\\text {EDC}}(t)|}\\le 1. \\end{aligned}$$ (10) The EDC interface shares
    the IT and cooling power consumption with the scenario. It also defines the EDC
    overall power demand, \\(p_{\\text {EDC}}^{\\text {demand}}(t)\\), as the sum
    of the IT and cooling power. Finally, the EDC interface computes the PUE of the
    EDC, \\(\\text {pue}_{\\text {EDC}}(t)\\): $$\\begin{aligned} \\text {pue}_{\\text
    {EDC}}(t)=\\frac{p_{\\text {EDC}}^{\\text {demand}}(t)}{p_{\\text {EDC}}^{\\text
    {IT}}(t)}=1+\\frac{p_{\\text {EDC}}^{\\text {cool}}(t)}{p_{\\text {EDC}}^{\\text
    {IT}}(t)}. \\end{aligned}$$ (11) When a UE requests to open a new session, the
    AP that provides connectivity to the UE forwards the request to the nearest EDC.
    The interface of this EDC is in charge of determining which EDC should create
    this session. First, the EDC interface specifies the candidate EDCs of the RAN
    with enough resources to host the new session: $$\\begin{aligned} \\mathbf{edc}
    _{\\text {candidate}}(t)=\\left\\{ \\text {EDC}\\in \\mathbf{EDC} | u_{\\text
    {EDC}}^{\\text {stdby}}(t)<1\\right\\} . \\end{aligned}$$ (12) Note that we only
    consider the hot standby utilization of the EDCs. In this way, even if an EDC
    has numerous computing resources, but none of them is on hot standby, its perceived
    utilization would be 1. Therefore, this EDC would not be considered a valid candidate.
    If there are no candidate EDCs [i.e., \\(\\mathbf{edc} _{\\text {candidate}}(t)=\\emptyset\\)],
    it forwards the request to the public cloud. Otherwise, it dispatches the new
    session request to one of the candidate EDCs. The selected EDC depends on the
    EDC mapping policy, \\(\\text {edc}_{\\text {EDC}}(t)\\). The proposed model allows
    multiple EDC mapping policies depending on the use case. In this research, we
    favor the nearest candidate EDC: $$\\begin{aligned} \\begin{aligned} \\text {edc}_{\\text
    {EDC}} (t)=\\underset{\\text {edc}\\in \\mathbf{edc} _{\\text {candidate}}(t)}{\\text
    {arg min}}&d(\\text {edc},\\text {EDC}). \\end{aligned} \\end{aligned}$$ (13)
    This EDC mapping policy enhances computation, reduces the overall latency, and
    avoids communication bottlenecks and general system failures. If the selected
    EDC corresponds to the one computing the mapping, the open session request is
    forwarded to the EDC’s resource manager. Otherwise, the request is sent to the
    selected EDC. In the proposed model, requests forwarded from other EDCs are directly
    forwarded to the resource manager. In this way, we avoid requests starvation (i.e.,
    requests forwarded from one EDC to another indefinitely). Figure 4 depicts a flowchart
    of the EDC mapping process enforced by the EDC interface. Fig. 4 Workflow diagram
    of the EDC mapping process Full size image 3.2 Cloud The presented edge computing
    infrastructure shows multiple advantages (e.g., low latency and less overall network
    congestion). However, it also introduces new challenges. For instance, computing
    resources are not unlimited, as EDCs are dimensioned to satisfy the demand of
    a limited area. Thus, an abnormal demand peak may lead to resource shortages in
    the edge infrastructure. Furthermore, due to unexpected features, the proposed
    solution must be resilient to potential EDC shutdowns. A collaboration between
    the edge computing infrastructure and cloud facilities can alleviate the negative
    effect of these problems. Our research focuses on resource management techniques
    for edge computing facilities, and we had not considered the cloud infrastructure
    in previous works. Here we introduce a simple cloud model that serves as a backup
    for situations where the available edge computing resources are scarce. This cloud
    model represents a simple collaboration between EDCs and clouds. In future work,
    we will explore alternative collaboration patterns to define more complex applications
    that use both edge and cloud resources. We divide the proposed cloud model into
    the Internet connection and the cloud facility. The Internet connection is modeled
    as a delay buffer that retains messages before forwarding them to their corresponding
    receiver. The Internet delay for a message MSG is computed as follows: $$\\begin{aligned}
    \\text {delay}(\\text {MSG})=t_{\\text {prop}}+\\frac{\\text {size}(\\text {MSG})}{v_{\\text
    {trans}}} \\;\\left( \\hbox {s}\\right) , \\end{aligned}$$ (14) where \\(t_{\\text
    {prop}}\\) is the propagation time from the RAN to the public cloud (in s), \\(\\text
    {size}(\\text {MSG})\\) is the size of the message (in b), and \\(v_{\\text {trans}}\\)
    is the mean transmission speed between the cloud and the RAN (in \\(\\hbox {b}\\,\\hbox
    {s}^{-1}\\)). Alternatively, the cloud facility has virtually unlimited computing
    resources. In contrast with the EDCs, the cloud can simultaneously process any
    number of service sessions. Once a service session is opened, the cloud will take
    \\(T_{\\text {cloud,SRV}}^{\\text {proc}}\\) seconds before sending a response
    to the client. Table 3 resumes the most relevant edge computing model-related
    parameters previously discussed. Table 3 Summary of the edge computing model Full
    size table 4 Consumer-centric smart grid model Figure 5 shows the proposed consumer-centric
    smart grid model. Fig. 5 Consumer-centric smart grid model Full size image This
    model comprises one energy provider PROVR and a set of smart grid consumers \\(\\mathbf{CONSR}\\).
    Each consumer \\(\\text {CONSR}\\in \\mathbf{CONSR}\\) may use energy from the
    energy provider to satisfy its energy needs. In this paper, there is one smart
    grid consumer for every EDC of the edge computing federation. 4.1 Energy provider
    The energy provider supplies electricity to smart grid consumers. Consumers pay
    the energy provider according to the electricity they consume from the grid. At
    time t, the energy provider offers electricity at \\(\\text {price}(t)\\) $/Wh.
    The provider revises this price with an update cycle of \\({T_{\\text {PROVR}}}\\)
    seconds. 4.2 Smart grid consumers Consumers include four submodules: power demand,
    energy source(s), storage unit, and storage controller. The power demand, \\(p^\\text
    {demand}_{\\text {CONSR}}(t)\\), represents the energy rate (in W) required by
    a consumer at time t. We describe the rest of the submodules below. 4.2.1 Energy
    sources Energy sources provide electricity without buying it from the energy provider
    [e.g., photovoltaic (PV) systems or generator sets]. At time t, the energy sources
    of a consumer provide \\(p^{\\text {gen}}_{\\text {CONSR}}(t)\\) W. The power
    generated by the consumer reduces its overall electricity costs. The consumer
    power surplus, \\(p^{\\text {surplus}}_{\\text {CONSR}}(t)\\), is the difference
    between the consumer’s power generation and the power demand: $$\\begin{aligned}
    p^{\\text {surplus}}_{\\text {CONSR}}(t) = p^{\\text {gen}}_{\\text {CONSR}}(t)
    - p^{\\text {demand}}_{\\text {CONSR}}(t)\\;\\left( \\hbox {W}\\right) . \\end{aligned}$$
    (15) Ideally, it would be 0 W always (i.e., consumers generate the same power
    they demand). In this scenario, the electricity consumption from the energy provider
    would be 0 W. A negative power surplus would imply that the consumer requires
    more power than it can generate. In this case, the consumer would need to buy
    the remaining energy from the provider. On the other hand, a positive surplus
    indicates that power generation is greater than the demand, and the energy surplus
    is returned to the grid. This scenario is also undesirable since consumers usually
    sell the energy surplus at a lower price than the energy providers, even to the
    point of not receiving any economic benefit [51]. 4.2.2 Storage unit Consumers
    use energy storage units (e.g., batteries) to store their energy surplus and use
    it later when consumers generate less power than their demand. Each consumer can
    store up to \\(\\text {CAP}^{\\text {max}}_{\\text {CONSR}}\\) Wh in its energy
    storage unit. The capacity of the consumer’s storage unit at time t, \\(\\text
    {cap}_{\\text {CONSR}}(t)\\), must be between 0 and \\(\\text {CAP}^{\\text {max}}_{\\text
    {CONSR}}\\) Wh: $$\\begin{aligned} 0\\le \\text {cap}_{\\text {CONSR}}(t)\\le
    \\text {CAP}^{\\text {max}}_{\\text {CONSR}}. \\end{aligned}$$ (16) The consumer
    charging power, \\(p^{\\text {charge}}_{\\text {CONSR}}(t)\\), corresponds to
    the electric power (in W) used to charge/discharge the consumer’s energy storage
    unit. If \\(p^{\\text {charge}}_{\\text {CONSR}}(t)\\) is greater than 0, the
    energy storage unit is charging. On the other hand, when the charging power is
    less than 0, the consumer subtracts energy from the storage unit. The capacity
    of the storage unit at time t is obtained according to Eq. (17): $${\\text{cap}}_{{{\\text{CONSR}}}}
    (t) = {\\text{ CAP}}_{{{\\text{CONSR}}}}^{{{\\text{init}}}} + \\frac{1}{{3600}}\\int_{0}^{t}
    {p_{{{\\text{CONSR}}}}^{{{\\text{charge}}}} } (\\tau )d\\tau \\;\\left( {{\\text{Wh}}}
    \\right),$$ (17) where \\(\\text {CAP}^{\\text {init}}_{\\text {CONSR}}\\) is
    the initial capacity (in Wh) of the storage unit. Note that, when integrating
    the charging power, the energy is expressed in J. We must divide the integration
    by 3600 to obtain the energy expressed in Wh. The charging power is limited to
    the maximum charge/discharge power the storage unit allows, \\(\\text {PMAX}^{\\text
    {charge}}_{\\text {CONSR}}\\): $$\\begin{aligned} -\\text {PMAX}^{\\text {charge}}_{\\text
    {CONSR}} \\le p^{\\text {charge}}_{\\text {CONSR}}(t) \\le \\text {PMAX}^{\\text
    {charge}}_{\\text {CONSR}}. \\end{aligned}$$ (18) 4.2.3 Storage controller This
    module is responsible for setting \\(p^{\\text {charge}}_{\\text {CONSR}}(t)\\)
    depending on the current power surplus of the consumer and the electricity price.
    By default, it sets \\(p^{\\text {charge}}_{\\text {CONSR}}(t)\\) to 0 W (i.e.,
    electricity is not stored nor subtracted from the storage unit). When the electricity
    price is low enough, providing that the storage unit is not entirely charged,
    the energy controller will charge its storage unit at a rate of \\(\\text {PMAX}^{\\text
    {charge}}_{\\text {CONSR}}\\) W. \\(\\text {PRICE}^{\\text {charge}}_{\\text {CONSR}}\\)
    refers to the maximum price (in $/Wh) a consumer is willing to pay for charging
    its energy storage unit using energy from the grid. If the electricity price is
    higher than \\(\\text {PRICE}^{\\text {charge}}_{\\text {CONSR}}\\), the storage
    controller explores different configurations depending on the consumer’s power
    surplus. When \\(p^{\\text {surplus}}_{\\text {CONSR}}(t)>0\\) (i.e., the consumer
    is generating more electric power than demanded), the storage controller sets
    \\(p^{\\text {charge}}_{\\text {CONSR}}(t)\\) to the minimum between the surplus
    and the storage unit’s maximum charging power until the capacity reaches its maximum
    value. On the other hand, when \\(p^{\\text {surplus}}_{\\text {CONSR}}(t)<0\\),
    the storage controller will only subtract energy from the storage unit if the
    electricity price is higher than \\(\\text {PRICE}^{\\text {discharge}}_{\\text
    {CONSR}}\\) $/Wh. The reason for this condition is that storing energy is a costly
    process. Thus, the storage controller must ensure that consuming the stored energy
    will provide a high enough decrease in the energy cost. Figure 6 shows a workflow
    diagram of the storage controller decision-making algorithm. Fig. 6 Workflow diagram
    for computing the charging power of the storage unit Full size image The power
    consumption of a consumer, \\(p^{\\text {cons}}_{\\text {CONSR}}(t)\\), represents
    the electric power (in W) that the smart grid consumer gets from the grid. It
    is obtained from the charging power, demand, and power generation: $$\\begin{aligned}
    \\begin{aligned} p^{\\text {cons}}_{\\text {CONSR}}(t)&= p^{\\text {charge}}_{\\text
    {CONSR}}(t) + p^{\\text {demand}}_{\\text {CONSR}}(t)- p^{\\text {gen}}_{\\text
    {CONSR}}(t) \\\\&= p^{\\text {charge}}_{\\text {CONSR}}(t) - p^{\\text {surplus}}_{\\text
    {CONSR}}(t). \\end{aligned} \\end{aligned}$$ (19) For each consumer, the electricity
    cost at time t, \\(cost_{\\text {CONSR}}(t)\\), is computed by summing the electricity
    price multiplied by the energy consumption during every pricing cycle, as shown
    in Eq. (20). Note that, in this model, smart grid consumers do not benefit from
    returning energy to the grid. Thus, we do not consider those periods in which
    power consumption is less than 0 W. $$\\begin{aligned}&cost_{\\text {CONSR}}(t)=
    \\sum \\limits _{k=0}^{\\left\\lfloor \\frac{t}{T_{\\text {PROVR}}}\\right\\rfloor
    } \\Biggl (\\text {price}(kT_{\\text {PROVR}})\\cdot \\frac{1}{3600} \\nonumber
    \\\\&\\quad \\cdot \\int _{kT_{\\text {PROVR}}}^{\\text {min}(t, (k+1)\\cdot T_{\\text
    {PROVR}})}\\text {max}\\left( 0,p^{\\text {cons}}_{\\text {CONSR}}(\\tau )\\right)
    d\\tau \\Biggr )\\;\\left( \\$\\right) . \\end{aligned}$$ (20) Table 4 describes
    of the most relevant parameters of the smart grid model. Table 4 Summary of the
    smart grid model Full size table 4.3 Smart grid-aware edge computing model The
    edge computing model presented in Sect. 3 can be combined with the smart grid
    model proposed in this section to reduce the operational expenses of edge computing
    facilities. Figure 7 shows a schematic of the proposed hybrid model. Fig. 7 Smart
    grid-aware edge computing model Full size image EDCs of the edge computing federation
    are also smart grid consumers. The power demand of an EDC corresponds to the power
    demand of its matching smart grid consumer. Additionally, the EDC interface adds
    smart grid-related fields to the EDC’s current status. Finally, EDCs’ interface
    and policy manager are also aware of the current electricity price offered by
    the energy provider. By doing so, they can incorporate these new fields into the
    search space and optimize the EDC configuration considering both edge computing
    and smart grid-related parameters. 5 Use case We first propose an edge computing
    scenario to show how edge computing can improve the QoS while reducing the network
    traffic to the Internet. After selecting a convenient configuration, we include
    smart grid elements and compare their simulation outcome to illustrate the benefits
    of connecting edge computing infrastructures to the smart grid. We executed the
    simulations sequentially on a MacBook Pro Retina, 15-in., Mid 2015 with a 2.5
    GHz Intel Core i7 processor, 16 GB 1600 MHz DDR3 memory, and macOS 12.5. 5.1 Scenario
    description The scenario is an ADAS application. Vehicles periodically capture
    images of their driver’s face to detect potentially dangerous situations (e.g.,
    distractions or eye fatigue) using an onboard predictive model. Cars use the edge
    computing federation to train their predictive models remotely. Each vehicle requests
    to open a new session every 20 min. As soon as this session starts, the automobile
    sends a batch containing new driver images with additional information gathered
    by the vehicle (e.g., GPS position, brake press, or steering angle). The session
    remains open for 15 min. During this time, the element hosting the session (i.e.,
    an EDC or the cloud) trains the model with new data gathered by the vehicle and
    other nearby vehicles. When the automobile closes the session, the computing element
    sends the re-trained model to the corresponding vehicle if this new model differs
    significantly from the one in the car. Computation offloading service demand corresponds
    to real mobility traces of taxis in San Francisco, USA [52]. The data set contains
    GPS coordinates of 535 taxis collected over May and June 2008 in the San Francisco
    Bay Area. Figure 8 shows the number of taxis in the scenario from May 17th to
    June 6th, both included. All the simulations in this research use the mobility
    traces of June 6th, 2008. Figure 8 depicts this day in orange. Fig. 8 Traffic
    flow in the scenario from May 17th to June 6th Full size image The PUs comprising
    the EDCs’ computation resources are AMD Sapphire Pulse Radeon RX 580 graphics
    processing units (GPUs). Table 5 contains all the configuration parameters of
    the PU model used in the simulations. This configuration is based on the work
    of Pérez et al. [53]. Table 5 Configuration parameters of processing units Full
    size table We set the propagation time between the cloud and the RAN, \\(t_{\\text
    {prop}}\\), to 80 ms. Alternatively, the mean transmission speed between the cloud
    and the RAN is 600 Mb s\\(^{-1}\\). We assume that cloud data centers use the
    same hardware to process service requests. Therefore, \\(T_{\\text {cloud,SRV}}^{\\text
    {proc}}\\) is 0.1 s. As shown in Fig. 8, the demand peak in the scenario is slightly
    greater than 200 vehicles. Every PU can host up to \\(\\frac{100}{U_{\\text {PU}}^{\\text
    {SRV}}}=5\\) simultaneous sessions. Thus, the edge federation must contain at
    least \\(\\frac{200}{5}=40\\) PUs. We decided to add redundant PUs to avoid system
    congestion in case an unusual demand peak occurs. The edge federation comprises
    three EDCs, each containing 20 PUs. In this way, the sessions are divided into
    three different geographic areas. Each EDC can host up to 100 sessions. The pump-driven
    two-phase immersion cooling system of the EDCs uses the 3M Novec 7100 fluid as
    refrigerant and water as condenser secondary working fluid (\\(\\rho \\approx
    {1}\\,\\hbox {g}\\,\\hbox {ml}^{-1}\\), \\(C_p={4.1813}\\,\\hbox {J}\\, \\hbox
    {g}^{-1}\\,\\hbox {K}^{-1}\\)). The difference between the outlet and inlet temperature
    of the water is 20 K. Each PU can consume up to 100 W, approximately. Therefore,
    the maximum IT power of each EDC is \\(100\\cdot 20={2}\\,\\hbox {kW}\\). Thus,
    according to Eq. (4), the pump used for driving the condenser fluid must be able
    to provide \\({0.086}\\,\\hbox {m}^{3}\\,\\hbox {h}^{-1}\\) (i.e., 1.434 l min\\(^{-1}\\)).
    We selected a 0142YA-12-15 micro diaphragm pump, which provides a flow of 1.5
    l min\\(^{-1}\\) consuming \\(p_{\\text {EDC}}^{\\text {cool}}(t)={15}\\,\\hbox
    {W}\\). On the other hand, we assume that the mean PUE of the cloud is 1.5. We
    used the allocation manager tool included with the Mercury M&S&O framework to
    determine the location of APs and EDCs [14]. This tool divides the scene into
    a grid of 40 m by 40 m cells. Next, it analyzes the user mobility traces to determine
    the maximum number of users inside every spatial cell during a time window of
    1 min. Figure 9 shows the spatial density of users in the scenario computed by
    the allocation manager. Fig. 9 Spatial density of users in the scenario Full size
    image After calculating the scenario density, the allocation manager applies the
    Same-Size K-Means algorithm to place APs and EDCs in suitable locations for distributing
    the service demand evenly. Figure 10 shows the scenario setup used in the experiments.
    Small dots correspond to user location traces used by the allocation manager.
    The 19 stars scattered in the scenario represent the location of APs. Additionally,
    three EDCs (represented as big squares) provide computation offloading to all
    the users. The color of every element corresponds to the preferred EDC. Fig. 10
    Scenario setup Full size image 5.2 Scenarios without smart grid integration First,
    we simulate several edge computing scenarios that do not integrate any aspect
    of the presented smart grid model. In particular, we simulated edge scenarios
    with different hot standby policies. 5.2.1 Static hot standby policies The first
    two policies are static (i.e., they do not vary throughout the simulation): None
    PUs do not work in hot standby mode and switch on only if they host one or more
    sessions. All all the PUs are on hot standby and are switched on even if they
    are idling. These static policies serve as yardsticks for assessing the efficacy
    of the remaining hot standby approaches. For instance, in the None policy, the
    hot standby utilization of the EDCs is always 1. Therefore, the edge computing
    federation forwards all the requests to the cloud. On the other hand, with the
    All policy, the EDCs accept all incoming requests. Consequently, this policy presents
    the lowest delay perceived by the users. The edge computing federation does not
    process requests with the None policy. Accordingly, the overall energy consumption
    of the EDCs is 0 Wh. Thus, the cloud is responsible for processing all the requests.
    On average, clients experienced delays of 160.000 ms for opening sessions, 273.334
    ms for sending information before starting the online training, and 160.267 ms
    when closing the session and receiving the newly trained model. Figure 11 shows
    the demand for cloud resources in this scenario. Fig. 11 Demand for cloud resources
    with None policy Full size image The cloud would need at least 41 PUs to support
    a peak demand of 203 clients. Figure 12 shows a power consumption estimation of
    the cloud facilities to support the scenario demand. The mean power consumption
    of the IT infrastructure is 4.20 kW, with peaks reaching 4.52 kW. Considering
    a PUE of 1.5, the total power consumption associated with the service is, on average,
    6.30 kW, with peaks approximating 6.79 kW. In this scenario, the total energy
    consumption of the cloud is 150.93 kWh. Fig. 12 Cloud power consumption estimation
    with None policy Full size image In contrast, the cloud is unnecessary with the
    All policy, as the EDCs can process all the requests. Figure 13 shows the demand
    for edge computing resources under this policy. Fig. 13 Demand for edge computing
    resources with All policy Full size image The EDC edc_2 is the most crowded one,
    as it is closer to the rest of the City of San Francisco. On the other hand, while
    the preferred region of the EDC edc_1 is the largest, it is located in a less
    crowded zone, and its average demand is the lowest. The delay experienced by clients
    for opening and closing sessions is negligible. On the other hand, the delay in
    sending information before starting the online training is very close to the processing
    time of the PUs (i.e., 100 ms). Figure 14 shows the power consumption of the edge
    computing federation. Fig. 14 Power consumption of edge computing resources with
    All policy Full size image With the All policy, the PUs of the EDCs are always
    on. The mean power consumption is 5.78 kW (i.e., 1.58 kW more than the power consumed
    by the cloud when using the None policy). However, the characteristics of the
    edge infrastructure lead to significantly lower PUEs. Figure 15 shows the PUE
    of the EDCs in the scenario. Note that the PUE is inversely proportional to the
    power consumption, as the cooling power remains constant regardless of the demand.
    Fig. 15 PUE of EDCs with All policy Full size image Overall, the total energy
    consumption of the edge federation is 139.00 kWh, which is 11.93 kWh less than
    the cloud with the None policy (i.e., 7.9% less). Thus, edge computing with no
    cloud cooperation allows us to provide a better QoS while reducing energy consumption
    of the infrastructure. 5.2.2 Proactive hot standby policies Next, we explore how
    proactive hot standby policies can improve the overall performance of the edge
    computing infrastructure while working with cloud. These proactive strategies
    rely on service demand predictions to modify the hot standby resources on each
    EDC. In this work, the service demand estimation corresponds to the daily average
    traffic flow in the scenario according to the 20 previous days before June 6th.
    However, modelers can implement their demand estimation model with the Mercury
    M&S&O framework. Figure 16 depicts the daily average traffic flow in the San Francisco
    Bay Area according to the mobility traces from May 17th to June 5th. Fig. 16 Hourly
    average traffic flow obtained from the previous 20 days Full size image The hot
    standby policy will dynamically change to ensure that each EDC maintains as many
    PUs as necessary in hot standby to accept the expected service requests without
    switching on any additional PU. At time t, the number of PUs on hot standby of
    a given EDC is computed as follows: $$\\begin{aligned} n_{\\text {EDC,SRV}}^{\\text
    {stdby}}(t)=\\left\\lceil \\text {estim}_{\\text {EDC,SRV}}(t)\\cdot (1+\\alpha
    )\\cdot U_{\\text {PU}}^{\\text {SRV}}\\right\\rceil , \\end{aligned}$$ (21) where
    \\(\\alpha\\) is a correction factor that assumes that the demand will be a fixed
    percentage higher than the prediction. While this estimation increment leads to
    higher power consumption, it is more robust against anomalous demand peaks, achieving
    a better QoS. We explored six proactive policies with different values for the
    correction factor. Namely, \\(\\alpha\\) is set to 0.0, 0.1, 0.2, 0.3, 0.4, and
    0.5. Table 6 summarizes the results of the static and proactive hot standby policies.
    Table 6 Simulation results for static proactive hot standby policies Full size
    table The mean service delay perceived by clients decreases as we increase the
    value of \\(\\alpha\\). Compared to the All policy, the proactive policy with
    \\(\\alpha =0.0\\) exhibits a degradation of 9 ms. This degradation diminishes
    as \\(\\alpha\\) increases. For \\(\\alpha =0.5\\), the performance degradation
    is near to 200 ms. In contrast, the total energy consumption of the scenarios
    does not follow a trend proportional to \\(\\alpha\\). Figure 17 divides the infrastructure
    energy consumption into the EDCs and the cloud. Fig. 17 Energy consumption of
    proactive hot standby policies Full size image The energy consumption of all the
    EDCs increases with \\(\\alpha\\). For instance, edc_0 needs 19.81 kWh for \\(\\alpha
    =0.0\\), while it consumes 27.68 kWh for \\(\\alpha =0.5\\) (i.e., 39.73% more).
    However, as the edge infrastructure has more available resources, we can reduce
    the number of backup cloud resources. For example, for \\(\\alpha =0.0\\), the
    peak demand in the cloud is 73 clients. Thus, the cloud needs 15 PUs to fulfill
    the demand instead of the 41 PUs required with the None policy. In contrast, the
    peak cloud demand when \\(\\alpha =0.5\\) is 9 clients, and we only need 2 PUs.
    Thus, even if the edge federation consumes more energy, it is more efficient than
    the cloud, and the overall energy decreases. Choosing a convenient hot standby
    policy depends on the requirements of the service. If the application needs low
    response times, we should select a policy that puts the service delay before power
    consumption (e.g., All). We can use the Mercury M&S&O tool to explore different
    scenarios and find the configuration that better suits our requirements. For this
    use case, we selected the proactive hot standby policy with \\(\\alpha =0.3\\)
    (the one outlined in bold in Table 6) as the reference proactive hot standby policy
    because this scenario shows the lowest energy consumption and its average QoS
    degradation is below 1 ms. Table 7 shows more simulation results of this scenario.
    Table 7 Simulation results for proactive hot standby policy (\\(\\alpha =0.3\\))
    Full size table The EDC edc_2 is the most loaded EDC, with a mean utilization
    of 58.08%. On the other hand, the average utilization of EDCs edc_0 and edc_1
    is 43.36% and 31.24%, respectively. Figure 18 shows the resource utilization of
    EDCs throughout the simulation. Fig. 18 EDC utilization with proactive hot standby
    policy (\\(\\alpha = 0.3\\)) Full size image As depicted in Fig. 16, the hourly
    demand estimation from the previous 20 days presents similar results. While the
    resource utilization of the EDCs resembles the scenario demand, the EDCs aim to
    maximize their hot standby utilization to reduce their energy consumption. Figure
    19 shows the hot standby utilization of the EDCs. For every EDC, the mean hot
    standby utilization is between 80.46 and 90.12%. Fig. 19 EDC hot standby utilization
    with proactive hot standby policy (\\(\\alpha = 0.3\\)) Full size image Figure
    20 shows the power demand of the EDCs of the edge federation and the backup resources
    in the cloud. Fig. 20 Power consumption with proactive hot standby policy (\\(\\alpha
    =0.3\\)) Full size image The shape of the power demand curves of the EDCs is similar
    to their utilization. However, the priors are more staggered. This is because
    the static power of PUs (i.e., the power consumption of PUs when there is no activity)
    is significantly higher than their dynamic power (i.e., the power consumption
    increment when a PU is busy compared to its static power). As the number of PUs
    switched on changes depending on the service demand estimation, the power demand
    changes significantly every hour. In contrast, the power consumption of the cloud
    backup resources is stable at around 550 W, as all the PUs remain on hot standby
    throughout the simulation. The EDC edc_2 presents the best mean PUE (1.01), while
    the EDC edc_1 obtains the worst average PUE (1.03). 5.2.3 Hybrid hot standby policies
    Next, we explore the effect of using a hybrid hot standby policy to improve the
    QoS of the infrastructure. These hybrid strategies combine service demand predictions
    with the current service demand of the scenario. At time t, the number of PUs
    on hot standby of a given EDC is computed as follows: $$\\begin{aligned}&n_{\\text
    {EDC,SRV}}^{\\text {stdby}}(t)=\\left\\lceil \\text {max}(\\text {demand}_{\\text
    {EDC,SRV}}(t),\\text {estim}_{\\text {EDC,SRV}}(t))\\right. \\nonumber \\\\&\\quad
    \\left. \\cdot (1+\\alpha )\\cdot U_{\\text {PU}}^{\\text {SRV}}\\right\\rceil
    . \\end{aligned}$$ (22) The hybrid policy reacts faster to demand peaks, achieving
    a better QoS. However, the power consumption of the EDCs tends to be greater than
    with the proactive policy. We explored six hybrid policies with different values
    for the correction factor. Namely, \\(\\alpha\\) is set to 0.0, 0.1, 0.2, 0.3,
    0.4, and 0.5. Table 8 summarizes the results of the hybrid hot standby policies.
    Table 8 Simulation results for hybrid hot standby policies Full size table The
    mean service delay perceived by clients is considerably lower than the achieved
    by the proactive policy. The hybrid policy with \\(\\alpha =0.0\\) shows a degradation
    of 8 ms and diminishes as \\(\\alpha\\) increases. Figure 21 divides the infrastructure
    energy consumption into the EDCs and the cloud. Fig. 21 Energy consumption of
    hybrid hot standby policies Full size image Compared to proactive policies, the
    scenarios with hybrid policies use fewer cloud resources. For example, for \\(\\alpha
    =0.0\\), the peak demand in the cloud is 26 clients, which only demands 6 PUs
    (instead of the 15 PUs required with the proactive policy with \\(\\alpha =0.0\\)).
    When \\(\\alpha =0.5\\), the cloud is no longer needed, as the edge computing
    federation can process all the demand. Thus, the QoS matches the All policy using
    29.63% less energy. Choosing between proactive and hybrid hot standby policies
    depends on the requirements of the service. In this paper, we selected the hybrid
    hot standby policy with \\(\\alpha =0.2\\) (the one outlined in bold in Table
    8) because this scenario shows the lowest energy consumption. Furthermore, the
    average QoS degradation is below 50 ms. Table 9 shows more simulation results
    of this scenario. Table 9 Simulation results for hybrid hot standby policy (\\(\\alpha
    =0.2\\)) Full size table The utilization of the EDCs is similar to the utilization
    of the hybrid policy with \\(\\alpha =0.3\\). However, the hot standby utilization
    of the hybrid policy is slightly smaller than in the proactive, as the EDCs tend
    to keep more PUs in hot standby through the simulation. Figure 22 shows the hot
    standby utilization of EDCs throughout the simulation. Fig. 22 EDC hot standby
    utilization with hybrid hot standby policy (\\(\\alpha = 0.2\\)) Full size image
    Figure 23 shows the power consumption of the edge federation and the cloud. Fig.
    23 Power consumption with hybrid hot standby policy (\\(\\alpha = 0.2\\)) Full
    size image Compared to the proactive policy, the effect of the demand prediction
    is less obvious. For instance, the power demand curves of the EDCs are not staggered
    and resemble more to their utilization. Additionally, the power consumption of
    the backup cloud resources is 49.09% less than with the proactive policy with
    \\(\\alpha =0.3\\). The cloud resources remain idle most of the time and only
    receive requests at dawn. As the power consumption of this scenario is slightly
    greater than with the proactive policy, the PUE of this scenario is better. Figure
    24 shows the PUE of each EDC. Fig. 24 PUE of EDCs with hybrid hot standby policy
    (\\(\\alpha =0.2\\)) Full size image As the cooling system only requires 15 W
    per EDC, their PUE is close to 1 most of the time. Note that the PUE is inversely
    proportional to the service demand. Thus, EDCs edc_0 and edc_2 present the best
    mean PUE (1.01), while the EDC edc_1 obtains the worst average PUE (1.02). At
    dawn, the resource utilization of all the EDCs is lower than in the rest of the
    day, and their PUE is considerably greater comparing the rest of the day. 5.3
    Integrating smart grid to the scenario The next set of experiments integrates
    the smart grid model described in Sect. 4 to reduce the impact of the energy price
    fluctuations on the overall service cost. The electricity price in the simulations
    corresponds to the average hourly day-ahead wholesale energy price in California
    in 2017 (see Fig. 25). Fig. 25 Average hourly day-ahead energy price in scenario
    Full size image It is important to note that the wholesale price typically corresponds
    to 35% of the final retail price. However, it is possible to compare the results
    of different simulations. With this pricing scheme and the hybrid policy with
    \\(\\alpha =0.2\\), the edge federation infrastructure would spend 2.44 $ to satisfy
    its energy demand for this day. Figure 26 represents the accumulated energy cost
    per EDC when using the hybrid hot standby policy with \\(\\alpha =0.2\\). Fig.
    26 Accumulated energy cost per EDC with hybrid policy (\\(\\alpha =0.2)\\) Full
    size image The cost curves present significant increments from 06:00 to 09:00
    and from 18:00 to 23:00, coinciding with the energy price peaks of Fig. 25. Smart
    grid consumers can alleviate the overall cost by integrating energy generation
    sources and storage systems. Next, we present four scenarios that integrate the
    smart grid model to reduce the energy consumption and cost of the edge computing
    federation. The EDCs use the hybrid (\\(\\alpha =0.2\\)) hot standby policy in
    all the scenarios. Additionally, EDCs incorporate two BISTAR TP6L72M(H) 9BB crystalline
    silicon (c-Si) PV panels to reduce their power consumption. Each PV system provides
    a peak power of 910 W and presents 14% of system losses. We obtained the data
    traces of the PV panels’ power generation from the PVGIS-NSRDB database according
    to the approximate location of the EDCs in Fig. 10. The time precision provided
    by this database is 1 h. Due to the discrete-event nature of the simulator, the
    simulated power generation looks staggered, as shown in Fig. 27. Fig. 27 Power
    generation of PV panels of the EDCs in June 6th Full size image The EDCs have
    a low-voltage energy storage system. Depending on the scenario, the storage unit
    of the EDCs may correspond to a PylonTech US2000C battery (\\(\\text {CAP}^{\\text
    {max}}_{\\text {CONSR}}={2.28}\\,\\hbox {kWh}\\), \\(\\text {PMAX}^{\\text {charge}}_{\\text
    {CONSR}}={1.20}\\,\\hbox {kW}\\)) or a US3000C battery (\\(\\text {CAP}^{\\text
    {max}}_{\\text {CONSR}}={3.37}\\,\\hbox {kWh}\\), \\(\\text {PMAX}^{\\text {charge}}_{\\text
    {CONSR}}={1.78}\\,\\hbox {kW}\\)). The maximum charge/discharge power corresponds
    to the values recommended by the manufacturer. Even though the battery can charge/discharge
    faster, the storage controller will limit it to extend the battery life. The batteries
    of all the EDCs are fully discharged at the beginning of the simulation. We also
    explored two storage controller policies for the EDCs. The first one, called Loose,
    sets \\(\\text {PRICE}^{\\text {charge}}_{\\text {CONSR}}\\) to 20 $/MWh and \\(\\text
    {PRICE}^{\\text {discharge}}_{\\text {CONSR}}\\) to 35 $/MWh. The second policy,
    called Strict, sets more demanding requirements for charging or discharging the
    battery. Namely, it configures \\(\\text {PRICE}^{\\text {charge}}_{\\text {CONSR}}\\)
    to 19 $/MWh and \\(\\text {PRICE}^{\\text {discharge}}_{\\text {CONSR}}\\) to
    37 $/MWh. While we limit the set of experiments to one hot standby policy, two
    battery models, and two storage controller strategies, Mercury allows us to explore
    other scenario parameters to optimize the edge computing infrastructure’s overall
    performance (e.g., EDC mapping strategy or power generation modules). Table 10
    shows the simulation results obtained by the scenarios that integrate the smart
    grid model. The first row corresponds to the base case scenario with no smart
    grid integration. Note that the shown results do not include the energy consumption
    of the backup cloud resources, as these are not affected by the integration of
    smart grid technologies on the edge infrastructure. Table 10 Simulation results
    for smart grid scenarios Full size table The EDCs’ energy demand and the mean
    service delay of all the smart grid scenarios are the same as the base case. However,
    the smart grid scenarios can reduce the overall energy consumption by 20.3% due
    to the PV system installed in the EDCs. As the PV system and resource management
    policies are the same in all the smart grid experiments, there is no significant
    difference in the overall energy consumption. However, the energy cost is different
    for each configuration. Simulations with US2000C batteries show a 24.6/25.4% reduction
    in service cost. On the other hand, simulations with US3000C batteries achieved
    a 29.1/30.3% reduction. This is because the maximum capacity of these batteries
    is 47.8% greater than the US2000C model, allowing the EDCs to store more energy
    to reduce their energy consumption for a longer time when the electricity price
    is high. Alternatively, the Strict storage controller policy reduces the overall
    service cost compared to the Loose policy. We achieve this reduction by limiting
    battery usage to higher electricity prices. Figures 28 and 29 show the energy
    stored in the EDCs’ batteries for the scenarios with the US3000C battery and the
    Loose and Strict power storage policies, respectively. Green areas correspond
    to periods when the electricity price is equal to or less than \\(\\text {PRICE}^{\\text
    {charge}}_{\\text {CONSR}}\\). On the other hand, red areas represent periods
    when the electricity price is equal to or greater than \\(\\text {PRICE}^{\\text
    {discharge}}_{\\text {CONSR}}\\). The Loose policy starts to charge the battery
    at 2:00 AM when the electricity price reaches 20 $/MWh. EDCs can charge their
    batteries until 5:00 AM when the electricity price is 23 $/MWh. However, their
    batteries are fully charged one hour before, approximately. On the other hand,
    the Strict policy allows EDCs to charge their batteries at 3:00 AM when the electricity
    price is 19 $/MWh. Even though the Strict policy starts charging the batteries
    later, the charging time window is enough to charge the batteries while saving
    energy costs. Fig. 28 Energy stored by EDCs with US3000C battery and Loose power
    storage policy Full size image Fig. 29 Energy stored by EDCs with US3000C battery
    and Strict power storage policy Full size image From 7:00 AM to 8:00 AM (i.e.,
    the first red area), the electricity price is above \\(\\text {PRICE}^{\\text
    {discharge}}_{\\text {CONSR}}\\) in both scenarios. Thus, the storage controllers
    subtract energy from their batteries to reduce the EDCs’ power consumption. With
    the Loose policy, EDCs get electricity from the batteries until 9:00 AM. The capacity
    of the batteries decreases at different rates, depending on their corresponding
    power demand. None of the batteries fully discharges. However, their capacity
    is significantly less with the Loose policy, as EDCs use them for a prolonged
    period. From 10:00 AM to 4:00 PM, the storage controllers can consume power to
    charge the batteries. However, batteries reach their maximum capacity in less
    than an hour. Note that the scenario with the Loose policy requires more energy
    to charge the batteries, as they are initially less charged. With the Loose policy,
    EDCs start discharging their batteries at 6:00 PM to reduce their power consumption.
    Batteries are fully discharged around 8:00 PM, coinciding when the electricity
    price reaches its maximum value (59 $/MWh). In contrast, with the Strict policy,
    batteries provide energy from 7:00 PM to 9:00 PM, reducing the EDCs’ power consumption
    when electricity is more expensive. Again, choosing the best configuration depends
    on the specific use case. For instance, even though the US3000C batteries show
    better performance, we need to consider the trade-off between a higher capital
    expense for buying batteries with better performance and overall operational costs.
    We may also consider heterogeneous scenarios in which EDCs edc_0 and edc_1 have
    a US2000C battery while EDC edc_2 contains a US3000C battery, as its overall utilization
    is higher. Here we select the scenario with US3000C batteries and the Strict storage
    controller policy. Table 10 highlights this scenario in bold. Table 11 contains
    additional simulation results of the selected scenario. Table 11 Simulation results
    in smart grid scenario Full size table While the average utilization, PUE, and
    power demand are similar to the base case scenario (see Table 9), power consumption
    differs significantly. Figure 30 depicts the power consumption curves of all the
    EDCs of the edge computing federation. The behavior of the Strict storage controller
    policy is also displayed in this figure. Additionally, blue areas highlight periods
    when EDCs return energy to the grid. Fig. 30 Power consumption per EDC in smart
    grid scenario Full size image The power consumption curves show several abrupt
    changes throughout the simulation. These changes are mainly caused by the Strict
    storage controller policy of the scenario and coincide with changes in the EDCs’
    battery capacity. For example, from 3:00 AM to 5:00 AM, EDCs consume 1.78 kW more
    to charge their batteries. Then, at 7:00 AM, EDCs are allowed to discharge their
    battery. Thus, their power consumption drops to 0 W. Note that, at 7:30 AM, the
    power demand of EDC edc_2 is slightly higher than 1.78 kW (i.e., the maximum discharge
    power of the battery). Thus, its power consumption presents a small peak of 200
    kW. From 8:00 AM to 5:00 PM, the PV systems of the EDCs generate a considerable
    amount of power (see Fig. 27), resulting in a reduction in power consumption.
    At 10:00 AM, the EDCs charge their batteries again, leading to a power consumption
    peak. From 11:00 AM to 4:00 PM, all the batteries are fully charged. However,
    the power demand of EDC edc_1 is sometimes lower than the power generated by its
    PV system, leading to short periods of energy returned to the grid. At the end
    of the simulation, EDC edc_1 returned 16.11 Wh to the grid. Overall, EDCs edc_0,
    edc_1, and edc_2 reduced their energy consumption by 20.7%, 27.3%, and 16.0% compared
    to the base case scenario, respectively. Figure 31 compares the accumulated energy
    cost of the EDCs in the selected scenario. The figure depicts the behavior of
    the Strict storage controller policy as in the previous figures. Fig. 31 Accumulated
    energy cost per EDC in smart grid scenario Full size image When entering the green
    areas, the slope of the accumulated increments because the storage controllers
    consume additional energy to charge their corresponding storage unit. Once the
    storage units are completely charged, the slope of the curves decreases again.
    In contrast, when the curves enter a red area (i.e., storage controllers allow
    the use of energy stored in the storage units), the accumulated cost curves stagnate
    while batteries discharge. Once the storage units are discharged, the slope of
    the curves increments again. From 8:00 AM to 7:00 PM, the cost curves standstill
    due to the electricity generated by the PV systems. Overall, EDCs edc_0, edc_1,
    and edc_2 reduced their energy costs by 32.3%, 36.4%, and 25.2%, respectively.
    These results show that, with the proposed smart grid-aware architecture, EDCs
    can achieve greater cost savings than the power consumption decrease. 5.4 Discussion
    While edge computing aims to significantly improve the QoS of applications that
    require computation offloading services while reducing the carbon footprint of
    state-of-the-art cloud facilities, there is still room for improvement. The architecture
    proposed in this research integrates aspects of smart grids to provide edge computing
    infrastructure with new mechanisms to reduce both its energy consumption and associated
    costs. EDCs include energy sources and storage units to reduce their electricity
    consumption. Furthermore, they behave as smart grid consumers and change their
    power consumption patterns depending on the current electricity price. We extended
    the Mercury M&S&O framework to include this new proposed architecture. Now, Mercury
    allows us to explore the effect of different resource management techniques on
    the power consumption of the solution. Additionally, we can use Mercury to determine
    which electricity source and storage infrastructures our scenario may need. We
    presented several scenarios to illustrate our proposal and simulated them using
    Mercury. The simulation results show that cooperation between edge computing resources
    and smart grids can significantly reduce the energy consumption of the next-generation
    computing infrastructures. Note that the integration of smart grid infrastructures
    presented in this work is compatible with other studies with different approaches
    regarding the architecture and usage of edge computing infrastructures. These
    works can extend the behavior of edge nodes to include smart grid-related features
    and reduce their associated energy consumption and costs. With this regard, the
    hierarchical and modular approach of the DEVS formalism eased the integration
    of the presented model in Mercury. In the context of the M&S of complex systems,
    it is vital to follow a robust and modular approach that helps us when extending
    its functionality or reiterating its specification. 6 Conclusion We proposed an
    M&S&O approach for studying federated edge computing infrastructures connected
    to smart grids. The model presented in this paper allows us to define federated
    edge computing infrastructures that dynamically adapt to the current system load
    to optimize their energy consumption while meeting the required QoS. The EDCs
    of the federation integrate DTs of their resources to enforce high-level resource
    management policies. Furthermore, the EDCs can dynamically change these policies
    depending on their current state and the predicted service demand. If the edge
    computing infrastructure runs gets congested, it can redirect new requests to
    a cloud facility with backup computation resources. Additionally, we integrate
    smart grid-related parameters in the resource management policies of the EDCs
    to reduce the electricity consumption and overall cost of the federation. By doing
    so, we enable more sustainable edge computing infrastructures that rely on smart
    grid advantages (e.g., electricity demand curve flattening or self-generation)
    to reduce their carbon footprint and increase their economic feasibility. We implemented
    this model in Mercury, an M&S&O framework based on the DEVS mathematical formalism.
    With Mercury, it is possible to run multiple simulations of scenarios with different
    configurations (e.g., IT resources, energy storage units, or energy consumption
    policies) and compare their performance to find an optimal system design. To illustrate
    how Mercury works, we presented a realistic use case scenario of an edge computing
    federation that provides computation offloading services to taxis in the San Francisco
    Bay Area. We showed how the proposed approach helps edge computing federation
    operators to increase their resilience to electricity price fluctuations, reducing
    electricity consumption by 20.3% and operational expenses by 30.3%. In future
    work, we plan to include regional clouds in the edge computing model with more
    complex interrelationships. We also want to add temperature models for PUs to
    integrate the temperature into the resource management policies. Doing so can
    avoid hot spots within an EDC that may damage the IT equipment. Regarding the
    smart grid model, we want to expand the capabilities of the electricity providers
    to enable more sophisticated pricing schemes. We are currently working on adding
    a decision support system to Mercury. This system will automatically optimize
    multiple parameters of the scenario using different metaheuristic search methods
    (e.g., simulated annealing or Tabu search). Data availability The datasets generated
    and/or analyzed during the current study are publicly available in Mercury’s GitHub
    repository: https://github.com/greenlsi/mercury_mso_framework. References Gartner
    Survey Reveals 47% of Organizations Will Increase Investments in IoT Despite the
    Impact of COVID-19. Gartner, Inc. (2020). https://www.gartner.com/en/newsroom/press-releases/2020-10-29-gartner-survey-reveals-47-percent-of-organizations-will-increase-investments-in-iot-despite-the-impact-of-covid-19-
    Stergiou, C., Psannis, K.E., Kim, B.-G., Gupta, B.: Secure integration of IoT
    and cloud computing. Future Gener. Comput. Syst. 78, 964–975 (2018). https://doi.org/10.1016/j.future.2016.11.031
    Article   Google Scholar   Chang, H., Hari, A., Mukherjee, S., Lakshman, T.V.:
    Bringing the cloud to the edge. In: 2014 IEEE Conference on Computer Communications
    Workshops (INFOCOM WKSHPS), 2014, pp. 346–351 (2014). https://doi.org/10.1109/INFCOMW.2014.6849256
    Shi, W., Cao, J., Zhang, Q., Li, Y., Xu, L.: Edge computing: vision and challenges.
    IEEE Internet Things J. 3(5), 637–646 (2016). https://doi.org/10.1109/JIOT.2016.2579198
    Article   Google Scholar   Pan, J., McElhannon, J.: Future edge cloud and edge
    computing for Internet of Things applications. IEEE IoT J. 5(1), 439–449 (2018).
    https://doi.org/10.1109/JIOT.2017.2767608 Article   Google Scholar   Shi, W.,
    Cao, J., Zhang, Q., Li, Y., Xu, L.: Edge computing: vision and challenges. IEEE
    IoT J. 3(5), 637–646 (2016). https://doi.org/10.1109/JIOT.2016.2579198 Article   Google
    Scholar   Dileep, G.: A survey on smart grid technologies and applications. Renew.
    Energy 146, 2589–2625 (2020). https://doi.org/10.1016/j.renene.2019.08.092 Article   Google
    Scholar   Feng, C., Wang, Y., Chen, Q., Strbac, G., Kang, C.: Smart grid encounters
    edge computing: opportunities and applications. Adv. Appl. Energy (2020). https://doi.org/10.1016/j.adapen.2020.100006
    Article   Google Scholar   Jimenez-Castillo, G., Tina, G., Munoz-Rodriguez, F.,
    Rus-Casas, C.: Smart meters for the evaluation of self-consumption in zero energy
    buildings. In: 2019 10th International Renewable Energy Congress (IREC), 2019,
    pp. 1–6. IEEE (2019). https://doi.org/10.1109/IREC.2019.8754609 Oprea, S.V., Bâra,
    A., Ifrim, G.: Flattening the electricity consumption peak and reducing the electricity
    payment for residential consumers in the context of smart grid by means of shifting
    optimization algorithm. Comput. Ind. Eng. 122, 125–139 (2018). https://doi.org/10.1016/j.cie.2018.05.053
    Article   Google Scholar   Friedenthal, S., Moore, A., Steiner, R.: A Practical
    Guide to SysML: The Systems Modeling Language, 3rd edn. Elsevier, Amsterdam (2015).
    ISBN 978-0-12-800202-5 Mittal, S., Tolk, A.: Complexity Challenges in Cyber Physical
    Systems: Using Modeling and Simulation (M&S) to Support Intelligence, Adaptation
    and Autonomy. Stevens Institute Series on Complex Systems and Enterprises. Wiley,
    New York (2019). ISBN 9781119552468 Zeigler, B.P., Muzy, A., Kofman, E.: Theory
    of Modeling and Simulation: Discrete Event and Iterative System Computational
    Foundations, 3rd edn. Academic, San Diego (2019). ISBN 978-0-12-813370-5 Cárdenas,
    R., Arroba, P., Blanco, R., Malagón, P., Risco-Martín, J.L., Moya, J.M.: Mercury:
    a modeling, simulation, and optimization framework for data stream-oriented IoT
    applications. Simul. Model. Pract. Theory 101, 102037 (2020). https://doi.org/10.1016/j.simpat.2019.102037.
    (Modeling and Simulation of Fog Computing) Article   Google Scholar   Cárdenas,
    R., Arroba, P., Martín, J.L.R.: Bringing AI to the edge: a formal M&S specification
    to deploy effective IoT architectures. J. Simul. (2021). https://doi.org/10.1080/17477778.2020.1863755
    Article   Google Scholar   Khan, W.Z., Ahmed, E., Hakak, S., Yaqoob, I., Ahmed,
    A.: Edge computing: a survey. Future Gener. Comput. Syst. 97, 219–235 (2019).
    https://doi.org/10.1016/j.future.2019.02.050 Article   Google Scholar   Dong,
    Y., Guo, S., Liu, J., Yang, Y.: Energy-efficient fair cooperation fog computing
    in mobile edge networks for smart city. IEEE IoT J. 6(5), 7543–7554 (2019). https://doi.org/10.1109/JIOT.2019.2901532
    Article   Google Scholar   Etemadi, M., Ghobaei-Arani, M., Shahidinejad, A.: A
    cost-efficient auto-scaling mechanism for IoT applications in fog computing environment:
    a deep learning-based approach. Clust. Comput. (2021). https://doi.org/10.1007/s10586-021-03307-2
    Article   Google Scholar   Al-Zoubi, K., Wainer, G.: Fog and cloud collaboration
    to perform virtual simulation experiments. Simul. Model. Pract. Theory 101, 102032
    (2020). https://doi.org/10.1016/j.simpat.2019.102032 Article   Google Scholar   gan
    Zhang, D., hao Ni, C., Zhang, J., Zhang, T., Yang, P., xuWang, J., ran Yan, H.:
    A novel edge computing architecture based on adaptive stratified sampling. Comput.
    Commun. 183, 121–135 (2022). https://doi.org/10.1016/j.comcom.2021.11.012 Article   Google
    Scholar   Dong, R., She, C., Hardjawana, W., Li, Y., Vucetic, B.: Deep learning
    for hybrid 5G services in mobile edge computing systems: learn from a Digital
    Twin. IEEE Trans. Wirel. Commun. 18(10), 4692–4707 (2019). https://doi.org/10.1109/TWC.2019.2927312
    Article   Google Scholar   Tao, F., Zhang, H., Liu, A., Nee, A.Y.C.: Digital twin
    in industry: state-of-the-art. IEEE Trans. Ind. Inform. 15(4), 2405–2415 (2019).
    https://doi.org/10.1109/TII.2018.2873186 Article   Google Scholar   Qi, Q., Tao,
    F.: Digital twin and big data towards smart manufacturing and Industry 4.0: 360
    degree comparison. IEEE Access 6, 3585–3593 (2018). https://doi.org/10.1109/ACCESS.2018.2793265
    Article   Google Scholar   Chen, X., Lu, Z., Ni, W., Wang, X., Wang, F., Zhang,
    S., Xu, S.: Cooling-aware optimization of edge server configuration and edge computation
    offloading for wirelessly powered devices. IEEE Trans. Veh. Technology 70(5),
    5043–5056 (2021). https://doi.org/10.1109/TVT.2021.3076057 Article   Google Scholar   Zoie,
    R.C., DeliaMihaela, R., Alexandru, S.: An analysis of the power usage effectiveness
    metric in data centers. In: 2017 5th International Symposium on Electrical and
    Electronics Engineering (ISEEE), 2017, pp. 1–6. https://doi.org/10.1109/ISEEE.2017.8170650
    Masanet, E., Shehabi, A., Lei, N., Smith, S., Koomey, J.: Recalibrating global
    data center energy-use estimates. Science 367(6481), 984–986 (2020). https://doi.org/10.1126/science.aba375
    Article   Google Scholar   Jones, N.: How to stop data centres from gobbling up
    the world’s electricity. Nature 561(7722), 163–167 (2018). https://doi.org/10.1038/d41586-018-06610-y
    Article   Google Scholar   Ebrahimi, K., Jones, G.F., Fleischer, A.S.: A review
    of data center cooling technology, operating conditions and the corresponding
    low-grade waste heat recovery opportunities. Renew. Sustain. Energy Rev. 31, 622–638
    (2014). https://doi.org/10.1016/j.rser.2013.12.007 Article   Google Scholar   Li,
    J., Zhou, G., Tian, T., Li, X.: A new cooling strategy for edge computing servers
    using compact looped heat pipe. Appl. Therm. Eng. 187, 116599 (2021). https://doi.org/10.1016/j.applthermaleng.2021.116599
    Article   Google Scholar   Qayyum, T., Malik, A.W., Khattak, M.A.K., Khalid, O.,
    Khan, S.U.: FogNetSim++: a toolkit for modeling and simulation of distributed
    fog environment. IEEE Access 6, 63570–63583 (2018). https://doi.org/10.1109/ACCESS.2018.2877696
    Article   Google Scholar   Lera, I., Guerrero, C., Juiz, C.: YAFS: a simulator
    for IoT scenarios in fog computing. IEEE Access 7, 91745–91758 (2019). https://doi.org/10.1109/ACCESS.2019.2927895
    Article   Google Scholar   Brogi, A., Forti, S.: QoS-aware deployment of IoT applications
    through the fog. IEEE IoT J. 4(5), 1185–1192 (2017). https://doi.org/10.1109/JIOT.2017.2701408
    Article   Google Scholar   Gupta, H., VahidDastjerdi, A., Ghosh, S. K., Buyya,
    R.: iFogSim: a toolkit for modeling and simulation of resource management techniques
    in the Internet of Things, edge and fog computing environments. Softw. Pract.
    Exp. 47(9), 1275–1296 (2017). https://doi.org/10.1002/spe.2509 Article   Google
    Scholar   Sonmez, C., Ozgovde, A., Ersoy, C.: EdgeCloudSim: an environment for
    performance evaluation of edge computing systems. Trans. Emerg. Telecommun. Technol.
    (2018). https://doi.org/10.1002/ett.3493 Article   Google Scholar   Zeng, X.,
    Garg, S.K., Strazdins, P., Jayaraman, P.P., Georgakopoulos, D., Ranjan, R.: IOTSim:
    a simulator for analysing IoT applications. J. Syst. Archit. 72, 93–107 (2017).
    https://doi.org/10.1016/j.sysarc.2016.06.008 Article   Google Scholar   Greer,
    C., Wollman, D., Prochaska, D., Boynton, P., Mazer, J., Nguyen, C., FitzPatrick,
    G., Nelson, T., Koepke, G., Hefner, A., Pillitteri, V., Brewer, T., Golmie, N.,
    Su, D., Eustis, A., Holmberg, D., Bushby, S.: NIST Framework and Roadmap for Smart
    Grid Interoperability Standards, Release 4.0 (Draft). NIST, Gaithersburg (2021)
    Google Scholar   Cintuglu, M.H., Mohammed, O.A., Akkaya, K., Uluagac, A.S.: A
    survey on smart grid cyber–physical system testbeds. IEEE Commun. Surv. Tutor.
    19(1), 446–464 (2017). https://doi.org/10.1109/COMST.2016.2627399 Article   Google
    Scholar   Ahmed, N., Levorato, M., Li, G.P.: Residential consumer-centric demand
    side management. IEEE Trans. Smart Grid 9(5), 4513–4524 (2018). https://doi.org/10.1109/TSG.2017.2661991
    Article   Google Scholar   Hu, R.L., Skorupski, R., Entriken, R., Ye, Y.: A mathematical
    programming formulation for optimal load shifting of electricity demand for the
    smart grid. IEEE Trans. Big Data 6(4), 638–651 (2020). https://doi.org/10.1109/TBDATA.2016.2639528
    Article   Google Scholar   Varghese, A.C., Padmini, V., Kumar, G., Khaparde, S.A.:
    Smart grid consumer behavioral model using machine learning. In: International
    Conference on Innovative Smart Grid Technologies, ISGT Asia 2018, 2018, pp. 734–739.
    IEEE (2018). ISBN 9781538642917. https://doi.org/10.1109/ISGT-Asia.2018.8467824
    Yang, J., Zhao, J., Luo, F., Wen, F., Dong, Z.Y.: Decision-making for electricity
    retailers: a brief survey. IEEE Trans. Smart Grid 9(5), 4140–4153 (2018). https://doi.org/10.1109/TSG.2017.2651499
    Article   Google Scholar   Vaubourg, J., Presse, Y., Camus, B., Bourjot, C., Ciarletta,
    L., Chevrier, V., Tavella, J.-P., Morais, H.: Multi-agent multi-model simulation
    of smart grids in the MS4SG project. In: Demazeau, Y., Decker, K.S., Bajo Pérez,
    J., de la Prieta, F. (eds) Advances in Practical Applications of Agents, Multi-agent
    Systems, and Sustainability: The PAAMS Collection, pp. 240–251. Springer (2015).
    ISBN 978-3-319-18944-4. https://doi.org/10.1007/978-3-319-18944-4_20 Lin, H.,
    Veda, S.S., Shukla, S.S., Mili, L., Thorp, J.: GECO: global event-driven co-simulation
    framework for interconnected power system and communication network. IEEE Trans.
    Smart Grid 3(3), 1444–1456 (2012). https://doi.org/10.1109/TSG.2012.2191805 Article   Google
    Scholar   Rohjans, S., Lehnhoff, S., Schütte, S., Scherfke, S., Hussain, S.: Mosaik—a
    modular platform for the evaluation of agent-based Smart Grid control. In: 2013
    4th IEEE/PES Innovative Smart Grid Technologies Europe, ISGT Europe 2013, pp.
    1–5 (2013). ISBN 9781479929849. https://doi.org/10.1109/ISGTEurope.2013.6695486
    Samie, F., Bauer, L., Henkel, J.: Edge Computing for Smart Grid: An Overview on
    Architectures and Solutions, pp. 21–42. Springer, Cham (2019). ISBN 978-3-030-03640-9
    Huang, Y., Lu, Y., Wang, F., Fan, X., Liu, J., Leung, V.C.: An edge computing
    framework for real-time monitoring in smart grid. In: Proceedings—2018 IEEE International
    Conference on Industrial Internet, ICII 2018, 2018, no. icii, pp. 99–108. IEEE
    (2018). ISBN 9781538677711. https://doi.org/10.1109/ICII.2018.00019 Liu, Y., Yang,
    C., Jiang, L., Xie, S., Zhang, Y.: Intelligent edge computing for IoT-based energy
    management in smart cities. IEEE Netw. 33(2), 111–117 (2019). https://doi.org/10.1109/MNET.2019.1800254
    Article   Google Scholar   Gai, K., Wu, Y., Zhu, L., Xu, L., Zhang, Y.: Permissioned
    Blockchain and edge computing empowered privacy-preserving smart grid networks.
    IEEE IoT J. 6(5), 7992–8004 (2019). https://doi.org/10.1109/JIOT.2019.2904303
    Article   Google Scholar   Cárdenas, R., Arroba, P., Moya, J.M., Risco-Martín,
    J.L.: Multi-faceted modeling in the analysis and optimization of IoT complex systems.
    In: Proceedings of the 2020 Summer Simulation Conference. Virtual Event, July
    2020, pp. 1–12. Society for Computer Simulation International (2020) Cárdenas,
    R.: Mercury M&S&O Framework for Fog Computing. https://github.com/greenlsi/mercury_mso_framework
    Jäger-Waldau, A., Bucher, C., Frederiksen, K.H.B., Guerro-Lemus, R., Mason, G.,
    Mather, B., Mayr, C., Moneta, D., Nikoletatos, J., Roberts, M.B.: Self-consumption
    of electricity produced from PV systems in apartment buildings—comparison of the
    situation in Australia, Austria, Denmark, Germany, Greece, Italy, Spain, Switzerland
    and the USA. In: 2018 IEEE 7th World Conference on Photovoltaic Energy Conversion
    (WCPEC) (A Joint Conference of 45th IEEE PVSC, 28th PVSEC, 34th EU PVSEC), 2018,
    pp. 1424–1430 (2018). https://doi.org/10.1109/PVSC.2018.8547583 Piorkowski, M.,
    Sarafijanovoc-Djukic, N., Grossglauser, M.: A parsimonious model of mobile partitioned
    networks with clustering. In: The First International Conference on COMmunication
    Systems and NETworkS (COMSNETS), January 2009 (2009). https://doi.org/10.1109/COMSNETS.2009.4808865
    Pérez, S., Pérez, J., Arroba, P., Blanco, R., Ayala, J.L., Moya, J.M.: Predictive
    GPU-based ADAS management in energy-conscious smart cities. In: 2019 IEEE International
    Smart Cities Conference (ISC2), 2019, pp. 349–354. IEEE (2019). https://doi.org/10.1109/ISC246665.2019.9071685
    Download references Funding Open Access funding provided thanks to the CRUE-CSIC
    agreement with Springer Nature. This paper has been supported by the Spanish Ministry
    of Economic Affairs and Digital Transformation (MINECO) under Grant PID2019-110866RB-I00.
    We also would like to thank Google, Inc. for giving the Google Cloud Platform
    (GCP) Education Grant for this investigation. Author information Authors and Affiliations
    Universidad Politécnica de Madrid, Avenida Complutense 30, 28040, Madrid, Spain
    Román Cárdenas, Patricia Arroba & José M. Moya Carleton University, 1125 Colonel
    By Drive, Ottawa, ON, K1S 5B6, Canada Román Cárdenas Universidad Complutense de
    Madrid, C/ Prof. José García Santesmases 9, 28040, Madrid, Spain José L. Risco-Martín
    Contributions All authors contributed to the study conception and design. Material
    preparation, data collection and analysis were performed by RC. The first draft
    of the manuscript was written by RC and all authors commented on previous versions
    of the manuscript. All authors read and approved the final manuscript. Corresponding
    author Correspondence to Román Cárdenas. Ethics declarations Conflict of interest
    The authors have no relevant financial or non-financial interests to disclose.
    Additional information Publisher''s Note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article''s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article''s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Cárdenas, R., Arroba, P., Risco-Martín,
    J.L. et al. Modeling and simulation of smart grid-aware edge computing federations.
    Cluster Comput 26, 719–743 (2023). https://doi.org/10.1007/s10586-022-03797-8
    Download citation Received 21 December 2021 Revised 18 October 2022 Accepted 25
    October 2022 Published 11 November 2022 Issue Date February 2023 DOI https://doi.org/10.1007/s10586-022-03797-8
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Edge computing IoT MBSE Simulation Smart grid Use our pre-submission
    checklist Avoid common mistakes on your manuscript. Associated Content Part of
    a collection: Computer Science SDG 7: Affordable and Clean Energy Sections Figures
    References Abstract Introduction Related work Edge computing model Consumer-centric
    smart grid model Use case Conclusion Data availability References Funding Author
    information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Cluster Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Modeling and simulation of smart grid-aware edge computing federations
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ananth K.R.
  - Punna H.S.
  - Selvaraj K.
  - Rajagopal K.
  - Mahajan V.
  - Sakthivel S.
  citation_count: '0'
  description: Data creation in the network's periphery has increased exponentially
    as IoT devices have become more commonplace. To address the challenges this large
    data deluge poses, sustainable and efficient data processing technologies are
    required. The term 'Green IoT Edge Computing' refers to a new strategy that uses
    distributed data processing and renewable energy to improve the efficiency and
    longevity of Internet of Things (IoT) applications. In the Internet of Things
    (IoT), edge computing is used to process data locally, at the network's periphery,
    rather than forwarding it to a centralized data center. This method improves real-time
    decision-making, minimizes network latency, and helps preserve bandwidth. However,
    in locations with limited access to power or other resources, the high power consumption
    of edge computing devices continues to be a major issue. Green IoT edge computing
    addresses this challenge by combining renewable energy sources like solar panels
    and wind turbines with traditional edge computing facilities. This guarantees
    low- impact and long-term power for edge devices, cutting down on their environmental
    impact and operational expenses. This strategy harmonizes IoT operations with
    environmental sustainability targets by utilizing renewable energy sources. The
    ability to handle data in a distributed manner is also crucial to green IoT edge
    computing. Rather than a centralized server, a distributed group of edge devices
    processes data. This improves energy efficiency as well as fault tolerance and
    dependability. Edge devices working together to properly spread processing activities
    can reduce the demand for powerful, energy-hungry servers. Green IoT Edge Computing's
    primary invention is the 'Renewable Edge Optimizer.' This clever piece of code
    automatically adjusts data processing tasks in accordance with the availability
    of renewable power. When clean energy is plentiful, the optimizer may shift more
    data processing to distributed nodes, relieving pressure on primary data centers.
    Alternatively, it can prioritize vital activities or temporarily offload processing
    to cloud resources that are energy efficient at times of low renewable energy
    availability. Finally, green IoT edge computing, which uses renewable energy and
    is designed for distributed data processing, provides an effective and environmentally
    friendly response to the problems caused by the IoT data boom. It reduces energy
    use and carbon emissions, making it a desirable option for smart city and remote
    agriculture monitoring applications, among others. As an added bonus, this method
    also makes for a more environmentally friendly and sustainable future in the realm
    of Internet of Things (IoT) technologies.
  doi: 10.1109/UPCON59197.2023.10434734
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 10th IEEE Uttar Pradesh ...
    Green IoT Edge Computing Towards Sustainable and Distributed Data Processing Publisher:
    IEEE Cite This PDF K.R. Ananth; Hari Shankar Punna; Krishnamoorthy Selvaraj; Rajagopal
    K; Vaishali Mahajan; Sakthivel S. All Authors 18 Full Text Views Abstract Document
    Sections I. Introduction II. Related Study III. Methodology IV. Results and Discussions
    V. Conclusion Show Full Outline Authors Figures References Keywords Metrics Abstract:
    Data creation in the network''s periphery has increased exponentially as IoT devices
    have become more commonplace. To address the challenges this large data deluge
    poses, sustainable and efficient data processing technologies are required. The
    term “Green IoT Edge Computing” refers to a new strategy that uses distributed
    data processing and renewable energy to improve the efficiency and longevity of
    Internet of Things (IoT) applications. In the Internet of Things (IoT), edge computing
    is used to process data locally, at the network''s periphery, rather than forwarding
    it to a centralized data center. This method improves real-time decision-making,
    minimizes network latency, and helps preserve bandwidth. However, in locations
    with limited access to power or other resources, the high power consumption of
    edge computing devices continues to be a major issue. Green IoT edge computing
    addresses this challenge by combining renewable energy sources like solar panels
    and wind turbines with traditional edge computing facilities. This guarantees
    low- impact and long-term power for edge devices, cutting down on their environmental
    impact and operational expenses. This strategy harmonizes IoT operations with
    environmental sustainability targets by utilizing renewable energy sources. The
    ability to handle data in a distributed manner is also crucial to green IoT edge
    computing. Rather than a centralized server, a distributed group of edge devices
    processes data. This improves energy efficiency as well as fault tolerance and
    dependability. Edge devices working together to properly spread processing activities
    can reduce the demand for powerful, energy-hungry servers. Green IoT Edge Computing''s
    primary invention is the “Renewable Edge Optimizer.” This clever piece of code
    automatically adjusts data processing tasks in accordance with the availability
    of renewable power. When clean energy is plentiful, the optimizer may shift more
    data processing to distributed nodes, relieving press... (Show More) Published
    in: 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical,
    Electronics and Computer Engineering (UPCON) Date of Conference: 01-03 December
    2023 Date Added to IEEE Xplore: 26 February 2024 ISBN Information: ISSN Information:
    DOI: 10.1109/UPCON59197.2023.10434734 Publisher: IEEE Conference Location: Gautam
    Buddha Nagar, India SECTION I. Introduction Connecting billions of devices and
    producing an unparalleled volume of data, the Internet of Things (IoT) has evolved
    as a revolutionary technological paradigm. From healthcare and agriculture to
    transportation and industry, many sectors stand to benefit from this network of
    intelligent devices. Challenges in data processing, energy consumption, and sustainability
    have emerged in tandem with the IoT''s meteoric rise. Latency difficulties and
    wasteful use of network resources are common in traditional IoT systems because
    of their reliance on centralized data processing in far-flung data centers. IoT
    systems are less sustainable for the environment because of the energy needed
    to run their centralized data centers, which results in a considerable amount
    of carbon emissions. There is a paradigm change happening in response to these
    difficulties, and it''s called “Green IoT Edge Computing.” A. Iot and Its Challenges
    The Internet of Things (IoT) is a system of networked computing devices, electronic
    sensors, and mechanical or electrical actuators that together form a global computer
    network. These gadgets may be found anywhere, from urban neighborhoods to industrial
    complexes to secluded wilderness areas. They improve productivity, convenience,
    and safety in many different contexts by allowing for real- time monitoring, control,
    and decision-making. Managing the massive amounts of data produced by edge devices
    is a major obstacle in the Internet of Things. Sensors in everything from smart
    thermostats to the cameras in self- driving cars generate massive volumes of data
    in real time. Sending all this information to a few central cloud data centers
    for processing might cause bottlenecks in the network, significant delays in data
    transmission, and higher overall operational expenses. Also, for edge devices
    in off-grid or rural areas, energy efficiency is of paramount importance. Since
    many of these gadgets run on batteries, constant use might need regular battery
    changes or recharging, which is inconvenient, wasteful, and potentially harmful
    to the environment. It is essential for Internet of Things (IoT) applications
    to reduce their energy footprint without compromising performance or responsiveness.
    B. Enter Green Iot Edge Computing Green IoT Edge Computing is an all-encompassing
    strategy that aims to solve these problems by combining the following: Green Internet
    of Things: Its Core Concept Instead of sending all data to a remote data center,
    Edge Computing processes data at its origin, at the network''s periphery. Data
    collected by sensors, gateways, and edge servers is processed either at the device''s
    location or at a nearby data center. This ensures that answers to real-time events
    may be made as quickly as possible by minimizing data transfer and lowering latency.
    Edge Computing: To ensure the long-term viability of the Internet of Things (IoT),
    Green IoT Edge Computing incorporates renewable energy sources including solar
    panels, wind turbines, and micro-hydro generators to power edge nodes. These gadgets
    are able to function autonomously and with minimal influence on the environment
    since they draw power from renewable sources. This is of critical importance for
    deployments in distant areas, where access to conventional power sources may be
    restricted. Distributed Data Processing: In conventional IoT designs, a single
    server handles all of the data collection and analysis. In contrast, the focus
    of green IoT edge computing is on decentralized information management. Distributed
    data processing and analysis on the edge facilitates resilience, scalability,
    and efficient resource use. By working together, IoT systems are made more robust
    and efficient. Renewable Edge Optimizer: The “Renewable Edge Optimizer,” a smart
    software module that dynamically adjusts data processing workloads in response
    to the availability of renewable energy, is at the core of Green IoT Edge Computing.
    When clean energy is plentiful, the optimizer may shift more data processing to
    distributed nodes, relieving pressure on primary data centers. Alternatively,
    it can prioritize vital activities or temporarily offload processing to cloud
    resources that are energy efficient at times of low renewable energy availability.
    In conclusion, Green IoT Edge Computing is a ground- breaking strategy for tackling
    the dynamic difficulties of the Internet of Things, including data processing,
    energy efficiency, and sustainability. This change in thinking has the potential
    to considerably contribute to a greener and more sustainable future, in addition
    to enhancing the efficiency and cost-effectiveness of IoT applications. In the
    following sections, we''ll examine Green IoT Edge Computing in further detail
    and look at some real-world applications of this technology in action. SECTION
    II. Related Study The exponential increase in data collection brought about by
    the proliferation of IoT devices presents serious challenges in terms of both
    power usage and data processing. One promising approach to overcoming these issues
    is green internet of things edge computing. It does this by enabling resilient,
    decentralized data processing at the network''s periphery. An in-depth analysis
    of recent studies in green internet of things edge computing is presented in this
    review paper. The importance of this study in achieving energy efficiency, decreasing
    latency, and promoting sustainability is emphasized throughout the paper. The
    research also goes into subtopics such as edge device topologies, resource management
    strategies, and energy-efficient communication protocols. By thoroughly evaluating
    existing research and case examples, this paper highlights the potential for green
    IoT edge computing to transform data processing in IoT systems. The result will
    be a sustainable and productive future [1]. By facilitating the digitization of
    numerous activities and processes like water distribution, preventative maintenance,
    or smart manufacturing, IoT may help pave the path to the circular economy and
    to a more sustainable society. Despite their enormous promise for the digital
    transition to sustainability, IoT technologies and concepts like edge computing
    are not yet contributing to the sustainable growth of the IoT industry. In reality,
    this industry uses a lot of energy in its production, operation, and recycling
    procedures, leaving a sizable carbon footprint. However, the emergence of Edge
    Artificial Intelligence (Edge AI) [13], which mandates the consumption of more
    energy, directly conflicts with the Green IoT (G-IoT) paradigm''s sustainable
    ambition to decrease such a carbon footprint. In order to address this issue,
    this paper delves into the many factors that have an effect on edge-AI G-IoT system
    design and development. In addition, the essay provides a real-world example of
    Industry 5.0''s application of the various principles discussed. The proposed
    scenario centers on a smart factory from the fifth industrial revolution, with
    the goal of enhancing worker protections and keeping tabs on production. Mist
    computing, which is built from Internet of Things nodes with artificial intelligence
    capabilities, is used in this scenario. After the use case has been described,
    the energy used and any potential effects on the carbon footprint of various nations
    have been studied. In sum, this article offers advice that may be used by aspiring
    programmers to prepare for the difficulties they''ll encounter when working on
    the next generation of edge-AI G-IoT systems [2]. Overloaded IoT networks and
    widespread cloud/edge interactions have contributed to a meteoric increase in
    the Internet of Things'' (IoT) carbon footprint [14]. Thus, there is a growing
    interest in both business and education to facilitate the efficient use of computing
    infrastructures through the optimization of data centers and IoT resource management
    (including hardware, software, networks, and data) and the reduction of operational
    costs in order to cut greenhouse gas emissions and foster healthy environments.
    The role of cyber security in causing these ecological problems has been explored.
    However, most green security systems put more emphasis on establishing low-overhead
    encryption algorithms than on ensuring security while minimizing energy use. As
    one of the research approaches to promoting sustainable computing in green IoT,
    this study illuminates the growing concept of adaptive cybersecurity. For the
    purpose of reducing energy usage in green computing and resource-constrained IoT
    contexts, this article outlines three possible research topics and their accompanying
    approaches for building and deploying adaptive security in these settings. Greener
    and more environmentally friendly data-driven IoT security solutions will result
    from these kinds of activities [3]. Agriculture 4.0 technology, including IoT-driven
    Precision agriculture (PA), UAVs, and big data analytics are working together
    to revolutionize the agribusiness industry throughout the world[15]. Data from
    sensors and other IoT devices is sent to the cloud or edge for processing and
    then to data centers for storage in PA applications. Large amounts of power are
    required for PA applications due to the high volume of data being sent between
    the billions of linked devices. Greenhouse gases are being produced at an alarming
    rate due to the proliferation of IoT devices, and energy- hungry IoT components
    like sensors are becoming increasingly scarce as a result [16]. Therefore, it
    is necessary to embrace green solutions in order to further the development of
    IoT components that are both environmentally benign and economical. Motivated
    by the need to create a sustainable ecosystem for IoT, we first provide an overview
    of several data processing and energy- conserving solutions based on cloud, edge,
    and cloud computing [21]. We then go on to talk about and assess various Green
    IoT (GIoT) solutions [17] that may be put in place for GIoT-based PA utilizing
    UAVs, LPWANs, and 5G networks, among other implementation difficulties. The integration
    of GIoT components into Agriculture 5.0 as part of the effort to make the IoT
    more environmentally friendly through the use of 5G networks is also explored.
    Using the results of the current study as a starting point, we have developed
    a generalized Internet of Things (GIoT) framework for the development of environmentally
    conscious, low-cost, and widely accessible PA applications [22]. Finally, this
    article provides a comprehensive summary of the many security concerns present
    in the GIoT layers and an in-depth analysis of the countermeasures that may be
    implemented to deal with them [4]. SECTION III. Methodology This study''s methodology
    details the procedures used to research and implement Green IoT Edge Computing
    for sustainable and distributed data processing, with an emphasis on the incorporation
    of renewable energy sources and the creation of an optimizing system for this
    edge. A. Data Collection There are two main components of a data gathering project:
    1) Experimental Data Sensors and data loggers will be used in testbeds and pilot
    deployments to obtain real-world data on energy consumption, renewable energy
    availability, and IoT device performance. In order to assess Green IoT [23] Edge
    Computing''s viability in the real world, these information will be crucial. 2)
    Renewable Energy Integration The following procedures are involved in the incorporation
    of renewable energy sources: a) Energy Resource Assessment The sun irradiance,
    wind velocity, and hydrological parameters of the deployment sites will all be
    thoroughly analyzed. With this analysis in hand, renewable energy systems may
    be chosen and sized with precision. b) Hardware Implementation IoT edge devices
    will be placed and linked to solar panels, wind turbines, and other forms of renewable
    energy. In order to guarantee a constant flow of electricity, cutting- edge power
    management technologies, such as energy storage options, will be used. c) Data
    Monitoring Energy generation, consumption, and overall system efficiency will
    all be tracked via sensors and meters [25]. For the sake of analysis and improvement,
    these numbers will be recorded. B. Distributed Data Processing The following procedures
    are required to put into effect distributed data processing: 1) Edge Device Configuration
    To process and analyze data, edge devices including edge servers, gateways, and
    sensors will be set up to do so locally or inside a nearby edge computing cluster.
    2) Network Architecture To facilitate low-latency communication between edge devices,
    the network architecture will be built to accommodate distributed data processing.
    For streamlined communication, we''ll be using protocols like MQTT[18] and CoAP[19],
    [26]. 3) Load Balancing In order to maximize efficiency and guarantee scalability,
    load balancing algorithms will be developed to divide data processing responsibilities
    across edge devices. C. Renewable Edge Optimizer Development An integral part
    of this investigation is the Renewable Edge Optimizer, which entails the subsequent
    steps: 1) Algorithm Design The availability of renewable energy will be used to
    dynamically distribute data processing jobs, and a new optimization algorithm
    will be created to do so. This algorithm will take into account future energy
    production predictions and device capabilities in addition to current data. 2)
    Software Implementation Software modules containing the optimization technique
    will be deployed to both edge devices and back-end servers. Real-time choices
    will be made possible by its interaction with sensors and energy management systems.
    3) Testing and Validation The Renewable Edge Optimizer will be put through extensive
    testing and validation in a wide range of scenarios, such as those with highly
    variable renewable energy supply, data demands, and IoT application domains. D.
    Evaluation and Data Analysis Both the collected data and the simulation results
    will be examined using statistical and computational methods. Green IoT Edge Computing
    will be assessed based on its capacity to reduce energy consumption, decrease
    response times, and scale. Fig. 1. Proposed System Architecture Show All Equations
    Used: Renewable  Energy Forecasting Model: E − renewable(t)=f(t) (1) View Source
    In order to maximise energy efficiency, the Renewable Edge Optimizer relies heavily
    on this equation, which models the amount of renewable energy available at a particular
    moment (t), such as solar irradiance and wind speed. Energy Consumption Model:
    E consumption =P×t (2) View Source Here, P is the power used by an edge device
    throughout its operational period, and t is the number of seconds. This may be
    used to make educated guesses about the power needs of your Internet of Things
    gadgets. SECTION IV. Results and Discussions Green IoT Edge Computing for sustainable
    and distributed data processing, renewable energy integration, and the creation
    of the Renewable Edge Optimizer are the primary topics of discussion in this study''s
    results and discussion section, which represents a pivotal stage in which we analyze
    and interpret the findings obtained through our methodology. Here, we give the
    empirical findings and analyze them at length, illuminating the merits, difficulties,
    and prospective uses of our method. The sensitivity metric evaluates how well
    a model can classify positive examples from the ground truth. This would be the
    proportion of real-world illness instances when the model yields an accurate prognosis
    in clinical trials. The sensitivity of KNN algorithm is 81%, Random Forest algorithm
    has 86%, Support vector machine have recorded 83% and our proposed method has
    the highest among all 91%. A The degree to which a model accurately recognises
    negative data represents its specificity. In the context of medicine, this would
    be the proportion of healthy people who were appropriately classified as disease-free.
    The specificity of KNN is 83%, Random Forest has 87%, Support vector machine has
    92% and the proposed method has 95%. The F1 score is a harmonic mean of the accuracy
    and memory retention scores. Accuracy refers to how many correct positive forecasts
    there are. The F1 score is helpful when both false positives and negatives need
    to be taken into account, since it strikes a good compromise between the two.
    Our proposed method has the highest among all 74%,where SVM has 69%,Random Forest
    has 70% and the KNN has 64%. Fig. 2. Accuracy comparison graph Show All Table
    I. Performance metrices comparison of proposed method with existing methods The
    accuracy of the KNN has 86%,where the Random Forest has 90% followed by the Support
    vector machine has 93%. the proposed method has 97% of accuracy. SECTION V. Conclusion
    In conclusion, our research has shown the promising potential of Green IoT Edge
    Computing in supporting sustainability and efficiency in the face of the problems
    faced by the exponential rise of IoT data. We have found success by taking a methodical
    approach to renewable energy integration, distributed data processing, and the
    creation of the Renewable Edge Optimizer. Our data shows that using renewable
    energy to power IoT edge devices significantly lowers both energy needs and environmental
    impact. The Renewable Edge Optimizer is able to efficiently manage data processing
    demands, leading to improved energy efficiency and reduced costs. Our study also
    demonstrates how Green IoT Edge Computing may be used in real-world scenarios
    in other fields, such as smart agriculture, smart cities, and industrial IoT.
    These examples demonstrate the flexibility and versatility of our method. As time
    goes on, it becomes more and more obvious that Green IoT Edge Computing is the
    key to developing a resilient and effective IoT ecosystem. We can pave the road
    for a cleaner future and fulfill the rising needs of IoT applications by maximizing
    data processing at the edge and leveraging renewable energy. SECTION VI. Future
    Work Although this study has advanced our understanding of Green IoT Edge Computing
    considerably, there are still many uncharted territories to be discovered. Improve
    the Renewable Edge Optimizer''s capacity to make decisions in light of changing
    renewable energy supply and data processing needs with the use of more advanced
    machine learning and AI optimization techniques. Research methods for scaling
    Green IoT Edge Computing solutions to bigger and more complex IoT ecosystems,
    guaranteeing their smooth incorporation and functioning even in widespread rollouts.
    Authors Figures References Keywords Metrics More Like This Role of Internet of
    Things in Renewable Energy Sources: A Review 2023 Intelligent Computing and Control
    for Engineering and Business Systems (ICCEBS) Published: 2023 Mobile-Edge Computing
    and Internet of Things for Consumers: Part II: Energy efficiency, connectivity,
    and economic development IEEE Consumer Electronics Magazine Published: 2017 Show
    More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical,
    Electronics and Computer Engineering, UPCON 2023
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Green IoT Edge Computing Towards Sustainable and Distributed Data Processing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sahoo S.
  - Sahoo S.P.
  - Barik R.C.
  - Kabat M.R.
  citation_count: '0'
  description: This paper addresses the crucial fault control challenges for the regulation
    of system-level reliability in 5G enabled SDN-controlled Social Internet of Vehicles
    (SDSIoV) environment. To mitigate these challenges a new variant of optimal service
    scheduling has been proposed using Whale optimization algorithm for fault control
    (WOFC). Detection and prevention of fault is monitored in fog-computing vehicles
    (FCV) within SDSIoV Infrastructure is depicted in fog-computing RSUs (FCRSU) by
    load balancing the message passing-based interval in the chosen network traffic.
    Three SDSIoV scenarios (low, normal and high) of 10, 50 and 100 numbers of FCV
    nodes with 10, 25 and 40 services are considered with the routing lengths as 300m,
    600m and 1500m. Each scenario is tested for no-fault data, with 15% and 30% faulty
    services and nodes data. The favourable hit ratio is computed in terms of execution
    times and response time of services with available fault-free and faulty resources.
    The obtained results of the proposed optimization model outperform over existing
    techniques.
  doi: 10.1109/OCIT59427.2023.10430574
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 OITS International Confe...
    A Fault-tolerant technique for future Social IoV using Evolutionary computation
    based Whale Optimization Model Publisher: IEEE Cite This PDF Satyabrat Sahoo;
    Satya Prakash Sahoo; Ram Chandra Barik; Manas Ranjan Kabat All Authors 16 Full
    Text Views Abstract Document Sections I. Introduction II. Related Work III. Problem
    Formulation IV. Methodology: Whale Optimization for Fault Control (WOFC) in SDSIoV
    V. Simulation Outcomes and Discussions Show Full Outline Authors Figures References
    Keywords Metrics Abstract: This paper addresses the crucial fault control challenges
    for the regulation of system-level reliability in 5G enabled SDN-controlled Social
    Internet of Vehicles (SDSIoV) environment. To mitigate these challenges a new
    variant of optimal service scheduling has been proposed using Whale optimization
    algorithm for fault control (WOFC). Detection and prevention of fault is monitored
    in fog-computing vehicles (FCV) within SDSIoV Infrastructure is depicted in fog-computing
    RSUs (FCRSU) by load balancing the message passing-based interval in the chosen
    network traffic. Three SDSIoV scenarios (low, normal and high) of 10, 50 and 100
    numbers of FCV nodes with 10, 25 and 40 services are considered with the routing
    lengths as 300m, 600m and 1500m. Each scenario is tested for no-fault data, with
    15% and 30% faulty services and nodes data. The favourable hit ratio is computed
    in terms of execution times and response time of services with available fault-free
    and faulty resources. The obtained results of the proposed optimization model
    outperform over existing techniques. Published in: 2023 OITS International Conference
    on Information Technology (OCIT) Date of Conference: 13-15 December 2023 Date
    Added to IEEE Xplore: 19 February 2024 ISBN Information: DOI: 10.1109/OCIT59427.2023.10430574
    Publisher: IEEE Conference Location: Raipur, India SECTION I. Introduction The
    present growth of usage of connected devices in Internet of Things (IoT) driven
    networks, has steeply increased the data surge of various applications and services.
    One such service is Internet of Vehicles (IoV) that implements the Intelligent
    Transportation System (ITS) in urban, semi-urban and particularly the smart cities.
    The IoV architecture is a key component of the Internet of Things (IoT) which
    is grouped into application, transport and perception layers. The IoV perception
    layer frequently uses heterogeneous wireless sensor networks (HWSN), hence in
    [1], the typical urban highway administration gathering network is chosen as modeling
    subject by the authors. But it needs to be evaluated the system as a whole, and
    the nodes and their properties from two different angles. IoV offers a greater
    range of services with a lot of network traffic, as compared to VANET. Delivering
    excellent service at an excellent QoSwith reasonable cost becomes a major concern
    for network management given the enormous volume of traffic generated. Software-defined
    networking (SDN) technology with 5G communication tackles this issue. Additionally,
    this causes IoV to change into software-defined IoV network (SDIoVN). This SDN
    framework is needed to control the massive volume of traffic generated by vehicles,
    sensors, actuators, and connected devices in SDIoVN. For the future 5G enabled
    ITS, building social networks of vehicles on the SDIoV is crucial since millions
    of smart vehicles generate and relay data to assess traffic conditions [2]. The
    SDIoV architecture has also experienced vertical fragmentation of methodologies
    utilized to satisfy the requirements of various job areas. As a result, Social
    SDIoV (SDSIoV), a blend of SDIoV and social networking, is to be developed to
    address these purported issues. The extremely dynamic and unstable character of
    the SDIoV is one of the obstacles in social IoV (SIoV), as social ties between
    cars grow and drain quickly. To assist SDSIoV network users with their status,
    a wide variety of applications should receive prompt and accurate responses on
    IoV networks. Due to the fact that crucial decisions are based on the data presented,
    the system must be dependable in several areas. For a variety of applications,
    data transmission management that considers energy efficiency, cost, and load
    balancing is required. In accordance with these requirements there are two major
    challenges as resource management and real time response. The dynamisms, heterogeneity
    of IoV networks and the limitations of various resources are hindrance to a reliable
    system. Further some system services will not function properly if the response
    is not received in a timely manner. Road safety requires a reliable IoV system
    to deliver fault-free, real-time services. When real service deviates from anticipated
    service, a system failure that may be verified occurs. Hardware malfunction and
    software flaws are the two main reasons why faults happen. Failure usually involves
    some level of sensitivity depending on the application and domain concerns in
    any system. Like there are some situations that pose a risk to life or compromise
    safety. In a few instances, the result could be a severe financial setback. On
    the other side, the error can result in inaccurate report generation, which would
    only have small effects. Generally speaking, there are two types of failures in
    software-defined IoV: crash faults (fail-stop fault), and Byzantine faults. When
    a device has a crash fault, it will stop working properly. Malicious fault is
    another name for a byzantine fault. An asynchronous SDIoV with the partially synchronous
    assumption is studied in relation to the consensus problem in a fault-tolerant
    distributed system. Fig. 1: SDN-assisted 5G Social IoV (SDSIoV). Show All To find
    the flaws and irregularities in system behavior, numerous fault-control strategies
    were considered [4]. Using the general IoT architecture, fault modeling can help
    improve system safety, dependability, and control of the system’s power. Failure
    handling in distributed systems requires various aspects and actions at the system
    level. The taxonomy of required actions for fault handling are defined as - Fault
    Prediction and Prevention, Fault Tolerance or Avoidance, Fault Detection and Elimination
    or Recovery, Fault Reduction in the System [5]. The researchers in [6] proposed
    a fault tolerant reference architecture for ITS in IoV networks that covers functional
    requirements of a variety of services and perspectives. Some non-functional needs,
    such as those pertaining to dependability, privacy, and accessibility, are not
    taken into account. Hence the pertinent factors are to be identified and the interactions
    between them needs to be modeled to address these requirements, and planning for
    both resources and services is necessary. Given the extreme sensitivity of IoV
    and the potential for both human and economical losses, the systems should be
    dependable. Due to resource shortages, heterogeneity, and dynamic networks, resource
    management is one of the difficulties. The most crucial challenging concern in
    resource allocation is cost which takes into account resources like energy, time,
    money, and computing power. Some studies concentrated on resource management to
    guarantee system dependability as a whole. The authors in [7]–[9] ensured reliability
    by emphasizing resource management in the whole SDN enabled IoT driven autonomous
    vehicular network system. The proposed solutions are highly complex and the tasks’
    scheduling are not prioritized. The majority of research [10] has focused on the
    cost and transmission reliability of data. A. Motivation For the regulation of
    system-level reliability in SDSIoV networks, fault management is essential. The
    energy consumption of nodes and their constrained processing power make performance
    management crucial as well. This includes managing the system’s temporal, resource,
    operational and economic expenditure dimensions. To prevent road accidents, there
    is a need of hard real-time reaction to some critical services. The system will
    not perform some functions if the response is not received in a timely manner.
    Road safety requires a reliable IoV system to deliver fault-free real-time services.
    B. Contribution Our paper aims to propose an optimized fault-tolerant approach
    to conceal critical service failures and to improve real time response to critical
    operations at different layers of SDSIoV architecture as shown in figure Fig.
    1. The main objectives of this research are: We identify the relevant parameters
    to fault management such as- service prioritization, various time handling and
    node status monitoring. We apply Whale optimization algorithm for fault control
    (WOFC) that is fault prevention and fault detection through optimal service scheduling.
    C. Paper organisation The followed by remaining of the paper is organized in the
    order as Section.II Related Work, Section.III Problem formulation, Section.IV
    Methodology:WOFC, Section.V Simulation Outcomes and Discussions, and Section.VI
    Conclusion. SECTION II. Related Work There were numerous options offered to meet
    the quality criteria. To improve scalability, a number of researchers proposed
    techniques constructed using mathematical simulation and artificial intelligence
    models. For certain uses, including attributes like time parameters, several papers
    are offered. Research [11] is offered and is used to set restrictions caused by
    artificial intelligence algorithms and mathematical models for controlling the
    railway station scheduling. It is suggested for a particular application and only
    addresses temporal parameters. Authors in [12] examined the genetic algorithm
    (GA) and particle swarm optimization (PSO) for service structure based on fitness
    value and time. The three main QoS measures were cost of service, execution or
    burst or run time, and trustworthiness. Their conclusions showed that GA is evaluated
    to be better in suitability and performance. The search waiting time is not considered
    to achieve more scalability. A cost-conscious service scheduling in fog computing
    environment using GA proposed in [13]. They considered the cost criterions such
    as energy consumption and time. However, there is no way to verify the state of
    a node, a connection, or a device to manage faults. The management of energy loss
    in fuzzy systems using a PSO was covered in the work [14]. The authors used particle
    filter for handling of energy consumption. The criterions for processing maintenance
    and capacity of nodes are not considered. The researchers enhanced the reliability
    to promote the application of ITS where higher reliability is insisted. The authors
    in [15] proposed two consensus protocols for the road side units (RSU) and the
    vehicles named CPR and CPV in SDIoV infrastructure to achieve reliability through
    fault tolerance. They have covered only RSUs for operation in the protocols. To
    provide the real-time services in SDN architecture based IoT environments, the
    authors in [16] modeled a mathematical calculative namely Shared Risk Link Group
    (SRLG). They have achieved quality of services (QoS) through less delay packet
    jitter and packet loss, and quick fault recovery through improved service parameters.
    In order to preserve QoS through continuity of services, the article [17] proposed
    a dynamic design that emphasizes the required end-to-end mobility assistance.
    Additionally, a fault-tolerant SDN control plane with a stochastic network calculus
    (SNC) framework for managing mobile edge computing (MEC) is data flows is presented
    in this article. Closed-form problems were developed in respect to the entry procedures
    of various QoS classed data flows to ascertain the relationship between resource
    utilisation and the possibility of defying of each data flow. The research lacks
    the constraint of server load sharing with the heavy grow of the traffic in the
    network. The authors in [18] suggest the Fuzzy weighted non-dominated Sorting
    Genetic Algorithm (FWNSGA) approach for the SDN driven 5G IoV network to allocate
    resources to the best level in order to handle the fastest rising network traffic
    with a variety of QoS requirements. The fair and balanced sharing of computing
    capability and load distribution in case of any fault occurrence, is not covered
    in this study. The methodology for fault management that is presented in the study
    [4] takes into account costs and can address crucial factors in different architectural
    layers. They developed fault tolerant genetic algorithm (FTGA). Constraints related
    to lengthy run times and high complexity of execution at bigger sizes of traffic
    are not addressed. Based on the articles that were evaluated, it was discovered
    that several criterion categories of fault management were projected in earlier
    works with varying restrictions on the complexity and scalability of mathematical
    models based on optimization. For instance, GA coding is challenging and imprecise,
    whereas PSO is prone to early convergence and falls into a local optimum. Therefore,
    the shortcomings of current algorithms can be addressed by an algorithm that can
    boost computational power and particle diversity. Thus, the whale optimization
    approach (WOA) is suggested to be used in this research in order to refine population
    diversity and exit the local optimum. It is more advantageous to create a SDSIoV
    with fault tolerance service scheduling using the WOA evolutionary algorithm.
    Message passage balancing and service scheduling are concepts that were not addressed
    by the other previous solutions. We evaluate it in comparison to our proposed
    WOFC algorithm using time and fault tolerance parameters. SECTION III. Problem
    Formulation Let the set of fog-computing vehicles (FCV) in SDSIoV infrastructure
    be represented by v={ v 1 , v 2 ,…, v p } (1) View Source the set of fog-computing
    RSUs (FCRSU) be represented by r={ r 1 , r 2 ,…, r b } (2) View Source and the
    set of all services of SDSIoV network be represented by s={ s 1 , s 2 ,…, s a
    } (3) View Source Now let the key parameters for fault control recovery be revealed.
    For efficient resource allocation consider message passing parameters such as
    FCV and FCRSU capacity, and load balancing for the message passing-based interval
    in the chosen route. Tot Sys Ca= ∑ v=1 p FCV v p Ca+ ∑ r=1 b FCRSU r b Ca (4)
    View Source Consider message or data transmission time as a function of time spent
    on routing, and response and latency times of FCRSU. Transmission Message T= ∑
    r=1 b FCRSU r b RT+ ∑ r=1 b FCRSU r b LT + Routing Message T (5) View Source The
    service response time can be termed in terms of response and latency times of
    FCV, service computing time, and message passing time of whole SDSIoV system.
    Response Service T= ∑ v=1 p FCV v p RT+ ∑ v=1 p FCV v p LT + Computing Service
    T+ ∑ m=1 d Transmission Message T (6) View Source where the messages passed among
    the FCVs and FCRSUs are represented by m={ m 1 , m 2 ,…, m d } (7) View Source
    With the above mathematical manifests, our suggested WOA-based solution covers
    good QoS with greater scalable architectural freedom. SECTION IV. Methodology:
    Whale Optimization for Fault Control (WOFC) in SDSIoV Recent research demonstrates
    WOA’s outstanding capacity to resolve challenging engineering optimization issues
    [19]. Its obvious benefits, including simplicity, flexibility, quick convergence,
    and stochastic nature, attracted much attention from the contemporary academic
    community across a variety of disciplines. So for better service scheduling through
    fault management we have proposed a WOA based SDSIoV model. The WOA iterates until
    the suitable FCVs asociated with FCRSUs are selected for the scheduling of demanded
    services by the end objects. The algorithm 1 states the pseudocode of WOA and
    the fiure Fig. 2 depicts the flow-chart for WOA. A. Initialization The FCV nodes’
    initial status data are updated such as positions and capacities based on processing
    capability of allotted services. The FCVs are zoned on the basis of available
    FCRSUs and clustered as showed in Fig. 1 with a FCV leader (FCVL). B. Quick Reactive
    Fault Control The FCVs are assigned priorities considering the available capacity
    and accordingly services are scheduled on the FCVs. On the basis of routing status
    of FCVs, they are amended or removed from the candidate solution set. The services
    are prioritized based on response time, reliability and types of services like
    real-time or multimedia applications and allowable maximum burst-time by system.
    The constraints are applied to the governing of FCVs and services which aids fault
    prevention. The fitness values are computed for design of response optimized system
    using Eq. 6. This is an iterative step to keep on updating the service response
    time equation based fitness values to optimize the population with minimum reaction
    time. Fig. 2: Flow-chart of Whale Optimization Algorithm (WOA). Show All C. WOA
    approach for optimum solution In order to perform optimization, spiral bubble-net
    feeding behavior is used as the mathematical model which processes three operations
    to measure and update the time-dependent positions of individual objects. 1) Exploration
    The search agents’ position are updated in this phase with respect to a randomly
    picked search agent instead of the best agent found so far and hence the WOA algorithm
    is permitted to perform a global search as mentioned the algorithm steps from
    23 to 25. 2) Encircling Once the best solution is defined and unfolded upon according
    to the algorithm step 9, The other search agents can attempt to update their locations
    toward the best solution using the iterative steps 28 and 29. 3) Exploitation
    Humpback whales swim around their prey at the same time in a circle that gets
    smaller and follows a helix shaped spiraling path. The likelihood of choosing
    between the spiral pattern or the shortening encircling mechanism to enhance the
    location of whales during the optimization process is considered to be fifty percentage
    to get a paragon of this concurrent actions. The algorithm steps 33 and 34 implement
    theses actions of Bubblenet attacking behavior. Algorithm 1: Pseudo-code for WOFC
    SECTION V. Simulation Outcomes and Discussions Here we describe the simulation
    setup for performance evaluation of the proposed WOFC approach for SDSIoV network
    environment. We have considered three SDSIoV scenarios (low, normal and high)
    of 10, 50 and 100 numbers of FCV nodes with 10, 25 and 40 services and routing
    lengths as 300m, 600m and 1500m. Each scenario is tested for no-fault data, and
    with 15% and 30% faulty services and nodes data. The favorable hit ratio is computed
    in terms of execution times and response time of services with available fault-free
    and faulty resources. MATLAB 2018a is used for simulation of the proposed WOFC
    algorithm in SDSIoV environment. The parameters of simulation setup are noted
    in Table I. The scenario is generated as per the dataset available in [4]. Our
    results of optimization are compared with [4]. Numerous services are thought to
    serve distinct populations at the system level. Nodes with violation of a constraint
    produce faults. The service ought to be distributed to a different node, and the
    current node ought to be replaced. To assess the dependability of service responses,
    the burst and response times are contrasted. The success rate of service response
    is then assessed in scenarios where node-level failures occur or do not occur.
    A path for future work is offered as an analysis based on success rate at the
    conclusion of the simulation segment. For the nodes at system level, the two parameters
    of time such as average burst and response times of services are used to access
    the effectiveness of our proposed method. The burst time is analysed with respect
    to various number of services and nodes as already stated and show in Figure 3.
    The ecological and social environmental restrictions and greater system requirement
    coverage in various architectural designs depend on this time characteristic.
    The results revealed that, the execution time is not considerably longer as compared
    to [4]. The difference in execution time is tolerable even when the number of
    nodes and services is increased. Their differences are maintained at a fixed ratio.
    Since an immediate response is essential for connected cars to avoid collisions,
    the average response time metric satisfies social and ecological standards. As
    outcomed in Figure 4, our suggested solution has a shorter average response time
    for all services, meaning that the average response time operates effectively
    with respect to the increased nodes as resources for a fixed number of services
    without significantly changing reciprocation. The suggested approach refers to
    the processing capacity of nodes as a decision variable. The proposed strategy
    performed better in terms of service prioritization early in the operation, despite
    the fact that it did not greatly increase the number of nodes or services. This
    parameter demonstrates that the suggested solution outperforms the evaluated items
    in terms of effectiveness against defects. In order to compare the suggested algorithm
    with existing ones, the proposed algorithm is assessed in light of crucial factors
    that have a substantial impact on its performance in terms of response time, and
    execution time. The outcomes demonstrate that the suggested strategy achieves
    a respectable response time and hence a high success rate in case of failures.
    As a result, the suggested strategy performs better than the earlier suggestions.
    As the number of repeats, nodes, and defects rises, the average response time
    decreases. When the burst-time was measured, it is found that the WOFC’s burst-time
    had increased due to the addition of control steps for managing processes and
    sending messages. Furthermore, given the average parameters provided for the processes
    due to the average reaction time, WOFC beats [4]. TABLE I: Simulation setup Fig.
    3: Comparison of Burst Time. Show All Fig. 4: Comparison of Response Time. Show
    All SECTION VI. Conclusion This paper distinctly solves the crucial fault control
    aspect of nodes in chosen network traffic by early service prioritization within
    5G enabled SDN-controlled Social Internet of Vehicles (SDSIoV) environment. The
    proposed model outperforms over the other existing model employing whale optimization
    algorithm with 15% and 30% faulty node services and data. The prominent parameters
    considered as average messages count and size per process Speed of FCV nodes,
    average acceptable response time of system and per process. The fitness function
    has the best optimization in WOFC as the number of iterations increases, taking
    into account reaction times for various processes, and convergence of the more
    exceptional processes. When the number of repeats, nodes, and defects rises, the
    average response time decreases. Hence the proposed model performs better than
    the earlier methods. Authors Figures References Keywords Metrics More Like This
    MEC Empowered Internet of Vehicles for Smart City Optimisations 2023 IEEE Smart
    World Congress (SWC) Published: 2023 Privacy Aware Post Quantum Secure Ant Colony
    Optimization Ad Hoc On-Demand Distance Vector Routing in Intent Based Internet
    of Vehicles for 5G Smart Cities IEEE Access Published: 2023 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: '>'
  journal: OCIT 2023 - 21st International Conference on Information Technology, Proceedings
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A Fault-tolerant technique for future Social IoV using Evolutionary computation
    based Whale Optimization Model
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
