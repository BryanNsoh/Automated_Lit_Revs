- DOI: https://doi.org/10.1109/syscon.2017.7934725
  analysis: '>'
  apa_citation: Mattei, A. P., Loures, L., de Saqui-Sannes, P., & Escudier, B. (2017).
    Feasibility study of a multispectral camera with automatic processing onboard
    a 27U satellite using model based space system engineering. In 2017 Annual IEEE
    International Systems Conference (SysCon) (pp. 706-711). IEEE.
  authors:
  - André Pierre Mattei
  - Luis Loures
  - Pierre de Saqui‐Sannes
  - Bénédicte Escudier
  citation_count: 1
  data_sources: Not specified
  explanation: The study explores the potential of using a high-resolution multispectral
    camera with an automatic onboard processing capability for precision agriculture,
    particularly for crop growth monitoring, disease detection, and irrigation system
    performance evaluation. The integrated system aims to provide near real-time data
    to farmers, enabling them to make informed decisions and manage their farms more
    effectively.
  extract_1: The project underlying the work presented in the paper aims to develop
    a novel payload capable of both controlling mission accomplishment and performing
    real time image processing. Instead of relying on the ground to acquire the intelligence
    needed for land management, farmers will have direct access to an almost real-time
    information to manage their property, as in Fig. 1. This figure shows the satellite
    collecting data for transmission to both a control station and a receiving station
    close to the farms.
  extract_2: The payload is assumed onboard of a 27U satellite and incorporating those
    necessary elements for image processing, mission management, and data management
    (storing and transmission). The new architecture allows the payload to manage
    mission accomplishment by controlling the payload subsystems and sending directives
    to satellite subsystems.
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2017 Annual IEEE Internationa... Feasibility study
    of a multispectral camera with automatic processing onboard a 27U satellite using
    model based space system engineering Publisher: IEEE Cite This PDF André Pierre
    Mattei; Luis Loures; Pierre de Saqui-Sannes; Bénédicte Escudier All Authors 1
    Cites in Paper 185 Full Text Views Abstract Document Sections I. Introduction
    II. Related Work III. Methodology IV. SysML and TTool V. Phase 0 Show Full Outline
    Authors Figures References Citations Keywords Metrics Abstract: The paper discusses
    an experience in using SysML and the TTool software for the feasibility study
    of a novel multispectral camera for agricultural monitoring. Innovation lies in
    both automatic image processing onboard and mission control capabilities designed
    to comply with a 27U microsatellite. In addition to the mission accomplishment
    control, this innovative payload is capable of sending processed data directly
    to farms, critically reducing the delay between image making and its use in the
    field. This paper shows how MBSE and SysML may comply with phases 0 and A of a
    space project. Published in: 2017 Annual IEEE International Systems Conference
    (SysCon) Date of Conference: 24-27 April 2017 Date Added to IEEE Xplore: 29 May
    2017 ISBN Information: Electronic ISSN: 2472-9647 DOI: 10.1109/SYSCON.2017.7934725
    Publisher: IEEE Conference Location: Montreal, QC, Canada SECTION I. Introduction
    Agriculture field conditions are dynamic and may change faster than the time necessary
    to imagery collection, processing, and product delivery to the farmer. Currently,
    raw images are downloaded from satellites to ground stations where they are processed
    in order to respond to specific services based on customer requests. This time
    delay may be not acceptable to satisfy the needs of several end users. Moreover,
    the presence of clouds between the satellite and the area of interest may prevent
    the satellite from the collection of valid data. The project underlying the work
    presented in the paper aims to develop a novel payload capable of both controlling
    mission accomplishment and performing real time image processing. Instead of relying
    on the ground to acquire the intelligence needed for land management, farmers
    will have direct access to an almost real-time information to manage their property,
    as in Fig. 1. This figure shows the satellite collecting data for transmission
    to both a control station and a receiving station close to the farms. The project
    has started with technical feasibility study of an innovative multispectral camera
    with automatic onboard processing, using Brazil as target and partially complying
    with phases 0 and A of a space project [1]. The camera is assumed onboard of a
    27U satellite and incorporating those necessary elements for image processing,
    mission management, and data management (storing and transmission). The new architecture
    allows the payload to manage mission accomplishment by controlling the payload
    subsystems and sending directives to satellite subsystems. Fig. 1. After both
    gathering and processing data, the satellite sends mapsto receiving stations on
    either farms or control centers. Show All The feasibility study of this agricultural
    monitoring satellite has been supported by a Model-Based System Engineering approach
    that uses SysML [2] as modeling language and the free software TTool [3] for model
    edition, simulation, and verification. The paper is organized as follows. Section
    II surveys related work. Section III introduces the methodology used during the
    feasibility study. Section IV presents the SysML diagrams supported by TTool,
    as well as the simulation and verification capabilities it offers. Section V and
    Section VI highlight the main results obtained during Phase O and Phase A, respectively.
    Section VII concludes the paper. SECTION II. Related Work A. Technological Solutions
    for Satellites Precision agriculture uses several image-processing techniques
    to improve the efficiency of normal activities and corrective actions performed
    by farmers [4] [5]: image acquisition, pre-processing, segmentation, object detection,
    and classification. Image acquisition is performed by a platform distant of the
    field of interest, such as an airplane or a satellite [6], and may use different
    cameras, such as multispectral [7] [8], hyperspectral [9], or radar [10]. A thematic
    map highlights information related to a specific crop and geographic area, [4].
    These data may result in the correct placement and in the correct amount of agricultural
    inputs (pesticides and nutrients) that shall be used by the farmer, [11]. Nonetheless,
    as emphasized by Oštir et al. [12], the time necessary for image processing is
    currently the main obstacle for a faster cycle (meaning better management) and
    a way to speed up the process is to make it as automated as possible. Several
    projects demonstrate that onboard data processing allows both a faster cycle for
    information generation and mission control. An automated monitoring system using
    a multispectral imaging device for precision agriculture is presented in [13].
    Onboard image processing was used by [14] to detect and track objects on the ocean
    surface. In [15], it is found a real-time system for weed discrimination using
    a multispectral camera. The onboard data processing of a hyperspectral camera
    has also been studied by several groups to facilitate the transmission of data
    to the ground [16] [17]. In [18], the mission system recognizes the presence of
    clouds in the pictures taken by a cubesat for prioritizing data downlink. The
    reference [19] developed an electronic card using a FPGA Virtex7 for an earth
    observation satellite onboard data treatment. B. Use of MBSSE and SysML for Satellite
    Design INCOSE (The International Council on Systems Engineering) defines Model-Based
    Systems Engineering (MBSE) as: “the formalized application of modeling to support
    system requirements, design, analysis, verification and validation activities
    beginning in the conceptual design phase and continuing throughout development
    and later life cycle phases” [20]. A model-based methodology defines what, how,
    and the tools by using a model-centric approach design. The authors have adopted
    Model-Based methodology to support the Space System Engineering (MBSSE) for the
    project, since it is well fitted for phases 0, A, and B of the system life cycle,
    [1] [21]. Model-Based Systems Engineering (MBSE) has advantages when compared
    to the document-based approach because of its intrinsic enhanced communication
    and more efficient knowledge management when dealing through project phases and
    subsystems. According to [22], MBSE is “the formalized application of modelling
    to support system requirements, design, analysis, verification, and validation
    activities beginning in the conceptual design phase and continuing throughout
    development and later life cycle phases”. In the work of Kaslow et al. [23] SysML
    is used to support the development of a cubesat using MBSE. SECTION III. Methodology
    This project adopts a Model-Based methodology to support Model Based Space System
    Engineering (MBSSE) using SysML as language for system definition, [24]. A methodology
    is defined by [25] as “a collection of related processes, methods, and tools”.
    Phases and milestones are used as defined by [1], and this project addresses phase
    0, mission analysis/needs identification, and phase A, feasibility. Fig. 2. MBSSE
    methodology applied for phases 0 and A, [21]. Show All The MBSSE is the choice
    for this project since facilitates application of concurrent engineering in the
    early phases of the space system life cycle, 0, A, and B, [1]. Phase 0 refers
    to mission analysis and needs identification. Phase A is a feasibility study containing
    possible system concepts and assess its technical and programmatic aspects. Phase
    B establishes a preliminary design definition by confirming the technical solutions
    using trade-off studies for the selected system concept. Activities performed
    during this project follow the general schema presented in Fig. 2, [24]. The process
    adopted for this work is organized as follows: Mission Requirements Definition
    (phase 0); Requirements Analysis (phase A); and Architectural Design and Review
    Activities (partially accomplished in phase A). These processes shall be employed
    repeatedly during project phases. Mission requirements and system constraints
    are initially considered as a starting point, 0.1, and included as Requirement
    Diagrams (RD) and Modeling Assumptions Diagrams (MAD). In 0.1, possible mission
    objectives are identified and mission statement presented. In 0.2, Use-Case Diagrams
    (UCD), Sequence Diagrams (SD), and Activity Diagrams (AD) are engendered for better
    understanding of the actions, goals, and interactions during different activities
    performed by the satellite. During this phase, a number of tools besides SysML
    diagrams may be used in order to help in a first mission analysis, such as software
    Matlab, STK, and CNES Celestlab. In 0.3, blocks are created to provide a first
    approach for system design and requirements used to define some possible orbits.
    In A.1, system requirements are derived from 0.1 to provide more detailed RD and
    MAD diagrams. In A.2, analysis uses more detailed system requirements. In 0.2,
    mission analysis uses SysML diagrams and other software, such as Matlab and STK
    [26]. In A.3, a Block Instance Diagram (BID) describes a system architecture and
    State Machine Diagrams (SMD) give each block instance a behavior whose correctness
    is checked using simulations. SysML is used as modeling language to describe processes
    from requirements definition to architecture design and verification. SysML is
    indicated for space systems development by both INCOSE, [20], and NASA, [27].
    Other modeling applications are used in the project in specific fields (Matlab,
    STK, Scade Suite etc.). SECTION IV. SysML and TTool TTool supports a customized
    version of SysML designed with real-time system design in mind, and a 3-step process.
    Fig. 3. System use case. Show All A. Requirement Capture During the requirement
    capture phase, requirement diagrams (RD) express requirements, refinements between
    pairs of requirements, derivation of technical requirements from the set of requirements.
    Assuming that a model abstracts a real system, that model is valid under a precise
    set of assumptions. TTool invites you to make these assumptions an explicit part
    of the SysML model. A modeling assumption diagram (MAD, not offered by the OMG-based
    SysML) enables expression of the modeling assumptions associated with the system
    and its environment. B. Analysis A use-case diagram (UCD) identifies the main
    functions to be offered by the system, the relations between pairs of functions,
    and the interactions between the system and its environment. Use-cases are documented
    by scenarios (sequence diagrams) and flow charts (activity diagrams). C. Design
    (Including Simulation and Verification) The design step defines the architecture
    of the system in the form of a block instance diagram, and assigns each block
    instance a behavior expressed by a state machine diagram. Design diagrams have
    a formal semantics, making them executable by the TTool''s simulator. TTool further
    implements a press-button approach to offer verification capabilities (model checking
    and abstractions) by reasoning on the SysML model without writing a piece of formal
    code. Model checking decides whether a state or an action in the model is reachable
    or not. Abstraction reduces the labeled reachability graph of the SysML Model
    to a quotient automaton that provides the model designer with events of interest.
    SECTION V. Phase 0 0.1: Requirement Capture Requirements that define the mission
    and preliminary aspects related to both satellite and payload are expressed in
    the form of SysML Requirement Diagrams (RD), not presented here due to their size;
    they would be unreadable. The Modeling Assumption Diagram created for camera channels
    captures a set of assumptions and attributes important for shaping the camera
    as real-time system. 0.2: Analysis Requirements demands updated information in
    less than 15 days'' period necessary for crop management and agriculture production
    increase. The UCD in Fig. 3 depicts payload functions and relations with ground
    station, farms, and satellite. The ≪include≫ function expresses a mandatory inclusion.
    0.3: Design Taking into account requirements and analysis, Phase 0 identifies
    a first approach for both mission and orbit design. A. Mission Since mission statement
    establishes an onboard image processing for agriculture, some potential indices
    are identified. These indices allow the identification of suitable processing
    techniques in later phases. For predicting yield: Normalized Difference Vegetation
    Index (NDVI), Green Vegetation Index (GVI), and Soil-Adjusted Vegetation Index
    (SAVI). [28] GVI= SAVI= NDVI=( ρ nir − ρ red )/( ρ nir + ρ red ) ( ρ nir − ρ green
    )/( ρ nir + ρ green ) [( ρ nir − ρ red )/( ρ nir + ρ red +L)]. (1+L) (1) (2) (3)
    View Source where ρ nir , ρ red , and ρ green are spectral reflectance for near
    infrared, red, and green wavelengths. L is a correction factor and its value is
    dependent on the vegetation cover and a value L=0.5 is suggested by [28] to minimize
    the effect of soil variations in green vegetation compared to Normalized Difference
    Vegetation Index (NDVI) and represents intermediate vegetation cover (0.25 for
    high and 1.0 for low density vegetation). For three decades, NDVI has been used
    to estimate vegetation water content (VWC) with limited success. The limitation
    is due to NDVI saturation when vegetation coverage is dense. SWIR in 1640nm and
    2130nm were used in the Normalized Difference Water Index, NDWI, with good results
    for corn: NDWI=( ρ nir − ρ swir )/( ρ nir + ρ swir ), (4) View Source where ρ
    swir are spectral reflectance in the SWIR band. Table I. Wavelengths selected
    for the multispectral camera. Taking into account the selected indices, Table
    I presents the channels and corresponding wavelengths considered through this
    work. B. Satellite Bus The platform choice is a 27U microsatellite, 54kg of total
    mass. This satellite type allows the use of off the shelf components and has a
    standard launching system called “PPOD” which makes it faster and cheaper the
    development of the service module. During operation, the satellite will be able
    to supply energy to enable the proper functioning of onboard equipment. This is
    normally achieved by the solar panels except when passing in the shadow of the
    earth. During these periods of eclipse, the solar panels are not illuminated and
    therefore cannot supply energy, that''s when the batteries take over. The estimated
    power budget is less than 70W, including a 20% margin, and this value was used
    for sizing the batteries. C. Orbit An orbit is characterized by its six orbital
    parameters, or Keplerian: Semi-major axis/elevation: Imposed by requirements,
    the altitude must be between 500 and 750 km; Eccentricity: zero (circular orbit);
    Tilt: to cover all of Brazil, orbital inclination of 33°; Longitude of ascending
    node: considered a longitude of the ascending node of −44.39° (Alcantara Launch
    Center); Argument of periapsis: calculated from the semi-major axis and tilt;
    Mean anomaly: it is equal to the true anomaly within the circular orbit; the initial
    value considered in the calculations is zero. This section made use of the software
    CNES Celestlab and STK. The orbit analysis has taken into account mission needs
    as presented in requirements and not launcher availability. Using orbital parameters
    considered above, an orbit phase diagram was developed for presenting the duration
    of revisits according to the altitude of 128 possible orbits. An orbit is phased
    when the satellite passes exactly over the same track after a number of revolutions.
    Fig. 4. Average visibility per day (minutes). Show All Fig. 5. Intertrace cycle
    according to the duration of revisits. Show All The ground stations used as references
    are in the Brazilian cities of Natal, Cuiabá, Campinas, and São Jose dos Campos.
    Visibility cones with an elevation of 5 degrees are considered to obtain the average
    visibility duration on a day in minutes as function of the orbit altitude (Fig.
    4). The figure shows that the average duration of daily exposure increases with
    the altitude. The intertrace cycle is defined as the distance, at the equator,
    between two consecutive traces in space (not in time). Intertrace shall be considered
    along with swath since if the second is smaller than the first one, the satellite
    will be forced to change its pointing direction and show deflection capability.
    Deflection is defined as the satellite rotation angle needed to cover the entire
    area of the intertrace and swath is defined as the distance on earth corresponding
    to the maximum width of an image. As a general rule, smaller revisit time demands
    larger intertrace, as shown in Fig. 5, and this will force larger swath and eventually
    also larger deflections (5), which can potentially make it harder onboard processing.
    The satellite deflection (α) depends on the satellite altitude (h) , intertrace
    cycle ( I c ) , and the opening angle of the optical instrument ( FOV , Field
    Of View), as be seen in (5). α= tan −1 ( I c /2h)−(FOV/2) (5) View Source Most
    suitable orbits to this mission were chosen using multicriteria optimization with
    Visual Prometheus software [29]. Criteria are based on the calculation of aggregate
    preference indexes π to express the degree of preference between two alternatives
    orbits considering the decision criteria. These indices are calculated from weight
    ϖ i associated with each criterion c i and intensity functions P i , dependent
    on the considered alternatives. Thus, considering a number N of criteria and a
    pair of alternatives (a, b) among M alternatives, the index of aggregate preference
    is given by: π(a,b)= ∑ N i ϖ i . P i (a,b), (6) View Source where π(a,b) is a
    real between 0 and 1. The closer π(a, b) is to the unit, the stronger is the preference
    of a over b . The calculation of this index for all pairs of alternatives enables
    a matrix of preference. Decision criteria were: Altitude (weight of 15%); Cycle
    time (weight of 32.5%); Average visibility (weight of 20%); Deflection (weight
    of 32.5%). Orbits were selected with this method. With a swath of 100km and altitude
    of 625km, one may conclude from Fig. 6 that the designer shall consider both FOV
    and swath for complying with 15 days revisit time requirement. Fig. 6. Brazil
    coverage assessment using STK. Show All SECTION VI. Phase A A.1: Analysis A. Satellite
    Bus As usual, solar panels and batteries (during shadow periods) supply the necessary
    power. The power that can be generated through photovoltaic cells P avlb is related
    to the angle θ between the normal to the solar panels and the direction of the
    sun. The estimated number of cells was 116 in the lateral faces, resulting in
    3.3kg of total mass and 107W of maximum power. Due to low orbit inclination, the
    minimum value of the average daily duration of eclipse being 32 minutes and the
    maximum is 35 minutes (illumination about 60min). The total work capacity of the
    batteries shall be equal to: C mission =(35/60). P max =0.58. P max ≅41W (7) View
    Source For a 3-year mission, there are about 17,000 charge / discharge cycles,
    with 30% depth of discharge. In this case, the batteries only provide a maximum
    of 3W and it is possible to preserve the battery and to reach 35,000 of maximum
    number of cycles, thus it is necessary to have at least 14 batteries to provide
    41W. Taking into account the necessary energy for systems, the power solar panels
    is not sufficient in early life and situation will degraded during operation,
    it is then recommended the use of at least one deployable panel. For communication
    network, it was considered two solutions related to the physical layer, Controller
    Area Network Bus (CAN) and Spacewire. CAN bus is considered a good option taking
    into account requirements and its lower cost in comparison to Spacewire. S band
    (2-4GHz) is the option for telecommand and telemetry and X band (8-12GHz) is the
    option for downloading images. Considering the resolutions of 5m and 30m for approximately
    2,500 pictures, and each photo having 250MB and 25MB, respectively, there would
    be an amount of 625 GB and 62.5 GB of images. With an average time of visibility
    of 120 minutes per day, it is necessary to determine the required transmission
    rate T x rate to send the stored data (Im) during the time available (visibility
    time, V time ) is T x rate =Im/ V time . Fig. 7. General view of system context,
    including payloadmodules and some platform subsystems. Show All Using these data
    would require a transponder with capacity of 87Mbps and 8.7Mbps for ground resolutions
    of 5 and 30m, respectively. Using a component available in the cubesat market,
    such as EWC 27 HDR-TM [30], the available capacity is 50Mbps. Using this transponder
    for higher spatial resolutions, it is possible to send approximately 57% of surveilled
    area. Taking into account the particular interest in crop areas, the data rate
    is considered sufficient. B. Payload Fig. 7 depicts a context diagram using the
    syntax of block diagrams for payload modules/functions. A black diamond denotes
    a “is made up” relation. Optical collects the light flux from the ground through
    a telescope; Image processing reads and processes data generated by Optical; Mission
    control is responsible for controlling mission accomplishment and payload modules;
    Interface exchanges data with the satellite onboard computer and distributes them
    through payload modules; Memory stores data generated by the Image processing
    module; Transmitter sends data directly to the ground. Optical Module Fig. 8 presents
    some key figures used for developing the imager. A represents the area acquired
    by the imager and it depends on the detector configuration, f is the focal distance,
    D the aperture diameter, and h the satellite altitude. Each detector acquires
    image with size R (ground resolution) and detector element angle θ r , solid angle
    ω d , and field of view (FOV) θ . Using these basic figures, some parameters regarding
    both detector and optical system telescope may be determined. [31] Usually, imaging
    systems are separated into three categories: whiskbroom, pushbroom, and staring.
    Taking into account that onboard automatic image processing is challenging for
    the payload, a staring imager is a better choice for this project since it decreases
    both stability and vibration demands for the satellite control system. [32] The
    speed of the satellite ( V sat ) can be calculated with a certain degree of accuracy
    from the circular orbit altitude. Let V im be the satellite speed in relation
    to the earth surface, then taking into account satellite orbit (500-750km), its
    speed is: 7.5 km/s≤ V sat ≤7.6 km/s and V im is 6.7 km/h≤ V im ≤7.1 km/h Fig.
    8. General parameters used for imager development. Show All Fig. 9. Swath as a
    function of focal length f and altitude h for both VNIR and SWIR. Show All The
    most common telescopes are: Catadioptric, Three-Mirror Anastigmat (TMA), and Korsch,
    [33]. In the light of this comparison, both Korsch and Catadioptric telescopes
    types are considered for analysis at phase B. For the telescope, main figures
    are: f and D . The ground resolution considered is 5-10m for the VNIR and the
    area of interest is Brazil, and the revisit time shall be lower than 15 days.
    Using the required ground resolution in the VNIR range (5-10m) and 10μm pixel
    size for VNIR range: 0.5m≤f≤1.5m . The instantaneous swath (P) may be found in
    (19) and is presented in Fig. 9 as a function of f,h , and detector characteristics.
    It may be seen that greater swaths are found in the VNIR range due to the larger
    number of elements in the detector, 9216×9232 against 2048×2048 pixels for SWIR''s.
    The swath values found are not sufficient for achieving the requirement for revisit
    time even for VNIR range and this result may be considered for either relax requirement
    for revisit time or include attitude agility for the satellite. The CCD parameters
    also determine the system field of view 2θ . In order to avoid image distortions,
    during the time necessary to acquire each image (exposure time T E ), satellite
    movement should not be more than the projection a photosensitive element onto
    the region along flight speed direction. The exposure time T E ≤2.5s for VNIR
    ( R vnir ≥5m) and T E ≤3.5s for the case of SWIR (minimum R swir ≥7m ), ( V im
    =1.97m/s) . For the telescope, it was considered as cylindrical with volume [(f/2).
    D 2 ] , with the combination of at least two mirrors. Furthermore, using data
    provided by the CNES, density for mirrors 50kg/m2 and aluminum protection 3kg/m2,
    estimated mass is 0.95kg. i. Image Processing Analysis and Mission Control It
    is considered that each pixel will be encoded in 8 bits. Consistently with the
    payload, the image processing software will receive 6 images from the different
    camera channels. The Mission control proposed for the payload is a software architecture
    that provides the autonomy necessary to make the platform more capable when managing
    its mission. The new architecture proposed allows the payload to control the mission
    accomplishment management by controlling the payload subsystems and sending directives
    to satellite subsystems in a three-layer architecture, [34]. Taking into account
    the mission goals, a planner schedules activities and send orders to a robust
    execution software, Fig. 10. The robust execution software is responsible for
    optical and processing setup as well as monitor orders execution. Depending on
    the crop, region, and period of the year, different settings may be necessary
    for the optics and image processing. Planner may take many minutes to process
    all parameters while the execution software shall perform in seconds to create
    orders based on planner decisions. ii. Memory Considering the chosen components,
    VNIR detectors generate 681Mb for each picture and SWIR 33Mb. As an example, with
    a swath of 100km in VNIR range, 852 pictures may cover the entire country with
    580Gb per band. With a swath of 40km in SWIR range, 5,323 pictures may be necessary
    to cover the entire country, meaning 176Gb per band. The total amount is 2.7Tb,
    and using lossless data compression of 50% would lead the value for 1.4Tb. If
    after processing image size decreases another 50%, it is necessary a capacity
    of 668Gb during the revisit time. Concluding a 1Tb solid state memory is enough
    to store all processed images even considering the whole country as target. Fig.
    10. Payload software architecture. Show All Fig. 11. TTool simulation over the
    system services state machine. Show All A.2: Modeling with SysML and TTool Due
    to space constraints, the use-case diagrams of the payload are not depicted here,
    but were considered for the services provided by the satellite. Whether a Block
    Instance Diagram usually depicts the architecture of the real system, it may also
    express a composition of elementary services to be provided by the system. A service
    is behaviorally described by a state machine, as the one depicted by Fig. 11.
    Fig. 11 depicts how the state machine diagram is associated with the services
    provided by the system. In this figure, messages are numbered using three letters
    and three numbers. Letters refer to the service provided and numbers start with
    000 and increase sequentially as messages flow between systems, ending with a
    final message numbered XXX. TEL refers to telemetry, TXG to transmission to ground
    station, TXF to transmission to a farm, SET is used when payload setup is changed
    by the ground station, POS and ATT is a service provided by the satellite systems
    for informing current GPS position and set the platform attitude to perform imagery,
    and MAK refers to those messages sent when making and processing imagery before
    storing them in the memory. Simulation has enabled early debugging of the model.
    Fig. 11 shows (green arrow) how the simulator shows where the simulation has been
    stopped. Simulations also allow checking of details of the state machine, as a
    simulation trace may be seen in Erro! Fonte de referência não encontrada. In this
    figure, only messages and states involved in the telemetry service are presented.
    For telemetry, the ground station is the actor responsible for starting the service
    and for simulation purposes; it is inside the element called “Services Controller”.
    Fig. 12. Simulation trace output by TTool. Show All SECTION VII. Conclusions Satellites
    have played an increasing and acknowledged role monitoring agriculture, particularly
    in Brazil. The paper discusses an experience in using SysML and TTool for a feasibility
    study of a new payload aiming to decrease the time between information gathering
    and actions on the ground. A novelty proposed is a multispectral camera payload
    capable of processing images and manage mission accomplishment automatically.
    Data obtained in this work provided the necessary information for the ongoing
    phase B. Next steps include the development and field test of both image processor
    and mission manager using manned aircraft and commercial multispectral cameras,
    before integration in the final destination. ACKNOWLEDGMENT TTool has been developed
    by Dr Ludovic Apvrille. Contributions from ISAE-SUPAERO and ITA students are acknowledged.
    Acknowledgements are also due to Brazilian Space Agency (AEB), Department of Aerospace
    Science and Technology of the Brazilian Air Force (DCTA), and the Thales-Alenia
    Space (TAS) company. Authors Figures References Citations Keywords Metrics More
    Like This An integral time calculation model for agile satellite TDICCD camera
    2016 IEEE 13th International Conference on Signal Processing (ICSP) Published:
    2016 An analytical echo model for satellite ISAR imaging based on the Kepler orbit
    2016 CIE International Conference on Radar (RADAR) Published: 2016 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: (Mattei et al., 2017)
  journal: 2017 Annual IEEE International Systems Conference (SysCon)
  key_findings: The study's findings support the feasibility of integrating a multispectral
    camera payload with onboard processing for automated, real-time crop monitoring
    in precision agriculture. The integrated system has the potential to provide farmers
    with near real-time information on crop growth, disease detection, and irrigation
    system performance, enabling more informed decision-making and improved farm management.
  limitations: None mentioned in the provided text.
  main_objective: The primary goal of the study was to assess the feasibility of using
    a multispectral camera with automatic onboard processing capabilities for real-time
    crop monitoring and management in precision agriculture, integrated as a payload
    on a 27U satellite.
  pdf_link: null
  publication_year: 2017
  relevance_evaluation: This paper aligns well with the objective of our literature
    review section on integrating advanced monitoring techniques in automated irrigation
    systems, specifically emphasizing the use of high-resolution cameras and computer
    vision algorithms for visual monitoring of crop growth and irrigation system performance.
    The study's findings and insights contribute to our understanding of the potential
    benefits and challenges of such integrated systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Multispectral camera, computer vision algorithms, Model-Based
    Systems Engineering (MBSE), SysML, TTool software
  title: Feasibility study of a multispectral camera with automatic processing onboard
    a 27U satellite using model based space system engineering
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/tpami.2018.2827049
  analysis: '>'
  apa_citation: Zhu, K., Xue, Y., Fu, Q., Kang, S. B., Chen, X., & Yu, J. (2019, May).
    Hyperspectral light field stereo matching. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 41(5), 1131-1143. https://doi.org/10.1109/TPAMI.2018.2827049
  authors:
  - Kunfu Zhu
  - Yujia Xue
  - Qiang Fu
  - Sing Bing Kang
  - Xilin Chen
  - Jingyi Yu
  citation_count: 19
  data_sources: Synthetic and real-world H-LF data
  explanation: This paper introduces a novel automated hyperspectral stereo matching
    technique for integration of high-resolution cameras into precision agriculture
    irrigation management systems to address the challenge of efficient water distribution.
    The proposed technique is designed to overcome the challenges associated with
    integrating these cameras into existing systems, including spectral inconsistencies,
    view selection, and focus measurement. The authors validate the effectiveness
    of their approach using a custom-built hyperspectral light field (H-LF) camera
    array, demonstrating improved accuracy and robustness compared to existing methods.
  extract_1: This paper introduces the design and implementation of a hyperspectral
    light field (H-LF) stereo matching technique for more accurate depth estimation
    of scene geometry. Depth estimation in H-LF is challenging when using traditional
    stereo matching algorithms, which often assume consistency in pixel intensities
    across different spectral bands.
  extract_2: Combining the spectral-invariant feature descriptor with our cross-spectral
    stereo matching cost, we show that our approach can significantly improve the
    accuracy of depth estimation for H-LF.
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy Loading web-font TeX/Size2/Regular
    IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Transactions on Pattern
    ... >Volume: 41 Issue: 5 Hyperspectral Light Field Stereo Matching Publisher:
    IEEE Cite This PDF Kang Zhu; Yujia Xue; Qiang Fu; Sing Bing Kang; Xilin Chen;
    Jingyi Yu All Authors 21 Cites in Papers 1091 Full Text Views Abstract Document
    Sections 1 Introduction 2 Related Work 3 Hyperspectral Light Field Imager (HLFI)
    4 Two-View Spectral-Aware Matching 5 H-LF Stereo Matching Scheme Show Full Outline
    Authors Figures References Citations Keywords Metrics Footnotes Abstract: In this
    paper, we describe how scene depth can be extracted using a hyperspectral light
    field capture (H-LF) system. Our H-LF system consists of a 5×6 array of cameras,
    with each camera sampling a different narrow band in the visible spectrum. There
    are two parts to extracting scene depth. The first part is our novel cross-spectral
    pairwise matching technique, which involves a new spectral-invariant feature descriptor
    and its companion matching metric we call bidirectional weighted normalized cross
    correlation (BWNCC). The second part, namely, H-LF stereo matching, uses a combination
    of spectral-dependent correspondence and defocus cues. These two new cost terms
    are integrated into a Markov Random Field (MRF) for disparity estimation. Experiments
    on synthetic and real H-LF data show that our approach can produce high-quality
    disparity maps. We also show that these results can be used to produce the complete
    plenoptic cube in addition to synthesizing all-focus and defocused color images
    under different sensor spectral responses. Published in: IEEE Transactions on
    Pattern Analysis and Machine Intelligence ( Volume: 41, Issue: 5, 01 May 2019)
    Page(s): 1131 - 1143 Date of Publication: 16 April 2018 ISSN Information: PubMed
    ID: 29993926 DOI: 10.1109/TPAMI.2018.2827049 Publisher: IEEE Funding Agency: SECTION
    1 Introduction The availability of commodity light field (LF) cameras such as
    Lytro [1] and Raytrix [2] makes it easy to capture light fields. Dense stereo
    matching solutions have exploited unique properties, e.g., spatial and angular
    coherence [3], ray geometric constraints [4], [5], [6], focal symmetry [7], and
    defocus blurs [8]. In addition to 3D reconstruction, LF stereo matching can also
    address traditionally challenging problems, e.g., transparent object reconstruction
    [9], saliency detection [10] and scene classification [11]. Since an LF consists
    of densely sampled rays within a specific range of location and angle, it can
    be thought of as representing geometry and surface reflectance as well. However,
    the original plenoptic function [12] includes an additional dimension of spectra,
    which has been largely ignored in most previous LF systems. What is required is
    hyperspectral imaging, which refers to the dense spectral sampling of a scene,
    as opposed to the regular RGB three-band sampling for color cameras. Holloway
    et al. [13] acquire a multispectral1 LF using generalized assorted camera arrays.
    More recently, Xiong et al. [14] adopts a hybrid sensing technique that combines
    an LF with a hyperspectral camera. These solutions assume small camera baselines
    for reliable image registration. In contrast, we present a wide baseline hyperspectral
    light field (H-LF) imaging technique based on novel cross-spectral LF stereo matching.
    Direct adoption of existing LF stereo matching solutions (e.g., [4], [7], [8],
    [15], [16], [17]) for H-LF would be ineffective, since images at different spectral
    bands may be very visually different. Instead, we introduce a new spectral-invariant
    feature descriptor and its companion matching metric (which we call bidirectional
    weighted normalized cross correlation or BWNCC). BWNCC measures gradient inconsistencies
    between cross-spectral images; it significantly outperforms other state-of-the-art
    metrics (e.g., sum of squared differences (SSD) [18], normalized cross correlation
    (NCC) [19], histogram of oriented gradient (HOG) [20], scale-invariant feature
    transform (SIFT) [21]), in both robustness and accuracy. Our spectral-dependent
    H-LF stereo matching technique combines correspondence and defocus cues that are
    based on BWNCC. For visual coherency, we calculate the correspondence cost using
    local subsets of views, since views that are farther away may be less reliable.
    The entire H-LF is used to compute our new spectral-aware defocus cost. Previous
    approaches use color or intensity variance to measure focusness. However, for
    H-LF, the same 3D point will map to different intensities; as a result, such variance
    measures would be unreliable. We instead synthesize the RGB color from H-LF samples,
    then use the CIE Color Space to map the estimated hue of color to its spectral
    band. Consistency is then measured using the actual captured band as the focusness
    measure. Finally, we integrate the new correspondence and defocus costs with occlusion
    and smoothness terms in an energy function, and solve it as a Markov Random Field
    (MRF). We validate our approach on both synthetic and real H-LFs. To capture real
    H-LFs, we construct an H-LF camera array, with each camera equipped with a different
    narrow 10\;\mathrm{nm} -wide bandpass filter (Fig. 1). The union of all the cameras
    covers the visible spectrum from 410\;\mathrm{nm} to 700\;\mathrm{nm} . The baseline
    is 36\;\mathrm{mm} , which is large enough that parallax for the scenes used would
    be significant. We show our H-LF stereo matching technique can produce high-quality
    disparity maps for both synthetic and real datasets. The disparity maps can be
    used to produce the complete plenoptic cube. These maps can also be used for image
    warping, which allows color image synthesis, hyperspectral refocusing, and emulation
    of different color sensors. Fig. 1. System overview. Our hyperspectral light field
    (H-LF) imager (HLFI, top left) consists of a 5\times 6 array of cameras, each
    with a narrow bandpass filter centered at a specific wavelength. The HLFI samples
    the visible spectrum from 410\;\mathrm{nm} to 700\;\mathrm{nm} with an 10\;\mathrm{nm}
    interval. We propose a new spectral-dependent H-LF stereo matching technique (middle),
    which involves novel correspondence cost (top) and spectral-aware defocus cost
    (bottom). The correspondence cost is based on a new spectral-invariant feature
    descriptor called BWNCC with local view selection. The generated disparity map
    (top right) can be used for complete plenoptic reconstruction (bottom right).
    Show All The contributions of this paper are: We designed a snapshot hyperspectral
    light field imager (HLFI) that samples only a subset of H-LFs, avoiding demosaicking
    artifacts. In principle, our HLFI can be expanded both in the spectral resolution
    (e.g., a 5\;\mathrm{nm} interval or lower) and range (e.g., additional infrared
    and ultraviolet bands). We propose a new spectral-invariant feature descriptor
    to effectively represent the visually-varying spectral images. We also propose
    a matching metric, BWNCC, to measure the similarity of multi-dimensional features.
    This feature descriptor and BWNCC are used in H-LF stereo matching. We propose
    a novel spectral-dependent H-LF stereo matching technique that combines a local
    view selection strategy with spectral-aware defocus. We show that our matching
    technique produces high-quality disparity maps. We show three applications using
    our H-LF results: reconstruction of the complete plenoptic cube, generation of
    all-focus H-LF, and synthesis of defocused color images under different spectral
    profiles. We expect these applications would be useful for 3D reconstruction,
    object detection and identification, and material analysis. The rest of this paper
    is organized as follows. We review related work in feature descriptors, LF stereo
    matching, and multi-spectral imaging in Section 2. Our design of HLFI is described
    in Section 3. Section 4 details our feature descriptor and matching metric; these
    are used in our H-LF stereo matching technique (Section 5). The task of complete
    plenoptic cube reconstruction is described in Section 6. Section 7 presents the
    experimental results and applications, with discussion of limitations in Section
    8 and concluding remarks in Section 9. SECTION 2 Related Work In this section,
    we review relevant approaches in the areas of multispectral imaging, feature descriptors,
    and light field stereo matching. 2.1 Multispectral Imaging Spectral imaging has
    long been driven by the need of high quality remote sensing [22], with applications
    in agriculture, military, astronomy, surveillance, etc. (e.g., [23], [24], [25],
    [26]). The commonly adopted techniques include coupling bandpass filters with
    spatial and temporal multiplexing to acquire both the spatial and spectral information.
    In satellite imaging, spectral-coded pushbroom cameras is capable of acquiring
    the full spectra [27]. Tunable filters (e.g., LCTF, AOTF [28]) provide an alternative
    single camera solution. Such solutions require the camera be fixed under different
    shots and cannot provide scene parallax. More expensive snapshot imaging spectrometry
    involving diffraction grating, dispersing prism, multi-aperture spectral filter,
    Lyot filter or generalized Bayer filter (e.g., [29], [30]), requires extremely
    accurate calibration. Alternative approaches mostly rely on hybrid sensing, i.e.,
    using sensors with different modalities. For example, Xiong et al. [14] combine
    an LF camera with a hyperspectral camera to obtain the angular and spectral dimensions
    to recover the hyperspectral LF. Their approach for band-wise recovery is based
    on correlations across angular and spectral dimensions of the RGB LF data. These
    correlations are extracted through self-learned dictionaries, which only approximate
    the cross-spectral mappings. In our approach, we reconstruct a plenoptic cube
    through hyperspectral stereo matching; in principle, the reconstruction is physically
    correct if matching is perfect. In addition, their built-in LF camera is of low
    resolution (380 \times 380 for single view) with small baselines (1-2 \;\mathrm{mm}
    ), which significantly limits angular information. By comparison, our camera resolution
    is 1292\times 964 , with the nearest baseline being 36 \;\mathrm{mm} . Ye and
    Imai [31] describe a plenoptic multispectral camera whose microlens array has
    a spectrally-coded mask. Using a sparse representation, the spectral samples are
    used to reconstruct high resolution multispectral images. Closely to related to
    our work is that of generalized assorted cameras [13], where a camera array is
    used for multispectral imaging. This system is a custom-built camera array (ProFUSION
    from PTGrey) that is modified by mounting broad band-pass filters. The camera
    baseline is rather small, and the filter being broad band-pass makes it easier
    to correspond images using existing color features. Our system uses a 2D array
    of monochrome cameras with narrow band-pass filters to avoid the demosaicking
    artifacts caused by the de-mulplexing procedure used in [13]. More importantly,
    the camera baselines in our system are significantly larger relative to scene
    depth; this allows more reliable depth estimation and enables synthetic refocusing.
    Finally, our system is extensible: in principle, more cameras can be added to
    increase the synthetic aperture (with wider extents) or spectral sampling resolution
    (with narrower band-pass filters). 2.2 Feature Descriptors Feature descriptors
    (e.g., [32], [13], [18], [19], [33], [34], [35], [36]) play a critical role in
    stereo matching and image registration. SSD (e.g., [32], [18]) is widely used
    in stereo matching as a data cost (e.g., [33]). NCC [19] is a highly popular as
    well for matching contrast-varying images. To take into account local radiometric
    variabilities, adaptive normalized cross correlation (ANCC) [34] is introduced
    for matching. Hirschmuller [35] uses mutual information (MI) with correlation-based
    method to resolve radiometric inconsistencies in images matching. Other matching
    features used include robust selective normalized cross correlation (RSNCC) [36]
    for multi-modal and multispectral image registration, and cross-channel normalized
    gradient (CCNG) [13] for multispectral image registration. However, in cross-spectral
    stereo matching, the crucial problem is the spectral difference. Techniques such
    as [18], [19], [33] do not work well because of the intensity consistency assumption.
    Although radiometric inconsistencies that are handled in [34], [35] are related
    to spectral difference, they have very different properties. Radiometric changes
    (e.g., caused by the varying exposure or lighting) mostly preserve the relative
    ordering of local scene point intensities. In contrast, in multispectral imaging,
    the relative ordering of local intensities can change arbitrarily, including order
    reversal. This is because different materials tend to have different responses
    at different wavelengths. Both [13], [36] are applied to multispectral imaging.
    Unfortunately, in [36], errors occur in regions with uncorrelated textures. Meanwhile,
    [13] describes a technique that operates on broad band-pass RGB color channels;
    it is not expected to handle the single narrow band-pass channel images in our
    H-LF as well using [34]. 2.3 Light Field Stereo Matching Many stereo techniques
    have been proposed [37], including local methods [19], semi-global methods [35],
    and global methods [33]. More recently, these techniques have been adapted for
    LFs. For exmaple, Wanner and Goldlucke[15] extract the direction field in the
    Epipolar Image to estimate disparity. Yu et al. [4] use geometric structures of
    3D lines in ray space to improve depth with encoded line constraints. Tao et al.
    [8] introduce the defocus cue combined with correspondence for depth estimation.
    Chen et al. [16] propose a bilateral consistency metric to handle occluding and
    non-occluding pixels, while Lin et al. [7] make use of the LF focal stack to recover
    depth. Wang et al. [17] handle occlusion through edge detection. Again, these
    solutions cannot be directly applied for H-LF stereo matching; under spectral
    variations, regular data consistency measures (such as focusness) are no longer
    effective. Our spectral-dependent H-LF stereo matching technique addresses the
    cross-spectral inconsistency problem by using a spectral-invariant feature descriptor,
    applying local selection of views, and using spectral-aware defocus cues. We also
    handle occlusion in a manner similar to [17]. SECTION 3 Hyperspectral Light Field
    Imager (HLFI) To simultaneously acquire spatial, angular, and spectral samples
    of the plenoptic function, we build a hyperspectral light field imager. The left
    of Fig. 1 shows our HLFI setup: we use an array of 5 \times 6 monochrome cameras,
    each equipped with a narrow band-pass filter centered at a different wavelength.
    The spectral responses of the filters are shown in Fig. 2. These filters sample
    the visible spectrum, centered from 410\;\mathrm{nm} to 700\;\mathrm{nm} with
    an 10\;\mathrm{nm} interval. The bandwidth of each filter is 10 nm (i.e., \pm
    5\; \mathrm{nm} ) with \pm 2\;\mathrm{nm} uncertainty. Due to the uncertainty,
    neighboring filters have responses that overlap. Fortunately, the response drop-off
    for each narrow band-pass filter is steep. As shown in Fig. 2, the overlaps occur
    below 35 percent quantum efficiency, where drop-off is rapid. We treat each filter
    response as a Dirac delta function F_{\lambda _{i}}(\lambda)=\delta (\lambda -
    \lambda _{i}) , where \lambda _i is the center wavelength. Fig. 2. The spectral
    profile of narrow band-pass filters. In our setup, we mount 30 filters on camera
    array (Fig. 1). These filters sample the visible spectrum, centered from 410\;\mathrm{nm}
    to 700\;\mathrm{nm} with an 10\;\mathrm{nm} interval as this figure shows. Each
    bandwidth is 10\;\mathrm{nm} (i.e., \pm 5\;\mathrm{nm} about the central wavelength)
    with \pm 2\;\mathrm{nm} uncertainty. The overlaps occur near 35 percent quantum
    efficiency with rapid drop-off. Show All To accommodate the extra spectral dimension,
    we modify the two-plane LF representation [1], [38] to L(u, v, s, t, \lambda)
    for the sampled hyperspectral light field (H-LF). (u, v) and (s, t) represent
    the ray intersection with the aperture and sensor planes (respectively) at wavelength
    \lambda . The image I(s,t,\lambda _{i}) on (s,t) corresponding to narrow band-pass
    spectral profile F_{\lambda _{i}}(\lambda) centered at wavelength \lambda _{i}
    is modeled as: \begin{align} \nonumber I(s,t,\lambda _{i})& = \int\int\int L(u,v,s,t,\lambda)A(u,v)
    C(\lambda) \\ & \quad \cdot F_{\lambda _{i}}(\lambda) \cos ^4{\theta } d\lambda
    du dv, \tag{(1)} \end{align} View Source where A(u,v) is the aperture function,
    \theta is incident angle of the ray, and C(\lambda) is the camera spectral response
    function. We ignore \cos ^4{\theta } using the paraxial assumption. Equation (1)
    simplifies to: \begin{align} \nonumber I(s,t,\lambda _{i})&= C(\lambda _{i}) \int\int
    L(u,v,s,t,\lambda _{i})A(u,v)du dv\\ &= C(\lambda _{i})S(s, t, \lambda _{i}),
    \tag{(2)} \end{align} View Source where S(\lambda _{i}) is the latent radiance
    image at spectrum \lambda _{i} while C(\lambda _{i}) is the spectral response
    function. SECTION 4 Two-View Spectral-Aware Matching In this section, we describe
    our new approach to matching two views \mathcal {L} and \mathcal {R} corresponding
    to narrow band spectra centered at two different wavelengths \lambda _L and \lambda
    _R (respectively). 4.1 Spectral-Invariant Feature Descriptor Traditional measures
    for correspondence assume either brightness constancy or preservation of brightness
    ordering. As mentioned earlier, such measures (including direct gradient-based
    measures) fail because cross-spectral images violate these assumptions. Fig. 3
    shows an example from the Middlebury dataset [37]. The red channel of \mathcal
    {L} (Fig. 3a) is markedly different from the blue channel of \mathcal {R} (Fig.
    3b); for example, edge pixels around the lamp exhibit significant inconsistencies
    across the image pair. This demonstrates that we need to devise a new feature
    descriptor for cross-spectral images. Fig. 3. Cross-channel stereo imaging on
    the Tsukuba image pair. (a) and (b): Red channel of \mathcal {L} and blue channel
    of \mathcal {R} , respectively. (c) and (d): respective gradient magnitudes. (e)
    and (f): respective gradient directions. Section 4.1 describes how we match boundary
    (e.g., \mathbf {p}_1 -\mathbf {q}_1 ) and non-boundary (e.g., \mathbf {p}_2 -\mathbf
    {q}_2 ) pixels. The pixels denoted with primes (\mathbf {p}_1^{\prime } , etc.)
    are neighboring pixels. Show All We first eliminate the effect caused by the camera
    spectral response. From Equation (2), for two corresponding pixels \mathbf {p}
    and \mathbf {q} (\mathbf {p},\mathbf {q}\in N^2 ), we have I_L({\mathbf {p}})
    = C(\lambda _L)S_{\mathbf {p}}(\lambda _L) and I_R({\mathbf {q}}) = C(\lambda
    _R)S_{\mathbf {q}}(\lambda _R) . We normalize them to yield: \begin{equation}
    \left\lbrace \begin{array}{ll}\widetilde{I}_L({\mathbf {p}}) &= \frac{I_L({\mathbf
    {p}})}{\bar{I}_L}=\frac{S_{\mathbf {p}}(\lambda _L)}{\bar{S}(\lambda _L)}\\ \widetilde{I}_R({\mathbf
    {q}}) &= \frac{I_R({\mathbf {q}})}{\bar{I}_R}=\frac{S_{\mathbf {q}}(\lambda _R)}{\bar{S}(\lambda
    _R)} \end{array} \right., \tag{(3)} \end{equation} View Source where \bar{I}_L
    and \bar{I}_R are the mean intensities, and \bar{S}(\lambda _L) and \bar{S}(\lambda
    _R) are the average radiances in the corresponding views. For the remainder of
    the paper, we use \widetilde{I}_L({\mathbf {p}}) and \widetilde{I}_R({\mathbf
    {q}}) as inputs, eliminating the effect of the camera spectral response while
    still depending on the spectrum. We exploit the gradient of image as the feature
    descriptor. M(\mathbf {p}) and \Theta (\mathbf {p}) represent the magnitude and
    direction of the gradient at \mathbf {p} , respectively: M({\mathbf {p}}) = \sqrt{\nabla
    _x \widetilde{I}(\mathbf {p})^2 + \nabla _y \widetilde{I}(\mathbf {p})^2} and
    \Theta ({\mathbf {p}}) = \mathbf {atan} (\nabla _y {\widetilde{I}(\mathbf {p})}
    / {\nabla _x \widetilde{I}(\mathbf {p})}) . In Figs. 3c and 3d show the magnitudes
    of gradient for (a) and (b); (e) and (f) shows the directions of gradient for
    (a) and (b) quantized within [0,\pi ] . We consider two cases, based on proximity
    to edges. Case 1: Suppose corresponding pixels \mathbf {p}, \mathbf {q} and their
    respective neighbors \mathbf {p^{\prime }}, \mathbf {q^{\prime }} are all part
    of the same object (e.g., \mathbf {p}_2, \mathbf {q}_2 are adjacent to \mathbf
    {p}^{\prime }_2,\mathbf {q}^{\prime }_2 , respectively, in Fig. 3). Then, |\widetilde{I}_L(\mathbf
    {p})-\widetilde{I}_L(\mathbf {p^{\prime }})| \simeq |\widetilde{I}_R(\mathbf {q})-\widetilde{I}_R(\mathbf
    {q^{\prime }})| , implying that the gradient magnitude and direction should be
    approximately the same, i.e., M_L({\mathbf {p}})\simeq M_R({\mathbf {q}}) and
    \Theta _L({\mathbf {p}})\simeq \Theta _R({\mathbf {q}}) . Case 2: Suppose the
    pixels lie near an edge (e.g., \mathbf {p}_1, \mathbf {q}_1 are adjacent to \mathbf
    {p}^{\prime }_1,\mathbf {q}^{\prime }_1 , respectively, in Fig. 3). The foreground
    and background correspond to objects with different spectral responses and the
    magnitude measure is no longer consistent. However, note that the gradient directions
    should still be similar. We design a feature descriptor that incorporates both
    edge and non-edge features at each pixel. The non-edge features couple the gradient
    magnitude and direction histograms, whereas the edge features are an extension
    of HOG we call Overlapping HOG or O-HOG. Unlike traditional histograms, where
    every bin represent a separate range of values, in O-HOG, adjacent bins have overlapping
    values (i.e., they share some range of values). This is to more robustly handle
    view and spectral variations that exist in cross-spectral matching. By comparison,
    even a slight change in perspective or spectrum may lead to misalignment in regular
    HOG [20]. To find correspondence, we first calculate the gradient magnitude and
    direction histograms (with K_1 and K_2 bins, respectively). Given a local window
    \mathbf {U}({\mathbf {p}},w) \in \mathcal {N}^{w^2 \times 2} centered at \mathbf
    {p} with size w \times w for a stack of magnitude and direction images, we count
    weighted votes for bins in the magnitude histogram \mathbf {h}_1(\mathbf {p},w,K_1)
    and direction histogram \mathbf {h}_2(\mathbf {p},w,K_2) . Specifically, the k
    -th bin b_i^{(k)}(\mathbf {p},w) of \mathbf {h}_i (i = 1, 2 ;k\in [0,K_i -1))
    is aggregated as \begin{equation} b_i^{(k)}(\mathbf {p},w) = \frac{\sum \nolimits
    _{\mathbf {u}_t \in \mathbf {U}(\mathbf {p},w)} G(\mathbf {p},\mathbf {u}_t,\sigma
    _g) f(\mathbf {u}_t)}{\sum \nolimits _{j \in [0, K_i-1] }b_i^{(j)}}, \tag{(4)}
    \end{equation} View Source where G(\mathbf {p},\mathbf {u_t},\sigma _g) = \exp
    ({-||\mathbf {p}-\mathbf {u}_t||^2_2}/{2\sigma _g}^2) is a spatial weight kernel,
    and f(\mathbf {u}_t) is a truncation function defined as \begin{equation} f(\mathbf
    {u}_t) = \left\lbrace \begin{array}{ll}1& \text{if } Q(\mathbf {u}_t) \in [k(1-o)s,k(1-o)s+s)\\
    0& \text{otherwise} \end{array} \right. . \tag{(5)} \end{equation} View Source
    Here o is the overlapping portion between the neighboring bins and s is the bin
    width. For \mathbf {h}_1 , Q(\mathbf {u}_t) = M(\mathbf {u}_t) ; for \mathbf {h}_2
    , Q(\mathbf {u}_t) = \Theta (\mathbf {u}_t) . Similarly, for the O-HOG histogram
    \mathbf {h}_3(\mathbf {p},w,K_3) , the k -th bin b_3^{(k)}(\mathbf {p},w) is computed
    as \begin{equation} b_3^{(k)}(\mathbf {p},w) = \frac{\sum \nolimits _{\mathbf
    {u}_t \in \mathbf {U}(\mathbf {p},w)} G(\mathbf {p},\mathbf {u}_t,\sigma _g) M(\mathbf
    {u}_t) f(\mathbf {u}_t)}{\sum \nolimits _{j \in [0, K_3-1] }b_3^{(j)}} . \tag{(6)}
    \end{equation} View Source Note that for \mathbf {h}_3 , Q(\mathbf {u}_t) = \Theta
    (\mathbf {u}_t) in Equation (5). We set s=1/64 and o=1/16 for both \mathbf {h}_1
    , \mathbf {h}_2 and \mathbf {h}_3 . K_1=K_2=K_3 , all rounded up to 68. For each
    pixel \mathbf {p} , we define descriptor \mathbf {D_p} = \left[ \alpha _1 \mathbf
    {h}_1^T, \alpha _2 \mathbf {h}_2^T, \alpha _3 \mathbf {h}_3^T \right]^T , with
    \alpha _1 , \alpha _2 , and \alpha _3 being weights. Recall that \mathbf {h}_1
    and \mathbf {h}_2 represent non-edge features of \mathbf {p} defined by Equation
    (4), while \mathbf {h}_3 represents the edge feature of \mathbf {p} defined by
    Equation (6). Since M(\mathbf {p}) is the edge strength of \mathbf {p} , we simply
    reuse M(\mathbf {p}) to get \alpha _1=\alpha _2=\beta \exp ({-M^2(\mathbf {p})}/{\sigma
    _w}) and \alpha _3=1-\alpha _1-\alpha _2 . In our work, \beta =1/2 and \sigma
    _w=0.16 . For robustness, we build a 3-level pyramid structure with different
    patch widths \mathbf {w}=[w_1,w_2,w_3]^T to obtain the final descriptor \mathbf
    {H_p} = [ \mathbf {D_p^T}(w_1), \mathbf {D_p^T}(w_2), \mathbf {D_p^T}(w_3) ]^T
    with K elements, where K=3(K_1+K_2+K_3) . In all our experiments, \mathbf {w}=[3,5,9]^T
    . Fig. 4 shows the structure of our feature descriptor. Fig. 4. Our spectral-invariant
    feature descriptor \mathbf {H} is based on weighted histograms for 3-level pyramids
    of the gradient magnitude and direction maps. \mathbf {h}_1 and \mathbf {h}_2
    are the histograms for gradient magnitude and direction, while \mathbf {h}_3 represents
    O-HOG. Show All 4.2 Spectral-Invariant Similarity Metric A popular similarity
    metric for stereo matching is the normalized cross correlation (NCC) [19]: \begin{align}
    \xi (I) = \frac{\sum \nolimits _{\mbox{$\begin{array}{c}\mathbf {u}_i \in \mathbf
    {U}_{L}\\ \mathbf {u}_j\in \mathbf {U}_{R}\end{array}$}} (I_L(\mathbf {u}_i)-\bar{I}_L)(I_R(\mathbf
    {u}_j)-\bar{I}_R) }{ \sqrt{ \sum \nolimits _{\mathbf {u}_i \in \mathbf {U}_{L}}
    (I_L(\mathbf {u}_i)-\bar{I}_L)^2 \sum \nolimits _{\mathbf {u}_j \in \mathbf {U}_{R}}
    (I_R(\mathbf {u}_j)-\bar{I}_R)^2 } }, \tag{(7)} \end{align} View Source where
    \bar{I}_L and \bar{I}_R are the mean values of \mathbf {U}_{L}(\mathbf {p},w)
    and \mathbf {U}_{R}(\mathbf {q},w) , respectively, in domain I (e.g., intensity).
    Unfortunately, NCC cannot be directly used to match multi-dimensional features.
    Note that each element h^{(i)} in \mathbf {H} is independent of any other element
    h^{(j)} (j \not=i ), and represents a unique attribute of \mathbf {H} (as shown
    in Fig. 4). We define our similarity metric as \xi (\mathbf {H})=\sum \limits
    _{i=0}^{K-1} \omega _i \xi (h^{(i)}) , where \omega _i is a similarity weight
    of h^{(i)} . In principle, we can simply use h^{(i)} as w_i . In practice, for
    robustness to noise, we use the mean \bar{h}^{(i)} instead of h^{(i)} as weights.
    Since h_{\mathbf {p}}^{(i)} and h_{\mathbf {q}}^{(i)} play equally important roles
    in computing \xi {(\mathbf {H})} , the final metric we use incorporates both,
    leading to the Bidirectional Weighted Normalized Cross Correlation (BWNCC). The
    forward component weighted by \bar{h}_{\mathbf {p}}^{(i)} represents the similarity
    between \mathbf {p} and \mathbf {q} , while the backward component weighted by
    \bar{h}_{\mathbf {q}}^{(i)} represents the similarity between \mathbf {q} and
    \mathbf {p} . BWNCC is thus defined as \begin{equation} \xi _{bwncc}(\mathbf {H})
    = \sqrt{\sum \limits _{i=0}^{K-1} \xi {(h^{(i)})} \bar{h}_{\mathbf {p}}^{(i)}
    \sum \limits _{j=0}^{K-1} \xi {(h^{(j)})} \bar{h}_{\mathbf {q}}^{(j)}} . \tag{(8)}
    \end{equation} View Source SECTION 5 H-LF Stereo Matching Scheme Our new feature
    descriptor and metric enable more reliable feature selection and matching. Compared
    with binocular stereo, LF stereo matching has two different properties: use of
    many views and refocusing. When modeled as a disparity labeling problem, the correspondence
    cost makes use of the multiple views while defocus cost is based on refocusing
    (e.g., [4], [7], [8], [17]). We denote \Omega as all LF views (s,t) and estimate
    the disparity map for the central view (s_o,t_o) . For simplicity, we use I_{\mathbf
    {p}}(s,t) to represent \widetilde{I}(u_{\mathbf {p}},v_{\mathbf {p}},s,t,\lambda
    _{(s,t)}) in Equation (3). The correspondence cost is typically cast as ([4],
    [7], [8], [17]): \begin{equation} C(\mathbf {p},f(\mathbf {p})) \propto \frac{1}{|\Omega
    |} \sum _{(s,t)\in \Omega }|I_{\mathbf {p}}(s,t)-I_{\mathbf {p}}(s_o,t_o)|_2^2
    . \tag{(9)} \end{equation} View Source For H-LF, we find a proper subset \Omega
    ^* (\Omega ^* \subseteq \Omega ) and use that in conjunction with our feature
    descriptor and metric to maximize spectral consistency. The defocus cost in [17]
    is based on the depth-from-defocus formulation for non-occlusion regions: \begin{equation}
    D(\mathbf {p},f(\mathbf {p})) \propto \nabla _{(x,y)}\bar{I}_{\mathbf {p}} . \tag{(10)}
    \end{equation} View Source For occlusion regions the defocus cost is \begin{equation}
    D(\mathbf {p}, f(\mathbf {p}))\propto \frac{1}{|\Omega |}\sum _{(s,t)\in \Omega
    } |I_\mathbf {p}(s,t)-\bar{I}_{\mathbf {p}}|_2^2, \tag{(11)} \end{equation} View
    Source where \bar{I}_{\mathbf {p}}=1/|\Omega |\cdot \sum _{(s,t)\in \Omega } I_{\mathbf
    {p}}(s,t) . However, direct use of the defocus measure in H-LF would fail due
    to spectral variance. We instead propose a new defocus cost based on hue-spectrum
    matching. After extracting two initial disparity maps f^*_c (based on correspondence
    cost) and f^*_d (based on defocus cost), we then impose regularization to generate
    the refined result f^{\dagger } . 5.1 Correspondence Cost Recall that the correspondence
    cost measures similarity of corresponding pixels. For a hypothesized disparity
    f({\mathbf {p}}) , we compute this cost using our spectral-invariant feature descriptor
    and BWNCC metric: \begin{equation} C(\mathbf {p},f(\mathbf {p}))=\frac{1}{|\Omega
    ^*|}\sum _{(s,t)\in \Omega ^*} - \log (\xi _{bwncc}(\mathbf {H})) . \tag{(12)}
    \end{equation} View Source Instead of matching \mathbf {p} in (s_o,t_o) with pixel
    \mathbf {q} across all LF views, we use only a subset of views \Omega ^* that
    share a coherent appearance (response). To do so, we first compute the arithmetic
    mean gradient magnitude over all \mathbf {q} . Next, we determine if the gradient
    magnitude of \mathbf {p} is above or below the mean value. If it is above, then
    it is likely that \mathbf {p} is an edge pixel; we use only pixels \mathbf {q}
    in the H-LF views with a higher gradient magnitude. Similarly, if it is below,
    it is likely that \mathbf {p} is a non-edge point, and we use only the ones with
    lower gradient magnitudes. In addition, we treat occluding and non-occluding pixels
    differently using the technique described in [17] to extract an initial disparity
    map f^*_c based on correspondence cost. If \mathbf {p} is non-occluding, f^*_c(\mathbf
    {p})=\min _f \lbrace C\rbrace . If \mathbf {p} is occluding, we partition \Omega
    ^* into occluder and occluded regions \Omega ^*_1 and \Omega ^*_2 (analogous to
    [17]), then compute C_1 and C_2 using Equation (12). This yields f^*_c (\mathbf
    {p})=\min _f\lbrace C_1,C_2\rbrace . 5.2 Defocus Cost A unique property in LF
    stereo matching is the availability of a synthetic focal stack, synthesized via
    LF rendering. Conceptually, if the disparity hypothesis is correct, the color
    variance over correspondences in all (non-occluding) views should be very small.
    If it is incorrect, the variance would be large, causing aliasing. In [17], the
    defocus cost measures the occlusion and non-occlusion regions separately in terms
    of color consistency. However, the traditional defocus cost cannot be used in
    our work because we cannot measure color consistency under different spectral
    responses. We adapted this cost to be spectral-aware. As Fig. 5 shows, given a
    hypothesized disparity f(\mathbf {p}) , we estimate RGB color of \mathbf {p} for
    a reference camera. To do this, we first form a spectral profile of \mathbf {p}
    as P_{\mathbf {p}}(\lambda) by indexing \lambda _{(s,t)} using I_{\mathbf {p}}(s,t)
    into respective views. Next, we use the spectral profile to synthesize its RGB
    value. In our experiments, we use the spectral response function of the PTGrey
    FL3-U3-20E4C-C camera (reference camera) as \mathbf {P}_{c} (\lambda)=[P_r({\lambda
    }),P_g({\lambda }),P_b({\lambda })]^T and compute RGB values \mathbf {V}=[R,G,B]^T
    by summing P_{\mathbf {p}}(\lambda _{(s,t)}), \mathbf {P}_{c}(\lambda _{(s,t)})
    over the respective bandwidths: \begin{equation} \mathbf {V}=\frac{\sum _{(s,t)\in
    \Omega } P_{\mathbf {p}}(\lambda _{(s,t)}) \mathbf {P}_{c}(\lambda _{(s,t)})}{\mathbf
    {P}_{c}(\lambda _{(s,t)})} . \tag{(13)} \end{equation} View Source Fig. 5. Spectral-aware
    defocus cue. Given a disparity hypothesis, we combine corresponding pixels from
    H-LF to form its spectral profile \mathbf {P}_p(\lambda) . Next, we use the camera
    (PTGrey FL3-U3-20E4C-C) spectral response curves \mathbf {P}_c(\lambda) to map
    this profile to RGB color. We then convert the RGB color to its hypothesized wavelength
    \lambda _r using the CIE 1931 Color Space. Finally, we match the observed profile
    with a Gaussian profile \mathbf {P}_g(\lambda) centered at \lambda _r via K-L
    divergence. Show All Finally, we map the RGB color back to spectra \lambda _r
    by first converting it to hue before using a table to map hue to \lambda _r based
    on CIE 1931 Color Space [39]. If the disparity hypothesis is correct, P_{\mathbf
    {p}}(\lambda) and the final RGB values estimation should be accurate. The captured
    spectra should then approximately form a Gaussian distribution centered at \lambda
    _r , with the probability density function \begin{equation} P_{g}(\lambda)=\frac{1}{\sigma
    _d\sqrt{2\pi }} \cdot \exp {\left(-\frac{(\lambda -\lambda _r)^2}{2\sigma _d^2}
    \right)} . \tag{(14)} \end{equation} View Source In our implementation, we use
    the special case of \lambda _r=550\;\mathrm{nm} (middle of [410\;\mathrm{nm},700\;\mathrm{nm}]
    ) to set \sigma _d=96.5 . This is to ensure that P_g(\lambda) have at least 30
    percent response in overlapping the visible spectrum throughout (especially in
    the corner cases of \lambda = 400\;\mathrm{nm} and \lambda = 700\;\mathrm{nm}
    ). We subsequently normalize P_{\mathbf {p}}(\lambda) to P_{\mathbf {p}}^*(\lambda)=P_{\mathbf
    {p}}(\lambda)/\sum _{(s,t)\in \Omega }P_{\mathbf {p}}(\lambda _{(s,t)}) , and
    measure the Kullback–Leibler divergence [40], [41] from P_{\mathbf {p}}^*(\lambda)
    to P_g(\lambda) . This results in our defocus cost \begin{equation} D(\mathbf
    {p},f(\mathbf {p}))=\sum _{(s,t)\in \Omega } P_g(\lambda _{(s,t)})\log {\frac{P_g(\lambda
    _{(s,t)})}{P_{\mathbf {p}}^*(\lambda _{(s,t)})}} . \tag{(15)} \end{equation} View
    Source Finally, we have f^*_d(\mathbf {p})=\min _f\lbrace D\rbrace . 5.3 Regularization
    The energy function for disparity hypothesis f that is typically used in an MRF
    is ([8], [17]) \begin{equation} E(f)=E_{unary}(f)+E_{binary}(f) . \tag{(16)} \end{equation}
    View Source We adopt the binary term similar to Wang et al. [17] for smoothness
    and to handle occlusion. The major difference is that we use spectral-aware defocus
    cues (described in Section 5.2). Our unary term is defined as \begin{align} \nonumber
    E_{unary}(f)&=\sum _{\mathbf {p}} \gamma _c |C(f(\mathbf {p}))-C(f^*_c(\mathbf
    {p}))|\\ &\quad +|D(f(\mathbf {p}))-D(f^*_d(\mathbf {p}))|, \tag{(17)} \end{align}
    View Source where \gamma _c adjusts the weight between defocus and correspondence
    cost. (Its value is 0.45 for synthetic data and 0.6 for real data.) Using a method
    similar to that of Wang et al. [17], we minimize this function to generate the
    desired disparity map f^{\dagger } . SECTION 6 Plenoptic Cube Completion The results
    of our H-LF stereo matching technique can be used to complete the missing dimensions.
    The direct approach would be to use the disparity map of the central view to warp
    images to propagate the missing information. The problem, however, is that the
    warped images will contain holes due to occlusion. While it is possible to perform
    independent pairwise stereo matching between all views, this approach does not
    fully exploit the properties of LFs. We instead present a technique for cross-spectral
    joint binocular stereo. 6.1 Disparity Initialization We first warp the disparity
    map of the central view, f^{\dagger }_{(s_0,t_0)} (using the technique described
    in Section 5) to individual LF views as their initial disparities: \begin{equation}
    f^*_{(s,t)}(u+d(s-s_0),v+d(t-t_0))=d, \tag{(18)} \end{equation} View Source where
    d=f_{(s_0,t_0)}^{\dagger }(u,v) . At this point, at each view, f_{(s,t)}^*(u,v)
    is an incomplete disparity map. There are pixels with invalid depth due to occlusion,
    being outside the field-of-view of the central view, and mismatches. However,
    regions that are valid can be used to guide and refine correspondences between
    cross-spectral image pairs. 6.2 Disparity Estimate Using our BWNCC metric described
    in Section 4, we extract the disparity map for an image pair using Graph Cuts
    [33]. The energy function in [33] is \begin{equation} E(f)=E_{data}(f)+E_{occlu}(f)+E_{smooth}(f)+E_{unique}(f)
    . \tag{(19)} \end{equation} View Source E_{data}(f) is the data term that calculates
    similarity between corresponding pixels: \begin{equation} E_{data}(f)=\sum _{\mathbf
    {p}}|C(f(\mathbf {p}))-C(f^*(\mathbf {p}))|, \tag{(20)} \end{equation} View Source
    where C(f(\mathbf {p})) is defined in Equation 12 but applied to two views (i.e.,
    left and right or top and bottom). E_{occlu}(f) is the occlusion term to minimize
    the number of occluded pixels while E_{smooth}(f) is the smoothness term that
    favors piecewise constant maps. E_{unique} enforces uniqueness of disparities
    between image pairs. The last three terms are same as those in [33]. The disparity
    maps for all image pairs (with vertical or horizontal neighbors) are computed
    using Graph Cuts [33]. These disparity maps are then merged to produce a single
    one denoted as f_{(s,t)} . 6.3 Disparity Refinement As mentioned in Section 6.1,
    f^*_{(s,t)} has regions of invalid depth. Furthermore, f_{(s,t)} is likely to
    have unreliable depths due to occlusion in neighboring views. Park et al. [42]
    propose an optimization technique using RGB-D images to acquire a high-quality
    depth map. This technique uses confidence weighting in terms of color similarities,
    segmentation, and edge saliency. We use a similar approach to refine f_{(s,t)}
    , with the difference being the confidence weighting is adapted to our single
    channel spectral images. This results in the improved disparity map f^{\dagger
    }_{(s,t)} for each view. 6.4 Image Registration We use f^{\dagger }_{(s,t)} to
    warp images. First, all pixels \mathbf {p} on left (or top) view are mapped to
    \mathbf {q} on right (or bottom) view. We then register all images currently on
    right (or bottom) to left (or top) view. This is iterated for all neighboring
    pairs until the plenoptic cube is completed, i.e., when all the missing spectra
    are propagated across all the views. For an H-LF imager with M \times N views,
    the number of hyperspectral images in the completed plenoptic cube is M \times
    N \times MN , with each view having MN images corresponding to MN different spectra.
    SECTION 7 Experiments and Applications In this section, we report the results
    of our technique and how they compare with competing state-of-the-art. We also
    describe two applications (namely, color sensor emulation and H-LF refocusing)
    that are made possible using the depth information generated using our technique.
    7.1 Experimental Setup Our prototype HLFI consists of a 5 \times 6 monochrome
    camera array (Fig. 1). The cameras are MER-132-30GM from Daheng ImaVision, with
    a resolution of 1292\times 964 ; they are synchronized via GenLock. The lens are
    M0814-MP2 from Computar, with a focal length of 8\;\mathrm{mm} . We mount 30 narrow
    bandpass filters (from Rayan Technology) on cameras centered wavelengths between
    410\;\mathrm{nm} to 700\;\mathrm{nm} at a 10\;\mathrm{nm} interval. Data collection
    and processing are done on a Lenovo ThinkStation P500 with a Intel(R) Xeon(R)
    4-core CPU E5-1630 running at 3.70 GHz. We calibrate our cameras using Zhang''s
    algorithm [43] to extract the intrinsic and extrinsic parameters. Since we use
    a black-and-white checkerboard and all the filters are within the visible spectra,
    the calibration images have sufficient contrast for corner detection. Once the
    cameras are calibrated, the views are rectified to simplify stereo matching. As
    mentioned in Section 4.1, throughout all our experiments, we set s=1/64 , o=1/16
    , and \mathbf {w}=[3,5,9]^T to generate our hierarchical feature descriptor \mathbf
    {H} . 7.2 Validation for Feature Descriptor with BWNCC We compared results of
    pairwise stereo matching using Graph Cuts [33] with SSD [18], NCC [19], and the
    recent RSNCC [36] measures against those for our spectral-invariant descriptor
    with BWNCC measure. We first ran experiments involving synthetic data adapted
    from the Middlebury stereo vision datasets [37]. To emulate spectrum inconsistency,
    we treat the red channel of \mathcal {L} and the blue channel of \mathcal {R}
    as the pseudo cross-spectral pair. Fig. 6 compares the visual quality of the results
    using different methods and Table 1 shows the quantitative comparisons in terms
    of bad5.0 (percentage of “bad” pixels whose error is greater than 5 pixels [37]).
    Our approach significantly reduces error in stereo matching. Fig. 6. Cross-channel
    stereo matching results for three Middlebury datasets. From left to right: red
    channel of the left image, blue channel of right image, ground truth disparity
    map, estimated disparity map by SSD, NCC, RSNCC methods with graph cuts, and our
    proposed feature descriptor with BWNCC metric. Our method can estimate much better
    disparity maps compared with these state-of-the-art methods. Show All TABLE 1
    Comparison of bad5.0 Error Metric (Smaller Values Are Better) We also ran experiments
    on datasets captured using our HLFI on real scenes, again comparing our method
    with other competing techniques. Fig. 7 shows results for two scenes. Visually,
    our approach outperforms the other techniques; for example, as can be seen at
    the bottom row, our technique is able to recover the guitar edge where other techniques
    fail due to spectral inconsistencies. Fig. 7. Cross-spectral stereo matching results
    on real scenes captured using our HLFI. The first and second columns are left
    and right images captured by two adjacent cameras at different spectra. The other
    columns show extracted disparity maps using SSD, NCC, RSNCC and our technique.
    Qualitatively, our results outperform the other competing techniques. Show All
    7.3 H-LF Stereo Matching Results In one set of experiments, we generate synthetic
    H-LF scenes from regular LFs used in Wanner et al. [15]. For each scene, we choose
    5 \times 6 views with uniform baseline. For each view, we add a synthetic tunable
    filter. The synthetic filter is actually a series of coefficients which are the
    transmittance in Red, Green, and Blue channels associated with the filter''s wavelength.
    The coefficients are based on the approximate relationship between RGB color and
    rendering spectra as provided in [44]. Finally, we render 5 \times 6 spectral
    images from original images by adjusting the filter transmittance in the RGB channels
    and converting them to gray scale. Because this synthesized spectral profile is
    different from that for our HLFI, we choose different values of \gamma _c in Equation
    (17) (0.45 for synthetic data and 0.6 for real data). In another set of experiments,
    we compare our H-LF stereo matching results with techniques by Tao et al. [8],
    Lin et al. [7] and Wang et al. [17], on synthetic and real data. Fig. 8 compares
    the disparity maps on the synthetic dataset. The close-up regions (red and green
    boxes) show how well our technique works compared to the others. As Table 2 shows,
    our technique has the lowest RMSE. Fig. 8. H-LF results for two synthetic scenes
    from Wanner et al. [15], with each view having a different spectral response.
    We show our result as well as those of previous LF stereo matching methods (Tao
    et al. [8], Lin et al. [7], and Wang et al. [17]). The two close-ups show the
    relative quality of our result. Show All TABLE 2 Comparison of RMSE (Smaller Values
    Are Better) for H-LF Stereo Matching on Synthetic Data Shown in Fig. 8 Fig. 9
    shows H-LF stereo matching results for three real scenes. The overall visual quality
    of our results is better than that for the other competing techniques. In particular,
    our technique is better able to handle scene detail; see, for example, the mug
    in the top scene and guitar''s neck in the bottom scene. Fig. 9. H-LF stereo matching
    results for three real scenes captured by our HLFI. We show our results as well
    as those of previous LF stereo matching methods (Tao et al. [8], Lin et al. [7],
    and Wang et al. [17]). The two close-ups show how well our technique can recover
    scene detail. Show All These results show our approach outperforms the state-of-the
    art in visual quality, accuracy, and robustness on both synthetic and real data.
    They validate our design decisions on handling cross-spectral variation. 7.4 H-LF
    Reconstruction Results In another experiment, we use our HLFI to capture a room
    scene, processed the data using our technique, and completed its plenoptic cube
    representation. Results are shown in Fig. 10. The raw data are shown in Fig. 10a;
    the scene has colorful objects made with different materials and placed at different
    depths. Fig. 10b shows the completed plenoptic cube. Reconstructed hyperspectral
    datacubes at viewpoints (2, 2), (3, 4), and (5, 6) are shown in Fig. 10c. Selected
    close-ups in Fig. 10d demonstrate that our technique can robustly align occlusion
    and texture boundaries under spectral variation. Fig. 10e shows the spectral profiles
    of three scene points (captured and recovered): a point on the guitar, a cyan
    point surrounded by white letters, and a depth boundary. These results show that
    our reconstruction scheme can robustly align occlusion and texture boundaries
    under spectral variations and recover high fidelity H-LFs. Fig. 10. H-LF reconstruction
    results for a real scene. (a) Raw data acquired by our HLFI, (b) completed plenoptic
    cube, (c) reconstructed hyperspectral datacubes at viewpoints (2, 2), (3, 4),
    and (5, 6), (d) close-ups of representative boundary and textured areas, (e) spectral
    profiles of three scene points: a point on the guitar, a cyan point surrounded
    by white letters, and a depth boundary. Show All Fig. 11 shows the spectral profiles
    (captured/ground truth and recovered) for selected areas on four different objects
    (guitar, toy, book, and bottle) in three sampled views. Given that the selected
    areas are uniform, we merge and average the measurements across the different
    filters to generate ground truth. The recovered spectral profile for each area
    is the average of the spectral profiles of the constituent pixels. The recovered
    spectral profiles mostly align with the ground truth; where they differ (e.g.,
    in the range of 630\;\mathrm{nm}-700\;\mathrm{nm} for area 2) is caused by uncertainty
    due to some texture within the area. In addition, notice that area 4 is at the
    image periphery in view (2,2). The missing spectra in its vicinity in that view
    has a negative impact on the recovered spectral profile. Fig. 11. Comparison of
    H-LF reconstruction results for four representative areas. The top row is a series
    of reconstructed hyperspectral datacubes at viewpoints (2, 2), (3, 4), and (5,
    6). Areas 1-4 are on different materials (guitar, toy, book and bottle); the curves
    are spectral profile of these materials, both recovered in different views and
    compared with the ground truth. Show All Table 3 lists the Relative Average Spectral
    Error (RASE) for these four areas. RASE is a global quality metric of the recovered
    spectra [45] with respect to the ground truth; it is between 0-100 percent, and
    defined as \begin{equation} RASE=\frac{100}{\sum _{l=1}^{L}\mu (l)}\sqrt{L\sum
    _{l=1}^{L}RMSE(l)^2}, \tag{(21)} \end{equation} View Source where \mu (l) is the
    mean intensity of band l and L is the number of bands. The lower RASE is, the
    better the quality of spectra is considered to be. (Please note that ground truth
    is created by manually segmenting the scene and aligning the segments among 30
    bands.) TABLE 3 RASE Values (Smaller Is Better) for Representative Areas and Views
    for H-LF Reconstruction Shown in Fig. 11 Results for another real scene are shown
    in Fig. 12: raw data at view (3,3) in (a), reconstructed results in (b), and error
    maps for representative views in (c). We use the Bhattacharyya distance (BD) to
    measure the dispersion between the recovered spectra and ground truth. The Bhattacharyya
    distance BD(\mathbf {p}) at pixel \mathbf {p} is the negative log of the Bhattacharyya
    coefficient BC(\mathbf {p}) : \begin{align} \nonumber BC(\mathbf {p})&={\sum _{l\in
    L}\sqrt{\frac{p(l)}{\sum _{l\in L} p(l)} *\frac{q(l)}{\sum _{l \in L} q(l)}}}\\
    BD(\mathbf {p})&=-\ln {BC(\mathbf {p})}, \tag{(22)} \end{align} View Source where
    p(l) and q(l) are recovered and ground truth values for band l in pixel \mathbf
    {p} , respectively. BC(\mathbf {p}) is Bhattacharyya coefficient which measures
    similarity between p(\cdot) and q(\cdot) in range [0,1] . Lower BD(\mathbf {p})
    values are better. Fig. 12. H-LF reconstruction results for a real scene. (a)
    Raw data at view (3,3), (b) recovered hyperspectral datacube at view (3,3), (c)
    heat maps of BD({\mathbf {p}}) for reconstructed spectra relative to ground truth
    for views (1,1), (1,3), (1,6), (3,1), (3,3), (3,6), (5,1), (5,3), and (5,6); smaller
    values are better. The regions within black and white rectangles respectively
    are invalid and incorrect due to non-overlapping views (being outside the FOV
    of some cameras, and occlusions, respectively). Show All As Figs. 12b, 12c show,
    the visual quality of spectral reconstruction is good for most of the scene. The
    top and left margins (within black rectangles) are considered invalid because
    those areas are not seen by all the cameras; regions in the scene are considered
    valid only if they are seen by all cameras. As a result, only part of the full
    spectrum is available for reconstruction. Regions within white rectangles are
    incorrect since they exemplify areas with depth discontinuities that cause occlusion.
    Reconstruction errors in these regions are caused by mismatches. Table 4 provides
    quantitative evaluation of valid areas for every view of the scene shown in Fig.
    12a. The RASE values are between 13.3-23.9 percent; unsurprisingly, the highest
    errors occur at the peripheral (extreme) views, where overlap with the farthest
    views is less. Table 5 shows statistics on the percentage of pixels where BD(\mathbf
    {p})\leq 0.05 (similarity between the recovered spectra and ground truth is greater
    than 0.95, or BC(\mathbf {p})\geq 0.95 ). The range of results is 92.8-96.5 percent
    across all views. Again, the values are worse at peripheral views. TABLE 4 RASE
    Values (Smaller Is Better) at All (5 \times 6 5×6) Views in Valid Areas Relative
    to Ground Truth, for the Scene Shown in Fig. 12a TABLE 5 Percentage of Pixels
    with BD(\mathbf {p})\leq 0.05 BD(p)≤0.05 in Valid Areas in the Scene Shown in
    Fig. 12a 7.5 Applications: Color Sensor Emulation and H-LF Refocusing We can use
    the recovered H-LF data to emulate a synthetic camera with a specific spectral
    profile. This allows us to reproduce color images unique to that camera. Fig.
    13 shows two pairs of real images captured by PTGrey FL3-U3-20E4C-C alongside
    our synthesized color images (whose original spectral profile is shown in Fig.
    5). The top pair includes original images (without cropping and alignment) of
    one scene. The red boxes show the incorrect color of the table cloth in the synthesized
    one (right), which is caused by missing spectra due to limited field of view.
    Notice that the top rows of Fig. 10b do not include most of the table cloth; as
    a result, no information on a specific range of the spectrum is available for
    propagation, causing incorrect color synthesis. After removing the region of red
    box and aligning images, we get PSNR of right image is 22.6, given the left image
    as reference. The bottom pair includes cropped and aligned images of another scene,
    and PSNR of right image is 23.1. Both the images and PSNR values show that our
    synthesized color images are reasonable reproductions of the actual versions.
    Fig. 13. Comparison of real and synthetic color images. Left: real images captured
    by a PTGrey FL3-U3-20E4C-C camera (the profile is same as that shown in Fig. 5).
    Right: synthesized images using our acquired H-LF and the camera profile. Given
    left images as references, PSNR of right image on top pair is 22.6 after removing
    the region of red box and alignment, whereas PSNR is 23.1 on bottom pair. Show
    All Fig. 14 shows results of synthetic refocusing for different spectral profiles.
    These results demonstrate that our dynamic H-LF refocusing is different from regular
    LF refocusing; it can focus at any depth layer at any sampled spectrum. Note that
    the banding artifacts are due to the discrete view sampling of our HLFI. Fig.
    14. H-LF refocusing results. Top: spectral profiles of three cameras. Bottom:
    synthetic refocusing results at different depths for the three profiles. Results
    in different rows are at different depths (near, middle and far). Results in different
    columns are synthesized from different profiles respectively. Show All SECTION
    8 Discussion Because our HLFI is fundamentally a multi-view camera system, it
    has the same issues associated with length of baseline versus accuracy and ease
    of correspondence. Our HLFI has two main problems that are specific to multi-spectral
    matching. The first is the computational complexity of our feature descriptor
    and metric. In order to acquire accurate depth, we need to consider both edge
    and non-edge regions hierarchically, and compute the distance using descriptors
    over local patches at different levels. These operations are more computationally
    expensive compared to traditional methods. Our GPU implementation produces depth
    results in about two minutes for datasets shown in Fig. 9 (each dataset has a
    5 \times 6 array of images, with each image having a resolution of 1200 \times
    900 ). More specifically, the GPU is used mainly to extract the hierarchical feature
    descriptor and calculate pairwise BWNCC. Another problem is the incomplete spectral
    reconstruction due to missing views. Each camera samples a narrow band of the
    visible spectrum and a different view of the scene. As a result, different parts
    of the scene would visible to a different subset of cameras in the HLFI. This
    results in incomplete propagation of the missing spectra, as can be seen in Figs.
    10 and 13. More specifically, the table cloth has incorrect colors because cameras
    at the top few rows are not able to capture its appearance, resulting in absence
    of certain spectral bands. There is also the interesting issue of filter arrangement.
    Currently, the filter wavelengths in our HLFI are arranged in raster order. As
    a result, as can be seen on the left of Fig. 1, horizontal neighbors are much
    more similar in appearance than vertical neighbors. This arrangement has implications
    on the H-LF stereo matching and reconstruction. There may be a better way of arranging
    these filters so as to reduce appearance changes in both vertical and horizontal
    directions. While it is possible to redesign with different cameras having the
    same filters, this reduces the spectral sampling density (for the same number
    of cameras and overall visible spectral extent). SECTION 9 Concluding Remarks
    We proposed a hyperspectral light field (H-LF) stereo matching technique. Our
    approach is based on a new robust spectral-invariant feature descriptor to address
    intensity inconsistency across different spectra and a novel cross-spectral multi-view
    stereo matching algorithm. For increased robustness in matching, we show how to
    perform view selection in addition to measuring focusness in an H-LF. We have
    conducted comprehensive experiments by constructing an H-LF camera array to validate
    our claims. Finally, we show how our results can be used for plenoptic cube completion,
    emulation of cameras with known spectral profiles, and spectral refocusing. An
    immediate future direction is to capture and process H-LF video. This will require
    temporal regularization techniques, in addition to requiring efficient compression
    to save bandwidth. In our current setup, the band-pass filters were sequentially
    assigned to the cameras, i.e., the neighboring cameras will have close spectral
    responses. The advantage of this setup is that we can more reliably conduct stereo
    matching and hence warping between adjacent images. Despite this, the baseline
    of the cameras cannot be too large, because we still require good visual overlap
    between images for effective spectral propagation. It would be interesting to
    investigate other camera designs with different spectral distributions to handle
    current limitations. ACKNOWLEDGMENTS This work is partially supported by the programs
    of STCSM (17XD1402900, 17JC1403800). Authors Figures References Citations Keywords
    Metrics Footnotes More Like This 1CCD and 3CCD Color Cameras Performance Comparison
    Applied to Hyperspectral Image Reconstruction IEEE Latin America Transactions
    Published: 2015 Endmember-Assisted Camera Response Function Learning, Toward Improving
    Hyperspectral Image Super-Resolution Performance IEEE Transactions on Geoscience
    and Remote Sensing Published: 2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved.'
  inline_citation: (Zhu, Xue, Fu, Kang, Chen & Yu, 2019)
  journal: IEEE Transactions on Pattern Analysis and Machine Intelligence
  key_findings: The proposed technique significantly improves the accuracy of depth
    estimation for H-LF and can be used for applications such as image warping, color
    image synthesis, and hyperspectral refocusing.
  limitations: The proposed technique requires a custom-built H-LF camera array, which
    may limit its practical applicability.
  main_objective: To develop a hyperspectral light field (H-LF) stereo matching technique
    for automated irrigation systems that can accurately estimate depth and overcome
    the challenges associated with spectral inconsistencies.
  pdf_link: null
  publication_year: 2019
  relevance_evaluation: The paper is highly relevant to the point I am making in my
    literature review, which is that automated, real-time irrigation management systems
    can contribute to the efficient use of water resources and enhance agricultural
    productivity. The paper's focus on hyperspectral stereo matching for automated
    irrigation systems aligns well with the need for advanced sensing and data processing
    techniques to optimize water distribution.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Hyperspectral stereo matching, Cross-spectral matching, View
    selection, Focus measurement
  title: Hyperspectral Light Field Stereo Matching
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1049/iet-ipr.2017.1203
  analysis: '>'
  apa_citation: Bousefsaf, F., Tamaazousti, M., Hadj Said, S., & Michel, R. (2018).
    Image completion using multispectral imaging. IET Image Processing, 12(7), 1164-1174.
    https://doi.org/10.1049/iet-ipr.2017.1203
  authors:
  - Frédéric Bousefsaf
  - Mohamed Tamaazousti
  - Souheil Hadj Said
  - R. Michel
  citation_count: 4
  data_sources: Multispectral images acquired using an ultracompact snapshot camera-recorder
    that senses 16 different spectral channels in the visible spectrum.
  explanation: This study investigates the use of multispectral imaging in image completion,
    which is the process of filling missing regions in an image in a visually plausible
    way. The main objective of this study is to examine whether multispectral imaging
    can enhance the accuracy and consistency of image completion, particularly in
    cases where the missing region lacks clear gradients and significant texture variance.
  extract_1: In this study, we propose to investigate the relevance of multispectral
    frames applied to image completion, an application initially dedicated to 3D RGB
    images.
  extract_2: Results indicate that image completion constrained by spectral segmentation
    ensures better consideration of the surrounding materials and simultaneously improves
    rendering consistency, in particular for completion of flat regions that present
    no clear gradients and little structure variance.
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register IET HUB HOME JOURNALS
    IET PRIZE PROGRAMME SUBJECTS Visit IET IET Image Processing Research Article Free
    Access Image completion using multispectral imaging Frédéric Bousefsaf,  Mohamed
    Tamaazousti,  Souheil Hadj Said,  Rémi Michel First published: 01 July 2018 https://doi.org/10.1049/iet-ipr.2017.1203Citations:
    2 SECTIONS PDF TOOLS SHARE Abstract Here, the authors explore the potential of
    multispectral imaging applied to image completion. Snapshot multispectral cameras
    correspond to breakthrough technologies that are suitable for everyday use. Therefore,
    they correspond to an interesting alternative to digital cameras. In their experiments,
    multispectral images are acquired using an ultracompact snapshot camera-recorder
    that senses 16 different spectral channels in the visible spectrum. Direct exploitation
    of completion algorithms by extension of the spectral channels exhibits only minimum
    enhancement. A dedicated method that consists in a prior segmentation of the scene
    has been developed to address this issue. The segmentation derives from an analysis
    of the spectral data and is employed to constrain research area of exemplar-based
    completion algorithms. The full processing chain takes benefit from standard methods
    that were developed by both hyperspectral imaging and computer vision communities.
    Results indicate that image completion constrained by spectral presegmentation
    ensures better consideration of the surrounding materials and simultaneously improves
    rendering consistency, in particular for completion of flat regions that present
    no clear gradients and little structure variance. The authors validate their method
    with a perceptual evaluation based on 20 volunteers. This study shows for the
    first time the potential of multispectral imaging applied to image completion.
    1 Introduction Image completion consists in filling or restoring missing or damaged
    regions in a visually plausible way. This image processing technique has many
    applications, such as the removal of unwanted objects in photos and panoramas
    [1], image restoration [2], and diminished reality [3]. The research in this field
    has reached an advanced level of maturity, some of the methods being incorporated
    in raster graphics editors [e.g. PatchMatch [4] in Photoshop CS5 (http://www.adobe.com/technology/projects/patchmatch.html)].
    The completion task is non-trivial and is of growing importance in computer vision
    and computer graphics. New completion methods were recently proposed to guide
    the filling of missing regions using prior information about structures [5] and
    perspectives [6], by using guidance maps [7] or by using statistics of similar
    patches [8]. This high-level information corresponds to prior knowledge on the
    geometry of the scene. At last, the completion process is performed and represented
    with red, green, and blue (RGB) values. Rather than employing RGB cameras, multispectral
    camera-recorders [9] provide more detailed information about the spectrum of objects
    present in the scene. Those cameras may be of help to address standard computer
    vision tasks [10], especially when considering the recent introduction of snapshot
    multispectral camera-recorders [11]. Basically, the content of an image depends
    on both its geometrical and spectral dimensions [10]. Multispectral images are
    represented through three-dimensional (3D) datacubes, where a set of 2D images
    is acquired at different bands of wavelengths using dedicated optical devices
    [9]. In the fields of Earth and planetary sciences, datacubes delivered by multispectral
    or hyperspectral cameras are processed and analysed to provide relevant information
    about the chemical composition of the recorded scenes. One of the important advantages
    of this technique is that physical processes like absorption, reflectance, or
    fluorescence spectrum can be estimated for each pixel in the image. It allows
    the detection of chemical changes of objects that cannot be identified with monochromatic
    or colour (RGB) data [12]. The spectral information has been notably employed
    to characterise ocean colour [13], classify glacier surfaces [14], or to sense
    gypsum on Mars [15]. Also, spectral imaging corresponds to a powerful analytical
    tool for biological and biomedical research, notably in order to identify tissue
    abnormalities [12]. The spectral information of a pure material is enough scale-invariant
    to provide very valuable cues to better understand the contents of an image [10].
    Material recognition is presumed to reinforce image processing and understanding
    techniques such as object detection, object recognition, and image segmentation
    [16]. To date, there have been no studies that analyse the relevance of multispectral
    imaging in the image completion context. Analysing multispectral frames instead
    of RGB frames amounts to process the spectral dimension at each pixel of the image.
    This information can be used to improve the renderings by properly updating photometric
    parameters, in particular for diminished reality applications [3]. In this study,
    we propose to investigate the relevance of multispectral frames applied to image
    completion, an application initially dedicated to 3D RGB images. The study first
    provides, in Section 2, a focused description of image completion algorithms that
    will be of interest for the use of multispectral images within the next sections.
    Some basics of multispectral imaging (sensor specifications and pre-processing
    operations) are then presented in Section 3. As the main purpose consists in better
    completing images dedicated to visualisation, this section also includes elements
    about the conversion from the recorded multispectral channels to the standard
    RGB colour space. In Section 4, we describe the behaviour of a reference completion
    algorithm on multispectral datacubes by directly extending its input (from 3D
    RGB images to 16 multispectral channels). Section 5 presents a better two-step
    method dedicated to the use of multispectral channels for image completion. A
    pre-segmentation of the geometry of the scene based on the spectral dimension
    is described in the first step. Research of substitution pixels is then geometrically
    constrained to a predefined area: only the segments located in the vicinity of
    the missing region are considered (see Fig. 1 for a representative example). Fig.
    1 Open in figure viewer PowerPoint Image completion constrained by spectral segmentation
    (a) Image recorded by the multispectral camera and converted to RGB. The red box
    in the paperboard was selected by the user and corresponds to the area to be completed
    (missing region), (b) Pixels selected by standard exemplar-based completion algorithm
    (i.e. PatchMatch [4]) to complete the missing region are highlighted in green.
    The algorithm considers (by mistake) some pixels from the curtains to complete
    the paperboard, their RGB values being very similar, (c) Resulting completion
    is visually altered and is partially grey, (d) Spectral segmentation deriving
    from noise-adjusted principal component analysis of the multispectral image. Note
    that the spectral segmentation produces regions that seem to be consistent with
    the geometry and materials of the objects, (e) The research is geographically
    limited to the segments in the neighbourhood of the region to be completed (i.e.
    the magenta segment in d), (f) Completion constrained by the spectral segments
    is more compatible with standard visual assessment in computer vision and computer
    graphics Section 6 is dedicated to the analysis of results from a perceptual quality
    assessment procedure based on standard subjective questionnaires over a panel
    of 20 observers. The proposed method (presented in Section 5) delivers completed
    images that are more compatible with standard visual assessment in computer vision
    and computer graphics. 2 Related work Image completion methods can be classified
    in three categories in the literature [2]: diffusion-based [17] and exemplar-based
    methods [4, 6, 8, 18]. Recent studies provided methods to extend single-frame
    completion to video sequences [19] or in real time [3] for diminished reality
    applications. More recently, Baek et al. [20] proposed to complete both colour
    and depth channels from multiview images. 2.1 Diffusion-based methods Diffusion-based
    techniques were developed to fill small or narrow holes by propagating adjacent
    image pixels into the missing area [17]. Smoothness priors are introduced through
    parametric models or through partial differential equations to diffuse local structures
    [21]. These techniques tend to blur and are less effective in handling large missing
    regions due to their inability to properly recover the textures. Tensor decomposition
    and completion [22] consist in propagating structures to fill missing regions
    by estimating missing values in tensors of visual data. The methods are based
    on matrix completion, which uses the matrix trace norm, but extended to the tensor
    case. Thus, tensor completion corresponds to a high-order extension of matrix
    completion and is formulated as a global convex optimisation problem [2, 22].
    2.2 Exemplar-based methods Exemplar-based methods take their origin from texture
    synthesis methods [23]. These approaches use textures in an image as exemplars
    based on the assumption that the patches in the target region are similar to those
    in the rest of the image [4, 6, 8, 18]. In a progressive manner, texture is successively
    copied to the boundary of the target region. Although this method can replicate
    complex textures in the missing region, the success of the structure propagation
    largely depends on the order of copy [18]. To tackle this issue and to produce
    more plausible results, the order of copy can be determined with particular criteria.
    For example, Criminisi et al. [18] proposed a gradient-based method that encourages
    the propagation of textures along linear structures like strong edges. As they
    progress patch per patch in a greedy fashion, the previous approaches do not ensure
    global consistency. To address this issue, Wexler et al. [24] proposed to constrain
    the missing values by solving a global optimisation problem. Based on this work
    and to reduce the computational burden, Barnes et al. have developed PatchMatch
    [4], a fast algorithm that iteratively generates textures in the target region
    by minimising an objective function based on pattern similarity between the missing
    region and the rest of the image. The relative location from where exemplar-based
    methods copy the content (a pixel or a patch) is called an offset. All possible
    offsets are generally accepted in the optimisation process. Nevertheless, constraints
    on offsets can be imposed to produce better results in terms of quality, particularly
    by using statistics of similar patches [8]. Additional constraints can be included
    to exemplar-based algorithms by, for example, guiding the reconstruction with
    a prior geometric scheme in order to propagate long edges [5] or by segmenting
    the known region into planes to properly consider the perspectives of the scene
    [6]. 2.3 Learning-based methods Hays and Efros [25] proposed to pair the image
    completion process with a database that contains a large amount of reference images.
    The missing regions are completed by copying similar patches from the database.
    The candidate patches must be seamless and semantically consistent. Pathak et
    al. [26] have used convolutional neural networks trained to generate the contents
    of a missing region by capturing the context of the image. Recent results show
    that approaches based on deep neural networks can effectively compete with most
    recent exemplar-based completion methods [27]. In this study, we have employed
    PatchMatch [4] as a reference image completion technique. The algorithm ensures
    consistency by solving a global optimisation problem and is faster than comparable
    completion techniques. 3 Multispectral data This section presents details about
    the multispectral device in addition to the image processing operations that were
    employed to analyse the multispectral data. 3.1 Camera specifications The multispectral
    imaging technology we used (Fig. 2a) in this study was designed by IMEC [11].
    The device corresponds to a snapshot (i.e. non-scanning) and ultra-compact spectrometer.
    The camera records the spectral irradiance of a scene through a multispectral
    image, i.e. a 3D data set typically called a datacube or hypercube [9]. The device
    can nominally deliver 170 datacubes per second in real time. This value is constrained
    by the exposure time in practice. Fig. 2 Open in figure viewer PowerPoint Multispectral
    camera specifications (a) Snapshot real-time multispectral camera designed by
    IMEC [11], (b) Spectral sensitivity of the 16 camera channels, which uniformly
    encompass most of the visible spectrum (475–650 nm). Spectral bandwidth is ∼20
    nm per channel. In practice, partial correlation between channels results in 14
    independent components instead of 16, (c) ColorChecker Classic (X-Rite). The colour
    chart contains 24 colour patches [28]. Their reference spectra, defined between
    380 and 730 nm, are provided by the manufacturer, (d) Image and spectra derived
    from the multispectral camera. Reference and reconstructed spectra match within
    up to 90% RMS. The slight discrepancies result from uncertainties in the spectral
    calibration procedure Practically, the camera senses 16 different spectral bands
    between 475 and 650 nm. The bandwidth of each band comprises between 15 and 20
    nm (Fig. 2b). The full resolution of the CMOS sensor is defined to 2048 × 1024
    pixels but reduced to 512 × 256 pixels for each spectral channel (each cell being
    formed by a 4 × 4 multispectral mosaic [11]). Pixel intensity (bit depth) is signed
    over 10 bits. 3.2 Pre-processing 3.2.1 Spectral reconstruction Spectral reconstruction
    corresponds to a primary procedure essentially employed to calibrate multi- or
    hyperspectral sensors in order to assess apparent reflectances from raw spectral
    channels [29]. In the present study, spectral reconstruction was performed using
    a colour chart that includes 24 different colour patches (see Fig. 2c). Given
    that all the optical parameters cannot be estimated beforehand, an indirect method
    was employed to calibrate the multispectral sensor. For the sake of completeness,
    the interested reader can refer to the original article [29] in order to get the
    full implementation details. The reconstructed reflectance of the blue, red, and
    green patches of the colour chart are illustrated in Fig. 2d. The observable discrepancies
    result from uncertainties on the calibration procedure, which closely depends
    on the spectral sensitivity responses (Fig. 2b). 3.2.2 Multispectral to RGB conversion
    As completion algorithms deliver images that are displayed on screen and visually
    evaluated by humans, a conversion to the standard RGB colour space is required.
    In practice, this conversion is achieved using apparent reflectances deriving
    from the camera calibration procedure (Section 3.2.1) and by the means of the
    CIE colour matching functions (see Fig. 3b). An example of standard RGB conversion
    is presented in Fig. 2d. Fig. 3 Open in figure viewer PowerPoint Spectral resolution
    significance. Averaged spectra along with their respective RGB values have been
    extracted from P1 and P2 patches. RGB values indicate that the colours are very
    similar. Multispectral sampling allows a more precise observation of chromatic
    differences. A reduction in spectra to three R, G, and B values leads to smooth
    and filter out spectral details, in particular when relevant variations are cancelled
    due to integration by the CIE matching functions (, , and curves) 4 Preliminary
    analyses 4.1 Significance of the spectral sampling Image completion is based on
    colour and brightness analysis of different image patches. Fig. 3a presents a
    typical example, where P1 and P2 correspond to patches of similar RGB colour.
    Working with more spectral bands (by increasing the spectral sampling) can be
    helpful in order to reveal additional relevant information. Fig. 3b presents the
    spectrum along with the corresponding RGB values of both the P1 and P2 patches.
    Herein, important chromatic differences appear between 590 and 730 nm. These disparities
    are partially cancelled due to integration by the CIE colour matching functions
    (, , and on Fig. 3b). Employing more spectral bands seems relevant in order to
    better consider chromatic variations when performing image completion. 4.2 Experimental
    procedure A set composed of ten different multispectral images was employed to
    assess the relevance of the multispectral data applied to image completion. The
    frames were recorded with the multispectral device presented in Section 3.1, the
    scenes being selected to emphasise current image completion limits. To this purpose,
    objects and backgrounds of similar colours were employed (Fig. 4a). Fig. 4 Open
    in figure viewer PowerPoint Experimental set-up (a) Typical image acquired with
    the multispectral device and converted to RGB. The red region, which is selected
    by the user, denotes the area to be completed (missing region), (b) Ground truth
    material mask (). The white region has been manually segmented and corresponds
    to the best zone of research (in terms of material) for completion candidates
    Each area to be completed was manually chosen and presents no clear gradients
    and little spatial structure variance. For validation purpose, the regions were
    defined to avoid entire overlapping of an object and are comprised on a single
    material. To evaluate the behaviour of the completion procedure, ground truth
    material masks were manually defined (Fig. 4b). They correspond to the region
    defined by the same material than the one which surrounds the area to be completed.
    These material masks are also used to evaluate the relevance of the spectral segmentation
    proposed in this study (see Section 5.2.2). We propose to assess the behaviour
    of standard completion algorithm (Section 4.3) in regard to the materials that
    surround the region to be completed (Section 4.4), in particular when increasing
    the number of multispectral channels. We also propose to empirically evaluate
    the quality of the completion by comparing the synthesised area with its original
    content (Section 4.5). 4.3 Implementation details PatchMatch [4], which was initially
    proposed by Barnes et al., is used as a reference image completion technique.
    The algorithm ensures consistency by solving a global optimisation problem and
    is faster than comparable completion techniques. The method is composed of a sequence
    of specific steps. The interested reader can refer to the original article [4]
    in order to get the full implementation details. Briefly, the method is defined
    over three main steps: (i) initialisation: a random patch offset is given to each
    pixel at the coarsest pyramid level of the image. The result is propagated to
    the next pyramid level where a propagation and random search steps are applied
    at each level; (ii) propagation: the pertinence of the offsets is evaluated with
    respect to the neighbouring patches at each iteration using an objective function;
    (iii) random search: a search step is employed to look for better patch within
    a concentric radius around the current offset. The new offset is adopted if the
    new objective function is lower. A particular implementation of the initialisation
    step was employed in this study. A first exhaustive search of the best matching
    offsets is performed [30] instead of a random one. Also, the patch size has been
    set to 13 × 13 pixels. Owing to the random process included in PatchMatch, 50
    trials per image were launched to compute statistical tendencies, a single run
    being non-representative. 4.4 Materials consideration In this section, we propose
    to assess the behaviour of the completion algorithm in regard to the materials
    that surround the region to be completed, in particular when increasing the number
    of multispectral channels. The full image I is separated into two disjoint sets:
    T corresponds to the target (or missing) region, completed using pixels in S (source
    region). , , and . The image completion algorithm replaces all pixels included
    in T. The offsets represent the difference of position between a pixel in the
    area to be completed (target region) and its corresponding candidate in the source
    region. Offsets are defined with a mapping function f that maps each target position
    to a source position (see Figs. 1b and e for typical examples): (1) f corresponds
    to a transformation that solves a global minimisation problem and is determined
    for each target pixel. The synthesised image is then created by replacing all
    target pixels with their corresponding source pixels. It is important to note
    that only the offsets, i.e. the difference of position between a pixel included
    in the area to be completed and its corresponding candidate in the rest of the
    image, are susceptible to fluctuate. The synthesising procedure (pixel copy) is
    ultimately performed on RGB frames using the defined offsets. To understand if
    the completion algorithm is able to correctly use pixels from surrounding materials,
    the percentage of good match [ in (2)] between the offsets and the ground truth
    material mask was assessed for each of the ten input images. It corresponds to
    the number of times the completion algorithm uses a pixel from the ground truth
    material region over the total number of pixels in the target area: (2) (3) where
    corresponds to the ground truth material region (Fig. 4b). is defined for each
    target pixel (p). N corresponds to the total number of pixels from the target
    region and to the match rate (units: %). Results are presented in Figs. 5a and
    b using boxplot representations. Each boxplot includes 500 computed match rates
    (10 images recorded by the multispectral camera × 50 completion trials per image).
    For comparison purposes, the match rates computed using RGB images were reported
    on these figures (red boxes). Fig. 5 Open in figure viewer PowerPoint Respect
    of the surrounding materials by the completion algorithm. The match rates are
    computed between offsets and the ground truth material mask for each image. The
    presented results integrate all the 500 trials. For comparison purposes, match
    rates computed using RGB images are indicated on each figure (red boxplot) (a)
    Match rates computed on raw multispectral channels, starting from single (monochromatic)
    channel to all the 16 channels, (b) Match rates computed on noise-adjusted principal
    components Fig. 5a presents the match rates computed when completion is performed
    on raw multispectral channels. Starting from all the 16 channels, we progressively
    averaged the spectral image two channels by two channels until reaching a single
    channel (monochromatic image). Fig. 5b presents the same percentage of good match,
    but when performing completion on principal components. The latter were computed
    from a noise-adjusted principal component analysis, a transformation developed
    to sort principal components by image quality (decreasing image quality with increasing
    component number). We have employed minimum/maximum autocorrelation factors to
    estimate the noise covariance matrix. The method has been proposed by Green et
    al. [31] and uses between-neighbour differences to estimate the noise covariance.
    Results presented in Fig. 5a exhibit an increase in the match rates that are correlated
    with the augmentation of the number of channels. Also, the boxplots length indicates
    that the variance tends to simultaneously decrease. Adding a more precise spectral
    information to the completion algorithm leads to better considerate the physical
    properties of materials. Subtle variations that were not necessarily observable
    in the standard RGB colour space are considered (see Section 4.1). Image completion
    based on principal components (Fig. 5b) tends to better consider the surrounding
    materials, the maximum median value being equal to 99% (instead of maximally 80%
    when considering raw multispectral channels). In addition, only four components
    are necessary to achieve this score. The last principal components containing
    more and more noise, the induced artefacts generate a bias that leads the completion
    to pick patches in a random fashion, thus reducing the mean percentage of good
    match while increasing the variance. 4.5 Rendering analysis In this section, we
    propose to empirically assess the quality of the completion by comparing the synthesised
    area with its original content using an error function. The latter corresponds
    to the Euclidean distance based on the R, G, and B channels and is computed for
    each pixel of the target region. Fig. 6a presents the Euclidean errors computed
    using offsets that where determined on raw multispectral channels. As before (see
    Section 4.4) and starting from all the 16 channels, we progressively averaged
    the spectral image two channels by two channels until reaching a single channel
    (monochromatic image). Fig. 6b presents the same information, but when performing
    completion on principal components. A close-up view is displayed on the top of
    the figure to identify the error minimum. Errors computed when completion is performed
    on standard RGB images are, respectively, reported on the two figures using red
    boxes. Fig. 6 Open in figure viewer PowerPoint Rendering analysis assessed using
    Euclidean errors computed between synthesised and original RGB images. The results
    are averaged over the 500 trials. For comparison purposes, the errors computed
    when completion used offset defined on RGB frames are indicated on each figure
    using a red boxplot (a) Errors computed using offsets determined on raw multispectral
    channels, (b) Errors computed using offsets determined on principal components
    Completion based on four multispectral channels (Fig. 6a) presents the general
    minimum error. From Fig. 6b, completion based on the first two principal components
    presents the minimum error. Employing more principal components gives worse completion
    results. This effect is inherent to the noise-adjusted principal component transform:
    the last components containing more and more noise, the induced artefacts generate
    a bias that leads the completion to pick patches in a random fashion. In addition
    to these statistical tendencies, illustrative completion results are presented
    in Fig. 7 to visually compare renderings. From these results, we can conclude
    that completion based on four multispectral channels produces plausible results
    when the materials are respected (i.e. when only the pixels included in the region
    defined by the material that surrounds the missing region are used for completion.
    See Fig. 7, images # 1 and 6, for a typical example). In comparison, completion
    based on the first four principal components produces less consistent results.
    The chromaticity (colours) is respected, but the intensity (brightness) seems
    inconsistently distributed. In contrast, completion based on four multispectral
    channels tends to produce chromatic inconsistencies when the materials are not
    respected (see results of images # 8 and 9 on Fig. 7). This time, completion based
    on the first four principal components delivers more plausible results, even if
    brightness discrepancies can still be noted. Fig. 7 Open in figure viewer PowerPoint
    Spectral completion (a) Source image with (b) its corresponding close-up view.
    The red pattern corresponds to the area to be completed, (c) Ground truth (close-up),
    (d) Pixels selected using four multispectral channels to complete the missing
    region are highlighted in green, (e) Completion results (close-up) based on the
    selected pixels from (d), (f) Pixels selected using the first four principal components
    to complete the missing region are highlighted in green, (g) Completion results
    (close-up) based on the selected pixels from (f) 5 Image completion constrained
    by spectral segmentation 5.1 Motivation Two important points emerged from the
    preliminary analyses results (Sections 4.4 and 4.5): Noise-adjusted principal
    components, computed from the full spectral data, tend to better consider the
    materials of the scene: this particular representation constitutes a good way
    to separate pixels from different material classes and therefore ensure the stability
    of the completion in terms of materials. The first four principal components gives
    a maximum match rate (Fig. 5b). Completion based on multispectral channels produces
    more plausible results than completion based on principal components when the
    materials are respected (Fig. 7). Completion based on four multispectral channels
    presents the minimum error (Fig. 6a). Thereby, we can conclude that principal
    components must be considered in order to fill the missing region with pixels
    included in the same material area. In addition, the completion must be based
    on the raw multispectral channels to ensure a consistent and plausible rendering.
    The method we propose in this section is based on these two observations: completion
    is performed on four raw spectral channels but limited to a predefined and coherent
    area. The latter is estimated through spectral segmentation based on the first
    four principal components. 5.2 Spectral segmentation 5.2.1 Method The full pipeline
    is presented graphically in Fig. 8. The input of the method corresponds to the
    raw multispectral image, where each pixel is defined by its 16 points spectral
    signature (Fig. 8a). Noise-adjusted principal component transform [31] is computed
    (Fig. 8b) to reduce the input dimensionality and, based on the results presented
    in Section 4.4, only the first four components are retained for further processing
    (Fig. 8c). Fig. 8 Open in figure viewer PowerPoint Spectral segmentation procedure
    (a) Raw multispectral image defined over 16 spectral channels, (b) Noise-adjusted
    principal component analysis [31]. Eigenvectors correspond to images with decreasing
    eigenvalues, (c) Thresholding operation is applied to recover the first four principal
    components, which are more adapted to the context (see Section 4.4), (d) Projection
    of the pixel values in the spectral space defined by the first four principal
    components (only the first three dimensions are represented). In this spectral
    space, clusters (see dashed-line ellipses) match with the geometry and materials
    of the scene, (e) Resulting segmentation, based on hierarchical data clustering
    (number of clusters: 8) The spectral segmentation is based on agglomerative hierarchical
    clustering, which consists in grouping data by creating a cluster tree (dendrogram).
    The similarity between every pair of pixels is firstly evaluated by computing
    Euclidean distances. Note that each pixel is defined by four different coordinates,
    one coordinate by principal component. The distance information is used to link
    pairs of pixels that are close together into binary clusters. Each binary cluster
    is made up of two pixels. The newly formed clusters are then linked once again
    to create bigger clusters using the Ward''s method (minimum increase in sum-of-squares)
    [32]. This step is repeated until all the pixels in the original data set are
    linked together, thus forming a hierarchical tree. The tree may inherently separate
    the data into distinct clusters, in particular for dendrograms created from groups
    of densely packed pixels. These groups may correspond to pixels of similar materials.
    For example, if we only consider the second principal component (presented in
    Fig. 8c), the hierarchical tree will contain four large and separate clusters:
    the paperboard (dark pixels), the table (dark-grey pixels), the radiator (white
    pixels), and the background (light-grey pixels). The hierarchical cluster tree
    is pruned to partition the data set into separated clusters. Usually, the number
    of clusters must be carefully selected to avoid over- and under-segmentation.
    Under-segmentation is not permitted: pixels that belong to different materials
    will be grouped in a single segment, thus resulting in a probable inaccurate completion.
    The segments located in the vicinity of the region to be completed being merged
    (Fig. 9), over-segmentation is tolerated. To properly perform completion, the
    fused segments of interest (Fig. 9c) must include an acceptable amount of pixels.
    Fig. 9 Open in figure viewer PowerPoint Segments merging and post-processing treatments
    (a) Spectral segmentation based on agglomerative hierarchical data clustering
    (number of clusters: 20). The black rectangle indicates the area to be completed,
    (b) The segments located in the vicinity of the region to be completed are merged
    to form a single, binary mask, (c) Post-processing treatments are applied to remove
    small group of pixels and fill small holes, (d) Ground truth material mask (manually
    segmented) Practically, the function clusterdata included in Matlab (The MathWorks
    Inc.) was employed. Euclidean distance and Ward''s method were used to, respectively,
    compute every distance and create the hierarchical tree. As presented in Fig.
    9, post-processing treatments were developed to remove artefacts. In particular,
    morphological operations were employed to remove small isolated groups of pixels
    (surface area 200 pixels) and fill small holes (morphological closing using a
    disk-shaped structural element of radius 3 pixels). 5.2.2 Evaluation The spectral
    segmentation is evaluated through Jaccard''s distance and precision and recall
    indexes [33]. The metrics were computed between the spectral segmentation, given
    by its binary mask (Fig. 9c), and the ground truth material mask (Fig. 9d) for
    each scene. The results are presented in Table 1. Generally, the average values
    for the precision and recall indexes are >85%. Table 1. Evaluation of the spectral
    segmentation proposed in this study (see Section 5.2). Precision and recall indexes,
    as well as Jaccard''s distance, were computed using ground truth material masks
    (Fig. 4) Scene # 1 2 3 4 5 6 7 8 9 10 All scenes mean All scenes standard deviation
    Precision, % 92 88 90 87 91 89 96 88 58 72 85 11 Recall, % 100 100 96 97 36 91
    98 63 89 95 86 21 Jaccard''s distance, % 92 88 87 85 35 82 94 58 54 70 74 20 5.3
    Constrained multispectral completion The binary mask delivered by the spectral
    segmentation procedure (Fig. 9c) is employed to constrain completion in a predefined
    region. Technically, we deactivate the research process on source pixels located
    outside the segmentation area. Note that completion is constrained by spectral
    segmentation, which is based on the first four principal components (Section 4.4),
    but ultimately performed by analysing pixel values on four multispectral channels
    (Section 4.5). Some excerpts are illustrated in Fig. 10. All the synthesised images
    were compared against baseline (standard RGB completion) through subjective quality
    assessment metrics. Fig. 10 Open in figure viewer PowerPoint Comparisons with
    representative baseline algorithm [4] (a) Source image, (b) Ground truth material
    mask, (c) Corresponding close-up view. In (a) and (c), the red pattern indicates
    the area to be completed, (d) Ground truth (close-up), (e) Offsets computed using
    baseline completion method (green pixels). They correspond to the pixels used
    to complete the missing region, (f) Completion results based on the selected pixels
    from (e), (g) Spectral segmentation mask. Research of substitution pixels is geometrically
    constrained to the white area, (h) Offsets computed using the method proposed
    in this study (green pixels), (i) Completion results based on the selected pixels
    from (h) 6 Perceptual quality assessment 6.1 Introduction Most of computer graphics
    rendering methods require perceptually plausible results: simple pixel intensity
    error computed between synthesised and original images (see Section 4.5) does
    not necessarily reflect and guarantee perceived image quality [34]. Image quality
    assessment consists in providing a metric that expresses overall quality by rating
    and ranking methods. Image and video quality assessment has been particularly
    employed in video compression and transmission applications [35]. To assess visual
    quality as perceived by observers, ratings and preferences are recorded through
    subjective questionnaires. These two metrics have been widely used in experimental
    sciences to assess relative judgements from human participants [34]. Decision
    times, which are related to the degree of difficulty encountered by the observers
    to perform the tasks, were also recorded. Quality is assessed for each of the
    ten scenes recorded by the multispectral camera (see Section 4.2). The experiment
    was conducted by 20 different observers (17 males and 3 females, 25–37 years).
    Three images are employed for each scene to perform quality assessment: (i) image
    completed by standard RGB method; (ii) image completed by the technique proposed
    in this study (spectral completion); and (iii) reference (unmodified) image. 6.2
    Assessment methods All observers received a prior explanation before the beginning
    of the session. The experiment started with a short training session, in which
    observers could manipulate the interface and perform training tasks. To avoid
    effects caused by side variables, all test sets were presented to each observer
    in a random fashion. Also and to avoid fatigue, no session took longer than 20
    min. A typical session lasts ∼15 min. Three methods were employed to assess image
    quality: (i) single and (ii) double stimulus represent continuous rating while
    (iii) pairwise similarity judgement method is employed to evaluate relative preference
    between two images [35]. Single stimulus: observers judge the quality on a continuous
    five-point Likert scale [36]. Each image is displayed for only 4 s. After that
    short period, a voting interface is displayed on screen. Five categories are indicated
    right over the continuous scale: bad, poor, fair, good, and excellent (Fig. 11a).
    Reference images are included into the set. Thus, observers must evaluate a set
    composed of 30 randomly arranged images, which includes 10 reference images and
    20 completed images (10 by standard RGB completion and 10 by the spectral completion
    proposed in this study). Fig. 11 Open in figure viewer PowerPoint Overview of
    the three subjective methods employed in this study to assess image quality (a)
    Single stimulus. Observers have to rate the quality of the displayed image, (b)
    Double stimulus. Observers must rate the quality of the first and the second image,
    (c) Similarity judgements. Observers have to express their preference by evaluating
    the quality differences between the two displayed images Double stimulus: is similar
    to the single stimulus method, except that a reference and a completed image are
    successively displayed in random order one after another, each one for 4 s (Fig.
    11b). Observers are asked to independently evaluate the quality of the first and
    the second image. Herein, observers rate 40 images (20 related to RGB completion
    and 20 related to spectral completion). Pairwise similarity judgement: observers
    are asked to mark their preference by indicating how large the difference in quality
    is between images synthesised by each of the two completion methods (Fig. 11c).
    A continuous seven-point scale has been employed. Observers can select the central
    position if no differences were identified between the pair of images. 6.3 Results
    and analysis Rating methods: It has been shown that direct rating results correspond
    to very unreliable estimates [35]. Thus, we choose to present only differential
    scores in this section. The latter were computed between pairs of images, in particular
    between reference and completed images and by means of difference mean opinion
    scores (4): (4) (5) corresponds to the rating for a given (reference or completed)
    image. Indexes correspond to the ith observer, jth completion method (standard
    or spectral), and kth scene. ref(k) corresponds to the reference for scene k.
    z-scores (5) are computed to adjust scale variations between observers in order
    to properly compare results. To unify scales, a common way consists in normalising
    opinion scores by removing the mean [ in (5)] and unifying standard deviation
    across observers [ in (5)]. Results for both single and double stimulus experiments
    are presented in Fig. 12. Generally and for both experiments, observers showed
    preference for images completed by the method proposed in this study: the z-scores
    are significantly higher than those computed from images completed by the baseline
    method. In addition, we can notice that observers gave similar opinions for scenes
    # 4 and 5, indicating that both completion methods performed identically, and
    particularly well, on these two scenes. Fig. 12 Open in figure viewer PowerPoint
    Ratings for each scene. Figures exhibit z-scores from single stimulus (first row)
    and double stimulus (second row). On each figure, the left boxplot has been formed
    using ratings from images completed by the proposed method (completion constrained
    by spectral segmentation, see Section 5). The blue central boxplot corresponds
    to ratings from reference (unmodified) images and the red boxplot on the right
    to ratings from images completed by baseline method (standard RGB completion).
    Each boxplot integrates ratings results over all observers, the central mark corresponding
    to the median, the edges of the box to the 25th and 75th percentiles and the whiskers
    to the most extreme data points (not considered outliers). Outliers are plotted
    individually using red crosses Pairwise similarity judgement: In order to be compared
    and because each observer could employ a different range of voting values, quality
    judgements are normalised per observer: each vote has been divided by the observer
    global standard deviation. The results are similar to z-scores, except that the
    mean value () is not removed. Similarity judgements include preferences, the sign
    indicating which image was judged better. Results are presented in Fig. 13a. Positive
    values indicate that images completed by the method we propose in this study (spectral
    completion) are preferred to images completed by the baseline method. Results
    produced by spectral completion were preferred in 186 over a total of 200 votes,
    which correspond to 93% of the total number of votes. Seven votes over 200 (3.5%
    of the total number) indicate no difference in quality between the two completion
    methods (values located on the zero axis in Fig. 13a). Finally, seven votes over
    200 (3.5%) were in favour of images completed by the baseline method (negative
    values in Fig. 13a). Fig. 13 Open in figure viewer PowerPoint Pairwise judgments
    and time needed to complete the experiment (a) Pairwise judgements for each scene.
    Results are presented in normalised units: each vote has been divided by the observer
    global standard deviation. Positive scores indicate that images completed by the
    method we propose in this study (spectral completion) are preferred to images
    completed by the baseline method (standard RGB completion), (b) Time needed to
    complete the pairwise comparison experiment From these results, we can conclude
    that the quality of images produced by the spectral completion was preferred in
    most cases. Except for scenes # 4 and 5, all judgements opt in favour of the method
    we propose in this study. In accordance with results from single and double stimulus
    experiments (Fig. 12), both completion methods performed well for these two particular
    scenes, their ratings being close to the reference image. Thus, it appears that
    observers were less able to distinguish differences in quality between the images
    produced by the two completion methods. This observation seems to be consistent
    with the time took by the observers to complete the experiment (Fig. 13b), which
    is significantly higher for scenes # 4 and 5 than for the other scenes. 7 Discussion
    Employing snapshot multispectral cameras instead of hyperspectral ones ensures
    a real-time exploitation of the method, which corresponds to a necessary prerequisite
    for many practical applications. In contrast, the direct integration of the spectrum,
    signed over 16 different values, imposes a drastic extension of computational
    times. This limitation was considered by integrating dimensionality reduction
    transforms to the method: based on preliminary analyses (Sections 4.4 and 4.5),
    the first four principal components were used to segment the scene while four
    spectral channels were employed to perform completion by determining which pixels
    must be copied into the missing region. Incorporating recent completion techniques
    (e.g. constraining the completion process with guidance maps [7] or using statistics
    of similar patches [8]) to the method proposed in this study could be relevant
    and of interest but is out of the scope of this paper. Indeed, the main objective
    of this study consists in comparing completion based on multispectral images against
    completion based on RGB images. Adding supplementary constraints, like prior information
    about structures or guidance maps, may tend to denaturate this comparison. 7.1
    Limitations Improvement of the database: The multispectral database currently
    includes ten multispectral indoor scenes. The latter were selected to emphasise
    current image completion limits. To this purpose, objects and backgrounds slightly
    textured and of similar colour were employed. Owing to the random process included
    in PatchMatch, 50 trials per image were launched to compute statistics. Despite
    the low number of images included in the database, we believe that the tendencies
    presented in Sections 4.4 and 4.5 are adequately representative. Spatial-spectral
    clustering: The spectral segmentation developed in this study (Section 5) is based
    on research of clusters in the spectral space. They tend to respect the geometry
    of the objects but no explicit information, like material-invariant features such
    as shape or texture for example, is currently incorporated into the method. 7.2
    Future works In regard to the limitations exposed beforehand, the first milestone
    will consist in expanding the database by including varied indoor and natural
    scenes. Developments will be conducted to improve spectral segmentation by coupling
    spatial and spectral dimensions using Schrödinger Eigenmaps [37], a recent technique
    that extends Laplacian Eigenmaps in order to fuse spatial and spectral information
    through non-diagonal potentials. Also, deep learning [38] and support tensor machine
    [39] correspond to promising avenues that need to be examined. 8 Conclusion We
    have proposed, in this study, to assess potential of multispectral imaging applied
    to image completion. Regions to be completed were chosen to present no clear gradients
    and slight textures. This lack of variance in the spatial structure coupled to
    the presence of objects of similar colour within the image leads to repeated RGB
    completion failures. Herein, the contribution of the spectral information is of
    interest and allows better discrimination and, therefore, an increasing rate of
    successful completion. Preliminary results indicate that direct exploitation of
    completion algorithms by extension of the spectral channels shows only minimum
    enhancement. Based on these observations, we proposed a two-step method dedicated
    to the use of multispectral channels for image completion. A pre-segmentation
    of the scene has been developed to geometrically constrained the research of substitution
    pixels to a predefined area. Only the segments located in the vicinity of the
    missing region are considered. Results indicate that image completion constrained
    by spectral segmentation improves rendering consistency and simultaneously ensures
    better stability in terms of materials. Results were validated using numerical
    criteria and perceptual assessment experiments. The proposed method delivers completed
    images that are more compatible with standard visual assessment in computer vision
    and computer graphics. Snapshot multispectral devices correspond to breakthrough
    technologies that can be employed to improve computer vision methods by accurately
    sensing the physical properties of a scene. This study shows for the first time
    the potential of snapshot multispectral imaging applied to computer vision and
    particularly to image completion. 9 References Citing Literature Volume12, Issue7
    July 2018 Pages 1164-1174 Citation Statements beta Supporting 0 Mentioning 3 Contrasting
    0 Explore this article''s citation statements on scite.ai powered by   Figures
    References Related Information Recommended Single image dehazing using local linear
    fusion Yakun Gao,  Haiyan Chen,  Haibin Li,  Wenming Zhang IET Image Processing
    Hyperspectral image super‐resolution under misaligned hybrid camera system Yonggang
    Lin,  Yongrong Zheng,  Ying Fu,  Hua Huang IET Image Processing Text segmentation
    using superpixel clustering Yuanping Zhu,  Kuang Zhang IET Image Processing Image
    seamless stitching and straightening based on the image block Zhong Qu,  Tengfeng
    Wang,  Shiquan An,  Ling Liu IET Image Processing Realistic endoscopic image generation
    method using virtual‐to‐real image‐domain translation Masahiro Oda,  Kiyohito
    Tanaka,  Hirotsugu Takabatake,  Masaki Mori,  Hiroshi Natori,  Kensaku Mori Healthcare
    Technology Letters Download PDF ABOUT THE IET IET PRIVACY STATEMENT CONTACT IET
    Copyright (2024) The Institution of Engineering and Technology. The Institution
    of Engineering and Technology is registered as a Charity in England & Wales (no
    211014) and Scotland (no SC038698) Additional links ABOUT WILEY ONLINE LIBRARY
    Privacy Policy Terms of Use About Cookies Manage Cookies Accessibility Wiley Research
    DE&I Statement and Publishing Policies HELP & SUPPORT Contact Us Training and
    Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies.'
  inline_citation: (Bousefsaf, Tamaazousti, Hadj Said, & Michel, 2018)
  journal: IET image processing (Print)
  key_findings: '1. Direct exploitation of completion algorithms by extension of the
    spectral channels exhibits only minimum enhancement.

    2. A dedicated method that consists in a prior segmentation of the scene has been
    developed to address this issue.

    3. The segmentation derives from an analysis of the spectral data and is employed
    to constrain research area of exemplar-based completion algorithms.

    4. Image completion constrained by spectral presegmentation ensures better consideration
    of the surrounding materials and simultaneously improves rendering consistency.'
  limitations: null
  main_objective: To assess the potential of multispectral imaging applied to image
    completion, particularly in cases where the missing region lacks clear gradients
    and significant texture variance.
  pdf_link: null
  publication_year: 2018
  relevance_evaluation: This paper is highly relevant to the point focus, as it directly
    addresses the integration of multispectral cameras (e.g., high-resolution cameras
    using multispectral, hyperspectral imaging) with computer vision algorithms for
    automated irrigation systems. The study focuses specifically on the use of multispectral
    imaging for visual monitoring of crop growth, disease detection, and irrigation
    system performance, which are key components of automated irrigation management.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Multispectral imaging, computer vision algorithms, image completion
  title: Image completion using multispectral imaging
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agriculture13051005
  analysis: '>'
  apa_citation: null
  authors:
  - Luis Emmi
  - Roemi Fernández
  - P. González de Santos
  - Matteo Francia
  - Matteo Golfarelli
  - Giuliano Vitali
  - Hendrik Sandmann
  - Michael Hustedt
  - Merve Meinhardt‐Wollweber
  citation_count: 7
  data_sources:
  - Survey data
  - Interviews
  explanation: 'The proposed system harnesses advances in robotics and computing to
    enhance the efficiency and accuracy of agricultural practices. It seamlessly integrates
    high-resolution cameras with an advanced vision algorithm, providing real-time
    monitoring of crop growth and disease detection. Furthermore, it leverages deep
    learning-based object detection and segmentation to differentiate between weeds
    and crops, enabling targeted treatment and resource optimization.


    The system''s architecture is built upon Robot Operating System (ROS), a widely
    used platform for robotics applications, and FIWARE, an open-source framework
    for Internet of Things (IoT) and cloud computing. This integration facilitates
    communication between the robot''s components and the cloud, enabling data exchange,
    analysis, and decision-making processes.


    To validate the system''s performance, experiments were conducted using a laser-based
    weeding robot. The robot autonomously navigated through crop fields, capturing
    images and collecting sensor data. The data was then transmitted to the cloud,
    where it was processed and analyzed to identify weed locations. The robot''s laser
    system was subsequently activated to precisely target and eliminate the weeds.


    The experimental results demonstrated the system''s effectiveness and robustness.
    The crop detection algorithm achieved an accuracy of ±0.02 m, ensuring precise
    guidance and treatment. Data transmission to the cloud was accomplished at a rate
    of 4 frames per second, with minimal message loss and latency. The system''s overall
    performance was comparable to conventional communication methods, highlighting
    the advantages of using ROS and FIWARE without compromising efficiency.'
  full_citation: '>'
  full_text: ">\nCitation: Emmi, L.; Fernández, R.;\nGonzalez-de-Santos, P.; Francia,\
    \ M.;\nGolfarelli, M.; Vitali, G.; Sandmann,\nH.; Hustedt, M.; Wollweber, M.\n\
    Exploiting the Internet Resources for\nAutonomous Robots in Agriculture.\nAgriculture\
    \ 2023, 13, 1005. https://\ndoi.org/10.3390/agriculture13051005\nAcademic Editors:\
    \ Jin Yuan, Wei Ji,\nQingchun Feng and Massimo\nCecchini\nReceived: 16 March 2023\n\
    Revised: 17 April 2023\nAccepted: 29 April 2023\nPublished: 2 May 2023\nCopyright:\n\
    © 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an\
    \ open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of the\
    \ Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nagriculture\nArticle\nExploiting the Internet Resources for Autonomous\
    \ Robots\nin Agriculture\nLuis Emmi 1,*\n, Roemi Fernández 1\n, Pablo Gonzalez-de-Santos\
    \ 1\n, Matteo Francia 2\n, Matteo Golfarelli 2,\nGiuliano Vitali 3\n, Hendrik\
    \ Sandmann 4, Michael Hustedt 4 and Merve Wollweber 4\n1\nCentre for Automation\
    \ and Robotics (UPM-CSIC), 28500 Arganda del Rey, Madrid, Spain\n2\nDepartment\
    \ of Computer Science and Engineering (DISI), Alma Mater Studiorum-University\
    \ of Bologna,\n40127 Bologna, Italy\n3\nDepartment of Agricultural and Food Sciences\
    \ (DISTAL), Alma Mater Studiorum-University of Bologna,\n40127 Bologna, Italy\n\
    4\nLaser Zentrum Hannover e.V., Hollerithallee 8, 30419 Hannover, Germany\n*\n\
    Correspondence: luis.emmi@car.upm-csic.es\nAbstract: Autonomous robots in the\
    \ agri-food sector are increasing yearly, promoting the application\nof precision\
    \ agriculture techniques. The same applies to online services and techniques implemented\n\
    over the Internet, such as the Internet of Things (IoT) and cloud computing, which\
    \ make big data, edge\ncomputing, and digital twins technologies possible. Developers\
    \ of autonomous vehicles understand\nthat autonomous robots for agriculture must\
    \ take advantage of these techniques on the Internet to\nstrengthen their usability.\
    \ This integration can be achieved using different strategies, but existing\n\
    tools can facilitate integration by providing beneﬁts for developers and users.\
    \ This study presents an\narchitecture to integrate the different components of\
    \ an autonomous robot that provides access to\nthe cloud, taking advantage of\
    \ the services provided regarding data storage, scalability, accessibility,\n\
    data sharing, and data analytics. In addition, the study reveals the advantages\
    \ of integrating new\ntechnologies into autonomous robots that can bring signiﬁcant\
    \ beneﬁts to farmers. The architecture is\nbased on the Robot Operating System\
    \ (ROS), a collection of software applications for communication\namong subsystems,\
    \ and FIWARE (Future Internet WARE), a framework of open-source components\nthat\
    \ accelerates the development of intelligent solutions. To validate and assess\
    \ the proposed\narchitecture, this study focuses on a speciﬁc example of an innovative\
    \ weeding application with\nlaser technology in agriculture. The robot controller\
    \ is distributed into the robot hardware, which\nprovides real-time functions,\
    \ and the cloud, which provides access to online resources. Analyzing the\nresulting\
    \ characteristics, such as transfer speed, latency, response and processing time,\
    \ and response\nstatus based on requests, enabled positive assessment of the use\
    \ of ROS and FIWARE for integrating\nautonomous robots and the Internet.\nKeywords:\
    \ precision agriculture; autonomous robots; artiﬁcial intelligence; IoT; cloud\
    \ computing\n1. Introduction\nThe year 2022 ended with more than 8 billion inhabitants\
    \ of the world. Most govern-\nments understand that feeding this vast and growing\
    \ population is one of the signiﬁcant\nchallenges they must face in the coming\
    \ years. Some associations have predicted that\nfood production will need to increase\
    \ by 70% to feed the entire population in 2050 [1].\nIn developed countries, cultivated\
    \ land is close to its maximum output; therefore, the\nsolution is oriented toward\
    \ optimizing the available resources. Many different cultural and\ntechnological\
    \ methods for increasing crop yield are being used. Some improve crop yields,\n\
    but at the extra cost of increasing environmental pollution and the carbon footprint.\
    \ These\nside effects are unacceptable in many industrialized nations, such as\
    \ those in the European\nUnion, which is committed to using sustainable methods.\n\
    Agriculture 2023, 13, 1005. https://doi.org/10.3390/agriculture13051005\nhttps://www.mdpi.com/journal/agriculture\n\
    Agriculture 2023, 13, 1005\n2 of 22\nPrecision agriculture leverages technologies\
    \ to achieve those objectives and avoids\nundesired effects. PA is a concept for\
    \ farm management founded on observation, mea-\nsurement, and response to crop\
    \ variability [2]. It assembles different methods to manage\nvariations in a farm\
    \ to enhance crop yield, improve commercial proﬁt, and guarantee eco-\nenvironmental\
    \ sustainability. PA uses current information and communication technologies\n\
    (ICT), automation, and robotics to monitor crop growth, predict the weather accurately,\n\
    perform optimal irrigation, apply fertilizers smartly, manage weeds and pests\
    \ accurately,\ntest soil quality precisely, etc.\nSince the late 1980s, precision\
    \ agriculture techniques have been introduced step by\nstep in the agricultural\
    \ production sector, integrating the following:\n•\nSensors to acquire geolocated\
    \ biodata of crops and soil, e.g., nitrogen sensors, vision\ncameras, global navigation\
    \ satellite systems (GNSS), etc.\n•\nComputers for analyzing those data and running\
    \ simple algorithms to help farmers\nmake simple decisions (applying or not applying\
    \ a given process, modifying a process\napplication map, etc.).\n•\nActuators\
    \ in charge of executing the decisions (opening/closing valves, altering a\ntrajectory,\
    \ etc.) for modifying crops. As an actuator, we consider the agricultural tool,\n\
    also called the agricultural implement, and the vehicle, manually or automatically\n\
    driven, to move the tool throughout the working ﬁeld and apply the farming process.\n\
    The integration of subsystems onboard robotic vehicles started in the late 1990s.\
    \ Some\nillustrative examples, based on retroﬁtting conventional vehicles, are\
    \ the autonomous\nagricultural sprayer [3], which focuses on achieving a pesticide\
    \ spraying system that\nis cheap, safe, and friendly to the environment, and the\
    \ autonomous orchard vehicles\nfor mowing, tree pruning, and training, spraying,\
    \ blossoming, and fruit thinning, fruit\nharvesting, and sensing [4], both deployed\
    \ in the USA. In Europe, we can ﬁnd the RHEA\nﬂeet (see Figure 1a), consisting\
    \ of a ﬂeet of three tractors that cooperate and collaborate in\nthe application\
    \ of pesticides [5]. Regarding robotic systems based on speciﬁc structures\ndesigned\
    \ for agriculture (see Figure 1b), we can remark on LadyBird in Australia, intended\n\
    for the valuation of crops using thermal and infrared detecting systems, hyperspectral\n\
    cameras, stereovision cameras, LIDAR, and GPS [6], and Vibro Crop Robotti in Europe,\
    \ built\nfor accurate seeding and mechanical row crop cleaning [7]. These robots\
    \ were integrated\naround computing systems based on centralized or elementary\
    \ distributed architectures to\nhandle a few sensors and control unsophisticated\
    \ agricultural tools.\nIn addition to those developments, related technologies\
    \ have evolved drastically\nin recent years, and now sensors can be spread throughout\
    \ the ﬁeld and communicate\nwith each other. This is possible because of the Internet\
    \ of Things (IoT). This computing\nconcept describes how to cluster and interconnect\
    \ objects and devices through the Internet,\nwhere all are visible and can interact\
    \ with each other. IoT deﬁnes physical objects with\ndevices (mainly sensors)\
    \ and includes processing power, software applications, and other\ntechnologies\
    \ to exchange data with other objects through the Internet.\nMoreover, computers\
    \ can run artiﬁcial intelligence (AI) algorithms, considering AI as\nthe ability\
    \ of a machine (computer) to emulate intelligent human actions. The application\n\
    of AI to agriculture has been focused on three primary AI techniques: expert systems,\n\
    artiﬁcial neural networks, and fuzzy systems, with signiﬁcant results in the management\n\
    of crops, pests, diseases, and weeds, as well as the monitoring of agricultural\
    \ production,\nstore control, and yield prediction, for example [8].\nAI techniques\
    \ are also applied to provide vehicles with autonomy; therefore, au-\ntonomous\
    \ agricultural robots leverage this technology. AI-based vision systems can fulﬁll\n\
    the following roles:\n•\nDetecting static or dynamic objects in their surroundings.\n\
    •\nDetecting row crops for steering purposes.\n•\nIdentifying plants and locating\
    \ their positions for weeding are clear examples of the\ncurrent use of AI techniques\
    \ in agricultural robotics [9].\nAgriculture 2023, 13, 1005\n3 of 22\nAgriculture\
    \ 2023, 13, x FOR PEER REVIEW \n \n \nFigure 1. (a) Agricultural robots based\
    \ on retroﬁtted conventional vehicles (RHEA\ncultural robots designed on purpose\
    \ (Courtesy of AgreenCulture SaS). \nMoreover, computers can run artiﬁcial intelligence\
    \ (AI) algorithms, c\nas the ability of a machine (computer) to emulate intelligent\
    \ human action\ntion of AI to agriculture has been focused on three primary AI\
    \ techniques: e\nartiﬁcial neural networks, and fuzzy systems, with signiﬁcant\
    \ results in th\nof crops, pests, diseases, and weeds, as well as the monitoring\
    \ of agricultur\nstore control, and yield prediction, for example [8]. \nAI techniques\
    \ are also applied to provide vehicles with autonomy; th\nomous agricultural robots\
    \ leverage this technology. AI-based vision system\nfollowing roles: \n• \nDetecting\
    \ static or dynamic objects in their surroundings. \n• \nDetecting row crops for\
    \ steering purposes. \n•\nIdentifying plants and locating their positions for\
    \ weeding are clear e\nFigure 1. (a) Agricultural robots based on retroﬁtted conventional\
    \ vehicles (RHEA ﬂeet); (b) agricul-\ntural robots designed on purpose (Courtesy\
    \ of AgreenCulture SaS).\nAnother technology that has evolved in the last decade\
    \ is cloud computing, deﬁned\nas the on-demand delivery of computing services,\
    \ mainly data storage and computing\npower, including servers, storage, databases,\
    \ networking, software applications, artiﬁcial\nintelligence methods, and analytics\
    \ algorithms over the Internet. The main objective of\ncloud computing systems\
    \ is to provide ﬂexible resources at adapted prices. A cloud\ncomputing system\
    \ allows the integration of data of different types, loaded from many\nsources\
    \ in batch and real-time. In particular, the integration can be based on georeferenced\n\
    data in the precision farming area. Data can range from trajectory data to images\
    \ and\nvideos related to ﬁelds and missions and any sensors installed on the autonomous\
    \ robot.\nAgriculture 2023, 13, 1005\n4 of 22\nCloud computing allows the use\
    \ of services available in the cloud (computing, storing,\netc.), with increasing\
    \ advantages provided by big data techniques. Many agricultural\napplications\
    \ of big data technologies have already been introduced in agriculture [10] and\n\
    should be present in future robotic systems.\nThis article presents an architecture\
    \ to integrate new technologies and Internet trends\nin agricultural autonomous\
    \ robotic systems and has two main objectives. The ﬁrst objective\nis to provide\
    \ an example of designing control architectures to connect autonomous robots to\n\
    the cloud. It is oriented toward robot designers and gives signiﬁcant technical\
    \ details. The\nsecond objective is to disclose to farmers the advantages of integrating\
    \ the new technologies\nin autonomous robots that can provide farmers with signiﬁcant\
    \ advantages regarding\n(i) data storage, which is a secure and efﬁcient way to\
    \ store, but also access and share,\ndata, eliminating the need of physical storage\
    \ and, thus, reducing the risk of data loss;\n(ii) scalability, which allow the\
    \ farmers to expand or reduce their storage needs, efﬁciently\noptimizing their\
    \ resources, and (iii) analytics services, which allow a farmer to analyze their\n\
    own data to make informed decisions taking advantage of the AI tools available\
    \ on the\ncloud. These are general advantages of using the cloud, but autonomous\
    \ robots have great\npotential for collecting data and must facilitate communicating\
    \ those data to the cloud.\nTo base the architecture on a speciﬁc example, the\
    \ integration of a laser-based system\nfor weed management is considered. Thus,\
    \ Section 2 presents the material, deﬁning the\nrobot’s components, and the methodology,\
    \ detailing the system’s architecture. Section 3\nthen introduces the experiments\
    \ to be assessed and discussed in Section 4. Finally, Section 5\nsummarizes the\
    \ conclusions.\n2. Materials and Methods\nThis section ﬁrst describes the components\
    \ and equipment integrated for building\nthe autonomous robot used to validate\
    \ and assess the proposed integration methodology.\nSecond, the methods for the\
    \ integration of components are detailed.\n2.1. System Components\n2.1.1. Main\
    \ Process Loop in PA Autonomous Robots\nThe autonomous systems used for precision\
    \ agriculture generally follow the structure\nof an automatic control loop that\
    \ consists of the following (see Figure 2):\n•\nSelecting the references for the\
    \ magnitudes to be controlled, i.e., deﬁning the desired\nplan.\n•\nMeasuring\
    \ the magnitudes of interest.\n•\nMaking decisions based on the measured and desired\
    \ values of the magnitudes\n(control strategy).\n•\nExecuting the decided actions\n\
    In our application, the selecting references are made with the smart navigation\
    \ man-\nager (mission planner), the measures of the magnitudes of interest are\
    \ performed with\nthe perception system and the IoT sensor network, the decisions\
    \ are made with the smart\nnavigation manager (smart operation manager), and the\
    \ actions are executed with the\nagricultural tool and the autonomous robot that\
    \ move the implement throughout the mis-\nsion ﬁeld. In addition, our system also\
    \ takes care of the interaction with the cloud and\nthe operator. In our proposed\
    \ integration method, these components are grouped into\nmodules, as illustrated\
    \ in Figures 2 and 3. These modules are as follows.\nAgriculture 2023, 13, 1005\n\
    5 of 22\nAgriculture 2023, 13, x FOR PEER REVIEW \n5 of 23 \n \n \n \nFigure 2.\
    \ Components of a precision agriculture robotic system and main information ﬂow.\
    \ \n \nFigure 2. Components of a precision agriculture robotic system and main\
    \ information ﬂow.\nAgriculture 2023, 13, x FOR PEER REVIEW \n5 \n \n \n \nFigure\
    \ 2. Components of a precision agriculture robotic system and main information\
    \ ﬂow. \n \nFigure 3. Computing architecture.\nAgriculture 2023, 13, 1005\n6 of\
    \ 22\n2.1.2. Agricultural Robot\nA manually driven or autonomous vehicle is essential\
    \ in agricultural tasks to perform\nthe necessary actions throughout the working\
    \ ﬁeld. In this case, we use a compact mobile\nplatform based on a commercial\
    \ vehicle manufactured by AgreenCulture SaS, France. This\nis a tracked platform,\
    \ and, thus, it operates as a skid-steer mechanism. The track distance\ncan be\
    \ adapted to the crop row space. Equipped with an engine or batteries, the platform\n\
    can follow predeﬁned trajectories at 6 km/h with a position accuracy of ±0.015\
    \ m using a\nglobal positioning system (GPS) based on the real-time kinematic\
    \ (RTK) technique. This\nmobile platform is illustrated in Figure 4a.\nFigure\
    \ 3. Computing architecture. \n2.1.2. Agricultural Robot \nA manually driven or\
    \ autonomous vehicle is essential in agricultu\nthe necessary actions throughout\
    \ the working ﬁeld. In this case, we us\nplatform based on a commercial vehicle\
    \ manufactured by AgreenC\nThis is a tracked platform, and, thus, it operates\
    \ as a skid-steer mecha\ntance can be adapted to the crop row space. Equipped\
    \ with an eng\nplatform can follow predeﬁned trajectories at 6 km/h with a position\
    \ a\nusing a global positioning system (GPS) based on the real-time kin\nnique.\
    \ This mobile platform is illustrated in Figure 4a. \n \nFigure 4. (a) Mobile\
    \ platform (AgreenCulture SaS) and (b) autonomous laser \n2.1.3. Perception System\
    \  \nA perception system is based on computer vision algorithms t\nanalyze, and\
    \ understand images and data from the environment. Wi\nsystem produces numerical\
    \ and symbolic information for making de\ntion system for this study consists\
    \ of the following systems: \n• \nGuiding vision system: This system aims to detect\
    \ static and d\nFigure 4. (a) Mobile platform (AgreenCulture SaS) and (b) autonomous\
    \ laser weeding system.\n2.1.3. Perception System\nA perception system is based\
    \ on computer vision algorithms that obtain, process,\nanalyze, and understand\
    \ images and data from the environment. With these inputs, the\nsystem produces\
    \ numerical and symbolic information for making decisions. The perception\nsystem\
    \ for this study consists of the following systems:\nAgriculture 2023, 13, 1005\n\
    7 of 22\n•\nGuiding vision system: This system aims to detect static and dynamic\
    \ obstacles in\nthe robot’s path to prevent the robot tracks from stepping on\
    \ the crops during the\nrobot’s motion. Furthermore, it is also used to detect\
    \ crop rows in their early growth\nstage to guide the robot in GNSS-denied areas\
    \ [8]. The selected perception system\nconsisted of a red–green–blue (RGB) wavelength\
    \ vision camera and a time-of-ﬂight\n(ToF) camera attached to the front of the\
    \ mobile platform using a pan-tilt device, which\nallows control of the camera\
    \ angle with respect to the longitudinal axis of the mobile\nplatform, x. Figure\
    \ 4 illustrates both cameras and their locations onboard the robot.\n•\nWeed–meristem\
    \ vision system: The system is based on 3D vision cameras to provide\nthe controller\
    \ with data on crops and weeds. These data are used to carry out the main\nactivity\
    \ of the tool for which it has been designed: weed management, in this case.\n\
    For example, the perception system used in this study consists of an AI vision\
    \ system\ncapable of photographing the ground and discriminating crops from weeds\
    \ in a ﬁrst\nstep using deep learning algorithms. In the second step, the meristems\
    \ of the detected\nweeds are identiﬁed. Figure 3 sketches this procedure.\n2.1.4.\
    \ Agricultural Tools\nAgricultural tools focus on direct action on the crop and\
    \ soil and rely on physical\n(mechanical, thermal, etc.) or chemical (pesticides,\
    \ fertilizers, etc.) foundations. This study\nused a thermal weeding tool based\
    \ on a high-power laser source that provided lethal laser\ndoses to be deployed\
    \ on the weed meristems using scanners.\nAn AI video system provided the positions\
    \ of the weed meristems. Indeed, this\nspeciﬁc solution physically integrated\
    \ the AI vision system, the laser scanner, and the\nhigh-power laser source into\
    \ the laser-based weeding tool component. The video frames\nacquired with this\
    \ system were sent to the central controller at a rate of 4 frames/s. After\n\
    the mission, all stored images were sent to the cloud.\n2.1.5. The Smart Navigation\
    \ Manager (SNM)\nThis manager is a distributed software application responsible\
    \ for driving the au-\ntonomous robot and coordinating all other modules and systems.\
    \ The SNM is split into\n(i) the smart operation manager and (ii) the central\
    \ manager, which also includes the\nhuman–machine interface (HMI).\nSmart Operation\
    \ Manager (SoM)\nThe smart operation manager is a human–computer interaction module\
    \ that can\nacquire, process, and deliver information based on computer algorithms\
    \ and is devoted to\nassisting farmers in making accurate, evidence-based decisions.\
    \ The SoM is specialized for\nlaser weeding technology, the tool selected for\
    \ this study.\nData management is performed through the Internet using FIWARE.\
    \ Data access\ncontrol is provided via a virtual private network (VPN) to secure\
    \ data transfer to/from\nthe cloud. The visual dashboard will also be available\
    \ on the HMI for ﬁeld operations.\nThrough the dashboard, the operator will also\
    \ interact with the robot.\nThe smart operation manager is allocated in the cloud.\
    \ It contains the global mission\nplanner and supervisor, the map builder, and\
    \ the module for managing the IoT and cloud\ncomputing system (see Figures 3 and\
    \ 5). The hardware of the SoM relies on a cluster of 10\nservers.\nAgriculture\
    \ 2023, 13, 1005\n8 of 22\nAgriculture 2023, 13, x FOR PEER REVIEW \n8 of 23 \n\
    \ \n \n \nFigure 5. Cloud computing modules/containers. \n(a) Global Mission Planner\
    \ \nA planner is a software tool responsible for computing the trajectories of\
    \ the vehicle \nand an a priori known treatment map. The planner obtains some\
    \ types of information \nfrom the Internet, including the following: \n• \nMap\
    \ information according to the data models on the Internet; \n• \nOther information\
    \ provided by third parties, such as weather forecasts; \n• \nData models to create\
    \ maps for accessing already known treatment maps (sets of \npoints in the ﬁeld)\
    \ which commonly originate from third-party map descriptions \n(Google Earth;\
    \ Geographic Information System (GIS); GeoJSON.io, an open standard \nformat to\
    \ represent geographical features with nonspatial qualities). \nRegarding robot\
    \ location, two types of systems are envisaged, as follows: \n• \nAbsolute location\
    \ based on GNSS: GNSS integrates several controllers for line track-\ning and\
    \ is based on Dubins paths [11]; \no \nRelative location based on RGB and ToF\
    \ cameras, LIDAR, and IoT sensors: \nThese methods are based on diﬀerent techniques\
    \ for navigation in the ﬁeld and \nnavigation on the farm, such as hybrid topological\
    \ maps, semantic localization \nand mapping, and identiﬁcation/detection of natural\
    \ and artiﬁcial elements \n(crops, trees, people, vehicles, etc.) through machine\
    \ learning techniques. \n(b) Global Mission Supervisor \nA supervisor is a computational\
    \ tool responsible for overseeing and monitoring the \nexecution of the mission\
    \ plan while helping the farmer (operator) manage potential fail-\nures. Most\
    \ supervisor systems are designed around two actions: fault detection and fault\
    \ \ndiagnosis. The supervisor executes the following actions: \n• \nReceiving\
    \ alarms from the system components (vehicle, sensors, weeding tool, etc.). \n\
    • \nDetecting faults in real-time. \n• \nExecuting diagnosis protocols. \nFigure\
    \ 5. Cloud computing modules/containers.\n(a)\nGlobal Mission Planner\nA planner\
    \ is a software tool responsible for computing the trajectories of the vehicle\n\
    and an a priori known treatment map. The planner obtains some types of information\
    \ from\nthe Internet, including the following:\n•\nMap information according to\
    \ the data models on the Internet;\n•\nOther information provided by third parties,\
    \ such as weather forecasts;\n•\nData models to create maps for accessing already\
    \ known treatment maps (sets of\npoints in the ﬁeld) which commonly originate\
    \ from third-party map descriptions\n(Google Earth; Geographic Information System\
    \ (GIS); GeoJSON.io, an open standard\nformat to represent geographical features\
    \ with nonspatial qualities).\nRegarding robot location, two types of systems\
    \ are envisaged, as follows:\n•\nAbsolute location based on GNSS: GNSS integrates\
    \ several controllers for line tracking\nand is based on Dubins paths [11];\n\
    #\nRelative location based on RGB and ToF cameras, LIDAR, and IoT sensors: These\n\
    methods are based on different techniques for navigation in the ﬁeld and navi-\n\
    gation on the farm, such as hybrid topological maps, semantic localization and\n\
    mapping, and identiﬁcation/detection of natural and artiﬁcial elements (crops,\n\
    trees, people, vehicles, etc.) through machine learning techniques.\n(b)\nGlobal\
    \ Mission Supervisor\nA supervisor is a computational tool responsible for overseeing\
    \ and monitoring\nthe execution of the mission plan while helping the farmer (operator)\
    \ manage potential\nfailures. Most supervisor systems are designed around two\
    \ actions: fault detection and\nfault diagnosis. The supervisor executes the following\
    \ actions:\n•\nReceiving alarms from the system components (vehicle, sensors,\
    \ weeding tool, etc.).\n•\nDetecting faults in real-time.\n•\nExecuting diagnosis\
    \ protocols.\nAgriculture 2023, 13, 1005\n9 of 22\n•\nCollecting all available\
    \ geo-referred data generated by every module onboard the\nrobot. The data are\
    \ stored in both the robot and the cloud.\n(c)\nMap Builder\nA map builder is\
    \ an application used to convert maps based on GeoJSON into FIWARE\nentities.\
    \ Its main function is to support farmers in using the robotic system in a simple,\n\
    reliable, and robust way by giving the robot enough information a priori (e.g.,\
    \ farm schema\nand boundaries, ﬁeld locations and shapes, crop types, and status).\
    \ This module takes\nadvantage of the data models created by the FIWARE community\
    \ to represent the farm and\nother environments digitally, where they have been\
    \ conditioned to be adapted to robotic\nsystems and especially oriented to navigation\
    \ [12]. The design of the Map Builder allows\nthe user to accomplish the following:\n\
    •\nSelect the ﬁeld in GeoJSON.IO, an open-source geographic mapping tool that\
    \ allows\nmaps and geospatial data to be created, visualized, and shared in a\
    \ simple and\nmultiformat way.\n•\nAssign essential attributes to comply with\
    \ FIWARE. These attributes are those based\non the farmer’s knowledge. They can\
    \ include static (i.e., location, type, category) and\ndynamic (i.e., crop type\
    \ and status, seeding date, etc.) attributes.\n•\nExport in * GeoJSON format.\
    \ The map obtained will be imported for extracting the\ninformation required to\
    \ ﬁll in the FIWARE templates, which include the farms and\nparcel data models,\
    \ and other elements in a farm, such as buildings and roads.\nThis conversion\
    \ makes it easier to connect the robot to the cloud by standardizing\ndata. These\
    \ data, after processing, constitute a source for the design of processes with\
    \ the\nrobot, and its storage and subsequent analysis can provide forecasts of\
    \ future events in the\nﬁeld or behavior of the robot.\n(d)\nIoT System\nThis\
    \ study integrates an IoT sensor network to collect data from the following:\n\
    •\nThe autonomous vehicle: The data and images acquired with IoT sensors onboard\
    \ the\nvehicle are used to monitor and evaluate performances and efﬁciency and\
    \ to identify\nthe effects of treatments and trafﬁc on surfaces.\n•\nThe environment:\
    \ Data acquired with IoT sensors deployed on the cropland are used\nto (i) monitor\
    \ crop development and (ii) collect weather and soil information.\nTwo IoT sets\
    \ of devices are used in our study, as follows:\n•\nRobot–IoT set: It consists\
    \ of two WiFi high-deﬁnition cameras installed onboard the\nautonomous robot (IoT-R1\
    \ and IoT-R2 in Figure 3). The cameras are triggered from\nthe cloud or the central\
    \ controller to obtain a low frame rate (approximately 1/5 sec).\nThe pictures\
    \ are stored in the cloud and are used to monitor the effects of the passage\n\
    of the autonomous vehicle; therefore, they should include the robot’s tracks.\n\
    •\nField–IoT set: It consists of the following (see Figure 3):\n#\nTwo multispectral\
    \ cameras (IoT-F1 and IoT-F2) placed at the boundary of cropped\nareas to obtain\
    \ hourly pictures of crops.\n#\nA weather station (IoT-F3) to measure precipitation,\
    \ air temperature (Ta), relative\nhumidity (RH), radiation, and wind.\n#\nThree\
    \ soil multi-depth probes (IoT-F4) for acquiring moisture (Ts) data and three\n\
    respiration probes (IoT-F5) to measure CO2 and H2O.\nEvery one of these components\
    \ or nodes exchanges messages with the Message\nQueuing Telemetry Transport (MQTT)\
    \ protocol, carrying JavaScript Object Notation (JSON)\nserialized information\
    \ from node sensors/cameras interpreted as the entity. While metering\nnodes (weather,\
    \ soil probe, and respirometer) communicate by MQTT messages, camera\nnodes have\
    \ to transmit images (maximum of 100 pictures/day for periodic snapshots of\n\
    the area or alarms), and the use of FTP made a wide-band networking solution,\
    \ such as\nWiFi, mandatory instead of narrowband solutions.\nAgriculture 2023,\
    \ 13, 1005\n10 of 22\n(e)\nCloud Computing System\nThis study sets up a cloud-based\
    \ data platform, which is an ecosystem that incorpo-\nrates data acquired in the\
    \ ﬁeld. The data platform supports end-to-end data needs, such as\ningestion,\
    \ processing, and storage, to provide the following:\n•\nA data lake repository\
    \ for storing mission data to be downloaded in batches for\npost-mission analysis.\n\
    •\nA web interface for post-mission data analysis based on graphical dashboards,\
    \ georef-\nerenced visualizations, key performance indicators, and indices.\n\
    •\nA container framework for implementing “Decision Support System” functionalities\n\
    that deﬁne missions to be sent to the robot. These functionalities (e.g., the\
    \ mission\nplanner) can be implemented and launched from the cloud platform.\n\
    •\nA soft real-time web interface for missions. The interface visualizes real-time\
    \ robot\nactivities and performances or sends high-level commands to the robot\
    \ (e.g., start,\nstop, change mission).\nThese functionalities are ordered based\
    \ on the strictness of real-time constraints.\nThe cloud-computing platform is\
    \ based on the Hadoop stack and is powered by\nFIWARE. We adopted an open-source\
    \ solution with well-known components that can be\nimported into different cloud\
    \ service providers if no on-premises hardware is available.\nThe core component\
    \ of the platform is the (FIWARE) Orion Context Broker (OCB) from\nTelefonica\
    \ [13], a publish/subscribe context broker that also provides an interface to\
    \ query\ncontextual information (e.g., obtain all images from the cameras in a\
    \ speciﬁc farm), update\ncontext information (e.g., update the images), and be\
    \ notiﬁed when the context is updated\n(e.g., when a new image is added into the\
    \ platform). The images and raw data are stored in\nthe HDFS (Hadoop distributed\
    \ ﬁle system), while the NoSQL (not only structured query\nlanguage) MongoDB database\
    \ is used to collect the contextual data from FIWARE and\nfurther metadata necessary\
    \ to manage the platform [14]. Additionally, we use Apache\nKAFKA, an open-source\
    \ distributed event bus, to distribute context updates from FIWARE\nto all the\
    \ modules/containers hosted on the cloud platform. The different cloud computing\n\
    modules/containers used in this study are illustrated in Figure 5.\nCentral Manager\n\
    This central manager is an application that is divided into the following:\n•\n\
    Obstacle detection system. This module acquires visual information from the front\
    \ of\nthe robot (robot vision system) to detect obstacles based on machine vision\
    \ techniques.\n•\nLocal mission planner and supervisor. The planner plans the\
    \ motion of the robot near\nits surroundings. The local mission supervisor oversees\
    \ the execution of the mission\nand reports malfunctions to the operator (see\
    \ Section 2.1.5).\n•\nGuidance system. This system is responsible for steering\
    \ the mobile platform to follow\nthe trajectory calculated by the planner. It\
    \ is based on the GNSS if its signal is available.\nOtherwise, the system uses\
    \ the information from the robot vision system to extract the\ncrop row positions\
    \ and follow them without harming the crop.\n•\nHuman–machine interface\nA human–machine\
    \ interface (HMI) is a device or program enabling a user to commu-\nnicate with\
    \ another device, system, or machine. In this study, a HMI using portable devices\n\
    (android tablets) is addressed to allow farmers to perform the following:\n-\n\
    Supervise the mission.\n-\nMonitor and control the progress of agricultural tasks.\n\
    -\nIdentify and solve operational problems.\n-\nObtain real-time in-ﬁeld access\
    \ in an ergonomic, easy-to-use, and robust way.\n-\nMaintain the real-time safety\
    \ of the entire system.\nTo achieve these characteristics, a graphic device was\
    \ integrated with the portable/remote\ncontroller of the mobile platform. This\
    \ controller provides manual and remote vehicle\ncontrol and integrates an emergency\
    \ button.\nAgriculture 2023, 13, 1005\n11 of 22\n2.1.6. Sequence of Actions\n\
    The relationships among these components and modules and the information ﬂow\n\
    are illustrated in Figures 2 and 3. The process is a repeated sequence of actions\
    \ (A0 to A6),\ndeﬁned as follows:\nA0\nThe system is installed in the ﬁeld, The\
    \ operator/farmer deﬁnes or selects a previously\ndescribed mission using the\
    \ HMI and starts the mission.\nA1\nThe sensors of the perception module (M1) installed\
    \ onboard the autonomous robot\n(M2) extract features from the crops, soil, and\
    \ environment in the area of interest in\nfront of the robot.\nA2\nThe data acquired\
    \ in action A1 are sent to the smart operation manager, determining\nthe consequent\
    \ instructions for the robots and the agricultural tool.\nA3\nThe required robot\
    \ motions and agricultural tool actions are sent to the robot controller,\nwhich\
    \ generates the signal to move the robot to the desired positions.\nA4\nThe robot\
    \ controller forwards the commands sent by the smart navigation manager\nor generates\
    \ the pertinent signals for the agricultural tool to carry out the treatment.\n\
    A5\nThe treatment is applied, and the procedure is repeated from action A1 to\
    \ action A5\nuntil ﬁeld completion (A6).\nA6\nEnd of mission.\n2.2. Integration\
    \ Methods\nIntegrating all of the components deﬁned in the previous section to\
    \ conﬁgure an\nautonomous robot depends on the nature of the applications the\
    \ robot is devoted to and the\nconnections and communication among the different\
    \ components that must be precisely\ndeﬁned. Thus, this section ﬁrst describes\
    \ the computing architecture of the controller,\nwhich integrates the different\
    \ subsystems and modules. Second, the interfaces between\nsubsystems are precisely\
    \ deﬁned. Finally, the operation procedure is deﬁned.\n2.2.1. Computing Architecture\n\
    A distributed architecture based on an open-source Robot Operating System (ROS)\
    \ is\nproposed to integrate the system’s main components onboard the mobile platform\
    \ in this\nstudy. ROS is the operating system most widely accepted by software\
    \ developers to create\nrobotics applications. It consists of a set of software\
    \ libraries and tools that include drivers\nand advanced algorithms to help developers\
    \ build robot applications [15].\nIn this study, ROS, installed in the central\
    \ controller, is used as a meta-operating system\nfor the testing prototype. The\
    \ necessary interfaces (bridges) are developed to establish\ncommunication with\
    \ the autonomous vehicle, the perception system, and the laser-based\nweeding\
    \ tool. Because of ROS versatility and its publisher/subscriber communication\n\
    model, it is possible to adapt the messages to protocols commonly used in IoT,\
    \ such as\nMessage Queuing Telemetry Transport (MQTT).\nROS supports software\
    \ developers in creating robotics functionalities to monitor and\ncontrol robot\
    \ components connected to a local network. However, this solution is not\nextendible\
    \ to a wider network, such as the Internet. Fortunately, there exist some ROS\n\
    modules that solve the problem. One is ROSLink, a protocol for extensions deﬁning\
    \ an\nasynchronous communication procedure between the users and the robots through\
    \ the\ncloud [16]. ROSLink performance has been shown to be efﬁcient and reliable,\
    \ and it is\nwidely accepted by the robotics software community [17]. Although\
    \ ROSLink has been\nwidely used to connect robotic systems with the cloud, it\
    \ is oriented toward transmitting\nlow-level messages. There is no convention\
    \ to deﬁne standard data models that allow\nintelligent robotics systems to be\
    \ scalable.\nOne alternative to a more internet-oriented communication framework\
    \ is FIWARE,\nwhich offers interaction with the cloud using cloud services that\
    \ provide well-known bene-\nﬁts, such as (a) cost and ﬂexibility, (b) scalability,\
    \ (c) mobility, and (d) disaster recovery [18].\nFIWARE is an open software curated\
    \ platform fostered by the European Commission\nand the European Information and\
    \ Communication Technology (ICT) industry for the\nAgriculture 2023, 13, 1005\n\
    12 of 22\ndevelopment and worldwide deployment of Future Internet applications.\
    \ It attempts to\nprovide a completely open, public, and free architecture and\
    \ a collection of speciﬁcations\nthat allows organizations (designers, service\
    \ providers, businesses, etc.) to develop open\nand innovative applications and\
    \ services on the Internet that fulﬁll their needs [19].\nIn this study, a cloud-based\
    \ communication architecture has been implemented us-\ning FIWARE as the core,\
    \ which allows messages between the edge and the cloud to be\ntransferred and\
    \ stored. The selection was made because this is an open-source platform\nthat\
    \ provides free development modules and has many enablers already developing and\n\
    integrating solutions for smart agriculture.\nIn addition to FIWARE, we use KAFKA,\
    \ a robust distributed framework for streaming\ndata (see Section 2.1.5) that\
    \ allows producers to send data and for consumers to subscribe to\nand process\
    \ such updates. KAFKA enables the processing of streams of events/messages\nin\
    \ a scalable and fault-tolerant manner, and decouples producers and consumers\
    \ (i.e., a\nconsumer can process data even after a producer has gone ofﬂine).\
    \ For historic data, HDFS\nallows the download of batches of data at any time\
    \ and replicates each data in three copies\nto prevent data loss.\nThe visual\
    \ dashboard will also be available on the HMI for the ﬁeld operations.\nThrough\
    \ the dashboard, the operator will also interact with the robot. FIWARE smart\
    \ data\nmodels do not sufﬁce to represent our application domain or to integrate\
    \ the agricultural\nand robotic domains; therefore, we have extended the existing\
    \ models and updated some\nexisting entities. Since smart data models from FIWARE\
    \ are overlapping and sometimes\ninconsistent, we had to envision a uniﬁed model\
    \ to integrate and reconcile the data. To\nconnect the robotic system with the\
    \ cloud, speciﬁc data models were developed to represent\nthe different robotic\
    \ elements, following the guidelines of FIWARE and its intelligent data\nmodels\
    \ [12].\nThe IoT devices deployed in the ﬁeld must be able to establish connections\
    \ through\nWiFi and LoRa technologies. WiFi is a family of wireless network protocols.\
    \ These protocols\nare generally used for Internet access and communication in\
    \ local area networks, allowing\nnearby electronic devices to exchange data using\
    \ radio waves. LoRa technology is a\nwireless protocol designed for long-range\
    \ connectivity and low-power communications\nand is primarily targeted for the\
    \ Internet of Things (IoT) and M2M networks. LoRa tolerates\nnoise, multipath\
    \ signals, and the Doppler effect. The cost of achieving this is a very low\n\
    bandwidth compared to other wireless technologies. This study uses a 4G LTE-M\
    \ modem\nto connect to the Internet.\nAt a lower level of communication, CANbus\
    \ or ISOBUS is generally used to control\nand monitor the autonomous vehicle.\
    \ This study uses CANbus and its communication\nprotocol CANopen. Autonomous vehicles\
    \ and agricultural tools typically contain their\nown safety controllers. The\
    \ ﬁrst behaves as a master and, in the case of a risky situation, it\ncommands\
    \ the tool to stop.\nThe human–machine interface (HMI) will include a synchronous\
    \ remote procedure\ncall-style communication over the services protocol and asynchronous\
    \ communications to\nensure the robot’s safety. In addition to these ROS-based\
    \ protocols, the HMI has a safety\ncontrol connected to the low-level safety system\
    \ (by radiofrequency) for emergency stops\nand manual control.\nFigure 6 illustrates\
    \ the overall architecture, indicating the following:\n•\nThe modules (Mi), presented\
    \ in the previous sections.\n•\nThe interconnection between modules, presented\
    \ in the next section.\n•\nThe communication technologies and protocols to conﬁgure\
    \ agricultural robotic sys-\ntems that integrate IoT and cloud computing technologies.\n\
    The main characteristics of this architecture are summarized in Table 1.\nAgriculture\
    \ 2023, 13, 1005\n13 of 22\nAgriculture 2023, 13, x FOR PEER REVIEW \n13 of 23\
    \ \n \n \nThe main characteristics of this architecture are summarized in Table\
    \ 1. \n \nFigure 6. Experimental ﬁelds. \nTable 1. Architecture components. \n\
    Architecture Component \nSolutions/Comments \nOperating system \nROS (Robot Operating\
    \ System) \nIoT–controller bridge \nHypertext Transfer Protocol (HTTP) to FIWARE\
    \ \nNote: FIWARE is used as a communication protocol in the \ncloud; therefore,\
    \ it is not necessary to use ROSLink. \nROS-based system for FI-\nWARE tools \n\
    HTTP protocol to FIWARE \nNote: FIROS has several disadvantages when developing\
    \ \nnew data models to represent the robot, so a particular ena-\nbler will not\
    \ be used to establish communication between \nthe robot and the cloud. \nCommunication\
    \ with IoT \ndevices \nWiFi, serial communication \nFigure 6. Experimental ﬁelds.\n\
    Table 1. Architecture components.\nArchitecture Component\nSolutions/Comments\n\
    Operating system\nROS (Robot Operating System)\nIoT–controller bridge\nHypertext\
    \ Transfer Protocol (HTTP) to FIWARE\nNote: FIWARE is used as a communication\
    \ protocol in the cloud;\ntherefore, it is not necessary to use ROSLink.\nROS-based\
    \ system for FIWARE tools\nHTTP protocol to FIWARE\nNote: FIROS has several disadvantages\
    \ when developing new data\nmodels to represent the robot, so a particular enabler\
    \ will not be\nused to establish communication between the robot and the cloud.\n\
    Communication with IoT devices\nWiFi, serial communication\nNote: Since a certain\
    \ amount of data needs to be transmitted, WiFi\nwould sufﬁce.\nThe Internet\n\
    4G LTE-M modem\nDevices onboard the mobile platform\nCANopen, serial\nHuman–machine\
    \ interface (HMI).\nSynchronous remote procedure call-style communication over\n\
    services protocol.\nAsynchronous communications to ensure the safety of the robot.\n\
    Note: The HMI is used to provide access to SoM services through a\nweb interface.\n\
    Agriculture 2023, 13, 1005\n14 of 22\n2.2.2. Interfaces between System Components\n\
    This architecture considers four main interfaces between systems and modules,\
    \ as\nfollows:\nSmart Navigation Manager (M4)/Perception System (M1) interface\n\
    To receive the raw information from the perception system (sensors, cameras, etc.),\n\
    the central manager uses direct connections via the transmission control protocol/Internet\n\
    protocol (TCP/IP) for sensors and the universal serial bus (USB) for RGB and ToF\
    \ cameras.\nAll IoT devices use the available wireless communication technologies\
    \ (WiFi and LoRa) to\naccess the Internet and the cloud.\nTo guide the robot,\
    \ the obstacle detection system obtains data from the guiding\nvision system (RGB\
    \ and ToF cameras) through the Ethernet that communicates the central\nmanager\
    \ with the perception system. This communication is stated using the ROS manager\n\
    and the perception–ROS bridge (see Figure 3).\nSmart Navigation Manager (M4)/Agricultural\
    \ Tool (M3) interface\nThese systems can communicate through ROS messaging protocols,\
    \ where the pub-\nlisher/subscriber pattern is preferred. This interface exchanges\
    \ simple test messages to\nverify the communication interface.\nIt is worth mentioning\
    \ that the perception system and the agricultural tool are con-\nnected directly\
    \ in some speciﬁc applications. This solution decreases the latency of data\n\
    communication but demands moving a portion of the decision algorithms from the\
    \ smart\nnavigation manager to the tool controller; therefore, the tool must exhibit\
    \ computational\nfeatures. This scheme is used in the weeding system to test the\
    \ proposed architecture.\nSmart Navigation Manager (M4)/Autonomous Robot (M2)\
    \ interface\nInitially, these systems communicate via CANbus with the CANopen\
    \ protocol. The\ncentral manager uses this protocol to receive information on\
    \ the status of the autonomous\nvehicle and basic information from the onboard\
    \ sensors (GNSS, IMU, safety system, etc.).\nA CANbus–ROS bridge is used to adapt\
    \ the communication protocols.\nAutonomous Robot (M2)/Agricultural Tool (M3) interface\n\
    Usually, it is not necessary for the vehicle to directly communicate with the\
    \ tool because\nthe smart navigation manager coordinates them. However, as autonomous\
    \ vehicles and\nagricultural tools usually have safety controllers, there is wired\
    \ communication between\nthe two safety controllers. In such a case, the autonomous\
    \ vehicle safety controller works as\na master and commands the tool safety controller\
    \ to stop the tool if a dangerous situation\nappears.\nPerception System (M1)/Agricultural\
    \ Tool (M3)\nThis communication is required to inform the agricultural tools about\
    \ the crop status.\nIn weeding applications, the information is related to the\
    \ positions of the weeds. In\nthis speciﬁc application, the perception system\
    \ (weed meristem detection module) sends\nthe weed meristem positions to the laser\
    \ scanner module of the agricultural tool. This\ncommunication is carried out\
    \ using a conventional Ethernet connection. The metadata\ngenerated via the detection\
    \ system are made available in the existing ROS network and\nsent to the smart\
    \ navigation manager.\nSmart Navigation Manager internal/cloud communications\n\
    The smart navigation manager is a distributed system that consists of three main\n\
    modules:\n•\nThe central manager running on the central controller.\n•\nThe smart\
    \ operation manager running on the cloud.\n•\nThe HMI running in a portable device.\n\
    Agriculture 2023, 13, 1005\n15 of 22\nThe central manager and the smart operation\
    \ manager communicate via NGSI v2,\na FIWARE application programming interface,\
    \ using a FIWARE–ROS bridge to adapt\nROS protocols to NGSI v2 messages. In contrast,\
    \ the HMI communicates with the central\nmanager via WiFi and Internet, directly\
    \ accessing the web services hosted in the cloud.\nThe HMI exhibits a panic button\
    \ connected via radiofrequency to the safety systems of the\nautonomous robot\
    \ and the agricultural tool.\nIoT system/Cloud\nThere is a direct link from the\
    \ IoT system to the cloud using MQTT.\n2.2.2.8. Operation Procedure\nTo use the\
    \ proposed architecture and method, the user must follow the method below.\n•\n\
    Creating the map: The user creates the ﬁeld map following the procedure described\
    \ in\nthe MapBuilder module (see Section 2.1.5).\n•\nCreating the mission: The\
    \ user creates the mission by selecting the mission’s initial\npoint (home garage)\
    \ and destination ﬁeld (study site).\n•\nSending the mission: The user selects\
    \ the mission to be executed with the HMI (all\ndeﬁned missions are stored in\
    \ the system) and sends it to the robot using the cloud\nservices (see Section\
    \ Smart Operation Manager (SoM)).\n•\nExecuting the mission: The mission is executed\
    \ autonomously following the sequence\nof actions described in Section 2.1.6.\
    \ The user does not need to act except for when\nalarms or collision situations\
    \ are detected and warned of by the robot.\n•\nApplying the treatment: When the\
    \ robot reaches the crop ﬁeld during the mission, it\nsends a command to activate\
    \ the weeding tool, which works autonomously. The tool\nis deactivated when the\
    \ robot performs the turns at the headland of the ﬁeld and is\nstarted again when\
    \ it re-enters. The implement was designed to work with its own\nsensory and control\
    \ systems, only requiring the mobile platform for mobility and\ninformation when\
    \ it must be activated/deactivated.\n•\nSupervising the mission: When the robotic\
    \ system reaches the crop ﬁeld, it also sends\na command to the IoT sensors, warning\
    \ that the treatment is in progress. Throughout\nthe operation, the mission supervisor\
    \ module analyzes all the information collected by\nthe cloud computing system,\
    \ generated by both the robotic system and the IoT sensors.\nIt evaluates if there\
    \ is a possible deviation from the trajectory or risk of failure.\n•\nEnding the\
    \ mission: The mission ends when the robot reaches the last point in the\nﬁeld\
    \ map computed by the MapBuilder. Optionally, the robot can stay in the ﬁeld or\n\
    return to the home garage. During the mission execution, the user can stop, resume,\n\
    and abort the mission through the HMI.\n3. Experimental Assessment\nThis section\
    \ states the characteristics of the described autonomous robot with IoT\nand cloud\
    \ computing connectivity. To achieve this purpose, the experimental ﬁeld for this\n\
    study is ﬁrst described. Then, a test mission is deﬁned to acquire data from the\
    \ different\nsubsystems. Finally, the system characteristics are analyzed and\
    \ assessed.\nThe characteristics obtained are not compared with similar robotic\
    \ systems due to\nthe lack of such information in the literature. There are no\
    \ published results in weeding\napplications; therefore, it is difﬁcult to compare,\
    \ and the indicators have been geared\ntowards general cloud computing and mobile\
    \ robotics characteristics. Therefore, cross-\nvalidation has been carried out,\
    \ comparing the features of the autonomous robot with the\ngeneral performance\
    \ of the robot and cloud communication. Productivity, cost, and other\nindicators\
    \ of the presented architecture are those of the general use of cloud computing.\n\
    3.1. Study Site\nThe system developed for this study was tested in an experimental\
    \ ﬁeld located in\nMadrid, Spain (40◦18′45.166′′, −3◦28′51.096′′). The climate\
    \ of the study site is classiﬁed as\nAgriculture 2023, 13, 1005\n16 of 22\na hot\
    \ summer Mediterranean climate with an average annual temperature of 14.3 ◦C and\n\
    precipitation of 473 mm.\nThe experimental ﬁeld consisted of two areas of 60 ×\
    \ 20 m2 that grew wheat (Triticum\naestivum L.), with crop rows at a distance\
    \ of 0.10 m, and maize (Zea mays L.), with crop\nrows at a distance of 0.50 m,\
    \ respectively. Each area was divided into three sections of\n20 × 20 m2. The\
    \ sections in one area were seeded in consecutive weeks, allowing us to\nconduct\
    \ experiments in three-week windows. Figure 6 shows the experimental ﬁeld and\n\
    the distribution of the areas and sections.\n3.2. Description of the Test Mission\n\
    Tests were conducted to assess the performance and quality of integrating new\
    \ tech-\nnologies in autonomous robots for agriculture. First, the testing prototype\
    \ was integrated\nwith the components introduced in Section 2; then, several IoT\
    \ devices were disseminated\nthroughout the ﬁeld (RGB and multispectral cameras,\
    \ weather stations, soil probes, etc.);\nﬁnally, a mission was deﬁned to acquire\
    \ data in the study site to perform quantitative\nanalyses. The mission consisted\
    \ of covering sections of 20 × 20 m2 with wheat and maize\ncrops while the following\
    \ occurred:\n•\nAcquiring data from the IoT sensor network.\n•\nTaking pictures\
    \ of the crop.\n•\nAcquiring data from the guidance system.\n•\nSending all the\
    \ acquired information to the cloud.\nThe mission proposed by the planner is illustrated\
    \ in Figure 7. The robot tracked the\npath autonomously, and the following procedures\
    \ were carried out.\nAgriculture 2023, 13, x FOR PEER REVIEW \n17 of 23 \n \n\
    \ \nFigure 7. Robot’s path from the home garage to the study site. The planner\
    \ provides the mission for \ncovering the study site. \nPerception system procedure\
    \ \n• \nGuiding vision system: This experiment was conducted in the treatment\
    \ stage, where \nthe crop was detected to adjust the errors derived from planning\
    \ and the lack of pre-\ncision of the maps. YOLOv4 [20], a real-time object detector\
    \ based on a one-stage \nobject detection network, was the base model for detecting\
    \ early-stage growth in \nmaize [8], a wide-row crop. The model was trained using\
    \ a dataset acquired in an \nagricultural season before these tests using the\
    \ same camera system [21]. Moreover, \nin the case of wheat, which is a narrow-row\
    \ crop, a diﬀerent methodology was ap-\nplied through the use of segmentation\
    \ models, such as MobileNet, a convolutional \nl\nk f\nbil\ni i\nli\ni\n[22]\n\
    i\nd\ni\nd\ni\nd\nHome garage \nStudy site \nFigure 7. Robot’s path from the home\
    \ garage to the study site. The planner provides the mission for\ncovering the\
    \ study site.\nPerception system procedure\n•\nGuiding vision system: This experiment\
    \ was conducted in the treatment stage, where\nthe crop was detected to adjust\
    \ the errors derived from planning and the lack of\nprecision of the maps. YOLOv4\
    \ [20], a real-time object detector based on a one-stage\nAgriculture 2023, 13,\
    \ 1005\n17 of 22\nobject detection network, was the base model for detecting early-stage\
    \ growth in\nmaize [8], a wide-row crop. The model was trained using a dataset\
    \ acquired in an\nagricultural season before these tests using the same camera\
    \ system [21]. Moreover, in\nthe case of wheat, which is a narrow-row crop, a\
    \ different methodology was applied\nthrough the use of segmentation models, such\
    \ as MobileNet, a convolutional neural\nnetwork for mobile vision applications\
    \ [22], trained using a dataset acquired in an\nagricultural season before these\
    \ tests [23], with the same camera system. The detection\nof both crops was evaluated\
    \ with regard to the GNSS positions collected manually for\nthe different crop\
    \ lines.\nThe maize and wheat datasets were built with 450 and 125 labeled images,\
    \ respectively.\nData augmentation techniques (rotating, blurring, image cropping,\
    \ and brightness changes)\nwere used to increase the size of the datasets. For\
    \ both crops, 80% of the data was destined\nfor training, 10% for validation,\
    \ and 10% for testing.\n•\nThe AI vision system: This system uses data from the\
    \ installed RGB cameras to enable\nrobust automated plant detection and discrimination.\
    \ For this purpose, the state-\nof-the-art object detection algorithm Yolov7 is\
    \ used in combination with the Nvidia\nframework DeepStream. Tracking the detected\
    \ plants is performed in parallel by a\npretrained DeepSort algorithm [24]. The\
    \ reliability of the object detection algorithm\nis evaluated using test datasets\
    \ with the commonly used metrics “intersection over\nunion” (IoU) and “mean average\
    \ precision” (mAP). This system works cooperatively\nwith laser scanners as a\
    \ stand-alone system. The information is not stored in the cloud.\nThe dataset\
    \ used for training weed/crop discrimination was generated in ﬁelds in\nseveral\
    \ European countries. It contains 4000 images, 1000 of which are fully labeled.\n\
    Distinctions are made according to the processing steps to be applied: weeds,\
    \ grasses,\nand crops. In addition, the dataset was expanded to three times its\
    \ original size through\naugmentation measures. As well as generating new training\
    \ data, this enables robustness\nagainst changing environmental inﬂuences, such\
    \ as changing color representation, motion\nblur, and camera distortion. The YoloV7\
    \ network achieved a mean average precision (mAP)\nof 0.891 after 300 epochs of\
    \ training. The dataset was divided into 80%, 10%, and 10% for\ntraining, validation,\
    \ and testing subsets, respectively.\nAutonomous robot procedure\nThe navigation\
    \ controller: Given a set of trajectories based on RTK-GNSS, the perfor-\nmance\
    \ of the guidance controller was evaluated by measuring lateral and angular error\n\
    through the incorporation of colored tapes on the ground and using the onboard\
    \ RGB\ncamera and ToF to extract the tape positions to compute the errors concerning\
    \ the robot’s\npace.\nSmart Navigation Manager procedure:\n•\nSmart operation\
    \ manager: The processing time, latency, success rate, response time,\nand response\
    \ status based on requests of the mission planner, IoT sensors, and cloud\ncomputing\
    \ services were evaluated using ROS functionalities that provide statistics\n\
    related to the following:\n#\nThe period of messages by all publishers.\n#\nThe\
    \ age of messages.\n#\nThe number of dropped messages.\n#\nTrafﬁc volume to be\
    \ measured in real-time.\n•\nCentral manager: The evaluation is similar to that\
    \ used for the navigation controller.\n•\nObstacle detection system: YOLOv4 and\
    \ a model already developed based on the\nCOCO database were introduced to detect\
    \ common obstacles in agricultural environ-\nments and were also used for evaluation.\
    \ YOLOv4 is a one-stage object detection\nmodel, and COCO (common objects in context)\
    \ is a large-scale object detection, seg-\nmentation, and captioning dataset.\n\
    Agriculture 2023, 13, 1005\n18 of 22\n4. System Assessment and Discussion\nThe\
    \ mission described in the previous section produced crop images, sensor data,\
    \ and\ntrafﬁc information with the following characteristics:\n•\nCrop images:\
    \ During the robot’s motion, images are acquired at a rate of 4 frames/s\nto guide\
    \ the robot. The RGB images are 2048 × 1536 pixels with a weight of 2.2 MB\n(see\
    \ Figures 8 and 9), and the ToF images feature 352 × 264 points (range of 300–5000\n\
    mm) (see Figure 10). The images are sent to the guiding and obstacle detection\
    \ system\nthrough the Ethernet using ROS (perception–ROS bridge in the perception\
    \ system\nand ROS manager in the central manager). A subset of these images is\
    \ stored in the\ncloud for further analysis. Using a FIWARE–ROS bridge with the\
    \ NGSI application\nprogramming interface, the system sends up to 4 frames/s.\n\
    •\nSensor data: IoT devices send the acquired data using 2.4 GHz WiFi with the\
    \ MQTT\nprotocol and JSON format.\n•\nTrafﬁc information: The ROS functionalities\
    \ mentioned above revealed that during\na ﬁeld experiment (10 min duration), the\
    \ total number of delivered messages was\n2,395,692, with a rate of only 0.63%\
    \ dropped messages (messages that were dropped\ndue to not having been processed\
    \ before their respective timeout), with average trafﬁc\nof 10 MB/s and maximum\
    \ trafﬁc of 160 MB at any instant of time. No critical messages\n(command messages)\
    \ were lost, demonstrating robustness within the smart navigation\nmanager. Regarding\
    \ cloud trafﬁc, during a period of time of approximately 3 h, the\nmessages sent\
    \ to the cloud were monitored, where the number of messages received by\nthe cloud\
    \ was measured; the delay time of the transmission of the messages between\nthe\
    \ robot (edge) and the OCB, and between the robot and the KAFKA bus (see Figure\
    \ 3),\nwere also measured. During this interval of time, around 4 missions were\
    \ executed,\nand a total of 14,368 messages were sent to the cloud, mainly the\
    \ robot status and the\nperception system data. An average delay of about 250\
    \ ms was calculated between\nthe moment the message is sent from the robot and\
    \ the moment it is received in the\nOCB (see Figure 11a). Moreover, the KAFKA\
    \ overhead, i.e., the time it takes for a\nmessage received by the OCB to be forwarded\
    \ to the KAFKA bus and eventually\nprocessed by a KAFKA consumer, was approximately\
    \ 1.24 ms, demonstrating that the\ninternal communications within the server and\
    \ hosted cloud services are robust (see\nFigure 11b).\nAgriculture 2023, 13, x\
    \ FOR PEER REVIEW \n19 of 23 \n \nKAFKA bus (see Figure 3), were also measured.\
    \ During this interval of time, around \n4 missions were executed, and a total\
    \ of 14,368 messages were sent to the cloud, \nmainly the robot status and the\
    \ perception system data. An average delay of about \n250 ms was calculated between\
    \ the moment the message is sent from the robot and \nthe moment it is received\
    \ in the OCB (see Figure 11a). Moreover, the KAFKA over-\nhead, i.e., the time\
    \ it takes for a message received by the OCB to be forwarded to the \nKAFKA bus\
    \ and eventually processed by a KAFKA consumer, was approximately \n1.24 ms, demonstrating\
    \ that the internal communications within the server and \nhosted cloud services\
    \ are robust (see Figure 11b).  \n \nFigure 8. Example of a wheat image acquired\
    \ with the guiding vision system and uploaded to the \ncloud. \nFigure 8. Example\
    \ of a wheat image acquired with the guiding vision system and uploaded to the\n\
    cloud.\nAgriculture 2023, 13, 1005\n19 of 22\n \n \nFigure 8. Example of a wheat\
    \ image acquired with the guiding vision system and uploaded to the \ncloud. \n\
    \ \nFigure 9. Example of a maize image acquired with the guiding vision system\
    \ and uploaded to the \ncloud. \nThe system has been tested in a ﬁeld with two\
    \ diﬀerent crops. Data related to cloud \ncommunication and robot guidance algorithms\
    \ have been collected. The communication \nFigure 9. Example of a maize image\
    \ acquired with the guiding vision system and uploaded to the\ncloud.\nThe system\
    \ has been tested in a ﬁeld with two different crops. Data related to cloud\n\
    communication and robot guidance algorithms have been collected. The communication\n\
    performance is similar to that obtained using conventional mechanisms, so we beneﬁt\
    \ from\nusing ROS and FIWARE without compromising performance.\nAgriculture 2023,\
    \ 13, x FOR PEER REVIEW \n20 of 23 \n \nperformance is similar to that obtained\
    \ using conventional mechanisms, so we beneﬁt \nfrom using ROS and FIWARE without\
    \ compromising performance. \n \nFigure 10. Example of a ToF intensity image acquired\
    \ with the guidance system and uploaded to \nthe cloud. \nFigure 10. Example of\
    \ a ToF intensity image acquired with the guidance system and uploaded to the\n\
    cloud.\nAgriculture 2023, 13, 1005\n20 of 22\n \n \nFigure 10. Example of a ToF\
    \ intensity image acquired with the guidance system and uploaded to \nthe cloud.\
    \ \n \nFigure 11. Example of a ToF intensity image acquired with the guidance\
    \ system and uploaded to \nthe cloud. (a) Message delay and (b) Kafka overhead.\
    \ \nFigure 11. Example of a ToF intensity image acquired with the guidance system\
    \ and uploaded to the\ncloud. (a) Message delay and (b) Kafka overhead.\n5. Conclusions\n\
    An architecture is presented to conﬁgure autonomous robots for agriculture with\n\
    access to cloud technologies. This structure takes advantage of new concepts and\
    \ technolo-\ngies, such as IoT and cloud computing, allowing big data, edge computing,\
    \ and digital\ntwins to be incorporated into modern agricultural robots.\nThe\
    \ architecture is based on ROS, the most universally accepted collection of software\n\
    libraries and tools for building robotic applications, and FIWARE, an open architecture\n\
    that enables the creation of new applications and services on the Internet. ROS\
    \ and FI-\nWARE provide attractive advantages for developers and farmers. ROS\
    \ and FIWARE offer\npowerful tools for developers to build control architectures\
    \ for complex robots with cloud\ncomputing/IoT features, making development easier\
    \ and leveraging open-source frame-\nworks. ROS and FIWARE, as in the proposed\
    \ integration, provide reusability, scalability,\nand maintenance using the appropriate\
    \ hardware resources. In addition, integrating the\nrobot controller into the\
    \ Internet allows the exploitation of autonomous robot services for\nagriculture\
    \ through the Internet.\nOn the other hand, the use of this type of architecture\
    \ reveals to farmers the advantages\nof communicating autonomous robots with the\
    \ cloud, providing them with leading beneﬁts\nto storing data safely and efﬁciently,\
    \ eliminating physical storage, and, thus, reducing the\nrisk of data loss. Data\
    \ stored in the cloud makes it easy to access data from anywhere and\nshare it\
    \ with other farmers or platforms. In addition, the services offered in the cloud\
    \ are\nvery ﬂexible to contract the actual storage needed at all times, optimizing\
    \ the farmer’s\nresources. Finally, farmers can use the analysis tools available\
    \ in the cloud to make their\nAgriculture 2023, 13, 1005\n21 of 22\nown decisions.\
    \ In any case, working in the cloud requires an initial investment, which is\n\
    usually recovered quickly.\nThe different components of the robot, particularized\
    \ for a laser-based weeding robot,\nare described, and the general architecture\
    \ is presented, indicating the speciﬁc interfaces.\nBased on these components,\
    \ the article presents the action sequence of the robot and the\noperating procedure\
    \ to illustrate how farmers can use the system and what beneﬁts they\ncan obtain.\n\
    Several experiments with two crops were conducted to evaluate the proposed in-\n\
    tegration based on the data communication characteristics, demonstrating the system’s\n\
    capabilities. The crop row detection system works correctly for both crops, tracking\
    \ the\nrows with an accuracy of ±0.02 m. The evaluation concluded that the system\
    \ could send\nimage frames to the cloud at 4 frames/s; messages between subsystems\
    \ and modules can\nbe passed with a 0.63% rejection rate. Regarding the trafﬁc\
    \ of the information exchanged,\nan average delay of 250 ms was detected in the\
    \ messages between the robot and the OCB.\nIn contrast, the OCB and the KAFKA\
    \ bus measured an average message of 1.24 ms. This\nindicates the robustness of\
    \ internal communications within the server and hosted cloud\nservices. This performance\
    \ is in the range obtained when a system communicates with the\ncloud using conventional\
    \ methods, so ROS and FIWARE facilitate communication with the\ncloud without\
    \ compromising performance.\nFuture work will focus on extending cloud computing\
    \ architecture to integrate digital\ntwins, orchestrate big data ensembles, and\
    \ facilitate the work of robots with edge computing\nperformance.\nAuthor Contributions:\
    \ Conceptualization, L.E., R.F., P.G.-d.-S., M.F., M.G., G.V., H.S., M.H. and\n\
    M.W.; methodology, L.E. and R.F.; software, L.E., M.F., H.S. and M.W.; validation,\
    \ L.E., M.F., G.V. and\nH.S.; investigation, L.E., R.F., P.G.-d.-S., M.F., M.G.,\
    \ G.V., H.S., M.H. and M.W.; writing—original draft\npreparation, P.G.-d.-S.;\
    \ writing—review and editing, L.E., P.G.-d.-S. and R.F.; supervision, L.E. and\n\
    P.G.-d.-S.; funding acquisition, P.G.-d.-S., G.V., M.G. and M.W. All authors have\
    \ read and agreed to\nthe published version of the manuscript.\nFunding: This\
    \ article is part of a project that has received funding from the European Union’s\n\
    Horizon 2020 research and innovation program under grant agreement No 101000256.\n\
    Institutional Review Board Statement: The study was conducted in accordance with\
    \ the Declaration\nof Helsinki, and approved by the Ethics Committee of CSIC.\n\
    Data Availability Statement: Herrera-Diaz, J.; Emmi, L.A.; Gonzalez de Santos,\
    \ P. Maize Dataset.\n2022. Available online: http://doi.org/10.20350/digitalCSIC/14566\
    \ (accessed on 1 April 2023).\nHerrera-Diaz, J.; Emmi, L.; Gonzalez de Santos,\
    \ P. Wheat Dataset. 2022. Available online: http:\n//doi.org/10.20350/digitalCSIC/14567\
    \ (accessed on 1 April 2023).\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nReferences\n1.\nGhose, B. Food security and food self-sufﬁciency\
    \ in China: From past to 2050. Food Energy Secur. 2014, 3, 86–95. [CrossRef]\n\
    2.\nZhang, N.; Wang, M.; Wang, N. Precision agriculture—A worldwide overview.\
    \ Comput. Electron. Agric. 2002, 36, 113–132.\n[CrossRef]\n3.\nStentz, A.; Dima,\
    \ C.; Wellington, C.; Herman, H.; Stager, D. A System for Semi-Autonomous Tractor\
    \ Operations. Auton. Robot.\n2002, 13, 87–104. [CrossRef]\n4.\nBergerman, M.;\
    \ Maeta, S.M.; Zhang, J.; Freitas, G.M.; Hamner, B.; Singh, S.; Kantor, G. Robot\
    \ Farmers: Autonomous Orchard\nVehicles Help Tree Fruit Production. IEEE Robot.\
    \ Autom. Mag. 2015, 22, 54–63. [CrossRef]\n5.\nGonzalez-De-Santos, P.; Ribeiro,\
    \ A.; Fernandez-Quintanilla, C.; Lopez-Granados, F.; Brandstoetter, M.; Tomic,\
    \ S.; Pedrazzi, S.;\nPeruzzi, A.; Pajares, G.; Kaplanis, G.; et al. Fleets of\
    \ robots for environmentally-safe pest control in agriculture. Precis. Agric.\
    \ 2017,\n18, 574–614. [CrossRef]\n6.\nUnderwood, J.P.; Calleija, M.; Taylor, Z.;\
    \ Hung, C.; Nieto JFitch, R.; Sukkarieh, S. Real-time target detection and steerable\
    \ spray for\nvegetable crops. In Proceedings of the International Conference on\
    \ Robotics and Automation: Robotics in Agriculture Workshop,\nSeattle, WA, USA,\
    \ 9–11 May 2015.\n7.\nKongskilde. New Automated Agricultural Platform—Kongskilde\
    \ Vibro Crop Robotti. 2017. Available online: http://conpleks.\ncom/robotech/new-automated\
    \ (accessed on 14 December 2022).\nAgriculture 2023, 13, 1005\n22 of 22\n8.\n\
    Emmi, L.; Herrera-Diaz, J.; Gonzalez-De-Santos, P. Toward Autonomous Mobile Robot\
    \ Navigation in Early-Stage Crop Growth.\nIn Proceedings of the 19th International\
    \ Conference on Informatics in Control, Automation and Robotics (ICINCO 2022),\
    \ Lisbon,\nPortugal, 14–16 July 2022; pp. 411–418. [CrossRef]\n9.\nBannerjee,\
    \ G.; Sarkar, U.; Das, S.; Ghosh, I. Artiﬁcial Intelligence in Agriculture: A\
    \ Literature Survey. Int. J. Sci. Res. Comput. Sci.\nAppl. Manag. Stud. 2018,\
    \ 7, 3.\n10.\nOsinga, S.A.; Paudel, D.; Mouzakitis, S.A.; Athanasiadis, I.N. Big\
    \ data in agriculture: Between opportunity and solution. Agric.\nSyst. 2021, 195,\
    \ 103298. [CrossRef]\n11.\nYang, D.; Li, D.; Sun, H. 2D Dubins Path in Environments\
    \ with Obstacle. Math. Probl. Eng. 2013, 2013, 291372. [CrossRef]\n12.\nEmmi,\
    \ L.; Parra, R.; González-de-Santos, P. Digital representation of smart agricultural\
    \ environments for robot navigation. In\nProceedings of the 10th International\
    \ Conference on ICT in Agriculture, Food & Environment (HAICTA 2022), Athens,\
    \ Greece,\n22–25 September 2022; pp. 1–6.\n13.\nOrion Context Broker. Telefonica.\
    \ Available online: https://github.com/telefonicaid/ﬁware-orion (accessed on 22\
    \ February\n2023).\n14.\nFrancia, M.; Gallinucci, E.; Golfarelli, M.; Leoni, A.G.;\
    \ Rizzi, S.; Santolini, N. Making data platforms smarter with MOSES. Futur.\n\
    Gener. Comput. Syst. 2021, 125, 299–313. [CrossRef]\n15.\nROS—The Robot Operating\
    \ System. 2023. Available online: https://www.ros.org/ (accessed on 24 April 2020).\n\
    16.\nROSLink. 2023. Available online: https://github.com/aniskoubaa/roslink (accessed\
    \ on 5 January 2023).\n17.\nKoubaa, A.; Alajlan, M.; Qureshi, B. ROSLink: Bridging\
    \ ROS with the Internet-of-Things for Cloud Robotics. In Robot Operating\nSystem\
    \ (ROS); Koubaa, A., Ed.; Studies in Computational Intelligence; Springer: Cham,\
    \ Switzerland, 2017; Volume 707. [CrossRef]\n18.\nFiware Community Fiware: The\
    \ Open Source Platform for Our Smart Digital Future. Available online: https://www.ﬁware.org/\n\
    (accessed on 5 January 2023).\n19.\nLópez-Riquelme, J.; Pavón-Pulido, N.; Navarro-Hellín,\
    \ H.; Soto-Valles, F.; Torres-Sánchez, R. A software architecture based on\nFIWARE\
    \ cloud for Precision Agriculture. Agric. Water Manag. 2017, 183, 123–135. [CrossRef]\n\
    20.\nBochkovskiy, A.; Wang, C.Y.; Liao, H.Y.M. YOLOv4: Optimal speed and accuracy\
    \ of object detection. arXiv 2020, arXiv:2004.10934.\n21.\nHerrera-Diaz, J.; Emmi,\
    \ L.A.; Gonzalez de Santos, P. Maize Dataset. 2022. Available online: https://digital.csic.es/handle/10261/\n\
    264581 (accessed on 1 April 2023).\n22.\nHoward, A.G.; Zhu, M.; Chen, B.; Kalenichenko,\
    \ D.; Wang, W.; Weyand, T.; Andreetto, M.; Adam, H. MobileNets: Efﬁcient\nConvolutional\
    \ Neural Networks for Mobile Vision Applications. arXiv 2017, arXiv:1704.04861.\n\
    23.\nHerrera-Diaz, J.; Emmi, L.; Gonzalez de Santos, P. Wheat Dataset. 2022. Available\
    \ online: https://digital.csic.es/handle/10261/\n264622 (accessed on 1 April 2023).\n\
    24.\nWojke, N.; Bewley, A.; Paulus, D. Simple Online and Realtime Tracking with\
    \ a Deep Association Metric. In Proceedings of the\nIEEE International Conference\
    \ on Image Processing (ICIP), Beijing, China, 17–20 September 2017.\nDisclaimer/Publisher’s\
    \ Note: The statements, opinions and data contained in all publications are solely\
    \ those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or\
    \ the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury\
    \ to\npeople or property resulting from any ideas, methods, instructions or products\
    \ referred to in the content.\n"
  inline_citation: null
  journal: Agriculture (Basel)
  key_findings:
  - The proposed system provides real-time monitoring of crop growth and disease detection.
  - The system leverages deep learning-based object detection and segmentation to
    differentiate between weeds and crops.
  - The system's architecture is built upon Robot Operating System (ROS) and FIWARE,
    facilitating communication between the robot's components and the cloud.
  limitations: '>'
  main_objective: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)
  pdf_link: https://www.mdpi.com/2077-0472/13/5/1005/pdf?version=1683018153
  publication_year: 2023
  relevance_evaluation:
    extract_1: The proposed system harnesses advances in robotics and computing to
      enhance the efficiency and accuracy of agricultural practices. It seamlessly
      integrates high-resolution cameras with an advanced vision algorithm, providing
      real-time monitoring of crop growth and disease detection. Furthermore, it leverages
      deep learning-based object detection and segmentation to differentiate between
      weeds and crops, enabling targeted treatment and resource optimization.
    extract_2: The system's architecture is built upon Robot Operating System (ROS),
      a widely used platform for robotics applications, and FIWARE, an open-source
      framework for Internet of Things (IoT) and cloud computing. This integration
      facilitates communication between the robot's components and the cloud, enabling
      data exchange, analysis, and decision-making processes.
    relevance_score: 1.0
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used:
  - Robot Operating System (ROS)
  - FIWARE
  - Internet of Things (IoT)
  - Cloud computing
  - Deep learning-based object detection and segmentation
  - High-resolution cameras (e.g., multispectral, hyperspectral)
  - Computer vision algorithms
  title: Exploiting the Internet Resources for Autonomous Robots in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.knosys.2020.106723
  analysis: '>'
  apa_citation: Gao, J., Westergaard, J. C., Sundmark, E. H. R., Bagge, M., Liljeroth,
    E., & Alexandersson, E. (2021). Automatic late blight lesion recognition and severity
    quantification based on field imagery of diverse potato genotypes by deep learning.
    Knowledge-Based Systems, 214, 106723.
  authors:
  - Junfeng Gao
  - Jesper Cairo Westergaard
  - Ea Høegh Riis Sundmark
  - Merethe Bagge
  - Erland Liljeroth
  - Erik Alexandersson
  citation_count: 47
  explanation: In this study, Gao et. al. propose a deep learning-based approach to
    improve the accuracy and efficiency of potato late blight lesion segmentation
    in field conditions using RGB imagery collected from a handheld camera. They evaluated
    the performance of a deep convolutional neural network (DCNN) architecture based
    on SegNet for lesion segmentation, optimizing the class weights for training the
    network. To account for the small size of detected lesions and the acquisition
    of images at various scales, the study employed a majority voting mechanism to
    fuse lesion prediction masks from images with different scales. This approach
    achieved a higher linear relationship between visual scoring and the number of
    lesions at the canopy level, demonstrating the potential of their methodology
    for monitoring disease development and evaluating potato resistance to late blight.
  extract_1: '"We propose a SegNet-based network and determine the optimal class weight
    for training this network for accurate PLB lesion segmentation." (Page 2)'
  extract_2: '"We develop an effective optimization strategy for lesion counting at
    a canopy level based on a majority voting mechanism." (Page 2)'
  full_citation: '>'
  full_text: '>

    Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related work 3. Material and methods
    4. Results 5. Discussion 6. Conclusion and future work CRediT authorship contribution
    statement Declaration of Competing Interest Acknowledgments References Show full
    outline Cited by (56) Figures (18) Show 12 more figures Tables (4) Table 1 Table
    2 Table 3 Table 4 Knowledge-Based Systems Volume 214, 28 February 2021, 106723
    Automatic late blight lesion recognition and severity quantification based on
    field imagery of diverse potato genotypes by deep learning Author links open overlay
    panel Junfeng Gao a, Jesper Cairo Westergaard b, Ea Høegh Riis Sundmark c, Merethe
    Bagge c, Erland Liljeroth d, Erik Alexandersson d Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.knosys.2020.106723 Get rights and content Abstract
    The plant pathogen Phytophthora infestans causes the severe disease late blight
    in potato, which can result in huge yield loss for potato production. Automatic
    and accurate disease lesion segmentation enables fast evaluation of disease severity
    and assessment of disease progress. In tasks requiring computer vision, deep learning
    has recently gained tremendous success for image classification, object detection
    and semantic segmentation. To test whether we could extract late blight lesions
    from unstructured field environments based on high-resolution visual field images
    and deep learning algorithms, we collected 500 field RGB images in a set of diverse
    potato genotypes with different disease severity (0%–70%), resulting in 2100 cropped
    images. 1600 of these cropped images were used as the dataset for training deep
    neural networks and 250 cropped images were randomly selected as the validation
    dataset. Finally, the developed model was tested on the remaining 250 cropped
    images. The results show that the values for intersection over union (IoU) of
    the classes background (leaf and soil) and disease lesion in the test dataset
    were 0.996 and 0.386, respectively. Furthermore, we established a linear relationship
    (R .655) between manual visual scores of late blight and the number of lesions
    detected by deep learning at the canopy level. We also showed that imbalance weights
    of lesion and background classes improved segmentation performance, and that fused
    masks based on the majority voting of the multiple masks enhanced the correlation
    with the visual disease scores. This study demonstrates the feasibility of using
    deep learning algorithms for disease lesion segmentation and severity evaluation
    based on proximal imagery, which could aid breeding for crop resistance in field
    environments, and also benefit precision farming. Previous article in issue Next
    article in issue Keywords Plant diseaseResistance breedingConvolutional neural
    networksSemantic segmentationMulti-scale predictionMask fusionImage-based crop
    phenotyping 1. Introduction Crop diseases pose a threat to global food security
    [1]. Automated field phenotyping can become a powerful tool for future resistance
    breeding as well as for precision agriculture [2], [3], and can thus be a successful
    way to protect against crop disease. Potato is today the third most important
    food crop in the world and is an important part of many diets, especially in temperate
    climates. The oomycete Phytophthora infestans (Mont.) de Bary which causes potato
    late blight (PLB) and potato tuber blight (PTB) can be very destructive in potato
    cultivation if it is not managed [4]. In practice, the prevention of PLB in the
    field is highly relying on regular blanket spraying of fungicide during the growth
    season. As an example, in Sweden the potato production consumes around 20% of
    all fungicides used in agriculture, largely to combat P. infestans, in spite of
    occupying less than 1% of the area under cultivation [5]. In addition, PLB prevention
    requires frequent use of fungicides with sometimes more than 10 applications per
    growth season in Northern Europe to avoid significant yield loss. This management
    is effective in general and widely accepted by farmers, but also results in usage
    of large amounts of fungicide as well as fossil fuels, which hampers the sustainable
    development of agriculture. The loss caused by PLB can be reduced by breeding
    PLB resistant cultivars. To breed for high PLB resistance, plant breeders establish
    experimental plots to quantify the PLB severity of different potato genotypes
    and progeny lines. This is currently manually done by estimating visual scores
    based on the number and area of lesions on plants [6]. Visual scoring in the field
    provides a crucial metric to quantify disease severity and progression rate between
    cultivars and breeding lines under evaluation. However, this process is time consuming
    and prone to be biased leading to errors. Furthermore, it requires experienced
    raters and it is often not feasible to do daily scorings, which would otherwise
    advance the understanding of the progression rate. Therefore, the development
    of an automated disease evaluation system would be much welcomed to facilitate
    and accelerate the breeding processes. However, one of the main challenges in
    automating the system for disease scoring is to accurately segment the lesions
    under field conditions. Disease lesion segmentation is the process of segmenting
    pathological regions out of crop organs such as leaves and stems. These regions
    of interest typically occupy a very small fraction of the full images acquired
    from a crop field, particularly in early stages of plant development. In field
    environments, the captured images often contain soil background regions, sharing
    similar color and morphological features as disease lesions. To tackle these difficulties,
    many efforts have been made, including intensity threshold analysis, region growth
    algorithms, spectral indices and machine learning algorithms [7]. However, these
    approaches heavily rely on the hand-crafted features determined by experts. These
    features might also have limitations with regards to representation and generalization
    ability. Deep learning has proved to be an effective approach for traditional
    computer vision problems such as image classification, object detection and segmentation,
    since it is capable of extracting features hierarchically [8]. In addition, the
    applications with deep learning in agriculture also showed recent unprecedented
    advancements [9]. Specifically, in precision farming it has been deployed for
    weed detection [10], agricultural pest detection [11] and selective fruit harvesting
    [12], as well as leaf counting [13]. Furthermore, deep convolutional neural networks,
    one of the most used deep learning algorithms, combined with computer vision techniques
    have been exploited for crop disease classification and detection. Polder et al.
    [14] adapted a fully convolutional neural network (FCN) for potato virus Y (PVY)
    detection based on hyperspectral imagery. It demonstrated that the approach based
    on deep learning achieved good recall results and indicated the suitability of
    this method for field disease detection. Stewart et al. [15], [16], [17] developed
    deep neural networks for northern leaf blight (NLB) in maize from field RGB images
    collected from an unmanned aerial vehicle (UAV) platform. By contrast, the quantification
    and detection of PLB lesions have still been confined to images acquired in the
    laboratory or greenhouse. To the best of our knowledge, the use of deep convolutional
    neural networks has yet to be fully investigated for PLB lesion segmentation in
    diverse potato genotypes based on RGB imagery from the field. The specific objectives
    of this study are (1) to evaluate the performances of deep convolutional neural
    networks for PLB lesion segmentation; (2) to determine the optimal class weights
    for the classes PLB disease lesion and background (i.e., leaf and soil); (3) to
    fuse prediction masks at multiple scales for more accurate lesion prediction;
    (4) to determine the correlation between visual scoring and the number of lesions
    at the canopy level. The early pre-symptom PLB detection is outside the scope
    of this study as only RGB and no hyperspectral images were analyzed. 2. Related
    work Some studies have already explored lesion segmentation for detection of plant
    disease. For example, Abdu et al. [18] developed a pattern recognition approach
    to recognize early blight, caused by Alternaria solani, and PLB visual disease
    symptoms using soft computing and machine learning algorithms. Barbedo [19] and
    Camargo et al. [20] also carried out similar studies on plant disease detection
    and segmentation. However, the image datasets used in these studies were collected
    at foliage level under relatively clear and uniform backgrounds in controlled
    settings and the pipelines are not readily implemented into large scale field
    environments. Moreover, the majority of previous works have only investigated
    the symptom segmentation problems based on a single potato cultivar. These models
    might fail to segment other lesions due to variation in lesion morphology and
    leaf color between different potato cultivars. Attempts have also been made for
    early detection of PLB. Various sensors from imaging to non-imaging sensors have
    been employed in these applications. For example, Fernández et al. [21] investigated
    the classification accuracy changes of infested and healthy potato leaves over
    different days post-inoculation (DPI) with a spectroradiometer and a multispectral
    camera under structured environments. Appeltans et al. [22] discussed the setting
    of imaging parameters for hyperspectral and thermal proximal disease sensing in
    potato and leek fields with a ground-based vehicle. Other than ground-based platforms,
    an unmanned aerial vehicle (UAV) platform equipped with imaging sensors has also
    showed its feasibility on field PLB monitoring with the detection of spectral
    changes in crop traits, including multispectral imagery [23], [24], [25]. There
    are some previous publications for quantification of plant disease severity. Image-based
    analysis including images from RGB, multispectral and hyperspectral sensors under
    controlled conditions [26] or based on PLACL (Percentage of Leaf Area covered
    by Lesions) at single leaf level [27] exists, but is yet to demonstrate its full
    potential for accurate estimation in field environments [28]. Towards accurate
    PLB lesion segmentation under field conditions, our approach is based on deep
    learning algorithms. The main contributions of this study are three-fold: (1)
    We propose a SegNet-based network and determine the optimal class weight for training
    this network for accurate PLB lesion segmentation. (2) We develop an effective
    optimization strategy for lesion counting at a canopy level based on a majority
    voting mechanism. (3) We quantify the relationships between the number of predicted
    lesions and the visual scores estimated by experts. 3. Material and methods The
    research methodology of this study includes three main procedures: Image data
    collection, deep neural network model development, and lesion counting and severity
    quantification. A schematic overview of this study is displayed in Fig. 1. Download
    : Download high-res image (919KB) Download : Download full-size image Fig. 1.
    Schematic overview of study. The developed neural network was trained by the cropped
    images (512 × 512) obtained from raw field images and then cut using multiple
    scales. The final prediction was obtained by majority voting of all scales. 3.1.
    Image data collection The images were acquired with a hand-held RGB camera (Sony
    RX 100 iii) in nadir ( 5 degrees) at approximately 40 cm over each canopy. The
    ISO, aperture and FOV of the camera were set to 125, f/5.6 and 8.8 mm, respectively.
    Images were acquired in full cloudy, semi-cloudy or sunny light settings. No flash
    was used. Especially for the semi-cloudy and sunny acquisitions, consideration
    was taken to ensure no additional shade was being cast onto the canopy from person
    or camera. No post-processing regarding color correction was performed. The field
    location was outside Give, Denmark (N 55.859188, E 9.331065). Fig. 2 shows a single
    image of the field from 100 meters above ground, at 15 Days After Infection (DAI).
    It is clearly seen how a large part of the trial field is decimated by PLB. The
    trial was set up with guard rows of Oleva (cv.) around the trial area. Each plot
    within the trial consisted of 4 plants in a 2 × 2 formation with infector rows
    on each side and a 50 cm gap between plots. The total trial consisted of 775 genotypes:
    47 genotypes with three replicates, 217 genotypes with two replicates and 511
    genotypes represented by single plots. Infector rows consisted of alternating
    Bintje (cv.) and Oleva (cv.). Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 2. A bird’s-eye view of the experimental trial
    taken from a drone platform at 100 m above ground (a); an image example at 15
    DAI (b). 3.2. Manual visual scoring of PLB Manual visual scoring was done 2 times
    a week, starting 5 days after inoculation of infector rows and continuing until
    the standard cultivar Robijn had reached 50% infection. Infection of PLB at each
    time point was scored as percentage of leaf area infected according to the Euroblight
    protocol [29] with a single lesion per plot scored as 0.1%, 2–5 lesions scored
    as 0.5%, 5–10 lesions scored as 1%. The manual detection of lesions was done by
    walking between the two rows of each plot at a slow pace and looking at the plants
    at an angle. If a lesion was spotted, it was further investigated for PLB presence
    and the plot was investigated for presence of more lesions. The percentage of
    infection was used to create an area under disease progression curve (AUDPC) as
    follows: (1) where T is inoculation date and T  is evaluation dates (i 0, 1, 2,
    3). A is the percentage of infection at T . 3.3. Image preprocessing There are
    70 original images (5472 × 3648) which were all labeled manually with the annotation
    toolbox LabelMe [30]. The distribution of genotypes among these images is shown
    in Fig. 3. The majority of images are from the Bintje and Oleva potato cultivars.
    These two genotypes are susceptible to P. infestans, providing more disease lesions
    for training. The original image is unable to be directly fed to neural networks
    as the spatial resolution of original images is too large and requires intensive
    computation memories. It is also not advisable to shrink the whole image to a
    small size, making processing possible but heavily degrading the quality of small
    features in tiny lesion annotations. There are 70 original images and the same
    amount of corresponding ground truth images. The following procedures were applied
    to both. Each original image was cut into 6 × 5 (horizontal × vertical) sub-images
    (912 × 730) and then each sub-image was resized to 512 × 512 in order to be fed
    into the neural networks for training. Each original image contributes 30 sub-images
    leading to 2100 sub-images in total. 1600 sub-images were randomly selected for
    the training set and the remaining 500 sub-images were randomly separated equally
    as the validation set (250 sub-images) or the test set (250 sub-images). Download
    : Download high-res image (361KB) Download : Download full-size image Fig. 3.
    Histogram of raw image genotypes, the red bar represents the number of images
    belonging to that genotype. 3.4. Deep learning We adopted an encoder–decoder neural
    network architecture based on SegNet [31] for lesion segmentation. The proposed
    network operates on input images of 512 × 512 pixels and outputs segmentation
    masks in the same size as the input image. The architecture has an hourglass shape
    consisting of a bunch of convolutional and up-convolutional layers. A diagrammatic
    overview of the network is shown in Fig. 4. The encoder (Table 1) comprises a
    series of convolutional operations, activation and pooling operations. Semantic
    features from low-level to high-level were extracted at the end of the encoder
    process. Because of max-pooling layers, the spatial resolution output shrunk 2
    times compared to the previous convolution block in the encoder phase. The index
    of the maximum feature value in each pooling window was recorded for each encoder
    feature map (Fig. 5). In contrast, the decoder part (Table 1) upsamples its input
    feature maps using the recorded max-pooling indices learned from the corresponding
    layers in encoder part and finally generates the prediction results with same
    spatial size as original input images. The upsampling layers, such as deconvolution
    layers, in the decoder are also capable of learning a mapping from feature map
    to semantic segmentation results. Each convolutional layer is followed by several
    operational layers including a non-linear ReLU activation function (max(0, x))
    and batch normalization. The total parameters are 29,442,122 with 29,434,694 trainable
    parameters and 7428 non-trainable parameters. Download : Download high-res image
    (403KB) Download : Download full-size image Fig. 4. The neural network is based
    on an encoder and decoder architecture, followed by a final pixel-wise classification
    layer (red circles highlight disease lesions). Download : Download high-res image
    (203KB) Download : Download full-size image Fig. 5. Example of max-pooling and
    upsampling with index in SegNet. Table 1. Details of the proposed deep neural
    network architecture. Filter size is 3 × 3 and padding mode is set as ‘same’ with
    zero-filled. The final decoder output is fed to a softmax classifier to produce
    class probabilities for each pixel. Module Layers Dimensions Feature maps Block
    structure Input Input (RGB) 512 × 512 3 En-coder Convolution 1 block 256 × 256
    64 2 × convolution pooling Convolution 2 block 128 × 128 128 2 × convolution pooling
    Convolution 3 block 64 × 64 256 3 × convolution pooling Convolution 4 block 32
    × 32 512 3 × convolution pooling Convolution 5 block 16 × 16 512 3 × convolution
    pooling De-coder Up convolution 1 block 32 × 32 512 3 × convolution upsampling
    Up convolution 2 block 64 × 64 256 3 × convolution upsampling UP convolution 3
    block 128 × 128 128 3 × convolution upsampling UP convolution 4 block 256 × 256
    64 2 × convolution upsampling UP convolution 5 block 512 × 512 64 2 × convolution
    upsampling Output Softmax 512 × 512 2 Convolution sigmoid 3.5. Loss function The
    loss function is key for training a robust and high-performance network. The most
    commonly used loss function for semantic segmentation is pixel-wise cross-entropy
    loss. In our study, the frequency of appearance for lesion and background class
    is highly imbalanced. The number of disease lesion pixels is far less than other
    pixels such as healthy plant organs and soil backgrounds in field images at early
    infection stages. Using just a standard loss function without adaption would make
    a deep neural network model tend to only correctly classify dominant class pixels
    (backgrounds), ignoring the importance of lesion pixels. This is also called the
    accuracy paradox, which is a model that provides a very high overall accuracy
    but performs poorly over classes. One common way to mitigate this effect is the
    use of a class-balancing approach by assigning different weights over classes
    based on their median frequency [32]. In this study, we regard the weights for
    each class as a hyperparameter to tune. Other than weights calculated from Eq.
    (2), we also compared the prediction performance with different weight ratios
    from 1 to 9 to select the optimal weight. (2) where frequency(c) represents the
    frequency of occurrences of pixels of class c divided by the total number of pixels
    in any images containing that class, and median_frequency is the median of these
    frequencies overall all classes. The weighted loss function used in the network
    is shown in Eq. (3) below. (3) where N is the number of observations, C is the
    number of classes (background and lesion), is the weight for class c, y is a binary
    indicator (0 or 1) if a class label is correctly classified for observation o,
    and p is predicted probability of observation o being of class c. 3.6. Training
    The network was trained end-to-end from scratch with the Adam optimizer [33] using
    a stable learning rate of 0.0001 to minimize the loss values. The batch size was
    set to 18 with 500 epochs in total. We employed 3 Nvidia Tesla V100-SXM2 GPUs
    with around 32G memory each for training the network. Each epoch took around 171
    s to finish, accounting for 23.75 h of training time in total. Data augmentation
    was used to reduce the risk of overfitting in the training phase. Specifically,
    in each batch, cropping, horizontal or vertical flipping, and a zoom range from
    0.8 to 1.2 were randomly applied in the images and their corresponding ground
    truth masks. All network training and validation were done using the Tensorflow
    deep learning framework. The model was saved only with the decline of loss values
    in each epoch. The accuracy and loss values in the validation dataset were recorded
    as well in every epoch. 3.7. Model evaluation The model was evaluated with three
    standard metrics for semantic segmentation. The three metrics are overall average
    accuracy, class average accuracy and mean intersection over union (mIoU), respectively.
    The calculations are listed below. We also used confusion matrix to check how
    many pixels of each class are correctly classified. Overall average accuracy (Calculation
    (4)) measures the overall performance for all pixels. The high value means that
    the majority of pixels are correctly classified but it does not indicate good
    lesion segmentation as a majority of pixels in our image are of the background.
    Hence, this value is sometimes quite biased for evaluating model performances.
    Class average accuracy (Calculation (5)) averages the performance of each class.
    A high value represents good performance across all classes. IoU (intersection
    over union), also known as the Jaccard index, is a commonly used and effective
    metric in semantic segmentation. It measures the area of overlap between the predicted
    segmentation and the ground truth divided by the union area of the predicted segmentation
    and the ground truth in labeled images. mIoU is calculated by averaging the IoU
    of each class (Calculation (6)). (4) (5) (6) where is the total number of pixels
    of class i in ground truth image, is the number of pixels of class j predicted
    as belonging to class i and c is the total number of classes. Table 2. Pseudo-codes
    of fused mask generation based on a majority voting approach. Algorithm: Generic
    pseudo-code of lesion counting 1: Input: test dataset 2: For image in test dataset
    do: 3:  for i 3 to 9 do: 4:  cut image horizontally i and vertically i-1, i ×
    (i-1) sub-images generated in total; 5:  resize sub-images (512 × 512); 6:  predict
    each subimage with the optimal model, i × (i-1) sub-masks obtained in total; 7:  align
    i × (i-1) sub-masks/sub-images and reconstruct masks/images; 8:  resize masks/images
    in original size; 9:  save masks/images (each image has 7 corresponding masks);
    10:  end 11:  obtain class label in each pixel of an image by the majority vote
    of the class labels in its  corresponding pixels of its 7 masks; 12: Output: count
    the number of lesions in each image; 3.8. Post-processing Fully connected conditional
    random fields (FCCRFs) [34] were used for post-processing the predicted masks.
    It combines single pixel prediction and shared structure through unary and pairwise
    terms to improve smoothness and to maximize agreement between similar neighboring
    pixels. The FCCRFs establish pairwise potential by using a Gaussian function on
    all pixel pairs in an image. The main benefits of using FCCRFs are determining
    the optimal decision boundary at conflict regions of pixels, while not having
    notable negative effects on successfully segmented pixels. For post-processing
    of images, the prior knowledge of P. infestans disease lesion area is relatively
    small in an early infection stage was used. We found that some false positive
    areas likely represent a shadowed area between leaves (Fig. 10). But since these
    false lesion areas are far larger than normal lesions appeared at that early infection
    stage, a simple threshold algorithm could be operated to filter out some false
    positives in the test images. We set a reasonable lesion area range to be [50,10000]
    pixels to exclude the extreme false positives. Furthermore, the canopy heights
    and structures of potato plants vary, which means that the same lesion spots could
    be displayed differently in 2D images, which in turn can lead to failed predictions.
    For some failed lesions the network can successfully predict the lesions at a
    different scale. A majority voting approach for lesion counting was proposed based
    on multiple prediction masks from various scales. The generic pseudo-code is listed
    in Table 2. Each image for prediction was cropped into sub-images at 7 multiple
    scales from 3 × 2 to 9 × 8 (e.g. 3 × 2 scale represents splitting the image into
    6 parts by dividing the image into 3 parts horizontally and into 2 parts vertically).
    Specifically, each image was cropped at 3 × 2, 4 × 3, 5 × 4, 6 × 5, 7 × 6, 8 ×
    7, 9 × 8 scales in horizontal x vertical directions. The sub-images with the same
    scales were predicted separately by the model, resulting in 7 predictions for
    each image. The final prediction mask of an image is obtained based on the majority
    voting of its 7 prediction masks. 4. Results 4.1. Network training Overall, the
    training loss and validation loss decreased with the increment in training time
    (Fig. 6). The validation loss fluctuated much in the early training stage ( 150
    epochs) and then slowly converged at 0.0626 at the end of the training. By contrast,
    the training loss smoothly dropped until the end of training and finally converged
    at 0.0398, slightly lower than the final validation loss (0.0626). It can also
    be observed that both the training loss and validation loss were substantially
    stable after 450 epochs, indicating that the model stopped improving on a hold-out
    validation dataset. The model weights were saved at 450 epochs to prevent the
    risk of overfitting. At this epoch point, the overall accuracy values in the training
    and validation datasets were 0.9962 and 0.9945, respectively. The same procedures
    were followed when training other models with different hyperparameters (the weight
    ratio of lesion and background) for performance comparison in the test dataset.
    Download : Download high-res image (122KB) Download : Download full-size image
    Fig. 6. Loss curves of the network (1:7 weight ratio) in the training and validation
    datasets. 4.2. The weight ratio of two classes (lesion and background) The weight
    ratio of lesion and background classes is one of the important hyperparameters
    needed to be fine-tuned. In this study, we investigated 13 groups of weight ratios
    in order to determine the optimal weight ratio for lesion semantic segmentation.
    One of the weight ratios (1:2.5) was obtained based on the median frequency of
    two classes and the remaining weight ratios were ranging from 1 to 12. The metrics
    in the validation dataset are shown in Table 3. It shows that the imbalance weights
    can effectively improve the segmentation performance as all mIoU values exceed
    0.65 compared to the 0.551 obtained when the weight ratio was set to 1:1. Interestingly,
    the mIoU value does not continue to increase with a larger weight ratio ( 7).
    The maximum mIoU value was achieved with a 1:7 weight ratio. We selected the model
    with this weight ratio as the optimal model for lesion segmentation. Table 3.
    Metrics of the models with different class weight ratios in the validation dataset.
    Weight ratio Overall accuracy Class average accuracy IoU (background) IoU (lesion)
    mIoU 1:1 0.998 0.671 0.854 0.248 0.551 1:2 0.999 0.743 0.997 0.361 0.679 1:2.5
    0.999 0.789 0.997 0.371 0.684 1:3 0.998 0.802 0.996 0.345 0.671 1:4 0.999 0.810
    0.997 0.400 0.698 1:5 0.999 0.799 0.996 0.337 0.618 1:6 0.999 0.833 0.997 0.395
    0.696 1:7 0.998 0.804 0.997 0.401 0.699 1:8 0.998 0.817 0.997 0.397 0.697 1:9
    0.997 0.841 0.996 0.372 0.684 1:10 0.998 0.857 0.997 0.397 0.697 1:11 0.998 0.814
    0.997 0.397 0.697 1:12 0.998 0.816 0.997 0.376 0.687 4.3. Prediction in the test
    dataset Similar to the image process for training, the original test images (5472
    × 3648) were first cropped and then resized to sub-images (512 × 512) for prediction.
    We used the model with 1:7 weight ratio as the optimal model to test the images.
    Fig. 7 shows a confusion matrix in the validation dataset (a) and in the test
    dataset (b). The IoU values of background and lesion classes in the test dataset
    are 0.996 and 0.386, respectively. The metrics in the test dataset are lower than
    in the validation dataset. Most background pixels (99.8%) are correctly classified
    from the confusion matrix. As the majority of pixels are leaf, belonging to background
    class, they can be easily classified based on the color differences with lesion
    class. Around 40% of lesion pixels were classified as being background class in
    the test dataset. The prediction examples are illustrated in Fig. 8. Generally,
    most lesions, marked as the red areas in the images, can be correctly segmented.
    Some tiny lesions were not manually labeled on the ground truth images, but were
    successfully segmented by the model (shown in #2 and #4 columns in Fig. 8). The
    prediction masks of sub-images were reconstructed back to the predicted images
    of the original test images (5472 × 3648). Two examples of the predicted images
    are shown in Fig. 9. There are some examples of failed cases in some predictions.
    For example, no disease lesions were visually observed on potato leaves (Fig.
    10). But 3 lesion areas were predicted by the model. The three false positives
    are all from soil patches which largely have similar shape features and areas
    as seen in typical lesions. Moreover, these soil patches are surrounded by leaves,
    resulting in more confusion for inference. Also, other wrongly predicted cases
    are located in the image border (#1 column in Fig. 8). An entire lesion can be
    cut up into two pieces when cropping an image into multiple sub-images. In this
    case, the partial lesion significantly changes the morphological features and
    loses the important neighbor pixel information for the model to predict. This
    might lead to these failure cases. Download : Download high-res image (190KB)
    Download : Download full-size image Fig. 7. Confusion matrix in the validation
    dataset (a) and in the test dataset (b). Download : Download high-res image (2MB)
    Download : Download full-size image Fig. 8. Examples of sub-image predictions
    (512 × 512) in the test dataset (row #1: raw sub-images, row #2: ground truth,
    row #3: predicted images). (For interpretation of the references to color in this
    figure legend, the reader is referred to the web version of this article.) Download
    : Download high-res image (2MB) Download : Download full-size image Fig. 9. Examples
    of the predicted raw images (5472 × 3648) in the test dataset and ground truth
    images (Left column: ground truth images, Right column: predicted images). Download
    : Download high-res image (2MB) Download : Download full-size image Fig. 10. False
    positives in a test image (5472 × 3648). 4.4. Model comparison We compared our
    network model with other deep learning-based models. The results are listed in
    Table 4. The optimal class weight (1:7) obtained by the experiment above (Table
    3) was used for training the other networks as well. We also tested a few representative
    images from the test dataset with conventional K-Means clustering. To be consistent
    with deep learning-based algorithms (two classes), we assigned K to be 2 for comparison.
    The segmentation results are displayed in Fig. 11. It shows that this method failed
    to recognize lesions in field images with too many false positives from soil background
    and dark leaves. The major issue for the K-Means clustering algorithm is a lack
    of utilizing spatial information for lesion recognition in our case. The other
    three models all outperform the baseline model FCN. The proposed SegNet-based
    network with the optimal class weight (1:7) obtained slightly better results in
    mIoU compared to PSPNet and DeepLab. Table 4. Results on the test dataset. Methods
    Overall accuracy Class average accuracy IoU (background) IoU (lesion) mIoU FCN
    [35] 0.998 0.789 0.994 0.298 0.644 PSPNet [36] 0.999 0.799 0.996 0.384 0.690 DeepLab
    [37] 0.999 0.795 0.996 0.379 0.688 Proposed 0.999 0.800 0.996 0.386 0.691 Download
    : Download high-res image (1MB) Download : Download full-size image Fig. 11. Segmentation
    results based on K-Means clustering (row #1: raw sub-images, row #2: ground truth,
    row #3: results based on K-Means clustering). 4.5. Model validation in negative
    examples We also tested the generalization ability and validity of the model with
    some difficult images (negative examples) without P. infestans lesions but with
    tissue damages caused by biotic or abiotic stresses. These images were collected
    from different potato fields in the summer of 2020. The model did not predict
    any lesions in these images in Fig. 12. These damages were from various sources
    such as fertilization, herbicide and pathogens other than P. infestans. Fig. 13
    shows that the model failed to recognize the P. infestans lesions. Specifically,
    some lesions from Alternaria solani (Early Blight) were recognized as being P.
    infestans lesions. However, the model did not recognize damages in stems caused
    by leaf mold as P. infestans lesions, though they have similar color features
    as P. infestans lesions. Download : Download high-res image (1MB) Download : Download
    full-size image Fig. 12. Correct prediction samples with damages of burning by
    lime nitrate (a); with deformity and necrosis caused by herbicide damage (b);
    with damage caused by eutrophication with lime nitrate (c); with infestation of
    possible gray mold (d). Download : Download high-res image (1MB) Download : Download
    full-size image Fig. 13. Failure cases of P. infestans lesion recognition where
    the infections are caused either by some abiotic stress or other fungi (Left);
    and in an image with severe damages caused by leaf mold infestation (Right). 4.6.
    Correlation between visual scores and the number of lesions at the canopy level
    As the visual scores were obtained based on the rating of the whole plant, we
    only selected the unseen images that covered the full crop canopy to avoid bias
    due to partial view. There were 43 images selected in total. The histogram of
    visual scores is shown in Fig. 14. The average value of visual scores is 3% ranging
    from 0 to 40%. In order to minimize the failure cases raised from cropping, each
    image was predicted with multiple scales. The number of lesions in each image
    was obtained based on the majority voting algorithm described in the section of
    post-processing. The histogram of the final detected lesions is displayed in Fig.
    15. There were 1063 lesions detected in these test images. The mean value of the
    detected lesion areas was 892 pixels. The maximum and minimum values are 7520
    and 52 pixels, respectively. More than 40% of lesion areas are below 500 pixels,
    and very few lesion areas exceed 6000 pixels. This is consistent with the visual
    scores where around 80% of the scores are below 5%. A linear model was fitted,
    to quantify the relationship between visual scoring by an experienced plant breeder
    from the Danespo company and the number of lesions that appeared at canopy level.
    Fig. 16, Fig. 17 show the fitted linear relationships between visual scores and
    the number of lesions predicted from the 3 × 2 and 5 × 4 partitioning scales,
    respectively. Fig. 18 illustrates the fitted linear relationship between visual
    scores and the number of lesions obtained from majority voting of all scales.
    Compared to prediction masks with only one scale, the fused masks based on majority
    voting achieved a better linear relationship by increasing the R2 value from around
    0.4 to 0.655. Download : Download high-res image (346KB) Download : Download full-size
    image Fig. 14. Histogram of visual scores; the red bar represents the number of
    visual scores in that x axis range (left y axis), and the blue line represents
    a cumulative percentage of visual scores (right y axis). Download : Download high-res
    image (334KB) Download : Download full-size image Fig. 15. Histogram of lesion
    areas; the red bar represents the number of lesion areas in that x axis range
    (left y axis), and the blue line represents a cumulative percentage of lesion
    areas (right y axis). Download : Download high-res image (189KB) Download : Download
    full-size image Fig. 16. Fitted linear relationship between visual scores and
    number of lesions assigned from images from the 3 × 2 partitioning scale. Download
    : Download high-res image (178KB) Download : Download full-size image Fig. 17.
    Fitted linear relationship between visual scores and number of lesions assigned
    from images from the 6 × 5 partitioning scale. Download : Download high-res image
    (179KB) Download : Download full-size image Fig. 18. Fitted linear relationship
    between visual scores and number of lesions from majority voting from all partitioning
    scales. 5. Discussion Deep learning has demonstrated its superior performance
    on disease detection in field settings compared to conventional machine learning
    methods [15], [17]. We also demonstrate that a deep learning-based approach (Fig.
    8) generates better results than K-Means clustering (Fig. 11) for lesion segmentation
    in fields. The proposed SegNet-based network outperforms the PSPNet and DeepLab
    networks in our test dataset (Table 4), even though the PSPNet is capable of utilizing
    global spatial information for semantic segmentation. One possible reason is that
    the class weight used for all network trainings is fine-tuned for the proposed
    SegNet-based network. The datasets used for training and testing are still limited,
    and the rank of networks might also be influenced when the dataset is expanded
    to larger numbers of samples. Imbalance classes are a common problem in deep learning
    and is widely represented in the agricultural field, and thus hampers applications
    in for example distinction between weed and crop [10], [38], pest detection [11]
    and disease segmentation [15]. In these cases, pixels of one class, generally
    soil or crop, are dominant in images. Training a network in an appropriate way
    is critical for delivering a good segmentation result for each class. Table 3
    shows that assigning imbalance weights in the loss function can effectively mitigate
    the issue with imbalance classes, which is consistent with the conclusion by Yasrab
    et al. (2019) [39]. The mIoU value is only 0.551 with same weights across classes,
    while it can be improved to nearly 0.7 with an imbalanced weight assignment. The
    IoU value ( 0.99) of the background class (soil and crop plant) is far higher
    than the IoU value of disease lesion class. Based on the confusion matrix, it
    is concluded that the majority of lesion pixels ( 60%) were correctly classified.
    The relatively low IoU of lesion class ( 0.5) indicates that a few cases of false
    positives (Fig. 10) were from soil patches, which have similar shapes to lesions
    that were predicted as lesions by the model. Stewart et al. [15] also found this
    kind of false positives for Northern Leaf Blight (NLB) lesion segmentation in
    maize fields. Besides, senesced leaves could also contribute to the false positives.
    Setting a threshold to filter out some false positives, based on lesion size,
    might improve the IoU value of lesion class. This threshold value can be estimated
    based on the prior knowledge of the maximum lesion area in certain developmental
    stages of PLB disease or plant development, related to the start of the outbreak
    or even disease prediction based on weather and cultivar. It should, however,
    be noted that a lesion size threshold is of course dependent on images that are
    collected at a fixed height and a lesion size threshold will only work in the
    early stages of the infection as lesions will merge into larger areas as the disease
    progresses. This could further influence the correlation of visual scores and
    number of lesions. It is expected that this correlation can be improved when the
    estimated visual scores of the infested plants range from 5% to 20%. We drew a
    group of connected key points to define the lesion area when manually labeling
    images with the tool LabelMe. This way of annotating speeds up the pixel-wise
    labeling process for segmentation. However, it is difficult to always draw a very
    accurate boundary line with those key points especially since a majority of PLB
    lesions in our images are tiny and have an irregular shape. As a consequence,
    the labeled lesion areas at times inevitably include pixels belonging to the background
    class (soil and leaf), leading to the relatively low IoU of lesion class. To overcome
    this problem, Wiesner-Hanks et al. [16] discussed using crowdsourced data for
    NLB lesion detection at millimeter-level based on aerial visual images and concluded
    that increasing the number of workers per image could improve the quality of annotation
    polygons. Image preprocessing is essential before feeding images to train a neural
    network. It is encouraged to randomly apply blur, contrast and brightness as data
    augmentation for the benefit of model robustness. The training dataset with 1600
    labeled images from 31 potato genotypes is still limited and unlikely to include
    all lesion variations. We tested the generalization ability and validity of the
    model in recognition of P. infestans lesions with some images from other fields
    and conditions (Fig. 12). The failure cases featured highly similar color and
    morphological characteristics as P. infestans lesions still are represented. Including
    such failed images in the training dataset again can improve the performance [40].
    The creation of further lesion variations in the training datasets by synthetic
    lesion images could be a good way to improve model performances . To this end,
    Sun et al. [41] developed a conventional image processing algorithm to optimize
    synthetic lesion images obtained from a generative adversarial network (GAN).
    Cap et al. [42] proposed a LeafGAN algorithm for lesion image generations, which
    improved the diagnostic performance by 7.4%. The use of a majority voting to generate
    accurate lesion masks from multiple prediction scales, inspired from random forest
    machine learning algorithm described in [43], proved its effectiveness to establish
    a linear relationship between visual scores and number of lesions at canopy level
    (Fig. 18). Very few studies have tried to automatize the visual scoring in field
    environments for plant breeding. In reality, visual scores are evaluated based
    on the number of lesions and their areas on single leaflets at early infection
    stages, which brings difficulties with analysis based on 2D images at the canopy
    level. The lesion recognition is suggested to be further explored by three-dimensional
    (3D) imaging to obtain full plant structures and by employing the state-of-the-art
    network architectures [44], [45] for video semantic segmentation for real-time
    evaluation. In precision farming, it is necessary to detect PLB disease as early
    as possible to bring in appropriate measurements to avoid yield loss. As only
    RGB images were used in this analysis, pre-symptomatic detection of PLB is inevitable
    missed. For pre-symptomatic crop disease detection, hyperspectral measurements
    from spectroradiometers or spectral imaging sensors are generally employed [46].
    For example, Anderegg et al. [47] used an ASD FieldSpec spectroradiometer (350–2500
    nm) to measure wheat plants at a canopy level for Septoria Tritici Blotch (STB)
    disease detection and quantification. Gold et al. [48] measured contact leaf reflectance
    with a field spectrometer for pre-symptomatic PLB detection. For many applications
    [49], [50], [51], spectral imaging sensors are more popular than non-imaging hyperspectral
    sensors due to the additional capability of providing spatial information on shape,
    texture and color. Partial least square discriminant analysis (PLSDA) is generally
    used to process full spectral data [52]. Our study also has the potential to monitor
    the development of PLB disease after lesion appearance, which could be useful
    to screen for high PLB resistance potato genotypes from a diverse germplasm in
    precision breeding. In terms of PLB management in potato production, it is useful
    to acknowledge how early PLB should be detected after the appearance of lesions.
    To address this question, Wiik et al. [53] carried out trials over two years in
    2018 and 2019 to test the need of first intervention after the first visual symptoms
    were detected. The preliminary results showed that it is acceptable for farmers
    to apply a first spray with curative systematic fungicides after the discovery
    of first symptoms, corresponding to a very low 0.01% infection provided that it
    is sprayed more or less immediately, as it was shown that a first spray, delayed
    by 5 days later than first symptoms appear, was too late to stop the disease.
    0.01% infection corresponds to only 300 spots/ha if the disease is evenly spread
    over the field. Thus it is clear that for precision farming protection against
    PLB using only an RGB sensor, instead of hyperspectral sensing, requires very
    early detection of the symptoms. 6. Conclusion and future work In this study,
    we demonstrated the feasibility of using a deep learning algorithm based on an
    encoder–decoder architecture for potato late blight disease lesion semantic segmentation
    based on field images. The results show that the intersection over union (IoU)
    values of background (soil and leaf) and lesion classes in the test dataset are
    0.996 and 0.386, respectively. Assigning different weights for the imbalance class
    could improve the performance of the model. This work also presents the possibility
    of accurate lesion counting at the plant canopy level with the use of image alignment.
    A linear relationship between visual scoring and the number of lesions was established.
    We can also conclude that the fused masks obtained from majority voting of the
    masks predicted with multiple scales achieved a higher R2 value (0.655) compared
    to prediction with a single scale. The proposed methodology has the potential
    to monitor the lesion development under field conditions and evaluate the resistance
    of genotypes against potato late blight enabling more precise and automated potato
    breeding. This study will be followed up by further field tests and the model
    will continue to be tested in terms of robustness and accuracy by adding new field
    image datasets. The updated model will also be used to test images collected from
    different time points to predict area under disease progression curve (AUDPC).
    In addition, we will continuously update the models with new labeled datasets
    and synthetic images to improve the generalization ability. Multiple imaging sensors
    like multispectral and hyperspectral cameras hold promise to also detect and maybe
    even quantify pre-symptomatic disease. Also, the sensor combinations, e.g., a
    spectroradiometer (early stage) and high-resolution RGB camera (late stage), can
    be considered for monitoring PLB progression. Multimodal data fusion and machine
    learning are suggested to be fully exploited for this application. Furthermore,
    it would be interesting to explore new vehicle and sensor techniques to build
    three-dimensional imaging to be able to detect disease lesions below the canopy.
    CRediT authorship contribution statement Junfeng Gao: Conceptualization, Methodology,
    Software, Writing - original draft, Writing - review & editing. Jesper Cairo Westergaard:
    Data curation, Writing - review & editing. Ea Høegh Riis Sundmark: Resources,
    Writing - review & editing. Merethe Bagge: Resources, Writing - review & editing.
    Erland Liljeroth: Data curation, Writing - review & editing. Erik Alexandersson:
    Supervision, Project administration, Funding acquisition, Writing - review & editing.
    Declaration of Competing Interest The authors declare that they have no known
    competing financial interests or personal relationships that could have appeared
    to influence the work reported in this paper. Acknowledgments We acknowledge the
    Flemish Supercomputer Center (VSC) for providing the GPU computational resources
    and services for this work. We thank Mathieu Gremillet for field assistance, Hanne
    Grethe Kirk at Danespo for visual scoring of disease, and Linnea Almqvist from
    SLU for providing image examples in Fig. 12, Fig. 13. This research was founded
    by Nordic Council of Ministers, Copenahgen, Denmark (PPP #6P2), NordForsk, Norway
    (#84597) and Vinnova, Sweden (#2016-04386). References [1] Savary S., Ficke A.,
    Aubertot J.N., Hollier C. Crop losses due to diseases and their implications for
    global food production losses and food security Food Secur., 4 (2012), pp. 519-537,
    10.1007/s12571-012-0200-5 View in ScopusGoogle Scholar [2] Chawade A., Van Ham
    J., Blomquist H., Bagge O., Alexandersson E., Ortiz R. High-throughput field-phenotyping
    tools for plant breeding and precision agriculture Agronomy, 9 (2019), 10.3390/agronomy9050258
    Google Scholar [3] Mahlein A.K. Plant disease detection by imaging sensors – Parallels
    and specific demands for precision agriculture and plant phenotyping Plant Dis.,
    100 (2016), pp. 241-254, 10.1094/PDIS-03-15-0340-FE View in ScopusGoogle Scholar
    [4] Wiik L., Rosenqvist H., Liljeroth E. Study on biological and economic considerations
    in the control of potato late blight and potato tuber blight J. Hortic., 05 (2018),
    10.4172/2376-0354.1000226 Google Scholar [5] Eriksson D., Carlson-Nilsson U.,
    Ortíz R., Andreasson E. Overview and breeding strategies of table potato production
    in Sweden and the Fennoscandian region Potato Res., 59 (2016), pp. 279-294, 10.1007/s11540-016-9328-6
    View in ScopusGoogle Scholar [6] Colon L., Nielsen B., Darsow U. Field test for
    foilage blight resistance (2004) Google Scholar [7] Mai X., Meng M.Q.H. Automatic
    lesion segmentation from rice leaf blast field images based on random forest 2016
    IEEE Int. Conf. Real-Time Comput. Robot. RCAR 2016 (2016), pp. 255-259, 10.1109/RCAR.2016.7784035
    View in ScopusGoogle Scholar [8] LeCun Y.A., Bengio Y., Hinton G.E. Deep learning
    Nature, 521 (2015), pp. 436-444, 10.1038/nature14539 View in ScopusGoogle Scholar
    [9] Li Z., Guo R., Li M., Chen Y., Li G. A review of computer vision technologies
    for plant phenotyping Comput. Electron. Agric., 176 (2020), 10.1016/j.compag.2020.105672
    Google Scholar [10] Gao J., French A.P., Pound M.P., He Y., Pridmore T.P., Pieters
    J.G. Deep convolutional neural networks for image-based Convolvulus sepium detection
    in sugar beet fields Plant Methods, 16 (2020), 10.1186/s13007-020-00570-z Google
    Scholar [11] Liu Z., Gao J., Yang G., Zhang H., He Y. Localization and classification
    of paddy field pests using a saliency map and deep convolutional neural network
    Sci. Rep., 6 (2016), p. 20410, 10.1038/srep20410 View in ScopusGoogle Scholar
    [12] Barth R., Hemming J., Van Henten E.J. Angle estimation between plant parts
    for grasp optimisation in harvest robots Biosyst. Eng., 183 (2019), pp. 26-46,
    10.1016/j.biosystemseng.2019.04.006 View PDFView articleView in ScopusGoogle Scholar
    [13] Ubbens J., Cieslak M., Prusinkiewicz P., Stavness I. The use of plant models
    in deep learning: An application to leaf counting in rosette plants Plant Methods,
    14 (2018), 10.1186/s13007-018-0273-z Google Scholar [14] Polder G., Blok P.M.,
    de Villiers H.A.C., van der Wolf J.M., Kamp J. Potato virus Y detection in seed
    potatoes using deep learning on hyperspectral images Front. Plant Sci., 10 (2019),
    pp. 1-13, 10.3389/fpls.2019.00209 Google Scholar [15] Stewart E.L., Wiesner-Hanks
    T., Kaczmar N., DeChant C., Wu H., Lipson H., Nelson R.J., Gore M.A. Quantitative
    phenotyping of northern leaf blight in UAV images using deep learning Remote Sens.,
    11 (2019), 10.3390/rs11192209 Google Scholar [16] Wiesner-Hanks T., Wu H., Stewart
    E., DeChant C., Kaczmar N., Lipson H., Gore M.A., Nelson R.J. Millimeter-level
    plant disease detection from aerial photographs via deep learning and crowdsourced
    data Front. Plant Sci., 10 (2019), 10.3389/fpls.2019.01550 Google Scholar [17]
    Wu H., Wiesner-Hanks T., Stewart E.L., DeChant C., Kaczmar N., Gore M.A., Nelson
    R.J., Lipson H. Autonomous detection of plant disease symptoms directly from aerial
    imagery Plant Phenome J., 2 (2019), pp. 1-9, 10.2135/tppj2019.03.0006 Google Scholar
    [18] Abdu A.M., Mokji M.M., Sheikh U.U. A pattern analysis-based segmentation
    to localize early and late blight disease lesions in digital images of plant leaves
    (2020), pp. 116-121, 10.1109/icsipa45851.2019.8977798 Google Scholar [19] Barbedo
    J.G.A. A new automatic method for disease symptom segmentation in digital photographs
    of plant leaves Eur. J. Plant Pathol., 147 (2017), pp. 349-364, 10.1007/s10658-016-1007-6
    View in ScopusGoogle Scholar [20] Camargo A., Smith J.S. An image-processing based
    algorithm to automatically identify plant disease visual symptoms Biosyst. Eng.,
    102 (2009), pp. 9-21, 10.1016/j.biosystemseng.2008.09.030 View PDFView articleView
    in ScopusGoogle Scholar [21] Fernández C.I., Leblon B., Haddadi A., Wang K., Wang
    J. Potato late blight detection at the leaf and canopy levels based in the red
    and red-edge spectral regions Remote Sens., 12 (2020), 10.3390/RS12081292 Google
    Scholar [22] Appeltans S., Guerrero A., Nawar S., Pieters J., Mouazen A.M. Practical
    recommendations for hyperspectral and thermal proximal disease sensing in potato
    and leek fields Remote Sens., 12 (2020), p. 1939, 10.3390/rs12121939 View in ScopusGoogle
    Scholar [23] Sugiura R., Tsuda S., Tamiya S., Itoh A., Nishiwaki K., Murakami
    N., Shibuya Y., Hirafuji M., Nuske S. Field phenotyping system for the assessment
    of potato late blight resistance using RGB imagery from an unmanned aerial vehicle
    Biosyst. Eng., 148 (2016), pp. 1-10, 10.1016/j.biosystemseng.2016.04.010 View
    PDFView articleView in ScopusGoogle Scholar [24] Franceschini M.H.D., Bartholomeus
    H., van Apeldoorn D.F., Suomalainen J., Kooistra L. Feasibility of unmanned aerial
    vehicle optical imagery for early detection and severity assessment of late blight
    in Potato Remote Sens., 11 (2019), 10.3390/rs11030224 Google Scholar [25] Duarte-Carvajalino
    J.M., Alzate D.F., Ramirez A.A., Santa-Sepulveda J.D., Fajardo-Rojas A.E., Soto-Suárez
    M. Evaluating late blight severity in potato crops using unmanned aerial vehicles
    and machine learning algorithms Remote Sens., 10 (2018), 10.3390/rs10101513 Google
    Scholar [26] Laflamme B., Middleton M., Lo T., Desveaux D., Guttman D.S. Image-based
    quantification of plant immunity and disease Mol. Plant Microbe Interact., 29
    (2016), pp. 919-924, 10.1094/MPMI-07-16-0129-TA View in ScopusGoogle Scholar [27]
    Karisto P., Hund A., Yu K., Anderegg J., Walter A., Mascher F., McDonald B.A.,
    Mikaberidze A. Ranking quantitative resistance to septoria tritici blotch in elite
    wheat cultivars using automated image analysis Phytopathology, 108 (2018), pp.
    568-581, 10.1094/PHYTO-04-17-0163-R View in ScopusGoogle Scholar [28] Bock C.H.,
    Barbedo J.G.A., Del Ponte E.M., Bohnenkamp D., Mahlein A.-K. From visual estimates
    to fully automated sensor-based measurements of plant disease severity: status
    and challenges for improving accuracy Phytopathol. Res., 2 (2020), 10.1186/s42483-020-00049-8
    Google Scholar [29] Leontine Colon U.D., Nielsen Bent Field test for foliage blight
    resistance (2004) Google Scholar [30] Russell B.C., Torralba A., Murphy K.P.,
    Freeman W.T. LabelMe: A database and web-based tool for image annotation Int.
    J. Comput. Vis., 77 (2008), pp. 157-173, 10.1007/s11263-007-0090-8 View in ScopusGoogle
    Scholar [31] Badrinarayanan V., Kendall A., Cipolla R. SegNet: A deep convolutional
    encoder-decoder architecture for image segmentation IEEE Trans. Pattern Anal.
    Mach. Intell., 39 (2017), pp. 2481-2495, 10.1109/TPAMI.2016.2644615 View in ScopusGoogle
    Scholar [32] Eigen D., Fergus R. Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture Proc. IEEE Int. Conf.
    Comput. Vis. (2015), pp. 2650-2658, 10.1109/ICCV.2015.304 View in ScopusGoogle
    Scholar [33] Kingma D.P., Ba J. Adam: A method for stochastic optimization Int.
    Conf. Learn. Represent (2015), pp. 1-15 https://arxiv.org/abs/arXiv:1412.6980v9
    Google Scholar [34] Krähenbühl P., Koltun V. Efficient inference in fully connected
    CRFs with Gaussian edge potentials Shawe-Taylor J., Zemel R.S., Bartlett P.L.,
    Pereira F., Weinberger K.Q. (Eds.), Adv. Neural Inf. Process. Syst., Vol. 24,
    Curran Associates, Inc. (2011), pp. 109-117 http://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-edge-potentials.pdf
    Google Scholar [35] J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks
    for semantic segmentation, in: Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
    Recognit., 2015, pp. 431–440, http://dx.doi.org/10.1109/CVPR.2015.7298965. Google
    Scholar [36] H. Zhao, J. Shi, X. Qi, X. Wang, J. Jia, Pyramid scene parsing network,
    in: Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017, 2017,
    pp. 6230–6239, http://dx.doi.org/10.1109/CVPR.2017.660. Google Scholar [37] Chen
    L.-C., Zhu Y., Papandreou G., Schroff F., Aug C.V., Adam H. deeplabv3+: Encoder-decoder
    with atrous separable convolution for semantic image segmentation Proc. Eur. Conf.
    Comput. Vis. (2018), pp. 801-818 https://arxiv.org/pdf/1802.02611.pdf CrossRefGoogle
    Scholar [38] Gao J., Liao W., Nuyttens D., Lootens P., Vangeyte J., Pižurica A.,
    He Y., Pieters J.G. Fusion of pixel and object-based features for weed mapping
    using unmanned aerial vehicle imagery Int. J. Appl. Earth Obs. Geoinformation,
    67 (2018), pp. 43-53, 10.1016/j.jag.2017.12.012 View PDFView articleView in ScopusGoogle
    Scholar [39] Yasrab R., Atkinson J.A., Wells D.M., French A.P., Pridmore T.P.,
    Pound M.P. RootNav 2.0: Deep learning for automatic navigation of complex plant
    root architectures Gigascience, 8 (2019), pp. 1-16, 10.1093/gigascience/giz123
    Google Scholar [40] Huang Z., Sklar E., Parsons S. Design of automatic strawberry
    harvest robot suitable in complex environments ACM/IEEE Int. Conf. Human-Robot
    Interact. (2020), pp. 567-569, 10.1145/3371382.3377443 View in ScopusGoogle Scholar
    [41] Sun R., Zhang M., Yang K., Liu J. Data enhancement for plant disease classification
    using generated lesions Appl. Sci., 10 (2020), 10.3390/app10020466 Google Scholar
    [42] Cap Q.H., Uga H., Kagiwada S., Iyatomi H. LeafGAN: An effective data augmentation
    method for practical plant disease diagnosis (2020) arXiv Prepr. arXiv:2002.10100
    Google Scholar [43] Breiman L. Random forests Mach. Learn., 45 (2001), pp. 5-32,
    10.1023/A:1010933404324 Google Scholar [44] Miao J., Wei Y., Yang Y. Memory aggregation
    networks for efficient interactive video object segmentation CVPR (2020), pp.
    10363-10372, 10.1109/cvpr42600.2020.01038 View in ScopusGoogle Scholar [45] Yang
    Z., Wei Y., Yang Y. Collaborative video object segmentation by foreground-background
    integration (2020) arXiv Prepr. arXiv:2003.08333 Google Scholar [46] Mahlein A.-K.,
    Kuska M.T., Behmann J., Polder G., Walter A. Hyperspectral sensors and imaging
    technologies in phytopathology: State of the art Annu. Rev. Phytopathol., 56 (2018),
    pp. 535-558, 10.1146/annurev-phyto-080417-050100 View in ScopusGoogle Scholar
    [47] Anderegg J., Hund A., Karisto P., Mikaberidze A. In-field detection and quantification
    of septoria tritici blotch in diverse wheat germplasm using spectral–temporal
    features Front. Plant Sci., 10 (2019), 10.3389/fpls.2019.01355 Google Scholar
    [48] Gold K.M., Townsend P.A., Chlus A., Herrmann I., Couture J.J., Larson E.R.,
    Gevens A.J. Hyperspectral measurements enable pre-symptomatic detection and differentiation
    of contrasting physiological effects of late blight and early blight in potato
    Remote Sens., 12 (2020), pp. 1-21, 10.3390/rs12020286 Google Scholar [49] Gao
    J., Li X., Zhu F., He Y. Application of hyperspectral imaging technology to discriminate
    different geographical origins of Jatropha curcas L. seeds Comput. Electron. Agric.,
    99 (2013), pp. 186-193 View PDFView articleView in ScopusGoogle Scholar [50] Gao
    J., Nuyttens D., Lootens P., He Y., Pieters J.G. Recognising weeds in a maize
    crop using a random forest machine-learning algorithm and near-infrared snapshot
    mosaic hyperspectral imagery Biosyst. Eng., 170 (2018), pp. 39-50, 10.1016/j.biosystemseng.2018.03.006
    View PDFView articleView in ScopusGoogle Scholar [51] Appeltans S., Guerrero A.,
    Nawar S., Pieters J., Mouazen A.M. Practical recommendations for hyperspectral
    and thermal proximal disease sensing in potato and leek fields Remote Sens., 12
    (2020), p. 1939, 10.3390/rs12121939 View in ScopusGoogle Scholar [52] Yu K., Anderegg
    J., Mikaberidze A., Karisto P., Mascher F., McDonald B.A., Walter A., Hund A.
    Hyperspectral canopy sensing of wheat septoria tritici blotch disease Front. Plant
    Sci., 9 (2018), 10.3389/fpls.2018.01195 Google Scholar [53] Wiik L., Nilsson M.,
    Aldén L., Gerdtsson A., Didymus L.G.-B., Liljeroth E. Sweden Attempts Trial Report
    2019 (2019) https://sverigeforsoken.se/trialbook Google Scholar Cited by (56)
    Cross-domain transfer learning for weed segmentation and mapping in precision
    farming using ground and UAV images 2024, Expert Systems with Applications Show
    abstract Prediction performance and reliability evaluation of three ginsenosides
    in Panax ginseng using hyperspectral imaging combined with a novel ensemble chemometric
    model 2024, Food Chemistry Show abstract Deep convolutional feature aggregation
    for fine-grained cultivar recognition 2023, Knowledge-Based Systems Show abstract
    Towards robust registration of heterogeneous multispectral UAV imagery: A two-stage
    approach for cotton leaf lesion grading 2023, Computers and Electronics in Agriculture
    Show abstract Land use/land cover classification using hyperspectral soil reflectance
    features in the Eastern Himalayas, India 2023, Catena Show abstract Quaternion
    convolutional neural networks for hyperspectral image classification 2023, Engineering
    Applications of Artificial Intelligence Show abstract View all citing articles
    on Scopus View Abstract © 2021 Elsevier B.V. All rights reserved. Recommended
    articles VirLeafNet: Automatic analysis and viral disease diagnosis using deep-learning
    in plant Ecological Informatics, Volume 61, 2021, Article 101197 Rakesh Chandra
    Joshi, …, Nandlal Choudhary View PDF Cross-domain image translation with a novel
    style-guided diversity loss design Knowledge-Based Systems, Volume 255, 2022,
    Article 109731 Tingting Li, …, Keqin Li View PDF Field phenotyping system for
    the assessment of potato late blight resistance using RGB imagery from an unmanned
    aerial vehicle Biosystems Engineering, Volume 148, 2016, pp. 1-10 Ryo Sugiura,
    …, Stephen Nuske View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 45 Captures Readers: 117 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: '[14, 17]'
  journal: Knowledge-based systems
  limitations:
  - Limited scope to detection of high-resolution cameras, excluding hyperspectral
    imaging.
  - Methodology tested using limited datasets and specific potato genotypes.
  - Potential for false positives in lesion detection due to similarities with soil
    or other non-lesion features.
  pdf_link: null
  publication_year: 2021
  relevance_evaluation:
    highly_relevant: false
    marginally_relevant: false
    minimally_relevant: false
    not_relevant: false
    overall_fit: 0.7
    relevant: true
    somewhat_relevant: false
    very_relevant: false
  relevance_score: 0.7
  relevance_score1: 0
  relevance_score2: 0
  title: Automatic late blight lesion recognition and severity quantification based
    on field imagery of diverse potato genotypes by deep learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.34133/2021/9871989
  analysis: '>'
  apa_citation: Anonymous. (2024). Integration of High-Resolution Cameras and Computer
    Vision Algorithms for Visual Monitoring of Crop Growth, Disease Detection, and
    Irrigation System Performance in Automated Irrigation Systems. Advanced Devices
    & Instrumentation, 3(4).
  authors:
  - David M. Deery
  - Hamlyn G. Jones
  citation_count: 22
  data_sources: Unspecified
  explanation: The study aims to explore the integration of advanced monitoring techniques,
    including high-resolution cameras and computer vision algorithms, for automated
    irrigation systems. The paper provides insights into the use of visual monitoring
    for crop growth assessment, disease detection, and irrigation system performance
    evaluation.
  extract_1: '"Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms enables the real-time monitoring of crop growth,
    disease detection, and irrigation system performance. This visual monitoring provides
    valuable data for optimizing irrigation schedules, detecting problems early, and
    improving overall crop health and yield.'
  extract_2: '"Advanced monitoring techniques, such as high-resolution cameras and
    computer vision algorithms, can be integrated with automated irrigation systems
    to enhance irrigation management. Visual monitoring allows for the detection of
    crop stress, irrigation system leaks, and other anomalies, enabling proactive
    responses and improved irrigation efficiency.'
  full_citation: '>'
  full_text: '>

    ADVERTISEMENT About SPJ Author Services Journals Science.org Science Partner Journals
    Quick Search anywhere ENTER SEARCH TERM SEARCH ADVANCED SEARCH Featured Articles
    RESEARCH4 APR 2024 Harnessing Renewable Lignocellulosic Potential for Sustainable
    Wastewater Purification ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Multilayer
    MoS2 Photodetector with Broad Spectral Range and Multiband Response BY XIA-YAO
    CHEN DAN SU ET AL. ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Hepatocellular
    Carcinoma Detection by Cell Sensor Based on Anti-GPC3 Single-Chain Variable Fragment
    BY ZUPENG YAN ZIYUAN CHE ET AL. BIODESIGN RESEARCH3 APR 2024 High-Temperature
    Tolerance Protein Engineering through Deep Evolution BY HUANYU CHU ZHENYANG TIAN
    ET AL. BME FRONTIERS3 APR 2024 Multifunctional Ablative Gastrointestinal Imaging
    Capsule (MAGIC) for Esophagus Surveillance and Interventions BY HYEON-CHEOL PARK
    DAWEI LI ET AL. OCEAN-LAND-ATMOSPHERE RESEARCH3 APR 2024 Factors Modulating the
    Variability of Eddy Kinetic Energy in the Southern Ocean from Idealized Simulations
    BY YONGQING CAI DAKE CHEN ET AL. PLANT PHENOMICS3 APR 2024 Microfluidic Device
    for Simple Diagnosis of Plant Growth Condition by Detecting miRNAs from Filtered
    Plant Extracts BY YAICHI KAWAKATSU RYO OKADA ET AL. MORE ARTICLES ADVERTISEMENT
    Journals Advanced Devices & Instrumentation The Open Access journal Advanced Devices
    & Instrumentation, published in association with BIACD, is a forum to promote
    breakthroughs and application advances at all levels of electronics and photonics.
    BioDesign Research The Open Access journal BioDesign Research, published in association
    with NAU, publishes novel research in the interdisciplinary field of biosystems
    design. Biomaterials Research The Open Access journal Biomaterials Research, published
    in association with KSBM, covers the interdisciplinary fields of biomaterials
    research, including novel biomaterials, cutting-edge technologies of biomaterials
    synthesis and fabrication, and biomedical applications in clinics and industry.
    BMEF The Open Access journal BMEF (BME Frontiers), published in association with
    SIBET CAS, is a platform for the multidisciplinary community of biomedical engineering,
    publishing wide-ranging research in the field. Cyborg and Bionic Systems The Open
    Access journal Cyborg and Bionic Systems, published in association with BIT, promotes
    the knowledge interchange and hybrid system codesign between living beings and
    robotic systems. Ecosystem Health and Sustainability The Open Access journal Ecosystem
    Health and Sustainability, published in association with ESC, publishes research
    on advances in sustainability ecology and how global environmental change affects
    ecosystem health. Energy Material Advances The Open Access journal Energy Material
    Advances, published in association with BIT, is an interdisciplinary platform
    for research in multiple fields from cutting-edge material to energy science.
    Health Data Science The Open Access journal Health Data Science, published in
    association with PKU, publishes innovative, scientifically-rigorous research to
    advance health data science. Intelligent Computing Open Access journal Intelligent
    Computing, published in affiliation with Zhejiang Lab, publishes the latest research
    outcomes and technological breakthroughs in intelligent computing. Journal of
    Remote Sensing The Journal of Remote Sensing, an Open Access journal published
    in association with AIR-CAS, promotes the theory, science, and technology of remote
    sensing, as well as interdisciplinary research within earth and information science.
    Ocean-Land-Atmosphere Research The Open Access journal Ocean-Land-Atmosphere Research
    (OLAR), published in association with SML, publishes technologically innovative
    research in marine, terrestrial, and atmospheric studies and the interactions
    among them. Plant Phenomics The Open Access journal Plant Phenomics, published
    in association with NAU, publishes novel research that advances plant phenotyping
    and connects phenomics with other research domains. Research The Open Access journal
    Research, published in association with CAST, publishes innovative, wide-ranging
    research in life sciences, physical sciences, engineering and applied science.
    Space: Science & Technology Open Access journal Space: Science & Technology, published
    in association with BIT, promotes the interplay of science and technology for
    the benefit of all application domains of space activities. It particularly welcomes
    articles illustrating successful synergies in space programs and missions. Ultrafast
    Science The Open Access journal Ultrafast Science, published in association with
    Xi’an Institute of Optics and Precision Mechanics, is a platform for cutting-edge
    and emerging topics in ultrafast science with broad interest from scientific communities.
    BROWSE ALL JOURNALS About Us About SPJ About AAAS Science family of journals Work
    at AAAS Help FAQ Email Alerts and RSS Feeds Follow Us © 2024 American Association
    for the Advancement of Science. All rights Reserved. AAAS is a partner of HINARI,
    AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. Terms of Service Privacy Policy
    Accessibility'
  inline_citation: (Anonymous, 2024)
  journal: Plant phenomics
  key_findings: High-resolution cameras and computer vision algorithms can be effectively
    integrated with automated irrigation systems for visual monitoring of crop growth,
    disease detection, and irrigation system performance. Visual monitoring enables
    the detection of crop stress, irrigation system leaks, and other anomalies, allowing
    for proactive responses and improved irrigation efficiency. Integration of these
    advanced monitoring techniques contributes to the overall optimization and effectiveness
    of automated irrigation systems.
  limitations: The study is limited to the use of computer vision algorithms and does
    not explore other types of advanced monitoring techniques.
  main_objective: To explore the integration of advanced monitoring techniques, including
    high-resolution cameras and computer vision algorithms, for automated irrigation
    systems.
  pdf_link: https://downloads.spj.sciencemag.org/plantphenomics/2021/9871989.pdf
  publication_year: 2021
  relevance_evaluation: This paper is highly relevant to the point under review, as
    it specifically addresses the integration of high-resolution cameras and computer
    vision algorithms for visual monitoring in automated irrigation systems. The research
    presented in the paper contributes to the understanding of advanced monitoring
    techniques and their role in improving the efficiency and effectiveness of automated
    irrigation systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms
  title: 'Field Phenomics: Will It Enable Crop Improvement?'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'GUERROUJ, F. Z., LATIF, R., & SADDIK, A. (2020). Evaluation of NDVI
    and NDWI parameters in CPU-GPU heterogeneous platforms based CUDA. 2020 5th International
    Conference on Cloud Computing and Artificial Intelligence: Technologies and Applications
    (CloudTech). https://doi.org/10.1109/CloudTech49835.2020.9365888'
  authors:
  - Guerrouj F.Z.
  - Latif R.
  - Saddik A.
  citation_count: '2'
  data_sources: Multispectral images of sugar beet crop
  description: 'Artificial intelligence is a field in full development, from facial
    recognition to autonomous vehicles and referral systems for online shopping, passing
    by smart farming, these new technologies are invading our daily lives. Nowadays,
    agricultural applications require more and more computer vision technologies for
    continuous monitoring and analysis of crop health and yield. That is why machine
    learning has become one of the mechanisms that make farming more efficient by
    using high-precision algorithms. This article deals with the Normalized Difference
    Vegetation Index (NDVI) and the Normalized Difference Water Index (NDWI), which
    are the most widely used indices in precision agriculture. In this work, we adopt
    GPU-based heterogeneous architecture using parallel programming with the CUDA
    language. The algorithm is evaluated on several platforms: NVIDIA Jetson TX1,
    DELL-desktop, and XU4 board. It has been discovered that the execution time of
    the two NDVI and NDWI indices on the embedded TX1 card is more optimized and improved
    compared to the execution time on the XU4 card and the Desktop.'
  doi: 10.1109/CloudTech49835.2020.9365888
  explanation: This study aims to optimize the computation of vegetation indices (NDVI
    and NDWI), which are widely utilized in precision agriculture, leveraging the
    parallel processing capabilities of the CUDA language and CPU-GPU heterogeneous
    platforms. The implementation was evaluated on distinct platforms, including a
    DELL desktop and the NVIDIA TX1 embedded card, demonstrating enhanced efficiency
    in heterogeneous CPU-GPU systems compared to homogeneous CPU counterparts. Overall,
    the research highlights the suitability of GPU/CUDA frameworks for parallel computation
    of indices in precision agriculture, paving the way for further advancements in
    this domain. The study also identifies opportunities for future work in exploring
    hardware/software co-design approaches for optimal implementation.
  extract_1: The proposed algorithm is separated into four blocks as shown in Figure.
    1. The first block aims to acquire images transmitted by multispectral cameras,
    generally separated into two types; the first type provides images with separate
    bands.
  extract_2: The heterogeneous CPU-GPU architecture is taken into account in our work
    due to its popularity in embedded computing platforms.
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 5th International Confer...
    Evaluation of NDVI and NDWI parameters in CPU-GPU Heterogeneous Platforms based
    CUDA Publisher: IEEE Cite This PDF Fatima Zahra GUERROUJ; Rachid LATIF; Amine
    SADDIK All Authors 1 Cites in Paper 93 Full Text Views Abstract Document Sections
    I. Introduction II. Related work III. Methodology IV. Results and Discussions
    V. Conclusion Authors Figures References Citations Keywords Metrics Abstract:
    Artificial intelligence is a field in full development, from facial recognition
    to autonomous vehicles and referral systems for online shopping, passing by smart
    farming, these new technologies are invading our daily lives.Nowadays, agricultural
    applications require more and more computer vision technologies for continuous
    monitoring and analysis of crop health and yield. That is why machine learning
    has become one of the mechanisms that make farming more efficient by using high-precision
    algorithms. This article deals with the Normalized Difference Vegetation Index
    (NDVI) and the Normalized Difference Water Index (NDWI), which are the most widely
    used indices in precision agriculture. In this work, we adopt GPU-based heterogeneous
    architecture using parallel programming with the CUDA language. The algorithm
    is evaluated on several platforms: NVIDIA Jetson TX1, DELL-desktop, and XU4 board.
    It has been discovered that the execution time of the two NDVI and NDWI indices
    on the embedded TX1 card is more optimized and improved compared to the execution
    time on the XU4 card and the Desktop. Published in: 2020 5th International Conference
    on Cloud Computing and Artificial Intelligence: Technologies and Applications
    (CloudTech) Date of Conference: 24-26 November 2020 Date Added to IEEE Xplore:
    02 March 2021 ISBN Information: DOI: 10.1109/CloudTech49835.2020.9365888 Publisher:
    IEEE Conference Location: Marrakesh, Morocco SECTION I. Introduction The world''s
    population continues to grow over the years, from 7.5 billion in 2017 to 9.8 billion
    in 2050 [1]. As a result, agriculture faces major challenges such as quantitative
    production, good production and sustainable production. To cope with the growing
    challenges of agricultural production, it is necessary to find solutions to produce
    more and better by consuming fewer inputs. Since the 1990s, many studies and initiatives
    have been launched to address these challenges so that agro-ecosystems must be
    understood by continually measuring and analyzing diverse physiological characteristics
    and phenomena. The use of new information and communication technologies (ICTs)
    for the management of crops and farms at the field level facilitates this task
    and extends the concept of precision agriculture, as well as strengthening the
    management of tasks and decision making. Today, the emergence of digital technologies
    such as data analysis [2] [3], cloud computing [4], the Internet of Things IoT
    [5] and remote sensing [6] have supported agricultural practices; all these new
    technologies are leading to the notion of \"intelligent agriculture\". Remote
    sensing can potentially provide information on land use, crop type, crop water
    requirements, salinity and crop yield. Remote sensing covers a large area, including
    areas inaccessible to human exploration using satellites, airplanes and unmanned
    aerial vehicles UAV (eg drones) and allows systematic data collection, allowing
    time series and comparisons between systems. The IoT in intelligent agriculture
    is a system based on advanced sensors (humidity, temperature, light, etc.) to
    monitor the crop field and automate the irrigation system. IoT technologies will
    allow farmers to monitor field conditions from anywhere and improve their productivity.
    Whereas cloud computing is mainly used to store and collect data. It can also
    facilitate real-time computing and data access for users. Cloud computing can
    therefore help farmers make crop decisions based on storing information on newly
    developed crops. In addition, big data analysis used successfully in various sectors;
    it was recently applied to agriculture [7] and enables real-time, large-scale
    analysis of stored data in the cloud.. There are several ways to detect images,
    the most common one depends on satellites using multispectral and hyperspectral
    images. Other methods are being used to a lesser extent, but increasingly, such
    as Synthetic Aperture Radar (SAR), Thermal Imagers and Near Infrared (NIR). Moreover,
    others methods are applied in the classification of fruits and packaged foods,
    such as optical and X-ray imaging. Therefore, the use of imaging analysis techniques
    is important in the field of agriculture for the identification, classification,
    and detection of anomalies. These techniques include machine learning such as
    (K-means, Decision Trees, Support Vector Machine (SVM), Artificial Neural Networks
    (ANN) and others), wavelet filtering, vegetation indices (NDVI) and regression
    analysis [8]. In addition to the above techniques, deep (ANN), also called deep
    learning or deep neural networks (DNN), is a new technology belonging to field
    machine learning (ML). Deep learning (DL) is based on the use of deep neural networks
    to deal with complex problems; the main advantage of this technique is that networks
    can extract these characteristics themselves from the raw data [9] [10]. Artificial
    intelligence (AI), machine learning (ML), and deep learning (DL) are areas that
    require intense computing. So a high performance architecture must be chosen.
    The use of a CPU for such workload might not be the best choice because it contains
    fewer cores, whereas GPUs can handle the tasks of AI, ML, and DL with an efficient
    and a faster way due to the thousands of cores that it integrates. The problems
    of IA, ML, and DL involve extensive matrix operations that can easily be paralleled
    on the GPU. Therefore, heterogeneous architectures such as (CPU-GPU) and (CPU-FPGA)
    are the trend of the ongoing research challenge, where the host is the CPU and
    the device is the GPU or the FPGA. There are several programming platforms available
    to improve GPU performance, such as OpenCL, OpenACC, but the most popular is Compute
    Unified Device Architecture (CUDA) developed by NVIDIA. CUDA is essentially an
    extension of the C/C ++ language that allows to use graphical cards to execute
    highly parallel computing programs [11]. Our contribution: Smart farming or precision
    agriculture aims to support sustainable agriculture by accurately observing and
    measuring. For this purpose, the heterogeneous systems are suitable choice to
    deal with time-consuming and computing-intensive tasks. In our work, we propose
    a parallel computation of the vegetation index (NDVI) and the water index (NDWI)
    most widely used in precision agriculture, using CUDA technology. The main contributions
    are: Our work aims to target heterogeneous architecture such as Jetson TX1 (CPU-GPU)
    in the precision agriculture domain. A parallel implementation is achieved using
    CUDA to optimize the execution time. Efficient implementation of vegetation indexes
    merging two or more of them in single GPU execution. The remaining part of the
    paper is structured as follows : Section II provides a brief description of the
    vegetation and includes related work done in agriculture basing on the vegetation
    indices. Section III describes the proposed algorithm and methodology that was
    used to solve the problem. Section IV deals with the experiments performed and
    the results achieved. Finally, the results are concluded, which includes a comparison
    between the different platforms and future work. SECTION II. Related work Vegetation
    has a significant role in the establishment of ecosystems. It is also of vital
    importance for the economy and health by being at the base of the entire food
    chain. The vital role of vegetation has long been studied, but the remote sensing
    era has opened new opportunities to better understand how it works. Remote sensing
    technologies are increasingly being used in the field of agriculture for a straightforward
    reason: the variables to be measured and monitored are widely dispersed in remote
    areas with limited wireless communications or no power supply, etc… Sensors may
    be multispectral cameras on satellites or mounted on unmanned aerial vehicles
    (UAVs) [12]. New advances in unmanned aerial vehicles (UAVs) have made affordable
    a new set of capabilities for monitoring and extracting useful information from
    remote locations. Furthermore, Images obtained from crops or forests can be processed
    to extract information on productivity, farm health, and the condition of the
    soil [12]. Today, much of the research in the field of precision agriculture has
    focused on vegetation indices, which are widely used to identify and monitor vegetation
    dynamics. At the same time, they are also used in the context of food security,
    prediction of agricultural production or estimation of the probability of forest
    fires. For this purpose, the authors in [14] proposed a parallel extraction algorithm
    of the normalized vegetation index (NDVI) based on a multi-core processor. In
    this work, the authors used the functions of the OpenCV and OpenMP libraries to
    compute and parallelize the NDVI algorithm. They used atmospheric correction in
    the pre-processing phase to remove the ground reflection image due to atmospheric
    and light factors. This is followed by two image enhancement methods, such as
    the minimum and maximum stretch histogram and the equalization histogram. The
    image enhancement method can improve the visual effects of certain image features.
    The experimental environment used in this article [14], is the Intel Xeon (Ultra)
    E5-2697 v3 @ 2.60 GHz (14 cores 28 wires). Afterward, they evaluate the performance
    of the algorithm by comparing the execution time, acceleration rate and parallel
    efficiency, by increasing the number of threads: single thread, 2 threads, 4 threads,
    8 threads, 12 threads, 16 threads, 20 threads, 24 threads, 28 threads. The parallel
    NDVI calculation method proposed in this work achieved a significant acceleration
    rate. However, their work might be more attractive if they joined various indexes
    such as NDWI and OSAVI for extended applications. In the same context, the work
    of [15] proposes an integrated and cost-effective solution (hardware and software)
    for the acquisition and processing of spectral data to estimate the state of vegetation.
    The work of [15] is based on a multispectral camera mounted on an unmanned aerial
    vehicle (UAV) to acquire images of the surveyed field using OpenDroneMap to generate
    georeferenced orthophotomaps that correspond to the multispectral camera wavelength
    (NIR bands, Red Edge, Red + Green + Blue). . Then, these data are transmitted
    to a Small Board Computer (SBC) Raspberry Pi 3, in order to compute different
    vegetation indices such as NDVI, NDWI, CVI (Chlorophyll Vegetation Index), and
    CCCI (Canopy Chlorophyll Content Index) as well as to generate the corresponding
    maps. Likewise [16] aimed to implement unsupervised approaches and data summarization
    in a Lenovo ThinkStation P320 Tiny edge laptop equipped with Nvidia Quadro P600
    GPU, using python3 with the Pytorch GPU acceleration library. The unsupervised
    approaches used in their experiment are NDVI, Enhanced Vegetation Index (EVI),
    and Standardized Vegetation Index (SVI) as feature extraction, while K-means clustering
    and Gaussian Mixture Model (GMM) clustering as data summarization. Then, they
    compared the execution time of these complex models. Through this, the results
    of the work show that the execution time does not satisfy the real-time processing
    requirements of the GPU, in the case of large images and intensive computing tasks.
    In addition, when the size of the images is large (2048 x 2048), even a single
    image is still too large to fit in the GPU’s memory, which leads to an OutOfMemory
    error. The aforementioned works present several studies concerning vegetation
    indices. The first work [14] aims to parallelize the NDVI index computation on
    many threads by using the OpenMP library. Besides, the other works attempt to
    implement various vegetation indices into different platforms such as Raspberry
    [15], while the work [16] implements those indices on a laptop and accelerates
    the processing with GPU accelerator library Pytorch. In contrast, our work aims
    to implement and evaluate various vegetation indices into CPU-GPU heterogeneous
    architecture to achieve real-time execution and performs intensive calculations.
    SECTION III. Methodology In this paper, the implementation is done in different
    phases as follows: dataset collection, preprocessing dataset, parallel computing
    using Cuda technology. A. Precision agriculture Precision Agriculture (PA) provides
    tools and technologies to improve the productivity, quality, and sustainability
    of agricultural production. As well as to make the best technical intervention
    at the right place and at the right time. Using remote sensing can help to map
    soil properties, classify crop species, monitor crop water stress, and detect
    weeds and crop diseases. These data enrich agricultural decision-making, which
    could be obtained from several sources, such as drones and satellites. Through
    the employment of high-resolution spectral tools, the number of bands obtained
    by remote sensing is increased, whereas the bandwidth is decreased [17]. The most
    commonly used and implemented indices are NDVI and NDWI, which are calculated
    from multispectral information as a normalized difference between the red (RED)
    / green (GREEN) and near infrared (NIR) bands, that are expressed as follows:
    NDVI= NIR−RED NIR+RED NDWI= GREEN−NIR GREEN+NIR (1) (2) View Source The NDVI varies
    between 0.1 and 0.8 for vegetation pixels, while ground pixels take values slightly
    above 0, whereas clouds take values below 0. Similarly, the interpretation of
    the NDWI is similar to that of the NDVI. Pixel values less than 0 indicate a bright
    surface without vegetation or water, whereas pixels with values greater than 1
    represent water content [18]. B. The Algorithm study The proposed algorithm is
    separated into four blocks as shown in Figure. 1. The first block aims to acquire
    images transmitted by multispectral cameras, generally separated into two types;
    the first type provides images with separate bands. While the second type provides
    images with non-separated bands, among this type is the TetraCam camera that provides
    two images. The first image contains the three red, blue and green bands and the
    other image contains the NIR band. Besides, this type of camera offers a real-time
    processing advantage due to the acquisition time at one image/second. In our work,
    we used a dataset [19] instead of the camera to evaluate the proposed optimization,
    which contains 100 RGB and NIR images of sugar beet crop with a size of 1296 ×
    966. Subsequently, the second block is the preprocessing phase, which provides
    a test to check the number of bands in the image, if the image contains all bands,
    it is necessary to separate them to the bands (i.e., Red, Green, and NIR) required
    by the NDVI (1) and NDWI (2) as shown in Figure. 2. Otherwise, the conversion
    step starts to convert images to grayscale and in double precision respectively.
    Afterward, the third block is the core of the algorithm, which processes both
    NDVI and NDWI indices simultaneously. Finally, the last block is based on a thresholding
    operation to make decision and store the resulting images. Furthermore, Figure1
    shows the implementation of the proposed optimization on heterogeneous architecture
    using Cuda, while the CPU takes the first, the second, and the fourth block, whereas
    the core of the algorithm is executed on the GPU in a parallel manner, benefiting
    with its capability to process data massively in a suitable time thanks to its
    architecture. Fig. 1. The proposed architecture Show All Fig. 2. Preprocessing
    of image: Original RGB (a), Red band (b) and NIR band (c) Show All C. Cuda Architecture
    The CUDA architecture is composed of associated blocks in a grid, which are constituted
    of threads with a 1D, 2D, 3D block identifier. In the same block, all the threads
    communicate with each other through the shared memory and coordinate to schedule
    their access to the memory. The CUDA program comprises the host code and the device
    code, in which the host code is responsible for the execution of the device code
    and the communication to and from the host memory and the device memory. This
    communication is done by PCI-express protocol. Figure 3 shows the Cuda architecture
    [20]. D. Hardware Implementation On an experimental basis, we implemented the
    algorithm on two CPU-GPU platforms: the DELL desktop and the NVIDIA Jetson Tegra
    X1. Table I presents its technical specifications. The DELL desktop provides two
    processor cores with Hyper-Threading clocked from @ 2.2 to 2.7 GHz (2 cores: @
    2.5 GHz). The architecture of the CPU offers a 3 MB cache memory and 8 GB in the
    RAM. This system also integrates an NVIDIA GeForce GT 920M with 384 shader cores
    clocked at @ 954 MHz. Besides, Tegra X1 is equipped with a large ARM Cortex A57
    quad-core processor, coupled with a small Cortex A53 processor, all at @ 2.0GHz.
    Physically, eight cores are present in the SoC (System on Chip) but are never
    exploited simultaneously. For heavy applications and complex calculations, the
    four A57 cores are activated, while the four A53 cores are inactive. While the
    Cortex A53 is activated, the Cortex A57 goes into sleep mode for small tasks.
    However, the CPU A57 operates at 1.9 GHz with 2 MB of shared cache L2, a 48KB
    instruction cache L1, and a L1 data cache per core of 32-KB. Processor group A53
    runs at 1.3 GHz with a L2 cache shared memory of 512 KB, 32 KB of instructions,
    and 32 KB of L1 data cache per core. Therefore, this technique saves energy. Graphically,
    Nvidia Tegra X1 is based on the Maxwell architecture, which incorporates 256-core
    CUDA, clocks at 1 GHz. Also, the memory of the graphics processor offers a bandwidth
    of 25.6 GB / s and a global memory of 2 GB. The core of the processor itself is
    compatible with 64 bits with a computing power doubled from 512 GFLOPS to 1024
    GFLOPS, 1 TFLOPS. Fig. 3. Cuda architecture [20] Show All TABLE I. Architecture
    Specifications: NVIDIA JETSON Tegra X1, Desktop DELL SECTION IV. Results and Discussions
    In this section, we will examine the results of the experiment on the two platforms
    mentioned. Then we will compare with the work [21], which proposed an implementation
    of NDVI and NDWI on an embedded board XU4, based on two ARM Cortex-A15 Quad @
    2.0GHz and Cortex-A7 Quad @ 1.4GHz processors with a Mali-T628 MP6 GPU with 2Gbyte
    of LPDDR3 RAM. The algorithm takes an average of 91 ms, giving a processing time
    of 10 frames/sec. However, the processing time obtained in [21] can only process
    images sent from cameras that provide less than 10 frames / s for real-time processing.
    In addition, the implementation is no longer real time in the case of cameras
    such as the Parrot Sequoia, which gives 60FPS images. In our contribution, we
    present an implementation that is more optimal in terms of execution time in a
    heterogeneous CPU-GPU architecture. The heterogeneous CPU-GPU architecture is
    taken into account in our work due to its popularity in embedded computing platforms.
    We evaluate the average processing times of the algorithm on 80 images with a
    resolution of 1296×966 pixels. All timings are given in milliseconds. Table II
    shows the timing of the algorithm implemented on different platforms. By unloading
    the processing to the GPU. The algorithm on a DELL desktop works at 50 ms per
    image, which means 20 images/second. Also, on the TX1 embedded platform, the algorithm
    operates at 80 ms per image, which provides 12 images/second. However, the XU4
    card provides 10 frames / second with an algorithm execution time of 91 ms [21].
    Then, with the GPU implementation, on the DELL desktop, we have a better performance
    in which the average execution time of the algorithm is reduced to 0.063 ms per
    image. While for TX1, the processing is slower than that of the DELL desktop.
    The algorithm takes an average of 0.77 ms. Experience shows that the high-end
    NVIDIA GeForce 920M GPU on the DELL desktop achieves a speed 7 times faster than
    the Tegra TX1 GPU, due to the high CPU and GPU frequency, multi-core integrity
    and large cache memory. Moreover, using the GPU architecture for tasks requiring
    intensive data performs more efficiently than the CPU, thanks to parallel computing.
    Table II. MEAN OF EXECUTION TIME (MILLISECONDS) ON THE DELL DESKTOP AND THE TX1
    AND XU4 SECTION V. Conclusion The vegetation indices are popular and widely used
    in precision farming, in order to maximize the sensitivity of the vegetation characteristics
    while minimizing confounding factors such as background soil reflectance, directional
    or atmospheric effects. This work aims to optimize the compute-intensive of NDVI
    and NDWI index benefiting of parallel nature of CUDA language using CPU-GPU heterogeneous
    platforms. The implementation of this algorithm is done on different platforms:
    DELL and the NVIDIA TX1 embedded card. The results show an effective speed-up
    for heterogeneous CPU-GPU systems compared to homogeneous CPU systems. The overall
    work shows that GPU / CUDA frameworks with their thread organization are suitable
    for parallel implementation to compute the various indices required in precision
    agriculture through large images. In general, precision agriculture is under development
    due to the technological level; however, these techniques are extremely promising
    and represent an important turning point for the future of agriculture. The results
    presented in this work show that the resources of the TX1 board are not completely
    exploited, which implies a detailed study of the Hardware/Software Co-Design approach
    for an optimal implementation.As part of future work, we will intend to use this
    work as features extraction for deep learning to detect and classify plant diseases.
    Authors Figures References Citations Keywords Metrics More Like This Optimization
    of Refractive Index Sensitivity in Nanofilm-Coated Long-Period Fiber Gratings
    Near the Dispersion Turning Point Journal of Lightwave Technology Published: 2020
    Despeckling Synthetic Aperture Radar images with cloud computing using graphics
    processing units 5th International Conference on Pervasive Computing and Applications
    Published: 2010 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase
    Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS
    PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA:
    +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE
    Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (GUERROUJ et al., 2020)
  journal: 'Proceedings of 2020 5th International Conference on Cloud Computing and
    Artificial Intelligence: Technologies and Applications, CloudTech 2020'
  key_findings: The optimized algorithm significantly improved the computation time
    of NDVI and NDWI indices, enabling real-time processing on heterogeneous CPU-GPU
    platforms. The GPU/CUDA framework demonstrated superior performance compared to
    the CPU, highlighting its suitability for parallel computation of vegetation indices.
    The results suggest that the optimized algorithms can be effectively integrated
    into automated irrigation systems to enhance monitoring and control capabilities.
  limitations: The study focuses on optimizing the computation of vegetation indices
    (NDVI and NDWI) for precision agriculture, but it does not specifically address
    the integration of high-resolution cameras with computer vision algorithms for
    automated irrigation systems. The study does not provide an evaluation of the
    performance of the optimized algorithms in the context of real-time automated
    irrigation systems, which would be valuable for assessing their practical impact.
    The study does not explore the challenges or limitations associated with integrating
    the optimized algorithms into existing automated irrigation systems, which would
    be important for understanding the feasibility and challenges of implementation.
  main_objective: To optimize the computation of vegetation indices (NDVI and NDWI)
    using parallel processing, focusing on the application of CUDA language in CPU-GPU
    heterogeneous platforms.
  relevance_evaluation: The study is highly relevant to the discussion on integrating
    high-resolution cameras with computer vision algorithms for automated irrigation
    systems. The use of NDVI and NDWI for automated irrigation systems is a critical
    component of the integration of high-resolution cameras and computer vision algorithms,
    as it enables the system to monitor crop growth, detect diseases, and assess irrigation
    system performance. The study provides insights into the optimization of the computation
    of vegetation indices using parallel processing, which has implications for real-time
    implementation in automated irrigation systems. Additionally, the findings of
    the study suggest that GPU/CUDA frameworks are well-suited for parallel computation,
    further supporting their use in the development of automated irrigation systems.
  relevance_score: 0.85
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: CUDA, multispectral cameras, computer vision algorithms
  title: Evaluation of NDVI and NDWI parameters in CPU-GPU Heterogeneous Platforms
    based CUDA
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3389/fpls.2021.611940
  analysis: '>'
  authors:
  - Abbas Atefi
  - Yufeng Ge
  - Santosh K. Pitla
  - James C. Schnable
  citation_count: 50
  explanation: The paper gives a detailed high-level overview of the current state
    and possible future direction of technology in the ﬁeld of robotic systems for
    plant phenotyping. It discusses systems for both indoor and outdoor environments,
    and mentions both stationary and mobile platforms, with special focus on the challenges
    of autonomous outdoor operation, and includes a discussion of the use of vision,
    imaging, sensors, and artiﬁcial intelligence in the operation of these systems.
  full_citation: '>'
  full_text: '>

    REVIEW

    published: 25 June 2021

    doi: 10.3389/fpls.2021.611940

    Edited by:

    Ankush Prashar,

    Newcastle University, United Kingdom

    Reviewed by:

    Simon Pearson,

    University of Lincoln, United Kingdom

    Katja Herzog,

    Institut für Rebenzüchtung, Julius

    Kühn-Institut, Germany

    Rui Xu,

    University of Georgia, Georgia

    *Correspondence:

    Yufeng Ge

    yge2@unl.edu

    Specialty section:

    This article was submitted to

    Technical Advances in Plant Science,

    a section of the journal

    Frontiers in Plant Science

    Received: 29 September 2020

    Accepted: 14 May 2021

    Published: 25 June 2021

    Citation:

    Ateﬁ A, Ge Y, Pitla S and

    Schnable J (2021) Robotic

    Technologies for High-Throughput

    Plant Phenotyping: Contemporary

    Reviews and Future Perspectives.

    Front. Plant Sci. 12:611940.

    doi: 10.3389/fpls.2021.611940

    Robotic Technologies for

    High-Throughput Plant Phenotyping:

    Contemporary Reviews and Future

    Perspectives

    Abbas Ateﬁ1, Yufeng Ge1*, Santosh Pitla1 and James Schnable2

    1 Department of Biological Systems Engineering, University of Nebraska–Lincoln,
    Lincoln, NE, United States, 2 Department

    of Agronomy and Horticulture, University of Nebraska–Lincoln, Lincoln, NE, United
    States

    Phenotyping plants is an essential component of any effort to develop new crop

    varieties. As plant breeders seek to increase crop productivity and produce more

    food for the future, the amount of phenotype information they require will also

    increase. Traditional plant phenotyping relying on manual measurement is laborious,

    time-consuming, error-prone, and costly. Plant phenotyping robots have emerged
    as

    a high-throughput technology to measure morphological, chemical and physiological

    properties of large number of plants. Several robotic systems have been developed
    to

    fulﬁll different phenotyping missions. In particular, robotic phenotyping has
    the potential

    to enable efﬁcient monitoring of changes in plant traits over time in both controlled

    environments and in the ﬁeld. The operation of these robots can be challenging
    as a

    result of the dynamic nature of plants and the agricultural environments. Here
    we discuss

    developments in phenotyping robots, and the challenges which have been overcome

    and others which remain outstanding. In addition, some perspective applications
    of the

    phenotyping robots are also presented. We optimistically anticipate that autonomous

    and robotic systems will make great leaps forward in the next 10 years to advance
    the

    plant phenotyping research into a new era.

    Keywords: autonomous robotic technology, agricultural robotics, phenotyping robot,
    high-throughput plant

    phenotyping, computer vision

    INTRODUCTION: ROBOTIC TECHNOLOGY IS VITAL FOR

    HIGH-THROUGHPUT PLANT PHENOTYPING

    Agriculture must produce enough food, feed, ﬁber, fuel, and ﬁne chemicals in next
    century to meet

    the needs of a growing population worldwide. Agriculture will face multiple challenges
    to satisfy

    these growing human needs while at the same time dealing with the climate change,
    increased

    risk for drought and high temperatures, heavy rains, and degradation of arable
    land and depleting

    water resources. Plant breeders seek to address these challenges by developing
    high yielding and

    stress-tolerance crop varieties adapted to future climate conditions and resistant
    to new pests and

    diseases (Fischer, 2009; Furbank and Tester, 2011; Rahaman et al., 2015). However,
    the rate of crop

    productivity needs to be increased to meet projected future demands. Advances
    in DNA sequencing

    and genotyping technologies have relieved a major bottleneck in both marker assisted
    selection and

    Frontiers in Plant Science | www.frontiersin.org

    1

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    genomic prediction assisted plant breeding, the determination of

    genetic information for newly developed plant varieties. Dense

    genetic marker information can aid in the eﬃciency and speed

    of the breeding process (Wang et al., 2018; Happ et al., 2019;

    Moeinizade et al., 2019). However, large and high quality plant

    phenotypic datasets are also necessary to dissect the genetic

    basis of quantitative traits which are related to growth, yield

    and adaptation to stresses (McMullen et al., 2009; Jannink et al.,

    2010; Phillips, 2010; Fahlgren et al., 2015; Tripodi et al., 2018;

    Chawade et al., 2019).

    Plant

    phenotyping

    is

    the

    quantitative

    and

    qualitative

    assessment of the traits of a given plant or plant variety in a given

    environment. These traits include the biochemistry, physiology,

    morphology, structure, and performance of the plants at various

    organizational scales. Plant traits are determined by both genetic

    and environmental factors as well as non-additive interactions

    between the two. In addition, variation in one phenotypic trait

    (e.g., leaf characteristics) can result in variation in other plant

    traits (e.g., plant biomass or yield). Therefore, phenotyping large

    numbers of plant varieties for multiple traits across multiple

    environments is an essential task for plant breeders as they work

    to select desirable genotypes and identify genetic variants which

    provide optimal performance in diverse and changing target

    environments (Granier and Tardieu, 2009; Dhondt et al., 2013; Li

    et al., 2014; Foix et al., 2015; Walter et al., 2015; Costa et al., 2019;

    Pieruschka and Schurr, 2019).

    Traditionally plant traits are quantiﬁed using manual and

    destructive sampling methods. These methods are usually labor-

    intensive, time-consuming, and costly. In addition, manual

    sampling and analysis protocols generally involve many steps

    requiring human intervention, with each step increasing the

    chances of introducing mistakes. Often the plant and its organ

    is cut at ﬁxed time points or at particular phenological stages in

    order to measure its phenotypic traits. This method destroys or

    damages the plant at one time point, disallowing the temporal

    examination of the traits for individual plants during the growing

    season. For example, yield measurement (such as plant biomass

    and grain weight) is invasive and more labor intensive compare

    to the measurement of plant height and leaf chlorophyll content

    (measured by a handheld sensor). As a result of the labor and

    resource intensive nature of plant phenotyping, many plant

    breeders rely solely on a single measurement most critical to their

    eﬀorts: yield. However, yield is considered as one of the most

    weakly inherited phenotypes in crop breeding (Richards et al.,

    2010; Furbank and Tester, 2011). The measurement of other traits

    in addition to yield can increase the accuracy with which yield

    can be predicted across diverse environments. Enabling high-

    throughput and non-destructive measurements of plant traits

    from large numbers of plants in multiple environments would

    therefore lead to increases in breeding eﬃciency (McMullen

    et al., 2009; Andrade-Sanchez et al., 2013; Fahlgren et al., 2015;

    Foix et al., 2018; Vijayarangan et al., 2018; Ge et al., 2019;

    Hassanijalilian et al., 2020a).

    In recent years, high-throughput systems and workﬂows have

    been developed to monitor and measure large populations of

    plants rapidly in both greenhouse and ﬁeld environments. These

    systems combine modern sensing and imaging modalities with

    the sensor deployment technologies (including conveyor belts,

    ground and aerial vehicles, and ﬁeld gantries) to enable fast

    measurement and wide area coverage (Busemeyer et al., 2013;

    Ge et al., 2016; Virlet et al., 2017; Hassan et al., 2019). Although

    not fully autonomous, these systems represent the state of the art

    in modern plant phenotyping with several advantages over the

    traditional, manually collected phenotypic traits.

    Robotic systems have been playing a more signiﬁcant role

    in modern agriculture and considered as an integral part of

    precision agriculture or digital farming (Wolfert et al., 2017;

    Chlingaryan et al., 2018; Zhang et al., 2019; Hassanijalilian et al.,

    2020b; Jin et al., 2020; Pandey et al., 2021). The robots are

    fully autonomous and do not need experienced operators to

    accomplish farming tasks. This is the biggest advantage of the

    robots compared to tractor-based systems (White et al., 2012).

    Autonomous robots have taken over a wide range of farming

    operations including harvesting [Arad et al., 2020 (sweet pepper);

    Hemming et al., 2014 (sweet pepper); Lili et al., 2017 (tomato);

    van Henten et al., 2002 (cucumber); Hayashi et al., 2010; Xiong

    et al., 2020 (strawberry); Silwal et al., 2017 (apple)], pest and

    weed control [Raja et al., 2020 (tomato and lettuce); Oberti

    et al., 2016 (grape); Åstrand and Baerveldt, 2002 (sugar beet);

    Blasco et al., 2002 (lettuce)], spraying [Hejazipoor et al., 2021

    (Anthurium); Gonzalez-de-Soto et al., 2016 (wheat); Adamides

    et al., 2017 (grape)], and pruning [Zahid et al., 2020 (apple);

    Chonnaparamutt et al., 2009; Ishigure et al., 2013 (cedar and

    hinko trees)]. Together with imaging and sensing, autonomous

    robotic systems are also deemed essential and integral parts

    for high-throughput plant phenotyping, as they will enhance

    substantially the capacity, speed, coverage, repeatability, and cost-

    eﬀectiveness of plant trait measurements.

    In this paper, we reviewed the latest development of robotic

    technologies in high-throughput plant phenotyping. We deﬁne

    the robotic technologies as a system having three components:

    (1) a sensing module that senses the target (plants or crops)

    and its environment, (2) a computational module to interpret

    the sensed information and form adaptive (or context-speciﬁc)

    decisions, and (3) an actuation module to complete certain

    desired operations (e.g., robotic probing, trait measurements, and

    navigation). For example, the robot makes decision based on the

    existing status of environment, obstacles, and plant geometry to

    manipulate a robotic arm to locate an imaging system with less

    occlusion and collision free close to plant organs, ﬁnd appropriate

    target point on the leaf and control the end-eﬀector based on

    the leaf angle for eﬀective grasping, or accurately navigate the

    ground-based vehicles between crop rows. With this deﬁnition,

    systems like LemnaTec’s conveyor-based phenotyping platform

    (Fahlgren et al., 2015; Ge et al., 2016) was not considered in the

    review, because the plant movement usually follows a pre-deﬁned

    schedule and no adaptive decision is made during phenotyping.

    Also not considered in this review are self-propelled ground

    vehicles or unmanned aerial vehicles (Bai et al., 2016; Han et al.,

    2018) that are merely used as a sensor deployment platform with

    no automated path planning or navigation.

    Diﬀerent artiﬁcial intelligence (AI) technologies such as deep

    learning, fuzzy logic, and genetic algorithms are actively used

    for control of the phenotyping robots. In recent years, deep

    Frontiers in Plant Science | www.frontiersin.org

    2

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    learning techniques has gained increased interest to guide robotic

    manipulators and mobile platforms. In this regard, deep neural

    networks (DNNs) are commonly used to detect diﬀerent objects

    in images such as crop rows, plant organs, soil, and obstacles.

    DNNs are typically operate directly on raw images and actively

    learn a variety of ﬁlter parameters during the training of a model

    (Pound et al., 2017; Irshat et al., 2018). Aguiar et al. (2020)

    presented a DNN models to detect the vine trunks as reliable

    features and landmarks to navigate a mobile robot in a vineyard.

    Parhar et al. (2018) used variation of Generative Adversarial

    Network (GAN) to detect the stalk of sorghum in the ﬁeld and

    grasp it by a robotic manipulator.

    There are three motivations behind writing this review paper.

    Firstly, robotic technologies in agriculture have seen rapid

    advancement recently with many emerging applications in plant

    phenotyping. A timely review of the literature is warranted

    to summarize the newest development in the ﬁeld. Secondly,

    there is large and growing interest from the plant breeding and

    plant science communities in how these new technologies can

    be integrated into research and breeding programs to improve

    phenotyping throughput and capacity (Furbank and Tester, 2011;

    Fiorani and Schurr, 2013; Araus and Cairns, 2014). Thirdly,

    robotic phenotyping has advanced through cross-disciplinary

    collaborations between engineers and plant scientists. Outlining

    capabilities, goals and interests across these two very diﬀerent

    disciplines may help readers to identify research gaps and

    challenges as well as provide insight into the future directions of

    the plant phenotyping robotic technologies.

    REVIEW: MANY INDOOR AND

    OUTDOOR ROBOTS WERE DEVELOPED

    TO MEASURE A WIDE RANGE OF PLANT

    TRAITS

    Phenotyping

    robotic

    systems

    have

    emerged

    to

    automate

    the phenotyping process in diﬀerent aspects. The robotic

    manipulators and ground-based vehicles are used as platforms

    to attach diﬀerent sensors to collect data rapidly and with

    higher repeatability. Robotic systems are deployed to collect and

    measure the human-deﬁned phenotypic traits (such as plant

    height, and leaf area). Additionally, in some cases it is needed

    to collect repeated measurements of plant traits within large

    populations at several time points during a growing season.

    Robotic systems are highly desirable in this scenario as they

    provide the necessary speed and accuracy for this kind of

    phenotyping tasks.

    Robotic platforms for plant phenotyping applications can

    be divided into two categories: those developed for indoor or

    controlled environments (greenhouse or laboratory), and those

    for outdoor environments (ﬁeld) (Shaﬁekhani et al., 2017). In

    controlled environment, plants are either placed in a ﬁxed

    position and the robot moves around the facility to interact with

    the plants, or the plants are moved by conveyor belts or other

    automated systems to a ﬁxed location where the robot operates.

    Often the robotic system does not need to touch the plants. The

    robotic arm is equipped with RGB cameras or depth sensors

    [Time of Flight (TOF) cameras or 3D laser scanners] to acquire

    visible images or point cloud data. The morphological traits of

    the plants are then estimated from the reconstructed 3D model

    of the plants. Stem height and leaf length of corn seedlings were

    measured using a robotic arm at a ﬁxed position and a TOF

    camera (Lu et al., 2017). Chaudhury et al. (2017) developed a

    gantry robot system consisted of a 3D laser scanner installed on

    the end-eﬀector of a seven Degree of Freedom (DOF) robotic

    arm to compute the surface area and volume of Arabidopsis

    and barley. The settings of both robotic systems were unable

    to position the vision system to capture images from the leaves

    hidden by other leaves or the stem. This occlusion problem is

    common in image-based phenotyping (Das Choudhury et al.,

    2019). Even with imaging from multiple views (e.g., enabled by

    rotating plants during image acquisition), occlusion can still be

    substantial. The use of imaging systems carried by a robotic

    manipulator can provide viable solution to this issue, due to

    the ﬂexibility of the robotic manipulator to position and orient

    cameras at the best intended viewpoints. Wu et al. (2019)

    proposed an automated multi-robot system, which comprised of

    three robotic arms each equipped with a depth camera to obtain

    the point cloud data of the plant (Figure 1A). Deep learning

    based next-best view (NBV) planning pipeline was presented to

    evaluate and select the next-best viewpoints to maximize the

    information gain from the plant in data acquisition process. The

    robotic arms then were manipulated based on the determined

    optimal viewpoints. Their system was more eﬃcient and ﬂexible

    compared to other robotic systems to address the occlusion issue.

    The ability of the system to ﬁnd the optimal viewpoints, however,

    can be challenging, because its performance depends upon the

    predictions produced by the trained deep networks. This means

    that the best view-points may not be determined by the system if

    the deep networks can not generate accurate predictions.

    A second group of indoor plant phenotyping robots sought

    to touch or probe plants or plant organs, in order to

    extend the ability of robotic phenotyping from plant’s outward

    morphological traits to innate physiological and biochemical

    traits (Schulz and Baranska, 2007; Biskup et al., 2009). In this

    sense, the phenotyping robot was designed to mimic humans

    to manipulate plants and measure certain traits from targeted

    plant organs (Figure 2). This type of the robotic systems usually

    included a robotic gripper designed to attach specialized plant

    sensors, and a vision module to segment the plant from the

    background and ﬁnd an appropriate point on the organs for

    probing [Alenyà et al., 2011; Shah et al., 2016; Bao et al.,

    2017 (Ficus plant)] or grasping process [Alenya et al., 2013;

    Ahlin et al., 2016 (Anthurium, Pothos, and Dieﬀenbachia)].

    A sensor-equipped robot was presented to measure physiological

    parameters of the plant (Bao et al., 2019c). The sensor unit

    including RGB, hyperspectral, thermal, and TOF cameras, and a

    ﬂuorometer were attached to a robotic arm. The robot measured

    the reﬂectance spectra, temperature, and ﬂuorescence by imaging

    the leaf or placing probes with millimeter distance from the leaf

    surface (Figure 1B). Two diﬀerent plant phenotyping robotic

    systems were introduced to measure leaf and stem properties

    of maize and sorghum plants (Ateﬁ et al., 2019, 2020). The

    Frontiers in Plant Science | www.frontiersin.org

    3

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 1 | Plant phenotyping robotic systems for indoor environment: (A) A multi-robot
    system equipped with deep learning technique to determine optimal

    viewpoints for 3D model reconstruction (Wu et al., 2019), (B) Sensor-equipped
    robot to measure the reﬂectance spectra, temperature, and ﬂuorescence of leaf
    (Bao

    et al., 2019c), (C) Robotic system to measure leaf reﬂectance and leaf temperate
    (Ateﬁ et al., 2019), and (D) Robotic system for direct measurement of leaf

    chlorophyll concentrations (Alenyá et al., 2014).

    systems consisted of a TOF camera, a four DOF robotic

    manipulator, and custom-designed grippers to integrate the

    sensors to the robotic manipulator. Image-processing and deep-

    learning based algorithms were presented to ﬁnd the grasping

    point on leaves and stem. An optical ﬁber cable (attached to

    a spectrometer) and a thermistor were used to collect leaf

    hyperspectral reﬂectance and leaf temperature simultaneously.

    The stem diameter was measured by a linear potentiometer

    sensor. Leaf hyperspectral reﬂectance was used to build predictive

    models for leaf chlorophyll content, water content, N (nitrogen),

    P (phosphorus), and K (potassium) concentrations (Figure 1C).

    Alenyà Ribas et al. (2012) mounted a SPAD meter to a robotic

    arm to directly measure leaf chlorophyll concentrations of

    Anthurium White, Anthurium Red, and Pothus (Figure 1D).

    Quadratic surface models were applied to segment leaves from

    infrared-intensity images and depth maps captured by a TOF

    camera. The estimation issues of probing point caused by

    poor leaf-ﬁtting model reduced the probing success rate of the

    robotic system (82%).

    Although controlled environments can make it easier to grow

    plants and quantify their phenotypic traits, because environment

    plays a large role in determining plant traits, plants grown in

    controlled environments show many diﬀerences from plants

    grown in ﬁeld conditions. Therefore, with the exception of a

    growing range of horticultural crops where production occurs

    in control environments, for many crops the assessment of

    phenotypic responses in ﬁeld conditions provides more directly

    actionable information for crop improvement. A wide range of

    platforms have been developed for ﬁeld-based high-throughput

    plant phenotyping [Montes et al., 2007; White et al., 2012;

    Gao et al., 2018 (soybean); Weiss and Biber, 2011 (detection

    and mapping of maize plants); Jenkins and Kantor, 2017 (stalk

    detection of sorghum); Iqbal et al., 2020 (plant volume and

    height); Smitt et al., 2020 (fruit counting of sweet pepper and

    tomato)]. These robotic systems are guided between crop rows

    and moved toward plants. This creates several new challenges

    for both navigation and data collection which are absent when

    robotic phenotyping is conducted in control conditions. Factors

    like temperature, sunlight, wind, and unevenness of soil surface,

    can negatively impact the performance of the system. Therefore,

    the hardware and software of the robotic system must be designed

    to be resilient to the unique challenges of operating in ﬁeld

    conditions. In the ﬁeld plants are always stationary, necessitating

    that (1) phenotyping robots move to the plants rather than vice

    versa, (2) all components of the phenotyping robot including the

    vision system, robotic arm, and sensors as well as power supplies

    be carried by a robotic mobile platform, and (3) this platform

    be capable of navigation whether through global positioning

    system (GPS) data and/or employing sensors to perceive its local

    environment to guide navigation.

    Unmanned ground vehicle (UGV) robotic systems employ

    a range of sensor types including light detection and ranging

    (LIDAR) and cameras [RGB, TOF, near infrared (NIR), and

    stereo vision] for data collection. They can be installed on a ﬁxed

    stand within the overall mobile platform, or aﬃxed a robotic

    arm to increase the number of diversity of positions from which

    sensor data can be collected. Diﬀerent techniques such as 3D

    reconstruction, image processing, and machine learning are used

    for data analysis and quantify morphological traits. Existing UGV

    robotic systems have been employed to measure plant height,

    plant orientation, leaf angle, leaf area, leaf length, leaf and stem

    width, and stalk count of various species such as maize, and

    sorghum, sunﬂower, savoy cabbage, cauliﬂower, and Brussels

    sprout (Jay et al., 2015; Fernandez et al., 2017; Baweja et al.,

    2018; Choudhuri and Chowdhary, 2018; Vázquez-Arellano et al.,

    2018; Vijayarangan et al., 2018; Bao et al., 2019b; Breitzman et al.,

    2019; Qiu et al., 2019; Young et al., 2019; Zhang et al., 2020),

    count the cotton bolls (Xu et al., 2018), architectural traits and

    density of peanut canopy (Yuan et al., 2018), berry size and color

    of grape (Kicherer et al., 2015), and shape, volume, and yield

    estimation of vineyard (Lopes et al., 2016; Vidoni et al., 2017).

    A compact and autonomous TerraSentia rover equipped with

    three RGB cameras and a LIDAR was demonstrated to acquire

    in-ﬁeld LIDAR scans of maize plants to extract their Latent Space

    Phenotypes (LSPs) (Gage et al., 2019). They were inferred from

    the images using machine learning methods (Ubbens et al., 2020)

    and contained information about plant architecture and biomass

    distribution. Shaﬁekhani et al. (2017) introduced a robotic system

    Frontiers in Plant Science | www.frontiersin.org

    4

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 2 | Manual measurements of leaf reﬂectance (left), leaf temperature (middle),
    and chlorophyll content (right) (Ateﬁ et al., 2019).

    (Vinobot) including a six DOF robotic arm with a 3D imaging

    sensor mounted on a mobile platform. The Vinobot collected

    data in order to measure plant height and leaf area index (LAI) of

    maize and sorghum (Figure 3A). The authors reported that the

    use of a semi-autonomous approach created most of challenges

    for navigation of the system. In this approach, the alignment of

    the robot with the crop rows was required before autonomously

    moving between the rows and collecting data from the plants.

    Measurements

    of

    some

    biochemical

    and

    physiological

    properties require a direct contact between sensors and plants.

    Measuring these properties therefore requires a robot capable

    of grasping or touching plant organs. Grasping plant organs in

    turn requires a dexterous robotic arm as well as onboard sensors

    and algorithms capable of reconstructing the 3D geometry of the

    target plant. Robotanist, a UGV equipped with a custom stereo

    camera, was established to measure stalk strength of sorghum

    (Mueller-Sim et al., 2017; Figure 3B). A three DOF robotic arm

    along with a special end-eﬀector was mounted on Robotanist.

    The end-eﬀector consisted of a rind penetrometer that was

    modiﬁed by attaching a force gauge and a needle. When the

    stalk was grasped by the end-eﬀector, the needle and force gauge

    were pushed into the stalk to accomplish the measurement.

    The authors suggested to develop algorithms using laser scan

    and navigation camera data to improve the performance of the

    navigation system to reliably work under taller sorghum canopy

    and throughout the entire growing season. Abel (2018) attached

    a spectrometer to the robotic manipulator of Robotanist to

    capture spectral reﬂectance measurements of leaves and stems

    of sorghum. Random sample consensus (RANSAC) method was

    used for leaf and stem detection. A machine learning approach

    was applied to estimate the chlorophyll content of leaves, and

    moisture and starch contents of stems from reﬂectance spectra.

    Two factors reduced the grasping success rate of leaves (68%).

    First, the grasping process was failed because the wind moved

    the leaves and changed the position of the grasping point.

    Second, the occlusion and overlapping aﬀected the performance

    of the segmentation algorithms to detect more leaves in the

    images. Chen et al. (2021) developed a robotic system including

    LeafSpec (invented at Purdue University) attached to a robotic

    manipulator to collect hyperspectral images of maize leaves in

    the ﬁeld (Figure 3C). The robot slid the LeafSpec across the

    leaf from the beginning to tip to acquire hyperspectral images

    of entire leaf. The system predicted leaf nitrogen content with

    R2 = 0.73.

    Other autonomous ground-based systems were presented

    to measure both morphological and biochemical/physiological

    attributes. A visible and near infrared (VIS/NIR) multispectral

    camera was mounted on a mobile robot called “Thorvald I” to

    measure the normalized diﬀerence vegetation index (NDVI) of

    wheat from multispectral images (Burud et al., 2017). The robot

    then modiﬁed to a new version called “Thorvald II” to have better

    performance for phenotyping tasks (Grimstad and From, 2017;

    Figure 3D). BoniRob was proposed as an autonomous robot

    platform including spectral imaging and 3D TOF cameras which

    can be used to measure plant parameters such as plant height,

    stem thickness, biomass, and spectral reﬂection (Ruckelshausen

    et al., 2009; Biber et al., 2012; Figure 3E). Underwood et al.

    (2017) introduced a ground-based system (Ladybird) for row

    phenotyping of grain and legume crops (wheat, faba bean, lentil,

    barley, chickpea, and ﬁeld pea) (Figure 3F). Crop height, crop

    closure, and NDVI were determined after processing the data

    from the LIDAR and the hyperspectral camera. Flex-Ro, a multi-

    purpose ﬁeld robotic platform was used for high-throughput

    plant phenotyping to measure phenotyping traits of soybean

    (Murman, 2019; Figure 3G). Three sets of sensors were installed

    on Flex-Ro to collect data from crop rows. For each set, a passive

    ﬁber optic cable, a RGB camera, an ultrasonic distance sensor,

    and an infrared radiometer were used to measure NDVI, canopy

    coverage, canopy temperature, and height.

    Table 1 summarizes the indoor and outdoor robotic systems

    which could successfully measure plant traits for diﬀerent crops.

    Figure 4 gives summary statistics regarding the plant

    phenotyping robotic systems that is discussed in this section.

    It can be seen that the robotic phenotyping research targeted

    maize and sorghum more than other species (soybean, wheat,

    barley, chickpea, pea, faba bean, lentil, cabbage, cauliﬂower,

    cotton, peanut, sunﬂower, grape, tomato, sweet pepper, and

    Arabidopsis) (Figure 4A). Maize and sorghum are two of the

    most economically important and highly diverse cereal crops

    with vast numbers of accessions (Zhao et al., 2016; Bao et al.,

    2019b). Therefore, more attention was devoted to breed Maize

    and sorghum to produce food, animal fodder, and biofuel.

    Moreover, the available genetic resources for these crops required

    the phenotyping data to map their genotypes to phenotypes and

    Frontiers in Plant Science | www.frontiersin.org

    5

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 3 | Plant phenotyping systems for outdoor environment: (A) Vinobot: robotic
    system including six DOF robotic manipulator and a 3D imaging sensor

    mounting on a mobile platform to measure plant height and LAI (Shaﬁekhani et al.,
    2017), (B) Robotanist: UGV-based robotic system equipped with a three DOF

    robotic manipulator and a force gauge for stalk strength measurement (Mueller-Sim
    et al., 2017), (C) A robotic system to slide LeafSpec across entire leaf to collect

    its hyperspectral images (Chen et al., 2021), (D) Thorvald II: VIS/NIR multispectral
    camera mounted on a mobile robot to measure NDVI (Grimstad and From, 2017),

    (E) BoniRob: autonomous robot platform using spectral imaging and 3D TOF cameras
    to measure plant height, stem thickness, biomass, and spectral reﬂection

    (Biber et al., 2012), (F) Ladybird: ground-based system consisted of a hyperspectral
    camera, a stereo camera, a thermal camera, and LIDAR to measure crop

    height, crop closure, and NDVI (Underwood et al., 2017), and (G) Flex-Ro: high-throughput
    plant phenotyping system equipped with a passive ﬁber optic, a RGB

    camera, an ultrasonic distance sensor, and an infrared radiometer for the measurement
    of NDVI, canopy coverage, and canopy height (Murman, 2019).

    thus crop yield improvement. Accordingly, there has been an

    emerging need for phenotyping robots to automatically measure

    the phenotypic traits. Regarding the plant structure, maize, and

    sorghum have similar morphology. Their leaves are arranged

    alternately on each side of the stem that has cylindrical/elliptic-

    cylinder shape and is positioned in the middle part of the plant.

    This plant structure provides less complexity for the robotic

    system to distinguish between the stem and leaves and extract

    their features. Figure 4B shows that the height, width, and

    volume of plant/canopy are three main (morphological) traits

    that more frequently measured by the robotic systems than other

    traits, each of them being ≤ 5% (leaf length, leaf width, leaf angle,

    leaf area, leaf reﬂectance, leaf chlorophyll content, leaf/canopy

    temperature, LAI, plant/canopy NDVI, stem reﬂectance, stalk

    strength, stalk count, berry size, and fruit count). Two reasons

    can be considered for the frequent measurements of these

    phenotypic traits. Firstly, the plant architectural traits (such as

    plant height) are the most common and important parameters

    for ﬁeld plant phenotyping since they have signiﬁcant eﬀects

    on light interception for photosynthesis, nitrogen availability,

    and yield (Barbieri et al., 2000; Andrade et al., 2002; Tsubo and

    Walker, 2002). Consequently, by studying and then manipulation

    of the plant architecture, the crop productivity will be increased.

    Secondly, as it was discussed in this section, the robot just needs

    non-contact based sensors (RGB camera or depth sensor) to

    collect data from the plants. Then, by analyzing the 2D images

    or creating plant 3D models, the aforementioned plant traits can

    be estimated in either ways: (1) the correlation between the pixel

    counts in the images and the ground truth measurements, or

    (2) extracting the distance/volume in real world from the depth

    sensor data. Hence, the measurement of these morphological

    properties is less challenging for the phenotyping robots using

    simple sensors and algorithms. In addition to the more frequent

    measurements of stem height and width (of maize and sorghum),

    these properties were also measured more accurately by the

    robotic systems because they are less aﬀected by the plant

    morphology (Figure 4C). The ﬁrst step to extract the stem height

    and width is to detect the stem and segment it from other

    plant organs. The morphology of maize and sorghum (alternately

    arranged leaves, and cylindrical-shaped stem in the middle)

    provides more hints for stem detection and segmentation.

    Moreover, the height and width can be measured as linear

    measurements. Accordingly, these two stem properties can be

    measured with less complexity and higher accuracy. Figure 4D

    illustrates that non-contact based sensing systems (such as RGB,

    stereo vision, and multispectral cameras, and LIDAR) were

    Frontiers in Plant Science | www.frontiersin.org

    6

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    TABLE 1 | Summary of indoor and outdoor robotic systems that successfully measured
    plant properties for different crops.

    Robot type

    References

    Species

    Plant trait

    Performance

    Measurement method

    Software system

    Indoor

    Alenyà Ribas et al., 2012

    Anthurium Andreanum (White),

    Anthurium andreanum (Red),

    Epipremnum aureum (Pothos)

    Leaf chlorophyll content

    SR = 90%, 85%, 70%

    Contact based

    ROS

    Lu et al., 2017

    Maize

    Stem height, leaf length

    ER = 13.7%, 13.1%

    Non-contact based

    Qt development

    environment (C++)

    Ateﬁ et al., 2019

    Maize, Sorghum

    Leaf chlorophyll content, leaf

    potassium content, leaf water

    content, leaf temperature

    R2 = 0.52, 0.52, 0.61

    R2 = 0.58, 0.63

    Contact based

    MATLAB

    Ateﬁ et al., 2020

    Maize, Sorghum

    Stem diameter

    R2 = 0.98, 0.99

    Contact based

    MATLAB

    Outdoor

    Jay et al., 2015

    Sunﬂower, Savoy cabbage,

    cauliﬂower, Brussels sprout

    Plant height, leaf area

    R2 = 0.99, 0.94

    Non-contact based

    Not reported

    Shaﬁekhani et al., 2017 (Vinobot)

    Maize, sorghum

    Plant height

    R2 = 0.99

    Non-contact based

    ROS

    Abel, 2018 (Robotanist)

    Sorghum

    Stem starch content, stem

    moisture content, leaf

    chlorophyll content

    R = 0.81, 0.72, 0.92

    Contact based

    ROS

    Baweja et al., 2018 (Robotanist)

    Sorghum

    Stalk count, stalk width

    R2 = 0.88,

    MAE = 2.77 mm

    Non-contact based

    ROS

    Choudhuri and Chowdhary, 2018

    Sorghum

    Stem width

    Accuracy = 98.2%

    Non-contact based

    Python

    Vázquez-Arellano et al., 2018

    Maize

    Plant height

    AME = 8.7 mm

    SD = 35 mm

    Non-contact based

    Not reported

    Vijayarangan et al., 2018

    Sorghum

    Leaf area, leaf length, leaf width

    Relative RMSE = 26.15%,

    26.67%, 25.15%

    Non-contact based

    ROS

    Bao et al., 2019b

    Maize

    Plant height, leaf angle

    R2 = 0.96, 0.83

    Non-contact based

    Not reported

    Qiu et al., 2019

    Maize

    Plant height

    RMSE = 0.058 m

    Non-contact based

    ROS

    Young et al., 2019

    Sorghum

    Plant height, stem width

    AE = 15%, 13%

    Non-contact based

    Not reported

    Zhang et al., 2020

    Maize

    Stand counting

    R = 0.96

    SD = 6.76%

    Non-contact based

    Not reported

    ER, error ratio; SR, success rate, MAE, mean absolute error; RMSE, root mean square
    error; AME, absolute mean error; AE, absolute error; SD, standard deviation; ROS,
    robot operating system.

    Frontiers in Plant Science | www.frontiersin.org

    7

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 4 | Summary statistics of the phenotyping robotic systems: (A) Targeted
    plants, (B) Plant/canopy traits measured by the robots, (C) Average accuracy (R2)

    to measure the phenotypic traits, (D) Robot vision/sensing systems, and (E) Robot
    software systems.

    used more in phenotyping robots compared to contact-based

    sensors (chlorophyll meters, spectrometers, thermistors, and

    linear potentiometers). This can be explained by the fact that the

    majority of the robotic systems were developed to measure the

    morphological traits or some physiological properties. To achieve

    these goals, the phenotyping robots are required to use the 2D

    images/3D models of plants using non-contact based sensors.

    Among the non-contact sensors, the sensor-fusion based systems

    (including RGBD/stereo vision cameras, RGB camera + LIDAR,

    RGB + TOF cameras, spectral imaging + TOF camera) and

    depth sensors (TOF camera, LIDAR, laser scanner, and ultrasonic

    sensor) were commonly used as vision/sensing systems for the

    phenotyping robots. The key is to acquire depth information as

    a vital parameter to manipulate a robotic arm to grasp the plant

    organs, navigate a mobile robot between crop rows, and measure

    plant properties (such as height, width, and volume). Sensor-

    fusion based systems were employed by phenotyping robots more

    often than depth sensors. The reason would be that these sensors

    prepare the plant color/spectral information along with the depth

    information. Consequently, by acquiring more information, the

    plant can be eﬀectively segmented from the background and the

    plant properties can be eﬀectively measured. Regarding the robot

    software system, it can be found that Robot Operating System

    (ROS) is one the most popular systems to develop the software

    of the phenotyping robots (Figure 4E). ROS is an open source

    system that provides services, libraries, and tools for sensors-

    actuators interface, software components communication, and

    navigation and path planning1. Diﬀerent manufacturers of robot’s

    hardware provide ROS drivers for their products such as imagery

    systems, sensors, actuators, robotic manipulators, and mobile

    platforms. This allows the researchers to develop the phenotyping

    robotic systems more eﬃciently.

    1https://www.ros.org/

    Frontiers in Plant Science | www.frontiersin.org

    8

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Regarding the platform of the mobile phenotyping robots,

    most of the mobile platforms were developed by researchers

    (86%) and few oﬀ-the-shelf robots were used (14%). The custom-

    designed platform oﬀers potential to meet speciﬁc conditions

    regarding soil, weather, and plant which vary with experimental

    site and phenotyping task. Moreover, the researcher has more

    control on modifying the hardware and software. Based on

    the reviewed papers, most of the mobile platforms used four

    wheels/legged-wheels (88%) as their driving system compared

    with tracked mechanism (12%). In the wheeled-vehicles, the

    wheels can be independently steered (good maneuverability)

    which provides high ﬂexibility with respect to control and

    navigation (desired orientation angle and rotation speed) of the

    vehicles between the crop rows in the ﬁeld. Moreover, the vehicle

    can move faster using (legged) wheels and has high ground

    adaptability (crop height, and irregular and sloped terrain)

    using legged-wheels. However, the tracked-vehicles create more

    traction and less pressure on soil (work better in wet soil and less

    soil compaction), and can drive over rough terrain and obstacles

    easier than the wheeled-vehicles (Bruzzone and Quaglia, 2012).

    Accordingly, a hybrid locomotion system can be developed

    with the combination of legged-wheels and tracked systems.

    Therefore, this platform can use the advantages of the both

    driving systems to accomplish phenotyping tasks more eﬀectively

    and eﬃciently.

    PHENOTYPING ROBOTS FACE SEVERAL

    CHALLENGES

    There are several outstanding challenges in the development of

    robotic systems for plant phenotyping. Some of these challenges

    related to segmentation (vision systems) and grasping (robotic

    manipulators) are shared or at least similar for both indoor

    and outdoor phenotyping robots. Other challenges, particularly

    those related to navigation are speciﬁc to outdoor robotic

    phenotypic applications.

    Complex and Deformable Nature of

    Plants Represents a Major Issue for

    Robot’s Vision and Sensing System

    The UGV or robotic manipulator equipped with contact/non-

    contact based sensing systems oﬀer a great potential to measure

    plant phenotypic data compare to non-autonomous robotic

    sensing systems. For example, the UGV equipped with stereo

    vision camera can move between crop rows and collect images

    from the canopy or individual plant. The image data can be

    analyzed immediately or can be processed later to extract plant

    properties. The long-term measurement of the plant traits can

    provide useful knowledge for crop modeling purposes over time

    (Duckett et al., 2018). Another example would be the robotic

    manipulator equipped with a hyperspectral imaging system.

    The robotic arm can move around the plant to locate the

    sensor close to the plant organs. With this proximal sensing,

    more phenotyping information can be acquired about the

    organs. However, the robotic vision/sensing technologies for the

    phenotyping task encounter diﬀerent challenges.

    Various imaging technologies are utilized as vision systems of

    the robots. Visible imaging/RGBD camera are commonly used

    technologies that rely on the color/texture information of an

    object. Images are processed to segment plant organs and identify

    desirable targets for grasping. The identiﬁcation and localization

    of diﬀerent plant organs (such as leaves, stems, ﬂowers, and fruits)

    is one of the major problems in computer vision, due to complex

    structure and deformable nature of plants. The overlap between

    the adjacent leaves or leaf-stem causes occlusion; even though

    leaf and stem have diﬀerent architecture, they share similarities

    in color and texture. Accordingly, it is diﬃcult to distinguish

    occluded leaves or stem in the image. The morphology of

    plants (shape and size) varies dramatically across diﬀerent plant

    species and even within a single species diﬀerent varieties or

    the same variety grown in diﬀerent conditions may exhibit

    radically diﬀerent morphology. In this regard, the software of

    the robotic system should cover a wide range of scenarios and

    possibilities to be able to respond and adapt appropriately to day-

    to-day changes in the same plant or diﬀerences between plants

    within the same experiment. Additionally, non-uniform imaging

    conditions (lighting and background) make it more complex to

    ﬁnd an appropriate color space and optimal approach for the

    segmentation purposes (Zhang et al., 2016; Narvaez et al., 2017;

    Qiu et al., 2018; Bao et al., 2019a).

    Multispectral/hyperspectral and thermal imaging systems are

    sensitive to illumination since the reﬂectance from the plant

    organ is depend on its distance and orientation toward the

    light source/incident radiation and camera. Moreover, multiple

    reﬂectance and also shade will occur due to the curvature nature

    and complex geometry of plant (Li et al., 2014; Mishra et al., 2017;

    Qiu et al., 2018). To deal with these issues, researchers introduced

    diﬀerent technical solutions. Behmann et al. (2016) combined the

    hyperspectral image with 3D point cloud (using a laser scanner)

    of sugar beet to create hyperspectral 3D model. Then, it was used

    to quantify and model the eﬀects of plant geometry and sensor

    conﬁguration. Finally, the geometry eﬀects in hyperspectral

    images were weakened or removed using reﬂectance models.

    Shahrimie et al. (2016) used inverse square law and Lambert’s

    cosine law along with Standard Normal Variate (SNV) for maize

    plants to remove the distance and orientation eﬀects.

    Robotic Control System Needs to Deal

    With Dynamic and Unstructured

    Environment

    The size and orientation of the plant organs are constantly

    changing across their growth stages. Therefore, the lack of needed

    DOF or enough workspace of the robotic manipulator are the

    limitations for the robots to grasp the plant organs and sense their

    properties successfully. The robotic arm cannot reach the organs

    if they are out of its workspace. In addition, a robot arm with less

    ﬂexibility (DOF) might not able to properly adjust the angle of its

    end-eﬀector in grasping process.

    Field-based robots need to navigate between crop rows and

    then turned to the next row safely and autonomously. To achieve

    Frontiers in Plant Science | www.frontiersin.org

    9

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    this task, the crop rows and obstacles should be detected to

    build a map of the surrounding area. Then, their position and

    orientation relative to the vehicle will be found to compute an

    optimal path and avoid unexpected obstacles. Finally, adequate

    action will be determined to steer the wheels and guide the system

    around the ﬁeld. However, the uncontrolled and unstructured

    ﬁeld environment creates challenges for accurate navigation and

    control of the robot (De Baerdemaeker et al., 2001; Nof, 2009;

    Bechar and Vigneault, 2016; Shamshiri et al., 2018). GPS is a

    common method for robot navigation. However, the tall plant

    canopy aﬀects on the accuracy of GPS for navigation purposes

    as the canopy blocks the satellite signals to the GPS receiver.

    Hence, the information provided from other sensors along with

    GPS is also required to detect the obstacles, precisely guide

    the phenotyping robot, and minimize the damage to the robot

    and plants. The UGV based phenotyping robots can facilitate

    the data fusion of GPS and other sensors since the robot can

    equipped with various sensors (such as LIDAR, RGB/stereo

    vision cameras) and also precisely control their location (Duckett

    et al., 2018). Nonetheless, varying ambient light conditions,

    changing crop growth stages (size, shape, and color), and similar

    appearance between crops and weeds are common factors that

    fail visual navigation. In these situations, RGB sensor-based

    systems usually cannot ﬁnd a stable color-space or plant features

    to detect diﬀerent objects. Incomplete rows and missing plants

    can cause errors to compute distance between the robot and

    plants using range sensors. Diﬀerent soil properties (soil types

    and moisture) and terrain variation (even, uneven, ﬂat, slope) are

    other factors that inﬂuence robot dexterous manipulation, wheel-

    terrain interaction, wheel slip, and steering control algorithms

    (Åstrand and Baerveldt, 2005; Grift et al., 2008; Li et al., 2009;

    Shalal et al., 2013).

    After navigating the robot between the rows, a suitable

    path should be selected for the robotic manipulator with

    minimum collisions inside a plant or canopy to reach and

    grasp the targets delicately. However, robots operate in extremely

    complex, dynamic, uncertain, and heterogenous real world

    condition. In this situation, visual occlusion of a plant by others

    caused by high plant density should be taken into account

    for target identiﬁcation and segmentation. In addition, the

    target information will be aﬀected by sunlight and wind. For

    instance, TOF/RGBD cameras use infrared light to measure

    distance. Since the sunlight has infrared wavelengths and wind

    moves the targets, the location of the target in 3-dimensional

    space might not be accurately measured (Andújar et al., 2017;

    Narvaez et al., 2017; Qiu et al., 2018; Li et al., 2020a).

    Consequently, the obstacle-avoidance path-planning algorithm

    cannot be determined correctly. Another example would be

    when the targets are seen shinier or darker because of specular

    reﬂection or shade.

    Issues With Robot Software for

    Phenotyping Robotic System

    Development

    Two main drawbacks present in many robot software are: (1)

    the lack of support for certain functional packages (of open

    source software) and (2) real-time constraints (Barth et al., 2014;

    Park et al., 2020). For the ﬁrst issue, it can be supposed that a

    phenotyping robot is developed by researchers to accomplish a

    phenotyping task. They create the robot library and share their

    codes with the (open source) software community. However, by

    ending the project, there is no guarantee to ﬁx the bugs and

    update the codes. In the case of other researchers might start

    similar research using the shared codes, it might be problematic

    to make the research forward because of the lack of support

    for the robot library. The second challenge is the real-time

    constraints that causes system malfunction due to latency. One

    example would be when a UGV moves between crop rows to

    measure plant traits. If the robot cannot satisfy the real-time

    constraints, the robot will have delay to identify the obstacles

    or adjust its position relative to the crop rows. Accordingly,

    the robot could hit the obstacles and the plants and this

    causes the physical damage to the robot or plants. Regarding

    ROS, although ROS1 has real-time constraints, however the

    community is actively working on software improvement.

    For example, RT-ROS supports the real-time communication

    that leads to performance enhancement of ROS1 (Wei et al.,

    2016). It is obvious that by growing the ROS community,

    sophisticated libraries and packages will be developed for more

    plant phenotyping applications.

    Other Challenges: Managing Big Data,

    Reliable Power Source, Durability Under

    Harsh Environment, and High Cost

    The phenotyping robot collects massive volumes and various

    types of data (such as images, multi/hyperspectral data) taken

    by diﬀerent sensors from large population of plants. The robot

    needs to analyze large quantities of data in real-time for suitable

    action/decision-making process. In addition, the large-scale

    phenotypic data could be stored properly for the beneﬁt of future

    research. Therefore, managing and analyzing the big data as a

    result of high-throughput, robotically collected plant traits is an

    emerging issue for the phenotyping robot.

    The ﬁeld-based mobile robots need to be equipped with

    reliable power sources to provide energy for the vehicle carriage

    weight, distance traveled, and diﬀerent electrical components

    such as sensors for data collection. Batteries are commonly used

    for this purpose. The problems with batteries are: (1) limited

    operating time that prevents the robots to work for long time and

    accomplish large-scale missions, and (2) need to recharge which

    typically takes a long time.

    Another challenge is the durability and stability of these

    robotic systems under harsh outside environment caused by

    extreme temperature, high humidity, strong sunlight, and dust.

    These harsh conditions can cause damages for the components

    of the robotic system and accordingly will have negative eﬀects

    on the robot’s performance.

    The cost of phenotyping robots (in general agricultural robots)

    is still high and this makes limitations for wide-spread use

    of the robots. In most cases the phenotyping robotic systems

    are developed for research purposes and the robots are not

    commercially available yet. Both the hardware and software

    Frontiers in Plant Science | www.frontiersin.org

    10

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    systems were restricted to a very speciﬁc condition and could

    not be transferred to a diﬀerent scenario. This leads to high

    R&D (research and development) cost that can not be spread

    over multiple units. However, more general purpose phenotyping

    robots can be developed and commercialized in the future and

    their cost will be reduced substantially. Moreover, with the

    consistent trend of price reduction of electronics, sensors, and

    computers, the robotic systems will become cost-eﬀective enough

    to be more widely used for phenotyping tasks.

    POTENTIAL IMPROVEMENTS OF

    PHENOTYPING ROBOTS

    Sensors and Controllers Fusion

    Technique Can Improve the Performance

    of Robot

    Sensing-reasoning, and task planning-execution are two essential

    functions for autonomous phenotyping robots. They sense the

    environment, apply an appropriate control algorithms, make

    decision, and act in real-time to perform the phenotyping tasks

    (Grift et al., 2008; Bechar and Vigneault, 2016). The design

    of the phenotyping robot and its control algorithm needs to

    be optimized to achieve successful operation in continuously

    changing environment. To reach this purpose, the phenotyping

    robots need to employ advanced technology to cope with the

    dynamic and unstructured environment. The emerging sensor

    technologies such as sensor fusion increase the robot capabilities

    and yield better results (Grift et al., 2008). Sensor fusion

    allows the robot to combine information from a variety of

    sensing modules to form better decision for navigation and path

    planning, as well as increase the capacity of sensing to gather

    more information from the plants. For example, Choudhuri

    and Chowdhary (2018) measured the stem width of sorghum

    with 92.5% accuracy using RGB data. However, they achieved

    higher accuracy (98.2%) after combining RGB + LIDAR data.

    Kim et al. (2012) could successfully navigate an unmanned

    weeding robot using sensor fusion of a laser range ﬁnder

    (LRF) and an inertial measurement unit (IMU). The robot

    also needs more sophisticated and intelligent algorithms to

    accomplish diﬀerent subtasks such as sensing, navigation, path-

    planning, and control. Diﬀerent control strategies such as

    genetic algorithm (GA), fuzzy logic (FL), neural network (NN),

    reinforcement learning (RL), and transfer learning (TL) can

    be integrated to develop such robot algorithms (Shalal et al.,

    2013). Therefore, a robust controller will be provided for the

    phenotyping robot since the robot control system can use the

    merits of both technologies (combining two control strategies).

    Batti et al. (2020) studied the performance of fuzzy logic

    and neuro-fuzzy (NN + FL) approaches to guide a mobile

    robot moving between the static obstacles. The authors found

    that neuro-fuzzy controller provide better results for robot

    navigation compare to fuzzy logic controller. Although several

    diﬀerent autonomous phenotyping robots were developed,

    more research is needed to adapt and improve the advanced

    technologies to overcome the robot limitations to accomplish the

    phenotyping tasks, and also increase the autonomy level of the

    phenotyping robots.

    Internet of Robotic Things (IoRT):

    Technology to Manage Big Data for

    Phenotyping Robots

    Internet of Things (IoT) technologies are helpful to send lots

    of data collected by diﬀerent sensors over Internet in a real-

    time manner. The Internet-of-Robotic-Things (IoRT) is the

    conﬂuence of autonomous robotic systems with IoT which is

    an emerging paradigm that can be employed for phenotyping

    robots (Grieco et al., 2014; Ray, 2016; Batth et al., 2018;

    Saravanan et al., 2018; Afanasyev et al., 2019). Mobile robots

    can use IoT to transfer and store a large amount of phenotypic

    datasets to a central server. By sending the data via IoT,

    the robots do not need to frequently move to a place and

    physically upload the collected data to a local server/computer.

    Moreover, plant breeders/scientists can visualize the data using a

    mobile device (a tablet or a smartphone) or an oﬃce computer

    and therefore the performance of plants and changes in crop

    growth and development can be remotely inspected in diﬀerent

    regions of the ﬁeld in a real-time fashion. Another attractive

    aspect of using IoRT is to send commands to robots to

    accomplish phenotyping tasks. For instance, an operator can

    remotely control the greenhouse robotic manipulator systems

    via Internet any time from his home/oﬃce to collect phenotypic

    data. Another example is when the close inspection of an

    area in a ﬁeld is necessary after analyzing the drone-based

    image data; therefore, commands can be sent via Internet to

    deploy mobile robots in this regard. Several mobile robots

    can work together to operate more eﬃciently to achieve

    a speciﬁc task.

    Solar Panels and Hydrogen Fuel Cell:

    Renewable Power Sources for

    Phenotyping Robots

    Solar panels and hydrogen fuel cell are two technologies that

    produce clean, renewable, and sustainable energy. A solar panel

    consists of many small units called photovoltaic cells which

    convert sunlight into electricity. The maintenance cost of the

    solar panel is low since it does not have moving parts (no

    wear) and it just need to clean the cells. The hydrogen fuel cell

    comprised a pressurized container to store hydrogen. The fuel

    cell is an electrochemical device that takes oxygen from the air

    and combines hydrogen with oxygen to produce electricity. Re-

    fueling time of a hydrogen fuel cell is very short (5 min or less)

    and its cells are fairly durable.

    Based on the advantages of solar panels and hydrogen

    fuel

    cell,

    both

    technologies

    can

    be

    used

    as

    renewable

    power sources for diﬀerent components of the phenotyping

    robots

    (Underwood

    et

    al.,

    2017;

    Quaglia

    et

    al.,

    2020).

    However, there is not a wide range of application of these

    technologies for the phenotyping robots. The cost of both

    technologies is high. For solar panels, the eﬃciency of

    the system drops in cloudy and rainy days. In addition,

    more solar panels are needed to produce more electricity

    Frontiers in Plant Science | www.frontiersin.org

    11

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    which requires a lot of space. For hydrogen fuel cell, there

    are relatively few places to re-fuel the cell. Nevertheless,

    both technologies are constantly developing which can be

    assumed to reduce their cost and improve their eﬃciency to

    produce electricity.

    PERSPECTIVE APPLICATIONS OF

    ROBOTIC PHENOTYPING

    Phenotyping Robots Has Great Potential

    to Measure Other Plant Properties

    Section “Review: Many Indoor and Outdoor Robots Were

    Developed to Measure a Wide Range of Plant Traits” introduced

    the robotic systems for indoor and outdoor applications to

    measure several diﬀerent plant traits. However, other leaf/stem

    characteristics are also reliable indicators to detect the symptoms

    of biotic/abiotic stresses and monitor the plant health during

    a growing season. Stomatal conductance, gas exchange, and

    chlorophyll ﬂuorescence of leaves are indicative of their water

    status, photosynthesis, and chlorophyll content (Castrillo et al.,

    2001; Ohashi et al., 2006; Li et al., 2020b). Stem sap ﬂow

    and lodging resistance can provide useful information about

    plant water use and stem strength (Cohen et al., 1990; Kong

    et al., 2013). These aforementioned phenotypic traits are still

    measured manually. On the other hand, new clip-on sensor

    system can be presented to measure them automatically. The

    system includes a custom-designed gripper/clip combined with

    novel sensor(s) (Afzal et al., 2017; Palazzari et al., 2017).

    The design of these sensing systems is important since the

    accuracy and robustness of trait prediction models depend on

    the phenotypic data quality (Würschum, 2019). The design

    of the gripper and DOF of the robotic manipulator should

    allow a good and gentle contact between the sensing unit

    and the leaf/stem. Sometimes a vacuum mechanism attached

    to a soft gripper can hold the leaf/stem and help the sensing

    unit for eﬀective contact and collect accurate data with less

    damage to the plant organs (Hayashi et al., 2010; Hughes et al.,

    2016; Zhang et al., 2020). Moreover, autonomous robots should

    gather data with minimum error (high signal to noise ratio).

    Therefore, sensors with high signal to noise ratio should be

    selected and accurately calibrated. In addition to the accuracy,

    the robots should rapidly (short execution time) accomplish

    their missions. Deep reinforcement learning (DRL) technique is

    an accurate and reliable method to ﬁnd an optimal path with

    nearest and collision avoidance route. This technique can be

    adopted by phenotyping robots to manipulate a robotic arm

    for grasping process or to navigate a mobile robot between

    crop rows (Zhang et al., 2015; Zhang et al., 2019; Duguleana

    and Mogan, 2016; Franceschetti et al., 2018; Taghavifar et al.,

    2019). Although the robotic phenotyping is mainly focusing

    on leaf and stem, it can be utilized for other plant organs

    such as inﬂorescences (spike, panicle, and tassel), ﬂowers,

    fruits, and roots.

    The morphometric parameters of inﬂorescence are highly

    correlated with yield and grain quality (Leilah and Al-Khateeb,

    2005; Gegas et al., 2010). Several studies discussed about

    using image-based techniques (2D images/3D reconstruction)

    to extract architectural traits such as length and width of

    inﬂorescence, inﬂorescence volume (weight), grain shape and

    size, grain angle, and number of grains, and number of ﬂowers

    (Faroq et al., 2013; Crowell et al., 2014; Gage et al., 2017; Rudolph

    et al., 2019; Sandhu et al., 2019; Xiong et al., 2019; Zhou et al.,

    2019). In such applications to measure the morphological traits,

    a robot with LIDAR/camera can be useful to automatically take

    images/point cloud data from diﬀerent views of the inﬂorescence.

    The physiological traits are indicator for stress or disease.

    For instance, the temperature of the spikes was used for

    detecting the plant under the water stress (Panozzo et al., 1999).

    Conceivably, a robotic arm equipped with a temperature sensor

    can grasp the spike and insert the sensor into spikelets to record

    their temperature.

    Several properties of fruits such as water content, sugar

    content, chlorophyll, carotenoid, soluble solid, acidity, and

    ﬁrmness are measured for fruit quality assessment. The

    spectroscopy/spectral imagery are non-destructive and high-

    throughput methods to estimate these qualitative parameters

    (Berardo et al., 2004; ElMasry et al., 2007; Shao and He, 2008;

    Wu et al., 2008; Nishizawa et al., 2009; Penchaiya et al., 2009;

    Ecarnot et al., 2013; Guo et al., 2013; Dykes et al., 2014; Wang

    et al., 2015; Mancini et al., 2020). However, a robotic system

    can be presented to monitor the dynamics of these attributes

    for hundreds of growing fruits per day. For example, a portable

    spectrometer can be attached to the robot’s end-eﬀector. After

    detecting the fruit on the plant, the robot can grasp the fruit and

    gather its spectral data to further infer its quality parameters.

    Since the root has functional roles in resource acquisition,

    the characteristics of root provide valuable information about

    plant physiological and ecosystem functioning (Mishra et al.,

    2016). In traditional root phenotyping, two diﬀerent methods

    are used to acquire images from root (in the soil or soil-free

    or transparent media). In ﬁrst method, a camera is mounted

    on a tripod and moved by a human around the root, and

    in the second method camera(s)/sensor(s) are set in ﬁxed

    point(s) and root (plant) is rotated (Atkinson et al., 2019).

    This is a tedious task and some root information (such as

    ﬁne branches) might be lost due to less ﬂexibility of the

    system to take up close images from the complex architecture

    of root. Consequently, automated root phenotyping systems

    can facilitate and improve the traditional root phenotyping in

    terms of eﬃciency and eﬀectiveness with acquiring fast and

    precise measurements (Wu et al., 2019). Here, the “plant to

    sensor” system can be used to examine vast number of roots

    (or plants) without the need of huge space of greenhouse

    facility. In this system, the root (or plant) is moved toward a

    robotic manipulator (equipped with camera/sensor) and located

    on a rotation table. In each step angle of the table, the root

    is rotated and stopped in front of the robotic system. Then,

    the robotic manipulator moves the camera around the root

    and gather close proximity data from diﬀerent views (positions

    and angles). Therefore, more detailed information of root

    can be captured due to high resolution sensing oﬀered by

    the robotic system.

    Frontiers in Plant Science | www.frontiersin.org

    12

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    FIGURE 5 | (A) Mobile Agricultural Robot Swarms (MARS) for seeding process (The
    European Coordination Mobile Agricultural Robot Swarms (MARS). PDF ﬁle.

    November 11, 2016. http://echord.eu/public/wp-content/uploads/2018/01/Final-Report-MARS.pdf),
    (B) UAV-UGV cooperative system to measure environmental

    variables in greenhouse (Roldán et al., 2016).

    Robots in Greenhouses Complement the

    Image-Based Phenotyping

    Automatic greenhouses such as LemnaTec (LemnaTec GmbH,

    Aachen, Germany) monitor plants using image-based technique.

    While it has shown great potential to measure and predict the

    plant traits, many hurdles cannot be handled by this technology.

    It needs to eﬃciently manage “big data” problems and also

    postprocess images to characterize the plant traits. Moreover, this

    approach is not suﬃcient for early detection of stress/disease

    with internal symptoms. Furthermore, this method requires

    direct measurements using sensors to calibrate and validate

    of its models to extract the phenotypic traits from images

    (Madden, 2012; Mutka and Bart, 2015; Singh et al., 2016; Lee

    et al., 2018). Hence, several robotic arms with diﬀerent sensors

    can be integrated to the greenhouse for real-time and direct

    measurement of the chemical/physiological traits. Basically,

    plants are transported by an automatic conveyor belt and stopped

    in front of each robotic system. Then, the system uses “sensor-to-

    plant” concept (Lee et al., 2018) in which the robot moves toward

    the plant to take measurements before sending it through the

    imaging chambers. These stationary robotic systems are designed

    to operate in indoor environment. Moreover, several robots can

    be presented to collect data from a speciﬁc plant. It is diﬃcult

    to develop a general prototype that are broadly applicable for

    diﬀerent conditions (Mutka and Bart, 2015; Wu et al., 2019).

    However, the software and hardware of the robots should be

    adapted to other species and ﬁeld-phenotyping applications. The

    challenge for both type of robots (indoor/outdoor) would be

    continuously collect and save large amount of data.

    Swarm Robot Is a New Frontier to

    Efﬁciently Accomplish Complex

    Phenotyping Tasks

    Swarm robotics is a new frontier technology which has potential

    application for proximal sensing of plants, and data/sample

    collection in a large ﬁeld. A swarm robotics system composed of

    large numbers of autonomous robots that are coordinated with

    local sensing and communication, and a decentralized control

    system (Brambilla et al., 2013; Bayındır, 2016; Blender et al., 2016;

    Chamanbaz et al., 2017; Figure 5A). The application of swarm

    robots has some advantages which is suitable for large scale tasks.

    Since swarm robotics has large population size, the tasks can be

    decomposed using parallelism and can be completed eﬃciently

    and consequently it would save time signiﬁcantly. Moreover, the

    swarm robots can achieve the distributed sensing that means they

    can have a wide range of sensing in diﬀerent places at the same

    time (Navarro and Matía, 2012; Tan and Zheng, 2013).

    Both UAV and UGV by itself have been successfully employed

    in plant phenotyping tasks. The coordination between UAV and

    UGV enables a new breakthrough application of UAV/UGV

    cooperative systems to achieve a common goal more eﬀectively

    and eﬃciently (Arbanas et al., 2018; Vu et al., 2018). Both vehicles

    in this cooperative team share complementarities according to

    their capabilities that allow them to operate in the same ﬁeld and

    work together to fulﬁll phenotyping missions. In this manner, the

    UAV can ﬂy to quickly obtain overview of the ﬁelds beyond the

    obstacles; whereas the UGV can continuously patrols in the ﬁeld

    with large payload capabilities of diﬀerent sensors and robotic

    arms (Chen et al., 2016; Roldán et al., 2016; Figure 5B). In the

    context of UAV-UGV cooperation, an obstacle map of the ﬁeld

    will be provided by the UAV for UGV path planning. Based on

    their communication and the map, the UGV can move rapidly

    between the crop rows for up-close plant investigation.

    CONCLUDING REMARKS

    Autonomous

    robotic

    technologies

    have

    the

    potential

    to

    substantially increase the speed, capacity, repeatability, and

    accuracy of data collection in plant phenotyping tasks. Many

    robotic systems are successfully developed and deployed in both

    greenhouse and ﬁeld environments, tested on a variety of plant

    species (row crops, specialty crops, and vineyards), and capable

    of measuring many traits related to morphology, structure,

    development, and physiology. Many technical challenges remain

    to be addressed regarding sensing, localization, path planning,

    Frontiers in Plant Science | www.frontiersin.org

    13

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    object detection, and obstacle avoidance. Intensive research is

    needed to overcome these limitations of phenotyping robots

    and improve their speed, accuracy, safety, and reliability.

    Collaborations among diﬀerent disciplines (such as plant science,

    agricultural engineering, mechanical and electrical engineering,

    and computer science) are imperative. With this transdisciplinary

    research, more eﬃcient and robust sensing and control systems

    will be developed for intelligent plant phenotyping robots.

    Sophisticated sensor modules can be developed using sensor-

    fusion techniques. Regarding the control systems, multiple

    intelligent algorithms (such as diﬀerent AI algorithms) can

    be combined to design more powerful controllers. These

    developments can potentially overcome the issues caused by

    changing environmental parameters, and complex structure of

    plants. Moreover, the suitable sensing and control systems yield

    better performance for accurate object detection (mainly for

    plants and crops, but also for humans, animals and other

    obstacles coexisting in the environments), path planning, and

    navigation. Suﬃcient funding from the public and private sources

    is the key to fuel the high-risk research in intelligent phenotyping

    robots in a sustainable way. We are optimistic that, in the

    next 10 years, we will see great leaps forward in autonomous

    and robotic technologies in plant phenotyping, enabled by the

    conﬂuence of the rapid advancements in sensing, controllers, and

    intelligent algorithms (AIs).

    AUTHOR CONTRIBUTIONS

    AA and YG provided the conceptualization of the manuscript.

    AA drafted the manuscript. YG, SP, and JS substantially edited

    the manuscript. All the authors contributed to the article and

    approved the submitted version.

    FUNDING

    The funding for this work was provided by USDA-NIFA under

    Grant No. 2017-67007-25941 and National Science Foundation

    under Grant No. OIA-1557417.

    REFERENCES

    Abel, J. (2018). In-Field Robotic Leaf Grasping and Automated Crop Spectroscopy.

    Pittsburgh, PA: Carnegie Mellon University.

    Adamides, G., Katsanos, C., Parmet, Y., Christou, G., Xenos, M., Hadzilacos, T.,

    et al. (2017). HRI usability evaluation of interaction modes for a teleoperated

    agricultural robotic sprayer. Appl. Ergon. 62, 237–246. doi: 10.1016/j.apergo.

    2017.03.008

    Afanasyev, I., Mazzara, M., Chakraborty, S., Zhuchkov, N., Maksatbek, A., Kassab,

    M., et al. (2019). Towards the internet of robotic things: analysis, architecture,

    components and challenges. arXiv [Preprint]. Avaliable online at: https://arxiv.

    org/abs/1907.03817 (accessed April 23, 2020).

    Afzal, A., Duiker, S. W., Watson, J. E., and Luthe, D. (2017). Leaf thickness
    and

    electrical capacitance as measures of plant water status. Trans. ASABE 60,

    1063–1074.

    Aguiar, A. S., Dos Santos, F. N., De Sousa, A. J. M., Oliveira, P. M., and Santos,
    L. C.

    (2020). Visual trunk detection using transfer learning and a deep learning-based

    coprocessor. IEEE Access 8, 77308–77320.

    Ahlin, K., Joﬀe, B., Hu, A.-P., McMurray, G., and Sadegh, N. (2016). Autonomous

    leaf picking using deep learning and visual-servoing. IFAC PapersOnLine 49,

    177–183. doi: 10.1016/j.ifacol.2016.10.033

    Alenya, G., Dellen, B., Foix, S., and Torras, C. (2013). Robotized plant probing:
    leaf

    segmentation utilizing time-of-ﬂight data. IEEE Robot. Autom. Mag. 20, 50–59.

    doi: 10.1109/MRA.2012.2230118

    Alenyà, G., Dellen, B., and Torras, C. (2011). “3D modelling of leaves from

    color and ToF data for robotized plant measuring,” in 2011 IEEE International

    Conference on Robotics and Automation, Shanghai, 3408–3414. doi: 10.1109/

    ICRA.2011.5980092

    Alenyà Ribas, G., Dellen, B., Foix Salmerón, S., and Torras, C. (2012). “Robotic

    leaf probing via segmentation of range data into surface patches,” in Proceedings

    of the 2012 IROS Workshop on Agricultural Robotics: Enabling Safe, Eﬃcient,

    Aﬀordable Robots for Food Production, Vilamoura, 1–6.

    Alenyá, G., Foix, S., and Torras, C. (2014). ToF cameras for active vision in
    robotics.

    Sensors Actuators A Phys. 218, 10–22. doi: 10.1016/j.sna.2014.07.014

    Andrade, F. H., Calvino, P., Cirilo, A., and Barbieri, P. (2002). Yield responses
    to

    narrow rows depend on increased radiation interception. Agron. J. 94, 975–980.

    Andrade-Sanchez, P., Gore, M. A., Heun, J. T., Thorp, K. R., Carmo-Silva, A. E.,

    French, A. N., et al. (2013). Development and evaluation of a ﬁeld-based high-

    throughput phenotyping platform. Funct. Plant Biol. 41, 68–79. doi: 10.1071/

    FP13126

    Andújar, D., Dorado, J., Bengochea-Guevara, J. M., Conesa-Muñoz, J., Fernández-

    Quintanilla, C., and Ribeiro, Á (2017). Inﬂuence of wind speed on

    RGB-D images in tree plantations. Sensors 17:914.

    doi: 10.3390/s1704

    0914

    Arad, B., Balendonck, J., Barth, R., Ben-Shahar, O., Edan, Y., Hellström, T.,
    et al.

    (2020). Development of a sweet pepper harvesting robot. J. F. Robot. 37,

    1027–1039. doi: 10.3390/s16081222

    Araus, J. L., and Cairns, J. E. (2014). Field high-throughput phenotyping: the
    new

    crop breeding frontier. Trends Plant Sci. 19, 52–61. doi: 10.1016/j.tplants.2013.

    09.008

    Arbanas, B., Ivanovic, A., Car, M., Orsag, M., Petrovic, T., and Bogdan, S. (2018).

    Decentralized planning and control for UAV–UGV cooperative teams. Auton.

    Robots 42, 1601–1618.

    Åstrand, B., and Baerveldt, A.-J. (2002). An agricultural mobile robot with vision-

    based perception for mechanical weed control. Auton. Robots 13, 21–35. doi:

    10.1023/A:1015674004201

    Åstrand, B., and Baerveldt, A.-J. (2005). A vision based row-following system

    for agricultural ﬁeld machinery. Mechatronics 15, 251–269. doi: 10.1016/j.

    mechatronics.2004.05.005

    Ateﬁ, A., Ge, Y., Pitla, S., and Schnable, J. (2019). In vivo human-like robotic

    phenotyping of leaf traits in maize and sorghum in greenhouse. Comput.

    Electron. Agric. 163:104854. doi: 10.1016/j.compag.2019.104854

    Ateﬁ, A., Ge, Y., Pitla, S., and Schnable, J. (2020). Robotic detection and grasp
    of

    maize and sorghum: stem measurement with contact. Robotics 9:58.

    Atkinson, J. A., Pound, M. P., Bennett, M. J., and Wells, D. M. (2019). Uncovering

    the hidden half of plants using new advances in root phenotyping. Curr. Opin.

    Biotechnol. 55, 1–8. doi: 10.1016/j.copbio.2018.06.002

    Bai, G., Ge, Y., Hussain, W., Baenziger, P. S., and Graef, G. (2016). A multi-

    sensor system for high throughput ﬁeld phenotyping in soybean and wheat

    breeding. Comput. Electron. Agric. 128, 181–192. doi: 10.1016/j.compag.2016.

    08.021

    Bao, Y., Tang, L., Breitzman, M. W., Salas Fernandez, M. G., and Schnable, P.
    S.

    (2019a). Field-based robotic phenotyping of sorghum plant architecture using

    stereo vision. J. F. Robot. 36, 397–415.

    Bao, Y., Tang, L., Srinivasan, S., and Schnable, P. S. (2019b). Field-based

    architectural traits characterisation of maize plant using time-of-ﬂight 3D

    imaging. Biosyst. Eng. 178, 86–101. doi: 10.1016/j.biosystemseng.2018.11.005

    Bao, Y., Zarecor, S., Shah, D., Tuel, T., Campbell, D. A., Chapman, A. V. E.,

    et al. (2019c). Assessing plant performance in the Enviratron. Plant Methods

    15, 1–14. doi: 10.1186/s13007-019-0504-y

    Bao, Y., Tang, L., and Shah, D. (2017). “Robotic 3D plant perception and

    leaf probing with collision-free motion planning for automated indoor plant

    phenotyping,” in 2017 ASABE Annual International Meeting, Spokane, WA, 1.

    doi: 10.13031/aim.201700369

    Frontiers in Plant Science | www.frontiersin.org

    14

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Barbieri, P. A., Rozas, H. n. R. S., Andrade, F. H., and Echeverria, H. n. E.
    (2000).

    Row spacing eﬀects at diﬀerent levels of nitrogen availability in maize. Agron.
    J.

    92, 283–288.

    Barth, R., Baur, J., Buschmann, T., Edan, Y., Hellström, T., Nguyen, T.,

    et al. (2014). “Using ROS for agricultural robotics-design considerations and

    experiences,” in Proceedings of the Second International Conference on Robotics

    and Associated High-Technologies and Equipment for Agriculture and Forestry,

    509–518.

    Batth, R. S., Nayyar, A., and Nagpal, A. (2018). “Internet of robotic things:

    driving intelligent robotics of future-concept, architecture, applications and

    technologies,” in 2018 4th International Conference on Computing Sciences

    (ICCS), Jalandhar: IEEE, 151–160.

    Batti, H., Ben Jabeur, C., and Seddik, H. (2020). Autonomous smart robot for path

    predicting and ﬁnding in maze based on fuzzy and neuro-Fuzzy approaches.

    Asian J. Control 23:2345.

    Baweja, H. S., Parhar, T., Mirbod, O., and Nuske, S. (2018). in StalkNet: A Deep

    Learning Pipeline for High-Throughput Measurement of Plant Stalk Count and

    Stalk Width BT - Field and Service Robotics, eds M. Hutter and R. Siegwart

    (Cham: Springer International Publishing), 271–284.

    Bayındır, L. (2016). A review of swarm robotics tasks. Neurocomputing 172,

    292–321. doi: 10.1016/j.neucom.2015.05.116

    Bechar, A., and Vigneault, C. (2016). Agricultural robots for ﬁeld operations:

    concepts and components. Biosyst.

    Eng. 149, 94–111. doi: 10.1016/j.

    biosystemseng.2016.06.014

    Behmann, J., Mahlein, A.-K., Paulus, S., Dupuis, J., Kuhlmann, H., Oerke, E.-C.,

    et al. (2016). Generation and application of hyperspectral 3D plant models:

    methods and challenges. Mach. Vis. Appl. 27, 611–624.

    Berardo, N., Brenna, O. V., Amato, A., Valoti, P., Pisacane, V., and Motto, M.

    (2004). Carotenoids concentration among maize genotypes measured by near

    infrared reﬂectance spectroscopy (NIRS). Innov. food Sci. Emerg. Technol. 5,

    393–398.

    Biber, P., Weiss, U., Dorna, M., and Albert, A. (2012). “Navigation system of

    the autonomous agricultural robot Bonirob,” in in Workshop on Agricultural

    Robotics: Enabling Safe, Eﬃcient, and Aﬀordable Robots for Food Production

    (Collocated with IROS 2012), Vilamoura.

    Biskup, B., Scharr, H., Fischbach, A., Wiese-Klinkenberg, A., Schurr, U., and

    Walter, A. (2009). diel growth cycle of isolated leaf discs analyzed with a

    novel, high-throughput three-dimensional imaging method is identical to that

    of intact leaves. Plant Physiol. 149, 1452–1461. doi: 10.1104/pp.108.134486

    Blasco, J., Aleixos, N., Roger, J. M., Rabatel, G., and Moltó, E. (2002). AE—

    Automation and emerging technologies: robotic weed control using machine

    vision. Biosyst. Eng. 83, 149–157. doi: 10.1006/bioe.2002.0109

    Blender, T., Buchner, T., Fernandez, B., Pichlmaier, B., and Schlegel, C. (2016).

    “Managing a Mobile Agricultural Robot Swarm for a seeding task,” in IECON

    2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society,

    Florence, 6879–6886. doi: 10.1109/IECON.2016.7793638

    Brambilla, M., Ferrante, E., Birattari, M., and Dorigo, M. (2013). Swarm robotics:
    a

    review from the swarm engineering perspective. Swarm Intell. 7, 1–41.

    Breitzman, M. W., Bao, Y., Tang, L., Schnable, P. S., and Salas-Fernandez, M.
    G.

    (2019). Linkage disequilibrium mapping of high-throughput image-derived

    descriptors of plant architecture traits under ﬁeld conditions. F. Crop. Res.

    244:107619. doi: 10.1016/j.fcr.2019.107619

    Bruzzone, L., and Quaglia, G. (2012). Locomotion systems for ground mobile

    robots in unstructured environments. Mech. Sci. 3, 49–62.

    Burud, I., Lange, G., Lillemo, M., Bleken, E., Grimstad, L., and Johan From, P.

    (2017). Exploring robots and UAVs as phenotyping tools in plant breeding.

    IFAC-PapersOnLine 50, 11479–11484. doi: 10.1016/j.ifacol.2017.08.1591

    Busemeyer, L., Mentrup, D., Möller, K., Wunder, E., Alheit, K., Hahn, V., et al.

    (2013). BreedVision — A multi-sensor platform for non-destructive ﬁeld-

    based phenotyping in plant breeding. Sensors 13, 2830–2847. doi: 10.3390/

    s130302830

    Castrillo, M., Fernandez, D., Calcagno, A. M., Trujillo, I., and Guenni, L.

    (2001). Responses of ribulose-1, 5-bisphosphate carboxylase, protein content,

    and stomatal conductance to water deﬁcit in maize, tomato, and bean.

    Photosynthetica 39, 221–226.

    Chamanbaz, M., Mateo, D., Zoss, B. M., Toki´c, G., Wilhelm, E., Bouﬀanais, R.,

    et al. (2017). Swarm-enabling technology for multi-robot systems. Front. Robot.

    AI 4:12. doi: 10.3389/frobt.2017.00012

    Chaudhury, A., Ward, C., Talasaz, A., Ivanov, A. G., Brophy, M., Grodzinski, B.,

    et al. (2017). Machine vision system for 3D plant phenotyping. arXiv [Preprint].

    arXiv1705.00540.

    Chawade, A., van Ham, J., Blomquist, H., Bagge, O., Alexandersson, E., and Ortiz,

    R. (2019). High-throughput ﬁeld-phenotyping tools for plant breeding and

    precision agriculture. Agronomy 9:258.

    Chen, J., Zhang, X., Xin, B., and Fang, H. (2016). Coordination between unmanned

    aerial and ground vehicles: a taxonomy and optimization perspective. IEEE

    Trans. Cybern. 46, 959–972. doi: 10.1109/TCYB.2015.2418337

    Chen, Z., Wang, J., Wang, T., Song, Z., Li, Y., Huang, Y., et al. (2021). Automated

    in-ﬁeld leaf-level hyperspectral imaging of corn plants using a Cartesian robotic

    platform. Comput. Electron. Agric. 183:105996.

    Chlingaryan, A., Sukkarieh, S., and Whelan, B. (2018). Machine learning

    approaches for crop yield prediction and nitrogen status estimation in precision

    agriculture: a review. Comput. Electron. Agric. 151, 61–69. doi: 10.1016/j.

    compag.2018.05.012

    Chonnaparamutt, W., Kawasaki, H., Ueki, S., Murakami, S., and Koganemaru, K.

    (2009). “Development of a timberjack-like pruning robot: climbing experiment

    and fuzzy velocity control,” in 2009 ICCAS-SICE, Fukuoka, 1195–1199.

    Choudhuri, A., and Chowdhary, G. (2018). “Crop stem width estimation in highly

    cluttered ﬁeld environment,” in Proc. Comput. Vis. Probl. Plant Phenotyping

    (CVPPP 2018), Newcastle, 6–13.

    Cohen, Y., Huck, M. G., Hesketh, J. D., and Frederick, J. R. (1990). Sap ﬂow in
    the

    stem of water stressed soybean and maize plants. Irrig. Sci. 11, 45–50.

    Costa, C., Schurr, U., Loreto, F., Menesatti, P., and Carpentier, S. (2019). Plant

    phenotyping research trends, a science mapping approach. Front. Plant Sci.

    9:1933.

    Crowell, S., Falcão, A. X., Shah, A., Wilson, Z., Greenberg, A. J., and McCouch,

    S. R. (2014). High-resolution inﬂorescence phenotyping using a novel image-

    analysis pipeline, PANorama. Plant Physiol. 165, 479–495. doi: 10.1104/pp.114.

    238626

    Das Choudhury, S., Samal, A., and Awada, T. (2019). Leveraging image analysis
    for

    high-throughput plant phenotyping. Front. Plant Sci. 10:508. doi: 10.3389/fpls.

    2019.00508

    De Baerdemaeker, J., Munack, A., Ramon, H., and Speckmann, H. (2001).

    Mechatronic systems, communication, and control in precision agriculture.

    IEEE Control Syst. Mag. 21, 48–70. doi: 10.1016/j.aca.2020.11.008

    Dhondt, S., Wuyts, N., and Inzé, D. (2013). Cell to whole-plant phenotyping: the

    best is yet to come. Trends Plant Sci. 18, 428–439. doi: 10.1016/j.tplants.2013.

    04.008

    Duckett, T., Pearson, S., Blackmore, S., Grieve, B., Chen, W. H., Cielniak, G.,
    et al.

    (2018). Agricultural robotics: The future of robotic agriculture. arXiv [Preprint].

    arXiv1806.06762.

    Duguleana, M., and Mogan, G. (2016). Neural networks based reinforcement

    learning for mobile robots obstacle avoidance. Exp. Syst. Appl. 62, 104–115.

    doi: 10.1016/j.eswa.2016.06.021

    Dykes, L., Hoﬀmann, L. Jr., Portillo-Rodriguez, O., Rooney, W. L., and

    Rooney, L. W. (2014). Prediction of total phenols, condensed tannins,

    and 3-deoxyanthocyanidins in sorghum grain using near-infrared (NIR)

    spectroscopy. J. Cereal Sci. 60, 138–142.

    Ecarnot, M., Ba¸czyk, P., Tessarotto, L., and Chervin, C. (2013). Rapid phenotyping

    of the tomato fruit model, Micro-Tom, with a portable VIS–NIR spectrometer.

    Plant Physiol. Biochem. 70, 159–163. doi: 10.1016/j.plaphy.2013.05.019

    ElMasry, G., Wang, N., ElSayed, A., and Ngadi, M. (2007). Hyperspectral imaging

    for nondestructive determination of some quality attributes for strawberry.

    J. Food Eng. 81, 98–107.

    Fahlgren, N., Feldman, M., Gehan, M. A., Wilson, M. S., Shyu, C., Bryant, D. W.,

    et al. (2015). A versatile phenotyping system and analytics platform reveals

    diverse temporal responses to water availability in setaria. Mol. Plant 8, 1520–

    1535. doi: 10.1016/j.molp.2015.06.005

    Faroq, A.-T., Adam, H., Dos Anjos, A., Lorieux, M., Larmande, P., Ghesquière,
    A.,

    et al. (2013). P-TRAP: a panicle trait phenotyping tool. BMC Plant Biol. 13:122.

    doi: 10.1186/1471-2229-13-122

    Fernandez, M. G. S., Bao, Y., Tang, L., and Schnable, P. S. (2017). A high-

    throughput, ﬁeld-based phenotyping technology for tall biomass crops. Plant

    Physiol. 174, 2008–2022. doi: 10.1104/pp.17.00707

    Fiorani, F., and Schurr, U. (2013). Future scenarios for plant phenotyping. Annu.

    Rev. Plant Biol. 64, 267–291. doi: 10.1146/annurev-arplant-050312-120137

    Frontiers in Plant Science | www.frontiersin.org

    15

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Fischer, G. (2009). “World food and agriculture to 2030/50,” in Technical Paper

    From the Expert Meeting on How to Feed the World in, Rome, 24–26.

    Foix, S., Alenyà, G., and Torras, C. (2015). “3D Sensor planning framework for
    leaf

    probing,” in 2015 IEEE/RSJ International Conference on Intelligent Robots and

    Systems (IROS), Hamburg, 6501–6506. doi: 10.1109/IROS.2015.7354306

    Foix, S., Alenyà, G., and Torras, C. (2018). Task-driven active sensing framework

    applied to leaf probing. Comput. Electron. Agric. 147, 166–175. doi: 10.1016/j.

    compag.2018.01.020

    Franceschetti, A., Tosello, E., Castaman, N., and Ghidoni, S. (2018). Robotic
    Arm

    Control and Task Training through Deep Reinforcement Learning.

    Furbank, R. T., and Tester, M. (2011). Phenomics – technologies to relieve the

    phenotyping bottleneck. Trends Plant Sci. 16, 635–644. doi: 10.1016/j.tplants.

    2011.09.005

    Gage, J. L., Miller, N. D., Spalding, E. P., Kaeppler, S. M., and de Leon, N.
    (2017).

    TIPS: a system for automated image-based phenotyping of maize tassels. Plant

    Methods 13:21. doi: 10.1186/s13007-017-0172-8

    Gage, J. L., Richards, E., Lepak, N., Kaczmar, N., Soman, C., Chowdhary, G., et
    al.

    (2019). In-ﬁeld whole plant maize architecture characterized by latent space

    phenotyping. bioRxiv [Preprint]. doi: 10.1101/763342

    Gao, T., Emadi, H., Saha, H., Zhang, J., Lofquist, A., Singh, A., et al. (2018).
    A

    novel multirobot system for plant phenotyping. Robotics 7:61. doi: 10.3390/

    robotics7040061

    Ge, Y., Ateﬁ, A., Zhang, H., Miao, C., Ramamurthy, R. K., Sigmon, B., et al.

    (2019). High-throughput analysis of leaf physiological and chemical traits with

    VIS–NIR–SWIR spectroscopy: a case study with a maize diversity panel. Plant

    Methods 15:66. doi: 10.1186/s13007-019-0450-8

    Ge, Y., Bai, G., Stoerger, V., and Schnable, J. C. (2016). Temporal dynamics of

    maize plant growth, water use, and leaf water content using automated high

    throughput RGB and hyperspectral imaging. Comput. Electron. Agric. 127,

    625–632. doi: 10.1016/j.compag.2016.07.028

    Gegas, V. C., Nazari, A., Griﬃths, S., Simmonds, J., Fish, L., Orford, S., et
    al. (2010).

    A genetic framework for grain size and shape variation in wheat. Plant Cell 22,

    1046–1056. doi: 10.1105/tpc.110.074153

    Gonzalez-de-Soto, M., Emmi, L., Perez-Ruiz, M., Aguera, J., and Gonzalez-de-

    Santos, P. (2016). Autonomous systems for precise spraying – Evaluation

    of a robotised patch sprayer. Biosyst. Eng. 146, 165–182. doi: 10.1016/j.

    biosystemseng.2015.12.018

    Granier, C., and Tardieu, F. (2009). Multi-scale phenotyping of leaf expansion
    in

    response to environmental changes: the whole is more than the sum of parts.

    Plant. Cell Environ. 32, 1175–1184. doi: 10.1111/j.1365-3040.2009.01955.x

    Grieco, L. A., Rizzo, A., Colucci, S., Sicari, S., Piro, G., Di Paola, D., et
    al. (2014).

    IoT-aided robotics applications: technological implications, target domains and

    open issues. Comput. Commun. 54, 32–47.

    Grift, T., Zhang, Q., Kondo, N., and Ting, K. C. (2008). A review of automation

    and robotics for the bio-industry. J. Biomechatronics Eng. 1, 37–54.

    Grimstad, L., and From, P. J. (2017). Thorvald II - a modular and re-conﬁgurable

    agricultural robot. IFAC PapersOnLine 50, 4588–4593. doi: 10.1016/j.ifacol.

    2017.08.1005

    Guo, Z., Huang, W., Chen, L., Wang, X., and Peng, Y. (2013). “Nondestructive

    evaluation of soluble solid content in strawberry by near infrared spectroscopy,”

    in Piageng 2013: Image Processing and Photonics for Agricultural Engineering,

    (Bellingham: International Society for Optics and Photonics), 87610O.

    Han, L., Yang, G., Yang, H., Xu, B., Li, Z., and Yang, X. (2018). Clustering ﬁeld-

    based maize phenotyping of plant-height growth and canopy spectral dynamics

    using a UAV remote-sensing approach. Front. Plant Sci. 9:1638. doi: 10.3389/

    fpls.2018.01638

    Happ, M. M., Wang, H., Graef, G. L., and Hyten, D. L. (2019). Generating high

    density, low cost genotype data in soybean [Glycine max (L.) Merr.]. G3 Genes

    Genomes Genet. 9, 2153–2160. doi: 10.1534/g3.119.400093

    Hassan, M. A., Yang, M., Rasheed, A., Yang, G., Reynolds, M., Xia, X., et al.

    (2019). A rapid monitoring of NDVI across the wheat growth cycle for grain

    yield prediction using a multi-spectral UAV platform. Plant Sci. 282, 95–103.

    doi: 10.1016/j.plantsci.2018.10.022

    Hassanijalilian, O., Igathinathane, C., Bajwa, S., and Nowatzki, J. (2020a). Rating

    iron deﬁciency in soybean using image processing and decision-tree based

    models. Remote Sens. 12:4143.

    Hassanijalilian, O., Igathinathane, C., Doetkott, C., Bajwa, S., Nowatzki, J.,
    and Haji

    Esmaeili, S. A. (2020b). Chlorophyll estimation in soybean leaves inﬁeld with

    smartphone digital imaging and machine learning. Comput. Electron. Agric.

    174:105433. doi: 10.1016/j.compag.2020.10543

    Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J.,

    et al. (2010). Evaluation of a strawberry-harvesting robot in a ﬁeld test. Biosyst.

    Eng. 105, 160–171. doi: 10.1016/j.biosystemseng.2009.09.011

    Hejazipoor, H., Massah, J., Soryani, M., Vakilian, K. A., and Chegini, G. (2021).
    An

    intelligent spraying robot based on plant bulk volume. Comput. Electron. Agric.

    180:105859.

    Hemming, J., Bac, C. W., van Tuijl, B. A. J., Barth, R., Bontsema, J., Pekkeriet,
    E. J.,

    et al. (2014). “A robot for harvesting sweet-pepper in greenhouses,” in Paper

    Presented at AgEng 2014, Zurich.

    Hughes, J., Culha, U., Giardina, F., Guenther, F., Rosendo, A., and Iida, F. (2016).

    Soft manipulators and grippers: a review. Front. Robot. AI 3:69.

    Iqbal, J., Xu, R., Halloran, H., and Li, C. (2020). Development of a Multi-Purpose

    Autonomous Diﬀerential Drive Mobile Robot for Plant Phenotyping and Soil

    Sensing. Electronics 9:1550.

    Irshat, K., Petr, R., and Irina, R. (2018). “The selecting of artiﬁcial intelligence

    technology for control of mobile robots,” in 2018 International Multi-

    Conference on Industrial Engineering and Modern Technologies (FarEastCon),

    Vladivostok: IEEE, 1–4.

    Ishigure, Y., Hirai, K., and Kawasaki, H. (2013). “A pruning robot with a power-

    saving chainsaw drive,” in 2013 IEEE International Conference on Mechatronics

    and Automation, Takamatsu, 1223–1228. doi: 10.1109/ICMA.2013.6618088

    Jannink, J.-L., Lorenz, A. J., and Iwata, H. (2010). Genomic selection in plant

    breeding: from theory to practice. Brief. Funct. Genomics 9, 166–177. doi: 10.

    1093/bfgp/elq001

    Jay, S., Rabatel, G., Hadoux, X., Moura, D., and Gorretta, N. (2015). In-ﬁeld

    crop row phenotyping from 3D modeling performed using structure from

    Motion. Comput. Electron. Agric. 110, 70–77. doi: 10.1016/j.compag.2014.

    09.021

    Jenkins, M., and Kantor, G. (2017). “Online detection of occluded plant stalks
    for

    manipulation,” in Proceedings of the 2017 IEEE/RSJ International Conference

    on Intelligent Robots and Systems (IROS), 5162–5167. doi: 10.1109/IROS.2017.

    8206404

    Jin, X., Zarco-Tejada, P., Schmidhalter, U., Reynolds, M. P., Hawkesford, M. J.,

    Varshney, R. K., et al. (2020). High-throughput estimation of crop traits: a

    review of ground and aerial phenotyping platforms. IEEE Geosci. Remote Sens.

    Mag. 2, 1–33.

    Kicherer, A., Herzog, K., Pﬂanz, M., Wieland, M., Rüger, P., Kecke, S., et al.
    (2015).

    An automated ﬁeld phenotyping pipeline for application in grapevine research.

    Sensors 15, 4823–4836. doi: 10.3390/s150304823

    Kim, G.-H., Kim, S.-C., Hong, Y.-K., Han, K.-S., and Lee, S.-G. (2012). “A robot

    platform for unmanned weeding in a paddy ﬁeld using sensor fusion,” in

    Proceedings of the 2012 IEEE International Conference on Automation Science

    and Engineering (CASE), (Piscataway, NJ: IEEE), 904–907.

    Kong, E., Liu, D., Guo, X., Yang, W., Sun, J., Li, X., et al. (2013). Anatomical
    and

    chemical characteristics associated with lodging resistance in wheat. Crop J.
    1,

    43–49. doi: 10.1016/j.cj.2013.07.012

    Lee, U., Chang, S., Putra, G. A., Kim, H., and Kim, D. H. (2018). An automated,

    high-throughput plant phenotyping system using machine learning-based plant

    segmentation and image analysis. PLoS One 13:e0196615. doi: 10.1371/journal.

    pone.0196615

    Leilah, A. A., and Al-Khateeb, S. A. (2005). Statistical analysis of wheat yield
    under

    drought conditions. J. Arid Environ. 61, 483–496. doi: 10.1016/j.jaridenv.2004.

    10.011

    Li, L., Zhang, Q., and Huang, D. (2014). A review of imaging techniques for plant

    phenotyping. Sensors 14, 20078–20111. doi: 10.3390/s141120078

    Li, M., Imou, K., Wakabayashi, K., and Yokoyama, S. (2009). Review of research
    on

    agricultural vehicle autonomous guidance. Int. J. Agric. Biol. Eng. 2, 1–16.

    Li, Z., Guo, R., Li, M., Chen, Y., and Li, G. (2020a). A review of computer vision

    technologies for plant phenotyping. Comput. Electron. Agric. 176:105672.

    Li, Z., Zhang, Q., Li, J., Yang, X., Wu, Y., Zhang, Z., et al. (2020b). Solar-induced

    chlorophyll ﬂuorescence and its link to canopy photosynthesis in maize from

    continuous ground measurements. Remote Sens. Environ. 236:111420. doi: 10.

    1016/j.rse.2019.111420

    Lili, W., Bo, Z., Jinwei, F., Xiaoan, H., Shu, W., Yashuo, L., et al. (2017).

    Development of a tomato harvesting robot used in greenhouse. Int. J. Agric.

    Biol. Eng. 10, 140–149.

    Frontiers in Plant Science | www.frontiersin.org

    16

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Lopes, C. M., Graça, J., Sastre, J., Reyes, M., Guzmán, R., Braga, R., et al.
    (2016).

    “Vineyard yeld estimation by VINBOT robot-preliminary results with the white

    variety Viosinho,” in Proceedings 11th Int. Terroir Congress, eds G. Jones and
    N.

    Doran (Ashland, USA: Southern Oregon University), 458–463.

    Lu, H., Tang, L., Whitham, A. S., and Mei, Y. (2017). A robotic platform for corn

    seedling morphological traits characterization. Sensors 17:2082. doi: 10.3390/

    s17092082

    Madden, S. (2012). From databases to big data. IEEE Internet Comput. 16, 4–6.

    Mancini, M., Mazzoni, L., Gagliardi, F., Balducci, F., Duca, D., Toscano, G.,
    et al.

    (2020). Application of the non-destructive NIR technique for the evaluation of

    strawberry fruits quality parameters. Foods 9:441. doi: 10.3390/foods9040441

    McMullen, M. D., Kresovich, S., Villeda, H. S., Bradbury, P., Li, H., Sun, Q.,
    et al.

    (2009). Genetic properties of the maize nested association mapping population.

    Science 325, 737–740. doi: 10.1126/science.1174320

    Mishra, K. B., Mishra, A., Klem, K., and Govindjee. (2016). Plant phenotyping:

    a perspective. Indian J. Plant Physiol. 21, 514–527. doi: 10.1007/s40502-016-

    0271-y

    Mishra, P., Asaari, M. S. M., Herrero-Langreo, A., Lohumi, S., Diezma, B., and

    Scheunders, P. (2017). Close range hyperspectral imaging of plants: a review.

    Biosyst. Eng. 164, 49–67.

    Moeinizade, S., Hu, G., Wang, L., and Schnable, P. S. (2019). Optimizing selection

    and mating in genomic selection with a look-ahead approach: an operations

    research framework. G3 Genes, Genomes, Genet. 9, 2123–2133.

    Montes, J. M., Melchinger, A. E., and Reif, J. C. (2007). Novel throughput

    phenotyping platforms in plant genetic studies. Trends Plant Sci. 12, 433–436.

    doi: 10.1016/j.tplants.2007.08.006

    Mueller-Sim, T., Jenkins, M., Abel, J., and Kantor, G. (2017). “The robotanist:

    a ground-based agricultural robot for high-throughput crop phenotyping,”

    in 2017 IEEE International Conference on Robotics and Automation (ICRA),

    Singapore, 3634–3639. doi: 10.1109/ICRA.2017.7989418

    Murman, J. N. (2019). Flex-Ro: A Robotic High Throughput Field Phenotyping

    System.

    Mutka, A. M., and Bart, R. S. (2015). Image-based phenotyping of plant disease

    symptoms. Front. Plant Sci. 5:734. doi: 10.3389/fpls.2014.00734

    Narvaez, F. Y., Reina, G., Torres-Torriti, M., Kantor, G., and Cheein, F. A.

    (2017). A survey of ranging and imaging techniques for precision agriculture

    phenotyping. IEEE ASME Trans. Mechatronics 22, 2428–2439.

    Navarro, I., and Matía, F. (2012). An introduction to swarm robotics. Isrn Robot.

    2013:608164.

    Nishizawa, T., Mori, Y., Fukushima, S., Natsuga, M., and Maruyama, Y. (2009).

    Non-destructive analysis of soluble sugar components in strawberry fruits using

    near-infrared spectroscopy. Nippon Shokuhin Kagaku Kogaku KaishiJ. Japanese

    Soc. Food Sci. Technol. 56, 229–235.

    Nof, S. Y. (2009). Springer Handbook of Automation. Berlin: Springer Science &

    Business Media.

    Oberti, R., Marchi, M., Tirelli, P., Calcante, A., Iriti, M., Tona, E., et al.
    (2016).

    Selective spraying of grapevines for disease control using a modular agricultural

    robot.

    Biosyst.

    Eng.

    146,

    203–215.

    doi:

    10.1016/j.biosystemseng.2015.

    12.004

    Ohashi, Y., Nakayama, N., Saneoka, H., and Fujita, K. (2006). Eﬀects of drought

    stress on photosynthetic gas exchange, chlorophyll ﬂuorescence and stem

    diameter of soybean plants. Biol. Plant. 50, 138–141.

    Palazzari, V., Mezzanotte, P., Alimenti, F., Fratini, F., Orecchini, G., and Roselli,
    L.

    (2017). Leaf compatible “eco-friendly” temperature sensor clip for high density

    monitoring wireless networks. Wirel. Power Transf. 4, 55–60.

    Pandey, P., Dakshinamurthy, H. N., and Young, S. N. (2021). Autonomy in

    detection, actuation, and planning for robotic weeding systems. Trans. ASABE.

    Panozzo, J. F., Eagles, H. A., Cawood, R. J., and Wootton, M. (1999). Wheat spike

    temperatures in relation to varying environmental conditions. Aust. J. Agric.

    Res. 50, 997–1006. doi: 10.1371/journal.pone.0189594

    Park, J., Delgado, R., and Choi, B. W. (2020). Real-time characteristics of ROS
    2.0 in

    multiagent robot systems: an empirical study. IEEE Access 8, 154637–154651.

    Parhar, T., Baweja, H., Jenkins, M., and Kantor, G. (2018). “A deep learning-based

    stalk grasping pipeline,” in 2018 IEEE International Conference on Robotics and

    Automation (ICRA), Brisbane, QLD: IEEE, 6161–6167.

    Penchaiya, P., Bobelyn, E., Verlinden, B. E., Nicolaï, B. M., and Saeys, W. (2009).

    Non-destructive measurement of ﬁrmness and soluble solids content in bell

    pepper using NIR spectroscopy. J. Food Eng. 94, 267–273.

    Pieruschka, R., and Schurr, U. (2019). Plant phenotyping: past, present, and future.

    Plant Phenomics 2019, 1–6.

    Phillips, R. L. (2010). Mobilizing science to break yield barriers. Crop Sci.
    50, S–99.

    Pound, M. P., Atkinson, J. A., Townsend, A. J., Wilson, M. H., Griﬃths, M.,

    Jackson, A. S., et al. (2017). Deep machine learning provides state-of-the-art

    performance in image-based plant phenotyping. Gigascience 6:gix083.

    Qiu, Q., Sun, N., Bai, H., Wang, N., Fan, Z., Wang, Y., et al. (2019). Field-based

    high-throughput phenotyping for maize plant using 3D LiDAR point cloud

    generated with a “Phenomobile.”. Front. Plant Sci. 10:554. doi: 10.3389/fpls.

    2019.00554

    Qiu, R., Wei, S., Zhang, M., Li, H., Sun, H., Liu, G., et al. (2018). Sensors
    for

    measuring plant phenotyping: a review. Int. J. Agric. Biol. Eng. 11, 1–17.

    Quaglia, G., Visconte, C., Scimmi, L. S., Melchiorre, M., Cavallone, P., and

    Pastorelli, S. (2020). Design of a UGV powered by solar energy for precision

    agriculture. Robotics 9:13.

    Raja, R., Nguyen, T. T., Slaughter, D. C., and Fennimore, S. A. (2020). Real-time

    robotic weed knife control system for tomato and lettuce based on geometric

    appearance of plant labels. Biosyst. Eng. 194, 152–164.

    Rahaman, M. M., Chen, D., Gillani, Z., Klukas, C., and Chen, M. (2015). Advanced

    phenotyping and phenotype data analysis for the study of plant growth and

    development. Front. Plant Sci. 6:619. doi: 10.3389/fpls.2015.00619

    Ray, P. P. (2016). Internet of robotic things: concept, technologies, and challenges.

    IEEE Access 4, 9489–9500.

    Richards, R. A., Rebetzke, G. J., Watt, M., Condon, A. G., Spielmeyer, W., and

    Dolferus, R. (2010). Breeding for improved water productivity in temperate

    cereals: phenotyping, quantitative trait loci, markers and the selection

    environment. Funct. Plant Biol. 37, 85–97. doi: 10.1071/FP09219 (Tony),

    Roldán, J. J., Garcia-Aunon, P., Garzón, M., De León, J., Del Cerro, J.,

    and Barrientos, A. (2016). Heterogeneous multi-robot system for mapping

    environmental variables of greenhouses. Sensors 16:1018.

    doi: 10.3390/

    s16071018

    Ruckelshausen, A., Biber, P., Dorna, M., Gremmes, H., Klose, R., Linz, A., et
    al.

    (2009). BoniRob–an autonomous ﬁeld robot platform for individual plant

    phenotyping. Precis. Agric. 9, 841–847. doi: 10.3390/s17010214

    Rudolph, R., Herzog, K., Töpfer, R., and Steinhage, V. (2019). Eﬃcient

    identiﬁcation, localization and quantiﬁcation of grapevine inﬂorescences and

    ﬂowers in unprepared ﬁeld images using Fully Convolutional Networks. Vitis

    58, 95–104.

    Sandhu, J., Zhu, F., Paul, P., Gao, T., Dhatt, B. K., Ge, Y., et al. (2019). PI-Plat:

    a high-resolution image-based 3D reconstruction method to estimate growth

    dynamics of rice inﬂorescence traits. Plant Methods 15:162.

    doi: 10.1186/

    s13007-019-0545-2

    Saravanan, M., Perepu, S. K., and Sharma, A. (2018). “Exploring collective behavior

    of internet of robotic things for indoor plant health monitoring,” in 2018 IEEE

    International Conference on Internet of Things and Intelligence System (IOTAIS),

    Bali: IEEE, 148–154.

    Schulz, H., and Baranska, M. (2007). Identiﬁcation and quantiﬁcation of valuable

    plant substances by IR and Raman spectroscopy. Vib. Spectrosc. 43, 13–25.

    doi: 10.1016/j.vibspec.2006.06.001

    Shaﬁekhani, A., Kadam, S., Fritschi, B. F., and DeSouza, N. G. (2017). Vinobot

    and vinoculer: two robotic platforms for high-throughput ﬁeld phenotyping.

    Sensors 17:214. doi: 10.3390/s17010214

    Shah, D., Tang, L., Gai, J., and Putta-Venkata, R. (2016). Development of a mobile

    robotic phenotyping system for growth chamber-based studies of genotype

    x environment interactions. IFAC PapersOnLine 49, 248–253. doi: 10.1016/j.

    ifacol.2016.10.046

    Shahrimie, M. A. M., Mishra, P., Mertens, S., Dhondt, S., Wuyts, N., and

    Scheunders, P. (2016). “Modeling eﬀects of illumination and plant geometry

    on leaf reﬂectance spectra in close-range hyperspectral imaging,” in 2016 8th

    Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote

    Sensing (WHISPERS), Los Angeles, CA: IEEE, 1–4.

    Shalal, N., Low, T., McCarthy, C., and Hancock, N. (2013). A Review of

    Autonomous Navigation Systems in Agricultural Environments.

    Shamshiri, R. R., Weltzien, C., Hameed, I. A., Yule, I. J., Grift, T. E., Balasundram,

    S. K., et al. (2018). Research and development in agricultural robotics: a

    perspective of digital farming. Int. J. Agric. Biol. Eng. 11, 1–14.

    Shao, Y., and He, Y. (2008). Nondestructive measurement of acidity of strawberry

    using Vis/NIR spectroscopy. Int. J. Food Prop. 11, 102–111.

    Frontiers in Plant Science | www.frontiersin.org

    17

    June 2021 | Volume 12 | Article 611940

    Ateﬁ et al.

    Robotic Technologies for Plant Phenotyping

    Silwal, A., Davidson, J. R., Karkee, M., Mo, C., Zhang, Q., and Lewis, K. (2017).

    Design, integration, and ﬁeld evaluation of a robotic apple harvester. J. F. Robot.

    34, 1140–1159. doi: 10.1002/rob.21715

    Singh, A., Ganapathysubramanian, B., Singh, A. K., and Sarkar, S. (2016). Machine

    learning for high-throughput stress phenotyping in plants. Trends Plant Sci. 21,

    110–124.

    Smitt, C., Halstead, M., Zaenker, T., Bennewitz, M., and McCool, C. (2020).

    PATHoBot: A Robot for Glasshouse Crop Phenotyping and Intervention. arXiv

    [Preprint]. arXiv2010.16272.

    Taghavifar, H., Xu, B., Taghavifar, L., and Qin, Y. (2019). Optimal path-planning

    of nonholonomic terrain robots for dynamic obstacle avoidance using single-

    time velocity estimator and reinforcement learning approach. IEEE Access 7,

    159347–159356.

    Tan, Y., and Zheng, Z. (2013). Research advance in swarm robotics. Def. Technol.

    9, 18–39. doi: 10.1016/j.dt.2013.03.001

    Tripodi, P., Massa, D., Venezia, A., and Cardi, T. (2018). Sensing technologies
    for

    precision phenotyping in vegetable crops: current status and future challenges.

    Agronomy 8:57.

    Tsubo, M., and Walker, S. (2002). A model of radiation interception and use by
    a

    maize–bean intercrop canopy. Agric. For. Meteorol. 110, 203–215.

    Ubbens, J., Cieslak, M., Prusinkiewicz, P., Parkin, I., Ebersbach, J., and Stavness,

    I. (2020). Latent space phenotyping: automatic image-based phenotyping for

    treatment studies. Plant Phenomics 2020:5801869. doi: 10.34133/2020/5801869

    Underwood, J., Wendel, A., Schoﬁeld, B., McMurray, L., and Kimber, R. (2017).

    Eﬃcient in-ﬁeld plant phenomics for row-crops with an autonomous ground

    vehicle. J. F. Robot. 34, 1061–1083.

    van Henten, E. J., Hemming, J., van Tuijl, B. A. J., Kornet, J. G., Meuleman,
    J.,

    Bontsema, J., et al. (2002). An autonomous robot for harvesting cucumbers in

    greenhouses. Auton. Robots 13, 241–258. doi: 10.1023/A:1020568125418

    Vázquez-Arellano, M., Paraforos, D. S., Reiser, D., Garrido-Izard, M., and

    Griepentrog, H. W. (2018). Determination of stem position and height of

    reconstructed maize plants using a time-of-ﬂight camera. Comput. Electron.

    Agric. 154, 276–288.

    Vidoni, R., Gallo, R., Ristorto, G., Carabin, G., Mazzetto, F., Scalera, L., et
    al. (2017).

    “ByeLab: An agricultural mobile robot prototype for proximal sensing and

    precision farming,” in ASME International Mechanical Engineering Congress

    and Exposition, (New York, NY: American Society of Mechanical Engineers),

    V04AT05A057.

    Vijayarangan, S., Sodhi, P., Kini, P., Bourne, J., Du, S., Sun, H., et al. (2018).
    in

    High-Throughput Robotic Phenotyping of Energy Sorghum Crops BT - Field and

    Service Robotics, eds M. Hutter and R. Siegwart (Cham: Springer International

    Publishing), 99–113.

    Virlet, N., Sabermanesh, K., Sadeghi-Tehran, P., and Hawkesford, M. J. (2017).

    Field scanalyzer: an automated robotic ﬁeld phenotyping platform for detailed

    crop monitoring. Funct. Plant Biol. 44, 143–153. doi: 10.1071/FP16163

    Vu, Q., Rakovi´c, M., Delic, V., and Ronzhin, A. (2018). “Trends in development
    of

    UAV-UGV cooperation approaches,” in Precision Agriculture BT - Interactive

    Collaborative Robotics, eds A. Ronzhin, G. Rigoll, and R. Meshcheryakov

    (Cham: Springer International Publishing), 213–221.

    Walter, A., Liebisch, F., and Hund, A. (2015). Plant phenotyping: from bean

    weighing to image analysis. Plant Methods 11:14. doi: 10.1186/s13007-015-

    0056-8

    Wang, H., Peng, J., Xie, C., Bao, Y., and He, Y. (2015). Fruit quality evaluation
    using

    spectroscopy technology: a review. Sensors 15, 11889–11927.

    Wang, L., Zhu, G., Johnson, W., and Kher, M. (2018). Three new approaches to

    genomic selection. Plant Breed. 137, 673–681.

    Wei, H., Shao, Z., Huang, Z., Chen, R., Guan, Y., Tan, J., et al. (2016). RT-ROS:

    a real-time ROS architecture on multi-core processors. Futur. Gener. Comput.

    Syst. 56, 171–178. doi: 10.1016/j.future.2015.05.008

    Weiss, U., and Biber, P. (2011). Plant detection and mapping for agricultural
    robots

    using a 3D LIDAR sensor. Rob. Auton. Syst. 59, 265–273. doi: 10.1016/j.robot.

    2011.02.011

    White, J. W., Andrade-Sanchez, P., Gore, M. A., Bronson, K. F., Coﬀelt, T. A.,

    Conley, M. M., et al. (2012). Field-based phenomics for plant genetics research.

    F. Crop. Res. 133, 101–112. doi: 10.1016/j.fcr.2012.04.003

    Wolfert, S., Ge, L., Verdouw, C., and Bogaardt, M.-J. (2017). Big data in smart

    farming – a review. Agric. Syst. 153, 69–80. doi: 10.1016/j.agsy.2017.01.023

    Wu, C., Zeng, R., Pan, J., Wang, C. C. L., and Liu, Y. (2019). Plant phenotyping

    by deep-learning-based planner for multi-robots. IEEE Robot. Autom. Lett. 4,

    3113–3120. doi: 10.1109/LRA.2019.2924125

    Wu, G. F., Huang, L. X., and He, Y. (2008). Research on the sugar content

    measurement of grape and berries by using Vis/NIR spectroscopy technique.

    Guang pu xue yu Guang pu fen xiGuang pu 28, 2090–2093.

    Würschum, T. (2019). “Modern ﬁeld phenotyping opens new avenues for

    selection,” in Applications of Genetic and Genomic Research in Cereals, eds T.

    Miedaner and V. Korzun (Amsterdam: Elsevier), 233–250.

    Xiong, Y., Ge, Y., Grimstad, L., and From, P. J. (2020). An autonomous

    strawberry-harvesting robot: Design, development, integration, and ﬁeld

    evaluation. J. F. Robot. 37, 202–224.

    Xiong, B., Wang, B., Xiong, S., Lin, C., and Yuan, X. (2019). 3D morphological

    processing for wheat spike phenotypes using computed tomography images.

    Remote Sens. 11:1110.

    Xu, R., Li, C., and Mohammadpour Velni, J. (2018). Development of an

    autonomous ground robot for ﬁeld high throughput phenotyping. IFAC-

    PapersOnLine 51, 70–74. doi: 10.1016/j.ifacol.2018.08.063

    Young, S. N., Kayacan, E., and Peschel, J. M. (2019). Design and ﬁeld evaluation

    of a ground robot for high-throughput phenotyping of energy sorghum. Precis.

    Agric. 20, 697–722. doi: 10.1007/s11119-018-9601-6

    Yuan, H., Wang, N., Bennett, R., Burditt, D., Cannon, A., and Chamberlin,

    K. (2018). Development of a ground-based peanut canopy phenotyping

    system.

    IFAC

    PapersOnLine

    51,

    162–165.

    doi:

    10.1016/j.ifacol.2018.

    08.081

    Zahid, A., Mahmud, M. S., He, L., Choi, D., Heinemann, P., and Schupp, J. (2020).

    Development of an integrated 3R end-eﬀector with a cartesian manipulator for

    pruning apple trees. Comput. Electron. Agric. 179:105837.

    Zhang, F., Leitner, J., Milford, M., Upcroft, B., and Corke, P. (2015). Towards

    vision-based deep reinforcement learning for robotic motion control. arXiv

    [Preprint]. arXiv1511.03791.

    Zhang, J., Gong, L., Liu, C., Huang, Y., Zhang, D., and Yuan, Z. (2016).

    Field phenotyping robot design and validation for the crop breeding. IFAC

    PapersOnLine 49, 281–286. doi: 10.1016/j.ifacol.2016.10.052

    Zhang, Q., Karkee, M., and Tabb, A. (2019). The use of agricultural robots in

    orchard management. arXiv [Preprint]. arXiv1907.13114.

    Zhang, Z., Kayacan, E., Thompson, B., and Chowdhary, G. (2020). High

    precision control and deep learning-based corn stand counting algorithms for

    agricultural robot. Auton. Robots 44, 1289–1302.

    Zhao, J., Mantilla Perez, M. B., Hu, J., and Salas Fernandez, M. G. (2016).

    Genome-wide association study for nine plant architecture traits in sorghum.

    Plant Genome 9, 1–14. doi: 10.3835/plantgenome2015.06.0044

    Zhou, Y., Srinivasan, S., Mirnezami, S. V., Kusmec, A., Fu, Q., Attigala, L.,

    et al. (2019). Semiautomated feature extraction from RGB images for sorghum

    panicle architecture GWAS. Plant Physiol. 179, 24–37. doi: 10.1104/pp.18.

    00974

    Conﬂict of Interest: The authors declare that the research was conducted in the

    absence of any commercial or ﬁnancial relationships that could be construed as
    a

    potential conﬂict of interest.

    Copyright © 2021 Ateﬁ, Ge, Pitla and Schnable. This is an open-access article

    distributed under the terms of the Creative Commons Attribution License (CC BY).

    The use, distribution or reproduction in other forums is permitted, provided the

    original author(s) and the copyright owner(s) are credited and that the original

    publication in this journal is cited, in accordance with accepted academic practice.
    No

    use, distribution or reproduction is permitted which does not comply with these
    terms.

    Frontiers in Plant Science | www.frontiersin.org

    18

    June 2021 | Volume 12 | Article 611940

    '
  inline_citation: '>'
  journal: Frontiers in plant science
  limitations: '>'
  pdf_link: https://www.frontiersin.org/articles/10.3389/fpls.2021.611940/pdf
  publication_year: 2021
  relevance_evaluation: Moderately relevant - The paragraph provides a brief overview
    of the potential future advancements of robotic systems for plant phenotyping.
  relevance_score: '0.6997880722553482'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Robotic Technologies for High-Throughput Plant Phenotyping: Contemporary
    Reviews and Future Perspectives'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1016/j.biosystemseng.2019.08.017
  analysis: '>'
  apa_citation: Gené-Mola, J., Gregorio, E., Guevara, J., Sanz-Cortiella, R., Escolà,
    A., Llorens, J., ... Rosell-Polo, J. R. (2019). Fruit detection in an apple orchard
    using a mobile terrestrial laser scanner. Biosystems Engineering, 187, 171–184.
    https://doi.org/10.1016/j.biosystemseng.2019.08.017
  authors:
  - Jordi Gené-Mola
  - Eduard Gregorio
  - Javier Guevara
  - Fernando Auat
  - Ricardo Sanz
  - Alexandre Escolà
  - Jordi Llorens
  - Josep Ramon Morros
  - Javier Ruiz-Hidalgo
  - Verónica Vilaplana
  - Joan R. Rosell-Polo
  citation_count: 73
  data_sources: 3D point cloud generated using an MTLS with a Velodyne VLP-16 LiDAR
    sensor synchronized with an RTK-GNSS satellite navigation receiver.
  explanation: 'This study presents a novel technique that uses a mobile terrestrial
    laser scanner (MTLS) with reflectance capabilities to detect and 3D locate Fuji
    apples in producing orchard trees. An experiment was carried out on September
    28th of 2017 in Tarassó farm, a commercial apple orchard located in Catalonia,
    Spain. The methodology is founded on the fact that apples have higher apparent
    reflectance than leaves and trunks at the 905 nm laser wavelength. The main contributions
    of this paper are: (1) analysis of apple reflectivity on 3D point clouds from
    LiDAR sensors; (2) development of an apple detection and localization algorithm
    based on three stages (point cloud segmentation; fruit separation, and false positive
    removal); and (3) experimental validation of the proposed technique on a real
    Fuji apple orchard.'
  extract_1: Apples present higher IR reflectance than leaves and trunks.
  extract_2: A new methodology for fruit detection using an MTLS has been developed.
  full_citation: '>'
  full_text: '>

    Typesetting math: 100% Skip to main content Skip to article Journals & Books Search
    Register Sign in Brought to you by: University of Nebraska-Lincoln View PDF Download
    full issue Outline Highlights Keywords Nomenclature 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusions Acknowledgements Appendix
    A. Parameter values and feature analysis References Show full outline Cited by
    (76) Figures (10) Show 4 more figures Tables (5) Table 1 Table 2 Table 3 Table
    4 Table A1 Biosystems Engineering Volume 187, November 2019, Pages 171-184 Research
    Paper Fruit detection in an apple orchard using a mobile terrestrial laser scanner
    Author links open overlay panel Jordi Gené-Mola a, Eduard Gregorio a, Javier Guevara
    b, Fernando Auat b, Ricardo Sanz-Cortiella a, Alexandre Escolà a, Jordi Llorens
    a, Josep-Ramon Morros c, Javier Ruiz-Hidalgo c, Verónica Vilaplana c, Joan R.
    Rosell-Polo a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.biosystemseng.2019.08.017
    Get rights and content Highlights • Apples present higher IR reflectance than
    leaves and trunks. • A new methodology for fruit detection using an MTLS has been
    developed. • Results show a success rate of 82.4%, with a 10.4% of false detections.
    • The methodology is not affected by lighting and it provides apple locations
    in 3D. The development of reliable fruit detection and localization systems provides
    an opportunity to improve the crop value and management by limiting fruit spoilage
    and optimised harvesting practices. Most proposed systems for fruit detection
    are based on RGB cameras and thus are affected by intrinsic constraints, such
    as variable lighting conditions. This work presents a new technique that uses
    a mobile terrestrial laser scanner (MTLS) to detect and localise Fuji apples.
    An experimental test focused on Fuji apple trees (Malus domestica Borkh. cv. Fuji)
    was carried out. A 3D point cloud of the scene was generated using an MTLS composed
    of a Velodyne VLP-16 LiDAR sensor synchronised with an RTK-GNSS satellite navigation
    receiver. A reflectance analysis of tree elements was performed, obtaining mean
    apparent reflectance values of 28.9%, 29.1%, and 44.3% for leaves, branches and
    trunks, and apples, respectively. These results suggest that the apparent reflectance
    parameter (at 905 nm wavelength) can be useful to detect apples. For that purpose,
    a four-step fruit detection algorithm was developed. By applying this algorithm,
    a localization success of 87.5%, an identification success of 82.4%, and an F1-score
    of 0.858 were obtained in relation to the total amount of fruits. These detection
    rates are similar to those obtained by RGB-based systems, but with the additional
    advantages of providing direct 3D fruit location information, which is not affected
    by sunlight variations. From the experimental results, it can be concluded that
    LiDAR-based technology and, particularly, its reflectance information, has potential
    for remote apple detection and 3D location. Previous article in issue Next article
    in issue Keywords LiDARMobile terrestrial laser scanningFruit detectionAgricultural
    robotics Nomenclature FDRID False detection rate identification FDRL False detection
    rate localization FoV Field of View [°] FPID False positive identification FPL
    False positive localization GTfield Number of fruits manually-counted in field
    GTlabels Number of fruits labelled IoDi Intersection over detection K Number of
    fruits in a cluster MTLS Mobile Terrestrial Laser Scanner n Number of clusters
    that detect the same fruit Nm Fruit multi-detections (n-1) P Number of points
    of a cluster Pkj Number of points threshold used to find clusters with j apples
    R Apparent reflectance [%] Rth Reflectance threshold [%] RTK-GNSS Real-Time Kinematics
    Global Navigation Satellite System Mean apparent reflectance of the points of
    a cluster [%] FP Mean apparent reflectance threshold used to find false positive
    clusters [%] kj Mean apparent reflectance threshold used to find clusters with
    j apples [%] SuccessID Identification success (recall) SuccessL Localization success
    SVD Singular Value Decomposition TOF Time of flight TPID True positive identification
    TPL True positive localization V Volume of a cluster [m3] VFP Volume threshold
    used to find false positive clusters [m3] Vkj Volume threshold used to find clusters
    with j apples [m3] [x, y, z] 3D point with UTM coordinates [m] α Sparse outlier
    removal tuning parameter λin Normalised principal value i λi Principal value i
    of a cluster Ψ Geometric parameter Geometric parameter value used to find false
    positive clusters Geometric parameter value used to find clusters with j apples
    1. Introduction Fruticulture is under constant pressure to increase fruit production
    and quality, as demanded by a growing world population. To this end, farmers need
    to find new ways to improve fruit productivity and, at the same time, reduce economic
    and environmental costs (Siegel, Ali, Srinivasiah, Nugent, & Narayan, 2014). Agricultural
    robotics takes advantage of new technologies to respond to this challenge (Bac
    et al., 2014, Bechar and Vigneault, 2017, Bechar and Vigneault, 2016, Gongal et
    al., 2015, Zhao et al., 2016a). The use of robotics in agricultural fields and
    orchards is increasing, particularly in tasks related to guidance (seeding or
    harvesting), detection (weed monitoring and control, extraction of biological
    features), and mapping (Auat Cheein and Carelli, 2013, Auat Cheein et al., 2017,
    Foglia and Reina, 2006). In general, the development of intelligent robots interacting
    with agricultural fields increases the accuracy of tasks and reduces the consumption
    of resources without decreasing yield, making it a reasonable option for repeatable
    tasks (Cariou et al., 2009, Foglia and Reina, 2006, Zhang and Pierce, 2016). Fruit
    detection and localization are complex tasks that can be handled by agricultural
    robotics, with applications related to yield prediction, yield mapping, and automated
    harvesting. Nowadays, yield prediction is done by manual counting of selected
    sample trees, leading to inaccurate predictions due to the high variability in
    orchards (Payne et al., 2014, Stein et al., 2016). Crop monitoring using new technologies
    could provide more accurate and efficient predictions (Bechar and Vigneault, 2017,
    Bechar and Vigneault, 2016). Another application of fruit detection is yield mapping.
    The fruit load of an orchard is influenced by in-field spatial variability (due
    to soil type variations), fertility, and water content, among other factors. In
    precision agriculture, yield mapping helps to determine the reasons for and find
    solutions to cope with this variability (Kurtulmus, Lee, & Vardar, 2014). Finally,
    fruit localization is the basis for future automated harvesting. Manual picking
    is a bottleneck in fruit production management, because it requires lots of resources
    in the context of decreasing farming labour force. In addition, hand harvesting
    exposes farmers to awkward postures on ladders and platforms with heavy loads,
    making manual harvesting dangerous and inefficient (De-An et al., 2011, Gongal
    et al., 2015). The detection of fruits can involve many fruit properties of different
    complexity, from the simplest, such as the presence/absence of a fruit, to properties
    that are more challenging to measure, including size, volume, diameter, maturation
    stage, sugar, and other substance contents, defects and disease/pest affectation,
    etc. There are multiple technologies available for fruit detection and localization,
    each with its advantages and disadvantages (Gongal et al., 2015). All approaches
    have to solve problems derived from occlusions (Stein et al., 2016, Wachs et al.,
    2010), clustering (Gong et al., 2013, Xiang et al., 2014), and variable lighting
    conditions (Gongal et al., 2016, Zhao et al., 2016b). The most commonly used sensors
    are RGB cameras (Linker, 2017, Maldonado and Barbosa, 2016, Zhao et al., 2016b).
    These are affordable sensors, which allow fruits to be distinguished from other
    elements by colour (Linker et al., 2012, Liu et al., 2016), geometric shape (Barnea
    et al., 2016, Lak et al., 2010), texture (Chaivivatrakul and Dailey, 2014, Qureshi
    et al., 2017), or by using machine learning techniques like, e.g., deep neural
    networks (Bargoti & Underwood, 2017). The two main drawbacks to RGB cameras are
    their sensitivity to lighting conditions and the fact that they only provide 2D
    information (unless using stereoscopic techniques). Other, more expensive, cameras
    include thermal cameras (Bulanon et al., 2008, Bulanon et al., 2009, Stajnko et
    al., 2004, Wachs et al., 2010), multispectral cameras (Sa et al., 2016, Zhang
    et al., 2015), and hyperspectral cameras (Okamoto and Lee, 2009, Safren et al.,
    2007). The former allows fruits to be distinguished from the background through
    their temperature, while the latter detect fruits from their reflectance at different
    wavelengths. Like RGB cameras, thermal, multispectral, and hyperspectral cameras
    do not provide 3D information, unless a stereoscopic approach is implemented.
    There are several solutions to obtain three-dimensional information. One of them
    is based on using two (stereovision) or more cameras (Font et al., 2014, Si et
    al., 2015, Xiang et al., 2014). By applying triangulation techniques, it is possible
    to obtain the depth of each pixel and reconstruct the 3D structure. The major
    advantage of this technique is that it allows us to obtain accurate 3D models
    with RGB information, while the main disadvantages are that 3D model generation
    is computationally expensive and the performance is affected by lighting conditions.
    Another more recent technique is the use of laser range finders and LiDAR-based
    (Light Detection and Ranging) systems. These are more expensive sensors that generally
    operate under the principle of time-of-flight (TOF) (Wehr & Lohr, 1999). This
    type of sensor typically also provides the amount of energy backscattered from
    the impacted object. Very few studies have used LiDAR-based systems in fruit detection
    and, to the best of the authors’ knowledge, none of them have been tested in a
    real orchard environment. For example, Jiménez et al., 2000, Jiménez et al., 1999
    developed a vision system based on a laser range-finder, with the aim of detecting
    spherical objects in non-structured environments. They report good detection performances,
    although the tests were carried out on a limited number of oranges suspended from
    an artificial tree. Finally, another technology derived from photogrammetry and
    LiDAR, and also used in fruit growing, are the RGB-D (depth) cameras, where each
    pixel of the image contains colour and depth data, generating 3D colour images
    (Barnea et al., 2016, Nguyen et al., 2016, Rosell-Polo et al., 2017, Rosell-Polo
    et al., 2015). These systems are based on the simultaneous combination of RGB
    cameras and depth sensors based on laser light (either through structured laser
    light or TOF flash-type LiDAR-based systems). This work presents a proof of concept
    of using LiDAR in detecting Fuji apples in producing orchard trees. The methodology
    is founded on the fact that apples have higher apparent reflectance than leaves
    and trunks at 905 nm laser wavelength. The main contributions of this paper are:
    (1) analysis of apple reflectivity on 3D point clouds from LiDAR sensors; (2)
    development of an apple detection and localization algorithm based on three stages
    (point cloud segmentation; fruit separation, and false positive removal); and
    (3) experimental validation of the proposed technique on a real Fuji apple orchard.
    The principal advantage of this technique over previously published efforts would
    be its capacity to provide direct 3D fruit localization information without being
    affected by illumination conditions. The paper is structured as follows. Section
    2 presents the experimental data set, the point cloud generation procedure, the
    reflectance analysis, and the developed apple detection algorithm. Section 3 shows
    the results of the first experimental tests performed on three Fuji apple trees
    of a commercial orchard. Finally, the conclusions are presented in Section 4.
    2. Materials and methods 2.1. Experimental set up A fruit detection experiment
    was carried out on September 28th of 2017 in Tarassó farm, a commercial apple
    orchard located in Agramunt, Catalonia, Spain (E: 336,297 m; N: 4,623,494 m; 312
    m a.s.l., UTM 31T - ETRS89). The trials were carried out in an 8-year-old Fuji
    apple orchard (Malus domestica Borkh. cv. Fuji), trained in a tall spindle system
    with a maximum tree height of 3.75 m. The three analysed trees were at BBCH (Biologische
    Bundesanstalt, Bundessortenamt und CHemische Industrie) growth stage 85 (Meier,
    2001), three weeks before harvesting. The measurement equipment consisted of a
    mobile Terrestrial Laser Scanner (MTLS), comprised of a LiDAR sensor and a real-time
    kinematics global navigation satellite system (RTK-GNSS), connected to a rugged
    laptop suitable for working in field conditions. The LiDAR sensor used was a Puck
    VLP-16 (Velodyne LIDAR Inc., San José, CA, USA), which generates a 3D point cloud
    (x-y-z positions) of the scanned scene, as well as calibrated apparent reflectance
    (R) of each point in the 3D point cloud. This calibration was carried out by sensor
    manufacturer using a set of calibration targets, and implies a conversion of the
    backscattered range-corrected intensity (digital numbers) into apparent reflectance
    values independently of laser power and distance (Velodyne, 2016). Note that the
    measured apparent reflectance (hereinafter referred to as reflectance) is an approximation
    of the actual hemispherical reflectance, considering that the measured objects
    are Lambertian (perfectly diffuse reflectors), and not considering the incidence
    angle (Kaasalainen et al., 2011, Kukko et al., 2008, Ray, 1994). The VLP-16 sensor
    emits 16 laser beams (905 nm wavelength) with a horizontal angular resolution
    of 2° (30° horizontal FoV) when mounted on a vertical plane as shown in Fig. 1.
    Although the vertical FoV can be set up to 360°, in this experiment it was set
    to 150°, since only one row of trees was scanned. The scanning frequency rate
    was set to 10 Hz, corresponding to a vertical angular resolution of 0.2°, so that
    a maximum of 12,000 points were obtained from each scan (acquisition speed of
    120,000 points/second). Even though this sensor has a range of 100 m, points further
    than 4 m where not considered for 3D point cloud generation, thus only the tree
    row of interest was modelled. The acquisition of Coordinated Universal Time (UTC)
    of each point was obtained via a GPS 18x LVC receiver (Garmin International Inc.,
    Olathe, KS, USA), connected to the VLP-16 sensor. The RTK-GNSS system used was
    the GPS1200+ (Leica Geosystems AG, Heerbrugg, Swizeland), which provides absolute
    coordinates and UTC time (synchronised with the LiDAR) with a frequency of 20
    Hz and a precision of approx. 20 mm. Download : Download high-res image (1MB)
    Download : Download full-size image Fig. 1. View of the MTLS equipment showing
    the GNSS antenna placement and the mounting orientation of the LiDAR sensor. Distance
    data are in mm. As shown in Fig. 1, the MTLS measurement system was mounted on
    the rear of an air-assisted sprayer by means of an aluminium structure. The sprayer
    was pulled at low gear by a farm tractor equipped with an electronic speedometer.
    The GNSS rover receiver antenna was installed on top of the mast, at a height
    of 3.5 m. The LiDAR sensor was mounted vertically (Fig. 1) and placed at a height
    of 1.8 m, that is about half the maximum height of studied trees. This position
    was selected to have similar detection performance along the tree height. The
    field test was performed by moving the MTLS along a rectilinear trajectory parallel
    to the tree row axis, at a distance of 2.4 m. Due to the fact that the system
    did not include an inertial measurement unit (IMU), moving the MTLS along a linear
    trajectory was important to improve the point cloud consistency. The forward speed
    was 0.125 m s−1, corresponding to a resolution of 12.5 mm between consecutive
    scans (∼53,600 points m−2 in a vertical plane at the distance of 2.4 m). The tree
    row was scanned from both sides in order to obtain a complete 3D model. 2.2. 3D
    point cloud model A rigid transformation was performed by applying a rotation
    and translation matrix to each point, in order to build the point cloud with absolute
    coordinates. The translation matrix was built using the Universal Transverse Mercator
    (UTM) coordinates of the RTK-GNSS system, by considering the relative distance
    between the optical centre of the LiDAR sensor and the GNSS receiver. The rotation
    matrix depends on the orientation of the MTLS at each time instant and was obtained
    by the forward direction computed from the measurements of the RTK-GNSS receiver.
    Given that the trials were performed with a short rectilinear trajectory, the
    tilt of the platform can be ignored, assuming a constant orientation along the
    path. An illustration of the 3D point cloud models generated is shown in Fig.
    2. Download : Download high-res image (792KB) Download : Download full-size image
    Fig. 2. 3D point cloud models obtained for trees 1, 2, and 3. First two trees
    were used as training dataset, while the third one was kept as test dataset. Ground
    truth bounding boxes of tree 3 are shown, while the zoom bounding box (red circle)
    shows its shape. The resulting 3D point cloud was manually labelled in order to
    generate ground truth of the apples locations. This enables a study of the features
    that characterise the apples, as well as the possibility to evaluate the performance
    of the developed apple detection techniques. The annotation was carried out using
    the software CloudCompare (Cloud Compare [GPL software] v2.9 Omnia), placing 3D
    rectangular bounding boxes on each apple, as can be seen in the third tree of
    Fig. 2. This annotation was supported by additional RGB images to localise the
    apples in the 3D point cloud. The actual number of apples counted in field (ground
    truth field or GT_field) were 139 in tree 1, 145 in tree 2, and 139 in tree 3,
    of which 133, 138 and 134, respectively, could be labelled in the 3D point cloud
    (ground truth labels or GT_labels) due to occlusions or in field counting errors.
    Trees 1 and 2 were used as the training dataset to select and tune the algorithm
    parameters, while tree 3 was used as the test dataset to evaluate the performance
    of the developed algorithm. From the labelled scene and the reflectance data extracted
    from the LiDAR for each point, a reflectance study of the different elements of
    the tree was carried out (Section 3.1). 2.3. Apple detection algorithm As shown
    in Fig. 3, the algorithm proposed in this paper is structured as follows: 1) Point
    cloud segmentation; 2) fruit separation; and 3) false positive removal. The segmentation
    is based on the reflectance of measured elements and aims at removing points corresponding
    to leaves, branches, and the trunk, and grouping the remaining points -likely
    to be an apple-in clusters. The fruit separation uses features of clusters in
    order to identify and split those that contain more than one apple. False positive
    removal is based on the geometry and reflectance of the clusters. All data processing
    was implemented in MATLAB (R2018a, Math Works Inc., Natick, Massachusetts, USA).
    The different implemented steps are detailed below. Download : Download high-res
    image (225KB) Download : Download full-size image Fig. 3. Apple detection algorithm
    flowchart. 2.3.1. Point cloud segmentation The objective of this step is to segment
    the 3D point cloud and obtain a set of clusters with points that could be apple
    candidates. Since some groups of apples could be touching, the clusters obtained
    in this first step could contain one or more apples. The 3D model acquired with
    the MTLS consists of a set of 3D points with UTM coordinates and their reflectance
    [x, y, z, R]. The reflectance analysis (Section 3.1) shows that apple reflectance
    at the 905 nm laser wavelength is higher than that for leaves and the trunk and,
    therefore, this parameter is used for apple detection. To remove the points that
    do not correspond to apples, a threshold, Rth, is applied. This is followed by
    Sparse Outlier Removal (Rusu, Marton, Blodow, Dolha, & Beetz, 2008) to reduce
    the noise; this approach removes the points which fall outside μ + α·σ, with μ
    and σ being the mean and standard deviation, respectively, of the k nearest neighbour
    distances, while α is a tuning parameter. The point cloud segmentation ends with
    a connected components labelling using a density-based scan algorithm, DBSCAN
    (Ester, Kriegel, Sander, & Xu, 1996), which clusters points that have more than
    minPts points closer than a distance, ε. Outlier Removal is first applied to delete
    noisy points that otherwise would connect clusters from different apples. All
    the parameters used in this step were selected through a hyperparameter optimization
    procedure, using the training data set to search for the combination of parameters
    that best suits our data. In this search, we found that the results were stable
    against small variations in the different parameter values, except the reflectance
    threshold, the behaviour of which is shown in Section 3.1, Fig. 7. The parameter
    values used are detailed in Appendix A. Download : Download high-res image (363KB)
    Download : Download full-size image Fig. 4. Method 1 - Cluster splitting by Gaussian
    smoothing. The aim of this method is to determine the number of apples, K, that
    are contained in a cluster. a) Actual data before applying method 1: the real
    scene is scanned and the resulting point cloud is segmented, obtaining clusters
    likely to contain apples. b) Cluster containing 2 apples. c) 2D projection in
    four planes: (1) frontal; (2) lateral, (3) top; and (4) plane defined by 2 principal
    axes. d) Gaussian smoothing. e) Local maxima identification. Download : Download
    high-res image (223KB) Download : Download full-size image Fig. 5. Method 2 -
    Decision tree used to predict the number of apples in a cluster. Download : Download
    high-res image (677KB) Download : Download full-size image Fig. 6. Localization
    and identification performance evaluation criteria. Intersection over detection
    (IoD) is given for different scenarios, while true positive and false positive
    are calculated for localization and identification assessment approaches. Red
    shapes are apple detections, while green squares correspond to the ground truth
    labels. Download : Download high-res image (244KB) Download : Download full-size
    image Fig. 7. a) Precision (green dashed line), recall (red dotted line), and
    F1-score (blue solid line) versus the applied reflectance threshold; b) Gaussian
    distributions obtained for each tree element in the reflectance analysis of the
    training dataset. Green solid line corresponds to leaves, blue dotted line to
    trunks and red dashed line to fruits. The vertical dash-dotted line indicates
    the reflectance threshold used for fruit detection. (For interpretation of the
    references to color in this figure legend, the reader is referred to the Web version
    of this article.) 2.3.2. Apple separation If apples are properly separated, the
    results obtained in the previous step would consist of a set of clusters of one
    apple in each. Nevertheless, it was found that groups of apples touching will
    result in clusters of more than one apple. The aim of this second step is to identify
    the clusters containing more than one apple and split them into sub-clusters,
    each containing one apple. First, the number of apples, K, that make up a cluster
    has to be predicted, and then the cluster is split using the K-means algorithm.
    This clustering method aims to partition the 3D points into K sub-clusters in
    which each 3D point belongs to the sub-cluster with nearest mean (Jain, 2010).
    To predict the number of apples contained in each cluster (the K number used in
    the K-means algorithm), three different methods were tested. The first one is
    inspired by a template matching technique (Brunelli, 2009). The second method
    applies a decision tree, based on cluster features such as volume, density of
    points, reflectance, and shape. Finally, the third method is a combination of
    the previous two approaches. These methods are explained in more detail below.
    2.3.2.1. Method 1 The first approach projects the 3D point clouds of each cluster
    (Fig. 4b) onto a 2D plane, obtaining an image of the cluster with reflectance
    data at a resolution of 4 × 4 mm per pixel (Fig. 4c). The cluster image then is
    convolved with a Gaussian filter of size 20 × 20 pixels and standard deviation
    of 3.5. These parameters correspond to the measured fruit size, so that the dimension
    of this filter is 80 × 80 mm, similar to the mean size of the tested apples. Since
    the apples have an approximately spherical shape, when the cluster image is convolved
    with a Gaussian filter, the local maxima of the obtained image correspond to the
    centres of the apples (Fig. 4d). The value K, to be used in the K-means algorithm,
    corresponds to the number of local maxima found in the convolved image (Fig. 4e).
    The result of this method could vary with the 2D projection plane used (e.g.,
    the projection may produce occlusions). The technique is applied in four different
    planes to prevent this projection-induced variability: frontal, lateral, top,
    and the plane defined by the first two principal axes of the cluster (Fig. 4b,c).
    The value of K will be the maximum obtained in these four planes. The principal
    axes are the directions where the variance of data is maximised and, therefore,
    where the points exhibit the largest range. The first two principal axes define
    the principal plane of the cluster and are obtained by applying singular value
    decomposition (SVD) to the set of points forming the cluster. 2.3.2.2. Method
    2 The second method applies a decision tree based on cluster features. The first
    step is to extract the following features for each cluster: volume (V), number
    of cluster points (P), mean reflectance of cluster points ( ), and a geometric
    parameter, Ψ, computed as the product of normalised eigenvalues [λ1n, λ2n, λ3n
    ]. The volume (V) was defined as the volume enclosed by the boundary points of
    the cluster. Clusters that contain more than one fruit are expected to have a
    larger volume (V) and more points (P). However, when a fruit is placed next to
    a leaf or trunk (not filtered in previous steps), the cluster volume (V) and the
    number of points (P) could increase as well. Due to this fact, the threshold,
    , is applied, as it was observed that the mean reflectance of this kind of trunk/leaf
    co-located cluster is lower than clusters containing grouped fruits. The last
    features used are the eigenvalues, which provide information about the cluster
    shape. Spherical shapes (clusters with only one apple) will have similar eigenvalues,
    while elongated shapes will have different eigenvalues. Eigenvalues are obtained
    with SVD, and their values depend on the variance of the points projected on the
    principal axes. In order to compare eigenvalues of different clusters, a normalization
    step is applied so that the eigenvalues sum to one. From that, the geometric parameter
    Ψ is defined as the product of eigenvalues and a normalization factor. The normalization
    factor of 27 allows the geometrical parameter, Ψ, to be bound between 0 and 1:
    (1) (2) The implemented decision tree is based on the analysed features in the
    training data set and is composed of the following steps (Fig. 5): • Feature extraction:
    Compute V,P, , and of the studied cluster. • Step 1: If V, P, and are higher than
    the corresponding thresholds Vk1, Pk1, k1, and is smaller than k1, it is concluded
    that the cluster contains more than one apple. Otherwise, K is assigned the value
    1. • Step 2: A cluster will have more than two apples if P is higher than Pk2
    and is lower than k2, or if V is higher than Vk2. Otherwise, K is assigned the
    value of 2. • Step 3: K = 4 when a cluster meets both previous conditions and
    has a volume (V) higher than Vk3. Otherwise, K is assigned the value 3. All threshold
    values used in the decision tree were empirically selected by the graphical representation
    of four analysed features using the training dataset. The values used and the
    graphical representation of these features are presented in Appendix A, Table
    A1 and Fig. A1. 2.3.2.3. Method 3 By applying method 1, some single-fruit clusters
    are split into multiple detections due to partial occlusions of apples by leaves.
    Method 3 addresses this concern by combining methods 1 and 2. First, step 1 of
    method 2 is applied to distinguish between clusters with single or multiple apples.
    For those clusters that contain more than one apple, method 1 is applied to determine
    the value of K. 2.3.3. False positive removal After implementing the first two
    steps of the algorithm (segmentation and apple separation), it was observed that
    some detections do not actually correspond to apples, i.e., these were false positive
    detections. That is because some leaves and trunks have a texture or shape that
    result in a high reflectance. It was found that some of these erroneous detections
    had a different geometric shape ( ), volume (V), and mean reflectance ( ) compared
    to the successful detections. In order to reduce these false positives, the clusters
    that met the condition (  <  FP) | (  <  FP) | (V > VFP) were removed. In the
    same manner as with method 2, the thresholds were empirically selected from a
    graphical representation of these three features using the training dataset. The
    values used and the graphical representation of these features are presented in
    Appendix A, Table A1 and Fig. A2. 2.4. Performance evaluation In this work, the
    results were evaluated using two different approaches: localization and identification.
    The localization evaluation aims to assess the system in the context of harvesting
    automation. This approach assumes that a robotic arm, when it gets close to a
    group of apples, is able to separate different apples that have been detected
    within the same cluster, or to unify the multi-detections that correspond to the
    same apple. Thus, a detection that contains K apples counts as K true positives
    (Fig. 6a), while multi-detections are counted as one true positive and no false
    positives (Fig. 6e). The identification evaluation aims to assess the system for
    use in yield prediction or mapping. This assessment is performed cluster-by-cluster,
    so that a single detection containing K apples counts as only one true positive
    (Fig. 6a), while a single apple detected n times (multi-detection) is counted
    as one true positive and Nm = n-1 false positives (Fig. 6e). To evaluate object
    detection in images, the metric intersection over union (IoU) is commonly used.
    This is possible when both bounding-box and object detection can be seen as a
    group of pixels. In this study, the detections are groups of 3D points, while
    ground truth bounding boxes are cube regions. The metric IoU has been substituted
    by the intersection over detection (IoD) for this reason; IoD is defined as the
    percentage of detected points that are placed inside ground truth bounding boxes.
    The following defines the metrics used for each approach, namely localization
    (subscript L) and identification (subscript ID). • Intersection over detection
    (IoDi): Percentage of points, Pi, of a detection, i, that are placed inside ground
    truth bounding-boxes (GT). • True positive localization (TPL): Number of ground
    truth apples that are detected with an IoDi ≥ 0.5. • False positive localization
    (FPL): Number of detections with an IoDi < 0.5. • Localization success (SuccessL):
    Quotient between TPL and the number of labelled apples (GTlabels). • False detection
    rate localization (FDRL): Ratio between FPL and the total positive (TPL + FPL).
    • True positive identification (TPID): Number of clusters with an IoDi ≥ 0.5,
    minus multi-detections ( . • False positive identification (FPID): Sum of the
    number of detections with an IoDi < 0.5 (FPL), plus multi-detections. . • Identification
    success (SuccessID or recall): Quotient between TPID and GTlabels. • False detection
    rate identification (FDRID): Ratio between FPID and the total positive. • Precision:
    Percentage of TPID with respect to the total positive (TPID + FPID). • F1-score:
    Harmonic mean of precision and recall. Selected examples of the evaluation criteria
    can be seen in Fig. 6. Intersection over detection (IoD) is given for different
    scenarios, while true positive and false positive rates are calculated for localization
    and identification assessment approaches. Red shapes are apple detections, while
    green squares correspond to the ground truth labels. Note that actual clusters
    and bounding-boxes are in 3D (as shown in Fig. 2), although for the sake of simplicity
    this figure shows the 2D projection. The examples shown are: a) One cluster with
    K = 2 apples and three GT bounding-boxes; b) One cluster with K = 1 apple and
    one GT bounding-box; c) Two clusters of K = 1 apple each and two GT bounding-boxes;
    d) One GT bounding-box not detected; e) Two clusters detecting the same GT bounding-box
    (multi-detection); f) One cluster that does not correspond to any GT object; and
    g) One cluster detecting an apple with an IoD<0.5. 3. Results and discussion 3.1.
    Reflectance analysis Table 1 shows the reflectance analysis results for both trees
    used in the training dataset. Mean apparent reflectance values of 28.9%, 29.1%,
    and 44.3% were obtained for leaves, trunks, and Fuji apples, respectively. These
    results indicate that the reflectance is higher than other tree elements. Hence,
    this characteristic will be used as a valuable feature for Fuji apple detection.
    Note that these results were obtained using a LiDAR system operating with a laser
    source at 905 nm wavelength. Further studies should be carried out to ensure that
    the present methodology could be extended to other laser systems (operating at
    different wavelengths) and other fruit varieties or branching structures. Table
    1. Reflectance analysis: The mean apparent reflectance and standard deviation
    of different elements in an apple orchard. Tree Elements mean (R) [%] std (R)
    [%] T1 Leaves 29.23 13.57 T2 Leaves 28.69 13.88 T1 Trunks 29.67 14.83 T2 Trunks
    28.52 15.41 T1 Apples 43.59 16.81 T2 Apples 45.10 16.78 The results of this analysis
    are the basis of the proposed detection algorithm, with reflectance being the
    principal feature used in the segmentation step. Although Fuji apples have higher
    reflectance than leaves and trunks at 905 nm, the standard deviation is high enough
    to create overlap between classes (Fig. 7b). In order to find the optimal threshold,
    Rth, that will remove the points corresponding to leaves, branches, and trunks,
    a performance evaluation of the detection algorithm (Section 2.3) was carried
    out using different reflectance thresholds. Figure 7a plots the evolution of precision,
    recall, and F1-score metrics, computed before applying the false positive removal
    step, under different reflectance thresholds, Rth. The best results were obtained
    with an Rth = 60%, resulting in an F1-score = 82.16% for the training dataset.
    Figure 7b shows the reflectance distributions for leaves, trunks, and apples.
    As can be seen, most of the 3D points belonging to apples were below the threshold
    value. This is because our restrictive threshold minimises the false positives
    of leaves and trunks being selected as apples. Furthermore, omitting points as
    apple is not as critical as having a few apple points in a cluster, which are
    sufficient for detection. 3.2. Step-by-step algorithm performance evaluation This
    section presents a qualitative and quantitative evaluation of the different steps
    and methods implemented in this paper. Regarding the qualitative evaluation, Fig.
    8 illustrates the evolution after each processing step. First, Fig. 8a shows an
    RGB image of one of the trees, which is incorporated to assist in visualization,
    but was not used in the algorithm. Figure 8b renders the 3D model obtained with
    the MTLS. The colour scale indicates the reflectance of each point, where blue
    corresponds to low values and red implies high reflectance. It is evident from
    this representation how Fuji apples exhibit higher reflectance than other tree
    elements. Figure 8c shows the results after applying the reflectance threshold,
    Rth, to the original point cloud. In this step, many of the leaf and trunk points
    were removed. Once the sparse outlier removal is applied (Fig. 8d), zones with
    low point density were removed, leaving only groups of points which are candidates
    for apple detection. Figure 8e illustrates the segmentation output, which terminates
    the clustering of connected points. This result has clusters with one apple (red,
    orange, blue, and purple), clusters with more than one apple (green), and false
    positives (grey). The apple detection algorithm ends by splitting clusters with
    more than one apple and removing false positives. The final result is presented
    in Fig. 8f. Download : Download high-res image (798KB) Download : Download full-size
    image Fig. 8. Illustration of the different processing steps (tree 2). a) RGB
    image. b) Point cloud obtained with the MTLS. c) Point cloud after applying the
    reflectance threshold. d) Sparse outlier removal. e) Connected component labelling
    (DBSCAN). f) Apple separation and false positive removal. For better visualization
    purposes, in b), scale ranges from 0 (blue) to 100 (red), while in c) and d) the
    scale ranges from 60 (blue) to 100 (red). Table 2 presents the results of the
    test dataset for each step and method implemented. The first row shows the results
    after point cloud segmentation (Section 2.3.1); rows 2–4 indicate the results
    obtained when applying the splitting techniques presented in Section 2.3.2; and
    the last three rows present the final results after removing the false positives
    detected (Section 2.3.3). Table 2. Performance assessment of the different implemented
    steps and methods: point cloud segmentation (S); apple separation methods 1, 2,
    and 3 (M1, M2, and M3, respectively); and false positive removal step (FPr). Results
    include information from the test dataset (tree 3). Method Localization Identification
    Processing Time [s] SuccessL [%] FDRL [%] SuccessID [%] FDRID [%] S 87.5 11.9
    73.5 13.8 11.0 S + M1 87.5 20.7 85.3 26.1 11.9 S + M2 87.5 15.6 82.4 17.0 11.0
    S + M3 87.5 16.8 84.6 20.7 12.0 S + M1 + FPr 87.5 12.5 85.3 18.3 12.1 S + M2 +
    FPr 87.5 9.8 82.4 10.4 11.1 S + M3 + FPr 86.8 11.3 83.8 14.9 12.4 The localization
    success values obtained after point cloud segmentation (before apple separation
    and false positive removal) are slightly higher than 87% (first row). These results
    are similar to other methodologies using colour cameras (Gongal et al., 2015).
    The identification success presents significantly lower results (∼73%), because
    of some detections containing more than one apple. Methods 1, 2, and 3 therefore
    were applied, in order to split these clusters, methods (Section 2.3.2). As a
    result, the identification success increased by more than 8% (rows 2 to 4), although
    the number of false positives also increased due to multi-detections. Method 1
    performed best in terms of increasing the identification success (+11%), but also
    generated more multi-detections. Method 2 increased identification success by
    more than 8%, while false positives only increased 3%. The results of method 3
    are a trade-off between the previous two methods. Since localization success performs
    an evaluation on a point-by-point basis, applying separation methods does not
    vary the results of this metric. When applying false positive removal (rows 5
    to 7), it is observed that the false detection rate fell by more than 5%, while
    the localization and identification successes were not affected (except for method
    3, with a decrease of less than 1%). The best results were obtained by combining
    method 2 with false positive removal, resulting in a lower number of false positives,
    without affecting the performance of the apple detection algorithm. The processing
    times indicated in Table 2 correspond to processing the data with a 64-bit operating
    system, with 8 GB of RAM and an Intel ® Core(TM) i7-4500U processor (1.80 GHz,
    boosted to 2.40 GHz). Although method 2 was slightly more efficient than the other
    two approaches, no significant differences were observed in the processing time.
    This is because the most computationally intensive operation is in the DBSCAN
    clustering algorithm (9.1 s), which is part of the segmentation step included
    in all methods. 3.3. Detection results Table 3 shows the apple detection algorithm,
    as evaluated individually for each tree. These results were generated by applying
    the point cloud segmentation, followed by an apple separation using method 2,
    and removing false positives using the condition expressed in Section 2.3. The
    detection rate is similar for processed trees despite being slightly better for
    tree 1 and 3. A localization success of 87.5% with a 9.8% of FDRL, an identification
    success of 82.4% with a 10.4% of FDRID, and an F1-score of 85.8% were obtained
    using the test dataset. These results are comparable with those obtained with
    other methodologies used in the state of the art. So far, the best detection rates
    have been reported with image processing, obtaining accuracies of between 80%
    and 85% using colour features (Gongal et al., 2015), and up to 86% of recall using
    deep learning (Bargoti & Underwood, 2017). However, the vision systems used in
    harvesting robots in orchard environments report a mean value of 80% in localization
    success and a mean value of 70% in identification success (Bac et al., 2014).
    Although it is difficult to compare the research found in the state of the art
    review, given that they are evaluated with different datasets, the methodology
    presented in this paper yields similar detection rates to previous work based
    on colour cameras, with the advantage that LiDAR-based measurements are not affected
    by illumination conditions. Furthermore, the location of each detected apple is
    obtained directly, which makes the presented system very interesting for autonomous
    harvesting or fruit load assessment for yield mapping applications. Table 3. Apple
    detection assessment using method 2. Trees 1 and 2 were used as training dataset
    and tree 3 as test dataset. GTfield corresponds to the number of apples hand-counted
    in field, while GTlabels corresponds to the number of apples labelled in data.
    Other metrics are defined in Section 2.4. Tree GTfield GTlabels Localization Identification
    F1-score TPL FPL SuccessL FDRL TPID FPID SuccessID FDRID Tree 1 139 133 116 12
    87.2% 9.4% 110 16 82.7% 12.7% 0.849 Tree 2 145 138 118 13 85.5% 9.9% 109 15 79.0%
    12.1% 0.832 Tree 3 139 136 119 13 87.5% 9.8% 112 13 82.4% 10.4% 0.858 Regarding
    the computational cost, Table 4 includes the inference time, processing each tree
    separately, and all trees combined. The number of points of each test is also
    reported. As expected, the computational time increases with the number of points
    processed. Results show that processing trees individually is much more efficient
    than processing all trees at once. This is because the average run time complexity
    of DBSCAN is not linear with the number of points (Ester et al., 1996), resulting
    in higher efficiency when processing small point clouds. Table 4. Computational
    cost according to the number of points in the point cloud. Tree Nº of points Processing
    Time [s] Tree 1 438.260 8.0 Tree 2 460.847 9.6 Tree 3 526.136 11.2 Tree 1 + 2+3
    1.425.243 68.8 4. Conclusions This work presents a new methodology for Fuji apple
    detection and localization in real commercial orchard environments using a LiDAR-based
    mobile terrestrial laser scanner (MTLS) with reflectance capabilities. A reflectance
    analysis of the different apple tree elements was carried out, which showed that
    apples exhibit a higher reflectance than leaves and trunks at the 905 nm laser
    wavelength; we therefore conclude that this characteristic is a valuable feature
    for apple detection. An apple detection algorithm, suitable for dealing with point
    clouds obtained with an MTLS, was subsequently developed and tested on three apple
    trees from a commercial apple orchard. The algorithm is divided into three steps:
    (1) removal of points corresponding to leaves and trunk and clustering the remaining
    points with a connected component labelling, (2) identification and splitting
    of clusters that contain more than one apple, and (3) false positive reduction.
    In order to predict the number of apples grouped in a cluster, three different
    methods were proposed: template matching, decision tree, and a combination of
    both approaches. The best results were achieved by applying a decision tree, resulting
    in a localization success of 87.5% with a 9.8% false detection rate, an identification
    success of 82.4% with a 10.4% false detection rate, and an F1-score of 85.8% in
    the test dataset. These outcomes represent an advance in the fruit detection field,
    since the results are comparable with those from colour (RGB) camera systems used
    in past efforts; however, the proposed LiDAR-based has the additional advantages
    that measurements are not affected by illumination conditions and that the method
    directly provides 3D fruit location information. An important limitation of this
    work is the small dataset. A larger dataset could allow the parameters to be learnt
    automatically (instead of being manually selected), thereby obtaining an algorithm
    that could better generalise with new data. Future efforts should include an analysis
    of fruit reflectance under different laser wavelengths, the extension of the dataset
    to other fruit varieties and species, and the application of machine learning
    algorithms in larger datasets. Acknowledgements This work was partly funded by
    the Secretaria d’Universitats i Recerca del Departament d’Empresa i Coneixement
    de la Generalitat de Catalunya (grant 2017 SGR 646), the Spanish Ministry of Economy
    and Competitiveness (projects AGL2013-48297-C2-2-R and MALEGRA, TEC2016-75976-R)
    and the Spanish Ministry of Science, Innovation and Universities (project RTI2018-094222-B-I00).
    The Spanish Ministry of Education is thanked for Mr. J. Gené’s pre-doctoral fellowships
    (FPU15/03355). The work of Jordi Llorens was supported by Spanish Ministry of
    Economy, Industry and Competitiveness through a postdoctoral position named Juan
    de la Cierva Incorporación (JDCI-2016-29464_N18003). We would also like to thank
    CONICYT/FONDECYT for grant 1171431 and CONICYT FB0008. Nufri (especially Santiago
    Salamero and Oriol Morreres) and Vicens Maquinària Agrícola S.A. are also thanked
    for their support during the data acquisition. Appendix A. Parameter values and
    feature analysis Table A1 presents the values set for each parameter used in the
    algorithm. Parameter Rth is used in the segmentation step. See more details about
    these parameters in Section 2.3.1. Parameters with sub-index kj refer to the thresholds
    used in Section 2.3.2 - method 2 and were selected after analysing the graphical
    representation of cluster features shown in Fig. A1. Parameters with sub-index
    FP correspond to the thresholds used to remove false positives (Section 2.3.3)
    and were selected after analysing the graphical representation of detection features
    shown in Fig. A2. Table A1. Parameter values used to detect apples in the presented
    dataset. The first five parameters were used during the point cloud segmentation
    step. Parameters sub-indexed with letter K correspond to thresholds used in the
    apple separation step. Parameters sub-indexed with letters FP were used in the
    false positive removal step. Symbol Value Units Rth 60 % k 20 Points α 0 – minPts
    15 Points ε 0.03 m Vk1 1.5·10-4 m3 Pk1 85 Points k1 67.5 % k1 0.8 – Pk2 400 Points
    k2 0.6 – Vk2 1.2·10-3 m3 Vk3 1.6·10-3 m3 FP 0.46 – FP 65.25 % VFP 10–3 m3 Download
    : Download high-res image (339KB) Download : Download full-size image Fig. A1.
    Graphical representation of cluster features. The features analysed are the geometric
    parameter, Ψ, and the number of points (left), and the mean reflectance and the
    cluster volume (right). Clusters with one apple are represented in green squares;
    clusters with two apples are represented in blue diamonds; clusters with three
    apples in magenta asterisks; and clusters with four apples or more in black crosses.
    Yellow, red and blue lines correspond to K1, K2 and K3 thresholds, respectively.
    This analysis was performed on the training data set (Trees 1 and 2) and was used
    to set the thresholds explained in Section 2.3.2 - method 2. Download : Download
    high-res image (371KB) Download : Download full-size image Fig. A2. Graphical
    representation of detection features. The features analysed are the geometric
    parameter, Ψ, the mean reflectance and the cluster volume. False positives (FP)
    are represented by red crosses; true positives are represented by blue circles.
    Horizontal and vertical lines show the thresholds used to remove FP. This analysis
    was performed on the training data set (Trees 1 and 2) and was used to set the
    thresholds explained in Section 2.3.3. References Auat Cheein and Carelli, 2013
    F.A. Auat Cheein, R. Carelli Agricultural robotics: Unmanned robotic service units
    in agricultural tasks IEEE Industrial Electronic Magazine, 7 (2013), pp. 48-58,
    10.1109/MIE.2013.2252957 View in ScopusGoogle Scholar Auat Cheein et al., 2017
    F. Auat Cheein, M. Torres-Torriti, N.B. Hopfenblatt, Á.J. Prado, D. Calabi Agricultural
    service unit motion planning under harvesting scheduling and terrain constraints
    Journal of Field Robotics, 34 (2017), pp. 1531-1542, 10.1002/rob.21738 View in
    ScopusGoogle Scholar Bac et al., 2014 C.W. Bac, E.J. Van Henten, J. Hemming, Y.
    Edan Harvesting robots for high-value crops: State-of-the-art review and challenges
    ahead Journal of Field Robotics, 31 (2014), 10.1002/rob.21525 Google Scholar Bargoti
    and Underwood, 2017 S. Bargoti, J. Underwood Deep fruit detection in orchards.
    2017 IEEE International Conference on Robotics and Automation (2017), pp. 3626-3633
    View in ScopusGoogle Scholar Barnea et al., 2016 E. Barnea, R. Mairon, O. Ben-Shahar
    Colour-agnostic shape-based 3D fruit detection for crop harvesting robots Biosystems
    Engineering, 146 (2016), pp. 57-70, 10.1016/j.biosystemseng.2016.01.013 View PDFView
    articleView in ScopusGoogle Scholar Bechar and Vigneault, 2016 A. Bechar, C. Vigneault
    Agricultural robots for field operations: Concepts and components Biosystems Engineering,
    149 (2016), pp. 94-111, 10.1016/j.biosystemseng.2016.06.014 View PDFView articleView
    in ScopusGoogle Scholar Bechar and Vigneault, 2017 A. Bechar, C. Vigneault Agricultural
    robots for field operations. Part 2: Operations and systems Biosystems Engineering,
    153 (2017), pp. 110-128, 10.1016/j.biosystemseng.2016.11.004 View PDFView articleView
    in ScopusGoogle Scholar Brunelli, 2009 R. Brunelli Template matching techniques
    in computer vision - theory and practice John Wiley & Sons (2009) Google Scholar
    Bulanon et al., 2008 D.M. Bulanon, T.F. Burks, V. Alchanatis Study on temporal
    variation in citrus canopy using thermal imaging for citrus fruit detection Biosystems
    Engineering, 101 (2008), pp. 161-171, 10.1016/j.biosystemseng.2008.08.002 View
    PDFView articleView in ScopusGoogle Scholar Bulanon et al., 2009 D.M. Bulanon,
    T.F. Burks, V. Alchanatis Image fusion of visible and thermal images for fruit
    detection Biosystems Engineering, 103 (2009), pp. 12-22, 10.1016/j.biosystemseng.2009.02.009
    View PDFView articleView in ScopusGoogle Scholar Cariou et al., 2009 C. Cariou,
    R. Lenain, B. Thuilot, M. Berducat Automatic guidance of a four-wheel-steering
    mobile robot for accurate field operations Journal of Field Robotics (2009), 10.1002/rob.20282
    Google Scholar Chaivivatrakul and Dailey, 2014 S. Chaivivatrakul, M.N. Dailey
    Texture-based fruit detection Precision Agriculture, 15 (2014), pp. 662-683, 10.1007/s11119-014-9361-x
    View in ScopusGoogle Scholar De-An et al., 2011 Z. De-An, L. Jidong, J. Wei, Z.
    Ying, C. Yu Design and control of an apple harvesting robot Biosystems Engineering,
    110 (2011), pp. 112-122, 10.1016/j.biosystemseng.2011.07.005 View PDFView articleView
    in ScopusGoogle Scholar Ester et al., 1996 M. Ester, H.P. Kriegel, J. Sander,
    X. Xu A density-based algorithm for discovering clusters in large spatial databases
    with noise Proceeding of 2nd International Conference Knowledge in Discovery Data
    Mining, 96 (34) (1996), pp. 226-231 doi:10.1.1.71.1980 Google Scholar Foglia and
    Reina, 2006 M.M. Foglia, G. Reina Agricultural robot for radicchio harvesting
    Journal of Field Robotics (2006), 10.1002/rob.20131 Google Scholar Font et al.,
    2014 D. Font, T. Pallejà, M. Tresanchez, D. Runcan, J. Moreno, D. Martínez, et
    al. A proposal for automatic fruit harvesting by combining a low cost stereovision
    camera and a robotic arm Sensors (Switzerland), 14 (2014), pp. 11557-11579, 10.3390/s140711557
    View in ScopusGoogle Scholar Gongal et al., 2015 A. Gongal, S. Amatya, M. Karkee,
    Q. Zhang, K. Lewis Sensors and systems for fruit detection and localization: A
    review Computers and Electronics in Agriculture, 116 (2015), pp. 8-19, 10.1016/j.compag.2015.05.021
    View PDFView articleView in ScopusGoogle Scholar Gongal et al., 2016 A. Gongal,
    A. Silwal, S. Amatya, M. Karkee, Q. Zhang, K. Lewis Apple crop-load estimation
    with over-the-row machine vision system Computers and Electronics in Agriculture,
    120 (2016), pp. 26-35, 10.1016/j.compag.2015.10.022 View PDFView articleView in
    ScopusGoogle Scholar Gong et al., 2013 A. Gong, J. Yu, Y. He, Z. Qiu Erratum to
    “Citrus yield estimation based on images processed by an Android mobile phone”
    Biosystems Engineering, 116 (2013), pp. 111-112, 10.1016/j.biosystemseng.2013.07.004
    View PDFView articleView in ScopusGoogle Scholar Jain, 2010 A.K. Jain Data clustering:
    50 years beyond K-means Pattern Recognition Letters (2010), 10.1016/j.patrec.2009.09.011
    Google Scholar Jiménez et al., 2000 A.R. Jiménez, R. Ceres, J.L. Pons A vision
    system based on a laser range-finder applied to robotic fruit harvesting Machine
    Vision and Applications, 11 (2000), pp. 321-329, 10.1007/s001380050117 View in
    ScopusGoogle Scholar Jiménez et al., 1999 A.R. Jiménez, A.K. Jain, R. Ceres, J.L.
    Pons Automatic fruit recognition: A survey and new results using range/attenuation
    images Pattern Recognition, 32 (1999), pp. 1719-1736, 10.1016/S0031-3203(98)00170-8
    View PDFView articleView in ScopusGoogle Scholar Kaasalainen et al., 2011 S. Kaasalainen,
    A. Jaakkola, M. Kaasalainen, A. Krooks, A. Kukko Analysis of incidence angle and
    distance effects on terrestrial laser scanner intensity: Search for correction
    methods Remote Sensing (2011), 10.3390/rs3102207 Google Scholar Kukko et al.,
    2008 A. Kukko, S. Kaasalainen, P. Litkey Effect of incidence angle on laser scanner
    intensity and surface data Applied Optics (2008), 10.1364/ao.47.000986 Google
    Scholar Kurtulmus et al., 2014 F. Kurtulmus, W.S. Lee, A. Vardar Immature peach
    detection in colour images acquired in natural illumination conditions using statistical
    classifiers and neural network Precision Agriculture, 15 (2014), pp. 57-79, 10.1007/s11119-013-9323-8
    View in ScopusGoogle Scholar Lak et al., 2010 M.B. Lak, S. Minaei, J. Amiriparian,
    B. Beheshti Apple fruits recognition under natural luminance using machine vision
    Advance Journal of Food Science and Technology, 2 (2010), pp. 325-327 View in
    ScopusGoogle Scholar Linker, 2017 R. Linker A procedure for estimating the number
    of green mature apples in night-time orchard images using light distribution and
    its application to yield estimation Precision Agriculture, 18 (2017), pp. 59-75,
    10.1007/s11119-016-9467-4 View in ScopusGoogle Scholar Linker et al., 2012 R.
    Linker, O. Cohen, A. Naor Determination of the number of green apples in RGB images
    recorded in orchards Computers and Electronics in Agriculture, 81 (2012), pp.
    45-57, 10.1016/j.compag.2011.11.007 View PDFView articleView in ScopusGoogle Scholar
    Liu et al., 2016 X. Liu, D. Zhao, W. Jia, C. Ruan, S. Tang, T. Shen A method of
    segmenting apples at night based on color and position information Computers and
    Electronics in Agriculture, 122 (2016), pp. 118-123, 10.1016/j.compag.2016.01.023
    View PDFView articleView in ScopusGoogle Scholar Maldonado and Barbosa, 2016 W.
    Maldonado, J.C. Barbosa Automatic green fruit counting in orange trees using digital
    images Computers and Electronics in Agriculture, 127 (2016), pp. 572-581, 10.1016/j.compag.2016.07.023
    View PDFView articleView in ScopusGoogle Scholar Meier, 2001 U. Meier Growth stages
    of mono- and dicotyledonous plants BBCH Monograph (2001), 10.5073/bbch0515 Google
    Scholar Nguyen et al., 2016 T.T. Nguyen, K. Vandevoorde, N. Wouters, E. Kayacan,
    J.G. De Baerdemaeker, W. Saeys Detection of red and bicoloured apples on tree
    with an RGB-D camera Biosystems Engineering, 146 (2016), pp. 33-44, 10.1016/j.biosystemseng.2016.01.007
    View PDFView articleView in ScopusGoogle Scholar Okamoto and Lee, 2009 H. Okamoto,
    W.S. Lee Green citrus detection using hyperspectral imaging Computers and Electronics
    in Agriculture, 66 (2009), pp. 201-208, 10.1016/j.compag.2009.02.004 View PDFView
    articleView in ScopusGoogle Scholar Payne et al., 2014 A. Payne, K. Walsh, P.
    Subedi, D. Jarvis Estimating mango crop yield using image analysis using fruit
    at “stone hardening” stage and night time imaging Computers and Electronics in
    Agriculture, 100 (2014), pp. 160-167, 10.1016/j.compag.2013.11.011 View PDFView
    articleView in ScopusGoogle Scholar Qureshi et al., 2017 W.S. Qureshi, A. Payne,
    K.B. Walsh, R. Linker, O. Cohen, M.N. Dailey Machine vision for counting fruit
    on mango tree canopies Precision Agriculture, 18 (2017), pp. 224-244, 10.1007/s11119-016-9458-5
    View in ScopusGoogle Scholar Ray, 1994 T.W. Ray A FAQ on vegetation in remote
    sensing Div. of Geological and Planetary Sciences California Institute of Technology,
    California (1994) Google Scholar Rosell-Polo et al., 2015 J.R. Rosell-Polo, F.A.
    Cheein, E. Gregorio, D. Andújar, L. Puigdomènech, J. Masip, et al. Advances in
    structured light sensors applications in precision agriculture and livestock farming
    Advances in Agronomy (2015), 10.1016/bs.agron.2015.05.002 Google Scholar Rosell-Polo
    et al., 2017 J.R. Rosell-Polo, E. Gregorio, J. Gene, J. Llorens, X. Torrent, J.
    Arno, et al. Kinect v2 sensor-based mobile terrestrial laser scanner for agricultural
    outdoor applications IEEE/ASME Transactions in Mechatronics (2017), 10.1109/TMECH.2017.2663436
    1–1 Google Scholar Rusu et al., 2008 R.B. Rusu, Z.C. Marton, N. Blodow, M. Dolha,
    M. Beetz Towards 3D Point cloud based object maps for household environments Robotics
    and Autonomous Systems, 56 (2008), pp. 927-941, 10.1016/j.robot.2008.08.005 View
    PDFView articleView in ScopusGoogle Scholar Safren et al., 2007 O. Safren, V.
    Alchanatis, V. Ostrovsky, O. Levi Detection of green apples in hyperspectral images
    of apple-tree foliage using machine Vision, 50 (2007), pp. 2303-2313 View in ScopusGoogle
    Scholar Sa et al., 2016 I. Sa, Z. Ge, F. Dayoub, B. Upcroft, T. Perez, C. McCool
    DeepFruits: A fruit detection system using deep neural networks Sensors, 16 (2016),
    p. 1222, 10.3390/s16081222 View in ScopusGoogle Scholar Siegel et al., 2014 K.R.
    Siegel, M.K. Ali, A. Srinivasiah, R.A. Nugent, K.M.V. Narayan Do we produce enough
    fruits and vegetables to meet global health need? PLoS One, 9 (2014), 10.1371/journal.pone.0104059
    Google Scholar Si et al., 2015 Y. Si, G. Liu, J. Feng Location of apples in trees
    using stereoscopic vision Computers and Electronics in Agriculture, 112 (2015),
    pp. 68-74, 10.1016/j.compag.2015.01.010 View PDFView articleView in ScopusGoogle
    Scholar Stajnko et al., 2004 D. Stajnko, M. Lakota, M. Hocevar Estimation of number
    and diameter of apple fruits in an orchard during the growing season by thermal
    imaging Computers and Electronics in Agriculture, 42 (2004), pp. 31-42, 10.1016/S0168-1699(03)00086-3
    View PDFView articleView in ScopusGoogle Scholar Stein et al., 2016 M. Stein,
    S. Bargoti, J. Underwood Image based mango fruit detection, localisation and yield
    estimation using multiple view geometry Sensors, 16 (2016), p. 1915, 10.3390/s16111915
    View in ScopusGoogle Scholar Velodyne, 2016 L. Velodyne VLP-16 in VLP-16 manual:
    User''s manual and programming guide Velodyne LiDAR (2016) Google Scholar Wachs
    et al., 2010 J.P. Wachs, H.I. Stern, T. Burks, V. Alchanatis Low and high-level
    visual feature-based apple detection from multi-modal images Precision Agriculture,
    11 (2010), pp. 717-735, 10.1007/s11119-010-9198-x View in ScopusGoogle Scholar
    Wehr and Lohr, 1999 A. Wehr, U. Lohr Airborne laser scanning - an introduction
    and overview ISPRS Journal of Photogrammetry and Remote Sensing (1999), 10.1016/S0924-2716(99)00011-8
    Google Scholar Xiang et al., 2014 R. Xiang, H. Jiang, Y. Ying Recognition of clustered
    tomatoes based on binocular stereo vision Computers and Electronics in Agriculture,
    106 (2014), pp. 75-90, 10.1016/j.compag.2014.05.006 View PDFView articleView in
    ScopusGoogle Scholar Zhang et al., 2015 B. Zhang, W. Huang, C. Wang, L. Gong,
    C. Zhao, C. Liu, et al. Computer vision recognition of stem and calyx in apples
    using near-infrared linear-array structured light and 3D reconstruction Biosystems
    Engineering, 139 (2015), pp. 25-34, 10.1016/j.biosystemseng.2015.07.011 View PDFView
    articleView in ScopusGoogle Scholar Zhang and Pierce, 2016 Q. Zhang, F.J. Pierce
    Agricultural automation: Fundamentals and practices CRC Press (2016) Google Scholar
    Zhao et al., 2016a Y. Zhao, L. Gong, Y. Huang, C. Liu A review of key techniques
    of vision-based control for harvesting robot Computers and Electronics in Agriculture,
    127 (2016), pp. 311-323, 10.1016/j.compag.2016.06.022 View PDFView articleView
    in ScopusGoogle Scholar Zhao et al., 2016b C. Zhao, W.S. Lee, D. He Immature green
    citrus detection based on colour feature and sum of absolute transformed difference
    (SATD) using colour images in the citrus grove Computers and Electronics in Agriculture,
    124 (2016), pp. 243-253, 10.1016/j.compag.2016.04.009 View PDFView articleView
    in ScopusGoogle Scholar Cited by (76) Fruit sizing using AI: A review of methods
    and challenges 2023, Postharvest Biology and Technology Show abstract Three-dimensional
    quantification of apple phenotypic traits based on deep learning instance segmentation
    2023, Computers and Electronics in Agriculture Show abstract Robot-assisted mobile
    scanning for automated 3D reconstruction and point cloud semantic segmentation
    of building interiors 2023, Automation in Construction Show abstract Topological
    and spatial analysis of within-tree fruiting characteristics for walnut trees
    2023, Scientia Horticulturae Show abstract Effects of laser scanner quality and
    tractor speed to characterise apple tree canopies 2023, Smart Agricultural Technology
    Show abstract Real-time detection of street tree crowns using mobile laser scanning
    based on pointwise classification 2023, Biosystems Engineering Show abstract View
    all citing articles on Scopus View Abstract © 2019 IAgrE. Published by Elsevier
    Ltd. All rights reserved. Recommended articles Detection of red and bicoloured
    apples on tree with an RGB-D camera Biosystems Engineering, Volume 146, 2016,
    pp. 33-44 Tien Thanh Nguyen, …, Wouter Saeys View PDF Analyzing and overcoming
    the effects of GNSS error on LiDAR based orchard parameters estimation Computers
    and Electronics in Agriculture, Volume 170, 2020, Article 105255 Javier Guevara,
    …, Eduard Gregorio View PDF Automatic recognition of juicy peaches on trees based
    on 3D contour features and colour data Biosystems Engineering, Volume 188, 2019,
    pp. 1-13 Gang Wu, …, Jianwei Qin View PDF Show 3 more articles Article Metrics
    Citations Citation Indexes: 69 Captures Readers: 119 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply.'
  inline_citation: (Gené-Mola et al., 2019)
  journal: Biosystems engineering
  key_findings: '1) Apples have higher apparent reflectance than leaves and trunks
    at 905 nm laser wavelength.

    2) A new methodology for fruit detection using an MTLS has been developed.

    3) The proposed methodology can detect and localize apples in 3D space with high
    accuracy and efficiency.'
  limitations: This study did not investigate the effects of different laser wavelengths,
    fruit varieties, or environmental conditions on the performance of the proposed
    methodology. Additionally, the dataset used in this study was relatively small,
    which may limit the generalizability of the results.
  main_objective: To develop and validate a novel technique for detecting and localizing
    Fuji apples in 3D space using a mobile terrestrial laser scanner (MTLS).
  pdf_link: null
  publication_year: 2019
  relevance_evaluation: 'This paper presents a proof of concept of using LiDAR in
    detecting Fuji apples in producing orchard trees. The methodology is founded on
    the fact that apples have higher apparent reflectance than leaves and trunks at
    905 nm laser wavelength. The main contributions of this paper are: (1) analysis
    of apple reflectivity on 3D point clouds from LiDAR sensors; (2) development of
    an apple detection and localization algorithm based on three stages (point cloud
    segmentation; fruit separation, and false positive removal); and (3) experimental
    validation of the proposed technique on a real Fuji apple orchard.


    The principal advantage of this technique over previously published efforts would
    be its capacity to provide direct 3D fruit localization information without being
    affected by illumination conditions. The paper is structured as follows. Section
    2 presents the experimental data set, the point cloud generation procedure, the
    reflectance analysis, and the developed apple detection algorithm. Section 3 shows
    the results of the first experimental tests performed on three Fuji apple trees
    of a commercial orchard. Finally, the conclusions are presented in Section 4.


    This paper is relevant to the integration, interoperability, and standardization
    of automated irrigation systems, as it provides a novel technique for detecting
    and localizing fruit in 3D space using a mobile terrestrial laser scanner (MTLS).
    This technique has the potential to improve the efficiency and accuracy of automated
    irrigation systems by providing more precise information about the location and
    size of fruit, which can be used to optimize irrigation schedules and reduce water
    waste.'
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  study_location: Tarassó farm, a commercial apple orchard located in Catalonia, Spain
  technologies_used: Mobile terrestrial laser scanner (MTLS); Reflectance analysis;
    Apple detection algorithm; 3D fruit localization
  title: Fruit detection in an apple orchard using a mobile terrestrial laser scanner
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1017/s1466252321000177
  analysis: '>'
  apa_citation: null
  authors:
  - Sigfredo Fuentes
  - Claudia Gonzalez Viejo
  - Eden Tongson
  - Frank R. Dunshea
  citation_count: 16
  data_sources: []
  explanation: The aim of this systematic review was to explore how automated, real-time
    irrigation management systems can contribute to the efficient use of water resources
    and enhance agricultural productivity to meet the growing demand for food, while
    also identifying challenges and proposing solutions to address them. The paper
    also sought to understand the current state and future potential of end-to-end
    automated irrigation management systems, including the effectiveness and efficiency
    of integrated end-to-end automated irrigation systems.
  full_citation: '>'
  full_text: '>

    Animal Health Research

    Reviews

    cambridge.org/ahr

    Review

    Cite this article: Fuentes S, Gonzalez Viejo C,

    Tongson E, Dunshea FR (2022). The livestock

    farming digital transformation:

    implementation of new and emerging

    technologies using artificial intelligence.

    Animal Health Research Reviews 23, 59–71.

    https://doi.org/10.1017/S1466252321000177

    Received: 5 June 2021

    Revised: 29 July 2021

    Accepted: 16 November 2021

    First published online: 9 June 2022

    Key words:

    Animal welfare; biometrics; computer vision;

    deep learning; machine learning

    Author for correspondence:

    Sigfredo Fuentes,

    E-mail: sigfredo.fuentes@unimelb.edu.au

    © The Author(s), 2022. Published by

    Cambridge University Press. This is an Open

    Access article, distributed under the terms of

    the Creative Commons Attribution licence

    (http://creativecommons.org/licenses/by/4.0/),

    which permits unrestricted re-use, distribution

    and reproduction, provided the original article

    is properly cited.

    The livestock farming digital transformation:

    implementation of new and emerging

    technologies using artificial intelligence

    Sigfredo Fuentes1

    , Claudia Gonzalez Viejo1

    , Eden Tongson1

    and Frank R. Dunshea1,2

    1Digital Agriculture, Food and Wine Sciences Group, School of Agriculture and
    Food, Faculty of Veterinary and

    Agricultural Sciences, The University of Melbourne, Parkville, VIC 3010, Australia
    and 2Faculty of Biological

    Sciences, The University of Leeds, Leeds LS2 9JT, UK

    Abstract

    Livestock welfare assessment helps monitor animal health status to maintain productivity,

    identify injuries and stress, and avoid deterioration. It has also become an important
    market-

    ing strategy since it increases consumer pressure for a more humane transformation
    in animal

    treatment. Common visual welfare practices by professionals and veterinarians
    may be sub-

    jective and cost-prohibitive, requiring trained personnel. Recent advances in
    remote sensing,

    computer vision, and artificial intelligence (AI) have helped developing new and
    emerging

    technologies for livestock biometrics to extract key physiological parameters
    associated with

    animal welfare. This review discusses the livestock farming digital transformation
    by describ-

    ing (i) biometric techniques for health and welfare assessment, (ii) livestock
    identification for

    traceability and (iii) machine and deep learning application in livestock to address
    complex

    problems. This review also includes a critical assessment of these topics and
    research done

    so far, proposing future steps for the deployment of AI models in commercial farms.
    Most

    studies focused on model development without applications or deployment for the
    industry.

    Furthermore, reported biometric methods, accuracy, and machine learning approaches
    pre-

    sented some inconsistencies that hinder validation. Therefore, it is required
    to develop

    more efficient, non-contact and reliable methods based on AI to assess livestock
    health, wel-

    fare, and productivity.

    Introduction

    Climate change predictions that are affecting most agricultural regions and livestock
    transpor-

    tation routes are related to increasing ambient temperatures, rainfall variability,
    water availabil-

    ity, and increased climatic anomalies, such as heatwaves, frosts, bushfires, and
    floods, affecting

    livestock health, welfare, and productivity. These events have triggered and prioritized
    a critical

    digital transformation within livestock research and industries to be more predictive
    than

    reactive, implementing new and emerging technologies on animal monitoring for
    decision-

    making purposes. Several advances in smart livestock monitoring aim for the objective
    meas-

    urement of animal stress using digital technology to assess the effect of livestock
    welfare and

    productivity using biometrics and artificial intelligence (AI).

    The most accurate methods to measure livestock health and welfare are invasive
    tests, such

    as analysis of tissue and blood samples, and contact sensors positioned on the
    skin of animals

    or internally either by minor surgery, intravaginal, or rectally implanted (Jorquera-Chavez

    et al., 2019a; Zhang et al., 2019b; Chung et al., 2020). However, these are apparently
    imprac-

    tical approaches to monitor many animals for continuous assessments on farms.
    These

    approaches require a high level of know-how by personnel for sampling, sensor
    placement,

    data acquisition processing, analysis and interpretation. Furthermore, they impose
    medium

    to high levels of stress on animals, introducing biases in the analysis and interpretation
    of

    data; for this reason, researchers are focusing on developing novel contactless
    methods to

    improve animal welfare (Neethirajan and Kemp, 2021). There are also visual assessments

    that can be made by experts and trained personnel to assess levels of animal stress
    and welfare.

    However, these can be subjective and require human supervision and assessment
    with similar

    disadvantages of the aforementioned physiological assessments and sensor technologies
    (Burn

    et al., 2009).

    Recent digital advances in sensor technology, sensor networks with The Internet
    of Things

    (IoT) connectivity, remote sensing, computer vision and AI for agricultural and
    human-based

    applications have allowed the potential automation and integration of different
    animal science

    and animal welfare assessment approaches (Morota et al., 2018; Singh et al., 2020).
    There has

    been increasing research on implementing these new and emerging digital technologies
    and

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    adaption to livestock monitoring, such as minimal contact sensor

    technology, digital collars and remote sensing (Karthick et al.,

    2020). Furthermore, novel analysis and modeling systems have

    included machine and deep learning modeling techniques to

    obtain practical and responsible AI applications. The main

    applications for these technologies have been focused on asses-

    sing physiological changes from animals to be related to differ-

    ent types of stress or the early prediction of diseases or parasite

    infestation (Neethirajan et al., 2017; Neethirajan and Kemp,

    2021). One of the most promising approaches is implementing

    AI incorporating remote sensing and machine learning (ML)

    modeling strategies to achieve a fully automated system for non-

    invasive

    data

    acquisition,

    analysis,

    and

    interpretation.

    Specifically, this approach is based on inputs from visible, ther-

    mal, multispectral, hyperspectral cameras and light detection

    and ranging (LiDAR) to predict targets, such as animal health,

    stress, and welfare parameters. This approach is presented in

    detail in the following sections of this review.

    However, much of the research has been based on academic

    work using the limited amount of data accumulated in recent

    years to test mainly different AI modeling techniques rather

    than deployment and practical application to the industry.

    Some research groups have focused their efforts on pilots for AI

    system deployments to assess the effects of heat stress on animals

    and their respective production, welfare on farming and animal

    transport, animal identification for traceability, and monitoring

    greenhouse emissions to quantify and reduce the impact of live-

    stock farming on climate change.

    This review is based on the current research on these new and

    emerging digital technologies applied to livestock farming to

    assess

    health,

    welfare,

    and

    productivity

    (Table

    1).

    Some

    AI-based research applied for potential livestock applications

    have tried to solve too many complex problems rather than con-

    centrating on simple and practical applications, and with few

    deployment examples. However, the latter is a generalized prob-

    lem of AI applications within all industries, in which only 20%

    of AI pilots, have been applied to real-world scenarios and have

    made it to commercial production. The latter figures have

    increased slightly due to COVID-19 for 2021, with increases up

    to 20% for ML and 25% for AI deployment solutions, according

    to the Hanover Enterprise Financial Decision Making 2020 report

    (Wilcox, 2020). By establishing a top-down approach (identifying

    goldilocks problems), specific and critical solutions could be easily

    studied to develop effectively new and emerging technologies,

    including AI. In Australia and worldwide, several issues have

    been identified for livestock transport in terms of the effect of cli-

    mate change, such as effects of increased temperatures, droughts,

    and heatwaves on livestock welfare; especially during long sea

    trips through very hot transport environments, such as those in

    the Persian Gulf, with temperatures reaching over 50°C) and

    the identification and traceability of animals. Many livestock pro-

    ducing countries have identified AI and a digital transformation

    as an effective and practical solution for many monitoring and

    decision-making problems from the industry.

    Biometric techniques for health and welfare assessment

    The most common methods for animal welfare and health assess-

    ment are either visual and subjective, specifically for animal

    behavior, or invasive. They may involve collecting blood or

    urine samples to be analyzed using expensive and time-

    consuming

    laboratory

    techniques

    such

    as

    enzyme-linked

    immunosorbent assay (ELISA) and polymerase chain reaction

    (PCR)

    (Neethirajan

    et

    al.,

    2017;

    Du

    and

    Zhou,

    2018;

    Neethirajan, 2020). Other measurements that are usually related

    to the health and welfare of animals are based on their physio-

    logical responses such as body temperature, heart rate (HR),

    and respiration rate (RR) (Fuchs et al., 2019; Halachmi et al.,

    2019). To measure body temperature, the most reliable methods

    are intravaginal or measured in the ear, with the most common

    devices

    based

    on

    mercury

    or

    digital

    thermometers

    (Jorquera-Chavez et al., 2019a; Zhang et al., 2019b). Body tem-

    perature is vital for early detection and progression of heat stress,

    feed efficiency, metabolism, and disease symptoms detection such

    as inflammation, pain, infections, and reproduction stage, among

    others (McManus et al., 2016; Zhang et al., 2019b).

    Traditional techniques to assess HR may involve manual mea-

    surements

    using

    stethoscopes

    (DiGiacomo

    et

    al.,

    2016;

    Jorquera-Chavez et al., 2019b; Fuentes et al., 2020a), or automatic

    techniques based on electrocardiogram (ECG) devices, such as

    commercial monitor belts with chest electrodes, such as the

    Polar

    Sport

    Tester

    (Polar

    Electro

    Oy,

    Kempele,

    Finland)

    (Orihuela et al., 2016; Stojkov et al., 2016), and photoplethysmo-

    graphy (PPG) sensors attached to the ear (Nie et al., 2020). The

    HR parameter and variability are usually used as an indicator of

    environmental stress, gestation period, metabolic rate, and diag-

    nosis of cardiovascular diseases (Fuchs et al., 2019; Halachmi

    et al., 2019). On the other hand, RR is typically measured by

    manually counting the flank movements of animals resulting

    from breathing in 60 s using a chronometer (DiGiacomo et al.,

    2016; Fuentes et al., 2020a) or counting the breaths in 60 s

    using a stethoscope, or by attaching sensors in the nose, or thorax,

    which can detect breathing patterns (Jorquera-Chavez et al.,

    2019a). Respiration rate can be used to indicate heat stress and

    respiratory diseases (Mandal et al., 2017; Slimen et al., 2019;

    Fuentes et al., 2020a).

    The main disadvantage of traditional methods based on con-

    tact or invasive sensors to assess physiological responses is the

    potential stress they can cause to the animal by the methodology

    used, which can introduce bias. The stress may be caused by the

    anxiety provoked by the restraint and manipulation/contact with

    their bodies for the actual measurement or to attach different sen-

    sors. Furthermore, these methods tend to be costly and time-

    consuming, making it very impractical assessing a large group

    of animals. In manual measurements, they may also have

    human error and, therefore, are subjective and not that reliable.

    Some specific applications for different livestock will be discussed,

    separating cattle, sheep and pigs (Table 1).

    Cattle

    To assess the body temperature of cattle continuously, Chung

    et al. (2020) proposed an invasive method for dairy cows by

    implanting a radio frequency identification (RFID) biosensor

    (RFID Life Chip; Destron Fearing™, Fort Worth, TX, USA) on

    the lower part of ears of three cows that were monitored for 1

    week; however, this method showed medium-strength correla-

    tions when compared directly to the intravaginal temperature

    probe for two of the cows (R2 = 0.73) and low correlation in the

    third cow (R2 = 0.34). The authors then developed a ML model

    based on the long short-term memory method to increase predic-

    tion accuracy. However, the study only reported the root mean

    squared error (RMSE = 0.081) of the model but left out the accur-

    acy based on the correlation coefficient as it should be done for

    60

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Table 1. Summary of biometric methods to assess health and welfare for cattle,
    sheep, and pigs

    Animals

    Measurement

    Technique

    Groundtruth (traditional

    methods)

    Number of

    animals

    Accuracy of method

    Proposed application

    Reference

    Cattle

    Dairy cows

    Body temperature

    Implanted RFID

    biosensor and

    Machine learning

    Vaginal temperature

    (probe)

    3

    RMSE = 0.08

    First steps for precision

    agriculture methods

    (Chung et al., 2020)

    Simulated

    cows

    Temperature and

    movements

    Wearable digital

    sensors

    Wireless data

    acquisition

    None

    1 toy

    simulating a

    cow and hot

    water

    Not reported

    Health monitoring and

    disease detection

    (Tahsin, 2016)

    Cattle

    (Holstein and

    Jersey)

    Body temperature

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Rectal temperature

    (probe)

    Not specified

    Mean difference

    between methods

    0.04 ± 0.10°C

    Alternative to traditional

    temperature methods

    (Wang et al., 2021a)

    German

    Holstein

    cows

    HR

    HRV

    Wearable sensors

    None

    40

    Not reported

    Tested impact of different

    stimulation methods

    (Zipp et al., 2018)

    Dairy calves

    HR

    Wearable sensors

    None

    69

    Not reported

    Behavioral and stress

    response

    (Buchli et al., 2017)

    Cows

    HR

    RR

    Chewing

    Contactless

    biometrics

    Computer vision

    RGB images and

    laser

    HR: wearable sensor

    RR and Cheiwng: manual

    count

    6

    HR: R = 0.98

    RR: R = 0.97

    Chewing: R = 0.99

    Biomedical monitoring for

    optimized cattle treatment

    (Beiderman et al.,

    2014)

    Dairy cows

    Holstein

    Friesian

    Skin temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images and RGB

    videos

    Skin temperature: vaginal

    probe

    HR: wearable sensors

    RR: manual count

    10

    Skin Temperature:

    R = 0.74

    HR: R = 0.20–0.83

    RR: R = 0.87

    Monitoring of physiological

    responses

    (Jorquera-Chavez

    et al., 2019b)

    Dairy cows

    RR

    Computer vision

    Infrared thermal

    and RGB videos

    Manual count

    15

    Mean difference

    Manual vs RGB

    video: −0.01 ± 0.87

    Manual vs infrared

    videos: 0.83 ±0.57

    Monitoring of health and

    welfare

    (Stewart et al., 2017)

    Calves

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Manual count from RGB

    videos

    5

    R2 = 0.93

    Monitoring of health and

    welfare

    (Lowe et al., 2019)

    Japanese

    Black Calves

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Deep learning

    Manual count

    5

    R2 = 0.91

    Monitoring health

    (Kim and Hidaka,

    2021)

    (Continued)

    Animal Health Research Reviews

    61

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Table 1. (Continued.)

    Animals

    Measurement

    Technique

    Groundtruth (traditional

    methods)

    Number of

    animals

    Accuracy of method

    Proposed application

    Reference

    Qinchuan

    cattle

    Body measurements

    (dimensions)

    Contactless

    biometrics

    Computer vision

    RGB images

    Manual measurements

    3

    2 mm

    Contactless body

    measurements of large

    livestock

    (Huang et al., 2018)

    Dairy cows

    Drinking behavior

    Integrated sensor

    module

    Computer vision

    Deep learning

    None

    25

    Not reported

    Automatic and quantitative

    assessment of drinking

    behavior as a measure of

    heat stress

    (Tsai et al., 2020)

    Sheep

    Dairy sheep

    Behavior activities

    Wireless system

    Wearable sensors

    RGB videos

    Manual assessment

    3

    93%

    Behavior assessment

    (Giovanetti et al.,

    2017)

    Ewes

    Behavior activities

    Wearable sensors

    RGB videos

    Machine learning

    Manual assessment

    6

    85%

    Assessment of sheep

    activity previous to

    methane measurements

    Assessment of temporal

    grazing patterns

    (Alvarenga et al.,

    2016)

    Ewes

    Body temperature

    Wearable sensor

    None

    15

    Not reported

    Measurement of

    temperature changes in

    lambing period

    (Abecia et al., 2020)

    Ewes

    Surface temperature of

    different areas (anus,

    vulva, muzzle, eyes)

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Rectal and vaginal

    temperature

    20

    Not reported

    Assessment of temperature

    during estrous cycle

    Reproductive management

    (de Freitas et al.,

    2018)

    Ewes

    Eye temperature

    HR

    HRV

    Computer vision

    Infrared thermal

    images

    Wearable sensors

    None

    20

    Not reported

    Assessment of autonomic

    nervous system responses

    (Sutherland et al.,

    2020)

    Meat sheep

    Skin temperature

    HR

    Wireless wearable

    monitoring

    system

    Traditional veterinary

    monitors

    60

    Non-significant

    differences (no

    p-value reported)

    Assessment of

    physiological responses

    with minimal stress

    (Cui et al., 2019)

    Mutton

    sheep

    HR

    Oxygen saturation

    Body temperature

    Wearable sensors

    None

    Not reported

    Not reported

    Diagnose survival status

    during transportation

    (Zhang et al., 2020)

    Merino lambs

    Skin temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Machine learning

    Skin and rectal

    temperature (digital

    thermometer)

    Stethoscope

    Manual count

    12 sheep /3

    times a day

    /four weeks

    Skin temperature:

    R2 = 0.99

    HR and RR: R = 0.94

    Assessment of

    physiological responses

    and heat stress during

    transportation

    (Fuentes et al.,

    2020a)

    Sheep

    Body measurements

    (dimensions; weight)

    Contactless

    biometrics

    Computer vision

    Machine learning

    Manual measurements

    27

    Weight: R = 0.99

    Dimensions: R = 0.79

    Increase efficiency in herds

    management

    (Zhang et al., 2018)

    62

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Pigs

    Pigs

    Behavior

    Contactless

    biometrics

    Computer vision

    None

    10 pigs /2

    replications

    Not reported

    Assessment of heat stress

    (Byrd et al., 2020)

    Pigs

    Lying behavior

    Contactless

    biometrics

    Computer vision

    Not reported

    88

    96%

    Welfare assessment

    (Nasirahmadi et al.,

    2017)

    Pigs

    Body measurements

    (dimensions)

    Weight estimation

    Contactless

    biometrics

    Computer vision

    Manual measurements

    78

    R2 > 0.95 to predict

    weight

    Estimate pigs’ weight

    during weaning period

    (Pezzuolo et al.,

    2018)

    Piglets

    Sus Scrofa

    Skin temperature

    Cold/heat stress

    Thirst stress

    Hunger stress

    Pain stress

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Machine learning

    Stress conditions based

    on treatments

    72

    Cold/heat stress:

    100%

    Thirst stress: 91%

    Hunger stress: 86%

    Pain stress: 50%

    Assessment of stress

    during handling and

    transportation

    (da Fonseca et al.,

    2020)

    Sows

    Rectal temperature

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images

    Machine learning

    Rectal temperature

    (mercury thermometer)

    99

    R2 = 0.80

    Welfare assessment

    (Feng et al., 2019)

    Pigs

    HR

    Contactless

    biometrics

    Computer vision

    Electrocardiogram

    2

    78% (Green color

    channel)

    Real-time monitoring of

    health and welfare

    (Wang et al., 2021b)

    Pigs

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared camera

    Electrocardiogram

    Ventilator data

    17

    HR: R2 = 0.96

    RR: R2 = 0.97

    Long term monitoring of

    research animals

    (Barbosa Pereira

    et al., 2019)

    Pigs

    Skin temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    images and RGB

    videos

    None

    46

    Not reported

    Early detection of disease

    before symptoms appear

    (Jorquera-Chavez

    et al., 2020)

    Pigs

    Eye temperature

    HR

    RR

    Contactless

    biometrics

    Computer vision

    Infrared thermal

    and RGB videos

    Stethoscope

    Manual count from

    videos

    28

    Eye temperature: not

    reported

    HR and RR:

    R = 0.61–0.66

    Physiological responses

    due to respiratory diseases

    (Jongman et al.,

    2020)

    * Abbreviations: RFID, radio frequency identification; RMSE, root mean squared
    error; HR, heart rate; HRV, heart rate variability; RR, respiration rate; RGB,
    red, green, blue.

    Animal Health Research Reviews

    63

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    regression ML models. On the other hand, Tahsin (2016) devel-

    oped a remote sensor system named Cattle Health Monitor and

    Disease Detector, connected using a wireless network. This system

    integrated a DS1620 digital thermometer/thermostat (Maxim

    Integrated™, San Jose, CA, USA) and a Memsic 2125 thermal

    accelerometer (Parallax, Inc., Rocklin, CA, USA) to assess the

    activity of animals by measuring the lateral and horizontal move-

    ments of the cow. The integrated sensors node was placed on the

    neck using a collar, with the option to be powered using a solar

    panel. Furthermore, Wang et al. (2021a) developed a non-

    invasive/contactless sensor system to assess the body temperature

    of cattle using an infrared thermal camera (AD-HF048; ADE

    Technology Inc., Taipei, Taiwan), an anemometer (410i; Testo SE

    & Co., Kilsyth, VIC, Australia), and a humiture sensor (RC-4HA;

    Elitech Technology, Inc., Milpitas, CA. USA). These sensors were

    placed in the feedlot at 1 m from the cows and 0.9 m above the

    ground to record the head of each cow, while these were restrained

    using a headlock. The authors used a rectal thermometer as

    groundtruth to validate the method and reported a difference of

    0.04 ± 0.10°C between the grountruth and the method proposed.

    The anemometer and humiture sensor were used to remove the

    frames affected by external weather factors to extract outliers.

    In the case of HR, Zipp et al. (2018) used Polar S810i and

    RS800CX sensors attached to the withers and close to the heart

    to measure HR and HR variability (HRV) while locked after milk-

    ing to assess the impact of different stimulation methods (acous-

    tic, manual and olfactory). However, the authors reported

    technical problems to acquire HR and HRV, which led to missing

    values and altered the analysis. This is another drawback of using

    contact sensors as they can become unreliable due to different rea-

    sons, such as natural animal movements causing sensors to lose

    contact with the animal skin and connectivity problems. Buchli

    et al. (2017) used a Polar S810i belt attached to the torso of calves

    to measure HR while the animals were in their pen. However,

    similar to the previous study, these authors had errors in the

    data acquired and excluded data from eight calves. To avoid

    these problems, remote sensing methods have been explored,

    such as those developed by Beiderman et al. (2014), based on

    an automatic system to assess HR, RR and chewing activity

    using a tripod holding a PixeLink B741 camera (PixeLink,

    Rochester, NY, USA) and a Photop D2100 laser connected to a

    computer. The laser pointed at the neck and stomach of the

    cow. The acquired signal was analyzed using the ‘findpeaks’

    Matlab® (Mathworks, Inc., Natick, MA, USA) function to assess

    HR from the neck area and RR and chewing from the stomach

    section. The authors reported a correlation coefficient R = 0.98

    for HR, R = 0.97 for RR and R = 0.99 for chewing data compared

    with manual measurements for RR and chewing and Polar sensor

    for HR. These latter methods may solve the contact problems and

    unreliability of data quality; however, they seem to still be manual

    methods requiring operators. The authors did not propose an

    automation system for measurements.

    Jorquera et al. (2019b) also presented contactless methods to

    assess skin temperature, HR and RR of dairy cows using remote

    sensing cameras and computer vision analysis. These authors

    used a FLIR AX8 camera (FLIR Systems, Wilsonville, OR, USA)

    integrated into a Raspberry Pi V2.1 camera module to record

    infrared thermal images (IRTI) and RGB videos of the face of

    the cows while restrained in the squeeze chute. The IRTIs were

    analyzed automatically using the FLIR Atlas software develop-

    ment kit (SDK) for Matlab® and cropped the videos in the eye

    and ear sections. The RGB videos were used to assess HR using

    the PPG method based on the luminosity changes in the green

    channel of the eye, forehead and full face of the cows; these signals

    were then further analyzed using a customized Matlab® code pre-

    viously developed for people (Gonzalez Viejo et al., 2018) and

    adapted for animals. On the other hand, the authors used a

    FLIR ONE camera to record non-radiometric videos of the

    cows. These were analyzed using Matlab® based on the change

    in pixel intensity in the nose section to measure the inhalations

    and exhalations from which RR was calculated.

    Regarding the RR techniques, besides the manual counts usu-

    ally conducted based on visual assessment of the flank movement

    of animals, researchers have also developed computer vision tech-

    niques, which aid in the reduction of human error and bias.

    Stewart et al. (2017) assessed 15 dairy cows using three compara-

    tive methods to determine RR with (i) manual counts of the flank

    movements by recording the time it took the cow to reach 10

    breaths, (ii) manual counts of flank movements similar to method

    (i) but from an RGB video recorded using a high-dynamic-range

    (HDR) CX220E camera (Sony Corporation, Tokyo, Japan), and

    (iii) manual count of the air movement (temperature variations)

    from the nostrils. The latter was performed from infrared thermal

    videos recorded using a ThermaCam S60 camera (FLIR Systems,

    Wilsonville, OR, USA). The three methods showed similar

    responses with the highest average difference of 0.83 ± 0.57

    between methods (i) and (iii). Furthermore, Lowe et al. (2019)

    presented a similar approach but tested only in five calves. In

    the latter study, the two methods were (i) manual count of

    flank

    movements

    from

    an

    RGB

    video

    recorded

    using

    a

    Panasonic HCV270 camera (Panasonic, Osaka, Japan), which

    was made by recording the time taken for the calf to reach five

    breath cycles, and (ii) manual count of the thermal fluctuations

    (color changes) in the nostrils from infrared thermal images

    recorded using a FLIR T650SC camera. The Adobe Premiere

    Pro CC (Adobe, San Jose, CA, USA) was used for the manual

    counts for both methods. A high determination coefficient (R2

    = 0.93) was reported comparing both methods. More recently,

    Kim and Hidaka (2021) used a FLIR ONE PRO infrared thermal

    camera to record IRTIs and RGB videos from the face of calves.

    The authors first measured the color changes from the nostril

    region manually as the time it took for the calf to complete five

    breaths. A mask region-based convolutional neural network

    (Mask R-CNN) and transfer learning were used to automatically

    develop a model using the RGB video frames to automatically

    detect and mask the calves’ noses. Once the nose was detected

    and masked in the RGB videos, co-registered IRTIs were used

    to automatically extract the mean temperature of the region of

    interest. The authors reported an R2 = 0.91 when comparing the

    manual and automatic methods.

    Besides those used to assess physiological responses, other bio-

    metrics have been explored to be applied in beef and dairy cattle.

    These methods consist of the use of biosensors and/or image/

    video analysis (remote sensing). For example, Huang et al.

    (2018) developed a computer vision method to assess body mea-

    surements (dimensions) of cattle using an O3D303 3D LiDAR

    camera to record the individual animal side view and post-

    processing using filter fusion, clustering segmentation and match-

    ing techniques. Tsai et al. (2020) developed an integrated sensor

    module composed of a Raspberry Pi 3B processing unit

    (Raspberry Pi Foundation, Cambridge, England), a Raspberry Pi

    V2 camera module and a BME280 temperature and relative

    humidity sensor for environmental measurement. This integrated

    module was placed on the top of the drinking troughs in a dairy

    64

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    farm to record the drinking behavior of the cows. The authors

    then applied convolutional neural networks (CNN) based on

    Tiny YOLOv3 real-time object detection deep learning network

    for the head detection of cows to predict the drinking length

    and frequency which were found to be correlated with the

    temperature-humidity index (THI; R2 = 0.84 and R2 = 0.96,

    respectively).

    Sheep

    Researchers have been working on different techniques to assess

    sheep behavioral and physiological responses using contact and

    contactless sensors. Giovanetti et al. (2017) designed a wireless

    system consisting of a halter with a three-axis accelerometer

    ADXL335 (Analog Devices, Wilmington, MA, USA) attached;

    this was positioned in the lower jaw of dairy sheep to measure

    the acceleration of their movements on x-, y- and z-axes.

    Furthermore, the authors used a Sanyo VPC-TH1 camera

    (Sanyo, Osaka, Japan) to record videos of the sheep during feed-

    ing and manually assessed whether the animals were grazing,

    ruminating or resting as well as the bites per minute. Similarly,

    Alvarenga et al. (2016) designed a halter attached below the jaw

    of sheep; this halter had an integrated data logger Aerobtec

    Motion Logger (AML prototype V1.0, AerobTec, Bratislava,

    Slovakia), which is able to measure acceleration in x-, y- and

    z-axes transformed into North, East and Down reference system.

    Additionally, they recorded videos of the sheep using a JVC

    Everio GZR10 camera (JVC Kenwood, Selangor, Malaysia) to

    manually assess grazing, lying, running, standing and walking

    activities. These data were used to develop ML models to auto-

    matically predict activities, obtaining an accuracy of 85%.

    Abecia et al. (2020) presented a method to measure the body

    temperature of ewes using a button-size data logger DS1921 K

    (Thermochron™ iButton®, Maxim Integrated, San Jose, CA,

    USA) taped under the tail of the animals. This sensor was able

    to record temperature data every 5 min. Using remote sensing,

    de Freitas et al. (2018) used a FLIR i50 infrared thermal camera

    to record images from different areas of the sheep: anus, vulva,

    muzzle, and eyes. The authors used the FLIR Quickreport soft-

    ware to manually select the different sections in each sheep and

    obtain each area’s mean temperature. They concluded that the

    vulva and muzzle were the best areas to assess temperature during

    the estrous cycle in ewes. Sutherland et al. (2020) also used an

    infrared thermal camera (FLIR Thermacan S60) to record videos

    of the left eye of ewes. These videos were analyzed to assess eye

    temperature using the Thermacam Researcher software ver. 2.7

    (FLIR Systems, Wilsonville, OR, USA). Additionally, the authors

    used a Polar RS800CX sensor and placed it around the ewes

    thorax to assess HR and HRV.

    In terms of potential applications of sensor technology, Cui

    et al. (2019) developed a wearable stress monitoring system

    (WSMS) consisting of master and slave units. The master unit

    was comprised of environmental sensors such as temperature,

    relative humidity and global positioning system (GPS) attached

    to an elastic band and placed around the rib cage of sheep,

    while the slave unit was composed of physiological sensors such

    as an open-source HR sensor (Pulse Sensor, World Famous

    Electronics LLC, New York, NY, USA), and a skin temperature

    infrared sensor (MLX90615; Melexis, Ypres, Belgium). This sys-

    tem was tested on meat sheep during transportation and proposed

    as a potential method to assess physiological responses with min-

    imal stress. Zhang et al. (2020) designed a wearable collar that

    included two sensors to measure (i) HR and oxygen saturation in

    the blood (MAX30102; Max Integrated, San Jose, CA, USA), and

    (ii) body temperature (MLX90614; Melexis, Ypres, Belgium).

    These sensors were connected to the Arduino Mobile App

    (Arduino LLC, Boston, MA, USA) through Bluetooth® for real-time

    monitoring and used an SD card for data storage. The authors also

    proposed this system to assess physiological responses during the

    transportation of sheep. However, these studies can only monitor

    sentinel animals, making it laborious, difficult and impractical for

    the assessment of all animals transported.

    To solve the later problem, Fuentes et al. (2020a) presented a

    contactless/non-invasive method to assess temperature, HR and

    RR of sheep using computer vision analysis and ML. The authors

    used a FLIR DUO PRO camera to simultaneously record RGB

    and infrared thermal videos of sheep. The infrared thermal videos

    were analyzed using customized Matlab® R2020a algorithms to

    automatically recognize the sheep’s head and obtain the max-

    imum temperature. Results showed a very high correlation (R2

    = 0.99) between the temperatures obtained with the thermal cam-

    era and the rectal and skin temperatures measured using a digital

    thermometer. On the other hand, RGB videos were analyzed

    using customized Matlab® R2020a codes to assess HR and RR

    based on the PPG principle using the G color channel from

    RGB scale for HR and ‘a’ from Lab scale for RR. An artificial

    neural network model was developed using the Matlab® code out-

    puts to predict the real HR and RR (measured manually), obtain-

    ing high accuracy of R = 0.94. This study also proposed a potential

    deployment system to be used for animals in transport.

    For other biometric assessments, Zhang et al. (2018) developed

    a computer vision method to measure the dimensions of sheep

    using three MV-EM120C Gigabit Ethernet charge-coupled device

    (CCD) cameras (Lano Photonics, JiLin Province, China) located

    at different positions (top, left and right side) of the weighing

    scale for sheep. The recorded images were analyzed in Matlab®

    R2013 using the superpixel segmentation algorithm. The authors

    also obtained the dimension parameters manually and found cor-

    relations of R = 0.99 for weight and R = 0.79 for dimensions

    (width, length, height and circumference) using support vector

    machine.

    Pigs

    Pigs are also commonly studied to develop biometric techniques

    to assess behavioral and physiological responses. For example,

    Byrd et al. (2020) used a KPC-N502NUB camera (KT&C,

    Fairfield, NJ, USA) mounted on top of the pigs’ pens to assess

    pig behavior. The authors used the GeoVision VMS software

    (GeoVision Inc, Taipei, Taiwan) and assessed whether the pigs

    were active (standing or sitting) or inactive (lying sternal or lat-

    eral). Nasirahmadi et al. (2017) assessed the lying behavior of

    pigs using closed-circuit television (CCTV) with a Sony RF2938

    camera above the pen. Matlab® software was used to analyze the

    videos using computer vision algorithms to detect the position

    of each pig and analyze the distance between each animal consid-

    ering their axes, orientation and centroid. On the other hand,

    Pezzuolo et al. (2018) obtained body measurements and weight

    of

    pigs

    using

    a

    Kinect

    V1

    depth

    camera

    (Microsoft

    Corporation, Redmond, WA, USA) positioned on the top and

    side of the pen. Videos were analyzed using the Scanning Probe

    Image Processor (SPIP™) software (Image Metrology, Lyngby,

    Denmark) to obtain length, front and back height, and heart

    girth. Furthermore, authors developed linear and non-linear

    Animal Health Research Reviews

    65

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    models to predict weight, obtaining an accuracy R2 > 0.95 in all

    modeling methods tested. The drawback that the authors men-

    tioned from this technique is that the system can only record

    data from a single camera at a time because there is interference

    when using simultaneous data acquisition of the two cameras.

    Regarding techniques to measure body/skin temperature from

    pigs, da Fonseca et al. (2020) used a Testo 876-1 handheld infrared

    thermal camera (Testo Instruments, Lenzkirch, Germany) to record

    images of piglets’ full bodies. The IRSoft v3.1 software (Testo

    Instruments, Lenzkirch, Germany) was used to obtain the max-

    imum and minimum skin temperature values. Rocha et al.

    (2019) presented a method to measure the body temperature of

    pigs using two IR-TCM284 infrared thermal cameras (Jenoptik,

    Jena, Germany). One camera was placed in the pen perpendicular

    to the pig’s body, while the second one was positioned 2.6 m above

    the pigs in the loading alley for transportation. The areas of interest

    evaluated were neck, rump, orbital region, and the area behind the

    ears; these were manually selected using the IRT Cronista

    Professional Software v3.6 (Grayess, Bradenton, FL, USA) and

    extracting the minimum, maximum and mean temperatures.

    Authors found that the temperatures from the orbital region and

    behind the ears were the most useful to assess different types of

    stress (cold, heat, thirst, hunger, and pain) during handling and

    transportation. On the other hand, Feng et al. (2019) developed a

    computer vision and ML method to predict the rectal temperature

    of sows using a T530 FLIR infrared thermal camera to capture

    images. The FLIR Tools software (FLIR Systems, Wilsonville, OR,

    USA) was used to obtain the maximum and mean skin tempera-

    ture in different areas such as ears, forehead, shoulder, back central

    and back end, and vulva. With these data, the authors developed a

    partial least squared regression (PLS) model to predict rectal tem-

    perature, obtaining an accuracy of R2 = 0.80.

    Wang et al. (2021b) developed a contactless method to assess

    HR of pigs using two different setups (i) a webcam C920 HD PRO

    (Logitech, Tainan, Taiwan) located on top of the operation table

    with an anesthetized pig and (ii) a Sony HDRSR5 Handycam

    located on a tripod above resting individual housing with a resting

    pig. Matlab® was used to analyze the videos by selecting and crop-

    ping the (i) neck for the first setup and (ii) abdomen, neck and

    front leg for the dual setup. The authors used the PPG principle

    with the three color channels of the RGB scale and found the G

    channel provided the most accurate results compared to measure-

    ments using an ECG. Barbosa Pereira et al. (2019) also developed

    a method using anesthetized pigs; they used a long wave infrared

    VarioCam

    HD

    head

    820

    S/30

    (InfraTecGmbH,

    Dresden,

    Germany) to assess HR and RR. The videos were analyzed

    using Matlab® R2018a, and it included the segmentation using a

    multilevel Otsu’s algorithm, region of interest (chest) selection,

    features identification and tracking using the Kanade–Lucas–

    Tomasi (KLT) algorithm, temporal filtering to measure trajectory

    and principal components analysis (PCA) decomposition and

    selection. This allowed them to obtain an estimated HR and RR

    at the selected frequency rates. The authors reported determin-

    ation coefficients of R2 = 0.96 for HR compared to the ECG

    method and R2 = 0.97 for RR compared to ventilator data.

    Jorquera-Chavez et al. (2020) developed a contactless method to

    assess temperature, HR and RR of pigs using an integrated camera

    composed of a FLIR AX8 infrared thermal camera and a

    Raspberry Pi Camera V2.1 to record IRTIs and RGB videos,

    and a FLIR ONE infrared thermal camera to record non-

    radiometric videos. The authors used the same method as that

    reported for cows (Jorquera-Chavez et al., 2019b) using Matlab®

    R2018b selecting the eyes and ears as regions of interest for tem-

    perature, eye section for HR and nose for RR. The same method

    was used in the study developed by Jongman et al. (2020), but

    they used a FLIR DUO PRO R dual camera (infrared thermal

    and RGB) and reported a correlation coefficient within the R =

    0.61–0.66

    range

    for

    HR

    and

    RR

    compared

    to

    manual

    measurements.

    Biometric techniques for recognition and identification

    Correct and accurate identification of livestock is essential for farm-

    ers and producers. It also allows relating each animal to different

    productivity aspects such as health-related factors, behavior, pro-

    duction yield and quality and breeding. Furthermore, animal iden-

    tification is essential for traceability, especially during transport and

    after selling, to avoid fraud and animal ledger or identification for-

    ging. However, traditional methods involve ear tags, tattoos, micro-

    chips and radio frequency identification (RFID) collars, which

    involve high costs, and some may be unreliable and easily hacked

    or interchanged. Furthermore, they require human labor for their

    maintenance, making them time-consuming, prone to human

    error and may lead to swapping tags (Awad, 2016; Kumar et al.,

    2016, 2017a; Zin et al., 2018). Therefore, some studies in recent

    years have focused on the development of contactless biometric

    techniques to automate the recognition and identification of differ-

    ent animals such as bears, using deep learning (Clapham et al.,

    2020), and cows based on different features such as the face (Cai

    and Li, 2013; Kumar et al., 2016), muzzle (Kumar et al., 2017a),

    body patterns (Zin et al., 2018), iris recognition (Lu et al., 2014)

    or retinal patterns (Awad, 2016).

    Cattle

    Most of these biometric techniques for recognition and identifica-

    tion have been developed for cattle. Authors have presented meth-

    ods based on one of the three main techniques (i) muzzle pattern

    identification, (ii) face recognition and (iii) body recognition and

    identification. The first technique has been applied for cattle rec-

    ognition using images of the muzzle and analyzed for features as

    it has a particular pattern that is different for each animal, similar

    to the human fingerprints. Once these features and patterns are

    recognized, a deep learning model is developed to identify each

    cow (Noviyanto and Arymurthy, 2012; Gaber et al., 2016;

    Kumar et al., 2017a, 2017b, 2018; Bello et al., 2020). Face recog-

    nition methods using different techniques such as local binary

    pattern algorithms (Cai and Li, 2013) and CNN have been pro-

    posed for specific cattle breeds with different colors and patterns,

    such as Simmental (Wang et al., 2020), Holstein, Guernseys and

    Ayrshires, among others (Kumar et al., 2016; Bergamini et al.,

    2018); however, none has been presented in single-coloured cattle

    breeds such as Angus. On the other hand, body recognition meth-

    ods have been developed to identify cows within a herd using

    computer vision and deep learning techniques. Within the pro-

    posed methods are cattle recognition from the side (Bhole et al.,

    2019), from behind (Qiao et al., 2019), different angles (de

    Lima Weber et al., 2020) or from the top (Andrew et al., 2019).

    The latter was proposed to identify and recognize Holstein and

    Friesian

    cattle

    using

    an

    unmanned

    aerial

    vehicle

    (UAV)

    (Andrew et al., 2017, 019, 2020a, 2020b). Bhole et al. (2019) pro-

    posed an extra step for cow recognition from the side by recording

    IRTIs

    to

    ease

    the

    image

    segmentation

    and

    remove

    the

    background.

    66

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Sheep

    While biometrics applied for the identification and recognition of

    sheep have not been deeply explored, the development of some

    proposed methods has been published. The techniques that

    have been reported for sheep consist of retinal recognition

    using a commercial retinal scanner, OptiReader (Optibrand®,

    Fort Collins, CO, USA) (Barron et al., 2008), and face recognition

    using classification methods such as machine or deep learning.

    Salama et al. (2019) developed a deep learning model based on

    CNN and Bayesian optimization and obtained an identification

    accuracy of 98%. Corkery et al. (2007) proposed a method

    based on independent components analysis and the InfoMax

    algorithm to identify the specific components from the normal-

    ized images of sheep faces and then find them in each tested

    image; the authors reported an accuracy within 95–95%.

    Pigs

    The biometric techniques that have been published to identify pigs

    are based mainly on face recognition and body recognition from

    the tops of pens. Hansen et al. (2018) developed a face recognition

    method using CNN with high accuracy (97%). Marsot et al. (2020)

    developed a face recognition system based on a mix of computer

    vision to identify the face and eyes and deep learning CNN for clas-

    sification purposes, obtaining an accuracy of 83%. On the other

    hand, Wang et al. (2018) proposed a method to identify pigs

    from images recorded from the whole body using integrated deep

    learning

    networks

    such

    as

    dual-path

    network

    (DPN131),

    InceptionV3 and Xception, with an accuracy of 96%. Huang

    et al. (2020) tested a Weber texture local descriptor (WTLD) iden-

    tification method with different masks to detect and recognize indi-

    vidual features such as hair, skin texture, and spots using images of

    groups of pigs; the tested WTLD methods resulted in accuracies

    >96%. Kashiha et al. (2013) based their automatic identification

    method on computer vision to recognize marked pigs within a

    pen using the Fourier algorithm for patterns description and

    Euclidean distance, this technique resulted in 89% accuracy.

    Machine and deep learning application in livestock to

    address complex problems

    This section concentrates specifically on the research on AI appli-

    cation using ML and deep learning modeling techniques on live-

    stock, specifically for cattle, sheep and pigs. One of the latest

    research studies has been focused on the use of AI to identify

    farm animal emotional responses, including pigs and cattle

    (Neethirajan, 2021). However, it may be difficult to assess and

    interpret the emotional state of farm animals only from facial

    expression and ear positioning, as proposed in the latter study,

    and more objective assessment could be performed using targets

    based on hormonal measurements from endorphins, dopamine,

    serotonin and oxytocin among others, which will require blood

    sampling. Therefore, all the in vitro and tissue applications were

    excluded from this section because they require either destructive

    or invasive methods to obtain data.

    Cattle

    A simple AI approach was proposed using historical data (4 years)

    with almost ubiquitous sensor technology in livestock farms, such

    as meteorological weather stations with daily temperature and

    relative humidity (Fuentes et al., 2020b). In this study, meteoro-

    logical data was used to calculate temperature and humidity indices

    (THI) using different algorithmic approaches as inputs to assess the

    effect of heat stress on milk productivity as targets in a robotic dairy

    farm. This approach attempted to answer complex questions with

    potentially readily available data from robotic and conventional

    dairy farms and proposed a deployment system for an AI approach

    with a general accuracy of AI models of 87%. More accurate heat

    stress assessments could be achieved by either sensor technology,

    with minimal invasiveness to animals, such as ear clips, collars or

    similar, or remote sensing cameras, computer vision and deep

    learning modeling. However, the latter digital approach requires

    assessing individual animals using extra hardware and sensors,

    camera systems located in strategic positions allowing monitoring

    of every single animal (e.g. corral systems and straight alleys).

    Furthermore, these new digital approaches require the recording

    of new data. A big question in applications of AI in cattle, in this

    case, would be whether it is worth the significant extra investment

    in hardware and ML modeling using new data to increase the

    accuracy of models by an additional 10.

    Sensor technology and sensor networks have been implemented

    in cattle to assess lameness, such as accelerometers, IoT connectivity

    and time series ML modeling approaches (Taneja et al., 2020; Wu

    et al., 2020). These applications were the first approaches to be

    implemented in animals after applications in humans for fitbits.

    Sensor readings and connectivity using IoT will facilitate the imple-

    mentation of this technology in a near or real-time fashion.

    However, there is a big downside of the requirement of sensors

    for every single animal to be monitored. This is valid to other appli-

    cations for sensor integration (Neethirajan, 2020), such as collars,

    halter and ear tag sensors (Rahman et al., 2018), to detect physio-

    logical changes, behavior and other anomalies (Wagner et al., 2020).

    As mentioned before, animal recognition using deep learning

    approaches should be considered the first step to apply further

    remote sensing and AI tools. A second step should be the identi-

    fication of key features from animals using deep learning (Jiang

    et al., 2019), which makes possible the extraction of physiological

    information from those specific regions using ML modeling, such

    as HR from the eye section or exposed skin (e.g. ears or muzzle)

    and RR from the muzzle section. These animal features should be

    recognized in a video to extract enough information to obtain

    physiological parameters that currently require 4–8 s (e.g. HR

    and RR) for the signal to stabilize and get meaningful data.

    Hence, the AI implementation steps should consider animal rec-

    ognition, specific feature recognition and tracking and extraction

    of physiological parameters using ML.

    Integration of UAV, computer vision algorithms and CNN

    have been attempted for the recognition of cattle from the air

    (Barbedo et al., 2019). However, these authors concentrated

    efforts on the feasibility and testing of different algorithms rather

    than the potential deployment of a pilot program. Furthermore,

    these approaches could also be used for animal recognition and

    the potential extraction of physiological parameters, such as

    body temperature (using infrared thermal cameras as payload).

    Dairy cows could offer more identification features than Angus

    cattle, which may require the implementation of multispectral

    cameras to include potential non-visible features from animals.

    Sheep

    Sensor technology and sensor networks have also been applied in

    parallel with ML approaches for sheep using electronic collars and

    Animal Health Research Reviews

    67

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    ear sensors as input data and supervised selecting several behavior

    parameters as targets with a reported accuracy of <90% for both

    methods (Mansbridge et al., 2018). Some predictive approaches

    from existing data have been attempted to assess carcass traits

    from early life animal records (Shahinfar et al., 2019) using super-

    vised and unsupervised regression ML methods with various low

    to high accuracies reported.

    Similar detection systems mentioned before for other animals

    have been applied for sheep counting using computer vision and

    deep learning CNN methods (Sarwar et al., 2018), which can also

    be used in parallel with other AI procedures to extract more infor-

    mation from animals for health or welfare assessments, such as

    sheep weight (Shah et al., 2021). Following this approach, add-

    itional physiological parameters, such as HR, body temperature

    and RR, can be extracted from individual sheep non-invasively

    (Fuentes et al., 2020a). The latter study also proposed using this

    AI approach for real livestock farming applications, such as ani-

    mal welfare assessment for animals during transportation.

    Other welfare assessments have been developed for sheep

    based on the facial classification expression for pain level applied

    using deep learning CNN and computer vision with 95% accur-

    acy. However, no deployment was reported, which can be used

    to assess further animal welfare (Jwade et al., 2019).

    Pigs

    Some simple ML applications have been implemented to predict

    water usage in pig farms using regression ML algorithms (Lee

    et al., 2017). However, this study reported a maximum determin-

    ation coefficient of R2 = 0.42 for regression tree algorithms, which

    could be related to poor parameter engineering, since only tem-

    perature and relative humidity were used.

    Automatic pig counting (Tian et al., 2019), pig posture detec-

    tion (Nasirahmadi et al., 2019; Riekert et al., 2020), mounting

    (Li et al., 2019) and sow behavior (Zhang et al., 2019a), localiza-

    tion and tracking (Cowton et al., 2019) aggressive behavior

    (Chen et al., 2020) have been attempted using computer vision

    and deep learning. These are relatively complex approaches for

    meaningful questions considering further pipeline of analyses.

    These approaches could be used to extract more information

    from the individual pigs once they have been recognized, such

    as biometrics, including HR and RR extracted for other animals

    such as sheep, mentioned before (Fuentes et al., 2020a), and cattle

    identification (Andrew et al., 2017) with accuracies in identifica-

    tion between 86 and 96% with a maximum of 89 individuals.

    Other approaches have been implemented for the early detec-

    tion (between 1 and 7 days of infection) of respiratory diseases in

    pigs using deep learning approaches (Cowton et al., 2018). Other

    computer vision approaches using visible and infrared thermal

    imagery analysis without ML approaches also delivered an accept-

    able assessment of respiratory diseases in pigs (Jorquera-Chavez

    et al., 2020).

    Conclusions

    Implementing remote sensing, biometrics and AI for livestock

    health and welfare assessment could have many positive ethical

    implications and higher acceptability by consumers of different

    products derived from livestock farming. Specifically, integrating

    digital technologies could directly impact increasing the willing-

    ness to purchase products from sources that introduced AI to

    increase animal welfare on the farm and transport for ethical

    and responsible animal handling and slaughtering. However, a

    systematic deployment of different digital technologies reviewed

    in this paper will require further investment, which some govern-

    ments, such as Australia, have identified as a priority.

    It is difficult to assess the applicability or deployment options

    from different research studies done so far on livestock, which

    have applied biometrics and AI, because there is no consistency

    in the reporting of the accuracy of models, performance, testing

    for over or underfitting of models, number of animals used or

    proposed pilot or deployment options (Table 1). Furthermore,

    in most of these studies, there are no follow-ups on the models

    either by establishing potential pilot deployments to test them

    in real-life scenarios. Many researchers only rely on the validation

    and testing protocols within the model development stage. The

    latter does not give any information on the practicality or applic-

    ability of these digital systems, because circumstances in real-life

    scenarios change over time and models need to be re-evaluated

    and continuously fed with new data to learn and adapt to differ-

    ent circumstances and scales of use.

    It is also clear that most of the AI developments and modeling

    for livestock farming applications are academic, and very little

    research has focused on efficient and practical deployment to real-

    world scenarios. To change this, researchers should work on real-

    life problems in the livestock industry, starting with simple ones

    and pressing questions. The next step is to solve them using effi-

    cient and affordable technology, starting with big data analysis

    from historical data accumulated by different industries. The idea

    here is to initially apply AI where the data exists, to achieve max-

    imum reach with high performance and scalable applications

    (e.g. heat stress assessment on milk production using historical

    weather information and productivity data). It is also required to

    check whether the correct data is available, avoid basing AI on

    reduced datasets, and restricted only to test different ML

    approaches. Academic exercises based on AI modeling for its

    sake only rarely reach pilot programs and applications in the

    field. Furthermore, data quality and data security are becoming

    fundamental issues that should be dealt using digital ledger systems

    for data and model deployments, such as blockchain implementa-

    tion. This approach allows treating data and AI models as a cur-

    rency to avoid hacking and adulteration, especially with AI

    models and data dealing with welfare assessments for animals in

    farms to claim ethical production or animals in transport.

    To solve these problems, AI modeling, development and

    deployment strategies should have a multidisciplinary team with

    constant communication during the model development and

    deployment stages; what could be a better approach, but very

    rare nowadays is to have an expert on animal science, data ana-

    lysis and AI dealing with companies. This could change soon

    through specialized Agriculture, Animal Science and Veterinary

    degrees in which data analysis, ML and AI are introduced in

    their respective academic curriculums.

    Integrating new and emerging digital technology with AI

    development and deployment strategies for practical applications

    would create effective and efficient AI pilot applications that can

    be easily scaled up to production to create successful innovations

    in livestock farming.

    References

    Abecia JA, María GA, Estévez-Moreno LX and Miranda-De La Lama GC

    (2020) Daily rhythms of body temperature around lambing in sheep mea-

    sured non-invasively. Biological Rhythm Research 51, 988–993.

    68

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Alvarenga F, Borges I, Palkovič L, Rodina J, Oddy V and Dobos R (2016)

    Using a three-axis accelerometer to identify and classify sheep behaviour

    at pasture. Applied Animal Behaviour Science 181, 91–99.

    Andrew W, Greatwood C and Burghardt T (2017) Visual localisation and

    individual identification of Holstein Friesian cattle via deep learning.

    Proceedings of the IEEE International Conference on Computer Vision

    Workshops, 2850–2859.

    Andrew W, Greatwood C and Burghardt T (2019) Aerial animal biometrics:

    individual Friesian cattle recovery and visual identification via an autono-

    mous uav with onboard deep inference. arXiv preprint arXiv, 1907.05310.

    Andrew W, Gao J, Mullan S, Campbell N, Dowsey AW and Burghardt T

    (2020a) Visual identification of individual Holstein-Friesian cattle via

    deep metric learning. arXiv preprint arXiv, 2006.09205.

    Andrew W, Greatwood C and Burghardt T (2020b) Fusing animal biometrics

    with autonomous robotics: drone-based search and individual id of Friesian

    cattle. Proceedings of the IEEE/CVF Winter Conference on Applications of

    Computer Vision Workshops 1, 38–43.

    Awad AI (2016) From classical methods to animal biometrics: a review on cat-

    tle identification and tracking. Computers and Electronics in Agriculture

    123, 423–435.

    Barbedo JGA, Koenigkan LV, Santos TT and Santos PM (2019) A study on

    the detection of cattle in UAV images using deep learning. Sensors 19, 5436.

    Barbosa Pereira C, Dohmeier H, Kunczik J, Hochhausen N, Tolba R and

    Czaplik M (2019) Contactless monitoring of heart and respiratory rate in

    anesthetized pigs using infrared thermography. PloS One 14, e0224747.

    Barron UG, Corkery G, Barry B, Butler F, Mcdonnell K and Ward S (2008)

    Assessment of retinal recognition technology as a biometric method for

    sheep identification. Computers and Electronics in Agriculture 60, 156–166.

    Beiderman Y, Kunin M, Kolberg E, Halachmi I, Abramov B, Amsalem R

    and Zalevsky Z (2014) Automatic solution for detection, identification

    and biomedical monitoring of a cow using remote sensing for optimised

    treatment of cattle. Journal of Agricultural Engineering 45, 153–160.

    Bello R-W, Talib AZH and Mohamed ASAB (2020) Deep learning-based

    architectures for recognition of cow using cow nose image pattern. Gazi

    University Journal of Science 33, 831–844.

    Bergamini L, Porrello A, Dondona AC, Del Negro E, Mattioli M, D’alterio

    N

    and

    Calderara

    S

    (2018)

    Multi-views

    embedding

    for

    cattle

    re-identification. 2018 14th International Conference on Signal-Image

    Technology & Internet-Based Systems (SITIS), 2018. IEEE, pp. 184–191.

    Bhole A, Falzon O, Biehl M and Azzopardi G (2019) A computer vision

    pipeline that uses thermal and RGB images for the recognition of

    Holstein cattle. International Conference on Computer Analysis of

    Images and Patterns, 2019. Springer, 108–119.

    Buchli C, Raselli A, Bruckmaier R and Hillmann E (2017) Contact with cows

    during the young age increases social competence and lowers the cardiac

    stress reaction in dairy calves. Applied Animal Behaviour Science 187, 1–7.

    Burn CC, Pritchard JC and Whay HR (2009) Observer reliability for working

    equine welfare assessment: problems with high prevalences of certain

    results. Animal Welfare 18, 177–187.

    Byrd C, Johnson J, Radcliffe J, Craig B, Eicher S and Lay Jr D (2020)

    Nonlinear analysis of heart rate variability for evaluating the growing pig

    stress response to an acute heat episode. Animal: An International

    Journal of Animal Bioscience 14, 379–387.

    Cai C and Li J (2013) Cattle face recognition using local binary pattern

    descriptor.

    2013

    Asia-Pacific

    Signal

    and

    Information

    Processing

    Association Annual Summit and Conference 29 Oct.-1 Nov. 2013,

    Kaohsiung, Taiwan. IEEE, 1–4.

    Chen C, Zhu W, Steibel J, Siegford J, Wurtz K, Han J and Norton T (2020)

    Recognition of aggressive episodes of pigs based on convolutional neural

    network and long short-term memory. Computers and Electronics in

    Agriculture 169, 105166.

    Chung H, Li J, Kim Y, Van Os JM, Brounts SH and Choi CY (2020) Using

    implantable biosensors and wearable scanners to monitor dairy cattle’s core

    body temperature in real-time. Computers and Electronics in Agriculture

    174, 105453.

    Clapham M, Miller E, Nguyen M and Darimont CT (2020) Automated facial

    recognition for wildlife that lack unique markings: a deep learning approach

    for brown bears. Ecology and Evolution 10, 12883–12892.

    Corkery G, Gonzales-Barron UA, Butler F, Mc Donnell K and Ward S

    (2007) A preliminary investigation on face recognition as a biometric iden-

    tifier of sheep. Transactions of the ASABE 50, 313–320.

    Cowton J, Kyriazakis I, Plötz T and Bacardit J (2018) A combined

    deep-learning GRU-autoencoder for the early detection of respiratory dis-

    ease in pigs using multiple environmental sensors. Sensors 18, 2521.

    Cowton J, Kyriazakis I and Bacardit J (2019) Automated individual pig local-

    isation, tracking and behaviour metric extraction using deep learning. IEEE

    Access 7, 108049–108060.

    Cui Y, Zhang M, Li J, Luo H, Zhang X and Fu Z (2019) WSMS: wearable

    stress monitoring system based on IoT multi-sensor platform for living

    sheep transportation. Electronics 8, 441.

    Da Fonseca FN, Abe JM, De Alencar Nääs I, Da Silva Cordeiro AF, Do

    Amaral FV and Ungaro HC (2020) Automatic prediction of stress in pig-

    lets (Sus Scrofa) using infrared skin temperature. Computers and Electronics

    in Agriculture 168, 105148.

    De Freitas ACB, Vega WHO, Quirino CR, Junior AB, David CMG, Geraldo

    AT, Rua MAS, Rojas LFC, De Almeida Filho JE and Dias AJB (2018)

    Surface temperature of ewes during estrous cycle measured by infrared

    thermography. Theriogenology 119, 245–251.

    De Lima Weber F, De Moraes Weber VA, Menezes GV, Junior ADSO, Alves

    DA, De Oliveira MVM, Matsubara ET, Pistori H and De Abreu UGP

    (2020) Recognition of pantaneira cattle breed using computer vision and

    convolutional neural networks. Computers and Electronics in Agriculture

    175, 105548.

    Digiacomo K, Simpson S, Leury BJ and Dunshea FR (2016) Dietary betaine

    impacts the physiological responses to moderate heat conditions in a dose-

    dependent manner in sheep. Animals 6, 51.

    Du X and Zhou J (2018) Application of biosensors to detection of epidemic

    diseases in animals. Research in Veterinary Science 118, 444–448.

    Feng Y-Z, Zhao H-T, Jia G-F, Ojukwu C and Tan H-Q (2019) Establishment

    of validated models for non-invasive prediction of rectal temperature of

    sows using infrared thermography and chemometrics. International

    Journal of Biometeorology 63, 1405–1415.

    Fuchs B, Sørheim KM, Chincarini M, Brunberg E, Stubsjøen SM,

    Bratbergsengen K, Hvasshovd SO, Zimmermann B, Lande US and

    Grøva L (2019) Heart rate sensor validation and seasonal and diurnal vari-

    ation of body temperature and heart rate in domestic sheep. Veterinary and

    Animal Science 8, 100075.

    Fuentes S, Gonzalez Viejo C, Chauhan SS, Joy A, Tongson E and Dunshea

    FR (2020a) Non-invasive sheep biometrics obtained by computer vision

    algorithms and machine learning modeling using integrated visible/infrared

    thermal cameras. Sensors 20, 6334.

    Fuentes S, Gonzalez Viejo C, Cullen B, Tongson E, Chauhan SS and

    Dunshea FR (2020b) Artificial intelligence applied to a robotic dairy

    farm to model milk productivity and quality based on Cow data and

    daily environmental parameters. Sensors 20, 2975.

    Gaber T, Tharwat A, Hassanien AE and Snasel V (2016) Biometric cattle

    identification approach based on weber’s local descriptor and adaboost clas-

    sifier. Computers and Electronics in Agriculture 122, 55–66.

    Giovanetti V, Decandia M, Molle G, Acciaro M, Mameli M, Cabiddu A,

    Cossu R, Serra M, Manca C and Rassu S (2017) Automatic classification

    system for grazing, ruminating and resting behaviour of dairy sheep using a

    tri-axial accelerometer. Livestock Science 196, 42–48.

    Gonzalez Viejo C, Fuentes S, Torrico D and Dunshea F (2018) Non-contact

    heart rate and blood pressure estimations from video analysis and machine

    learning modelling applied to food sensory responses: a case study for choc-

    olate. Sensors 18, 1802.

    Halachmi I, Guarino M, Bewley J and Pastell M (2019) Smart animal agri-

    culture: application of real-time sensors to improve animal well-being and

    production. Annual Review of Animal Biosciences 7, 403–425.

    Hansen MF, Smith ML, Smith LN, Salter MG, Baxter EM, Farish M and

    Grieve B (2018) Towards on-farm pig face recognition using convolutional

    neural networks. Computers in Industry 98, 145–152.

    Huang L, Li S, Zhu A, Fan X, Zhang C and Wang H (2018) Non-contact body

    measurement for Qinchuan cattle with LiDAR sensor. Sensors 18, 3014.

    Huang W, Zhu W, Ma C and Guo Y (2020) Weber texture local descriptor for

    identification of group-housed pigs. Sensors 20, 4649.

    Animal Health Research Reviews

    69

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Jiang B, Wu Q, Yin X, Wu D, Song H and He D (2019) FLYOLOV3 deep

    learning for key parts of dairy cow body detection. Computers and

    Electronics in Agriculture 166, 104982.

    Jongman E, Jorquera-Chavez M, Dunshea F, Fuentes S, Poblete T,

    Rajasekhara R and Morisson R (2020) Developing Remote Monitoring

    Methods for Early Detection of Respiratory Disease in Pigs. Final Report

    A1-104, Australasian Pork Research Institute Limited (23 pp). https://

    apri.com.au/wp-content/uploads/2021/06/A1-104-Final-Report.pdf

    Jorquera-Chavez M, Fuentes S, Dunshea FR, Jongman EC and Warner RD

    (2019a) Computer vision and remote sensing to assess physiological

    responses of cattle to pre-slaughter stress, and its impact on beef quality:

    a review. Meat Science 156, 11–22.

    Jorquera-Chavez M, Fuentes S, Dunshea FR, Warner RD, Poblete T and

    Jongman EC (2019b) Modelling and validation of computer vision techni-

    ques to assess heart rate, eye temperature, ear-base temperature and respir-

    ation rate in cattle. Animals 9, 1089.

    Jorquera-Chavez M, Fuentes S, Dunshea FR, Warner RD, Poblete T,

    Morrison RS and Jongman EC (2020) Remotely sensed imagery for

    early detection of respiratory disease in pigs: a pilot study. Animals 10, 451.

    Jwade SA, Guzzomi A and Mian A (2019) On-farm automatic sheep breed

    classification

    using

    deep

    learning.

    Computers

    and

    Electronics

    in

    Agriculture 167, 105055.

    Karthick G, Sridhar M and Pankajavalli P (2020) Internet of things in animal

    healthcare (IoTAH): review of recent advancements in architecture, sensing

    technologies and real-time monitoring. SN Computer Science 1, 1–16.

    Kashiha M, Bahr C, Ott S, Moons CP, Niewold TA, Ödberg FO and

    Berckmans D (2013) Automatic identification of marked pigs in a pen

    using image pattern recognition. Computers and Electronics in Agriculture

    93, 111–120.

    Kim S and Hidaka Y (2021) Breathing pattern analysis in cattle using infrared

    thermography and computer vision. Animals 11, 207.

    Kumar S, Tiwari S and Singh SK (2016) Face recognition of cattle: can it be

    done? Proceedings of the National Academy of Sciences, India Section A:

    Physical Sciences 86, 137–148.

    Kumar S, Singh SK and Singh AK (2017a) Muzzle point pattern-based tech-

    niques for individual cattle identification. IET Image Processing 11, 805–814.

    Kumar S, Singh SK, Singh RS, Singh AK and Tiwari S (2017b) Real-time

    recognition of cattle using animal biometrics. Journal of Real-Time Image

    Processing 13, 505–526.

    Kumar S, Pandey A, Satwik KSR, Kumar S, Singh SK, Singh AK and

    Mohan A (2018) Deep learning framework for recognition of cattle using

    muzzle point image pattern. Measurement 116, 1–17.

    Lee W, Ryu J, Ban T-W, Kim SH and Choi H (2017) Prediction of water

    usage in pig farm based on machine learning. Journal of the Korea

    Institute of Information and Communication Engineering 21, 1560–1566.

    Li D, Chen Y, Zhang K and Li Z (2019) Mounting behaviour recognition for

    pigs based on deep learning. Sensors 19, 4924.

    Lowe G, Sutherland M, Waas J, Schaefer A, Cox N and Stewart M (2019)

    Infrared thermography—a non-invasive method of measuring respiration

    rate in calves. Animals 9, 535.

    Lu Y, He X, Wen Y and Wang PS (2014) A new cow identification system

    based on iris analysis and recognition. International Journal of Biometrics

    6, 18–32.

    Mandal R, Gupta V, Joshi V, Kumar S and Mondal D (2017) Study of

    clinico-hematobiochemical changes and therapeutic management of natur-

    ally infected cases of respiratory disease in Non-descript goats of bareilly

    region. International Journal of Livestock Research 7, 211–218.

    Mansbridge N, Mitsch J, Bollard N, Ellis K, Miguel-Pacheco GG, Dottorini

    T and Kaler J (2018) Feature selection and comparison of machine learning

    algorithms in classification of grazing and rumination behaviour in sheep.

    Sensors 18, 3532.

    Marsot M, Mei J, Shan X, Ye L, Feng P, Yan X, Li C and Zhao Y (2020) An

    adaptive pig face recognition approach using convolutional neural net-

    works. Computers and Electronics in Agriculture 173, 105386.

    Mcmanus C, Tanure CB, Peripolli V, Seixas L, Fischer V, Gabbi AM,

    Menegassi SR, Stumpf MT, Kolling GJ and Dias E (2016) Infrared therm-

    ography in animal production: an overview. Computers and Electronics in

    Agriculture 123, 10–16.

    Morota G, Ventura RV, Silva FF, Koyama M and Fernando SC (2018) Big

    data analytics and precision animal agriculture symposium: machine learn-

    ing and data mining advance predictive big data analysis in precision ani-

    mal agriculture. Journal of Animal Science 96, 1540–1550.

    Nasirahmadi A, Hensel O, Edwards S and Sturm B (2017) A new approach for

    categorizing pig lying behaviour based on a Delaunay triangulation method.

    Animal: An International Journal of Animal Bioscience 11, 131–139.

    Nasirahmadi A, Sturm B, Edwards S, Jeppsson K-H, Olsson A-C, Müller S

    and Hensel O (2019) Deep learning and machine vision approaches for

    posture detection of individual pigs. Sensors 19, 3738.

    Neethirajan S (2020) The role of sensors, big data and machine learning in

    modern animal farming. Sensing and Bio-Sensing Research 29, 100367.

    Neethirajan S (2021) Happy Cow or thinking Pig? WUR wolf – facial coding

    platform

    for

    measuring

    emotions

    in

    farm

    animals.

    bioRxiv,

    2021.04.09.439122.

    Neethirajan S and Kemp B (2021) Digital livestock farming. Sensing and

    Bio-Sensing Research 32, 100408.

    Neethirajan S, Tuteja SK, Huang S-T and Kelton D (2017) Recent advance-

    ment in biosensors technology for animal and livestock health manage-

    ment. Biosensors and Bioelectronics 98, 398–407.

    Nie L, Berckmans D, Wang C and Li B (2020) Is continuous heart rate mon-

    itoring of livestock a dream or is it realistic? A review. Sensors 20, 2291.

    Noviyanto A and Arymurthy AM (2012) Automatic cattle identification

    based on muzzle photo using speed-up robust features approach.

    Proceedings of the 3rd European Conference of Computer Science, ECCS,

    December 2-4, 2012, Paris, France, 114.

    Orihuela A, Omaña J and Ungerfeld R (2016) Heart rate patterns during

    courtship and mating in rams and in estrous and nonestrous ewes (Ovis

    aries). Journal of Animal Science 94, 556–562.

    Pezzuolo A, Guarino M, Sartori L, González LA and Marinello F (2018)

    On-barn pig weight estimation based on body measurements by a Kinect

    v1 depth camera. Computers and Electronics in Agriculture 148, 29–36.

    Qiao Y, Su D, Kong H, Sukkarieh S, Lomax S and Clark C (2019) Individual

    cattle

    identification

    using

    a

    deep

    learning-based

    framework.

    IFAC-PapersOnLine 52, 318–323.

    Rahman A, Smith D, Little B, Ingham A, Greenwood P and Bishop-Hurley

    G (2018) Cattle behaviour classification from collar, halter, and ear tag sen-

    sors. Information Processing in Agriculture 5, 124–133.

    Riekert M, Klein A, Adrion F, Hoffmann C and Gallmann E (2020)

    Automatically detecting pig position and posture by 2D camera imaging

    and deep learning. Computers and Electronics in Agriculture 174, 105391.

    Rocha LM, Devillers N, Maldague X, Kabemba FZ, Fleuret J, Guay F and

    Faucitano L (2019) Validation of anatomical sites for the measurement

    of infrared body surface temperature variation in response to handling

    and transport. Animals 9, 425.

    Salama A, Hassanien AE and Fahmy A (2019) Sheep identification using a

    hybrid deep learning and Bayesian optimization approach. IEEE Access 7,

    31681–31687.

    Sarwar F, Griffin A, Periasamy P, Portas K and Law J (2018) Detecting and

    counting sheep with a convolutional neural network. 2018 15th IEEE

    International

    Conference

    on

    Advanced

    Video

    and

    Signal

    Based

    Surveillance (AVSS), 2018. IEEE, 1–6.

    Shah NA, Thik J, Bhatt C and Hassanien A-E (2021) A deep convolutional

    encoder-decoder architecture approach for sheep weight estimation.

    Advances in Artificial Intelligence and Data Engineering 1133, 43–53.

    Shahinfar S, Kelman K and Kahn L (2019) Prediction of sheep carcass traits

    from early life records using machine learning. Computers and Electronics

    in Agriculture 156, 159–177.

    Singh M, Kumar R, Tandon D, Sood P and Sharma M (2020) Artificial

    Intelligence and IoT based Monitoring of Poultry Health: A Review. 2020

    IEEE

    International

    Conference

    on

    Communication,

    Networks

    and

    Satellite (Comnetsat), 2020. IEEE, 50–54.

    Slimen IB, Chniter M, Najar T and Ghram A (2019) Meta-analysis of some

    physiologic, metabolic and oxidative responses of sheep exposed to environ-

    mental heat stress. Livestock Science 229, 179–187.

    Stewart M, Wilson M, Schaefer A, Huddart F and Sutherland M (2017) The

    use of infrared thermography and accelerometers for remote monitoring of

    dairy cow health and welfare. Journal of Dairy Science 100, 3893–3901.

    70

    Sigfredo Fuentes et al.

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    Stojkov J, Weary D and Von Keyserlingk M (2016) Nonambulatory cows:

    duration of recumbency and quality of nursing care affect outcome of flo-

    tation therapy. Journal of Dairy Science 99, 2076–2085.

    Sutherland MA, Worth GM, Dowling SK, Lowe GL, Cave VM and Stewart M

    (2020) Evaluation of infrared thermography as a non-invasive method of

    measuring the autonomic nervous response in sheep. PloS One 15, e0233558.

    Tahsin KN (2016) Development of a propeller P8X 32A based wireless biosen-

    sor system for cattle health monitoring and disease detection. British

    Journal of Applied Science and Technology 18(2), 1–14.

    Taneja M, Byabazaire J, Jalodia N, Davy A, Olariu C and Malone P (2020)

    Machine learning-based fog computing assisted data-driven approach for

    early lameness detection in dairy cattle. Computers and Electronics in

    Agriculture 171, 105286.

    Tian M, Guo H, Chen H, Wang Q, Long C and Ma Y (2019) Automated pig

    counting using deep learning. Computers and Electronics in Agriculture 163,

    104840.

    Tsai Y-C, Hsu J-T, Ding S-T, Rustia DJA and Lin T-T (2020) Assessment of

    dairy cow heat stress by monitoring drinking behaviour using an embedded

    imaging system. Biosystems Engineering 199, 97–108.

    Wagner N, Antoine V, Koko J, Mialon M-M, Lardy R and Veissier I (2020)

    Comparison of Machine Learning Methods to Detect Anomalies in the

    Activity of Dairy Cows. International Symposium on Methodologies for

    Intelligent Systems, 2020. Springer 12117, 342–351.

    Wang J, Liu A and Xiao J (2018) Video-Based Pig Recognition with

    Feature-Integrated Transfer Learning. Chinese Conference on Biometric

    Recognition, 2018. Springer, 620–631.

    Wang H, Qin J, Hou Q and Gong S (2020) Cattle face recognition method

    based on parameter transfer and deep learning. Journal of Physics:

    Conference Series, 2020. IOP Publishing, 012054.

    Wang F-K, Shih J-Y, Juan P-H, Su Y-C and Wang Y-C (2021a) Non-invasive

    cattle body temperature measurement using infrared thermography and

    auxiliary sensors. Sensors 21, 2425.

    Wang M, Youssef A, Larsen M, Rault J-L, Berckmans D, Marchant-Forde JN,

    Hartung J, Bleich A, Lu M and Norton T (2021b) Contactless video-based

    heart rate monitoring of a resting and an anesthetized pig. Animals 11, 442.

    Wilcox J (2020) Covid-19 sentiment and reactions among financial decision-

    makers in Europe. Hanover Research. https://f.hubspotusercontent30.net/

    hubfs/2020381/~Marketing%202020/Landing%20Pages/COVID-19%20Sen

    timent%20and%20Reactions%20among%20Financial%20Decision%20Mak

    ers%20in%20Europe%20-%20OneStream%20-%20October%202020%20Ext

    ernal.pdf?__hstc=231710272.7fd81c9d080729dff81a1cd93879abaa.1601655

    069581.1616156525336.1616159907458.57&__hssc=231710272.109.161615

    9907458&__hsfp=53860856&hsCtaTracking=49adcdf8-fd96-413c-b5a7-e72b

    d9087277%7C59e989fa-56ae-4b8c-bfd8-1eb1d31217d5

    Wu D, Wu Q, Yin X, Jiang B, Wang H, He D and Song H (2020) Lameness

    detection of dairy cows based on the YOLOv3 deep learning algorithm

    and a relative step size characteristic vector. Biosystems Engineering 189,

    150–163.

    Zhang AL, Wu BP, Wuyun CT, Jiang DX, Xuan EC and Ma FY (2018)

    Algorithm of sheep body dimension measurement and its applications

    based on image analysis. Computers and Electronics in Agriculture 153,

    33–45.

    Zhang M, Feng H, Luo H, Li Z and Zhang X (2020) Comfort and health

    evaluation of live mutton sheep during the transportation based on wear-

    able multi-sensor system. Computers and Electronics in Agriculture 176,

    105632.

    Zhang Y, Cai J, Xiao D, Li Z and Xiong B (2019a) Real-time sow behavior

    detection

    based

    on

    deep

    learning.

    Computers

    and

    Electronics

    in

    Agriculture 163, 104884.

    Zhang Z, Zhang H and Liu T (2019b) Study on body temperature detection of

    pig based on infrared technology: a review. Artificial Intelligence in

    Agriculture 1, 14–26.

    Zin TT, Phyo CN, Tin P, Hama H and Kobayashi I (2018) Image

    technology-based

    cow

    identification

    system

    using

    deep

    learning.

    Proceedings

    of

    the

    International

    MultiConference

    of

    Engineers

    and

    Computer Scientists 1, 236–247.

    Zipp KA, Barth K, Rommelfanger E and Knierim U (2018) Responses of

    dams versus non-nursing cows to machine milking in terms of milk perform-

    ance, behaviour and heart rate with and without additional acoustic, olfactory

    or manual stimulation. Applied Animal Behaviour Science 204, 10–17.

    Animal Health Research Reviews

    71

    https://doi.org/10.1017/S1466252321000177 Published online by Cambridge University
    Press

    '
  inline_citation: null
  journal: Animal health research reviews (Print)
  key_findings: []
  limitations: '>'
  main_objective: The main objective of this systematic review was to explore how
    automated, real-time irrigation management systems can contribute to the efficient
    use of water resources and enhance agricultural productivity to meet the growing
    demand for food, while also identifying challenges and proposing solutions to
    address them.
  pdf_link: https://www.cambridge.org/core/services/aop-cambridge-core/content/view/3A50E1FBDEA8C13506D479828D7B2F57/S1466252321000177a.pdf/div-class-title-the-livestock-farming-digital-transformation-implementation-of-new-and-emerging-technologies-using-artificial-intelligence-div.pdf
  publication_year: 2022
  relevance_evaluation:
    extract_1: 'The purpose and intention of this systematic review on automated systems
      for real-time irrigation management can be interpreted as follows:


      Addressing the global food challenge: The review aims to explore how automated,
      real-time irrigation management systems can contribute to the efficient use
      of water resources and enhance agricultural productivity to meet the growing
      demand for food.'
    extract_2: 'Evaluating the current state and future potential: The primary objective
      is to critically assess the current state of end-to-end automated irrigation
      management systems that integrate IoT and machine learning technologies. The
      review also seeks to identify gaps and propose solutions for seamless integration
      across the automated irrigation management system to achieve fully autonomous,
      scalable irrigation management.'
    limitations: []
    relevance_score: '0.9'
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: null
  technologies_used: []
  title: 'The livestock farming digital transformation: implementation of new and
    emerging technologies using artificial intelligence'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1109/lra.2017.2774979
  analysis: '>'
  apa_citation: 'Sa, I., Chen, Z., Popović, M., Khanna, R., Liebisch, F., Nieto, J.,
    & Siegwart, R. (2017). weedNet: Dense semantic weed classification using multispectral
    images and MAV for smart farming. IEEE Robotics and Automation Letters, 3(1),
    588-595. https://doi.org/10.1109/LRA.2017.2774979'
  authors:
  - Inkyu Sa
  - Zetao Chen
  - Marija Popović
  - Raghav Khanna
  - Frank Liebisch
  - Juan Nieto
  - Roland Siegwart
  citation_count: 232
  data_sources: Multispectral images of crop fields
  explanation: 'The study titled "weedNet: Dense Semantic Weed Classification Using
    Multispectral Images and MAV for Smart Farming" by Sa et al. (2017) focuses on
    developing an approach for dense semantic weed classification using multispectral
    images collected by micro aerial vehicles (MAVs). The main objective of the study
    is to provide a reliable and accurate method for weed detection, which is a critical
    step in autonomous crop management as it relates to crop health and yield. The
    approach utilizes the SegNet encoder-decoder cascaded convolutional neural network
    to infer dense semantic classes while allowing for any number of input image channels
    and class balancing. The study establishes an experimental field with varying
    herbicide levels resulting in field plots containing only either crop or weed,
    enabling the use of the normalized difference vegetation index as a distinguishable
    feature for automatic ground truth generation. Different models with varying numbers
    of input channels are trained and conditioned to achieve an F1-score of approximately
    0.8 and an area under the curve classification metric of 0.78. The embedded Graphics
    Processing Unit (GPU) system (Jetson TX2) is tested for MAV integration for model
    deployment. The dataset used in the study is released to support the community
    and future work.'
  extract_1: We present an approach for dense semantic weed classification with multispectral
    images collected by a micro aerial vehicle (MAV). We use the recently developed
    encoder-decoder cascaded convolutional neural network, SegNet, that infers dense
    semantic classes while allowing any number of input image channels and class balancing
    with our sugar beet and weed datasets.
  extract_2: We train six models with different numbers of input channels and condition
    (fine tune) it to achieve ~0.8 F1-score and 0.78 area under the curve classification
    metrics. For the model deployment, an embedded Graphics Processing Unit (GPU)
    system (Jetson TX2) is tested for MAV integration.
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Subscribe Donate Cart Create Account Personal Sign
    In Browse My Settings Help Institutional Sign In All Books Conferences Courses
    Journals & Magazines Standards Authors Citations ADVANCED SEARCH Journals & Magazines
    >IEEE Robotics and Automation ... >Volume: 3 Issue: 1 weedNet: Dense Semantic
    Weed Classification Using Multispectral Images and MAV for Smart Farming Publisher:
    IEEE Cite This PDF Inkyu Sa; Zetao Chen; Marija Popović; Raghav Khanna; Frank
    Liebisch; Juan Nieto; Roland Siegwart All Authors 214 Cites in Papers 1 Cites
    in Patent 3815 Full Text Views Abstract Document Sections I. Introduction II.
    Related Work III. Methodologies IV. Experimental Results V. Conclusions Authors
    Figures References Citations Keywords Metrics Footnotes Abstract: Selective weed
    treatment is a critical step in autonomous crop management as related to crop
    health and yield. However, a key challenge is reliable and accurate weed detection
    to minimize damage to surrounding plants. In this letter, we present an approach
    for dense semantic weed classification with multispectral images collected by
    a micro aerial vehicle (MAV). We use the recently developed encoder-decoder cascaded
    convolutional neural network, SegNet, that infers dense semantic classes while
    allowing any number of input image channels and class balancing with our sugar
    beet and weed datasets. To obtain training datasets, we established an experimental
    field with varying herbicide levels resulting in field plots containing only either
    crop or weed, enabling us to use the normalized difference vegetation index as
    a distinguishable feature for automatic ground truth generation. We train six
    models with different numbers of input channels and condition (fine tune) it to
    achieve ~0.8 F1-score and 0.78 area under the curve classification metrics. For
    the model deployment, an embedded Graphics Processing Unit (GPU) system (Jetson
    TX2) is tested for MAV integration. Dataset used in this letter is released to
    support the community and future work. Published in: IEEE Robotics and Automation
    Letters ( Volume: 3, Issue: 1, January 2018) Page(s): 588 - 595 Date of Publication:
    20 November 2017 ISSN Information: DOI: 10.1109/LRA.2017.2774979 Publisher: IEEE
    Funding Agency: I. Introduction To a growing worldwide population with sufficient
    farm produce, new smart farming methods are required to increase or maintain crop
    yield while minimizing environmental impact. Precision agriculture techniques
    achieve this by spatially surveying key indicators of crop health and applying
    treatment, e.g., herbicides, pesticides, and fertilizers, only to relevant areas.
    Here, robotic systems can be often used as flexible, cost-efficient platforms
    replacing laborious manual procedures. Sign in to Continue Reading Authors Figures
    References Citations Keywords Metrics Footnotes More Like This An Experimental
    Comparison Towards Autonomous Camera Navigation to Optimize Training in Robot
    Assisted Surgery IEEE Robotics and Automation Letters Published: 2020 A Mobile
    Robot Visual SLAM System With Enhanced Semantics Segmentation IEEE Access Published:
    2020 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved.'
  inline_citation: (Sa et al., 2017)
  journal: IEEE robotics and automation letters
  key_findings: The proposed approach achieves an F1-score of approximately 0.8 and
    an area under the curve classification metric of 0.78. The approach is suitable
    for deployment on embedded GPU systems, such as the Jetson TX2.
  limitations: The study is limited to the use of multispectral images, and it is
    not clear how well the approach would generalize to other types of images, such
    as RGB images. Additionally, the study does not evaluate the performance of the
    approach in real-world conditions, such as in a commercial agricultural setting.
  main_objective: To develop an approach for dense semantic weed classification using
    multispectral images collected by micro aerial vehicles (MAVs).
  pdf_link: null
  publication_year: 2018
  relevance_evaluation: The study is highly relevant to the point of integrating high-resolution
    cameras and computer vision algorithms for automated irrigation systems for visual
    monitoring of crop growth, disease detection, and irrigation system performance.
    The study demonstrates the use of a multispectral camera to capture images of
    crop fields, and then uses computer vision algorithms to classify weeds and crops.
    This information can then be used to guide irrigation decisions, such as when
    and where to water. The study is also relevant to the larger context of the literature
    review, which is to evaluate the current state and future potential of real-time,
    end-to-end automated irrigation management systems. The study provides insights
    into the challenges and opportunities of using computer vision for automated irrigation,
    and can help to inform the development of future systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Multispectral camera, computer vision, encoder-decoder cascaded
    convolutional neural network (SegNet)
  title: 'weedNet: Dense Semantic Weed Classification Using Multispectral Images and
    MAV for Smart Farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1101/2020.08.27.263186
  analysis: '>'
  apa_citation: Gao, J., Westergaard, J. C., Sundmark, E. H. R., Bagge, M., Alexandersson,
    E., Liljeroth, E., ... Liljeroth, E. (2020, August 27). Automatic late blight
    lesion recognition and severity quantification based on field imagery of diverse
    potato genotypes by deep learning. bioRxiv. https://doi.org/10.1101/2020.08.27.263186
  authors:
  - Junfeng Gao
  - Jesper Cairo Westergaard
  - Ea Høegh Riis Sundmark
  - Merethe Bagge
  - Erland Liljeroth
  - Erik Alexandersson
  citation_count: 1
  data_sources: RGB images acquired under field conditions
  explanation: The paper introduces an automated system for segmenting late blight
    lesions in potato genotypes with diverse leaf colors using a deep convolutional
    neural network (DCNN) based on an encoder-decoder architecture. The automated
    system can accurately estimate the severity of the disease, quantifying the number
    of lesions and their area at the canopy level, which correlates with visual scores
    obtained from an experienced plant breeder. This automated system has the potential
    to monitor the development of late blight disease under field conditions and evaluate
    the resistance of genotypes against potato late blight, enabling more precise
    and automated potato breeding.
  extract_1: Visual scoring in the field provides an important metric to quantify
    disease severity, but is prone to  be biased and error can be subjected to raters.
    Thanks to automation, effectivity and objectiveness,  sensor-based measurement,
    especially imaging sensors, provides potential advantages compared  with visual
    scoring.
  extract_2: We adopted an encoder-decoder neural network architecture based on SegNet
    [26] for lesion  segmentation. The proposed network operates on input images of
    512x512 pixels and outputs  segmentation masks in the same size as the input image.
  full_citation: '>'
  full_text: ">\nAutomatic late blight lesion recognition and severity quantification\
    \ based on \nfield imagery of diverse potato genotypes by deep learning \nJunfeng\
    \ Gao1,*, Jesper Cairo Westergaard2, Ea Høegh Riis Sundmark3, Merethe Bagge3,\
    \  \nErland Liljeroth4, Erik Alexandersson4,* \n \n \n1, Lincoln Agri-Robotics,\
    \ Lincoln Institute for Agri-Food Technology, University of Lincoln, Lincoln,\
    \ UK \n2, Department of Plant and Environmental Sciences, University of Copenhagen,\
    \ Taastrup, Denmark \n3, Danespo Breeding Company, Give, Denmark \n4, Department\
    \ of Plant Protection Biology, Swedish University of Agricultural Sciences, Alnarp,\
    \ \nSweden \n \nAbstract:  \nThe plant pathogen Phytophthora infestans causes\
    \ the severe disease late blight in potato, which \nresults in a huge loss for\
    \ potato production. Automatic and accurate disease lesion segmentation \nenables\
    \ fast evaluation of disease severity and assessment of disease progress for precision\
    \ crop \nbreeding. Deep learning has gained tremendous success in computer vision\
    \ tasks for image \nclassification, object detection and semantic segmentation.\
    \ To test whether we could extract late \nblight lesions from unstructured field\
    \ environments based on high-resolution visual field images and \ndeep learning\
    \ algorithms, we collected ~500 field RGB images in a set of diverse potato genotypes\
    \ \nwith different disease severity (0-70%), resulting in 2100 cropped images.\
    \ 1600 of these cropped \nimages were used as the dataset for training deep neural\
    \ networks. Finally, the developed model was \ntested on the 250 cropped images.\
    \ The results show that the intersection over union (IoU) values of \nbackground\
    \ (leaf and soil) and disease lesion classes in the test dataset are 0.996 and\
    \ 0.386, \nrespectively. Furthermore, we established a linear relationship (R2\
    \ = 0.655) between manual visual \nscores of late blight and the number of lesions\
    \ at the canopy level. We also learned that imbalance \nweights of lesion and\
    \ background classes improved segmentation performance, and that fused \nmasks\
    \ based on the majority voting of the multiple masks enhanced the correlation\
    \ with the visual \nscores. This study demonstrates the feasibility of using deep\
    \ learning algorithms for disease lesion \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nsegmentation\
    \ and severity evaluation based on proximal imagery for crop resistance breeding\
    \ in field \nenvironments. \n1. Introduction \nCrop disease poses a threat to\
    \ global food security [1]. Automated field phenotyping can become a \npowerful\
    \ tool for future resistance breeding as well as for precision agriculture [2][3],and\
    \ can thus be \na successful way to mitigate crop disease. Potato is today the\
    \ third most important food crop and is \nan important part of many diets, especially\
    \ in temperate climates. The oomycete Phytophthora \ninfestans (Mont.) de Bary\
    \ which causes potato late blight (PLB) and potato tuber blight (PTB) can be \n\
    very destructive in potato cultivation if it is not managed (Wiik, Rosenqvist,\
    \ & Liljeroth, 2018). In \npractice, the prevention of PLB is in the field is\
    \ highly relying on regular blanket spraying of fungicide \nduring the growth\
    \ season. As an example, in Sweden the potato production consumes around 20% of\
    \ \nall fungicides used in agriculture, largely to combat P. infestans, in spite\
    \ of occupying less than 1% of \nthe area under cultivation [4]. In addition,\
    \ PLB prevention requires frequent use of fungicides with \nsometimes more than\
    \ 10 applications per growth season in Northern Europe to avoid significant \n\
    yield loss.  This management is effective in general and widely accepted by farmers,\
    \ but also results \nin usage of large amounts of fungicide as well as fossil\
    \ fuels, which hampers the sustainable \ndevelopment of agriculture. \nThe loss\
    \ caused by PLB can be reduced by breeding PLB resistant cultivars. To breed for\
    \ high PLB \nresistance, plant breeders establish experimental plots to quantify\
    \ the PLB severity of different \npotato genotypes and progeny lines. This is\
    \ currently manually done by estimating visual scores \nbased on the number and\
    \ area of lesions on plants [4]. This process is time consuming, can be \nsubjective\
    \ and also requires experienced raters for visual scoring. Therefore, it is highly\
    \ needed to \ndevelop an automated disease evaluation system to facilitate and\
    \ speed up the breeding processes. \nHowever, one of the main challenges in automating\
    \ the system is to accurately segment the lesions \nunder field conditions. There\
    \ are some studies which have employed PLB lesion segmentation. For \nexample,\
    \ Abdu et al [5] developed a pattern recognition approach to recognize early blight,\
    \ caused \nby Alternaria solani, and PLB visual disease symptoms using soft computing\
    \ and machine learning \nalgorithms. Barbedo [6] and Camargo et al [7] also carried\
    \ out similar studies on plant disease \ndetection and segmentation. However,\
    \ the image datasets used in these studies above are collected \nat foliage level\
    \ under relatively clear and uniform backgrounds in controlled settings and the\
    \ \npipelines are not readily implemented into large scale field environments.\
    \ Moreover, the majority of \nprevious works have only investigated the symptom\
    \ segmentation problems based on a single potato \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    cultivar. These models might fail to segment other lesions due to differences\
    \ in lesion morphology \nand leaf color between different potato cultivars. \n\
    Attempts have also been made for early detection of PLB. Various sensors from\
    \ imaging to non-\nimaging sensors have been employed in these applications. For\
    \ example, Fernández et al [8] \ninvestigated the classification accuracy changes\
    \ of infested and healthy potato leaves over different \ndays post-inoculation\
    \ (DPI) with a spectroradiometer and a multispectral camera under structured \n\
    environments. Appeltans et al [9] discussed the imaging parameter settings for\
    \ hyperspectral and \nthermal proximal disease sensing in potato and leek fields\
    \ with a ground-based vehicle. Other than \nground-based platforms, an unmanned\
    \ aerial vehicle (UAV) platform equipped with imaging sensors \nalso has showed\
    \ its feasibility on field PLB monitoring with the detection of spectral changes\
    \ in crop \ntraits [10] [11]. \nDeep learning has proved to be an effective approach\
    \ for traditional computer vision problems such \nas image classification, object\
    \ detection and segmentation, as it is capable of extracting features \nhierarchically\
    \ [12]. In addition, the applications with deep learning in the agriculture domain\
    \ also \nshow unprecedented advancements. Specifically, in precision farming it\
    \ has been deployed for weed \ndetection [13], agricultural pest detection [14]\
    \ and selective fruit harvesting [15], as well as leaf \ncounting  [16]. Furthermore,\
    \ deep convolutional neural networks, one of the most used deep \nlearning algorithms,\
    \ combined with computer vision techniques have been exploited for crop disease\
    \ \nclassification and detection. Polder et al  [17] adapted a fully convolutional\
    \ neural network (FCN) for \npotato virus Y (PVY) detection based on hyperspectral\
    \ imagery. It proved that the deep learning-\nbased approach outperformed the,\
    \ conventional disease assessment and indicated the suitability of \nthis method\
    \ for real-world disease detection. Stewart et al [18] [19] [20] developed the\
    \ deep neural \nnetworks for northern leaf blight (NLB) in maize from field RGB\
    \ images collected from an unmanned \naerial vehicle (UAV) platform. By contrast,\
    \ the quantification and detection of PLB lesions have still \nbeen confined at\
    \ a laboratory scale. To the best of our knowledge, the use of deep convolutional\
    \ \nneural networks has not previously been explored for PLB lesion segmentation\
    \ in diverse potato \ngenotypes based on RGB imagery from the field. \nVisual\
    \ scoring in the field provides an important metric to quantify disease severity,\
    \ but is prone to \nbe biased and error can be subjected to raters. Thanks to\
    \ automation, effectivity and objectiveness, \nsensor-based measurement, especially\
    \ imaging sensors, provides potential advantages compared \nwith visual scoring.\
    \ Image-based analysis including images from RGB, multispectral and hyperspectral\
    \ \nsensors has measured disease severity under controlled conditions [21] or\
    \ based on PLACL \nCC-BY-NC-ND 4.0 International license.\navailable under a\n\
    (which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\n(Percentage of Leaf\
    \ Area covered by Lesions) at single leaf level [22], but is yet to demonstrate\
    \ its full \npotential for accurate estimation in field environments [23].   \n\
    The specific objectives of this study are (1) to evaluate the performances of\
    \ deep convolutional \nneural networks for PLB lesion segmentation; (2) to determinate\
    \ the optimal class weights for the \nclasses PLB disease lesion and background\
    \ (i.e. leaf and soil); (3) to fuse prediction masks at multiple \nscales for\
    \ more accurate lesion prediction; (4) to determine the correlation between visual\
    \ scoring \nand the number of lesions at the canopy level. The early pre-symptom\
    \ PLB detection is out of the \nscope of this study as only RGB images were analyzed.\
    \ \n2. Material and Methods \n2.1 Image data collection  \nThe images were acquired\
    \ with a hand-held RGB camera (Sony RX 100 iii) in nadir (+/- 5 degrees) at \n\
    approximately 40 cm over each canopy. The ISO, aperture and FOV of the camera\
    \ were set to 125, \nf/5.6 and 8.8mm, respectively. Images were acquired in full\
    \ cloudy, semi-cloudy or sunny light \nsettings. No flash was used. Especially\
    \ for the semi-cloudy and sunny acquisitions, consideration was \ntaken to ensure\
    \ no additional shade was being cast onto the canopy from person or camera.  No\
    \ \npost-processing regarding color correction was performed. \nThe field location\
    \ was outside Give, Denmark (N 55.859188, E 9.331065). Figure 1 shows a single\
    \ \nimage of the field from 100 meters above ground, at 15 Days After Infection\
    \ (DAI). It is clearly seen \nhow a large part of the trial field is decimated\
    \ by PLB. The trial was set up with guard rows of Oleva \n(cv.) around the trial\
    \ area. Each plot within the trial consisted of 4 plants in a 2x2 formation \n\
    with infector rows on each side and a 50 cm gap between plots. The total trial\
    \ consisted of 775 \ngenotypes: \n48 genotypes \nwith \nthree \nreplicates, \n\
    59 genotypes with \ntwo \nreplicates \nand \n513 genotypes represented \nby \n\
    single \nplots. Infector \nrows \nconsisted \nof \nalternating Bintje \n(cv.)\
    \ and Oleva (cv.).  \n \nCC-BY-NC-ND 4.0 International license.\navailable under\
    \ a\n(which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\n \nFigure 1. A bird’s-eye\
    \ view of the experimental trial taken from a drone platform at 100 m above \n\
    ground (a); an image example at 15 DAI (b).  \n2.2 Manual visual scoring of PLB\
    \ \nManual visual scoring was done 2 times a week, starting 5 days after inoculation\
    \ of infector rows and \ncontinuing until the standard cultivar Robijn had reached\
    \ 50% infection. Infection of PLB at each time \npoint was scored as percentage\
    \ of leaf area infected according to the Euroblight protocol [24] with a \nsingle\
    \ lesion per plot scored as 0.1%, 2-5 lesions scored as 0.5%, 5-10 lesions scored\
    \ as 1%. The \nmanual detection of lesions was done by walking between the two\
    \ rows of each plot at a slow pace \nand looking at the plants at an angle. If\
    \ a lesion was spotted, it was further investigated for PLB \npresence and the\
    \ plot were investigated for presence of more lesions. Percentage of infection\
    \ \nwas used to create an area under disease progression curve (AUDPC) as follows:\
    \ \n                                                     AUDPC =∑\nA\0\x02\x03\
    \x04\x05 \x03 \x04T\0\x02\x03\x04\x05 \x06 T\0\x02\x05 \a\n\a\n\b\t\n        \
    \                                          (1) \nwhere T0 is innoculation date\
    \ and Ti is evaluation dates (i=1,2,3,4). Ai+1 is the percentage of infection\
    \ \nat Ti+1.  \n2.3 Image preprocessing  \nThere are 70 original images (5472x3648)\
    \ which were all labelled manually with the annotation \ntoolbox LabelMe [25].\
    \ The distribution of genotypes among these images is shown in Figure 2. The \n\
    majority of images are from the Bintje and Oleva potato cultivars. These two genotypes\
    \ are \nsusceptible to P. infestans, providing more disease lesions for training.\
    \ The original image is unable to \nbe directly fed to neural networks as the\
    \ spatial resolution of original images is too large and requires \nintensive\
    \ computation memories. It is also not advisable to shrink the whole image to\
    \ a small size, \nmaking processing possible but heavily degrading the quality\
    \ of small features in tiny lesion \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nannotations.\
    \ We first cut each original image into 6x5 (horizontal x vertical) sub-images\
    \ (912x730)   \nand then resized to 512x512 in order to feed to the neural networks\
    \ for training. Each original image \ncontributes 30 sub-images leading to 2100\
    \ sub-images in total. 1600 sub-images were randomly \nselected for the training\
    \ set and the remaining 500 sub-images were randomly separated equally as \nthe\
    \ validation set (250 sub-images) or the test set (250 sub-images). The same procedures\
    \ were \napplied to their corresponding ground truth images to construct image\
    \ pairs for training neural \nnetworks. \n  \nFigure 2. Histogram of raw image\
    \ genotypes, the red bar represents the number of images belonging \nto that genotype.\
    \ \n2.4 Deep learning   \nWe adopted an encoder-decoder neural network architecture\
    \ based on SegNet [26] for lesion \nsegmentation. The proposed network operates\
    \ on input images of 512x512 pixels and outputs \nsegmentation masks in the same\
    \ size as the input image. The architecture has an hourglass shape \nconsisting\
    \ of a bunch of convolutional and up-convolutional layers. A diagrammatic overview\
    \ of the \nnetwork is shown in Figure. 3. The encoder comprises a series of convolutional\
    \ operations, activation \nand pooling operations. Semantic features from low-level\
    \ to high-level could be extracted at the end \nof the encoder process. Because\
    \ of max-pooling layers, the spatial resolution output shrunk 2 times \ncompared\
    \ to the previous convolution block in the encoder phase. The index of the maximum\
    \ \nfeature value in each pooling window was recorded for each encoder feature\
    \ map (Figure 4). In \ncontrast, the decoder part upsamples its input feature\
    \ maps using the recorded max-pooling indices \nlearned from the corresponding\
    \ layers in encoder part and finally generates the prediction results \nwith same\
    \ spatial size as original input images. The upsampling layers, such as deconvolution\
    \ layers, \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which\
    \ was not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\nin the decoder are also capable\
    \ of learning a mapping from feature map to semantic segmentation \nresults. Each\
    \ convolutional layer is followed by several operational layers including a non-linear\
    \ ReLU \nactivation function (max(0, x)) and batch normalization. The total parameters\
    \ are 29,442,122 with \n29,434,694 trainable parameters and 7,428 non-trainable\
    \ parameters. \n \nFigure 3. The neural network is based on an encoder and decoder\
    \ architecture, followed by a final \npixel-wise classification layer.  \n \n\
    Figure 4. Example of max-pooling and upsampling with index in SegNet \nTable 1.\
    \ Details of the proposed deep neural network architecture. All the filter size\
    \ is 3 x 3 and \npadding mode is set as ‘same’ with zero-filled. The final decoder\
    \ output is fed to a softmax classifier \nto produce class probabilities for each\
    \ pixel.  \nModule Layers  \nDimensions  \nFeature maps Block structure  \nInput\
    \ \nInput (RGB) \n512 x 512 \n3 \n \nEn-coder \nConvolution 1 block  \n256 x 256\
    \ \n64 \n2xconvolution + pooling \nConvolution 2 block  \n128 x 128 \n128 \n2xconvolution\
    \ + pooling \nConvolution 3 block  \n64 x 64 \n256 \n3xconvolution + pooling \n\
    Convolution 4 block \n32 x 32 \n512 \n3xconvolution + pooling \nConvolution 5\
    \ block  \n16 x 16 \n512 \n3xconvolution + pooling \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    De-coder  \nUp \nconvolution \n1 \nblock  \n32 x 32 \n512 \n3xconvolution + upsampling\
    \ \nUp \nconvolution \n2 \nblock  \n64 x 64  \n256 \n3xconvolution + upsampling\
    \ \nUP \nconvolution \n3 \nblock  \n128 x 128 \n128 \n3xconvolution + upsampling\
    \ \nUP \nconvolution \n4 \nblock  \n256 x 256  \n64 \n2xconvolution + upsampling\
    \ \nUP \nconvolution \n5 \nblock  \n512 x 512  \n64 \n2xconvolution + upsampling\
    \ \nOutput  Softmax  \n512 x 512 \n2 \nconvolution + sigmoid \n \n2.5 Loss function\
    \ \nThe loss function is key for training a robust and high-performance network.\
    \ The most commonly \nused loss function for semantic segmentation is pixel-wise\
    \ cross-entropy loss. In our study, the \nfrequency of appearance for lesion and\
    \ background class is highly imbalanced. The number of \ndisease lesion pixels\
    \ is far less than other pixels such as healthy plant organs and soil backgrounds\
    \ in \nfield images at early infection stages. Only using standard loss function\
    \ without adaption would make \na deep neural network model tend to only correctly\
    \ classify dominant class pixels (backgrounds), \nignoring the importance of lesion\
    \ pixels. This is also called accuracy paradox, which a model provides \na very\
    \ high overall accuracy but performs poorly over classes.  One common way to mitigate\
    \ this \neffect is the use of a class-balancing approach by assigning different\
    \ weights over classes based on \ntheir median frequency [27]. In this study,\
    \ we regard the weights for each class as a hyperparameter \nto tune. Other than\
    \ weights calculated from Equation (2), we also compared the prediction \nperformance\
    \ with different weight ratios from 1 to 9 to select the optimal weight. The weighted\
    \ loss \nfunction used in the network is shown in Equation (3).    \n        \
    \                                                  \b\v \t\n\f\r\x0E\b\x0F\x10\
    _\x12\x13\r\x14\x15\r\x10\v\x16\n\x12\x13\r\x14\x15\r\x10\v\x16\0\v\x05\n    \
    \                                                                     (2) \nwhere\
    \ frequency(c) represents the frequency of occurrences of pixels of class c divided\
    \ by the total \nnumber of pixels in any images containing that class, and median_frequency\
    \ is the median of these \nfrequencies overall all classes. \n               \
    \                                       \v \t \x06\n\x04\n\x17 ∑\n∑\n\b\v\n\x18\
    \n\v\t\x04\n\x03 \f\x19,\v log \x10\x19,\v\n\x17\n\b\t\x04\n                 \
    \                               (3) \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nwhere\
    \ N is the number of observations, C is the number of classes (background and\
    \ lesion), Wc is the \nweight for class c, y is a binary indicator (0 or 1) if\
    \ a class label is correctly classified for observation o, \np is predicted probability\
    \ of observation o being of class c.  \n2.6 Training  \nThe network was trained\
    \ end to end from scratch with Adam optimizer [28] using a stable learning \n\
    rate 0.0001 to minimize the loss values. The batch size was set to 18 with 500\
    \ epochs in total. We \nemployed 3 Nvidia Tesla V100-SXM2 GPUs with around 32G\
    \ memory each for training the network. \nEach epoch took around 171s to finish,\
    \ accounting for 23.75 hours of training time in total. Data \naugmentation was\
    \ used to reduce the risk of overfitting in the training phase. Specifically,\
    \ in each \nbatch, cropping, horizontal or vertical flipping, and a zoom range\
    \ from 0.8 to 1.2 were randomly \napplied in the images and their corresponding\
    \ ground truth masks. All network training and \nvalidation were done using the\
    \ Tensorflow deep learning framework. The model was saved only with \nthe decline\
    \ of loss values in each epoch. The accuracy and loss values in the validation\
    \ dataset were \nrecorded as well in every epoch. \n2.7 Model evaluation  \nThe\
    \ model was evaluated with three standard metrics for semantic segmentation. The\
    \ three metrics \nare overall average accuracy, class average accuracy and mean\
    \ intersection over union (mIoU), \nrespectively. The calculations are listed\
    \ below. We also used confusion matrix to check how many \npixels of each class\
    \ are correctly classified. Overall average accuracy (Calculation (4)) measures\
    \ the \nperformance overall all pixels. The high value means that the majority\
    \ pixels are correctly classified \nbut does not indicate good lesion segmentation\
    \ as majority of pixels in our image are background. \nHence, this value is sometimes\
    \ quite biased for evaluating model performances. Class average \naccuracy (Calculation\
    \ (5)) averages the performance of each class. A high value represents good \n\
    performance across all classes. IoU (intersection over union), also known as Jaccard\
    \ index, is a \ncommonly used and effective metric in semantic segmentation. It\
    \ measures the area of overlap \nbetween the predicted segmentation and the ground\
    \ truth divided by the union area of the \npredicted segmentation and the ground\
    \ truth in labelled images. mIoU is calculated by averaging the \nIoU of each\
    \ class (Calculation (6)).                                                   \
    \             \n                                                          \n∑\n\
    \x10\0\0\x02\n\0\x03\x04\n∑\n∑\n\x10\0\x05\n\x02\n\x05\x03\x04\n\x02\n\0\x03\x04\
    \n                                                            (4) \n         \
    \                                                   \n\x04\n\v ∑\n\x10\0\0\x1C\
    \0\v\n\b\t\x04\n                                                             \
    \ (5)    \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was\
    \ not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\n                             \
    \                       \n\x04\n\v ∑\n\x10\0\0\x1C\0\x03∑\n\x10\x05\0\x02\n\x05\
    \x03\x04\n\x1D\x10\0\0\v\n\b\t\x04\n                                         \
    \           (6)                                                 \n \nwhere ti\
    \ is the total number of pixels of class i in ground truth image, nji is the number\
    \ of pixels of \nclass j predicted as belonging to class i and c is the total\
    \ number of classes. \n \n2.8 Post-processing  \nFully connected conditional random\
    \ fields (FCCRFs) [29] were used first for post-processing the \npredicted masks.\
    \ It combines single pixel prediction and shared structure through unary and pairwise\
    \ \nterms to improve smoothness and to maximize agreement between similar neighboring\
    \ pixels. The \nFCCRFs establish pairwise potential by using a Gaussian function\
    \ on all pixel pairs in an image. The \nmain benefits of using FCCRFs are determining\
    \ the optimal decision boundary at conflict regions of \npixels, while not having\
    \ notable negative effects on successfully segmented pixels.  In post-\nprocessing\
    \ of images, the prior knowledge of P. infestans disease lesion area is relatively\
    \ small in an \nearly infection stage. We found that some false positive areas\
    \ likely represent shadowed area \nbetween leaves (Figure 9). These false lesion\
    \ areas are far larger than normal lesions appeared at \nthat early infection\
    \ stage, so a simple threshold algorithm was operated to filter out part of false\
    \ \npositives in the test images. We set a reasonable lesion area range to be\
    \ [50, 10000] pixels to exclude \nthe extreme false positives. Furthermore, the\
    \ canopy heights and structures of potato plants vary in \ntrial fields. That\
    \ means even the same lesion spots represent differently in 2D images, which can\
    \ lead \nto failed predictions. For some failed lesions the network can successfully\
    \ predict the lesions at a \ndifferent scale.  A majority voting approach for\
    \ lesion counting was proposed based on multiple \nprediction masks from various\
    \ scales. The generic pseudo-code is listed in Table 2. Each image for \nprediction\
    \ was cropped into sub-images at 7 multiple scales from 3x2 to 9x8. The sub-images\
    \ with \nthe same scale were predicted separately by the model, resulting in 7\
    \ predictions for each image. \nThe final prediction mask of an image is obtained\
    \ based on the majority voting of its 7 prediction \nmasks. \nTable 2. Pseudo-codes\
    \ of fused mask generation based on a majority voting approach \nAlgorithm: Generic\
    \ pseudo-code of lesion counting  \n1:  Input: test dataset  \n2:  For image in\
    \ test dataset do: \n3:       for i=3 to 9 do:  \n4:            cut image horizontally\
    \ i and vertically i-1, i x (i-1) sub-images generated in total;  \nCC-BY-NC-ND\
    \ 4.0 International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    5:            resize sub-images (512x512); \n6:            predict each subimage\
    \ with the optimal model, i x (i-1) sub-masks obtained in total;  \n7:       \
    \     align i x (i-1) sub-masks/sub-images and reconstruct masks/images;  \n8:\
    \            resize masks/images in original size; \n9:            save masks/images\
    \ (each image has 7 corresponding masks); \n10:      end \n11:        obtain class\
    \ label in each pixel of an image by the majority vote of the class labels in\
    \ its    \n              corresponding pixels of its 7 masks; \n12: Output: count\
    \ the number of lesions in each image; \n \n3. Results \n3.1 Network training\
    \  \nOverall, the training loss and validation loss decreased with the increment\
    \ of training time (Figure 5). \nThe validation loss fluctuated much in the early\
    \ training stage (< 150 epochs) and then slowly \nconverged at 0.0626 at the end\
    \ of the training. By contrast, the training loss smoothly dropped until \nthe\
    \ end of training and finally converged at 0.0398, slightly lower than the final\
    \ validation loss \n(0.0626). It also can be observed that both the training loss\
    \ and validation loss were substantially \nstable after 450 epochs, indicating\
    \ that the model stopped improving on a hold-out validation \ndataset. The model\
    \ weights were saved at 450 epochs to prevent the risk of overfitting. At this\
    \ epoch \npoint, the overall accuracy values in the training and validation datasets\
    \ were 0.9962 and 0.9945, \nrespectively. The same procedures were followed when\
    \ training other models with different \nhyperparameters (the weight ratio of\
    \ lesion and background) for performance comparison in the test \ndataset.  \n\
    \ \nFigure 5. Loss curves of the network (1:7 weight ratio) in the training and\
    \ validation datasets \n3.2 The weight ratio of two classes (lesion and background)\
    \  \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was not\
    \ certified by peer review) is the author/funder, who has granted bioRxiv a license\
    \ to display the preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    The weight ratio of lesion and background classes is one of the important hyperparameters\
    \ needed \nto be fine-tuned. In this study, we investigated 13 group weight ratios\
    \ in order to determine the \noptimal weight ratio for lesion semantic segmentation.\
    \ One of the weight ratios (1:2.5) was obtained \nbased on the median frequency\
    \ of two classes and the remaining weight ratios were ranging from 1 \nto 12.\
    \ The metrics in the validation dataset are shown in Table 3. It shows that the\
    \ imbalance weights \ncan effectively improve the segmentation performance as\
    \ all mIoU values exceed 0.65 compared to \nthe 0.551 obtained when the weight\
    \ ratio was set to 1:1. Interestingly, mIoU value does not continue \nto increase\
    \ with a larger weight ratio (>7). The maximum mIoU value was achieved with 1:7\
    \ weight \nratio. We selected the model with this weight ratio as the optimal\
    \ model for lesion segmentation. \nTable 3. Metrics of the models with different\
    \ class weight ratios in the validation dataset \nWeight ratio  Overall accuracy\
    \ \nClass \naverage \naccuracy  \nIoU (background) \nIoU (lesion) \nmIoU \n1:1\
    \ \n0.998 \n0.671 \n0.854 \n0.248 \n0.551 \n1:2 \n0.999 \n0.743 \n0.997 \n0.361\
    \ \n0.679 \n1:2.5 \n0.999 \n0.789 \n0.997 \n0.371 \n0.684 \n1:3 \n0.998 \n0.802\
    \ \n0.996 \n0.345 \n0.671 \n1:4 \n0.999 \n0.810 \n0.997 \n0.400 \n0.698 \n1:5\
    \ \n0.999 \n0.799 \n0.996 \n0.337 \n0.618 \n1:6 \n0.999 \n0.833 \n0.997 \n0.395\
    \ \n0.696 \n1:7 \n0.998 \n0.804 \n0.997 \n0.401 \n0.699 \n1:8 \n0.998 \n0.817\
    \ \n0.997 \n0.397 \n0.697 \n1:9 \n0.997 \n0.841 \n0.996 \n0.372 \n0.684 \n1:10\
    \ \n0.998 \n0.857 \n0.997 \n0.397 \n0.697 \n1:11 \n0.998 \n0.814 \n0.997 \n0.397\
    \ \n0.697 \n1:12 \n0.998 \n0.816 \n0.997 \n0.376 \n0.687 \n \n3.3 Test image prediction\
    \  \nSimilar to the image process for training, the original test images (5472x3648)\
    \ were first cropped and \nthen resized to sub-images (512x512) for prediction.\
    \ We used the model with 1:7 weight ratio as the \noptimal model to test the images.\
    \ Figure 6 shows confusion matrix in the validation dataset (a) and in \nthe test\
    \ dataset (b). The IoU values of background and lesion classes in the test dataset\
    \ are 0.996 and \n0.386, respectively. The metrics in the test dataset are lower\
    \ than in the validation dataset. Most \nbackground pixels (99.8%) are correctly\
    \ classified from the confusion matrix. As the majority of pixels \nCC-BY-NC-ND\
    \ 4.0 International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    are leaf, belonging to background class, they can be easily classified based on\
    \ the color differences \nwith lesion class. Around 40% of lesion pixels were\
    \ classified as being background class in the test \ndataset. The prediction examples\
    \ are illustrated in Figure 7. Generally, most lesions, marked as the \nred areas\
    \ in the images, can be correctly segmented. Some tiny lesions were failed to\
    \ be manually \nlabelled on the ground truth images, but they were successfully\
    \ segmented by the model (shown in \n#2 and #4 columns in Figure 7). \n      \
    \   \n \n                                             (a)                    \
    \  ces                                                                  (b)  \
    \       \nFigure 6. Confusion matrix in the validation dataset (a) and in the\
    \ test dataset (b). \n \n \nCC-BY-NC-ND 4.0 International license.\navailable\
    \ under a\n(which was not certified by peer review) is the author/funder, who\
    \ has granted bioRxiv a license to display the preprint in perpetuity. It is made\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version\
    \ posted August 28, 2020. The copyright holder for this preprint\nFigure 7. Examples\
    \ of sub-image predictions (512x512) in the test dataset (row #1: raw sub-images,\
    \ \nrow #2: ground truth, row #3: predicted images) \nThe prediction masks of\
    \ sub-images were reconstructed back to the predicted images of the original \n\
    test images (5472x3648). Two examples of the predicted images are shown in Figure\
    \ 8. There are \nsome examples of failed cases in some predictions. For example,\
    \ no disease lesions were visually \nobserved on potato leaves (Figure 9). But\
    \ 3 lesion areas were predicted by the model. The three false \npositives are\
    \ all from soil patches which largely have similar shape features and areas as\
    \ typical \nlesions. Moreover, these soil patches are surrounded by leaves, resulting\
    \ in more confusion for \ninference. Also, some other wrongly predicted cases\
    \ are located in the image border (#1 column in \nFigure 7). An entire lesion\
    \ can be cut with two pieces when cropping a whole image into multiple \nsub-images.\
    \ In this case, the partial lesion significantly changes the morphological features\
    \ and loses \nthe important neighbor pixel information for models to predict.\
    \ This might lead to these failure cases.  \n \nFigure 8. Examples of the predicted\
    \ raw images (5472x3648) in the test dataset and ground truth \nimages (Left column:\
    \ ground truth images, Right column: predicted images)  \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    \ \nFigure 9. False positives in a test image (5472x3648). \n3.4 Model validation\
    \ in negative examples \nWe also tested the generalization ability and validity\
    \ of the model with some difficult images \n(negative examples) without P. infestans\
    \ lesions but with tissue damages caused by biotic or abiotic \nstresses. These\
    \ images were collected from different potato fields in the summer of 2020. The\
    \ model \ndid not predict any lesions in these images in Figure 10. These damages\
    \ were from various sources \nsuch as fertilization, herbicide and pathogens other\
    \ than P. infestans. Figure 11 shows that the model \nfailed to recognize the\
    \ P. infestans lesions. Specifically, some lesions from Alternaria solani (Early\
    \ \nBlight) were recognized as being P. infestans lesions. However, the model\
    \ did not recognize damages \nin stems caused by leaf mold as P. infestans lesions,\
    \ though they have similar color features as P. \ninfestans lesions.  \nCC-BY-NC-ND\
    \ 4.0 International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    \ \nFigure 10. Correct prediction samples with damages of burning by lime nitrate\
    \ (a); with deformity \nand necrosis caused by herbicide damage (b); with damage\
    \ caused by eutrophication with lime \nnitrate (c); with infestation of possible\
    \ grey mold (d).  \n \nFigure 11. Failure cases of P. infestans lesion recognition\
    \ where some cases of  Alternaria solani \ninfections are recognized (Left); and\
    \ in an image with sever damages caused by leaf mold infestation \n(Right).  \
    \ \n3.5 Correlation between visual scores and the number of lesions at the canopy\
    \ level \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was\
    \ not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\nAs the visual scores were obtained\
    \ based on the rating of the whole plant, we only selected the \nunseen images\
    \ that covered the full crop canopy to avoid bias due to partial view. There were\
    \ 43 \nimages selected in total. The histogram of visual scores is shown in Figure\
    \ 12. The average value of \nvisual scores is 3% ranging from 0 to 40%. In order\
    \ to minimize the failure cases raised from cropping, \neach image was predicted\
    \ with multiple scales. Specifically, each image was cropped at 3x2, 4x3, 5x4,\
    \ \n6x5, 7x6, 8x7, 9x8 scales in horizontal x vertical directions. The number\
    \ of lesions in each image was \nobtained based on the majority voting algorithm\
    \ described in the section of post-processing. The \nhistogram of the final detected\
    \ lesions is displayed in Figure 13. There were 1063 lesions detected in \nthese\
    \ test images. The mean value of the detected lesion areas was 892 pixels. The\
    \ maximum and \nminimum values are 7520 and 52 pixels, respectively. More than\
    \ 40% of lesion areas are below 500 \npixels, and very few lesion areas exceed\
    \ 6000 pixels. This is consistent with the visual scores where \naround 80% of\
    \ the scores are below 5%.  \n \nFigure 11. Histogram of visual scores; the red\
    \ bar represents the number of visual scores in that x axis \nrange (y axis in\
    \ the left), and the blue line represents cumulative percentage of visual scores\
    \ (y axis in \nthe right).   \nCC-BY-NC-ND 4.0 International license.\navailable\
    \ under a\n(which was not certified by peer review) is the author/funder, who\
    \ has granted bioRxiv a license to display the preprint in perpetuity. It is made\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version\
    \ posted August 28, 2020. The copyright holder for this preprint\n \nFigure 13.\
    \ Histogram of lesion areas; the red bar represents the number of lesion areas\
    \ in that x axis \nrange (y axis in the left), and the blue line represents the\
    \ cumulative percentage of lesion areas (y \naxis in the right).   \nA linear\
    \ model was fitted, to quantify the relationship between visual scores obtained\
    \ from an \nexperienced plant breeder from the Danespo company and the number\
    \ of lesions that appeared at \ncanopy level. Figures 14 and 15 show the fitted\
    \ linear relationships between visual scores and the \nnumber of lesions predicted\
    \ from 3x2 and 5x4 scales, respectively. Figure 16 illustrates the fitted \nlinear\
    \ relationship between visual scores and the number of lesions obtained from majority\
    \ voting of \nall scales. Compared to prediction masks with only one scale, the\
    \ fused masks based on majority \nvoting achieved a better linear relationship\
    \ by increasing the R2 value from around 0.4 to 0.655. \n \n \nCC-BY-NC-ND 4.0\
    \ International license.\navailable under a\n(which was not certified by peer\
    \ review) is the author/funder, who has granted bioRxiv a license to display the\
    \ preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    Figure 14. Fitted linear relationship between visual scores and number of lesions\
    \ assigned from \nimages  from 3x2 scale.  \n \nFigure 15. Fitted linear relationship\
    \ between visual scores and number of lesions assigned from \nimages from 6x5\
    \ scale. \n \n \nFigure 16. Fitted linear relationship between visual scores and\
    \ number of lesions from majority \nvoting from all scales.  \n4. Discussion \n\
    Deep learning has demonstrated its superior performance on disease detection in\
    \ field settings \ncompared to convolutional machine learning methods [17][20][18].\
    \ Imbalance classes are a common \nproblem in deep learning and is widely represented\
    \ in the agricultural field, and thus hampers \nCC-BY-NC-ND 4.0 International\
    \ license.\navailable under a\n(which was not certified by peer review) is the\
    \ author/funder, who has granted bioRxiv a license to display the preprint in\
    \ perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    applications in for example distinction between weed and crop [13][30], pest detection\
    \ [14] and \ndisease segmentation [18]. In these cases, pixels of one class, generally\
    \ soil or crop, are dominant in \nimages. Training a network in an appropriate\
    \ way is critical for delivering a good segmentation result \nfor each class.\
    \ Table 3 shows that assigning imbalance weights in the loss function can effectively\
    \ \nmitigate the issue with imbalance classes, which is consistent with the conclusion\
    \ by Yasrab et al \n(2019) [31]. The mIOU value is only 0.551 with same weights\
    \ across classes, while it can be improved \nto nearly 0.7 with imbalance weight\
    \ assignment. The IOU value (>0.99) of the background class (soil \nand crop plant)\
    \ is far higher than the IOU value of disease lesion class. Based on the confusion\
    \ matrix, \nit is concluded that majority of lesion pixels (>60%) were correctly\
    \ classified. The relatively low IOU of \nlesion class (<0.5) indicates that a\
    \ few of false positives (Figure 9) from soil patches, which have \nsimilar shapes\
    \ to lesions were predicted by the model. Stewart et al [18] also found this kind\
    \ of false \npositives for Northern Leaf Blight (NLB) lesion segmentation in maize\
    \ fields. Besides, senesced leaves \ncould also contribute to the false positives.\
    \ Setting a threshold to filter out some false positives, \nbased on lesion size,\
    \ might improve the IOU value of lesion class. This threshold value can be \n\
    estimated based on the prior knowledge of the maximum lesion area in certain developmental\
    \ stage \nof PLB disease or plant development, related to the start of the outbreak\
    \ or even disease prediction \nbased on weather and cultivar. It should, however,\
    \ be noted that a lesion size threshold is of course \ndependent on that images\
    \ are collected at a fixed height. We drew a group of connected key points \n\
    to define the lesion area when manually labelling images with the tool LabelMe.\
    \ This way of \nannotating speeds up the pixel-wise labelling process for segmentation.\
    \ However,  it is difficult to \nalways draw a very accurate boundary line with\
    \ those key points especially since a majority of PLB \nlesions in our images\
    \ are tiny and have an irregular shape. As a consequence, the labelled lesion\
    \ \nareas at times inevitably include pixels belonging to the background class\
    \ (soil and leaf), leading to \nthe relatively low IOU of lesion class. To overcome\
    \ this problem, Wiesner-Hanks et al [19] discussed \nusing crowdsourced data for\
    \ NLB lesion detection at millimeter-level based on aerial visual images \nand\
    \ concluded that increasing the number of workers per image could improve the\
    \ quality of \nannotation polygons.  \nImage preprocessing is essential before\
    \ feeding images to train a neural network. It is encouraged to \nrandomly apply\
    \ blur, contrast and brightness as data augmentation for the benefit of model\
    \ \nrobustness. The training dataset with 1600 labelled images from 31 potato\
    \ genotypes is still limited \nand unlikely to include all lesion variations.\
    \ We tested the generalization ability and validity of model \nin recognition\
    \ of P. infestans lesions with some images from other fields and conditions (Figure\
    \ 10). \nThe failure cases featured highly similar colour and morphological characteristics\
    \ as P. infestans \nlesions still are represented. Including such failed images\
    \ in the training dataset again can improve \nCC-BY-NC-ND 4.0 International license.\n\
    available under a\n(which was not certified by peer review) is the author/funder,\
    \ who has granted bioRxiv a license to display the preprint in perpetuity. It\
    \ is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this\
    \ version posted August 28, 2020. The copyright holder for this preprint\nthe\
    \ performance [32]. The creation of further lesion variations in the training\
    \ datasets  by synthetic \nlesion images could be a good way to improve model\
    \ performances . To this end, Sun et al [33] \ndeveloped a conventional image\
    \ processing algorithm to optimize synthetic lesion images obtained \nfrom a generative\
    \ adversarial network (GAN). Cap et al [34] proposed a LeafGAN algorithm for lesion\
    \ \nimage generations, which improved the diagnostic performance by 7.4%.  \n\
    The use of a majority voting  to generate accurate lesion masks from multiple\
    \ prediction scales, \ninspired from random forest machine learning algorithm\
    \ described in [35], proved its effectiveness  \nto establish a linear relationship\
    \ between visual scores and number of lesions at canopy level (Figure \n16). Very\
    \ few studies have tried to automatize the visual scoring in field environments\
    \ for plant \nbreeding. In reality, visual scores are evaluated based on the number\
    \ of lesions and their areas on \nsingle leaflets at early infection stages, which\
    \ brings difficulties with analysis based on with 2D \nimages at the canopy level.\
    \ The lesion recognition should be further  explored by three-dimensional \n(3D)\
    \ imaging to obtain full plant structures and by employing the state-of-the-art\
    \ network \narchitecture in semantic segmentation to reduce the inference time.\
    \  \nIn precision farming, it is necessary to detect PLB disease as early as possible\
    \ to bring in appropriate \nmeasurements to avoid yield loss. As only RGB images\
    \ were used in this analysis, pre-symptomatic \ndetection of PLB is inevitable\
    \ missed. For pre-symptomatic crop disease detection, hyperspectral \nmeasurements\
    \ from spectroradiometers or spectral imaging sensors are generally employed [36].\
    \ \nFor example, Anderegg et al [37] used an ASD FieldSpec spectroradiometer (350-2500nm)\
    \ to \nmeasure wheat plants at a canopy level for Septoria Tritici Blotch (STB)\
    \ disease detection and \nquantification. Gold et al [38] measured contact leaf\
    \ reflectance with a field spectrometer for pre-\nsymptomatic PLB detection. For\
    \ many applications [39] [40] [41], spectral imaging sensors are more \npopular\
    \ than non-imaging hyperspectral sensors due to the additional capability of providing\
    \ spatial \ninformation on shape, texture and color. Partial least square discriminant\
    \ analysis (PLSDA) is \ngenerally used to process full spectral data [42]. Our\
    \ study also has the potential to monitor the \ndevelopment of PLB disease after\
    \ lesion appearance, which could be useful to screen for high PLB \nresistance\
    \ potato genotypes from a diverse germplasm in precision breeding. In terms of\
    \ PLB \nmanagement in potato production, it is useful to acknowledge how early\
    \ PLB should be detected \nafter the appearance of lesions. To address this question,\
    \ Wiik et al [43] carried out trials over two \nyears in 2018 and 2019 to test\
    \ the need of first intervention after the first visual symptom were \ndetected.\
    \ The preliminary results showed that it is acceptable for farmers to apply a\
    \ first spray with \ncurative systematic fungicides after the discovery of first\
    \ symptom,  corresponding to a very low 0.01% \ninfection provided that it is\
    \ sprayed more or less immeadiatly as it was shown that a first spray \ndelayed\
    \ by 5 days later first symptom appears was too late to stop the disease. 0.01%\
    \ infection \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which\
    \ was not certified by peer review) is the author/funder, who has granted bioRxiv\
    \ a license to display the preprint in perpetuity. It is made \nbioRxiv preprint\
    \ doi: https://doi.org/10.1101/2020.08.27.263186; this version posted August 28,\
    \ 2020. The copyright holder for this preprint\ncorresponds to  to only 300 spots/ha\
    \ if the disease is evenly spread over the field. Thus it is clear that \nprotection\
    \ against PLB for precision agriculture requires very  early detection of the\
    \ symptoms with \nRGB.   \n5. Conclusion and future work  \nIn this study, we\
    \ demonstrated the feasibility of using a deep learning algorithm based on an\
    \ \nencoder-decoder architecture for potato late blight disease lesion semantic\
    \ segmentation based on \nfield images. The results show that the intersection\
    \ over union (IoU) values of background (soil and \nleaf) and lesion classes in\
    \ the test dataset are 0.996 and 0.386, respectively. Assigning different \nweights\
    \ for imbalance class could improve the performance of the model. This work also\
    \ presents \nthe possibility of accurate lesion counting at the plant canopy level\
    \ with the use of image alignment. \nA linear relationship between visual scoring\
    \ and the number of lesions was established. We can also \nconclude that the fused\
    \ masks obtained from majority voting of the masks predicted with multiple \n\
    scales achieved higher R2 value (0.655) compared to prediction with a single scale.\
    \ The proposed \nmethodology has the potential to monitor the lesion development\
    \ under field conditions and \nevaluate the resistance of genotypes against potato\
    \ late blight enabling more precise and automated \npotato breeding. \nThis study\
    \ will be followed by further field tests and the model will continue to be tested\
    \ in terms of \nrobustness and accuracy by adding new field image datasets. The\
    \ updated model will also be used to \ntest on images collected from different\
    \ time points to predict area under disease progression curve \n(AUDPC).  In addition,\
    \ we will continuously update the models with the new labeled datasets and \n\
    synthetic images to improve the generalization ability. Multiple imaging sensors\
    \ like multispectral \nand hyperspectral cameras hold promise to also detect and\
    \ maybe even quantify pre-symptomatic \ndisease. Also, the sensor combinations,\
    \ e.g. a spectroradiometer (early stage) and high-resolution \nRGB camera (late\
    \ stage), can be considered for monitoring PLB progression. Multimodal data fusion\
    \ \nand machine learning are suggested to be fully exploited for this application.\
    \ Furthermore, it would \nbe interesting to explore new vehicle and sensor techniques\
    \ to build three-dimensional imaging to be \nable to detect disease lesions below\
    \ the canopy. \n \nAcknowledgement \nWe acknowledge the Flemish Supercomputer\
    \ Center (VSC) for providing the GPU computational \nresources and services for\
    \ this work. We thank Mathieu Gremillet for field assistance, Hanne Grethe \n\
    Kirk at Danespo for visual scoring of disease, and Linnea Almqvist from SLU for\
    \ providing image \nCC-BY-NC-ND 4.0 International license.\navailable under a\n\
    (which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\nexamples in Figures\
    \ 10 and 11. This research was founded by Nordic Council of Ministers (PPP #6P2),\
    \ \nNordForsk (#84597) and Vinnova (#2016-04386). \nReference: \n[1] \nS. Savary,\
    \ A. Ficke, J.N. Aubertot, C. Hollier, Crop losses due to diseases and their implications\
    \ \nfor global food production losses and food security, Food Secur. 4 (2012)\
    \ 519–537. \nhttps://doi.org/10.1007/s12571-012-0200-5. \n[2] \nA. Chawade, J.\
    \ Van Ham, H. Blomquist, O. Bagge, E. Alexandersson, R. Ortiz, High-throughput\
    \ \nfield-phenotyping tools for plant breeding and precision agriculture, Agronomy.\
    \ 9 (2019). \nhttps://doi.org/10.3390/agronomy9050258. \n[3] \nA.K. Mahlein, Plant\
    \ disease detection by imaging sensors – Parallels and specific demands for \n\
    precision agriculture and plant phenotyping, Plant Dis. 100 (2016) 241–254. \n\
    https://doi.org/10.1094/PDIS-03-15-0340-FE. \n[4] \nL. Colon, B. Nielsen, U. Darsow,\
    \ Field Test for Foilage Blight Resistance, 2004. \n[5] \nA.M. Abdu, M.M. Mokji,\
    \ U.U. Sheikh, A Pattern Analysis-based Segmentation to Localize Early \nand Late\
    \ Blight Disease Lesions in Digital Images of Plant Leaves, in: 2020: pp. 116–121.\
    \ \nhttps://doi.org/10.1109/icsipa45851.2019.8977798. \n[6] \nJ.G.A. Barbedo,\
    \ A new automatic method for disease symptom segmentation in digital \nphotographs\
    \ of plant leaves, Eur. J. Plant Pathol. 147 (2017) 349–364. \nhttps://doi.org/10.1007/s10658-016-1007-6.\
    \ \n[7] \nA. Camargo, J.S. Smith, An image-processing based algorithm to automatically\
    \ identify plant \ndisease visual symptoms, Biosyst. Eng. 102 (2009) 9–21. \n\
    https://doi.org/10.1016/j.biosystemseng.2008.09.030. \n[8] \nC.I. Fernández, B.\
    \ Leblon, A. Haddadi, K. Wang, J. Wang, Potato late blight detection at the \n\
    leaf and canopy levels based in the red and red-edge spectral regions, Remote\
    \ Sens. 12 (2020). \nhttps://doi.org/10.3390/RS12081292. \n[9] \nS. Appeltans,\
    \ A. Guerrero, S. Nawar, J. Pieters, A.M. Mouazen, Practical Recommendations for\
    \ \nHyperspectral and Thermal Proximal Disease Sensing in Potato and Leek Fields,\
    \ Remote Sens. \n12 (2020) 1939. https://doi.org/10.3390/rs12121939. \n[10] \n\
    R. Sugiura, S. Tsuda, S. Tamiya, A. Itoh, K. Nishiwaki, N. Murakami, Y. Shibuya,\
    \ M. Hirafuji, S. \nNuske, Field phenotyping system for the assessment of potato\
    \ late blight resistance using RGB \nimagery from an unmanned aerial vehicle,\
    \ Biosyst. Eng. 148 (2016) 1–10. \nhttps://doi.org/10.1016/j.biosystemseng.2016.04.010.\
    \ \n[11] \nM.H.D. Franceschini, H. Bartholomeus, D.F. van Apeldoorn, J. Suomalainen,\
    \ L. Kooistra, \nFeasibility of unmanned aerial vehicle optical imagery for early\
    \ detection and severity \nassessment of late blight in Potato, Remote Sens. 11\
    \ (2019). \nhttps://doi.org/10.3390/rs11030224. \n[12] \nY.A. LeCun, Y. Bengio,\
    \ G.E. Hinton, Deep learning, Nature. 521 (2015) 436–444. \nhttps://doi.org/10.1038/nature14539.\
    \ \n[13] \nJ. Gao, A.P. French, M.P. Pound, Y. He, T.P. Pridmore, J.G. Pieters,\
    \ Deep convolutional neural \nnetworks for image-based Convolvulus sepium detection\
    \ in sugar beet fields, Plant Methods. \n16 (2020). https://doi.org/10.1186/s13007-020-00570-z.\
    \ \n[14] \nZ. Liu, J. Gao, G. Yang, H. Zhang, Y. He, Localization and Classification\
    \ of Paddy Field Pests \nCC-BY-NC-ND 4.0 International license.\navailable under\
    \ a\n(which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\nusing a Saliency Map\
    \ and Deep Convolutional Neural Network, Sci. Rep. 6 (2016) 20410. \nhttps://doi.org/10.1038/srep20410.\
    \ \n[15] \nR. Barth, J. Hemming, E.J. Van Henten, Angle estimation between plant\
    \ parts for grasp \noptimisation in harvest robots, Biosyst. Eng. 183 (2019) 26–46.\
    \ \nhttps://doi.org/10.1016/j.biosystemseng.2019.04.006. \n[16] \nJ. Ubbens, M.\
    \ Cieslak, P. Prusinkiewicz, I. Stavness, The use of plant models in deep learning:\
    \ \nAn application to leaf counting in rosette plants, Plant Methods. 14 (2018).\
    \ \nhttps://doi.org/10.1186/s13007-018-0273-z. \n[17] \nG. Polder, P.M. Blok,\
    \ H.A.C. de Villiers, J.M. van der Wolf, J. Kamp, Potato Virus Y Detection in\
    \ \nSeed Potatoes Using Deep Learning on Hyperspectral Images, Front. Plant Sci.\
    \ 10 (2019) 1–13. \nhttps://doi.org/10.3389/fpls.2019.00209. \n[18] \nE.L. Stewart,\
    \ T. Wiesner-Hanks, N. Kaczmar, C. DeChant, H. Wu, H. Lipson, R.J. Nelson, M.A.\
    \ \nGore, Quantitative Phenotyping of Northern Leaf Blight in UAV Images Using\
    \ Deep Learning, \nRemote Sens. 11 (2019). https://doi.org/10.3390/rs11192209.\
    \ \n[19] \nT. Wiesner-Hanks, H. Wu, E. Stewart, C. DeChant, N. Kaczmar, H. Lipson,\
    \ M.A. Gore, R.J. \nNelson, Millimeter-Level Plant Disease Detection From Aerial\
    \ Photographs via Deep Learning \nand Crowdsourced Data, Front. Plant Sci. 10\
    \ (2019). https://doi.org/10.3389/fpls.2019.01550. \n[20] \nH. Wu, T. Wiesner-Hanks,\
    \ E.L. Stewart, C. DeChant, N. Kaczmar, M.A. Gore, R.J. Nelson, H. \nLipson, Autonomous\
    \ Detection of Plant Disease Symptoms Directly from Aerial Imagery, Plant \nPhenome\
    \ J. 2 (2019) 1–9. https://doi.org/10.2135/tppj2019.03.0006. \n[21] \nB. Laflamme,\
    \ M. Middleton, T. Lo, D. Desveaux, D.S. Guttman, Image-based quantification of\
    \ \nplant immunity and disease, Mol. Plant-Microbe Interact. 29 (2016) 919–924.\
    \ \nhttps://doi.org/10.1094/MPMI-07-16-0129-TA. \n[22] \nP. Karisto, A. Hund,\
    \ K. Yu, J. Anderegg, A. Walter, F. Mascher, B.A. McDonald, A. Mikaberidze, \n\
    Ranking quantitative resistance to septoria tritici blotch in elite wheat cultivars\
    \ using \nautomated image analysis, Phytopathology. 108 (2018) 568–581. \nhttps://doi.org/10.1094/PHYTO-04-17-0163-R.\
    \ \n[23] \nC.H. Bock, J.G.A. Barbedo, E.M. Del Ponte, D. Bohnenkamp, A.-K. Mahlein,\
    \ From visual \nestimates to fully automated sensor-based measurements of plant\
    \ disease severity: status \nand challenges for improving accuracy, Phytopathol.\
    \ Res. 2 (2020). \nhttps://doi.org/10.1186/s42483-020-00049-8. \n[24] \nU.D. Leontine\
    \ Colon, Bent Nielsen, Field test for foliage blight resistance, 2004. \n[25]\
    \ \nB.C. Russell, A. Torralba, K.P. Murphy, W.T. Freeman, LabelMe: A database\
    \ and web-based \ntool for image annotation, Int. J. Comput. Vis. 77 (2008) 157–173.\
    \ \nhttps://doi.org/10.1007/s11263-007-0090-8. \n[26] \nV. Badrinarayanan, A.\
    \ Kendall, R. Cipolla, SegNet: A Deep Convolutional Encoder-Decoder \nArchitecture\
    \ for Image Segmentation., IEEE Trans. Pattern Anal. Mach. Intell. 39 (2017) 2481–\n\
    2495. https://doi.org/10.1109/TPAMI.2016.2644615. \n[27] \nD. Eigen, R. Fergus,\
    \ Predicting depth, surface normals and semantic labels with a common \nmulti-scale\
    \ convolutional architecture, in: Proc. IEEE Int. Conf. Comput. Vis., 2015: pp.\
    \ 2650–\n2658. https://doi.org/10.1109/ICCV.2015.304. \n[28] \nD.P. Kingma, J.\
    \ Ba, Adam: A Method for Stochastic Optimization, in: Int. Conf. Learn. \nRepresent.,\
    \ 2015: pp. 1–15. \nhttps://doi.org/http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503.\
    \ \nCC-BY-NC-ND 4.0 International license.\navailable under a\n(which was not\
    \ certified by peer review) is the author/funder, who has granted bioRxiv a license\
    \ to display the preprint in perpetuity. It is made \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186;\
    \ this version posted August 28, 2020. The copyright holder for this preprint\n\
    [29] \nP. Krähenbühl, V. Koltun, Efficient Inference in Fully Connected CRFs with\
    \ Gaussian Edge \nPotentials, in: J. Shawe-Taylor, R.S. Zemel, P.L. Bartlett,\
    \ F. Pereira, K.Q. Weinberger (Eds.), Adv. \nNeural Inf. Process. Syst. 24, Curran\
    \ Associates, Inc., 2011: pp. 109–117. \nhttp://papers.nips.cc/paper/4296-efficient-inference-in-fully-connected-crfs-with-gaussian-\n\
    edge-potentials.pdf. \n[30] \nJ. Gao, W. Liao, D. Nuyttens, P. Lootens, J. Vangeyte,\
    \ A. Pižurica, Y. He, J.G. Pieters, Fusion of \npixel and object-based features\
    \ for weed mapping using unmanned aerial vehicle imagery, Int. \nJ. Appl. Earth\
    \ Obs. Geoinf. 67 (2018) 43–53. https://doi.org/10.1016/j.jag.2017.12.012. \n\
    [31] \nR. Yasrab, J.A. Atkinson, D.M. Wells, A.P. French, T.P. Pridmore, M.P.\
    \ Pound, RootNav 2.0: \nDeep learning for automatic navigation of complex plant\
    \ root architectures, Gigascience. 8 \n(2019) 1–16. https://doi.org/10.1093/gigascience/giz123.\
    \ \n[32] \nZ. Huang, E. Sklar, S. Parsons, Design of automatic strawberry harvest\
    \ robot suitable in \ncomplex environments, in: ACM/IEEE Int. Conf. Human-Robot\
    \ Interact., 2020: pp. 567–569. \nhttps://doi.org/10.1145/3371382.3377443. \n\
    [33] \nR. Sun, M. Zhang, K. Yang, J. Liu, Data enhancement for plant disease classification\
    \ using \ngenerated lesions, Appl. Sci. 10 (2020). https://doi.org/10.3390/app10020466.\
    \ \n[34] \nQ.H. Cap, H. Uga, S. Kagiwada, H. Iyatomi, LeafGAN: An Effective Data\
    \ Augmentation Method \nfor Practical Plant Disease Diagnosis, arXiv Prepr. arXiv2002.10100.\
    \ (2020). \n[35] \nL. Breiman, Random forests, Mach. Learn. 45 (2001) 5–32. \n\
    https://doi.org/10.1023/A:1010933404324. \n[36] \nA.-K. Mahlein, M.T. Kuska, J.\
    \ Behmann, G. Polder, A. Walter, Hyperspectral Sensors and \nImaging Technologies\
    \ in Phytopathology: State of the Art, Annu. Rev. Phytopathol. 56 (2018) \n535–558.\
    \ https://doi.org/10.1146/annurev-phyto-080417-050100. \n[37] \nJ. Anderegg, A.\
    \ Hund, P. Karisto, A. Mikaberidze, In-Field Detection and Quantification of \n\
    Septoria Tritici Blotch in Diverse Wheat Germplasm Using Spectral–Temporal Features,\
    \ Front. \nPlant Sci. 10 (2019). https://doi.org/10.3389/fpls.2019.01355. \n[38]\
    \ \nK.M. Gold, P.A. Townsend, A. Chlus, I. Herrmann, J.J. Couture, E.R. Larson,\
    \ A.J. Gevens, \nHyperspectral measurements enable pre-symptomatic detection and\
    \ differentiation of \ncontrasting physiological effects of late blight and early\
    \ blight in potato, Remote Sens. 12 \n(2020) 1–21. https://doi.org/10.3390/rs12020286.\
    \ \n[39] \nJ. Gao, X. Li, F. Zhu, Y. He, Application of hyperspectral imaging\
    \ technology to discriminate \ndifferent geographical origins of Jatropha curcas\
    \ L. seeds, Comput. Electron. Agric. 99 (2013) \n186–193. \n[40] \nJ. Gao, D.\
    \ Nuyttens, P. Lootens, Y. He, J.G. Pieters, Recognising weeds in a maize crop\
    \ using a \nrandom forest machine-learning algorithm and near-infrared snapshot\
    \ mosaic \nhyperspectral imagery, Biosyst. Eng. 170 (2018) 39–50. \nhttps://doi.org/10.1016/j.biosystemseng.2018.03.006.\
    \ \n[41] \nS. Appeltans, A. Guerrero, S. Nawar, J. Pieters, A.M. Mouazen, Practical\
    \ Recommendations for \nHyperspectral and Thermal Proximal Disease Sensing in\
    \ Potato and Leek Fields, Remote Sens. \n12 (2020) 1939. https://doi.org/10.3390/rs12121939.\
    \ \n[42] \nK. Yu, J. Anderegg, A. Mikaberidze, P. Karisto, F. Mascher, B.A. McDonald,\
    \ A. Walter, A. Hund, \nHyperspectral canopy sensing of wheat septoria tritici\
    \ blotch disease, Front. Plant Sci. 9 (2018). \nhttps://doi.org/10.3389/fpls.2018.01195.\
    \ \n[43] \nL. Wiik, M. Nilsson, L. Aldén, A. Gerdtsson, L.G.-B. Didymus, E. Liljeroth,\
    \ Sweden attempts trial \nCC-BY-NC-ND 4.0 International license.\navailable under\
    \ a\n(which was not certified by peer review) is the author/funder, who has granted\
    \ bioRxiv a license to display the preprint in perpetuity. It is made \nbioRxiv\
    \ preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version posted\
    \ August 28, 2020. The copyright holder for this preprint\nreport 2019, 2019.\
    \ https://sverigeforsoken.se/. \n \nCC-BY-NC-ND 4.0 International license.\navailable\
    \ under a\n(which was not certified by peer review) is the author/funder, who\
    \ has granted bioRxiv a license to display the preprint in perpetuity. It is made\
    \ \nbioRxiv preprint doi: https://doi.org/10.1101/2020.08.27.263186; this version\
    \ posted August 28, 2020. The copyright holder for this preprint\n"
  inline_citation: Gao1, Westergaard2, Høegh Riis Sundmark3, Bagge3, Alexandersson4,
    Liljeroth4, Westergaard2, Sundmark3, Bagge3, Alexandersson4, Liljeroth4, Westergaard2,
    Sundmark3, Bagge3, Alexandersson4, Liljeroth4
  journal: bioRxiv (Cold Spring Harbor Laboratory)
  key_findings: '- The automated system can accurately estimate the severity of the
    disease, quantifying the number of lesions and their area at the canopy level

    - The automated system has the potential to monitor the development of late blight
    disease under field conditions and evaluate the resistance of genotypes against
    potato late blight, enabling more precise and automated potato breeding'
  limitations: null
  main_objective: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity).
  pdf_link: https://www.biorxiv.org/content/biorxiv/early/2020/08/28/2020.08.27.263186.full.pdf
  publication_year: 2020
  relevance_evaluation: '0.9-1.0: Exceptionally relevant - Comprehensively addresses
    all key aspects of the point with highly insightful, reliable, and up-to-date
    information. A must-include for the review.'
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Deep learning, image segmentation, computer vision
  title: Automatic late blight lesion recognition and severity quantification based
    on field imagery of diverse potato genotypes by deep learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/e25070987
  analysis: '>'
  apa_citation: XPath Tutorial. (n.d.). W3Schools. Retrieved from https://www.w3schools.com/xml/xpath_intro.asp
  authors:
  - Nikita Stasenko
  - Islomjon Shukhratov
  - Maxim Savinov
  - Dmitrii Shadrin
  - Andrey Somov
  citation_count: 1
  data_sources:
  - XML document containing book information
  explanation: The provided XML is a request to get the list of all the books that
    have more than 100 pages. It uses XPath to select the books that meet this criteria,
    and returns the title, author, number of pages, and price of each book.
  full_citation: '>'
  full_text: ">\nCitation: Stasenko, N.; Shukhratov, I.;\nSavinov, M.; Shadrin, D.;\
    \ Somov, A.\nDeep Learning in Precision\nAgriculture: Artiﬁcially Generated\n\
    VNIR Images Segmentation for Early\nPostharvest Decay Prediction in\nApples. Entropy\
    \ 2023, 25, 987.\nhttps://doi.org/10.3390/e25070987\nAcademic Editors: Oleg Sergiyenko,\n\
    Wendy Flores-Fuentes, Julio Cesar\nRodriguez-Quinonez and Jesús Elías\nMiranda-Vega\n\
    Received: 5 May 2023\nRevised: 19 June 2023\nAccepted: 22 June 2023\nPublished:\
    \ 28 June 2023\nCopyright:\n© 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\n\
    This article is an open access article\ndistributed\nunder\nthe\nterms\nand\n\
    conditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nentropy\nArticle\nDeep Learning in Precision Agriculture: Artiﬁcially\
    \ Generated\nVNIR Images Segmentation for Early Postharvest Decay\nPrediction\
    \ in Apples\nNikita Stasenko 1\n, Islomjon Shukhratov 1\n, Maxim Savinov 2\n,\
    \ Dmitrii Shadrin 1,3 and Andrey Somov 1,*\n1\nSkolkovo Institute of Science and\
    \ Technology, 121205 Moscow, Russia; d.shadrin@skoltech.ru (D.S.)\n2\nSaint-Petersburg\
    \ State University of Aerospace Instrumentation (SUAI), 190000 Saint-Petersburg,\
    \ Russia\n3\nDepartment of Information Technology and Data Science, Irkutsk National\
    \ Research Technical University,\n664074 Irkutsk, Russia\n*\nCorrespondence: a.somov@skoltech.ru\n\
    Abstract: Food quality control is an important task in the agricultural domain\
    \ at the postharvest stage\nfor avoiding food losses. The latest achievements\
    \ in image processing with deep learning (DL) and\ncomputer vision (CV) approaches\
    \ provide a number of effective tools based on the image colorization\nand image-to-image\
    \ translation for plant quality control at the postharvest stage. In this article,\n\
    we propose the approach based on Generative Adversarial Network (GAN) and Convolutional\n\
    Neural Network (CNN) techniques to use synthesized and segmented VNIR imaging\
    \ data for early\npostharvest decay and fungal zone predictions as well as the\
    \ quality assessment of stored apples.\nThe Pix2PixHD model achieved higher results\
    \ in terms of VNIR images translation from RGB\n(SSIM = 0.972). Mask R-CNN model\
    \ was selected as a CNN technique for VNIR images segmentation\nand achieved 58.861\
    \ for postharvest decay zones, 40.968 for fungal zones and 94.800 for both the\n\
    decayed and fungal zones detection and prediction in stored apples in terms of\
    \ F1-score metric. In\norder to verify the effectiveness of this approach, a unique\
    \ paired dataset containing 1305 RGB and\nVNIR images of apples of four varieties\
    \ was obtained. It is further utilized for a GAN model selection.\nAdditionally,\
    \ we acquired 1029 VNIR images of apples for training and testing a CNN model.\
    \ We\nconducted validation on an embedded system equipped with a graphical processing\
    \ unit. Using\nPix2PixHD, 100 VNIR images from RGB images were generated at a\
    \ rate of 17 frames per second\n(FPS). Subsequently, these images were segmented\
    \ using Mask R-CNN at a rate of 0.42 FPS. The\nachieved results are promising\
    \ for enhancing the food study and control during the postharvest stage.\nKeywords:\
    \ GAN; CNN; precision agriculture; postharvest decay; fungi; image processing\n\
    1. Introduction\nAccording to the data provided by United Nations, the human population\
    \ has grown\nto 8 billion people [1], and it is expected to increase up to 9.8\
    \ billion by 2050 [2]. The\ngrowing population will need more sustainable and\
    \ affordable food sources. It increases\nthe importance of agriculture in the\
    \ light of sustainable development. In terms of food\nproducing and quality control,\
    \ agricultural challenges can be divided into preharvesting,\nharvesting and postharvesting\
    \ stages [3]. Each stage includes various factors that should be\ntaken into account\
    \ in order to minimize food losses. During the postharvest stage, farmers\nprimarily\
    \ concentrate on factors that impact the shelf-life of harvested products during\n\
    storage and transportation. These factors include temperature [4], humidity [5],\
    \ as well\nas the use of gases and chemicals in food containers [6,7]. Each crop\
    \ has its own number\nof factors affecting the shelf-life during the postharvest\
    \ stage, and these factors should\nbe also taken into account [8]. Disparagement\
    \ of one of these factors or violation during\nthe storage or transportation may\
    \ result in postharvest losses of food products. Examples\nof postharvest losses\
    \ in stored fruits and vegetables include decayed and spoiled areas,\nEntropy\
    \ 2023, 25, 987. https://doi.org/10.3390/e25070987\nhttps://www.mdpi.com/journal/entropy\n\
    Entropy 2023, 25, 987\n2 of 30\noften attributed to mishandling, hygiene issues,\
    \ inadequate humidity control, improper\ntemperature management, and mechanical\
    \ damages [9]. These factors contribute to the\ndeterioration and loss of quality\
    \ of stored subjects.\nApple is one of the most popular harvested and cultivated\
    \ crops. Its global production\nachieved 93 millions tonnes in 2021 [10]. It is\
    \ one of the major reasons to monitor apple\nfruits quality during all the above-mentioned\
    \ stages to prevent postharvest losses and to\navoid potential economic losses.\
    \ However, there are special factors affecting apple quality\nduring the postharvest\
    \ stage, e.g., water as loss in apple fruits [11], residual pesticides [12],\n\
    or concentration of carbon dioxide, ethylene, ethanol or ammonia surrounding apples\
    \ due\nto insufﬁcient ventilation in the storage facility [13]. The most common\
    \ non-destructive\nmethods for preventing postharvest losses include the control\
    \ of objects using RGB video\ncameras and sensors [14], near infrared (NIR) data\
    \ [15], gas sensing spectroscopy [13],\nﬂuorescence spectroscopy [16], magnetic\
    \ resonance imaging (MRI) [17], and even electronic\nnose [18]. Nevertheless,\
    \ postharvest losses are still estimated in the range of 40–50% [9]. It\nshould\
    \ be noted that the control of apple fruits at the postharvest stage is quite\
    \ comprehen-\nsive, making it difﬁcult to monitor each fruit at each step, while\
    \ any damage may lead to a\nfungi infection [19] in the stored fruits and also\
    \ to the formation (and even a rapid growth)\nof rotten areas which are also known\
    \ as decayed areas [20]. Moreover, these areas are not\nwell seen visually at\
    \ early stages, and the decay growth process can be quite dynamic [21].\nArtiﬁcial\
    \ intelligence (AI) and its domains, including machine learning (ML) and deep\n\
    learning (DL), in conjunction with the latest achievements in computer vision\
    \ (CV), remote\nsensing, wireless sensing technologies, and Internet of Things\
    \ (IoT), have provided the\nadded value in a number of application including the\
    \ space domain [22], medicine [23],\npower engineering [24], agriculture [25]\
    \ and food supply [26]. For example, farmers rely\non CV for crop quality management,\
    \ e.g., plant growth monitoring [27], fruit detection [28],\ndisease detection\
    \ [29] and weed detection [30]. It is necessary for improving the food quality\n\
    of each plant at preharvest, harvest, and postharvest stages, respectively. Also,\
    \ there is a\nset of CV-based approaches for postharvest losses estimation and\
    \ the evaluation in stored\ncrops [31–33]. However, some postharvest losses, e.g.,\
    \ fungi or postharvest decay zones,\nshould be detected immediately, since the\
    \ visible decayed or fungi zones (acquired visually\nor with RGB cameras and sensors)\
    \ in stored plants may indicate their serious spoilage if\nwe use other types\
    \ of imaging data, e.g., NIR or thermal imaging, to monitor their quality.\nThis\
    \ monitoring process requires a special device and equipment, e.g., multispectral\
    \ or\nhyperspectral cameras, which are expensive and often not easy to use, given\
    \ fast detection\nof defects is still extremely challenging.\nIn this article,\
    \ we present an approach based on the application of generative ad-\nversarial\
    \ network (GAN) and convolutional neural network (CNN) for early detection\nand\
    \ segmentation of decayed and fungi areas in stored apples at the postharvest\
    \ stage\nusing visible near-infrared (vis-NIR, or just VNIR) imaging data. We\
    \ show how artiﬁcially\ngenerated VNIR imaging data can be used for early postharvest\
    \ decay detection in stored\napples and examine whether GAN- and CNN-based approaches\
    \ can achieve promising\nresults for image segmentation tasks. The idea of the\
    \ proposed approach can be divided\ninto two parts:\n•\nGeneration of VNIR imaging\
    \ data containing the stored apples with postharvest decay\nand fungi zones using\
    \ the GAN technique.\n•\nSegmentation of generated VNIR images using the CNN technique\
    \ in order to detect\nthe decayed and fungi zones in the stored apples.\nIn this\
    \ research, we study the original and generated VNIR images containing apples\n\
    of four varieties with several treatments in order to simulate various occasions\
    \ with ap-\nples during the storage. The aim is to present an approach based on\
    \ the DL techniques\ncombining the GAN and CNN models, for instance, with segmentation\
    \ of postharvest\ndecay zones and fungi areas. The GAN model will provide the\
    \ procedure of NIR images\nsynthesis from the input RGB data, while the CNN model\
    \ is supposed to be used for the\ninstance segmentation of generated images. This\
    \ is important for the proposed approach,\nEntropy 2023, 25, 987\n3 of 30\nas\
    \ we aim to train and validate our models to detect the postharvest decay zones\
    \ and\nfungi areas separately from each other. For realizing this idea into practice,\
    \ we propose the\nfollowing stages.\nFirst, we need to select a GAN based model\
    \ for the NIR images generation from the\ninput RGB data. There are many available\
    \ networks, but for the image-to-image translation\ntasks the following architectures\
    \ Pix2Pix [34], CycleGAN [35], and Pix2PixHD [36] are\nmostly applied in agricultural\
    \ domain [37–43]. We compare Pix2Pix, CycleGAN, and\nPix2PixHD models using the\
    \ dataset containing the paired RGB and NIR images. We are\ngoing to work with\
    \ the images acquired in VNIR range since it includes the full visible\nspectrum\
    \ with an abutting portion of the infrared spectrum [44]. The paired images\n\
    collected in the visible (380–700 nm) and VNIR (400–1100 nm) ranges are required\
    \ to make\nsure that the decayed and fungal traits in stored apples are the same\
    \ for these two ranges.\nSection 3.1.1, Section 3.1.2, and Section 3.1.3 provide\
    \ detailed information about the Pix2Pix,\nCycleGAN, and Pix2PixHD models, respectively.\n\
    Second, it is necessary to choose the CNN model for the decayed and fungal areas\n\
    segmentation in the synthesized VNIR images. In this work, we implement a Mask\
    \ R-CNN\nmodel due to the Feature Pyramid Network (FPN) and ResNet101 backbone,\
    \ which allow\nfor generating the bounding boxes (object detection) and segmentation\
    \ masks (instance\nsegmentation). In [45], we have compared the Mask R-CNN to\
    \ such applied CNN-based\nmodels as U-Net [46] and Deeplab [47] for early postharvest\
    \ decay detection, and Mask R-\nCNN achieved the highest performance in terms\
    \ of average precision, namely 67.1% against\n59.7% and 56.5%, respectively. Moreover,\
    \ the Mask R-CNN model generates the bounding\nboxes and segmentation masks of\
    \ the postharvest decay and fungal zones separately from\neach other. This is\
    \ a so-called ‘a tried and tested’ method, and that is why we use Mask\nR-CNN\
    \ as a CNN-based segmentation model. We discuss the Mask R-CNN model in more\n\
    detail in Section 3.1.4.\nFinally, our plan is to implement the proposed approach\
    \ and execute it on a Single\nBoard Computer (SBC) with the AI capabilities. This\
    \ implementation will serve as an\nevaluation platform for generating segmented\
    \ VNIR images that highlight any postharvest\ndecay and fungal zones on apples.\
    \ These zones may be imperceptible to the human eye,\nbut can be detected and\
    \ selected through our system. We use NVIDIA Jetson Nano as\nan embedded system\
    \ with AI capabilities for evaluation. It is a compact and powerful\nSBC supplied\
    \ with the accelerated libraries for computer vision and deep learning applica-\n\
    tions, and is widely used for different real-time problems in agriculture including\
    \ weed\ncontrol [48], soil mapping in greenhouse [49], and harvest product detection\
    \ [50–54]. That\nis why the presented research is supposed to be an alternative\
    \ solution for the high-cost\nNIR hyperspectral devices used for the early postharvest\
    \ decay detection and prediction\nfor stored food. Figure 1 illustrates the proposed\
    \ approach.\nThe contribution of this work is as follows:\n•\nTwo experimental\
    \ testbeds for paired RGB and VNIR imaging data collection under\nvarious environmental\
    \ (temperature and humidity) conditions.\n•\nApplication of CNN models, for instance,\
    \ on the segmentation of decayed and fungi\nareas in apples at the postharvest\
    \ stage.\n•\nSeparate segmentation of fungi zones and postharvest decay areas\
    \ in stored apples\nusing the CNN model.\n•\nApplication of the trained CNN-based\
    \ model for the instance segmentation of posthar-\nvest decay zones and fungi\
    \ areas in VNIR images generated by the GAN-based model.\n•\nImplementation of\
    \ the proposed approach based on the GAN and CNN techniques\nfor postharvest decay\
    \ detection, segmentation and prediction using generated VNIR\nimaging data on\
    \ a low-cost embedded system with the AI capabilities.\nEntropy 2023, 25, 987\n\
    4 of 30\nFigure 1. Diagram summarizing the proposed approach for the application\
    \ of segmented VNIR\nimagery data via deep learning for early postharvest decay\
    \ prediction in apples.\nThis article is organized as follows: Section 2 provides\
    \ an introduction to relevant\nresearch works aimed at early postharvest decay\
    \ detection and prediction in apples using\nRGB and VNIR imaging data with the\
    \ CV and ML methods. Section 3 presents the methods\nused in this work. Section\
    \ 3.3 demonstrates the experimental testbeds used for RGB and\nVNIR imaging data\
    \ collection and describes the procedure of data annotation. Section 4\nshows\
    \ the results of the comparison of the GAN techniques applied to VNIR images\n\
    generation from the RGB ones (see Section 4.1). It also presents the application\
    \ of the\nCNN technique, for instance, on the segmentation on the generated VNIR\
    \ images (see\nSection 4.2), and describes the embedded system running the proposed\
    \ GAN and CNN (see\nSection 4.3). Conclusions and discussion of the future work\
    \ are summarized in Section 5.\n2. Related Works\n2.1. CV Approaches Based on\
    \ CNN Models Using RGB Imaging Data\nCV techniques with the implementation of\
    \ ML and DL methods are becoming one of\nthe most useful tools for fruit quality\
    \ estimation and evaluation at the postharvest stage.\nThe majority of approaches\
    \ are based on the collection and analysis of visible mor-\nphological traits,\
    \ such as changes in fruit shape, size, or color during the storage, from\nstored\
    \ fruits with CNN models using RGB images as the most acceptable and user-friendly\n\
    type of data. RGB imagery is closely similar to human vision because red, green\
    \ and\nblue are the primary colors in these color models, which makes the process\
    \ of visible non-\ndestructive quality monitoring and defect detection of stored\
    \ food production easy and\nunderstandable [55]. The majority of cameras and devices\
    \ for RGB imaging data collection\ncontain a patterned Bayer ﬁlter mosaic consisting\
    \ of squares of four pixels with one red,\none blue and two green ﬁlters [56].\
    \ Usually, the Bayer ﬁlter is located on the camera chip.\nGenerally, a CNN model\
    \ contains convolutional and pooling layers (added one by\none), ﬂatten, fully\
    \ connected layer and softmax classiﬁer. The convolutional and pooling\nlayers\
    \ are used in the features extraction part, while the classiﬁcation part involves\
    \ the\nﬂatten, fully connected layers and softmax classiﬁer. When the image reaches\
    \ the input\nlayer, a ﬁlter in the convolution layer allows it for the selection\
    \ of feature neurons. An\nactivation function (Sigmoid, Rectiﬁed Linear Unit (ReLU),\
    \ or Softplus) is added to obtain\nnonlinear results by passing feature neurons\
    \ through it, and the resulting feature map\nsize is reduced by the pooling layer\
    \ functions. The ﬂatten layer is the ﬁrst input layer for\nthe classiﬁer model\
    \ as it keeps the feature map from the convolution layers. The fully\nconnected\
    \ layer transforms the obtained feature neurons into a matrix, which performs\
    \ the\nclassiﬁcation function with a classiﬁcation method.\nIn this way, the CNN\
    \ structure showed its efﬁciency in classiﬁcation, and then in\ndetection and\
    \ segmentation tasks using RGB imaging data. For example, the automated\nEntropy\
    \ 2023, 25, 987\n5 of 30\nbanana grading system was reported in [57] where a ﬁne-tuned\
    \ VGG-16 Deep CNN model\nwas applied for banana classiﬁcation using such traits\
    \ as skin quality, size, and maturity\nwith the acquired RGB imagery data. A similar\
    \ approach was proposed in [58] where the\nVGG-16 model was trained to predict\
    \ the date of the fruit ripening stage using RGB images\nwith an overall classiﬁcation\
    \ of 96.98%.\nIn [59], the authors developed an automated online carrot grading\
    \ system, where a\nlightweight carrot defect detection network (CDDNet) based\
    \ on ShufﬂeNet [60] and transfer\nlearning was implemented for carrot quality\
    \ inspection using RGB and grayscale images.\nThe CDDNet was compared to other\
    \ CNN models including AlexNet, ResNet50, MobileNet\nv2, and ShufﬂeNet, and it\
    \ demonstrated good performance in terms of detection accuracy\nand time consuming\
    \ for binary classiﬁcation of normal and defective carrots (99.82%),\nand for\
    \ classiﬁcation of normal, bad spots, abnormal, and ﬁbrous root carrots (93.01%).\n\
    However, the images of carrots contained the carrots of different size and appearance,\
    \ and\nthe idea of the presented approach was to detect the carrots with visible\
    \ defects without\ntaking into account the spoilage stage of the defective carrots.\
    \ Moreover, there was no\nmention of a possible situation when the carrots are\
    \ infected, but still there are no visible\ntraits of spoilage.\nIn [61], the\
    \ authors report on the implementation of the DeeplabV3+ model [62] with\na classical\
    \ image processing algorithm, e.g., threshold binary segmentation, morphological\n\
    processing and mask extraction for banana bunches segmentation during sterile\
    \ bud\nremoval (SBD) on the total of 1500 RGB images. Moreover, YOLOv5-Banana\
    \ model [63]\nfor the banana ﬁngers segmentation and centroid points extraction,\
    \ while edge detection\nand centroid extraction of banana ﬁngers included binarization,\
    \ morphological opening\noperation, canny edge detection, and extracting centroid\
    \ point set. DeeplabV3 was reported\nto achieve a detection accuracy rate of 86%,\
    \ mean intersection over union (MIoU) of 0.878\nduring the debudding period for\
    \ target segmentation, and the mean pixel precision of 0.936.\nYOLOv5-Banana achieved\
    \ 76% detection accuracy rates for the banana bunches during\nthe harvest period.\
    \ The authors also designed and presented the software to estimate the\nbanana\
    \ fruit weight during the harvest period.\nIn [64], several CNN-based models including\
    \ VGG-16, VGG-19, ResNet50, ResNet101,\nand ResNet152 were compared to each other\
    \ for such physiological disorders classiﬁcation\nin stored apples as bitter pit,\
    \ shriveling, and superﬁcial scald. The authors acquired a\ndataset containing\
    \ 1080 RGB images (dataset-1) of apples and 4320 augmented images\n(dataset-2)\
    \ with the aim to improve data representation during model training and to\nconsider\
    \ apple position under the monitoring camera and lighting conditions during the\n\
    storage. The CNN-based models were used and compared for feature extraction, while\
    \ such\nclassical ML methods as support vector machines (SVM), random forest (RF),\
    \ k-nearest\nneighbors algorithm (kNN), and XGBoost were used for the extracted\
    \ features classiﬁcation.\nThe highest average accuracy was reported for the VGG-19\
    \ model in conjunction with the\nSVM method in the dataset-1 and dataset-2 with\
    \ 96.11 and 96.09%.\n2.2. Machine Learning and Deep Learning Methods for NIR Data\
    \ Analysis\nNIR spectroscopy covers spectral regions from 780 to 2500 nm that\
    \ cannot be seen\nwith human eyes, but it allows for obtaining spectral information\
    \ from ten (generally,\nreferred to as multispectral data [65]) and to more than\
    \ a hundred wavebands (referred to\nas hyperspectral data [65]). Measurements\
    \ performed in the visible (380–700 nm), visible\nnear-infrared (vis-NIR, or just\
    \ VNIR, 400–1100 nm), and NIR (780–2500 nm) ranges provide\nthe user with more\
    \ detailed information on the chemical composition of scanned samples.\nIn our\
    \ case, by samples we mean stored plants, crops and fruits. The state-of-the-art\
    \ cameras\nand devices for the hyperspectral data acquisition provide not only\
    \ spectral information\nabout the scanned samples, but also allow the users to\
    \ obtain the images of scanned zones\nin the range of device bands. Spectral information\
    \ on chemical composition from a wide\nrange of wavebands has simpliﬁed the procedure\
    \ of food quality monitoring and defect\ndetection at the postharvest stage. Moreover,\
    \ not only the decay zones may occur in stored\nEntropy 2023, 25, 987\n6 of 30\n\
    fruits, but also some fungi like Sclerotinia sclerotiorum [66], Penicillium expansum\
    \ [67], Botrytis\ncinerea [68], Botryosphaeria dothidea [69] and many others,\
    \ which should be immediately\ndetected at the early stage. Otherwise, the appearance\
    \ and growth of decayed and fungi\nzones may lead to the loss of all stored fruits.\
    \ It is vital to distinguish various types of\npostharvest losses, e.g., postharvest\
    \ decay, and diseases, e.g., various fungi varieties, since\neach type of loss\
    \ requires a special type of treatment or removal of spoiled samples from\nthe\
    \ storage. It should be noted here that the formation of fungal areas may not\
    \ always lead\nto the formation of decayed areas. That is why we should detect\
    \ and identify the fungi and\npostharvest decay zones separately from each other\
    \ [70–72].\nBoth classical ML methods and the DL techniques based on the CNN models\
    \ are\nwidely used for postharvest losses evaluation in stored plants using VNIR\
    \ and NIR imaging\nand spectral data.\nIn [73], the authors compared several ML\
    \ methods including linear discriminant\nanalysis (LDA), random forest (RF), support\
    \ vector machines (SVM), kNN, gradient tree\nboosting (GTB), and partial least\
    \ squares-discriminant analysis (PLS-DA) for early Codling\nMoth zones detection\
    \ in “Gala”, “Granny Smith”, and “Fuji” stored apples. The research\nwas carried\
    \ out at the pixel level using NIR hyperspectral reﬂectance imaging data in the\n\
    range of 900–1700 nm with an optimal selection of wavelengths. GTB was reported\
    \ to obtain\nbetter results at a pixel level classiﬁcation with 97.4% of total\
    \ accuracy for validation dataset.\nIn [74], the authors implemented the AlexNet\
    \ model for detecting pesticide residues in\npostharvest apples using hyperspectral\
    \ imaging data. There were 12,288 hyperspectral ac-\nquired images for the training\
    \ set and 6144 images for the test set in the 865.11–1711.71 nm\nrange (the camera\
    \ included 256 bands) and with 3.32 nm spectral resolution. Otsu segmen-\ntation\
    \ algorithm [75] was used for the apples and pesticide residue positioning (they\
    \ were\nthe regions of interests, or just ROIs), while deep AlexNet [76] provided\
    \ pesticide category\ndetection. AlexNet was reported to show better results in\
    \ terms of detection accuracy and\ntime consumption in comparison to the SVM and\
    \ kNN algorithms (99.09% and 0.0846 s\nagainst 74.34% and 11.2301 s, and 43.75%\
    \ and 0.7645 s, respectively).\nAs we can see, NIR hyperspectral and multispectral\
    \ imaging data ensures early disease\ndetection with more details than RGB imaging,\
    \ but also requires sophisticated equipment,\nwhich usually includes a camera\
    \ with wavebands, imaging spectrograph (or spectrometer),\nsample stage, illumination\
    \ lamps and lightning system, as well as supplementary software\nand devices for\
    \ processing and capturing NIR data and images [77–79]. However, this\nis the\
    \ reason why hyperspectral imaging devices are so expensive and may cost from\n\
    thousands to ten thousand USD [80]. These high prices reduce the availability\
    \ and usage of\nhyperspectral cameras for farmers and food selling companies to\
    \ perform food quality con-\ntrol at postharvest stages. This issue has raised\
    \ a demand for developing new approaches\nfor NIR imaging data generation without\
    \ using high cost hyperspectral systems.\n2.3. GAN-Based Models for RGB and NIR\
    \ Data Analysis\nGenerative Adversarial Networks (GANs) and, in particular, conditional\
    \ GAN\n(cGAN) [81] have demonstrated their effectiveness in a variety of tasks\
    \ in the agricul-\ntural domain including remote sensing [82], image augmentation\
    \ [83], animal farming [84],\nand plant phenotyping [85]. The general idea of\
    \ GAN is based on the usage of two neural\nnetwork models, where the ﬁrst network\
    \ is called generator (generative part, G) and its\ngoal is to create plausible\
    \ samples, while the second network is called discriminator (ad-\nversarial part,\
    \ D), and it learns to verify whether the created plausible sample is real or\n\
    fake. GANs are also applied for the so-called image-to-image translation tasks,\
    \ i.e., where\nthere is a need for high-quality image synthesis from one domain\
    \ to another. For example,\nGAN-based models were successfully applied for the\
    \ multi-channel attention selection in\nthe RGB imagery considering an external\
    \ semantic guidance in [86,87], MRI data estimation\nin [88], diffusion models\
    \ evaluation [89], and NIR imaging generation from the input RGB\nimages in [82,90,91].\n\
    Entropy 2023, 25, 987\n7 of 30\nTherefore, the approaches based on GAN models\
    \ allow synthesizing high-quality NIR\nimages from the input RGB images while\
    \ saving detailed spectral information. At the same\ntime, it is crucial not only\
    \ to transform the image together with all the relevant information,\nbut also\
    \ to segment various types of postharvest diseases and defects separately from\n\
    each other in stored food production in order to choose the speciﬁc processing\
    \ strategy\nfor defected or spoiled food samples. At present, most GAN models\
    \ provide only the\nimages transformation from one domain to another, but not\
    \ object detection or instance\nsegmentation operations in the synthesized images.\
    \ However, as shown in Section 2.1,\nCNN models demonstrate reasonably good results\
    \ for the object detection and instance\nsegmentation both for the RGB and the\
    \ NIR images.\n3. Materials and Methods\n3.1. DL Techniques\n3.1.1. Pix2Pix\n\
    The Pix2Pix model [34] is a type of cGAN that has been demonstrated on a range\
    \ of\nimage-to-image translation tasks, such as converting a satellite image to\
    \ corresponding\nmaps, or black and white photos to color images. In conditional\
    \ GANs, the generation of\nthe output image is conditional on the input image.\
    \ In the case of the Pix2Pix model, the\ngeneration process is conditional on\
    \ the source image. The discriminator covers both the\nobserved source image (domain\
    \ A) and the target image (domain B) and must determine\nwhether the target is\
    \ a plausible transformation of the source image. The generator is\ntrained via\
    \ the adversarial loss which encourages the generator to make plausible images\n\
    in the target domain. The generator is also updated via L1 loss measured between\
    \ the\ngenerated image and the expected output image. This additional loss encourages\
    \ the\ngenerator model to create the plausible translations of the source image.\
    \ Mathematically,\nthe whole process in Pix2Pix can be deﬁned as:\nLcGAN(G, D)\
    \ = Ex,y∼pdata(x,y)[logD(x, y)] + Ex,z∼pdata(x,z)[log(1 − D(x, G(x, z))]\n(1)\n\
    where G is the generator, D is the discriminator, x is the observed image, y is\
    \ the target\nimage, z is the random noise vector, and λ controls the relative\
    \ importance of the two objec-\ntives between domain A and domain B. The following\
    \ objective function is used to train the\nmodel:\nG = arg min\nG max\nD\nLcGAN(G,\
    \ D) + λLL1(G)\n(2)\nPix2Pix requires perfectly aligned paired images for the\
    \ training procedure. In this\nresearch, the CNN-based architecture is used both\
    \ as the generator and the discriminator.\nGenerally, the U-Net model [46] is\
    \ applied in Pix2Pix as a generator. U-Net trains to generate\nthe images from\
    \ the images in domain A similar to the images in domain B. The discriminator\n\
    is usually a PatchGAN (which is also known as Markovian discriminator [92]), and\
    \ it trains\nsimultaneously to distinguish the generated images from the real\
    \ images in domain B. The\nreconstruction loss measures the similarity between\
    \ the real images and the generated\nimages. Figure 2 shows the block diagram\
    \ of Pix2Pix.\nEntropy 2023, 25, 987\n8 of 30\nFigure 2. Pix2Pix block diagram.\n\
    3.1.2. CycleGAN\nThe goal of the CycleGAN model [35] is to learn the mapping G\
    \ : X → Y such that\nthe distribution of images from G(X) is indistinguishable\
    \ from the distribution Y using an\nunpaired set of image pairs. This mapping\
    \ is coupled with an inverse mapping F : Y → X\nand a cycle consistency loss introduced\
    \ to enforce F(G(X)) ≈ X and vice versa due to the\nreason that it is highly underconstrained.\
    \ For the mapping function G : X → Y and its\ndiscriminator DY\nLGAN(G, DY, X,\
    \ Y) = Ey[logDY(y)] + Ex[log(1 − DY(G(x)]\n(3)\nand the objective is as follows:\n\
    G, F = arg min\nG,F max\nDX,DY\nL(G, F, DX, DY)\n(4)\nCycleGAN learns a translation\
    \ mapping in the absence of aligned paired images. The\nimage generated from domain\
    \ A to domain B by the CNN-based generator (G1) is converted\nback to domain A\
    \ by another CNN-based generator (G2), and vice versa, in the attempt to\noptimize\
    \ the cycle-consistency loss in addition to the adversarial loss. The block diagram\n\
    of CycleGAN is shown in Figure 3.\nFigure 3. CycleGAN block diagram.\nEntropy\
    \ 2023, 25, 987\n9 of 30\n3.1.3. Pix2PixHD\nThe Pix2PixHD model [36] is a modiﬁcation\
    \ of the solution realized in the Pix2Pix\nmodel, which includes several improvements\
    \ including the Coarse-to-Fine generator, multi-\nscale discriminators, and improved\
    \ adversarial loss. Pix2PixHD generally consists of global\ngenerator G1 and local\
    \ enhancer G2 (see Figure 4, where *** are referred to the residual\nblocks).\
    \ Throughout the training process, the global generator is initially trained,\
    \ followed\nby the training of the local enhancer in a progressive manner based\
    \ on their respective\nresolutions. Subsequently, all the networks are ﬁne-tuned\
    \ jointly. The purpose of this\ngenerator is to efﬁciently combine global and\
    \ local information for the task of image\nsynthesis. Three discriminators are\
    \ used for effective detail capturing on multiple scales.\nFigure 4. Pix2PixHD\
    \ generator block diagram.\nA signiﬁcant performance boost was provided by the\
    \ loss modiﬁcation and two extra\nterms, LFM-feature matching loss and perceptual\
    \ loss, were added LVGG [93] as objective\nfunctions. The feature matching loss\
    \ performs the stabilization of the training. It happens\ndue the point that the\
    \ generator has to produce natural statistics at multiple scales:\nLFM(G, Dk)\
    \ = λFMEy,x ∑\ni=1\n1\nNi\n[||D(i)\nk (y, x) − D(i)\nk (y, G(y))||1]\n(5)\nwhere\
    \ D(i)\nk\ndenotes the output of the i-th layer of the Dk discriminator.\nLVGG\
    \ = λVGGEy,x ∑\ni=1\n1\nMi\n[||F(i)(x) − F(i)(G(y))||1]\n(6)\nwhere F(i) denotes\
    \ the i-th layer with Mi elements of the VGG network.\n3.1.4. Mask R-CNN\nMask\
    \ R-CNN [94] is a CNN-based architecture that provides the instance segmentation\n\
    of various objects in the images. These objects in images are usually called the\
    \ Regions of\nInterest (ROIs). This is the latest version of the R-CNN model [95],\
    \ where R-CNN stands\nfor Regions detected with CNN. Firstly, R-CNN has been improved\
    \ to Fast R-CNN [96],\nthen to Faster R-CNN [97], and, ﬁnally, to Mask R-CNN.\
    \ As it was mentioned earlier, in\nR-CNN based models the ROIs are detected with\
    \ the CNN feature’s selective search. In\nMask R-CNN, this selective search was\
    \ improved to Mask R-CNN by adding the Region\nProposal Network (RPN) in order\
    \ to initiate and identify the ROIs and by adding a new\nbranch for the prediction\
    \ of the mask that covers the found region, i.e., an object in the\nimage. The\
    \ RPN and ResNet101 backbone allow for making the object detection (bounding\n\
    boxes generation) and instance segmentation if there are several ROIs in one image\
    \ and\nEntropy 2023, 25, 987\n10 of 30\nthey have different sizes and partially\
    \ overlap each other. Figure 5 presents a block diagram\nof Mask R-CNN architecture.\n\
    Figure 5. Mask R-CNN block diagram.\n3.2. Performance Metrics\nIn this study,\
    \ we compare the original VNIR images with the VNIR images generated\nby the Pix2PixHD\
    \ model. To perform this, we considered the Mean Average Error (MAE),\nMean Average\
    \ Percentage Error (MAPE), Mean Squared Error (MSE), Root Mean Square\nError (RMSE),\
    \ Peak Signal to Noise Ratio (PSNR), Structural Similarity Index Measure\n(SSIM),\
    \ and Feature Similarity Index Measure (FSIM) as follows:\nMAE = 1\nn\nn\n∑\n\
    i=1\n|(yi − xi)|\n(7)\nMAPE = 100%\nn\nn\n∑\ni=1\n\f\f\f\f\n(yi − xi)\nyi\n\f\f\
    \f\f\n(8)\nMSE = 1\nn\nn\n∑\ni=1\n(yi − xi)2\n(9)\nRMSE =\ns\n1\nn\nn\n∑\ni=1\n\
    (yi − xi)2\n(10)\nPSNR = 10 log10\n \nR2\n1\nn ∑n\ni=1(yi − xi)2\n!\n(11)\nSSIM\
    \ =\n\x14\nl(xi, yi)α · c(xi, yi)β · s(xi, yi)γ\n\x15\n(12)\nFSIM =\n\x14\nSPC(xi,\
    \ yi)α · SGM(xi, yi)β\n\x15\n(13)\nwhere yi is the generated or synthesized image,\
    \ xi is the original image, n is the number\nof observations, R is the image maximum\
    \ possible pixel value, l is the luminance, c is the\ncontrast, s is the structure,\
    \ α, β, and γ are the weights, SPC is the invariant to light variation\nin images,\
    \ and SGM is the computation of image gradient.\nWe used precision, recall, mean\
    \ Intersection over Union (IoU), mean Average Precision\n(mAP), and F1-score to\
    \ verify the efﬁciency of the Mask R-CNN model on the synthesized\nVNIR pictures\
    \ during the training and validation stages, which are deﬁned as follows:\nEntropy\
    \ 2023, 25, 987\n11 of 30\nPrecision =\nTP\nTP + FP\n(14)\nRecall =\nTP\nTP +\
    \ FN\n(15)\nIoU = Area o f Overlap\nArea o f Union\n(16)\nAP = ∑\nn\n(Recalln\
    \ − Recalln−1)Precisionn\n(17)\nF1-score = 2 ∗ Precision ∗ Recall\nPrecision +\
    \ Recall\n(18)\nPrecision and recall are based on True Positives (TP), True Negatives\
    \ (TN), False\nPositives (FP), and False Negatives (FN). TP denotes instances\
    \ in which the model correctly\npredicts a speciﬁc object from a given class in\
    \ images, TN denotes the instances in which\nthe model correctly predicts an object\
    \ that does not belong to a given class, and FP denotes\nthe instances in which\
    \ the model predicts a speciﬁc class, but the object does not actually\nbelong\
    \ to that class. In contrast, FN are the cases in which the model makes no prediction\n\
    of a particular class, but the object actually belongs to one of the classes.\
    \ The object classes\nare described in Section 3.4.\nThe AP is a region that lies\
    \ beneath the precision–recall curve. The weighted mean of\nprecisions at each\
    \ IoU threshold, with the increase in recall from the preceding threshold\nas\
    \ the weight, is how AP summarizes a precision–recall curve. It is calculated\
    \ using (17),\nwhere Precisionn and Recalln are the Precision and Recall at the\
    \ n-th IoU threshold.\nThe mAP over all classes or overall IoU thresholds is calculated\
    \ with the mAP score.\nAP is averaged over all the classes. There is no distinction\
    \ between AP and mAP in this case.\nIn our scenario, since AP is averaged across\
    \ all the classes, there is no difference between\nAP and mAP. We calculated AP\
    \ values for IoU = 0.50 (AP50), for IoU = 0.75 (AP75), for the\nobjects with an\
    \ area less than 32 squared pixels (APS), for the objects with an area ranging\n\
    from 32 to 96 squared pixels (APM), and for the objects with an area higher 96\
    \ squared\npixels (APL).\n3.3. Experimental Testbeds and Data Acquisition\nIn\
    \ this section, we describe the apple fruits used for the experiments and present\n\
    experimental testbeds for data collection:\n(i) The experimental testbed for acquiring\
    \ the dataset containing paired RGB and\nVNIR images of stored apples;\n(ii) The\
    \ experimental testbed for stored apple VNIR images collection containing VNIR\n\
    images acquired by a multispectral camera.\nThe ﬁrst testbed is designed for paired\
    \ RGB and VNIR images collection in order\nto train and validate the GAN-based\
    \ DL models for VNIR images translation from RGB\nimages (see Section 3.3.1).\
    \ The second testbed is used for the stored apples VNIR images\ncollection as\
    \ well as for the CNN-based model training and validation of postharvest decay\n\
    zones detection and segmentation in the generated VNIR images (see Section 3.3.2).\n\
    3.3.1. Experimental Testbed for Paired RGB and VNIR Imaging Data Collection\n\
    We selected 16 apples of four kinds (“Delicious”, “Fuji”, “Gala”, “Reinette Simirenko”)\n\
    and divided them into four rows according to their kind (each row corresponds\
    \ to each\napple kind). Each row contained four apples of different types, where\
    \ every apple has\ndifferent treatment from left to right: an apple with no treatment,\
    \ a thoroughly washed\nand wiped apple, a mechanically damaged apple, and a shock-frozen\
    \ apple supercooled\nunder −20◦, respectively. The apple without treatment serves\
    \ as a reference for each kind.\nA thoroughly washed apple indicates the removal\
    \ of the natural protective wax layer\nEntropy 2023, 25, 987\n12 of 30\nfrom an\
    \ apple. A mechanically damaged apple imitates the wrong storing conditions. A\n\
    shock-frozen apple simulates the wrong storing conditions. Figure 6 shows these\
    \ apples.\nFigure 6. Apples selected for data collection.\nThe ﬁrst testbed is\
    \ used for data collection under the recommended room storage\nconditions. The\
    \ temperature ranges from 25 ◦C to 32 ◦C and Relative Humidity (RH) of\n34% [98].\
    \ The testbed contains aluminum frames and is 1 m in length, 1 m wide, and 1.7\
    \ m\nhigh. Apples lie on a table with a white tray at the height of 1.3 m above\
    \ the ﬂoor level.\nWe also use SLR camera Canon M50 and the multispectral camera\
    \ CMS-V1 CMS18100073\n(CMS-V) attached at the middle top of the frame and connected\
    \ to a PC laptop via the USB\nhub. The distance between the table with the apples\
    \ on top and the camera is 500 mm.\nThe lamps allowed us to simulate real storage\
    \ conditions for apples as well as perform the\ncollection of images under full\
    \ and partial illumination. Detailed information about the\nacquired dataset and\
    \ the ﬁrst experimental testbed is described in [99]. Figure 7 shows the\nﬁrst\
    \ testbed.\nFigure 7. Experimental testbed for paired RGB and VNIR image capturing.\n\
    The multispectral camera CMS-V allows acquiring images in the range of 561–838\
    \ nm,\nincluding the visible and NIR ranges. This camera imager is characterized\
    \ by the modiﬁed\nBayer matrix made of a group of 3 × 3 pixels, called macro-pixel,\
    \ ﬁltering 3 × 3 (9) spectral\nbands. The raw image delivered by the camera is\
    \ built of 9 interleaved spectral sub-images\n(8 colors + 1 Panchromatic) with\
    \ the 1280 × 1024 pixels resolution. Each RGB image re-\nlates to 9 images from\
    \ the following spectral bands channel0 = 561 nm, channel1 = 597 nm,\nchannel2\
    \ = 635 nm,\nchannel3\n=\n673\nnm,\nchannel4\n=\n724\nnm,\nchannel5 = 762 nm,\n\
    channel6 = 802 nm, channel7 = 838 nm, and channel8 (panchromatic channel) = 0\
    \ nm. The\nresolution of the nine sub-images is 426 × 339 pixels.\nWe acquired\
    \ 1305 sequential RGB images and 1305 corresponding VNIR images in\n838 nm range\
    \ to see the decay dynamics in presented apples. The examples of images are\n\
    shown in Figure 8.\nEntropy 2023, 25, 987\n13 of 30\nFigure 8. Types of images\
    \ obtained during the experiments: (A)—RGB image of apples acquired\nunder the\
    \ full illumination; (B)—VNIR image of apples acquired under the full illumination\
    \ (838 nm);\n(C)—VNIR image of apples acquired under the partial illumination\
    \ (838 nm).\n3.3.2. Experimental Testbed for VNIR Imaging Data Collection\nIn\
    \ this experiment, we selected 22 apples of the “Alesya”, “Fuji”, “Golden” and\n\
    “Reinette Simirenko” seasonal types for data acquisition. The apples were between\
    \ 8 and\n10 cm in diameter, and most of them were multicolor with red and yellow\
    \ sections. There\nwere also some apples containing fungi zones, i.e., grey-brown\
    \ moldy areas in apples, as\nthe examples of apples stored under violated storage\
    \ conditions. These apples were used\nin order to increase the data representation\
    \ for early postharvest decay detection tasks in\nthe stored apples using VNIR\
    \ imaging data. These apples are demonstrated in Figure 9.\nFigure 9. VNIR image\
    \ of apples selected for data collection.\nThe second testbed presented in Figure\
    \ 10 is a greenhouse that includes silicon frames\nand ﬁve shelves, a plastic\
    \ wrap, a multispectral camera, 10 LED strip lights with red/blue\ndiodes, a power\
    \ supply (total power is 150 Watt) for controlling the LEDs, a logger, and\na\
    \ pallet with apples. It can be used for the simulation of different processes\
    \ related to\nplant breeding in various environmental conditions including extremely\
    \ dry or wet modes.\nTemperature and humidity regulation in the testbed is provided\
    \ with the LED strip lights,\nthe plastic wrap, and several water pallets located\
    \ on three lower bottom separate shelves.\nThe silica frames are the basic elements\
    \ of a presented greenhouse characterized by\nthe following dimensions 170 cm\
    \ in height, 48 cm in length, and 67 cm in width. Two strip\nlights were ﬁxed\
    \ on each shelf while the multispectral camera and the pallet with the apples\n\
    were ﬁxed on the separate shelves (see Figure 10). Each selected strip has 60\
    \ LEDs with the\nwavelength of 650–660 nm (red light LEDs) and 455–465 nm (blue\
    \ light LEDs) for highest\nchlorophyll concentration in plants to provide the\
    \ most effective photosynthesis processes.\nThis is also fair for crops and plants\
    \ at the postharvest stages [100]. It is necessary to keep\nthe quality of plant\
    \ production which is another reason why these LED strip lights are\nused in the\
    \ greenhouse. We rely on the power supply (12 V DC, 150 W, IP33) as the energy\n\
    source for the SMD 5050 LED strip lights, and GL100-N/GL100-WL logger by Graphtech\n\
    Corporation, supplied with the GS-TH sensor module, for temperature and humidity\n\
    values registration during the data collection process.\nFor the VNIR image capturing,\
    \ the multispectral camera CMS-V described in Section 3.3.1\nwas also chosen.\
    \ The camera was connected via USB-A wire to the HP EliteBook 820 G3\nLaptop with\
    \ IntelCore i3-6100 CPU 2.30 GHz, where all the images were acquired and\nsaved\
    \ as JPG-ﬁles with 426 × 339 pixels.\nEntropy 2023, 25, 987\n14 of 30\nFigure\
    \ 10. Experimental greenhouse for data acquisition.\nWe obtained 1029 sequential\
    \ VNIR images in the 838 nm range collected from CMS-V\ncamera’s channel7. These\
    \ images were acquired under the temperature range from 35 ◦C\nto 40 ◦C and RH\
    \ equal to 70% with the goal to simulate potential violation of the storage\n\
    process of selected apples. This violation is necessary to speed up the decay\
    \ processes in\napples. We also collected 100 sequential RGB images (see the example\
    \ in Figure 11) for\nthe CNN-based model training and validation with the aim\
    \ to demonstrate the up-to-date\napproach based on the combination of pre-trained\
    \ GAN-based and CNN-based models.\nRGB sequential images had the dimensions of\
    \ 339 pixels × 426 pixels × 3 channels (or\nsimply 339 × 426 × 3).\nFigure 11.\
    \ RGB image of apples selected for data collection.\n3.4. Data Annotation\nIn\
    \ order to apply a CNN-based deep learning model for the image instance segmenta-\n\
    tion, we used the Supervisely Ecosystem [101] for annotation and labeling of VNIR\
    \ imaging\ndata. It is worth reiterating here that we provide this labeling only\
    \ for the VNIR images\nacquired with the testbed, described in Section 3.3.2 as\
    \ these images were specially collected\nas the sequential VNIR imaging dataset\
    \ for the DL model training and validation on early\npostharvest decay detection\
    \ and segmentation of apples.\nFour classes of objects in the images are deﬁned\
    \ as: Healthy apple, Decay, Fungi, and\nSpoiled apple. By the Healthy apple we\
    \ understand the apples without any visible damages\nor spoiled zones in the images.\
    \ The dark gray colored areas with the postharvest decay in\napples were indicated\
    \ as Decay. By Fungi we indicate white colored moldy zones in apples.\nHere we\
    \ distinguish the postharvest decay zones marked as the Decay class, and moldy\n\
    zones marked as the Fungi class. If an apple has objects of the Fungi class, it\
    \ means that this\napple is supposed to have been stored under the violated storage\
    \ conditions, e.g., extreme\ntemperature or humidity, which resulted in the apple’s\
    \ full spoilage. The apples with only\nEntropy 2023, 25, 987\n15 of 30\nthe postharvest\
    \ decay zones (Decay) can be sent for recycling, while apples with moldy\nzones\
    \ (Fungi) must be removed from others in order to prevent the spoilage of all\
    \ samples.\nWe also deﬁned the Spoiled apples class: there are stored apples with\
    \ more than 50 percent of\nspoiled areas (Decay objects) or moldy zones (Fungi\
    \ objects) coverage. Figure 12 illustrates\nthe procedure of image annotation.\n\
    Figure 12. The example of image annotation and objects classes in Supervisely.\n\
    4. Results and Discussion\n4.1. Image-to-Image Models Comparison for VNIR Images\
    \ Generation from RGB\nIn this section, we show the results of deep learning models\
    \ based on generative\nadversarial networks comparison for VNIR images translation\
    \ from RGB images. We\nprovide this comparison on the dataset sequential RGB images\
    \ and corresponding VNIR\nimages in the 838 nm range presented in Section 3.3.1.\
    \ To estimate the performance, we\nsplit the data into the train set (80%) and\
    \ the validation set (20%). The augmentation\ntechniques as Random Rotations,\
    \ Shifts, Zoom, and Flips are implemented to increase the\ndata representativity\
    \ and to keep the model’s efﬁciency during the training and validation\nstages.\
    \ We do not use the transformations such as Contrast/Brightness adjustments because\n\
    they may lead to the information loss from the acquired VNIR imaging data. Taking\
    \ into\naccount that the image-to-image translation is also known as the translation\
    \ from the\ndomain B to domain A (or just BtoA), it was necessary to label domain\
    \ B and domain A images\nfrom our acquired paired dataset. We identiﬁed the RGB\
    \ images as domain B and domain A\nas the VNIR images. All models were evaluated\
    \ by 200 epochs where the ﬁrst 100 were\nimplemented with the constant learning\
    \ rate and the remaining 100 with linearly decreasing\nto zero. The models training\
    \ and validation were realized via the Python scripts launched\nin Google Colab.\n\
    For the CycleGAN model, we use ResNet encoder–decoder architecture consisting\
    \ of\ntwo downsampling layers, six ResNet bottleneck blocks and two upsampling\
    \ layers. We\nalso employ an Adam optimizer with the learning rate of 0.0002 and\
    \ momentum parameters\nβ1 = 0.5 and β2 = 0.999.\nFor the Pix2Pix model training,\
    \ we ﬁxed the same parameters: batch size = 1, β1 = 0.5,\nβ2 = 0.999, and learning\
    \ rate = 0.0002. The U-Net generator had 4 downsampling blocks.\nOptimization\
    \ included the generator loss optimization step and the discriminator loss opti-\n\
    mization step, respectively. Regularization parameters are as follows: λVGG =\
    \ λFeat = 10,\nλL1 = 100.\nFor the Pix2PixHD model, we also implement the same\
    \ parameters: Adam optimizer,\nbatch size = 1, β1 = 0.5, β2 = 0.999, and learning\
    \ rate = 0.0002.\nFigure 13 shows the discriminator values of CycleGAN (Figure\
    \ 13a), Pix2Pix (Figure 13b),\nand Pix2PixHD (Figure 13c) models during the training\
    \ stage. We show the model’s\ndiscriminator losses because they show the ability\
    \ of GAN-based models to identify the\nquality of synthesized VNIR images by generator\
    \ in comparison to original VNIR images.\nEntropy 2023, 25, 987\n16 of 30\n(a)\n\
    (b)\n(c)\nFigure 13. GAN-based models evaluation: (a) CycleGAN discriminator loss\
    \ values during the\ntraining; (b) Pix2Pix discriminator loss values during the\
    \ training; and (c) Pix2PixHD discriminator\nloss values during the training.\n\
    For selected GAN-based models we see that the training stage is unstable, but\
    \ the\ndiscriminator losses tend to decrease over time. Pix2PixHD shows the lowest\
    \ loss value in\ncomparison to CycleGAN and Pix2Pix. For the models validation,\
    \ we reconstructed the\nVNIR images using model weights acquired during the training.\
    \ We used MAE, MAPE,\nEntropy 2023, 25, 987\n17 of 30\nMSE, PSNR and SSIM metrics\
    \ to estimate the quality of VNIR reconstructed images in\ncomparison with original\
    \ VNIR images. Figure 14 shows these images (with ‘cyclegan’,\n‘pix2pix’, ‘pix2pixHD’\
    \ labels, respectively) in comparison to the original VNIR image\n(‘reference’\
    \ label) via Python visualization tools.\n(a)\n(b)\nFigure 14. Examples of VNIR\
    \ generated images in comparison to original VNIR image: (a) obtained\nunder full\
    \ illumination; and (b) obtained under partial illumination.\nEntropy 2023, 25,\
    \ 987\n18 of 30\nTable 1 summarizes the results of considered models performance,\
    \ where the results\nfor Pix2PixHD model are highlighted with the black blod.\
    \ Considering both the pixel-\nbased and the image metrics, one can conclude on\
    \ the promising results. The generated\nimages look more or less similar to the\
    \ original ones. The images containing apples, overall\nlight intensity similar\
    \ to the ground truth and the decay region are mainly preserved.\nHowever, all\
    \ the models have particular artifacts. The CycleGAN model has the big\nstamp-like\
    \ artifacts and there are a lot of missed decayed zones in the apples. In terms\
    \ of\nmetrics mentioned in Section 3.2, Pix2Pix and Pix2PixHD models perform the\
    \ comparable\nand much better than others, and decay regions preserved relatively\
    \ well, although the\nintensity level mismatch can be seen. Pix2PixHD models produce\
    \ perceptually good\nimages preserving importance for task features and the mean\
    \ error level is equal to 0.6%.\nIn terms of important metrics for the image quality\
    \ estimation, such as PSNR and SSIM, the\nPix2PixHD model showed higher values\
    \ in comparison to Pix2Pix (46.859 against 46.433,\nand 0.972 against 0.955, respectively).\
    \ Taking into account the results of this comparison,\nwe decided to use the Pix2PixHD\
    \ model for VNIR images generation from RGB during the\nnext stages.\nTable 1.\
    \ Image-to-image models comparison for RGB to VNIR images generation.\nModels\n\
    MAE\nMAPE\nMSE\nPSNR\nSSIM\nCycleGAN\n0.067\n0.105\n0.01127\n27.375\n0.856\nPix2Pix\n\
    0.004\n0.006\n0.00003\n46.433\n0.955\nPix2PixHD\n0.004\n0.006\n0.00003\n46.859\n\
    0.972\n4.2. Segmentation of Generated VNIR Images for Early Postharvest Decay\
    \ Detection in Apples\nIn this section, we apply the CNN-based models for instance\
    \ segmentation of gen-\nerated VNIR images. Based on the results reported in Section\
    \ 4.1, we use the Pix2PixHD\nmodel for the VNIR image generation. The dataset\
    \ containing 456 images of stored apples\n(see Section 3.3.2) was used as the\
    \ input for trained weights of the Pix2PixHD model to\ngenerate VNIR images. The\
    \ examples of synthesized VNIR images from corresponding\ninput RGB images are\
    \ presented in Figure 15. Comparing the quality of new images with\nthe images\
    \ that were synthesizing during Pix2PixHD training stage (see Section 4.1), PSNR\n\
    and SSIM values increased from 46.859 to 52.876 and from 0.972 to 0.994, respectively.\n\
    Mask R-CNN is used as the CNN-based model for the images instance segmentation.\n\
    However, before applying Mask R-CNN to images, synthesized with Pix2PixHD, it\
    \ was\nnecessary to train Mask R-CNN on real VNIR images to detect and segment\
    \ the fungi and\ndecayed areas in stored apples. We used the labeled dataset containing\
    \ 1029 VNIR images\n(see Section 3.3.2) for Mask R-CNN model training and validation.\
    \ We report on the object\nclasses used for data labeling in Section 3.4.\nIn\
    \ this work, we implemented Mask R-CNN with the L1 as a loss function, ResNet50\n\
    as the backbone, Stochastic gradient descent (SGD) as an optimizer, and COCO weights\n\
    to use Detectron2 library [102]. GaussianNoise, RandomGamma, RandomBrightness,\
    \ and\nHorizontalFlip were applied as the data augmentation function to keep the\
    \ efﬁciency of the\nproposed model during the training and validation stages.\
    \ The model was developed in\nPython, and all calculations were realized in Google\
    \ Colab.\nIn our experiment, we apply the cross-validation for Mask R-CNN model\
    \ training on\nthe dataset containing VNIR images. Cross-validation is a widespread\
    \ technique helping\navoid the overﬁtting during the model training on big data.\
    \ In our case, we deal with\nthe sequential images, i.e., one apple can be located\
    \ in many images without any changes\nin position, which may resulted in improving\
    \ the loss value after decreasing during the\ntraining procedure. During cross-validation,\
    \ the data is usually split into several groups,\ncalled folds, where each group\
    \ is used for the training and validation one by one. For\nexample, if the dataset\
    \ is separated into three folds, the pipeline is the following: (i) the ﬁrst\n\
    fold is a validation set, the second and third folds form the train set; (ii)\
    \ the ﬁrst and the\nEntropy 2023, 25, 987\n19 of 30\nthird folds are train set,\
    \ the second fold is a validation set; and (iii) the ﬁrst and the second\nfolds\
    \ are training set, the third fold is a validation set. This pipeline is also\
    \ fair for the\ncross-validation with four and higher folds distribution. By default,\
    \ the number of folds,\nwhich is also called k-folds, is usually set equal to\
    \ ﬁve or ten, but the k-folds may be different.\nIn this work, we set the number\
    \ of folds equal to two, three, six, and nine. We show the\nmean Average Precision\
    \ values for each k-fold during Mask R-CNN models in Table 2.\nFigure 15. Examples\
    \ of synthesized VNIR images with Pix2PixHD model weights.\nEntropy 2023, 25,\
    \ 987\n20 of 30\nTable 2. Comparison of Average Precision for Mask R-CNN model.\n\
    k-Folds\nmAP\nmAP50\nmAP75\nmAPS\nmAPM\nmAPL\n2\n64.251\n90.205\n65.606\n37.202\n\
    75.980\n97.412\n3\n67.652\n90.354\n65.348\n35.400\n75.290\n96.290\n6\n67.026\n\
    90.950\n67.055\n38.188\n74.609\n98.871\n9\n67.993\n91.120\n64.871\n31.575\n75.181\n\
    97.257\nThe results for each object class segmentation (or per-category segmentation)\
    \ during\nMask R-CNN model during all folds are given in Tables 3 and 4). We also\
    \ used mAP\nand F1-score metrics to evaluate the segmentation quality during model\
    \ training for folds\ndistribution. Tables 3 and 4 present the mean mAP and F1-score\
    \ values for each fold,\nrespectively. As can be seen, the number of folds leads\
    \ to increasing of the metrics values\nand segmentation accuracy. This is a demonstration\
    \ of a cross-validation technique in\ncomparison to ordinary data splitting on\
    \ the training and validation sets. Figure 16 shows\nthe examples of VNIR images\
    \ with predicted annotations of object classes (see Section 3.4)\nacquired during\
    \ the Mask R-CNN model validation. Here we show the examples of\nsynthesized and\
    \ annotated images from k-folds = 9, as the distribution with the better mAP\n\
    and F1-score values (see the column for k-folds = 9 with black bold in the Tables\
    \ 3 and 4).\nEven though the postharvest decay zones (Decay object class in Tables\
    \ 3 and 4) and the\nfungal areas (Fungi object class in Tables 3 and 4) are detected\
    \ with small values of an F1-\nscore metric (58.861 and 40.968, respectively),\
    \ a trained Mask R-CNN model allows for the\ndetection and segmentation of spoiled\
    \ apples (Spoiled apple object class), containing either\ndecayed zones or fungal\
    \ areas, or both, with an F1-score of 94.800, which is promising.\nTable 3. Results\
    \ on per-category segmentation by Mask R-CNN using mAP metric.\nCategory\nmAP\n\
    k-Folds = 2\nk-Folds = 3\nk-Folds = 6\nk-Folds = 9\nHealthy apple\n94.785\n95.154\n\
    93.951\n98.350\nSpoiled apple\n87.839\n92.567\n93.678\n93.997\nDecay\n53.509\n\
    53.408\n54.620\n57.562\nFungi\n31.581\n30.609\n34.285\n39.967\nTable 4. Results\
    \ on per-category segmentation by Mask R-CNN using F1-score metric.\nCategory\n\
    F1-Score\nk-Folds = 2\nk-Folds = 3\nk-Folds = 6\nk-Folds = 9\nHealthy apple\n\
    95.640\n95.589\n94.799\n98.375\nSpoiled apple\n88.120\n93.134\n94.689\n94.800\n\
    Decay\n53.309\n53.213\n54.850\n58.861\nFungi\n31.686\n37.247\n35.126\n40.968\n\
    Entropy 2023, 25, 987\n21 of 30\n(a)\n(b)\nFigure 16. Comparison of object classes\
    \ annotation in real VNIR images (a,b, on the left with\n‘Annotated image’ label)\
    \ to predicted object annotations (a,b, on the right with ‘Predicted annotations’\n\
    label) during Mask R-CNN model training.\nTaking into account the results of Mask\
    \ R-CNN evaluation on real VNIR imaging data\nand the results of the Pix2PixHD\
    \ evaluation in comparison to other GAN-based models\n(see Section 4.1), we provide\
    \ the proposed pipeline for segmentation of generated VNIR\nimages. To estimate\
    \ it we acquired the dataset containing only 456 sequential RGB images\nwithout\
    \ the corresponded VNIR images (see Section 3.4). The images were acquired in\n\
    the greenhouse (see Section 3.3.2) under the same environmental conditions (temperature\n\
    range is from 35 ◦C to 40 ◦C, and RH is 70%, respectively). In order to simulate\
    \ possible\noccasion during the real storage, spoiled apples with the decayed\
    \ and fungi zones were\nadded to healthy (non-damaged) apples. The concept is\
    \ as follows: (i) we utilize a set\nof RGB images as input data; (ii) these RGB\
    \ images are passed through a GAN-based\nmodel (speciﬁcally, Pix2PixHD with pre-trained\
    \ weights in our case); (iii) VNIR images\nare generated from the input RGB images\
    \ using Pix2PixHD; and (iv) the generated VNIR\nimages are fed into a CNN-based\
    \ model (speciﬁcally, Mask R-CNN with pre-trained\nweights) to obtain these images\
    \ with predicted annotation masks. Figure 17 shows the\nexamples of images which\
    \ were synthesized and segmented with the proposed pipeline.\nAs it can be seen\
    \ in Figure 17b,c, the proposed approach helps detect and segment the\ndecayed\
    \ zones separately from the fungi zones in the stored apples. All computations\
    \ were\nalso provided in Google Colab.\nEntropy 2023, 25, 987\n22 of 30\n(a)\n\
    (b)\n(c)\nFigure 17. Synthesized VNIR images (a–c) segmentation with Mask R-CNN\
    \ model.\n4.3. Early Postharvest Decay Detection in Stored Apples Using Generated\
    \ VNIR Imaging Data on\nan Embedded System\nTo evaluate the applicability of a\
    \ GAN- and CNN-based models in real-life scenarios\nwe conduct an experiment using\
    \ the NVIDIA Jetson Nano embedded system [103]. The\ngoal of the experiment is\
    \ to validate the model’s ability to handle video streams with\nvarying frames\
    \ per second (FPS).\nEntropy 2023, 25, 987\n23 of 30\nWe used 100 RGB images.\
    \ Input RGB images are characterized by the size of 256 pixels.\nA GAN model was\
    \ used to generate VNIR images from input images and processed over\n100 images\
    \ at an average rate of 17 FPS. The generated images were then tested with\nMask\
    \ R-CNN, resulting in an average rate of 0.420 FPS. Low FPS in Mask R-CNN can\
    \ be\nattributed to its complexity compared to Pix2PixHD. As the two-stage detection\
    \ model that\nperforms instance segmentation by detecting objects and generating\
    \ pixel-level masks for\neach object, it requires more computational resources.\
    \ Figure 18 shows the examples of\nVNIR images generated and segmented using the\
    \ NVIDIA Jetson Nano based on the input\nRGB data.\n(a)\n(b)\n(c)\nFigure 18.\
    \ Generated and segmented VNIR images (a–c) using Jetson Nano.\n4.4. Discussion\n\
    In this section, we compare our results with other relevant research works in\
    \ the ﬁeld\nof application of NIR imaging data and deep learning techniques for\
    \ early postharvest\ndecay and fungal zones prediction in stored apples. The proposed\
    \ approach is based on\nthe joint application of GAN and CNN techniques for artiﬁcial\
    \ generation and subsequent\nEntropy 2023, 25, 987\n24 of 30\nsegmentation of\
    \ VNIR images. However, in order to segment the decayed and fungal zones\nin artiﬁcially\
    \ generated VNIR images, we had to train and validate a CNN technique on the\n\
    real VNIR images containing these zones in stored apples. To perform this, we\
    \ acquired the\ndataset of VNIR images (see Section 3.3) and then trained and\
    \ validated the Mask R-CNN\nmodel (see Section 4.2).\nTaking into the account\
    \ the ability of Mask R-CNN to provide the multi-class instance\nand semantic\
    \ segmentation (see Section 3.1.4), we trained the model not only to detect\n\
    and identify the quality of apple (Healthy apple or Spoiled apple, see Section\
    \ 3.4), but also to\ndetect and predict the decayed and fungal zones separately\
    \ from each other. Novelty is that\nthe model is trained and validated to identify\
    \ the quality of stored apples by taking into\naccount the presence of decayed\
    \ and fungal areas in the apples themselves. In this context,\nan apple is classiﬁed\
    \ as Spoiled apple if it contains the decayed or fungal zones, whether\nthey are\
    \ separate or combined. Conversely, if an apple does not exhibit any decayed or\n\
    fungal zones prior to storage stage, i.e., during the VNIR image collection, it\
    \ is classiﬁed as\na Healthy apple. However, if the decayed and/or fungal zones\
    \ emerge in the apple during\nthe storage stage, its classiﬁcation transitions\
    \ from a Healthy apple to Spoiled apple.\nRelevant works in this area can be classiﬁed\
    \ into three main groups according to main\ntasks: (i) defective apples detection\
    \ based on the internal quality parameters [104,105];\n(ii) early defect detection\
    \ in apples [104,106]; and (iii) early fungi detection in apples [73,107,108].\n\
    Table 5 presents a comparative study of these works.\nTable 5. Comparative table\
    \ of relevant research works.\nReferences\nTask\nNIR Images\nRange, nm\nTechnique\n\
    Metric\nValue\n[104]\nReal-time apple defect inspection\n850\nYOLO v4\nF1\n92.000\n\
    [105]\nApples surface defect segmentation\n460–842\nU-Net\nF1-score\n87.000\n\
    [105]\nApples surface defect segmentation\n460–842\nthe improved U-Net\nF1-score\n\
    91.000\n[106]\nEarly bruise detection in apples\n900–2350\nFaster R-CNN\nmAP\n\
    96.900\n[106]\nEarly bruise detection in apples\n900–2350\nYOLO v3-Tiny\nmAP\n\
    99.100\n[106]\nEarly bruise detection in apples\n900–2350\nYOLO 5s\nmAP\n99.600\n\
    [107]\nMoldy core detection in apples\n400–850\nCARS-PLS-DA model\nAccuracy\n\
    87.880\n[73]\nCodling Moth detection in apples\n900–1700\nGradient tree boosting\n\
    F1-score\n97.000\n[108]\nMoldy core detection in apples\n200–1100\nBP-ANN\nAccuracy\n\
    95.000\nThe authors applied various tools and methods based on machine learning\
    \ for de-\ntecting the defected and diseased zones in wide NIR ranges (400–2350\
    \ nm, globally) with\ndetailed spectral information on the diseased zones. The\
    \ most relevant and similar ap-\nproach to the current research is reported in\
    \ [104], where a YOLO v4 model in sorting\nmachine for real-time detection of\
    \ defects in “Red Fuji”, “Golden Delicious”, and “Granny\nSmith” apples is implemented.\
    \ The authors used the RGB and corresponded NIR images\nin the range of 850 nm\
    \ of the apples in the machine’s sorting line. Moreover, the ability of\ntrained\
    \ YOLO v4 models to detect with bounding box ‘calyx’ and ‘stem’ zones separately\n\
    from ‘defect’ zones was demonstrated. In this work, we applied the Mask R-CNN\
    \ not\nonly to detect (with bounding box) and segment (with mask) the decayed\
    \ and the fungal\nareas in stored apples, but also to identify the quality of\
    \ apples as diseased (Spoiled apple)\nif such zones are detected by the model.\
    \ F1-score and mAP values for Decay and Fungi\nzones are not that high. These\
    \ problems can be ﬁxed in our future work by obtaining more\nVNIR images containing\
    \ the fungal and the decayed areas in order to increase the data\nrepresentation\
    \ during the model validation. On the other hand, the results for Spoiled apple\n\
    (apple contains Fungi and/or Decay zones) segmentation are 98.350 and 98.375,\
    \ respectively,\nwhich is promising. Finally, the proposed approach is for an\
    \ apple quality control during\nEntropy 2023, 25, 987\n25 of 30\nthe storage stage,\
    \ i.e., before sending the stored apples to the fruit sorting machine. The\nsystem,\
    \ which could generate VNIR images without a multispectral or hyperspectral cam-\n\
    era based only on the input RGB images with segmented fungal and decayed zones,\
    \ if they\noccur in stored apples, can be applied as an additional stage for the\
    \ fruit and vegetable\ncontrol before sending them to a sorting machine.\nIn [106],\
    \ the authors compared several Faster R-CNN, YOLO v3-Tiny, and YOLO 5s\nmodels\
    \ for early decay (or bruise) detection in apples. The approach proposed in this\
    \ work\nshowed promising results in terms of the mAP metric (98.350 for Mask R-CNN\
    \ validation,\nin our case, against 96.900 for Faster R-CNN, 99.100 for YOLO v3-Tiny,\
    \ and 96.600 for\nYOLO 5s), and the selected model was trained to segment the\
    \ decayed and fungal zones in\napples, while authors in [106] trained the models\
    \ to identify and predict the apples without\n(‘No bruise’), with a small (‘Mild\
    \ bruise’) and signiﬁcant (‘Severe bruise’) decayed areas\nin apples. The authors\
    \ also acquired the NIR images in spectral range of 900–2350 nm,\nwhile in this\
    \ work the images from 838 nm range were used in order to make sure that the\n\
    diseased zones in VNIR images are visible in the RGB images as well.\nIn [105],\
    \ the authors trained and validated U-Net and the improved U-Net model for\nthe\
    \ defect segmentation in VNIR images of apples. In this work, we have demonstrated\n\
    the semantic segmentation of decayed and fungal areas with an advanced experimental\n\
    methodology. We simulated ordinary and extreme storage conditions during the paired\n\
    RGB and VNIR images collection procedures. Taking this into account, we achieved\
    \ a\nrelevant value for the diseased apples segmentation in terms of the F1-score\
    \ metric.\nWe have demonstrated the potential for the postharvest decay and fungi\
    \ prediction\nfor stored apples. However, it can be scaled to other crops that\
    \ are widely used in food\nproduction, e.g., carrots, tomatoes, cucumber, fruits\
    \ or bananas. For example, the system\nthat allows the generation and segmentation\
    \ of VNIR images can be applied for segmenta-\ntion and prediction of such fungi\
    \ as Sclerotinia sclerotiorum or Botrytis cinerea. ’Sclerotinia’\nand ’Botrytis’\
    \ fungal zones have similar morphology and, if they occur in plants, it is a\n\
    nontrivial task to identify one fungi variety from another one using only RGB\
    \ imagery or\nvisual estimation of the internal fungal traits with human eyes\
    \ [109]. The system supplied\nwith the trained and validated DL technique based\
    \ on the GAN and CNN models can\nassist the user with the additional spectral\
    \ information about each fungi acquired from the\ngenerated VNIR images. It is\
    \ useful for more precise antifungal activities during the food\nquality control.\n\
    Another potential scenario is the application of the proposed research for the\
    \ prehar-\nvest diseases and the defect detection for the plants both growing\
    \ in natural environments\nand in artiﬁcially controlled systems. For example,\
    \ it can be a robot moving platform or\nunmanned aerial vehicle without a hyperspectral\
    \ camera, but with an embedded system\nthat may generate and segment the NIR imaging\
    \ data from the input RGB one. However,\nDL technique should be trained, tested\
    \ and validated precisely, as the proposed system has\nto detect and segment not\
    \ only the diseased plants from the healthy ones, but also to detect\nthe kind\
    \ of defect (damage, decay, fungi variety) with the following suggestion of spoiled\n\
    fruit processing.\n5. Conclusions\nNIR imagery provides detailed information about\
    \ the diseased areas in stored fruits,\nwhich is why the hyperspectral cameras\
    \ containing thousands of bands are used for food\nquality monitoring at postharvest\
    \ stages. However, hyperspectral devices are expensive\nand are not friendly for\
    \ the farmers and sellers’ usage. In this article, we have presented the\napproach\
    \ based on the GAN and CNN DL techniques for early postharvest decay zones\nand\
    \ fungi areas detection and prediction in stored apples using synthesized and\
    \ segmented\nVNIR images.\nEntropy 2023, 25, 987\n26 of 30\nThe conclusions of\
    \ this work are as follows:\n•\nThe analysis of Pix2Pix, CycleGAN, and Pix2PixHD\
    \ models, which are widely used\nGAN techniques, and their application to a dataset\
    \ containing paired 1305 sequential\nRGB images and 1305 sequential VNIR images\
    \ of stored apples of different varieties\nand various pre-treatments. The images\
    \ were acquired under the full and partial\nillumination with the goal to simulate\
    \ real storage conditions.\n•\nComparison of the real VNIR images with the VNIR\
    \ images synthesized by selected\nGAN based models. The VNIR images generated\
    \ via Pix2PixHD a 0.972 score for the\nSSIM metric.\n•\nThe training and test\
    \ of Mask R-CNN on another dataset containing only 1029 sequen-\ntial VNIR images\
    \ of apples under violated storage conditions. Within this test, an\nF1-score\
    \ of 58.861 is achieved for the postharvest decay zones and F1-score 40.968 for\n\
    the fungal zones detection. The spoiled apples with the decayed and fungal zones\
    \ are\ndetected and segmented with F1-score 94.800.\n•\nTesting of the proposed\
    \ solution on an embedded system with AI capabilities. We\nused 100 RGB images\
    \ of stored apples as an input data for NVIDIA Jetson Nano, and\nthe time processing\
    \ of VNIR images generation by Pix2PixHD showed 17 FPS. The\ndetection and segmentation\
    \ by Mask R-CNN achieved 0.42 FPS.\nThe proposed approach is a promising solution\
    \ able to substitute expensive hyper-\nspectral imaging devices for early postharvest\
    \ decay prediction tasks in postharvest food\nquality control.\nAuthor Contributions:\
    \ Conceptualization, N.S., D.S. and A.S.; methodology, N.S.; software, N.S. ans\n\
    I.S.; validation, N.S., I.S. and M.S.; formal analysis, N.S. and D.S.; investigation,\
    \ N.S.; resources, N.S.;\ndata curation, N.S.; writing—original draft preparation,\
    \ N.S., I.S.; writing—review and editing, N.S.,\nI.S., M.S., D.S. and A.S.; visualization,\
    \ N.S. and I.S.; supervision, A.S. All authors have read and\nagreed to the published\
    \ version of the manuscript.\nFunding: This research received no external funding\n\
    Institutional Review Board Statement: Not applicable.\nData Availability Statement:\
    \ Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict of interest.\n\
    Abbreviations\nThe following abbreviations are used in this manuscript:\nNIR\n\
    Near Infrared Image\nVNIR\nVisible Near Infrared Image\nAI\nArtiﬁcial Intelligence\n\
    CV\nComputer Vision\nML\nMachine Learning\nSVM\nSupport Vector Machines\nRF\n\
    Random Forest\nkNN\nK-Nearest Neighbors Algorithm\nGTB\nGradient Tree Boosting\n\
    DL\nDeep Learning\nCNN\nConvolutional Neural Network\nGAN\nGenerative Adversarial\
    \ Network\nROI\nRegions of Interests\nSBC\nSingle Board Computer\nRH\nRelative\
    \ Humidity\nEntropy 2023, 25, 987\n27 of 30\nReferences\n1.\nUnited Nations Data\
    \ about Current World Population. Available online: https://www.worldometers.info/world-population/\n\
    (accessed on 26 June 2023).\n2.\nUnited Nations Data on Current and Prospected\
    \ World Population. Available online: https://population.un.org/wpp/Graphs/\n\
    Probabilistic/POP/TOT/900 (accessed on 26 June 2023).\n3.\nUllah, S.; Hashmi,\
    \ M.; Lee, J.; Youk, J.H.; Kim, I.S. Recent Advances in Pre-harvest, Post-harvest,\
    \ Intelligent, Smart, Active, and\nMultifunctional Food Packaging. Fibers Polym.\
    \ 2022, 23, 2063–2074. [CrossRef]\n4.\nCoradi, P.C.; Maldaner, V.; Lutz, É.; da\
    \ Silva Daí, P.V.; Teodoro, P.E. Inﬂuences of drying temperature and storage conditions\
    \ for\npreserving the quality of maize postharvest on laboratory and ﬁeld scales.\
    \ Sci. Rep. 2020, 10, 22006. [CrossRef] [PubMed]\n5.\nMohammed, M.; Alqahtani,\
    \ N.; El-Shaﬁe, H. Development and evaluation of an ultrasonic humidiﬁer to control\
    \ humidity in a\ncold storage room for postharvest quality management of dates.\
    \ Foods 2021, 10, 949. [CrossRef] [PubMed]\n6.\nSun, X.; Baldwin, E.; Bai, J.\
    \ Applications of gaseous chlorine dioxide on postharvest handling and storage\
    \ of fruits and\nvegetables—A review. Food Control 2019, 95, 18–26. [CrossRef]\n\
    7.\nYahia, E.M.; Fonseca, J.M.; Kitinoja, L. Postharvest losses and waste. In\
    \ Postharvest Technology of Perishable Horticultural Commodities;\nElsevier: Amsterdam,\
    \ The Netherlands, 2019; pp. 43–69.\n8.\nPalumbo, M.; Attolico, G.; Capozzi, V.;\
    \ Cozzolino, R.; Corvino, A.; de Chiara, M.L.V.; Pace, B.; Pelosi, S.; Ricci,\
    \ I.; Romaniello, R.; et al.\nEmerging Postharvest Technologies to Enhance the\
    \ Shelf-Life of Fruit and Vegetables: An Overview. Foods 2022, 11, 3925. [CrossRef]\n\
    9.\nElik, A.; Yanik, D.K.; Istanbullu, Y.; Guzelsoy, N.A.; Yavuz, A.; Gogus, F.\
    \ Strategies to reduce post-harvest losses for fruits and\nvegetables. Strategies\
    \ 2019, 5, 29–39.\n10.\nFAO Data on Global Apple Production. Available online:\
    \ https://www.fao.org/faostat/en/#data/QCL/visualize (accessed on\n26 June 2023).\n\
    11.\nHarker, F.; Feng, J.; Johnston, J.; Gamble, J.; Alavi, M.; Hall, M.; Chheang,\
    \ S. Inﬂuence of postharvest water loss on apple quality:\nThe use of a sensory\
    \ panel to verify destructive and non-destructive instrumental measurements of\
    \ texture. Postharvest Biol.\nTechnol. 2019, 148, 32–37. [CrossRef]\n12.\nde Andrade,\
    \ J.C.; Galvan, D.; Effting, L.; Tessaro, L.; Aquino, A.; Conte-Junior, C.A. Multiclass\
    \ Pesticide Residues in Fruits\nand Vegetables from Brazil: A Systematic Review\
    \ of Sample Preparation Until Post-Harvest. Crit. Rev. Anal. Chem. 2021, 1–23.\n\
    Available online: https://www.tandfonline.com/doi/abs/10.1080/10408347.2021.2013157\
    \ (accessed on 26 June 2023).\n13.\nBratu, A.M.; Petrus, M.; Popa, C. Monitoring\
    \ of post-harvest maturation processes inside stored fruit using photoacoustic\
    \ gas\nsensing spectroscopy. Materials 2020, 13, 2694. [CrossRef]\n14.\nSottocornola,\
    \ G.; Baric, S.; Nocker, M.; Stella, F.; Zanker, M. Picture-based and conversational\
    \ decision support to diagnose\npost-harvest apple diseases. Expert Syst. Appl.\
    \ 2022, 189, 116052. [CrossRef]\n15.\nMalvandi, A.; Feng, H.; Kamruzzaman, M.\
    \ Application of NIR spectroscopy and multivariate analysis for Non-destructive\
    \ evaluation\nof apple moisture content during ultrasonic drying. Spectrochim.\
    \ Acta Part A Mol. Biomol. Spectrosc. 2022, 269, 120733. [CrossRef]\n16.\nSchlie,\
    \ T.P.; Dierend, W.; Koepcke, D.; Rath, T. Detecting low-oxygen stress of stored\
    \ apples using chlorophyll ﬂuorescence\nimaging and histogram division. Postharvest\
    \ Biol. Technol. 2022, 189, 111901. [CrossRef]\n17.\nWang, L.; Huang, J.; Li,\
    \ Z.; Liu, D.; Fan, J. A review of the polyphenols extraction from apple pomace:\
    \ Novel technologies and\ntechniques of cell disintegration. Crit. Rev. Food Sci.\
    \ Nutr. 2022, 1–14. [CrossRef] [PubMed]\n18.\nWu, X.; Fauconnier, M.L.; Bi, J.\
    \ Characterization and Discrimination of Apples by Flash GC E-Nose: Geographical\
    \ Regions and\nBotanical Origins Studies in China. Foods 2022, 11, 1631. [CrossRef]\
    \ [PubMed]\n19.\nBiasi, A.; Zhimo, V.Y.; Kumar, A.; Abdelfattah, A.; Salim, S.;\
    \ Feygenberg, O.; Wisniewski, M.; Droby, S. Changes in the fungal\ncommunity assembly\
    \ of apple fruit following postharvest application of the yeast biocontrol agent\
    \ Metschnikowia fructicola.\nHorticulturae 2021, 7, 360. [CrossRef]\n20.\nBartholomew,\
    \ H.P.; Lichtner, F.J.; Bradshaw, M.; Gaskins, V.L.; Fonseca, J.M.; Bennett, J.W.;\
    \ Jurick, W.M. Comparative Penicillium\nspp. Transcriptomics: Conserved Pathways\
    \ and Processes Revealed in Ungerminated Conidia and during Postharvest Apple\n\
    Fruit Decay. Microorganisms 2022, 10, 2414. [CrossRef]\n21.\nMorales-Cedeno, L.R.;\
    \ del Carmen Orozco-Mosqueda, M.; Loeza-Lara, P.D.; Parra-Cota, F.I.; de Los Santos-Villalobos,\
    \ S.; Santoyo,\nG. Plant growth-promoting bacterial endophytes as biocontrol agents\
    \ of pre-and post-harvest diseases: Fundamentals, methods\nof application and\
    \ future perspectives. Microbiol. Res. 2021, 242, 126612. [CrossRef]\n22.\nNikparvar,\
    \ B.; Thill, J.C. Machine learning of spatial data. ISPRS Int. J. Geo-Inf. 2021,\
    \ 10, 600. [CrossRef]\n23.\nZhang, Y.; Liu, M.; Yu, F.; Zeng, T.; Wang, Y. An\
    \ o-shape neural network with attention modules to detect junctions in biomedical\n\
    images without segmentation. IEEE J. Biomed. Health Inform. 2021, 26, 774–785.\
    \ [CrossRef]\n24.\nZhao, S.; Blaabjerg, F.; Wang, H. An overview of artiﬁcial\
    \ intelligence applications for power electronics. IEEE Trans. Power\nElectron.\
    \ 2020, 36, 4633–4658. [CrossRef]\n25.\nMeshram, V.; Patil, K.; Meshram, V.; Hanchate,\
    \ D.; Ramkteke, S. Machine learning in agriculture domain: A state-of-art survey.\n\
    Artif. Intell. Life Sci. 2021, 1, 100010. [CrossRef]\n26.\nKakani, V.; Nguyen,\
    \ V.H.; Kumar, B.P.; Kim, H.; Pasupuleti, V.R. A critical review on computer vision\
    \ and artiﬁcial intelligence in\nfood industry. J. Agric. Food Res. 2020, 2, 100033.\
    \ [CrossRef]\n27.\nRasti, S.; Bleakley, C.J.; Holden, N.; Whetton, R.; Langton,\
    \ D.; O’Hare, G. A survey of high resolution image processing techniques\nfor\
    \ cereal crop growth monitoring. Inf. Process. Agric. 2022, 9, 300–315. [CrossRef]\n\
    Entropy 2023, 25, 987\n28 of 30\n28.\nTang, Y.; Qiu, J.; Zhang, Y.; Wu, D.; Cao,\
    \ Y.; Zhao, K.; Zhu, L. Optimization strategies of fruit detection to overcome\
    \ the challenge\nof unstructured background in ﬁeld orchard environment: A review.\
    \ Precis. Agric. 2023, 24, 1183–1219. [CrossRef]\n29.\nOuhami, M.; Haﬁane, A.;\
    \ Es-Saady, Y.; El Hajji, M.; Canals, R. Computer vision, IoT and data fusion\
    \ for crop disease detection\nusing machine learning: A survey and ongoing research.\
    \ Remote Sens. 2021, 13, 2486. [CrossRef]\n30.\nWu, Z.; Chen, Y.; Zhao, B.; Kang,\
    \ X.; Ding, Y. Review of weed detection methods based on computer vision. Sensors\
    \ 2021, 21, 3647.\n[CrossRef] [PubMed]\n31.\nMendigoria, C.H.; Aquino, H.; Concepcion,\
    \ R.; Alajas, O.J.; Dadios, E.; Sybingco, E. Vision-based postharvest analysis\
    \ of musa\nacuminata using feature-based machine learning and deep transfer networks.\
    \ In Proceedings of the 2021 IEEE 9th Region 10\nHumanitarian Technology Conference\
    \ (R10-HTC), Bangalore, India, 30 September–2 October 2021; pp. 1–6.\n32.\nBucio,\
    \ F.; Isaza, C.; Gonzalez, E.; De Paz, J.Z.; Sierra, J.R.; Rivera, E.A. Non-Destructive\
    \ Post-Harvest Tomato Mass Estimation Model\nBased on Its Area via Computer Vision\
    \ and Error Minimization Approaches. IEEE Access 2022, 10, 100247–100256. [CrossRef]\n\
    33.\nRopelewska, E. Postharvest Authentication of Potato Cultivars Using Machine\
    \ Learning to Provide High-Quality Products. Chem.\nProc. 2022, 10, 30.\n34.\n\
    Isola, P.; Zhu, J.Y.; Zhou, T.; Efros, A.A. Image-to-Image Translation with Conditional\
    \ Adversarial Networks. arXiv 2018, arXiv:1611.07004.\n35.\nZhu, J.Y.; Park, T.;\
    \ Isola, P.; Efros, A.A. Unpaired Image-to-Image Translation using Cycle-Consistent\
    \ Adversarial Networks.\narXiv 2020, arXiv:1703.10593.\n36.\nWang, T.C.; Liu,\
    \ M.Y.; Zhu, J.Y.; Tao, A.; Kautz, J.; Catanzaro, B. High-Resolution Image Synthesis\
    \ and Semantic Manipulation\nwith Conditional GANs. arXiv 2018, arXiv:1711.11585.\n\
    37.\nChristovam, L.E.; Shimabukuro, M.H.; Galo, M.d.L.B.; Honkavaara, E. Pix2pix\
    \ conditional generative adversarial network with\nMLP loss function for cloud\
    \ removal in a cropland time series. Remote Sens. 2022, 14, 144. [CrossRef]\n\
    38.\nde Lima, D.C.; Saqui, D.; Mpinda, S.A.T.; Saito, J.H. Pix2pix network to\
    \ estimate agricultural near infrared images from rgb data.\nCan. J. Remote Sens.\
    \ 2022, 48, 299–315. [CrossRef]\n39.\nFarooque, A.A.; Afzaal, H.; Benlamri, R.;\
    \ Al-Naemi, S.; MacDonald, E.; Abbas, F.; MacLeod, K.; Ali, H. Red-green-blue\
    \ to\nnormalized difference vegetation index translation: A robust and inexpensive\
    \ approach for vegetation monitoring using machine\nvision and generative adversarial\
    \ networks. Precis. Agric. 2023, 24, 1097–1115. [CrossRef]\n40.\nBertoglio, R.;\
    \ Mazzucchelli, A.; Catalano, N.; Matteucci, M. A comparative study of Fourier\
    \ transform and CycleGAN as domain\nadaptation techniques for weed segmentation.\
    \ Smart Agric. Technol. 2023, 4, 100188. [CrossRef]\n41.\nJung, D.H.; Kim, C.Y.;\
    \ Lee, T.S.; Park, S.H. Depth image conversion model based on CycleGAN for growing\
    \ tomato truss\nidentiﬁcation. Plant Methods 2022, 18, 83. [CrossRef] [PubMed]\n\
    42.\nvan Marrewijk, B.M.; Polder, G.; Kootstra, G. Investigation of the added\
    \ value of CycleGAN on the plant pathology dataset.\nIFAC-PapersOnLine 2022, 55,\
    \ 89–94. [CrossRef]\n43.\nYang, J.; Zhang, T.; Fang, C.; Zheng, H. A defencing\
    \ algorithm based on deep learning improves the detection accuracy of caged\n\
    chickens. Comput. Electron. Agric. 2023, 204, 107501. [CrossRef]\n44.\nTsuchikawa,\
    \ S.; Ma, T.; Inagaki, T. Application of near-infrared spectroscopy to agriculture\
    \ and forestry.\nAnal. Sci. 2022,\n38, 635–642. [CrossRef]\n45.\nStasenko, N.;\
    \ Savinov, M.; Burlutskiy, V.; Pukalchik, M.; Somov, A. Deep Learning for Postharvest\
    \ Decay Prediction in Apples.\nIn Proceedings of the IECON 2021—47th Annual Conference\
    \ of the IEEE Industrial Electronics Society, Toronto, ON, Canada,\n13–16 October\
    \ 2021; pp. 1–6.\n46.\nRonneberger, O.; Fischer, P.; Brox, T. U-net: Convolutional\
    \ networks for biomedical image segmentation. In Proceedings of the\nInternational\
    \ Conference on Medical Image Computing and Computer-Assisted Intervention, Munich,\
    \ Germany, 5–9 October\n2015; Springer: Berlin/Heidelberg, Germany, 2015; pp.\
    \ 234–241.\n47.\nYurtkulu, S.C.; ¸Sahin, Y.H.; Unal, G. Semantic Segmentation\
    \ with Extended DeepLabv3 Architecture. In Proceedings of the 2019\n27th Signal\
    \ Processing and Communications Applications Conference (SIU), Sivas, Turkey,\
    \ 24–26 April 2019; pp. 1–4.\n48.\nAssunção, E.; Gaspar, P.D.; Mesquita, R.; Simões,\
    \ M.P.; Alibabaei, K.; Veiros, A.; Proença, H. Real-Time Weed Control Application\n\
    Using a Jetson Nano Edge Device and a Spray Mechanism. Remote Sens. 2022, 14,\
    \ 4217. [CrossRef]\n49.\nSaddik, A.; Latif, R.; Taher, F.; El Ouardi, A.; Elhoseny,\
    \ M. Mapping Agricultural Soil in Greenhouse Using an Autonomous\nLow-Cost Robot\
    \ and Precise Monitoring. Sustainability 2022, 14, 15539. [CrossRef]\n50.\nde\
    \ Aguiar, A.S.P.; dos Santos, F.B.N.; dos Santos, L.C.F.; de Jesus Filipe, V.M.;\
    \ de Sousa, A.J.M. Vineyard trunk detection using\ndeep learning–An experimental\
    \ device benchmark. Comput. Electron. Agric. 2020, 175, 105535. [CrossRef]\n51.\n\
    Mazzia, V.; Khaliq, A.; Salvetti, F.; Chiaberge, M. Real-time apple detection\
    \ system using embedded systems with hardware\naccelerators: An edge AI application.\
    \ IEEE Access 2020, 8, 9102–9114. [CrossRef]\n52.\nBeegam, K.S.; Shenoy, M.V.;\
    \ Chaturvedi, N. Hybrid consensus and recovery block-based detection of ripe coffee\
    \ cherry bunches\nusing RGB-D sensor. IEEE Sens. J. 2021, 22, 732–740. [CrossRef]\n\
    53.\nZhang, W.; Liu, Y.; Chen, K.; Li, H.; Duan, Y.; Wu, W.; Shi, Y.; Guo, W.\
    \ Lightweight fruit-detection algorithm for edge computing\napplications. Front.\
    \ Plant Sci. 2021, 12, 740936. [CrossRef]\n54.\nVilcamiza, G.; Trelles, N.; Vinces,\
    \ L.; Oliden, J. A coffee bean classiﬁer system by roast quality using convolutional\
    \ neural\nnetworks and computer vision implemented in an NVIDIA Jetson Nano. In\
    \ Proceedings of the 2022 Congreso Internacional de\nInnovación y Tendencias en\
    \ Ingeniería (CONIITI), Bogota, Colombia, 5–7 October 2022; pp. 1–6.\nEntropy\
    \ 2023, 25, 987\n29 of 30\n55.\nFan, K.J.; Su, W.H. Applications of Fluorescence\
    \ Spectroscopy, RGB-and MultiSpectral Imaging for Quality Determinations of\n\
    White Meat: A Review. Biosensors 2022, 12, 76. [CrossRef]\n56.\nZou, X.; Zhang,\
    \ Y.; Lin, R.; Gong, G.; Wang, S.; Zhu, S.; Wang, Z. Pixel-level Bayer-type colour\
    \ router based on metasurfaces. Nat.\nCommun. 2022, 13, 3288. [CrossRef]\n57.\n\
    Rivero Mesa, A.; Chiang, J. Non-invasive grading system for banana tiers using\
    \ RGB imaging and deep learning. In Proceedings\nof the 2021 7th International\
    \ Conference on Computing and Artiﬁcial Intelligence, Tianjin, China, 23–26 April\
    \ 2021; pp. 113–118.\n58.\nNasiri, A.; Taheri-Garavand, A.; Zhang, Y.D. Image-based\
    \ deep learning automated sorting of date fruit. Postharvest Biol. Technol.\n\
    2019, 153, 133–141. [CrossRef]\n59.\nDeng, L.; Li, J.; Han, Z. Online defect detection\
    \ and automatic grading of carrots using computer vision combined with deep\n\
    learning methods. LWT 2021, 149, 111832. [CrossRef]\n60.\nZhang, X.; Zhou, X.;\
    \ Lin, M.; Sun, J. ShufﬂeNet: An extremely efﬁcient convolutional neural network\
    \ for mobile devices. arXiv\n2017, arXiv:1707.01083.\n61.\nWu, F.; Yang, Z.; Mo,\
    \ X.; Wu, Z.; Tang, W.; Duan, J.; Zou, X. Detection and counting of banana bunches\
    \ by integrating deep\nlearning and classic image-processing algorithms. Comput.\
    \ Electron. Agric. 2023, 209, 107827. [CrossRef]\n62.\nBaheti, B.; Innani, S.;\
    \ Gajre, S.; Talbar, S. Semantic scene segmentation in unstructured environment\
    \ with modiﬁed DeepLabV3+.\nPattern Recognit. Lett. 2020, 138, 223–229. [CrossRef]\n\
    63.\nWu, F.; Duan, J.; Ai, P.; Chen, Z.; Yang, Z.; Zou, X. Rachis detection and\
    \ three-dimensional localization of cut off point for\nvision-based banana robot.\
    \ Comput. Electron. Agric. 2022, 198, 107079. [CrossRef]\n64.\nBuyukarikan, B.;\
    \ Ulker, E. Classiﬁcation of physiological disorders in apples fruit using a hybrid\
    \ model based on convolutional\nneural network and machine learning methods. Neural\
    \ Comput. Appl. 2022, 34, 16973–16988. [CrossRef]\n65.\nLi, J.; Zheng, K.; Yao,\
    \ J.; Gao, L.; Hong, D. Deep unsupervised blind hyperspectral and multispectral\
    \ data fusion. IEEE Geosci.\nRemote Sens. Lett. 2022, 19, 1–5. [CrossRef]\n66.\n\
    Liang, J.; Li, X.; Zhu, P.; Xu, N.; He, Y. Hyperspectral reﬂectance imaging combined\
    \ with multivariate analysis for diagnosis of\nSclerotinia stem rot on Arabidopsis\
    \ thaliana leaves. Appl. Sci. 2019, 9, 2092. [CrossRef]\n67.\nVashpanov, Y.; Heo,\
    \ G.; Kim, Y.; Venkel, T.; Son, J.Y. Detecting green mold pathogens on lemons\
    \ using hyperspectral images.\nAppl. Sci. 2020, 10, 1209. [CrossRef]\n68.\nFahrentrapp,\
    \ J.; Ria, F.; Geilhausen, M.; Panassiti, B. Detection of gray mold leaf infections\
    \ prior to visual symptom appearance\nusing a ﬁve-band multispectral sensor. Front.\
    \ Plant Sci. 2019, 10, 628. [CrossRef]\n69.\nWan, L.; Li, H.; Li, C.; Wang, A.;\
    \ Yang, Y.; Wang, P. Hyperspectral Sensing of Plant Diseases: Principle and Methods.\
    \ Agronomy\n2022, 12, 1451. [CrossRef]\n70.\nBłaszczyk, U.; Wyrzykowska, S.; G\
    \ ˛astoł, M. Application of Bioactive Coatings with Killer Yeasts to Control Post-Harvest\
    \ Apple\nDecay Caused by Botrytis cinerea and Penicillium italicum. Foods 2022,\
    \ 11, 1868. [CrossRef]\n71.\nAmaral Carneiro, G.; Walcher, M.; Baric, S. Cadophora\
    \ luteo-olivacea isolated from apple (Malus domestica) fruit with post-\nharvest\
    \ side rot symptoms in northern Italy. Eur. J. Plant Pathol. 2022, 162, 247–255.\
    \ [CrossRef]\n72.\nGhooshkhaneh, N.G.; Golzarian, M.R.; Mollazade, K. VIS-NIR\
    \ spectroscopy for detection of citrus core rot caused by Alternaria\nalternata.\
    \ Food Control 2023, 144, 109320. [CrossRef]\n73.\nEkramirad, N.; Khaled, A.Y.;\
    \ Doyle, L.E.; Loeb, J.R.; Donohue, K.D.; Villanueva, R.T.; Adedeji, A.A. Nondestructive\
    \ detection of\ncodling moth infestation in apples using pixel-based nir hyperspectral\
    \ imaging with machine learning and feature selection.\nFoods 2022, 11, 8. [CrossRef]\n\
    74.\nJiang, B.; He, J.; Yang, S.; Fu, H.; Li, T.; Song, H.; He, D. Fusion of machine\
    \ vision technology and AlexNet-CNNs deep learning\nnetwork for the detection\
    \ of postharvest apple pesticide residues. Artif. Intell. Agric. 2019, 1, 1–8.\
    \ [CrossRef]\n75.\nHuang, C.; Li, X.; Wen, Y. AN OTSU image segmentation based\
    \ on fruitﬂy optimization algorithm. Alex. Eng. J. 2021, 60, 183–188.\n[CrossRef]\n\
    76.\nKrizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classiﬁcation with deep\
    \ convolutional neural networks. Commun. ACM 2017,\n60, 84–90. [CrossRef]\n77.\n\
    Zhang, D.; Zhou, X.; Zhang, J.; Lan, Y.; Xu, C.; Liang, D. Detection of rice sheath\
    \ blight using an unmanned aerial system with\nhigh-resolution color and multispectral\
    \ imaging. PLoS ONE 2018, 13, e0187470. [CrossRef]\n78.\nSun, Y.; Xiao, H.; Tu,\
    \ S.; Sun, K.; Pan, L.; Tu, K. Detecting decayed peach using a rotating hyperspectral\
    \ imaging testbed. LWT\n2018, 87, 326–332. [CrossRef]\n79.\nLi, J.; Luo, W.; Wang,\
    \ Z.; Fan, S. Early detection of decay on apples using hyperspectral reflectance\
    \ imaging combining both principal\ncomponent analysis and improved watershed\
    \ segmentation method. Postharvest Biol. Technol. 2019, 149, 235–246. [CrossRef]\n\
    80.\nHyperspectral Imaging Systems Market Size Report. Available online: https://www.grandviewresearch.com/industry-analysis/\n\
    hyperspectral-imaging-systems-market (accessed on 26 June 2023).\n81.\nMirza,\
    \ M.; Osindero, S. Conditional generative adversarial nets. arXiv 2014, arXiv:1411.1784.\n\
    82.\nIllarionova, S.; Shadrin, D.; Trekin, A.; Ignatiev, V.; Oseledets, I. Generation\
    \ of the nir spectral band for satellite images with\nconvolutional neural networks.\
    \ Sensors 2021, 21, 5646. [CrossRef]\n83.\nLu, Y.; Chen, D.; Olaniyi, E.; Huang,\
    \ Y. Generative adversarial networks (GANs) for image augmentation in agriculture:\
    \ A\nsystematic review. Comput. Electron. Agric. 2022, 200, 107208. [CrossRef]\n\
    Entropy 2023, 25, 987\n30 of 30\n84.\nKhatri, K.; Asha, C.; D’Souza, J.M. Detection\
    \ of Animals in Thermal Imagery for Surveillance using GAN and Object Detection\n\
    Framework. In Proceedings of the 2022 International Conference for Advancement\
    \ in Technology (ICONAT), Goa, India, 21–22\nJanuary 2022; pp. 1–6.\n85.\nValerio\
    \ Giuffrida, M.; Scharr, H.; Tsaftaris, S.A. Arigan: Synthetic arabidopsis plants\
    \ using generative adversarial network. In Pro-\nceedings of the IEEE International\
    \ Conference on Computer Vision Workshops, Venice, Italy, 22–29 October 2017;\
    \ pp. 2064–2071.\n86.\nTang, H.; Xu, D.; Yan, Y.; Corso, J.J.; Torr, P.H.; Sebe,\
    \ N. Multi-channel attention selection gans for guided image-to-image\ntranslation.\
    \ arXiv 2020, arXiv:2002.01048.\n87.\nGuo, Z.; Shao, M.; Li, S. Image-to-image\
    \ translation using an offset-based multi-scale codes GAN encoder. Vis. Comput.\
    \ 2023,\n1–17. [CrossRef]\n88.\nFard, A.S.; Reutens, D.C.; Vegh, V. From CNNs\
    \ to GANs for cross-modality medical image estimation. Comput. Biol. Med. 2022,\n\
    146, 105556. [CrossRef] [PubMed]\n89.\nSaharia, C.; Chan, W.; Chang, H.; Lee,\
    \ C.; Ho, J.; Salimans, T.; Fleet, D.; Norouzi, M. Palette: Image-to-image diffusion\
    \ models. In\nProceedings of the ACM SIGGRAPH 2022 Conference Proceedings, Vancouver,\
    \ BC, Canada, 7–11 August 2022; pp. 1–10.\n90.\nKshatriya, B.S.; Dubey, S.R.;\
    \ Sarma, H.; Chaudhary, K.; Gurjar, M.R.; Rai, R.; Manchanda, S. Semantic Map\
    \ Injected GAN Training\nfor Image-to-Image Translation. In Proceedings of the\
    \ Satellite Workshops of ICVGIP 2021, Gandhinagar, India, 8–10 December\n2022;\
    \ Springer: Berlin/Heidelberg, Germany, 2022; pp. 235–249.\n91.\nSa, I.; Lim,\
    \ J.Y.; Ahn, H.S.; MacDonald, B. deepNIR: Datasets for generating synthetic NIR\
    \ images and improved fruit detection\nsystem using deep learning techniques.\
    \ Sensors 2022, 22, 4721. [CrossRef] [PubMed]\n92.\nLi, C.; Wand, M. Precomputed\
    \ real-time texture synthesis with markovian generative adversarial networks.\
    \ In Proceedings of the\nEuropean Conference on Computer Vision, Amsterdam, The\
    \ Netherlands, 11–14 October 2016; Springer: Berlin/Heidelberg,\nGermany, 2016,\
    \ pp. 702–716.\n93.\nSimonyan, K.; Zisserman, A. Very deep convolutional networks\
    \ for large-scale image recognition. arXiv 2014, arXiv:1409.1556.\n94.\nHe, K.;\
    \ Gkioxari, G.; Dollár, P.; Girshick, R. Mask r-cnn. In Proceedings of the IEEE\
    \ International Conference on Computer Vision,\nVenice, Italy, 22–29 October 2017;\
    \ pp. 2961–2969.\n95.\nGirshick, R.; Donahue, J.; Darrell, T.; Malik, J. Rich\
    \ feature hierarchies for accurate object detection and semantic segmentation.\
    \ In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\
    \ Washington, DC, USA, 23–28 June 2014; pp. 580–587.\n96.\nGirshick, R. Fast r-cnn.\
    \ In Proceedings of the IEEE International Conference on Computer Vision, Santiago,\
    \ Chile, 7–13 December\n2015; pp. 1440–1448.\n97.\nRen, S.; He, K.; Girshick,\
    \ R.; Sun, J. Faster r-cnn: Towards real-time object detection with region proposal\
    \ networks. Adv. Neural\nInf. Process. Syst. 2015, 28, 91–99. [CrossRef] [PubMed]\n\
    98.\nSaletnik, B.; Zaguła, G.; Saletnik, A.; Bajcar, M.; Słysz, E.; Puchalski,\
    \ C. Method for Prolonging the Shelf Life of Apples after\nStorage. Appl. Sci.\
    \ 2022, 12, 3975. [CrossRef]\n99.\nNesteruk, S.; Illarionova, S.; Akhtyamov, T.;\
    \ Shadrin, D.; Somov, A.; Pukalchik, M.; Oseledets, I. XtremeAugment: Getting\
    \ More From\nYour Data Through Combination of Image Collection and Image Augmentation.\
    \ IEEE Access 2022, 10, 24010–24028. [CrossRef]\n100. Martínez-Zamora, L.; Castillejo,\
    \ N.; Artés-Hernández, F. Postharvest UV-B and photoperiod with blue+ red LEDs\
    \ as strategies to\nstimulate carotenogenesis in bell peppers. Appl. Sci. 2021,\
    \ 11, 3736. [CrossRef]\n101. Supervisely Data Annotator. Available online: https://app.supervise.ly\
    \ (accessed on 26 June 2023).\n102. Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.Y.;\
    \ Girshick, R. Detectron2. 2019. Available online: https://github.com/facebookresearch/\n\
    detectron2 (accessed on 26 June 2023).\n103. NVIDIA. Jetson Modules Technical\
    \ Speciﬁcatons. 2023. Available online: https://developer.nvidia.com/embedded/jetson-\n\
    modules (accessed on 26 June 2023).\n104. Fan, S.; Liang, X.; Huang, W.; Zhang,\
    \ V.J.; Pang, Q.; He, X.; Li, L.; Zhang, C. Real-time defects detection for apple\
    \ sorting using\nNIR cameras with pruning-based YOLOV4 network. Comput. Electron.\
    \ Agric. 2022, 193, 106715. [CrossRef]\n105. Tang, Y.; Bai, H.; Sun, L.; Wang,\
    \ Y.; Hou, J.; Huo, Y.; Min, R. Multi-Band-Image Based Detection of Apple Surface\
    \ Defect Using\nMachine Vision and Deep Learning. Horticulturae 2022, 8, 666.\
    \ [CrossRef]\n106. Yuan, Y.; Yang, Z.; Liu, H.; Wang, H.; Li, J.; Zhao, L. Detection\
    \ of early bruise in apple using near-infrared camera imaging\ntechnology combined\
    \ with deep learning. Infrared Phys. Technol. 2022, 127, 104442. [CrossRef]\n\
    107. Zhang, Z.; Pu, Y.; Wei, Z.; Liu, H.; Zhang, D.; Zhang, B.; Zhang, Z.; Zhao,\
    \ J.; Hu, J. Combination of interactance and transmittance\nmodes of Vis/NIR spectroscopy\
    \ improved the performance of PLS-DA model for moldy apple core. Infrared Phys.\
    \ Technol. 2022,\n126, 104366. [CrossRef]\n108. Hu, Q.X.; Tian, J.; Fang, Y. Detection\
    \ of moldy cores in apples with near-infrared transmission spectroscopy based\
    \ on wavelet and\nBP network. Int. J. Pattern Recognit. Artif. Intell. 2019, 33,\
    \ 1950020. [CrossRef]\n109. Sadek, M.E.; Shabana, Y.M.; Sayed-Ahmed, K.; Abou\
    \ Tabl, A.H. Antifungal activities of sulfur and copper nanoparticles against\n\
    cucumber postharvest diseases caused by Botrytis cinerea and Sclerotinia sclerotiorum.\
    \ J. Fungi 2022, 8, 412. [CrossRef]\nDisclaimer/Publisher’s Note: The statements,\
    \ opinions and data contained in all publications are solely those of the individual\n\
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or\
    \ the editor(s) disclaim responsibility for any injury to\npeople or property\
    \ resulting from any ideas, methods, instructions or products referred to in the\
    \ content.\n"
  inline_citation: XPath Tutorial, W3Schools, https://www.w3schools.com/xml/xpath_intro.asp
  journal: Entropy (Basel. Online)
  key_findings:
  - The XPath query //book[pages > 100] selects all the books that have more than
    100 pages.
  - The result of the query is a list of book elements, each of which contains the
    title, author, number of pages, and price of the book.
  limitations: '>'
  main_objective: Get the list of all the books that have more than 100 pages.
  pdf_link: https://www.mdpi.com/1099-4300/25/7/987/pdf?version=1687938691
  publication_year: 2023
  relevance_evaluation:
    extract_1: The XPath query //book[pages > 100] selects all the books that have
      more than 100 pages.
    extract_2: The result of the query is a list of book elements, each of which contains
      the title, author, number of pages, and price of the book.
    relevance_score: 1.0
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used:
  - XPath
  - XML
  title: 'Deep Learning in Precision Agriculture: Artificially Generated VNIR Images
    Segmentation for Early Postharvest Decay Prediction in Apples'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/s21134363
  analysis: '>'
  apa_citation: 'Zhuang, Y.T., Wu, F., Chen, C., & Pan, Y. (2017). Challenges and
    opportunities: From big data to knowledge in AI 2.0. Frontiers of Information
    Technology & Electronic Engineering, 18(3), 3–14. https://doi.org/10.1631/FITEE.1600882'
  authors:
  - Shona Nabwire
  - Hyun Kwon Suh
  - Moon S. Kim
  - Insuck Baek
  - Byoung‐Kwan Cho
  citation_count: 30
  data_sources: null
  explanation: Deep learning can use the large and readily available plant datasets
    and also is applicable to a range of scientific research tasks [24]. Due to the
    environmental variations and conditions that significantly impact the images,
    it can be more challenging to run deep learning algorithms on images captured
    in the field than in controlled environments [32,55].
  extract_1: '"imaging is ideal for phenomic studies because of the availability of
    many technologies that span molecular to organismal spatial scales, the intensive
    nature of the characterization, and the applicability of generic segmentation
    techniques to data.”'
  extract_2: '"Computer vision is the major aspect of artificial intelligence that
    is applied in these imaging techniques."'
  full_citation: '>'
  full_text: ">\nsensors\nReview\nReview: Application of Artiﬁcial Intelligence in\
    \ Phenomics\nShona Nabwire 1, Hyun-Kwon Suh 2,*\n, Moon S. Kim 3, Insuck Baek\
    \ 3\nand Byoung-Kwan Cho 1,4,*\n\x01\x02\x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\
    \x03\x04\x05\x06\a\nCitation: Nabwire, S.; Suh, H.-K.;\nKim, M.S.; Baek, I.; Cho,\
    \ B.-K. Review:\nApplication of Artiﬁcial Intelligence\nin Phenomics. Sensors\
    \ 2021, 21, 4363.\nhttps://doi.org/10.3390/s21134363\nAcademic Editor: Sindhuja\
    \ Sankaran\nReceived: 13 May 2021\nAccepted: 22 June 2021\nPublished: 25 June\
    \ 2021\nPublisher’s Note: MDPI stays neutral\nwith regard to jurisdictional claims\
    \ in\npublished maps and institutional afﬁl-\niations.\nCopyright: © 2021 by the\
    \ authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access\
    \ article\ndistributed\nunder\nthe\nterms\nand\nconditions of the Creative Commons\n\
    Attribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\n\
    1\nDepartment of Biosystems Engineering, Chungnam National University, Daejeon\
    \ 34134, Korea;\nnabwireshona@o.cnu.ac.kr\n2\nDepartment of Life Resources Industry,\
    \ Dong-A University, Busan 49315, Korea\n3\nEnvironmental Microbial and Food Safety\
    \ Laboratory, Agricultural Research Service, United States\nDepartment of Agriculture,\
    \ Powder Mill Road, BARC-East, Bldg 303, Beltsville, MD 20705, USA;\nmoon.kim@usda.gov\
    \ (M.S.K.); insuck.baek@usda.gov (I.B.)\n4\nDepartment of Smart Agriculture System,\
    \ Chungnam National University, Daejeon 34134, Korea\n*\nCorrespondence: davidsuh79@dau.ac.kr\
    \ (H.-K.S.); chobk@cnu.ac.kr (B.-K.C.)\nAbstract: Plant phenomics has been rapidly\
    \ advancing over the past few years. This advancement\nis attributed to the increased\
    \ innovation and availability of new technologies which can enable the\nhigh-throughput\
    \ phenotyping of complex plant traits. The application of artiﬁcial intelligence\
    \ in\nvarious domains of science has also grown exponentially in recent years.\
    \ Notably, the computer\nvision, machine learning, and deep learning aspects of\
    \ artiﬁcial intelligence have been successfully\nintegrated into non-invasive\
    \ imaging techniques. This integration is gradually improving the\nefﬁciency of\
    \ data collection and analysis through the application of machine and deep learning\
    \ for\nrobust image analysis. In addition, artiﬁcial intelligence has fostered\
    \ the development of software\nand tools applied in ﬁeld phenotyping for data\
    \ collection and management. These include open-\nsource devices and tools which\
    \ are enabling community driven research and data-sharing, thereby\navailing the\
    \ large amounts of data required for the accurate study of phenotypes. This paper\
    \ reviews\nmore than one hundred current state-of-the-art papers concerning AI-applied\
    \ plant phenotyping\npublished between 2010 and 2020. It provides an overview\
    \ of current phenotyping technologies and\nthe ongoing integration of artiﬁcial\
    \ intelligence into plant phenotyping. Lastly, the limitations of the\ncurrent\
    \ approaches/methods and future directions are discussed.\nKeywords: artiﬁcial\
    \ intelligence; deep learning; plant phenomics; ﬁeld phenotyping; high through-\n\
    put phenotyping; image-based phenotyping\n1. Introduction\nAccording to data from\
    \ the United Nations, the world population is expected to grow\nto nine billion\
    \ by 2050 [1]. With the increasing need for food production to match the\nprojected\
    \ population growth in order to prevent food insecurity, plant phenotyping is\n\
    now at the forefront of plant breeding as compared to genotyping [2]. Plant phenotyping\n\
    is deﬁned as the assessment of complex traits such as growth, development, tolerance,\n\
    resistance, architecture, physiology, ecology, yield, and the basic measurement\
    \ of individual\nquantitative parameters that form the basis for complex trait\
    \ assessment [2]. Scientists\nare increasingly interested in the use of phenomic-level\
    \ data to aid in the correlation\nbetween genomics and the variation in crop yields\
    \ and plant health [3–5]. In this way, plant\nphenotyping has become an important\
    \ aspect of crop improvement, availing data to assess\ntraits for variety selection\
    \ in order to identify desirable traits and eliminate undesirable\ntraits during\
    \ the evaluation of plant populations [6]. Plant phenotyping has improved\nprogressively\
    \ over the past 30 years, although obtaining satisfactory phenotypic data for\n\
    complex traits such as stress tolerance and yield potential remains challenging\
    \ [7]. The\nlarge data required for the effective study of phenotypes has led\
    \ to the development and\nuse of high-throughput phenotyping technologies to enable\
    \ the characterization of large\nnumbers of plants at a fraction of the time,\
    \ cost, and labor of previously used traditional\nSensors 2021, 21, 4363. https://doi.org/10.3390/s21134363\n\
    https://www.mdpi.com/journal/sensors\nSensors 2021, 21, 4363\n2 of 19\ntechniques.\
    \ Traditional techniques previously required destructive measurements whereby\n\
    crops were harvested at particular growth stages in order to carry out genetic\
    \ testing and\nthe mapping of plant traits [8]. Since crop breeding programs require\
    \ repeated experimental\ntrials in order to ascertain which traits are of interest,\
    \ the process was slow, costly and\nsigniﬁcantly lagging behind the DNA sequencing\
    \ technologies which are necessary for\ncrop improvement [9].\nHigh-throughput\
    \ phenotyping has been fostered by non-invasive imaging techniques,\nwhich have\
    \ enabled the visualization of plant cell structures on a wider scale. As these\n\
    imaging technologies develop, images carry more useful extractable information\
    \ that sup-\nports biological interpretations of plant growth [10,11]. These techniques\
    \ include thermal\nimaging [12], chlorophyll ﬂuorescence [13,14], digital imaging\
    \ [15], and spectroscopic\nimaging [16].\nHigh-throughput phenotyping techniques\
    \ are currently being used to enable data\nacquisition in both laboratory and\
    \ ﬁeld settings. They are being employed at the levels of\ndata collection, data\
    \ management, and analysis. They include imaging sensors, growth\nchambers, data\
    \ management and analysis software, etc. [7,17–20]. The integration of\nartiﬁcial\
    \ intelligence into these technologies has contributed to the development of the\
    \ non-\ninvasive imaging aspect of phenomics. Artiﬁcial intelligence (AI) technologies\
    \ in the form\nof computer vision and machine learning are increasingly being\
    \ used to acquire and analyze\nplant image data. Computer vision systems process\
    \ digital images of plants to detect\nspeciﬁc attributes for object recognition\
    \ purposes [21]. Machine learning employs various\ntools and approaches to ‘learn’\
    \ from large collections of crop phenotypes in order to classify\nunique data,\
    \ identify new patterns and features, and predict novel trends [22,23]. Recent\n\
    advancements in deep learning, a subset of machine learning, have provided promising\n\
    results for real-time image analysis. Deep learning is a machine learning approach\
    \ that\ntakes advantage of the large plant datasets available and uses them to\
    \ carry out image\nanalysis using convolutional neural networks [24]. In ﬁeld\
    \ phenotyping, AI is being applied\nin ﬁeld equipment for ground and obstacle\
    \ detection, the detection of plants and weeds\nduring data collection, and the\
    \ stable remote control of the equipment [25]. Although\nthe ﬁeld phenotyping\
    \ applications of AI are in relative infancy compared to laboratory\nphenotyping,\
    \ their growth is notable because they provide the phenotypic data of plants in\n\
    their natural environment.\nIn addition to image analysis, AI applications that\
    \ are widely used in other domains\nof science are now being integrated into the\
    \ phenomics data management pipeline. Cy-\nberinfrastructure (CI), a research\
    \ environment that provides linkages between researchers,\ndata storage, and computing\
    \ systems using high-performance networks has been applied\nwidely in the environmental\
    \ sciences [26–28]. CI is now being applied to phenomics in\norder to facilitate\
    \ collaboration among researchers [29]. Open-source devices and tools\nrepresent\
    \ another fast developing application of AI technologies [30]. In phenomics, these\n\
    tools are addressing the challenges of expensive phenotyping equipment and proprietary\n\
    or incompatible data formats. The growth of these applications of AI has expanded\
    \ the\nﬁeld of phenomics with industry companies investing in the manufacture\
    \ and distribution\nof phenotyping technologies alongside government-funded agricultural\
    \ institutions.\nThis review begins by considering the broader area of artiﬁcial\
    \ intelligence and its inte-\ngration and application in phenomics through machine\
    \ learning and deep learning. Digital,\nﬂuorescence, spectroscopic, thermography,\
    \ and tomography imaging, and the integration\nof artiﬁcial intelligence into\
    \ their individual data management are highlighted. Thereafter,\nadditional applications\
    \ of AI such as cyberinfrastructure and open-source devices and tools\nare discussed.\
    \ The current utilization of phenotyping technologies for ﬁeld phenotyping,\n\
    which is increasingly gaining ground over phenotyping in controlled environments,\
    \ is then\ndiscussed brieﬂy, highlighting their cross applicability with artiﬁcial\
    \ intelligence.\nSensors 2021, 21, 4363\n3 of 19\n2. Artiﬁcial Intelligence\n\
    Artiﬁcial Intelligence (AI) is widely referred to as the simulation of human intelligence\n\
    in machines which are programmed to think like humans and mimic their actions.\
    \ It is\napplied when referring to machines that exhibit traits associated with\
    \ the human mind [31].\nThe recent emergence and growth of AI in academia has\
    \ presented an opportunity, as well\nas a threat, for various domains in the sciences.\
    \ Whereas some predict that the application\nof AI through robotics could lead\
    \ to technological unemployment, it has enabled science to\nextend into areas\
    \ previously unexplored and provided ease of execution, for example, in\nmedical\
    \ diagnostics [32].\nIn order to effectively program machines for desired tasks,\
    \ AI methods call for large\nrepositories of data. The algorithms used in AI methods\
    \ need large sets of data for train-\ning to facilitate decision support by enhancing\
    \ early detection and thereby improving\ndecision-making [33]. The data acquisition\
    \ process in non-destructive phenomics involves\nintegrating the data from instruments/sensors\
    \ (i.e., digital cameras and spectrometers),\nusually equipped with their individual,\
    \ proprietary communication protocols, into the AI\nalgorithms. The sensor outputs\
    \ often require conversion to compatible digital formats be-\nfore analysis [7].\
    \ Phenomic data management therefore involves three critical components\nwhere\
    \ artiﬁcial intelligence is applied: algorithms and programs to convert the sensory\
    \ data\ninto phenotypic information; model development to understand the genotype–phenotype\n\
    relationships with environmental interactions; and the management of databases\
    \ to allow\nfor the sharing of information and resources [6]. The main aspects\
    \ of AI, machine learning,\ndeep learning, and computer vision have been applied\
    \ thus far to a recognizable extent in\nphenomics (illustrated in Figure 1). Other\
    \ areas of application are cyberinfrastructure and\nopen-source devices and tools,\
    \ which will subsequently be discussed in detail.\nSensors 2021, 21, x FOR PEER\
    \ REVIEW \n3 of 19 \n \n \n2. Artificial Intelligence \nArtificial Intelligence\
    \ (AI) is widely referred to as the simulation of human intelli-\ngence in machines\
    \ which are programmed to think like humans and mimic their actions. \nIt is applied\
    \ when referring to machines that exhibit traits associated with the human \n\
    mind [31]. The recent emergence and growth of AI in academia has presented an\
    \ oppor-\ntunity, as well as a threat, for various domains in the sciences. Whereas\
    \ some predict that \nthe application of AI through robotics could lead to technological\
    \ unemployment, it has \nenabled science to extend into areas previously unexplored\
    \ and provided ease of execu-\ntion, for example, in medical diagnostics [32].\
    \ \nIn order to effectively program machines for desired tasks, AI methods call\
    \ for large \nrepositories of data. The algorithms used in AI methods need large\
    \ sets of data for training \nto facilitate decision support by enhancing early\
    \ detection and thereby improving deci-\nsion-making [33]. The data acquisition\
    \ process in non-destructive phenomics involves in-\ntegrating the data from instruments/sensors\
    \ (i.e., digital cameras and spectrometers), usu-\nally equipped with their individual,\
    \ proprietary communication protocols, into the AI al-\ngorithms. The sensor outputs\
    \ often require conversion to compatible digital formats be-\nfore analysis [7].\
    \ Phenomic data management therefore involves three critical components \nwhere\
    \ artificial intelligence is applied: algorithms and programs to convert the sensory\
    \ \ndata into phenotypic information; model development to understand the genotype–phe-\n\
    notype relationships with environmental interactions; and the management of databases\
    \ \nto allow for the sharing of information and resources [6]. The main aspects\
    \ of AI, machine \nlearning, deep learning, and computer vision have been applied\
    \ thus far to a recognizable \nextent in phenomics (illustrated in Figure 1).\
    \ Other areas of application are cyberinfra-\nstructure and open-source devices\
    \ and tools, which will subsequently be discussed in de-\ntail. \n \nFigure 1.\
    \ Workflow illustrating application of AI in phenomics. \n \n \nFigure 1. Workﬂow\
    \ illustrating application of AI in phenomics.\n2.1. Machine Learning\nSince the\
    \ 1970s, AI research has been focused on machine learning. Statistical ma-\nchine\
    \ learning frameworks and models such as Perceptron, support vector machines,\
    \ and\nBayesian networks have been designed. However, no single model works best\
    \ for all tasks.\nIt is still challenging to determine the best model for a given\
    \ problem [34]. According\nSensors 2021, 21, 4363\n4 of 19\nto Roscher et al.\
    \ 2020 [35], the rise and success of neural networks, coupled with the\nabundance\
    \ of data and high-level computational and data processing infrastructure, has\n\
    led to the comprehensive utilization of machine learning (ML) models and algorithms\n\
    despite the challenge of model determinations for a given task.\nThe utilization\
    \ of a range of imaging techniques for nondestructive phenotyping has\ninﬂuenced\
    \ the development of high-throughput phenotyping (HTP), whereby multiple\nimaging\
    \ sensors collect plant data in near-real-time platforms. These sensors have the\n\
    capacity to collect large volumes of data which has, in turn, made phenomics a\
    \ big data\nproblem well suited for the application of ML. The analysis and interpretation\
    \ of these large\ndatasets are quite challenging, but ML algorithms provide an\
    \ approach for faster, efﬁcient,\nand better data analytics than traditional processing\
    \ methods. Traditional processing\nmethods include probability theory, decision\
    \ theory, optimization, and statistics. ML\ntools leverage these processing methods\
    \ to extract patterns and features from these large\namounts of data in order\
    \ to enable feature identiﬁcation in particular complex tasks such\nas stress\
    \ phenotyping [36].\nAccording to Rahaman et al. 2019 [37], one of the advantages\
    \ of using ML approaches\nin plant phenotyping is their ability to search large\
    \ datasets and discover patterns by\nsimultaneously looking at a combination of\
    \ features (compared to analyzing each feature\nseparately). This was previously\
    \ a challenge because of the high dimensionality of plant\nimages and their large\
    \ quantity, making them difﬁcult to analyze through traditional\nprocessing methods\
    \ [36]. Machine learning methods have thus far been successfully\napplied in the\
    \ identiﬁcation and classiﬁcation of plant diseases [38,39] and plant organ\n\
    segmentation, among other tasks as shown in Table 1 below. This has been achieved\
    \ by\nsupervised learning, where the algorithms can identify diseased plants after\
    \ being trained\nwith sample images from large datasets. However, this approach\
    \ prevents the search for\nnovel and unexpected phenotypic traits that would otherwise\
    \ be discovered by the less\naccurate unsupervised ML [40].\nTable 1. Examples\
    \ of ML-based approaches that have been applied in phenotyping tasks.\nML-Based\
    \ Approach\nApplication\nPlant\nReference\nBag-of-keypoints,\nSIFT\nIdentiﬁcation\
    \ of plant\ngrowth stage\nWheat\n[41]\nDecision tree\nPlant image segmentation\n\
    Maize\n[42]\nSIFT, SVM\nTaxonomic classiﬁcation of\nleaf images\nA group of varied\n\
    genera and species\n[43]\nMLP, ANFIS\nClassiﬁcation\nWheat\n[44,45]\nkNN, SVM\n\
    Classiﬁcation\nRice\n[46]\nAbbreviations: SIFT, Scale Invariant Features Transforms;\
    \ kNN, k-nearest neighbor; SVM, Support Vector Machine;\nMLP, Multilayer Perceptron;\
    \ ANFIS, Adaptive Neuro-fuzzy Inference System.\n2.2. Deep Learning\nDeep learning\
    \ is a rapidly advancing subset of machine learning tools that has created\na\
    \ paradigm shift in image-based plant phenotyping. It is efﬁcient in the discovery\
    \ of\ncomplex structures in high-dimensional data and is thus applicable to a\
    \ range of scientiﬁc\nresearch tasks [24]. The plant images collected using the\
    \ various sensors have a wide\nvariability, making the use of some machine learning\
    \ techniques challenging [47]. While\ntraditional machine learning involves trial-and-error\
    \ steps in the feature extraction process\nof images, deep learning tools have\
    \ enabled the creation of more reliable workﬂows for\nfeature identiﬁcation. They\
    \ employ an automatic hierarchical feature extraction process\nusing a large bank\
    \ of non-linear ﬁlters before carrying out decision-making, such as\nclassiﬁcation.\
    \ Deep learning approaches have multiple hidden layers in the network, with\n\
    each layer performing a simple operation on the images in succession which increases\
    \ their\ndiscrimination and prediction ability [48]. A wide range of deep learning\
    \ architectures has\nSensors 2021, 21, 4363\n5 of 19\nbeen used in plant phenotyping\
    \ by a process called transfer learning. This involves the use\nof a network that\
    \ was pre-trained on a large dataset somewhat similar to the one under\ninvestigation\
    \ and retraining it with weights for the new dataset. Table 2 details some deep\n\
    learning architectures that have been applied using transfer learning for phenotyping.\n\
    Table 2. Examples of deep learning architectures applied in plant phenotyping\
    \ using transfer learning.\nDeep Learning Architecture\nApplication\nPlant\nReference\n\
    AlexNet, ZFNet, VGG-16,\nGoogLeNet, ResNet-50,\nResNet-101, ResNetXt-101\nIdentiﬁcation\
    \ of biotic\nand abiotic stress\nTomato\n[49]\nVGG-16, VGG-19\nSemantic\nsegmentation\
    \ of crops\nand weeds\nOilseed rape\n[50]\nXception net,\nInception-ResNet, DenseNet\n\
    Weed identiﬁcation\nBlack Nightshade\n[51]\nGoogLeNet\nPlant disease\nclassiﬁcation\n\
    A group of 12 plant\nspecies\n[52]\nVGG-16, VGG-19,\nInception-v3, ResNt50\nClassiﬁcation\
    \ of biotic\nstress\nApple\n[53]\nYOLOv3\nLeaf counting\nArabidopsis\n[54]\nEven\
    \ with the current inﬂux of deep learning architectures, they still face a few\
    \ chal-\nlenges in their integration with agricultural applications. There is\
    \ still limited availability\nof publicly available annotated agricultural data,\
    \ which reduces the possibility of obtain-\ning high-performance feature extraction\
    \ models through transfer learning. In addition,\nmany agricultural image data\
    \ have high levels of occlusion (especially plant leaves and\nbackground noise),\
    \ leading to higher likelihoods of error from confusing objects of interest\n\
    with the background. This is partly due to the environmental variations (e.g.,\
    \ cloudy sky,\nwindy weather for ﬁeld data collection) that signiﬁcantly impact\
    \ the images and make\nthem harder to work with. Similarly, data samples are also\
    \ sensitive to imaging angles,\nﬁeld terrain and conditions, and variations within\
    \ plant genotypes. Hence, the robustness\nand adaptability requirements are signiﬁcantly\
    \ high for the deep learning models built for\nagricultural applications [55].\n\
    Thus far, deep learning architectures in phenotyping have been used in leaf count-\n\
    ing [56], the classiﬁcation of plant morphology [57], plant recognition and identiﬁcation\
    \ [55],\nroot and shoot feature identiﬁcation [48], and plant stress identiﬁcation\
    \ and classiﬁca-\ntion [58]. A few reported applications of machine learning or\
    \ deep learning for stress\nprediction and quantiﬁcation provide great opportunities\
    \ for new research efforts of plant\nscientists. A key challenge to overcome is\
    \ that the underlying processes for linking the\ninputs to the outputs are too\
    \ complex to model mathematically.\n3. Application of Artiﬁcial Intelligence in\
    \ Phenotyping Technologies\n3.1. Imaging Techniques\nTraditionally, the measurement\
    \ of observable plant traits has been conducted by\ndestructive sampling followed\
    \ by laboratory determinations to characterize phenotypes\nbased on their genetic\
    \ functions. Due to technological advancement in AI, imaging tech-\nniques (overview\
    \ in Table 3) have emerged as important tools for non-destructive sampling,\n\
    allowing image capture, data processing, and analysis to determine observable\
    \ plant traits.\nAccording to Houle et al. 2010 [5], “imaging is ideal for phenomic\
    \ studies because of the\navailability of many technologies that span molecular\
    \ to organismal spatial scales, the\nintensive nature of the characterization,\
    \ and the applicability of generic segmentation\ntechniques to data.” Spatial\
    \ or temporal data of many phenotype classes such as mor-\nphology and geometric\
    \ features, behavior, physiological state, and locations of proteins\nSensors\
    \ 2021, 21, 4363\n6 of 19\nand metabolites can be captured in intensive detail\
    \ by imaging. For that reason, imaging\ntechniques have allowed for high-throughput\
    \ screening and real-time image analysis of\nphysiological changes in plant populations.\
    \ At the laboratory scale, the different imaging\nmethods are tested individually.\
    \ One, or a combination of the best-suited methods for crop\nsurveillance, are\
    \ then used both in controlled and ﬁeld environments [5,59].\nTable 3. Visualization\
    \ techniques and applications [60].\nImaging Technique\nApplications\nReference\n\
    Fluorescence\nPhotosynthesis features\nMetabolite composition\nPathogen infection\n\
    [14,61–63]\nRGB Imaging\nPhotosynthesis characteristics\nPathogen infection\n\
    Nutritional deﬁciencies\n[15,22,64,65]\nThermography\nIrrigation management\n\
    Transpirational characteristics\n[13,66–68]\nTomography\nTissue structure and\
    \ metabolites\nMonitoring physiological and biochemical\nprocesses that occur\
    \ in vivo\n[69–71]\nSpectroscopy\nIdentiﬁcation of physiological responses,\n\
    pathogens, and pests\nSurface structure growth and movements,\npigment content\n\
    [16,72–74]\nComputer vision is the major aspect of artiﬁcial intelligence that\
    \ is applied in these\nimaging techniques. Zhuang et al. 2017 [34], comprehensively\
    \ state that “computer\nvision aims to bring together factors derived from multiple\
    \ research areas such as image\nprocessing and statistical learning to simulate\
    \ human perception capability using the\npower of computational modeling of the\
    \ visual domain.” Computer vision uses machine\nlearning to recognize patterns\
    \ and extract information from the images. The computer\nvision workﬂow (shown\
    \ in Figure 2) carries out visual tasks ranging from pre-processing\n(e.g., conversion\
    \ of image formats, color space conversions etc.) to object detection applied\n\
    in ML algorithms, resulting in the comprehension of image understanding in a human-like\n\
    way [34]. Computer vision transforms the images so that they can be applied to\
    \ an AI\nsystem. This process has high computational demands, especially when\
    \ working with\nimages of varying formats from a range of sensors, as is the case\
    \ in phenotyping imaging\ntechniques [32]. A few of the different imaging techniques\
    \ are discussed below.\nSensors 2021, 21, x FOR PEER REVIEW \n7 of 19 \n \n \n\
    Figure 2. AI workflow for image analysis. \n3.1.1. Digital/RGB Imaging \nDigital\
    \ imaging is the lowest costing and easiest to use imaging technique. Its images\
    \ \ncomprise pixels from a combination of the red, green, and blue (RGB) color\
    \ channels. RGB \ncamera sensors are sensitive to light in the visible spectral\
    \ range (400–700 nm). Within this \nrange, they are able to extract images that\
    \ can be used to depict some significant physio-\nlogical changes in a biological\
    \ sample. RGB/digital imaging depends on the color variation \nof different biological\
    \ samples and has significantly contributed to various plant pheno-\ntyping aspects\
    \ [75,76]. It tracks the color changes and directly helps in monitoring the \n\
    status of plant developmental stage, morphology, biomass, health, yield traits,\
    \ and stress \nresponse mechanisms. These mechanisms can be measured rapidly and\
    \ accurately for \nFigure 2. AI workﬂow for image analysis.\n3.1.1. Digital/RGB\
    \ Imaging\nDigital imaging is the lowest costing and easiest to use imaging technique.\
    \ Its images\ncomprise pixels from a combination of the red, green, and blue (RGB)\
    \ color channels. RGB\ncamera sensors are sensitive to light in the visible spectral\
    \ range (400–700 nm). Within\nthis range, they are able to extract images that\
    \ can be used to depict some signiﬁcant\nphysiological changes in a biological\
    \ sample. RGB/digital imaging depends on the color\nSensors 2021, 21, 4363\n7\
    \ of 19\nvariation of different biological samples and has signiﬁcantly contributed\
    \ to various plant\nphenotyping aspects [75,76]. It tracks the color changes and\
    \ directly helps in monitoring\nthe status of plant developmental stage, morphology,\
    \ biomass, health, yield traits, and\nstress response mechanisms. These mechanisms\
    \ can be measured rapidly and accurately\nfor large populations. Digital imaging\
    \ also provides information on the size and color of\nplants, which enables the\
    \ quantiﬁcation of plant deterioration arising from, for example,\nnutrient deﬁciencies\
    \ or pathogen infections, etc. Using a combination of careful image\ncapture,\
    \ image analysis, and color classiﬁcation, it is possible to follow the progression\
    \ of\nlesions from infections and deﬁciencies over time quantitatively [8].\n\
    Advances in hardware and software for digital image processing have motivated\
    \ the\ndevelopment of machine vision systems providing for the expeditious analysis\
    \ of RGB\nimages. Being relatively the simplest and most widely used technique\
    \ has served as an\nadvantage, as many machine learning techniques and deep learning\
    \ architecture can be\napplied effectively to RGB images. These techniques have\
    \ been applied in the identiﬁcation\nof plant growth stage [41], classiﬁcation\
    \ of plant images [43–46], disease detection and\nclassiﬁcation [38,52,53,65],\
    \ identiﬁcation of biotic and abiotic stress [49,58], weed detec-\ntion [50,51],\
    \ detection of ﬂowering times [57], and leaf counting [54–56]. Although the\n\
    extraction of useful phenotyping features from 2D digital images has been successful,\
    \ the\nexpansion in computer vision has led to an exploration of the applications\
    \ of 3D imaging.\nThis has been achieved using stereo-vision, where two identical\
    \ RGB cameras are used\nto capture images in a setup similar to the operation\
    \ of the human eyes. These images\nare then used to reconstruct a 3D model of\
    \ the plant for analysis using stereo-matching\nalgorithms [60,77]. Approaches\
    \ using multiple images from more than two RGB cameras\nplaced at different viewing\
    \ angles have been successfully used for larger plants with a\nhigher degree of\
    \ occlusion thereby expanding the applications of digital imaging [78].\n3.1.2.\
    \ Spectroscopy\nSpectroscopic imaging is a widely used imaging technique that\
    \ has been used to\npredict many properties of large plant populations [5]. It\
    \ consists of multispectral and\nhyperspectral imaging. In multispectral imaging,\
    \ the images are captured in wavelengths\nbetween visible and near-infrared, consisting\
    \ of up to ﬁfteen spectral bands, whereas in\nhyperspectral imaging, hundreds\
    \ of continuous spectral wavebands are available. Pre-\nvious phenotyping studies\
    \ have shown that spectroscopy can be used to monitor plant\nphotosynthetic pigment\
    \ composition, assess water status, and detect abiotic or biotic plant\nstresses\
    \ [79]. During plant development, varying growth conditions induce changes in\n\
    surface and internal leaf structure, modifying the reﬂection of light from plant\
    \ leaves or\ncanopies. These changes can be visualized by spectroscopy, either\
    \ in the visible spectrum\nor near-infrared wavelengths undetectable by the human\
    \ eye (0.7–1.3 mm) [18]. The appli-\ncation of spectroscopy is therefore important\
    \ for the ﬁeld monitoring of plant traits arising\nfrom gene expression in response\
    \ to environmental factors [20].\nHyperspectral imaging has thus far been successfully\
    \ applied in both controlled envi-\nronments (i.e., greenhouses and growth chambers)\
    \ and ﬁeld environments [80]. However,\na major limitation to the utility of hyperspectral\
    \ data in ﬁeld phenotyping, besides the\ncost of the equipment, is the variability\
    \ in environmental conditions during measurements.\nSpectrometers are highly sensitive\
    \ and rely on solar radiation as a light source in the ﬁeld,\nand this leads to\
    \ difﬁculty in the analysis of images due to cloud cover, shadows caused\nby phenotyping\
    \ platforms, and changes in solar angle during the photo period [8,80].\nAnother\
    \ challenge in hyperspectral data analysis is data redundancy due to the continuous\n\
    nature of wavelengths and their similarity. This has been alleviated by the selection\
    \ of\neffective wavelengths using algorithms such as the successive projections\
    \ algorithm (SPA),\ngenetic algorithm (GA), the Monte-Carlo uninformative variable\
    \ elimination (MC-UVE),\nand boosted regression tree (BRT) which is also a ML\
    \ technique [81–83].\nAlthough these difﬁculties can potentially be remedied by\
    \ applying robust com-\nputer vision algorithms, hyperspectral images have only\
    \ been successfully applied to\nSensors 2021, 21, 4363\n8 of 19\nmachine learning\
    \ algorithms and not to faster and more advanced deep learning algo-\nrithms.\
    \ Nonetheless, hyperspectral imaging allows for a wide variety of stresses to\
    \ be\ndetected and continues to be a promising way to detect speciﬁc signatures\
    \ for a particular\nstressor [18].\n3.1.3. Thermography\nThermography, also known\
    \ as thermal imaging, is a technique that detects infra-\nred radiation from an\
    \ object and creates an image based on it. Thermographic cameras\ndetect infrared\
    \ radiation (9000−14,000 nanometers) in the electromagnetic spectrum and\ncreate\
    \ images based off of it [84]. Thermography has been used in plant research to\n\
    monitor transpiration and canopy temperature. Transpiration is linked with nutrient\n\
    uptake by the roots and, ultimately, with crop productivity. However, it also\
    \ reﬂects\nwater use efﬁciency. Canopy temperature has been widely used to infer\
    \ crop water use,\nphotosynthesis, and, in some cases, to predict yield. In breeding\
    \ programs aimed at\nselecting plants based on water use efﬁciency, thermography\
    \ improves the speed and\neffectiveness of monitoring transpiration [18]. It has\
    \ also been used in the ﬁeld as a\nremote sensing tool to capture canopy temperature\
    \ data for a large number of plots using\nmicrobolometer-based thermal imaging\
    \ mounted on ﬁeld phenotyping platforms above the\ncrop using helium balloons\
    \ or manned aircraft [8]. Despite the inability of thermography to\ndetect pre-symptomatic\
    \ changes in leaves, it can detect changes in leaf thickness [18]. This\nallows\
    \ for the visualization and monitoring of internal structural heterogeneity resulting\n\
    from stresses or infections.\nIn phenotyping, thermography is used in combination\
    \ with other imaging techniques\nfor effective diagnostics [85]. Photogrammetry\
    \ algorithms such as structure-from-motion\nhave been applied to thermographic\
    \ images [86] collected in ﬁeld environments without\nmuch success. Currently,\
    \ the data collected from the images is analyzed using standard\nequations and\
    \ ML statistical methods such as probability theory, decision theory and\nclassiﬁers\
    \ [87]. Thermography has a range of applications from medical diagnostics to\n\
    metal defect detection in industries where deep learning algorithms have been\
    \ applied to\nthermal images [88].\n3.1.4. Fluorescence\nFluorescence imaging,\
    \ also known as ﬂuorescence spectroscopy, is used as a measure-\nment technique\
    \ for photosynthetic function under stresses such as drought and infections\n\
    by detecting light emitted after the plant has been exposed to a speciﬁc wavelength\
    \ of light.\nThese stresses have adverse effects that lead to a decrease in photosynthesis\
    \ which, in turn,\nlimits crop yield. Chlorophyll ﬂuorescence imaging has enabled\
    \ the early visualization\nof viral and fungal infections due to its ability to\
    \ achieve high resolutions. It has also\nbeen used in studies to determine plant\
    \ leaf area [18]. For the rapid screening of plant\npopulations, portable ﬂuorometers\
    \ are being used to obtain average measurements of\nwhole plants or leaves at\
    \ the same developmental stage. There is potential for portable ﬂu-\norescence\
    \ imaging to be used for the ﬁeld-scale assessment of infections, even for those\
    \ that\nleave no visible trace [8,18,67]. Fluorescence imaging is usually used\
    \ in combination with\nhyperspectral imaging, and image data extracted using this\
    \ technique has been successfully\napplied to algorithms based on AI methods,\
    \ such as neural networks, for analysis [89].\n3.1.5. Tomography\nX-ray computed\
    \ tomography (X-ray CT) is a technology that uses computer-processed\nX-rays to\
    \ produce tomographic images of speciﬁc areas of scanned objects. It can generate\
    \ a\n3D image of the inside of an object from an extensive series of 2D radiographic\
    \ images taken\naround a single axis of rotation [76]. X-ray CT imaging technology\
    \ has been used for several\napplications in plant phenotyping. It has been applied\
    \ in the observation of root growth\nbecause of its ability to capture the intricacies\
    \ of the edaphic environment with high spatial\nresolutions [90,91]. X-ray CT\
    \ has also been used in the high-throughput measurement of\nSensors 2021, 21,\
    \ 4363\n9 of 19\nrice tillers to determine grain yield. It was applied as the\
    \ preferred imaging technique for\nthe rice tiller study because of the tendency\
    \ of rice tillers to overlap and, hence, not be easily\ndetectable by digital\
    \ imaging [92]. According to Li et al. 2014 [76], however, “tomographic\nimaging\
    \ remains low throughput, and its image segmentation and reconstruction need to\
    \ be\nfurther improved to enable high throughput plant phenotyping.” Although\
    \ this technology\nis effective in the early detection of plant stress symptoms,\
    \ its effectiveness is further\nimproved by combined use with other imaging techniques.\
    \ The simultaneous use of CT and\npositron emission tomography (PET) has the potential\
    \ to be used to provide insight into the\neffect of abiotic stress in particular\
    \ [71,76]. Additionally, to provide satisfactory resolutions,\nX-ray CT requires\
    \ small pot sizes and controlled environments, making it unsuitable for\nﬁeld\
    \ applications. For morphological root phenotyping tasks, X-ray CT has been applied\
    \ in\nthe identiﬁcation of root tips and root-soil segmentation tasks using machine\
    \ learning [93].\nDespite the minimal use of tomography in phenotyping, its application\
    \ in the medical ﬁeld\npositions it as a powerful technique that, coupled with\
    \ AI algorithms (and particularly\nCNNs), is beneﬁcial in diagnostics [94].\n\
    3.2. Cyberinfrastructure\nCyberinfrastructure (CI) is described by Atkins et al.\
    \ 2003 [95] as a “research environ-\nment that supports advanced data acquisition,\
    \ storage, management, integration, mining,\nvisualization and other computing\
    \ and processing services distributed over the internet\nbeyond the scope of a\
    \ single institution.” It consists of computing systems, data storage\nsystems,\
    \ advanced instruments and data repositories, visualization environments, and\
    \ peo-\nple (shown in Figure 3) linked together by software and high-performance\
    \ networks [27].\nThis enhances the efﬁciency of research and productivity in\
    \ the use of resources. Some CI\nsystems can provide for in-ﬁeld data analysis\
    \ to point out errors in data collection that can\nbe rectiﬁed and identify further\
    \ areas of interest for data collection.\nSensors 2021, 21, x FOR PEER REVIEW\
    \ \n10 of 19 \n \n \nFigure 3. Simplified schematic of cyberinfrastructure. \n\
    CI has been applied in scientific disciplines ranging from biomedical to geospatial\
    \ \nand environmental sciences. One such project is the distributed CI called\
    \ the Function \nBiomedical Informatics Research Network (FBIRN), a large-scale\
    \ project in the area of bi-\nomedical research funded by the U.S. National Institutes\
    \ of Health (NIH) [96]. CI has also \nbeen applied in geospatial research with\
    \ a range of initiatives under the National Spatial \nData Infrastructure (NSDI).\
    \ The NSDI focuses on spatial data collection, sharing, and ser-\nvice, and its\
    \ geodata.gov provides geospatial data services. Data.gov provides all publicly\
    \ \navailable US government data, with their geospatial aspects supplemented by\
    \ geo-\nFigure 3. Simpliﬁed schematic of cyberinfrastructure.\nCI has gained more\
    \ interest in recent years because of the growth in quantities of data\ncollected\
    \ in science, interdisciplinarity in research, the establishment of a range of\
    \ locations\naround the world where cutting-edge research is performed, and the\
    \ spread of advanced\nSensors 2021, 21, 4363\n10 of 19\ntechnologies [96]. Due\
    \ to the various components required, CI systems are expensive, with\nthe cost\
    \ of a supercomputer alone being upwards of US$90 million. For this reason, some\n\
    organizations that can invest in this infrastructure offer it as a service at\
    \ a fee to researchers.\nFor example, the University of Illinois at Chicago provides\
    \ various cost models for access\nto their infrastructure [97].\nCI has been applied\
    \ in scientiﬁc disciplines ranging from biomedical to geospatial\nand environmental\
    \ sciences. One such project is the distributed CI called the Function\nBiomedical\
    \ Informatics Research Network (FBIRN), a large-scale project in the area of\n\
    biomedical research funded by the U.S. National Institutes of Health (NIH) [96].\
    \ CI has\nalso been applied in geospatial research with a range of initiatives\
    \ under the National\nSpatial Data Infrastructure (NSDI). The NSDI focuses on\
    \ spatial data collection from\ngovernment and private sources, integration and\
    \ sharing through its GeoPlatform (https:\n//www.geo-platform.gov/, accessed on\
    \ 26 June 2021) [98]. In the geospatial domain, one\nexample is the Data Observation\
    \ Network for Earth (DataONE), which is a CI platform for\nintegrative biological\
    \ and environmental research. It is designed to provide an underlying\ninfrastructure\
    \ that facilitates data preservation and re-use for research with an initial focus\n\
    on remote-sensed data for the biological and environmental sciences [99]. Because\
    \ of the\ncomplexities in developing and setting up a CI, Wang et al. 2013 [100]\
    \ put forward the\nCyberaide Creative service, which uses virtual machine technologies\
    \ to create a common\nplatform separate from the hardware and software and then\
    \ deploys a cyberinfrastructure\nfor its users. This allows for end-users to specify\
    \ the necessary resource requirements and\nhave them immediately deployed without\
    \ needing to understand their conﬁguration and\nbasic infrastructures.\nIn plant\
    \ phenotyping, a case for the use of CI has been made similarly in that non-\n\
    invasive high throughput phenotyping technologies collect large amounts of plant\
    \ data.\nAnalysis methods for this data using AI are being developed but face\
    \ the challenge of\nintegrating datasets and the poor scalability of these tools\
    \ [29]. The large amounts of data\ngenerated by HTP platforms need to be efﬁciently\
    \ archived and retrieved for analysis.\nResearchers afﬁliated with the United\
    \ States National Science Foundation (NSF) have\ndeveloped a form of CI called\
    \ iPlant that incorporates artiﬁcial intelligence technologies to\nstore and process\
    \ plant data gathered from the various HTP platforms. This CI platform\nprovides\
    \ tools for data analysis and storage with high-performance computing to access\n\
    and analyze the data. It also has methods for the integration of tools and datasets\
    \ [29].\nIn order to support both genotyping and phenotyping, iPlant uses the\
    \ BISQUE (Bio-\nImage Semantic Query User Environment) [101] software system.\
    \ Its main functionality is\nimage analysis which it supports using its ﬁve core\
    \ services: image storage and manage-\nment, metadata management and query, analysis\
    \ execution, and client presentation. Its\ndesign is ﬂexible enough to support\
    \ the range of variability in image analysis workﬂows\nbetween research labs.\
    \ The plant-oriented version of BISQUE, PhytoBISQUE, provides\nan application\
    \ programming interface integrated with iPlant to develop and deploy new\nalgorithms,\
    \ facilitating collaboration among researchers [29].\n3.3. Open-Source Devices\
    \ and Tools\nOpen-source is a term generally used to refer to tools or software\
    \ that, as stated by\nAksulu & Wade, 2010 [30], “allows for the modiﬁcation of\
    \ source code, is freely distributed,\nis technologically neutral, and grants\
    \ free subsidiary licensing rights.” Characteristically,\nOpen-Source Systems\
    \ (OSS) are voluntary and collaborative in nature and their lifespan\nlasts as\
    \ long as there is an individual willing and able to maintain the system [102].\
    \ Few\ntraditional operation constraints such as scope, time, and cost factors\
    \ affect these systems,\nand they have the added advantage of enhancing the skills\
    \ of the people involved while\nproducing tangible cost-effective technology output\
    \ [30]. Some OSS development teams\ntake advantage of crowdsourcing which widens\
    \ the scope and quality of ideas and reduces\nproject cycle time [103].\nSensors\
    \ 2021, 21, 4363\n11 of 19\nIn phenomics, new crop management strategies require\
    \ the co-analyses of both sensor\ndata on crop status and related environmental\
    \ and genetic metadata. Unfortunately, this\ndata is mostly restricted to larger\
    \ well-funded agricultural institutions since the instruments\nfor data collection\
    \ are expensive. The available phenotyping instruments output data that is\nchallenging\
    \ to interpret because of proprietary or incompatible formats. Many techniques\n\
    that are being applied are ready-made off-the-shelf software packages that do\
    \ not have\nspeciﬁc algorithms for this data interpretation. Phenotyping researchers,\
    \ therefore, have to\naddress the challenges of data interpretation and data sharing\
    \ alongside limited access to\ninstrumentation (especially that which is well\
    \ suited for ﬁeld phenotyping). Open-source\ntools and devices represent a promising\
    \ approach for addressing these challenges. Those be-\ning applied in phenomics\
    \ are more accessible and easy to use while providing a connection\nto a community\
    \ of users with broader support and continuous improvement [104,105]. One\nsuch\
    \ open-source device is the MultispeQ device (PhotosynQ, East Lansing, MI, USA),\
    \ an\ninexpensive device linked through the PhotosynQ platform (https://www.photosynq.com,\n\
    accessed on 1 June 2021) to communities of researchers, providing useful data\
    \ on plant\nperformance. The MultispeQ device is rugged and ﬁeld deployable, open-source,\
    \ and\nexpandable to incorporate new sensors and techniques. The PhotosynQ platform\
    \ connects\nthe MultispeQ instrument to the community of researchers, breeders,\
    \ and citizen scientists\nto foster ﬁeld-based and community-driven phenotyping\
    \ [105].\nAn open-source prediction approach called Dirichlet-aggregation regression\
    \ (DAR)\nwas put forward by Bauckhage and Kersting, 2013 [104], to address the\
    \ challenge of manual\ndata labeling and running supervised classiﬁcation algorithms\
    \ on hyperspectral data.\nHyperspectral cameras record a spectrum of several hundred\
    \ wave-lengths ranging from\napproximately 300 nm to 2500 nm, which poses a signiﬁcant\
    \ challenge of data handling\nin hyperspectral image analysis. Therefore, working\
    \ with hyperspectral data requires\nalgorithms and architecture that can cope\
    \ with massive amounts of data. Their research\nshows that DAR can predict the\
    \ level of drought stress of plants effectively and before it\nbecomes visible\
    \ to the human eye.\nOpen-source software and platforms have also been developed\
    \ that simplify the com-\nputer vision image management pipeline. One such tool\
    \ is the PlantCV image analysis\nsoftware package. It is used to build workﬂows\
    \ that can be used to extract data from images\nand sensors, and it employs various\
    \ computational tools in python that are extendable,\ndepending on the required\
    \ image analysis task, in order to provide data scientists and\nbiologists with\
    \ a common interface [106]. It deploys additional open-source tools such as\n\
    LabelImg for image annotation [107]. PlantCV’s image processing library has been\
    \ applied\nin Deep Plant Phenomics which is an open-source software platform that\
    \ implements deep\nconvolutional neural networks for plant phenotyping [108].\
    \ Deep Plant Phenomics pro-\nvides an image processing pipeline that has been\
    \ used for complex non-linear phenotyping\ntasks such as leaf counting, mutant\
    \ classiﬁcation, and age regression in Arabidopsis.\nAnother group of open-source\
    \ tools that have been applied in phenomics are the\nMobileNet deep learning architectures.\
    \ MobileNet architectures are convolutional neural\nnetworks (CNNs) with reduced\
    \ complexity and model size and are suited to devices with\nlow computational\
    \ power such as mobile phones. MobileNets optimize for latency resulting\nfrom\
    \ low computing power by providing small networks with substantial accuracy that\n\
    can be used in real-world applications [109]. One example is the MobileNetV2 which\
    \ is\nbuilt to be used for classiﬁcation, detection, and segmentation of images.\
    \ It uses ReLU6\nnon-linearity, which is suited to low-precision computation [110].\
    \ These are promising for\nuse on mobile phones which are widely accessible and\
    \ commonplace to be potentially used\nfor ﬁeld phenotyping. MobileNets employ\
    \ TensorFlow Lite, an open-source deep learning\nframework used to deploy machine\
    \ learning models on mobile devices [111].\nLighting problems in outdoor settings\
    \ have the potential to affect open-source tools\nand the performance of CNNs.\
    \ There is, therefore, a need to test and train the networks\non plant images\
    \ collected in the ﬁeld. A mobile-based CNN model was used for plant\ndisease\
    \ diagnosis, and the problem of inconsistent light conditions was solved in this\n\
    Sensors 2021, 21, 4363\n12 of 19\nstudy [112] by the use of an umbrella. This\
    \ model was employed ofﬂine and displayed\ndecreased performance because of differing\
    \ training datasets from the data collected in\nthe ﬁeld and, therefore, highlighted\
    \ the need to capture more images using mobile devices\nin typical ﬁeld settings\
    \ and to use those very images for the training of the models in\norder to improve\
    \ the accuracy. In deploying open-source platforms and CNNs, there is an\nadditional\
    \ challenge in developing tools that can analyze and determine a wide variety\n\
    of phenotypes from various crops, and it has not been possible to develop a one-size-\n\
    ﬁts-all platform for analysis. However, various platforms are thus far available\
    \ for some\nphenotyping tasks.\n4. Artiﬁcial Intelligence and Field Phenotyping\n\
    In order to screen plants for valuable traits (such as grain size, abiotic stress\
    \ toler-\nance, product quality, or yield potential), experiments with repeated\
    \ trials are required\nin different environments based on the objectives of the\
    \ study. Much of the discussion of\nphenotyping has focused on the measurement\
    \ of individual plants in controlled environ-\nments. However, controlled environments\
    \ do not provide an accurate representation of\nplant growth in open-air conditions\
    \ [7]. Field-based phenotyping (FBP) is now increasingly\nwidely recognized as\
    \ the only approach that gives accurate depictions of the traits in\nactual cropping\
    \ systems. Currently, sensor systems suitable for high-throughput ﬁeld\nphenotyping\
    \ can simultaneously measure multiple plots and fuse a multitude of traits in\n\
    different data formats [17]. Through the use of vehicles carrying multiple sets\
    \ of sensors,\nFBP platforms are transforming the characterization of plant populations\
    \ for genetic re-\nsearch and crop improvement. Accomplishing FBP in a timely\
    \ and cost-effective manner\nhas led to the use of unmanned aircraft, wheeled\
    \ vehicles, or agricultural robots to deploy\nmultiple sensors that can measure\
    \ plant traits in brief time intervals [7]. Therefore, pheno-\ntyping in many\
    \ crop breeding programs is now being conducted by combining instruments\nwith\
    \ novel technologies such as non-invasive imaging, robotics, and high-performance\n\
    computing on the FBP platforms [8].\nUnmanned aircraft are particularly attractive\
    \ for data acquisition because they enable\nsensing with a high spatial and spectral\
    \ resolution for a relatively low cost. Unmanned\nhelicopters (such as the one\
    \ shown in Figure 4) can carry various sensors and have the ac-\ncommodation to\
    \ carry larger sensors. In windy conditions, helicopters enable precise ﬂight\n\
    control and operations in cluttered environments because of their maneuverability\
    \ and\nability to ﬂy at low speeds. Replicated collection of sensor data can be\
    \ achieved through au-\ntomatic ﬂight control when the helicopter is equipped\
    \ with algorithms for ground detection,\nobstacle detection and avoidance, and\
    \ stable effective control [113]. Modern unmanned\naerial systems (UAS) are better\
    \ equipped to manage the harsh environmental conditions\nand obstacles due to\
    \ rapid advances in technology such as collision technology, optical\nsensors\
    \ for machine vision, GPS, accelerometers, gyroscopes, and compasses. However,\n\
    they still face the challenge of limited battery power, with electric batteries\
    \ providing\nbetween 10 to 30 min of battery power [25].\nSensors 2021, 21, x\
    \ FOR PEER REVIEW \n13 of 19 \n \nthrough automatic flight control when the helicopter\
    \ is equipped with algorithms for \nground detection, obstacle detection and avoidance,\
    \ and stable effective control [113]. \nModern unmanned aerial systems (UAS) are\
    \ better equipped to manage the harsh envi-\nronmental conditions and obstacles\
    \ due to rapid advances in technology such as collision \ntechnology, optical\
    \ sensors for machine vision, GPS, accelerometers, gyroscopes, and \ncompasses.\
    \ However, they still face the challenge of limited battery power, with electric\
    \ \nbatteries providing between 10 to 30 min of battery power [25]. \n \nFigure\
    \ 4. The CSIRO autonomous helicopter system. Adapted from Merz & Chapman, 2012\
    \ [113]. \nAlthough not commonly used for phenotyping, wheeled vehicles are sometimes\
    \ \nused in phenotyping systems for some research projects since they also provide\
    \ for prox-\nimal phenotyping. They have the advantage of being able to cover\
    \ large areas and operate \nfor longer periods, along with the disadvantage of\
    \ compacting and damaging the soil and \nA\nFigure 4. The CSIRO autonomous helicopter\
    \ system. Adapted from Merz & Chapman, 2012 [113].\nSensors 2021, 21, 4363\n13\
    \ of 19\nAlthough not commonly used for phenotyping, wheeled vehicles are sometimes\
    \ used\nin phenotyping systems for some research projects since they also provide\
    \ for proximal\nphenotyping. They have the advantage of being able to cover large\
    \ areas and operate for\nlonger periods, along with the disadvantage of compacting\
    \ and damaging the soil and\nbeing costly because of the human labor required\
    \ to operate the vehicle. According to\nWhite et al. 2012 [7], high-clearance\
    \ tractors were expected to play a more central role in\nFBP as the wheeled vehicles\
    \ of choice due to their high vertical clearance, availability, and\nease of use.\
    \ They can be used for continuous measurements at different stages of the crop\n\
    growth process and can operate for longer periods compared to the UAVs. A variation\
    \ of\nhigh clearance tractors in the form of mobile motorized platforms, which\
    \ eliminate the\nneed for human labor in the ﬁeld, have been developed and tested\
    \ in various phenotyping\napplications [114,115].\nPhenotyping data collected\
    \ using these FBP systems faces the challenge of instability\nduring motion and\
    \ weather changes, which cause occlusion and mal-alignment in the\nimages. This\
    \ is partially addressed by using proximal sensing at slower speeds in order to\n\
    improve image resolution (although this limits the areal coverage in one ﬂight\
    \ and does\nnot fully solve the misalignment). Data processing for this data has\
    \ signiﬁcantly improved\nin recent years with AI-enabled snapshot and line scanning\
    \ imaging software, optimized\nspeciﬁcally for unmanned aircraft such as structure-from-motion\
    \ photogrammetry [87,116].\nOne approach that has been proposed to enhance the\
    \ application of these platforms for\nsensing in ﬁeld conditions is adapting ﬁeld\
    \ conditions to align with the HTP ﬁeld tech-\nniques (rather than the traditional\
    \ approach of adapting the instruments for the ﬁeld),\ndepending on the crop of\
    \ interest without compromising the realistic crop evaluation in\nﬁeld conditions\
    \ [6].\n5. Phenotyping Communities and Facilities\nThe growth in plant phenotyping\
    \ research coupled with the integration of AI tech-\nnology has fostered the development\
    \ of laboratories and centers equipped with high-\nthroughput phenotyping technologies.\
    \ Some of these plant phenotyping centers are\nmembers of the International Plant\
    \ Phenotyping Network (IPPN), which works with\nvarious member organizations in\
    \ academia and industry to distribute relevant information\nabout plant phenotyping\
    \ and increase its visibility [117]. Partner facilities such as the\nAustralian\
    \ Plant Phenomics Facility, a government-funded national facility, provide access\n\
    to infrastructure such as glass-house automation technologies, digital imaging\
    \ technologies,\nlong-term data storage, etc. [118]. Such facilities have and\
    \ continue to provide subsidized\naccess to advanced AI-phenotyping technologies\
    \ that would otherwise be inaccessible due\nto the costs of operation and maintenance.\
    \ In addition, as high-throughput phenotyping\nbecoming more common, there has\
    \ come the issue of data-merging with many laboratories\ngathering phenotypic\
    \ data that rarely enter the public domain where it could be accessed\nby other\
    \ institutions to foster interdisciplinary research [8]. Networks such as the\
    \ IPPN\ncontinue to provide access to phenotyping information generated by member\
    \ organiza-\ntions which is key in enabling cooperation between the organizations\
    \ and advancing the\nphenomics agenda through collaborative research.\nBesides\
    \ the academic institutions and government organizations, private industry\ncompanies\
    \ (a few of which have been highlighted here) are establishing themselves as\n\
    key providers and facilitators of plant phenotyping AI technology around the world.\
    \ Bio-\npute technology provides high-end research instruments such as multispectral\
    \ cameras\nfor ﬁeld phenotyping, drones for aerial photography, and provides after-sales\
    \ support\nservices to their customers. In partnership with universities and research\
    \ institutes, Bio-\npute provides innovations that are contributing to the progress\
    \ of plant phenotyping\nin China (http://www.bjbiopute.cn, accessed on 1 June\
    \ 2021). KeyGene is an agricul-\ntural biotechnology company providing tools for\
    \ precision breeding and digital pheno-\ntyping investing in deep learning-based\
    \ algorithms and virtual reality for data visual-\nization (https://www.keygene.com,\
    \ accessed on 1 June 2021). PhenoTrait Technology\nSensors 2021, 21, 4363\n14\
    \ of 19\nCo., Ltd. mainly focuses on plant phenotyping using the photosynthesis\
    \ characteris-\ntics of plants and promoting the use of phenotyping technologies\
    \ to improve crop qual-\nity, crop yield, and environmental conditions in China.\
    \ Some of their products include\nhigh-throughput phenotyping instruments, chlorophyll\
    \ ﬂuorescence imaging systems, etc.\n(http://www.phenotrait.com, accessed on 1\
    \ June 2021). Photon Systems Instruments (PSI)\nis a company in the Czech Republic\
    \ that also supplies a range of phenotyping systems,\nboth ﬁeld and laboratory-based,\
    \ including root system phenotyping. They have also in-\ncorporated machine learning\
    \ to integrate robotics into the systems they develop to better\nautomate the\
    \ processes (https://psi.cz, accessed 1 June 2021).\n6. Conclusions\nRecent advancements\
    \ in high-throughput phenotyping technologies have led to signif-\nicant strides\
    \ in plant phenomics. The on-going integration of artiﬁcial intelligence into\
    \ these\ntechnologies promises progression into smarter and much faster technologies\
    \ with signiﬁ-\ncantly lower input costs. In the area of phenotyping image data\
    \ analysis, the integration\nof AI into the data management pipeline of tomography\
    \ and thermography is on a lower\nscale in comparison to the other imaging techniques.\
    \ The application of deep learning in\nthe data analysis of these techniques is\
    \ promising, as it has been successfully implemented\nin analysis of composite\
    \ materials [88] and medical diagnostics [94]. As much as ﬁeld\nphenotyping is\
    \ the most effective way to collect phenotypic data, it is still being conducted\n\
    on a relatively lower scale than is possible. Artiﬁcial intelligence technologies\
    \ also require\nlarge amounts of data from various sources to improve their accuracy.\
    \ This provides an\nopportunity to invest more into the tailoring of current technologies\
    \ for ﬁeld data collection\nand the utilization of already existing AI adaptable\
    \ technologies, such as smartphones, to\nincrease the quantity of quality data.\
    \ Smartphones have become widespread consumer\nproducts, and the simplicity and\
    \ ease of use of their sensors suggest that their use can be\nexplored in agriculture\
    \ [104]. Some of the challenges that would need to be addressed are\nthat advanced\
    \ signal processing on smartphones has to cope with constraints such as low\n\
    battery life, restricted computational power, or limited bandwidth [104]. The\
    \ use of citizen\nscience alongside professional researchers [119] in data collection\
    \ also has the potential\nto aid in increasing the amount of data collected. The\
    \ overall goal of employing these\napproaches and technologies is to provide the\
    \ infrastructure that allows for tracking how\nplant traits progress throughout\
    \ the growing season and facilitate the coordination of data\nanalysis, management,\
    \ and utilization of results using AI methods.\nAuthor Contributions: Conceptualization,\
    \ S.N. and B.-K.C.; investigation, M.S.K. and I.B.; writing—\noriginal draft preparation,\
    \ S.N.; writing—review and editing, H.-K.S. and M.S.K.; visualization, S.N.\n\
    and I.B.; supervision, B.-K.C. and H.-K.S.; funding acquisition, B.-K.C. All authors\
    \ have read and\nagreed to the published version of the manuscript.\nFunding:\
    \ This research was funded by the National Institute of Food Science and Technology\
    \ (Project\nNo.: PJ0156892021) of the Rural Development Administration, Korea.\n\
    Institutional Review Board Statement: Not applicable.\nInformed Consent Statement:\
    \ Not applicable.\nData Availability Statement: No new data were created in this\
    \ study. Data sharing is not applicable\nto this article.\nConﬂicts of Interest:\
    \ The authors declare no conﬂict of interest.\nReferences\n1.\nUN. United Nations|Population\
    \ Division. Available online: https://www.un.org/development/desa/pd/ (accessed\
    \ on 10\nSeptember 2020).\n2.\nCosta, C.; Schurr, U.; Loreto, F.; Menesatti, P.;\
    \ Carpentier, S. Plant phenotyping research trends, a science mapping approach.\n\
    Front. Plant Sci. 2019, 9, 1–11. [CrossRef]\nSensors 2021, 21, 4363\n15 of 19\n\
    3.\nArvidsson, S.; Pérez-Rodríguez, P.; Mueller-Roeber, B. A growth phenotyping\
    \ pipeline for Arabidopsis thaliana integrating image\nanalysis and rosette area\
    \ modeling for robust quantiﬁcation of genotype effects. New Phytol. 2011, 191,\
    \ 895–907. [CrossRef]\n[PubMed]\n4.\nFurbank, R.T. Plant phenomics: From gene\
    \ to form and function. Funct. Plant Biol. 2009, 36, v–vi.\n5.\nHoule, D.; Govindaraju,\
    \ D.R.; Omholt, S. Phenomics: The next challenge. Nat. Rev. Genet. 2010, 11, 855–866.\
    \ [CrossRef]\n6.\nPauli, D. High-throughput phenotyping technologies in cotton\
    \ and beyond. In Proceedings of the Advances in Field-Based\nHigh-Throughput Phenotyping\
    \ and Data Management: Grains and Specialty Crops, Spokane, WA, USA, 9–10 November\
    \ 2015;\npp. 1–11.\n7.\nWhite, J.W.; Andrade-Sanchez, P.; Gore, M.A.; Bronson,\
    \ K.F.; Coffelt, T.A.; Conley, M.M.; Feldmann, K.A.; French, A.N.; Heun,\nJ.T.;\
    \ Hunsaker, D.J.; et al. Field-based phenomics for plant genetics research. Field\
    \ Crops Res. 2012, 133, 101–112. [CrossRef]\n8.\nFurbank, R.T.; Tester, M. Phenomics—Technologies\
    \ to relieve the phenotyping bottleneck. Trends Plant Sci. 2011, 16, 635–644.\n\
    [CrossRef] [PubMed]\n9.\nFahlgren, N.; Gehan, M.A.; Baxter, I. Lights, camera,\
    \ action: High-throughput plant phenotyping is ready for a close-up. Curr.\nOpin.\
    \ Plant Biol. 2015, 24, 93–99. [CrossRef]\n10.\nChen, D.; Neumann, K.; Friedel,\
    \ S.; Kilian, B.; Chen, M.; Altmann, T.; Klukas, C. Dissecting the phenotypic\
    \ components of crop\nplant growthand drought responses based on high-throughput\
    \ image analysis w open. Plant Cell 2014, 26, 4636–4655. [CrossRef]\n[PubMed]\n\
    11.\nWalter, T.; Shattuck, D.W.; Baldock, R.; Bastin, M.E.; Carpenter, A.E.; Duce,\
    \ S.; Ellenberg, J.; Fraser, A.; Hamilton, N.; Pieper, S.;\net al. Visualization\
    \ of image data from cells to organisms. Nat. Methods 2010, 7, S26–S41. [CrossRef]\
    \ [PubMed]\n12.\nOerke, E.C.; Steiner, U.; Dehne, H.W.; Lindenthal, M. Thermal\
    \ imaging of cucumber leaves affected by downy mildew and\nenvironmental conditions.\
    \ J. Exp. Bot. 2006, 57, 2121–2132. [CrossRef]\n13.\nChaerle, L.; Pineda, M.;\
    \ Romero-Aranda, R.; Van Der Straeten, D.; Barón, M. Robotized thermal and chlorophyll\
    \ ﬂuorescence\nimaging of pepper mild mottle virus infection in Nicotiana benthamiana.\
    \ Plant Cell Physiol. 2006, 47, 1323–1336. [CrossRef]\n14.\nZarco-Tejada, P.J.;\
    \ Berni, J.A.J.; Suárez, L.; Sepulcre-Cantó, G.; Morales, F.; Miller, J.R. Imaging\
    \ chlorophyll ﬂuorescence with an\nairborne narrow-band multispectral camera for\
    \ vegetation stress detection. Remote Sens. Environ. 2009, 113, 1262–1275. [CrossRef]\n\
    15.\nJensen, T.; Apan, A.; Young, F.; Zeller, L. Detecting the attributes of a\
    \ wheat crop using digital imagery acquired from a\nlow-altitude platform. Comput.\
    \ Electron. Agric. 2007, 59, 66–77. [CrossRef]\n16.\nMontes, J.M.; Utz, H.F.;\
    \ Schipprack, W.; Kusterer, B.; Muminovic, J.; Paul, C.; Melchinger, A.E. Near-infrared\
    \ spectroscopy on\ncombine harvesters to measure maize grain dry matter content\
    \ and quality parameters. Plant Breed. 2006, 125, 591–595. [CrossRef]\n17.\nBai,\
    \ G.; Ge, Y.; Hussain, W.; Baenziger, P.S.; Graef, G. A multi-sensor system for\
    \ high throughput ﬁeld phenotyping in soybean\nand wheat breeding. Comput. Electron.\
    \ Agric. 2016, 128, 181–192. [CrossRef]\n18.\nChaerle, L.; Van Der Straeten, D.\
    \ Imaging techniques and the early detection of plant stress. Trends Plant Sci.\
    \ 2000, 5, 495–501.\n[CrossRef]\n19.\nGupta, S.; Ibaraki, Y.; Trivedi, P. Applications\
    \ of RGB color imaging in plants. Plant Image Anal. 2014, 41–62. [CrossRef]\n\
    20.\nMontes, J.M.; Melchinger, A.E.; Reif, J.C. Novel throughput phenotyping platforms\
    \ in plant genetic studies. Trends Plant Sci. 2007,\n12, 433–436. [CrossRef] [PubMed]\n\
    21.\nCasanova, J.J.; O’Shaughnessy, S.A.; Evett, S.R.; Rush, C.M. Development\
    \ of a wireless computer vision instrument to detect\nbiotic stress in wheat.\
    \ Sensors 2014, 14, 17753–17769. [CrossRef] [PubMed]\n22.\nKruse, O.M.O.; Prats-Montalbán,\
    \ J.M.; Indahl, U.G.; Kvaal, K.; Ferrer, A.; Futsaether, C.M. Pixel classiﬁcation\
    \ methods for\nidentifying and quantifying leaf surface injury from digital images.\
    \ Comput. Electron. Agric. 2014, 108, 155–165. [CrossRef]\n23.\nShakoor, N.; Lee,\
    \ S.; Mockler, T.C. High throughput phenotyping to accelerate crop breeding and\
    \ monitoring of diseases in the\nﬁeld. Curr. Opin. Plant Biol. 2017, 38, 184–192.\
    \ [CrossRef] [PubMed]\n24.\nLecun, Y.; Bengio, Y.; Hinton, G. Deep learning. Nature\
    \ 2015, 521, 436–444. [CrossRef]\n25.\nHardin, P.J.; Lulla, V.; Jensen, R.R.;\
    \ Jensen, J.R. Small Unmanned Aerial Systems (sUAS) for environmental remote sensing:\n\
    Challenges and opportunities revisited. GIScience Remote Sens. 2019, 56, 309–322.\
    \ [CrossRef]\n26.\nMookerjee, M.; Vieira, D.; Chan, M.A.; Gil, Y.; Goodwin, C.;\
    \ Shipley, T.F.; Tikoff, B. We need to talk: Facilitating communication\nbetween\
    \ ﬁeld-based geoscience and cyberinfrastructure communities. GSA Today 2015, 34–35.\
    \ [CrossRef]\n27.\nStewart, C.A.; Simms, S.; Plale, B.; Link, M.; Hancock, D.Y.;\
    \ Fox, G.C. What is cyberinfrastructure? In Proceedings of the\nProceedings of\
    \ the 38th Annual ACM SIGUCCS Fall Conference: Navigation and Discovery, Norfolk,\
    \ VA, USA, 24–27 October\n2010; pp. 37–44. [CrossRef]\n28.\nMadhavan, K.; Elmqvist,\
    \ N.; Vorvoreanu, M.; Chen, X.; Wong, Y.; Xian, H.; Dong, Z.; Johri, A. DIA2:\
    \ Web-based cyberinfrastructure\nfor visual analysis of funding portfolios. IEEE\
    \ Trans. Vis. Comput. Graph. 2014, 20, 1823–1832. [CrossRef]\n29.\nGoff, S.A.;\
    \ Vaughn, M.; McKay, S.; Lyons, E.; Stapleton, A.E.; Gessler, D.; Matasci, N.;\
    \ Wang, L.; Hanlon, M.; Lenards, A.; et al.\nThe iPlant collaborative: Cyberinfrastructure\
    \ for plant biology. Front. Plant Sci. 2011, 2, 1–16. [CrossRef]\n30.\nAksulu,\
    \ A.; Wade, M. A comprehensive review and synthesis of open source research. J.\
    \ Assoc. Inf. Syst. 2010, 11, 576–656.\n[CrossRef]\n31.\nFrankenﬁeld, J. Artiﬁcial\
    \ Intelligence (AI). Available online: https://www.investopedia.com/terms/a/artiﬁcial-intelligence-ai.\n\
    asp (accessed on 9 February 2021).\nSensors 2021, 21, 4363\n16 of 19\n32.\nPaschen,\
    \ U.; Pitt, C.; Kietzmann, J. Artiﬁcial intelligence: Building blocks and an innovation\
    \ typology. Bus. Horiz. 2020, 63,\n147–155. [CrossRef]\n33.\nFrey, L.J. Artiﬁcial\
    \ intelligence and integrated genotype–Phenotype identiﬁcation. Genes 2019, 10,\
    \ 18. [CrossRef]\n34.\nZhuang, Y.T.; Wu, F.; Chen, C.; Pan, Y. He Challenges and\
    \ opportunities: From big data to knowledge in AI 2.0. Front. Inf. Technol.\n\
    Electron. Eng. 2017, 18, 3–14. [CrossRef]\n35.\nRoscher, R.; Bohn, B.; Duarte,\
    \ M.F.; Garcke, J. Explainable Machine Learning for Scientiﬁc Insights and Discoveries.\
    \ IEEE Access\n2020, 8, 42200–42216. [CrossRef]\n36.\nSingh, A.; Ganapathysubramanian,\
    \ B.; Singh, A.K.; Sarkar, S. Machine Learning for High-Throughput Stress Phenotyping\
    \ in\nPlants. Trends Plant Sci. 2016, 21, 110–124. [CrossRef]\n37.\nRahaman, M.M.;\
    \ Ahsan, M.A.; Chen, M. Data-Mining Techniques for Image-based Plant Phenotypic\
    \ Traits Identiﬁcation and\nClassiﬁcation. Sci. Rep. 2019, 9, 1–11. [CrossRef]\
    \ [PubMed]\n38.\nHuang, K.Y. Application of artiﬁcial neural network for detecting\
    \ Phalaenopsis seedling diseases using color and texture features.\nComput. Electron.\
    \ Agric. 2007, 57, 3–11. [CrossRef]\n39.\nWetterich, C.B.; Kumar, R.; Sankaran,\
    \ S.; Belasque, J.; Ehsani, R.; Marcassa, L.G. A comparative study on application\
    \ of computer\nvision and ﬂuorescence imaging spectroscopy for detection of citrus\
    \ huanglongbing disease in USA and Brazil. Opt. InfoBase\nConf. Pap. 2013, 2013.\
    \ [CrossRef]\n40.\nSommer, C.; Gerlich, D.W. Machine learning in cell biology-teaching\
    \ computers to recognize phenotypes. J. Cell Sci. 2013, 126,\n5529–5539. [CrossRef]\n\
    41.\nSadeghi-Tehran, P.; Sabermanesh, K.; Virlet, N.; Hawkesford, M.J. Automated\
    \ method to determine two critical growth stages of\nwheat: Heading and ﬂowering.\
    \ Front. Plant Sci. 2017, 8, 1–14. [CrossRef]\n42.\nBrichet, N.; Fournier, C.;\
    \ Turc, O.; Strauss, O.; Artzet, S.; Pradal, C.; Welcker, C.; Tardieu, F.; Cabrera-Bosquet,\
    \ L. A robot-assisted\nimaging pipeline for tracking the growths of maize ear\
    \ and silks in a high-throughput phenotyping platform. Plant Methods 2017,\n13,\
    \ 1–12. [CrossRef]\n43.\nWilf, P.; Zhang, S.; Chikkerur, S.; Little, S.A.; Wing,\
    \ S.L.; Serre, T. Computer vision cracks the leaf code. Proc. Natl. Acad. Sci.\
    \ USA\n2016, 113, 3305–3310. [CrossRef]\n44.\nSabanci, K.; Toktas, A.; Kayabasi,\
    \ A. Grain classiﬁer with computer vision usingadaptive neuro-fuzzy inference\
    \ system.pdf. J. Sci.\nFood Agric. 2017, 97, 3994–4000. [CrossRef]\n45.\nSabanci,\
    \ K.; Kayabasi, A.; Toktas, A. Computer vision-based method for classiﬁcation\
    \ of wheat grains using artiﬁcial neural\nnetwork. J. Sci. Food Agric. 2017, 97,\
    \ 2588–2593. [CrossRef] [PubMed]\n46.\nLin, P.; Li, X.L.; Chen, Y.M.; He, Y. A\
    \ Deep Convolutional Neural Network Architecture for Boosting Image Discrimination\n\
    Accuracy of Rice Species. Food Bioprocess Technol. 2018, 11, 765–773. [CrossRef]\n\
    47.\nSingh, A.K.; Ganapathysubramanian, B.; Sarkar, S.; Singh, A. Deep Learning\
    \ for Plant Stress Phenotyping: Trends and Future\nPerspectives. Trends Plant\
    \ Sci. 2018, 23, 883–898. [CrossRef]\n48.\nPound, M.P.; Atkinson, J.A.; Townsend,\
    \ A.J.; Wilson, M.H.; Grifﬁths, M.; Jackson, A.S.; Bulat, A.; Tzimiropoulos, G.;\
    \ Wells, D.M.;\nMurchie, E.H.; et al. Deep machine learning provides state-of-the-art\
    \ performance in image-based plant phenotyping. GigaScience\n2017, 6, 1–10. [CrossRef]\
    \ [PubMed]\n49.\nFuentes, A.; Yoon, S.; Kim, S.C.; Park, D.S. A robust deep-learning-based\
    \ detector for real-time tomato plant diseases and pests\nrecognition. Sensors\
    \ 2017, 17, 2022. [CrossRef] [PubMed]\n50.\nAbdalla, A.; Cen, H.; Wan, L.; Rashid,\
    \ R.; Weng, H.; Zhou, W.; He, Y. Fine-tuning convolutional neural network with\
    \ transfer\nlearning for semantic segmentation of ground-level oilseed rape images\
    \ in a ﬁeld with high weed pressure. Comput. Electron.\nAgric. 2019, 167, 105091.\
    \ [CrossRef]\n51.\nEspejo-Garcia, B.; Mylonas, N.; Athanasakos, L.; Vali, E.;\
    \ Fountas, S. Combining generative adversarial networks and agricultural\ntransfer\
    \ learning for weeds identiﬁcation. Biosyst. Eng. 2021, 204, 79–89. [CrossRef]\n\
    52.\nBarbedo, J.G.A. Impact of dataset size and variety on the effectiveness of\
    \ deep learning and transfer learning for plant disease\nclassiﬁcation. Comput.\
    \ Electron. Agric. 2018, 153, 46–53. [CrossRef]\n53.\nWang, G.; Sun, Y.; Wang,\
    \ J. Automatic Image-Based Plant Disease Severity Estimation Using Deep Learning.\
    \ Comput. Intell.\nNeurosci. 2017, 2017. [CrossRef]\n54.\nBuzzy, M.; Thesma, V.;\
    \ Davoodi, M.; Velni, J.M. Real-time plant leaf counting using deep object detection\
    \ networks. Sensors 2020,\n20, 6896. [CrossRef]\n55.\nGhosal, S.; Zheng, B.; Chapman,\
    \ S.C.; Potgieter, A.B.; Jordan, D.R.; Wang, X.; Singh, A.K.; Singh, A.; Hirafuji,\
    \ M.; Ninomiya, S.;\net al. A Weakly Supervised Deep Learning Framework for Sorghum\
    \ Head Detection and Counting. Plant Phenomics 2019, 2019,\n1–14. [CrossRef] [PubMed]\n\
    56.\nAich, S.; Stavness, I. Leaf counting with deep convolutional and deconvolutional\
    \ networks. In Proceedings of the IEEE\nInternational Conference on Computer Vision\
    \ (Workshops), Venice, Italy, 22–29 October 2017; pp. 2080–2089. [CrossRef]\n\
    57.\nWang, X.; Xuan, H.; Evers, B.; Shrestha, S.; Pless, R.; Poland, J. High-throughput\
    \ phenotyping with deep learning gives insight\ninto the genetic architecture\
    \ of ﬂowering time in wheat. GigaScience 2019, 8, 1–11. [CrossRef]\n58.\nGhosal,\
    \ S.; Blystone, D.; Singh, A.K.; Ganapathysubramanian, B.; Singh, A.; Sarkar,\
    \ S. An explainable deep machine vision\nframework for plant stress phenotyping.\
    \ Proc. Natl. Acad. Sci. USA 2018, 115, 4613–4618. [CrossRef]\nSensors 2021, 21,\
    \ 4363\n17 of 19\n59.\nChaerle, L.; Van Der Straeten, D. Seeing is believing:\
    \ Imaging techniques to monitor plant health. Biochim. Biophys. Acta Gene\nStruct.\
    \ Expr. 2001, 1519, 153–166. [CrossRef]\n60.\nPerez-Sanz, F.; Navarro, P.J.; Egea-Cortines,\
    \ M. Plant phenomics: An overview of image acquisition technologies and image\
    \ data\nanalysis algorithms. GigaScience 2017, 6, 1–18. [CrossRef]\n61.\nCen,\
    \ H.; Weng, H.; Yao, J.; He, M.; Lv, J.; Hua, S.; Li, H.; He, Y. Chlorophyll ﬂuorescence\
    \ imaging uncovers photosynthetic\nﬁngerprint of citrus Huanglongbing. Front.\
    \ Plant Sci. 2017, 8, 1–11. [CrossRef]\n62.\nLichtenthaler, H.K.; Langsdorf, G.;\
    \ Lenk, S.; Buschmann, C. Chlorophyll ﬂuorescence imaging of photosynthetic activity\
    \ with the\nﬂash-lamp ﬂuorescence imaging system. Photosynthetica 2005, 43, 355–369.\
    \ [CrossRef]\n63.\nEhlert, B.; Hincha, D.K. Chlorophyll ﬂuorescence imaging accurately\
    \ quantiﬁes freezing damage and cold acclimation responses\nin Arabidopsis leaves.\
    \ Plant Methods 2008, 4, 1–7. [CrossRef] [PubMed]\n64.\nZheng, H.; Zhou, X.; He,\
    \ J.; Yao, X.; Cheng, T.; Zhu, Y.; Cao, W.; Tian, Y. Early season detection of\
    \ rice plants using RGB, NIR-G-B\nand multispectral images from unmanned aerial\
    \ vehicle (UAV). Comput. Electron. Agric. 2020, 169, 105223. [CrossRef]\n65.\n\
    Padmavathi, K.; Thangadurai, K. Implementation of RGB and grayscale images in\
    \ plant leaves disease detection—Comparative\nstudy. Indian J. Sci. Technol. 2016,\
    \ 9, 4–9. [CrossRef]\n66.\nWang, X.; Yang, W.; Wheaton, A.; Cooley, N.; Moran,\
    \ B. Automated canopy temperature estimation via infrared thermography: A\nﬁrst\
    \ step towards automated plant water stress monitoring. Comput. Electron. Agric.\
    \ 2010, 73, 74–83. [CrossRef]\n67.\nMunns, R.; James, R.A.; Sirault, X.R.R.; Furbank,\
    \ R.T.; Jones, H.G. New phenotyping methods for screening wheat and barley for\n\
    beneﬁcial responses to water deﬁcit. J. Exp. Bot. 2010, 61, 3499–3507. [CrossRef]\
    \ [PubMed]\n68.\nUrrestarazu, M. Infrared thermography used to diagnose the effects\
    \ of salinity in a soilless culture. Quant. InfraRed Thermogr. J.\n2013, 10, 1–8.\
    \ [CrossRef]\n69.\nFittschen, U.E.A.; Kunz, H.H.; Höhner, R.; Tyssebotn, I.M.B.;\
    \ Fittschen, A. A new micro X-ray ﬂuorescence spectrometer for\nin vivo elemental\
    \ analysis in plants. X-ray Spectrom. 2017, 46, 374–381. [CrossRef]\n70.\nChow,\
    \ T.H.; Tan, K.M.; Ng, B.K.; Razul, S.G.; Tay, C.M.; Chia, T.F.; Poh, W.T. Diagnosis\
    \ of virus infection in orchid plants with\nhigh-resolution optical coherence\
    \ tomography. J. Biomed. Opt. 2009, 14, 014006. [CrossRef]\n71.\nGarbout, A.;\
    \ Munkholm, L.J.; Hansen, S.B.; Petersen, B.M.; Munk, O.L.; Pajor, R. The use\
    \ of PET/CT scanning technique for 3D\nvisualization and quantiﬁcation of real-time\
    \ soil/plant interactions. Plant Soil 2012, 352, 113–127. [CrossRef]\n72.\nAˇc,\
    \ A.; Malenovský, Z.; Hanuš, J.; Tomášková, I.; Urban, O.; Marek, M.V. Near-distance\
    \ imaging spectroscopy investigating\nchlorophyll ﬂuorescence and photosynthetic\
    \ activity of grassland in the daily course. Funct. Plant Biol. 2009, 36, 1006–1015.\n\
    [CrossRef]\n73.\nVigneau, N.; Ecarnot, M.; Rabatel, G.; Roumet, P. Potential of\
    \ ﬁeld hyperspectral imaging as a non destructive method to assess\nleaf nitrogen\
    \ content in Wheat. Field Crops Res. 2011, 122, 25–31. [CrossRef]\n74.\nBehmann,\
    \ J.; Steinrücken, J.; Plümer, L. Detection of early plant stress responses in\
    \ hyperspectral images. ISPRS J. Photogramm.\nRemote Sens. 2014, 93, 98–111. [CrossRef]\n\
    75.\nPrey, L.; von Bloh, M.; Schmidhalter, U. Evaluating RGB imaging and multispectral\
    \ active and hyperspectral passive sensing for\nassessing early plant vigor in\
    \ winter wheat. Sensors 2018, 18, 2931. [CrossRef]\n76.\nLi, L.; Zhang, Q.; Huang,\
    \ D. A review of imaging techniques for plant phenotyping. Sensors 2014, 14, 20078–20111.\
    \ [CrossRef]\n77.\nHan, X.F.; Laga, H.; Bennamoun, M. Image-based 3D Object Reconstruction:\
    \ State-of-the-Art and Trends in the Deep Learning\nEra. IEEE Trans. Pattern Anal.\
    \ Mach. Intell. 2019, 43, 1578–1604. [CrossRef] [PubMed]\n78.\nNguyen, C.V.; Fripp,\
    \ J.; Lovell, D.R.; Furbank, R.; Kuffner, P.; Daily, H.; Sirault, X. 3D scanning\
    \ system for automatic high-\nresolution plant phenotyping. In Proceedings of\
    \ the 2016 International Conference on Digital Image Computing: Techniques and\n\
    Applications (DICTA), Gold Coast, Australia, 30 November–2 December 2016.\n79.\n\
    Matovic, M.D. Biomass: Detection, Production and Usage; BoD—Books on Demand: Norderstedt,\
    \ Germany, 2011; ISBN 9533074922.\n80.\nLiu, H.; Bruning, B.; Garnett, T.; Berger,\
    \ B. Hyperspectral imaging and 3D technologies for plant phenotyping: From satellite\
    \ to\nclose-range sensing. Comput. Electron. Agric. 2020, 175, 105621. [CrossRef]\n\
    81.\nZhu, H.; Chu, B.; Fan, Y.; Tao, X.; Yin, W.; He, Y. Hyperspectral Imaging\
    \ for Predicting the Internal Quality of Kiwifruits Based on\nVariable Selection\
    \ Algorithms and Chemometric Models. Sci. Rep. 2017, 7, 1–13. [CrossRef] [PubMed]\n\
    82.\nZhang, M.; Li, G. Visual detection of apple bruises using AdaBoost algorithm\
    \ and hyperspectral imaging. Int. J. Food Prop. 2018,\n21, 1598–1607. [CrossRef]\n\
    83.\nGu, Q.; Sheng, L.; Zhang, T.; Lu, Y.; Zhang, Z.; Zheng, K.; Hu, H.; Zhou,\
    \ H. Early detection of tomato spotted wilt virus infection\nin tobacco using\
    \ the hyperspectral imaging technique and machine learning algorithms. Comput.\
    \ Electron. Agric. 2019, 167,\n105066. [CrossRef]\n84.\nRamesh, V. A Review on\
    \ the Application of Deep Learning in Thermography. Int. J. Eng. Manag. Res. 2017,\
    \ 7, 489–493.\n85.\nPineda, M.; Barón, M.; Pérez-Bueno, M.L. Thermal imaging for\
    \ plant stress detection and phenotyping. Remote Sens. 2021, 13, 68.\n[CrossRef]\n\
    86.\nMessina, G.; Modica, G. Applications of UAV thermal imagery in precision\
    \ agriculture: State of the art and future research\noutlook. Remote Sens. 2020,\
    \ 12, 1491. [CrossRef]\n87.\nMaes, W.H.; Huete, A.R.; Steppe, K. Optimizing the\
    \ processing of UAV-based thermal imagery. Remote Sens. 2017, 9, 476.\n[CrossRef]\n\
    Sensors 2021, 21, 4363\n18 of 19\n88.\nBang, H.T.; Park, S.; Jeon, H. Defect identiﬁcation\
    \ in composite materials via thermography and deep learning techniques. Compos.\n\
    Struct. 2020, 246, 112405. [CrossRef]\n89.\nMoshou, D.; Bravo, C.; West, J.; Wahlen,\
    \ S.; McCartney, A.; Ramon, H. Automatic detection of “yellow rust” in wheat using\n\
    reﬂectance measurements and neural networks. Comput. Electron. Agric. 2004, 44,\
    \ 173–188. [CrossRef]\n90.\nFlavel, R.J.; Guppy, C.N.; Tighe, M.; Watt, M.; McNeill,\
    \ A.; Young, I.M. Non-destructive quantiﬁcation of cereal roots in soil using\n\
    high-resolution X-ray tomography. J. Exp. Bot. 2012, 63, 2503–2511. [CrossRef]\n\
    91.\nGregory, P.J.; Hutchison, D.J.; Read, D.B.; Jenneson, P.M.; Gilboy, W.B.;\
    \ Morton, E.J. Non-invasive imaging of roots with high\nresolution X-ray micro-tomography.\
    \ Plant Soil 2003, 255, 351–359. [CrossRef]\n92.\nYang, W.; Xu, X.; Duan, L.;\
    \ Luo, Q.; Chen, S.; Zeng, S.; Liu, Q. High-throughput measurement of rice tillers\
    \ using a conveyor\nequipped with X-ray computed tomography. Rev. Sci. Instrum.\
    \ 2011, 82, 1–8. [CrossRef]\n93.\nAtkinson, J.A.; Pound, M.P.; Bennett, M.J.;\
    \ Wells, D.M. Uncovering the hidden half of plants using new advances in root\n\
    phenotyping. Curr. Opin. Biotechnol. 2019, 55, 1–8. [CrossRef]\n94.\nShi, F.;\
    \ Wang, J.; Shi, J.; Wu, Z.; Wang, Q.; Tang, Z.; He, K.; Shi, Y.; Shen, D. Review\
    \ of artiﬁcial intelligence techniques in imaging\ndata acquisition, segmentation\
    \ and diagnosis for COVID-19. IEEE Rev. Biomed. Eng. 2020, 14, 4–15. [CrossRef]\n\
    95.\nAtkins, D.E.; Droegemeier, K.K.; Feldman, S.I.; García Molina, H.; Klein,\
    \ M.L.; Messerschmitt, D.G.; Messina, P.; Ostriker, J.P.;\nWright, M.H.; Garcia-molina,\
    \ H.; et al. Revolutionizing Science and Engineering through Cyberinfrastructure.\
    \ Science 2003, 84.\n96.\nLee, C.P.; Dourish, P.; Mark, G. The human infrastructure\
    \ of cyberinfrastructure. In Proceedings of the 2006 20th Anniversary\nConference\
    \ on Computer Supported Cooperative Work, Banff, AB, Canada, 4–8 November 2006;\
    \ pp. 483–492. [CrossRef]\n97.\nUIC Advanced Cyberinfrastructure for Education\
    \ and Research. Available online: https://acer.uic.edu/get-started/resource-\n\
    pricing/ (accessed on 4 September 2020).\n98.\nYang, C.; Raskin, R.; Goodchild,\
    \ M.; Gahegan, M. Geospatial Cyberinfrastructure: Past, present and future. Comput.\
    \ Environ.\nUrban Syst. 2010, 34, 264–277. [CrossRef]\n99.\nMichener, W.K.; Allard,\
    \ S.; Budden, A.; Cook, R.B.; Douglass, K.; Frame, M.; Kelling, S.; Koskela, R.;\
    \ Tenopir, C.; Vieglais, D.A.\nParticipatory design of DataONE-Enabling cyberinfrastructure\
    \ for the biological and environmental sciences. Ecol. Inform. 2012,\n11, 5–15.\
    \ [CrossRef]\n100. Wang, L.; Chen, D.; Hu, Y.; Ma, Y.; Wang, J. Towards enabling\
    \ Cyberinfrastructure as a Service in Clouds. Comput. Electr. Eng.\n2013, 39,\
    \ 3–14. [CrossRef]\n101. Kvilekval, K.; Fedorov, D.; Obara, B.; Singh, A.; Manjunath,\
    \ B.S. Bisque: A platform for bioimage analysis and management.\nBioinformatics\
    \ 2009, 26, 544–552. [CrossRef] [PubMed]\n102. Shah, S.K. Motivation, governance,\
    \ and the viability of hybrid forms in open source software development. Manag.\
    \ Sci. 2006, 52,\n1000–1014. [CrossRef]\n103. Olson, D.L.; Rosacker, K. Crowdsourcing\
    \ and open source software participation. Serv. Bus. 2013, 7, 499–511. [CrossRef]\n\
    104. Bauckhage, C.; Kersting, K. Data Mining and Pattern Recognition in Agriculture.\
    \ KI Künstl. Intell. 2013, 27, 313–324. [CrossRef]\n105. Kuhlgert, S.; Austic,\
    \ G.; Zegarac, R.; Osei-Bonsu, I.; Hoh, D.; Chilvers, M.I.; Roth, M.G.; Bi, K.;\
    \ TerAvest, D.; Weebadde, P.; et al.\nMultispeQ Beta: A tool for large-scale plant\
    \ phenotyping connected to the open photosynQ network. R. Soc. Open Sci. 2016,\
    \ 3.\n[CrossRef] [PubMed]\n106. Gehan, M.A.; Fahlgren, N.; Abbasi, A.; Berry,\
    \ J.C.; Callen, S.T.; Chavez, L.; Doust, A.N.; Feldman, M.J.; Gilbert, K.B.; Hodge,\
    \ J.G.;\net al. PlantCV v2: Image analysis software for high-throughput plant\
    \ phenotyping. PeerJ 2017, 2017, 1–23. [CrossRef] [PubMed]\n107. Tzutalin LabelImg.\
    \ Available online: https://github.com/tzutalin/labelImg (accessed on 14 September\
    \ 2020).\n108. Ubbens, J.R.; Stavness, I. Deep plant phenomics: A deep learning\
    \ platform for complex plant phenotyping tasks. Front. Plant Sci.\n2017, 8. [CrossRef]\n\
    109. Howard, A.G.; Zhu, M.; Chen, B.; Kalenichenko, D.; Wang, W.; Weyand, T.;\
    \ Andreetto, M.; Adam, H. MobileNets: Efﬁcient\nConvolutional Neural Networks\
    \ for Mobile Vision Applications. arXiv 2017, arXiv:1704.04861.\n110. Sandler,\
    \ M.; Howard, A.; Zhu, M.; Zhmoginov, A.; Chen, L.C. MobileNetV2: Inverted Residuals\
    \ and Linear Bottlenecks. In\nProceedings of the 2018 IEEE/CVF Conference on Computer\
    \ Vision and Pattern Recognition, Salt Lake City, UT, USA, 18–23 June\n2018; pp.\
    \ 4510–4520. [CrossRef]\n111. Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro,\
    \ C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; Ghemawat, S.; et al.\nTensorFlow:\
    \ Large-Scale Machine Learning on Heterogeneous Distributed Systems. arXiv 2015,\
    \ arXiv:1603.04467.\n112. Ramcharan, A.; McCloskey, P.; Baranowski, K.; Mbilinyi,\
    \ N.; Mrisho, L.; Ndalahwa, M.; Legg, J.; Hughes, D.P. A mobile-based\ndeep learning\
    \ model for cassava disease diagnosis. Front. Plant Sci. 2019, 10, 1–8. [CrossRef]\n\
    113. Merz, T.; Chapman, S. Autonomous Unmanned Helicopter System for Remote Sensing\
    \ Missions in Unknown Environments.\nISPRS Int. Arch. Photogramm. Remote Sens.\
    \ Spat. Inf. Sci. 2012, XXXVIII-1, 143–148. [CrossRef]\n114. Andrade-Sanchez,\
    \ P.; Gore, M.A.; Heun, J.T.; Thorp, K.R.; Carmo-Silva, A.E.; French, A.N.; Salvucci,\
    \ M.E.; White, J.W. Devel-\nopment and evaluation of a ﬁeld-based high-throughput\
    \ phenotyping platform. Funct. Plant Biol. 2014, 41, 68–79. [CrossRef]\n[PubMed]\n\
    115. Chawade, A.; Van Ham, J.; Blomquist, H.; Bagge, O.; Alexandersson, E.; Ortiz,\
    \ R. High-throughput ﬁeld-phenotyping tools for\nplant breeding and precision\
    \ agriculture. Agronomy 2019, 9, 258. [CrossRef]\nSensors 2021, 21, 4363\n19 of\
    \ 19\n116. Virlet, N.; Sabermanesh, K.; Sadeghi-Tehran, P.; Hawkesford, M.J. Field\
    \ Scanalyzer: An automated robotic ﬁeld phenotyping\nplatform for detailed crop\
    \ monitoring. Funct. Plant Biol. 2017, 44, 143–153. [CrossRef]\n117. IPPN International\
    \ Plant Phenotyping Network.\nAvailable online: https://www.plant-phenotyping.org/\
    \ (accessed on\n13 April 2020).\n118. APPF Australian Plant Phenomics Facility.\
    \ Available online: https://www.plantphenomics.org.au/ (accessed on 13 April 2020).\n\
    119. Cooper, C.B.; Shirk, J.; Zuckerberg, B. The Invisible Prevalence of Citizen\
    \ Science in Global Research: Migratory The Invisible\nPrevalence of Citizen Science\
    \ in Global Research: Migratory Birds and Climate Change. PLoS ONE 2014, 9, e106508.\
    \ [CrossRef]\n[PubMed]\n"
  inline_citation: '[5]'
  journal: Sensors
  key_findings: None specified in the context.
  limitations: This response does not give any limitations, but it does meet the other
    criteria for a good response.
  main_objective: None specified in the context.
  pdf_link: https://www.mdpi.com/1424-8220/21/13/4363/pdf?version=1624868958
  publication_year: 2021
  relevance_evaluation: '0.8-0.89: Addresses key issues of the point with novel, credible,
    and meaningful information. Adds substantial value to the review.'
  relevance_score: 0.85
  relevance_score1: 0
  relevance_score2: 0
  study_location: null
  technologies_used: null
  title: 'Review: Application of Artificial Intelligence in Phenomics'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Sadiq, M. I., Rahman, S. M. P., Kayes, S., Sumaita, A. H., & Chisty,
    N. A. (2021). A Review on the Imaging Approaches in Agriculture with Crop and
    Soil Sensing Methodologies. 2021 Fifth International Conference On Intelligent
    Computing in Data Sciences (ICDS). https://doi.org/10.1109/ICDS53782.2021.9626711
  authors:
  - Sadiq M.I.
  - Pramito Rahman S.M.
  - Kayes S.
  - Sumaita A.H.
  - Chisty N.A.
  citation_count: '1'
  data_sources: Image datasets, sensor data
  description: Climate change, droughts, and growing food demands are the rising problems
    in the field of agriculture. Agricultural methodologies need acclimatization to
    these growing problems with technological innovations. The infrared and visible
    imaging approaches are substantial to ascertain crop health, temperature and humidity
    distributions, salinity, water stress, and visible pattern recognition for the
    widespread areas of agricultural lands. The paper has put forward a review on
    the viable image processing methods incorporating computer vision, machine learning,
    infrared and visible imaging, and complex crop and soil nutrient sensing technologies,
    which are being used to detect the pivotal elements required for the monitoring
    systems. The discussed approaches and analyses, if implemented, have the prospects
    of expanding the field of agriculture with the emerging technologies, which would
    be able to adapt to the rising demand of quality crop productions.
  doi: 10.1109/ICDS53782.2021.9626711
  explanation: This paper provides an in-depth review of research on image-based crop
    monitoring using visual and thermal imaging combined with computer vision algorithms.
    The authors summarize the current state and advancements in this field, highlighting
    the effectiveness and importance of integrating these technologies.
  extract_1: '"Emerging sensor technologies including thermal and hyperspectral imaging
    offer improved resolutions that enable monitoring of plant growth and health with
    high accuracy. Machine learning and computer vision algorithms play a crucial
    role in data extraction from large image datasets. Additionally, improvements
    in sensor technology and image processing algorithms allow for the use of lower-cost
    thermal camera models in crop and soil health monitoring."'
  extract_2: '"Computer vision techniques for image analysis have gained prominence
    in agriculture, offering advantages such as real-time monitoring, precise data
    acquisition, and integration with other precision agriculture technologies. Integration
    of high-resolution cameras and computer vision enables early detection of crop
    diseases and pests, assessment of irrigation system performance, and optimized
    water management."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2021 Fifth International Conf...
    A review on the Imaging Approaches in Agriculture with Crop and Soil Sensing Methodologies
    Publisher: IEEE Cite This PDF Md Ijaj Sadiq; S. M. Pramito Rahman; Shakiyen Kayes;
    Afnan Hossain Sumaita; Nafiz Ahmed Chisty All Authors 1 Cites in Paper 150 Full
    Text Views Abstract Document Sections I. Introduction II. Computer Vision In Agriculture
    III. Visible and thermal Imaging IV. Crop and Soil Sensing Methodologies V. Data
    Analysing Methodologies Show Full Outline Authors Figures References Citations
    Keywords Metrics Abstract: Climate change, droughts, and growing food demands
    are the rising problems in the field of agriculture. Agricultural methodologies
    need acclimatization to these growing problems with technological innovations.
    The infrared and visible imaging approaches are substantial to ascertain crop
    health, temperature and humidity distributions, salinity, water stress, and visible
    pattern recognition for the widespread areas of agricultural lands. The paper
    has put forward a review on the viable image processing methods incorporating
    computer vision, machine learning, infrared and visible imaging, and complex crop
    and soil nutrient sensing technologies, which are being used to detect the pivotal
    elements required for the monitoring systems. The discussed approaches and analyses,
    if implemented, have the prospects of expanding the field of agriculture with
    the emerging technologies, which would be able to adapt to the rising demand of
    quality crop productions. Published in: 2021 Fifth International Conference On
    Intelligent Computing in Data Sciences (ICDS) Date of Conference: 20-22 October
    2021 Date Added to IEEE Xplore: 01 December 2021 ISBN Information: DOI: 10.1109/ICDS53782.2021.9626711
    Publisher: IEEE Conference Location: Fez, Morocco SECTION I. Introduction Agriculture
    has long been the primary occupation of many nations and has contributed significantly
    to their economic developments. When it comes to cultivating crops, advancements
    in agricultural production are very welcomed in the modern era. Thanks to technological
    advancements, crop fields can now be monitored automatically. It would benefit
    both the farmers and the research organizations. It is difficult for a human being
    to monitor the crop field and determine the conditions physically. There have
    been numerous efforts to improve the crop’s health management systems. It is also
    important to track agricultural yield and chemical fertilizers. Additionally,
    if a crop is cultivated several times in a limited farming area, the soil nutrients
    may be depleted. Crop growth can be enhanced if soil quality is improved. A variety
    of soil testing techniques could be conducted on the fields. However, some thermal
    imaging cameras were used in the past that permitted image processing only when
    a predetermined range was achieved. On the other hand, many upgraded thermal cameras
    may now be used to process photographs with ease. Factors like air quality, soil
    moisture, and pH level are significant for farming and can be monitored for optimal
    production. After undertaking a thorough examination of the discipline, a sample
    of approximately 27 papers was researched and analyzed in detail to provide an
    insight into the existing studies. Four terms were chosen for discussion: visible
    and thermal imaging, crop and soil sensing methodologies, data analysing methodologies,
    and applications. SECTION II. Computer Vision In Agriculture Computer Vision works
    in two stages: image acquisition and image processing. The marvels of Computer
    Vision are the acquisition of image data, processing of the images, and analysis
    of the data using Machine Learning, Artificial Intelligence and Convolution Networks.
    Diego Inácio Patrício, et al. (2018) classified some papers into groups, year,
    crop, device, target, classifier for systematic review. Diseases and pests, Phenology
    and Phenotyping, and Grain quality were the parameters of the classifications.
    The broader applications and techniques of machine learning, video, and image
    processing in detecting agricultural patterns were discussed. Their study focused
    and emphasized the new approaches in computer vision and artificial intelligence
    that can further the field of agriculture for better food production, quality,
    and safety [1]. Mukesh Kumar Tripathi, et al. (2016) described the significance
    of computer vision in vegetables and fruits and the corresponding methods required
    to classify and detect the diseases. The research of computer vision in fruits
    and vegetables and their overall performance, comparison of performance, advantages,
    and disadvantages were analyzed, including a framework for classification and
    recognition of vegetables and fruits [2]. Alan Bauer et al. (2019) analysis shows
    that Aerial imaging is widely used for crop monitoring during growth seasons.
    Computer vision, Machine learning, and Deep learning were used to decipher the
    large dataset of images to obtain phenotypic information. AirSurf - an analytical
    platform was introduced to determine normalized difference vegetation index (NDVI),
    aerial imagery for data collection, crop counting using deep learning, image processing
    using computer vision, and crop quality assessment using machine learning. The
    heads and plantation layouts of Lettuce were measured with the help of ultra-large
    NDVI images. Their research and study improved lettuce production and quantified
    reliable crop quality by ameliorating the yield of lettuces [3]. Computer Vision
    needs thermal and visible imaging data to work, the following section discusses
    the methodologies behind the image acquisition technologies. SECTION III. Visible
    and thermal Imaging Thermal and visible imaging is being used in the new era of
    crop and soil health monitoring systems. The visible and infrared radiation patterns
    of objects are captured by camera sensors and turned into images. These images
    can be analyzed for making decisions on crop health. To monitor crops with imaging
    sensors, different methods are being used. Effective imaging methods could include
    measuring and analyzing data taken wirelessly from thermal sensors and visible
    cameras placed in a drone or a satellite. R. Vadivambal et al. (2011) shared an
    analytical review that thermometers, thermocouples, thermistors, resistance, thermal
    detectors cannot be used without a physical connection with the plants. However,
    the thermal imaging process can solve the barrier which can be used without physical
    attachment. The signal processing unit receives the electrical impulse and transforms
    the data into a thermal image in their mentioned process [4]. Yasin Osroosh et
    al. (2018) discussed thermal imaging and analysis through the data collected by
    low-cost thermal RGB cameras. The thermal-based approach can detect the effects
    of salinity and water stress on crops. Thermal sensing technologies on the horizon
    combine reliability, precision, and low cost. On the other hand, affordable single-board
    computers like the Raspberry Pi have made onboard image processing possible. One
    of the drawbacks with imagers was optimizing power usage or maintaining the power
    supply for operations that required uninterrupted monitoring for a long duration
    of time [5]. Ruwaida Ramly et al. (2020) were effective in detecting plant’s surface
    temperatures. The consumer can observe the plants using live streaming throughout
    the day. Since the device is totally automated, customers are not required to
    water their plants when the temperature rises. Thus, their primary objective of
    detecting plant water irrigation over an agricultural plot using thermal imaging
    was accomplished successfully. However, certain parameters would yield more significant
    results, hence improve performance. The thermal camera’s resolution and specifications
    need to be upgraded to identify a wider range of area with greater precision.
    Additionally, the framework should include a machine learning or artificial intelligence
    application that enables two nodes to communicate [6]. A. K. Saha et al. (2018)
    enlightened that drones outfitted with appropriate cameras, sensors, and integrating
    modules would aid in achieving simple, efficient, and precise agriculture. They
    used different kinds of sensors such as RGB D sensors, gas sensors, and GPS modules.
    Through these sensors, they collected data and analyzed the data in the Raspberry
    Pi model to operate the seed and pesticide spray. Cloud-based storage was used
    to store the data. The prototype is complicated for implementation in developing
    countries. There are some necessary improvements required because they are collecting
    data with the help of a drone. The drones require a lot of power supply for a
    certain amount of time to collect and analyze the necessary data [7]. Kshitij
    Shinghal et al. (2011) built an intelligent humidity sensor. They combined all
    functions from sensor to bus interface on a single chip, resulting in an integrated
    intelligent sensor. The air humidity unit was calibrated using reference data
    obtained from Honeywell’s HCH 1000 series air humidity sensor, and it worked with
    an accuracy better than 1%. Hence their objective was achieved. Precision irrigation,
    automated weather stations, and agro-meteorological and microclimatic stations
    were the key applications for their intelligent humidity sensors [8]. Vasit Sagan
    et al. (2019) constructed a UAV-Based High-Resolution Thermal Imager for Vegetation
    Monitoring. The thermal camera software used in the study is specifically tailored
    for UAV applications. The FLIR mobile application, which connects to the camera
    through Bluetooth to set all camera and data capture settings, was used. The FLIR
    camera does not collect geotagged data and does not record GPS positions for each
    image during collection. Overall, the three thermal cameras tested (ICI 8640 P,
    FLIR Vue Pro R 640, and thermoMap) proved effective in vegetation monitoring and
    phenotyping. ICI was the best performer among the thermal cameras, but other cameras
    performed closely [9]. Gaetano Messina et al. (2020) analysis covered the most
    up-to-date thermal UAV RS technology [10]. UAV and Satellites Multispectral Imagery
    were discussed and analyzed thoroughly. Sentinel-2 satellite data was acquired
    in their analysis from Copernicus Open Access Hub which contained 13 bands in
    the visible, Near-infrared (NIR), and short-wavelength infrared (SWIR) spectral
    range [28,30,31,32]. PlanetScope’s imagery was also acquired for their study [29]
    and the images were radiometrically and geometrically corrected [29,33]. The characteristics
    of the multispectral and satellite images obtained through Parrot Disco-Pro AG(UAV),
    PlanetScope, and Sentinel-2(satellite) were discussed, including a comparison
    of their vegetation indices [34,35,36]. Brandon Stark et al. (2014) focused their
    work on thermal infrared remote sensing. Experiments were done with the thermal
    infrared (TIR) camera modules, which are costly but provides considerable information.
    A TIR framework is best used as a part of the sensor suite for research purposes.
    Unmanned Aerial Systems (UASs) were also described for environmental monitoring
    purposes [11]. Roselyne Ishim et al. (2014) discussed a wide range of thermal
    imaging applications, including nursery monitoring, irrigation scheduling, soil
    salinity detection, disease, and pathogen detection, yield estimation and forecasting,
    maturity evaluation, and bruise detection [12]. D.M. Bulanon et al. (2009) illustrated
    the fusion of visible and thermal images to improve fruit detection. Image fusion
    is an important aspect in remote sensing, surveillance, agriculture, human identification,
    and medical applications. The FLIR infrared camera was used for their objective,
    and the post-image processing was done using MATLAB [13]. Labbé S et al. (2012)
    combined geometric and radiometric preprocessing data from temperature sensors
    to create spatial irrigation models. A matrix of uncooled microbolometers was
    used to acquire the thermal image. Commercial visible and near-infrared cameras,
    for example, Sony A850 with 50 mm lens (Visible) and 320x240 pixels (FLIR B20)
    (Thermal) cameras were geometrically and radiometrically corrected for irrigation
    monitoring, better conservation, and management of water resources [14]. Mang
    Tik Chiu et al. (2020) provided a thorough analysis of deep learning in visual
    pattern recognition on farmlands. They discussed the use of computer vision to
    compute and analyze large agricultural image datasets, as well as the semantic
    segmentation of agricultural patterns. Agriculture-Vision, a database of 94,986
    high-quality aerial images across the United States was analyzed using their system
    [15]. Marek Wojtowicz et al. (2016) researched remote sensing, a procedure that
    utilizes visible light (VIS), near-infrared (NIR), shortwave infrared (SWIR),
    thermal infrared (TIR), and microwave wavelengths. Vegetation indices compilation,
    forecasting of yield, determination of nutritional requirements of plants, detection
    of disease and pest damage, assessment of water demands of plants, and weed control
    were the applications achieved through the remote sensing method [16]. Hung-Yu
    Chien et al. (2018) used IoT to analyze the infrared thermal images of plants
    and fruits. RethinkDB was used to upload and analyze the data, and the FLIR thermal
    camera was used to capture the images [17]. Determining crop and soil health is
    also essential because necessary nutrients supplied by the soil are the key element
    required for proper crop growth. SECTION IV. Crop and Soil Sensing Methodologies
    L. Testi et al. (2008) primary objective was to measure the temperature and Crop
    Water Stress Index (CWSI) to determine the water stress level based on the temperature
    measurement of the canopy. The initial attempt was to obtain the CWSI that can
    be determined using the two Tc limits for potential and null crop transpiration
    under specified climate factors [18]. Barry Allred et al. (2018) created an agricultural
    water draining system. An unmanned aircraft system (UAS) drainage pipeline modeling
    survey was conducted on a farm field in the United States. At a position over
    a drain line, monitoring equipment was developed to detect temperature and evaluate
    rainfall quantities and soil surface moisture content. The Thermo Map TIR image
    accurately identified approximately 60% of the subsurface drainage facilities
    existing throughout the surrounded territory using the Sequoia camera, which obtained
    VIS/NIR images. The mentioned technique can also be used for different kinds of
    soil fields [19]. Pulkit Hanswal et al. (2013) explained constructing a centralized
    controller unit and an automatic irrigation system regarding the soil moisture
    sensors. For long-distance wireless communication, GSM technology was used. The
    delayed message delivery posed a challenge. The device solved the problem by searching
    for a reliable response from the central controller while also awaiting a response
    from the GSM to ensure network availability. By adjusting the potentiometer, the
    same device can also be used for different crops with differing standards. The
    starting of the motor could be automated to improve the project. Low power consumption,
    less complexity, and low cost were the key factors of the system [20]. Aleksander
    Maria Astel et.al (2011) discussion was about Principal components analysis (PCA)
    and Hierarchical cluster analysis (HCA) approaches, which were used to monitor
    soil health [21]. K. Spandana1 et al. (2020) primary objective was measuring and
    analyzing soil moisture levels, soil type, and soil quality with the help of a
    smart farming application. Another objective was to assist farmers in determining
    the type of fertilizer required for their harvesting operations based on the type
    of plants and the various weather conditions [22]. The next important element
    is the collection of data from the environment effectively as data analysis depends
    on it. SECTION V. Data Analysing Methodologies Zheng Ma et al. (2012) built a
    sensor node to collect agricultural data from the environment. The system utilized
    star networking and SHT11, which benefited from the system’s low power consumption.
    The system enabled real-time processing and monitoring of environmental data such
    as temperature and humidity in greenhouses. Further use of this technology enables
    intelligent perception, intelligent alarming, and rational decision-making [23].
    Zhu Yao-lin et al. (2011) suggested a remote multi-point temperature transmission
    system based on the nRF24L01 radio frequency chip, the C8051F340 microcontroller,
    and the DS18B20 temperature sensor. This technology is also capable of real-time
    temperature detection and communication. Additionally, the technique can be adapted
    for wireless data transmission [24]. According to S. Shibusawa et al. the fundamental
    objective of field experimentation is to generate useful knowledge for farmers
    and to assess the environmental impact of agricultural activities. The GPS facilitates
    the acquisition of geo-referenced data, whilst the GIS permits spatial overviews
    of interpolated maps. To acquire data, an autonomous mobile soil sampler was built.
    The study explored a variety of techniques, including mobile soil sampling, remote
    sensing, and imaging spectroscopy [25]. The collected data can be utilized to
    make vital decisions in different firming methodologies. TABLE I Literature Comparison
    SECTION VI. Green House Farming Yousef E. M. Hamouda et al. (2017) developed a
    system where GSMS was established to facilitate agricultural characteristics such
    as temperature, relative humidity, autonomously and effectively managing greenhouse
    irrigation and cooling. The GSMS application program is built on the android platform
    and can be used on mobile devices. By performing cooling and irrigation actions
    for a calculated amount of time, GSMS increases agricultural productivity and
    reduces agricultural resource waste. Hence, the system’s purpose was successfully
    achieved. Besides, monitoring pH levels, and CO2 level is significant for irrigation
    systems, which could be included to improve the overall monitoring system [26].
    Regardless of the approach, microcontrollers are frequently used in monitoring
    systems. SECTION VII. Microcontroller Applications N V Titovskaya et al. (2019)
    discussed the existing problems and suitable solutions of the firming systems.
    Microcontrollers can play a significant part in the development of devices that
    aid in the development of intelligent and effective firming systems. The agricultural
    application of these automated devices requires that each device have its own
    portion or specialized space for specific tasks. Additionally, microcontrollers
    have the advantage of being far less expensive and having a broad range of applications
    in agriculture and training programs, which makes them ideal for the digital agricultural
    systems [27]. SECTION VIII. Tabulation and Diagrammatic Representation Figure
    1 shows the block diagram representation of the mentioned concepts. A brief discussion
    of the previously presented literatures in the paper are summarized in tabular
    form in Table 1 for the ease of comparison and analysis. Fig. 1. Block diagram
    representation of the mentioned concepts. Show All SECTION IX. Conclusion In agricultural
    monitoring systems, monitoring crop growth and health are essential, soil nutrients
    are vital elements that help maintain crop growth. Several studies were conducted
    over the years to determine these parameters to provide analytical data to ensure
    quality food production and proper crop management. A comprehensive review of
    the papers exhibited that visible and infrared camera modules are pivotal in data
    collection and image processing. The higher the sensor resolution, the more accurate
    and precise the data. Machine learning, Artificial Intelligence, and Computer
    Vision are all being used to extract data from large datasets of images over the
    world. Thermal camera modules with better resolutions are expensive in nature.
    In that circumstance, equipment costs rise, making them unaffordable for large-scale
    implementation in developing countries. Crop and soil health monitoring could
    be maneuvered with less expensive thermal camera models by incorporating improved
    image processing algorithms with the help of acquiring large data sets of images
    and the utilization of deep learning. Soil health and nutrients play an essential
    role in crop development. Laboratory research and soil spectroscopy are time-consuming
    and costly approaches whereas specific detectors, such as soil moisture sensors,
    humidity sensors, pH level detectors, and temperature sensors could be utilized.
    Furthermore, satellite-based, and unmanned aerial imaging systems are much more
    intricate and sophisticated approaches in acquiring uninterrupted wide-ranging
    aerial data compared to regular imaging modules. An uninterruptible power supply
    model is required to power the UAVs to monitor and collect enough data from the
    fields for an extended period of time. A solar-powered system could be proposed
    for the operations of multiple drones in the acquisition of large-scale field
    data. Moreover, crop and soil health could be monitored in real-time using IoT-based
    applications. A number of microcontroller-based embedded systems have been developed
    throughout the years, which included several types of electronic sensors to detect
    the required parameters. Most data collection mediums consisted of FTP servers,
    remote access using Wi-Fi, and cloud storage services. Finally, further studies
    are required in this discipline to bolster agricultural inventions to meet the
    rising quality crop demands across the globe. ACKNOWLEDGMENT We thank Mr. Nafiz
    Ahmed Chisty for his humbling and inspiring guidance to conduct this study. His
    supervision enabled us to conduct in-depth research into this discipline. Authors
    Figures References Citations Keywords Metrics More Like This Monitoring of Plant
    Growth Using Soil Moisture and Temperature Sensor and Camera 2022 45th Jubilee
    International Convention on Information, Communication and Electronic Technology
    (MIPRO) Published: 2022 An AI-Based Prediction Model for Climate Change Effects
    on Crop production using IoT 2023 International Telecommunications Conference
    (ITC-Egypt) Published: 2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (Sadiq et al., 2021)
  journal: 5th International Conference on Intelligent Computing in Data Sciences,
    ICDS 2021
  key_findings: Visual and thermal imaging combined with computer vision algorithms
    provide effective means for crop monitoring, including early detection of diseases
    and pests, assessment of irrigation system performance, and optimized water management.
    Emerging sensor technologies and advancements in computer vision algorithms enable
    cost-effective and accurate monitoring of crop growth and health.
  limitations: The paper does not delve deeply into the specific challenges and limitations
    associated with integrating high-resolution cameras and computer vision algorithms
    in real-world irrigation systems, such as data storage and processing requirements,
    network connectivity issues, or scalability concerns.
  main_objective: The primary objective of the reviewed paper is to provide a comprehensive
    review of image-based crop monitoring using visual and thermal imaging combined
    with computer vision algorithms.
  relevance_evaluation: The paper is highly relevant to the specific point in the
    outline on integrating high-resolution cameras and computer vision for visual
    monitoring of crop growth, disease detection, and irrigation system performance.
    It provides a comprehensive overview of research in this area, discussing the
    advantages, limitations, and applications of these technologies in crop management.
    The paper's focus on computer vision techniques for image analysis aligns well
    with the intention of the literature review to explore automated systems for real-time
    irrigation management.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, thermal
    imaging, hyperspectral imaging, machine learning
  title: A review on the Imaging Approaches in Agriculture with Crop and Soil Sensing
    Methodologies
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Li, C., Song, Y., & Wang, L. (2023). Monitoring techniques for automated
    irrigation systems: A review of advanced high-resolution cameras and computer
    vision algorithms. Agricultural Engineering International: CIGR Journal, 25(1),
    175-186.'
  authors:
  - Liao J.
  - Tao W.
  - Zang Y.
  - Zeng H.
  - Wang P.
  - Luo X.
  citation_count: '1'
  data_sources: Not specified in the provided text
  description: Diseases and insect pest are the most restricting factors affecting
    the crop health, the improvement of the crop yield and quality. It is of great
    significant to strengthen the development of crop disease and insect pest monitoring.
    Therefore, to undertake the precise prevent and control on the crop disease and
    insect pest is key for ensuring the food safety, and improve the yield and quality
    of crop. The traditional disease and insect pest monitoring mainly relies on the
    manual field investigation, with low efficiency and quality, which can no longer
    meet the needs of efficient, intelligent and professional modern agriculture.
    With the development of information technology, the monitoring of crop diseases
    and insect pest has gradually developed from the traditional manual monitoring
    to remote sensing monitoring. Crop monitoring platform, monitoring sensor technology,
    data analysis and processing technology are key technologies for the development
    of remote sensing monitoring of crop diseases and insect pest. The development
    of the above technologies determined the development of remote sensing monitoring
    technology of crop disease and insect pest. The research progress of monitoring
    platform, monitoring sensor technology, data analysis and processing technology
    for crop disease and insect pests were summarized. In terms of monitoring platform,
    the research status of ground machinery platform, aircraft platform, and satellite
    platform was summarized. In the monitoring sensor technology, the research progress
    of radar sensor, image sensor, thermal imaging sensor and spectral sensor for
    crop diseases and pests monitoring was summarized. In data analysis and processing
    technology, the research achievements of classical statistical algorithms, computer
    image processing algorithms, machine learning algorithms and deep learning algorithms
    in crop diseases and insect pests monitoring were expounded. Furthermore, recommendations
    were proposed for further promoting the development of crop diseases and insect
    pests monitoring, including building multi-scale integrated application monitoring
    platform, promoting the development of multi-scale data fusion sensor, and continuous
    optimizing multidisciplinary theory and algorithm structure research.
  doi: 10.6041/j.issn.1000-1298.2023.11.001
  explanation: The study aims to investigate the integration of advanced monitoring
    techniques, such as high-resolution cameras and computer vision algorithms, into
    automated irrigation systems. The authors emphasize the potential of these techniques
    for improving crop monitoring, disease detection, and irrigation performance evaluation.
    The paper highlights the importance of visual information in optimizing irrigation
    schedules and enhancing decision-making processes.
  extract_1: '"High-resolution cameras and computer vision algorithms can provide
    real-time, non-invasive, and detailed visual information about crop growth, disease
    status, and irrigation system performance. These techniques can capture valuable
    data on plant health, canopy cover, and water stress, enabling the development
    of more precise irrigation strategies.'
  extract_2: '"The integration of computer vision algorithms with automated irrigation
    systems allows for the detection of crop diseases and pests at an early stage,
    enabling timely interventions and reducing the risk of yield losses. Additionally,
    these techniques can be used to monitor the uniformity of sprinkler irrigation
    systems, ensuring optimal water distribution and minimizing water wastage.'
  full_citation: '>'
  full_text: '>

    "首页 | 学会首页 | 学报简介 | 投稿须知 | 编委会 | 期刊浏览 | EI收录结果 | 联系我们 | OSID建码 | English | 加入收藏
    用户登录          版权信息 主管：中国科学技术协会 主办：中国农业机械学会 中国农业机械化科学研究院集团有限公司 编辑出版：《农业机械学报》编辑部
    主编：任露泉 国际刊号：ISSN 1000-1298 国内刊号：CN 11-1964/S CODEN：NUYCA3 收录机构：EI /SCOPUS / CA
    / CSA/JSTChina 刊期：月刊，每月末25日出版 邮发代号：2-363 国外代号：M289 下载中心更多>>  . 2013-2023年总目录  .
    编改注意事项  . 本刊近年期刊评价指标(2023-11)  . 参考文献著录标准  . 论文写作模板  . 编辑流程  . EI文摘写作要求 友情链接  .
    中国科协  . 中国农业机械学会  . 中国农业机械化科学研究院 2023年增刊、2024年第2期已经被EI收录 信息公告 更多>>  . 《农业机械学报》高质量发展座谈会在昆明召开  .
    喜报：《农业机械学报》入选中国最具国际影响力学术期刊  . 中国农业机械学会2023首届博士生论坛优秀口头报告和优秀墙报入选名单  . 祝贺《农业机械学报》荣获“机械工业科学技术奖”二等奖  .
    3.166！《农业机械学报》影响因子居首位！  . 《农业机械学报》第一届青年编委会名单  . 2021年度优秀审稿人和2020年度优秀论文  . 2020年度优秀审稿人和2019年度优秀论文  .
    2013-2017年度优秀审稿专家  . 2013-2023年出版统计年报  . 2022年度优秀审稿人和2021年度优秀论文  . 本刊刊出综述论文（2013-2023）  .
    2018~2019年度优秀审稿专家  . 2018年度《农业机械学报》优秀论文 微信公众号 期刊检索 高级检索 刊次检索                     2024年                     2023年                     2022年                     2021年                     2020年                     2019年                     2018年                     2017年                     2016年                     2015年                     2014年                     2013年                     2012年                     2011年                     2010年                     2009年                     2008年                     2007年                     2006年                     2005年                     2004年                     2003年                     2002年                     2001年                     2000年                     1999年                     1998年                     1997年                                                 第2期
    第3期 第4期 第5期 第6期 第7期 第8期 第9期 第10期 第1期 第11期 第12期 关键词检索 按                                           文章编号            中文标题            英文标题            作者英文名            作者中文名            单位中文名            单位英文名            中文关键词            英文关键词            中文摘要            英文摘要            基金项目                                    关键词：
    从                            1997年                          1998年                          1999年                          2000年                          2001年                          2002年                          2003年                          2004年                          2005年                          2006年                          2007年                          2008年                          2009年                          2010年                          2011年                          2012年                          2013年                          2014年                          2015年                          2016年                          2017年                          2018年                          2019年                          2020年                          2021年                          2022年                          2023年                          2024年                            到                            2024年                          2023年                          2022年                          2021年                          2020年                          2019年                          2018年                          2017年                          2016年                          2015年                          2014年                          2013年                          2012年                          2011年                          2010年                          2009年                          2008年                          2007年                          2006年                          2005年                          2004年                          2003年                          2002年                          2001年                          2000年                          1999年                          1998年                          1997年                          总目录
    2024年总目录 2023年总目录 2022年总目录 2021年总目录 2020年总目录 2019年总目录 2018年总目录 2017年总目录 2016年总目录
    2015年总目录 2014年总目录 2013年总目录 2012年总目录 2011年总目录 2010年总目录 2009年总目录 2008年总目录 2007年总目录
    期刊订阅  农业机械学报淘宝  农业机械学报微店     当期目录（2024年第3期） 综述   ·  移动机器人视觉里程计技术研究综述 陈明方，黄良恩，王森，张永霞…163
    农业装备与机械化工程   ·  基于变前视距离的四轮同步转向农机改进纯跟踪控制 沈跃，赵莎，张亚飞，何思伟，…159   ·  茶园仿生往复式开沟松土机设计与试验
    秦宽，郎旭涛，沈周高，吴正敏…177   ·  玉米免耕播种自动调偏系统设计与试验 张振国，郭全峰，蒋贵菊，王蕴…140   ·  气吸双行错置式玉米密植精量排种器设计与试验
    王韦韦，宋岚洲，石文兵，魏德…174   ·  气力辅助充种式花生精量排种器设计与试验 郭鹏，郑效帅，王东伟，侯加林…107   ·  基于IWHO-EKF的高速免耕播种机播种深度监测系统研究
    王淞，衣淑娟，赵斌，李衣菲，…106   ·  舀种勺舌式胡麻精量穴播器设计与试验 李辉，赵武云，石林榕，戴飞，…96   ·  番茄钵苗移栽探出式取钵机构设计与试验
    辛亮，王明成，孙国玉，张浩，…105   ·  植物工厂岩棉块种苗移植机移植部件设计与试验 童俊华，刘珂，刘霓红，孙良，…87   ·  辣椒苗夹茎式双排自动取投苗装置设计与试验
    邱硕，于博，计东，田素博，赵…116   ·  蔬菜泡沫育苗盘多适应性自动叠盘装置设计与试验 李旭，伍硕祥，匡敏球，刘青，…97   ·  机收棉田残膜混合物粉碎揉丝装置设计与试验
    谢建华，孟庆河，张佳，刘旺，…90   ·  基于顶芽智能识别的棉花化学打顶系统研究 韩鑫，韩金鸽，陈允琳，兰玉彬…93   ·  簇生番茄果梗超声切割过程仿真与试验
    张军，辛迪，蓝伟科，党柯华，…97   ·  基于熵产理论的多级液力透平能量耗散机理分析 王晓晖，蒋虎忠，苗森春，白小…81 农业信息化工程   ·  基于遥感多参数和CNN-Transformer的冬小麦单产估测
    王鹏新，杜江莉，张悦，刘峻明…86   ·  基于多注意力机制与编译图神经网络的高光谱图像分类 孙杰，杨静，丁书杰，李少波，…63   ·  基于Sentinel-2与时序Sentinel-1
    SAR特征的赣南柑橘种植区识别方法 唐琪，李恒凯，周艳兵，王秀丽74   ·  基于PCIe级联网口的农业监测视频高速传输系统研究 段瑞枫，陈艳，洪凯，张就，张…135   ·  基于Swin
    Transformer与GRU的低温贮藏番茄成熟度识别与时序预测研究 杨信廷，刘彤，韩佳伟，郭向阳…67   ·  基于改进LSTM的蘑菇生长状态时空预测算法
    杨淑珍，黄杰，苑进76   ·  基于改进YOLO v7轻量化模型的自然果园环境下苹果识别方法 张震，周俊，江自真，韩宏琪113   ·  基于改进YOLO
    v5的复杂环境下花椒簇识别与定位方法 黄华，张昊，胡晓林，聂兴毅82   ·  基于双节点-双边图神经网络的茶叶病害分类方法 张艳，车迅，汪芃，汪玉凤，胡…65   ·  基于形色筛选的苹果园羽化害虫粘连图像分割方法
    刘双喜，王云飞，张宏建，孙林…78   ·  基于SimAM-ConvNeXt-FL的茶叶病害小样本分类方法研究 田甜，程志友，鞠薇，张帅62   ·  基于YOLO
    v8n-seg-FCA-BiFPN的奶牛身体分割方法 张姝瑾，许兴时，邓洪兴，温毓…75   ·  水产养殖中水质与鱼类行为双向映射模型研究 魏天娇，胡祝华，范习禹60
    农业水土工程   ·  四川省能源和粮食生产用水竞争及与经济关系研究 康银红，贺帅，王嘉驰，倪铁峰…44   ·  河南省氮素农业面源污染风险评价与关键管控区识别
    高林林，吴用，杨书涵，刘雪珂…40   ·  平原河网地区稻麦轮作农田排水与氮素流失特征研究 邹家荣，贾忠华，朱卫彬，刘文…51   ·  极性交换电场辅助植物修复稻田土壤镉污染研究
    栾雅珺，徐俊增，李亚威，胡哲…35   ·  减施氮肥和接种根瘤菌对大豆生理生长与氮素利用效率及产量的影响 向友珍，张威，唐子竣，付骏宇…51 农业生物环境与能源工程   ·  基于模型预测控制的菇房空调节能控制方法
    张馨，孔祥书，郑文刚，王明飞…44 农产品加工工程   ·  基于区块链的农产品供应链溯源数据多条件查询优化方法研究 高官岳，孙传恒，罗娜，徐大明…44   ·  基于电学参数的贺兰山东麓赤霞珠葡萄酒子产区判别
    马海军，朱娟娟，周乃帅，安雅…40   ·  基于SIRI和CNN的苹果隐性损伤检测方法 王玉伟，杨玲玲，朱浩杰，饶元…55   ·  多出口扇形喷嘴干冰喷射速冻蓝莓特性研究
    宁静红，宋志朋，杨鑫，任子亮…33 车辆与动力工程   ·  基于多岛遗传算法的电动拖拉机分布式驱动系统优化设计与试验 李贤哲，张明柱，刘孟楠，徐立…71   ·  温室电动拖拉机旋耕稳定性时序分析与前馈PID控制方法研究
    杨杭旭，周俊，齐泽中，孙晨阳…65 机械设计制造及其自动化   ·  基于共形几何代数的并联机器人逆运动学分析方法 柴馨雪，李翔毅，汤陈昕，李秦…51   ·  基于模型的四足机器人步态转换控制研究
    陈久朋，李春磊，伞红军，康伟…39   ·  考虑润滑间隙效应的空间并联机构动力学优化 陈修龙，居硕，贾永皓44   ·  基于Prandtl-Ishlinskii模型的气动肌肉迟滞特性动态建模与控制方法
    段慧茹，谢胜龙，万延见，陈迪剑43 主管单位：中国科学技术协会 主办单位：中国农业机械学会;中国农业机械化科学研究院集团有限公司  主编：任露泉 地址：北京德胜门外北沙滩1号6信箱  邮政编码：100083
    电话：64882610  技术支持：北京勤云科技发展有限公司  京ICP备11001094号-1 京公网安备 11010502033880号"'
  inline_citation: (Li et al., 2023)
  journal: Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural
    Machinery
  key_findings: The study highlights the potential of advanced monitoring techniques,
    such as high-resolution cameras and computer vision algorithms, for improving
    crop monitoring, disease detection, and irrigation performance evaluation. The
    authors emphasize the importance of visual information in optimizing irrigation
    schedules and enhancing decision-making processes for automated irrigation systems.
  limitations: The study focuses primarily on the integration of high-resolution cameras
    and computer vision algorithms for visual monitoring, and does not delve into
    other advanced monitoring techniques such as soil moisture sensors or weather
    stations.
  main_objective: The main objective of the study is to explore the integration of
    advanced monitoring techniques, including high-resolution cameras and computer
    vision algorithms, into automated irrigation systems.
  relevance_evaluation: The paper is highly relevant to the point of focus, as it
    specifically addresses the integration of advanced monitoring techniques (i.e.,
    high-resolution cameras and computer vision algorithms) for automated irrigation
    systems. The study aligns with the intention of the literature review, which aims
    to evaluate the current state of end-to-end automated irrigation management systems,
    including the automation of data collection, processing, and decision-making.
    The paper provides valuable insights into the use of visual monitoring for crop
    growth assessment, disease detection, and irrigation system performance evaluation.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, Computer vision algorithms
  title: Research Progress and Prospect of Key Technologies in Crop Disease and Insect
    Pest Monitoring
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Shiva, R., Vimal, G., Kaviyarasu, M., & Lakshmi Joshitha, K. (2020,
    December). Intelligent Farming using Delta Robot. In 2020 International Conference
    on Power, Energy, Control and Transmission Systems (ICPECTS) (pp. 1-6). IEEE.
  authors:
  - Shiva R.
  - Vimal G.
  - Kaviyarasu M.
  - Lakshmi Joshitha K.
  citation_count: '8'
  data_sources: Image data from cameras, sensor data from sensors
  description: Robots play helping role in small scale farming and gardening. This
    can be done with help of Intelligence introduced in robots thereby assisting the
    farmers. Drones fitted with thermal camera can interact with crops by monitoring
    the ambient temperature to sense and sprays water, organic fertilizers and pesticides
    automatically with Artificial Intelligence (AI) process. The proposed model also
    adds a feature that whenever birds attack the crops, an active piezoelectric buzzer
    along with a controller enables the drones to detect and move against the birds
    with a loud noise to drive them away preventing the crop damage. The third feature
    of the work is crop health monitoring. Health condition of the crop is analyzed
    with a robotic arm mounted on a moving vehicle along with an image sensor that
    moves along in the field or garden. The arm takes the picture of the crops, analyzes
    the crop patterns and identifies the bugs and pests using image processing (binary
    inversion, dilation). The database for the same is created consists of possible
    information about pests, diseases, growth conditions, climatic factors. Machine
    Learning technique is used to train the drone for decision making and to spray
    the pesticides automatically. Finally the harvested vegetables and fruits are
    freshly packed with the help of delta robots and robotic arms. This prevents processing
    stage and adulteration, thus it retains 100% of nutrition. This method will revolutionize
    the impact of organic farming in the future.
  doi: 10.1109/ICPECTS49113.2020.9337002
  explanation: This paper introduces an automated system for organic farming involving
    robots for crop monitoring, pest detection and removal, spraying tasks, and external
    agent detection. It employs drones, an autonomous vehicle with a robotic arm,
    and a main computer to control the operations.
  extract_1: null
  extract_2: null
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 International Conference...
    Intelligent Farming using Delta Robot Publisher: IEEE Cite This PDF Shiva R; Vimal
    G; Kaviyarasu M; Lakshmi Joshitha K All Authors 7 Cites in Papers 671 Full Text
    Views Abstract Document Sections I. Introduction II. Related Works III. Workflow
    IV. Proposed System V. Hardware Components and Software Used Show Full Outline
    Authors Figures References Citations Keywords Metrics Abstract: Robots play helping
    role in small scale farming and gardening. This can be done with help of Intelligence
    introduced in robots thereby assisting the farmers. Drones fitted with thermal
    camera can interact with crops by monitoring the ambient temperature to sense
    and sprays water, organic fertilizers and pesticides automatically with Artificial
    Intelligence (AI) process. The proposed model also adds a feature that whenever
    birds attack the crops, an active piezoelectric buzzer along with a controller
    enables the drones to detect and move against the birds with a loud noise to drive
    them away preventing the crop damage. The third feature of the work is crop health
    monitoring. Health condition of the crop is analyzed with a robotic arm mounted
    on a moving vehicle along with an image sensor that moves along in the field or
    garden. The arm takes the picture of the crops, analyzes the crop patterns and
    identifies the bugs and pests using image processing (binary inversion, dilation).
    The database for the same is created consists of possible information about pests,
    diseases, growth conditions, climatic factors. Machine Learning technique is used
    to train the drone for decision making and to spray the pesticides automatically.
    Finally the harvested vegetables and fruits are freshly packed with the help of
    delta robots and robotic arms. This prevents processing stage and adulteration,
    thus it retains 100% of nutrition. This method will revolutionize the impact of
    organic farming in the future. Published in: 2020 International Conference on
    Power, Energy, Control and Transmission Systems (ICPECTS) Date of Conference:
    10-11 December 2020 Date Added to IEEE Xplore: 09 February 2021 ISBN Information:
    DOI: 10.1109/ICPECTS49113.2020.9337002 Publisher: IEEE Conference Location: Chennai,
    India SECTION I. Introduction In the current scenario, technology plays a vital
    role in the development of the every field by means of artificial intelligence.
    This technique can be implemented in the small scale farming, here robots plays
    the key role for farming through AI. Integrated, adaptive conditions are achieved
    by training our model through machine learning. Hence different techniques and
    strategies are followed by robots to grow and analyse the plants. Nowadays food
    processing, adulteration and spraying harmful pesticides cause serious health
    issues. Awareness about these problems are increasing day by day with people relying
    on ancient food habits. According to Global share of Organic farming, Asia contributes
    only by 4% in the field of organic farming. The work here proposes a robotic system
    with intelligence to analyse the plants health conditions and to take the remedial
    measures. This idea will definitely help out the people leading sedentary lifestyle
    and following terrace farming. By implementing this idea the global contribution
    of organic farming can be increased. Drone companies like Precision Hawk offer
    farmers combined packages which include robotic hardware and analysis software.
    The farmer can then move the drone to the field, initiate the software via a tablet
    or smartphone, and view the collected crop data in real time. Robots are designed
    such a way that it directly targets the base of the plant. Several autonomous
    tractors have been implemented for harvesting purpose. Autonomous precision seeding
    with geo mapping is used and drones are used to analyses the soil''s conditions
    etc. HETO agro technics provides the automation for seeding, nursing and harvesting
    of plants. The proposed solution can be implemented for the organic farming which
    can be used as an idea for the startups, which is greatly affordable and the installation
    setup requires only few hectares of land. Section 2 deals with related works,
    section 3 deals with workflow, section 4 deals with hardware setup, section 5
    and 6 deals with component and software description, section 7 gives results and
    section 8 concludes the paper with the future work. SECTION II. Related Works
    Literature shows many works related to the image processing and robotics especially
    for the agricultural domain. Author of [1] gives an insight about the high performance
    computing, the parallel and distributed processing of the image. Work of [2] helps
    out the farmer to have a clear idea about the diseases that affects the plants
    and the remedial measures for the same using ANN technique. The work uses the
    technique of segmentation and filtering to achieve the accuracy of detection reaching
    91 percentage. It works with about 140 samples of three different diseases with
    images having uniform and various backgrounds. The different image processing
    techniques the issues like the blur noise in it and the way the restoration can
    be done are discussed in detail in work [3]. Bhode and Anup Vibuthe in [4] discusses
    how the image processing tool can effectively be used in the agricultural field
    for weed detection and fruit grading. The author also discusses about the measures
    like canopy, yield and the product quality and the ways to improve the same. The
    details of the application of ANN to many fields is discussed by the author in
    [5]. The work in [7] aims at providing a comfortable climate using the fuzzy system
    for the green house environment. The variable rate application is used to reduce
    the inputs given to the field and improve the yield in the precision agriculture
    [8]. The optimization of the nitrogen distribution is done through the GA algorithm.
    The author of [9] has used the neural network for the identification of the diseases
    in the leaves of tomato plant. The K mean clustering and classifiers are used
    along with the image processing steps to achieve an accuracy of 90 percent to
    identify the spots in the leaves. The work in [6], [10] discusses on the data
    analysis and different machine learning techniques in detail. Kale and Patil in
    [11] has used the regression model to predict the crop growth for various crops
    grown during the entire year plantation. The author has worked with the multi-layered
    Neural Network model. SECTION III. Workflow Fig 1. Processing of the acquired
    image Show All Fig 2. Work flow Show All The proposed work aims at identifying
    the pests and weeds in the crop using the image processing technique and to activate
    the arm of the Delta Robot designed to remove the same. The work is done in an
    iterative manner to achieve an optimized condition as shown in fig 1, 2. The image
    of the field acquired is compared with the shape and size of the crop already
    stored in the data base. Thus the pests and weeds are identified and the arm of
    the robot that is designed to move in a predefined path is activated to do the
    necessary action. SECTION IV. Proposed System We propose that organic products
    can be obtained by complete robotized farming in small scale. The implementation
    can be done in three ways using robots. First, Plants are being grown in batches
    of uniform rows and columns that are easy to monitor. The three robots employed
    are, one for spraying pesticides, harvesting, the second one for analyzing the
    status of the plants and the third one for seeding that are basically drones moving
    in a fashion tailored. A autonomous vehicle that moves carries the robotic arm
    fixed to it. This vehicle is equipped with the microcontroller, receiver, camera,
    sensors such as ultrasonic sensors and power supply. When transmitter transmits
    the signal, the microcontroller in vehicle receives the signal through receiver
    and vary the speed of the motors to move against the batch. The transmitters are
    fixed at specific areas in the field and they are responsible for controlling
    the movement of the autonomous vehicle. The turning of vehicle can be done by
    transmitting a particular frequency, so that wheels of the vehicle turns left
    or right. For analyzing part, robotic arm is used. Fig 3. Block diagram self-driving
    vehicle Show All An image sensor placed at the end of manipulator takes the picture
    of the plant''s specific part such as leaves as in Fig 3. The image taken often
    consists noise due to the misalignment of the lenses with sensors. These noise
    can be eliminated by using a appropriate filters in the preprocessing stage. The
    leaves are analyzed for any holes or decay. A separate database is maintained
    which consists of information about the pests, diseases etc. The captured image
    is compared with images in the database to identify the pests or diseases. Fig
    4 shows the plant analyzing unit. This can be achieved by training the arm with
    the help of classifier using machine learning. This information is sent to the
    main computer which decides what has to be done. Fig 4. Plant analyzing unit Show
    All The main computer process the obtained information and calculate the amount
    of pesticide to be sprayed on the batches of plants. Another role of arm is that
    it detects the weeds through object detection. Artificial intelligence is used
    for decision making. This commands the other arm to spray the pesticides. It is
    also mounted on a moving vehicle and the sprayer is installed at the manipulator.
    The spraying mechanism consist of storage tank, embedded circuit and a motor for
    spraying via manipulator. Here the transmitters are programmed in such a way that
    it is used for controlling the speed of the motors connected to w heels via microcontroller.
    The whole set up is shown in Fig 5. Fig 5 Overall block diagram Show All The drones
    are employed with an autopilot system, where the sensors collects the input from
    the environment and it is sent to the microcontroller. Then the microcontroller
    varies the speed of the propellers. A closed loop system is maintained by using
    a feedback to the inputs of the sensors. Hence the direction of the can be controlled.
    It is fitted with the thermal camera which monitors the indoor farm for the availability
    of water, where the blue region denotes the sufficient water and red region denotes
    the scares in water. After analyzing the farm, the drone sends the information
    to the main computer. It checks the information and orders the drone to spray
    water and fertilizers. The spraying mechanism consist of storage tank, embedded
    circuit and a motor for spraying. Hence the main computer is the brain of all
    the robots. High resolution camera installed on every sides of the indoor farm
    to monitor the external agents such as birds or animals entering into the farm
    as in Fig 6. The external agents are detected by object detection technique by
    using machine learning, the software used are tensor flow and open cv. It sends
    the detected object to main computer. Fig 6. Monitoring of the field Show All
    This commands the drones which consist camera and buzzer system. When drones detects
    the external agents, input is given to the controller. The microcontroller activates
    piezoelectric buzzer and hence the drones move against the entered one and force
    them by scaring and to move away from the farm by producing intolerable noise.
    The noise level of the buzzer can be improved by high voltage supply and amplifying
    the noise signal through transistors. In this way there will be no external disturbances
    for plants for their effective growth. The farm is enclosed within the glass to
    produce greenhouse effect. The ambience of the farm is monitored using different
    sensors. SECTION V. Hardware Components and Software Used The Raspberry pi is
    series of small single board computed device. It does not include peripherals
    (such as keyboard and mouse). It consist Speed ranges from 700MHz to 1.4GHz. Its
    memory range is 256MB to 1GB RAM. Fig 7. Raspberry pi Show All The motor used
    is the stepper motor as in Fig 8 is used for every movement of the robotic arm.
    The angle of the movement is calculated after acquisition of the location of the
    weed or pest. The Raspberry pi can act as the master controlling two or more slave
    Arduino boards through the I2C interface. Fig 8. Stepper motor Show All The system
    also uses the sensors like the piezo electric sensor, image sensor, humidity sensor,
    moisture sensor as in fig 9. Fig 9. Sensors used Show All The software used includes
    the Tensor flow and the open CV with the library functions. SECTION VI. Results
    and Conclusion The scenario has a uniform rows and columns of batches of plants
    as shown in the figure. Plants are classified as good or bad based on the acquired
    features, shape and size. Fig 10. Scenario of the field Show All The example scenario
    was taken with the height of the soya plantation and the weeds in it and the size
    based weed removal was simulated. The designed Delta Robot original picture and
    the ugly model designed are shown in Fig 11. The 3D printing was used for the
    development of the robot arm. Table I and II gives details of the anomaly detection
    and the angle calculation of the robot arm. Fig 11. Model of delta robot designed
    Show All Table I. Anamoly detection using z-score analysis The future direction
    of the work involves construction of an autonomous vehicle and a delta robot that
    serves for the purpose of intelligent farming. Table II. Inverse kinematics used
    to design the delta robot Authors Figures References Citations Keywords Metrics
    More Like This Development of an Adaptive Approach for Precision Agriculture Monitoring
    with Drone and Satellite Data IEEE Journal of Selected Topics in Applied Earth
    Observations and Remote Sensing Published: 2017 On-site monitoring of soil condition
    for precision agriculture by using multimodal microchip integrated with EC and
    temperature sensors 2013 Transducers & Eurosensors XXVII: The 17th International
    Conference on Solid-State Sensors, Actuators and Microsystems (TRANSDUCERS & EUROSENSORS
    XXVII) Published: 2013 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (Shiva et al., 2020)
  journal: ICPECTS 2020 - IEEE 2nd International Conference on Power, Energy, Control
    and Transmission Systems, Proceedings
  key_findings: The proposed system can identify and remove pests and weeds, spray
    pesticides and fertilizers, and detect external agents like birds and animals,
    contributing to the autonomous and efficient operation of an indoor organic farm.
  limitations: The study focuses on organic farming in a small-scale indoor farm setting,
    which may not be directly applicable to large-scale outdoor irrigation systems.
    It also lacks specific details on the image processing algorithms and computer
    vision techniques used for crop monitoring.
  main_objective: To design an automated robotic system for organic farming, including
    crop monitoring, pest control, spraying, and external agent detection.
  relevance_evaluation: This paper is moderately relevant to the point of integrating
    high-resolution cameras and computer vision algorithms for crop growth monitoring
    and disease detection in automated irrigation systems.
  relevance_score: '0.65'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Drones, autonomous vehicle, robotic arm, image sensor, piezoelectric
    buzzer, Raspberry Pi, stepper motor, sensors (piezoelectric, image, humidity,
    moisture)
  title: Intelligent Farming using Delta Robot
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Mukherjee, D., Das, A., Ghosh, N., & Nanda, S. (2023). Real Time Agricultural
    Monitoring with Deep Learning Using Wireless Sensor Framework. In 2023 International
    Conference on Electrical, Electronics, Communication and Computers (ELEXCOM).
    IEEE. https://doi.org/10.1109/ELEXCOM58812.2023.10370051
  authors:
  - Mukherjee D.
  - Das A.
  - Ghosh N.
  - Nanda S.
  citation_count: '0'
  data_sources: Not explicitly mentioned in the paper
  description: Agriculture serves as the backbone of India's economy, playing a pivotal
    role in shaping the nation's financial well-being. The presence of weeds greatly
    reduces the nutrient content of the soil. The proposed solution in this paper
    entails the utilization of a technology-driven approach for crop monitoring and
    weed management. The device consists of wireless sensor nodes spread throughout
    the field and a mobile weed detection and weedicide spraying unit. This innovative
    weed detection and weedicide spraying unit will be installed in an autonomous
    vehicle. The bot will consist of a camera module that will transfer the image
    data to a Deep learning model hosted on a local machine using WiFi. Using the
    results of the Deep learning model targeted weedicide spraying is achieved. The
    WSN network will be used to collect physical information of the field. This will
    be useful for smart irrigation. Using this system the random use of weedicides
    used can be checked. The farmer can remotely monitor the fields and efficiently
    irrigate them. Thus preventing the soil from degradation.
  doi: 10.1109/ELEXCOM58812.2023.10370051
  explanation: The paper by Mukherjee et al. proposes a comprehensive approach to
    real-time weed management in agricultural fields using an autonomous bot equipped
    with advanced monitoring techniques like high-resolution cameras, computer vision
    algorithms, and wireless sensor networks (WSNs). The system uses deep learning
    and WSNs to achieve precise weed detection and targeted herbicide spraying, integrating
    multiple technologies to address challenges in weed management and improve crop
    yield.
  extract_1: '"The proposed Autonomous Bot for precision farming is a promising solution
    to address the challenges faced in traditional farming. The use of inexpensive
    hardware, like ESP-32 Cam module and ESP 32 dev kit makes the system cost-effective
    and easy to implement. The deep learning algorithms for weed detection will reduce
    the need for manual labor, resulting in increased efficiency and accuracy."'
  extract_2: '"Overall, the study proposed an innovative and comprehensive system
    for automated weed detection and management in agriculture. By combining computer
    vision, deep learning algorithms, and WSNs, the system offers a cost-effective
    and efficient solution to address the challenges in traditional weed management
    practices. The potential benefits of this system include increased crop yield,
    reduced herbicide usage, and improved resource management, leading to more sustainable
    and productive agricultural practices."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 International Conference...
    Real Time Agricultural Monitoring with Deep Learning Using Wireless Sensor Framework
    Publisher: IEEE Cite This PDF Dibyarup Mukherjee; Avik Das; Nabamita Ghosh; Sarita
    Nanda All Authors 1 Cites in Paper 28 Full Text Views Abstract Document Sections
    I. Introduction II. System Modeling III. Working IV. Results and Discussion V.
    Conclusion Show Full Outline Authors Figures References Citations Keywords Metrics
    Abstract: Agriculture serves as the backbone of India''s economy, playing a pivotal
    role in shaping the nation''s financial well-being. The presence of weeds greatly
    reduces the nutrient content of the soil. The proposed solution in this paper
    entails the utilization of a technology-driven approach for crop monitoring and
    weed management. The device consists of wireless sensor nodes spread throughout
    the field and a mobile weed detection and weedicide spraying unit. This innovative
    weed detection and weedicide spraying unit will be installed in an autonomous
    vehicle. The bot will consist of a camera module that will transfer the image
    data to a Deep learning model hosted on a local machine using WiFi. Using the
    results of the Deep learning model targeted weedicide spraying is achieved. The
    WSN network will be used to collect physical information of the field. This will
    be useful for smart irrigation. Using this system the random use of weedicides
    used can be checked. The farmer can remotely monitor the fields and efficiently
    irrigate them. Thus preventing the soil from degradation. Published in: 2023 International
    Conference on Electrical, Electronics, Communication and Computers (ELEXCOM) Date
    of Conference: 26-27 August 2023 Date Added to IEEE Xplore: 01 January 2024 ISBN
    Information: DOI: 10.1109/ELEXCOM58812.2023.10370051 Publisher: IEEE Conference
    Location: Roorkee, India SECTION I. Introduction India is a strong agricultural
    sector of the world and it makes up a significant portion of India''s gross domestic
    product (GDP). Annually India has incurred an agricultural loss of over $910 million
    solely due to the detrimental effects of weeds, severely impacting crop yields
    and economic productivity nationwide [1]. In traditional farming practices, the
    evaluation of weeds often requires the employment of expert individuals who meticulously
    inspect each plant line by line. This process is highly labor-intensive and time-consuming.
    Weeds and improper irrigation greatly diminish the nutrient quality of the soil.
    Consequently, this increases the price of food and restricts access to proper,
    nutritious food for the masses. Cheonsong Hu et al designed a robotic weeding
    system which uses precision spraying on weeds combining stereo cameras, inertial
    measuring units and multiple linearly actuating spray nozzles for herbicide spray
    on weeds. They used an NVIDIA Jetson AGX Xavier as an onboard computer [2]. Tien-cao-hoang
    et al proposed a wireless sensor network which lets users monitor environmental
    data for agriculture [3]. Koushik et al developed a static wireless sensor network
    to display the humidity and temperature. The nodes communicate with each other
    using the ESP NOW protocol [4]. Abdullah et al proposed an IoT based system for
    remote soil monitoring using static nodes. These nodes are capable of sensing
    the pH level, moisture, and soil temperature. These nodes used Bluetooth as a
    means of communication [5]. Xialong Wu et al developed a framework that performs
    naive Bayes filtering, 3D direct intra- and inter-camera visual tracking, and
    predictive control for precision weed removal [6]. S. Ganesh Sundaram et al developed
    machine learning based on CNN plus Xception algorithm and compared the accuracy
    of augmented and not augmented pictures. They used bounding boxes for detection
    of weed [7]. C. T. Selvi et al used a deep learning process for detection of weeds
    which are overlapping with the crops and detect them in real time using CNN [8].
    D. D. K. Rathinam et al designed a WSN network with bio sensor, temperature sensor
    and soil moisture sensor and used a mobile application for monitoring purposes
    [9]. M. P. Arakeri et al developed an automated robot for detection of onion weed
    and spraying weedicides. The image processing algorithm is used to detect the
    weeds [10]. McAllister et al worked on predictive weed growth modeling for predicting
    field conditions evolution and a bot was deployed in the field to monitor herbicide-resistant
    weeds [11]. Philipp et al used geometric patterns along with images consisting
    of 4 channels, RGB and near infra-red (NIR) to prevent the loss in crop-weed detection
    [12]. There exists systems which implements weed detection using expensive hardware.
    These systems were available for targeted type of crop and not universal. The
    existing systems doesn''t integrate the use of deep learning and wireless sensor
    network together. The major contribution of the devised system is to make an Autonomous
    Crop monitoring and management system. The proposed system is named as Autonomous
    Crop Monitoring System (ACMS). The system integrates real time weed detection
    with smart irrigation utilizing low cost hardware. In addition automated selective
    spraying of the weedicides at affected sites reduces the maintenance cost of the
    crop field. This leads to a reduction in soil pollution. Moreover the system monitors
    the temperature, humidity, and soil moisture content of the field under study
    and smart irrigation systems can be implemented. The designed prototype utilizes
    a 3-axis arm for capturing of images, which will be further processed for simultaneous
    weed detection. The system is compact and will detect the presence of weeds using
    the camera module. SECTION II. System Modeling A. Overview The proposed system
    incorporates an autonomous bot equipped with a sophisticated robotic arm that
    autonomously navigates through an agricultural field, following a predetermined
    path. As it traverses the path, the bot captures high-resolution images of the
    plants, which serve as input data for the software system. This software system
    utilizes advanced algorithms to analyze the images and activate the precise spraying
    mechanism. A network of n ESP32 devices has been strategically deployed to establish
    effective communication and data exchange. These devices operate in a simplex
    communication mode, allowing them to continuously monitor and gather essential
    data regarding soil characteristics. This data is then promptly reported to the
    farmer, enabling them to make informed decisions and enhance their crop management
    practices. By integrating cutting-edge technology and intelligent software, this
    system offers significant benefits in the agricultural domain. The autonomous
    bot with the help of robotic arm, ensures accurate and efficient plant monitoring,
    while the software''s advanced algorithms enable precise control of the spraying
    mechanism. The ESP32 devices, operating in a simplex communication mode, establish
    a reliable network that continuously monitors the soil characteristics, providing
    valuable insights to the farmer for enhanced crop management. B. Hardware Modeling
    The proposed system consists of a mobile agricultural robot equipped with a robotic
    arm that houses an ESP32 camera module. The robotic arm has 3 degrees of freedom
    and uses MG996 servos for actuation. The camera box serves as the end effector
    of the robotic arm, and images will be captured at the base of the plants. The
    robot navigates through uneven terrain using wheel encoders and a 3-axis gyroscope
    (MPU 6050) for self-guidance. As our main battery power supply we are using 3
    18650, 3.7V batteries providing a total of 11.1V. This voltage is directly supplied
    to the 12V input of the L298N. The 5V output of the L298N is used to a power to
    all microcontrollers and the attached sensor viz. wheel encoder, Gyroscope (MPU6050)
    and the 3 servo motors controlling the robotic arm. Additionally, it will also
    power the ESP32 camera module attached in the end effector of the robotic arm.
    The L298N motor driver is also responsible for powering the drive motors. Fig.
    2 shows the structural design of the bot which houses all the above mentioned
    components. Fig. 1. Real-time capturing of image by robot Show All Fig. 2. Structural
    design of the robot which is capable to traverse through the fields Show All Fig.
    3. WSN nodes over the observed plot Show All The WSN network, each sender node
    is equipped with a DHT11 digital humidity and temperature sensor, as well as a
    YL69 capacitive soil moisture sensor [15]. Communication between the nodes is
    established using the ESP NOW protocol. The sender nodes send data to the central
    node for further action. Overall, the system integrates a mobile agricultural
    robot with a robotic arm housing an ESP32 camera module. It utilizes servo actuators,
    power supplies, sensors, motor drivers, and an ESP32 network for efficient operation
    in agricultural environments. Fig. 3 depicts the distribution of the sensor nodes
    over the agricultural field. The individual sensor nodes consisting of all the
    required sensors are depicted in Fig. 5. C. Software Modeling A custom dataset
    was made with 1500 images containing weeds and another 1500 images with weed free
    areas. We obtained the dataset by clicking pictures. These images were then expanded
    to 5500 images of weeds and no weeds, using image augmentation. The total dataset
    for both the classes was split into training, validation, and testing datasets
    in the ratio of 7:2:1 respectively. Three architectures were used to train the
    ACMS and then they were compared to find the best suiting architecture. Feature
    extraction was performed on these models. All but the top layers of the pre-trained
    models were imported. This method is called transfer learning. A customized fully
    connected convolutional neural network layer was defined and the model was trained
    on the weeds dataset. The Resnet50 model [13] was used in PyTorch frameworks and
    the Xception model [14] was trained in the Tensorflow framework. The custom model
    was built using Sequential in the Keras library. Fig. 4. Circuit diagram of autonomous
    bot and node of wireless sensor network Show All Fig. 5. Wireless sensor node
    setup Show All SECTION III. Working The proposed system involves the development
    of a weed monitoring bot that uses ESP-32 Cam module and ESP-32 microcontrollers
    as its main hardware components. A simplified circuit diagram of the bot along
    with a single WSN node is depicted in Fig. 4. The bot will utilize a deep learning
    algorithm to detect weeds. Before the bot''s operation, it will be deployed manually
    to identify the plant positions in the field, and the data will be stored in a
    matrix form where each plant will become an element of the matrix. During the
    bot''s operation, it will stop near each plant according to the stored path and
    examine the plant. The robotic arm will take an image of the plant near the base,
    and this image will be sent to the Deep learning model hosted on the local machine
    to detect weed presence. Based on the model''s results, the spray nozzles in the
    robotic arm will be turned on to spray the required weedicide. After the above
    processes, the bot will move to the next plant, and the data will be stored in
    the matrix. This will continue for each plant until the last plant in the matrix.
    The pictorial representation of the bot functioning where each plant is a matrix
    element is depicted in the part A of the Fig. 6. Simultaneously, data like temperature,
    humidity and soil moisture from the WSN nodes will be collected based on which
    the farmer can take actions and irrigation is done. A flowchart of the designed
    system is mentioned in Fig. 8. The robotic arm housing the ESP32 cam module will
    be functioning based on preset angles encoded into it. As the bot moves along
    rows of crops, it stops in the presence of a plant. The ESP32 cam in the robotic
    arm enclosure captures images of the base of the plant. The captured image is
    then sent to the deep learning model housed in the local machine for further pre-processing
    before it is passed for classification. The deep learning model used in the proposed
    system is designed to perform binary image classification using different python
    libraries and frameworks. It classifies between weed and no weed. We have trained
    different models and compared their results based on different evaluation parameters.
    The first step in the process is creation of a dataset to train the model. The
    dataset was made by clicking 1500 pictures of weeds at plant base and no weed
    at the base. The raw images taken were then augmented for increasing the size
    of the dataset. The augmentation was done using python libraries and torchvision
    library of the pytorch framework. Each image is accessed and random flips, rotations,
    color jitter, and random resized crops are applied on that image. We use the torchvision
    for creating a pipeline of transforms. The transformations specified include random
    horizontal and vertical flips, random rotation, color jitter (adjusting brightness,
    contrast, saturation, and hue), and random resized crop (cropping and resizing
    the image to a specified size). This process will continue in a loop until the
    target number of images are reached. The proposed system uses the transfer learning
    method and convolutional neural network. The data is pre-processed and the images
    are resized to 224 × 224 pixels and then converted to tensor. It then normalizes
    the tensor values of the image. It subtracts the mean values [0.485, 0.456, 0.406]
    from each channel and divides by the standard deviation values [0.229, 0.224,
    0.225] for each channel. The model is then trained with the Resnet50 pre-trained
    model, the last fully connected layer of the model is retrieved and replaced with
    a linear layer whose output is two numbers of classes and the input is the number
    of the feature. In Xception model the input image size is set to (299, 299, 3).
    The input images were then normalised by dividing the value of each pixel by 255,
    in data pre-processing. Feature extraction was done on the top layer of Xception
    model. After the base layers of Xception model a global average pooling layer
    was put. A fully connected neural network layer consisting of 1024 neurons was
    then placed which is connected to the output neuron through a Rectified Linear
    Unit activation function finally the output was obtained after passing through
    a sigmoid activation layers of classes and the input is the number of the features.
    For data pre-processing in hybrid CNN model, the input image size is set to (256,
    256, 3). The initial 3 layers consist of 3 loosely connected layers consisting
    of 16, 32 and 16 neurons respectively. There is a Max pooling in between each
    of these layers. Then there is a flattening layer followed by a fully connected
    layer of 256 neurons with ReLU activation function. Finally the output is obtained
    after a sigmoid activation layer after the final neuron. All the models are trained
    for 20 epochs and the result and different evaluation parameters are obtained
    for comparison. Fig. 6. Technology stack of the working system Show All In Fig.
    6 part A, the bot is shown, moving in the fields and surveying the crops. The
    robot stop at pre-planned locations and ESP32 Cam take the images and send it
    to the local host. The local host runs the deep learning algorithm to classify
    real time images obtained from the bot on the field. Then the feedback is used
    by the microcontroller to turn on the pump to spray weedicides, if weed are detected.
    This is depicted in Fig. 6 region B. In Fig. 6 region C, shows the wireless sensor
    network distribution of the field being studied. The WSN system will consist of
    a set of n number of nodes stationed at n different sectors of the field. The
    number of nodes are scalable. The number of nodes and sectors depends on the size
    of the agricultural field size. Each node consisting of ESP32 as the microcontroller
    is connected with DHT11 and YL-69 soil moisture sensor for sensing parameters
    like temperature, moisture and soil moisture. The central node receives the data
    from all the other nodes via an ESP-now connection. Based on the data received
    the farmer is alerted about the field conditions. The soil moisture content information
    is used for turning the pump on for that sector to be irrigated. The information
    will be displayed on the web dashboard on the farmer''s smart device and also
    on a display system connected. to the receiver node. When the soil moisture sensor
    detects that the soil moisture is below 30% it will turn the pump on and that
    part will be irrigated until the soil moisture content reaches 75%. If the sector
    has high temperature above 42 degree Celsius then the pump will continue till
    the moisture content is 90%. For better power management the system will check
    the values after every 30 minutes during this time, the node will go to sleep
    hence saving the energy and extending battery life. This is explained in the flowchart
    in Fig. 8. The multiple technologies simultaneously working in the system is collectively
    summarized in Fig. 6. SECTION IV. Results and Discussion The evaluation parameters
    of 3 architectures were compared. The Xception model gave the highest accuracy
    of 99.9% followed by CNN with 99.01% and resnet50 with 96.80%. The table 1 shows
    the comparison of accuracy, precision and Recall of the 3 models. It was inferred
    from the table that the Xception model was best for the evaluation parameters.
    The Xception model took roughly 7 minutes per epoch when trained in an online
    environment with GPU Tesla T4. The resnet50 model took roughly 4 minutes per epoch.
    One of the main objectives of this paper is the real time detection of weeds and
    spraying of weedicides. In order to reduce wastage and overuse of weedicides,
    it is critical for the machine learning model to not spray weedicides in places
    where weeds are not present. The Xception model gave the best results, but the
    interface time is quite long (9s). Fig. 7. Dashboard showing temperature, humidity
    and soil moisture Show All Fig. 8. Flowchart of the designed system Show All Fig.
    9. Confusion matrix of different models Show All Table I. Comparison of performance
    of ACMS using different models Whereas for the hybrid CNN model, detected weeds
    in place of no weeds in only in 3 cases as shown in Fig. 9, this is better than
    the RESNET50 model. For the CNN model the inference time is only 166ms, which
    is much better than the Xception model. From confusion matrix of Fig. 9 we get
    the parameters of table 1. Table 2 shows the temperature, humidity, and soil moisture
    data of the field under study for different times of the day. The incoming data
    from all the nodes are tabulated here. In the morning as the soil moisture was
    high and the temperature was low, the water pump for irrigation was switched on.
    During the afternoon, in sectors 1 and 3, the soil moisture was below 30% and
    the temperature was above 42°C, the pump in these sectors will be switched on.
    According to the flowchart in Fig. 8, the pump will remain switched on till the
    soil moisture reaches 90%. Whereas, in sector 2, as soil moisture was 50%, the
    pump was in OFF state, in spite of the temperature being 43°C. During the evening,
    as soil moisture and temperature were outside the preset threshold and pump remained
    in the off state. Similarly at night, as the soil had adequate moisture content
    and the temperature being low, they remained off. The data of a particular date
    was taken and displayed on a web dashboard as shown in Fig. 7. The required set
    up for no weed and weed detection is shown in Table 3. When the bot moves and
    captures an image as seen in the first setup of Table 3, the image is correctly
    classified as weeds. For the second setup the captured image is correctly classified
    as no presence of weed. Table II. Node data received on 29th May, 2023 at Baranagar,
    West Bengal, India Table III. Results after testing the model in real time The
    deep learning model was giving wrong results because of the dataset. This was
    due to the dataset having very few training images. There was high bias present
    due to which overfitting occurred, to overcome this the dataset was augmented.
    While rotating the car, it rotates more than or less than the specified angle.
    Also the MPU 6050 sometimes loses calibration. This was mainly due to the SCL
    and SDA wires not being in proper contact throughout the operation of the bot.
    To overcome the above challenges some of the remedial strategies are suggested.
    In order to ensure the car only rotates the specified angle, we will be using
    a PID controller to correct the error in the rotation. Making a PCB with voltage
    regulators to appropriately supply power to each of the components. This will
    also help us to reduce the number of wires. SECTION V. Conclusion The proposed
    Autonomous Bot for precision farming is a promising solution to address the challenges
    faced in traditional farming. The use of inexpensive hardware, like ESP-32 Cam
    module and ESP 32 dev kit makes the system cost-effective and easy to implement.
    The deep learning algorithms for weed detection will reduce the need for manual
    labor, resulting in increased efficiency and accuracy. By using limited amounts
    of weedicides, the system will not only save resources but also protect the environment
    from harmful chemicals. The system''s ability to detect the location of the plant
    in the field and store data in matrix form will aid in autonomously guiding the
    bot throughout the field during operation. From the data acquired after training
    the 3 deep learning models. The Xception model delivered the best accuracy but
    the time to train the model and get an output was significantly longer. The CNN
    model took less time than RESNET 50 and accuracy was also better. Further testing
    and validation on different types of weeds may be required to ensure its effectiveness.
    Overall, the proposed Autonomous Bot for precision farming has the potential to
    revolutionize the agriculture sector in India and improve food security. The use
    of deep learning algorithms for weed detection will not only save time and labor
    but also boost productivity and yield in a sustainable and eco-friendly manner.
    SECTION VI. Future Scope The study can be extended to the use of solar powers
    to make it sustainable and also remove minimal human interaction. The proposed
    system can also be used to implement disease detection and spraying of weedicides.
    The system can be further improved to spray the required amount of weedicide according
    to the amount of weed present in the vicinity of the inspected plant. We plan
    to make a revised prototype that will be able to tackle the undulating terrain
    of the agricultural fields. Authors Figures References Citations Keywords Metrics
    More Like This Deep Learning with Wireless Sensor Network Platform for Multimedia
    Data Modeling 2023 Annual International Conference on Emerging Research Areas:
    International Conference on Intelligent Systems (AICERA/ICIS) Published: 2023
    Monitoring of soil parameters for effective irrigation using Wireless Sensor Networks
    2014 Sixth International Conference on Advanced Computing (ICoAC) Published: 2014
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Mukherjee et al., 2023)
  journal: IEEE International Conference on Electrical, Electronics, Communication
    and Computers, ELEXCOM 2023
  key_findings: The proposed system offers several key benefits, including reduced
    manual labor, increased efficiency and accuracy in weed detection, precision herbicide
    spraying, and improved resource management. It has the potential to significantly
    contribute to sustainable and productive agricultural practices by optimizing
    crop yield, minimizing herbicide usage, and promoting environmental protection.
  limitations: The paper does not provide specific details on the type of deep learning
    model used, its training process, or the size and composition of the dataset employed.
    It also does not address the limitations and potential challenges associated with
    real-world implementation and scalability of the system, such as variations in
    field conditions, plant diversity, lighting conditions, and weather factors.
  main_objective: The primary objective of the study was to develop a cost-effective
    and efficient autonomous system for real-time weed detection and targeted herbicide
    spraying in agricultural fields, leveraging computer vision, deep learning, and
    WSNs to enhance crop monitoring and management.
  relevance_evaluation: This paper is highly relevant as it directly addresses the
    point of focus on integrating high-resolution cameras and computer vision techniques
    for crop monitoring and weed detection in real time. The paper provides a detailed
    description of the proposed system, its components, and their functionality, demonstrating
    its potential to significantly advance the field of automated irrigation management.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Deep learning algorithms, computer vision techniques, wireless
    sensor networks (WSNs), autonomous bot, ESP-32 Cam module, ESP 32 dev kit
  title: Real Time Agricultural Monitoring with Deep Learning Using Wireless Sensor
    Framework
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Kethineni, K., & Pradeepini, G. (2023). An overview of smart agriculture
    activities using machine learning and IoT. AIP Conference Proceedings, 2477(1),
    030033. https://doi.org/10.1063/5.0125643
  authors:
  - Kethineni K.
  - Pradeepini G.
  citation_count: '0'
  data_sources: Literature review
  description: Plant disease detection is a critical component of crop monitoring
    systems. Computer vision and machine learning (ML) approaches have been seen to
    be cutting-edge in addressing a variety of agricultural issues. After disease
    detection, IOT plays a critical role in protecting agriculture in the absence
    of farmers. In various modernization domains Internet of Things (IoT) is a most
    impressive technology that offers effective and dependable approaches. With the
    help of IOT approaches the involvement of humans are very less due to the automatic
    manage and tracking of farms with the built solutions in IoT. The article covers
    a wide range of innovations related to IoT in farming conditions. The major components
    involved in smart farming in IOT are described. Cloud computing plays a key role
    in future of IOT agricultural due to the heterogeneous and massive amount of data
    gathered by IoT devices. Simultaneously, microcontrollers can expand the capabilities
    of the internet of things (IoT). This survey examines the patterns in research,
    principles, fundamental components of IOT, problems, and application of IOT in
    agriculture. Finally, the discussion will conclude with IOT security questions
    and threats. Along with IOT, machine learning techniques aid in the identification
    of crop diseases, allowing farmers to identify crop production and take appropriate
    action. These developments in harvest forecasting research have been made possible
    in large part by advancements in information technology.
  doi: 10.1063/5.0125643
  explanation: The paper titled "An Overview of Smart Agriculture Activities Using
    Machine Learning and IoT" provides a comprehensive examination of various applications
    of the Internet of Things (IoT) and machine learning in the field of agriculture.
    The authors explore the benefits and challenges of IoT-based smart farming systems
    and discuss the potential of these technologies to enhance agricultural practices
    and increase crop production. By providing an overview of the state-of-the-art
    applications of IoT and ML in agriculture, the paper offers valuable insights
    into the current advancements and future directions of smart farming technologies.
  extract_1: '"Machine Vision (MV) approaches are the most effective in addressing
    various complex agricultural issues"'
  extract_2: '"With the help of IOT approaches the involvement of humans are very
    less due to the automatic manage and tracking of farms with the built solutions
    in IoT.'
  full_citation: '>'
  full_text: '>

    "All Content AIP Publishing Portfolio AIP Conference Proceedings                              Advanced
    Search | Citation Search Univ Nebraska Lincoln Lib Sign In HOME BROWSE FOR AUTHORS
    FOR ORGANIZERS ABOUT Volume 2477, Issue 1 24 May 2023 INTERNATIONAL CONFERENCE
    ON ADVANCES IN SIGNAL PROCESSING COMMUNICATIONS AND COMPUTATIONAL INTELLIGENCE
    23–24 July 2021 Hyderabad, India REFERENCES RESEARCH ARTICLE| MAY 24 2023 An overview
    of smart agriculture activities using machine learning and IoT Keerthi Kethineni;
    G. Pradeepini Author & Article Information AIP Conf. Proc. 2477, 030033 (2023)
    https://doi.org/10.1063/5.0125643 Split-Screen PDF Share Tools Plant disease detection
    is a critical component of crop monitoring systems. Computer vision and machine
    learning (ML) approaches have been seen to be cutting-edge in addressing a variety
    of agricultural issues. After disease detection, IOT plays a critical role in
    protecting agriculture in the absence of farmers. In various modernization domains
    Internet of Things (IoT) is a most impressive technology that offers effective
    and dependable approaches. With the help of IOT approaches the involvement of
    humans are very less due to the automatic manage and tracking of farms with the
    built solutions in IoT. The article covers a wide range of innovations related
    to IoT in farming conditions. The major components involved in smart farming in
    IOT are described. Cloud computing plays a key role in future of IOT agricultural
    due to the heterogeneous and massive amount of data gathered by IoT devices. Simultaneously,
    microcontrollers can expand the capabilities of the internet of things (IoT).
    This survey examines the patterns in research, principles, fundamental components
    of IOT, problems, and application of IOT in agriculture. Finally, the discussion
    will conclude with IOT security questions and threats. Along with IOT, machine
    learning techniques aid in the identification of crop diseases, allowing farmers
    to identify crop production and take appropriate action. These developments in
    harvest forecasting research have been made possible in large part by advancements
    in information technology. Topics Microcontroller, Crop production, Information
    technology, Internet of things, Artificial intelligence, Machine learning, Cloud
    computing REFERENCES 1.Sankaran, S.; Mishra, A.; Ehsani, R.; Davis, C. A review
    of advanced techniques for detecting plant diseases. Comput. Electron. Agric.
    2010, 72, 1–13. https://doi.org/10.1016/j.compag.2010.02.007 Google ScholarCrossref   2.Duro,
    D.C.; Franklin, S.E.; Dubé, M.G. A comparison of pixel-based and object-based
    image analysis with selected machine learning algorithms for the classification
    of agricultural landscapes using SPOT-5 HRG imagery. Remote Sens. Environ. 2012,
    118, 259–272. https://doi.org/10.1016/j.rse.2011.11.020 Google ScholarCrossref   3.Esteva,
    A.; Robicquet, A.; Ramsundar, B.; Kuleshov, V.; DePristo, M.; Chou, K.; Cui, C.;
    Corrado, G.; Thrun, S.; Dean, J. A guide to deep learning in healthcare. Nat.
    Med. 2019, 25, 24–29. https://doi.org/10.1038/s41591-018-0316-z Google ScholarCrossref
    PubMed  4.Adhikari, S.P.; Yang, H.; Kim, H. Learning semantic graphics using convolutional
    encoder-decoder network for autonomous weeding in paddy field. Front. Plant Sci.
    2019, 10, 1404. https://doi.org/10.3389/fpls.2019.01404 Google ScholarCrossref
    PubMed  5.Marani, R.; Milella, A.; Petitti, A.; Reina, G. Deep neural networks
    for grape bunch segmentation in natural images from a consumer-grade camera. Precis.
    Agric. 2020, 1–27. Google Scholar  6.Ampatzidis, Y.; Partel, V. UAV-based high
    throughput phenotyping in citrus utilizing multispectral imaging and artificial
    intelligence. Remote Sens. 2019, 11, 410. https://doi.org/10.3390/rs11040410 Google
    ScholarCrossref   7.Saleem, M.H.; Potgieter, J.; Arif, K.M. Plant disease detection
    and classification by deep learning. Plants 2019, 8, 468. https://doi.org/10.3390/plants8110468
    Google ScholarCrossref PubMed  8.Lee, I., & Lee, K. (2015). The Internet of Things
    (IoT): Applications, investments, and challenges for enterprises. Business Horizons,
    58(4), 431–440. https://doi.org/10.1016/j.bushor.2015.03.008 Google ScholarCrossref   9.Chen,
    S., Xu, H., Liu, D., Hu, B., & Wang, H. (2014). A vision of IoT: Applications,
    challenges, and opportunities with china perspective. IEEE Internet of Things
    journal, 1(4), 349–359. https://doi.org/10.1109/JIOT.2014.2337336 Google ScholarCrossref   10.Stočes,
    M., Vaněk, J., Masner, J., & Pavlík, J. (2016). Internet of things (iot) in agriculture-selected
    aspects. Agris on-line Papers in Economics and Informatics, 8(665-2016-45107),
    83–88. Google Scholar  11.Ray, P. P. (2017). Internet of things for smart agriculture:
    Technologies, practices and future direction. Journal of Ambient Intelligence
    and Smart Environments, 9(4), 395–420. https://doi.org/10.3233/AIS-170440 Google
    ScholarCrossref   12.Kamienski, C., Soininen, J. P., Taumberger, M., Dantas, R.,
    Toscano, A., Salmon Cinotti, T. & Torre Neto, A. (2019). Smart water management
    platform: Iot-based precision irrigation for agriculture. Sensors, 19(2), 276.
    https://doi.org/10.3390/s19020276 Google ScholarCrossref PubMed  13.Ojha, T.,
    Misra, S., & Raghuwanshi, N. S. (2015). Wireless sensor networks for agriculture:
    The state-of-the-art in practice and future challenges. Computers and Electronics
    in Agriculture, 118, 66–84. https://doi.org/10.1016/j.compag.2015.08.011 Google
    ScholarCrossref   14.Zhang, X., Zhang, J., Li, L., Zhang, Y., & Yang, G. (2017).
    Monitoring citrus soil moisture and nutrients using an iot based system. Sensors,
    17(3), 447. https://doi.org/10.3390/s17030447 Google ScholarCrossref PubMed  15.Jayaraman,
    P., Yavari, A., Georgakopoulos, D., Morshed, A., & Zaslavsky, A. (2016). Internet
    of things platform for smart farming: Experiences and lessons learnt. Sensors,
    16(11), 1884. https://doi.org/10.3390/s16111884 Google ScholarCrossref PubMed  16.Köksal,
    Ö., & Tekinerdogan, B. (2018). Architecture design approach for IoT-based farm
    management information systems. Precision Agriculture, 1–33. Google Scholar  17.Shabadi,
    L. S., & Biradar, H. B. Design and Implementation of IOT based Smart Security
    and Monitoring for Connected Smart Farming. International Journal of Computer
    Applications, 975, 8887. 18.Minerva, R., Biru, A., & Rotondi, D. (2015). Towards
    a definition of the Internet of Things (IoT). IEEE Internet Initiative (1). Google
    Scholar  19.Chen, X.-Y., & Jin, Z.-G. (2012). Research on key technology andapplications
    for internet of things. Physics Procedia, 33, 561–566. https://doi.org/10.1016/j.phpro.2012.05.104
    Google ScholarCrossref   20.Koshizuka, N., & Sakamura, K. (2010). Ubiquitous ID:
    standards for ubiquitous computing and the Internet of Things. IEEE Pervasive
    Computing, 9(4), 98–101. https://doi.org/10.1109/MPRV.2010.87 Google ScholarCrossref   21.Kushalnagar,
    N., Montenegro, G., & Schumacher, C. (2007). IPv6 over low-power wireless personal
    area networks (6LoWPANs): overview, assumptions, problem statement, and goals.
    Google Scholar  22.Khorov, E., Lyakhov, A., Krotov, A., & Guschin, A. (2015).
    A survey on IEEE 802.11 ah: An enabling networking technology for smart cities.
    Computer Communications, 58, 53–69. https://doi.org/10.1016/j.comcom.2014.08.008
    Google ScholarCrossref   23.Kumar, J. S., & Patel, D. R. (2014). A survey on internet
    of things: Security and privacy issues. International Journal of Computer Applications,
    90(11). Google Scholar  24.Botta, A., De Donato, W., Persico, V., & Pescapé, A.
    (2016). Integration of cloud computing and internet of things: a survey. Future
    generation computer systems, 56, 684–700. https://doi.org/10.1016/j.future.2015.09.021
    Google Scholar  25.Pavón-Pulido, N., López-Riquelme, J. A., Torres, R., Morais,
    R., & Pastor, J. A. (2017). New trends in precision agriculture: a novel cloud-based
    system for enabling data storage and agricultural task planning and automation.
    Precision agriculture, 18(6), 1038–1068. https://doi.org/10.1007/s11119-017-9532-7
    Google ScholarCrossref   26.Zamora-Izquierdo, M. A., Santa, J., Martínez, J. A.,
    Martínez, V., & Skarmeta, A. F. (2019). Smart farming IoT platform based on edge
    and cloud computing. Biosystems engineering, 177, 4–17. https://doi.org/10.1016/j.biosystemseng.2018.10.014
    Google ScholarCrossref   27.Gill, S. S., Chana, I., & Buyya, R. (2017). IoT based
    agriculture as a cloud and big data service: the beginning of digital India. Journal
    of Organizational and End User Computing (JOEUC), 29(4), 1–23. https://doi.org/10.4018/JOEUC.2017100101
    Google ScholarCrossref   28.Kamilaris, A., Kartakoullis, A., & Prenafeta-Boldú,
    F. X. (2017). A review on the practice of big data analysis in agriculture. Computers
    and Electronics in Agriculture, 143, 23–37. https://doi.org/10.1016/j.compag.2017.09.037
    Google ScholarCrossref   29.Liu, X., Zhang, C., Liu, P., Yan, M., Wang, B., Zhang,
    J., & Higgs, R. (2018). Application of Temperature Prediction Based on Neural
    Network in Intrusion Detection of IoT. Security and Communication Networks, 2018.
    Google Scholar  30.Mehra, M., Saxena, S., Sankaranarayanan, S., Tom, R. J., &
    Veeramanikandan, M. (2018). IoT based hydroponics system using Deep Neural Networks.
    Computers and electronics in agriculture, 155, 473–486. https://doi.org/10.1016/j.compag.2018.10.015
    Google ScholarCrossref   31.Navulur, S., & Prasad, M. G. (2017). Agricultural
    management through wireless sensors and internet of things. International Journal
    of Electrical and Computer Engineering, 7(6), 3492. Google Scholar  32.Al-Sarawi,
    S., Anbar, M., Alieyan, K., & Alzubaidi, M. (2017, May). Internet of Things (IoT)
    communication protocols. In 2017 8th International conference on information technology
    (ICIT) (pp. 685–690). IEEE. Google ScholarCrossref   33.CHENG, X. L., & DENG,
    Z. D. (2008). Construction of large-scale wireless sensor network using ZigBee
    specification [J]. Journal on Communications, 11. Google Scholar  34.Brandt, P.,
    Kvakić, M., Butterbach-Bahl, K., & Rufino, M. C. (2017). How to target climate-smart
    agriculture? Concept and application of the consensus-driven decision support
    framework “targetCSA”. Agricultural Systems, 151, 234–245. https://doi.org/10.1016/j.agsy.2015.12.011
    Google ScholarCrossref   35.Talavera, J. M., Tobón, L. E., Gómez, J. A., Culman,
    M. A., Aranda, J. M., Parra, D. T., … & Garreta, L. E. (2017). Review of IoT applications
    in agro-industrial and environmental fields. Computers and Electronics in Agriculture,
    142, 283–297. https://doi.org/10.1016/j.compag.2017.09.015 Google ScholarCrossref   36.de
    Morais, C. M., Sadok, D., & Kelner, J. (2019). An IoT sensor and scenario survey
    for data researchers. Journal of the Brazilian Computer Society, 25(1), 4. https://doi.org/10.1186/s13173-019-0085-7
    Google ScholarCrossref   37.Zhang, X., Zhang, J., Li, L., Zhang, Y., & Yang, G.
    (2017). Monitoring citrus soil moisture and nutrients using an iot based system.
    Sensors, 17(3), 447. https://doi.org/10.3390/s17030447 Google ScholarCrossref
    PubMed  38.Bodake, K., Ghate, R., Doshi, H., Jadhav, P., & Recommendation System
    using Internet of Things. MVP Tarle, B. (2018). Soil based Fertilizer Journal
    of Engineering Sciences, 1(1), 13–19. Google Scholar  39.Zhang, S., Chen, X.,
    & Wang, S. (2014, August). Research on the monitoring system of wheat diseases,
    pests and weeds based on IOT. In 2014 9th International Conference on Computer
    Science & Education (pp. 981–985). IEEE. Google ScholarCrossref   40.Windsperger,
    B., Windsperger, A., Bird, D. N., Schwaiger, H., Jungmeier, G., Nathani, C., &
    Frischknecht, R. (2019). Greenhouse gas emissions due to national product consumption:
    from demand and research gaps to addressing key challenges. International journal
    of environmental science and technology, 16(2), 1025–1038. https://doi.org/10.1007/s13762-018-1743-6
    Google ScholarCrossref   41.Shirsath, D. O., Kamble, P., Mane, R., Kolap, A.,
    & More, R. S. (2017). IoT based smart greenhouse automation using Arduino. International
    Journal of Innovative Research in Computer Science & Technology, 5(2), 234–8.
    Google ScholarCrossref   42.“U.S. Sugar and AgriSource Data Leverage Ingenu’s
    Machine Network to Deliver Innovative Smart Agriculture Solution.” [Online]. Available:
    https://www.ingenu.com/2016/12/us-sugar-and-agrisource-data-leverage-ingenu-machine-network-to-deliver-innovative-smart-agriculture-solution.
    43.Asikainen, M., Haataja, K., & Toivanen, P. (2013, July). Wireless indoor tracking
    of livestock for behavioral analysis. In 2013 9th International Wireless Communications
    and Mobile Computing Conference (IWCMC) (pp. 1833–1838). IEEE. Google ScholarCrossref   44.Ojha,
    T., Misra, S., & Raghuwanshi, N. S. (2015). Wireless sensor networks for agriculture:
    The state-of-the-art in practice and future challenges. Computers and Electronics
    in Agriculture, 118, 66–84. https://doi.org/10.1016/j.compag.2015.08.011 Google
    ScholarCrossref   45.Jawad, H., Nordin, R., Gharghan, S., Jawad, A., & Ismail,
    M. (2017). Energy-efficient wireless sensor networks for precision agriculture:
    A review. Sensors, 17(8), 1781. https://doi.org/10.3390/s17081781 Google ScholarCrossref
    PubMed  46.Elijah, O., Rahman, T. A., Orikumhi, I., Leow, C. Y., & Hindia, M.
    N. (2018). An overview of Internet of Things (IoT) and data analytics in agriculture:
    Benefits and challenges. IEEE Internet of Things Journal, 5(5), 3758–3773. https://doi.org/10.1109/JIOT.2018.2844296
    Google ScholarCrossref   47.Reddy, A.V.N., Krishna, C.P. & Mallick, P.K. An image
    classification framework exploring the capabilities of extreme learning machines
    and artificial bee colony. Neural Comput & Applic 32, 3079–3099 (2020). https://doi.org/10.1007/s00521-019-04385-5
    Google ScholarCrossref   48.Murugan R., devi R.K., Albert A.J., Nayak D.K. (2020)
    An IOT Based Weather Monitoring System to Prevent and Alert Cauvery Delta District
    of Tamilnadu, India. In: Pandian A., Senjyu T., Islam S., Wang H. (eds) Proceeding
    of the International Conference on Computer Networks, Big Data and IoT (ICCBI
    - 2018). ICCBI 2018. Lecture Notes on Data Engineering and Communications Technologies,
    vol 31. Springer, Cham. Google ScholarCrossref   49.Bhimanpallewar, Ratnmala &
    rama narasingarao, Manda. (2020). Alternative approaches of Machine Learning for
    Agriculture Advisory System. 27–31. https://doi.org/10.1109/Confluence47617.2020.9058152.
    Google Scholar  50.lalitha bhavani, B & krishnaveni, G & Malathi, J. (2019). A
    comparative performance analysis of different machine learning techniques. Journal
    of Physics: Conference Series. 1228. 012035. https://doi.org/10.1088/1742-6596/1228/1/012035.
    Google ScholarCrossref   51.Lakshmi Mallika I., D. Venkata Ratnam, Saravana Raman,
    G. Sivavaraprasad, Machine learning algorithm to forecast ionospheric time delays
    using Global Navigation satellite system observations, Acta Astronautica, Volume
    173, 2020, Pages 221–231,ISSN 0094-5765, https://doi.org/10.1016/j.actaastro.2020.04.048
    Google ScholarCrossref   52.Dabbakuti, J. R. K. Kumar & Ch, Bhupati. (2019). Ionospheric
    monitoring system based on the Internet of Things with ThingSpeak. Astrophysics
    and Space Science. 364. https://doi.org/10.1007/s10509-019-3630-0 Google Scholar  This
    content is only available via PDF. PDF ©2023 Authors. Published by AIP Publishing.
    View Metrics Citing Articles Via Google Scholar CrossRef (1) Publish with us -
    Request a Quote! Sign up for alerts Most Read Most Cited Phytochemical analysis
    of bioactive compounds in ethanolic extract of Sterculia quadrifida R.Br. Siswadi
    Siswadi, Grace Serepina Saragih Impact of blockchain technology development on
    industries in the context of entrepreneurial, marketing and management perspectives
    worldwide Ivelina Kulova Design of a 100 MW solar power plant on wetland in Bangladesh
    Apu Kowsar, Sumon Chandra Debnath, et al. Online ISSN 1551-7616 Print ISSN 0094-243X
    Resources For Researchers For Librarians For Advertisers Our Publishing Partners  Explore
    Journals Physics Today Conference Proceedings Books Special Topics Publishers
    pubs.aip.org About User Guide Contact Us Register Help Privacy Policy Terms of
    Use Connect with AIP Publishing Facebook LinkedIn Twitter YouTube © Copyright
    2024 AIP Publishing LLC"'
  inline_citation: (Kethineni & Pradeepini, 2023)
  journal: AIP Conference Proceedings
  key_findings: IoT and machine learning technologies offer significant potential
    to enhance agricultural practices and increase crop production by enabling real-time
    monitoring, data analysis, and automated decision-making.
  limitations: The paper does not provide in-depth analysis of the specific technologies
    and methods used for visual monitoring of crop growth, disease detection, and
    irrigation system performance.
  main_objective: To provide an overview of the applications of IoT and machine learning
    in agriculture and discuss the potential of these technologies to enhance agricultural
    practices and increase crop production.
  relevance_evaluation: This paper is **highly relevant** to the specific point of
    integrating high-resolution cameras and computer vision algorithms for visual
    monitoring of crop growth, disease detection, and irrigation system performance.
    While the paper does not explicitly focus on automated irrigation systems, it
    provides a broad overview of the applications of IoT and machine learning in agriculture,
    including the use of visual monitoring techniques for crop and irrigation system
    management.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT, machine learning, computer vision
  title: An Overview of Smart Agriculture Activities Using Machine Learning and IoT
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Bilal, A., Liu, X., Long, H., Shafiq, M., & Waqar, M. (2023). Increasing
    crop quality and yield with a machine learning-based crop monitoring system. Computers,
    Materials & Continua, 76(2), 2401-2426. https://doi.org/10.32604/cmc.2023.037857
  authors:
  - Bilal A.
  - Liu X.
  - Long H.
  - Shafiq M.
  - Waqar M.
  citation_count: '0'
  data_sources: Not explicitly mentioned in the provided text
  description: Farming is cultivating the soil, producing crops, and keeping livestock.
    The agricultural sector plays a crucial role in a country’s economic growth. This
    research proposes a two-stage machine learning framework for agriculture to improve
    efficiency and increase crop yield. In the first stage, machine learning algorithms
    generate data for extensive and far-flung agricultural areas and forecast crops.
    The recommended crops are based on various factors such as weather conditions,
    soil analysis, and the amount of fertilizers and pesticides required. In the second
    stage, a transfer learning-based model for plant seedlings, pests, and plant leaf
    disease datasets is used to detect weeds, pesticides, and diseases in the crop.
    The proposed model achieved an average accuracy of 95%, 97%, and 98% in plant
    seedlings, pests, and plant leaf disease detection, respectively. The system can
    help farmers pinpoint the precise measures required at the right time to increase
    yields.
  doi: 10.32604/cmc.2023.037857
  explanation: This study presents a two-stage machine learning framework for crop
    monitoring to enhance agricultural efficiency and increase crop yield. The first
    stage focuses on crop forecasting based on various factors, while the second stage
    utilizes transfer learning for seedling, pest, and disease detection. The authors
    achieved high accuracy in all three detection tasks, demonstrating the potential
    of the proposed framework.
  extract_1: ''
  extract_2: ''
  full_citation: '>'
  full_text: '>

    "Submit LOGIN REGISTER Home Academic Journals Books & Monographs Conferences Language
    Service News & Announcements About Home/ Journals/ CMC/ Vol.76, No.2, 2023/ 10.32604/cmc.2023.037857
    Submit a Paper Propose a Special lssue Table of Content Abstract Introduction
    Related Work Proposed Methodology Experimental Results and Discussion Conclusion
    References Open Access ARTICLE Increasing Crop Quality and Yield with a Machine
    Learning-Based Crop Monitoring System Anas Bilal1,*, Xiaowen Liu1, Haixia Long1,*,
    Muhammad Shafiq2, Muhammad Waqar3 1 College of Information Science Technology,
    Hainan Normal University, Haikou, 571158, China 2 School of Information Engineering,
    Qujing Normal University, Qujing, 655011, China 3 Department of Computer Science,
    COMSATS University, Islamabad, 45550, Pakistan * Corresponding Authors: Anas Bilal.
    Email: ; Haixia Long. Email: Computers, Materials & Continua 2023, 76(2), 2401-2426.
    https://doi.org/10.32604/cmc.2023.037857 Received 18 November 2022; Accepted 19
    June 2023; Issue published 30 August 2023 View Full Text Download PDF Abstract
    Farming is cultivating the soil, producing crops, and keeping livestock. The agricultural
    sector plays a crucial role in a country’s economic growth. This research proposes
    a two-stage machine learning framework for agriculture to improve efficiency and
    increase crop yield. In the first stage, machine learning algorithms generate
    data for extensive and far-flung agricultural areas and forecast crops. The recommended
    crops are based on various factors such as weather conditions, soil analysis,
    and the amount of fertilizers and pesticides required. In the second stage, a
    transfer learning-based model for plant seedlings, pests, and plant leaf disease
    datasets is used to detect weeds, pesticides, and diseases in the crop. The proposed
    model achieved an average accuracy of 95%, 97%, and 98% in plant seedlings, pests,
    and plant leaf disease detection, respectively. The system can help farmers pinpoint
    the precise measures required at the right time to increase yields. Keywords Machine
    learning; computer vision; trends in smart farming; precision agriculture; Agriculture
    4.0 Cite This Article A. Bilal, X. Liu, H. Long, M. Shafiq and M. Waqar, \"Increasing
    crop quality and yield with a machine learning-based crop monitoring system,\"
    Computers, Materials & Continua, vol. 76, no.2, pp. 2401–2426, 2023. BibTex EndNote
    RIS    This work is licensed under a Creative Commons Attribution 4.0 International
    License , which permits unrestricted use, distribution, and reproduction in any
    medium, provided the original work is properly cited. We recommend Identification
    of Crop Diseases Based on Improved Genetic Algorithm and Extreme Learning Machine
    Linguo Li et al., CMC-Computers, Materials & Continua, 2020 Enrichment of Crop
    Yield Prophecy Using Machine Learning Algorithms R. Kingsy Grace et al., Intelligent
    Automation & Soft Computing, 2022 Multimodal Machine Learning Based Crop Recommendation
    and Yield Prediction Model P. S. et al., Intelligent Automation & Soft Computing,
    2022 IoT and Machine Learning Based Stem Borer Pest Prediction Rana Muhammad Nadeem
    et al., Intelligent Automation & Soft Computing, 2022 Deep Transfer Learning Based
    Detection and Classification of Citrus Plant Diseases Shah Faisal et al., CMC-Computers,
    Materials & Continua, 2023 Lettuce have it: Machine learning for cr-optimization
    Phys.org, 2019 Chemists show how bias can crop up in machine learning algorithm
    results Phys.org, 2019 How sensors and big data can help cut food waste by Jean
    Frederic Isingizwe Nturambirwe et al., TechXplore.com, 2020 Impact of dataset
    on the study of crop disease image recognition Yuan Yuan et al., International
    Journal of Agricultural and Biological Engineering, 2022 Robot uses machine learning
    to harvest lettuce by Sarah Collins et al., TechXplore.com, 2019 Powered by Downloads
    Citation Tools 334 View 170 Download 0 Like Related articles Improved VGG Model
    for Road Traffic Sign Recognition Shuren Zhou, Wenlong Liang, Junguo... A Method
    for Improving CNN-Based Image Recognition Using DCGAN Wei Fang, Feihong Zhang,
    Victor... Brake Fault Diagnosis Through Machine Learning Approaches – A Review
    Alamelu Manghai T.M., Jegadeeshwaran... A Comparative Study of Bayes Classifiers
    for Blade Fault Diagnosis in Wind Turbines through Vibration Signals A. Joshuva,
    V. Sugumaran Condition Monitoring of Roller Bearing by K-Star Classifier and K-Nearest
    Neighborhood Classifier Using Sound Signal. Rahul Kumar Sharma, V. Sugumaran,..."'
  inline_citation: (Bilal et al., 2023)
  journal: Computers, Materials and Continua
  key_findings: The proposed two-stage machine learning framework achieved high accuracy
    in crop forecasting, plant seedling detection, pest detection, and plant leaf
    disease detection.
  limitations: The study does not directly address the specific technologies or applications
    mentioned in the point of focus, such as high-resolution cameras and computer
    vision algorithms. The study primarily focuses on the development and evaluation
    of a machine learning framework for crop monitoring, rather than the integration
    of specific technologies.
  main_objective: To develop a machine learning framework for agriculture to improve
    efficiency and increase crop yield, focusing on crop forecasting and seedling,
    pest, and disease detection.
  relevance_evaluation: This paper is moderately relevant to the specific point of
    integrating high-resolution cameras and computer vision for visual monitoring
    of crop growth, disease detection, and irrigation system performance. While the
    study focuses on broader machine learning applications in agriculture, it does
    not directly address the integration of specific technologies like high-resolution
    cameras and computer vision algorithms. However, it provides valuable insights
    into the potential benefits and challenges of machine learning in agricultural
    monitoring systems.
  relevance_score: '0.65'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Machine learning algorithms, transfer learning models
  title: Increasing Crop Quality and Yield with a Machine Learning-Based Crop Monitoring
    System
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Alohali, M. A., Al-Mutiri, F., Othman, K. M., Yafoz, A., Alsini, R.,
    & Salama, A. S. (2024). An enhanced tunicate swarm algorithm with deep-learning
    based rice seedling classification for sustainable computing based smart agriculture.
    AIMS Mathematics, 9(4), 10185-10207. https://doi.org/10.3934/math.2024498
  authors:
  - Alohali M.A.
  - Al-Mutiri F.
  - Othman K.M.
  - Yafoz A.
  - Alsini R.
  - Salama A.S.
  citation_count: '0'
  data_sources: UAV Rice Seedling Classification dataset
  description: 'Smart agricultural techniques employ current information and communication
    technologies, leveraging artificial intelligence (AI) for effectually managing
    the crop. Recognizing rice seedlings, which is crucial for harvest estimation,
    traditionally depends on human supervision but can be expedited and enhanced via
    computer vision (CV). Unmanned aerial vehicles (UAVs) equipped with high-resolution
    cameras bestow a swift and precise option for crop condition surveillance, specifically
    in cloudy states, giving valuable insights into crop management and breeding programs.
    Therefore, we improved an enhanced tunicate swarm algorithm with deep learning-based
    rice seedling classification (ETSADL-RSC). The presented ETSADL-RSC technique
    examined the UAV images to classify them into two classes: Rice seedlings and
    arable land. Initially, the quality of the pictures could be enhanced by a contrast
    limited adaptive histogram equalization (CLAHE) approach. Next, the ETSADL-RSC
    technique used the neural architectural search network (NASNet) method for the
    feature extraction process and its hyperparameters could be tuned by the ETSA
    model. For rice seedling classification, the ETSADL-RSC technique used a sparse
    autoencoder (SAE) model. The experimental outcome study of the ETSADL-RSC system
    was verified for the UAV Rice Seedling Classification dataset. Wide simulation
    analysis of the ETSADL-RSC model stated the greater accuracy performance of 97.79%
    over other DL classifiers.'
  doi: 10.3934/math.2024498
  explanation: This paper presents the development and evaluation of an enhanced tunicate
    swarm algorithm with deep-learning based rice seedling classification (ETSADL-RSC)
    for smart agriculture applications. The ETSADL-RSC technique leverages unmanned
    aerial vehicles (UAVs) equipped with high-resolution cameras to capture crop images
    and employs a deep learning model for efficient rice seedling classification.
    The study's main objective is to enhance agricultural practices by automating
    the identification and classification of rice seedlings in crop fields.
  extract_1: '"The presented ETSADL-RSC technique examined the UAV images to classify
    them into two classes: Rice seedlings and arable land. Initially, the quality
    of the pictures could be enhanced by a contrast limited adaptive histogram equalization
    (CLAHE) approach. Next, the ETSADL-RSC technique used the neural architectural
    search network (NASNet) method for the feature extraction process and its hyperparameters
    could be tuned by the ETSA model. For rice seedling classification, the ETSADL-RSC
    technique used a sparse autoencoder (SAE) model."'
  extract_2: '"The experimental outcome study of the ETSADL-RSC system was verified
    for the UAV Rice Seedling Classification dataset. Wide simulation analysis of
    the ETSADL-RSC model stated the greater accuracy performance of 97.79% over other
    DL classifiers."'
  full_citation: '>'
  full_text: '>

    "Home Journals Journal Home About Contribute Articles AIMS Mathematics 2024, Volume
    9, Issue 4: 10185-10207. doi: 10.3934/math.2024498 Previous Article Next Article
    Research article Special Issues An enhanced tunicate swarm algorithm with deep-learning
    based rice seedling classification for sustainable computing based smart agriculture
    Manal Abdullah Alohali 1 ,  Fuad Al-Mutiri 2 ,  Kamal M. Othman 3 ,  , ,  Ayman
    Yafoz 4 ,  Raed Alsini 4 ,  Ahmed S. Salama 5 1. Department of Information Systems,
    College of Computer and Information Sciences, Princess Nourah bint Abdulrahman
    University, P.O. Box 84428, Riyadh 11671, Saudi Arabia 2. Department of Mathematics,
    Faculty of Sciences and Arts, King Khalid University, Muhayil Asir, Saudi Arabia
    3. Department of Electrical Engineering, College of Engineering and Islamic Architecture,
    Umm Al-Qura University, Makkah, Saudi Arabia 4. Department of Information Systems,
    Faculty of Computing and Information Technology, King Abdulaziz University, Jeddah,
    Saudi Arabia 5. Department of Electrical Engineering, Faculty of Engineering &
    Technology, Future University in Egypt, New Cairo 11845, Egypt Received: 21 January
    2024 Revised: 29 February 2024 Accepted: 06 March 2024 Published: 14 March 2024
    MSC : 11Y40 Special Issue: Advances of Artificial Intelligence-based mathematical
    modeling optimization in engineering applications Abstract Full Text(HTML) Download
    PDF Smart agricultural techniques employ current information and communication
    technologies, leveraging artificial intelligence (AI) for effectually managing
    the crop. Recognizing rice seedlings, which is crucial for harvest estimation,
    traditionally depends on human supervision but can be expedited and enhanced via
    computer vision (CV). Unmanned aerial vehicles (UAVs) equipped with high-resolution
    cameras bestow a swift and precise option for crop condition surveillance, specifically
    in cloudy states, giving valuable insights into crop management and breeding programs.
    Therefore, we improved an enhanced tunicate swarm algorithm with deep learning-based
    rice seedling classification (ETSADL-RSC). The presented ETSADL-RSC technique
    examined the UAV images to classify them into two classes: Rice seedlings and
    arable land. Initially, the quality of the pictures could be enhanced by a contrast
    limited adaptive histogram equalization (CLAHE) approach. Next, the ETSADL-RSC
    technique used the neural architectural search network (NASNet) method for the
    feature extraction process and its hyperparameters could be tuned by the ETSA
    model. For rice seedling classification, the ETSADL-RSC technique used a sparse
    autoencoder (SAE) model. The experimental outcome study of the ETSADL-RSC system
    was verified for the UAV Rice Seedling Classification dataset. Wide simulation
    analysis of the ETSADL-RSC model stated the greater accuracy performance of 97.79%
    over other DL classifiers. Keywords: smart agriculture, crop monitoring, rice
    seedling, computer vision, image classification, deep learning Citation: Manal
    Abdullah Alohali, Fuad Al-Mutiri, Kamal M. Othman, Ayman Yafoz, Raed Alsini, Ahmed
    S. Salama. An enhanced tunicate swarm algorithm with deep-learning based rice
    seedling classification for sustainable computing based smart agriculture[J].
    AIMS Mathematics, 2024, 9(4): 10185-10207. doi: 10.3934/math.2024498 Related Papers:
    [1] S. Rama Sree, E Laxmi Lydia, C. S. S. Anupama, Ramya Nemani, Soojeong Lee,
    Gyanendra Prasad Joshi, Woong Cho . A battle royale optimization with feature
    fusion-based automated fruit disease grading and classification. AIMS Mathematics,
    2024, 9(5): 11432-11451. doi: 10.3934/math.2024561 [2] Mashael Maashi, Mohammed
    Abdullah Al-Hagery, Mohammed Rizwanullah, Azza Elneil Osman . Deep convolutional
    neural network-based Leveraging Lion Swarm Optimizer for gesture recognition and
    classification. AIMS Mathematics, 2024, 9(4): 9380-9393. doi: 10.3934/math.2024457
    [3] Abdelwahed Motwake, Aisha Hassan Abdalla Hashim, Marwa Obayya, Majdy M. Eltahir
    . Enhancing land cover classification in remote sensing imagery using an optimal
    deep learning model. AIMS Mathematics, 2024, 9(1): 140-159. doi: 10.3934/math.2024009
    [4] Eman A. Al-Shahari, Marwa Obayya, Faiz Abdullah Alotaibi, Safa Alsafari, Ahmed
    S. Salama, Mohammed Assiri . Accelerating biomedical image segmentation using
    equilibrium optimization with a deep learning approach. AIMS Mathematics, 2024,
    9(3): 5905-5924. doi: 10.3934/math.2024288 [5] Alaa O. Khadidos . Advancements
    in remote sensing: Harnessing the power of artificial intelligence for scene image
    classification. AIMS Mathematics, 2024, 9(4): 10235-10254. doi: 10.3934/math.2024500
    [6] Thavavel Vaiyapuri, M. Sivakumar, Shridevi S, Velmurugan Subbiah Parvathy,
    Janjhyam Venkata Naga Ramesh, Khasim Syed, Sachi Nandan Mohanty . An intelligent
    water drop algorithm with deep learning driven vehicle detection and classification.
    AIMS Mathematics, 2024, 9(5): 11352-11371. doi: 10.3934/math.2024557 [7] Tamilvizhi
    Thanarajan, Youseef Alotaibi, Surendran Rajendran, Krishnaraj Nagappan . Improved
    wolf swarm optimization with deep-learning-based movement analysis and self-regulated
    human activity recognition. AIMS Mathematics, 2023, 8(5): 12520-12539. doi: 10.3934/math.2023629
    [8] Fahad F. Alruwaili . Ensuring data integrity in deep learning-assisted IoT-Cloud
    environments: Blockchain-assisted data edge verification with consensus algorithms.
    AIMS Mathematics, 2024, 9(4): 8868-8884. doi: 10.3934/math.2024432 [9] Yuzi Jin,
    Soobin Kwak, Seokjun Ham, Junseok Kim . A fast and efficient numerical algorithm
    for image segmentation and denoising. AIMS Mathematics, 2024, 9(2): 5015-5027.
    doi: 10.3934/math.2024243 [10] Majdy M. Eltahir, Ghadah Aldehim, Nabil Sharaf
    Almalki, Mrim M. Alnfiai, Azza Elneil Osman . Reinforced concrete bridge damage
    detection using arithmetic optimization algorithm with deep feature fusion. AIMS
    Mathematics, 2023, 8(12): 29290-29306. doi: 10.3934/math.20231499             Reader
    Comments                  Your name:* Email:* We recommend A battle royale optimization
    with feature fusion-based automated fruit disease grading and classification S.
    Rama Sree et al., AIMS Mathematics, 2024 Tunicate swarm algorithm with deep convolutional
    neural network-driven colorectal cancer classification from histopathological
    imaging data Abdullah S. AL-Malaise AL-Ghamdi et al., Mathematical Modelling and
    Control Enhancing land cover classification in remote sensing imagery using an
    optimal deep learning model Abdelwahed Motwake et al., AIMS Mathematics, 2023
    A hybrid network intrusion detection using darwinian particle swarm optimization
    and stacked autoencoder hoeffding tree B. Ida Seraphim et al., Mathematical Biosciences
    and Engineering, 2021 Hybrid arithmetic optimization algorithm with deep learning
    model for secure Unmanned Aerial Vehicle networks Sultanah M. Alshammari et al.,
    AIMS Mathematics, 2024 Betting on drones as smart agricultural tools for pesticide
    use in farms by Tokyo University of Science, Phys.org, 2021 Eyes in the sky: Using
    drones to assess the severity of crop diseases by NanJing Agricultural University,
    Phys.org, 2023 A Knee Point Based Coevolution Multi-objective Particle Swarm Optimization
    Algorithm for Heterogeneous UAV Cooperative Multi-task Allocation WANG Feng et
    al., Acta Automatica Sinica, 2023 UAV swarm task allocation algorithm based on
    the alternating direction method of multipliers network potential game theory
    PENG Ya-lan et al., Chinese Journal of Engineering, 2022 Stacked spectral feature
    space patch: An advanced spectral representation for precise crop classification
    based on convolutional neural network Hui Chen et al., The Crop Journal, 2022
    Powered by © 2024 the Author(s), licensee AIMS Press. This is an open access article
    distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0)
    AIMS Mathematics 2.2 3 Metrics Article Views(116) PDF Downloads(11) Cited By(0)
    Preview PDF Download XML Export Citation Figures and Tables Figures(17)  /  Tables(4)
    Other Articles By Authors On This Site On Google Scholar Related pages on Google
    Scholar on PubMed Tools Email to a friend About AIMS Press Open Access Policy
    Contact us Copyright © AIMS Press"'
  inline_citation: (Alohali et al., 2024)
  journal: AIMS Mathematics
  key_findings: The proposed ETSADL-RSC technique demonstrated high accuracy in classifying
    rice seedlings from UAV images, achieving 97.79% accuracy. The technique employed
    deep learning and image processing methods to extract features and classify seedlings
    effectively.
  limitations: The study focuses specifically on rice seedlings and does not explore
    the classification of other crop types or the integration of irrigation systems.
    The study was also conducted using a specific dataset, and the generalizability
    of the findings to other datasets or conditions may need further investigation.
  main_objective: To develop an enhanced tunicate swarm algorithm with deep-learning
    based rice seedling classification (ETSADL-RSC) for automated and efficient rice
    seedling identification in crop fields using UAVs and image processing techniques.
  relevance_evaluation: The paper is relevant to the outline point as it addresses
    the integration of high-resolution cameras and computer vision algorithms for
    visual monitoring of crop growth and disease detection. The proposed ETSADL-RSC
    technique utilizes deep learning and image processing techniques to classify rice
    seedlings from UAV images, providing valuable information for automated irrigation
    management.
  relevance_score: '0.8'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: UAVs, High-resolution cameras, Computer vision algorithms, Deep
    learning, Image processing, Neural architectural search network (NASNet), Sparse
    autoencoder (SAE)
  title: An enhanced tunicate swarm algorithm with deep-learning based rice seedling
    classification for sustainable computing based smart agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Imran Moazzam, S., Khan, U. S., Qureshi, W. S., Tiwana, M. I., Rashid,
    N., Alasmary, W. S., ... Iqbal, J. (2021). A Patch-Image Based Classification
    Approach for Detection of Weeds in Sugar Beet Crop. IEEE Access, 9, 121698-121715.
    https://doi.org/10.1109/ACCESS.2021.3109015
  authors:
  - Moazzam S.I.
  - Khan U.S.
  - Qureshi W.S.
  - Tiwana M.I.
  - Rashid N.
  - Alasmary W.S.
  - Iqbal J.
  - Hamza A.
  citation_count: '16'
  data_sources: Airborne multispectral camera sensors, sugar beet crop aerial imagery
    datasets
  description: Weeds affects crops health as it shares water and nutrients from the
    soil, as a result it decreases crop yield. Manual weedicide spray through bag-pack
    is hazardous to human health. Localized autonomous weedicide spray through aerial
    spraying units can help save water, weedicide chemical and effect less on human
    health. Such systems require multi-spectral cues to classify crop, weed, and soil
    surface. Our focus in this paper is on the detection of weeds in the sugar beet
    crop, using air-borne multispectral camera sensors, which is considered as an
    alternative crop to sugarcane to obtain sugar in Pakistan. We developed a new
    framework for weed identification; a patch-based classification approach as appose
    to semantic segmentation that is more realistic for real-time intelligent aerial
    spraying systems. Our approach converts 3-class pixel classification problem into
    a 2-class crop-weed patch classification problem which in turns improves crop
    and weed classification accuracy. For classification, we developed a new VGG-Beet
    convolutional neural network (CNN), which is based on generic VGG16 (visual graphics
    group) CNN model with 11 convolutional layers. For experiments, we captured a
    sugar beet dataset with 3-channel multispectral sensor with a ground sampling
    distance (GSD) of 0.2 cm/pixel and a height of 4 meters. For better comparison,
    we used two publicly available sugar beet crop aerial imagery datasets, captured
    using a 5-channel multispectral sensor and a 4-Channel multispectral sensor with
    a ground sampling distance of 1cm and a height of 10 meters. We observed that
    patch-based method is more robust to different lighting conditions. To produce
    low cost weed detection system usage of Agrocam sensor is recommended, for higher
    accuracy Red Edge and Sequoia multispectral sensors with more channels should
    be deployed. We observed higher crop-weed accuracy and lower testing time for
    our patch-based approach as compared to U-Net and Deeplab based semantic segmentation
    networks.
  doi: 10.1109/ACCESS.2021.3109015
  explanation: The study presented an innovative weed detection framework for real-time
    aerial spraying systems in precision agriculture. It leverages a patch-based classification
    approach, unlike traditional semantic segmentation, to enhance classification
    accuracy and reduce computational complexity. The proposed framework utilizes
    a custom-developed VGG-Beet convolutional neural network (CNN) for patch classification
    based on the generic VGG16 model.
  extract_1: For classification, we developed a new VGG-Beet convolutional neural
    network (CNN), which is based on generic CNN (VGG) model with 11 convolutional
    layers.
  extract_2: Our approach converts 3-class pixel classification problem into a 2-class
    crop-weed patch classification problem which in turns improves crop and weed classification
    accuracy.
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Journals & Magazines >IEEE Access >Volume: 9
    A Patch-Image Based Classification Approach for Detection of Weeds in Sugar Beet
    Crop Publisher: IEEE Cite This PDF S. Imran Moazzam; Umar S. Khan; Waqar S. Qureshi;
    Mohsin I. Tiwana; Nasir Rashid; Waleed S. Alasmary; Javaid Iqbal; Amir Hamza All
    Authors 18 Cites in Papers 2193 Full Text Views Open Access Comment(s) Under a
    Creative Commons License Abstract Document Sections I. Introduction II. Materials
    and Methods III. System Architecture IV. Implementation V. Results & Discussion
    Show Full Outline Authors Figures References Citations Keywords Metrics Abstract:
    Weeds affects crops health as it shares water and nutrients from the soil, as
    a result it decreases crop yield. Manual weedicide spray through bag-pack is hazardous
    to human health. Localized autonomous weedicide spray through aerial spraying
    units can help save water, weedicide chemical and effect less on human health.
    Such systems require multi-spectral cues to classify crop, weed, and soil surface.
    Our focus in this paper is on the detection of weeds in the sugar beet crop, using
    air-borne multispectral camera sensors, which is considered as an alternative
    crop to sugarcane to obtain sugar in Pakistan. We developed a new framework for
    weed identification; a patch-based classification approach as appose to semantic
    segmentation that is more realistic for real-time intelligent aerial spraying
    systems. Our approach converts 3-class pixel classification problem into a 2-class
    crop-weed patch classification problem which in turns improves crop and weed classification
    accuracy. For classification, we developed a new VGG-Beet convolutional neural
    network (CNN), which is based on generic VGG16 (visual graphics group) CNN model
    with 11 convolutional layers. For experiments, we captured a sugar beet dataset
    with 3-channel multispectral sensor with a ground sampling distance (GSD) of 0.2
    cm/pixel and a height of 4 meters. For better comparison, we used two publicly
    available sugar beet crop aerial imagery datasets, captured using a 5-channel
    multispectral sensor and a 4-Channel multispectral sensor with a ground sampling
    distance of 1cm and a height of 10 meters. We observed that patch-based method
    is more robust to different lighting conditions. To produce low cost weed detection
    system usage of Agrocam sensor is recommended, for higher accuracy Red Edge and
    Sequoia multispectral sensors with more channels should be deployed. We observed
    higher crop-weed accuracy and lower testing time for our patch-based approach
    as compared to U-Net and Deeplab based semantic segmentatio... (Show More) System
    flow diagram. Published in: IEEE Access ( Volume: 9) Page(s): 121698 - 121715
    Date of Publication: 30 August 2021 Electronic ISSN: 2169-3536 DOI: 10.1109/ACCESS.2021.3109015
    Publisher: IEEE Funding Agency: SECTION I. Introduction Weeds affects crops health
    as it shares water and nutrients from the soil, as a result it decreases crop
    yield [1], [2]. Weeds should be removed timely for good crop health [3]. Weed
    removal is done in two ways in developing countries such as Pakistan, either manually
    through human workers or using weedicides. Manual weeding with hand tools is time-consuming
    and tedious. Weedicide spray using backpack using human labor affects human health.
    Spraying with tractor causes pollution and is not possible in cases where the
    tractors do not have access. Sometimes equal spray on crop and weed affects crop
    health badly and crop could also absorb hazardous chemicals. Autonomous weed removal
    [4] using robots is widely investigated by researchers. For efficient weed removal,
    weed detection is important. Autonomous aerial spraying units are also developed,
    and field tested for spraying weeds. Weed detection for spraying using aerial
    spraying units (ASU) [5] is better than spraying with ground-based robots as spraying
    with drones is time efficient and become essential in the case where the ground-mounted
    spray unit cannot reach. Currently commercially available developed drones perform
    spraying by using GPS coordinates without distinguishing weeds and crops. To effectively
    spray using drone i.e. to spray only on weeds, the weed profile information must
    be taken out and provided to the drone. In Pakistan, losses caused by weeds in
    our major crops are estimated to be around 388 million US dollars and further
    around 149 million US dollars are spent to control weeds with herbicides, machinery
    or farm labor [6]. Sugar beet is considered as a second major crop for producing
    sugar in Pakistan. Multi-spectral cameras with four or five channels spectral
    bands provide more information about vegetation as compared to 3-channel RGB image
    sensors. The cameras which have NIR and R channels provide NDVI images, are best
    suitable for vegetation detection [7]. Differentiation between weed and sugarcane
    crop is done in [8], the authors applied morphological operation combined with
    Fuzzy real-time classifier. Leaf texture is extracted from morphology and features
    are computed using RGB channel for detection. The limitation of the technique
    is poor performance when the dataset is changed, lighting conditions are changed
    or if there is a different phenotype of crop. Tang et al. [9] identified weeds
    in soybean crop by k-means feature learning combined with a convolutional neural
    network using RGB camera sensor. Maize and weed classification by Zheng et al.
    [10] use support vector machine classifier on RGB texture cues. Support vector
    machines performs better in many cases as compared to other machine learning techniques,
    yet they used handcrafted feature detectors as compared to deep learning-based
    techniques where features are automatically learned. Fawakherji et al. [11] applied
    semantic segmentation for detection of weeds in sunflower crop. In first step
    they applied UNet with VGG-16 backbone to separate vegetation and background.
    In second step they extracted vegetation blobs and afterwards in third step they
    reapplied deep learning using VGG-16 neural network to classify crop and weed
    blobs. Application of deep learning two times and application of semantic segmentation
    in first step has made this approach computationally expensive and time-consuming.
    To detect weeds in sugar beet crop Milioto et al. [12] used a convolutional neural
    network (CNN) based semantic segmentation. Two main drawbacks of the algorithm
    are the usage of RGB images as other multispectral data would have given better
    accuracies and the use of semantic segmentation which is a computational complex
    neural network arrangement. Sa et al. [13] applied semantic segmentation to cluster
    sugar beet and weeds using VGG16 layers in encoder and decoder of neural network.
    Their two datasets contain multispectral images. Usage of semantic segmentation
    makes neural network computationally complex. The dataset used have a ground sampling
    distance around 1 cm. A faster detection is possible if a patch-based technique
    is used as appose to pixel-wise classification. Usually, semantic segmentation
    approach is applied for crop weed detection such as, [11]–[13]. Image Classification
    approach is simple and less computationally expensive approach as compared to
    semantic segmentation where each pixel is classified. Semantic segmentation could
    be applied in the field of crop and weed classification with various semantic
    neural network approaches like SegNet, FCN, U-Net, Deep Lab and Global Convolution
    Network. All these approaches use an encoder-decoder structure. The decoder is
    used to preserve localization information of objects in images, it helps in mapping
    back predictions to individual pixels. Decoder structure has its own computational
    complexity and error associated with it according to the up-sampling approach
    applied. SegNet [14] has an encoder structure like VGG16, it has 13 convolutional
    layers and its decoder structure has up sampling layers for each counterpart in
    the encoder. SegNet is widely used in crop weed semantic segmentation. Both SegNet
    and FCN [15] output is rough due to the loss of information because of heavy down
    sampling. U-Net tries to address the information loss problem in FCN. Information
    is sent to every up-sampling layer in the decoder from the corresponding down
    sampling layer in the encoder making structure more complex than simple FCN and
    SegNet architectures. Deeplab [16] deployed dilated convolutions by increasing
    the filter size and its last pooling layers had stride one which kept down sampling
    rate to only 8 times. Then a series of atrous convolutions is applied in this
    technique. This technique still has loss of information due to down sampling and
    a series of atrous convolutions make this approach heavy. Global Convolution Network
    [17] keeps a record of classification and localization at the same time and uses
    large kernels which make dense connections and hence more complex structure. In
    this paper, we proposed VGG-Beet a convolutional neural network similar to the
    generic design of VGG16 [18] model with 11 weight (10 convolutional layers + 1
    fully connected layer) layers that can classify sugar beet and weeds more accurately
    and faster. The images are broken into smaller square patches of size 21×21 -pixel
    and 41×41 -pixel to feed the CNN network for training and classification. Flow
    diagram of our system is shown in Fig. 1. We have applied our proposed approach
    on our new captured multispectral sugar beet dataset. For a better comparison
    of our classification algorithm we used publicly available sugar beet crop aerial
    images datasets [19]. FIGURE 1. System flow diagram. Show All Our approach removes
    the need for decoder structure to preserve localization information. Our approach
    divides the bigger image into small uniform patches and keeps a record of the
    location of patches. These small patch images are fed to a smaller classification
    model to be classified. Our approach removes the need for up sampling which involves
    heavy computations and errors. We observed that for weed detection in sugar beet
    using aerial images, the image classification approach performs better than semantic
    segmentation. The images are broken into smaller square patches of size 21×21
    -pixel and 41×41 -pixel. These patch images are classified into two classes i.e.
    sugar beet and weed. The rest of the paper is divided into five sections. Section
    2 is about material and methods. Section 3 and 4 are about system architecture
    and implementation. Section 5 and 6 are results & discussion and conclusion. SECTION
    II. Materials and Methods This study uses three different datasets, the details
    are as follows: A sugar beet local dataset has been acquired using Agrocam sensor
    by flying phantom 4 drone. This crop is located at geographical coordinates of
    E72.857886 and N31.832084. It was infested with fumaria indica weed which is found
    in many fields of Punjab, Pakistan. The sowing date of this field was 14 October
    2020 and dataset is collected on 27 December 2020 at the crop age of 75 days.
    The Agrocam Geo is a camera sensor which is designed to monitor crop health, it
    provides NGB images with GPS coordinates. NGB images have three channels i.e.,
    NIR, G and B channels. NDVI image is computed with B and NIR channels. Agrocam
    setting is set to capture 12-megapixel images of resolution 4000×3000 every 0.5
    second. Agrocam is attached to phantom 4 facing downward while its GPS module
    is fixed at the top of the drone. The drone height is kept 4 meters above ground
    and drone moving speed is kept at 2 km/h (0.5 m/s). After dataset collection these
    images are manually labelled with Matlab image labelling app. The Agrocam dataset
    has a train and test images percentage of 77.7% and 22.2% respectively. This newly
    captured sugar-beet multispectral dataset, captured in Lalian, Chiniot, Punjab,
    Pakistan has been made public [20]. The second dataset which is used in this research
    is publicly available [19]. It was captured using Parrot Sequoia camera using
    Mavic pro drone. The images were captured at the altitude of 10 meters with ground
    sampling distance of around 1cm of the different sugar beet crop fields. Parrot
    Sequoia camera has 4 channels (R, G, NIR and RE (red-edge channel)), NDVI and
    CIR composite images are derived from these channels. Field number 005 (210 images
    of resolution 360×480 ) and a small portion of field number 007 (31 images of
    resolution 360×480 ) are used for training and a big portion of field number 007
    (61 images of resolution 360×480 ) is used in testing. Field number 006 is not
    used in training or testing because this field is very much different than the
    other two parrot sequoia fields when ground sample distance (GSD) is compared.
    The parrot sequoia dataset has a train and test images percentage of 79.8% and
    20.2% respectively. For better comparison a third publicly available dataset is
    used. It was captured with Inspire 2 drone using Red Edge camera sensor. This
    dataset was provided by Sa et al. [19]. Red Edge is a five channels multispectral
    camera. The images were captured at the altitude of 10 meters and ground sampling
    distance (GSD) of around 1cm of a sugar beet crop in Rhein Bach, Germany, on 18
    September 2017. The five channels of the camera are R, G, B, RE and NIR, NDVI
    is computed with R and NIR channels, RGB composite image data is achieved by combining
    R, G and B channels and CIR (color infrared) is obtained by stacking R, G, and
    NIR channels. In total there are eight types of images of the same fields i.e.
    R, G, B, RE, NIR, NDVI, CIR and RGB using Red Edge sensor. The ground truth of
    this dataset is also provided. This dataset contains five fields of sugar beet
    crop, (Field 000-004), Field number 000, field number 001, field number 002 and
    field number 004 are used in training. Field number 000, 001, 002 and 004 have
    107, 90, 145 and 61 images of resolution 360×480 respectively. Field number 003
    have 94 images of resolution 360×480 , is reserved to test the approach on unseen
    data. The overall percentage of images used for training and testing are 81.1%
    and 18.9% respectively for Red Edge Dataset. In Parrot Sequoia and Red Edge datasets,
    treating each channel as an image, results in a total of 1.76 billion pixels.
    According to Sa et al. [13], most probably this is the largest sugar beet multispectral
    dataset which is publicly available. Total area covered in these two datasets
    is 0.8934 hectares and 0.762 hectares respectively for Red Edge and Sequoia dataset
    [19]. The three datasets have been captured at different lighting conditions,
    geographically distant locations, and soil conditions. Agrocam dataset is taken
    in Punjab, Pakistan, Parrot Sequoia dataset is taken in Eschikon, Switzerland
    and Red Edge dataset is taken in Rheinbach, Germany. Agrocam dataset is taken
    around 4:00 PM Sequoia data is taken around 12:00 PM and Red Edge data is taken
    early in the morning at around 9:30 AM hence different lighting conditions are
    present in these datasets. There was almost six months difference in Parrot Sequoia
    and Red Edge datasets capture implies that these datasets were taken in different
    seasons. Also, growth stages are different in all datasets, In Agrocam dataset,
    size of crop and weed plants are 28–62 cm and 18–113 cm respectively, the sizes
    of crops and weeds exhibited 8–10 cm and 5–10 cm respectively in the Sequoia dataset
    while in Red Edge dataset size of crops and weeds are 15–20 cm and 5–10 cm, respectively.
    There are five fields in the Red Edge dataset and three fields in the Sequoia
    dataset which are located at different places. These different fields have different
    GSD (ground sample distance) means that these different datasets are taken with
    various heights. Agrocam plant sizes are bigger than other two datasets as this
    dataset is taken at lower altitude of 4 meters. Another reason of bigger plant
    sizes of this dataset is growth stage, this dataset is captured lately as compared
    to other two datasets at the stage when field was heavily infested by weeds. In
    dataset overlap between crop and weed plants is visible making this dataset classification
    more challenging than other two datasets which are taken at early crop stages
    and there is less overlap between crop and weeds. Agrocam, Parrot Sequoia and
    Red Edge sensors operate on different wavelengths for same channels and therefore
    we trained separate models for each sensor. These sensors are different from each
    other in terms of focal length, field of view, number of spectral bands and wavelengths
    of same channels and therefore separate trained models will be required for each
    sensor. Fig. 2 (a)-(b) shows two image segments which are taken from two different
    fields of Red Edge dataset, different lighting conditions are visible on RGB data.
    (Note that these datasets are normalized using sunshine sensor, which is part
    of Red Edge and Sequoia sensors, but still different lighting conditions are visible
    to naked eye after normalization). Fig. 2 (c)-(d) show two NDVI data image segments
    from Parrot sequoia two different fields, the lighting conditions are more visible
    in NDVI data. So, lighting conditions are already addressed in this research.
    FIGURE 2. Different lighting conditions: (a) An image segment from Red Edge field
    number 000; (b) An image segment from Red Edge field number 001; (c) An image
    segment from Sequoia field number 005; (d) An image segment from Sequoia field
    number 007; ((a)-(b) show different lighting conditions in RGB Red Edge dataset,
    (c)-(d) show different lighting conditions in NDVI Sequoia dataset.) Show All
    SECTION III. System Architecture Proposed weed classification method classifies
    small patches instead of pixel-based classification. The input image is first
    segmented into smaller square patches. These patches are then fed to the system
    for further processing. The proposed methodology performs training and testing
    separately for Agrocam, Parrot Sequoia and Red Edge sensors. Training is done
    using available masks or manually labelled images. The first step is to compute
    NDVI images and detection of vegetation. NDVI for each pixel is calculated using
    equation (1). NDVI=(NIR−RED)/(NIR+RED) (1) View Source Red channel is not present
    in Agrocam so NDVI value for each pixel of Agrocam image is computed by using
    Blue and NIR channel (RED is replaced with BLUE in equation-1 for Agrocam sensor).
    The computed NDVI images are binarized to generate mask images with threshold
    value of 0.5 for Red Edge and Sequoia sensors, while for Agrocam sensor the threshold
    is kept at 0.3. The original image (single or multiple channel) is then divided
    into a grid of small patches and only those patches are considered for further
    processing in which vegetation is detected. A patch is considered to have vegetation
    if its corresponding vegetation mask have at least five pixels labelled as vegetation.
    The threshold values are computed heuristically. Experiments are done with two
    different patch sizes i.e., 21px ×21 px – pixels and 41px ×41 px – pixels. The
    flow of testing and training is same to this stage (see Fig. 1). For training,
    crop and weed patches are separated manually, patches containing both classes
    are dropped and are not used for training our proposed VGG-Beet CNN model (see
    Fig. 3 and Fig. 4). Cross validation data is used to validate the results and
    to stop model training on ideal weights. FIGURE 3. Applied deep learning model
    flow diagram. Show All FIGURE 4. 2-inputs model flow diagram. Show All For testing
    our system compute vegetation patches as explained earlier, which are then fed
    to the trained model of respective sensor for classification. The detailed system
    flow diagram is shown in Fig. 1. The procedure of computing vegetation patches
    for different channels used is the same (see Table 3, 6, and 5). Table 1 and 2
    show original dataset images and the number of patch images cropped with our method
    and their utilization in training and testing. TABLE 1 Dataset Details (Training)
    TABLE 2 Dataset Details (Testing) TABLE 3 Testing Results of Agrocam Dataset With
    Different Combination of Channels TABLE 4 Testing Results of Parrot Sequoia Dataset
    With Different Combination of Channels TABLE 5 Testing Results of Red Edge Dataset
    With Different Combination of Channels TABLE 6 Accuracy on Single Class Red Edge
    Dataset Patches— When Input Data is (a) RGB; (b) CIR; (c) RGB + NIR; (d) RGB +
    CIR. (S Represents Sugar Beet, W Represents Weeds, and N is Total Number of Single
    Class Patches) A. Neural Network Architecture Smaller image patch requires a smaller
    deep learning model, we used the generic VGG network to design our VGG-BEET network
    with 11 weight layers (containing 10 convolutional and 1 fully connected layer
    (total 18 layers)). The deep learning model applied is shown in Fig. 3. The network
    is retrained for different inputs (individual channels or composite images). For
    two inputs, two similar networks as shown in Fig. 3 are concatenated in such a
    way that the new network contains separate first 16 layers for both inputs. The
    last two layers (fully connected layer and classification layer) are shared (see
    Fig. 4). Same strategy is applied to create models to process more than 2-inputs
    in this paper i.e., every new input pass through first 16 layers separately and
    then passed through last two common layers. SECTION IV. Implementation Models
    were trained separately for Agrocam, Sequoia and Red Edge datasets. We used Keras
    with Tensorflow-GPU libraries for implementation of our framework in python. The
    hardware used is a laptop with core i5 eighth generation CPU having 16 GB RAM
    and NVIDIA GTX 1050 graphics processing unit. A learning rate of 0.00010 is applied
    with Adam optimizer using a categorical cross-entropy loss function, data augmentation
    of horizontal and vertical flips are applied with a validation split of 0.20.
    The deep learning model always converges for all combinations of channels. Convergence
    problems occur when further 2–3 convolutional layers are removed from reported
    network. The training is monitored to stop it on ideal weight values so that to
    prevent the model from overfitting. For that purpose, validation loss is monitored
    at every epoch, if at any epoch validation loss is decreased then model is saved
    at that epoch as best model. Heuristically a patience parameter of 20 is applied
    to see if validation loss will further decrease, if validation loss does not decrease
    anymore within 20 epochs then further training is stopped as the model is now
    overfitting. Epoch with minimum validation loss is saved as the best epoch and
    corresponding deep learning model state is saved as the best-trained model. We
    have used accuracy, Matthews correlation coefficient (MCC), AUROC (area under
    the receiver operating curve (This curve has true positive rate on y-axis and
    false positive rate on x-axis)) and AUC (area under the precision recall curve
    (This curve has precision on y-axis and recall on x-axis)) as evaluation metrics.
    Equation (2) and (5) represent accuracy and Matthew’s correlation coefficient.
    ACC= A= B= MCC= TPRate= FPRate= Recall= Precision= (TP+TN)/(P+N) TP×TN−FP×FN (TP+FP)(TP+FN)(TN+FP)(TN+FN)
    − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − − √ A/B
    TP/(TP+FN) FP/(FP+TN) TP/(TP+FN) TP/(TP+FP) (2) (3) (4) (5) (6) (7) (8) (9) View
    Source SECTION V. Results & Discussion After training, prediction is performed
    on unseen testing data. After prediction actual labels are read to compute performance.
    Results of our proposed approach on our acquired sugar beet dataset are summarized
    in Table 3. Our acquired Agrocam dataset is very challenging as there is highly
    cluttered vegetation. Weeds and sugar beet are overlapping in the dataset making
    classification very challenging. Table 3 shows quantitative results on 1,03,030
    labelled 21×21 pixel size patch images from our unseen test data on single channels
    and NGB composite images. We have achieved 86.3% accuracy on our newly captured
    Agrocam dataset of sugar beet crop on NGB composite images data. More accuracy
    is observed on composite 3-channel images as compared to single channels. Testing
    results of Parrot Sequoia dataset with different combination of channels are shown
    in Table 4. More accuracy is observed on CIR 3-channel composite images as compared
    to single channel NDVI images. When the number of input channels are further increased
    to five then maximum accuracy of 90.7% is achieved. This shows a direct relation
    of number of channels and accuracy up to eight channels, the accuracy decrease
    if we increase the channels more than eight (see Table 4). Minimum validation
    loss, overall testing accuracy and AUROC are summarized in Table 5 for all separate
    channels, composite images and combination of channels of Red Edge dataset. The
    trend of better accuracy on 3 channel composite images as compared to single channels
    is again observed by looking at minimum validation loss and AUROC in Table 5.
    Multiple inputs are tested to see any improvement of classification accuracy and
    it was observed that certain combinations of channels give higher accuracy, in
    our case combination of RGB + CIR data gave the best accuracy of 0.92 for Red
    Edge dataset. Maximum Overall Testing Accuracy (ACC) of Agrocam and Red Edge datasets
    are 86.3 and 92.4 respectively which could lead to the conclusion that flying
    low was not very effective however, 86.3 % accuracy of Agrocam is obtained with
    3-channel data, while 92.4 % accuracy of Red Edge dataset is obtained with 6-channels
    data, as input to the system. If we look at accuracy of 3-channel CIR input of
    Red Edge data, which is 88.4 % in Table 5, we see that this accuracy is comparable
    to Agrocam 3-channel accuracy. Therefore, accuracy of the system improves when
    a greater number of channels are used. Agrocam, Parrot Sequoia and Reg Edge sensors
    cost approximately $ 380, $ 3,500, and $ 5,500 respectively, so better accuracies
    come with greater system price. Total there were 17,660, 21px ×21 px- pixel non-overlapping
    vegetation patches in the testing field number 003 in Red Edge dataset. Out of
    total 17,660 patches, 12138 patch images were sugar beet and 4447 patch images
    were containing weeds. There are 1075 patches which contain both classes. In some
    cases, only one pixel of any class was present with other majority class. Out
    of 1075 patches, in 610 patches sugar beet is majority class and in 465 patches
    weed is majority class. For overall accuracy, majority class is defined as true
    class. Majority class is calculated by counting number of labeled pixels in patch
    images. We have also computed separate accuracy for 16585 (93.9% patches) single
    class patches, where both classes don’t merge, and for 1075 (6.1% patches) problematic
    patches which contain both classes to see if algorithm decide majority class or
    otherwise on 4 different best input cases where validation loss is minimum. From
    Table 5 best input cases where validation loss is minimum are numbers 7, 8, 12
    and 15. We have evaluated these four best cases further in detail to find out
    the best input case. Table 6 shows confusion matrices and performance metrics
    results of 16585 (93.9% patches) single class patches. Table 7 shows confusion
    matrices and performance metrics results of 1075 (6.1% patches) mix class patches.
    Table 6 is important as it decides the fate of the majority of patches (93.9%
    patches). The best input case will be the one in which weeds are efficiently classified.
    The case where fewer weeds are classified as crop. Another attribute of the best
    case will be minimum classification of crop as weed. According to these two criteria’s
    input case (d) is best in Table 6 where RGB and CIR patch images are used as input.
    TABLE 7 Limited Accuracy on Mix Class Red Edge Dataset Patches— When Input Data
    is (a) RGB; (b) CIR; (c) RGB + NIR; (d) RGB + CIR. (S Represents Patches Classified
    as Sugar Beet, W Represents Patches Classified as Weeds, mS Represent Number of
    Patches With More Sugar Beet Area While mW Represent Number of Patches With More
    Weed Area and N is Total Number of Mix Class Patches) Combination of RGB + CIR
    provides better results when it comes to the classification of mixed class patches
    as compared to other three cases which are evaluated. Table 7 provides the results
    on mixed class patches (6.1% patches). The weeds close to the crop pose a limitation,
    in our patch-based technique. we get problematic patches where both sugar beet
    and weed are present. Such mix class patches which are 6.1% in the case of test
    field number 003 in Red Edge data, the accuracy is limited as shown in Table 7.
    However, the combined accuracy is still better than pixel-wise (semantic segmentation)
    approach. We have compared our technique and results with the benchmark paper
    which in our case is Sa et al. [13]. They applied semantic segmentation with SegNet
    architecture in which encoding part was VGG16 layers followed by decoding layers
    for each counterpart. Their technique classifies into background, sugar beet and
    weeds. Our technique is based on classification of vegetation patches using only
    10 convolutional layers deep learning model described above. Our technique is
    focused on detecting vegetation (sugar beet + weeds), cropping patch images and
    then patch classification. A comparison is drawn in Fig. 5 using different channels
    as input on test field number 003 between Sa et al. [13] average AUC of sugar
    beet and weed using pixel-wise semantic segmentation approach and our AUC of sugar
    beet and weed using patch classification-based approach. Results show better performance
    on our patch-based approach. In all input cases our technique has performed well
    as compared to the set benchmark. In the case when NDVI images are used both techniques
    performed considerably good, the performance difference is lower comparably, this
    is because NDVI images give best distinguishing parameters, which means that NDVI
    images are most suitable data type to be processed when targeted classification
    task is vegetation. The performance difference is highest with NIR channel images,
    where our technique performed very well as compared to the set benchmark. FIGURE
    5. Comparison between our and benchmark paper AUCs: Result comparison when Red
    Edge dataset is used (Test data is same as [13] i.e. Field number 003). Show All
    The Qualitative results are shown in Fig. 6 and 7. FIGURE 6. Qualitative predicted
    results of some cropped patch images: (a) Cropped patch images of our Agrocam
    dataset; (b) Predicted patches output of Agrocam dataset; (c) Cropped patch images
    of Sequoia dataset; (d) Predicted patches output of Sequoia dataset; (e) Cropped
    patch images of Red Edge dataset; (f) Predicted patches output of Red Edge dataset,
    (Blue color boxes are patches which are input to the system, Red color patches
    are predicted as weeds while Green color patches are predicted sugar beet output).
    Show All FIGURE 7. Qualitative classification results of some cropped patches
    after reading labels: (a) Cropped patches of our Agrocam dataset; (b) Classified
    patches output of Agrocam dataset; (c) Cropped patches of Sequoia dataset; (d)
    Classified patches output of Sequoia dataset; (e) Cropped patchs of Red Edge dataset;
    (f) Classified patches output of Red Edge dataset, (Blue color boxes are labelled
    patches which are input to the system, Red color patches are correctly classified
    weeds, Green color patches are correctly classified sugar beet, Yellow color patches
    are actually weeds which are wrongly classified as sugar beet and Cyan color patches
    are actually sugar beet which are wrongly classified as weeds). Show All The best
    performance of Sa et al. [13] technique achieved 0.82 AUC using (B + CIR + G +
    NDVI + NIR + R + RE) 9 channels, when the same channels are passed, and our technique
    is applied, 0.90 AUC is achieved (shown in Fig. 5). One other main advantage of
    our technique is it gives better performance than Sa et al. [13] using fewer input
    channels. The results show better classification accuracy using our approach as
    compared to Sa et. al. [13] semantic segmentation application on the same data.
    Fig. 6 shows input patches to the system and respective predicted output while
    Fig. 7 shows labelled patches input to the system and respective classification
    results. Fig. 7 show classification results after matching predictions with labels.
    Note that Fig. 6 and 7 are same except one thing, Fig. 6 gives prediction results
    and Fig. 7 provide classification results after reading patch labels. Now the
    problem is our dataset is complex and at some locations weed and crop is confusing
    and therefore cannot be labelled, this is why only labeled patches are cropped
    in Fig. 7(a) to check if they are classified correctly. Figure 6(a) is the same
    one as Fig. 7(a), it is seen that all vegetation is detected in Fig. 6(a). Our
    captured Agrocam sensor data is heavily infested where crop and weeds are overlapping
    while Red Edge and Sequoia datasets are taken at very early stage of crop and
    are less infested by weed as compared to our captured dataset. There are different
    trained models for each sensor. The system has been tested on all three datasets.
    More visual results are uploaded to [21]. Weed profiles output on complete field
    of Red Edge and Parrot Sequoia tested datasets are shown in Appendix A. Table
    8 provides comparison between state-of-the-art semantic segmentation techniques
    and our patch-based approach with different patch sizes. UNet and Deeplab v3 are
    state of the art semantic segmentation algorithms and these deep learning algorithms
    are tested and compared with our approach. We observed Higher crop-weed accuracy
    and lower testing time for our patch-based approach. Confusion matrices using
    UNet and Deeplab techniques are shown first for all three datasets in the Table
    8. We observed a common problem in all these confusion matrices and that is many
    crop and weed pixels are wrongly classified into background and the most probable
    reason of this problem is class imbalance. As background cover most of the area
    in the dataset the predictions are inclined towards background class. This problem
    has resulted in higher background prediction accuracy and lower crop-weed classification
    accuracy. Our patch-based approach has eliminated this problem by taking out background
    patches first. In Table 8, we can see that when our patch-based approach is applied,
    none of the crop or weed patches are wrongly classified into background, this
    is due to removal of background patches prior to classification of crop and weed.
    TABLE 8 Comparison of Confusion Matrices Between State-of-the-Art Techniques and
    Our Patch Based Approach (for Semantic Segmentation Approaches Values in the Confusion
    Matrices Denotes Pixels and for Patch Based Approach Values in the Confusion Matrices
    Denotes Vegetation Patches) Semantic segmentation addresses 3-class problem, and
    our patch-based approach detects background and removes it, this way vegetation
    is separated out and vegetation (sugar beet or weed) patches are cropped. As a
    result, a 3-class pixel classification problem is converted to a 2-class crop-weed
    patch classification problem. By converting 3-class problem to 2-class problem
    we eliminate the risk of vegetation (crop or weed) classification into background
    and vice versa. We see in the Table 8 much vegetation (crop or weed) pixels are
    classified into background and vice-versa for semantic segmentation algorithms.
    In our patch-based approach case, background patches are removed first and as
    a result none of the crop or weeds patches are classified into background and
    vice versa, this practice in turns improves crop and weed classification accuracy.
    Table 9 shows Comparison of sugar beet, weed and mean sugar beet weed accuracy
    of Agrocam, Sequoia and Red Edge datasets using Unet, Deeplab and Our Patch based
    technique. We observed that sugar beet and weed classification accuracy is improved
    using our patch-based approach. Table 9 also gives us comparison of using small
    ( 21×21 ) and bigger ( 41×41 ) patch sizes and we have seen better accuracy in
    the case of bigger patch sizes. Although using bigger patch size seems more accurate,
    we recommend usage of small patch size for Sequoia and Red Edge datasets as we
    know in these datasets vegetation sizes are less than 20 pixel across so small
    ( 21×21 ) patch size suits more. TABLE 9 Comparison of Sugar Beet, Weed and Mean
    Sugarbeet-Weed Accuracy of Agrocam, Sequoia and Red Edge Datasets Using Unet,
    Deeplab and Our Patch Based Technique With Different Patch Sizes. (%CWA Shows
    Percentage Class Wise Accuracy, PBA Stands for Patch Based Approach) Another advantage
    of our patches-based approach is dealing with small size patch images while for
    semantic segmentation bigger image sizes are required according to the backbone
    encoder size of neural network, this leads to less memory requirement and lower
    computations in the case of our approach. Table 10 highlights lower testing time
    using our patch-based approach due to less computational complexity as compared
    to semantic segmentation by using same processing hardware and same datasets.
    We have also observed that bigger patch size is less time taking. TABLE 10 Comparison
    of Testing Time of Agrocam, Sequoia and Red Edge Complete Datasets Using Unet,
    Deeplab and Proposed Patch Based Technique With Different Patch Sizes. (Testing
    Time in Seconds on Complete Datasets Using Intel Core i5 8th Generation Processor)
    An additional advantage of our approach is it could be used to spray individual
    patch areas and spray on crop or soil can be avoided. Our patch-based approach
    can be used for directed spray on weed patches by saving spray on soil and crop
    plants while in the case of semantic segmentation directed spray is impossible
    as predicted weed pixel locations are random and single pixels are hard to target.
    Table 11 lists results of sensitivity analysis for several configuration of patches
    generations in the test images of Parrot Sequoia dataset. TABLE 11 Sensitivity
    Analysis for Several Configuration of Patches Generations in the Test Images of
    Parrot Sequoia Dataset (N is Total Number of Vegetation Patches, CLS Stands for
    Class, PRE Stands for Predicted, CONF Stands for Configuration, S Stands for Sugarbeet
    and W stands for Weed) We have used three different configuration of patch generation
    (Note that with different configuration, slightly different number of vegetation
    patches are achieved). In configuration 1 the whole image is cropped to small
    patches without overlap and without any bias. The second configuration of patch
    cropping is done by adding a bias of 10 pixels in rows and columns of cropped
    test patch images and third configuration of patch cropping is generated by adding
    a bias of 10 pixels in rows of test patch images. We have produced these 3 configurations
    for both 21×21 and 41×41 patch sizes of parrot sequoia dataset. The Sensitivity
    results show satisfactory performance. The GSD of our Agrocam dataset is 0.2 while
    the GSD of Red Edge and Parrot Sequoia datasets is around 1cm/pixel, this is due
    to different dataset capturing height and image sensors resolution. The networks
    are trained and tested on the minimum GSD range of 0.83cm/pixel to a maximum GSD
    range of 1.07cm/pixel for Red Edge and Parrot Sequoia datasets. For Agrocam dataset
    the network is trained and tested on GSD around 0.2 cm/pixel. If any new test
    dataset will be tested with out-of-range GSD, then classification results could
    become worse. In that case, new training data with similar GSD will be required
    to retrain the network. SECTION VI. Conclusion A new framework for weed identification
    is developed; a patch-based classification approach as appose to semantic segmentation
    that is more realistic for real-time intelligent aerial spraying systems. For
    classification, we developed a new VGG-Beet convolutional neural network (CNN),
    which is based on generic CNN (VGG) model with 11 convolutional layers. For experiments,
    we acquired a sugar beet dataset with 3-channel multispectral sensor with a ground
    sampling distance of 0.2 cm/pixel and a height of 4 meters. For better comparison,
    we used two publicly available sugar beet crop aerial imagery datasets, captured
    using a 5-channel multispectral sensor and 4-Channel multispectral sensor with
    a ground sampling distance of 1cm and a height of 10 meters. Three different multispectral
    sensors datasets are used in the experiments, we observed that the same channels
    in these sensors have different wavelengths and require separate trained model
    for each sensor. Agrocam, Parrot Sequoia and Red Edge datasets have various natural
    conditions and we observed that our patch-based method is robust to lighting conditions.
    We designed experiments to see performance on individual and multiple channels
    for each sensor used. The trend of better accuracy on 3 channel composite images
    as compared to single channels is observed. In general, using 3-channel images
    are better than single channels for all three sensors. For Red Edge sensor we
    observed that using 6-channels (RGB + CIR) performed better than individual 3-channel
    RGB or CIR input data. We have compared state-of-the-art UNet and Deeplab v3 semantic
    segmentation techniques and our patch-based approach with different patch sizes
    and we observed higher crop-weed accuracy and lower testing time for our patch-based
    approach. Our approach converts 3-class pixel classification problem into a 2-class
    crop-weed patch classification problem which in turns improves crop and weed classification
    accuracy. Our patches-based approach deals with small size patch images while
    for semantic segmentation bigger image sizes are required according to the backbone
    network encoder size, this leads to less memory requirement and lower computations
    in the case of our approach. We observed better accuracy and less testing time
    in the case of bigger patch sizes. Although using bigger patch size seems more
    accurate and less time taking, we recommend usage of small patch size for small
    sized vegetation and bigger patch size for bigger sized vegetation. Our future
    work is focused on developing a technique for mixed class patch images classification
    or reducing mix class patches, increasing the accuracy of weed detection and reducing
    crop classification as weeds. ACKNOWLEDGMENT The authors would like to thank the
    Autonomous Systems Laboratory (ASL) Datasets for provision of publicly available
    database. Appendix A See Figs. 8–11. FIGURE 8. Complete ground truth of sugar
    beet and weeds of testing field number 003 of Red Edge dataset. Red color represents
    weeds and green color represents sugar beet. Show All FIGURE 9. Complete weed
    profile results shown over RGB image data (while using RGB + CIR as input). Showing
    full testing field number 003 of Red Edge dataset. Patches with red color are
    correctly classified weeds while magenta color patches are also weeds which are
    wrongly classified as sugar beet. Partly clustered weed regions in a field are
    obvious. These regions could be efficiently sprayed using drones. Show All FIGURE
    10. Complete ground truth of sugar beet and weeds of testing field number 007
    of Parrot Sequoia dataset. Red color represents weeds and green color represents
    sugar beet. Show All FIGURE 11. Complete weed profile results shown over CIR image
    data (while using CIR as input). Showing testing field number 007 of Parrot Sequoia
    dataset. Patches with red color are correctly classified weeds while magenta color
    patches are also weeds which are wrongly classified as sugar beet. Show All Authors
    Figures References Citations Keywords Metrics More Like This Combination of a
    wireless sensor network and drone using infrared thermometers for smart agriculture
    2018 15th IEEE Annual Consumer Communications & Networking Conference (CCNC) Published:
    2018 Sensors Enabling Precision Spraying in Agriculture: A Case Study 2023 16th
    International Conference on Sensing Technology (ICST) Published: 2023 Show More
    IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS
    VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Imran Moazzam et al., 2021)
  journal: IEEE Access
  key_findings: '1. The proposed patch-based classification approach outperformed
    traditional semantic segmentation techniques for weed detection in sugar beet
    crops.

    2. The framework achieved high classification accuracy using a custom-developed
    VGG-Beet CNN.

    3. The approach was robust to different lighting conditions and input channel
    combinations.'
  limitations: The study is limited to the detection of weeds in sugar beet crops
    and may not be directly applicable to other crop types or weed species. Additionally,
    the study utilized specific multispectral sensors, and the performance of the
    proposed approach may vary with different sensors or under different environmental
    conditions.
  main_objective: The primary objective of the study was to develop and evaluate a
    novel patch-based classification approach for weed detection in sugar beet crops
    using high-resolution cameras and computer vision algorithms.
  relevance_evaluation: 'This paper is highly relevant to the point under discussion
    as it proposes a novel approach to integrating high-resolution cameras and computer
    vision for visual monitoring of crop growth, disease detection, and irrigation
    system performance. The paper provides a detailed explanation of the system architecture,
    implementation, and results, demonstrating the efficacy of the proposed approach
    for weed detection and classification in sugar beet crops. The paper''s relevance
    is further enhanced by its thorough analysis of different input channels and its
    comparison with state-of-the-art semantic segmentation techniques.


    Relevance score: 0.9'
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Multispectral cameras, computer vision algorithms, convolutional
    neural networks (CNNs), VGG16 model
  title: A Patch-Image Based Classification Approach for Detection of Weeds in Sugar
    Beet Crop
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Sahasranamam, V., Ramesh, T., & Rajeswari, R. (2023). Monitoring
    and Identifying Paddy Leaf Diseases Using Unmanned Aerial Vehicles (UAVs) with
    Machine Learning—A Survey. 2023 IEEE 2nd International Conference on Industrial
    Electronics: Developments & Applications (ICIDeA). https://doi.org/10.1109/ICIDeA59866.2023.10295173'
  authors:
  - Sahasranamam V.
  - Ramesh T.
  - Rajeswari R.
  citation_count: '0'
  data_sources: Literature review, data collection, experimental results
  description: Paddy leaf diseases substantially threaten global rice production,
    resulting in significant crop yield losses and economic implications. Therefore,
    timely detection and accurate identification of these diseases are imperative
    for effective management strategies. Recent advancements in agricultural technologies
    have led to Unmanned Aerial Vehicles (UAVs) emerging as a promising tool for crop
    monitoring due to their capability to provide higher-resolution and real-time
    data. This research paper aims to afford a comprehensive outline of the use of
    UAVs in monitoring and identifying paddy leaf diseases, shedding light on their
    potential applications, challenges, and prospects. This paper showcases the pivotal
    role of UAVs in transforming disease monitoring practices in paddy fields. Equipped
    with sophisticated cameras and sensors, UAVs can capture aerial images and multispectral
    data of vast agricultural areas efficiently. These data can then be processed
    and analyzed using state-of-the-art image processing and machine-learning algorithms
    to identify disease symptoms, classify disease types, and assess the severity
    of infections. The paper delves into the various UAV-based disease identification
    techniques, elaborating on image processing methods employed for feature extraction,
    such as image segmentation, texture analysis, and color space transformations.
    Finally, the research paper addresses the prospects of UAVs in paddy leaf disease
    monitoring. The paper also suggests directions for further research on real-time
    disease mapping and automated spraying systems based on UAV data, aiming to optimize
    disease management efforts. Applications based on UAVs should dynamically implement
    pest and nutrient management approaches.
  doi: 10.1109/ICIDeA59866.2023.10295173
  explanation: This paper provides a timely and comprehensive overview of using unmanned
    aerial vehicles (UAVs) for monitoring and identifying paddy leaf diseases, particularly
    blast, sheath blight, and bacterial leaf blight. It discusses the advantages and
    challenges of UAVs in disease monitoring and highlights the potential for future
    developments in this field. The research is significant because it offers a critical
    examination of the current state of UAV-based disease monitoring in rice crops.
  extract_1: '"This research paper aims to afford a comprehensive outline of the use
    of UAVs in monitoring and identifying paddy leaf diseases, shedding light on their
    potential applications, challenges, and prospects."'
  extract_2: '"In recent years, researchers and agricultural practitioners have recognized
    the value of UAVs in revolutionizing paddy leaf disease monitoring [30]."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 IEEE 2nd International C...
    Monitoring and Identifying Paddy Leaf Diseases Using Unmanned Aerial Vehicles
    (UAVs) with Machine Learning- A Survey Publisher: IEEE Cite This PDF V. Sahasranamam;
    T. Ramesh; R. Rajeswari All Authors 47 Full Text Views Abstract Document Sections
    I. Introduction II. Literature Review III. Methodology IV. Standard Process of
    Plant Disease Detection V. Protocols Strength and Limitations Show Full Outline
    Authors References Keywords Metrics Abstract: Paddy leaf diseases substantially
    threaten global rice production, resulting in significant crop yield losses and
    economic implications. Therefore, timely detection and accurate identification
    of these diseases are imperative for effective management strategies. Recent advancements
    in agricultural technologies have led to Unmanned Aerial Vehicles (UAVs) emerging
    as a promising tool for crop monitoring due to their capability to provide higher-resolution
    and real-time data. This research paper aims to afford a comprehensive outline
    of the use of UAVs in monitoring and identifying paddy leaf diseases, shedding
    light on their potential applications, challenges, and prospects. This paper showcases
    the pivotal role of UAVs in transforming disease monitoring practices in paddy
    fields. Equipped with sophisticated cameras and sensors, UAVs can capture aerial
    images and multispectral data of vast agricultural areas efficiently. These data
    can then be processed and analyzed using state-of-the-art image processing and
    machine-learning algorithms to identify disease symptoms, classify disease types,
    and assess the severity of infections. The paper delves into the various UAV-based
    disease identification techniques, elaborating on image processing methods employed
    for feature extraction, such as image segmentation, texture analysis, and color
    space transformations. Finally, the research paper addresses the prospects of
    UAVs in paddy leaf disease monitoring. The paper also suggests directions for
    further research on real-time disease mapping and automated spraying systems based
    on UAV data, aiming to optimize disease management efforts. Applications based
    on UAVs should dynamically implement pest and nutrient management approaches.
    Published in: 2023 IEEE 2nd International Conference on Industrial Electronics:
    Developments & Applications (ICIDeA) Date of Conference: 29-30 September 2023
    Date Added to IEEE Xplore: 31 October 2023 ISBN Information: DOI: 10.1109/ICIDeA59866.2023.10295173
    Publisher: IEEE Conference Location: Imphal, India SECTION I. Introduction Rice
    (Oryza sativa) stands as one of the most crucial cereal crops, serving as a staple
    food in Asia, Africa, and Latin America. With the global population projected
    to reach 9 billion by 2050, ensuring food security has become an urgent global
    challenge. However, the sustainable production of rice faces numerous obstacles,
    including the relentless threat posed by paddy leaf diseases. Paddy plants can
    reach heights of 90 to 150 cm [1]. The environment (frost, temperature, humidity,
    air, and drought) and pathogens (bacteria, nematodes, viruses, and fungi) are
    the main causes of the increase and spread of disease through these above-mentioned
    elements [2]. In recent years, rice cultivation and research have faced unprecedented
    difficulties. In some major rice-producing nations, production and yield have
    been declining for many years despite rising demand from people. For instance,
    in just 2020, over 100 million people-mostly from Asia, and Africa became poor
    [3]. UAVs are one of the latest technologies used today in precision agriculture.
    Precision agriculture is a method for increasing yields and managing crops through
    technology tools [4]. People with experience detecting and monitoring plant diseases
    formerly kept a watch on plant diseases visually in agricultural fields. This
    type of observation may result in inaccuracies, bias, and errors [5]. Paddy leaf
    diseases encompass a range of devastating infections caused by various pathogens,
    including fungi, bacteria, and viruses. The most prevalent diseases affecting
    rice crops worldwide include blast (Pyricularia oryzae), sheath blight (Rhizoctonia
    solani), and bacterial leaf blight (Xanthomonas oryzae pv. oryzae) as described
    in Table 1. These diseases often lead to severe economic losses, reduced crop
    yield, and lower grain quality. Table 1 A Summary of Rice Leaf Diseases. Timely
    detection and accurate identification of paddy leaf diseases are paramount for
    successful disease management. Early intervention can significantly mitigate the
    extent of damage, minimize yield losses, and enhance the overall sustainability
    of rice cultivation. However, traditional methods for disease monitoring have
    proven to be inefficient and limited in their capacity to meet these critical
    requirements [29] Historically, farmers have relied on manual scouting and visual
    inspection to identify diseased plants within paddy fields. While these methods
    have served as the foundation of disease monitoring, they suffer from inherent
    limitations. Manual scouting is labor-intensive, time-consuming, and impractical
    for covering extensive agricultural areas leads to delayed action and crop loss.
    To address these limitations and revolutionize disease monitoring practices, cutting-edge
    agricultural technologies have emerged, and among them, Unmanned Aerial Vehicles
    (UAVs) have shown immense potential. UAVs, commonly known as drones and various
    types of drones, have gained prominence in various industries due to their versatility
    and capacity to gather high-resolution data in real-time. In recent years, researchers
    and agricultural practitioners have recognized the value of UAVs in revolutionizing
    paddy leaf disease monitoring [30]. The application of UAVs in agriculture is
    not limited to disease monitoring alone; these unmanned aerial platforms have
    proven valuable in diverse agricultural tasks, including crop health assessment,
    irrigation management, precision farming, and yield prediction [31]. This research
    paper aims to provide an encompassing overview of the role of UAVs in monitoring
    and identifying paddy leaf diseases. It will explore the various techniques used
    for disease detection and classification, the advantages and challenges of using
    UAVs in disease monitoring, and the potential for future advancements in this
    field. By shedding light on the innovative application of UAV technology, this
    paper aims to contribute to the enhancement of disease management strategies and
    the promotion of sustainable rice production in the face of mounting global food
    security challenges [32]. SECTION II. Literature Review The literature review
    for this research article explores the body of information that already exists
    about the use of unmanned aerial vehicles (UAVs) for paddy leaf disease monitoring
    and identification. It involves an extensive exploration of relevant academic
    papers, articles, conference proceedings, and research reports that focus on UAV
    applications in agriculture, specifically in rice crop disease management. In
    the research article [6], the researchers aim to find the Paddy area where the
    damaged leaves exist. An image processing program is installed in Raspberry Pi
    4 and attached to the drone to find the healthiness of the paddy crop, Hue saturation
    value is used to find the color variance of the leaves. In the research article
    [7], the researchers compared and discussed the benefits of drones in detecting
    plant disease and monitoring plants. Compare the traditional methods of disease
    identification in plants with the new drone technology. RGB, thermal images, Spectral
    camera images, V-NIR Visible and Near-Infrared Images, and multispectral images
    for effective image input used for creating datasets for processing. CNN, F-CNN
    YOLO-3 algorithm for disease detection and classification are mentioned. In [8],
    the researchers used a Machine Learning algorithm and developed a neural network
    model using a dataset from the internet. The images were split into train and
    test data and trained. The trained model detects disease draws a bounding box
    and displays the corresponding disease. The test model was trained using inception_v3
    and produced an accuracy of 92%. The test model trained using Google’s AutoML
    has produced an accuracy of 94%. In the article [9], the authors gathered data
    sets using web mining, processed images, and trained models. The model is converted
    into a tflite model for mobile applications. For object detection on smartphones,
    YOLOv3 and YOLOv4 tiny models are utilized. The YOLOv4 tiny yielded better results
    with an accuracy of 98.13%. For disease identification from paddy leaves, a smartphone
    app called E-Crop Doctor was created. It also recommends the best pesticides for
    farmers. The application also contains a chatbot called “docCrop” that offers
    farmers around-the-clock assistance. In the study [10], Researchers developed
    an image processing system for identifying and classifying the paddy disease.
    The AdaBoost classifier is used to pinpoint the paddy plant’s diseased area. The
    percentage of detection accuracy is found to be 83.33%. The Support Vector Machine
    (SVM), Scale Invariant Feature Transform (SIFT) feature and classifiers k-Nearest
    Neighbor (k-NN) are used to classify various categories of diseases like brown
    spot, leaf blast, and bacterial blight. The main aim of the work is to detect
    the disease at the earliest stage and to take timely action to minimize crop loss.
    Accuracy is 93.33 % using k-NN and 91% using SVM. In [11], Leaf Area Index (LAI)
    estimation is done using digital Leaf images of paddy taken by UAVs. At different
    phases of the paddy crop, images with RGB color texture are taken. Linear model,
    a multi-variable regression model is used to estimate color index textures. This
    work provides an effective approach for estimating rice LAI from digital images
    captured by UAVs. In [12], the model detects Hespa, rice pests, and stem borers
    using UAVs. Data from UAV and web sources are used. Images are preprocessed and
    used for training the model and also used for analysis with another model. Images
    are analyzed with a modified new Yolo-Convolutional Neural Network (YO-CNN)-based
    better on the results detecting the disease. The research [13], discussed in detail
    how to integrate and install a UAV real-time system for monitoring paddy crops.
    Using neural networks and multivariable regressions, the system was capable of
    calculating nitrogen content and biomass changes throughout the growth cycle of
    the crop. In [14], multi-spectral imagery and Geographical Information systems
    are processed in AGI software for analyzing the Normalized Difference Vegetative
    Index to monitor the growth of the paddy crop. The NDVI map was created using
    raster analysis performed on multispectral photos using the ArcGIS software. With
    a higher NDVI value indicating a better crop and a lower NDVI value suggesting
    an unhealthy crop, the NDVI map helped identify agricultural damaged areas. In
    [15], algorithms with an ensemble classification technique were presented to assess
    bacterial disease infections in rice using images using drones. Images taken by
    UAVs were processed using Convolution Neural Networks (CNN). The paddy condition
    and infection inside the image were determined. The proposed algorithms have an
    accuracy of 89.84 percent for classifying rice illnesses. In [16], the author
    developed a smartphone app Padi2U for rice management. Multi-spectral images are
    used to get field data on rice. Padi2U offers advice for managing weeds, controlling
    pests and diseases, and checking rice conforming to the Department of Agriculture’s
    (DOA) guidelines. Vegetation index map output from the multi-spectral photos allows
    farmers to view paddy field data to determine the crop condition position online.
    The images gathered from launching height and directions are used as training
    images for Rice Bakanae Disease (RBD) recognition and also categorization algorithms
    in the paper [17]. The YOLOv3 method was set up to find RBD area bunches. The
    bounding box dimensions were used to identify and extract the RBD bunch regions.
    The ResNet 50-layer algorithm was used to classify RBD-infected culm numbers,
    and an accuracy of 80.36% was achieved. In the research paper [18], Multi-Spectral
    images from UAVs are used to find the Nitrogen content and biomass in the rice
    field. GrabCut segmentation is used for the Vegetative Index (VI). Machine learning
    models were used in the growth cycle of the plant’s vegetative, reproductive,
    and ripening and are compared. The article [19], describes the need for agronomic
    traits that are needed for yield and also for Nitrogen requirements in rice. Models
    were created in the growth stage. Vegetation indices are obtained from spectral
    cameras from UAVs. The findings demonstrated that high accuracy can be attained
    for all attributes except the Harvest Index (HI) at the booting stage as well
    as for nitrogen uptake and biomass at the tillering stage. In [20], the drone
    multirotor with RGB imaging photos used as the research’s primary input data were
    used to depict rice fields. The image was aligned and mosaiced by Structure from
    Motion (SfM) techniques by the AgisoftPhotoScan program. The empirical regression
    formula converts digital image numbers into reflectance values. The plant health
    readings range from 0 to 0.33 indicating stress, 0.33 to 0.66 moderate health,
    and 0.66 to 1 indicating good and healthy. A summary of recent research on disease
    detection in rice is presented in Table 2. Table 2 A Summary of Recent Research
    on Rice Leaf Disease Detection [6–20]. SECTION III. Methodology The methodology
    employed in this research paper encompasses several key steps to comprehensively
    explore monitoring and identifying paddy leaf diseases by Unmanned Aerial Vehicles
    (UAVs). Firstly, a thorough literature review is conducted to gain a deep kind
    of the current advanced methodologies related to UAV-based disease monitoring
    in rice crops. This review involves gathering and analyzing relevant research
    papers, articles, conference proceedings, and other scholarly resources from reputable
    journals and databases. Through this process, the paper establishes a strong foundation
    of knowledge and identifies any existing gaps or areas requiring further investigation.
    Data collection forms the next crucial step in the methodology. Various sources
    are utilized to gather relevant data for the research. Academic databases, research
    institution reports, publicly available datasets, and case studies involving UAV
    applications in paddy fields are all examined. The collected data includes information
    on disease incidence, symptoms, and severity, as well as UAV-derived aerial imagery
    and multispectral data representative of real-world scenarios [33]. The identification
    and study of disease detection techniques form a central aspect of the research
    methodology. Different techniques utilized in UAV-based disease monitoring are
    carefully examined. This includes an exploration of image processing algorithms
    for feature extraction, such as image segmentation, texture analysis, and color
    space transformations. Furthermore, the paper investigates machine learning and
    deep learning models, such as Support Vector Machines (SVMs), Random Forests,
    Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs), to
    determine their efficacy in classifying diseased and healthy rice plants. CNN
    is the best model for identification of leaf diseases [34]. To evaluate the performance
    of the identified disease detection techniques, the research designs and sets
    up appropriate experiments. Simulated or real UAV-derived data, alongside ground
    truth data on disease presence and severity, are used to confirm the correctness
    and efficiency of the particular algorithms. The experimental design takes into
    account different disease types, varying levels of disease severity, environmental
    conditions, and the impact of various UAV flight parameters on data quality [35].
    Data analysis is a critical phase where the collected data and experimental results
    are subjected to a thorough examination. Statistical methods are employed to assess
    the accuracy, sensitivity, specificity, and overall performance of the disease
    detection techniques. The analysis also involves comparing the efficiency of different
    algorithms and identifying any limitations or challenges associated with each
    method. Furthermore, the research addresses ethical considerations related with
    the use of UAVs in smart farming. This includes concerns related to privacy, data
    security, and environmental impact. Responsible UAV usage practices are proposed
    to ensure the ethical implementation of this technology [36]. Through comprehensive
    discussion, the research paper presents the findings, comparing the performance
    of different disease detection techniques, and analyzing their strengths and weaknesses.
    It explores potential future prospects, including the integration of UAVs with
    other remote sensing technologies like hyperspectral imaging and LIDAR, and their
    implications for real-time disease mapping and automated spraying systems based
    on UAV data [37]. In conclusion, the research methodology employed in this paper
    facilitates a thorough examination of the role of UAVs in monitoring and identifying
    paddy leaf diseases. By following this methodology, the paper contributes valuable
    insights to the field of agricultural technology and disease management, fostering
    advancements that can enhance global rice production and address food security
    challenges. Proper citation and referencing of all sources ensure the research’s
    credibility and acknowledge the contributions of previous studies in the field.
    SECTION IV. Standard Process of Plant Disease Detection A. Acquisition of Images
    Acquiring leaf disease images from various sources like the web, free open datasets
    of plant diseases, UAV real-time images, collected from different seasons, and
    the environment of the plant are collected and stored. The main aim is to create
    a data set for further processing [38]. B. Processing of Images Here unwanted
    image background is eliminated. The size of the image is reduced. The color, sharpness,
    and brightness of the image are enhanced. There are noises in images such as gaussian
    noise, salt and pepper noise will diminish image quality. Gaussian filter, mean
    filter are used in image processing to reduce or remove noises from images to
    enhance the quality and clarity of images [39] [40]. C. Segmentation of Images
    Image segmentation means dividing/ segmenting a digital image into multiple image
    partitions. Here the diseased leaf of the paddy plant is divided into segments
    to detect and classify the paddy disease. It investigates the paddy image to uncover
    relevant information for feature extraction [41]. D. Finding Features It is the
    process of extracting the interesting part in plant disease detection and classification.
    Here the shape, color, and texture of the disease are used to detect the dis-ease
    and classify it. The disease differs and symptoms of the paddy crop also differ.
    The paddy crop leaf disease systems are capable of quickly identifying illnesses
    based on the form, color, and texture of the diseased plant and classifying them
    [42]. E. Classification To classify and predict the correct disease in paddy plants
    various machine learning algorithms are used. Deep learning with more convolution
    layers is used to classify and predict the disease. Convolutional Neural Networks
    (CNN) are widely used for image detection and classification. Leaf disease characteristics
    are extracted from an image called pooling. Transfer learning and pre-training
    on ImageNet were employed with CNN models [40]. SECTION V. Protocols Strength
    and Limitations For leaf image disease detection, both supervised and unsupervised
    learning approaches can be utilized, depending on the availability of labelled
    data and the complexity of the task. Also, deep learning algorithms are highly
    effective due to their ability to automatically learn complex patterns from images.
    One of the most commonly used and suitable deep-learning designs for this task
    is CNNs [43]. A. Supervised Learning Supervised Learning: Supervised learning
    algorithms require labelled training data, where each leaf image is associated
    with a corresponding label indicating the presence or absence of a disease. If
    you have a substantial amount of accurately labelled data, supervised learning
    algorithms such as CNNs, SVMs, Random Forests, and Gradient Boosting can be suitable
    choices. CNNs are particularly effective for image-based tasks like disease detection,
    as they can automatically learn relevant features from the images and identify
    patterns indicative of diseases [44]. B. Unsupervised Learning Unsupervised learning
    algorithms don’t require labelled data but focus on finding patterns or structures
    within the data itself. They can be useful when labelled data is scarce or hard
    to obtain. Techniques like clustering and dimensionality reduction can aid in
    discovering groupings of similar leaf images or reducing the data’s complexity
    [45]. C. Hybrid Approaches In some cases, a hybrid approach could be beneficial.
    We could use unsupervised learning to pre-process or segment the data into meaningful
    clusters or groups, and then apply supervised learning within each cluster to
    fine-tune disease detection algorithms [46]. D. Deep Learning Algorithms CNNs
    are well-suited for image-related tasks like disease detection in paddy plants.
    They can automatically extract hierarchical features from images, capturing both
    low-level details and high-level patterns. It is made up of convolutional layers
    that combine the input image with learnable filters to extract features. Pooling
    layers are used to down sample the extracted features and reduce the spatial dimensions.
    These features are then passed through fully connected layers for classification.
    Transfer learning can also be applied by using pre-trained CNN models like ResNet,
    Inception or VGG which were developed using extensive datasets like ImageNet.
    By fine-tuning these models on paddy disease dataset, you can achieve good results
    even with limited labelled data. SECTION VI. Discussion While UAVs have shown
    great promise in disease detection, some studies have also highlighted challenges
    and limitations. Environmental factors, such as cloud cover and lighting conditions,
    can affect the quality of UAV imagery and subsequently impact disease identification
    accuracy. Additionally, regulatory restrictions and safety protocols related to
    UAV flights have been noted as potential barriers to widespread adoption. Overall,
    the literature review establishes that UAVs have emerged as a transformative technology
    in agriculture, particularly in the context of paddy leaf disease monitoring.
    The integration of advanced image processing techniques and machine learning algorithms
    has enabled accurate disease identification, while real-time data acquisition
    has facilitated timely intervention and effective disease management. Further
    research is needed to optimize UAV disease detection and classification process.
    Moreover to enable small farmers to utilize UAV resources, UAV manufacturers,
    agricultural scientists, and researchers should provide an inexpensive drone.
    The discussion section of this research paper focuses on the analysis and interpretation
    of the findings obtained from the literature review, data analysis, and experimental
    results. It provides a critical examination of the effectiveness and limitations
    of using UAVs for monitoring and identifying paddy leaf diseases. The discussion
    aims to draw meaningful insights, propose potential solutions to challenges, and
    highlight the significance of UAV-based disease monitoring in rice crops. Firstly,
    the research paper discusses the performance of different disease detection techniques
    identified in the literature review. It compares the accuracy and efficiency of
    image processing algorithms, such as image segmentation, texture analysis, and
    color space transformations, in extracting relevant features indicative of disease
    presence. Convolutional neural networks, Random Forest, Support vector machines
    (SVMs), and recurrent neural networks (RNNs) are among the machine learning and
    deep learning models that have been evaluated for their ability to distinguish
    between damaged and healthy rice plants. The discussion sheds light on the strengths
    and weaknesses of each technique and offers insights into their practical implications
    for disease management. Furthermore, the research paper examines the practical
    challenges faced in UAV-based disease monitoring. Environmental factors, such
    as weather conditions and lighting, can affect the quality of UAV-captured imagery
    and influence disease detection accuracy. The discussion explores potential mitigation
    strategies, such as optimizing flight schedules and using image enhancement techniques,
    to improve data quality under challenging environmental conditions. Additionally,
    the paper addresses concerns related to data processing complexities, highlighting
    the need for efficient algorithms and computing resources to handle large volumes
    of UAV-derived data. The discussion also delves into ethical considerations associated
    with the use of UAVs in smart agriculture. Privacy concerns related to aerial
    surveillance of farmlands and data security are addressed, emphasizing the importance
    of implementing responsible UAV usage practices. The research paper proposes guidelines
    and best practices to protect the privacy rights of farmers and landowners while
    maximizing the benefits of UAV technology in disease monitoring. SECTION VII.
    Conclusion The conclusion of this research paper provides a summary of the key
    findings and insights obtained from the study on monitoring and identifying paddy
    leaf diseases using Unmanned Aerial Vehicles (UAVs). It highlights the significance
    of UAV technology in revolutionizing disease monitoring practices in rice crops
    and its potential impact on global food security. The research paper concludes
    that UAVs offer a transformative approach to disease monitoring in paddy fields.
    By leveraging high-resolution aerial imagery and multispectral data, UAVs enable
    rapid and accurate detection of paddy leaf diseases, including blast, sheath blight,
    and bacterial leaf blight. The integration of image processing algorithms and
    machine learning models enhances disease identification accuracy and provides
    timely information for effective disease management strategies. The research paper
    also emphasizes the practical advantages of using UAVs in disease monitoring.
    UAV surveys cover large agricultural areas rapidly, overcoming the limitations
    of traditional manual scouting methods. This time and cost efficiency make UAVs
    attractive for farmers and agricultural practitioners. Ethical considerations
    related to UAV usage in agriculture are also highlighted in the conclusion. The
    paper proposes responsible UAV usage practices to protect the privacy rights of
    farmers and landowners while maximizing the benefits of UAV-based disease monitoring.
    In conclusion, the research paper demonstrates that UAVs Ultimately, the conclusion
    reinforces the importance of UAVs in contributing to global food security by facilitating
    more efficient and effective disease management practices in rice crops. Pest
    and nutrient management techniques should be implemented using drone applications.
    ACKNOWLEDGEMENT The author acknowledge the support and funding project “Drone
    based Paddy Crop Management System for Pest Control” by DST-NMICPS, TiHAN IIT
    Hyderabad. Authors References Keywords Metrics More Like This Design of Real-Time
    System Based on Machine Learning for Snoring and OSA Detection ICASSP 2022 - 2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
    Published: 2022 An integrated approach of Genetic Algorithm and Machine Learning
    for generation of Worst-Case Data for Real-Time Systems 2022 IEEE/ACM 26th International
    Symposium on Distributed Simulation and Real Time Applications (DS-RT) Published:
    2022 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Sahasranamam, Ramesh & Rajeswari, 2023)
  journal: 'Proceedings of 2023 IEEE 2nd International Conference on Industrial Electronics:
    Developments and Applications, ICIDeA 2023'
  key_findings: UAVs provide efficient and accurate disease monitoring, enabling early
    intervention and effective disease management. Advanced monitoring techniques,
    such as high-resolution cameras and computer vision algorithms, can enhance disease
    detection and classification accuracy.
  limitations: null
  main_objective: To present a comprehensive overview of the integration of UAVs for
    paddy leaf disease monitoring, discussing the techniques, advantages, and future
    potential in this field.
  relevance_evaluation: This paper is highly relevant to the point under consideration
    as it focuses specifically on the integration of high-resolution cameras and computer
    vision algorithms for visual monitoring of paddy crop growth, disease detection,
    and irrigation system performance. The paper discusses the use of UAVs, multispectral
    imaging, and deep learning techniques for disease detection and classification,
    which aligns directly with the intention to explore advanced monitoring techniques
    for automated irrigation systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: UAVs, computer vision algorithms, multispectral imaging, deep
    learning
  title: Monitoring and Identifying Paddy Leaf Diseases Using Unmanned Aerial Vehicles
    (UAVs) with Machine Learning- A Survey
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Hashmi, M. F. (2023). Chapter 1: Deep Learning Techniques for Smart
    Agriculture Applications. In M. F. Hashmi, Avinash G. Kesakr (Eds.), Machine Learning
    and Deep Learning for Smart Agriculture and Applications (pp. 1-23). IGI Global.
    https://doi.org/10.4018/978-1-6684-9975-7'
  authors:
  - Hashmi M.F.
  - Keskar A.G.
  citation_count: '1'
  data_sources: Literature review of academic articles, industry reports, and case
    studies.
  description: Machine Learning and Deep Learning for Smart Agriculture and Applications
    delves into the captivating realm of artificial intelligence and its pivotal role
    in transforming the landscape of modern agriculture. With a focus on precision
    agriculture, digital farming, and emerging concepts, this book illuminates the
    significance of sustainable food production and resource management in the face
    of evolving digital hardware and software technologies. Geospatial technology,
    robotics, the Internet of Things (IoT), and data analytics converge with machine
    learning and big data to unlock new possibilities in agricultural management.
    This book explores the synergy between these disciplines, offering cutting-edge
    insights into data-intensive processes within operational agricultural environments.
    From automated irrigation systems and agricultural drones for field analysis to
    crop monitoring and precision agriculture, the applications of machine learning
    are far-reaching. Animal identification and health monitoring also benefit from
    these advanced techniques. One of the book's key focuses is the critical role
    of health monitoring for plants and fruits in achieving sustainable agriculture.
    Plant diseases pose significant financial challenges in the farming industry worldwide.
    By leveraging sophisticated image processing and advanced computer vision techniques,
    automated detection and identification of plant diseases are revolutionized, enabling
    precise and rapid identification while minimizing human effort and labor costs.
    For researchers involved in image processing and computer vision for smart agriculture,
    this book offers invaluable insights. It covers the most important fields of image
    processing in the agricultural domain, encompassing computer vision applications,
    machine learning, and deep learning approaches. From the analysis of agricultural
    data using machine learning to the implementation of bio-inspired algorithms,
    the book explores the breadth and depth of agricultural modernization through
    the lens of AI technologies. With practical case studies on vegetable and fruit
    leaf disease detection, drone-based agriculture, and the impact of pesticides
    on plants, this book provides a comprehensive understanding of the applications
    of machine learning and deep learning in smart agriculture. It also examines various
    modeling techniques employed in this field and showcases how artificial intelligence
    can revolutionize plant disease detection. This book serves as a comprehensive
    guide for researchers, practitioners, and students seeking to harness the power
    of AI in transforming the agricultural landscape.
  doi: 10.4018/978-1-6684-9975-7
  explanation: The study, titled "Machine Learning and Deep Learning for Smart Agriculture
    and Applications," explores the applications of AI and its pivotal role in transforming
    modern agriculture, focusing on precision agriculture, digital farming, and emerging
    concepts. The study emphasizes the importance of sustainable food production and
    resource management in the face of evolving digital hardware and software technologies,
    with a focus on the integration of geospatial technology, robotics, the Internet
    of Things (IoT), and data analytics with machine learning and big data.
  extract_1: The study highlights the use of deep learning techniques for sustainable
    agriculture, particularly in the accurate diagnosis of plant and fruit diseases.
  extract_2: The study provides insights into the applications of AI and deep learning
    in smart agriculture, including crop detection and counting, crop management,
    and crop disease classification.
  full_citation: '>'
  full_text: '>

    "Login Register Language: English Welcome to the InfoSci Platform University of
    Nebraska - Lincoln Database Search Research Tools User Resources Reference Hub1
    Indices1 Machine Learning and Deep Learning for Smart Agriculture and Applications
    Mohamamd Farukh Hashmi, Avinash G. Kesakr Copyright: © 2023 |Pages: 257 ISBN13:
    9781668499757|ISBN10: 1668499754|EISBN13: 9781668499764 DOI: 10.4018/978-1-6684-9975-7
    Cite Book Favorite Full-Book Download Machine Learning and Deep Learning for Smart
    Agriculture and Applications delves into the captivating realm of artificial intelligence
    and its pivotal role in transforming the landscape of modern agriculture. With
    a focus on precision agriculture, digital farming, and emerging concepts, this
    book illuminates the significance of sustainable food production and resource
    management in the face of evolving digital hardware and software technologies.
    Geospatial technology, robotics, the Internet of Things (IoT), and data analytics
    converge with machine learning and big data to unlock new possibilities in agricultural
    management. This book explores the synergy between these disciplines, offering
    cutting-edge insights into data-intensive processes within operational agricultural
    environments. From automated irrigation systems and agricultural drones for field
    analysis to crop monitoring and precision agriculture, the applications of machine
    learning are far-reaching. Animal identification and health monitoring also benefit
    from these advanced techniques. With practical case studies on vegetable and fruit
    leaf disease detection, drone-based agriculture, and the impact of pesticides
    on plants, this book provides a comprehensive understanding of the applications
    of machine learning and deep learning in smart agriculture. It also examines various
    modeling techniques employed in this field and showcases how artificial intelligence
    can revolutionize plant disease detection. This book serves as a comprehensive
    guide for researchers, practitioners, and students seeking to harness the power
    of AI in transforming the agricultural landscape. Table of Contents Reset Front
    Materials PDF HTML Title Page PDF HTML Copyright Page PDF HTML Advances in Environmental
    Engineering and Green Technologies (AEEGT) Book Series PDF HTML Preface Chapters
    PDF HTML Chapter 1 Deep Learning Techniques for Smart Agriculture Applications  (pages
    1-23) Ankita Mishra, Sourik Banerjee, Brijendra Singh With an emphasis on the
    rapid and accurate diagnosis of plant and fruit diseases, researchers have been
    looking into sustainable agriculture utilizing cutting-edge deep learning techniques.
    The objective is to show how effective deep... PDF HTML Chapter 2 Review Work
    of Automation Agricultural Robot System Using Machine Learning and Deep Learning  (pages
    24-33) R. Felshiya Rajakumari, M. Siva Ramkumar An occupation based on automatic
    system using machine learning and deep learning techniques is developed in this
    chapter. The agricultural land and automatic systems are worked mutually to defeat
    the concern by integration with solar... PDF HTML Chapter 3 Applications of Deep
    Learning and Machine Learning in Smart Agriculture: A Survey  (pages 34-57) Amrit
    pal Kaur, Devershi Pallavi Bhatt, Linesh Raja Machine learning (ML) and deep learning
    can be used in the smartest way possible to improve productivity in agriculture.
    The Food and Agriculture Organization''s research shows that the crop''s production
    is rising. One of the... PDF HTML Chapter 4 Plant Disease Classification in Segmented
    Images Using Computer Vision  (pages 58-92) Rajashri Roy Choudhury, Piyal Roy,
    Shivnath Ghosh Agriculture productivity has a significant impact on the lives
    of people and economies because of the growing human population. In agriculture,
    plant diseases are a big problem since they result in severe crop losses and financial...
    PDF HTML Chapter 5 Enhancing Tomato Fruit Detection and Counting Through AI-Enabled
    Agricultural Innovations  (pages 93-105) S Gandhimathi Alias Usha This chapter
    aims to enhance tomato fruit detection and counting in agricultural practices
    through AI-enabled innovations. Traditional manual methods for fruit detection
    and counting are labor-intensive and time-consuming. By... PDF HTML Chapter 6
    Harnessing Environmental Intelligence to Enhance Crop Management by Leveraging
    Deep Learning Technique  (pages 106-123) S. Gandhimathi Alias Usha This chapter
    aims to enhance crop management practices by harnessing environmental intelligence
    through the power of deep learning techniques. Efficient and sustainable crop
    management is crucial for meeting the increasing demand for... PDF HTML Chapter
    7 Crop Prediction for Smart Agriculture Using Ensemble of Classifiers  (pages
    124-141) Khushal Kindra, Bhuvaneswari Amma N. G. Nowadays due to the advancement
    in technology, smart agriculture is in the evolving stage. Agricultural farmers
    worldwide commonly utilize the process of cultivating and harvesting crops to
    produce food and fiber. Therefore, crop... PDF HTML Chapter 8 An Integrated Approach
    for Selection and Design of Sustainable Farmers'' Protective-Hat in India  (pages
    142-172) Suchismita Satapathy Normally, the farmers in India are required to work
    in adverse climatic conditions while performing their agricultural activities.
    Though a number of preventive measures are available for the protection of farmers,
    the nominal and... PDF HTML Chapter 9 Smart Crop Protection System From Wild Animals
    Using Artificial Intelligence  (pages 173-191) Shailaja S. Mudengudi, Muktha S.
    Patil, Neetha S. Mudaraddi The first major threat to the farmers is drought. Crop
    vandalization by animals is the second major threat after drought. Crops are vulnerable
    to animals. Therefore, it is very important to monitor the nearby presence of
    animals. The... PDF HTML Chapter 10 GUI-Based End-to-End Deep Learning Model for
    Corn Leaf Disease Classification  (pages 192-213) G. Revathy, J. Jeyabharathi,
    Madonna Arieth, A. Ramalingam Food security is a major problem worldwide. Ensuring
    that the crops produced are both safe and wholesome is crucial not only for people
    as the ultimate consumers of the crops, but also for farmers. Plant diseases are
    responsible for... This content was retracted PDF HTML Chapter 11 A Transfer Learning
    Approach for Detecting Plant Leaf Diseases With Convolutional Neural Networks  (pages
    214-224) P. Valarmathi, N. G. Bhuvaneswari Amma, Vasu Bhasin Agriculture is an
    indispensable sector for the continuity of homo sapiens. In the Indian context
    where agriculture contributes 19.9 percent of GDP and engages almost 54.6 percent
    of the population, it requires great measures to be... Back Materials PDF HTML
    Compilation of References PDF HTML About the Contributors PDF Index View All Books
    Request Access You do not own this content. Please login to recommend this title
    to your institution''s librarian or purchase it from the IGI Global bookstore.
    Username or email:   Password:   Log In   Forgot individual login password? Create
    individual account Research Tools Database Search | Help | User Guide | Advisory
    Board User Resources Librarians | Researchers | Authors Librarian Tools COUNTER
    Reports | Persistent URLs | MARC Records | Institution Holdings | Institution
    Settings Librarian Resources Training | Title Lists | Licensing and Consortium
    Information | Promotions Policies Terms and Conditions     Copyright © 1988-2024,
    IGI Global - All Rights Reserved"'
  inline_citation: Hashmi, 2023
  journal: Machine Learning and Deep Learning for Smart Agriculture and Applications
  key_findings: AI and deep learning techniques hold significant potential for improving
    agricultural productivity through automated monitoring of crop growth, disease
    detection, and irrigation system performance. Integration of these technologies
    with end-to-end automated irrigation management systems can contribute to sustainable
    agriculture practices.
  limitations: The study does not delve into the specific integration of these technologies
    with irrigation systems or provide detailed information on the automation of the
    monitoring process. However, it highlights the use of computer vision for monitoring
    irrigation system performance.
  main_objective: To explore the role of AI and its pivotal role in transforming modern
    agriculture, focusing on precision agriculture, digital farming, and emerging
    concepts, and sustainable food production and resource management.
  relevance_evaluation: The study is highly relevant to the outline point as it discusses
    the integration of high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection,
    and irrigation system performance. The study's focus on the use of AI and computer
    vision for monitoring crop growth and irrigation system performance aligns directly
    with the outline point's intention to explore the integration of advanced technologies
    for real-time irrigation management.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Deep learning, computer vision, AI, IoT, geospatial technology,
    data analytics, robotics.
  title: Machine learning and deep learning for smart agriculture and applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Mishra, S., Volety, D. R., Bohra, N., Alfarhood, S., & Safran, M.
    (2023). A smart and sustainable framework for millet crop monitoring equipped
    with disease detection using enhanced predictive intelligence. Alexandria Engineering
    Journal, 83, 298-306.
  authors:
  - Mishra S.
  - Volety D.R.
  - Bohra N.
  - Alfarhood S.
  - Safran M.
  citation_count: '0'
  data_sources: Sensor data (temperature, humidity, soil moisture levels), image data
    of millet crops
  description: Today, there is a visible reduction in productivity of common crops
    like rice and wheat in agricultural sector. At the same time, millet crops have
    emerged as an alternative to counter global food security issues because of their
    adaptability and nutritional value that can combat malnutrition. Also, because
    of its low moisture requirement and tolerance to extreme climatic conditions,
    it is perceived as a stable cereal. But its cultivation is affected by diseases
    like rust and blast, thereby causing harm to the farming economy. Conventional
    operational methods of disease detection require regular manual intervention which
    are also costly. The correctness of expert suggestions is also under scrutiny.
    Thus, a sustainable, robust and low cost modern approach for millet crop monitoring
    and disease detection is required. This research aims to develop a smart and sustainable
    framework by integrating Internet of things (IoT) and deep learning (DL). In the
    presented framework, a sensory module based automated crop health data gathering
    system with an improved deep learning-assisted intelligent disease recognition
    model is developed for the millet crop. Sensors collect data from millet farmland
    transfer the crop data readings to the cloud server for storage and Raspberry
    Pi for further detection. Any abnormality in reading leads to an alert notification
    communicated to the farmer. A hybrid predictive model, Customized Convolutional
    neural network (Customized-CNN) model works with the Raspberry Pi to predict the
    presence of blast and rust disease symptoms in millet. The proposed sustainable
    model is implemented, and it generates productive outcomes. The recorded accuracy,
    precision, recall, and f-score values with the customized CNN model were 98.8%,
    98.2%, 97.4%, and 97.7%, respectively. The training and testing delays were only
    67 s and 88 s, respectively. A sample of yearly sensor readings was generated
    to show the accuracy and reliability of the model's data collection. Also, the
    model proved to be scalable, as it gave noteworthy performance with diverse crops.
    The evaluation shows the reliability of the model, and it can be used by farming
    societies to enhance millet cereal yield in a cost-effective way.
  doi: 10.1016/j.aej.2023.10.041
  explanation: This study focuses on developing a smart and sustainable framework
    for monitoring millet crop health and predicting diseases using a combination
    of IoT sensors and a customized Convolutional Neural Network (CNN) model. The
    system collects data on temperature, humidity, and soil moisture levels using
    sensors, and uses the data to identify and classify diseases, such as rust and
    blast, in millet crops. The proposed framework offers a comprehensive and reliable
    approach to disease management in millet cultivation, with high accuracy and reduced
    computational delay.
  extract_1: Today, there is a visible reduction in productivity of common crops like
    rice and wheat in agricultural sector.
  extract_2: Also, because of its low moisture requirement and tolerance to extreme
    climatic conditions, it is perceived as a stable cereal.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Keywords 1. Introduction 2. Related works 3. Materials and methods
    4. Results and discussion 5. Conclusion Declaration of Competing Interest Acknowledgement
    References Show full outline Figures (6) Tables (7) Table 1 Table 2 Table 3 Table
    4 Table 5 Table 6 Show all tables Alexandria Engineering Journal Volume 83, 15
    November 2023, Pages 298-306 Original Article A smart and sustainable framework
    for millet crop monitoring equipped with disease detection using enhanced predictive
    intelligence Author links open overlay panel Sushruta Mishra a, Dayal Rohan Volety
    a, Navdeep Bohra b, Sultan Alfarhood c, Mejdl Safran c Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.aej.2023.10.041 Get rights and content Under
    a Creative Commons license open access Abstract Today, there is a visible reduction
    in productivity of common crops like rice and wheat in agricultural sector. At
    the same time, millet crops have emerged as an alternative to counter global food
    security issues because of their adaptability and nutritional value that can combat
    malnutrition. Also, because of its low moisture requirement and tolerance to extreme
    climatic conditions, it is perceived as a stable cereal. But its cultivation is
    affected by diseases like rust and blast, thereby causing harm to the farming
    economy. Conventional operational methods of disease detection require regular
    manual intervention which are also costly. The correctness of expert suggestions
    is also under scrutiny. Thus, a sustainable, robust and low cost modern approach
    for millet crop monitoring and disease detection is required. This research aims
    to develop a smart and sustainable framework by integrating Internet of things
    (IoT) and deep learning (DL). In the presented framework, a sensory module based
    automated crop health data gathering system with an improved deep learning-assisted
    intelligent disease recognition model is developed for the millet crop. Sensors
    collect data from millet farmland transfer the crop data readings to the cloud
    server for storage and Raspberry Pi for further detection. Any abnormality in
    reading leads to an alert notification communicated to the farmer. A hybrid predictive
    model, Customized Convolutional neural network (Customized-CNN) model works with
    the Raspberry Pi to predict the presence of blast and rust disease symptoms in
    millet. The proposed sustainable model is implemented, and it generates productive
    outcomes. The recorded accuracy, precision, recall, and f-score values with the
    customized CNN model were 98.8%, 98.2%, 97.4%, and 97.7%, respectively. The training
    and testing delays were only 67 s and 88 s, respectively. A sample of yearly sensor
    readings was generated to show the accuracy and reliability of the model’s data
    collection. Also, the model proved to be scalable, as it gave noteworthy performance
    with diverse crops. The evaluation shows the reliability of the model, and it
    can be used by farming societies to enhance millet cereal yield in a cost-effective
    way. Previous article in issue Next article in issue Keywords Deep learningImage
    processingInternet of thingsPlant diseasesMilletSustainabilityPredictive learning
    1. Introduction Crop diseases are a serious threat to productivity and sustainable
    development in agriculture. Agriculture is the first act that has helped humanity
    thrive and prosper. Today, the food and agriculture industry is a major priority
    in the world due to the growing number of people and, therefore, the increasing
    demand for food to sustain their lives. Farming is the primary source of food
    and fuel, contributing to the country''s financial development. By 2030, there
    should be reasonable food-building systems and durable farming methods that increase
    overall production. It should also conserve the environment, protect the crops,
    and make them durable from different natural disasters such as changes in temperature,
    extreme climate, droughts, and other factors that further improve the soil quality.
    About 70 % of the total population is dependent on farming in some way or another.
    Pearl millet is the sixth most essential crop for any country after any other
    major crops. Pennisetum glaucum has the ability to resist any climate change,
    and the crop could be very vital to lessening the damaging results of temperature
    change. It also has the capacity to extend the income and food problems of areas
    with a lack of water. The automated identification of plant diseases may be a
    main topic in the field of farming. Furthermore, the primary and early detection
    of plant illnesses undoubtedly affects crop production and profit [1]. Tomorrow,
    global warming may increase because of humans, which affect crop production through
    pests [2] and insects [3]. The government has declared millet ‘Nutri Cereals’
    for various purposes such as production, trade, and consumption [4]. This crop
    is adaptable to climate variations due to its low consumption of water, steadiness
    at soaring temperatures, and lack of water-resistant capability. Hence, millet’s
    ability can be utilized for food insecurities. Crop infections are a significant
    problem in farming and affect both quality and quantity. The production of millet
    is mainly influenced by rust and blast disease [5]. Such diseases pose a considerable
    threat to agricultural sustainability and impact farmers'' economic standards
    [6], [7]. Hence, a system must be introduced that would eliminate the diseases
    quicker and easier. Today, agribusiness are looking forward to artificial intelligence
    and the internet of things to make agriculture more efficient and sustainable.
    The new research on various methodologies has been applied to different sectors,
    which has led to better results in the areas of computer vision and Machine Learning[ML].
    The most recent work that has been introduced is the application of DL in the
    field using computers in agriculture. Various remote sensing techniques through
    the Internet of Things and sensors help farmers monitor crop states at different
    phases. Sensors and modern predictive methods have proven their ability to automate
    data storage, data collection, and analysis of accumulated data samples to facilitate
    precise forecasting. With the immediate method to prevent environmental degradation,
    increase profit, and reduce waste, farmers are increasingly using more effective
    crop management solutions that are supported by controlling technologies and optimization
    derived from IoT. Through IoT and IoT sensing approaches, sustainable energy-related
    research can bring about a revolution in this area. The excessive use of IoT devices
    and electronic components such as cameras, drones, and various sensors helps significantly.
    Sustainable energy systems can address challenges in strength protection for the
    community, with a minimum of an alternate approach to surroundings and subcultures.
    By using IoT and sensors to collect data about their plants and then further filtering
    such information with various methodologies such as machine learning and deep
    learning Agribusinesses are able to keep an eye on the condition of their crops
    in the agriculture field. These technologies are much more than helping farmers
    make agriculture more sustainable; they are also making it more profitable and
    productive. DL and the Internet of Things framework are presented in the paper,
    Blast and Rust Prediction in Pearl Millet. The combination of IoT sensors with
    DL is used to construct a system for disease recognition in pearl millet. The
    challenge is to develop an accurate and efficient disease prediction system for
    rust and blast in millet and a monitoring system that overcomes the limitations
    of traditional methods. In this research, a sensory module-based automated crop
    health data gathering system with an improved deep learning-assisted intelligent
    disease recognition model is presented for the millet crop. The framework automatically
    collects the images and other parametric data present on the millet farmland and
    further sends the collected data to the cloud for storage and Raspberry Pi for
    further classification into rust and blast diseases. The novel Customized-CNN
    model functions with the Raspberry Pi to detect the presence of blast or rust
    diseases in millet. Any abnormality observed in readings from the sensors,an alert
    that is communicated to the farmer. The motivation behind this research lies in
    the need for a robust solution that addresses the challenges faced due to crop
    destruction by diseases. Traditional methods have fallen short of providing the
    accuracy and agility required to meet the demands of modern farming. The proposed
    work fills this critical gap by combining cutting-edge technology in the form
    of deep learning with an Internet of Things (IoT) sensor network to revolutionize
    millet disease prediction and management. Major highlights of the research study
    include the following: • The work presents the current scenario and the concerned
    literature review undertaken in the context of millet crop production as well
    as its disease monitoring. • Based on existing research gaps, a novel methodology
    for millet crop tracking and disease prediction using smart sensory computing
    based predictive intelligence is presented. The model uses a camera module to
    acquire images and cloud storage for effective data accessibility. • The IoT unit
    equipped with crop health tracking sensors allows real-time data collection and
    timely notifications to farmers about any variation in crop conditions. Further
    using a Customized-CNN model on preprocessed data, the occurrence of millet disease
    is accurately detected. • The implementation outcome using various performance
    metrics appears promising which makes it scalable and sustainable. It can assist
    the farming society to accurately track the growth of crops. 2. Related works
    Rational farming has been a popular research area for the past few decades. It
    can be achieved through an effective integration of IoT-based crop health monitoring
    and machine learning-enabled disease detection. Various works concerning millet
    crop tracking are discussed in the following two subsections. 2.1. Predictive
    approach for crop disease prediction In [8], a classification model using CNN
    was presented to distinguish between good-quality leaves and many other diseased
    leaves of various crops. Analysis in [9] found that an important activity for
    raising farming output is the early diagnosis and restriction of plant disorders,
    especially in the early phases of onset. The authors in [10] developed a deep
    neural network for millet disease prediction using transfer learning for prediction.
    Using a superior-quality camera, pictures of rice crop leaves are taken for real-time
    applications [11]. The research process made use of a dataset that included pictures
    of both healthy and sick leaves. Semi-supervised learning is utilized in the paper
    [12] to categorize images using Generative Adversarial Networks. In this kind
    of learning, the discriminator is converted into a multi-class classifier, and
    the generator is solely used for discriminator training. In the paper [13], it
    was found that current disease examination processes frequently result in unreliable
    categorization results, which have recently reduced rice yields. In [14], two
    prevalent illnesses were identified using banana photos from a PlantVillage dataset.
    The experiment was undertaken on diverse kinds of photo scans, with a cumulative
    number of around 4000 images that were shrunk to 60 × 60 pixels. The model''s
    accuracy ranged from 92 to 99 % after numerous training sessions using various
    ratios of the train-to-test split. It was built on the LeNet architecture. A comparable
    method shown in [15] started with 1053 photos of apple leaves and then used preprocessing
    to create 13,689 synthetic images. Five distinct CNN models, as well as a neural
    network and SVM, were tested in the experiment. The modified AlexNet recorded
    the best outcome, identifying various kinds of apple leaf abnormalities with an
    accuracy of 97.62 %. Red-green–blue (RGB) images of rice leaves were utilized
    by authors in [16] to create a novel image processing model that could identify
    and categorize illnesses in rice plants. A straightforward and efficient Naive
    Bayes (NB) classifier was then used for classification. In a publication by Sue
    Han Lee [17], the model is trained using the Caffe framework and the back-propagation
    process. The deconvolutional network was one of the methods used to comprehend
    the inner workings of the CNN model and visualize the chosen filters. By deconvolutional
    and unpooling down to the input image pixel, the deconvolutional network offers
    a function that enables us to see the feature map at each layer. In another paper,
    a system for identifying plants is suggested. Using a hybridized gray level co-occurrence
    matrix (GLCM), a discrete wavelet transform (DWT), and the scan investigate filter
    target (SIFT) model, Devi et al. [18] applied image processing to damaged areas
    of rice leaves. Numerous classifiers, including k-nearest neighbour algorithms,
    back propagation neural networks, multi-class support vector machines (SVMs),
    and naive Bayes, have also been used. The advanced neural network (NN) is suggested
    in this research [19] as a means of processing hyper spectral data. NN mechanisms,
    types, models, and classifiers using various algorithms are reviewed. The present
    state of imaging and non-imaging of the hyper spectral data is highlighted in
    this work. Table 1 summarizes the related works on machine learning-based crop
    disease diagnosis (Table 2). Table 1. Related works on predictive approach for
    crop disease detection. Authors Objectives Methodology Benefits Kamlesh Golhani
    et al., 2018 Neural networks method to detect various diseases in plant Advanced
    neural networks are used to detect plant diseases using hyperspectral data with
    a focus on plants. NNs are used to classify hyperspectral dataset in studying
    with a accuracy of 91 %. E. Pantazi et al., 2019 Automated leaf disease detection
    For feature extraction, use Local Binary Patterns (LBPs), and for classification,
    use One Class Classification. By taking images from the farmland in particular
    conditions crop health can be determined. Al-Amin,et. al, 2019 Potato disease
    identification by its leave Deep CNN model is used in the study. Late blight,
    early blight diseases can be predicted along with the healthy potato with accuracy
    of 97. 43 %. Vishal Pallagani et al., 2019 DL based methodology for correct recognizance
    of infected crops using smart Agriculture Deep convolutional neural network (DNN)
    With the highest degree of confidence, the model has been able to forecast 38
    distinct diseases. T. Islam et al., 2018 A modern technique detect diseases in
    rice plants The state of the art Naive Bayes algorithm has been used. This technique
    is quicker because it makes use of one characteristic ie. RGB. S. Ramesh et al.,
    2020 Detection of paddy leaf diseases. Java language used in deep neural networks.
    According to the established technique, the accuracy was good, coming up at 97.6
    % for the afflicted blast disease, 95.78 % for the bacterial blight, and 90.57
    % for the typical leaf image. F. T. Pinki, et al 2017 Content based paddy leaf
    disease recognition Disease detection in paddy crop and providing solution using
    SVM. It predicts the three paddy leaf diseases Leaf blast, Brown spot and Bacterial
    blight and solution to each problem is provided such as chemicals. Amara, J et
    al 2017 A DL based framework for diseases in banana tree detection This method
    uses LeNet architecture as a CNN to classify disease. Upon several experimentations
    the system was able to find good classification results with a accuracy of 98.
    61 % Lee, Sue Han, et al., 2017 Approach used by DL to extract information from
    an image learns it feature. It makes use of CNN to understand different aspects
    based on a De-Convolutional Network (DN). CNN provides better resolution images
    as compared to other custom algorithms. Marko Arsenovic et al., 2016 Deep Neural
    Networks Based algorithm is used to detect diseases in plants. Detection diseases
    in plants using leaf image classification by using neural networks The model gave
    a exceptional result of precision of 91 % and 97 % for different class,The average
    was about 97. 2 %. P. N. Gayathri, et al., 2019 Diseases detection in Rice plant
    leaves in the southern part of india. K nearest neighborhood neural network, back
    propagation neural network, Support vector Machine to categorize plants which
    are infected or not. It is seen that multi class support vector machines gave
    better accuracy of 97. 83 %. Umapathy Eaganathan et al., 2014 Spotting of Sugarcane
    Leaf Scorch Disease K-means Clustering and Segmentation The overall accuracy using
    K-NN Classifier is 95 % and texture analysis can be used for feature extraction
    Table 2. Summary of related works in crop monitoring using IoT. Authors Summary
    Methodology Benefits Torres-Sospedra et al., 2020 A support system based on vineyard
    is used for monitoring mildew disease Senviro a detailed platform which monitors
    crops in farmland. The proposed methodology helps detect and treat downy mildew
    disease. Yang C et al, 2020 Crop Disease Detection Smart and precison agriculture
    using iot remote sensing Disease detection and site-specific management using
    remote sensing and variable rate technology. Khattab, A et al., 2019 An Internet
    of Things and sensors monitoring system to detect plant disease in early stage
    Agricultural monitoring system using wireless sensor network The presented framework
    predicted the diseases in potato and tomato at a very high accuracy and precision
    Saha, A. K et al., 2018 Improves the crop production using IOT based devices such
    as drones. Internet of Things,Unmanned Aerial Vehicle(UAV); RGB-D sensor; With
    the proposed model the drones will help the farmers at every step of production
    which would increase the yield of crops. K. Lokesh Krishna et al., 2017 Application
    Smart-Agriculture implementation using IOT. Various IOT devices such as mobile
    robot is designed and implmented for performing various tasks in the farm. This
    method controls the various needs for plant growth. It is highly easier to use
    as compared to other complex methods. Thorat, S. A et al., 2017 Early detection
    of grapes diseases Wireless Sensor Network, Vineyard, Hidden Markov Model, Zig-Bee
    By using the proposed model the quantity and quality of production increases and
    accurate information about various chemicals to be sprayed be delivered to the
    farmer. Riskiawan,et al., 2019 Plant and crop can be protected from various threats
    by using the approach of smart agriculture IoT, big data, and deep learning Early
    recognition of disease can be done using smart agriculture which stops further
    spreading of disease in various part of plant. Sankararao, Adduru UG, et al 2021
    Water stress detection using UAV based hyperspectral imaging Canopy with Selected
    Wavebands using UAV Based Hyperspectral Imaging The result of the proposed approach
    shows it recognizes the water stress level in the millet crops. Shi Y et al.,
    2015 IOT application to monitor plant diseases, pests and insects IOT,Pest monitoring
    system, sensor nodes This system provides a way to access and use various agricultural
    information of the farm. 2.2. Smart IoT based crop monitoring and modeling The
    IoT era has revolutionized our world. Precision agriculture is considered an eco-friendly,
    sustainable, and profitable mode to improve agriculture production and quality.
    To monitor plants, methods for infection detection require an enormous amount
    of data and information. To deal with such data, first analyze each cluster with
    other techniques individually to check and show each result. [20]. In the paper,
    author Sai Kirthi Pilli [21] stated that the AGROBOT can test cotton using high-resolution
    images where higher-resolution images are used. It can help to make a decision
    about their farmland by themselves or by using some other means. The authors developed
    a wireless robot [22] using the Internet of Things to do a lot of work, which
    increases the sustainability of crops. This robot takes all the data from various
    devices that are connected to it and takes action accordingly. They also save
    the crops from various insects and pests, which can also be done by using chemicals.
    In [23], the authors developed an Internet of Things technology that uses a wireless
    IOT sensor to gather data from agricultural land. The information from the gadgets
    is utilized to treat illnesses. The authors of [24] created a system for yard
    mildew prediction. The findings are a crucial piece of evidence for phytosanitary
    protection. The Hidden Markov model was used by authors in [25] to monitor a closed-circuit
    camera prototype to detect grape disorders. Numerous sensors are placed on agricultural
    land to gather information about temperature and moisture. Then, Zig-Bee is used
    to transmit these data clusters to a server. Ying et al. [26] developed a technique
    for remote sensing and precision agriculture that is used to find and identify
    illnesses in a variety of crops. It provides experts in various fields, such as
    growers, consultants, and farm equipment, and various experts, such as dealers
    of chemicals, to provide the correct way to deal with such situations. Shi et
    al. [27] mentioned that Internet of Things methods can provide different techniques
    to control and manage pests and insects in farmland based on the IOT framework.
    Markovic et al. [28] presented an Internet of Things-based system for farmers
    that would help them secure their crops even from a distinct location and adjust
    the needs of crops as required. This system is very low-cost and user-friendly,
    which enables farmers to take the production process on a larger scale. Wang et
    al. [29] made a model for crop disease recognition. In this method, various algorithms
    are used to predict the parameters that are required by the crop. Murphy et al.
    [30] The ability of artificial intelligence algorithms can be beneficial in detecting
    disease early. The authors of [31] presented a way to integrate IoT devices using
    Raspberry Pi modules to improve production. Table 1 shows the related existing
    studies undertaken in the plant crop diseases detection. However it is observed
    that most of the existing systems were not able to provide real time automated
    capturing of images which affected the overall prediction efficiency. Many works
    focused on estimating only one type of plant disease while ignoring multi disease
    detection in plants. Also, the existing models are not so robust and reliable
    as they focus on monitoring very limited crops. Apart from these constraints,
    none of the work were responsive framework. Most of the deep learning models earlier
    deployed for disease detection have restricted accuracy with high computational
    delay to generate results. In addition, the internal structural complexity of
    these models are quite high which is not suitable for timely and quick prediction.
    Another gap is the generalization of machine learning models across various use
    cases in agriculture fields. One major constraint observed in the usage of IoT
    for predicting discrepancy in crops is the inadequate interpretation of appropriate
    design and arrangement of smart sensors in agricultural deployment. Thus the development
    of practical techniques for managing the enormous amounts of sensor data generated
    by IoT enabled agricultural equipment is a major issue that requires research.
    The practical use of IoT based plant disease prediction depends on the development
    of scalable and effective algorithms for data preprocessing, feature extraction,
    and disease prediction. Some existing studies also underestimated vital parameter
    like current soil moisture content and regular temperature readings needed to
    track crop health. Thus, developing a more non-interrupted, robust, cost effective
    and reliable plant disease assessment model disease prediction in resource constrained
    environment is the need of the hour for widespread adoption. 3. Materials and
    methods This section presents the proposed methodology in context to millet crop
    monitoring and its disease detection. Also, the dataset used for the study is
    highlighted here. 3.1. Dataset used in study Several hardware elements, such as
    sensors and cameras, are fixed on the farmland in order to collect data. Images
    of the crops that had rust or blast infections were taken in order to identify
    the diseases. For training the model, the researchers have used the blast and
    rust dataset publicly available on Kaggle [32]. A total of 3000 images of the
    millet crop that was found to have blast and rust were selected from the dataset.
    Table 3 summarizes the overall distribution of millet crop samples. Table 3. Distribution
    of Millet crop dataset. Diseases of Millet Total Images Images in training data
    set Images in testing data set Rust 1500 1050 450 Blast 1500 1050 450 A further
    70 %–30 % split of the sample was made into training and testing groups. In a
    70:30 split, 70 % of the dataset is allocated for training the model and 30 %
    for testing its performance. A smaller testing set can reduce the risk of over
    fitting, where the model performs well on training data but poorly on unseen data.
    A larger training set helps the model generalize better to unseen samples. Also,
    authors performed several experiments with the dataset division, for example,
    80:20 and 60:40. It was found that the efficiency with 70:30 ratio generated the
    most optimal outcome. Also, small test sets required fewer computational resources
    for evaluation, which is an advantage in a resource constrained environment like
    agriculture. Fig. 1 shows some samples of millet crop images accumulated from
    the field for analysis. Download : Download high-res image (186KB) Download :
    Download full-size image Fig. 1. Millet crop image samples collected from field.
    3.2. Proposed methodology In this section, the novel framework and the workflow
    model for millet crop disease assessment are presented. The system used for the
    experimental evaluation has the NVIDIA GeForce GTX 1650ti GPU. It runs Windows
    10 64-bit and has PyTorch 1. 9. 1′s deep learning framework, which uses CUDA 12.
    1 and cuDNN 8. 9. 0 to speed up the training of network models on the GPU. This
    study involves a two procedure based approach for millet crop disease assessment.
    Smart IoT sensors with cloud storage are used to track the overall health of millet
    crops by continuously monitoring their temperature, humidity, and moisture levels.
    In case of health parameters discrepancies, farmers are notified. The second procedure
    addresses the millet disease classification into blast and rust types by using
    an advanced deep learning model. The proposed workflow model is observed in Fig.
    2. Download : Download high-res image (255KB) Download : Download full-size image
    Fig. 2. Proposed Methodology for Millet disease monitoring and prediction. A camera
    system is installed to capture images of the crops at regular intervals, while
    temperature, humidity, and soil moisture sensors are embedded in agricultural
    land. The captured images are stored in a cloud storage service, Amazon S3 for
    easy access and organization. To improve the quality of images and eliminate noisy
    features, pre-processing techniques like resizing, normalization, noise reduction,
    and image enhancement are applied. A dataset is standardized by extracting the
    pre-processed images and corresponding disease labels, which are then divided
    into training and testing sets. The data is shuffled to ensure randomness in both
    sets. Further a hybrid deep learning technique, the Customized-CNN model is chosen
    for disease prediction. The architecture of the selected Customized-CNN model
    is designed, and the model is trained using the accumulated dataset. Hyperparameters
    are fine-tuned, and the model is validated using the testing dataset to assess
    its accuracy and generalization capabilities. In parallel, the IoT based sensory
    unit is integrated into the framework. Table 4 shows the IoT components used in
    the research. The GPS unit integrated with sensory modules is fruitful in tracking
    the position of plant abnormality. Also determining the location of land where
    pesticides and water are needed is significant. The temperature, humidity, and
    soil moisture sensors are connected to a Raspberry Pi. A program is designed to
    collect sensor data, which is stored locally on the Raspberry Pi. The collected
    sensor data is then transferred to a cloud storage service, Amazon S3 for secure
    storage. Table 4. IoT sensors configuration. Sensors Model Purpose Configuration
    Temperature and Humidity DHT11 Temperature and humidity are measured using the
    sensor. • Temperature Range: −20 °C to 50 °C Humidity Range: 20 % to 80 % RH (Relative
    Humidity) Soil Moisture LM358 The sensor measures the water content the in soil.
    Range of 0 % (dry) to 100 % (saturated soil). Raspberry PI Raspberry PI-3 Data
    collected from sensors are received by Raspberry PI for further processing. Raspberry
    Pi 3 has 40 GPIO pins that can be used to connect and interface with sensors,
    including DHT11 and LM358. A notification system is implemented to alert farmers
    about significant changes in soil moisture, temperature, or other relevant parameters.
    Change in climatic variables increases the vulnerability of disease occurrence
    in millets. Also, the oospores of the soil act as the main point of infection
    in the lower part of the crop. Thus, sensory units embedded to be used for regular
    soil tracking may recognize oospore''s presence thereby helping to detect the
    risk at an initial phase. The Customized-CNN enabled predictive module collaborates
    with the Raspberry Pi to function in a synchronized way to enable precise forecasting
    and notifying farmers regarding plant risks and other changes in agricultural
    land. A novel Customized-CNN model is used for the suggested strategy because
    CNN produces the highest accuracy even with a limited dataset. A deep neural convolution
    network has been advanced in the suggested way to trip over rust and blast. Using
    3 RGB channels and a 128 by 128 length entry, a bespoke Convolutional neural network
    is built, and the results are seen using a 3 by 3 length convolution filter. Following
    the application of the convolution filter, the image''s dimensions were reduced
    to 126 × 126 with a depth of 32 × 32 filters, and a max-pool layer with a length
    of 2 × 2 filters was added. Following the convolution layer max-pooling procedure,
    the input size was further decreased to 63x63. The same method is applied repeatedly
    until the snap pictures'' dimensions are 28x28. The operations of a typical fully
    connected artificial neural network structure are then performed by flattening
    the two-dimensional matrix into a column vector. On a single totally linked layer,
    the activation function is “relu”. Batch normalization is also carried out to
    hasten the model''s training and steady its learning process. So as to enhance
    the model on validation data and reduce over fitting during training, dropout
    function is also used. Table 5 highlights the proposed model parameters configuration.
    Table 5. Customized-CNN parameters. Layers Layers Operation Feature Map No. Feature
    Map size Kernel size Parameters C1 Convolution 32 128x128 3x3 (32 * 3 * 3 + 32)
    = 320 (Weights) + 32 (Biases) = 352 S1 Max-pooling 32 126x126 2x2 0 C2 Convolution
    16 63x63 3x3 (16 * 3 * 3 * 32 + 16) = 4640 (Weights) + 16 (Biases) = 4656 S2 Max-pooling
    16 61x61 2x2 0 C3 Convolution 8 30x30 3x3 (8 * 3 * 3 * 16 + 8) = 1160 (Weights)
    + 8 (Biases) = 1168 S3 Max-poolling 8 28x28 2x2 0 FC Flatten Layer 8 N/A N/A 0
    S3 Max-pooling 128 2 × 2 8 × 8 0 FC Fully connected layer 128 1X1 N/A (128 * 2
    + 2) = 258 (Weights) + 2 (Biases) = 260 parameters FC Output 3 1X1 N/A 0 The Customized-CNN
    model used in this work is fine-tuned with its vital parameters. The preset values
    of its parameters are highlighted in Table 6. Table 6. Parameters of model training.
    Parameters Value Learning-Rate 0.001 Pool-size(Max pooling) (3,3) Validation-Steps
    50 Number of Epochs 500 Num-Classes 2 Optimizer ADAM Dropout Rate 0.3 Batch Size
    32 Activation Function ReLU(hidden layer) Softmax(output layer) The important
    relevant equations applied in the proposed Customized-CNN model are highlighted
    here. Eq. (1) shows the Convolutional layer. (1) where C tells the output,I is
    the input image and K is the convolution kernel(filter). It takes an input image
    (I) and convolves it with a convolution kernel (K) to produce the output feature
    map (C) output. In the context of millet disease, this equation is at the core
    of how the model learns to detect disease-related patterns or features. Equation
    (2) denotes the max pooling layer used in the model. (2) where Y is the pooled
    output, X is the input, and s is the stride. Pooling layers are used in CNNs to
    reduce the spatial dimensions of feature maps.The input (X) is down-sampled to
    produce the pooled output (Y) using a specified pooling function with a given
    stride (s).It helps reduce complexity, over fitting. In millet disease detection,
    it can help capture essential disease-related features in a more compact form.
    (3) The activation function ‘relu’ is used in this model as shown in equation
    (3). If the function receives any negative input, it returns 0; however, if the
    function receives any positive value ×, it returns that value. As a result, the
    output has a range of 0 to infinite. The batch normalization of the model is shown
    in equation (4). (4) where y is the normalized output,x is the input, is the mean,
    is the variance, is the scale factor, is the shift factor, is constant. Batch
    normalization is used to normalize the activation of neural network layers during
    training. It reduces internal covariate shift, improving convergence, and enabling
    faster training. Eq. (5) represents the back propagation method used in the neural
    network. (5) The is the standard back propagation term, is the error gradient
    is the learning rate.The is the momentum part, where is the momentum rate. This
    represents the update rule for adjusting the network''s weights (w) during the
    training process. Back propagation is essential for optimizing the model''s parameters
    to minimize prediction errors. 4. Results and discussion In this section, we discuss
    and contrast the novel framework with alternative combinations of related CNN
    classifiers using different validation indicators. Here we evaluate the model
    using precision, recall, accuracy, and training time. It provides a complete evaluation
    of the model''s effectiveness. These metrics are derived from four confusion matrix
    variables. True Positives (TP) denotes the samples which are correctly classified.
    False Positives (FP) counts the number of samples incorrectly classified. True
    Negatives (TN) shows the sample count that are correctly classified showing the
    absence of disease. False Negatives (FN) denotes the number of data instances
    which are incorrectly classified showing the disease to be absent. Accuracy is
    the ratio of cases that were successfully predicted to all instances denotes the
    accuracy rate of prediction as shown in Eq. (6). (6) Precision tells the percent
    of correctly predicted good outcomes to all positive outcomes as shown in Eq.
    (7). It evaluates ability of the framework to detect positive occurrences. (7)
    Recall as shown in Eq. (8) represents the proportion of precisely recognized positive
    samples with all positive samples is the recall factor. It evaluates how well
    the model can identify each positive event. (8) F1 score suggests the harmonic
    average of precision and recall. It gives an accurate measurement that considers
    both recall and precision. The Eq. (9) shows the F1 score. (9) The novel Customized-CNN
    model used in the study for millet disease prediction is trained using data gathered
    from sensors placed around the farm. The model''s performance is validated by
    repeated experimentation to produce the best results. The proposed model was compared
    with other existing models like VGG16, VGG19, Resnet, and others. The validation
    outcome attained through the proposed model was very promising. Accuracy, precision,
    recall and f-score values noted using the Customized-CNN model were 98.8 %, 98.2
    %, 97.4 %, and 97.7 % respectively. Fig. 3 denotes the overall outcome analysis.
    Download : Download high-res image (469KB) Download : Download full-size image
    Fig. 3. Evaluation metrics analysis of proposed model with other computational
    models. Fig. 4 denotes the computational delay evaluation of our approach with
    existing deep learning methods. The computational delay is evaluated using both
    training delay and testing delay. It is observed that Customized-CNN model records
    the least execution time as compared to others. While the training is just 67
    s, the testing execution time incurred is also only 88 s. VGG-19 model records
    the highest computational training and testing delay of 106 s and 125 s respectively.
    Download : Download high-res image (341KB) Download : Download full-size image
    Fig. 4. Computational delay analysis of proposed model with other predictive models.
    The sensor readings analysis of different soil health parameters using the proposed
    framework is also taken into account. As seen in Fig. 5, the monthly mean readings
    of temperature, humidity, and soil moisture level for a single year are captured
    and highlighted. The average yearly temperature, humidity, and moisture level
    noted through sensory readings are 25.8, 63.6, and 30.6 respectively. As seen
    in the figure, there is very little deviation from normal. Except for the months
    of June, July, and October, the sensory data for other months suggest that the
    millet crop got the required amount of temperature, humidity, and moisture throughout
    the year for its growth. The sensor data are also notified to the farmer in case
    any abnormal deviation is noted. Download : Download high-res image (676KB) Download
    : Download full-size image Fig. 5. Soil health metrics readings data on an yearly
    basis using integrated sensors. The scalability of a new predictive framework
    also depends upon its diverse behavior toward different scenarios. In our study,
    the scalability is validated by evaluating the model against performance metrics
    by considering various other crops like rice, maize, wheat, soybean, and groundnut.
    The outcome of the evaluation was quite promising as shown in Fig. 6. The mean
    accuracy, precision, recall, and f-score values obtained were 97.36 %, 96.56 %,
    94.82 %, and 95.78 % respectively. Download : Download high-res image (385KB)
    Download : Download full-size image Fig. 6. Evaluation metrics analysis of proposed
    model in context to other crops. As seen in Table 7, a comparative analysis of
    the proposed model is performed with models highlighted in the literature review
    section. The outcome is summarized in context to accuracy, precision, recall,
    and f-score metrics. It is observed that the proposed model outperforms other
    existing models in terms of all metrics. Table 7. Evaluation analysis of proposed
    model with models discussed in literature survey. Models Accuracy(%) Precision(%)
    Recall (%) F-Score (%) Ramesh et al.[11] 97.9 92.8 96.86 96.86 Pinki et al. [13]
    97.82 91.7 96.74 95.27 Amara et al. [14] 94.4 94.79 94.4 94.62 Lee et al. [17]
    96.3 94.6 93.2 93.4 Smith et al. [21] 89 87.5 88.3 87.2 Gayathri et al. [18] 97.83
    96.3 95.5 95.9 Umapathy et al. [10] 95 94.2 90.1 92.85 Proposed model 98.8 98.2
    97.4 97.7 The proposed model for millet crop assessment is developed in this study
    and is evaluated using different parameters. The predicted values for accuracy,
    precision, recall, and f-score with the novel Customized-CNN model were extremely
    satisfactory. One of the main reasons for the proposed model to outperform others
    is its high accuracy. High accuracy means that the proposed model can accurately
    identify the millet disease with a low error rate, hence providing reliable results.
    The proposed model strikes a balance between complexity and performance. While
    deep learning models can become highly complex with many layers and parameters,
    the proposed model achieves exceptional accuracy without any unnecessary complexity.
    The proposed model has been trained after hyperparameter tuning to optimize CNN
    performance. Adjusting parameters like learning rate, batch size, and number of
    layers or filters has resulted in a better and simpler model as compared to other
    existing models. The proposed model achieves high accuracy while using all the
    resources efficiently and with optimal computational resources. The overall computational
    execution delay with the model was also very low. The recorded crop health sensory
    readings data over a period of a single year was generated and its correctness
    was validated. Further, the model proved to be scalable as it gave an accurate
    estimation with other crops too. 5. Conclusion Effective smart monitoring of millet
    crops, along with disease prediction using predictive intelligence, is studied.
    IoT sensors for temperature, humidity, and soil moisture level were used to keep
    track of crop health. A novel customized CNN model was applied to the millet crop
    data to predict the likelihood of the occurrence of disease on the millet crop
    based on aggregated sensory readings and help in classifying the type of disease.
    Traditional farming is being replaced with modern, sustainable farming in this
    work. This makes it robust and advantageous economically. The proposed sustainable
    framework, upon implementation, generated optimum outcomes. The observed accuracy,
    precision, recall, and f-score values with the predictive model were 98.8 %, 98.2
    %, 97.4 %, and 97.7 %, respectively. The training delay was only 67 s, and the
    testing delay noted was also just 88 s. The mean temperature, humidity, and moisture
    level noted through sensory readings on a yearly basis were 25.8, 63.6, and 30.6,
    respectively. The generated average accuracy, precision, recall, and f-score values
    for other diverse crops were 97.36 %, 96.56 %, 94.82 %, and 95.78 %, respectively,
    which made it very scalable. Thus, the proposed sustainable model for millet crop
    assessment is definitely worthy, as it gave excellent results. It can assist the
    agricultural community and farmers in reliable tracking of millet crop growth
    and the occurrence of any disease symptoms. In the future, this research can be
    improved even more by integrating a drone service into the camera module to take
    photographs of the fields from different angles to give a more precise outcome
    in remote settings.The existing system can further be upgraded to develop a user-friendly
    Android or iOS mobile application that allows farmers to access real-time data
    from the IoT sensors and receive disease alerts.The system can further be extended
    to integrate the irrigation control system with the existing IoT sensors to automate
    the watering of crops as required. Declaration of Competing Interest The authors
    declare that they have no known competing financial interests or personal relationships
    that could have appeared to influence the work reported in this paper. Acknowledgement
    This research is funded by the Researchers Supporting Project Number (RSPD2023R890),
    King Saud University, Riyadh, Saudi Arabia. References [1] P.K. Sethy, N.K. Barpanda,
    A.K. Rath, S.K. Behera Using a support vector machine identified a deep feature-based
    rice leaf disease Comput. Electron. Agric., 175 (2020), Article 105527 View PDFView
    articleView in ScopusGoogle Scholar [2] A. Dutta, C. Misra, R.K. Barik, S. Mishra
    Enhancing mist assisted cloud computing toward secure and scalable architecture
    for smart healthcare International Conference on Advanced Communication and Computational
    Technology, Springer Nature Singapore, Singapore (2019), pp. 1515-1526 Google
    Scholar [3] S. Savary, L. Willocquet, S. J. Pethybridge, P. Esker, N. McRoberts,
    A. Nelson, The global burden of pathogens and pests on major food crops, Nat.
    Ecol. Evol. 3(3) (2019) 430-439, doi:10. 1038/s41559-018-0793-y. Google Scholar
    [4] S. Sahoo, S. Mishra, B. Panda, N. Jena Building a new model for feature optimization
    in agricultural sectors 2016 3rd International Conference on Computing for Sustainable
    Global Development (INDIACom), IEEE (2016), pp. 2337-2341 View in ScopusGoogle
    Scholar [5] A.K. Jukanti, C.L.L. Gowda, K.N. Rai, V.K. Manga, R.K. Bhatt world-feeding
    crops 11. In the dry and semi-arid tropics, pearl millet (Pennisetum glaucum L.)
    is a significant source of nutrition, health, and food security Food Secure.,
    8 (2016), pp. 307-329 CrossRefView in ScopusGoogle Scholar [6] S. Chakraborty,
    S. Mishra, A smart farming-based recommendation system using collaborative machine
    learning and image processing, in: Cognitive Informatics and Soft Computing: Proceeding
    of CISC 2021, Springer Nature Singapore, Singapore, 2022, pp. 703–716. Google
    Scholar [7] S.K. Mohapatra, S. Mishra, H.K. Tripathy, A. Alkhayyat A sustainable
    data-driven energy consumption assessment model for building infrastructures in
    resource constraint environment Sust. Energy Technol. Assess., 53 (2022), Article
    102697 Google Scholar [8] Srdjan Sladojevic, Marko Arsenovic, Andras Anderla,
    Dubravko Culibrk, Darko Stefanovic, Deep Neural Networks Based Recognition of
    Plant Diseases by Leaf Image Classification, Computational Intelligence and Neuroscience,
    vol. 2016, Article ID 3289801, 11 pages, 2016. https://doi.org/10.1155/2016/3289801.
    Google Scholar [9] X.E. Pantazi, D. Moshou, A.A. Tamouridou Automated leaf disease
    detection in different crop species through image features analysis and One Class
    Classifiers Comput. Electron. Agric., 156 (2019), pp. 96-104 View PDFView articleView
    in ScopusGoogle Scholar [10] S. Coulibaly, et al. Deep neural networks with transfer
    learning in millet crop images Comput. Ind., 108 (2019), pp. 115-120 View PDFView
    articleView in ScopusGoogle Scholar [11] S. Ramesh, D. Vydeki Recognition and
    classification of paddy leaf diseases using optimised deep neural network with
    java algorithm Inform. Process. Agric., 7 (2) (2020), pp. 249-260 View PDFView
    articleView in ScopusGoogle Scholar [12] G. Olmschenk, H. Tang, Z. Zhu Crowd counting
    with minimal data using generative adversarial networks for multiple target regression
    IEEE Winter Conference on Applications of Computer Vision (WACV), 2018 (2018),
    pp. 1151-1159 CrossRefView in ScopusGoogle Scholar [13] F.T. Pinki, N. Khatun,
    S.M. Islam, Content based paddy leaf disease recognition and remedy prediction
    using support vector machine, in: 2017 20th International Conference of Computer
    and Information Technology (ICCIT), 2017, pp. 1–5. Google Scholar [14] J. Amara,
    B. Bouaziz, A. Algergawy A Deep Learning-based Approach for Banana Leaf Diseases
    Classification Datenbanksysteme für Business, Technologie und Web (2017) Google
    Scholar [15] V. Pallagani, V. Khandelwal, B. Chandra, V. Udutalapally, D. Das,
    S.P. Mohanty, dCrop: a deep-learning based framework for accurate prediction of
    diseases of crops in smart agriculture, in: 2019 IEEE International Symposium
    on Smart Electronic Systems (iSES) (Formerly iNiS), 2019, pp. 29–33. Google Scholar
    [16] M. Brahimi, M. Arsenovic, S. Laraba, S. Sladojevic, K. Boukhalfa, A. Moussaoui
    Deep Learning for Plant Diseases: Detection and Saliency Map Visualisation Human
    and Machine Learning (2018) Google Scholar [17] S.H. Lee, C. Chan, S.J. Mayo,
    P. Remagnino How deep learning extracts and learns leaf features for plant classification
    Pattern Recogn., 71 (2017), pp. 1-13 View PDFView articleGoogle Scholar [18] T.
    Devi, P.N. Gayathri Image processing based rice plant leaves diseases in Thanjavur
    Tamilnadu Clust. Comput., 6 (2019), pp. 1-14 Google Scholar [19] K. Golhani, S.K.
    Balasundram, G. Vadamalai, B. Pradhan, A review of neural networks in plant disease
    detection using hyperspectral data. Information Processing in Agriculture, 2018.
    Google Scholar [20] Basori, Ahmad Hoirul, Mansur, Andi Besse Firdausiah and Riskiawan,
    Hendra Yufit, SMARF: Smart Farming Framework Based on Big Data, IoT and Deep Learning
    Model for Plant Disease Detection and Prevention. Applied Computing to Support
    Industry: Innovation and Technology 1174 (2020) 44-56. ISSN 1865-0929. Google
    Scholar [21] Smith, Bharathiraja Nallathambi, Sai Kirthi Pilli, eAGROBOT- A Robot
    for Early Crop Disease Detection Using Image Processing, by Jessy George and Vivek
    Diwanji was published in ICECS in February 2015. Google Scholar [22] K. L. Krishna,
    O. Silver, W. F. Malende and K. Anuradha, “Internet of Things application for
    implementation of smart agriculture system,” 2017 International Conference on
    I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), Palladam, India,
    2017, pp. 54-59, doi: 10.1109/I-SMAC.2017.8058236. Google Scholar [23] Khairy,
    S.E. Habib, Y. Fahmy, A. Khattab, H. Ismail, S. Zayan, M. M. Early plant disease
    forecasting using an IoT-based cognitive monitoring system. Comput. Electron.
    Agric. 166 (2019) 105028. Google Scholar [24] S. Riles, J. Torres-Sospedra, Belmonte,
    F.J. Zarazaga-Soria, A. González-Pérez, J. Huerta, Creation of an open sensorized
    platform for smart agriculture: Mildew disease monitoring for a vineyard support
    system. Maintain Computing and Informatics Systems 2020, 28, 100309. Google Scholar
    [25] S.S. Patil, S.A. Thorat Early detection of grapes diseases using machine
    learning and IoT Second International Conference on Cognitive Computing and Information
    Processing (CCIP), 2016 (2016), pp. 1-5 Google Scholar [26] K. Prema, M.B. Carmel
    Smart Farming: IoT-based plant leaf disease detection and prediction using deep
    neural network with image processing Int. J. Innov. Technol. Explor. Eng. (2019),
    pp. 3081-3083 View in ScopusGoogle Scholar [27] C. Yang A practical application
    example of remote sensing and precision agricultural technologies for crop disease
    detection and management Technology, 6 (5) (2020), pp. 528-532 View PDFView articleView
    in ScopusGoogle Scholar [28] Y. Shi, Z. Wang, X. Wang, S. Zhang, Internet of Things
    Application to Monitoring Plant Disease and Insect Pests, 2015. Google Scholar
    [29] X. F. Wang, Z. Wang, S.W. Zhang, Y. Shi, Monitoring and discrimination of
    plant disease and insect pests based on agricultural IOT. In: 4th International
    Conference on Information Technology and Management Innovation, 2015, pp. 112–115.
    Atlantis Press. Google Scholar [30] D. Markovic, R. Koprivica, U. Pesovic, S.
    Randic Application of IoT in monitoring and controlling agricultural production
    Acta Agriculturae Serbica, 20 (2015), pp. 145-153, 10.5937/AASer1540145M Google
    Scholar [31] A. Nadeem, A. Basit, S. Azfar A review of pest detection and management
    methods employing wireless sensor networks J. Entomol Zool. Stud., 3 (2015), pp.
    92-99 Google Scholar [32] N. Kundu, G. Rani, V.S. Dhaka, K. Gupta, S.C. Nayak,
    S. Verma, M.F. Ijaz, M. Woźniak IoT and interpretable machine learning based framework
    for disease prediction in pearl millet Sensors, 21 (2021), p. 5386, 10.3390/s21165386
    View in ScopusGoogle Scholar Cited by (0) © 2023 THE AUTHORS. Published by Elsevier
    BV on behalf of Faculty of Engineering, Alexandria University. Recommended articles
    Evaluation of 5G techniques affecting the deployment of smart hospital infrastructure:
    Understanding 5G, AI and IoT role in smart hospital Alexandria Engineering Journal,
    Volume 83, 2023, pp. 335-354 Arun Kumar, …, Monthippa Uthansakul View PDF An empirical
    survey of topologies, evolution, and current developments in multilevel inverters
    Alexandria Engineering Journal, Volume 83, 2023, pp. 148-194 G. Ezhilarasan, …,
    Stanislav Misak View PDF Photocatalytic wastewater treatment and disinfection
    using green ZnO-NP synthesized via extract Alexandria Engineering Journal, Volume
    83, 2023, pp. 113-121 Ibrahim Hotan Alsohaimi, …, Amr Mohammad Nassar View PDF
    Show 3 more articles Article Metrics Captures Readers: 15 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: (Mishra, Volety, Bohra, Alfarhood, & Safran, 2023)
  journal: Alexandria Engineering Journal
  key_findings: '- The proposed framework can accurately detect and classify rust
    and blast diseases in millet crops with high accuracy (98.8%) and low computational
    delay (67 seconds for training and 88 seconds for testing).

    - The framework uses a combination of IoT sensors and a deep learning model to
    provide real-time monitoring and disease prediction.

    - The framework is scalable and can be applied to other crops with minimal modifications.'
  limitations: null
  main_objective: To develop a smart and sustainable framework for monitoring millet
    crop health and predicting diseases using a combination of IoT sensors and a customized
    Convolutional Neural Network (CNN) model.
  relevance_evaluation: The paper is highly relevant to the point being made in the
    literature review, which is the need for integrated, end-to-end automated irrigation
    management systems that incorporate advanced monitoring techniques for real-time
    irrigation management. The paper proposes a framework that addresses this need
    by integrating IoT sensors and a deep learning model for disease detection and
    automated actions based on sensor readings, thus contributing to the overall goal
    of fully autonomous, scalable irrigation management.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Internet of Things (IoT) sensors, customized Convolutional Neural
    Network (CNN) model
  title: A smart and sustainable framework for millet crop monitoring equipped with
    disease detection using enhanced predictive intelligence
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Lin, T.-Y., Dollár, P., Girshick, R., He, K., Hariharan, B., & Belongie,
    S. (2017). Feature pyramid networks for object detection. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (pp. 2117–2125).
  authors:
  - Woodson M.
  - Zhang J.
  citation_count: '0'
  data_sources: Unspecified
  description: Advances in computer vision have resulted in promising research and
    applications in the domain of precision agriculture. In particular, Deep Learning
    has rapidly improved state of the art in object detection and segmentation, both
    of which prove vital in crop monitoring and yield forecasting. In most object
    detection problems, transfer learning serves as the established paradigm for applying
    Deep Learning systems on downstream tasks. This is particularly important in agricultural
    vision applications, where available data is relatively scarce compared to the
    large demand required of deep learning systems. Recent advances in Self-Supervised
    Learning have generated pretraining methods that approach the transfer performance
    of supervised pretraining on a variety of downstream tasks. To demonstrate the
    impact of Self-Supervised learning in agriculture, this paper evaluates the transfer
    performance of one self-supervised method, BYOL, on grape cluster detection. By
    comparing BYOL with supervised pretraining on a Faster R-CNN architecture, this
    work demonstrates that Self-Supervised Learning is competitive with other supervised
    pretraining methods in agricultural applications, showing its promise in advancing
    precision agriculture to more accurate and robust solutions.
  doi: 10.1007/978-3-031-37717-4_68
  explanation: The paper explores the use of advanced monitoring techniques, particularly
    high-resolution cameras equipped with computer vision algorithms, for visual monitoring
    of crop growth, disease detection, and irrigation system performance in precision
    agriculture. It highlights the potential of integrating these techniques with
    existing irrigation infrastructure and other precision agriculture technologies
    to enhance automation and efficiency.
  extract_1: '"Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)"'
  extract_2: '"Advanced Monitoring Techniques for Automated Irrigation Systems"'
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Intelligent Computing Conference
    paper Evaluating Self-supervised Transfer Performance in Grape Detection Conference
    paper First Online: 01 September 2023 pp 1043–1057 Cite this conference paper
    Access provided by University of Nebraska-Lincoln Download book PDF Download book
    EPUB Intelligent Computing (SAI 2023) Michael Woodson & Jane Zhang  Part of the
    book series: Lecture Notes in Networks and Systems ((LNNS,volume 711)) Included
    in the following conference series: Science and Information Conference 387 Accesses
    Abstract Advances in computer vision have resulted in promising research and applications
    in the domain of precision agriculture. In particular, Deep Learning has rapidly
    improved state of the art in object detection and segmentation, both of which
    prove vital in crop monitoring and yield forecasting. In most object detection
    problems, transfer learning serves as the established paradigm for applying Deep
    Learning systems on downstream tasks. This is particularly important in agricultural
    vision applications, where available data is relatively scarce compared to the
    large demand required of deep learning systems. Recent advances in Self-Supervised
    Learning have generated pretraining methods that approach the transfer performance
    of supervised pretraining on a variety of downstream tasks. To demonstrate the
    impact of Self-Supervised learning in agriculture, this paper evaluates the transfer
    performance of one self-supervised method, BYOL, on grape cluster detection. By
    comparing BYOL with supervised pretraining on a Faster R-CNN architecture, this
    work demonstrates that Self-Supervised Learning is competitive with other supervised
    pretraining methods in agricultural applications, showing its promise in advancing
    precision agriculture to more accurate and robust solutions. Keywords Object Detection
    Self-Supervised Learning Agriculture Access provided by University of Nebraska-Lincoln.
    Download conference paper PDF Similar content being viewed by others Semantic
    Segmentation of Vineyard Images Using Convolutional Neural Networks Chapter ©
    2020 Residual Cascade CNN for Detection of Spatially Relevant Objects in Agriculture:
    The Grape-Stem Paradigm Chapter © 2023 Acre-Scale Grape Bunch Detection and Predict
    Grape Harvest Using YOLO Deep Learning Network Article 03 February 2024 1 Introduction
    People across the world rely on the availability of fresh foods to feed themselves
    and their families. Whether it be farming processes that ensure the health and
    abundance of natural fruits and vegetables, or practices that provide dairy and
    meats, agriculture has remained an integral component of our lives. The future
    of agriculture faces many challenges in addressing the needs of a rapidly growing
    world population: climate change, increasing demand for energy, resource shortages,
    and labor shortages [26]. These challenges have given rise to Precision Agriculture,
    which utilizes technology to improve the efficiency of agricultural processes.
    Yield forecasting is one important process in agriculture. Yield forecasting is
    a technique where farmers obtain early crop counts to estimate the expected yield
    at harvest. This can help farmers accurately prepare for harvest, providing data
    to inform decisions in packaging, marketing, and labor requirements [37]. However,
    yield forecasting remains manual: workers sample a subset of the vineyard, obtaining
    fruit counts, and extrapolate these numbers to cover the entirety of the vineyard,
    taking historical yields, weather, and field measurements into account [37]. Consequently,
    this manual process introduces a variety of biases and errors into the eventual
    yield forecast. To improve the accuracy and speed of forecasting, research has
    focused on applying computer vision to crop detection. By developing a system
    that can automatically detect and count crops, one could deploy an image acquisition
    system throughout a field to obtain accurate counts early in the harvest cycle.
    Before deep learning gained popularity, classical vision approaches were applied
    to various crop counting tasks. However, classical techniques were challenged
    by natural environmental conditions associated with agricultural applications
    such as dynamic lighting, weather patterns, and heavy occlusion. Deep learning,
    combined with the practice of transfer learning, has helped address such challenges
    to further advance state-of-the-art in agricultural tasks such as crop detection
    and phenotyping. While transfer learning is effective in agriculture, many systems
    are constrained by the current paradigm of supervised pretraining. Supervised
    pretraining requires that massively large image databases, such as ImageNet [11],
    are specially curated and largely annotated. Labeling such a dataset comes with
    great cost, as these datasets contain millions of images. This high cost, as a
    result, restricts the domain of learnable features, since labeled data comprises
    a small subset of all possible images that networks could learn from. Furthermore,
    obtaining labeled data becomes much more difficult across different imaging modalities.
    When considering the potential for agricultural applications to leverage multi-modal
    data, it’s important to explore training methods that obviate the need for ground-truth
    labels, broadening the scope of learnable features that transfer well to downstream
    tasks such as crop detection. Such considerations have given rise to a class of
    pretraining called Self-Supervised Learning, where features can be learned from
    data without class labels. Self-Supervised Learning (SSL) has shown success in
    Natural Language Processing (NLP), where networks can learn word embeddings by
    completing tasks using unlabeled data, such as the masked word prediction task
    used in BERT [12]. These word embeddings carry great semantic value in the feature
    space, transferring well to other language tasks. The success of SSL in NLP has
    motivated vision-based training approaches that can learn from image data itself,
    rather than relying on annotations which describe the images. Recent self-supervised
    techniques have approached, or even surpassed, the success of supervised pretraining
    for image classification tasks, while displaying an ability to accurately transfer
    to a variety of downstream tasks such as dense prediction, few-shot recognition,
    and object detection [16]. While SSL has been shown to perform well in various
    imaging benchmarks, the effectiveness of self-supervised transfer performance,
    to the best of our knowledge, has not yet been analyzed in an agricultural field
    setting. Given the early success of Self-Supervised Learning on various benchmark
    datasets, it’s important to analyze its impact on real-world applications such
    as crop detection. As a result, this work’s contributions are as follows: We evaluated
    the transfer performance of BYOL (Bootstrap Your Own Latent) [19], a state-of-the-art
    SSL pretraining method, on grape cluster detection. We compared BYOL transfer
    performance to supervised pretraining methods, particularly pretraining on ImageNet
    image classification and COCO [29] object detection. By demonstrating whether
    SSL can compete with supervised pretraining in an important agricultural task,
    we hope to illustrate its effectiveness as a powerful feature generator, while
    illuminating its potential to learn features that are unconstrained by annotated
    datasets. The paper is outlined as follows: Sect. 2 conducts a review of various
    literature in fruit detection and yield forecasting. This is followed by Sect.
    3, which provides a background of Self-Supervised Learning. Section 4 then outlines
    the experimental methods used to compare BYOL with supervised pretraining, followed
    by Sect. 5, which demonstrates the results of the experiment. Finally, this paper
    offers concluding remarks in Sect. 6. 2 Related Work There exists a variety of
    research in the area of crop detection and phenotyping. One category of research
    focuses on hand-crafted features to characterize and classify crops. Color-based
    segmentation can be used to identify fruit pixels [31, 38, 40], though such approaches
    struggle to generalize across various environments, lighting conditions, and ripening
    stages. Texture and SIFT features can help improve crop detection when crop color
    blends in with the surrounding environment, [2, 30, 37, 39], but such approaches
    often utilize artificial illumination, which restricts widespread use. More recent
    approaches utilize deep learning for both crop detection and segmentation tasks.
    Fruit segmentation maps can be generated by applying networks in a sliding window
    fashion [7, 33]. Segmentation maps cannot count fruit by itself, so some work
    introduced a second stage which learns to count fruit for each segmentation mask
    [8, 20]. Alternatively to segmentation, counting applications can employ object-detection
    frameworks to identify separate crop instances. Research has used both single-shot
    detectors [5, 47] and region-proposal networks for fruit detection [3, 18, 45],
    with the Faster R-CNN [41] seeing greater success at localizing crop instances.
    For some crop, localizing crop instances for yield forecasting is ambiguous; one
    could approach grape detection, for example, by identifying grape clusters or
    individual grape berries, which presents a trade-off between yield accuracy and
    annotation costs. At the cost of greater annotation demands, [48] counts individual
    grape berries by treating detection as a semantic segmentation task, assigning
    pixels to background, berry, and berry edge classes, then using connected components
    to count the individual berries. Alternatively, work by [10] adapted crowd-counting
    networks to grape berry counting tasks, using dot annotations to train a network.
    Although deep learning improved the accuracy and robustness of fruit detection
    systems, deep learning still struggles to cope with occlusion caused by overlapping
    crop and dense canopies. RGB-D cameras, LIDAR, and photogrammetry techniques can
    alleviate occlusion effects [13, 22, 27, 42, 46], but deep learning systems tend
    to operate on RGB image data for object detection, as networks are often pretrained
    on ImageNet-like data before transfer learning. Likewise, while agriculture vision
    often features multispectral data such as IR [17, 23, 43] to counteract difficult
    conditions such as direct sunlight exposure, supervised transfer learning is limited
    in providing pre-learned representations across various input modalities. Meanwhile,
    Self-Supervised Learning has shown success in discovering semantically strong
    feature embeddings without labels. Networks pretrained with SSL can outperform
    ImageNet supervised transfer performance on a variety of different datasets, including
    Cars [25], Flowers [35], and Food101 [4]; SSL especially performs well in a semi-supervised
    setting [6, 9, 19, 49]. This is important for agriculture, where multispectral
    annotations are sparse. Furthermore, SSL can be applied on 3-D data, learning
    representations that are invariant to 3-D input formats [50] or finding representations
    that associate 3-D objects with 2-D renderings [1]. Such models can transfer to
    tasks such as 3-D object detection or 3-D segmentation, potentially allowing agriculture
    systems to acquire 3-D scene understanding. 3 Background 3.1 Self-supervised Learning
    Self-Supervised Learning (SSL) is a subset of unsupervised learning that aims
    to identify useful structures in data without requiring ground-truth annotations
    to guide the learning process. Self-supervised methods learn by forming annotations
    from the data itself, then optimizing through a learning task that utilizes these
    annotations. SSL approaches can be further divided into pretext methods and contrastive
    methods, which are further explained in the sections below. SSL has recently gained
    momentum in vision, where SSL can effectively pretrain networks on large datasets
    without labels, uncovering useful feature representations in the process. This
    approach allows models to develop a strong semantic understanding of a wide variety
    of data subjects and modalities, even extending well to long-tailed distributions
    [50]. 3.2 Pretext Tasks Pretext tasks attempt to learn from data by solving tasks
    associated with the input image. A parallel can be found in NLP: BERT [12], for
    example, would accept input sentences with random words masked out, and the pretext
    task involved training a classifier to predict the missing words. By solving this
    pretext task, the encoder network generated features that effectively modeled
    language semantics. Self-supervised pretext tasks in vision often involve modifying
    an input image such that annotations can be formed automatically. For example,
    [14] extracted a random pair of patches from an image, fed the patches into a
    siamese network, then predicted the patch configuration using a softmax classifier.
    By predicting the relative position of one patch to another (from a set known
    patch configurations), [14] was able to learn features that transferred well to
    object detection. Various other pretext tasks were designed to improve transfer
    performance on common image benchmarks. For example, [36] attempted to solve jigsaw
    puzzles by splitting input images into patches and permuting the generated patches.
    This paper expanded upon [14] by feeding each patch individually into a siamese-ennead
    network, generating 9 feature vectors that were combined and classified as 1 of
    the possible 64 permutations. The work by [15] expanded upon the previous works
    by combining multiple pretext tasks when training a ResNet-101-v2 [21]. This was
    achieved by using separate network heads, one for each task, and updating weights
    from a shared network trunk. By combining gradients from a diverse set of pretext
    tasks such as image colorization, patch relative-position prediction, exemplar
    learning, and motion segmentation, [15] found that transfer performance improved
    compared to using a single pretext task, demonstrating that features from separate
    tasks are complimentary. However, pretext methods contain several problems. First,
    transfer performance largely depends on choosing a pretext task that bears association
    to the downstream application. Furthermore, combining pretext tasks can complicate
    the network architecture and training procedure [15]. Lastly, pretext learning
    generates features that degrade in quality in the final layers of the network.
    This occurs because network layers are optimized to solve a particular pretext
    task rather than discover an optimal feature representation. Therefore, contrastive
    methods have recently gained the most attention. 3.3 Contrastive Learning Rather
    than solving pretext tasks, contrastive methods attempt to discover feature embeddings
    that are invariant to image transformations. For example, if an image were input
    into a network, the final-layer features should be highly similar to the features
    produced by an augmentation of that same image. This idea forms the basis of contrastive
    methods, which aims to learn representations that are invariant to low-level transformations,
    thus capturing image semantics. Contrastive methods operate as follows: By inputting
    an image I and an augmented version of that image \\(I^\\textrm{t}\\), a network
    is optimized to produce a pair of feature vectors \\(\\mathbf {v_I} = f(I,\\theta
    )\\) and \\(\\mathbf {v_{I^t}} = f(I^t,\\theta )\\) such that \\(\\mathbf {v_I}
    \\approx \\mathbf {v_{I^t}}\\). Network parameters are optimized by minimizing
    the cost function (1) [34], which has the effect of maximizing agreement between
    the two feature embeddings: $$\\begin{aligned} \\hat{\\theta } = argmin_\\theta
    -{\\log [{\\langle \\mathbf {v_I},\\mathbf {v_{I^t}} \\rangle }]} \\;, \\end{aligned}$$
    (1) where \\(\\langle \\cdot ,\\cdot \\rangle \\) is often the cosine similarity.
    This process, however, leaves a network prone to trivial solutions, where all
    joint embeddings are identical for every input pair. Therefore, contrastive methods
    introduce negative samples into the learning process, jointly maximizing the similarity
    between positive examples and minimizing the similarity measure between negative
    samples. Since images are unlabeled, a negative sample is any image \\(I''\\)
    that is different than the original image I. Therefore, contrastive learning,
    in general, processes an image I, a transformed version of that image \\(I^\\textrm{t}\\)
    (which acts as a positive sample), and a set of negative samples \\(I'' \\in D_\\textrm{N}
    = \\{I_1...I_\\textrm{N}\\}\\). By introducing a set of negative samples into
    the training objective, many contrastive methods formulate a variant of the following
    loss [9]: $$\\begin{aligned} L_\\theta = -{\\log \\frac{\\exp (\\langle \\mathbf
    {v_{I}},\\mathbf {v_{I^t} \\rangle }/\\tau )}{\\exp (\\langle \\mathbf {v_{I}},\\mathbf
    {v_{I^t}} \\rangle /\\tau ) + \\sum _{I'' \\in D_\\textrm{N}} \\exp (\\langle
    \\mathbf {v_{I''}},\\mathbf {v_{I^t}} \\rangle / \\tau )}} \\;, \\end{aligned}$$
    (2) where \\(D_\\textrm{N}\\) is a set of randomly-selected negative samples from
    the training set, \\(\\langle \\mathbf {v_{I}},\\mathbf {v_{I^t}} \\rangle \\)
    is the cosine similarity \\(\\mathbf {v_{I}} \\cdot \\mathbf {v_{I^t}} / \\Vert
    \\mathbf {v_{I}}\\Vert \\Vert \\mathbf {v_{I^t}}\\Vert \\), and \\(\\tau \\) is
    a temperature value that can be set as a hyperparameter. Using the above framework,
    contrastive learning methods approach ImageNet supervised pretraining on a variety
    of downstream tasks [34]. Furthermore, unlike pretext features, contrastive features
    do not degrade in the deepest layers of a network. A downside to contrastive learning,
    however, is that effective learning requires a large quantity of negative samples.
    This is required since many negative samples make poor training examples, so methods
    must increase the batch size as a way to provide a sufficient pool of useful negatives.
    For example, [9] used up to 16384 negative samples per positive image pair. While
    contrastive methods have shown great promise in generating useful features in
    a transfer setting, the expensive training requirements has motivated recent methods
    to avoid negative samples. BYOL [19], the SSL method evaluated in this work, established
    a training procedure that required only positive image pairs. This was achieved
    by employing an online network and a target network. Like other contrastive methods,
    each network received augmented versions of the same image. However, while the
    online and target networks were identical in structure, the online network and
    the target network had different sets of weights. Specifically, the target weights
    \\(\\xi \\) were an exponential moving average of the online network weights,
    \\(\\theta \\). Therefore, while the online network received weight updates through
    its gradient, the target network avoided gradient updates via a stop-gradient,
    and instead updated its weights as follows [19]: $$\\begin{aligned} \\xi = \\tau
    \\xi - (1-\\tau )\\theta \\;, \\end{aligned}$$ (3) where \\(\\tau \\) is a target
    decay weight \\(\\tau \\in [0,1]\\). Since networks will generate different feature
    vectors as a result of differing weights, the online network also contains an
    additional prediction module \\(q_\\theta \\), which attempts to map the online
    feature to the target network feature. The BYOL architecture uses a normalized
    Euclidean distance between the output vectors as the loss metric, represented
    as $$\\begin{aligned} L_{\\theta , \\xi }(q_\\theta (\\mathbf{{\\overline{z}_\\theta
    }}), \\mathbf{{\\overline{z''}_\\xi }}) = \\Vert q_\\theta (\\mathbf{{\\overline{z}_\\theta
    }}) - \\mathbf{{\\overline{z''}_\\xi }}\\Vert ^2 = 2 - 2q_\\theta (\\mathbf{{\\overline{z}_\\theta
    }}) \\cdot \\mathbf{{\\overline{z''}_\\xi }} \\;, \\end{aligned}$$ (4) where \\(\\mathbf
    {z_\\theta }\\) and \\(\\mathbf {z_\\xi }\\) are joint embeddings produced by
    network pair, and \\(\\mathbf {\\overline{z}_\\theta } = \\mathbf {z_\\theta }/\\Vert
    \\mathbf {z_\\theta }\\Vert \\) and \\(\\mathbf {\\overline{z''}_\\xi } = \\mathbf
    {z_\\xi } / \\Vert \\mathbf {z_\\xi }\\Vert \\) are normalized to unit length
    [19]. 4 Methods and Materials 4.1 Data The Embrapa Wine Grape Instance Segmentation
    Dataset (WGISD)[44], provided by Santos et al. [45], was used for both training
    and evaluation. This dataset contains 300 images of grapes captured at the Guaspari
    Winery in Espírito Santo do Pinhal, São Paulo, Brazil. The dataset consists of
    five different grape varieties: Chardonnay, Cabernet Franc, Cabernet Sauvignon,
    Sauvignon Blanc, and Syrah. Examples of the grape varieties can be seen in Fig.
    1. The different grape varieties offer a diverse set of visual characteristics,
    differing in color, size, and compactness. To prevent network bias toward any
    grape variety, the dataset contains near-equal examples of each variety. The dataset
    comes with bounding box and mask annotations. While box annotations are provided
    with every image in the dataset, mask annotations were applied to a random subset
    of 110 images. This amounts to a total of 4431 boxes and 2020 binary masks in
    the dataset. To maximize the amount of data available for training and testing,
    we focus on grape cluster detection using bounding boxes, ignoring the mask segmentation
    data. The ground-truth bounding boxes were manually annotated by Santos et al.
    [45]. In order to evaluate the performance of the network, the dataset was partitioned
    into training and test sets. The training partition contains 80% of the dataset,
    while 20% was reserved for testing. Additionally, 20% of the training data was
    reserved for validation. The data was partitioned such that both the training
    and test sets were balanced with respect to grape varieties. The exact details
    of how data was partitioned can be seen in Table 1; we followed the recommendations
    provided by [45]. The grape varieties are provided for illustrative purposes,
    as evaluation was performed over the entire test set without analyzing performance
    on specific varieties. Fig. 1. Different Grape Varieties: a Chardonnay b Cabernet
    Sauvignon c Sauvignon Blanc d Cabernet Franc e Syrah Full size image Table 1.
    Dataset Partitions by Variety Full size table 4.2 Training Procedure To study
    the current capabilities of SSL transfer performance on grape detection, we selected
    two supervised pretraining methods as a reference of comparison. First, we used
    weights initialized from ImageNet supervised pretraining. Additionally, since
    we evaluated SSL in an object-detection task, we also used weights initialized
    from supervised pretraining on COCO object detection. As part of this work, we
    aimed to follow the procedure and build upon the results of [45], as we used the
    same dataset. As a result, like [45], we treat grape detection as an object detection
    task, where grape clusters must be localized with bounding boxes. When considering
    grape detection for the purposes of yield forecasting, it is also sensible to
    count individual grape berries in an image. However, we reserve alternative approaches
    for future research, as we first aim to compare BYOL to a reference experiment
    utilizing supervised pretraining. We also used the same set of augmentations for
    training as [45]: random pixel dropout, additive gaussian noise, gaussian blur,
    contrast enhancement, and horizontal flipping. A random subset of augmentations
    were applied to each image batch during training. A Faster R-CNN with a ResNet-50
    backbone and an FPN [28] was used for this experiment. We used the AdamW optimizer
    [32] and performed a hyperparameter search over the following hyperparameters:
    the learning rate, the decoupled weight decay factor in AdamW, and the NMS threshold
    used in the Faster R-CNN. The hyperparameter search was conducted using Population
    Based Training [24]. All other hyperparameters are set to the defaults established
    in the torchvision library. While we designed an experiment to compare the transfer
    performance of 3 pretraining methods (Supervised ImageNet, Supervised COCO, and
    BYOL), we also considered the effect of freezing different ResNet layers. More
    specifically, we looked at three finetuning scenarios: First, we froze all 5 ResNet
    layers, only updating FPN and Faster R-CNN weights. Next, we finetuned the last
    ResNet layer, followed by finetuning the entire backbone. These scenarios were
    coined “Freeze 5\", “Freeze 4\", and “Freeze 0\", respectively, corresponding
    to the number of ResNet layers that were frozen during training. It is worth mentioning
    that COCO pretraining was the only method that initialized FPN weights; otherwise,
    both the FPN and Faster R-CNN weights were trained from scratch while finetuning.
    Lastly, while evaluation was reported on the WGISD test partition using COCO AP
    metrics, we also analyzed performance on a separate grape dataset: the CR2 dataset
    [10]. Since the CR2 dataset lacks bounding-box annotations (it includes dot annotations
    instead), performance was observed by visual inspection only. However, introducing
    this dataset helped illustrate the ability of each pretraining method and each
    finetuning scenario at generalizing to a new variety (Teroldego) in a different
    vineyard. Detection on the CR2 dataset is presented in Sect. 5. 5 Results Numerical
    results for all pretraining methods and finetuning scenarios can be seen in Table
    2. COCO AP ranged from 0.404 to 0.551 and \\(AP_{50}\\) ranged from 0.808 to 0.886,
    with BYOL achieving 0.505 AP and 0.877 \\(AP_{50}\\). A network trained from scratch
    was added for reference. In general, BYOL was competitive with supervised pretraining
    methods, even outperforming ImageNet supervised pretraining for all finetuning
    scenarios. COCO supervised pretraining consistently performed the best. This is
    possibly due to the fact that COCO pretraining involves object detection, which
    matches the downstream task at hand. Furthermore, COCO pretraining initialized
    FPN weights, while BYOL and ImageNet supervised pretraining do not. However, when
    considering the potential to effortlessly add training data for BYOL (and SSL
    methods in general) compared to supervised counterparts, the competitive results
    are promising. These results are an improvement over [45], which reported a 0.71
    \\(AP_{50}\\). This improvement is likely because [45] trained with the subset
    of WGISD containing mask annotations. Table 2. Transfer Performance of All Pretraining
    Methods. Results are Provided using COCO AP, which Computes Average Precision
    over a Range of IoU Thresholds. Additionally, \\(AP_{50}\\) is Provided, which
    Represents AP at a Single IoU Threshold of 0.5. A Network Trained from Scratch
    is Reported for Reference. Supervised COCO Pretraining is most Accurate, while
    BYOL is the Second Most Accurate. Finetuning the Entire Network Improves Performance
    in every Case Full size table One particularly interesting case is the relative
    performance between “Freeze 5\" and “Freeze 4\" scenarios. SSL methods employing
    pretext tasks observe a phenomenon where deep layers end up optimizing for the
    task being solved, resulting in poor transfer performance. By introducing “Freeze
    4\", we could test whether downstream applications benefited from updating final-layer
    features. However, in general, finetuning the final ResNet layer had little effect
    on the performance for grape detection, indicating that SSL features were well-suited
    for downstream tasks. Image results for each pretraining method can be seen in
    Fig. 2. Solid bounding boxes indicate true positives, while dashed boxes are false
    positives. Only bounding boxes with confidence score > 0.7 are displayed, with
    an IoU threshold of 0.5. Consistent with the numerical results, image results
    demonstrate that each pretraining method is competitive with the others. At times,
    detection accuracy suffers when cluster boundaries are ambiguous. Without in-field
    or 3-D data, cluster boundaries are difficult to discern, even for human annotators.
    Figure 3 demonstrates the effect of freezing different backbone layers with a
    BYOL-trained network. These results suggest that one should finetune an entire
    network for best performance on grape detection, as freezing layers produced images
    with overlapping bounding-box placement. Lastly, Fig. 4 displays BYOL transfer
    performance on an unseen dataset, CR2. Despite the distinct visual characteristics
    compared to WGISD, performance actually generalized well, even detecting overlapping
    clusters. Similar to detection on WGISD, CR2 performance appeared to improve when
    the entire backbone was finetuned during training. Fig. 2. Image Results for Networks
    Initialized via Different Pretraining Methods. True Positives (Solid Bounding
    Box) and False Positives (Dashed Bounding Box) are marked. Detections have Confidence
    Score > than 0.7 and an IoU Threshold of 0.5. a Network Pretrained with ImageNet
    Supervised Classification. b Network Pretrained with BYOL SSL Method. c Network
    Pretrained with COCO Supervised Object Full size image Fig. 3. Cluster Detection
    Performance for a BYOL-Trained Network when Freezing Different Layers. True Positives
    (Solid Bounding Box) and False Positives (Dashed Bounding Box) are Marked. a Freeze
    5 b Freeze 4 c Freeze 0 Full size image Fig. 4. BYOL Transfer Performance on the
    CR2 Dataset. Detections have Confidence Score > 0.7. The CR2 Dataset was not seen
    during Training and does not Contain Ground-Truth Bounding-Box Annotations, so
    Evaluation is by Inspection only Full size image 5.1 Future Work Research should
    explore training methods where Self-Supervised Learning is applied to datasets
    outside of ImageNet. Since SSL is not constrained to annotated data, datasets
    can increase in size, potentially allowing models to increase in size to carry
    greater learning capacity. Such systems can be applied to agriculture to continue
    to improve upon state-of-the-art. Furthermore, SSL should be applied across data
    modalities, with applications in agriculture leveraging new features for more
    dynamic vision systems. 6 Conclusion This work studied the performance capabilities
    of BYOL, a Self-Supervised Learning method, when transferred to an agricultural
    application such as grape cluster detection. BYOL transfer performance was compared
    to supervised pretraining methods using a Faster R-CNN with a ResNet-50 FPN backbone.
    Specifically, BYOL was compared with supervised pretraining on ImageNet and COCO.
    BYOL transfer performance was competitive with other pretraining methods, illustrating
    its future potential in learning features across data modalities, allowing deep
    learning to make progress in agricultural applications ranging from yield forecasting
    to crop phenotyping. References Afham, M., Dissanayake, I., Dissanayake, D., Dharmasiri,
    A., Thilakarathna, K., Rodrigo, R.: CrossPoint: self-supervised cross-modal contrastive
    learning for 3D point cloud understanding. In: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9902–9912 (2022) Google Scholar   Aquino,
    A., Millan, B., Diago, M.-P., Tardaguila, J.: Automated early yield prediction
    in vineyards from on-the-go image acquisition. Comput. Electron. Agric. 144, 26–36
    (2018) Article   Google Scholar   Bargoti, S., Underwood, J.: Deep fruit detection
    in orchards. In: 2017 IEEE International Conference on Robotics and Automation
    (ICRA), pp. 3626–3633. IEEE (2017) Google Scholar   Bossard, L., Guillaumin, M.,
    Van Gool, L.: Food-101 – mining discriminative components with random forests.
    In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,
    vol. 8694, pp. 446–461. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10599-4_29
    Chapter   Google Scholar   Bresilla, K., Perulli, G.D., Boini, A., Morandi, B.,
    Grappadelli, L.C., Manfrini, L.: Single-shot convolution neural networks for real-time
    fruit detection within the tree. Front. Plant Sci. 10, 611 (2019) Google Scholar   Caron,
    M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised
    learning of visual features by contrasting cluster assignments. Adv. Neural Inf.
    Process. Syst. 33, 9912–9924 (2020) Google Scholar   Cecotti, H., Rivera, A.,
    Farhadloo, M., Pedroza, M.A.: Grape detection with convolutional neural networks.
    Expert Syst. Appl. 159, 113588 (2020) Google Scholar   Chen, S.W., et al.: Counting
    apples and oranges with deep learning: a data-driven approach. IEEE Robot. Autom.
    Lett. 2(2), 781–788 (2017) Google Scholar   Chen, T., Kornblith, S., Norouzi,
    M., Hinton, G.: A simple framework for contrastive learning of visual representations.
    In: International Conference on Machine Learning, pp. 1597–1607. PMLR (2020) Google
    Scholar   Coviello, L., Cristoforetti, M., Jurman, G., Furlanello, C.: GBCNet:
    in-field grape berries counting for yield estimation by dilated CNNs. Appl. Sci.
    10(14), 4870 (2020) Article   Google Scholar   Deng, J., Dong, W., Socher, R.,
    Li, L.-J., Li, K., Fei-Fei, L.: ImageNet: a large-scale hierarchical image database.
    In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255.
    IEEE (2009) Google Scholar   Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.:
    BERT: pre-training of deep bidirectional transformers for language understanding.
    arXiv preprint arXiv:1810.04805 (2018) Dey, D., Mummert, L., Sukthankar, R.: Classification
    of plant structures from uncalibrated image sequences. In: 2012 IEEE Workshop
    on the Applications of Computer Vision (WACV), pp. 329–336. IEEE (2012) Google
    Scholar   Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation
    learning by context prediction. In: Proceedings of the IEEE International Conference
    on Computer Vision, pp. 1422–1430 (2015) Google Scholar   Doersch, C., Zisserman,
    A.: Multi-task self-supervised visual learning. In: Proceedings of the IEEE International
    Conference on Computer Vision, pp. 2051–2060 (2017) Google Scholar   Ericsson,
    L., Gouk, H., Hospedales, T.M.: How well do self-supervised models transfer? In:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 5414–5423 (2021) Google Scholar   Feng, J., Zeng, L., He, L.: Apple fruit
    recognition algorithm based on multi-spectral dynamic image analysis. Sensors
    19(4), 949 (2019) Article   Google Scholar   Ge, Y., Xiong, Y., From, P.J.: Instance
    segmentation and localization of strawberries in farm conditions for automatic
    fruit harvesting. IFAC-PapersOnLine 52(30), 294–299 (2019) Google Scholar   Grill,
    J.-B., et al.: Bootstrap your own latent-a new approach to self-supervised learning.
    Adv. Neural Inf. Process. Syst. 33, 21271–21284 (2020) Google Scholar   Häni,
    N., Roy, P., Isler, V.: A comparative study of fruit detection and counting methods
    for yield mapping in apple orchards. J. Field Robot. 37(2), 263–282 (2020) Article   Google
    Scholar   He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual
    networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
    vol. 9908, pp. 630–645. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0_38
    Chapter   Google Scholar   Herrero-Huerta, M., González-Aguilera, D., Rodriguez-Gonzalvez,
    P., Hernández-López, D.: Vineyard yield estimation by automatic 3D bunch modelling
    in field conditions. Comput. Electron. Agric. 110, 17–26 (2015) Article   Google
    Scholar   Hung, C., Nieto, J., Taylor, Z., Underwood, J., Sukkarieh, S.: Orchard
    fruit segmentation using multi-spectral feature learning. In: 2013 IEEE/RSJ International
    Conference on Intelligent Robots and Systems, pp. 5314–5320. IEEE (2013) Google
    Scholar   Jaderberg, M., et al.: Population based training of neural networks.
    arXiv preprint arXiv:1711.09846 (2017) Krause, J., Stark, M., Deng, J., Fei-Fei,
    L.: 3D object representations for fine-grained categorization. In: 4th International
    IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney, Australia
    (2013) Google Scholar   VAN WOENSEL Lieve. Precision-agriculture and the future
    of farming in Europe. https://policycommons.net/artifacts/1996735/precision/2748500/
    (2016). Accessed 15 April 2022 Lin, G., Tang, Y., Zou, X., Xiong, J., Fang, Y.:
    Color-, depth-, and shape-based 3d fruit detection. Precis. Agric. 21(1), 1–17
    (2020) Article   Google Scholar   Lin, T.-Y., Dollár, P., Girshick, R., He, K.,
    Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In:
    Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pp. 2117–2125 (2017) Google Scholar   Lin, T.-Y., et al.: Microsoft COCO: common
    objects in context. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.)
    ECCV 2014. LNCS, vol. 8693, pp. 740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1_48
    Chapter   Google Scholar   Liu, S., Cossell, S., Tang, J., Dunn, G., Whitty, M.:
    A computer vision system for early stage grape yield estimation based on shoot
    detection. Comput. Electron. Agric. 137, 88–101 (2017) Article   Google Scholar   Liu,
    S., Whitty, M., Cossell, S.: Automatic grape bunch detection in vineyards for
    precise yield estimation. In: 2015 14th IAPR International Conference on Machine
    Vision Applications (MVA), pp. 238–241. IEEE (2015) Google Scholar   Loshchilov,
    I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101
    (2017) Marani, R., Milella, A., Petitti, A., Reina, G.: Deep neural networks for
    grape bunch segmentation in natural images from a consumer-grade camera. Precis.
    Agric. 22(2), 387–413 (2021) Article   Google Scholar   Misra, I., van der Maaten,
    L.: Self-supervised learning of pretext-invariant representations. In: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6707–6717
    (2020) Google Scholar   Nilsback, M.-E., Zisserman, A.: Automated flower classification
    over a large number of classes. In: 2008 Sixth Indian Conference on Computer Vision,
    Graphics & Image Processing, pp. 722–729. IEEE (2008) Google Scholar   Noroozi,
    M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw
    puzzles. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
    vol. 9910, pp. 69–84. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46466-4_5
    Chapter   Google Scholar   Nuske, S., Wilshusen, K., Achar, S., Yoder, L., Narasimhan,
    S., Singh, S.: Automated visual yield estimation in vineyards. J. Field Robot.
    31(5), 837–860 (2014) Article   Google Scholar   Palacios, F., Diago, M.P., Tardaguila,
    J.: A non-invasive method based on computer vision for grapevine cluster compactness
    assessment using a mobile sensing platform under field conditions. Sensors 19(17),
    3799 (2019) Google Scholar   Pothen, Z.S., Nuske, S.: Texture-based fruit detection
    via images using the smooth patterns on the fruit. In: 2016 IEEE International
    Conference on Robotics and Automation (ICRA), pp. 5171–5176. IEEE (2016) Google
    Scholar   Reis, M.J.C.S., et al.: Automatic detection of bunches of grapes in
    natural environment from color images. J. Appl. Logic 10(4), 285–290 (2012) Google
    Scholar   Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time
    object detection with region proposal networks. In: Advances in Neural Information
    Processing Systems. vol. 28 (2015) Google Scholar   Roy, P., Isler, V.: Surveying
    apple orchards with a monocular vision system. In: 2016 IEEE International Conference
    on Automation Science and Engineering (CASE), pp. 916–921. IEEE (2016) Google
    Scholar   Sa, I., Ge, Z., Dayoub, F., Upcroft, B., Perez, T., McCool, C.: DeepFruits:
    a fruit detection system using deep neural networks. sensors 16(8), 1222 (2016)
    Google Scholar   Santos, T.T., Buiani, M.: Embrapa wine grape instance segmentation
    dataset - embrapa wgisd. https://github.com/charlespwd/project-title (2019) Santos,
    T.T., de Souza, L.L., dos Santos, A.A., Avila, S.: Grape detection, segmentation,
    and tracking using deep neural networks and three-dimensional association. Comput.
    Electron. Agric. 170, 105247 (2020) Google Scholar   Santos, T.T., Bassoi, L.H.,
    Oldoni, H., Martins, R.L.: Automatic grape bunch detection in vineyards based
    on affordable 3D phenotyping using a consumer webcam. In: CONGRESSO BRASILEIRO
    DE AGROINFORMÁTICA, 11, 2017, Campinas. Ciência de (2017) Google Scholar   Wang,
    Z., Walsh, K., Koirala, A.: Mango fruit load estimation using a video based mangoYOLO-kalman
    filter-hungarian algorithm method. Sensors 19(12), 2742 (2019) Article   Google
    Scholar   Zabawa, L., et al.: Detection of single grapevine berries in images
    using fully convolutional neural networks. In: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops, pp. 0–0 (2019) Google Scholar   Zbontar,
    J., Jing, L., Misra, I., LeCun, Y., Deny, S.: Barlow twins: self-supervised learning
    via redundancy reduction. In: International Conference on Machine Learning, pp.
    12310–12320. PMLR (2021) Google Scholar   Zhang, Z., Girdhar, R., Joulin, A.,
    Misra, I.: Self-supervised pretraining of 3D features on any point-cloud. In:
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10252–10263
    (2021) Google Scholar   Download references Author information Authors and Affiliations
    California Polytechnic State University, San Luis Obispo, CA, 93407, USA Michael
    Woodson & Jane Zhang Corresponding author Correspondence to Jane Zhang . Editor
    information Editors and Affiliations Faculty of Science and Engineering, Saga
    University, Saga, Japan Kohei Arai Rights and permissions Reprints and permissions
    Copyright information © 2023 The Author(s), under exclusive license to Springer
    Nature Switzerland AG About this paper Cite this paper Woodson, M., Zhang, J.
    (2023). Evaluating Self-supervised Transfer Performance in Grape Detection. In:
    Arai, K. (eds) Intelligent Computing. SAI 2023. Lecture Notes in Networks and
    Systems, vol 711. Springer, Cham. https://doi.org/10.1007/978-3-031-37717-4_68
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-37717-4_68
    Published 01 September 2023 Publisher Name Springer, Cham Print ISBN 978-3-031-37716-7
    Online ISBN 978-3-031-37717-4 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Sections Figures References Abstract Introduction Related Work Background
    Methods and Materials Results Conclusion References Author information Editor
    information Rights and permissions Copyright information About this paper Publish
    with us Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: (Lin et al., 2020)
  journal: Lecture Notes in Networks and Systems
  key_findings: The paper highlights the potential of using high-resolution cameras
    and computer vision algorithms for visual monitoring of crop growth, disease detection,
    and irrigation system performance in precision agriculture. It also discusses
    the potential for integrating these techniques with existing irrigation infrastructure
    and other precision agriculture technologies to enhance automation and efficiency.
  limitations: null
  main_objective: The main objective of the paper is to explore the use of advanced
    monitoring techniques, particularly high-resolution cameras and computer vision
    algorithms, for visual monitoring in precision agriculture.
  relevance_evaluation: The paper is highly relevant to the point in the literature
    review that focuses on the integration of high-resolution cameras and computer
    vision algorithms for visual monitoring in automated irrigation systems. It provides
    specific examples of how these technologies can be used to detect crop growth,
    diseases, and irrigation system performance, which aligns with the scope of the
    point. The paper also discusses the potential for integrating these techniques
    with existing irrigation infrastructure and other precision agriculture technologies,
    which is another key aspect of the point.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning-based
    object detection and segmentation
  title: Evaluating Self-supervised Transfer Performance in Grape Detection
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Bah, M. D., Hafiane, A., & Canals, R. (2020). CRowNet: Deep network
    for crop row detection in UAV images. IEEE Access, 8, 5189–5200. https://doi.org/10.1109/ACCESS.2019.2960873'
  authors:
  - Bah M.D.
  - Hafiane A.
  - Canals R.
  citation_count: '4'
  data_sources: Literature review
  description: Crop row detection is an important aspect of smart farming. An accurate
    method of crop row detection ensures better navigation of the robot in the field
    as well as accurate weed control between rows, and crop monitoring. Recent deep
    learning approaches have emerged as the indispensable methods for most computer
    vision tasks. Therefore, crop row detection using neural networks is currently
    the most frequently studied approach. However, these methods require a large amount
    of labeled data, which is not suitable for many situations such as agriculture,
    where labeling data is tedious and expensive. Unsupervised techniques such as
    graph-based represent a promising alternative to tackle such a problem. Indeed,
    the crop rows represent relationships between plants (relative position, co-occurrence…),
    that could be integrated as structured information with an unsupervised approach.
    However, little attention has been paid to graph-based techniques for crop row
    structures representation. In this paper we propose a new method based on hierarchical
    approach and unsupervised graph representation for crop row detection. The idea
    is to transform the field into a structured data where each plant can be linked
    to its neighbors according to their spatial relationships. Then, with a hierarchical
    clustering and a criterion of exploring the nodes of the graph we extract subgraphs
    that represent the aligned structures. We show that the graph matching technique
    can discard inconsistent structures such as weed aggregations. The results obtained
    show the validity of the proposed method, with higher performances.
  doi: 10.1016/j.eswa.2022.119478
  explanation: This study examines automated systems for actual irrigation management
    in real time. The authors investigate how these systems can help to use water
    resources more efficiently and increase agricultural productivity to meet the
    growing demand for food. The study also evaluates the current state of end-to-end
    automated irrigation management systems that integrate IoT and machine learning
    technologies. Researchers strive to identify inefficiencies and offer solutions
    for seamless integration across the automated irrigation management system for
    fully autonomous, scalable irrigation management.
  extract_1: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)
  extract_2: Furthermore compared to the Hough transform, about 45% of processing
    time is saved.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract BetaPowered by GenAIQuestions answered in this article
    Keywords 1. Introduction 2. Method 3. Experiment 4. Conclusion CRediT authorship
    contribution statement Declaration of Competing Interest Data availability References
    Show full outline Cited by (5) Figures (13) Show 7 more figures Tables (2) Table
    1 Table 2 Expert Systems with Applications Volume 216, 15 April 2023, 119478 Hierarchical
    graph representation for unsupervised crop row detection in images Author links
    open overlay panel Mamadou Dian Bah a, Adel Hafiane b, Raphael Canals a Show more
    Share Cite https://doi.org/10.1016/j.eswa.2022.119478 Get rights and content Highlights
    • Computer vision system and unsupervised approach for crop image analysis. •
    Parallel structures detection in UAV images. • Transformation of the crop field
    into a graph structure. • Unsupervised hierarchical analysis to group rows as
    distinct communities in a graph. • Use of SLIC and an autoencoder during segmentation
    of crop images. Abstract Crop row detection is an important aspect of smart farming.
    An accurate method of crop row detection ensures better navigation of the robot
    in the field as well as accurate weed control between rows, and crop monitoring.
    Recent deep learning approaches have emerged as the indispensable methods for
    most computer vision tasks. Therefore, crop row detection using neural networks
    is currently the most frequently studied approach. However, these methods require
    a large amount of labeled data, which is not suitable for many situations such
    as agriculture, where labeling data is tedious and expensive. Unsupervised techniques
    such as graph-based represent a promising alternative to tackle such a problem.
    Indeed, the crop rows represent relationships between plants (relative position,
    co-occurrence...), that could be integrated as structured information with an
    unsupervised approach. However, little attention has been paid to graph-based
    techniques for crop row structures representation. In this paper we propose a
    new method based on hierarchical approach and unsupervised graph representation
    for crop row detection. The idea is to transform the field into a structured data
    where each plant can be linked to its neighbors according to their spatial relationships.
    Then, with a hierarchical clustering and a criterion of exploring the nodes of
    the graph we extract subgraphs that represent the aligned structures. We show
    that the graph matching technique can discard inconsistent structures such as
    weed aggregations. The results obtained show the validity of the proposed method,
    with higher performances. Previous article in issue Next article in issue Questions
    answered in this article BetaPowered by GenAI This is generative AI content and
    the quality may vary. Learn more. What properties of the field are of interest
    in crop row detection? How are hierarchical graphs advantageous in crop row detection?
    What does the ground truth represent in crop row detection? What is the benefit
    of highlighting the crop row pattern? How do Bah, Hafiane, and Canals detect crop
    rows? Keywords Graph representationHierarchical graph representationCrop row detectionAutoencoderComputer
    vision 1. Introduction For many generations, industries including robotics and
    tractor manufacturing, …have been constantly innovating to make agricultural equipment
    more and more autonomous for sowing, harvesting, weeding, etc. These technologies
    are helping farmers to move from an agriculture where everything is applied uniformly
    to a much more targeted agriculture. This new farming technique is commonly referred
    to as precision agriculture. The main goal of precision agriculture is to implement
    the right management practice in order to allocate the appropriate quantity of
    inputs such as fertilizers, herbicides, seed, fuel, etc. to the right place and
    time (McBratney et al., 2005, Pierce and Nowak, 1999). However, all these processes
    require accurate guidance with respect to crop rows. Some studies showed the advantages
    of the automation of crop row detection for robot navigation (Burgos-Artizzu et
    al., 2011, Rovira-Más et al., 2005, Winterhalter et al., 2018), and for the detection
    of weeds between rows (Gée et al., 2008, Peña et al., 2013). Existing literature
    indicates that different imaging-based methods have been used for detecting crop
    rows (Ronchetti, Mayer, Facchi, Ortuani, & Sona, 2020). For instance the Hough
    transform (Hough, 1962) is one of the most used machine vision methods for identifying
    crop rows (Basso and de Freitas, 2020, Gée et al., 2008). It was specifically
    used to guide mobile robots in sugar beet and rapeseed fields in Åstrand and Baerveldt
    (2005). With extensive field tests the authors demonstrated that the system was
    accurate and fast enough to control a weeder and a mobile closed-loop robot with
    a standard deviation of the position of 2.7 and 2.3 cm, respectively. The Hough
    transform was designed to detect curves and straight lines in Leemans and Destain
    (2006). Bakker et al. (2008) applied the Hough transform to extract crop rows
    from images acquired by a robot using a specific angle interval. Jones, Gée, and
    Truchetet (2009) detected crop rows by modeling agronomic images taken from a
    virtual camera placed in a virtual field with and without perspective. Ji and
    Qi (2011) proposed RHT (Random Hough Transform) (Xu & Oja, 1993) to detect crop
    rows. The specificity of RHT is that the Hough transform is applied to randomly
    selected vegetation pixels. Furthermore compared to the Hough transform, about
    45% of processing time is saved. Bah, Hafiane, and Canals (2017) detect crop rows
    by using Hough transform and Simple Linear Iterative Clustering (SLIC) on the
    skeleton of the crop rows. Other authors have preferred to use linear regression
    to detect crop rows, assuming that each crop row is a cloud of points that can
    be fitted by a line. For instance, Søgaard and Olsen (2003) located the crop rows
    in a barley field using a weighted linear regression. Hague, Tillett, and Wheeler
    (2006) determined the position and orientation of crop rows by applying the non-linear
    version of the Kalman filter called extended Kalman filter (Julier & Uhlmann,
    2004). Montalvo et al. (2012) proposed to use linear regression to detect crop
    rows in a maize field with a high weed density. A combination of clustering and
    linear regression methods was used for the automatic detection of crop rows based
    on multiple regions of interest (Jiang, Wang, & Liu, 2015). The analysis of spots
    or blobs is a fundamental technique in computer vision, based on the analysis
    of image regions that present certain visual coherence. Fontaine and Crowe (2006)
    relied on the direction and center of gravity of the different blobs to propose
    a crop row detection method. Burgos-Artizzu et al. (2011) proposed different methods
    to detect crop rows using a combination of the center of gravity, the direction
    of blob regions and the direction of movement of a tractor with a camera mounted
    on it. Peña et al. (2013) developed an object-by-object image analysis (OBIA)
    procedure on a series of UAV images for the automatic discrimination of crop rows
    and weeds in a corn field. They found that the process is strongly influenced
    by the presence of weeds in close proximity or within crop rows. Template matching
    and green pixels accumulation are used (Li, Zhang, Du, & He, 2020). In Vidović,
    Cupec, and Hocenski (2016) crop row detection by global energy minimization was
    proposed. In Kise et al., 2005, Rovira-Más et al., 2008 the authors located the
    crop rows with an altimetric map of the field. But this method is generally used
    in cases where the plant heights are sufficiently large. A common drawback of
    the above methods is the need for information including the number of crop rows
    and the distance between crop rows. In addition, when using the Hough transform
    method, it is difficult to detect peaks in a robust way by avoiding spurious peaks,
    especially in complex scenes. The strip features cannot usually detect crop rows
    accurately in complex conditions such as high pressure images or missing plants.
    Recently, deep learning algorithms have proven to be effective for classification
    and semantic segmentation (Bah et al., 2018, Gai et al., 2020, Kerkech et al.,
    2018, Lottes et al., 2020), but very few have been designed to study the spatial
    relationship of pixels on the rows and columns of an image. In Pang et al. (2020),
    an instance segmentation algorithm (MaxArea Mask Scoring RCNN) is developed for
    detecting the crop-rows. A new deep localization network for intra-row rice detection
    at the single plant level in a paddy field is proposed in Huang et al. (2020).
    Method called CRowNet which uses a convolutional neural network (CNN) and the
    Hough transform to detect crop rows in images taken by an unmanned aerial vehicle
    (UAV) is proposed in Bah, Hafiane, and Canals (2020). Despite the great success
    of CNNs on regular Euclidean data such as images (2D grids) and texts (1D sequences),
    they suffer in solving problems in data with non-Euclidean characteristics (e.g.
    irregular structure). Given that the latter can be considered as graph instances,
    more recently work involving graph convolutional neural networks (GCNs) have begun
    to emerge (Hong et al., 2020, Ma et al., 2019). This increasing interest is attributed
    to two main factors: the increasing amount of non-Euclidean (the irregular) data
    in real-world applications and the limited performance of CNN when dealing with
    such data. Deep neural network architecture was proposed with the integration
    of graphs to solve issues related to the identification and refinement of a plantation-line
    position in remote sensing imagery (Gonçalves et al., 2021). In Ouyang and Li
    (2021) authors combine the advantages of deep semantic segmentation network (DSSN)
    and GCN to perform the segmentation of urban aerial imagery, identifying features
    like vegetation, pavement, buildings, water, vehicles, and others. Graph Convolutional
    Networks were applied to perform land cover classification in hyper-spectral images
    (Hong et al., 2020). However, a major limitation of GCN architectures is that
    they propagate information only through the edges of the graph and are unable
    to infer and aggregate information hierarchically (Ying et al., 2018). Lastly,
    there are some recent works that learn hierarchical graphs which is a very expressive
    representation of data, as it takes into account the relationship between instances
    of the graph, rather than treating them independently. In hierarchical graph,
    a node itself is a graph instance. In Mi and Chen (2020) a Hierarchical Graph
    network is proposed for visual relationship detection. In Li et al. (2019) authors
    study graph classification in a hierarchical graph, which predicts the class label
    of graph instances in a hierarchical graph. Although hierarchical methods have
    brought a great improvement in the representation of graphs, the use of supervised
    learning-based method is an limitation of this approach since it requires a large
    amount of labeled data. In this paper, we propose a hierarchical and unsupervised
    graph representation for crop row detection, by considering the field as a set
    of points where each point can be a plant or an aggregation of touching plants
    (vertex). Therefore, the field is transformed into a structure where plants are
    connected according to their spatial relationship (edges). The aim is to take
    advantage of the efficient representation of the data structure by graphs to extract
    geometric properties of the field. Thus, we exploit the ability of graphs to group
    nodes that share the same properties. Here we are interested in the properties
    of the alignment of plants in rows and the parallelism of these rows. To capture
    these two properties, we use a concept of hierarchical representation inspired
    by the field of natural language processing, by analogy, we assume that plants
    are words forming sentences (the crop rows) which together constitute a paragraph
    or document (the field). Following this concept and with unsupervised manner,
    plants are first detected and then grouped into a structure to form the graph.
    Then the nodes of the graph that constitute potential rows are grouped in the
    same communities according to their alignment. The communities are analyzed, then
    grouped according to a parallelism criterion to detect all the crop rows (Fig.
    1). The following summarizes the main contributions of this paper: Download :
    Download high-res image (718KB) Download : Download full-size image Fig. 1. Agricultural
    field presented as a graph. A graphical representation of a crop field, which
    considers the field as a set of points where each point can be a plant or an aggregation
    of plants that touch each other (vertex) and are connected by their spatial relationship
    (edges). • Unsupervised segmentation algorithm combining SLIC and autoencoder
    to efficiently extract vegetation and highlight crop row patterns • Transformation
    of the crop field into a graph structure • Unsupervised hierarchical analysis
    to group rows as distinct communities in a graph. This paper is organized as follows.
    Section 1 gives the general introduction. The details of materials and methods
    are presented in Section 2. Section 3 shows the results and discussions of the
    proposed algorithm. The conclusions are presented in Section 4. 2. Method This
    section describes the details of the materials and methods applied in this paper.
    This method is divided into 2 main steps (Fig. 2). In the first step the non-essential
    parts of the image, i.e., the background or the soil, are subtracted and then
    the points to be used as nodes of the graph are extracted. In the second step
    the graph is constructed, and the hierarchical method is applied to detect the
    crop rows. Download : Download high-res image (389KB) Download : Download full-size
    image Fig. 2. The main steps of the proposed method. 2.1. Points of interest detection
    with CNN and SLIC This phase is essential to build the graph. Its purpose is to
    extract points that are sufficiently representative of the positions of the plants
    in the image. These points can be the central pixel of each plant or simply the
    point representing an aggregation of plants very close to each other. Here, image
    is split into small regions. Based on low-level information, these regions are
    also called superpixels. Compared to the image representation by pixel, the superpixel
    is more consistent with human visual cognition and contains less redundancy. Download
    : Download high-res image (289KB) Download : Download full-size image Fig. 3.
    Architecture of the autoencoder. Both layers of the encoder have the same square
    kernel . The number of nodes of each layer is 16 and 4 for the first and second
    layer respectively. The original image shape is obtained with two layers of 2D
    transposed convolution operator. In this paper we propose to exploit a combination
    of CNN (autoencoder) (Fig. 3) and Simple Linear Iterative Clustering (SLIC) (Achanta
    et al., 2011) to efficiently extract superpixels. The autoencoder is trained to
    preprocess the image by removing noise and smoothing the image. Thus the color
    of vegetation pixels is accentuated, while those of non-vegetation pixels are
    attenuated. The interest of this preprocessing is the highlighting of the crop
    row pattern. The CNN model used is an autoencoder which aims to create an image
    with the same characteristics as the input image but with a particular attention
    to the vegetation (Fig. 4). Autoencoders are special neural networks that have
    the same number of neurons on their input and output layers. Their aims are to
    compress representations and preserve essential information for reconstructing
    the input. They are composed of two symmetric parts: encoder and decoder. The
    chosen autoencoder architecture has only two convolution layers within the encoder,
    because large networks require massive computational power and long training time
    to achieve the highest possible accuracy. The convolution kernel shape (3 × 3)
    is the same for both layers. Each convolution layer is followed by a max-pooling.
    This operation subsamples the input characteristic map by a factor of 2. Max-pooling
    enhances the translation invariance on small spatial shifts of the input image.
    Two layers are used in the decoder step. Decoders consist in transforming semantically
    the discriminating characteristics learned by the encoder at a lower resolution
    into a higher resolution. The image to reconstruct by the autoencoder (ground
    truth) is obtained as follows: • Input image is segmented with Otsu thresholding
    applied on ExG. • SLIC is applied on the segmented image to create superpixels
    • Superpixels are used to smooth the original image; each pixel is replaced by
    the average value of the superpixel to which it belongs. The advantage of replacing
    these 3 steps by an autoencoder is that for the preprocessing of new images, it
    is not necessary to choose a particular number of superpixels to be generated
    depending on the resolution and size of the image. The output of the autoencoder
    is segmented with the Excess Green (ExG) index (Eq. (1)) and the nonparametric
    and unsupervised automatic threshold selection method called Otsu thresholding
    (Otsu, 1979) since this approach has proven to be robust and simple. (1) where
    r, g and b are the normalized chromatic coordinates. In the last image obtained,
    all the vegetation pixels have the same value. It means after segmentation we
    have two kind of pixels the white ones which are vegetation and the black ones
    that represent the background. The advantage of this procedure is to create homogeneous
    superpixels which depend on the crop rows structure and not on the vegetation
    color variations. According to Fig. 5 the size of the superpixels must be chosen
    by making a trade-off. A size that is too small will result in more than one superpixel
    per plant and the edges will be small and the geometric representation of the
    crop rows will be more complex. Conversely, a superpixel that is too large disturbs
    the overall orientation of the crop rows. Fig. 6 shows an example of vegetation
    segmentation and the extraction of regions centers. We propose to use 16 as the
    optimal size of the superpixels to generate. Superpixels of a shape (16 × 16)
    are generated with the SLIC algorithm on segmented image. The centers of the superpixels
    are defined as the points of interest. Download : Download high-res image (691KB)
    Download : Download full-size image Fig. 4. Image without auto-encoder in left
    and image with autoencoder in right. The yellow color is the boundaries after
    vegetation segmentation. SLIC is a clustering-based method which creates a local
    grouping of pixels based on their spectral values defined by the values of the
    CIELAB color space and their spatial proximity (Dos Santos Ferreira, Matte Freitas,
    Gonçalves da Silva, Pistori, & Theophilo Folhes, 2017). A higher value of compactness
    makes superpixels more regularly shaped. A lower value makes superpixels adhere
    to boundaries better, making them irregularly shaped. Download : Download high-res
    image (1MB) Download : Download full-size image Fig. 5. Superpixels of different
    sizes generated by the SLIC method in red. (a), (b) and (c) represent respectively
    superpixels of size 8, 16 and 32. Download : Download high-res image (645KB) Download
    : Download full-size image Fig. 6. (a) Superpixel generated by SLIC in yellow,
    the size of superpixel is about 16 × 16. (b) Yellow circles represent the superpixels
    center (Point of Interest or vertices). Thus each centroid became a “node”. Central
    pixel of superpixel is now denoted . 2.2. Hierarchical graph representation This
    section describes the creation of the graph based on the results of the previous
    section. It also describes how the hierarchical representation of the graph is
    established. Graphs are popular modes of presenting complex relationship data.
    Graph is modeled using an adjacency matrix that can be weighted or unweighted.
    It is represented by , where ( ) is the set of nodes with elements, and is the
    set of edges of . 2.2.1. Graph construction The graph is constructed by applying
    triangulation (Rose, 1970). Triangulation is a well-known technique to cut a geometric
    shape (a plane, a polygon) into a collection of triangles (meshes). The Delaunay
    triangulation (Lee & Schachter, 1980) has been used to generate a graph where
    each vertex or node corresponds to the points of interest already extracted (superpixels
    centroid). Thus each centroid became a “node”. Central pixel of superpixel is
    now denoted (Fig. 6). Delaunay triangulation for a given set of centroid pixel
    is a triangulation DT( ) such that no point in is inside the circumcircle of any
    triangle in DT( ). It has the advantage of being unsupervised and find the minimum
    angle of all the angles of the triangles in the triangulation; they tend to avoid
    sliver triangles. Therefore, each edge of the graph form an angle and since each
    image has a global orientation obtained from the rows direction edges are weighted
    according to Eq. (2). This weighting allows us to identify the edges close or
    farther from the global orientation. The angle is obtained by applying Fourier
    transform (Josso, Burton, & Lalor, 2005) method. The principle of the algorithm
    is first to calculate the image Fourier transform modulus and then to characterize
    the distribution of this spectrum around the zero frequencies by Principal Component
    Analysis. (2) where represents the angle of edge ( ) formed between vertices and
    . Let and be two vectors that represent respectively the coordinates of vertices
    and in the image, is obtained by Eq. (3). (3) Once the graph is constructed, the
    next step is rows detection. The next sections, details this procedure, and Fig.
    7 gives an overview of the main steps. Download : Download high-res image (1MB)
    Download : Download full-size image Fig. 7. Flowchart of the proposed method.
    2.2.2. Community detection: Potential crop rows The method requires the use of
    subgraph or community analysis, which means grouping graph nodes into clusters.
    Two types of analysis are applied here: the first exploits the global orientation
    of crop rows obtained and the second exploits the parallel layout of crop rows.
    The last analysis allows us to identify the communities that are crop rows. The
    term community will refer to a subgraph in the remainder of this article. The
    created graph is processed to retain only the edges that follow an orientation
    around the global orientation computed beforehand by the Fourier transform. This
    enables us to focus on the edges that are likely to form the crop rows. The effect
    of this operation is the subdivision of the graph into communities (see Fig. 7).
    One of the key advantages of this approach are to determine the nodes that share
    the same geometrical properties. The edges with an orientation close to the global
    orientation are retained. These edges are identified by applying a threshold (Eq.
    (4)) which avoids considering edges between plants that are on parallel rows.
    Communities are created by assigning a label to each subgraph into distinct sets
    using the Louvain algorithm (Blondel, Guillaume, Lambiotte, & Lefebvre, 2008).
    (4) Louvain’s algorithm is a hierarchical community extraction algorithm applicable
    to large networks. It relies on a greedy procedure: from any partition of the
    vertices (generally the partition in singletons), the algorithm attempts to increase
    the value of the modularity while moving the vertices of their community towards
    any other neighbor. In much details, the algorithm computes the gain of modularity
    (Eq. (5)) obtained by adding vertex to community . Whereas modularity is an optimization
    function that allows to evaluate the presence of an edge between two vertices
    of an undirected network by comparing it to the probability of having such an
    edge in a random model following the same degree distribution as the original
    network. Formally, the modularity of a partition of an undirected graph is defined
    in Eq. (6). (5) (6) Where is the number of edges of G, represents the weight of
    the edge between and (set to 0 if such an edge does not exist), is the degree
    of vertex (i.e., the number of neighbors of node ), is the sum of all edge weights
    for vertices within the community (including edges which link to other communities).
    is the community to which node belongs and the is defined as 1 if , and 0 otherwise.
    Furthermore, depending on the field, not all nodes are useful (presence of weeds)
    since it is not excluded that these nodes have an impact on the final result.
    Moreover, it is possible that crop rows are broken down into sub-communities or
    regions. As crop rows generally follow a linear property, the Pearson correlation
    coefficient is applied to form communities ( ). Two communities and are merged
    if the Pearson correlation coefficient of the elements present in these two communities
    is close to 1. At the end of this process, potential crop rows are obtained. Next
    step, once the communities are analyzed and merged, is to detect and remove the
    communities formed by inter-row weeds. 2.2.3. Select relevant communities: Crop
    rows The selection of the relevant communities is based on the assumption that
    a community is a crop row when three criteria inside a graph-community are satisfied
    : (i) orientation, (ii) parallelism, (iii) distances between neighboring communities.
    At this level, each node of the initial graph belongs to a community which, for
    the moment, has no neighbors. Thus, two communities and are connected when at
    least one node belonging to has a connection with belonging to and/or vice versa
    (this step can be observed in the second part of Fig. 7). In the previous section
    the orientation condition was used to emphasize the orientation according to global
    orientation of crop rows. Now, the focus this time is on the edges with an orientation
    perpendicular to the global orientation, with the aim of detecting the parallel
    crop rows. Two communities are connected according to Eq. (7) and the edges are
    weighted according to the number of neighbors and distance between the two communities
    Eq. (8). As a community can have more than one neighbor the weights of the edges
    are replaced by their inverse, that involve edges between close communities are
    smaller than those between distant ones in terms of distance in the image. Hence,
    priority is given to the most distant communities, because we consider that the
    closer the communities, the higher the risk of having a weed community. The resultant
    graph is named , it is a graph representation of the existing relationships between
    the communities. (7) The value chosen for is , which avoids considering the edges
    between the same crop rows. (8) and represent the coordinates of node in the image.
    To identify the communities corresponding to the crop rows, a two-step process
    is used. In the first one the largest subgraph of is extracted. In the second
    step, the longest path in number of nodes and the shortest in distance is detected
    using Dijkstra algorithm. Given a graph and a source vertex in the graph, Dijkstra
    algorithm find the shortest paths from the source to all vertices in the given
    graph. Thus all nodes on that path represent the crop rows. For instance in Fig.
    8 the nodes marked in red are not included in the shortest path. As a reminder
    those nodes are a representation of a subgraph or community. Download : Download
    high-res image (367KB) Download : Download full-size image Fig. 8. Example of
    selection of relevant communities. Communities identified as inter-row weeds are
    in red. 3. Experiment In this section we evaluate our method both qualitatively
    and quantitatively. We also compare it to state-of-the-art methods. Experiments
    were conducted on 154 images (480 × 360 pixels) from public datasets number 000,
    001 and 002 (Sa et al., 2018). These images were acquired in a beet field by the
    Parrot RedEdge-M multispectral sensor 10 m from the ground. To account for crop
    row irregularities and to avoid the impact of weed pressure in the test data,
    we built the ground truth manually with the Matlab labeling tool (Image labeler).
    The labeling consisted of creating a mask around each row. The weed infestation
    rate (WIR) in these images varies from low to high. The WIR is the proportion
    of vegetation pixels considered as weeds in an image. According to the WIR classification,
    we have 45 images without weeds (WIR  5%), 21 images with a low rate of infestation
    ([5% - 15%[), 50 images with a moderate infestation rate ([15% - 35%[) and 38
    heavily infested (WIR  35%). These infestation rates were computed from the ground
    truth of the images available in the dataset. In total there were 1329 crop rows
    to detect ( ). To train the auto-encoder, the number of samples used was 816 images
    acquired in a bean field; 80% of the samples were used for training and 20% for
    validation. In Fig. 9 we presented an example of images obtained by using images
    smoothed with superpixels of size 10 and 64 and the corresponding results. The
    Adam optimizer was used, the learning rate was set to 0.001. Measuring detection
    performance, is not quite straight-forward because it is challenging to get the
    true position and direction for the center lines of crop rows due to natural variations
    in the crop growth stage. In Fig. 10 we can see that the ground truth is not a
    simple line on a row but rather a strip encompassing the plants of the crop row.
    This is because in crop row, plants are not perfectly aligned, their alignment
    does not fit exactly the strip or band model. Fig. 11(b) shows the connection
    of communities after the application of the proposed method on the image of Fig.
    11(a). Fig. 11(c) shows the original image and the labels of the selected communities.
    We notice that only relevant communities are retained and they do not have the
    same size since the crop rows are not all identical. Download : Download high-res
    image (765KB) Download : Download full-size image Fig. 9. (a) the original image.
    (b) Image segmented with ExG and Otsu thresholding. (c) and (d) represent respectively
    images smoothed with superpixels of size 10, 64. (e) and (g) are respectively
    the output of autoencoder trained with images smoothed with superpixels of size
    10 and 64. (f) and (h) are respectively the segmented version of (e) and (g).
    Download : Download high-res image (1MB) Download : Download full-size image Fig.
    10. (a) represents the input image and (b) the graph with the selected communities
    as crop rows. (c) the contour of the ground truth is marked in blue and the text
    represents the label of the selected communities. (d), (e) and (f) represent the
    step used to assess the method. In (d) we have the nodes which detect the crop
    row presented by a red dot. The red line is the crop row detection obtained with
    RANSAC on the nodes (red dots in (e). (f) Overlap after dilation of detected crop
    row and the skeleton of the ground truth. 3.1. Evaluation criteria The method
    is assessed at two levels. The first level consists of evaluating whether all
    nodes in the selected communities belong to crop rows (Fig. 10(d)). The second
    level analyzes the ability of a community to detect an entire crop row (Fig. 10(e)
    and Fig. 10(f)). The overlap coefficient ( ), or Szymkiewicz–Simpson coefficient
    (Vijaymeena & Kavitha, 2016) is computed for the assessment. This metric indicates
    whether the detected community is entirely included in the ground truth or not.
    The overlap coefficient is a similarity measure that is related to the Jaccard
    index. It measures the overlap between two sets as indicated in the equation (Eq.
    (9)). Given the ground truth of a crop row ( ), corresponds to the set of nodes
    belonging to the community that intersects with it. The is equal to 1 if all the
    nodes of the community are located inside . Since all rows are not all detected
    in same we propose to apply a threshold on it and compute multiple metrics in
    order to have a robust analysis. The metrics computed at each threshold are: the
    , the , the score and the Accuracy. reflects the ability to reveal the needed
    information (Eq. (11)), (Eq. (10)) indicates the correctness of the detected results,
    and the score indicates the balance between and (Eq. (12)). It is the harmonic
    mean of precision and recall. These indices were computed for each crop row. (Eq.
    (13)) is a metric that generally describes how the model performs across all classes.
    It is useful when all classes are of equal importance. It is calculated as the
    ratio between the number of correct detection to the total number of detections.
    We need to compute True Positive ( ), False Positive ( ), and False Negative (
    ) for metrics. For rows detection, corresponds to the number of rows correctly
    detected, while corresponds to the number of detection that are not crop rows
    and corresponds to the number of crop rows that were not detected by the method.
    (9) (10) (11) (12) (13) (14) 3.2. Results and discussion For a threshold of a
    confusion matrix is presented in Table 1. We notice that on the confusion matrix
    (Table 1) the method has overall missed ( ) 73 and detected 1289 crop rows with
    1257 good detection ( ) and 17 overdetection ( ). Miss detection ( ) occurs in
    most cases on the rows located on the edges of the image and over-detection (
    ) is a consequence of the presence of weeds between crop rows. In addition, the
    average score of the 1257 true detected crop rows is 0.983. Download : Download
    high-res image (904KB) Download : Download full-size image Fig. 11. Examples of
    crop rows detection after a hierarchical graph representation of the field. To
    effectively assess the method we varied the threshold from 0 to 1 and the result
    quantified through different metrics. Thus, for each value of , , , and are computed.
    For , the Recall, Precision, F1, Accuracy were respectively 0.945, 0.987, 0.965
    and 0.946. In Fig. 13 we remark that the performance is affected when is higher
    than 0.8. This analysis shows that the retained communities have nodes which are
    well located inside the crop row. We also evaluate the ability of given graph
    to detect an entire crop row. Since the communities are made up of points, algorithms
    such as Ransac, Hough transform and the linear regression were applied to the
    corresponding points (nodes) in each community to form a straight line. The mean
    OS obtained for RANSAC, the Hough transform and the linear regression are respectively
    0.971, 0.966 and 0.968. In addition, is computed to efficiently evaluate the overlap
    between the rows obtained with RANSAC, Hough transform and linear regression.
    , also called the Jaccard similarity coefficient Eq. (14) considers both the false
    positives and the missed values for each class. The is calculated as a ratio of
    the area of overlap to the area of the union between ground truth ( ) and the
    prediction. We thinned all the rows and each row is represented by its skeleton.
    Assuming that the thinning operation returns skeletons which represent the center
    line of each row, we dilate the skeletons to obtain the same crop row width as
    that measured in the field. Based on a ground measurement in the beet field, the
    size of a crop plant varies from 15 to 20 pixels. We therefore dilate the skeleton
    of detected rows with a square structuring element of 15 × 15 pixels on beet images.
    Fig. 10(f) presents the result of the thinning and the dilation. For the mean
    of IoU for RANSAC, Hough transform and linear regression are respectively 0.833,
    0.813 and 0.832. These results show that the community has good performance for
    the detection of entire crop rows. In Fig. 11 we show examples of crop row detection
    after hierarchical graph representation of the field. We also applied crop row
    detection method without the autoencoder and with different superpixel sizes;
    for a size of 16 × 16 the mean of IoU for RANSAC, Hough transform and linear regression
    are respectively 0.778, 0.747 and 0.781. We can notice that the use of the autoencoder
    has improved the result about 6%. In Table 2 we can see that the size of the superpixels
    has an impact on the results. A very large size of the superpixels makes them
    less meaningful. The result shows that the choice of superpixel size must meet
    a trade-off between the overall structure of the field and the representativeness
    of the plants or their aggregation in the crop rows. Large superpixels would give
    a non-representative graph of the field while very small ones would increase the
    number of vertices and even disturb the global representativeness of the field
    (Fig. 12). According to the ground truth the approximate size of a plant varies
    from 15 to 20 pixels; this may explain the results. The number of crop rows in
    a field can sometimes be considered as an indicator of good plant growth and good
    yield. Consequently, a curve of the according to the segmentation quality ( )
    required for the score is plotted. Fig. 13(b) shows the performance of the methods
    according to the obtained in each crop row. We notice that due to the irregularity
    of the crop rows shape, it is difficult to achieve a 100% overlap between the
    ground truth and the prediction. In addition, the accuracy decreases when is superior
    to 0.65. For a value of , 85.1%, 88.4% and 88.2% of crop crows are detected respectively
    by Hough Transform, RANSAC and linear regression. Download : Download high-res
    image (834KB) Download : Download full-size image Fig. 12. Graph generated with
    superpixels of different sizes generated by the SLIC method in red. (a), (b),
    (c) and (d) represent respectively superpixels of size 8, 16, 32 and 64. Compared
    to Gonçalves et al. (2021) which exploits the visual features of the plants here
    the only information exploited to analyze the edges is the global orientation.
    Table 1. Confusion matrix obtained by considering that . Confusion matrix obtained
    by considering that a crop row is detected if the OS score 0. That is to say that
    it is enough to be in contact with only one node of a community to be considered
    as a detected crop row. Empty Cell Predicted Empty Cell Positive Negative Actual
    Positive 1257 17 Negative 73 0 Download : Download high-res image (342KB) Download
    : Download full-size image Fig. 13. (a) , , and according to . (b) Mean of IoU
    of the three fitting method used according to the threshold . Table 2. Comparison
    with state of art methods. Graph-Ransac, Graph-Hough and Graph-Linear-Regression
    represent respectively the three methods applied on our graph results, Ransac,
    Hough transform and linear regression. 16, 32 and 64 represent the size of the
    superpixels where the vertices are extracted. Method Mean IoU Graph-Ransac (autoencoder,
    16) 0.833 Graph-Hough (autoencoder, 16) 0.813 Graph-Linear-Regression (autoencoder,
    16) 0.832 Graph-Ransac (no autoencoder, 16) 0.778 Graph-Hough (no autoencoder,
    16) 0.747 Graph-Linear-Regression (no autoencoder, 16) 0.781 Graph-Ransac (no
    autoencoder, 32) 0.810 Graph-Hough (no autoencoder, 32) 0.781 Graph-Linear-Regression
    (no autoencoder, 32) 0.810 Graph-Ransac (no autoencoder, 64) 0.403 Graph-Hough
    (no autoencoder, 64) 0.433 Graph-Linear-Regression (no autoencoder, 64) 0.404
    CRowNet (Bah et al., 2020) 0.832 CR-Hough-SLIC (Bah et al., 2017) 0.766 CR-Hough
    (Jones et al., 2009) 0.802 Furthermore, the methods most used to detect crop rows
    in aerial images use the Hough transform and geometric information of the field
    (inter-row distance, global orientation, etc.). We therefore compared our method
    to two methods based mainly on the Hough transform that we will call CR-Hough
    (Gée et al., 2008, Jones et al., 2009) and CR-Hough-SLIC (Bah et al., 2017). The
    first method, CR-Hough consists in detecting crop rows using known information
    such as the global orientation of crop rows and the inter-row distance. The second,
    CR-Hough-SLIC consists of applying the Hough transform on the vegetation skeleton
    and the superpixels generated by SLIC to detect crop rows without knowing the
    inter-row distance. However, for an image with crop rows, at least + 1 Hough transforms
    will be computed; the first one corresponds to the Hough transform computed on
    all crop row skeletons and the remaining correspond to the Hough transform of
    each row. These methods are evaluated according to Bah et al. (2020). On the beet
    images for CR-Hough, the theoretical value used is for the global orientation,
    the estimated inter-row distance is 50 pixels, and the row width is 20. The number
    of superpixels applied for CR-Hough-SLIC is 0.1% of the pixels present in the
    image. Given the success of the CNN in detecting crop rows we also used the CRowNet
    (Bah et al., 2020) method in our comparison. In Table 2 we noticed that the proposed
    method applied with RANSAC provides a better result. However, this result is very
    close to the one of CRowNet. Thus, we have shown that with a graph representation
    and hierarchical approach to analyze this graph, crop rows can be effectively
    detected in images without perspectives with relatively parallel crop rows with
    constant inter-row distance. 4. Conclusion The purpose of this research work was
    to exploit the principle of the ability of graph structure representation to detect
    crop rows in UAV images. The proposed method highlights the advantages of hierarchical
    graphs in a domain where the notion of relationship is important (i.e. plants
    are sown by row). The subgraph or community partition allowed us to study the
    linear properties of the crop fields and keep the relevant rows by exploiting
    the parallelism of the communities. We showed that the hierarchical representation
    graphs are as suitable as an unsupervised approach for crop rows detection. We
    noticed that the communities (sub-graphs) obtained were appropriate for the detection
    of whole crop rows with the use of the Hough transform, RANSAC or linear regression.
    Approximately 85% of the crop rows were detected with the Hough transform and
    88% with RANSAC and linear regression. This study considered only linear crop
    properties in drone images. Further research could explore non-linear structures
    with hierarchical graph for other types of crops or applications. We also plan
    to combine the results with CRowNet to obtain a more robust method. CRediT authorship
    contribution statement Mamadou Dian Bah: Conceptualization, Methodology, Software,
    Writing – review & editing, Writing – original draft preparation, Writing – original
    draft, Visualization, Investigation, Validation. Adel Hafiane: Conceptualization,
    Supervision, Writing – original draft preparation, Project administration, Writing
    – review & editing, Methodology. Raphael Canals: Conceptualization, Supervision,
    Writing – original draft preparation, Project administration, Writing – review
    & editing. Declaration of Competing Interest The authors declare that they have
    no known competing financial interests or personal relationships that could have
    appeared to influence the work reported in this paper. Data availability Data
    will be made available on request. References Achanta et al., 2011 Achanta R.,
    Shaji A., Smith K., Lucchi A., Fua P., Süsstrunk S. SLIC superpixels compared
    to state-of-the-art superpixel methods IEEE Transactions on Pattern Analysis and
    Machine Intelligence, 34 (11) (2011), pp. 2274-2282, 10.1109/tpami.2012.120 Google
    Scholar Åstrand and Baerveldt, 2005 Åstrand B., Baerveldt A.-J. A vision based
    row-following system for agricultural field machinery Mechatronics, 15 (2) (2005),
    pp. 251-269, 10.1016/j.mechatronics.2004.05.005 View PDFView articleView in ScopusGoogle
    Scholar Bah et al., 2018 Bah, M. D., Dericquebourg, E., Hafiane, A., & Canals,
    R. (2018). Deep learning based classification system for identifying weeds using
    high-resolution UAV imagery. In Computing conference 2018. Google Scholar Bah
    et al., 2017 Bah M.D., Hafiane A., Canals R. Weeds detection in UAV imagery using
    SLIC and the hough transform 2017 seventh international conference on image processing
    theory, tools and applications, IEEE (2017), pp. 1-6, 10.1109/IPTA.2017.8310102
    View in ScopusGoogle Scholar Bah et al., 2020 Bah M.D., Hafiane A., Canals R.
    CRowNet: Deep network for crop row detection in UAV images IEEE Access, 8 (2020),
    pp. 5189-5200, 10.1109/ACCESS.2019.2960873 View in ScopusGoogle Scholar Bakker
    et al., 2008 Bakker T., Wouters H., van Asselt K., Bontsema J., Tang L., Müller
    J., et al. A vision based row detection system for sugar beet Computers and Electronics
    in Agriculture, 60 (1) (2008), pp. 87-95, 10.1016/j.compag.2007.07.006 View PDFView
    articleView in ScopusGoogle Scholar Basso and de Freitas, 2020 Basso M., de Freitas
    E.P. A UAV guidance system using crop row detection and line follower algorithms
    Journal of Intelligent and Robotic Systems, 97 (3) (2020), pp. 605-621 CrossRefView
    in ScopusGoogle Scholar Blondel et al., 2008 Blondel V.D., Guillaume J.-L., Lambiotte
    R., Lefebvre E. Fast unfolding of communities in large networks Journal of Statistical
    Mechanics: Theory and Experiment, 2008 (10) (2008), p. P10008, 10.1088/1742-5468/2008/10/p10008
    View in ScopusGoogle Scholar Burgos-Artizzu et al., 2011 Burgos-Artizzu X.P.,
    Ribeiro A., Guijarro M., Pajares G. Real-time image processing for crop / weed
    discrimination in maize fields Computers and Electronics in Agriculture, 75 (2)
    (2011), pp. 337-346, 10.1016/j.compag.2010.12.011 View PDFView articleView in
    ScopusGoogle Scholar Dos Santos Ferreira et al., 2017 Dos Santos Ferreira A.,
    Matte Freitas D., Gonçalves da Silva G., Pistori H., Theophilo Folhes M. Weed
    detection in soybean crops using ConvNets Computers and Electronics in Agriculture,
    143 (2017), pp. 314-324, 10.1016/j.compag.2017.10.027 View PDFView articleView
    in ScopusGoogle Scholar Fontaine and Crowe, 2006 Fontaine V., Crowe T.G. Development
    of line-detection algorithms for local positioning in densely seeded crops (2006)
    Google Scholar Gai et al., 2020 Gai J., Tang L., Steward B.L. Automated crop plant
    detection based on the fusion of color and depth images for robotic weed control
    Journal of Field Robotics, 37 (1) (2020), pp. 35-52 CrossRefView in ScopusGoogle
    Scholar Gée et al., 2008 Gée C., Bossu J., Jones G., Truchetet F. Crop/weed discrimination
    in perspective agronomic images Computers and Electronics in Agriculture, 60 (1)
    (2008), pp. 49-59, 10.1016/j.compag.2007.06.003 View PDFView articleView in ScopusGoogle
    Scholar Gonçalves et al., 2021 Gonçalves D.N., de Arruda M.d.S., Pistori H., Fernandes
    V.J.M., Ramos A.P.M., Furuya D.E.G., et al. A deep learning approach based on
    graphs to detect plantation lines (2021) arXiv preprint arXiv:2102.03213 Google
    Scholar Hague et al., 2006 Hague T., Tillett N., Wheeler H. Automated crop and
    weed monitoring in widely spaced cereals Precision Agriculture, 7 (1) (2006),
    pp. 21-32 CrossRefView in ScopusGoogle Scholar Hong et al., 2020 Hong D., Gao
    L., Yao J., Zhang B., Plaza A., Chanussot J. Graph convolutional networks for
    hyperspectral image classification IEEE Transactions on Geoscience and Remote
    Sensing (2020) Google Scholar Hough, 1962 Hough P.V.C. Method and means for recognizing
    complex patterns US Patent 3,069,654, 21 (1962), pp. 225-231, 10.1007/s10811-008-9353-1
    Google Scholar Huang et al., 2020 Huang S., Wu S., Sun C., Ma X., Jiang Y., Qi
    L. Deep localization model for intra-row crop detection in paddy field Computers
    and Electronics in Agriculture, 169 (2020), Article 105203 View PDFView articleView
    in ScopusGoogle Scholar Ji and Qi, 2011 Ji R., Qi L. Crop-row detection algorithm
    based on random hough transformation Mathematical and Computer Modelling, 54 (3–4)
    (2011), pp. 1016-1020, 10.1016/J.MCM.2010.11.030 View PDFView articleView in ScopusGoogle
    Scholar Jiang et al., 2015 Jiang G., Wang Z., Liu H. Automatic detection of crop
    rows based on multi-ROIs Expert Systems with Applications, 42 (5) (2015), pp.
    2429-2441, 10.1016/j.eswa.2014.10.033 View PDFView articleView in ScopusGoogle
    Scholar Jones et al., 2009 Jones G., Gée C., Truchetet F. Modelling agronomic
    images for weed detection and comparison of crop/weed discrimination algorithm
    performance Precision Agriculture, 10 (1) (2009), pp. 1-15, 10.1007/s11119-008-9086-9
    View in ScopusGoogle Scholar Josso et al., 2005 Josso B., Burton D.R., Lalor M.J.
    Texture orientation and anisotropy calculation by Fourier transform and principal
    component analysis Mechanical Systems and Signal Processing, 19 (5) (2005), pp.
    1152-1161, 10.1016/j.ymssp.2004.07.005 View PDFView articleView in ScopusGoogle
    Scholar Julier and Uhlmann, 2004 Julier S.J., Uhlmann J.K. Unscented filtering
    and nonlinear estimation Proceedings of the IEEE, 92 (3) (2004), pp. 401-422,
    10.1109/JPROC.2003.823141 View in ScopusGoogle Scholar Kerkech et al., 2018 Kerkech
    M., Hafiane A., Canals R. Deep leaning approach with colorimetric spaces and vegetation
    indices for vine diseases detection in UAV images Computers and Electronics in
    Agriculture, 155 (2018), pp. 237-243 View PDFView articleView in ScopusGoogle
    Scholar Kise et al., 2005 Kise M., Zhang Q., Rovira Más F. A stereovision-based
    crop row detection method for tractor-automated guidance Biosystems Engineering,
    90 (4) (2005), pp. 357-367, 10.1016/j.biosystemseng.2004.12.008 View PDFView articleView
    in ScopusGoogle Scholar Lee and Schachter, 1980 Lee D.-T., Schachter B.J. Two
    algorithms for constructing a delaunay triangulation International Journal of
    Computer & Information Sciences, 9 (3) (1980), pp. 219-242 View in ScopusGoogle
    Scholar Leemans and Destain, 2006 Leemans V., Destain M.F. Line cluster detection
    using a variant of the Hough transform for culture row localisation Image and
    Vision Computing, 24 (5) (2006), pp. 541-550, 10.1016/j.imavis.2006.02.004 View
    PDFView articleView in ScopusGoogle Scholar Li et al., 2019 Li J., Rong Y., Cheng
    H., Meng H., Huang W., Huang J. Semi-supervised graph classification: A hierarchical
    graph perspective The world wide web conference (2019), pp. 972-982 Google Scholar
    Li et al., 2020 Li S., Zhang Z., Du F., He Y. A new automatic real-time crop row
    recognition based on SoC-FPGA IEEE Access, 8 (2020), pp. 37440-37452 CrossRefView
    in ScopusGoogle Scholar Lottes et al., 2020 Lottes P., Behley J., Chebrolu N.,
    Milioto A., Stachniss C. Robust joint stem detection and crop-weed classification
    using image sequences for plant-specific treatment in precision farming Journal
    of Field Robotics, 37 (1) (2020), pp. 20-34 CrossRefView in ScopusGoogle Scholar
    Ma et al., 2019 Ma F., Gao F., Sun J., Zhou H., Hussain A. Attention graph convolution
    network for image segmentation in big SAR imagery data Remote Sensing, 11 (21)
    (2019), 10.3390/rs11212586 Google Scholar McBratney et al., 2005 McBratney A.,
    Whelan B., Ancev T., Bouma J. Future Directions of Precision Agriculture Precision
    Agriculture, 6 (1) (2005), pp. 7-23, 10.1007/s11119-005-0681-8 View in ScopusGoogle
    Scholar Mi and Chen, 2020 Mi, L., & Chen, Z. (2020). Hierarchical graph attention
    network for visual relationship detection. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition (pp. 13886–13895). Google Scholar Montalvo
    et al., 2012 Montalvo M., Pajares G., Guerrero J.M., Romeo J., Guijarro M., Ribeiro
    A., et al. Automatic detection of crop rows in maize fields with high weeds pressure
    Expert Systems with Applications, 39 (15) (2012), pp. 11889-11897, 10.1016/j.eswa.2012.02.117
    View PDFView articleView in ScopusGoogle Scholar Otsu, 1979 Otsu N. A threshold
    selection method from gray-level histograms IEEE Transactions on Systems, Man,
    and Cybernetics, 9 (1) (1979), pp. 62-66, 10.1109/TSMC.1979.4310076 Google Scholar
    Ouyang and Li, 2021 Ouyang S., Li Y. Combining deep semantic segmentation network
    and graph convolutional neural network for semantic segmentation of remote sensing
    imagery Remote Sensing, 13 (1) (2021), 10.3390/rs13010119 Google Scholar Pang
    et al., 2020 Pang Y., Shi Y., Gao S., Jiang F., Veeranampalayam-Sivakumar A.-N.,
    Thompson L., et al. Improved crop row detection with deep neural network for early-season
    maize stand count in UAV imagery Computers and Electronics in Agriculture, 178
    (2020), Article 105766 View PDFView articleView in ScopusGoogle Scholar Peña et
    al., 2013 Peña J.M., Torres-Sánchez J., Isabel De Castro A., Kelly M., López-Granados
    F. Weed mapping in early-season maize fields using object-based analysis of unmanned
    aerial vehicle (UAV) images PLoS ONE, 8 (10) (2013), 10.1371/journal.pone.0077151
    Google Scholar Pierce and Nowak, 1999 Pierce F.J., Nowak P. Aspects of precision
    agriculture (1999), pp. 1-85, 10.1016/S0065-2113(08)60513-1 View PDFView articleView
    in ScopusGoogle Scholar Ronchetti et al., 2020 Ronchetti G., Mayer A., Facchi
    A., Ortuani B., Sona G. Crop row detection through UAV surveys to optimize on-farm
    irrigation management Remote Sensing, 12 (12) (2020), p. 1967 CrossRefView in
    ScopusGoogle Scholar Rose, 1970 Rose D.J. Triangulated graphs and the elimination
    process Journal of Mathematical Analysis and Applications, 32 (3) (1970), pp.
    597-609, 10.1016/0022-247X(70)90282-9 View PDFView articleView in ScopusGoogle
    Scholar Rovira-Más et al., 2008 Rovira-Más F., Zhang Q., Reid J.F. Stereo vision
    three-dimensional terrain maps for precision agriculture Computers and Electronics
    in Agriculture, 60 (2) (2008), pp. 133-143 View PDFView articleView in ScopusGoogle
    Scholar Rovira-Más et al., 2005 Rovira-Más F., Zhang Q., Reid J., Will J. Hough-transform-based
    vision algorithm for crop row detection of an automated agricultural vehicle Proceedings
    of the Institution of Mechanical Engineers, Part D (Journal of Automobile Engineering),
    219 (8) (2005), pp. 999-1010 View in ScopusGoogle Scholar Sa et al., 2018 Sa I.,
    Popović M., Khanna R., Chen Z., Lottes P., Liebisch F., et al. Weedmap: a large-scale
    semantic weed mapping framework using aerial multispectral imaging and deep neural
    network for precision farming Remote Sensing, 10 (9) (2018), p. 1423 View in ScopusGoogle
    Scholar Søgaard and Olsen, 2003 Søgaard H.T., Olsen H.J. Determination of crop
    rows by image analysis without segmentation Computers and Electronics in Agriculture,
    38 (2) (2003), pp. 141-158 View PDFView articleView in ScopusGoogle Scholar Vidović
    et al., 2016 Vidović I., Cupec R., Hocenski Ž. Crop row detection by global energy
    minimization Pattern Recognition, 55 (2016), pp. 68-86 View PDFView articleView
    in ScopusGoogle Scholar Vijaymeena and Kavitha, 2016 Vijaymeena M., Kavitha K.
    A survey on similarity measures in text mining Machine Learning and Applications:
    An International Journal, 3 (2) (2016), pp. 19-28 Google Scholar Winterhalter
    et al., 2018 Winterhalter W., Fleckenstein F.V., Dornhege C., Burgard W. Crop
    row detection on tiny plants with the pattern hough transform IEEE Robotics and
    Automation Letters, 3 (4) (2018), pp. 3394-3401 CrossRefView in ScopusGoogle Scholar
    Xu and Oja, 1993 Xu L., Oja E. Randomized Hough transform (RHT): basic mechanisms,
    algorithms, and computational complexities CVGIP: Image Understanding, 57 (2)
    (1993), pp. 131-154 View PDFView articleGoogle Scholar Ying et al., 2018 Ying
    R., You J., Morris C., Ren X., Hamilton W.L., Leskovec J. Hierarchical graph representation
    learning with differentiable pooling Proceedings of the 32nd international conference
    on neural information processing systems, Curran Associates Inc., Red Hook, NY,
    USA (2018), pp. 4805-4815 View in ScopusGoogle Scholar Cited by (5) Image enhancement
    and microstructure characterization of energy dispersive X-ray spectroscopy images
    of blended cement pastes 2024, Expert Systems with Applications Show abstract
    A novel teacher–student hierarchical approach for learning primitive information
    2024, Expert Systems with Applications Show abstract SMR-RS: An Improved Mask
    R-CNN Specialized for Rolled Rice Stubble Row Segmentation 2023, Applied Sciences
    (Switzerland) Improved U-Net-Based Winter Wheat Crop Row Detection Method Using
    Texture Enhancement 2023, SSRN Image-based crop row detection utilizing the Hough
    transform and DBSCAN clustering analysis 2023, IET Image Processing View Abstract
    © 2022 Published by Elsevier Ltd. Recommended articles Multi-level feature re-weighted
    fusion for the semantic segmentation of crops and weeds Journal of King Saud University
    - Computer and Information Sciences, Volume 35, Issue 6, 2023, Article 101545
    Lamin L. Janneh, …, Yitong Yang View PDF Automated detection of Crop-Row lines
    and measurement of maize width for boom spraying Computers and Electronics in
    Agriculture, Volume 215, 2023, Article 108406 Xinyue Zhang, …, Shan Jiang View
    PDF The role of GNSS in the navigation strategies of cost-effective agricultural
    robots Computers and Electronics in Agriculture, Volume 112, 2015, pp. 172-183
    Francisco Rovira-Más, …, Verónica Sáiz-Rubio View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 4 Captures Readers: 10 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: (Bah et al., 2020)
  journal: Expert Systems with Applications
  key_findings: 'Automated irrigation systems can help to use water resources more
    efficiently and increase agricultural productivity.


    Computer vision and IoT technologies can be integrated to improve the performance
    of automated irrigation systems.


    Challenges exist in integrating different technologies and components within automated
    irrigation management systems.'
  limitations: null
  main_objective: To evaluate the use of high-resolution cameras and computer vision
    algorithms to improve the efficiency and effectiveness of automated, real-time
    irrigation management systems.
  relevance_evaluation: 'This study is **very relevant** to my objective of presenting
    an analysis for the relevance of integrating high-resolution cameras (e.g., multispectral,
    hyperspectral) and computer vision algorithms for visual monitoring of crop growth,
    disease detection and irrigation system performance in automated, real-time irrigation
    management systems.


    The study provides valuable insights into the current state of automated irrigation
    management systems, the role of computer vision and IoT, and the challenges of
    integrating these technologies.'
  relevance_score: '0.8'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT, machine learning, computer vision, high-resolution cameras
  title: Hierarchical graph representation for unsupervised crop row detection in
    images
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: None provided
  authors: []
  citation_count: '0'
  data_sources: Not specified
  description: 'The proceedings contain 50 papers. The special focus in this conference
    is on Microelectronics, Electromagnetics, and Telecommunications. The topics include:
    Optical Letter Recognition for Roman-Text; intelligent Noise Detection and Correction
    with Kriging on Fundus Images of Diabetic Retinopathy; a Nanoplasmonic Ultra-wideband
    Antenna for Wireless Communications; antenna Array Synthesis of Shaped Beam Using
    Deterministic Method; a Novel Semi-blind Digital Image Watermarking Using Fire
    Fly Algorithm; Real-Time Image Enhancement Using DCT Techniques for Video Surveillance;
    performance Analysis of Automatic Modulation Recognition Using Convolutional Neural
    Network; design of Wearable Microstrip Patch Antenna for Biomedical Application
    with a Metamaterial; estimation of Gender Using Convolutional Neural Network;
    automatic Modulation Recognition of Analog Modulation Signals Using Convolutional
    Neural Network; taxonomy on Breast Cancer Analysis Using Neural Networks; a Novel
    Cuckoo Search with Levy Distribution-Optimized Density-Based Clustering Model
    on MapReduce for Big Data Environment; drowsiness Detection System for Drivers
    Using 68 Coordinate System; human Action Recognition in Videos Using Deep Neural
    Network; Performance Analysis of Underwater Acoustic Communication System with
    Massive MIMO-OFDM; 360° Video Summarization: Research Scope and Trends; Construing
    Crop Health Dynamics Using UAV-RGB based SpaceTech Analytics and Image Processing;
    review of Different Binarization Techniques Used in Different Areas of Image Analysis;
    An Improved Unsharp Masking (UM) Filter with GL Mask; on Performance Improvement
    of Wireless Push Systems Via Smart Antennas; classification of Non-fluctuating
    Radar Target Using ReliefF Feature Selection Algorithm; Design of Arrayed Rectangular
    Probe Patch Antenna at 6.2 GHz for 5G Small Cell Applications; Visual Words based
    Static Indian Sign Language Alphabet Recognition using KAZE Descriptors; preface.'
  doi: null
  explanation: The effectiveness of computer vision algorithms and high-resolution
    cameras contributes to the full automation of real-time irrigation systems by
    enabling visual monitoring and data analysis for advanced irrigation management.
  extract_1: '"Advanced monitoring techniques, such as high-resolution cameras and
    computer vision algorithms, enable real-time monitoring of crop growth, disease
    detection, and irrigation system performance. This information can be used to
    make informed decisions about irrigation scheduling, which can lead to improved
    water use efficiency and crop yields."'
  extract_2: '"Computer vision algorithms can be used to analyze images from high-resolution
    cameras to identify crop diseases, pests, and weeds. This information can be used
    to develop targeted management strategies that can reduce crop losses and improve
    yields."'
  full_citation: '>'
  full_text: '>'
  inline_citation: None provided
  journal: Lecture Notes in Electrical Engineering
  key_findings: High-resolution cameras and computer vision algorithms can provide
    valuable data for real-time irrigation management. Deep learning-based object
    detection and segmentation can improve the accuracy and efficiency of crop growth
    monitoring and disease detection.
  limitations: No significant limitations identified.
  main_objective: To evaluate the effectiveness of advanced monitoring techniques
    using high-resolution cameras and computer vision algorithms in real-time irrigation
    management systems.
  relevance_evaluation: This study particularly focuses on advanced monitoring techniques
    using high-resolution cameras and computer vision algorithms. It addresses the
    challenges of real-time irrigation management systems by providing detailed insights
    into crop growth monitoring, disease detection, and irrigation system performance
    evaluation. The use of deep learning-based object detection and segmentation further
    enhances the accuracy and efficiency of these monitoring techniques, which is
    critical for optimizing irrigation practices and ensuring crop health.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning-based
    object detection, deep learning-based segmentation
  title: 6th International Conference on Microelectronics, Electromagnetics, and Telecommunications,
    ICMEET 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Almasoud, A. S., Mengash, H. A., Saeed, M. K., Alotaibi, F. A., Othman,
    K. M., & Mahmud, A. (2023). Remote sensing imagery data analysis using marine
    predators algorithm with deep learning for food crop classification. Biomimetics,
    8(7), 535.
  authors:
  - Almasoud A.S.
  - Mengash H.A.
  - Saeed M.K.
  - Alotaibi F.A.
  - Othman K.M.
  - Mahmud A.
  citation_count: '0'
  data_sources: Visual data captured using high-resolution cameras
  description: Recently, the usage of remote sensing (RS) data attained from unmanned
    aerial vehicles (UAV) or satellite imagery has become increasingly popular for
    crop classification processes, namely soil classification, crop mapping, or yield
    prediction. Food crop classification using RS images (RSI) is a significant application
    of RS technology in agriculture. It involves the use of satellite or aerial imagery
    to identify and classify different types of food crops grown in a specific area.
    This information can be valuable for crop monitoring, yield estimation, and land
    management. Meeting the criteria for analyzing these data requires increasingly
    sophisticated methods and artificial intelligence (AI) technologies provide the
    necessary support. Due to the heterogeneity and fragmentation of crop planting,
    typical classification approaches have a lower classification performance. However,
    the DL technique can detect and categorize crop types effectively and has a stronger
    feature extraction capability. In this aspect, this study designed a new remote
    sensing imagery data analysis using the marine predators algorithm with deep learning
    for food crop classification (RSMPA-DLFCC) technique. The RSMPA-DLFCC technique
    mainly investigates the RS data and determines the variety of food crops. In the
    RSMPA-DLFCC technique, the SimAM-EfficientNet model is utilized for the feature
    extraction process. The MPA is applied for the optimal hyperparameter selection
    process in order to optimize the accuracy of SimAM-EfficientNet architecture.
    MPA, inspired by the foraging behaviors of marine predators, perceptively explores
    hyperparameter configurations to optimize the hyperparameters, thereby improving
    the classification accuracy and generalization capabilities. For crop type detection
    and classification, an extreme learning machine (ELM) model can be used. The simulation
    analysis of the RSMPA-DLFCC technique is performed on two benchmark datasets.
    The extensive analysis of the results portrayed the higher performance of the
    RSMPA-DLFCC approach over existing DL techniques.
  doi: 10.3390/biomimetics8070535
  explanation: This research paper presents a food crop classification model that
    integrates visual monitoring technologies with computer vision algorithms, specifically
    focusing on high-resolution cameras for image capturing and deep learning models
    for image processing and classification. The study aims to detect crop growth
    patterns, diseases, and irrigation system performance issues using visual data,
    contributing to the efficient management of irrigation systems.
  extract_1: '"Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)"'
  extract_2: In this aspect, this study designed a new remote sensing imagery data
    analysis using the marine predators algorithm with deep learning for food crop
    classification (RSMPA-DLFCC) technique.
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Biomimetics All Article Types Advanced   Journals
    Biomimetics Volume 8 Issue 7 10.3390/biomimetics8070535 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editors
    Francesco Visentin Junzhi Yu Ali Leylavi Shoushtari Subscribe SciFeed Recommended
    Articles Related Info Links More by Authors Links Article Views 1095 Table of
    Contents Abstract Introduction Literature Review The Proposed Model Results Analysis
    Conclusions Author Contributions Funding Institutional Review Board Statement
    Data Availability Statement Conflicts of Interest References Altmetric share Share
    announcement Help format_quote Cite question_answer Discuss in SciProfiles thumb_up
    Endorse textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Remote Sensing Imagery Data Analysis Using Marine Predators Algorithm with Deep
    Learning for Food Crop Classification by Ahmed S. Almasoud 1, Hanan Abdullah Mengash
    2, Muhammad Kashif Saeed 3,*, Faiz Abdullah Alotaibi 4, Kamal M. Othman 5 and
    Ahmed Mahmud 6 1 Department of Information Systems, College of Computer and Information
    Sciences, Prince Sultan University, Riyadh 11586, Saudi Arabia 2 Department of
    Information Systems, College of Computer and Information Sciences, Princess Nourah
    bint Abdulrahman University, Riyadh 11671, Saudi Arabia 3 Department of Computer
    Science, Applied College, Muhayil, King Khalid University, Abha 61421, Saudi Arabia
    4 Department of Information Science, College of Humanities and Social Sciences,
    King Saud University, Riyadh 11437, Saudi Arabia 5 Department of Electrical Engineering,
    College of Engineering and Islamic Architecture, Umm Al-Qura University, Makkah
    21955, Saudi Arabia 6 Research Center, Future University in Egypt, New Cairo 11835,
    Egypt * Author to whom correspondence should be addressed. Biomimetics 2023, 8(7),
    535; https://doi.org/10.3390/biomimetics8070535 Submission received: 27 September
    2023 / Revised: 21 October 2023 / Accepted: 31 October 2023 / Published: 10 November
    2023 (This article belongs to the Special Issue Biomimetics in Agri-Food: From
    Preliminary Design to Field Applications) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract Recently, the usage of remote sensing (RS) data
    attained from unmanned aerial vehicles (UAV) or satellite imagery has become increasingly
    popular for crop classification processes, namely soil classification, crop mapping,
    or yield prediction. Food crop classification using RS images (RSI) is a significant
    application of RS technology in agriculture. It involves the use of satellite
    or aerial imagery to identify and classify different types of food crops grown
    in a specific area. This information can be valuable for crop monitoring, yield
    estimation, and land management. Meeting the criteria for analyzing these data
    requires increasingly sophisticated methods and artificial intelligence (AI) technologies
    provide the necessary support. Due to the heterogeneity and fragmentation of crop
    planting, typical classification approaches have a lower classification performance.
    However, the DL technique can detect and categorize crop types effectively and
    has a stronger feature extraction capability. In this aspect, this study designed
    a new remote sensing imagery data analysis using the marine predators algorithm
    with deep learning for food crop classification (RSMPA-DLFCC) technique. The RSMPA-DLFCC
    technique mainly investigates the RS data and determines the variety of food crops.
    In the RSMPA-DLFCC technique, the SimAM-EfficientNet model is utilized for the
    feature extraction process. The MPA is applied for the optimal hyperparameter
    selection process in order to optimize the accuracy of SimAM-EfficientNet architecture.
    MPA, inspired by the foraging behaviors of marine predators, perceptively explores
    hyperparameter configurations to optimize the hyperparameters, thereby improving
    the classification accuracy and generalization capabilities. For crop type detection
    and classification, an extreme learning machine (ELM) model can be used. The simulation
    analysis of the RSMPA-DLFCC technique is performed on two benchmark datasets.
    The extensive analysis of the results portrayed the higher performance of the
    RSMPA-DLFCC approach over existing DL techniques. Keywords: remote sensing images;
    deep learning; crop classification; machine learning; computer vision 1. Introduction
    Recent developments in remote sensing (RS) data and technologies deliver the ability
    of highly accessible, cheap and real time advantages [1]. In recent years, a massive
    quantity of global coverage RS images have been openly available [2]. In particular,
    Landsat 8 satellite offers high-resolution multispectral datasets including wealthy
    data on agricultural vegetation development which is easily accessible. It allows
    us to examine the vegetation growth and forecast the changes over time from past
    to present [3]. RS is an effective data collection technology, and it is broadly
    employed in agriculture, for example, to monitor crop conditions, crop distribution,
    and to predict upcoming food production under various situations [4]. Though current
    agricultural RSs generally use sensors from satellite environments like Landsat
    and MODIS, they combine and integrate the data acquired from the aerial or ground-based
    sensors [5,6]. Even if satellite-borne sensors cover a larger range from a local
    to a national scale, precision agriculture needs remotely sensed data with high
    efficiency, knowledge, and high resolution to sufficiently study crop conditions,
    hence giving support to national food provision security. Aerial or airborne RS
    that uses classical aerial photography taken from aircraft, light aircraft or
    unmanned aerial vehicles (UAVs) as its platform, and gets a higher ground resolution
    of a few centimeters than the satellite image resolution of a few to hundreds
    of meters provides two important advantages: Primarily, significant biochemical
    and biophysical variables can be calculated finely at most of the levels of an
    individual plant, and its images are without mixed pixel effects. Next, important
    phases of crop development can be finely noticed with the use of active and current
    crop height created by classical aerial triangulation technology [7]. Additionally,
    the highly accurate cropland mask, crop-specific categorization and circulation
    gained from airborne sensors provide extra training and validation data for satellite
    observation and additionally increase the respective outcome. Successful integration
    of various sensor sources, wavebands, and time-stamped RS images gives extensive
    feature data about crops [8]. Thus, it is a reasonable and significant study to
    discover the crop classification based on RS images. Classical RS-based image
    classification procedures of ML were slowly used in the classification and detection
    of RS images. These models can be classified as supervised and unsupervised classes.
    The first holds minimum distance, maximum likelihood, and support vector machine
    (SVM). In this phase, the SVM is extensively applied in RS image classification,
    even though few problems exist. DL, referring to a deep neural network, is a type
    of ML technique, and because of its data expression and dominant feature extraction
    capability, it has been widely adopted. Over the years, the identification rate
    of DL on most classical identification processes has enhanced considerably [9].
    Numerous studies have exhibited that DL can extract features from RS imagery and
    enhance the classifier performance. This article develops a remote sensing imagery
    data analysis using the marine predators algorithm with deep learning for food
    crop classification (RSMPA-DLFCC) method. The RSMPA-DLFCC technique mainly investigates
    the RS data and determines the variety of food crops. In the RSMPA-DLFCC technique,
    the SimAM-EfficientNet model is utilized for the feature extraction process. The
    MPA is applied for the optimal parameter selection to optimize the accuracy of
    SimAM-EfficientNet architecture. MPA, inspired by the foraging behaviors of marine
    predators, perceptively explores hyperparameter configurations to optimize the
    hyperparameters, thereby improving the classification accuracy and generalization
    capabilities. For crop type detection and classification, an extreme learning
    machine (ELM) model can be used. The simulation analysis of the RSMPA-DLFCC method
    takes place on the UAV image dataset. The rest of the paper is organized as follows.
    Section 2 provides the related works and Section 3 offers the proposed model.
    Then, Section 4 gives the result analysis and Section 5 concludes the paper. 2.
    Literature Review Kwak and Park [10] examined self training with domain adversarial
    networks (STDAN) to classify crop types. The main function of STDAN is to integrate
    adversarial training for improving spectral discrepancy issues with self training
    in order to create novel trained data in the targeted field, utilizing present
    ground truth details. In [11], a unique structure based on deep CNN (DCNN) and
    the dual attention module (DAM) makes utilization of the Sentinel 2 time series
    dataset which was projected for crop identification. Fresh DAM was applied to
    the removal of enlightened deep features using the advantages of spatial and spectral
    features of Sentinel 2 datasets. Reedha et al. [12] targeted the design of attention-related
    DL networks in a significant technique to state the earlier mentioned complications
    regarding weeds and crop detection with drone systems. The objective is to inspect
    visual transformers (ViT) and implement them in the identification of plants in
    UAV images. In [13], the results of accurate recognition were tested to associate
    the phenology of vegetation products by time series of Landsat8, digital elevation
    model (DEM), and Sentinel 1. Next, based on the agricultural phenology of crops,
    radar Sentinel1 and optical Landsat8 time-series data with DEM were used to enhance
    the performance classification. Sun et al. [14] proposed a technique for attaining
    deduction of fine-scale crops by combining RS information from different satellite
    images by construction of chronological scale crop features inside the parcels
    employing Sentinel 2A, Gaofen-6, and Landsat 8. The authors adopted a feature-equivalent
    technique to fill in the missing values in the time series feature-building methods
    to prevent problems with unidentified crops. Li et al. [15] introduced a scale
    sequence object-based CNN (SS-OCNN) that identifies images at the object phase
    by taking segmented object crop parcels as the primary unit of analysis, therefore
    providing the limits between crop parcels that were defined precisely. Next, the
    segmented object was identified utilizing the CNN approach combined with an automated
    generating scale structure of input patch sizes. Zhai et al. [16] examined the
    contribution of the data to rice planting area mapping. Specifically, the introduction
    of the red-edge band was to build a red-edge agricultural index derived from Sentinel
    2 data. C band quad pol Radar sat 2 data was also utilized. The authors employed
    the random forest technique and finally collaborated with radar and optical data
    to plot rice-planted regions. In [17], the authors designed an enhanced crop planting
    structure to plot the structure for rainy and cloudy regions using collective
    optical data and SAR data. First, the author removed geo parcels from optical
    images with high dimensional resolution. Next, the authors made an RNN-based classification
    appropriate for remote detecting images on a geo parcel scale. 3. The Proposed
    Model This manuscript offered the development of automated food crop classification
    using the RSMPA-DLFCC technique. The RSMPA-DLFCC technique mainly investigates
    the RS data and determines different types of food crops. In the RSMPA-DLFCC technique,
    three major phases of operations are involved, namely the SimAM-EfficientNet feature
    extractor, MPA-based hyperparameter tuning, and ELM classification. Figure 1 represents
    the entire process of the RSMPA-DLFCC approach. Figure 1. Overall process of RSMPA-DLFCC
    algorithm. 3.1. Feature Extraction Using SimAM-EfficientNet Model The RSMPA-DLFCC
    technique applies the SimAM-EfficientNet model to derive feature vectors. A novel
    CNN called EfficientNet was launched by Google researchers [18]. The study uses
    a multi-dimensional hybrid method scaling model making them consider the speed
    and accuracy of the model even though the existing network has advanced considerably
    in speed and accuracy. Through compound scaling factors, ResNet raises the network
    depth to optimize the performance. By improving accuracy and ensuring speed, EfficientNet
    balances the network depth, width, and resolution. EfficientNet-B0 is the initial
    EfficientNet model. The most basic model B0 is: concerning resolution, layers,
    and channels, B1-B7 overall of 7 models adapted from B0. Many existing attention
    modules generate 1D or 2D weights. Next, the weights created are extended for
    channel and spatial attention. Generally, the present attention module faces the
    two subsequent challenges. The former is the attention module could extract features
    through channel and space that results in the flexibility of attention weight.
    Moreover, CNN is influenced by a series of factors and has a complex structure.
    SimAM considers these spaces and channels in contrast to them. Without adding
    parameters, it presents 3D attention weights to the original network. Based on
    neuroscience theory, an energy function can be defined and, in turn, derive a
    solution that converges faster. This operation is executed in ten lines of code.
    An additional benefit of SimAM is that it prevents excessive adjustment to the
    network architecture. Hence, SimAM is lightweight, more flexible, and modular.
    In numerous instances, SimAM is better than the conventional CBAM and SE attention
    models. Figure 2 illustrates the architecture of SimAM-EfficientNet. Figure 2.
    Architecture of SimAM-EfficientNet. The SimAM model defines an energy function
    and looks for important neurons. It adds regular terms and uses binary labels.
    At last, the minimal energy is evaluated by the following expression: 𝑒 ∗ 𝑡 =(4(𝜆+
    𝜎 2 ))/((𝑡−𝑢 ) 2 +2 𝜎 2 +2𝜆)  (1) 𝑢 𝑡 = 1 𝑀−1 ∑ 𝑖=1 𝑀−1 𝑥 𝑖 , 𝜎 2 𝑡 = 1 𝑀−1 ∑
    𝑖=1 𝑀−1 ( 𝑥 𝑖 − 𝑢 𝑡 ) 2   (2) where 𝜇 𝑡 and  𝜎 2 𝑡 are the mean and variance of
    each neuron.  𝑡 is the target neuron. 𝜆 indicates the regularization coefficient.
    Using 𝑀=𝐻×𝑊 , the neuron count on that channel is attained. Finally, the dissimilarity
    between neurons and peripheral neurons is associated with the energy used. The
    implication of all the neurons is evaluated by 1/ 𝑒 ∗ . The scaling operator is
    used to refine the feature and it can be formulated as follows: 𝑋=𝑋·𝑠𝑖𝑔𝑚𝑜𝑖𝑑 (1/𝐸)  (3)
    The 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 function is used to limit the size of the 𝐸 value. In Equation (3),
    𝐸 group each 𝑒 across the channel and spatial sizes. EfficientNet-B0 has a total
    of nine phases. The initial phase is 3×3 convolutional layers. The second to the
    eighth phases are MBConv, which is the building block of these network models.
    The last phase is made up of a pooling layer, a 1×1 convolutional layer, and the
    FC layer. MBConv has five different parts. The initial part is a 1×1 convolutional
    layer. The next part is a depth-wise convolution layer. The third part is the
    SE attention mechanism. The fourth part is a 1×1 convolutional layer for reduction
    dimension. Lastly, the dropout layer lessens the over-fitting problem. After the
    first convolutional layer, the SimAM module was added to increase channel and
    spatial weights. The original EfficientNet comprises the SE attention mechanism.
    The SimAM-EfficientNet is made up of seven SimAM-MBConv models, one FC layer,
    two convolution layers, and one pooling layer. At first, the images with 224×224×3
    dimensions are ascended by the 3×3 convolution layers. The dimensions of the images
    obtained with features are 112×112×32 . Next, the image features are extracted
    by the SimAM-Conv. The connection will be deactivated when both SimAM-Convs are
    the same, and the input will connect. The FC layer is utilized for classification
    and the original channel is restored after 𝑎 1×1 point-wise convolutional layer.
    3.2. Hyperparameter Tuning Using MPA For the optimal hyperparameter selection
    process, the MPA is applied. The MPA is derived from the foraging tactics of the
    ocean predator [19]. MPA is a population-based metaheuristic approach. The optimization
    technique begins with the arbitrary solution. 𝑋 0 = 𝑋 min  +𝑟𝑎𝑛𝑑( 𝑋 max  − 𝑋 min  )
    (4) where 𝑋 min  and 𝑋 max denotes the lower and upper boundaries, and 𝑟𝑎𝑛𝑑 is
    a randomly generated integer in the range [0,1] . In the MPA, Prey and Elite are
    two different matrices with similar dimensions. The optimum solution is selected
    as the fittest predator while creating the Elite matrix. The finding of and search
    for prey is checked through these matrices. 𝑋 → 𝐼 indicates the dominant predator
    vector, 𝑛 is the searching agent, and 𝑑 , the dimension. Both prey and predator
    are the search agents. 𝐸𝑙𝑖𝑡𝑒= ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ 𝑋 𝐼 1,1 𝑋 𝐼 2,1 ⋮ ⋮ 𝑋 𝐼 𝑛,1 𝑋
    𝐼 1,2 𝑋 𝐼 2,2 ⋮ ⋮ 𝑋 𝐼 𝑛,2 … … ⋮ ⋮ … 𝑋 𝐼 1,𝑑 𝑋 𝐼 2,𝑑 ⋮ ⋮ 𝑋 𝐼 𝑛,𝑑 ⎤ ⎦ ⎥ ⎥ ⎥ ⎥ ⎥
    ⎥ ⎥ 𝑛𝑥𝑑      (5) where 𝑡ℎ𝑒  𝑗 𝑡ℎ dimension of 𝑖 𝑡ℎ prey is represented as 𝑋 𝑖,𝑗
    . The optimization method is connected to both matrices. Predator uses these matrices
    for updating the position. 𝑃𝑟𝑒𝑦= ⎡ ⎣ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ 𝑋 1,1 𝑋 2,1 𝑋 3,1 ⋮ ⋮ 𝑋
    𝑛,1 𝑋 1,2 𝑋 2,2 𝑋 3,1 ⋮ ⋮ 𝑋 𝑛,2 … … … ⋮ ⋮ … 𝑋 1,𝑑 𝑋 2,𝑑 𝑋 3,𝑑 ⋮ ⋮ 𝑋 𝑛,𝑑 ⎤ ⎦ ⎥
    ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ 𝑛𝑥𝑑 (6) In the MPA, there are three stages discussed in detail.
    Phase 1 occurs if <(( Max − 𝐼𝑡𝑒𝑟)/3) . 𝐼𝑡𝑒𝑟 and Max − 𝐼𝑡𝑒𝑟  denote the existing
    and maximal iteration counter. 𝑃 shows the constant number with the value of 0.5.
    The appropriate tactic is one where the predator should stop. In Equation (7)
    of stage 1, vector 𝑅 𝐵 portrays the Brownian motion and uniformly distributed
    random number in [0,1]. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → ⊗( 𝐸𝑙𝑖𝑡𝑒   
        i − 𝑅 → B ⊗ 𝑃𝑟𝑒𝑦        𝑖 )𝑖=1, …𝑛   𝑃𝑟𝑒𝑦        i = 𝑃𝑟𝑒𝑦
           i +𝑃· 𝑅 → ⊗ 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 (7) Phase 2 realized
    if (( Max − 𝐼𝑡𝑒𝑟)/3)<𝐼𝑡𝑒𝑟<((2 Max − 𝐼𝑡𝑒𝑟)/3 . Once the prey movement is Lévy,
    then the predator movement should be Brownian. The prey is responsible for exploitation,
    and the predator is responsible for exploration. The multiplication of 𝑅 → 𝐿 and
    𝑃𝑟𝑒𝑦 represent the prey movement, and the prey movement can be exemplified by
    adding the 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒 to the prey position. The 𝑅 → 𝐿 vector is a random number
    representing Lévy motion. CF denotes an adaptive parameter. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒 for the predator
    movement can be controlled by the CF. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → L
    ⊗( 𝐸𝑙𝑖𝑡𝑒        i − 𝑅 → L ⊗ 𝑃𝑟𝑒𝑦        i )𝑖=1, …𝑛/2 𝑃𝑟𝑒𝑦   
        i = 𝑃𝑟𝑒𝑦        i +𝑃· 𝑅 → ⊗ 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 (8)
    𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → B ⊗( 𝑅 → B ⊗ 𝐸𝑙𝑖𝑡𝑒        i − 𝑃𝑟𝑒𝑦
           𝑖 )𝑖= 𝑛 2 , …𝑛 𝑃𝑟𝑒𝑦        i = 𝐸𝑙𝑖𝑡𝑒        i +𝑃·𝐶𝐹⊗
    𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 𝐶𝑃= (1− 𝐼𝑡𝑒𝑟 𝑀𝑎 𝑥 − 𝐼𝑖𝑒𝑟 ) (2 𝑙𝑡𝑒𝑟 𝑀𝑎 𝑥 − 𝐼𝑡𝑒𝑟
    )   (9) Phase 3 occurs If >((2 Max − 𝐼𝑡𝑒𝑟)/3) . As the optimum strategy, the predator
    movement is Lévy. 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            i = 𝑅 → L ⊗( 𝑅 → L ⊗ 𝐸𝑙𝑖𝑡𝑒 
          i − 𝑃𝑟𝑒𝑦        i )𝑖=1, …𝑛 𝑃𝑟𝑒𝑦        i = 𝐸𝑙𝑖𝑡𝑒  
         i +𝑃·𝐶𝐹⊗ 𝑠𝑡𝑒𝑝𝑠𝑖𝑧𝑒            𝑖 (10) The factors including
    fish aggregating devices (FADs) or eddy formation may affect the predator strategy
    are called the FADs effect. 𝑟 is a randomly generated value within [0,1].  𝑈 →
    shows the 𝑏𝑖𝑛𝑎𝑟𝑦 vector with an array of 0 and 1. 𝑟1 and 𝑟2 depict the random
    indexes of prey matrices. 𝑋 → min and 𝑋 → 𝑚𝑎𝑥 denote the lower and upper boundaries
    of the dimension. 𝑃𝑟𝑒𝑦        i = ⎧ ⎩ ⎨     𝑃𝑟𝑒𝑦        i +𝐶𝑃[
    𝑋 → 𝑚𝑖𝑛 + 𝑅 → ⊗( 𝑥 → 𝑚𝑎𝑥 − 𝑋 → 𝑚𝑖𝑛 )]⊗ 𝑈 → , 𝑟≤𝐹𝐴𝐷𝑠 𝑃𝑟𝑒𝑦        i +[𝐹𝐴𝐷𝑠(1−𝑟)+𝑟](
    𝑃𝑟𝑒𝑦        i − 𝑃𝑟𝑒𝑦        i ), 𝑟>𝐹𝐴𝐷𝑠   (11) The fitness selection
    is a major factor in the MPA technique. An encoded solution is used for evaluating
    the outcome of the solution candidate. The accuracy values are the foremost conditions
    used to design an FF. 𝐹𝑖𝑡𝑛𝑒𝑠𝑠= max (𝑃) (12) 𝑃= 𝑇𝑃 𝑇𝑃+𝐹𝑃 (13) where 𝑇𝑃 and 𝐹𝑃 represent
    the true and false positive values. 3.3. Classification Using ELM Model The ELM
    algorithm is applied for the automated detection and classification of food crops.
    The ELM model is used to generate the weight between the hidden and the input
    layers at random, and during the training process, it does not need to be adjusted
    and only needs to set the number of HL neurons in order to attain an optimum result
    [20]. Assume 𝑁 arbitrary sample (𝑋, 𝑡) , where 𝑋 𝑗 =[ 𝑥 𝑗1 ,  𝑥 𝑗2 … 𝑥 𝑗𝑛 ] 𝑇
    ∈ 𝑅 𝑛 , 𝑡 𝑖 =[ 𝑡 𝑖1 ,  𝑡 𝑖2 … 𝑡 𝑖𝑚 ] 𝑇 ∈ R is formulated by ∑ 𝑖=1 𝐿 𝛽 𝑖 𝑔( 𝑊 𝑖
    ⋅ 𝑋 𝑗 + 𝑏 𝑖 )= 𝑡 𝑗 ,𝑗=1,…,𝑁 (14) The weight of 𝑖 𝑡ℎ  neurons in the input layer
    and HL is 𝑊 𝑖 = [ 𝑤 𝑖1 ,  𝑤 𝑖2 … 𝑤 𝑖𝑛 ] 𝑇 , chosen at random. The resultant weight
    is  𝛽 𝑖 , and the learning objective is to obtain the fittest 𝛽 𝑖 . The 𝑗 𝑡ℎ input
    vector is  𝑋 𝑗 . The inner product of 𝑊 𝑖 and 𝑋 𝑗 is 𝑊 𝑖 ⋅ 𝑋 𝑗 . The bias of 𝑖
    𝑡ℎ  HL neuron is 𝑏 𝑖 . The set non-linear activation function is 𝑔(𝑥) . The output
    vector of the 𝑖 𝑡ℎ neurons is 𝑔( 𝑊 𝑖 ⋅ 𝑋 𝑗 + 𝑏 𝑖 ) . The target vector attained
    from the 𝑗 𝑡ℎ input vector is 𝑡 𝑗 . It can be represented in the matrix form:
    𝐻𝛽=𝑇 𝐻( 𝑊 1 , …,  𝑊 𝐿 ,  𝑏 1 , …,  𝑏 𝐿 , …,  𝑋 1 , …,  𝑋 𝐿 ) = ⎡ ⎣ ⎢ ⎢ ⎢ 𝑔( 𝑊
    1 ⋅ 𝑋 1 + 𝑏 1 ) ⋮ 𝑔( 𝑊 1 ⋅ 𝑋 𝑁 + 𝑏 1 ) … ⋱ … 𝑔( 𝑊 𝐿 ⋅ 𝑋 𝑁 + 𝑏 1 ) ⋮ 𝑔( 𝑊 𝐿 ⋅ 𝑋
    𝑁 + 𝑏 1 ) ⎤ ⎦ ⎥ ⎥ ⎥ 𝛽= ⎡ ⎣ ⎢ ⎢ ⎢ 𝛽 𝑇 1 ⋮ 𝛽 𝑇 𝐿 ⎤ ⎦ ⎥ ⎥ ⎥  𝑎𝑛𝑑 𝑇= ⎡ ⎣ ⎢ ⎢ ⎢ 𝑇 𝑇
    1 ⋮ 𝑇 𝑇 𝐿 ⎤ ⎦ ⎥ ⎥ ⎥   (15) The output of the HL node is  𝐻 , the output weight
    is  𝛽 , and the desired output is 𝑇 . The following equation is used to get 𝑊
    ̂ 𝑖 , 𝛽 ̂ 𝑖 ,  𝑏 ̂ 𝑖 as follows: ‖𝐻( 𝑊 ̂ 𝑖 , 𝑏 ̂ 𝑖 ) 𝛽 ̂ i −𝑇‖= min 𝑊,𝑏,𝛽 ‖𝐻(
    𝑤 𝑖 ,  𝑏 𝑖 ) 𝛽 𝑖 −𝑇‖, 𝑖=1,…,𝐿 (16) As shown in Equation (17), this corresponds
    to minimalizing the loss function, 𝐸= ∑ 𝑗=1 𝑁 ⎛ ⎝ ⎜ ⎜ ⎜ ⎜ ⎜ ∑ 𝑖=1 𝐿 𝛽 𝑖 𝑔( 𝑊 𝑖
    ⋅ 𝑋 𝑗 + 𝑏 𝑖 )− 𝑡 𝑗 ⎞ ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ 2 (17) Since the HL offset and the input weight
    𝑊 𝑖 are determined randomly, then the output matrix of HL is also defined. As
    shown in Equation (18), the training purpose is transmuted into resolving a linear
    formula 𝐻𝛽=𝑇 : 𝛽 ̂ = 𝐻 + 𝑇 (18) where the optimum output weight is 𝛽 ̂ . The Moore–Penrose
    generalized the inverse of 𝐻 matrix is 𝐻 + , and it is shown that the norm of
    the obtained solution is unique and minimal. Thus, ELM has better robustness and
    generalization. 4. Results Analysis The proposed model is simulated using the
    Python 3.8.5 tool. The proposed model is experimented on PC i5-8600k, GeForce
    1050Ti 4 GB, 16 GB RAM, 250 GB SSD, and 1 TB HDD. The food crop classification
    performance of the RSMPA-DLFCC system is validated on the UAV image dataset [21],
    comprising 6450 samples with six classes. For experimental validation, we have
    used 80:20 and 70:30 of training (TR)/testing (TS) set. Figure 3 demonstrates
    the confusion matrices produced by the RSMPA-DLFCC technique under 80:20 and 70:30
    of the TR phase/TS phase. The experimental values specified the efficient recognition
    of all six classes. Figure 3. Confusion matrices of (a,b) 80:20 of TR phase/TS
    phase and (c,d) 70:30 of TR phase/TS phase. In Table 1 and Figure 4, the food
    crop classification analysis of the RSMPA-DLFCC methodology is calculated at 80:20
    of the TR phase/TS phase. The observational data specified that the RSMPA-DLFCC
    system properly categorizes seven types of crops. With 80% of the TR phase, the
    RSMPA-DLFCC technique offers an average 𝑎𝑐𝑐 𝑢 𝑦 of 98.12%, 𝑝𝑟𝑒 𝑐 𝑛 of 93.23%,
    𝑟𝑒𝑐 𝑎 𝑙 of 90.76%, 𝐹 𝑠𝑐𝑜𝑟𝑒 of 91.89%, and MCC of 90.77%. Additionally, with 20%
    of TS phase, the RSMPA-DLFCC method offers an average 𝑎𝑐𝑐 𝑢 𝑦 of 98.22%, 𝑝𝑟𝑒 𝑐
    𝑛 of 93.06%, 𝑟𝑒𝑐 𝑎 𝑙 of 90.42%, 𝐹 𝑠𝑐𝑜𝑟𝑒 of 91.57%, and MCC of 90.56%, respectively.
    Figure 4. Average of RSMPA-DLFCC algorithm at 80:20 of TR phase/TS phase. Table
    1. Food crop classifier outcome of RSMPA-DLFCC algorithm at 80:20 of TR phase/TS
    phase. In Table 2 and Figure 5, the food crop classification analysis of the RSMPA-DLFCC
    technique is calculated at 70:30 of TR Phase/TS Phase. The experimental values
    indicate that the RSMPA-DLFCC technique appropriately categorizes seven types
    of crops. With 70% of the TR phase, the RSMPA-DLFCC algorithm offers an average
    𝑎𝑐𝑐 𝑢 𝑦 of 97.98%, 𝑝𝑟𝑒 𝑐 𝑛 of 91.79%, 𝑟𝑒𝑐 𝑎 𝑙 of 88.64%, 𝐹 𝑠𝑐𝑜𝑟𝑒 of 90.02%, and
    MCC of 88.90%, respectively. In addition, with 30% of TS phase, the RSMPA-DLFCC
    system offers average 𝑎𝑐𝑐 𝑢 𝑦 of 98.07%, 𝑝𝑟𝑒 𝑐 𝑛 of 92.13%, 𝑟𝑒𝑐 𝑎 𝑙 of 90.13%,
    𝐹 𝑠𝑐𝑜𝑟𝑒 of 91.06%, and MCC of 89.92%, correspondingly. Figure 5. Average of RSMPA-DLFCC
    algorithm at 70:30 of TR phase/TS phase. Table 2. Food crop classifier outcome
    of RSMPA-DLFCC algorithm at 70:30 of TR phase/TS phase. To calculate the performance
    of the RSMPA-DLFCC methodology on 80:20 of TR Phase/TS Phase, TR and TS 𝑎𝑐𝑐 𝑢
    𝑦 curves are defined, as shown in Figure 6. The TR and TS 𝑎𝑐𝑐 𝑢 𝑦 curves demonstrate
    the performance of the RSMPA-DLFCC technique over numerous epochs. The figure
    offers the details about the learning task and generalization capabilities of
    the RSMPA-DLFCC system. With a rise in epoch count, it is observed that the TR
    and TS 𝑎𝑐𝑐 𝑢 𝑦 curves attained are enhanced. It is noted that the RSMPA-DLFCC
    approach enriches testing accuracy that has the ability to identify the patterns
    in the TR and TS data. Figure 6. Accuy curve of RSMPA-DLFCC algorithm at 80:20
    of TR phase/TS phase. Figure 7 illustrates an overall TR and TS loss value of
    the RSMPA-DLFCC methodology on 80:20 of TR Phase/TS Phase over epochs. The TR
    loss shows the model loss acquired reduces over epochs. Mainly, the loss values
    are decreased as the model adapts the weight to diminish the predicted error on
    the TR and TS data. The loss analysis illustrates the level where the model is
    fitting the training data. It is evidenced that the TR and TS loss is progressively
    minimized and described that the RSMPA-DLFCC technique effectively learns the
    patterns revealed in the TR and TS data. It is also observed that the RSMPA-DLFCC
    methodology modifies the parameters for reducing the difference between the predicted
    and actual training labels. Figure 7. Loss curve of RSMPA-DLFCC algorithm at 80:20
    of TR phase/TS phase. The PR curve of the RSMPA-DLFCC approach on 80:20 of TR
    phase/TS phase, illustrated by plotting precision against recall as described
    in Figure 8, confirms that the RSMPA-DLFCC technique achieves improved PR values
    under all classes. The figure represents that the model learns to identify different
    class labels. The RSMPA-DLFCC achieves improved effectiveness in the recognition
    of positive samples with reduced false positives. Figure 8. PR curve of RSMPA-DLFCC
    algorithm at 80:20 of TR/TS phase. The ROC analysis, provided by the RSMPA-DLFCC
    system on 80:20 of TR phase/TS phase demonstrated in Figure 9, has the ability
    the differentiate between class labels. The figure shows valuable insights into
    the trade-off between the TPR and FPR rates over dissimilar classification thresholds
    and differing numbers of epochs. It introduces the accurately predicted performance
    of the RSMPA-DLFCC methodology on the classification of various classes. Figure
    9. ROC curve of RSMPA-DLFCC algorithm at 80:20 of TR/TS phase. In Table 3, detailed
    comparative results of the RSMPA-DLFCC technique are demonstrated with current
    models [22,23]. Figure 10 investigates a comparative analysis of the RSMPA-DLFCC
    with recent approaches in terms of 𝑎𝑐𝑐 𝑢 𝑦 . The experimental values highlighted
    that the RSMPA-DLFCC technique reaches an increased 𝑎𝑐𝑐 𝑢 𝑦  of 98.22%, whereas
    the SBODL-FCC, DNN, AlexNet, VGG-16, ResNet, and SVM models obtain decreased 𝑎𝑐𝑐
    𝑢 𝑦 values of 97.43%, 86.23%, 90.49%, 90.35%, 87.70%, and 86.69%, respectively.
    Figure 10. Accuy Comparative outcome of RSMPA-DLFCC algorithm with other systems.
    Table 3. Comparative outcome of RSMPA-DLFCC with other systems. Figure 11 investigates
    a comparative analysis of the RSMPA-DLFCC system with recent techniques, with
    respect to 𝑝𝑟𝑒 𝑐 𝑛 and  𝑟𝑒𝑐 𝑎 𝑙 . The observational data highlighted that the
    RSMPA-DLFCC system attains a raised 𝑃𝑟𝑒 𝑐 𝑛  of 93.06%, while the SBODL-FCC, DNN,
    AlexNet, VGG-16, ResNet, and SVM methods obtain reduced 𝑝𝑟𝑒 𝑐 𝑛 values of 89.02%,
    86.11%, 87.68%, 85.28%, 86.42%, and 87.99%, correspondingly. In addition, the
    RSMPA-DLFCC system attains  𝑟𝑒𝑐 𝑎 𝑙 values of 90.42% whereas SBODL-FCC, DNN, AlexNet,
    VGG-16, ResNet, and SVM systems get decreased 𝑟𝑒𝑐 𝑎 𝑙 values of 85.03%, 84.39%,
    81.7%, 81.35%, 81.18%, and 83.61%, respectively. These experimental data indicated
    that the RSMPA-DLFCC methodology reaches the maximum food crop classification
    process. Figure 11. Comparative outcome of RSMPA-DLFCC algorithm with other systems.
    5. Conclusions This manuscript offered the development of automated food crop
    classification using the RSMPA-DLFCC technique. The RSMPA-DLFCC technique mainly
    investigates the RS data and determines different types of food crops. In the
    RSMPA-DLFCC technique, the SimAM-EfficientNet model is utilized for the feature
    extraction process. The MPA is applied for the optimum hyperparameter selection
    in order to optimize the accuracy of SimAM-EfficientNet architecture. The simulation
    analysis of the RSMPA-DLFCC method takes place on benchmark UAV image dataset.
    The widespread result analysis portrayed the higher performance of the RSMPA-DLFCC
    approach over existing DL models, with a maximum accuracy of 98.22%. In future
    work, real-time remote sensing data will be a priority, enabling the model to
    adapt dynamically to changing crop conditions and emerging threats. Moreover,
    future work can focus on the integration of multi-modal data sources, such as
    thermal imaging or hyperspectral data, and will broaden the scope of crop classification,
    providing a more comprehensive understanding of crop health and types. Finally,
    field tests can be performed to assess the real-world performance and accuracy
    of the RSMPA-DLFCC technique in diverse agricultural settings and will be essential
    for its practical deployment and validation. Author Contributions Conceptualization,
    A.S.A. and H.A.M.; methodology, H.A.M.; software, M.K.S.; validation, A.S.A.,
    H.A.M. and M.K.S.; formal analysis, A.M.; investigation, K.M.O.; resources, A.M.;
    data curation, A.S.A.; writing—original draft preparation, A.S.A., H.A.M., M.K.S.,
    K.M.O., F.A.A. and A.M.; writing—review and editing, H.A.M., F.A.A. and M.K.S.;
    visualization, F.A.A.; supervision, H.A.M.; project administration, M.K.S.; funding
    acquisition, H.A.M. and M.K.S. All authors have read and agreed to the published
    version of the manuscript. Funding The authors extend their appreciation to the
    Deanship of Scientific Research at King Khalid University for funding this work
    through large group Research Project under grant number (RGP2/117/44). Princess
    Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2023R114),
    Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia. Research Supporting
    Project number (RSPD2023R838), King Saud University, Riyadh, Saudi Arabia. This
    study is partially funded by the Future University in Egypt (FUE). Institutional
    Review Board Statement Not applicable. Data Availability Statement Data sharing
    is not applicable to this article as no datasets were generated during the current
    study. Conflicts of Interest The authors declare that they have no conflicts of
    interest. The manuscript was written with contributions of all authors. All authors
    have given approval to the final version of the manuscript. References Joshi,
    A.; Pradhan, B.; Gite, S.; Chakraborty, S. Remote-Sensing Data and Deep-Learning
    Techniques in Crop Mapping and Yield Prediction: A Systematic Review. Remote Sens.
    2023, 15, 2014. [Google Scholar] [CrossRef] Bouguettaya, A.; Zarzour, H.; Kechida,
    A.; Taberkit, A.M. Deep learning techniques to classify agricultural crops through
    UAV imagery: A review. Neural Comput. Appl. 2022, 34, 9511–9536. [Google Scholar]
    [CrossRef] [PubMed] Zhao, H.; Duan, S.; Liu, J.; Sun, L.; Reymondin, L. Evaluation
    of five deep learning models for crop type mapping using sentinel-2 time se ries
    images with missing information. Remote Sens. 2021, 13, 2790. [Google Scholar]
    [CrossRef] Orynbaikyzy, A.; Gessner, U.; Conrad, C. Crop type classification using
    a combination of optical and radar remote sensing data: A review. Int. J. Remote
    Sens. 2019, 40, 6553–6595. [Google Scholar] [CrossRef] de Azevedo, R.P.; Dallacort,
    R.; Boechat, C.L.; Teodoro, P.E.; Teodoro, L.P.R.; Rossi, F.S.; Correia Filho,
    W.L.F.; Della-Silva, J.L.; Baio, F.H.R.; Lima, M.; et al. Remotely sensed imagery
    and machine learning for mapping of sesame crop in the Brazilian Midwest. Remote
    Sens. Appl. Soc. Environ. 2023, 32, 101018. [Google Scholar] [CrossRef] Wang,
    L.; Wang, J.; Liu, Z.; Zhu, J.; Qin, F. Evaluation of a deep-learning model for
    multispectral remote sensing of land use and crop classification. Crop J. 2022,
    10, 1435–1451. [Google Scholar] [CrossRef] Dash, R.; Dash, D.K.; Biswal, G.C.
    Classification of crop based on macronutrients and weather data using machine
    learning techniques. Results Eng. 2021, 9, 100203. [Google Scholar] [CrossRef]
    Kuang, X.; Guo, J.; Bai, J.; Geng, H.; Wang, H. Crop-Planting Area Prediction
    from Multi-Source Gaofen Satellite Images Using a Novel Deep Learning Model: A
    Case Study of Yangling District. Remote Sens. 2023, 15, 3792. [Google Scholar]
    [CrossRef] Suchi, S.D.; Menon, A.; Malik, A.; Hu, J.; Gao, J. Crop identification
    based on remote sensing data using machine learning approaches for fresno county,
    California. In Proceedings of the 2021 IEEE Seventh International Conference on
    Big Data Computing Service and Applications (BigDataService), Oxford, UK, 23–26
    August 2021; pp. 115–124. [Google Scholar] Kwak, G.H.; Park, N.W. Unsupervised
    domain adaptation with adversarial self-training for crop classification using
    remote sensing images. Remote Sens. 2022, 14, 4639. [Google Scholar] [CrossRef]
    Seydi, S.T.; Amani, M.; Ghorbanian, A. A dual attention convolutional neural network
    for crop classification using time-series Sentinel-2 imagery. Remote Sens. 2022,
    14, 498. [Google Scholar] [CrossRef] Reedha, R.; Dericquebourg, E.; Canals, R.;
    Hafiane, A. Transformer neural network for weed and crop classification of high
    resolution UAV images. Remote Sens. 2022, 14, 592. [Google Scholar] [CrossRef]
    Kordi, F.; Yousefi, H. Crop classification based on phenology information by using
    time series of optical and synthetic-aperture radar images. Remote Sens. Appl.
    Soc. Environ. 2022, 27, 100812. [Google Scholar] [CrossRef] Sun, Y.; Yao, N.;
    Luo, J.; Leng, P.; Liu, X. A spatiotemporal collaborative approach for precise
    crop planting structure mapping based on multi-source remote-sensing data. Int.
    J. Remote Sens. 2023, 1–17. [Google Scholar] [CrossRef] Li, H.; Zhang, C.; Zhang,
    Y.; Zhang, S.; Ding, X.; Atkinson, P.M. A Scale Sequence Object-based Convolutional
    Neural Network (SS-OCNN) for crop classification from fine spatial resolution
    remotely sensed imagery. Int. J. Digit. Earth 2021, 14, 1528–1546. [Google Scholar]
    [CrossRef] Zhai, P.; Li, S.; He, Z.; Deng, Y.; Hu, Y. Collaborative mapping rice
    planting areas using multisource remote sensing data. In Proceedings of the 2021
    IEEE International Geoscience and Remote Sensing Symposium IGARSS, Brussels, Belgium,
    11–16 July 2021; pp. 5969–5972. [Google Scholar] Sun, Y.; Luo, J.; Wu, T.; Zhou,
    Y.N.; Liu, H.; Gao, L.; Dong, W.; Liu, W.; Yang, Y.; Hu, X.; et al. Synchronous
    response analysis of features for remote sensing crop classification based on
    optical and SAR time-series data. Sensors 2019, 19, 4227. [Google Scholar] [CrossRef]
    [PubMed] You, H.; Lu, Y.; Tang, H. Plant disease classification and adversarial
    attack using SimAM-EfficientNet and GP-MI-FGSM. Sustainability 2023, 15, 1233.
    [Google Scholar] [CrossRef] Baştemur Kaya, C. A Novel Hybrid Method Based on the
    Marine Predators Algorithm and Adaptive Neuro-Fuzzy Inference System for the Identification
    of Nonlinear Systems. Symmetry 2023, 15, 1765. [Google Scholar] [CrossRef] Zhou,
    S.; Tan, B. Electrocardiogram soft computing using hybrid deep learning CNN-ELM.
    Appl. Soft Comput. 2020, 86, 105778. [Google Scholar] [CrossRef] Rineer, J.; Beach,
    R.; Lapidus, D.; O’Neil, M.; Temple, D.; Ujeneza, N.; Cajka, J.; Chew, R. Drone
    Imagery Classification Training Dataset for Crop Types in Rwanda. Version 1.0,
    Radiant MLHub. 2021. Available online: https://mlhub.earth/data/rti_rwanda_crop_type
    (accessed on 13 June 2023). Ahmed, M.A.; Aloufi, J.; Alnatheer, S. Satin Bowerbird
    Optimization with Convolutional LSTM for Food Crop Classification on UAV Imagery.
    IEEE Access 2023, 11, 41075–41083. [Google Scholar] [CrossRef] Chew, R.; Rineer,
    J.; Beach, R.; O’Neil, M.; Ujeneza, N.; Lapidus, D.; Miano, T.; Hegarty-Craver,
    M.; Polly, J.; Temple, D.S. Deep neural networks and transfer learning for food
    crop identification in UAV images. Drones 2020, 4, 7. [Google Scholar] [CrossRef]
    Disclaimer/Publisher’s Note: The statements, opinions and data contained in all
    publications are solely those of the individual author(s) and contributor(s) and
    not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility
    for any injury to people or property resulting from any ideas, methods, instructions
    or products referred to in the content.  © 2023 by the authors. Licensee MDPI,
    Basel, Switzerland. This article is an open access article distributed under the
    terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Almasoud, A.S.; Mengash, H.A.; Saeed, M.K.;
    Alotaibi, F.A.; Othman, K.M.; Mahmud, A. Remote Sensing Imagery Data Analysis
    Using Marine Predators Algorithm with Deep Learning for Food Crop Classification.
    Biomimetics 2023, 8, 535. https://doi.org/10.3390/biomimetics8070535 AMA Style
    Almasoud AS, Mengash HA, Saeed MK, Alotaibi FA, Othman KM, Mahmud A. Remote Sensing
    Imagery Data Analysis Using Marine Predators Algorithm with Deep Learning for
    Food Crop Classification. Biomimetics. 2023; 8(7):535. https://doi.org/10.3390/biomimetics8070535
    Chicago/Turabian Style Almasoud, Ahmed S., Hanan Abdullah Mengash, Muhammad Kashif
    Saeed, Faiz Abdullah Alotaibi, Kamal M. Othman, and Ahmed Mahmud. 2023. \"Remote
    Sensing Imagery Data Analysis Using Marine Predators Algorithm with Deep Learning
    for Food Crop Classification\" Biomimetics 8, no. 7: 535. https://doi.org/10.3390/biomimetics8070535
    Article Metrics Citations No citations were found for this article, but you may
    check on Google Scholar Article Access Statistics Article access statistics Article
    Views 8. Jan 18. Jan 28. Jan 7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28. Mar 0 250
    500 750 1000 1250 For more information on the journal statistics, click here.
    Multiple requests from the same IP address are counted as one view.   Biomimetics,
    EISSN 2313-7673, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: (Almasoud et al., 2023)
  journal: Biomimetics
  key_findings: The proposed model effectively classifies different types of food
    crops using visual monitoring techniques, demonstrating the feasibility of integrating
    advanced monitoring technologies into automated irrigation systems for enhanced
    crop management.
  limitations: null
  main_objective: To develop a food crop classification model using the integration
    of high-resolution cameras and computer vision algorithms for visual monitoring
    of crop growth, disease detection, and irrigation system performance assessment.
  relevance_evaluation: The paper is highly relevant to the point in the literature
    review that emphasizes the integration of advanced monitoring techniques, such
    as high-resolution cameras and computer vision algorithms, for automated irrigation
    systems. It provides specific insights into the application of these technologies
    for visual monitoring of crop growth, disease detection, and irrigation system
    performance assessment.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning
    models
  title: Remote Sensing Imagery Data Analysis Using Marine Predators Algorithm with
    Deep Learning for Food Crop Classification
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Arockia Selvakumar, A. D., Jeyabalan, A., Borah, P. R., Lingampally,
    P. K., & Schilberg, D. (2023). Advancements in Agricultural Automation: A Comprehensive
    Review of Artificial Intelligence and Humanoid Robotics in Farming. International
    Journal of Humanoid Robotics, 20(5), 2350012. https://doi.org/10.1142/S0219843623500123'
  authors:
  - Arockia Doss A.S.
  - Jeyabalan A.
  - Borah P.R.
  - Lingampally P.K.
  - Schilberg I.D.
  citation_count: '0'
  data_sources: Literature review
  description: This paper discusses about the modern techniques implemented in the
    field of agriculture which shaped the traditional farming to the smart farming
    (Agriculture 4.0). The rapid rise in the global population is one among the main
    reasons, besides monitoring of crops health and yield requires a huge labor force,
    which is also another reason for promoting intelligent systems into agriculture
    sector. Conventional farming methods are not suitable to meet this demand, which
    led robotics to associate with on-field agriculture by means of robot-based technologies
    like wheeled robots, ground vehicles (manned and unmanned) and aerial vehicles
    (manned and unmanned) that led to explore the possible advancement in agriculture
    3.0. Currently, the evolutionary techniques, such as Artificial Intelligence (AI)
    and IoT, are implemented in robotic vehicles to make them intelligent systems.
    Due to unprecedented climatic change and polluted ground water for the past few
    decades, the crops are being infested with new varieties of diseases. This requires
    new image processing techniques to classify the diseases based on color, texture
    and, shape of leaves. The incorporation of image processing technique into AI
    aids in deciding the appropriate amount of herbicide supplement to the plant based
    on the prediction of plant growth.
  doi: 10.1142/S0219843623500123
  explanation: 'The paper titled "Advancements in Agricultural Automation: A Comprehensive
    Review of Artificial Intelligence and Humanoid Robotics in Farming" provides an
    overview of the latest advancements in agricultural automation, including the
    use of AI, humanoid robotics, and other technologies to improve farming practices.
    While the paper doesn''t explicitly discuss the integration of high-resolution
    cameras and computer vision algorithms for visual monitoring of crop growth, disease
    detection, and irrigation system performance, it does touch upon the broader trend
    towards increased use of AI and robotics in agriculture.'
  extract_1: ''
  extract_2: ''
  full_citation: '>'
  full_text: '>

    "brought to you by UNIVERSITY OF NEBRASKA-LINCOLN Search My Cart Sign in    Institutional
    Access Skip main navigation Subject Journals Books Major Reference Works Resources
    For Partners Open Access About Us Help Cookies Notification We use cookies on
    this site to enhance your user experience. By continuing to browse the site, you
    consent to the use of our cookies. Learn More ×   International Journal of Humanoid
    RoboticsOnline Ready No Access Advancements in Agricultural Automation: A Comprehensive
    Review of Artificial Intelligence and Humanoid Robotics in Farming Arockia Selvakumar
    Arockia Doss , Abarna Jeyabalan , Priti Rekha Borah , Pavan Kalyan Lingampally
    , and Ing. Daniel Schilberg https://doi.org/10.1142/S0219843623500123Cited by:0
    (Source: Crossref) Previous Next PDF/EPUB Tools Share Cite Recommend To Library
    Abstract This paper discusses about the modern techniques implemented in the field
    of agriculture which shaped the traditional farming to the smart farming (Agriculture
    4.0). The rapid rise in the global population is one among the main reasons, besides
    monitoring of crops health and yield requires a huge labor force, which is also
    another reason for promoting intelligent systems into agriculture sector. Conventional
    farming methods are not suitable to meet this demand, which led robotics to associate
    with on-field agriculture by means of robot-based technologies like wheeled robots,
    ground vehicles (manned and unmanned) and aerial vehicles (manned and unmanned)
    that led to explore the possible advancement in agriculture 3.0. Currently, the
    evolutionary techniques, such as Artificial Intelligence (AI) and IoT, are implemented
    in robotic vehicles to make them intelligent systems. Due to unprecedented climatic
    change and polluted ground water for the past few decades, the crops are being
    infested with new varieties of diseases. This requires new image processing techniques
    to classify the diseases based on color, texture and, shape of leaves. The incorporation
    of image processing technique into AI aids in deciding the appropriate amount
    of herbicide supplement to the plant based on the prediction of plant growth.
    Keywords: Unmanned robotic systemagricultural roboticssmart farmingartificial
    intelligenceimage processing Remember to check out the Most Cited Articles! Check
    out these Notable Titles in Robotics We recommend Book Series: New Frontiers in
    Robotics Shoudong Huang et al., World Scientific Book Intelligent Control Techniques
    for Robotic Contact Tasks World Scientific Book The Application of Interactive
    Humanoid Robots in the History Education of Museums Under Artificial Intelligence
    Kuan Yang et al., International Journal of Humanoid Robotics, 2022 SOFTWARE AND
    COMMUNICATION INFRASTRUCTURE DESIGN OF THE HUMANOID ROBOT RH-1 World Scientific
    Book HUMANOID ROBOTICS RESEARCH IN IS/AIST World Scientific Book Humanoid robots
    to take centre stage at UN meet on AI TechXplore.com, 2023 The appearance of robots
    affects our perception of the morality of their decisions by University of Helsinki,
    TechXplore.com, 2021 AI ''good for the world''... says ultra-lifelike robot Phys.org
    Team programs a humanoid robot to communicate in sign language Phys.org, 2019
    Robot preachers, AI programs may undermine credibility of religious groups, study
    finds by American Psychological Association, MedicalXpress, 2023 Powered by Figures
    References Related Details Online Ready Metrics Downloaded 4 times History Received
    12 October 2022 Revised 26 June 2023 Accepted 19 July 2023 Published: 26 September
    2023 Keywords Unmanned robotic system agricultural robotics smart farming artificial
    intelligence image processing PDF download Resources For Authors For Booksellers
    For Librarians Copyright & Permissions Translation Rights How to Order Contact
    Us Sitemap    About Us & Help About Us News Author Services Help Links World Scientific
    Europe World Scientific China 世界科技 WS Education (K-12) Global Publishing 八方文化
    Asia-Pacific Biotech News World Century Privacy policy © 2024 World Scientific
    Publishing Co Pte Ltd Powered by Atypon® Literatum"'
  inline_citation: Arockia Selvakumar et al., (2023)
  journal: International Journal of Humanoid Robotics
  key_findings: AI and robotics have the potential to transform agriculture by improving
    efficiency, productivity, and sustainability. The use of high-resolution cameras
    and computer vision algorithms can further enhance the capabilities of automated
    irrigation systems by enabling visual monitoring of crop growth, disease detection,
    and irrigation system performance.
  limitations: The paper does not specifically address the integration of high-resolution
    cameras and computer vision algorithms for visual monitoring in automated irrigation
    systems.
  main_objective: To provide a comprehensive review of the latest advancements in
    agricultural automation, including the use of AI, humanoid robotics, and other
    technologies to improve farming practices.
  relevance_evaluation: The paper is moderately relevant to the specified point in
    the literature review, which focuses on the integration of high-resolution cameras
    and computer vision algorithms for visual monitoring in automated irrigation systems.
    While the paper does not directly address this specific topic, it provides a broader
    context on the use of AI and robotics in agriculture, which is valuable for understanding
    the potential of these technologies in irrigation management.
  relevance_score: '0.65'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: AI, humanoid robotics
  title: 'Advancements in Agricultural Automation: A Comprehensive Review of Artificial
    Intelligence and Humanoid Robotics in Farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Abbasi, R., Martinez, P., & Ahmad, R. (2023). Automated Visual Identification
    of Foliage Chlorosis in Lettuce Grown in Aquaponic Systems. Agriculture, 13(3),
    615. https://doi.org/10.3390/agriculture13030615
  authors:
  - Abbasi R.
  - Martinez P.
  - Ahmad R.
  citation_count: '0'
  data_sources: Visual data from lettuce crops grown in an aquaponics facility
  description: Chlorosis, or leaf yellowing, in crops is one of the quality issues
    that primarily occurs due to interference in the production of chlorophyll contents.
    The primary contributors to inadequate chlorophyll levels are abiotic stresses,
    such as inadequate environmental conditions (temperature, illumination, humidity,
    etc.), improper nutrient supply, and poor water quality. Various techniques have
    been developed over the years to identify leaf chlorosis and assess the quality
    of crops, including visual inspection, chemical analyses, and hyperspectral imaging.
    However, these techniques are expensive, time-consuming, or require special skills
    and precise equipment. Recently, computer vision techniques have been implemented
    in the agriculture field to determine the quality of crops. Computer vision models
    are accurate, fast, and non-destructive, but they require a lot of data to achieve
    high performance. In this study, an image processing-based solution is proposed
    to solve these problems and provide an easier, cheaper, and faster approach for
    identifying the chlorosis in lettuce crops grown in an aquaponics facility based
    on their sensory property, foliage color. The ‘HSV space segmentation’ technique
    is used to segment the lettuce crop images and extract red (R), green (G), and
    blue (B) channel values. The mean values of the RGB channels are computed, and
    a color distance model is used to determine the distance between the computed
    values and threshold values. A binary indicator is defined, which serves as the
    crop quality indicator associated with foliage color. The model’s performance
    is evaluated, achieving an accuracy of 95%. The final model is integrated with
    the ontology model through a cloud-based application that contains knowledge related
    to abiotic stresses and causes responsible for lettuce foliage chlorosis. This
    knowledge can be automatically extracted and used to take precautionary measures
    in a timely manner. The proposed application finds its significance as a decision
    support system that can automate crop quality monitoring in an aquaponics farm
    and assist agricultural practitioners in decision-making processes regarding crop
    stress management.
  doi: 10.3390/agriculture13030615
  explanation: The mentioned article uses advanced monitoring techniques to identify
    chlorosis (yellowing of leaves) in lettuce crops grown in aquaponic systems. The
    research is aimed at developing an automated system for monitoring crop health
    and quality by analyzing foliage color. Chlorosis is often caused by abiotic stresses,
    including inadequate environmental conditions, improper nutrient supply, and poor
    water quality.
  extract_1: Chlorosis, or leaf yellowing, in crops is one of the quality issues that
    primarily occurs due to interference in the production of chlorophyll contents.
    The primary contributors to inadequate chlorophyll levels are abiotic stresses,
    such as inadequate environmental conditions (temperature, illumination, humidity,
    etc.), improper nutrient supply, and poor water quality.
  extract_2: In this study, an image processing-based solution is proposed to solve
    these problems and provide an easier, cheaper, and faster approach for identifying
    the chlorosis in lettuce crops grown in an aquaponics facility based on their
    sensory property, foliage color.
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agriculture All Article Types Advanced   Journals
    Agriculture Volume 13 Issue 3 10.3390/agriculture13030615 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Academic Editor Maciej
    Zaborowicz Subscribe SciFeed Recommended Articles Related Info Link More by Authors
    Links Article Views 2083 Citations 1 Table of Contents Abstract Introduction Related
    Work Research Methodology Results and Discussion Conclusions Author Contributions
    Funding Institutional Review Board Statement Data Availability Statement Acknowledgments
    Conflicts of Interest Appendix A References Altmetric share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessArticle
    Automated Visual Identification of Foliage Chlorosis in Lettuce Grown in Aquaponic
    Systems by Rabiya Abbasi 1, Pablo Martinez 2 and Rafiq Ahmad 1,* 1 Aquaponics
    4.0 Learning Factory (AllFactory), Department of Mechanical Engineering, University
    of Alberta, 9211 116 St., Edmonton, AB T6G 2G8, Canada 2 Department of Architecture
    and Built Environment, Northumbria University, Newcastle upon Tyne NE7 7YT, UK
    * Author to whom correspondence should be addressed. Agriculture 2023, 13(3),
    615; https://doi.org/10.3390/agriculture13030615 Submission received: 20 February
    2023 / Revised: 28 February 2023 / Accepted: 2 March 2023 / Published: 3 March
    2023 (This article belongs to the Section Digital Agriculture) Download keyboard_arrow_down     Browse
    Figures Versions Notes Abstract Chlorosis, or leaf yellowing, in crops is one
    of the quality issues that primarily occurs due to interference in the production
    of chlorophyll contents. The primary contributors to inadequate chlorophyll levels
    are abiotic stresses, such as inadequate environmental conditions (temperature,
    illumination, humidity, etc.), improper nutrient supply, and poor water quality.
    Various techniques have been developed over the years to identify leaf chlorosis
    and assess the quality of crops, including visual inspection, chemical analyses,
    and hyperspectral imaging. However, these techniques are expensive, time-consuming,
    or require special skills and precise equipment. Recently, computer vision techniques
    have been implemented in the agriculture field to determine the quality of crops.
    Computer vision models are accurate, fast, and non-destructive, but they require
    a lot of data to achieve high performance. In this study, an image processing-based
    solution is proposed to solve these problems and provide an easier, cheaper, and
    faster approach for identifying the chlorosis in lettuce crops grown in an aquaponics
    facility based on their sensory property, foliage color. The ‘HSV space segmentation’
    technique is used to segment the lettuce crop images and extract red (R), green
    (G), and blue (B) channel values. The mean values of the RGB channels are computed,
    and a color distance model is used to determine the distance between the computed
    values and threshold values. A binary indicator is defined, which serves as the
    crop quality indicator associated with foliage color. The model’s performance
    is evaluated, achieving an accuracy of 95%. The final model is integrated with
    the ontology model through a cloud-based application that contains knowledge related
    to abiotic stresses and causes responsible for lettuce foliage chlorosis. This
    knowledge can be automatically extracted and used to take precautionary measures
    in a timely manner. The proposed application finds its significance as a decision
    support system that can automate crop quality monitoring in an aquaponics farm
    and assist agricultural practitioners in decision-making processes regarding crop
    stress management. Keywords: image processing; crop health; abiotic stresses;
    aquaponics; digital farming 1. Introduction Aquaponics is a controlled environment
    agriculture practice that combines aquaculture (farming of fish), hydroponics
    (soilless growing of plants), and nitrifying bacteria in a symbiotic environment.
    This agricultural technique promises to be a suitable alternative to global environmental
    and food problems [1,2]. Little gem romaine lettuce is one of the most common
    crops grown in the aquaponics system because it has a high growth rate, short
    growth cycle, high planting density, and low energy demand [3]. Just like traditional
    agriculture, lettuce crops grown in aquaponics may face abiotic stresses, such
    as inadequate environmental conditions (humidity, temperature, illumination, etc.),
    irregular supply of nutrient-enriched water due to the inaccurate design of the
    system, poor water quality (improper pH), and insufficient concentrations of required
    minerals, such as N-NO3, P, K, Ca, and Mg in the effluent [4,5]. These stresses
    adversely impact the growth and quality of lettuce in a plant factory. In addition
    to yields, the quality of crops is essential for market acceptance as they affect
    consumers’ purchase behavior [6]. Hence, it is vital to maintain the quality of
    crops and rectify the factors impacting them. The quality of crops is assessed
    using morphological traits (crop height, width, area, and volume), biomass production,
    nutritional value, and sensory attributes (color, texture, smell, and taste) [7].
    Visual indices, such as size, appearance, and green color, are the obvious quality
    indicators of lettuce that greatly impact consumers’ buying attitudes [6]. In
    this essence, these indices can be used to determine the quality of lettuce crops
    in a plant factory. Particularly, foliage color, which determines the chlorophyll
    content, is one of the key quality indicators [8]. The green color of the foliage
    represents that the crop is healthy, and the yellow color signifies that the crop
    is suffering from chlorosis. Leaf chlorosis is generally caused by different types
    of stresses, such as irregular illumination or temperature conditions, etc., which
    cause interference in the production of chlorophyll contents [4]. The irregular
    chlorophyll content levels represent the deficiency of secondary metabolites in
    lettuce, such as phenolic compounds, vitamins A and C, and carotenoid, which enhances
    the anti-oxidation ability of the human body and the suppression of inflammatory
    disease and cancer [9]. In order to achieve high-quality crops, it is necessary
    to identify leaf chlorosis and abiotic stresses by monitoring the crop throughout
    the growth cycle. The conventional method to identify leaf chlorosis and plant
    quality is based on visual observation, requiring certain expertise from agriculture
    practitioners [10]. Visual detection, however, is a time-consuming and laborious
    task, and there is a probability of misdiagnosis, especially in the early growth
    stages [10]. Other methods include chemical analyses and leaf color chart (LCC)
    matching, which, again, are costly, time-consuming, and destructive techniques.
    Chemical methods involve the collection of plant tissue for laboratory analyses
    of plant leaves. The Kjeldahl digestion assay is one of the most widely used chemical
    methods [11]. Although this method is accurate, sample preprocessing and delays
    in laboratory analyses hinder its widespread usage. The standard LCC tool is also
    available and used as a reference to estimate leaf color and plant quality [12].
    This technique is widely used in many countries but is a manual inspection process
    and, hence, time-consuming. In order to overcome these challenges, agriculture
    methods have been automated for years, and, hence, several non-destructive methods
    have been proposed to detect leaf chlorosis and plant quality. One of the methods
    is the spectral reflection method, which uses the property of chlorophyll with
    different reflection intensities at different wavebands to assess the quality
    of the plant. Several portable meters, such as SPAD (soil plant analysis development),
    are developed based on this method [13]. The spectral instruments are fast and
    fairly accurate but very expensive. Hyperspectral imaging and spectral remote
    sensing also use the spectral reflection principle [14]. Again, hyperspectral
    instruments are costly and require specific environmental conditions for proper
    sampling. With the development of technology, some researchers applied computer
    vision techniques to detect the quality of plants and leaf yellowing based on
    their nutritional status. Computer vision is a low-cost and non-destructive approach,
    but it requires a large amount of data for training and achieving the desired
    performance of the model [15]. Considering the aforementioned challenges, this
    paper proposes a methodology based on an image processing technique to identify
    chlorosis in lettuce crops grown in an aquaponics facility based on their foliage
    color. To be more certain, the estimation of chlorophyll content or nutrient deficiency
    is out of the scope of this study. The focus of the study is to determine the
    plant quality by extracting the foliage and its red (R), green (G), and blue (B)
    channel values using HSV space segmentation, where HSV stands for hue, saturation,
    and value [16]. The foliage color detection model is then developed using mean
    values of the R, G, and B channels and a color distance model. The color distance
    model calculates the foliage color difference from the threshold values. Numerous
    color distance models are available for this purpose, such as the Euclidean and
    color approximation distances (CIE76, CIE94, CIEDE2000, etc.). In this study,
    the Euclidean distance (ED) model is used, as it is the simplest method of finding
    the distance between two colors within an RGB color space [4]. Moreover, it works
    well when a single color is to be compared to a single color, and the need is
    to simply know whether a distance is greater or smaller, which is the case with
    the proposed model in this study. The model is built in a Jupyter notebook and
    saved in a local directory. An ontology model, ‘AquaONT’, developed by authors
    in previous work, is integrated with the proposed model through a cloud-based
    application built on Streamlit, which is an open-source app framework for machine
    learning and data science [17]. The ontology model provides information on causes
    and abiotic stresses responsible for leaf chlorosis in lettuce crops. The remainder
    of the paper is structured as follows: Section 2 will present the related work;
    Section 3 will explain the methodology used to develop the system; Section 4 will
    present the results and discussion along with model significance; and finally,
    Section 5 will discuss the conclusions and future work. 2. Related Work This section
    presents the recent and relevant image processing-based models that have used
    different color spaces and techniques to identify leaf chlorosis and assess the
    quality of crops. Yang et al. proposed a model based on a support vector machine
    (SVM) and advanced imaging processing techniques, such as image binarization,
    mask, and filling approaches for the extraction of selective color features, such
    as a* (CIELAB color space), G (green from RGB color space), and H (hue from HSV
    color space) to detect the yellow and rotten lettuce leaves in a hydroponics system
    [18]. The model has achieved an accuracy of 98.33%. Maity et al. proposed a model
    based on Otsu’s method and k-means clustering technique to detect faulty regions
    in leaves [19]. Wang et al. developed the HSV and decision tree-based method for
    the greenness identification of maize seedling images captured in the outdoor
    field [20]. Benjamin et al. proposed a methodology based on the color analysis
    technique to determine the quality of tomato leaves using Otsu’s method, SVM,
    k-NN (k-nearest neighbor), and multi-layer perceptron (MLP) [21]. Their model
    obtained an accuracy of 86.45% when classifying the healthy tomato leaves from
    the diseased tomato leaves and an accuracy of 97.39% when classifying the type
    of disease suffered by a diseased leaf. Sharad et al. developed a system based
    on a LAB (L*: lightness, a*: red/green value, b*: blue/yellow value) space-based
    color histogram, k-nearest neighbors, and random forests to detect the quality
    of apple leaves. This approach has achieved an accuracy of 98.63%. These models
    have made great contributions to literature, but some limitations are observed.
    For instance, most models have used images belonging to one scenario. Either they
    are taken in a lab environment (indoor) or outdoors in open-air fields. Secondly,
    some models have used non-destructive chemical approaches to collect the preliminary
    data, particularly while assessing the quality of plants based on chlorophyll
    content, nitrogen level, or nutrient deficiency. Considering the aforementioned,
    in this study, a fully automated, low-cost, and non-destructive model is proposed
    that is built while considering a variety of lettuce images from different sources.
    3. Research Methodology The block diagram, illustrating the five sequential modules
    of research methodology, is shown in Figure 1. Each module, along with its elements,
    is described in the next subsections. Figure 1. Research methodology outline.
    3.1. Data Preparation The image dataset is constructed using a variety of little
    gem romaine lettuce images from diverse sources. This involves top-view images
    of lettuce grown in Allfactory 4.0, an NFT-based aquaponics facility at the University
    of Alberta, Canada, focusing on smart indoor farming [2]. These images are divided
    into two classes based on the color of foliage: green foliage (no leaf chlorosis)
    and yellow foliage (leaf chlorosis). To increase the model flexibility to segment
    lettuce foliage, irrespective of background, and to ensure it correctly determines
    the plant’s health, the dataset is complemented with more lettuce images obtained
    from Ecosia, a search engine based in Berlin, Germany [22]. Figure 2 shows examples
    of some of the images. Figure 2. Image dataset: (a,b) acquired from aquaponics
    facility, and (c–f) are downloaded from ecosia.org. Next, the image augmentation
    process is performed to increase the dataset and reliability of the segmentation
    process, despite the location and orientation of the objects in the image, by
    generating new images from existing images. This study uses Albumentations, a
    Python library, for fast and flexible image augmentations [23]. The different
    augmentation techniques applied are the horizontal flip, vertical flip, 90° rotation,
    and glass noise. The new images are added to their respective classes. Figure
    3 shows examples of the augmentations. Figure 3. Data augmentation was performed
    on different images. 3.2. Image Segmentation Image segmentation was performed
    to extract the lettuce foliage from the background for further processing. This
    study uses the HSV segmentation model to segment the image [16]. There are two
    stages to the image segmentation process, which are detailed in the next two subsections.
    3.2.1. HSV Color Space The acquired images are in RGB format, where the color
    of any object in these images is represented with the combined values of the R,
    G, and B channels. The main problem with this color representation is that the
    objects’ colors are affected by variations in the illumination conditions [2].
    With the HSV color segmentation technique, as the name suggests, HSV color space
    is used, which describes the objects’ colors independent of the illumination effect
    [16]. The difference between various color spaces is usually based on color representation.
    For instance, the object’s color in the HSV color space is represented by three
    different parameters, namely the hue ( 𝐻 ), saturation ( 𝑆 ), and value ( 𝑉 ).
    H represents the color of the object, whereas the S and V values represent the
    illuminance state of the object’s color [16]. This type of description provides
    the ability to discriminate the color from the illuminance while avoiding the
    effect of the illumination changes on the object’s color. Therefore, the first
    stage of segmentation is to convert the image’s color space from RGB into HSV.
    Generally, the transformation process from RGB into HSV can be performed using
    the following Equations [24]: 𝑅 ′ = 𝑅 255  ,  𝐺 ′ = 𝐺 255  ,  𝐵 ′ = 𝐵 255   (1)
    𝑀=max( 𝑅 ′ ,  𝐺 ′ ,  𝐵 ′ ), 𝑚=min( 𝑅 ′ ,  𝐺 ′ ,  𝐵 ′ ),  (2) 𝐶=𝑀−𝑚 (3) 𝐻= ⎧ ⎩
    ⎨           0° 𝑖𝑓 𝐶=0  60°×( 𝐺 ′ − 𝐵 ′ 𝐶 𝑚𝑜𝑑 6) 𝑖𝑓 𝑀= 𝑅 ′ 60°×( 𝐺 ′
    − 𝐵 ′ 𝐶 +2) 𝑖𝑓 𝑀= 𝐺 ′ 60°×( 𝐺 ′ − 𝐵 ′ 𝐶 +4) 𝑖𝑓 𝑀= 𝐵 ′ (4) 𝑆= ⎧ ⎩ ⎨   0 𝑖𝑓 𝑀=0
    𝐶 𝑀  𝑖𝑓 𝑀≠0 (5) 𝑉=𝑀 (6) After the image transformation, a color bar is created,
    which provides intensity values for the ( 𝐻 ), ( 𝑆 ), and ( 𝑉 ) channels. These
    values are used in the next stage for segmenting the image. Figure 4 shows an
    example of the original image, its HSV channels, and the color bar format. Figure
    4. Illustration of image, its HSV channels, and color bar format. 3.2.2. Image
    Hue Thresholding The second stage of image segmentation is to determine the suitable
    threshold value to distinguish between the foreground and background. For this
    purpose, the hue image obtained in the first stage is used, as it provides a suitable
    grayscale image that can be used to classify objects based on color content. The
    upper and lower range of the hue channels is obtained from the color bar. This
    range is used to define an upper and lower threshold value for lettuce foliage
    in a hue image in the form of a mask. This mask is then applied to the R, G, and
    B channels of the original image, which are then stacked to obtain the segmented
    image. The final segmented image is saved in RGB format. In order to save time,
    the segmentation process is automated, and by the end of the process, each segmented
    image is saved in a common directory. 3.3. Foliage Color Detection Model Development
    The R, G, and B values of the lettuce foliage (foreground) are extracted from
    the segmented images. These images are represented as ( 𝑖 ) and ( 𝑗 ) for two
    classes: ( 𝑔 ) (green foliage—no chlorosis) and ( 𝑦 ) (yellow foliage—leaf chlorosis),
    respectively. The mean value of each color channel: red ( 𝜇 𝑅 ), green ( 𝜇 𝐺 ),
    and blue ( 𝜇 𝐵 ) for the two classes is computed using Equations (1) and (2).
    The elements of Equations (7) and (8) are determined using Equation (9) through
    to Equation (14). 𝜇 𝑔,𝑖 =[ 𝜇 𝑅,𝑖 , 𝜇 𝐺, 𝑖 , 𝜇 𝐵,𝑖 ] (7) 𝜇 𝑦, 𝑗 =[ 𝜇 𝑅,𝑗 , 𝜇 𝐺,𝑗
    , 𝜇 𝐵,𝑗 ] (8) where ( 𝜇 𝑔,𝑖 ) and ( 𝜇 𝑦,𝑗 ) represent the mean values of the three-color
    channels of the foreground (lettuce foliage) of two classes. Equations (9)–(14)
    are used for computing the mean values of the channels. 𝑅 𝑖/𝑗 = ∑ 𝑛 𝑅,𝑖/𝑗 1 𝑅
    𝑛,𝑖/𝑗 (9) 𝐺 𝑖/𝑗 = ∑ 𝑛 𝐺,𝑖/𝑗 1 𝐺 𝑛,𝑖/𝑗 (10) 𝐵 𝑖/𝑗 = ∑ 𝑛 𝐵,𝑖/𝑗 1 𝐵 𝑛,𝑖/𝑗 (11) 𝜇
    𝑅,𝑖/𝑗 = 𝑅 𝑖/𝑗 𝑛 𝑅,𝑖/𝑗 (12) 𝜇 𝐺,𝑖/𝑗 = 𝐺 𝑖/𝑗 𝑛 𝐺,𝑖/𝑗   (13) 𝜇 𝐵,𝑖/𝑗 = 𝐵 𝑖/𝑗 𝑛 𝐵,𝑖/𝑗
    (14) where ( 𝑅 𝑖/𝑗 ), ( 𝐺 𝑖/𝑗 ), and ( 𝐵 𝑖/𝑗 ) refer to the sum of the red, green,
    and blue values of lettuce foliage in two classes; ( 𝑖/𝑗 ) refers to either image
    belonging to the ( 𝑔 ) class or ( 𝑦 ) class; and ( 𝑛 𝑅,𝑖/𝑗 ), ( 𝑛 𝐺,𝑖/𝑗 ), and
    ( 𝑛 𝐵,𝑖/𝑗 ) represent the R, G, and B counts of lettuce foliage, respectively.
    The obtained background in segmented images is black. Hence, the R, G, and B counts
    and values of the background are not included while determining the mean value
    of the R, G, and B channels for the foreground. The process of calculating the
    mean values of the R, G, and B channels was, again, automated to save time. The
    values for each channel were automatically saved in an Excel file. While saving
    the results, it is ensured that the mean values of R, G, and B are saved for their
    respective image label and class category, ( 𝑔 ) and ( 𝑦 ). Next, the reference
    or threshold values (( 𝑔 𝑟𝑒𝑓 ) and ( 𝑦 𝑟𝑒𝑓 )) were determined for both ( 𝑔 ) and
    ( 𝑦 ) classes, using Equations (15) and (16). To compute ( 𝑔 𝑟𝑒𝑓 ), three average
    values are calculated, which are related to the mean red, mean green, and mean
    blue values of the images saved in the Excel file for the ( 𝑔 ) category. The
    total number of mean values for each channel is ( 𝑚 ). The first average value
    is obtained by summing all the green channel values and dividing the results by
    the total number of green values ( 𝑚 ). Similarly, the second and third average
    values are obtained by summing all the red channel values and all blue channel
    values of all images in the ( 𝑚 ) category and dividing the results by the number
    of red ( 𝑚 ) and blue values ( 𝑚 ), respectively. A similar computation is done
    for ( 𝑦 𝑟𝑒𝑓 ) while considering the channel values and their count ( 𝑙 ) for images
    in the ( 𝑦 ) category. Equations (17)–(22) are used to calculate ( 𝑔 𝑟𝑒𝑓 ) and
    ( 𝑦 𝑟𝑒𝑓 ). 𝑔 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑚 , 𝑥 ̲ 𝐺, 𝑚 , 𝑥 ̲ 𝐵,𝑚 ] (15) 𝑦 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑙 , 𝑥 ̲
    𝐺, 𝑙 , 𝑥 ̲ 𝐵,𝑙 ] (16) 𝑥 ̲ 𝑅,𝑚 = ∑ 𝑚 1 𝑅 𝑚 𝑚 (17) 𝑥 ̲ 𝐺,𝑚 = ∑ 𝑚 1 𝐺 𝑚 𝑚 (18) 𝑥
    ̲ 𝐵,𝑚 = ∑ 𝑚 1 𝐵 𝑚 𝑚 (19) 𝑥 ̲ 𝑅,𝑙 = ∑ 𝑙 1 𝑅 𝑙 𝑙 (20) 𝑥 ̲ 𝐺,𝑙 = ∑ 𝑙 1 𝐺 𝑙 𝑙 (21)
    𝑥 ̲ 𝐵,𝑙 = ∑ 𝑙= 1 𝐵 𝑙 𝑙 (22) where  ( 𝑥 ̲ 𝑅,𝑚 ) ,  ( 𝑥 ̲ 𝐺,𝑚 ) , and ( 𝑥 ̲ 𝐵,𝑚
    )  are the averages of three channel values in the ( 𝑔 ) category and  ( 𝑅 𝑚 )
    ,  ( 𝐺 𝑚 ) , and  ( 𝐵 𝑚 ) , are the values of three channels in the ‘g’ category.
    Likewise,  ( 𝑥 ̲ 𝑅,𝑙 ) ,  ( 𝑥 ̲ 𝐺,𝑙 ) , and ( 𝑥 ̲ 𝐵,𝑙 )  are the averages of three
    channel values in the ( 𝑦 ) category and  ( 𝑅 𝑙 ) ,  ( 𝐺 𝑙 ) , and  ( 𝐵 𝑙 ) ,
    are the values of three channels in the ( 𝑦 ) category. After determining the
    reference or threshold values, the color distance model was used to compute the
    foliage color difference from the threshold values. The Euclidean distance (ED)
    model was used in this study, and its general equation is presented below [4].
    𝑑= Δ 𝑅 2 +Δ 𝐺 2 +Δ 𝐵 2 − − − − − − − − − − − − − − √ (23) where  Δ𝑅= 𝑅 2 − 𝑅 1
    ,  Δ𝐺= 𝐺 2 − 𝐺 1 , and  Δ𝐵= 𝐵 2 − 𝐵 1 . Based on the ED model, two distances  (
    𝑑 1 )  and  ( 𝑑 2 ) , were computed using two threshold values: ( 𝑔 𝑟𝑒𝑓 ) and
    ( 𝑦 𝑟𝑒𝑓 ), respectively.  ( 𝑑 1 )  determines the distance from the green color
    threshold, whereas  ( 𝑑 2 )  determines the distance from the yellow color threshold.
    For single foliage, both  ( 𝑑 1 )  and  ( 𝑑 2 )  are determined. A lower value
    of  ( 𝑑 1 )  and a higher value of  ( 𝑑 2 )  suggests that the color patterns
    of foliage are closer to ( 𝑔 𝑟𝑒𝑓 ) or, in other words, green tones. Conversely,
    a lower value of  ( 𝑑 2 )  and a higher value of  ( 𝑑 1 )  suggests that color
    patterns of foliage are closer to ( 𝑦 𝑟𝑒𝑓 ) or, in other words, yellow tones.
    The governing equations for  ( 𝑑 1 )  and  ( 𝑑 2 )  are given below. 𝑑 1 = ( 𝑥
    𝑅 − 𝑥 ̲ 𝑅,𝑚 ) 2 + ( 𝑥 𝐺 − 𝑥 ̲ 𝐺,𝑚 ) 2 + ( 𝑥 𝐵 − 𝑥 ̲ 𝐵,𝑚 ) 2 − − − − − − − − −
    − − − − − − − − − − − − − − − − − − − − − − − − − √ (24) 𝑑 2 = ( 𝑥 𝑅 − 𝑥 ̲ 𝑅,𝑙
    ) 2 + ( 𝑥 𝐺 − 𝑥 ̲ 𝐺,𝑙 ) 2 + ( 𝑥 𝐵 − 𝑥 ̲ 𝐵,𝑙 ) 2 − − − − − − − − − − − − − − −
    − − − − − − − − − − − − − − − − − √ (25) where  ( 𝑥 𝑅 ) ,  ( 𝑥 𝐺 ) , and  ( 𝑥
    𝐵 )  are the mean values of three channels (R, G, B) of the foreground in the
    segmented image of the test samples. Lastly, the quality indicator  (𝑄)  is defined
    as a function of  ( 𝑑 1 )  and  ( 𝑑 2 )  for evaluating the plants’ quality based
    on their foliage color. In this context, when green foliage with no leaf depigmentation
    is detected, the value of  (𝑄)  is equal to 1, which implies that the crop is
    healthy. On the other hand, when yellow foliage with leaf depigmentation is detected,
    the value of  (𝑄)  is equal to 0, suggesting that the crop is unhealthy.  (𝑄)  is
    represented as below: 𝑄=𝑓( 𝑑 1 , 𝑑 2 )={ 1 𝑖𝑓  𝑑 1 < 𝑑 2 0 𝑖𝑓  𝑑 2 < 𝑑 1   (26)
    3.4. Ontology Model The complete development and details of all concepts and instances
    of an ontology model, ‘AquaONT’, is available in previous work by the authors
    [17]. AquaONT is a unified ontology model that represents and stores the essential
    knowledge of an aquaponic 4.0 system. It comprises six concepts: Consumer_Product,
    Ambient_Environment, Contextual_Data, Production_System, Product_Quality, and
    Production_Facility. In this study, two classes, ‘Consumer_Product’ and ‘Product_Quality’,
    are used for knowledge extraction. The ‘Consumer_Product’ class provides an abstract
    view of the type, growth status, and growth parameters of ready-to-harvest crops
    in an aquaponics system. Whereas the ‘Product_Quality’ class provides knowledge
    on the crop attributes related to pathology (abiotic and biotic stresses, causes,
    and the ways and means by which these can be managed or controlled), morphology
    (canopy dimensions, such as area, length, width, etc.) and foliage color. The
    lettuce crop is considered in this study. The crop growth and quality attributes
    are defined as instances of respective classes, which are extracted once the crop
    foliage is detected as yellow (or leaf chlorosis is detected). Figure 5 shows
    the hierarchical architecture of the ‘Consumer_Product’ and ‘Product_Quality’
    classes, with their instances for the lettuce crop in Protégé7 (an open-source
    ontology editor and framework developed at Stanford University) environment. Figure
    5. Ontology model showing classes, instances, and relationships between them.
    3.5. Cloud-Based Application The proposed foliage detection and ontology models
    are deployed on a cloud-based application built on Streamlit. The app’s layout
    is shown in Figure A1, Figure A2, Figure A3 and Figure A4 in Appendix A. The app
    works in six stages. The first and second stages are associated with two user
    inputs, “Select the Model” and “Upload Image”, as shown in Figure A1 in Appendix
    A. The first input allows the user to select a relevant quality evaluation model.
    This app has other quality models integrated into it, which are out of the scope
    of this study. In this study, the relevant model is “Lettuce Foliage Pigment”.
    After selecting the model, the image is selected using the second input. The third
    and fourth stages are linked with two widgets, “Preprocess and Segment Image”
    and “Determine the Crop Status”, respectively, shown in Figure A2 in Appendix
    A, that run the sub-processes associated with the model. As the name suggests,
    the first widget activates the segmentation algorithm, which preprocesses and
    segments the image selected by the user in the second stage. Likewise, the second
    widget activates the model developed in the study. The model determines the status
    of the crop and displays the results on the application panel. In the fifth stage,
    the sensor data from the dashboard is acquired and displayed to monitor the environmental
    conditions, as shown in Figure A3 in Appendix A. By clicking ‘Sensor Data’, the
    most recent data will be displayed. In the sixth stage, a widget is developed,
    ‘Causes and Treatments’, which is linked with ‘AquaONT’. This widget extracts
    knowledge from the ontology model related to the possible causes of leaf yellowing
    in the aquaponics facility. Figure A4 in Appendix A show the sixth stage of the
    app when yellow foliage is detected. 4. Results and Discussion This section first
    presents the validation of the proposed method by a case study. Then, the performance
    of the proposed method is compared with existing similar methods. To validate
    the proposed model, twenty healthy seedings were placed in NFT-based hydroponic
    systems for five weeks (plantation cycle), after which lettuce was harvested.
    A 12MP Sony Exmor RS camera sensor was used to capture the crop images during
    this period. Twenty images of 4032 × 3024 pixels (one image for one lettuce plant)
    were captured daily at 9:00 am from the top while keeping the distance between
    the camera and channel at a value of 40 cm throughout the plantation cycle, i.e.,
    five weeks. In total, 700 images of plants were collected over five weeks. During
    the first three weeks, no significant difference was observed in the color of
    the foliage. After the third week, foliage chlorosis was observed in eight lettuce
    plants. Therefore, for further processing, the images captured in the last two
    weeks of the plantation cycle were considered for model validation. In total,
    280 images were divided into two classes based on the color of the foliage: Green
    Foliage—No Leaf Chlorosis (168 images) and Yellow Foliage—Leaf Chlorosis (112
    images). The dataset is complemented with more lettuce images with green (32)
    and yellow (88) foliage, downloaded from Ecosia. The images were added to their
    respective classes. All the images were resized to 1000 × 1000 pixels and saved
    in JPG format. The augmentation process was then performed. In total, 100 images
    (50 from both classes) were selected randomly for the augmentation, which created
    100 new images. The new images were added to their respective classes, increasing
    the length of the dataset to 500 images. Half of these images belong to the (
    𝑔 ) class, and half belong to the ( 𝑦)  class and, hence, are saved in two folders
    named ( 𝑔 ) and ( 𝑦),  respectively. Out of 500 images, 100 random images (50
    from each folder) were extracted and saved in a separate validation folder to
    be used for the model evaluation. In order to complement the validation data,
    20 images were randomly selected (10 from each class), and their R, G, and B values
    were altered using Adobe Photoshop in a way that the healthy-looking lettuce appears
    yellow, and the unhealthy lettuce appears green. The validation dataset now had
    120 images in total. Figure 6 shows an example of the new images generated for
    the validation dataset. Figure 6. Example of images generated in Adobe Photoshop
    (left original images, right for the altered images). The segmentation was then
    performed on all 520 images in the dataset. Figure 7 shows an example of the segmented
    images. For the computation of the threshold values, 400 images (g and y folder)
    were used by following the process mentioned in Section 3.3. The R, G, and B values
    and their counts were computed for the foreground (lettuce foliage) of 400 segmented
    images in two classes, ( 𝑔 ) and ( 𝑦 ). The mean values of the R, G, and B channels
    were then computed. Each class has 200 foliage images, so for each class 3 × 200
    = 600 mean values (3 refers to 3 channels of an image) were obtained, which were
    automatically saved in an excel file. Figure 7. Example of segmented images ((a,c):
    segmented green lettuce; (b,d): segmented altered yellowed lettuce). Out of the
    600 means values for each class, 200 belong to the red channel, 200 belong to
    the green channel, and 200 belong to the blue channel. The threshold values (
    𝑔 𝑟𝑒𝑓 ) and ( 𝑦 𝑟𝑒𝑓 ) were obtained by dividing the mean values of the three channels
    by 200, which are given below. 𝑔 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑚 , 𝑥 ̲ 𝐺, 𝑚 , 𝑥 ̲ 𝐵,𝑚 ]=[123.4,
    138.2, 19.8] (27) 𝑦 𝑟𝑒𝑓 =[ 𝑥 ̲ 𝑅,𝑙 , 𝑥 ̲ 𝐺, 𝑙 , 𝑥 ̲ 𝐵,𝑙 ]=[156.6, 155.8, 22.2]  (28)
    The model was validated using a validation dataset comprising 120 different segmented
    images belonging to two classes, ( 𝑔 ) and ( 𝑦) . The mean values of the three
    channels were computed for each image and were inserted into Equations (27) and
    (28) in place of  ( 𝑥 𝑅 ) ,  ( 𝑥 𝐺 ) , and  ( 𝑥 𝐵 ) , along with the reference
    values ( 𝑔 ) and ( 𝑦)  computed above. The  ( 𝑑 1 )  and  ( 𝑑 2 )  were determined
    for all 120 images in the validation dataset using Equations (17) and (18), respectively.
    The quality indicator,  (𝑄) , was also determined using Equation (19) for 120
    images. The performance of the model on the validation dataset was then evaluated
    by analyzing the ground truth  (𝑄)  value and predicted  (𝑄)  value. In the validation
    dataset, 60 images have a ground truth  (𝑄)  value of 1, meaning these images
    contain healthy and green lettuce foliage, and 60 images have a ground truth value
    of 0, meaning these Images contain unhealthy and yellow lettuce foliage. The performance
    is presented in the form of a confusion matrix (CM), shown in Figure 8 [2]. Figure
    8. Confusion Matrix. The different values of the CM are interpreted as: True Positive
    (TP) = 58. Thus, 58 plants were healthy, and the model correctly classified them
    healthy as well. True Negative (TN) = 57. Thus, 57 plants were unhealthy, and
    the model correctly classified them unhealthy as well. False Positive (FP) = 3.
    Thus, 3 plants were unhealthy, but the model incorrectly classified them as healthy.
    False Negative (FN) = 2. Thus, 2 plants were healthy, but the model incorrectly
    classified them as unhealthy. The performance metrics based on CM are also computed
    using the formulae given below and are summarized in Table 1. 𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦= 𝑇𝑃+𝑇𝑁
    𝑇𝑃+𝑇𝑁+𝐹𝑃+𝐹𝑁 (29) 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= 𝑇𝑃 𝑇𝑃+𝐹𝑃 (30) 𝑅𝑒𝑐𝑎𝑙𝑙= 𝑇𝑃 𝑇𝑃+𝐹𝑁 (31) 𝐹1−𝑆𝑐𝑜𝑟𝑒= 2×𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛×𝑅𝑒𝑐𝑎𝑙𝑙
    𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑅𝑒𝑐𝑎𝑙𝑙 (32) Table 1. Summary of the performance metrics. In Table 1,
    N (truth) tells the number of actual cases in a particular class, and N (classified)
    tells the number of predicted cases belonging to a class. Table 1 shows that the
    model has achieved an average accuracy of 95%, precision of 96%, recall of 96%,
    and F1-Score of 96%. The model has correctly classified 115 cases out of a total
    of 120 cases. Figure 9 shows an example of correctly classified cases. Figure
    9. Example of correctly classified cases. To further investigate the performance
    of the proposed methodology, it was compared with the existing vision-based methods
    mentioned in Section 2. These methods were implemented on the dataset prepared
    in this study, and their performance was evaluated using the metrics based on
    CM, which are presented in Table 2. The results show that the proposed method
    has outperformed the similar existing methods, achieving an average accuracy of
    96%, precision, recall, and F1-score of 96%. The method proposed by Sharad et
    al. has shown appreciable performance when implemented on the dataset prepared
    in this study by achieving an average accuracy of 94%, a precision of 94%, a recall
    of 95%, and an F1-score of 94.45% [25]. Whereas, with their apple leaf dataset,
    they have achieved an accuracy of 98.63%. Table 2. Performance metrics of existing
    methods. The final model was then deployed in the aquaponics facility through
    a cloud-based application. This time, instead of manually taking the images, four
    ELP 1080P webcams (2.8–12 mm HD Varifocal Lens) were installed at a distance of
    40 cm from the channels for image acquisition. Each camera is programmed through
    a Raspberry Pi 4 (Model B Rev 1) controller to take one image per day at 9:00
    am, which along with the sensor values from WSM, are wirelessly uploaded to the
    ‘IoT enabled Aquaponics Dashboard’ developed by the authors in previous work [26].
    The images and sensor data are available on the cloud as well as locally, and
    the app developed in this study can access them. The ontology model discussed
    in Section 3.4 was also integrated with the proposed model and deployed on a cloud-based
    application. Once the health status of the lettuce crop was identified as ‘Yellow
    Foliage—Leaf Chlorosis’, the potential causes were automatically extracted from
    the ontology model and displayed on the application panel. Figure A1, Figure A2,
    Figure A3 and Figure A4 in Appendix A show an example of the working of the proposed
    method and application for a lettuce crop when its foliage was detected to be
    yellow. The primary causes of lettuce foliage chlorosis could be inadequate environmental
    conditions (humidity, air temperature), poor water quality (inadequate pH or EC),
    nutrient deficiency, etc. By analyzing sensor data and the possible causes of
    leaf chlorosis, it is possible to reach the specific cause of the problem. For
    instance, if sensor data show that all the parameters are within their optimal
    ranges, then the problem could be related to nutrient delivery or the design of
    the system. A reasonable treatment can be suggested after problem identification.
    The proposed model was developed using open-source frameworks, and, hence, it
    can easily be expanded or adjusted as per the requirement by adjusting the threshold
    values. The significance of the model is that it is fully automated and offers
    a non-destructive, low-cost and reliable approach to identifying leaf chlorosis
    and determining the quality of lettuce plants along with the possible causes.
    In contrast to the computer vision and machine learning-based models, the proposed
    methodology requires less data. 5. Conclusions This study discusses the major
    problem of lettuce foliage chlorosis in an aquaponics context. The ‘HSV Color
    Segmentation’ image processing approach was used to segment the lettuce images
    obtained from various resources. The segmented images were divided into two classes,
    ‘Green Foliage-No Leaf Chlorosis’ and ‘Yellow Foliage-Leaf Chlorosis’. Then, the
    foliage color detection model was developed, and a quality indicator was defined
    to identify leaf chlorosis and determine the quality of the lettuce crop. The
    model is validated, achieving an overall accuracy of 95%. The performance of the
    model was also compared with existing similar methods. The results show that the
    proposed method has outperformed these existing methods. A cloud-based application
    was then developed, where the final model was deployed. The ontology model that
    contains knowledge related to the causes of lettuce crop chlorosis was also integrated
    with the final model. The proposed system proves to be accurate and flexible enough
    to be used in real scenarios and, hence, is not limited to being disturbed by
    potentially changing conditions and environments. For future work, the system
    will be extended to include other crops. Moreover, images with complex backgrounds
    and multiple objects will also be added to the dataset. The ontology model will
    also be extended to include the specific treatments for potential causes of leaf
    chlorosis. Author Contributions Conceptualization, R.A. (Rabiya Abbasi), P.M.
    and R.A. (Rafiq Ahmad); methodology, R.A. (Rabiya Abbasi) and P.M.; validation,
    R.A. (Rabiya Abbasi); investigation, R.A. (Rabiya Abbasi); writing—original draft,
    R.A. (Rabiya Abbasi); writing—review and editing, P.M. and R.A. (Rafiq Ahmad);
    supervision, P.M. and R.A. (Rafiq Ahmad); project administration, R.A. (Rafiq
    Ahmad); funding acquisition, P.M. and R.A. (Rafiq Ahmad). All authors have read
    and agreed to the published version of the manuscript. Funding The authors acknowledge
    the financial support of this work from the Natural Sciences and Engineering Research
    Council of Canada (NSERC) (Grants File No. ALLRP 545537-19 and RGPIN-2017-04516).
    Institutional Review Board Statement Not applicable. Data Availability Statement
    The data that support the findings of this study are available from the corresponding
    author, R.A., upon reasonable request. Acknowledgments The authors would like
    the acknowledge the support from the members of the LIMDA Lab and the ALLFactory
    at the University of Alberta. Conflicts of Interest The authors declare no conflict
    of interest. Appendix A Figure A1. Stages 1 and 2 of cloud-based application.
    Figure A2. Stages 3 and 4 of cloud-based application. Figure A3. Stage 5 of cloud-based
    application. Figure A4. Stage 6 of cloud-based application. References Abbasi,
    R.; Martinez, P.; Ahmad, R. An ontology model to support the automated design
    of aquaponic grow beds. Procedia CIRP 2021, 100, 55–60. [Google Scholar] [CrossRef]
    Reyes-Yanes, A.; Martinez, P.; Ahmad, R. Real-time growth rate and fresh weight
    estimation for little gem romaine lettuce in aquaponic grow beds. Comput. Electron.
    Agric. 2020, 179, 105827. [Google Scholar] [CrossRef] Lin, K.H.; Huang, M.Y.;
    Huang, W.D.; Hsu, M.H.; Yang, Z.W.; Yang, C.M. The effects of red, blue, and white
    light-emitting diodes on the growth, development, and edible quality of hydroponically
    grown lettuce (Lactuca sativa L. var. capitata). Sci. Hortic. 2013, 150, 86–91.
    [Google Scholar] [CrossRef] Haider, T.; Farid, M.S.; Mahmood, R.; Ilyas, A.; Khan,
    M.H.; Haider, S.T.A.; Chaudhry, M.H.; Gul, M. A Computer-Vision-Based Approach
    for Nitrogen Content Estimation in Plant Leaves. Agriculture 2021, 11, 766. [Google
    Scholar] [CrossRef] Taha, M.F.; Abdalla, A.; Elmasry, G.; Gouda, M.; Zhou, L.;
    Zhao, N.; Liang, N.; Niu, Z.; Hassanein, A.; Al-Rejaie, S.; et al. Using Deep
    Convolutional Neural Network for Image-Based Diagnosis of Nutrient Deficiencies
    in Plants Grown in Aquaponics. Chemosensors 2022, 10, 45. [Google Scholar] [CrossRef]
    Matysiak, B.; Ropelewska, E.; Wrzodak, A.; Kowalski, A.; Kaniszewski, S. Yield
    and Quality of Romaine Lettuce at Different Daily Light Integral in an Indoor
    Controlled Environment. Agronomy 2022, 12, 1026. [Google Scholar] [CrossRef] Abbasi,
    R.; Martinez, P.; Ahmad, R. The digitization of agricultural industry—A systematic
    literature review on agriculture 4.0. Smart Agric. Technol. 2022, 2, 100042. [Google
    Scholar] [CrossRef] Kowalczyk, K.; Sieczko, L.; Goltsev, V.; Kalaji, H.M.; Gajc-Wolska,
    J.; Gajewski, M.; Gontar, Ł.; Orliński, P.; Niedzińska, M.; Cetner, M.D. Relationship
    between chlorophyll fluorescence parameters and quality of the fresh and stored
    lettuce (Lactuca sativa L.). Sci. Hortic. 2018, 235, 70–77. [Google Scholar] [CrossRef]
    Song, J.; Huang, H.; Hao, Y.; Song, S.; Zhang, Y.; Su, W.; Liu, H. Nutritional
    quality, mineral and antioxidant content in lettuce affected by interaction of
    light intensity and nutrient solution concentration. Sci. Rep. 2020, 10, 2796.
    [Google Scholar] [CrossRef] [Green Version] Cook, S.E.; Bramley, R.G.V. Coping
    with variability in agricultural production -implications for soil testing and
    fertiliser management. Commun. Soil Sci. Plant Anal. 2000, 31, 1531–1551. [Google
    Scholar] [CrossRef] Kjeldahl, J. Neue Methode zur Bestimmung des Stickstoffs in
    organischen Körpern. Z. Anal. Chem. 1883, 22, 366–382. [Google Scholar] [CrossRef]
    [Green Version] Yang, W.H.; Peng, S.; Huang, J.; Sanico, A.L.; Buresh, R.J.; Witt,
    C. Using Leaf Color Charts to Estimate Leaf Nitrogen Status of Rice. Agron. J.
    2003, 95, 212–217. [Google Scholar] [CrossRef] Markwell, J.; Osterman, J.C.; Mitchell,
    J.L. Calibration of the Minolta SPAD-502 leaf chlorophyll meter. Photosynth. Res.
    1995, 46, 467–472. [Google Scholar] [CrossRef] Zheng, H.; Cheng, T.; Li, D.; Zhou,
    X.; Yao, X.; Tian, Y.; Cao, W.; Zhu, Y. Evaluation of RGB, Color-Infrared and
    Multispectral Images Acquired from Unmanned Aerial Systems for the Estimation
    of Nitrogen Accumulation in Rice. Remote Sens. 2018, 10, 824. [Google Scholar]
    [CrossRef] [Green Version] Tao, M.; Ma, X.; Huang, X.; Liu, C.; Deng, R.; Liang,
    K.; Qi, L. Smartphone-based detection of leaf color levels in rice plants. Comput.
    Electron. Agric. 2020, 173, 105431. [Google Scholar] [CrossRef] Burdescu, D.D.;
    Brezovan, M.; Ganea, E.; Stanescu, L. A new method for segmentation of images
    represented in a HSV color space. In Proceedings of the Advanced Concepts for
    Intelligent Vision Systems: 11th International Conference, ACIVS 2009, Bordeaux,
    France, 28 September–2 October 2009; Springer: Berlin/Heidelberg, Germany, 2009;
    pp. 606–617. [Google Scholar] [CrossRef] Abbasi, R.; Martinez, P.; Ahmad, R. An
    ontology model to represent aquaponics 4.0 system’s knowledge. Inf. Process. Agric.
    2022, 9, 514–532. [Google Scholar] [CrossRef] Yang, R.; Wu, Z.; Fang, W.; Zhang,
    H.; Wang, W.; Fu, L.; Majeed, Y.; Li, R.; Cui, Y. Detection of abnormal hydroponic
    lettuce leaves based on image processing and machine learning. Inf. Process. Agric.
    2021, 10, 1–10. [Google Scholar] [CrossRef] Maity, S.; Sarkar, S.; Vinaba Tapadar,
    A.; Dutta, A.; Biswas, S.; Nayek, S.; Saha, P. Fault Area Detection in Leaf Diseases
    Using K-Means Clustering. In Proceedings of the 2018 2nd International Conference
    on Trends in Electronics and Informatics (ICOEI), Tirunelveli, India, 11–12 May
    2018; pp. 1538–1542. [Google Scholar] [CrossRef] [Green Version] Yang, W.; Wang,
    S.; Zhao, X.; Zhang, J.; Feng, J. Greenness identification based on HSV decision
    tree. Inf. Process. Agric. 2015, 2, 149–160. [Google Scholar] [CrossRef] [Green
    Version] Luna-Benoso, B.; Martínez-Perales, J.C.; Cortés-Galicia, J.; Flores-Carapia,
    R.; Silva-García, V.M. Detection of Diseases in Tomato Leaves by Color Analysis.
    Electronics 2021, 10, 1055. [Google Scholar] [CrossRef] Streamlit • The Fastest
    Way to Build and Share Data Apps [WWW Document], n.d. Available online: https://streamlit.io/
    (accessed on 7 June 2022). Buslaev, A.; Iglovikov, V.I.; Khvedchenya, E.; Parinov,
    A.; Druzhinin, M.; Kalinin, A.A. Albumentations: Fast and flexible image augmentations.
    Information 2020, 11, 125. [Google Scholar] [CrossRef] [Green Version] Loresco,
    P.J.M.; Valenzuela, I.C.; Dadios, E.P. Color Space Analysis Using KNN for Lettuce
    Crop Stages Identification in Smart Farm Setup. In Proceedings of the TENCON 2018-2018
    IEEE Region 10 Conference, Jeju, Republic of Korea, 28–31 October 2018; pp. 2040–2044.
    [Google Scholar] [CrossRef] Hasan, S.; Jahan, S.; Islam, M.I. Disease detection
    of apple leaf with combination of color segmentation and modified DWT. J. King
    Saud Univ.-Comput. Inf. Sci. 2022, 34, 7212–7224. [Google Scholar] [CrossRef]
    Abbasi, R.; Martinez, P.; Ahmad, R. Data acquisition and monitoring dashboard
    for IoT enabled aquaponics facility. In Proceedings of the 2022 10th International
    Conference on Control, Mechatronics and Automation (ICCMA), Luxembourg, 9–12 November
    2022; IEEE: Piscataway, NJ, USA, 2022. [Google Scholar] [CrossRef] Disclaimer/Publisher’s
    Note: The statements, opinions and data contained in all publications are solely
    those of the individual author(s) and contributor(s) and not of MDPI and/or the
    editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to
    people or property resulting from any ideas, methods, instructions or products
    referred to in the content.  © 2023 by the authors. Licensee MDPI, Basel, Switzerland.
    This article is an open access article distributed under the terms and conditions
    of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).
    Share and Cite MDPI and ACS Style Abbasi, R.; Martinez, P.; Ahmad, R. Automated
    Visual Identification of Foliage Chlorosis in Lettuce Grown in Aquaponic Systems.
    Agriculture 2023, 13, 615. https://doi.org/10.3390/agriculture13030615 AMA Style
    Abbasi R, Martinez P, Ahmad R. Automated Visual Identification of Foliage Chlorosis
    in Lettuce Grown in Aquaponic Systems. Agriculture. 2023; 13(3):615. https://doi.org/10.3390/agriculture13030615
    Chicago/Turabian Style Abbasi, Rabiya, Pablo Martinez, and Rafiq Ahmad. 2023.
    \"Automated Visual Identification of Foliage Chlorosis in Lettuce Grown in Aquaponic
    Systems\" Agriculture 13, no. 3: 615. https://doi.org/10.3390/agriculture13030615
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   1
    Google Scholar   [click to view] Article Access Statistics Article access statistics
    Article Views 8. Jan 18. Jan 28. Jan 7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28.
    Mar 0 500 1000 1500 2000 2500 For more information on the journal statistics,
    click here. Multiple requests from the same IP address are counted as one view.   Agriculture,
    EISSN 2077-0472, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: Abbasi, Martinez and Ahmad (2023)
  journal: Agriculture (Switzerland)
  key_findings: '1. The proposed image processing-based approach can accurately identify
    chlorosis in lettuce crops with an accuracy of 95%.

    2. The system integrates computer vision algorithms to analyze foliage color,
    enabling automated monitoring of crop health and quality.

    3. The study provides a novel solution for precision agriculture, contributing
    to improved crop management and increased productivity.'
  limitations: The study is limited to identifying chlorosis in lettuce crops grown
    in aquaponic systems. Further research is needed to extend the proposed approach
    to other crops and varying growing conditions.
  main_objective: To develop an automated visual identification system for foliage
    chlorosis in lettuce grown in aquaponic systems using advanced monitoring techniques.
  relevance_evaluation: The article is highly relevant to the point being made in
    the literature review as it provides a specific and detailed approach to integrating
    high-resolution cameras, computer vision algorithms, and advanced monitoring techniques
    for automated irrigation management systems. The study contributes to the broader
    objective of enhancing agricultural productivity and addressing the global food
    challenge by enabling precise monitoring and control of irrigation systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Image processing, computer vision algorithms, high-resolution
    cameras
  title: Automated Visual Identification of Foliage Chlorosis in Lettuce Grown in
    Aquaponic Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Al-Shammari, H., Karim, G., Ben Ltaifa, I., Krichen, M., Ben Ammar,
    L., & Mahmood, M. (2022). Olive disease classification based on vision transformer
    and CNN models. Computational Intelligence and Neuroscience, 2022, 1-10. https://doi.org/10.1155/2022/3998193
  authors:
  - Sarantakos T.
  - Gutierrez D.M.J.
  - Amaxilatis D.
  citation_count: '0'
  data_sources: Olive leaf images captured by drones
  description: The use of computer vision, deep learning, and drones has revolutionized
    agriculture by enabling efficient crop monitoring and disease detection. Still,
    many challenges need to be overcome due to the vast diversity of plant species
    and their unique regional characteristics. Olive trees, which have been cultivated
    for thousands of years, present a particularly complex case for leaf-based disease
    diagnosis as disease symptoms can vary widely, both between different plant variations
    and even within individual leaves on the same plant. This complexity, coupled
    with the susceptibility of olive groves to various pathogens, including bacterial
    blight, olive knot, aculus olearius, and olive peacock spot, has hindered the
    development of effective disease detection algorithms. To address this challenge,
    we have devised a novel approach that combines deep learning techniques, leveraging
    convolutional neural networks, vision transformers, and cloud computing-based
    models. Aiming to detect and classify olive tree diseases the experimental results
    of our study have been highly promising, demonstrating the effectiveness of the
    combined transformer and cloud-based machine learning models, achieving an impressive
    accuracy of approximately 99.6% for multiclass classification cases including
    healthy, aculus olearius, and peacock spot infected leaves. These results highlight
    the potential of deep learning models in tackling the complexities of olive leaf
    disease detection and the need for further research in the field.
  doi: 10.1007/978-3-031-49361-4_2
  explanation: The study aimed to detect and classify olive leaf infections caused
    by fungal pathogens, bacteria, or viruses, utilizing a combination of high-resolution
    cameras (e.g., multispectral, hyperspectral), computer vision algorithms, and
    deep learning techniques. The researchers leveraged the capabilities of drones
    to capture detailed images of olive trees from various angles, enabling them to
    scan vast olive tree groves and identify potentially infected trees with remarkable
    speed and precision.
  extract_1: '"The use of computer vision, deep learning, and drones has revolutionized
    agriculture by enabling efficient crop monitoring and disease detection. Still,
    many challenges need to be overcome due to the vast diversity of plant species
    and their unique regional characteristics."'
  extract_2: '"Aiming to detect and classify olive tree diseases the experimental
    results of our study have been highly promising, demonstrating the effectiveness
    of the combined transformer and cloud-based machine learning models, achieving
    an impressive accuracy of approximately 99.6% for multiclass classification cases
    including healthy, aculus olearius, and peacock spot infected leaves."'
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Algorithmic Aspects of Cloud
    Computing Conference paper Olive Leaf Infection Detection Using the Cloud-Edge
    Continuum Conference paper First Online: 14 December 2023 pp 25–37 Cite this conference
    paper Access provided by University of Nebraska-Lincoln Download book PDF Download
    book EPUB Algorithmic Aspects of Cloud Computing (ALGOCLOUD 2023) Themistoklis
    Sarantakos , Daniel Mauricio Jimenez Gutierrez & Dimitrios Amaxilatis   Part of
    the book series: Lecture Notes in Computer Science ((LNCS,volume 14053)) Included
    in the following conference series: International Symposium on Algorithmic Aspects
    of Cloud Computing 85 Accesses Abstract The use of computer vision, deep learning,
    and drones has revolutionized agriculture by enabling efficient crop monitoring
    and disease detection. Still, many challenges need to be overcome due to the vast
    diversity of plant species and their unique regional characteristics. Olive trees,
    which have been cultivated for thousands of years, present a particularly complex
    case for leaf-based disease diagnosis as disease symptoms can vary widely, both
    between different plant variations and even within individual leaves on the same
    plant. This complexity, coupled with the susceptibility of olive groves to various
    pathogens, including bacterial blight, olive knot, aculus olearius, and olive
    peacock spot, has hindered the development of effective disease detection algorithms.
    To address this challenge, we have devised a novel approach that combines deep
    learning techniques, leveraging convolutional neural networks, vision transformers,
    and cloud computing-based models. Aiming to detect and classify olive tree diseases
    the experimental results of our study have been highly promising, demonstrating
    the effectiveness of the combined transformer and cloud-based machine learning
    models, achieving an impressive accuracy of approximately 99.6% for multiclass
    classification cases including healthy, aculus olearius, and peacock spot infected
    leaves. These results highlight the potential of deep learning models in tackling
    the complexities of olive leaf disease detection and the need for further research
    in the field. Keywords Computer Vision Olive Leaf Infection Machine Learning Image
    Analysis Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF Similar content being viewed by others Lite-Agro: Exploring Light-Duty
    Computing Platforms for IoAT-Edge AI in Plant Disease Identification Chapter ©
    2024 Intelligent detection for sustainable agriculture: A review of IoT-based
    embedded systems, cloud platforms, DL, and ML for plant disease detection Article
    06 February 2024 Multi-class Plant Leaf Disease Classification on Real-Time Images
    Using YOLO V7 Chapter © 2023 1 Introduction The use of advanced technologies in
    agriculture has revolutionized traditional farming practices, enabling more efficient
    and precise methods for crop monitoring and disease detection. Among these transformative
    technologies, computer vision and drones have emerged as powerful tools for the
    detection of leaf infections in olive trees, offering new possibilities for early
    disease diagnosis and effective disease management. By leveraging the capabilities
    of computer vision algorithms and aerial surveillance provided by drones, farmers
    and researchers can now detect and respond to leaf infections in olive trees more
    accurately and promptly than ever before. Olive trees are an integral part of
    the agricultural landscape, providing valuable yields of olives and olive oil.
    However, they are susceptible to various diseases, including leaf infections caused
    by fungal pathogens, bacteria, and viruses. Identifying and addressing these infections
    in their early stages is crucial for preventing severe crop damage and yield losses.
    Traditional methods of disease detection often involve manual inspection of individual
    trees, which can be time-consuming, labor-intensive, and prone to human error.
    Fortunately, computer vision technologies and drones have emerged as promising
    solutions to overcome these limitations. Computer vision refers to the field of
    artificial intelligence that enables machines to analyze and interpret visual
    data, such as images or videos. By employing sophisticated algorithms, computer
    vision systems can extract meaningful information from visual inputs, enabling
    them to identify patterns, detect anomalies, and classify objects accurately.
    In the context of olive tree leaf infection detection, computer vision algorithms
    can be trained to recognize specific disease symptoms, such as discoloration,
    spots, lesions, or other visible signs of infection. By analyzing high-resolution
    images captured by drones, these algorithms can quickly scan vast olive tree groves
    and identify potentially infected trees with remarkable speed and precision. Drones,
    or unmanned aerial vehicles (UAVs), have rapidly gained popularity in agriculture
    due to their versatility and ability to provide an aerial perspective of crops
    and farmland. Equipped with high-resolution cameras and advanced sensors, drones
    can capture detailed images of olive trees from various angles and altitudes.
    These images offer a comprehensive view of the entire grove, allowing farmers
    and researchers to monitor the health and condition of individual trees remotely.
    Combined with computer vision algorithms, drones can autonomously survey large
    areas and generate real-time insights on the presence and severity of leaf infections.
    This enables farmers to make data-driven decisions and implement targeted interventions
    for disease control. The integration of computer vision technologies and drones
    for olive tree leaf infection detection offers several advantages over traditional
    methods. Firstly, it significantly reduces the time and effort required for disease
    surveillance, allowing farmers to cover large areas quickly and accurately. Secondly,
    by enabling early detection, this technology facilitates timely intervention,
    thereby minimizing the spread of infections and potential crop losses. Additionally,
    the use of drones eliminates the need for manual tree inspection, reducing the
    risk of human error and improving overall efficiency in disease management. The
    rest of the paper is structured as follows: Section 2 presents the literature
    state of the art on the fields of computer vision, machine learning, and edge
    computing technologies. Section 3 describes our methodology and the proposed architecture
    for identifying the condition of olive leaves. The evaluation of this methodology
    and its comparison with other systems is showcased in Sect. 4. Finally, in Sect.
    5 we present our conclusions and future directions. 2 Related Work Image processing
    and analysis have been utilized by many proposed solutions in past years in literature
    to efficiently detect plant and crop diseases. In [3] disorders in tomato plants
    are detected in leaf images using contour tracing, feature extraction, and Convolutional
    Neural Network (CNN) or K-Nearest Neighbor (KNN) machine learning approaches.
    [6] showcases a similar system that was developed in the Kingdom of Saudi Arabia
    that uses Deep Learning techniques like ResNet-50 and MobileNet models to classify
    olive leaf images collected through remote-controlled UAVs with high accuracy
    reaching up to 97%. X-FIDO [2] is a vision-based program to detect symptoms of
    Olive Quick Decline Syndrome. It showcases how transfer learning can be leveraged
    when collecting thousands of new leaf images is impossible, using only a few hundred
    images of healthy and infected leaves reaching over 98% accuracy. Olive tree disease
    analysis and classification is also to target of [8]. In more details, the infected
    area is isolated using the histogram thresholding and k-means segmentation methodologies
    with the latter being evaluated as more accurate. Finally, [10] solves a problem
    close to the one we face, by providing an automated methodology for detecting
    and counting olive trees using a multi-step classification system using colored
    satellite images. In [1] the authors explore the utilization of transformers for
    multiclass classification to predict olive leaf disease. They present their findings,
    highlighting the effectiveness of this approach in accurately identifying and
    categorizing various diseases affecting olive leaves. 2.1 Computer Vision Approaches
    Convolutional Neural Networks (CNNs) are a powerful class of deep learning models
    specifically designed for computer vision applications. CNNs excel at automatically
    learning and extracting meaningful features from visual data, such as images or
    videos. They employ a series of convolutional layers that apply learnable filters
    to capture local patterns and features. These filters scan the input data, enabling
    the network to detect edges, textures, shapes, and other visual attributes. By
    stacking multiple convolutional layers, CNNs can progressively learn more complex
    and abstract representations of the input. Additionally, pooling layers are used
    to downsample the learned features and retain important information while reducing
    spatial dimensions. The final layers of a CNN typically consist of fully connected
    layers that map the learned features to specific outputs, such as object classes
    or semantic labels. CNNs have demonstrated remarkable performance in various computer
    vision tasks, including image classification, object detection, image segmentation,
    and image generation. Their ability to automatically learn and extract relevant
    features from visual data has made them a fundamental and widely-used tool in
    the field of computer vision. Transfer learning is a machine learning technique
    where knowledge gained from training one model on a specific task is applied to
    a related task. A pre-trained model is used as a starting point and adapted to
    the new task by fine-tuning or retraining specific parts leveraging the pre-trained
    model’s learned features and representations, leading to improved performance,
    especially when data is limited. ResNet-50 is a highly regarded transfer learning
    model in the field of computer vision. Originally trained on the extensive ImageNet
    dataset, it has acquired intricate image representations. As a transfer learning
    technique, ResNet-50 ’s pre-trained layers can serve as feature extractors for
    novel tasks. This capability allows researchers to achieve remarkable performance,
    even when faced with limited annotated data. Due to its adaptability, ResNet-50
    finds extensive utility across diverse computer vision applications, including
    image classification, object detection, and image segmentation. The versatility
    and effectiveness of ResNet-50 make it a widely favored choice in the field of
    computer vision. VGG-19, like ResNet-50, is a renowned convolutional neural network
    architecture that stands out as a potent transfer learning model. Like its counterpart,
    VGG-19 was initially trained on the ImageNet dataset, which has played a crucial
    role in shaping its effectiveness across diverse computer vision tasks. By harnessing
    the pre-trained weights and learned features of VGG-19, researchers can tap into
    its vast knowledge and extend its applicability to new domains or tasks, particularly
    in scenarios where labeled data is scarce. The extensive capabilities of VGG-19
    make it a valuable asset for leveraging transfer learning in the realm of computer
    vision. You Only Look Once (YOLO) stands as a groundbreaking object detection
    framework, ushering in a new era in computer vision. It diverges from traditional
    methods by adopting a singular-stage approach, resulting in exceptional speed
    and efficiency. YOLO revolutionizes object detection by dividing the input image
    into a grid and directly predicting bounding boxes, object probabilities, and
    class labels from each grid cell. Through the ingenious employment of a solitary
    neural network, YOLO concurrently identifies multiple objects in a single pass,
    achieving remarkable real-time performance that proves invaluable in time-sensitive
    applications. With its precise and swift object detection capabilities, YOLO has
    become an indispensable tool across diverse domains, including autonomous driving,
    surveillance systems, and image analysis. Amazon Rekognition stands at the forefront
    of cutting-edge computer vision technology, offering a comprehensive and powerful
    solution for image and video analysis. Developed by Amazon Web Services (AWS),
    this state-of-the-art service provides an array of advanced features, including
    object recognition, facial analysis, emotion detection, text recognition, and
    more. With its deep learning algorithms and extensive training on vast datasets,
    Amazon Rekognition exhibits remarkable accuracy and robust performance, allowing
    users to extract valuable insights from visual content. Roboflow Roboflow is a
    powerful platform and suite of tools designed to streamline the process of building
    computer vision models and managing image datasets. It offers a comprehensive
    set of functionalities that simplify and accelerate various stages of the computer
    vision workflow. With Roboflow, users can annotate and label images, enabling
    the creation of high-quality datasets for training models. The platform also provides
    pre-processing capabilities, allowing users to resize, augment, and transform
    their image data to enhance model performance. Furthermore, Roboflow offers integration
    with popular machine learning frameworks and APIs, making it easy to train and
    deploy models in different environments. 2.2 Edge Computing Edge computing is
    a distributed computing approach that brings processing and storage closer to
    the devices and sensors at the edge of the network. It reduces latency by processing
    data locally and enables real-time decision-making. Edge computing offers benefits
    such as improved efficiency, enhanced privacy and security, offline capabilities,
    and scalability. It is applied in various industries and use cases to optimize
    data processing, reduce network bandwidth usage, and enable faster insights and
    better user experiences. Using edge computing in agriculture enables farmers to
    make timely decisions based on on-site data processing, reduces reliance on cloud
    connectivity, and ensures continuous farm operations even in remote areas. Edge
    computing empowers farmers with actionable insights, improves resource allocation,
    and promotes sustainable farming practices [5, 7, 9]. 2.3 Learning at the Cloud-Edge
    Continuum Federated Learning is a decentralized machine learning approach that
    prioritizes data privacy by keeping training data on local or edge devices instead
    of a central server or cloud. In this method, the model is distributed to edge
    devices for local training, with only model updates transmitted back and aggregated
    to improve the global model. The key objective is to protect data privacy and
    security by minimizing data sharing risks. Federated learning facilitates distributed
    model training while respecting privacy constraints and reducing the need for
    large-scale data transfers. Its benefits include personalized model training on
    individual devices, preserving data privacy, collaborative learning from diverse
    sources, and applicability in various scenarios such as mobile and IoT devices
    as well as edge computing environments. 2.4 Leaf Datasets Most of the systems
    presented above utilize a set of existing open datasets that contain curated images
    of plant leaves and fruits. These images contain mostly clean views of plant leaves,
    in neutral backgrounds, so their classification will be most accurate. One such
    dataset is the PlantVillage [4] dataset. It is a widely used and publicly available
    dataset for plant disease detection and classification research created by the
    PlantVillage project, that aims to assist in the development and evaluation of
    computer vision algorithms for plant disease diagnosis. It consists of 54303 healthy
    and unhealthy leaf images divided into 38 categories by species and diseases.
    The Olive Leaf Dataset 1 was created for olive tree disease detection and classification
    using convolutional neural networks (CNNs). It is intended to aid researchers
    and developers working on olive leaf disease analysis and machine learning algorithms
    for olive tree health monitoring and consists of high-resolution images, of three
    distinct classes: 2068 images of leaves infected by aculus orealus, 3717 images
    of leaves infected by peacock spot and 2155 healthy leaves. 3 Proposed Methodology
    Our proposed solution focuses on the analysis of the collected olive tree images
    collected using on-field hardware like drones or autonomous vehicles that can
    traverse the olive grove with limited or no human intervention. These images contain
    as expected a portion of the olive tree, with multiple leaves, photographed from
    a close or medium distance. The pipeline followed for the analysis of the images
    collected is depicted in Fig. 1. As depicted there, all collected images are bundled
    into a dataset, that are to be analyzed. This analysis consists of two parts:
    The segmentation of the leaves from each one of the tree images collected. The
    characterization of each detected leaf as healthy or unhealthy and the decision
    on the parasite it is infected with. These two processes can be either performed
    in two steps, using two discrete ML models, or as a single step using a single
    ML model trained to detect all 3 possible conditions. The first approach allows
    us to optimize the two ML models performing each operation, and increase their
    expected accuracy, resulting in better results, but on the other hand, requires
    more computational power for the analysis of each image. Using the second approach,
    we are able to perform both operations with a single prediction, that can both
    extract the olive leaves in each image and categorize them in one of the 3 available
    categories. Fig. 1. Leaf image collection pipeline across the Cloud-Edge continuum.
    Full size image After careful and extensive evaluation of all available software
    solutions, we were able to identify the best configuration for the two approaches
    we will evaluate in the rest of this paper. To implement the first approach and
    in scenarios where accuracy and flexibility are key, we determined that a combination
    of a self-trained YOLO ML model for image segmentation, complemented by a ResNet-50
    ML model using transfer learning techniques, is expected to provide the best outcome.
    This hybrid approach allows us to leverage the strengths of YOLO’s real-time object
    detection capabilities while benefiting from the high accuracy of ResNet-50. However,
    in situations where efficiency takes precedence over flexibility, we identified
    that leveraging the advanced capabilities of a single YOLO multi-class segmentation
    ML model would be a viable alternative. That is why we based on it the second
    approach described above. Moving parts of the leaf infection detection to the
    edge of the network offers a multitude of significant benefits. Firstly, it helps
    reduce the burden of data transfer to cloud services. By processing data locally
    at the edge of the network, the need to transmit the total volume of images or
    video footage to centralized cloud services is minimized, resulting in more efficient
    data management, lower network congestion and lower communication costs. Additionally,
    edge computing accelerates the delivery of results. With data processing occurring
    on-site or close to the data source, the latency associated with sending data
    to a distant data center is substantially reduced, leading to faster and more
    real-time outcomes. Finally, moving computing to the edge helps keep sensitive
    data under our own control, enhancing data security and privacy, and reducing
    the exposure of critical information to external networks. 3.1 Leaf Extraction
    The process of detecting the exact position of olive leaves inside a photo taken
    by an autonomous vehicle can be quite a complex task. Most applications that we
    investigated from the previous work (presented in Sect. 2) dealt with images of
    single leaves. These leaves were mostly placed in neutral backgrounds without
    any other information that could affect the process. Our goal was to be able to
    detect much more than single leaves and identify and extract each leaf from high-resolution
    images that contain portions or even whole olive trees. To achieve such a result
    we had to move of course from the single leaf images to branches of olive trees
    and finally high-resolution images as the ones available in Fig. 2. Fig. 2. Single
    leaf, branch of leaves and tree image used to evolve the leaf extraction ML model.
    Full size image In the pursuit of robust solution for object segmentation, our
    research led us to YOLOv8, an advanced framework renowned for its exceptional
    performance. In parallel, we delved into the available tools for the preparation
    of the datasets for YOLO ML model training, exploring two state-of-the-art applications:
    trainYOLO 2 and roboflow 3. With these applications, we could easily label and
    annotate the specific regions of interest in our experiments, the olive leaves.
    To ensure the model’s efficiency, we divided the dataset into training and validation
    subsets. Following an extensive training process, we evaluated our model against
    a diverse set of tree images that contained either single leaves, branches with
    multiple leaves in neutral backgrounds, or multiple leaves in images of trees
    in natural surroundings. Once the leaves were detected, we are able to use the
    YOLO bounding boxes detected to precisely crop and extract each individual leaf
    as a separate image, ready for their subsequent analysis (in the first approach)
    or their exact categorization (in the second approach). 3.2 Leaf Classification
    In the image classification part, our exploration led us to four distinct approaches.
    Firstly, we experimented with custom-built CNN trained from scratch, leveraging
    our domain expertise to develop a tailored architecture. The image classification
    model we employed is a sequential model with a series of convolutional and pooling
    layers followed by fully connected layers. The model begins with an initial convolutional
    layer with 8 filters and a 3\\(\\,\\times \\,\\)3 kernel, utilizing the ReLU activation
    function. Subsequently, a max pooling layer with a pool size of 2\\(\\,\\times
    \\,\\)2 and stride of 2 downsamples the feature maps. This is followed by two
    more convolutional layers, each with increasing filter size, and their corresponding
    max pooling layers. After the final pooling layer, the features are flattened
    into a 1D vector. Two fully connected layers are then applied, with the first
    layer consisting of 64 units and ReLU activation, and the final layer producing
    class probabilities using the softmax activation function. This model architecture
    enables extracting and learning hierarchical features from the input images, facilitating
    accurate image classification. The CNN model structure was carefully chosen through
    a comprehensive grid search process, which involved evaluating its performance
    on our specific dataset. Secondly, we evaluated models that harness the power
    of transfer learning, utilizing pre-trained VGG-19 and ResNet-50 models, originally
    trained on the ImageNet dataset, and further fine-tuned them with our own dataset.
    This approach allowed us to benefit from the models’ learned features and representations,
    enabling enhanced classification performance. These two models was selected based
    on thorough research and analysis of literature papers. We observed that VGG-19
    and ResNet-50 demonstrated superior performance on our dataset compared to other
    models or variations. Finally, we experimented with cloud computing tools, more
    specifically AWS Rekognition, to train a ML model using our own dataset. This
    cloud-based approach would provide us with easy-to-scale infrastructures for efficient
    training and inference operations. Throughout the implementation and evaluation
    of these diverse methodologies, we aimed to understand the best configurations
    for our system and find the optimal strategies for achieving accurate and reliable
    image classification results. 4 Evaluation 4.1 Metrics To evaluate the performance
    of classification ML models we use the most commonly used metrics of the field,
    accuracy, precision, recall, and f1 score. Each metric provides a different perspective
    on the model’s effectiveness. In more detail, accuracy (1) measures the proportion
    of correct predictions to the total number of predictions made; precision (2)
    calculates the ratio of true positive predictions to the total number of positive
    predictions, indicating how well the model correctly identifies positive instances;
    recall (3) calculates the ratio of true positive predictions to the total number
    of actual positive instances, indicating how well the model captures all positive
    instances; f1 score (4) is the harmonic mean of precision and recall providing
    a single metric that combines both precision and recall, giving a balanced measure
    of the model’s performance. TP: True Positive, TN: True Negative, FP: False Positive,
    FN: False Negative Fig. 3. Leaf detection on a branch and tree image using a YOLOv8
    image segmentation ML model. Full size image 4.2 Leaf Segmentation Figure 3 showcases
    the result of using the YOLO-based ML model we developed for the leaf segmentation
    operations of our system. This model is trained on a dataset consisting of more
    than 200 annotated images with more than 1000 objects belonging to one each of
    the three classes we use (healthy, aculus orealus, and peacock spot). In Fig.
    4 we showcase the evolution of all the available metrics during the training of
    our model for the whole 50 epochs of the training procedure. By minimizing the
    training losses (box loss, segmentation loss, classification loss, and localization
    loss) over time, the model improves its ability to accurately classify objects
    and predict their bounding box coordinates. This indicates that the model is learning
    to make more precise and accurate predictions. Simultaneously, the metrics such
    as precision, recall, mAP@0.5, and mAP@0.5:0.95 are all increasing. This suggests
    that the model’s performance in object detection is improving. Higher precision
    indicates a lower rate of false positive detections, while higher recall indicates
    a lower rate of false negative detections. Fig. 4. Metrics for the training of
    the YOLO-based segmentation model. Full size image Additionally, in Table 1 we
    present the results achieved by our YOLO-based segmentation ML model, for the
    two-step and approach. In the two-step approach, we can see the amount of leaves
    the system can detect out of the total number of leaves available in the dataset.
    This total number of leaves available is not the actual leaves presented in the
    image but the leaves that are usable, in focus, and in correct distance from the
    camera. Table 1. Statistics for the olive leaves detected in the YOLO-based approach.
    Full size table 4.3 Leaf Classification To identify the condition of each of the
    leaves detected by our system we needed to develop a ML model that is capable
    of classifying each cropped image to one of the 3 available categories. In our
    attempts to find the best possible ML model, we evaluated multiple configurations
    and design: a CNN-based ML model, a ResNet-50 ML model, a VGG-19 ML model and
    AWS Rekognition. Table 2. Accuracy scores achieved in the evaluation of multiple
    ML models for the classification of single olive leaves. Full size table The accuracy
    of each model is noted in Table 2. We can there identify that our custom CNN model
    has achieved the lower accuracy across all our attempts with \\(91.2\\%\\). The
    VGG-19 model showed a huge improvement, that reached \\(98.2\\%\\) as it is designed
    for similar tasks. ResNet-50 and AWS Rekognition were capable of reaching an accuracy
    score of more than \\(99\\%\\) using our combined dataset. 4.4 Comparison with
    Other Solutions In [6] a custom ML solution called MobilResNet was presented to
    facilitate the task of olive leaf classification. As this task is similar to our
    goal, based on transfer learning techniques in this section we will try to replicate
    these experiments and evaluate our own ML models in the same dataset to evaluate
    their performance. The same is with [1], where the authors use transformers for
    multi-class classification. Taking their exceptional achievements as the state
    of the art, characterized by high accuracy and remarkable precision, recall, and
    F1 score values, we aimed to surpass their performance. To achieve this, we leveraged
    the powerful Amazon Rekognition API, which proved to be crucial in our experiments
    and produced the best results. Table 3. Comparison of our cloud-based model with
    models described in [1, 6] Full size table The results of our experiments are
    presented in Table 3. Based on the outcome, it is evident that we achieved remarkable
    results, surpassing the initial objectives in our research on olive leaf classification.
    Our findings represent a significant advancement in this field, opening up new
    possibilities for further exploration and development. 5 Conclusions The combination
    of computer vision technologies and drones presents a promising approach to revolutionize
    the detection of leaf infections in olive trees. By harnessing the power of computer
    vision algorithms and aerial surveillance capabilities, farmers and researchers
    can gain valuable insights into the health of olive tree groves, detect infections
    at an early stage, and implement targeted interventions for effective disease
    control. As these technologies continue to advance, the future holds great potential
    for optimizing olive tree health and ensuring sustainable olive production. Our
    end-to-end system provides a robust solution for olive grove inspections, implemented
    with real-world photos, captured by UAVs within Mediterranean olive groves. The
    system is capable of detecting infections on images containing hundreds of leaves
    at a time, instead of individual leaves with remarkable accuracy even when compared
    to solutions existing in the literature. Notes 1. https://github.com/sinanuguz/CNN_olive_dataset.
    2. https://trainyolo.com/. 3. https://roboflow.com/. References Alshammari, H.,
    Karim, G., Ben Ltaifa, I., Krichen, M., Ben Ammar, L., Mahmood, M.: Olive disease
    classification based on vision transformer and cnn models. Computational Intelligence
    and Neuroscience 2022, 1–10 (07 2022). https://doi.org/10.1155/2022/3998193 Cruz,
    A.C., Luvisi, A., De Bellis, L., Ampatzidis, Y.: X-fido: an effective application
    for detecting olive quick decline syndrome with deep learning and data fusion.
    Front. Plant Sci. 8 (2017). https://doi.org/10.3389/fpls.2017.01741, https://www.frontiersin.org/articles/10.3389/fpls.2017.01741
    Harakannanavar, S.S., Rudagi, J.M., Puranikmath, V.I., Siddiqua, A., Pramodhini,
    R.: Plant leaf disease detection using computer vision and machine learning algorithms.
    Global Trans. Proc. 3(1), 305–310 (2022) Article   Google Scholar   Hughes, D.,
    Salathé, M., et al.: An open access repository of images on plant health to enable
    the development of mobile disease diagnostics. arXiv preprint arXiv:1511.08060
    (2015) Kalyani, Y., Collier, R.: A systematic survey on the role of cloud, fog,
    and edge computing combination in smart agriculture. Sensors 21(17) (2021). https://doi.org/10.3390/s21175922,
    https://www.mdpi.com/1424-8220/21/17/5922 Ksibi, A., Ayadi, M., Soufiene, B.O.,
    Jamjoom, M.M., Ullah, Z.: Mobires-net: a hybrid deep learning model for detecting
    and classifying olive leaf diseases. Applied Sciences 12(20) (2022). https://doi.org/10.3390/app122010278,
    https://www.mdpi.com/2076-3417/12/20/10278 O’Grady, M., Langton, D., O’Hare, G.:
    Edge computing: a tractable model for smart agriculture? Artif. Intell. Agricultu.
    3, 42–51 (2019) Google Scholar   Sinha, A., Shekhawat, R.S.: Olive spot disease
    detection and classification using analysis of leaf image textures. Proc. Comput.
    Sci. 167, 2328–2336 (2020). https://doi.org/10.1016/j.procs.2020.03.285https://www.sciencedirect.com/science/article/pii/S1877050920307511,
    international Conference on Computational Intelligence and Data Science Uddin,
    M.A., Ayaz, M., Mansour, A., Aggoune, e.H.M., Sharif, Z., Razzak, I.: Cloud-connected
    flying edge computing for smart agriculture. Peer-to-Peer Network. Appl. 14(6),
    3405–3415 (2021) Google Scholar   Waleed, M., Um, T.W., Khan, A., Khan, U.: Automatic
    detection system of olive trees using improved k-means algorithm. Remote Sensing
    12(5), 760 (2020) Article   Google Scholar   Download references Acknowledgements
    This work has been supported by the European Union’s Horizon 2020 research and
    innovation programme under Secure and Seamless Edge-to-Cloud Analytics (GA 957286),
    EU H2020, ICT-50-2020 - Software Technologies and Next Generation IoT as part
    of Next Generation Internet project (GA 957246), EU H2020, ICT-56-2020 - Next
    Generation Internet of Things. Author information Authors and Affiliations SparkWorks
    Ltd., Galway, Ireland Themistoklis Sarantakos & Dimitrios Amaxilatis Sapienza
    University of Rome, Rome, Italy Daniel Mauricio Jimenez Gutierrez Corresponding
    author Correspondence to Dimitrios Amaxilatis . Editor information Editors and
    Affiliations Sapienza University of Rome, Rome, Italy Ioannis Chatzigiannakis
    Ionian University, Corfu, Greece Ioannis Karydis Rights and permissions Reprints
    and permissions Copyright information © 2024 The Author(s), under exclusive license
    to Springer Nature Switzerland AG About this paper Cite this paper Sarantakos,
    T., Gutierrez, D.M.J., Amaxilatis, D. (2024). Olive Leaf Infection Detection Using
    the Cloud-Edge Continuum. In: Chatzigiannakis, I., Karydis, I. (eds) Algorithmic
    Aspects of Cloud Computing. ALGOCLOUD 2023. Lecture Notes in Computer Science,
    vol 14053. Springer, Cham. https://doi.org/10.1007/978-3-031-49361-4_2 Download
    citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-031-49361-4_2 Published
    14 December 2023 Publisher Name Springer, Cham Print ISBN 978-3-031-49360-7 Online
    ISBN 978-3-031-49361-4 eBook Packages Computer Science Computer Science (R0) Share
    this paper Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Publish with us Policies and ethics Sections Figures References Abstract
    Introduction Related Work Proposed Methodology Evaluation Conclusions Notes References
    Acknowledgements Author information Editor information Rights and permissions
    Copyright information About this paper Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: Al-Shammari et al., (2022)
  journal: Lecture Notes in Computer Science (including subseries Lecture Notes in
    Artificial Intelligence and Lecture Notes in Bioinformatics)
  key_findings: The proposed system achieved an accuracy of approximately 99.6% for
    multiclass classification of olive tree diseases, including healthy, aculus olearius,
    and peacock spot infected leaves.
  limitations: The study does not mention any significant limitations. However, it
    is important to note that the effectiveness of the proposed approach may vary
    depending on factors such as image quality, environmental conditions, and the
    diversity of olive tree varieties.
  main_objective: To evaluate the effectiveness of a computer vision-based system
    for detecting and classifying olive leaf infections using deep learning and drones.
  relevance_evaluation: The paper is highly relevant to the outline point, as it specifically
    focuses on integrating high-resolution cameras and computer vision algorithms
    for visual monitoring of crop growth, disease detection, and irrigation system
    performance. The study aligns well with the broader theme of automated irrigation
    systems and their potential to address the global food challenge.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning,
    drones
  title: Olive Leaf Infection Detection Using the Cloud-Edge Continuum
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Wolter-Salas, S., Canessa, P., Campos-Vargas, R., Opazo, M. C., Sepulveda,
    R. V., & Aguayo, D. (2023). WS-YOLO: An Agronomical and Computer Vision-Based
    Framework to Detect Drought Stress in Lettuce Seedlings Using IR Imaging and YOLOv8.
    In Advanced Research in Technologies, Information, Innovation and Sustainability
    (pp. 339–351). Springer, Cham.'
  authors:
  - Wolter-Salas S.
  - Canessa P.
  - Campos-Vargas R.
  - Opazo M.C.
  - V. Sepulveda R.
  - Aguayo D.
  citation_count: '0'
  data_sources: Infrared images of lettuce seedlings grown under drought stress conditions
  description: Lettuce (Lactuca sativa L.) is highly susceptible to drought and water
    deficits, resulting in lower crop yields, unharvested areas, reduced crop health
    and quality. To address this, we developed a High-Throughput Phenotyping platform
    using Deep Learning and infrared images to detect stress stages in lettuce seedlings,
    which could help to apply real-time agronomical decisions from data using variable
    rate irrigation systems. Accordingly, a comprehensive database comprising infrared
    images of lettuce grown under drought-induced stress conditions was built. In
    order to capture the required data, we deployed a Raspberry Pi robot to autonomously
    collect infrared images of lettuce seedlings during an 8-day drought stress experiment.
    This resulted in the generation of a database containing 2119 images through augmentation.
    Leveraging this data, a YOLOv8 model was trained (WS-YOLO), employing instance
    segmentation for accurate stress level detection. The results demonstrated the
    efficacy of our approach, with WS-YOLO achieving a mean Average Precision (mAP)
    of 93.62% and an F1 score of 89.31%. Particularly, high efficiency in early stress
    detection was achieved, being a critical factor for improving food security through
    timely interventions. Therefore, our proposed High-Throughput Phenotyping platform
    holds the potential for high-yield lettuce breeding, enabling early stress detection
    and supporting informed decision-making to mitigate losses. This interdisciplinary
    approach highlights the potential of AI-driven solutions in addressing pressing
    challenges in food production and sustainability. This work contributes to the
    field of precision agricultural technology, providing opportunities for further
    research and implementation of cutting-edge Deep Learning techniques for stress
    detection in crops.
  doi: 10.1007/978-3-031-48858-0_27
  explanation: The study aims to detect different levels of drought stress in lettuce
    seedlings using an autonomous phenotyping platform integrating computer vision
    (CV) and deep learning (DL), specifically using the YOLOv8 model (WS-YOLO). The
    platform utilizes infrared (IR) imaging techniques to capture detailed images
    of lettuce seedlings under controlled drought stress conditions.
  extract_1: '"WS-YOLO successfully from IR images lettuces exposed to different water
    stress levels."'
  extract_2: “The calculated precision shows that 90.63% of the instances predicted
    by the model are correct.”
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart International Conference on Advanced
    Research in Technologies, Information, Innovation and Sustainability ARTIIS 2023:
    Advanced Research in Technologies, Information, Innovation and Sustainability
    pp 339–351Cite as Home Advanced Research in Technologies, Information, Innovation
    and Sustainability Conference paper WS-YOLO: An Agronomical and Computer Vision-Based
    Framework to Detect Drought Stress in Lettuce Seedlings Using IR Imaging and YOLOv8
    Sebastian Wolter-Salas , Paulo Canessa , Reinaldo Campos-Vargas , Maria Cecilia
    Opazo , Romina V. Sepulveda & Daniel Aguayo  Conference paper First Online: 20
    December 2023 152 Accesses Part of the book series: Communications in Computer
    and Information Science ((CCIS,volume 1935)) Abstract Lettuce (Lactuca sativa
    L.) is highly susceptible to drought and water deficits, resulting in lower crop
    yields, unharvested areas, reduced crop health and quality. To address this, we
    developed a High-Throughput Phenotyping platform using Deep Learning and infrared
    images to detect stress stages in lettuce seedlings, which could help to apply
    real-time agronomical decisions from data using variable rate irrigation systems.
    Accordingly, a comprehensive database comprising infrared images of lettuce grown
    under drought-induced stress conditions was built. In order to capture the required
    data, we deployed a Raspberry Pi robot to autonomously collect infrared images
    of lettuce seedlings during an 8-day drought stress experiment. This resulted
    in the generation of a database containing 2119 images through augmentation. Leveraging
    this data, a YOLOv8 model was trained (WS-YOLO), employing instance segmentation
    for accurate stress level detection. The results demonstrated the efficacy of
    our approach, with WS-YOLO achieving a mean Average Precision (mAP) of 93.62%
    and an F1 score of 89.31%. Particularly, high efficiency in early stress detection
    was achieved, being a critical factor for improving food security through timely
    interventions. Therefore, our proposed High-Throughput Phenotyping platform holds
    the potential for high-yield lettuce breeding, enabling early stress detection
    and supporting informed decision-making to mitigate losses. This interdisciplinary
    approach highlights the potential of AI-driven solutions in addressing pressing
    challenges in food production and sustainability. This work contributes to the
    field of precision agricultural technology, providing opportunities for further
    research and implementation of cutting-edge Deep Learning techniques for stress
    detection in crops. Keywords Digital Agriculture Computer Vision High-Throughput
    Phenotyping Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF 1 Introduction Nowadays, the agricultural sector has been severely affected
    by water shortages affecting horticultural production. The effects of water stress
    on horticultural crops can induce physiological stress in plants, leading to stunted
    growth, diminished produce quality, and increased susceptibility to pests and
    diseases (Molina-Montenegro et al., 2011; Knepper y Mou, 2015; Kumar et al. 2021).
    The resulting water scarcity has significantly affected lettuce (Lactuca sativa
    L.) an extensively cultivated leafy vegetable, requiring an adequate water supply
    for optimal growth and quality. Lettuce stands as one of the most extensively
    cultivated leafy vegetables globally, encompassing a cultivation area of 1.3 million
    hectares and yielding approximately 29 million tons (Kim et al. 2016; Chen et
    al. 2019). Various cultivation methods are employed for lettuce, including hydroponic
    systems, greenhouses, and plant factories, while open-field cultivation remains
    prevalent (Donoso 2021). However, decreased water availability necessitates appropriate
    water management practices, influencing irrigation strategies and crop performance.
    Due to the water-intensive nature of lettuce cultivation, it is especially susceptible
    to water stress (Kumar et al. 2021). Optimal irrigation management during the
    seedling stage is closely linked to future productivity and the provision of healthy,
    uniform seedlings, thereby impacting the overall yield of horticultural crops
    (Shin et al. 2021). In seedling farms, the determination of irrigation management
    predominantly relies upon cultivation techniques and the visual discernment of
    the crop manager (Chen et al. 2014; Yang et al. 2020). Nonetheless, deficient
    and subjective cultivation methods yield undesirable consequences, including escalated
    labour and temporal requirements. In the context of water scarcity, the incorporation
    of Artificial Intelligence (AI) methodologies, specifically Deep Learning (DL),
    holds the potential for improving the well-being of vegetable crops, such as lettuce
    (Das Choudhury et al. 2019). DL algorithms excel at analyzing large volumes of
    data and extracting meaningful patterns, which can aid in optimizing irrigation
    management, predicting crop water needs, and improving resource-use efficiency
    (Cheng et al. 2020; Xiao et al. 2022; Gill et al. 2022). By leveraging the power
    of DL, it is possible to create predictive models that incorporate various environmental
    and crop-specific parameters to optimize irrigation scheduling, thereby reducing
    water usage and ensuring optimal plant growth (Kamarudin et al. 2021). In addition,
    DL algorithms can aid in the early detection and identification of plant stress
    symptoms caused by a lack of water, allowing for prompt intervention and mitigating
    yield losses (Kamarudin and Ismail 2022). For example, by analyzing Infrared (IR)
    imagery, these algorithms can detect minute changes in the observed phenotype,
    allowing for the early detection of crop stress phenotypes (Paulo et al. 2023).
    This early detection enables producers to implement targeted irrigation strategies,
    optimize resource allocation, and mitigate the adverse effects of water stress
    on crop health and yield (Islam and Yamane 2021; Chen et al. 2014). Thus, the
    precise and effective identification of stress-induced characteristics is imperative
    for the progression of our understanding of plant reactions to environmental stressors
    and the development of practical mitigation approaches. DL models, such as YOLO,
    have become prominent in the field of automated and High-Throughput Phenotyping
    (HTP) (Buzzy et al. 2020; Zhang and Li 2022; Cardellicchio et al. 2023; Xu et
    al. 2023). YOLO is a sophisticated object detection and instance segmentation
    model that utilizes deep neural networks to identify and precisely locate objects
    within images (Song et al. 2021). The cutting-edge architecture has exhibited
    exceptional efficacy in diverse Computer Vision (CV) assignments (Chen et al.
    2021). Using YOLO in plant science has significantly transformed phenotyping,
    providing a potent approach for automating stress-related phenotype identification
    and measurement (Chen et al. 2021, Xu et al. 2023). Through the process of training
    YOLO on extensive collections of plant images, the model can acquire the ability
    to identify and precisely locate stress symptoms (Mota-Delfin et al. 2022). Its
    real-time processing capability enables rapid analysis of large-scale datasets,
    facilitating HTP (James et al. 2022). This speed is critical for real-time monitoring
    of plant responses to stress and allows for timely interventions to mitigate damage
    and optimize crop management. Detailed knowledge of stress patterns in a plant
    population can guide targeted breeding tasks and precise agricultural interventions.
    1.1 Related Work  In recent years, there have been notable advancements in the
    field of crop phenotyping through the utilization of CV and DL approaches (Wang
    and Su 2022; Jiand and Li 2020; Chandra et al. 2020; Li et al. 2020). Particularly,
    instance segmentation refines the classic object detection task by identifying
    individual object instances and segmenting them pixel-wise. Recent studies have
    investigated the utilization of YOLO models in order to perform instance segmentation
    and object detection tasks specifically for plant phenotyping. In recent studies
    conducted by Khalid et al. (2023) and Qiu et al. (2022), a comparable methodology
    was employed, wherein numerous YOLO models were utilized for the timely identification
    of pests and illnesses in the field of agriculture. The primary objective of the
    research conducted by Khalid et al. (2023) is to discern and classify thistle
    caterpillars, red beetles, and citrus psylla pests. This was achieved through
    the utilization of a dataset of 9875 images, which were acquired under different
    lighting conditions. The YOLOv8 model demonstrates superior performance in the
    detection of tiny pests, surpassing prior studies with a mean Average Precision
    (mAP) of 84.7% and an average loss of 0.7939. Similarly, Rong et al. (2023) present
    a visual methodology for efficient point cloud processing of tomato organs, essential
    for automated crop management. The method involves segmenting tomato organs using
    instance segmentation and a strategy that utilizes point cloud constraints to
    match the organs. YOLOv5-4D detects the region of interest on tomatoes, achieving
    a mAP of 0.953, being slightly more accurate than the native YOLOv8 model. The
    proposed point cloud constraint-based search method effectively matches tomato
    organs in 3D space, yielding an 86.7% success rate in multiple real scenarios.
    However, the utilization of YOLOv5 in the agricultural sector, particularly in
    crop phenotype research, has gained significant traction and reached a level of
    maturity (Kong et al. 2023). Liu et al. (2023) proposed Small-YOLOv5 for automatically
    identifying the growth period of rice, which is crucial for producing high-yield
    and high-quality rice. The Small-YOLOv5 approach utilizes MobileNetV3 as the backbone
    feature extraction network, resulting in smaller model size and fewer parameters,
    thus improving the detection speed. Experimental results demonstrate that Small-YOLOv5
    outperforms other popular lightweight models, achieving a 98.7% mAP value at a
    threshold of 0.5 and a 94.7% mAP at a threshold range of 0.5 to 0.95. Moreover,
    Small-YOLOv5 significantly reduces the model parameters and volume. This is still
    project-dependent, as the work of Blekos et al. (2023) achieves the second-highest
    bounding box accuracy using YOLOv8 for grape maturity estimations utilizing their
    custom dataset of 2500 images. This result outperforms all YOLO versions, with
    an 11% accuracy margin compared to YOLOv3. Previous studies in this domain have
    not explored the application of YOLO for this purpose in lettuce. The closest
    related research is the work of Wang et al. (2023), which shares similarities
    but focuses on microscopic imaging of stomatal opening and closure. This excellent
    investigation has limitations, mainly in data acquisition and in the stage of
    abiotic stress. However, in our study, an IR camera is utilized to capture a complete
    frontal view of the entire plant. This approach eliminates the need for costly
    microscopic cameras and allows the use of ground drones for phenotyping purposes,
    enhancing water usage through the early detection of water stress. Herein we develop
    an HTP platform for the early detection of drought stress in lettuce using the
    DL model YOLOv8 by IR imaging. Accordingly, we developed an autonomous platform
    to generate a database that comprehends the different levels of stress that can
    affect lettuce over an extended period of water deficit. This database was used
    to train a YOLOv8 model that successfully from IR images lettuces exposed to different
    water stress levels. This development can be used to build novel strategies for
    efficient water use based on automatic stress detection. 2 Methodology 2.1 Plant
    Material and Experimental Conditions In order to define the state of water stress,
    it is necessary to grow lettuce seedlings. Lettuce seedlings (Lactuca sativa L.
    var. Capitata (L.) Janchen) were grown in a greenhouse under a controlled environment
    and irrigation. The lettuce seedlings were grown up to 10 days after the appearance
    of their true leaves continuing to the experimental phase. In this phase water
    was not administered to the experimental group for 8 days, maintaining a control
    group with normal water administration. A total of 72 individuals corresponded
    to the control group, meanwhile, 60 individuals corresponded to the experimental
    group. 2.2 Database Collection The water deficit stress level was defined using
    the morphological state of the lettuce seedlings. Based on this, an autonomous
    robot (named as High-Performance Autonomous Phenotyper, HPAP) with a camera capable
    of detecting and capturing digital images of lettuce seedlings in motion was built
    using Arduino. The images were using the Camera Module 2 Pi NoIR with a Sony IMX219
    8-megapixel sensor and calibrated with OpenCV library using Python. In addition,
    3 ultrasound sensor modules connected to the Raspberry Pi 3B were connected via
    a prototyping board and used to census the distance of its surroundings and make
    decisions on its trajectory automatically and correctively. HPAP is programmed
    to stay between 12 to 14 cm (cm) away from the surface where the lettuce seedlings
    are located, so if it is outside the threshold, it can correct its trajectory
    to maintain the proximity margin (Fig. 1). 2.3 Stress Detection The stress detection
    was assessed using YOLOv8 instance segmentation using a total of 2119 images (named
    Water Stress - YOLO, or WS-YOLO). The image resolution used was 640 × 480 pixels.
    The image annotation was automated using Supervision and fine-tuned Segment Anything
    Model (SAM) model using 4 states: ‘healthy lettuce’, ‘lettuce with mild stress’,
    ‘lettuce with moderate stress’ and ‘lettuce with severe stress’ based on the morphological
    characteristics and literature. The control group was annotated as ‘healthy’ as
    long as it did not show symptoms of water stress. Lettuces were annotated as ‘moderately
    stressed’ when they exhibited the first symptoms of water stress, such as slight
    wilting of the leaves and reduced turgor. In the case of ‘severe stress’ (or plant
    death) lettuces were annotated when they had traits such as wilting, reduced leaf
    area, leaf decay and loss of biomass. Finally, ‘mild’ (or early) stressed plants
    belonging to the experimental group (without irrigation) were annotated when they
    exhibited no stress symptoms since day 1. This was corroborated in the literature
    (see Discussion). Furthermore, the dataset was augmented by applying a horizontal
    flip, crop with a minimum zoom of 13% and a maximum zoom of 50%, rotation of 20°,
    saturation of 20%, exposure of 15%, and up to 5% pixel noise. The dataset was
    divided into training, validation, and test sets (70:20:10). The hyperparameters
    used in WS-YOLO model were defined to achieve a trade-off between the precision
    of the model and its computational efficiency. A batch size of 6 was used, resulting
    in good training times while preserving model stability. Additionally, an initial
    learning rate of 1*10^5 was implemented using ‘AdamW’ optimizer during 25 epochs.
    The incorporation of momentum of 0.85 and weight decay of 1*10^4 was implemented
    to expedite convergence and forestall overfitting. The PyTorch DL framework was
    utilized to implement the WS-YOLO model, which underwent training on a Windows
    System equipped with a 14-core Intel i5 CPU and an NVIDIA RTX 3070 Ti graphics
    card. The pre-trained model used was YOLOv8x-seg. 2.4 Model Evaluation The efficacy
    of the performed experiments on the WS-YOLO model was evaluated using Precision,
    Recall, F1-score, and Mean Average Precision (mAP) of 50% and between 50 and 95%
    of the Intersection over Union (IoU) as the evaluation metrics. The methodology
    for calculating is presented in the Eqs. (1–4). The abbreviations of these equations
    are: True positive (TP), False Positive (FP), False Negative (FN) and Average
    Precision (AP). $$\\mathbf{P}\\mathbf{r}\\mathbf{e}\\mathbf{c}\\mathbf{i}\\mathbf{s}\\mathbf{i}\\mathbf{o}\\mathbf{n}=
    \\frac{{\\text{TP}}}{\\mathrm{TP }+\\mathrm{ FP}}$$ (1) $$\\mathbf{R}\\mathbf{e}\\mathbf{c}\\mathbf{a}\\mathbf{l}\\mathbf{l}=
    \\frac{{\\text{TP}}}{\\mathrm{TP }+\\mathrm{ FN}}$$ (2) $$\\mathbf{F}1= \\frac{2
    *\\mathrm{ Precision }*\\mathrm{ Recall}}{\\mathrm{Precision }+\\mathrm{ Recall}}$$
    (3) $$\\mathbf{m}\\mathbf{A}\\mathbf{P}= \\frac{1}{{\\text{n}}}\\sum_{\\mathrm{k
    }= 1}^{\\mathrm{k }=\\mathrm{ n}}{{\\text{AP}}}_{\\mathrm{ k}}$$ (4) 3 Results
    and Discussion The trained WS-YOLO model successfully detects the experimentally-defined
    stress levels of lettuce seedlings. The model was evaluated using Precision, Recall,
    F1-score, mAP (50%), and mAP (50–95%) of the lettuce image analysis for evaluating
    the performance and effectiveness. Table 1 displays the experimental outcomes.
    Table 1. WS-YOLO performance. Full size table The calculated precision shows that
    90.63% of the instances predicted by the model are correct. This high precision
    value indicates a low false positive rate for this model. Furthermore, the recall
    value indicates that the model correctly identified 88.08% of real instances.
    This observation suggests a significant level of recall, indicating that the model
    exhibits a low rate of false negatives (Fig. 2). The F1 score is the harmonic
    mean of precision and recall, providing a balanced measure of both metrics. With
    an F1 score of 89.31%, this model has a good balance between precision and recall,
    obtaining the best score with 0.593 confidence (Fig. 3). The mAP at an IoU threshold
    of 50% measures the average precision across different segmented categories. A
    score of 93.62% indicates that the model achieves a high precision-recall tradeoff
    on average across the segmented categories. This result indicates good overall
    performance. The mAP assessed across a range of IoU thresholds from 50% to 95%
    provides a more stringent evaluation. The model demonstrates satisfactory performance
    in terms of precision and recall throughout a broader spectrum of IoU thresholds,
    albeit slightly lower than the mAP at 50%, with a value of 74.07%. With respect
    to other plant phenotyping research, the values presented in this study are within
    good parameters. In example, the work conducted by Lin et al. (2023) introduces
    YOLO-Tobacco, an improved YOLOX-Tiny network designed for detecting fungal-induced
    diseases. This network achieved a detection accuracy of 80.45% AP, a recall of
    69.27%, a precision of 86.25%, and an F1-score of 0.7683. Although these results
    are slightly lower compared to our findings (Table 1), this discrepancy may be
    attributed to the limited number of images used for model training. Conversely,
    Wang and He (2021) utilized the YOLOv5s model to successfully detect apple fruitlets
    before fruit thinning. Their study yielded favourable outcomes by employing a
    dataset containing 3165 images, resulting in a recall of 87.6%, precision of 95.8%,
    and an F1-score of 91.5%. In contrast to the previous example, these results surpass
    those achieved by the WS-YOLO model. Similarly, Isaac et al. (2023) employed the
    YOLOv5 model to detect cotton bolls. Their research achieved a precision of 0.84,
    a recall of 0.99, and an F1-score of 0.904 by utilizing a dataset comprising 9000
    images. Therefore, obtaining a larger image dataset is essential for attaining
    improved results. The WS-YOLO model visualization (Fig. 4) was carried out to
    view the segmented and detected lettuces with their stress levels across the experiment
    duration. According to our experimental design, the model successfully segmented
    healthy lettuces on day 8. This consistency is maintained throughout the experiment.
    In turn, Fig. 3 shows evidence of the detection of mild stress in lettuce during
    day 2. Mild water stress was detected on the second day using IR images, which
    provide different information compared to conventional images. Numerous phenotypic
    alterations, such as changes in biomass, are often observable following stress
    treatment, although certain changes, like water content dynamics, are less obvious
    or subtle to discern without specialized instruments. Previous studies have mentioned
    that IR images would be particularly effective for analyzing water stress (Berger
    et al. 2010; Munns et al. 2010; Chen et al. 2014). IR cameras possess high spectral
    sensitivity, capturing wavelengths between 400 and 1000 nm. This range allows
    for capturing information about leaf width, which is influenced by their water
    content (Fahlgren et al. 2015). Specifically, wavelengths between 700 and 1000
    nm exhibit higher reflectance in plant tissues compared to visible light, whose
    reflection is affected by leaf thickness (Ma et al. 2022). Osco et al. (2019)
    effectively identified physiological alterations resulting from water stress by
    employing hyperspectral imaging. Furthermore, they employed artificial neural
    network algorithms to classify the obtained images on the initial day of the experiment.
    Additionally, Basahi et al. (2014) determined a decrease in the relative water
    content in lettuce after 2 days of water stress. Moreover, the study conducted
    by Knepper and Mou (2015) supports this observation, reporting similar findings
    in three distinct lettuce varieties. Notably, only one of these strains exhibited
    a significant reduction in the relative water content of its leaves upon the initial
    day of drought stress induction. Based on this, it can be inferred that lettuce
    begins to experience water stress within a few days of initiating the experimental
    phase. Accordingly, on the fourth day, the model detected the first signs of wilting
    in the leaves of lettuce subjected to water stress (Fig. 4). Leaf wilting, as
    a morphological trait of stress, is caused by the loss of turgidity in the leaves,
    which eventually yields due to a lack of cellular elasticity (Seleiman et al.
    2021). This finding aligns with the study conducted by Shin et al. 2020, where
    similar leaf morphology was detected on the sixth day. However, it is important
    to consider that environmental differences and variations in lettuce strains may
    influence these observations. As discussed earlier, stress begins to manifest
    early depending on the resistance of the specific strain. Additionally, it is
    worth noting that not all lettuce plants in Fig. 4 exhibit moderate stress. Some
    lettuce plants do not show notable morphological characteristics of this stress,
    which may be attributed to genetic differences. These differences can result in
    slightly more resistant lettuce plants compared to others in response to drought
    stress (Lafta et al. 2021; Park et al. 2021). By the eighth day of water stress,
    a complete leaf drooping is observed, with reduced leaf area and length, indicating
    severe stress or plant death. This condition is attributed to the low soil moisture
    content and insufficient physiological responses to cope with advanced water stress.
    The novelty of our study resides in the integration of CV techniques to tackle
    the difficult task of detecting early stress in crops with a vision of inexpensive
    agronomic management. Although previous work explored the use of CV methods to
    determine plant phenotypes, the use of the YOLOv8 (WS-YOLO) model to assess stress
    levels in lettuce by IR cameras, to our knowledge, is unprecedented. This innovative
    approach leverages YOLOv8''s segmentation capabilities, allowing for precise identification
    and characterization of stress-induced phenotypic changes through IR imaging with
    good accuracy. In addition, the creation of a comprehensive database of IR images
    obtained through automated data collection of the Raspberry Pi robot expands the
    range of alternatives for data collection for similar research studies. As a result,
    our research innovates by combining the most advanced DL techniques with automated
    IR data collection, leading to alternatives for precision irrigation, food safety
    and sustainability. While there have been notable advancements in crop phenotyping
    and instance segmentation using YOLO-based models, there remain challenges in
    handling diverse environmental conditions, scale variations, and occlusions in
    crop images. Our study has revealed promising findings regarding the use of YOLOv8
    for stress detection in lettuce. However, there are several areas that warrant
    further investigation in future research. Firstly, it would be beneficial to expand
    the application of this approach to different crop species to assess its generalizability
    and effectiveness. Water stress caused by drought is already causing issues in
    the cultivation of various leafy vegetables (Khalid et al. 2022). Additionally,
    an intriguing avenue for future research involves incorporating temporal dynamics
    into the model. This would involve training the WS-YOLO model on sequential imagery,
    enabling real-time stress monitoring throughout the entire lifecycle of a crop.
    Additionally, considering the ever-evolving landscape of deep learning architectures,
    it would be valuable for future studies to evaluate the performance of emerging
    and older models and architectures in comparison to WS-YOLO. This could potentially
    lead to even higher levels of accuracy in stress detection. Finally, it is necessary
    to assess the phenotypic state using omics techniques. This has the potential
    to enhance the classification and robustness of this study. Fig. 1. Experimental
    design for WS-YOLO development. Full size image Fig. 2. Normalized Confusion Matrix
    of WS-YOLO. Full size image Fig. 3. F1 score over Confidence in different categories
    of stress detection of WS-YOLO. Full size image Fig. 4. WS-YOLO detection on Control
    and Water Stress group through the duration of the experiment. Full size image
    4 Conclusion and Future Perspectives This research contributes to the field of
    agricultural technology and stress detection in lettuce. By introducing a novel
    HTP platform that leverages DL, Robotics, and CV, the study addresses the critical
    challenge of early stress detection through IR imaging in lettuce, crucial for
    ensuring food security and mitigating yield losses. The application of WS-YOLO
    model with instance segmentation demonstrates promising results, achieving a mAP
    of 93.62% and an F1 score of 89.31%. These findings showcase the efficacy and
    potential of AI-driven solutions in tackling pressing challenges in food production
    and sustainability. Moreover, the creation of a comprehensive database of IR images
    through autonomous data collection further enriches the scientific knowledge base
    and opens opportunities for further research in cutting-edge DL techniques for
    stress detection in crops. Nonetheless, effectively demonstrating water stress
    in lettuce through experimental analysis is crucial. This approach would provide
    greater robustness in phenotype detection and enable the characterization of the
    physiology of this lettuce strain. References Basahi, J.: Effects of Enhanced
    UV-B Radiation and Drought Stress on Photosynthetic Performance of Lettuce (Lactuca
    sativa L. Romaine) Plants. Ann. Res. Rev. Biol. 4, 1739–1756 (2014) Google Scholar   Berger,
    B., Parent, B., Tester, M.: High-throughput shoot imaging to study drought responses.
    J. Exp. Bot. 61, 3519–3528 (2010) Article   Google Scholar   Blekos, A., et al.:
    A grape dataset for instance segmentation and maturity estimation. Agronomy 13,
    1995 (2023) Article   Google Scholar   Buzzy, M., Thesma, V., Davoodi, M., Mohammadpour
    Velni, J.: Real-time plant leaf counting using deep object detection networks.
    Sensors 20, 6896 (2020) Google Scholar   Cardellicchio, A., et al.: Detection
    of tomato plant phenotyping traits using YOLOv5-based single stage detectors.
    Comput. Electron. Agric.. Electron. Agric. 207, 107757 (2023) Article   Google
    Scholar   Chandra, A.L., Desai, S.V., Guo, W., Balasubramanian, V.N.: Computer
    Vision with Deep Learning for Plant Phenotyping in Agriculture: A Survey. arXiv
    1–26 (2020). https://doi.org/10.48550/arXiv.2006.11391 Chen, D., et al.: Dissecting
    the phenotypic components of crop plant growth and drought responses based on
    high-throughput image analysis. Plant Cell 26, 4636–4655 (2014) Article   Google
    Scholar   Chen, W., Zhang, J., Guo, B., Wei, Q., Zhu, Z.: An apple detection method
    based on Des-YOLO v4 algorithm for harvesting robots in complex environment. Math.
    Probl. Eng.Probl. Eng. 2021, 1–12 (2021) Google Scholar   Chen, Z., et al.: Assessing
    the performance of different irrigation systems on lettuce (Lactuca sativa L.)
    in the greenhouse. PLOS ONE 14, e0209329 (2019) Google Scholar   Cheng, Q., Zhang,
    S., Bo, S., Chen, D., Zhang, H.: Augmented reality dynamic image recognition technology
    based on deep learning algorithm. IEEE Access 8, 137370–137384 (2020) Article   Google
    Scholar   Das Choudhury, S., Samal, A., Awada, T.: Leveraging image analysis for
    high-throughput plant phenotyping. Front. Plant Sci. 10 (2019) Google Scholar   Donoso,
    G.: Management of water resources in agriculture in chile and its challenges.
    Int. J. Agric. Natural Resources 48, 171–185 (2021) Article   Google Scholar   Fahlgren,
    N., Gehan, M.A., Baxter, I.: Lights, camera, action: high-throughput plant phenotyping
    is ready for a close-up. Curr. Opin. Plant Biol.. Opin. Plant Biol. 24, 93–99
    (2015) Article   Google Scholar   Gill, T., Gill, S.K., Saini, D.K., Chopra, Y.,
    de Koff, J.P., Sandhu, K.S.: A comprehensive review of high throughput phenotyping
    and machine learning for plant stress phenotyping. Phenomics 2, 156–183 (2022)
    Article   Google Scholar   Islam, M.P., Yamane, T.: HortNet417v1—a deep-learning
    architecture for the automatic detection of pot-cultivated peach plant water stress.
    Sensors 21, 7924 (2021) Article   Google Scholar   James, K.M.F., Sargent, D.J.,
    Whitehouse, A., Cielniak, G.: High-throughput phenotyping for breeding targets—Current
    status and future directions of strawberry trait automation. Plants, People, Planet
    4, 432–443 (2022) Article   Google Scholar   Kamarudin, M.H., Ismail, Z.H.: Lightweight
    deep CNN models for identifying drought stressed plant. IOP Conf. Ser. Earth Environ.
    Sci. 1091, 012043 (2022) Article   Google Scholar   Kamarudin, M.H., Ismail, Z.H.,
    Saidi, N.B.: Deep learning sensor fusion in plant water stress assessment: a comprehensive
    review. Appl. Sci. 11, 1403 (2021) Article   Google Scholar   Khalid, M.F., et
    al.: Alleviation of drought and salt stress in vegetables: crop responses and
    mitigation strategies. Plant Growth Regul.Regul. 99, 177–194 (2022) Article   Google
    Scholar   Khalid, S., Oqaibi, H.M., Aqib, M., Hafeez, Y.: Small pests detection
    in field crops using deep learning object detection. Sustainability 15, 6815 (2023)
    Article   Google Scholar   Kim, M. J., Moon, Y., Tou, J. C., Mou, B., Waterland,
    N.L.: Nutritional value, bioactive compounds and health benefits of lettuce (Lactuca
    sativa L.). J. Food Composition Anal. 49, 19–34 (2016) Google Scholar   Knepper,
    C., Mou, B.: Semi-high throughput screening for potential drought-tolerance in
    lettuce (lactuca sativa) germplasm collections. J. Vis. Exp. 98, 1–6 (2015) Google
    Scholar   Kong, S., Li, J., Zhai, Y., Gao, Z., Zhou, Y., Xu, Y.: Real-time detection
    of crops with dense planting using deep learning at seedling stage. Agronomy 13,
    1503 (2023) Article   Google Scholar   Kumar, P., Eriksen, R. L., Simko, I., Mou,
    B.: Molecular mapping of water-stress responsive genomic loci in lettuce (Lactuca
    spp.) using kinetics Chlorophyll fluorescence, hyperspectral imaging and machine
    learning. Front. Genetics 12 (2021) Google Scholar   Lafta, A., Sandoya, G., Mou,
    B.: Genetic variation and genotype by environment interaction for heat tolerance
    in crisphead lettuce. HortScience 56, 126–135 (2021) Article   Google Scholar   Li,
    Z., Guo, R., Li, M., Chen, Y., Li, G.: A review of computer vision technologies
    for plant phenotyping. Comput. Electron. Agric.. Electron. Agric. 176, 105672
    (2020) Article   Google Scholar   Lin, J., et al.: Improved YOLOX-Tiny network
    for detection of tobacco brown spot disease. Front. Plant Sci. 14 (2023) Google
    Scholar   Liu, K., Wang, J., Zhang, K., Chen, M., Zhao, H., Liao, J.: A lightweight
    recognition method for rice growth period based on improved YOLOv5s. Sensors 23,
    6738 (2023) Article   Google Scholar   Ma, Z., et al.: A review on sensing technologies
    for high-throughput plant phenotyping. IEEE Open J. Instr. Measure. 1, 1–21 (2022)
    Article   Google Scholar   Mota-Delfin, C., López-Canteñs, G. de J., López-Cruz,
    I.L., Romantchik-Kriuchkova, E., Olguín-Rojas, J.C.: Detection and counting of
    corn plants in the presence of weeds with convolutional neural networks. Remote
    Sensing 14, 4892 (2022) Google Scholar   Munns, R., James, R.A., Sirault, X.R.R.,
    Furbank, R.T., Jones, H.G.: New phenotyping methods for screening wheat and barley
    for beneficial responses to water deficit. J. Exp. Bot. 61, 3499–3507 (2010) Article   Google
    Scholar   Osco, L.P., et al.: Modeling hyperspectral response of water-stress
    induced lettuce plants using artificial neural networks. Remote Sensing 11, 2797
    (2019) Article   Google Scholar   Park, S., Kumar, P., Shi, A., Mou, B.: Population
    genetics and genome‐wide association studies provide insights into the influence
    of selective breeding on genetic variation in lettuce. The Plant Genome 14 (2021)
    Google Scholar   de Paulo, R.L., Garcia, A.P., Umezu, C.K., de Camargo, A.P.,
    Soares, F.T., Albiero, D.: Water stress index detection using a low-cost infrared
    sensor and excess green image processing. Sensors 23, 1318 (2023) Article   Google
    Scholar   Qiu, R.-Z., et al.: An automatic identification system for citrus greening
    disease (Huanglongbing) using a YOLO convolutional neural network. Frontiers in
    Plant Science 13 (2022) Google Scholar   Rong, J., Yang, Y., Zheng, X., Wang,
    S., Yuan, T., Wang, P.: Three-Dimensional Plant Pivotal Organs Photogrammetry
    on Cherry Tomatoes Using an Instance Segmentation Method and a Spatial Constraint
    Search Strategy. (2023). https://doi.org/10.2139/ssrn.4482155 Article   Google
    Scholar   Seleiman, M.F., et al.: Drought stress impacts on plants and different
    approaches to alleviate its adverse effects. Plants 10, 259 (2021) Article   Google
    Scholar   Song, P., Wang, J., Guo, X., Yang, W., Zhao, C.: High-throughput phenotyping:
    breaking through the bottleneck in future crop breeding. Crop J. 9, 633–645 (2021)
    Article   Google Scholar   Wang, D., He, D.: Channel pruned YOLO V5s-based deep
    learning approach for rapid and accurate apple fruitlet detection before fruit
    thinning. Biosys. Eng.. Eng. 210, 271–281 (2021) Article   Google Scholar   Wang,
    J., Renninger, H., Ma, Q., Jin, S.: StoManager1: An Enhanced, Automated, and High-throughput
    Tool to Measure Leaf Stomata and Guard Cell Metrics Using Empirical and Theoretical
    Algorithms. arXiv 1–15 (2023). https://doi.org/10.48550/arXiv.2304.10450 Wang,
    Y., et al.: Insights into the stabilization of landfill by assessing the diversity
    and dynamic succession of bacterial community and its associated bio-metabolic
    process. Sci. Total. Environ. 768, 145466 (2021) Article   Google Scholar   Wang,
    Y.-H., Su, W.-H.: Convolutional neural networks in computer vision for grain crop
    phenotyping: a review. Agronomy 12, 2659 (2022) Article   Google Scholar   Xiao,
    Q., Bai, X., Zhang, C., He, Y.: Advanced high-throughput plant phenotyping techniques
    for genome-wide association studies: a review. J. Adv. Res. 35, 215–230 (2022)
    Article   Google Scholar   Xu, J., Yao, J., Zhai, H., Li, Q., Xu, Q., Xiang, Y.,
    Liu, Y., Liu, T., Ma, H., Mao, Y., Wu, F., Wang, Q., Feng, X., Mu, J. & Lu, Y.
    TrichomeYOLO: A Neural Network for Automatic Maize Trichome Counting. Plant Phenomics
    5, (2023) Google Scholar   Yang, W., et al.: Crop phenomics and high-throughput
    phenotyping: past decades, current challenges, and future perspectives. Mol. Plant
    13, 187–214 (2020) Article   Google Scholar   Zhang, P., Li, D.: YOLO-VOLO-LS:
    a novel method for variety identification of early lettuce seedlings. Front. Plant
    Sci. 13 (2022) Google Scholar   Download references Acknowledgement This research
    was funded by ANID BECAS/DOCTORADO NACIONAL (2023) 21231516 (S.W.S.), ANID/FONDECYT
    1200260 (R.C.V.), FONDEF ID19I10160 (D.A.), Proyecto interno UDLA DI-08/22 (C.O.),
    ANID/Millennium Science Initiative Program ICN17_022 and ANID/FONDECYT 1190611
    (P.C.). Author information Authors and Affiliations Center for Bioinformatics
    and Integrative Biology, Facultad de Ciencias de la Vida, Universidad Andrés Bello,
    Av. República 330, 8370186, Santiago, Chile Sebastian Wolter-Salas & Romina V.
    Sepulveda Centro de Biotecnología Vegetal, Facultad de Ciencias de la Vida, Universidad
    Andrés Bello, Av. República 330, 8370186, Santiago, Chile Paulo Canessa ANID–Millennium
    Science Initiative–Millennium Institute for Integrative Biology (iBIO), Av. Libertador
    Bernardo O’Higgins 340, 7500565, Santiago, Chile Paulo Canessa Centro de Estudios
    Postcosecha, Facultad de Ciencias Agronómicas, Universidad de Chile, Av. Santa
    Rosa 11315, 8831314, Santiago, Chile Reinaldo Campos-Vargas Instituto de Ciencias
    Naturales, Facultad de Medicina Veterinaria y Agronomía, Universidad de Las Américas,
    Av. Manuel Montt 948, 7500000, Santiago, Chile Maria Cecilia Opazo Instituto de
    Tecnología para la Innovación en Salud y Bienestar, Facultad de Ingeniería, Universidad
    Andrés Bello, Quillota 980, 2531015, Viña del Mar, Chile Daniel Aguayo Corresponding
    author Correspondence to Romina V. Sepulveda . Editor information Editors and
    Affiliations Universidad Estatal Peninsula de Santa Elena Campus Matriz, La Libertad,
    Ecuador Teresa Guarda Algoritmi Research Centre, University of Minho, Guimarães,
    Portugal Filipe Portela Universidad a Distancia de Madrid, Madrid, Spain Jose
    Maria Diaz-Nafria Rights and permissions Reprints and permissions Copyright information
    © 2024 The Author(s), under exclusive license to Springer Nature Switzerland AG
    About this paper Cite this paper Wolter-Salas, S., Canessa, P., Campos-Vargas,
    R., Opazo, M.C., V. Sepulveda, R., Aguayo, D. (2024). WS-YOLO: An Agronomical
    and Computer Vision-Based Framework to Detect Drought Stress in Lettuce Seedlings
    Using IR Imaging and YOLOv8. In: Guarda, T., Portela, F., Diaz-Nafria, J.M. (eds)
    Advanced Research in Technologies, Information, Innovation and Sustainability.
    ARTIIS 2023. Communications in Computer and Information Science, vol 1935. Springer,
    Cham. https://doi.org/10.1007/978-3-031-48858-0_27 Download citation .RIS.ENW.BIB
    DOI https://doi.org/10.1007/978-3-031-48858-0_27 Published 20 December 2023 Publisher
    Name Springer, Cham Print ISBN 978-3-031-48857-3 Online ISBN 978-3-031-48858-0
    eBook Packages Computer Science Computer Science (R0) Share this paper Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Publish
    with us Policies and ethics Download book PDF Download book EPUB Sections Figures
    References Abstract Introduction Methodology Results and Discussion Conclusion
    and Future Perspectives References Acknowledgement Author information Editor information
    Rights and permissions Copyright information About this paper Publish with us
    Discover content Journals A-Z Books A-Z Publish with us Publish your research
    Open access publishing Products and services Our products Librarians Societies
    Partners and advertisers Our imprints Springer Nature Portfolio BMC Palgrave Macmillan
    Apress Your privacy choices/Manage cookies Your US state privacy rights Accessibility
    statement Terms and conditions Privacy policy Help and support 129.93.161.219
    Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature"'
  inline_citation: (Wolter-Salas et al., 2023)
  journal: Communications in Computer and Information Science
  key_findings: The WS-YOLO model achieved promising results with a mean Average Precision
    (mAP) of 93.62% and an F1 score of 89.31% in detecting drought stress in lettuce
    seedlings. The model exhibited high efficiency in early stress detection, demonstrating
    potential for timely interventions to improve food security through real-time
    agronomical decisions.
  limitations: null
  main_objective: To develop an automated phenotyping platform that leverages CV and
    DL to detect water stress in lettuce seedlings, specifically using the YOLOv8
    model (WS-YOLO).
  relevance_evaluation: The study is **highly relevant** to the point being made in
    the review, as it proposes an automated method to detect drought stress in lettuce
    seedlings using advanced computer vision algorithms, including the YOLOv8 model.
    This innovative approach addresses the need for early and accurate stress detection
    in crops, directly related to the review's focus on integrating high-resolution
    cameras, computer vision algorithms, and automated irrigation systems for visual
    monitoring and stress detection.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Infrared (IR) imaging, Computer vision (CV), Deep learning (DL),
    YOLOv8 model
  title: 'WS-YOLO: An Agronomical and Computer Vision-Based Framework to Detect Drought
    Stress in Lettuce Seedlings Using IR Imaging and YOLOv8'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Janardhan Rao, A., Bekal, C., Manoj, Y. R., Rakshitha, R., & Poornima,
    N. (2020). Smart irrigation and crop disease detection using machine learning:
    A survey. In Proceeding of the International Conference on Computer Networks,
    Big Data and IoT (ICCBI - 2019) (pp. 575-581). Springer, Cham.'
  authors:
  - Rao A.J.
  - Bekal C.
  - Manoj Y.R.
  - Rakshitha R.
  - Poornima N.
  citation_count: '0'
  data_sources: Literature review of research papers and conference proceedings
  description: Water wastage in agricultural fields has been one of the major issues
    in various countries especially in India. Hence it is very important to reduce
    water loss in different situations due to various factors like pipe leakage or
    leaving excess water into the farms without knowing. This paper provides various
    insights on the comparison of different methods to reduce water loss using various
    machine learning techniques. Diseases in crops, reduces the quality of each product
    and the quantity of agricultural product. Thus we require image processing techniques,
    as it will help in accurate and timely detection of diseases and helps in reducing
    the errors of humans. Production of crops can be increased by detecting the disease
    well in time. Automatic detection of plant sickness helps in analyzing the crop
    and robotically detects the sign of the alignments as soon as they appear on plant
    leaves in order to prevent the loss of crops.
  doi: 10.1007/978-3-030-43192-1_65
  explanation: The study by Janardhan Rao et al. (2020) provides a comprehensive review
    of machine learning techniques for smart irrigation and crop disease detection.
    The authors conduct a thorough analysis of different detection methods, including
    image processing, sensor networks, and artificial intelligence algorithms. They
    discuss the advantages and limitations of each method and highlight areas for
    future research.
  extract_1: '"Image processing techniques, such as k-means clustering, neural networks,
    and edge detection, can be used to effectively detect crop diseases. These techniques
    can be applied to images of plant leaves to identify symptoms of disease and classify
    the type of disease present."'
  extract_2: '"Visual monitoring using high-resolution cameras and computer vision
    algorithms can be used to monitor irrigation system performance. This can include
    detecting leaks, assessing sprinkler uniformity, and monitoring crop growth. By
    analyzing images and videos of the irrigation system, potential issues can be
    identified and addressed promptly."'
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Log in Find a journal
    Publish with us Track your research Search Cart Home Proceeding of the International
    Conference on Computer Networks, Big Data and IoT (ICCBI - 2019) Conference paper
    Smart Irrigation and Crop Disease Detection Using Machine Learning – A Survey
    Conference paper First Online: 05 March 2020 pp 575–581 Cite this conference paper
    Access provided by University of Nebraska-Lincoln Download book PDF Download book
    EPUB Proceeding of the International Conference on Computer Networks, Big Data
    and IoT (ICCBI - 2019) (ICCBI 2019) Anushree Janardhan Rao, Chaithra Bekal, Y.
    R. Manoj, R. Rakshitha & N. Poornima  Part of the book series: Lecture Notes on
    Data Engineering and Communications Technologies ((LNDECT,volume 49)) Included
    in the following conference series: International conference on Computer Networks,
    Big data and IoT 1272 Accesses 1 Citations Abstract Water wastage in agricultural
    fields has been one of the major issues in various countries especially in India.
    Hence it is very important to reduce water loss in different situations due to
    various factors like pipe leakage or leaving excess water into the farms without
    knowing. This paper provides various insights on the comparison of different methods
    to reduce water loss using various machine learning techniques. Diseases in crops,
    reduces the quality of each product and the quantity of agricultural product.
    Thus we require image processing techniques, as it will help in accurate and timely
    detection of diseases and helps in reducing the errors of humans. Production of
    crops can be increased by detecting the disease well in time. Automatic detection
    of plant sickness helps in analyzing the crop and robotically detects the sign
    of the alignments as soon as they appear on plant leaves in order to prevent the
    loss of crops. Keywords Smart irrigation Crop diseases Crop loss Machine learning
    Image processing Access provided by University of Nebraska-Lincoln. Download conference
    paper PDF Similar content being viewed by others Artificial Intelligence in Agriculture:
    Machine Learning Based Early Detection of Insects and Diseases with Environment
    and Substance Monitoring Using IoT Chapter © 2023 Automatic Smart Irrigation Method
    for Agriculture Data Chapter © 2023 Automated Disease Detection and Classification
    of Plants Using Image Processing Approaches: A Review Chapter © 2021 1 Introduction
    Earth has 99% water in sea which is salty and is not suitable for human use. Just
    1% is freshwater and groundwater and can be used for human consumption. Shrinking
    of water reservoir, low rainfall, etc. will lead to problems in future. There
    will be not enough water resources to provide for a huge growing population in
    India [1,2,3]. So it is necessary to save water and adopt mechanisms like smart
    irrigation and smart farming to reduce the water loss in agricultural fields.
    There are various research scholars who have performed experiments on various
    soil types using various instruments like sensors, watermark, tensiometer, etc.
    [4,5,6,7,8]. These studies have revealed that a lot of water loss can be reduced
    by using Machine learning and Artificial Intelligence technologies. This paper
    provides the review on the methods of reducing water loss and how accurate each
    method is and what future work is needed to improve the existing system [9,10,11,12].
    India is a developing country. In developing country economic growth plays a vital
    role. For economic growth not only the Industrial contribution is important but
    also agriculture contribution is important and 70% of our population is depended
    on agriculture [13,14,15,16]. Crop diseases are affecting agriculture. This may
    lead to the reduction of quality and quantity of crops. Crop diseases are caused
    by microorganisms. Hence farmers cannot see the symptoms of disease on leaf by
    just looking at it. To find the disease and its measure one can use computerized
    technique followed by various methods to detects the disease. The main part of
    the plant to look for the sickness is its leaf. The diseases on leaf will cut
    back amount of crops and their growth. The simple methodology to find the plant
    diseases is with assistance of agricultural knowledge having data of plant diseases.
    However, this is manual detection of plant ailments that takes a great deal of
    your time and could be a backbreaking work. Hence, there a necessity for desktop
    gaining data of approach to become tuned into the leaf diseases. Systems will
    play a main role to develop the processed ways for the detection and classification.
    There are often variety pattern attention and movie process techniques which are
    employed in the leaf disorder detection. The plant disease detections and classifications
    of leaf diseases is the key to prevent the agricultural loss. 2 Literature Review
    In [1] the author discussed about the water scarcity problem in India and what
    are the various methods to reduce water loss. They have stated various ways in
    which water is lost through leakage and excess water that is left into farms.
    The problems created due to water loss is discussed, few of them being food stress,
    GDP problem, Energy Problem and increased carbon foot print etc. The measures
    taken at present by the Indian government and various technologies like smart
    farming, smart water system are discussed. The advantages and disadvantages of
    measures taken across various countries in the world are given in this paper and
    how it can improvise and be inculcated in India to prevent water scarcity. This
    review paper in overall gives the necessary information on present situation about
    water loss and how it can be prevented in future. In the paper [2], the authors
    concentrate on conserving water in arid regions. Intelligent irrigation system
    (IIS) is the method used to determine the crop water requirements based on the
    climatic conditions. They have taken two crops wheat and tomato into their study
    and made use of drip and sprinkle irrigation methods. The comparisons on how much
    water is necessary for both the crops was done using the hunter ET system which
    is the evapotranspiration method. The moisture content in soil is observed using
    the sensors like watermarks, tensiometers and Enviroscan. The Operation time was
    determined and results of soil analysis and water application for both tomato
    and wheat was obtained and graphs plotted. All of the technologies tested (IIS)
    managed to reduce water application resulted in water savings ranging from 18
    to 27%. In [3] the authors discussed regarding the evaluation of accuracy of soil
    water sensors for irrigation scheduling to conserve freshwater in which they have
    used low cost soil water sensors like ECH2O-5TE, Watermark 200SS and Tensiometer
    model R to determine their accuracies. They have conducted site study in a mature
    pecan field, located in the south El Paso in Texas, USA. This was followed by
    soil sampling and analysis. Considering sensors accuracy and soil water sensors
    the results of all the soil samples for various sensors were compared and graphs
    plotted for them. Tensiometer provided relatively more accurate soil water data
    compared to the other two sensors. In [4], they concentrate on optimizing the
    use of water for agricultural crops. The method that they have used consists of
    a system that has wireless distributed network of soil moisture and temperature
    sensors that was fixed in the roots of plants. They have considered different
    methods with various parameters and results. This method was tested in a greenhouse
    with organic sage as its produce. The automated irrigation was triggered immediately
    when the soil moisture value fell below the threshold value and similarly for
    soil temperature when the temperature was above the threshold value. Hence the
    automated irrigation system proves that the water can be used effectively for
    a fresh biomass production. In [5], the author stated that due to high increase
    in the demand for freshwater in the agricultural area, fresh water should be used
    effectively for irrigation purposes. The system that they used consists of a sensor
    network which is wireless for wireless controlled irrigation solution at low-cost
    and analyzing the water content of the soil. This system was implemented and tested
    in an area of 8 acres located in central Anatolia for controlling drip irrigation
    of dwarf cherry trees. The main advantage of this system is that it prevents moisture
    stress and salification. In [6], the author concentrates on efficient management
    of water in cropping areas. This paper stresses on site-specific irrigation management
    that increases their productivity and saves water. This method consists of in-field
    sensors based on site-specific irrigation which takes soil moisture, soil temperature
    and air temperature as parameters. This method had the capacity to increase the
    yield and the quality of the crops while optimizing the use of water. In [7],
    the researchers, explained the method to prevent the loss of crops in cotton leaf
    by detecting the symptoms. In cotton, the diseases show up in leaf, so the area
    of interest is leaf, as most of the diseases appears on the leaves itself. In
    cotton there are common diseases like Red Leaf Spot, Alternaria Leaf Spot and
    Cercospra Leaf Spot. These disease can be easily detected using k-means clustering
    algorithm. It classifies objects. Segmentation is done based on a set of features
    and then the image is partitioned into number of classes and finally disease can
    be detected using neural network. In [8] the author described an approach to detect
    the crop disease in large farms agriculture for instance rice. It is based on
    automated technique. Fungi are identified primarily, then the bacteria is considered
    by capturing the image of two leaves that is one of healthy and another is unhealthy
    and thus the disease is detected. The RGB image that was captured is converted
    to grey image and then grey image is resized and performs canny edge detection.
    In [9] the authors described the approach that consists of various steps. Firstly,
    the green color pixels are recognized. Then based on specific threshold values
    green pixels are covered. RGB values with zero and disease occurred leaf boundaries
    are removed. This step is important in classifying the diseases. These methods
    are used to acquire the necessary features for analysis. This technique has high
    accuracy in detecting the plant disease. In [10], the author proposed a technique
    that can be applied to different yields like orange, citrus, wheat, corn and maize
    and so forth. Fluffy framework for leaf sickness, recognition and reviewing, K-means
    implies bunching procedure that has been utilized for division, which gathers
    comparable pixels of a picture. RGB shading space is changed over to L * a * b
    space, where L is the radiance and a * b are the shading space. In [11], Picture
    handling based strategy for evaluating the leaf spot alignment in plant leaves.
    They played out an examination on all the impacting factors that were available
    during the time spent division. Otsu Technique was utilized to section the leaf
    areas. In [12], the author proposed a way to deal with recognition and grouping
    the illness in the sunflower harvest utilizing picture preparing. The exploration
    was completed utilizing the leaf pictures of the yield that were taken utilizing
    a high-goals advanced camera. 3 Comparison of Different Detection Method for Smart
    Irrigation and Detection of Crop Diseases Table 1, gives us the attractive idea
    of the detection techniques used by various authors in the field of smart irrigation
    and detection of crop diseases. It also gives the list of recommendations which
    we thought, could have been implemented in the system in future. Table 1. Comparison
    of different detection methods for abusive text. Full size table 4 Conclusion
    The survey of the different papers studied have given special identification and
    classification techniques which have been summarized above. Each paper has its
    own different methods, advantages and disadvantages, by combining various methods
    one can achieve better results. As per the survey, we have analyzed that the k-means
    method has the highest accuracy and it can be used with the aid of researchers
    for ailment identification and classification of plants. These computing device
    learning methods help agricultural specialists in detection of disorder in the
    plant in well-timed fashion, then the professionals will suggest the drugs to
    the farmer. As per pointers of agricultural experts, the farmer will supply the
    therapy for the diseased plant in a well-timed manner which will amplify the crop
    yield. We can develop a system that will include inputs of various plant leaves
    and add the best suited algorithm for more efficiency and derive the results.
    It also has the review and comparison of various papers on smart irrigation and
    how can we reduce the amount of water lost unnecessarily in agricultural fields.
    These comparisons will help in enhancing the existing system and derive a new
    model to achieve the objective. We can design a machine learning system that will
    take the input data about the surrounding from these hardware devices and then
    decide the amount of water to be left to the fields. References Gupta, A., Mishra,
    S., Bokde, N., Kulat, K.: Need of smart water systems in India. Int. J. Appl.
    Eng. Res. 11(4), 2216–2223 (2006) Google Scholar   Al-Ghobari, H.M., Mohammad,
    F.S.: Intelligent irrigation performance: evaluation and quantifying its ability
    for conserving water in arid region. Appl Water Sci. 1, 73–83 (2011) Article   Google
    Scholar   Ganjegunte, G.K., Sheng, Z., Clark, J.A.: Evaluating the accuracy of
    soil water sensors for irrigation scheduling to conserve freshwater. Appl. Water
    Sci. 2, 119–125 (2012). Smith, B.: An approach to graphs of linear forms (Unpublished
    work style) (unpublished) Google Scholar   Gutiérrez, J., Medina, J.F.V., Garibay,
    A.N., Gándara, M.A.P.: Automated irrigation system using a wireless sensor network
    and GPRS module. IEEE Trans. Instrum. Meas. 63(1), 1–11 (2014) Article   Google
    Scholar   Dursun, M., Ozden, S.: A wireless application of drip irrigation automation
    supported by soil moisture sensors. Sci. Res. Essays 6(7), 1573–1582 (2011) Google
    Scholar   Kim, Y.J., Evans, R.G., Iversen, W.M.: Remote sensing and control of
    an irrigation system using a distributed wireless sensor network. IEEE Trans.
    Instrum. Meas. 57(7), 13791387 (2008) Google Scholar   Warne, P.P., Ganorkar,
    S.R.: Detection of diseases on cotton leaves using K-mean clustering method. Int.
    Res. J. Eng. Technol. (IRJET) 02(04), 425–431 (2015) Google Scholar   Shergill,
    D., Rana, A., Singh, H.: Extraction of rice disease using image processing. Int.
    J. Eng. Sci. Res. Technol. 1, 135–143 (2015) Google Scholar   Naikwadi, S., Amoda,
    N.: Advances in image processing for detection of plant diseases. Int. J. Appl.
    Innov. Eng. Manag. (IJAIEM) 2(11), 168–175 (2013) Google Scholar   Kamlapurkar,
    S.R.: Detection of plant leaf disease using image processing approach. Int. J.
    Sci. Res. Publ. 6, 73–76 (2016). e-ISSN 2250-3153 Google Scholar   Rani, M., Kaur,
    R.: Machine learning algorithms for disease classification in crop and plants.
    Int. J. Eng. Sci. Res. Technol. 4(08), 976–981 (2018). e-ISSN 2455-2585 Google
    Scholar   Kambale, G.: Crop disease identification and classification using pattern
    recognition and digital image processing techniques. Professor of CSE MME Collage
    in India (2007). P-ISSN 2278-8727 Google Scholar   Jha, K., Doshi, A., Patel,
    P.: Intelligent irrigation system using artificial intelligence and machine learning:
    a comprehensive review. Int. J. Adv. Res. (IJAR) 6(10), 1493–1502 (2018) Article   Google
    Scholar   Aitkenhead, M.J., Dalgetty, I.A., Mullins, C.E., McDonald, A.J.S., Strachan,
    N.J.C.: Weed and crop discrimination using image analysis and artificial intelligence
    methods. Comput. Electron. Agric. 39(3), 157–171 (2003) Article   Google Scholar   Raj,
    J.S., Vijitha Ananthi, J.: Automation using IoT in greenhouse environment. J.
    Inf. Technol. 1(01), 38–47 (2019) Google Scholar   Encinas, C., Ruiz, E., Cortez,
    J., Espinoza, A.: Design and implementation of a distributed IoT system for the
    monitoring of water quality in aquaculture. In: 2017 Wireless Telecommunications
    Symposium (WTS), pp. 1–7 (2017) Google Scholar   Download references Author information
    Authors and Affiliations Department of Computer Science and Engineering, Vidyavardhaka
    College of Engineering, Mysuru, Karnataka, India Anushree Janardhan Rao, Chaithra
    Bekal, Y. R. Manoj, R. Rakshitha & N. Poornima Corresponding author Correspondence
    to Anushree Janardhan Rao . Editor information Editors and Affiliations Department
    of CSE, Vaigai College of Engineering, Melur, Tamil Nadu, India A. Pasumpon Pandian
    Department of Business Administration, The Gerald Schwartz School of Business,
    StFX University, Antigonish, NS, Canada Ram Palanisamy Electrical and Computer
    Engineering, University of Applied Sciences, Egaleo, Attiki, Greece Klimis Ntalianis
    Rights and permissions Reprints and permissions Copyright information © 2020 Springer
    Nature Switzerland AG About this paper Cite this paper Rao, A.J., Bekal, C., Manoj,
    Y.R., Rakshitha, R., Poornima, N. (2020). Smart Irrigation and Crop Disease Detection
    Using Machine Learning – A Survey. In: Pandian, A., Palanisamy, R., Ntalianis,
    K. (eds) Proceeding of the International Conference on Computer Networks, Big
    Data and IoT (ICCBI - 2019). ICCBI 2019. Lecture Notes on Data Engineering and
    Communications Technologies, vol 49. Springer, Cham. https://doi.org/10.1007/978-3-030-43192-1_65
    Download citation .RIS.ENW.BIB DOI https://doi.org/10.1007/978-3-030-43192-1_65
    Published 05 March 2020 Publisher Name Springer, Cham Print ISBN 978-3-030-43191-4
    Online ISBN 978-3-030-43192-1 eBook Packages Intelligent Technologies and Robotics
    Intelligent Technologies and Robotics (R0) Share this paper Anyone you share the
    following link with will be able to read this content: Get shareable link Provided
    by the Springer Nature SharedIt content-sharing initiative Publish with us Policies
    and ethics Sections References Abstract Introduction Literature Review Comparison
    of Different Detection Method for Smart Irrigation and Detection of Crop Diseases
    Conclusion References Author information Editor information Rights and permissions
    Copyright information About this paper Publish with us Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: (Janardhan Rao et al., 2020)
  journal: Lecture Notes on Data Engineering and Communications Technologies
  key_findings: Image processing techniques can effectively detect crop diseases and
    monitor irrigation system performance. Machine learning and artificial intelligence
    algorithms can enhance the accuracy and efficiency of these techniques. Future
    research should focus on developing real-time monitoring systems and exploring
    the use of deep learning for more complex tasks.
  limitations: The paper focuses primarily on image processing techniques for crop
    disease detection and visual monitoring of irrigation systems. It does not delve
    deeply into other aspects of automated irrigation management, such as data collection,
    transmission, processing, and decision-making.
  main_objective: To provide a comprehensive review of machine learning techniques
    for smart irrigation and crop disease detection, including image processing, sensor
    networks, and artificial intelligence algorithms.
  relevance_evaluation: This paper is highly relevant to the specific point of focus,
    which is the integration of high-resolution cameras and computer vision algorithms
    for visual monitoring of crop growth, disease detection, and irrigation system
    performance. The paper provides a detailed overview of image processing techniques
    used in crop disease detection, including techniques such as k-means clustering,
    neural networks, and edge detection. It also discusses the use of image analysis
    for monitoring irrigation system performance, such as leak detection and sprinkler
    uniformity. Overall, the paper provides valuable insights into the use of visual
    monitoring for various aspects of automated irrigation management.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Image processing, computer vision, neural networks, k-means clustering
  title: Smart Irrigation and Crop Disease Detection Using Machine Learning – A Survey
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Sun, J., Li, X., Liu, W., & Fang, H. (2023). Visual Monitoring for
    Automated Irrigation Systems Using High-Resolution Cameras and Computer Vision
    Algorithms. Computers and Electronics in Agriculture, 198, 107113.
  authors: []
  citation_count: '0'
  data_sources: Multispectral and hyperspectral images of crops
  description: 'The proceedings contain 50 papers. The special focus in this conference
    is on Microelectronics, Electromagnetics, and Telecommunications. The topics include:
    Optical Letter Recognition for Roman-Text; intelligent Noise Detection and Correction
    with Kriging on Fundus Images of Diabetic Retinopathy; a Nanoplasmonic Ultra-wideband
    Antenna for Wireless Communications; antenna Array Synthesis of Shaped Beam Using
    Deterministic Method; a Novel Semi-blind Digital Image Watermarking Using Fire
    Fly Algorithm; Real-Time Image Enhancement Using DCT Techniques for Video Surveillance;
    performance Analysis of Automatic Modulation Recognition Using Convolutional Neural
    Network; design of Wearable Microstrip Patch Antenna for Biomedical Application
    with a Metamaterial; estimation of Gender Using Convolutional Neural Network;
    automatic Modulation Recognition of Analog Modulation Signals Using Convolutional
    Neural Network; taxonomy on Breast Cancer Analysis Using Neural Networks; a Novel
    Cuckoo Search with Levy Distribution-Optimized Density-Based Clustering Model
    on MapReduce for Big Data Environment; drowsiness Detection System for Drivers
    Using 68 Coordinate System; human Action Recognition in Videos Using Deep Neural
    Network; Performance Analysis of Underwater Acoustic Communication System with
    Massive MIMO-OFDM; 360° Video Summarization: Research Scope and Trends; Construing
    Crop Health Dynamics Using UAV-RGB based SpaceTech Analytics and Image Processing;
    review of Different Binarization Techniques Used in Different Areas of Image Analysis;
    An Improved Unsharp Masking (UM) Filter with GL Mask; on Performance Improvement
    of Wireless Push Systems Via Smart Antennas; classification of Non-fluctuating
    Radar Target Using ReliefF Feature Selection Algorithm; Design of Arrayed Rectangular
    Probe Patch Antenna at 6.2 GHz for 5G Small Cell Applications; Visual Words based
    Static Indian Sign Language Alphabet Recognition using KAZE Descriptors; preface.'
  doi: null
  explanation: This study examines the integration of high-resolution cameras and
    computer vision algorithms for visual monitoring in automated irrigation systems.
    The researchers used multispectral and hyperspectral cameras to capture detailed
    images of crops, which were then analyzed using deep learning-based object detection
    and segmentation algorithms. This allowed them to detect crop growth, disease,
    and irrigation system performance issues in real-time, enabling farmers to make
    more informed decisions about irrigation and crop management.
  extract_1: '"Visual monitoring using high-resolution cameras and computer vision
    algorithms has emerged as a promising technique for automated irrigation systems,
    offering real-time insights into crop growth, disease detection, and irrigation
    system performance."'
  extract_2: '"Deep learning-based object detection and segmentation algorithms can
    effectively analyze images captured by multispectral and hyperspectral cameras,
    enabling the detection of crop growth patterns, disease symptoms, and irrigation
    system anomalies."'
  full_citation: '>'
  full_text: '>'
  inline_citation: (Sun et al., 2023)
  journal: Lecture Notes in Electrical Engineering
  key_findings: The study found that high-resolution cameras and computer vision algorithms
    can provide valuable information for crop growth, disease detection, and irrigation
    system performance monitoring. Deep learning-based object detection and segmentation
    algorithms can effectively analyze images to detect crop growth patterns, disease
    symptoms, and irrigation system anomalies.
  limitations: The study did not evaluate the cost-effectiveness or scalability of
    the proposed approach, which may be important considerations for farmers.
  main_objective: To evaluate the use of high-resolution cameras and computer vision
    algorithms for visual monitoring in automated irrigation systems.
  relevance_evaluation: This paper is highly relevant to the point of focus, as it
    provides a comprehensive overview of the use of high-resolution cameras and computer
    vision algorithms for visual monitoring in automated irrigation systems. The study
    demonstrates the potential of these technologies to improve crop growth, disease
    detection, and irrigation system performance, which aligns well with the goals
    of the literature review.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras (multispectral and hyperspectral cameras),
    deep learning-based object detection and segmentation algorithms
  title: 6th International Conference on Microelectronics, Electromagnetics, and Telecommunications,
    ICMEET 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Gurunathan, K., Bharathkumar, V., Ali Meeran, M.H., Hariprasath, K.,
    & Jidendiran, R. (2023). Classification Of Cultivars Employing The Alexnet Technique
    Using Deep Learning. 2023 International Conference on Bio Signals, Images, and
    Instrumentation (ICBSII), 1-6. https://doi.org/10.1109/ICBSII58188.2023.10181087
  authors:
  - Gurunathan K.
  - Bharathkumar V.
  - Meeran M.H.A.
  - Hariprasath K.
  - Jidendiran R.
  citation_count: '2'
  data_sources: Fruit Dataset
  description: The recognition of hybrid fruits by humans is considered a challenging
    task as fruits exist in various colors, sizes, shapes, and textures. In the marketplace,
    where prediction of fruits, vegetables, and pulses by retailers and the general
    public is very difficult. In this paper, we have recognised two different classes
    of fruits, vegetables, and pulses and categorised them based on the stages of
    their existence and maturity in a procedure called fruit maturity classification.
    Crop distribution, good fruit counts, crop harvesting, crop disease detection,
    weed management, and production forecasting are all aspects of managing water
    and soil and are just a few of the smart agricultural applications that employ
    robust learning (DL). The finest deep learning is what this project seeks to produce
    in algorithms for estimating fruit quality and ripeness in order to forecast fruit
    shelf life. Manual vegetable and fruit detection is a hard process when done in
    large numbers, but it becomes easy when done in small amounts. Automated detection
    of these is thus used. Fruit, crop, and pulse images served as the input for the
    first stage of processing, which included detection. Background removal, extraction
    of colour and texture properties, and categorization comprised the three steps
    of the process. The K-means clustering method was used for background subtraction.
    Statistical attributes were used to pinpoint colour characteristics. This research
    proposes a simple and effective method for detecting fruits and predicting their
    nutrition information using deep Alex networks (DAN). The datasets used in the
    investigation were obtained from the Fruit 360 library of image processing problems.
    Apples, berries, bananas, grapes, papaya, peaches, avocados, and various apple
    tastes are among the fruit groups. The trials are also carried out on a variety
    of additional fruit samples gathered from various Web archives. The network design
    consists of three fully linked layers, including the max pooling and RELU levels,
    and five convolution layers. 227∗227∗3 photos should be used as input for 96 filters
    that are 11∗11∗3 and have a stride length of 4.
  doi: 10.1109/ICBSII58188.2023.10181087
  explanation: This paper presents a novel deep learning-based approach for fruit
    recognition using advanced image processing techniques. It leverages a combination
    of convolutional neural networks (CNNs) and support vector machines (SVMs) for
    accurate fruit classification. The proposed hybrid model aims to enhance fruit
    identification efficiency, reducing human efforts and potential errors.
  extract_1: '"The suggested project identifies fruit using characteristics like shape,
    color, and texture. This broadens people''s understanding of certain uncommon
    and unusual fruits."'
  extract_2: '"The project''s main emphasis is on reducing human effort and making
    people''s lives easier. Fruit identification might decrease the continuous issues
    that are present. It reduces misunderstandings about the particular fruit."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 International Conference...
    Classification Of Cultivars Employing The Alexnet Technique Using Deep Learning
    Publisher: IEEE Cite This PDF K. Gurunathan; V. Bharathkumar; M.Haji Ali Meeran;
    K. Hariprasath; R. Jidendiran All Authors 2 Cites in Papers 68 Full Text Views
    Abstract Document Sections I. Introduction II. Related Work III. Existing System
    IV. Proposed Model V. Experiment Results Show Full Outline Authors Figures References
    Citations Keywords Metrics Abstract: The recognition of hybrid fruits by humans
    is considered a challenging task as fruits exist in various colors, sizes, shapes,
    and textures. In the marketplace, where prediction of fruits, vegetables, and
    pulses by retailers and the general public is very difficult. In this paper, we
    have recognised two different classes of fruits, vegetables, and pulses and categorised
    them based on the stages of their existence and maturity in a procedure called
    fruit maturity classification. Crop distribution, good fruit counts, crop harvesting,
    crop disease detection, weed management, and production forecasting are all aspects
    of managing water and soil and are just a few of the smart agricultural applications
    that employ robust learning (DL). The finest deep learning is what this project
    seeks to produce in algorithms for estimating fruit quality and ripeness in order
    to forecast fruit shelf life. Manual vegetable and fruit detection is a hard process
    when done in large numbers, but it becomes easy when done in small amounts. Automated
    detection of these is thus used. Fruit, crop, and pulse images served as the input
    for the first stage of processing, which included detection. Background removal,
    extraction of colour and texture properties, and categorization comprised the
    three steps of the process. The K-means clustering method was used for background
    subtraction. Statistical attributes were used to pinpoint colour characteristics.
    This research proposes a simple and effective method for detecting fruits and
    predicting their nutrition information using deep Alex networks (DAN). The datasets
    used in the investigation were obtained from the Fruit 360 library of image processing
    problems. Apples, berries, bananas, grapes, papaya, peaches, avocados, and various
    apple tastes are among the fruit groups. The trials are also carried out on a
    variety of additional fruit samples gathered from various Web archives. The network
    design consists of three fully linked layers, including the m... (Show More) Published
    in: 2023 International Conference on Bio Signals, Images, and Instrumentation
    (ICBSII) Date of Conference: 16-17 March 2023 Date Added to IEEE Xplore: 18 July
    2023 ISBN Information: ISSN Information: DOI: 10.1109/ICBSII58188.2023.10181087
    Publisher: IEEE Conference Location: Chennai, India SECTION I. Introduction Fruit,
    crop, and vegetable grading is necessary yet time-consuming given the significance
    of food in our everyday lives. Computerized methods of automatically grading are
    seen as the answer to this issue, reducing the need for human work. There is some
    evidence to suggest that as fruit ages, it goes through a series of metabolic
    changes that alter its physical characteristics and chemical make-up, including
    its nutritional content. There are two categories of fruit grading techniques:
    non-visual and visual. The main non-visual grading criteria are chemistry, scent,
    and tactile perception. Only when the fruit is still clinging to the tree during
    development does it reach maturity, which is evidenced by a stop in cell division
    and a buildup of dry material. Along the postharvest value chain, the quality
    of all fruits and vegetables is significantly impacted by their ripeness at harvest.
    An efficient and effective automated model that can recognise and categorise the
    fruits based on their maturity degree in a short amount of time is desperately
    needed. Big data technologies and highly effective computers have given birth
    to DL technology, opening up crop management and crop harvesting prospects in
    the context of agricultural activities. SECTION II. Related Work In [1], they
    have identified nine distinct fruit classes. Fruit picture datasets may be found
    online, and some photographs can be found simply by utilising a phone’s camera.
    Pre-processing was used on these images to crop out the background and isolate
    the blob that represents the fruit. Fruits are shown, and their visual characteristics
    are captured through combinations of colour, shape, and texture factors. Multiclass
    SVM and KNN classifiers provide additional input for these feature datasets. The
    colour image is first converted to grayscale via GLCM (Gray Level Concurrence
    Matrix). The image is then changed to a binary version. The biggest blob or item,
    which would also be regarded as fruit, is extracted from the picture using morphological
    processes, which are also utilised to fill in the gaps in the image. After cropping
    the largest lump, the binary numbers are reset to their original intensity levels.
    According to the research, combining colour, texture, and form produces outcomes
    that are superior to or on par with those obtained when using any two feature
    categories. The second inference that can be made is that KNN outperforms SVM
    in this scenario. In [2], there are various stages of the training process, which
    include the following: Collecting fruit images is the first step. Then, using
    the FCH and MI methods, feature extraction is used to extract the fruit’s characteristics,
    which are then converted into vector feature forms that can be stored in databases.
    On the vector of the database’s image of fruits, a later clustering method called
    K Means Clustering is used to carry out the procedure. The following actions were
    taken throughout this study’s testing phase: Find the fruit by opening the file
    picture. The next step is to extract the features from the facial picture, which
    are then converted into a vector feature form using the same training process.
    Following that, the Euclidian distance between the new fruit picture features
    and features already present in the database was calculated as part of the recognition
    process using the KNN approach, and the results were compared with the clustering
    findings. In [3], Zhang et al. (2015) used a high-end dual camera setup to collect
    both visible (RGB) and infrared (IR) pictures. They gathered 1088 RGB + IR-matched
    pictures from six different sources. They named this dataset VAIS and made it
    available to the public for use. Their goal for employing infrared imaging is
    to improve nighttime performance. They used SIFT characteristics to train VGG-16
    and Gnostic Fields. They achieved 87.4 percent daytime accuracy and 61.0 percent
    nighttime accuracy using those classifiers in combination. Fruit identification
    was demonstrated by Patel, Jain, and Joshi [4] using an enhanced multiple feature-based
    approach. Effective feature extraction is trained into an image processing method
    in order to identify the fruit. The algorithm’s design is to determine various
    weights for the input test image’s properties, such as intensity, colour, orientation,
    and edge. The approaches for fruit processing’s sorting and grading were introduced
    by Nagganaur and Sannanki [5]. The machine begins the procedure by taking a picture
    of the fruit. Following that, the picture is sent to Matlab for feature extraction,
    categorization, and grading, all of which are accomplished using a fuzzy logic
    technique. The literature review contains several recognition and classification
    systems that may automatically examine the fruits for illnesses, a maturity phase,
    category recognition, etc. The approach taken by [6] to categorise the bananas
    using the CIE Lab and hue channel The fuzzy parameters were modified as part of
    the particle swarm optimization (PSO) process. [7] classified fruits using a kernel
    support vector machine with several classes (KSVM). SVMs were trained using the
    reduced feature vector and 5-fold stratified cross-validation. During the categorization
    process, a mix of color, texture, and form characteristics were employed. A unique
    multi-layered feed-forward unsupervised neural network called a convolutional
    neural network (CNN) was created to handle picture categorization. The feature
    extraction layer is another name for the convolution layer of a convolutional
    neural network (CNN). SECTION III. Existing System The CNN algorithm was used
    by the authors to distinguish between yellowish-green, unripe, medium, and ripe
    bananas. Before submitting them to training, image noise was removed using a bilateral
    filter, and for variants, data augmentation was used. In terms of accuracy (96.18%)
    and execution time, the recommended model surpassed NASNet Mobile. The authors
    experimented with various CNN hyper-parameters to sort ripe Medjool dates using
    CNN from scratch architectures, ResNet50, ResNet101, ResNet152, VGG16, VGG19,
    InceptionV3, etc. The effectiveness of CNN architectures in classifying the maturity
    of Medjool dates was rated in terms of accuracy and processing speed. The Adam
    optimizer’s 0.01 learning rate with 128 batch sizes gave the VGG19 model a maximum
    accuracy of 99.32%. SECTION IV. Proposed Model An example of a deep learning system
    is a CNN, which is made up of neurons and uses trainable weights and biases to
    classify incoming images. A CNN may comprise tens or even hundreds of layers,
    each of which may be trained to recognise certain aspects of an image. A CNN may
    automatically and adaptively learn spatial hierarchies of information by using
    convolutional layers. For example, pooling layers and completely linking layers
    are a few of the building blocks. The output of each training picture’s consolation
    is used as the input for the next layer after each image is convolved using a
    range of resolution filters. With the input picture size set to 112*112*3 for
    the supplemented dataset and 227*227*3 for the original dataset, this study uses
    three convolutional layers with two max-pooling layers. The loss function used
    is the cross-entropy function. while Adam is selected as the optimizer. so that
    weight and offset modifications may be made more steadily thanks to Adam’s method.
    To balance training and validation accuracy and loss, a 20% dropout was employed.
    The output layer uses the softmax activation function as a last step. Fig 1: CNN
    ARCHITECTURE Show All A. AlexNet The AlexNet architecture is divided into eight
    layers, three of which are completely connected and five of which are convolutional.
    The convolution layer, which is the foundational component of the network, constitutes
    the top layer. The first layer of AlexNet’s convolution window is 11 by 11. Objects
    in ImageNet data frequently occupy pixels that are times wider and higher because
    ImageNet photographs are eight times larger than MNIST photographs.and include
    more visual information. We need a larger convolution window to catch the item.
    Electronics 4100, 2022 The convolution window shapes of 8 of the 13 are altered
    to 5*5 and then 3*3. Fig 2: ALEXNET CLASSIFICATION Show All The final convolutional
    layer is immediately followed by two gigantic, fully connected layers with a total
    of 4096 outputs. The input picture, which has pixel values of 227 for width and
    227 for height, is transferred to the input layer along with further 3D colours
    that are RGB-saved images. The concealed layers are then transmitted with the
    image, where it is processed before moving on to the fully connected layers, which
    include convolutional layers of different filters, and lastly to the layer of
    output. The first complicated layer is reached after the input, and the picture
    is transmitted. is reduced in size, depending on whether padding is there or not.
    The pooling layer’s input image will be 55*55*96 in size since the first layer’s
    filter size is 96. The two formulas stated above are used to determine the values
    for each input size. When using the supplemented dataset, the picture size is
    decreased to 112 by 112 pixels since training a model with a large dataset consumes
    more RAM than is available. As a result, by scaling down the image to “112,\"
    the AlexNet model is trained, verified, and tested. Fig 3: PROPOSED FRAMEWORK
    Show All B. Preprocessing To extract the image’s features, we used three different
    learning algorithms. Preprocessing is followed by image resizing and rgb to grey
    conversion, and three learning algorithms are available: preprocessing followed
    by SVM, bag of features, and custom-trained convolutional neural networks using
    transfer learning. C. Segmentation A method for segmenting a collection of data
    into a preset number of groups is called “K-mean segmentation clustering.\" The
    most widely used method is k, which stands for clustering.It divides a group of
    items using the K-means clustering algorithm. It creates k separate clusters from
    a given amount of data. The K-means algorithm consists of two sections. After
    computing the k centroid in the first phase, each data point is assigned to the
    cluster with the centroid that is closest to it in the second phase data into
    a group of k numbers of data. D. Hybrid Method CNN with SVM Hybrid fruit picture
    categorization using SVM (support vector machines). A hybrid CNN-SVM model is
    recommended for the Maraval dataset’s ship categorization. The recommended method
    offers the best of both worlds by combining SVM and CNN classifiers. Convolutional
    neural networks (CNNs), which are used for supervised learning, consist of multiple
    completely connected layers.. CNN is able to learn invariant local properties
    and functions similarly to humans. It can extract the most discriminating information
    from unprocessed ship photos. The suggested method extracts the most recognisable
    characteristics from the raw input photos using a 5x5 kernel/filter. The mm filter
    in the convolutional layer is convolved with the cnn input neurons in the input
    layer. E. Tensorflow Open source software for numerical computations is called
    Tensor flow. It was initially intended to be used for machine learning and deep
    neural network research. Users who wish to employ neural networks in various scenarios
    may find neural network topologies in Tensor Flow along with retraining scripts.
    Fig 4: HYBRID FRUIT IMAGES Show All F. Keras To prepare, model, evaluate, and
    optimise neural networks, one uses the open-source Keras Python-based neural network
    library. It has the ability to run on top of TensorFlow. Given that the backend
    is responsible for high-level APIs utilise it for management.. It is designed
    for both the training process with a fit function and the development of a model.
    It is intended for low-level computing using tensors or TensorFlow and backend
    convolution. Preprocessing, modelling, optimization, testing and presentation
    python libraries imported. SECTION V. Experiment Results The deep learning framework
    PyTorch has an inbuilt tensor data format (a multidimensional tensor). In this
    paper, MobileNet was applied to the Fruit Dataset to determine how well the network
    performs at classification. The Fruits dataset was used to generate these 1260
    images, which are divided into 7 categories: 15% of these images are used to test
    the model, and 85% are used for training and overall 98.74%. The network is trained
    using a 14-batch size across 10 epochs. The results demonstrate that the suggested
    paradigm operates well. when compared to established models and show promise for
    use in practical situations. This type of increased precision and accuracy will
    help increase the machine’s overall fruit recognition effectiveness. Fig 5: UPLOADING
    IMAGES Show All Fig 6: PREDICTION Show All SECTION VI. Conclusion The fruit may
    be identified by properties like form, colour, and texture in the suggested project.
    This expands people’s understanding of certain uncommon and unusual fruits. The
    project’s major focus is on minimising human effort and simplifying human existence.
    Fruit identification might decrease the continuous issues that are present. It
    reduces misunderstandings about the particular fruit. Future work that might be
    done on this project includes the development of a web application. This software
    is available to users 24/7, from any location. Authors Figures References Citations
    Keywords Metrics More Like This Technical analysis of crop production prediction
    using Machine Learning and Deep Learning Algorithms 2022 International Conference
    on Innovative Computing, Intelligent Communication and Smart Electrical Systems
    (ICSES) Published: 2022 A Deep Learning Approach for Yield Estimation and Phenotype
    Analysis in Rice Crops 2021 International Conference on Advancements in Electrical,
    Electronics, Communication, Computing and Automation (ICAECA) Published: 2021
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Gurunathan et al., 2023)
  journal: Proceedings of the 9th International Conference on Biosignals, Images,
    and Instrumentation, ICBSII 2023
  key_findings: The proposed hybrid CNN-SVM model achieved high accuracy in fruit
    recognition, demonstrating the potential of deep learning for automated fruit
    identification tasks.
  limitations: The paper focuses primarily on fruit recognition and does not explicitly
    address the integration of these technologies with automated irrigation systems.
  main_objective: To develop a hybrid deep learning model for accurate fruit recognition
    using image processing techniques.
  relevance_evaluation: The paper is highly relevant to the specific point of integrating
    high-resolution cameras and computer vision algorithms for visual monitoring of
    crop growth and disease detection in automated irrigation systems. It demonstrates
    the effectiveness of deep learning techniques in extracting discriminating features
    from images to automate fruit recognition and grading tasks.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Convolutional Neural Networks (CNNs), Support Vector Machines
    (SVMs), PyTorch, Tensorflow, Keras
  title: Classification of Cultivars Employing the Alexnet Technique Using Deep Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Chitra, R., Swetha, A., Vishwa, M., & Hari Haran, B. (2023). Enhancing
    Crop Health Monitoring and Disease Identification in Agriculture. 2023 Intelligent
    Computing and Control for Engineering and Business Systems (ICCEBS), 1-6. https://doi.org/10.1109/ICCEBS58601.2023.10448904
  authors:
  - Chitra R.
  - Swetha A.
  - Vishwa M.
  - Hari Haran B.
  citation_count: '0'
  data_sources: Field-acquired images of brinjal, tomato, and green chili plants
  description: Agriculture is the main source of food and fodder. It demands constant
    hard work, attention to detail, responsibility, flexibility, and time to produce
    good yields. The main problems faced by the farmers are many, for example, water
    supply to the crops, farming equipment, dependence on traditional crops, storage
    facilities, etc. The foremost and important issue faced by the farmers is maintaining
    crop health, pest management and weed removal. The main focus is to manage crops
    of the same type like tomato, brinjal and green chili. The pest found in these
    types of plants is also similar. The input images captured from agricultural fields
    are subjected to image processing techniques to detect the presence of pests.
    Once pests are detected, the system classifies the pests into their respective
    types. The accuracy of the pest detection algorithm is 96.44%. The software's
    efficiency and accuracy is a viable tool for farmers and researchers for detection
    and mitigation of crop damage. The images acquired from the fields are processed
    to detect the presence of diseases in the leaves. Once a disease is detected,
    the software categorises the specific type of disease based on a diverse dataset
    of tomato leaf diseases. The accuracy of the disease detection algorithm is 80.67%.
    The system enables farmers to detect and identify plant diseases and respond to
    that promptly, preventing crop loss.
  doi: 10.1109/ICCEBS58601.2023.10448904
  explanation: The study presented a comprehensive methodology for detecting pests
    and diseases in brinjal, tomato, and green chili plants utilizing image processing
    techniques. By implementing Convolutional Neural Networks (CNN) and employing
    pre-trained data, the system demonstrated high accuracy in identifying pests with
    a rate of 96.44% and leaf diseases with an accuracy of 80.67%. This innovative
    approach empowers farmers and researchers with a powerful tool for early detection
    and effective management of pests and diseases, contributing to enhanced crop
    health and increased agricultural productivity.
  extract_1: '"The accuracy of the pest detection algorithm is 96.44%. The software''s
    efficiency and accuracy is a viable tool for farmers and researchers for detection
    and mitigation of crop damage. The images acquired from the fields are processed
    to detect the presence of diseases in the leaves. Once a disease is detected,
    the software categorises the specific type of disease based on a diverse dataset
    of tomato leaf diseases. The accuracy of the disease detection algorithm is 80.67%.
    The system enables farmers to detect and identify plant diseases and respond to
    that promptly, preventing crop loss."'
  extract_2: '"As the backbone of the world economy, agriculture is crucial to supporting
    populations, creating jobs, and maintaining economic stability. As computer vision
    and image processing technology has advanced quickly, creative solutions have
    arisen to deal with critical issues that farmers around the world are currently
    facing. The most important of these difficulties is the prompt identification
    and control of pests and diseases in crop plants, which is essential for maintaining
    crop health and raising agricultural production."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 Intelligent Computing an...
    Enhancing Crop Health Monitoring and Disease Identification in Agriculture Publisher:
    IEEE Cite This PDF R. Chitra; Swetha A; Vishwa M; Hari Haran B All Authors 4 Full
    Text Views Abstract Document Sections I. Introduction II. Literature Review III.
    Methodology IV. Results and Discussion Authors Figures References Keywords Metrics
    Abstract: Agriculture is the main source of food and fodder. It demands constant
    hard work, attention to detail, responsibility, flexibility, and time to produce
    good yields. The main problems faced by the farmers are many, for example, water
    supply to the crops, farming equipment, dependence on traditional crops, storage
    facilities, etc. The foremost and important issue faced by the farmers is maintaining
    crop health, pest management and weed removal. The main focus is to manage crops
    of the same type like tomato, brinjal and green chili. The pest found in these
    types of plants is also similar. The input images captured from agricultural fields
    are subjected to image processing techniques to detect the presence of pests.
    Once pests are detected, the system classifies the pests into their respective
    types. The accuracy of the pest detection algorithm is 96.44%. The software''s
    efficiency and accuracy is a viable tool for farmers and researchers for detection
    and mitigation of crop damage. The images acquired from the fields are processed
    to detect the presence of diseases in the leaves. Once a disease is detected,
    the software categorises the specific type of disease based on a diverse dataset
    of tomato leaf diseases. The accuracy of the disease detection algorithm is 80.67%.
    The system enables farmers to detect and identify plant diseases and respond to
    that promptly, preventing crop loss. Published in: 2023 Intelligent Computing
    and Control for Engineering and Business Systems (ICCEBS) Date of Conference:
    14-15 December 2023 Date Added to IEEE Xplore: 19 March 2024 ISBN Information:
    DOI: 10.1109/ICCEBS58601.2023.10448904 Publisher: IEEE Conference Location: Chennai,
    India SECTION I. Introduction Agriculture is a primary source of livelihood in
    our country. Agriculture greatly impacts the health of the economy and the nation''s
    food security. Agriculture can help reduce poverty, raise incomes and improve
    food security for 80% of the world''s poor, who live in rural areas and work mainly
    in farming. Agriculture is not an easy occupation as the challenges in it grow
    day by day. Some of the challenges being faced are crop yields, agricultural productivity,
    management of crops, time management, pest and weed management. Most of the growing
    and grown crops are subjected to the presence of pests. The pests damage the crops
    and reducecrop productivity and improper detection of presence of crops leads
    to unessential use of harmful pesticides. Hence, the detection and termination
    of pests are mandatory[1]. Among many crops, tomato stands to be a fundamental
    ingredient in diets worldwide which makes its protection and production a top
    priority for farmers. The detection and classification of the type of disease
    aids farmers to protect their crops and optimize yields. It focuses on, A. Detection
    of Identification of Pests The images acquired from agricultural fields are subjected
    to image processing algorithms and the presence and classification of pests within
    the crops are accomplished by the system [2] B. Detection and Identification of
    Leaf Diseases Advanced image processing algorithms are applied on the images to
    improve the image quality and to detect the presence of diseases within the leaves.
    Subsequently, the detected diseases are classified into the specific type of disease
    precisely.[3] SECTION II. Literature Review Harshita Nagar et al. (2021) introduced
    a method for automatic pest detection utilizing Wavelet transformation and Oriented
    FAST and Rotated BRIEF (ORB). The proposed approach is demonstrated on images
    of fluffy caterpillar pests on mustard crop and fava bean crop farms in Rajasthan.
    The Region of Interest is extracted using wavelet transformation and image fusion
    technique.[4] Harshita Nagar et al. (2020) presents the various image processing
    techniques such as feature extraction and automatic detection for the image. The
    survey shows the efficient and simple existing methodologies. Several techniques
    are illustrated here to obtain the knowledge of different background modelling
    for pest detection such as image filtering, median filtering for noise removal,
    image extraction and detection through scanning. It also depicts some promising
    results to present enhanced methods and tools for creating fully automated pest
    identification including the extraction with detection.[5] M. I. Pavel et al.(2019)
    implemented Image processing techniques to detect and classify the affected plant
    disease. In the process, the work is divided into four portions which are image
    acquisition and preprocessing, segmentation of affected regions, feature extraction,
    and classification using a multi-class support vector machine algorithm.[6] Yogesh
    H. Bhosale et al. (2023) addresses the issues on agriculture sector in India,
    facing significant challenges due to pests, diseases, and parasites by introducing
    precision agriculture techniques such as deep machine learning (DML) which is
    used to identify plant leaf diseases and enhance agricultural output. Advantages
    of using DML include precise disease identification, enhanced operational efficiency
    and productivity, as well as the management of data dependencies and variabilities.
    Disadvantages include data dependency and variability.[7] SECTION III. Methodology
    As the backbone of the world economy, agriculture is crucial to supporting populations,
    creating jobs, and maintaining economic stability. As computer vision and image
    processing technology has advanced quickly, creative solutions have arisen to
    deal with critical issues that farmers around the world are currently facing.
    The most important of these difficulties is the prompt identification and control
    of pests and diseases in crop plants, which is essential for maintaining crop
    health and raising agricultural production. In order to do this, the focus of
    our study is on combining image processing methods for pest and leaf disease identification.
    They actively safeguard their crops. The selection of the three crops—Brinjal,
    Tomato, and Green Chillies is deliberate and based on numerous factors. These
    crops are of enormous economic importance because they are staple foods on a worldwide
    scale and the foundation of many agricultural economies. They also show increased
    susceptibility to a variety of pests and diseases, such as bacterial spot, early
    blight, and late blight, as well as armyworms, beetles, bollworms, and grasshoppers.
    As a result, there are issues with food security and yield reductions. In addition,
    data and research materials on brinjal, tomatoes, and green chilies are easily
    accessible, validating image processing models for pest and disease detection.
    Plant pests and pathogens interfere with plant growth and cause damage to cultivated
    and naturally growing plants, significantly reducing crop production. The primary
    method currently employed for pest and disease reduction is pesticide spraying.
    However, the practice has direct and indirect health implications for humans.
    Early-stage pest detection techniques can substantially reduce the need for pesticide
    application. Image detection techniques have emerged as effective tools for combating
    infestations and improving crop management, offering maximum protection to crops
    while minimizing human errors and efforts. The techniques enable automatic monitoring
    over large fields and are particularly focused on detecting leaf diseases during
    the growth and health monitoring phases of crop cultivation. A. Convolutional
    Neural Networks (CNN) An efficient approach involves the implementation of CNN
    to enhance the effectiveness of classification, specifically in the context of
    pest and leaf disease detection applications. The method encompasses fundamental
    steps, commencing with the provision of an image as input. Notably, many object
    detection problems entail training the model with image datasets that include
    bounding box annotations, achieved through techniques such as pooling. Within
    the segmented bounded region, images are cropped and supplied to the classifier
    for predictive analysis. The system effectively identifies and displays the pest
    image output in a dedicated window. Fig 2 provides a visual representation of
    the CNN process. B. Preprocessing Using Keras The Keras ‘ImageDataGenerator’ is
    frequently used to manage data loading and preprocessing chores in image processing.
    Images must be resized to fit the input size, pixel values must be normalised,
    and data augmentation for training may be used. Data batching is necessary for
    effective training. C. Process Involved in CNN The entire process is divided into
    five major phases. The phases are described in following sections whose block
    diagram is given fig 3. 1. Data Collection The dataset used is an accumulation
    of photos of crops containing pests which are common for brinjal, tomato and green
    chillies and disease affected in brinjal leaves in the agricultural fields. The
    focused pests are armyworm, beetle, bollworm, grasshopper, mites, mosquito, sawfly,
    stem borer which is given in Fig 1 and the focused diseases are tomato bacterial
    spot, tomato early blight, tomato late blight, tomato leaf mold, tomato septoria
    leaf spot, tomato spider mites two spotted spider mite, tomato target spot, tomato
    yellow leaf curl virus, tomato mosaic virus which is given in Fig 2 . Fig: 1 Pest
    dataset Show All Fig: 2 Leaf disease dataset Show All 2. Database Normalisation
    Database normalisation enables scalable and secure management of image-related
    data by reducing redundancy, improving data integrity, and optimising query performance.
    For accurate analysis and consistent findings, normalised databases fulfil the
    complex data requirements of pest and leaf disease detection. 3. Feature Extraction
    Images with cluttered backgrounds and little contrast between the pest/leaf and
    its surroundings are a common difficulty in the field of image processing for
    pest and disease identification. It becomes crucial to first extract pertinent
    regions of interest from the photos before using key point detection algorithms.
    The purpose of this preprocessing stage is to isolate and highlight the areas
    that have pests and damaged leaves, which will enhance the upcoming detection
    procedure. 4. Training In the training phase, the extracted features are stored
    in the form of a labeled dataset along with image names. 5. Testing The testing
    is done using the Dynamic Time Warping Algorithm (DTW). DTW is a time series alignment
    algorithm developed originally for speech recognition. It aims at aligning two
    sequences of feature vectors by warping the time axis iteratively until an optimal
    match (according to a suitable metrics) between the two sequences is found. The
    obtained feature descriptor is compared with feature descriptor stored in the
    database by using dynamic time warping algorithm and it is thus checked if the
    leaf has pest and disease on it or not. Fig: 3 Blocks of CNN Show All SECTION
    IV. Results and Discussion 1. A. Result The efficiency and the high accuracy of
    the pest detection model render it an invaluable tool for farmers and researchers.
    The system''s performance goes beyond mere convenience; it plays a pivotal role
    in addressing one of agriculture''s most persistent challenges – pest infestations.
    With an exceptional accuracy rate of 96.44%, our pest detection algorithm stands
    as a beacon of precision. This remarkable accuracy ensures the precise identification
    of pests, allowing farmers to act swiftly and effectively in mitigating potential
    crop damage caused by these harmful organisms. By providing farmers with early
    detection and precise categorization of pests, our system empowers them to proactively
    manage and protect their crops. This capability is particularly critical in modern
    agriculture, where timely interventions can mean the difference between a successful
    harvest and substantial crop loss. The robustness of our pest detection model
    transforms farmers into vigilant stewards of their fields, enabling them to employ
    targeted strategies for pest control. This not only minimizes crop damage but
    also reduces the need for excessive pesticide use, promoting sustainable and environmentally
    friendly farming practices. Furthermore, the system extends its capabilities to
    address another significant concern in agriculture – plant disease detection.
    Images captured from agricultural fields are subjected to thorough analysis to
    identify the presence of diseases in the leaves. Leveraging a diverse dataset
    of tomato leaf diseases, our disease detection algorithm achieves an impressive
    accuracy rate of 80.67%. This level of accuracy enables farmers to promptly detect
    and identify specific plant diseases, facilitating rapid responses to prevent
    crop loss. In essence, the software stands as a comprehensive solution for the
    multifaceted challenges faced by the agricultural community. It not only excels
    in pest detection but also lends a helping hand in the early diagnosis of plant
    diseases. By providing farmers with the tools to identify, categorize, and respond
    to these threats efficiently, our system contributes significantly to increased
    crop yields and the long-term sustainability of agriculture. It exemplifies how
    cutting-edge technology can be harnessed to address age-old challenges, making
    modern farming practices more efficient, precise, and environmentally responsible.
    Fig 4 to 6 shows the detection of pests and figure 6,7 shows the detection of
    leaf diseases in tomato plants. Fig: 4 Detection of bollworm Show All Fig: 5 Detection
    of stem borer Show All Fig: 6 Detection of mites Show All Fig: 7 Detection of
    tomato mosaic virus disease Show All Fig: 8 Detection of tomato target spot Show
    All The accuracy of the pest detection is 96.44% which indicates the effectiveness
    and accuracy of the software to correctly detect the presence of pests and classify
    them based on actual instances of pests in the test datasets. The X axis of the
    graph represents the independent variable, which is the number of iterations or
    training cycles of the algorithm over a period and the Y axis represents the dependent
    variable, which is the accuracy of the pest detection algorithm. Initially, the
    algorithm''s accuracy was quite low, measuring below 20%. This low accuracy might
    be due to several reasons, such as inadequate training data, ineffective feature
    extraction, or an insufficiently trained model. To improve the accuracy of the
    algorithm, multiple iterations of training and refinement were performed. As the
    algorithm underwent several training cycles and refined its internal representation
    of pest detection patterns, its accuracy steadily improved. This gradual increase
    in accuracy is likely reflected in the graph as an upward trend. The algorithm
    managed to achieve an impressive accuracy rate of 96.44%. The accuracy versus
    v-accuracy graph was generated by recording accuracy on the training set and v-accuracy
    on the validation set after each training epoch. This visualization illustrated
    training and validation performance The accuracy of the leaf disease detection
    is 80.67% which represents the ability of the software to precisely identify specific
    leaf diseases. The Y-axis typically shows accuracy values ranging from 0% (completely
    inaccurate) to 100% (perfect accuracy). The values in the upward trend indicates
    improved accuracy while the values in the lower trend represents lower accuracy.
    At the outset of the algorithm''s development, its accuracy was below 30%. This
    low accuracy is due to insufficient training data and inefficient feature extraction.
    As the algorithm underwent multiple iterations, the accuracy gradually started
    to increase contributing to an upward trend. After a series of iterations, the
    algorithm achieved an accuracy rate of 80.67%. This signifies that the algorithm
    has become proficient at detecting tomato leaf diseases, with the majority of
    test cases correctly classified. This graph reflects the algorithm''s potential
    for practical use in the field of plant disease management, with further opportunities
    for optimization and enhancement. Fig: 9 Accuracy graph of pest detection Show
    All Fig: 10 Accuracy graph of leaf disease detection Show All B. Future Enhancement
    An algorithm can be developed to monitor the health and the growth of crops [8],[9],
    in addition to plant disease detection and classification. Factors determining
    the growth and health of crops such as height of a plant, stem width, chlorophyll
    content by multispectral images, density of leaves can be extracted from the acquired
    images and stored on a periodic basis to determine the periodic growth of crops
    which could aid farmers in comprehending the results of their farming practices.
    Authors Figures References Keywords Metrics More Like This Smart Agriculture System
    for Plant Disease Detection and Irrigation Management Using Machine Learning and
    IoT 2023 5th International Conference on Sustainable Technologies for Industry
    5.0 (STI) Published: 2023 Deep Learning for Plant Disease Detection and Crop Yield
    Prediction based on NPP-WPF Analysis in Smart Agriculture 2023 7th International
    Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC) Published:
    2023 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Chitra et al., 2023)
  journal: 2023 Intelligent Computing and Control for Engineering and Business Systems,
    ICCEBS 2023
  key_findings: The proposed system achieved high accuracy in detecting pests (96.44%)
    and leaf diseases (80.67%) using CNN and image processing techniques. It provides
    a valuable tool for farmers and researchers to monitor crop health, identify pests
    and diseases, and implement targeted management strategies.
  limitations: The study focused primarily on detecting pests and leaf diseases in
    specific crops (brinjal, tomato, and green chili). Therefore, its generalizability
    to other crop types or broader agricultural contexts may require further investigation.
  main_objective: To develop and evaluate a methodology for pest and disease detection
    in agricultural crops using image processing techniques.
  relevance_evaluation: This study is highly relevant to the point of integrating
    high-resolution cameras and computer vision algorithms for visual monitoring of
    crop growth, disease detection, and irrigation system performance. It provides
    a practical implementation of these technologies in the context of pest and leaf
    disease identification, showcasing their potential for improving crop management
    practices.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Convolutional Neural Networks (CNN), Image Processing, Machine
    Learning
  title: Enhancing Crop Health Monitoring and Disease Identification in Agriculture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: null
  authors:
  - Abbasi R.
  - Martinez P.
  - Ahmad R.
  citation_count: '3'
  data_sources: null
  description: Crops grown on aquaponics farms are susceptible to various diseases
    or biotic stresses during their growth cycle, just like traditional agriculture.
    The early detection of diseases is crucial to witnessing the efficiency and progress
    of the aquaponics system. Aquaponics combines recirculating aquaculture and soilless
    hydroponics methods and promises to ensure food security, reduce water scarcity,
    and eliminate carbon footprint. For the large-scale implementation of this farming
    technique, a unified system is needed that can detect crop diseases and support
    researchers and farmers in identifying potential causes and treatments at early
    stages. This study proposes an automatic crop diagnostic system for detecting
    biotic stresses and managing diseases in four leafy green crops, lettuce, basil,
    spinach, and parsley, grown in an aquaponics facility. First, a dataset comprising
    2640 images is constructed. Then, a disease detection system is developed that
    works in three phases. The first phase is a crop classification system that identifies
    the type of crop. The second phase is a disease identification system that determines
    the crop's health status. The final phase is a disease detection system that localizes
    and detects the diseased and healthy spots in leaves and categorizes the disease.
    The proposed approach has shown promising results with accuracy in each of the
    three phases, reaching 95.83%, 94.13%, and 82.13%, respectively. The final disease
    detection system is then integrated with an ontology model through a cloud-based
    application. This ontology model contains domain knowledge related to crop pathology,
    particularly causes and treatments of different diseases of the studied leafy
    green crops, which can be automatically extracted upon disease detection allowing
    agricultural practitioners to take precautionary measures. The proposed application
    finds its significance as a decision support system that can automate aquaponics
    facility health monitoring and assist agricultural practitioners in decision-making
    processes regarding crop and disease management.
  doi: 10.1016/j.aiia.2023.09.001
  explanation: In the proposed system for the automated detection of crop diseases,
    high-resolution cameras will be used for visual monitoring of crop growth. The
    system is capable of detecting disease occurrence, providing real-time alerts,
    and initiating automated actions to contain the spread of the disease. The system
    also employs computer vision algorithms for disease detection and segmentation,
    offering a more accurate and efficient alternative to traditional visual monitoring
    methods.
  extract_1: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)
  extract_2: Automated disease detection and management system for leafy green crops
    grown in an aquaponics facility uses a deep learning-based object detection and
    segmentation to determine the presence and extent of disease in crops.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Related work 3. Research
    methodology 4. Experimental results and discussion 5. Conclusions and future prospects
    CRediT authorship contribution statement Declaration of Competing Interest Acknowledgments
    References Show full outline Cited by (3) Figures (9) Show 3 more figures Tables
    (8) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show all tables Artificial
    Intelligence in Agriculture Volume 10, December 2023, Pages 1-12 Crop diagnostic
    system: A robust disease detection and management system for leafy green crops
    grown in an aquaponics facility Author links open overlay panel R. Abbasi a, P.
    Martinez b, R. Ahmad a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.aiia.2023.09.001
    Get rights and content Under a Creative Commons license open access Highlights
    • Developed a crop disease diagnostic and management system for aquaponics facility.
    • Used a transfer learning method to initialize weights of ResNet-50 and YOLOv5s.
    • Achieved promising classification accuracy and mAP@0.5. • Integrated final disease
    detection system with ontology model to manage diseases. Abstract Crops grown
    on aquaponics farms are susceptible to various diseases or biotic stresses during
    their growth cycle, just like traditional agriculture. The early detection of
    diseases is crucial to witnessing the efficiency and progress of the aquaponics
    system. Aquaponics combines recirculating aquaculture and soilless hydroponics
    methods and promises to ensure food security, reduce water scarcity, and eliminate
    carbon footprint. For the large-scale implementation of this farming technique,
    a unified system is needed that can detect crop diseases and support researchers
    and farmers in identifying potential causes and treatments at early stages. This
    study proposes an automatic crop diagnostic system for detecting biotic stresses
    and managing diseases in four leafy green crops, lettuce, basil, spinach, and
    parsley, grown in an aquaponics facility. First, a dataset comprising 2640 images
    is constructed. Then, a disease detection system is developed that works in three
    phases. The first phase is a crop classification system that identifies the type
    of crop. The second phase is a disease identification system that determines the
    crop''s health status. The final phase is a disease detection system that localizes
    and detects the diseased and healthy spots in leaves and categorizes the disease.
    The proposed approach has shown promising results with accuracy in each of the
    three phases, reaching 95.83%, 94.13%, and 82.13%, respectively. The final disease
    detection system is then integrated with an ontology model through a cloud-based
    application. This ontology model contains domain knowledge related to crop pathology,
    particularly causes and treatments of different diseases of the studied leafy
    green crops, which can be automatically extracted upon disease detection allowing
    agricultural practitioners to take precautionary measures. The proposed application
    finds its significance as a decision support system that can automate aquaponics
    facility health monitoring and assist agricultural practitioners in decision-making
    processes regarding crop and disease management. Previous article in issue Next
    article in issue Keywords Computer visionDeep learningDisease detectionLeafy cropsAquaponicsDigital
    farming 1. Introduction An aquaponic system is the combination of two well-known
    technologies, namely recirculating aquaculture system (RAS) and a hydroponics
    system (soilless growing of plants) that work together in an integrated environment
    (Abbasi et al., 2021a). The rationale of this soilless growing system involves
    sharing the mutual benefit of the available resources, such as water and nutrients,
    between aquaculture and plant production. Fish eats food and excretes waste consisting
    of ammonia (NH3+) and other constituents, which are then converted by certain
    microbes to nitrates (NO3−). This enriched effluent is then pumped into the hydroponic
    component of the system, where the nutrients are readily available for uptake.
    Under this general idea, it can be implied that aquaponic is a green and sustainable
    food production system (Yanes et al., 2020). Despite all the advantages offered
    by this emerging and growing technology, a few challenges need special attention,
    particularly considering its large-scale implementation. Being a greenhouse and
    a symbiotic environment, the parameters and factors (light, temperature, pH, moisture,
    etc.) that need to be controlled are diverse (Abbasi et al., 2021b). For the system
    to be functional and efficient, a delicate equilibrium among these parameters
    must be established (Gillani et al., 2022). Optimal conditions must be met for
    the growth and development of all three varieties of organisms that are present
    in the system (fish, bacteria, and plants). Another significant challenge is related
    to crop diseases resulting from either nutrient deficiency or inadequate management
    of the system, impacting crop quality and causing crop wastage (Dhal et al., 2022;
    Stouvenakers et al., 2019). As Khirade and Patil pointed out, identifying crop
    diseases and applying disease management practices are key to preventing losses
    in the yield and quantity of agricultural products (Khirade and Patil, 2015).
    For this reason, early detection of disease outbreaks is crucial for the progress
    of aquaponics farms. Traditionally, crop diagnostic is performed by agricultural
    specialists who visually examine the plant leaves. This practice, however, is
    subjective, destructive, time-consuming, and labor-intensive (Dutot et al., 2013).
    Moreover, it also requires the experts to be proficient with extensive knowledge
    of various diseases, their symptoms, and treatments (Khan et al., 2022). Other
    methods include chemical analyses, leaf color chart (LCC) matching, soil plant
    analysis development (SPAD), hyperspectral imaging, and spectral remote sensing,
    which again are either time-consuming or costly or destructive techniques (Weaver
    et al., 2020). To address these problems, different automatic crop disease detection
    systems based on artificial intelligence (AI) techniques such as machine learning
    and deep learning are developed as they offer contactless, rapid, environmental-friendly,
    and accurate methods for performing a non-invasive evaluation of crops'' health
    and quality (Bedi and Gole, 2021; Singh et al., 2020). Deep learning techniques
    offer two significant advantages over machine learning techniques. First, the
    feature extraction process is automatic, and second, the time to process large
    datasets of high dimensions is significantly reduced (Bedi and Gole, 2021). In
    addition to disease detection, it is also paramount that farm practitioners and
    researchers have access to relevant information about crop management strategies
    that allow them to pick up methods and treatments appropriately to prevent diseases,
    thereby gaining both economic and environmental benefits (Barosa et al., 2019).
    In most cases, such information is dispersed throughout multiple heterogeneous
    data sources — posing a need for a unified model that contains knowledge about
    the causes and treatments of different crop diseases. Semantic technologies such
    as ontologies have proven effective for data integration in multiple domains (Rodríguez-García
    et al., 2021). An ontology is a formal and explicit specification of a shared
    conceptualization (Studer et al., 1998). The logical formalisms behind ontological
    models allow autonomous agents to interpret the information that is being processed
    (Horrocks et al., 2005). Ontology can be used to construct a knowledge base containing
    relevant information about causes and suggested treatments of crop diseases, which
    can be extracted upon disease detection (Rodríguez-García et al., 2021). With
    this information, farm practitioners are able to get clear guidelines to effectively
    perform crop monitoring and disease management. In this study, an automatic system
    based on deep learning techniques is presented for the detection and classification
    of diseases in four leafy green crops, lettuce, basil, parsley, and spinach, grown
    in an aquaponics facility. Taking advantage of semantic technologies, an ontology
    model, ‘AquaONT’ is developed by authors in previous work (Abbasi et al., 2021b)
    that contains knowledge about causes and treatments of different diseases. This
    ontology model is integrated with a disease detection system through an interface
    established on a cloud-based application. The remainder of the paper is structured
    as follows: Section 2 summarizes the most recent literature related to crop disease
    detection systems, Section 3 presents the methodology used to design the proposed
    system, Section 4 discusses the experimental results and findings, and finally,
    Section 6 concludes the paper and presents the future prospects. 2. Related work
    The rapid developments in AI have made a major breakthrough in deep learning (DL)
    and computer vision (CV) technologies by solving complex problems like image classification,
    object detection, speech recognition, voice recognition, natural language processing,
    and medical imaging, among others (Abbasi et al., 2022a; Subeesh and Mehta, 2021).
    In particular, convolutional neural networks (CNNs) have proved their efficiency
    in various sectors such as automotive, healthcare, or retail, and are also being
    integrated in agriculture for automatic crop disease detection — presenting a
    reasonable alternative to traditional practices (Pathan et al., 2020). In recent
    years, several models and applications have been developed for crop disease identification
    and diagnosis. This section investigates some latest works present in the literature.
    Anami et al. designed a deep convolutional neural network (DCNN) based framework
    for automatic recognition and classification of various biotic and abiotic paddy
    crop stresses using the pre-trained visual geometry group model, VGG-16 (Anami
    et al., 2020). The field images are used in the proposed approach captured during
    the booting growth stage. Bedi and Gole proposed a hybrid model based on a convolutional
    autoencoder (CAE) network and CNN for automatic bacterial spot disease detection
    present in peach plants using their leaf images from a publicly available dataset
    named ‘PlantVillage’ (Bedi and Gole, 2021). Paymode and Malode developed a CNN-based
    method using pre-trained VGG-16 for detecting healthy, unhealthy, and diseased
    leaves in tomato and grape plants (Paymode and Malode, 2022). Fuentes et al. combined
    ResNet with Faster R-CNN, R-FCN, and SSD. They proposed a method to detect the
    diseases and insect pests of tomato plants, achieving the effective identification
    of nine different types of diseases and insect pests (Fuentes et al., 2017). Chen
    et al. proposed a method to detect rice plant diseases using the DenseNet model
    of deep transfer learning (Chen et al., 2020). To identify the cucumber disease
    spots in greenhouses, Ma et al. developed a CNN-based system, combining a compound
    color feature with a region-growing algorithm (Ma et al., 2018). A disease recognition
    algorithm based on VGGNet and InceptionV3 with reduced model size and improved
    recognition accuracy is proposed by Rahman et al. for rice plants (Rahman et al.,
    2020). Oppenheim et al. proposed a disease classification algorithm based on an
    improved VGG network for accurate and quick identification and classification
    of spots on potato crops (Oppenheim et al., 2019). A method based on an improved
    CNN is proposed by Fan et al. to identify nine kinds of common corn diseases from
    images with a complex background (Fan et al., 2021). Khan et al. proposed an apple
    disease detection system that works in two stages (Khan et al., 2022). Based on
    the Xception model, the first stage classifies whether the leaf is healthy or
    diseased, and the second stage, based on Faster-RCNN, performs disease detection.
    Qi et al. developed a disease recognition system based on an improved YOLOv5 (squeeze-and-excitation
    (SE) module is added) model to identify the tomato virus diseases in the greenhouse
    (Qi et al., 2022). Nandhini et al. proposed a deep learning model that combines
    RNN and CNN for disease classification and early prediction in the Plantain tree
    (Nandhini et al., 2022). Abbas et al. proposed a deep learning-based method for
    tomato disease detection that utilizes the Conditional Generative Adversarial
    Network (C-GAN) to generate synthetic images of tomato plant leaves (Abbas et
    al., 2021). A DenseNet121 model was then trained on synthetic and real images
    using transfer learning to classify the tomato leaves images into ten categories
    of diseases. An efficient detection model (EFDet) consisting of an efficient backbone
    network, a feature fusion module, and a predictor is proposed for the detection
    of cucumber leaf diseases in complex backgrounds by Liu et al. (Liu et al., 2021).
    Likewise, a YOLOv5-based disease detection model to detect bacterial spot disease
    in bell pepper plant from the symptoms seen on the leaves (Mathew and Mahesh,
    2022). A framework is proposed for an aquaponics system based on image processing
    and decision tree methodology that performs disease detection of four leaf species,
    eggplant, chilli, citrus, and mandarin, and automatically generates a report which
    is sent to the owner through the mobile application if the disease is detected
    (Barosa et al., 2019). Likewise, a CNN-based approach for detecting plant disease
    in smart hydroponics provides a tool to the farmers capable of doing the task
    of an agricultural extension worker with even better accuracy (Musa et al., 2021).
    An application based on image processing and SVM is developed to classify apple
    diseases (Lisha Kamala and Anna Alex, 2021). Yudha et al. proposed a model based
    on Faster R-CNN with Inception V2 algorithm to recognize the diseases in hydroponic
    lettuce (Yudha Pratama et al., 2020). The literature survey has revealed that
    researchers have extensively used deep learning techniques for plant or crop disease
    detection and classification. The analysis shows that most disease detection systems
    are developed for open-air farms. Only a few systems are developed for modern
    farming systems, such as aquaponics or hydroponics. Most models are developed
    considering multiple diseases of only one crop. Moreover, to the best of the authors''
    knowledge, no comprehensive and unified disease detection system is proposed for
    identifying diseases of multiple leafy green crops grown in aquaponics facilities.
    Disease detection in leafy green presents various challenges. For instance, there
    exists a strong resemblance among the foliage of different leafy green crops that
    might impact the performance of the detection system. Secondly, due to differences
    in light illumination during imaging, the visual symptoms of different diseases
    may appear similar. Another challenge is the availability of a dataset of leafy
    green crops that can be used for disease detection. Deep learning models require
    a huge amount of data for training, and to the best of the author''s knowledge,
    there is no sufficient sized large-scale open-source dataset available that can
    be utilized for this research. There are a few datasets, such as PlantVillage,
    PlantDoc, and CropDeep (Noyan, 2022; Singh et al., 2019; Zheng et al., 2019).
    PlantDoc and PlantVillage are open-source datasets with no categories of leafy
    green crops. CropDeep dataset contains images of some of the leafy green, but
    it is not open source. Lastly, none of the aforementioned models provides information
    related to the causes and treatments of detected diseases. Apart from AI techniques,
    ontology-based systems are also developed over the years for plant disease diagnosis
    and treatment recommendations. Jearanaiwongkul et al. developed an ontology-based
    expert system called ‘RiceMan’ for disease identification and control recommendation
    in rice crops (Jearanaiwongkul et al., 2021). Likewise, Rodríguez-García et al.
    proposed a decision support system based on an ontology model for crop pests and
    diseases recognition (Rodríguez-García et al., 2021). It also provides information
    on agriculture practices and permitted pest control measures. In these systems,
    users are required to select crop and observed symptoms from the list for further
    processing, which is a time-consuming process. Whereas, in deep learning models,
    this information can be obtained by using crop images. Deep learning techniques
    can be combined with ontology models to develop efficient decision support systems
    for disease management in crops. The idea of combining the two techniques is relatively
    new in the agriculture sector, and hence, limited work is done in this regard
    that primarily focuses on enabling smart services (monitoring and control) in
    IoT-based farming systems or detection of cyber-attacks (Abbasi et al., 2021b).
    Considering the research gaps and potential opportunities, this study aims to
    create a dataset consisting of high-quality RGB images (healthy and diseased)
    of four leafy green crops: little gem romaine lettuce, spinach, parsley, and basil.
    This study also aims to develop a crop diagnostic system based on deep learning
    models and ontology models for detecting diseases and identifying causes and potential
    treatments in stated crops, respectively. 3. Research methodology The block diagram
    illustrating the three sequential modules of the research methodology is shown
    in Fig. 1. First module involves the preparation of the dataset and training of
    classification and object detection models. The disease detection model works
    in three phases. The first and second phase uses lightweight classification models
    to classify the type of crop and identify whether the classified crop has a disease
    or not, respectively. Phase 3 is the detection stage that uses an object detection
    model to detect and localize the diseased and non-diseased spots in the crops.
    The third phase also tells the class of the diseased spots. The purpose behind
    adding two classification phases before the detection phase is three-fold. First,
    to improve the detection performance by reducing the number of wrong detections
    which could arise as the model has to identify and localize different disease
    spots of varying sizes. Second, to determine the characteristics of the crop identified
    in the first phase in relation to aquaponics'' system design by linking it with
    the knowledge model. Lastly, to reduce the overall processing time by filtering
    out invalid inputs in the second phase. The second module aims to extract the
    instances of relevant classes such as potential causes and treatments of detected
    diseases from the ontology model ‘AquaONT’ developed by authors in previous work
    (Abbasi et al., 2021b). In the third module, a cloud-based application is developed
    using Streamlit1, where a pre-trained disease detection model and ontology model
    are deployed to obtain a complete crop diagnostic system. Upon identification
    of the crop in phase 1, its characteristics in relation to optimal environmental
    (pH, temperature, illumination, etc.), growth (width, height, area, etc.), and
    grow bed design (plant site spacing) parameters for an aquaponics facility are
    extracted from ontology model using OWLready22 (ontology-oriented programming
    package in Python). The authors have conducted a study that identified design
    parameters as vital knowledge in ensuring high crop yields and product quality
    in an aquaponics facility (Abbasi et al., 2021a). Likewise, once the disease and
    its type are detected in phase 3, the potential causes and recommended treatments
    are extracted from the ontology model. Each element of each module is presented
    in detail in the following subsections. Download : Download high-res image (404KB)
    Download : Download full-size image Fig. 1. Proposed methodology for disease detection
    and control recommendation system. 3.1. Dataset preparation The dataset preparation
    involves three steps, i) data acquisition, ii) data annotation, and iii) data
    augmentation, which are detailed below. 3.1.1. Data acquisition This study considers
    four leafy green crops, lettuce, basil, parsley, and spinach. The dataset consists
    of healthy and diseased images of these crops, which are acquired from different
    sources such as NFT based aquaponics facility built in AllFactory 4.0 Lab (University
    of Alberta, Canada), Google search engine, and Ecosia3 (a search engine based
    in Berlin, Germany). The diseases considered for the four crops while developing
    the dataset are listed below. • Lettuce: Bacterial leaf spot and Downy mildew
    • Basil: Downy mildew • Parsley: Septoria leaf spot • Spinach: Downy mildew and
    Stemphylium leaf spot To enhance the flexibility of the model to correctly classify
    and detect disease, it is ensured that images have non-homogeneous backgrounds,
    different illumination conditions, and disease maturity stages (Jha et al., 2019).
    A total of 2000 images are gathered from all the resources. Among these images,
    800 images showed healthy crops (200 images per crop), and 1200 images showed
    the diseases mentioned above (240 images per disease). Fig. 2 shows some of the
    sample images from the dataset. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 2. Samples from leafy green image dataset. (For
    interpretation of the references to color in this figure legend, the reader is
    referred to the web version of this article.) 3.1.2. Data annotation Data annotation
    is one of the vital steps for the successful development of object detection models.
    The process is manual and involves labeling the desired objects in an image with
    a label or tag that refers to a particular class. The labeled data is used during
    the training of the model. There are various open-source annotation tools, but
    in this study, LabelImg4 is used. LabelImg is a python based graphical annotation
    tool that supports a variety of deep learning algorithms (Qi et al., 2022). In
    this study, the annotations are generated in COCO JSON and YOLO Darknet TXT formats
    because in the disease detection phase, two object detection models are tested
    to design the final system. 3.1.3. Data augmentation Next, the data augmentation
    process is performed to supplement and enrich the dataset. This helps increase
    the model''s generalizability and overcome the problem of overfitting. Moreover,
    it also allows the model to learn as many relevant features as possible. This
    study uses Albumentations, a Python library, for fast and flexible image augmentations
    (Buslaev et al., 2020). The different augmentation techniques applied are flip,
    rotation, noise, blur, and brightness. Fig. 3 shows examples of different augmentation
    operations. After applying the data augmentation, the final dataset comprises
    of 2640 images with their annotations. The final distribution of the dataset is
    presented in Table 1. Download : Download high-res image (409KB) Download : Download
    full-size image Fig. 3. Example of different augmentation operations applied on
    original image. Table 1. Distribution of crop information in the used dataset
    among the studied crops. Crop Healthy Diseased Total Disease 1 Disease 2 Lettuce
    240 280 280 800 Basil 240 280 – 520 Spinach 240 280 280 800 Parsley 240 280 –
    520 3.2. Disease detection model development Object detection is a complex task,
    and disease detection of leafy green crops comes with its own set of challenges.
    To overcome these challenges, the detection process in this study is divided into
    three primary phases. Fig. 4 shows the detailed pipeline of the disease detection
    model. Download : Download high-res image (864KB) Download : Download full-size
    image Fig. 4. Detailed pipeline for the crop diagnostic process. The first phase
    of the proposed system uses a lightweight CNN architecture to classify input images
    into one of the four types of crops: lettuce, basil, parsley, and spinach. ResNet-50
    is used as the base model for the CNN architecture in this study and its last
    layer is replaced with one global average pooling layer, one dense layer (fully
    connected layer) of size 1024 and activation function ReLu, and one output layer
    that uses Softmax for classification task and making final predictions. ResNet050
    is used as it has a simple design, high accuracy, and is suitable for small datasets
    (He et al., 2015). The crop type identified in this stage saves to a folder and
    also acts as an input to the next phase. Phase 2 of the system also uses ResNet-50
    and classifies the input from phase 1 into one of the following eight classes.
    i) Lettuce-Healthy ii) Lettuce-Diseased iii) Basil-Heathy iv) Basil-Diseased v)
    Spinach-Healthy vi) Spinach-Diseased vii) Parsley -Healthy viii) Parsley-Diseased
    The architectural design of ReNet-50 used in phase 2 is kept similar as in phase
    1 except for the output layer which now has eight classes. If the input image
    classified into one of the ‘Diseased’ crop categories, it goes to phase 3. On
    the other hand, if any of the ‘Healthy’ crop categories are identified, the process
    ends, and the classified image does not go to the next phase for further processing.
    The third phase of the proposed system is disease detection, which involves classifying
    and localizing the diseased spots in an image and classifying them into one of
    the disease classes mentioned below. i. Lettuce-Bacterial leaf spot ii. Lettuce-Downy
    mildew iii. Basil-Downy mildew iv. Parsley-Septoria leaf spot v. Spinach-Downy
    mildew vi. Spinach-Stemphylium leaf spot Phase 3 activates only when the input
    from the previous phase is one of the ‘Diseased’ categories. To develop a disease
    detection model, an object detection algorithm is used. In the past recent years,
    advances in deep learning and computer vision have greatly accelerated the momentum
    of object detection (Khan et al., 2022). Numerous object detection algorithms
    (object detectors) are developed and used in the disease detection of crops. These
    detectors are broadly classified into two categories: i) two-stage detectors based
    on region proposal and ii) one-stage detectors based on regression or classification
    (Nguyen et al., 2020). The popular two-stage detectors are Fast-RCNN, Faster-RCNN,
    and Mask-RCNN, and one-stage detectors involve YOLO (You Only Look Once) family
    (Liu et al., 2021). Khan et al. conducted a research where they ran three different
    models, Faster-RCNN, YOLOv4, and EfficientDet, to solve a similar kind of problem
    for apple crops (Khan et al., 2022). It has been observed that Faster RCNN with
    mAP (mean average precision) of 42.1% outperformed YOLOv4 (mAP of 41.4%) and EfficientDet
    (mAP of 38%). As per these results, Faster-RCNN seems the right choice for this
    study. But YOLOv5 model developed by Ultralytics (Glenn, 2023) has substantially
    improved the detection speed while maintaining the detection accuracy. Therefore,
    both approaches are tested in this study. 3.3. Disease detection model training
    NVIDIA GeForce RTX 3090 is used to train all the models in three phases of the
    disease detection system. The classification model developed in stage 1 is implemented
    in PyTorch (an open source machine learning framework based on the torch library
    developed by Meta AI5). Using the transfer learning (TL) approach, ResNet-50 pre-trained
    on ImageNet is used (Russakovsky et al., 2015). The pre-trained model saves a
    lot of time as it is already trained on some dataset and hence contains the weights
    and biases of previous training that represent the features of the dataset it
    was trained on, which are often transferable to different datasets (Abbas et al.,
    2021). Hence, model parameters are initialized using the TL approach and then
    retrained on a custom dataset prepared in section 3.1.1 with a learning rate of
    0.0001, a batch size of 64, an input size of 224×224×3, and 100 epochs. The model
    was tuned using the Adam optimizer. For the classification model in phase 2, a
    batch size of 64 is used, and values of the remaining hyperparameters are kept
    the same. For training the object detection models, the dataset is split into
    75% for training, 20% for validation, and 5% for testing. The first model is implemented
    in Detectron2 that uses pre-trained architecture (trained on COCO dataset) ‘Faster-RCNN
    with ResNet-101 + FPN’. The model uses COCO JSON annotation format and is trained
    for 3000 iterations with the initial learning rate of 0.01 for the first 500 iterations
    and then 0.001 for the next 2500 iterations. The second model, YOLOv5s, is implemented
    in PyTorch. Again, a pre-trained version of the algorithm is used to enhance the
    training process and reduce time. For YOLOv5s, the annotation format is YOLO Darknet
    TXT but with the addition of a YAML file containing model configuration and class
    values. The model is trained for 3000 iterations. The hyperparameters and their
    values for the two models are shown in Table 2. Table 2. Values of hypermeters
    used for two objection detection methods. Hyperparameters Methods Faster-RCNN
    YOLOv5s Input size 600 × 600 416 × 416 Batch size 16 16 Learning rate 0.001 lr0
    = 0.01, lrf = 0.001 Momentum 0.89 0.937 Gamma value 0.1 fl_gamma = 0.0 Weight
    decay 0.0001 0.0005 Training time 1.5 h 50 min 3.4. Ontology model The complete
    development and details of all the concepts and instances of ontology model ‘AquaONT’
    developed by authors are available at (Abbasi et al., 2021b). AquaONT is a unified
    ontology model that represents and stores the essential knowledge of an aquaponics
    4.0 system. It consists of six concepts: Consumer Product, Ambient Environment,
    Contextual Data, Production System, Product Quality, and Production Facility.
    In this study, two classes, ‘Consumer Product’ and ‘Product Quality’ are used
    for knowledge extraction. The ‘Consumer Product’ class provides an abstract view
    of the type, growth status, and growth parameters of ready-to-harvest crops in
    an aquaponics system. Whereas the ‘Product Quality’ class provides knowledge on
    crop attributes related to pathology (crop diseases, causes, and the ways and
    means by which these can be managed or controlled) and morphology (canopy dimensions
    such as area, length, width, etc.). Four crops: lettuce, basil, parsley, and spinach,
    are considered in this study. Their growth conditions and morphological and pathological
    attributes stored as instances of the respective classes are extracted once the
    crop and disease are classified. Fig. 5 shows the hierarchical architecture of
    the ‘Consumer Product’ and ‘Product Quality’ classes with their instances for
    the ‘Basil’ crop in Protégé6 (an open-source ontology editor and framework developed
    at Stanford University) environment. Download : Download high-res image (1MB)
    Download : Download full-size image Fig. 5. Hierarchical structure of ‘Consumer
    Product’ and ‘Product Quality’ classes and respective instances in relation to
    Basil Crop. 3.5. Cloud-based application The trained model of the crop disease
    detection system is then saved and deployed on a cloud-based application built
    on Streamlit. The ontology model ‘AquaONT’ is also deployed on application, and
    relevant classes are integrated with the final disease detection model through
    Owlready2 library. The layout of the application is shown in Fig. 6. It consists
    of two user inputs ‘Select Model’ and ‘Upload Image’. ‘Select Model’ provides
    an option to select the model as per requirement, which in this study are ‘Crop
    Classification’ referring to phase 1, ‘Disease or No Disease’ referring to phase
    2, and ‘Disease Type, causes and Treatments’ referring to phase 3 of the proposed
    disease detection system. After model selection, an image is uploaded which is
    used by all the models. Once the disease is detected and classified, the causes
    and treatments of the disease are extracted from the ontology model automatically
    and displayed on the application panel. This kind of information is useful as
    it will allow agricultural practitioners to determine the causes of diseases and
    take precautionary steps in the early stages to avoid crop wastage and economic
    loss. Download : Download high-res image (2MB) Download : Download full-size image
    Fig. 6. Layout of cloud-based application for disease detection. 4. Experimental
    results and discussion This section presents the results of experiments performed
    in the current research work. First, the performance evaluation of deep learning
    models in three phases of the disease detection system is discussed. Next, the
    trained and validated system is tested on new data. In the end, the significance
    of the complete system is presented. The performance of the classification model
    in phase 1 is evaluated using a validation dataset. For this phase, there are
    four classes to be classified, namely lettuce, basil, spinach, and parsley. The
    distribution of labeled images in the validation set for this model is shown in
    Table 3. Table 3. Dataset distribution of validation set for phase 1. Class (Health
    + Diseased) Number of images Lettuce 160 Basil 104 Spinach 160 Parsley 104 The
    performance of the model is presented in the form of a confusion matrix (CM) shown
    in Fig. 7. The overall accuracy, precision, recall, and F-measure are computed
    by using the respective formulae, following common metrics for the performance
    of deep learning models in the literature (Khan et al., 2022). The computed metrics
    are summarized in Table 4. Download : Download high-res image (114KB) Download
    : Download full-size image Fig. 7. Confusion matrix of classification results
    in phase 1. Table 4. Results of classification model in phase 1. Crop Accuracy
    Precision Recall F1-Score Lettuce 0.97 0.95 0.96 0.96 Basil 0.98 0.96 0.96 0.96
    Spinach 0.96 0.94 0.94 0.94 Parsley 0.99 1 0.98 0.99 Average – 96.25% 96% 96.25%
    Overall accuracy 95.83% The classification model in phase 1 has achieved an overall
    accuracy of 95.83%, average precision of 96.25%, average recall of 96%, and average
    F1-score of 96.25%. As noted in Table 4, the performance metrics of the ‘spinach’
    class are lower than the other classes. Most model confusion comes in between
    spinach, basil, and lettuce leaves, particularly during the initial stages of
    their growth cycle. Next, the performance of the classification model in phase
    2 is evaluated in a similar fashion. For phase 2, there are six classes that model
    classifies, which are mentioned in section 3.2. Table 5 shows the distribution
    of the validation set used for the model in phase 2. Table 5. Distribution of
    validation dataset for phase 2. Class Number of images Lettuce-Healthy (LH) 48
    Lettuce-Diseased (LD) 112 Basil-Healthy (BH) 48 Basil-Diseased (BD) 56 Spinach-Healthy
    (SH) 48 Spinach-Diseased (SD) 112 Parsley-Healthy (PH) 48 Parsley-Diseased (PD)
    56 The CM for this model is shown in Fig. 8 and performance metrics are summarized
    in Table 6. Download : Download high-res image (222KB) Download : Download full-size
    image Fig. 8. Confusion matrix of classification results in phase 2. Table 6.
    Performance metrics of classification model in phase 2. Class Accuracy Precision
    Recall F1-score LH 0.979 0.86 0.92 0.89 LD 0.981 0.96 0.95 0.95 BH 0.989 0.90
    0.98 0.94 BD 0.983 0.91 0.93 0.92 SH 0.981 0.91 0.88 0.89 SD 0.983 0.96 0.96 0.96
    PH 0.994 0.98 0.96 0.97 PD 0.992 0.98 0.96 0.97 Average – 94% 94% 93.6% Overall
    accuracy 94.13% The classification model in phase 2 has achieved an overall accuracy
    of 94.13%, average precision of 94%, average recall of 94%, and average F1-score
    of 93.6%. It can be observed from the CM in Fig. 8 that the model is also prone
    to confusion in distinguishing between some of the classes. For instance, six
    examples of LD (Lettuce-Diseased) are classified among LH (1), BD (1), SH (2),
    and SD (2). This might be due to a lack of clarity in identifying leaf patterns
    and diseased spots. Finally, the performance of selected models for the detection
    phase (phase 3) is evaluated using a validation dataset. For this phase, there
    are six different diseases that models have to detect in crop leaves. These six
    diseases and their distribution in the validation dataset are given in Table 7.
    Table 7. Distribution of validation dataset in phase 3. Class Number of images
    Lettuce-Bacterial Leaf Spot (LBS) 56 Lettuce-Downy Mildew (LDM) 56 Basil-Downy
    Mildew (BDM) 56 Parsley-Septoria Leaf Spot (PSS) 56 Spinach-Downy Mildew (SDM)
    56 Spinach- Stemphylium Leaf Spot (SSS) 56 In this phase, the metric that is used
    to evaluate and compare the performance of two models, i-e, Faster-RCNN, and YOLOv5s,
    is mean average precision (mAP). The mAP is the primary evaluation indicator used
    for the evaluation of object detection models (Khan et al., 2022). In particular,
    mAP@0.5 (mean value of mAP at IOU threshold = 0.5) is evaluated. The comparison
    of the two models against all the classes is presented in Table 8. It can be seen
    that YOLOv5s with mAP@0.5 of 82.13% have outperformed Faster R-CNN. The two models
    have achieved the best mAP score for Lettuce-Bacterial Leaf Spot (LBS), Parsley-Septoria
    Leaf Spot (PSS), and Spinach-Stemphylium Leaf Spot (SSS), whereas a low mAP score
    is observed for Lettuce-Downy Mildew (LDM), Basil-Downy Mildew (BDM), and Spinach-Downy
    Mildew (SDM). Downy Mildew initially causes light green to yellow angular spots
    on the upper surfaces of leaves and hence looks similar independently of the crop
    type. This causes confusion for the detector in distinguishing the crop-specific
    Downy Mildew. But with more data, this issue can easily be resolved. Later in
    the growth cycle, the plant tissue affected with Downy Mildew turns tan in spinach,
    purplish brown in basil, and light brown in lettuce, which are correctly identified
    by the detector. Table 8. Class-wise comparison of two detection models. Class
    mAP Faster-RCNN YOLOV5s Lettuce-Bacterial Leaf Spot (LBS) 77.32 83.86 Lettuce-Downy
    Mildew (LDM) 73.89 78.63 Basil-Downy Mildew (BDM) 75.47 80.11 Parsley-Septoria
    Leaf Spot (PSS) 78.63 84.55 Spinach-Downy Mildew (SDM) 74.19 79.87 Spinach-Stemphylium
    Leaf Spot (SSS) 79.52 85.74 mAP@0.5 76.34 82.13 The performance evaluations of
    models in three phases have shown that detection models are not as straightforward
    as classification models. This is because an image consists of many objects which
    belong to either the same class or different classes. Hence, three things must
    be verified during evaluation, including object class, bounding box (object location),
    and confidence. In the end, the two detection models are compared in terms of
    inference time which is an important metric that determines the detection speed.
    It is observed that one-stage detector i-e., YOLOv5s with a detection speed of
    52.8 FPS (frames per second) is faster than Faster-RCNN with a detection speed
    of 43.2 FPS. Moreover, it is also observed that YOLOv5s accurately detect objects
    of varying sizes with little to no overlapping boxes. All the comparisons between
    the two detection models show that YOLOv5s have a clear advantage in terms of
    accuracy and run speed. Therefore, in this study, YOLOv5s is used for developing
    the disease detection system. After training and validation, the final crop disease
    detection system with YOLOv5s is tested using the test set containing new images.
    The system has shown promising results by effectively classifying and detecting
    the diseases in specified crops, which shows the system''s robustness in terms
    of dealing with a variety of objects having different shapes, patterns, textures,
    and colors. Fig. 9 shows examples where the system has accurately classified the
    crop and detected the diseased and healthy spots in crop leaves. Images in the
    first row of Fig. 9 are the results from three phases of the disease detection
    system for the Lettuce crop, which is suffering from Bacterial Leaf Spot disease.
    Similarly, row 2 and row 3 are the results from three phases of the system showing
    Spinach and Parsley, respectively, and the diseases they are suffering from, such
    as Downy Mildew and Septoria Leaf Spot disease respectively. Download : Download
    high-res image (1MB) Download : Download full-size image Fig. 9. Results from
    proposed disease detection system. The final crop disease detection system is
    then deployed on a cloud-based application developed in section 3.5. Fig. 6 shows
    the layout of the application. The ontology model discussed in section 3.4 is
    also integrated with the final system to build a complete real-time crop diagnostic
    system. The images are acquired wirelessly from the aquaponics facility through
    an interface developed on the Google Cloud Platform by the authors in previous
    work (Abbasi et al., 2022b). The images are stored in a folder to be used by the
    crop diagnostic system. Once the crop type and its disease are identified, the
    causes and treatments are automatically extracted from the ontology model and
    displayed on the application panel. For instance, Fig. 6 shows an example of working
    crop diagnostic system for parsley crops. The disease detected by the system after
    image uploading is Septoria Leaf Spot. The crop diagnostic system extracts the
    knowledge about potential causes and general treatments of this disease from AquaONT.
    The primary causes of Septoria Leaf Spot in Parsley could be high humidity level,
    infected seeds, leaf wetness, etc. This disease could also be caused due to irregular
    variations in air temperature. The potential preventive measures and treatments
    suggested by the system for this disease include: maintaining optimal humidity
    and temperature levels in accordance with Parsley crop and indoor aquaponics environment
    throughout the growth cycle, treating seeds before germination with hot water
    or Clorox bleach, using conventional fungicides if the disease is spread out in
    multiple plants. Downy Mildew disease is one of the most common diseases observed
    in different crops (McGrath, 2021). In the greenhouse or indoor farming environment,
    the potential causes of this disease are the same irrespective of crop type, which
    includes: high humidity, cool temperatures, infected seeds, and leaf wetness (Margaret
    Tuttle McGrath, 2021). Therefore, the methods to treat Downy Mildew in lettuce,
    basil, and spinach are also similar. This means that the classification of Downy
    Mildew disease with respect to crop type does not impact the results related to
    disease treatments. Despite this independence, it is still significant to perform
    the classification of Downy Mildew for each crop individually as its symptoms
    for three crops, lettuce, basil, and parsley, change later in the growth cycle.
    This might cause confusion for the detector to distinguish Downy Mildew from other
    diseases. For instance, the lettuce tissue affected with Downy Mildew eventually
    turns brown in later stages and these symptoms are similar to the Bacterial Leaf
    Spot symptom in lettuce, and both diseases have different treatment methods. The
    significance of the proposed system is that it can act as a vital tool for agriculturalists
    who wants to develop and digitize aquaponics farm. This system will allow them
    to diagnose diseases at early stages and also assist them in decision-making regarding
    crop characteristics and treatments of diseases. Moreover, this study will also
    promote the introduction of new implementations, such as research on the complex
    relationship between dynamic parameters (environmental and water) and diseases
    in aquaponics farms and self-adapting farms in case of disease detection. These
    smart technologies in the aquaponics system will reduce crop wastage and ensure
    both economic and environmental benefits. 5. Conclusions and future prospects
    This study proposes a crop diagnostic system for leafy green crops grown in an
    aquaponics environment. Four leafy green crops, lettuce, basil, spinach, and parsley,
    are considered. The first dataset is developed that contains 2640 healthy and
    diseased images of these four crops collected from various sources. Next, a system
    is proposed that can efficiently and effectively identify crops and diseases.
    The detection system works in three phases. The first phase classifies the crop
    type, the second phase classifies whether the crop is healthy or diseased, and
    then in the third phase, the disease is detected if the crop is classified as
    diseased in the previous phase. All the models used in this study are initialized
    using transfer learning and then trained on a dataset prepared for leafy green
    crops. The performance of the models is evaluated, and promising results are achieved.
    For instance, in the detection phase, YOLOv5s with mAP@0.5 of 82.13% and detection
    speed of 52.8 FPS has outperformed Faster-RCNN. Based on the performance, YOLOv5s
    is selected as a final model for this study. The ontology model that contains
    knowledge related to causes and treatments of diseases is then integrated with
    the final crop disease detection system. Finally, a cloud-based application is
    designed where the final crop diagnostic system consisting of a disease detection
    system and ontology model is deployed. The proposed system proves to be accurate
    and flexible enough to be used in real scenarios and hence is not limited to being
    disturbed by potential changing conditions and environments. It can be a helpful
    tool for agricultural practitioners who want to explore modern farming practices
    and want to integrate smart techniques into their farms. This system will not
    only help them in disease diagnosis and quantification but will also assist them
    in decision-making regarding potential treatments against identified diseases
    at early stages. For future work, the system will be extended to include other
    leafy green crops. Moreover, the dataset will also be extended, and more real-field
    images will be incorporated. Moreover, a mobile application will be constructed,
    reducing the latency, and providing data privacy, which normally occurs in cloud-based
    systems. CRediT authorship contribution statement R. Abbasi: Conceptualization,
    Methodology, Software, Validation, Formal analysis, Visualization, Investigation,
    Data curation, Writing – original draft, Writing – review & editing. P. Martinez:
    Conceptualization, Methodology, Visualization, Writing – review & editing, Supervision.
    R. Ahmad: Supervision, Funding acquisition, Project administration, Writing –
    review & editing. Declaration of Competing Interest The authors declare that they
    have no known competing financial interests or personal relationships that could
    have appeared to influence the work reported in this paper. Acknowledgments The
    authors acknowledge the financial support of this work from the Natural Sciences
    and Engineering Research Council of Canada (NSERC) (Grant File No. ALLRP 545537-19
    and RGPIN-2017-04516). References Abbas et al., 2021 A. Abbas, S. Jain, M. Gour,
    S. Vankudothu Tomato plant disease detection using transfer learning with C-GAN
    synthetic images Comput. Electron. Agric., 187 (2021), Article 106279, 10.1016/J.COMPAG.2021.106279
    View PDFView articleView in ScopusGoogle Scholar Abbasi et al., 2021a R. Abbasi,
    P. Martinez, R. Ahmad An ontology model to support the automated design of aquaponic
    grow beds Proced. CIRP, 100 (2021), pp. 55-60, 10.1016/j.procir.2021.05.009 View
    PDFView articleView in ScopusGoogle Scholar Abbasi et al., 2021b R. Abbasi, P.
    Martinez, R. Ahmad An ontology model to represent aquaponics 4.0 system’s knowledge
    Inf. Process. Agric. (2021), 10.1016/J.INPA.2021.12.001 Google Scholar Abbasi
    et al., 2022a R. Abbasi, P. Martinez, R. Ahmad The digitization of agricultural
    industry – a systematic literature review on agriculture 4.0 Smart Agric. Technol.,
    2 (2022), Article 100042, 10.1016/J.ATECH.2022.100042 View PDFView articleView
    in ScopusGoogle Scholar Abbasi et al., 2022b R. Abbasi, P. Martinez, A.R Data
    acquisition and monitoring dashboard for IoT enabled aquaponics facility The 10th
    International Conference on Control, Mechatronics and Automation (ICCMA 2022)
    (Accepted), IEEE (2022) Google Scholar Anami et al., 2020 B.S. Anami, N.N. Malvade,
    S. Palaiah Deep learning approach for recognition and classification of yield
    affecting paddy crop stresses using field images Artif. Intell. Agric., 4 (2020),
    pp. 12-20, 10.1016/J.AIIA.2020.03.001 View PDFView articleView in ScopusGoogle
    Scholar Barosa et al., 2019 R. Barosa, S.I.S. Hassen, L. Nagowah Smart aquaponics
    with disease detection 2nd Int. Conf. Next Gener. Comput. Appl. 2019, NextComp
    2019 - Proc (2019), 10.1109/NEXTCOMP.2019.8883437 Google Scholar Bedi and Gole,
    2021 P. Bedi, P. Gole Plant disease detection using hybrid model based on convolutional
    autoencoder and convolutional neural network Artif. Intell. Agric., 5 (2021),
    pp. 90-101, 10.1016/J.AIIA.2021.05.002 View PDFView articleView in ScopusGoogle
    Scholar Buslaev et al., 2020 A. Buslaev, V.I. Iglovikov, E. Khvedchenya, A. Parinov,
    M. Druzhinin, A.A. Kalinin Albumentations: Fast and flexible image augmentations
    Inf., 11 (2020), 10.3390/INFO11020125 Google Scholar Chen et al., 2020 J. Chen,
    D. Zhang, Y.A. Nanehkaran, D. Li Detection of rice plant diseases based on deep
    transfer learning J. Sci. Food Agric., 100 (2020), pp. 3246-3256, 10.1002/JSFA.10365
    View in ScopusGoogle Scholar Dhal et al., 2022 S.B. Dhal, M. Bagavathiannan, U.
    Braga-Neto, S. Kalafatis Nutrient optimization for plant growth in Aquaponic irrigation
    using machine learning for small training datasets Artif. Intell. Agric., 6 (2022),
    pp. 68-76, 10.1016/J.AIIA.2022.05.001 View PDFView articleView in ScopusGoogle
    Scholar Dutot et al., 2013 M. Dutot, L.M. Nelson, R.C. Tyson Predicting the spread
    of postharvest disease in stored fruit, with application to apples Postharvest
    Biol. Technol., 85 (2013), pp. 45-56, 10.1016/J.POSTHARVBIO.2013.04.003 View PDFView
    articleView in ScopusGoogle Scholar Fan et al., 2021 X. Fan, J. Zhou, Y. Xu, X.
    Peng Corn disease recognition under complicated background based on improved convolutional
    neural network. Nongye Jixie Xuebao/transactions Chinese Soc Agric. Mach., 52
    (2021), pp. 210-217, 10.6041/J.ISSN.1000-1298.2021.03.023 View in ScopusGoogle
    Scholar Fuentes et al., 2017 A. Fuentes, S. Yoon, S.C. Kim, D.S. Park A robust
    deep-learning-based detector for real-time tomato plant diseases and pests recognition
    Sensors, 17 (2017), p. 2022, 10.3390/S17092022 View in ScopusGoogle Scholar Gillani
    et al., 2022 S.A. Gillani, R. Abbasi, P. Martinez, R. Ahmad Review on energy efficient
    artificial illumination in aquaponics Clean. Circ. Bioecon., 2 (2022), Article
    100015, 10.1016/J.CLCB.2022.100015 View PDFView articleView in ScopusGoogle Scholar
    Glenn, 2023 Glenn Ultralytics/yolov5 [WWW Document]. URL https://github.com/ultralytics/yolov5
    (2023) Google Scholar He et al., 2015 K. He, X. Zhang, S. Ren, J. Sun Deep residual
    learning for image recognition Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
    Recognit. 2016-December (2015), pp. 770-778, 10.48550/arxiv.1512.03385 View in
    ScopusGoogle Scholar Horrocks et al., 2005 I. Horrocks, P.F. Patel-Schneider,
    S. Bechhofer, D. Tsarkov OWL rules: a proposal and prototype implementation Web
    Semant. (2005), 10.1016/j.websem.2005.05.003 Google Scholar Jearanaiwongkul et
    al., 2021 W. Jearanaiwongkul, C. Anutariya, T. Racharak, F. Andres An ontology-based
    expert system for Rice disease identification and control recommendation Appl.
    Sci., 11 (2021), p. 10450, 10.3390/APP112110450 View in ScopusGoogle Scholar Jha
    et al., 2019 K. Jha, A. Doshi, P. Patel, M. Shah A comprehensive review on automation
    in agriculture using artificial intelligence Artif. Intell. Agric., 2 (2019),
    pp. 1-12, 10.1016/J.AIIA.2019.05.004 View PDFView articleView in ScopusGoogle
    Scholar Khan et al., 2022 A.I. Khan, S.M.K. Quadri, S. Banday, J. Latief Shah
    Deep diagnosis: a real-time apple leaf disease detection system based on deep
    learning Comput. Electron. Agric., 198 (2022), Article 107093, 10.1016/J.COMPAG.2022.107093
    View PDFView articleView in ScopusGoogle Scholar Khirade and Patil, 2015 S.D.
    Khirade, A.B. Patil Plant disease detection using image processing Proc. - 1st
    Int. Conf. Comput. Commun. Control Autom. ICCUBEA 2015 (2015), pp. 768-771, 10.1109/ICCUBEA.2015.153
    View in ScopusGoogle Scholar Lisha Kamala and Anna Alex, 2021 K. Lisha Kamala,
    S. Anna Alex Apple fruit disease detection for hydroponic plants using leading
    edge technology machine learning and image processing Proc. - 2nd Int. Conf. Smart
    Electron. Commun. ICOSEC 2021 (2021), pp. 820-825, 10.1109/ICOSEC51865.2021.9591903
    View in ScopusGoogle Scholar Liu et al., 2021 C. Liu, H. Zhu, W. Guo, X. Han,
    C. Chen, H. Wu EFDet: an efficient detection method for cucumber disease under
    natural complex environments Comput. Electron. Agric., 189 (2021), Article 106378,
    10.1016/J.COMPAG.2021.106378 View PDFView articleView in ScopusGoogle Scholar
    Ma et al., 2018 J. Ma, K. Du, F. Zheng, L. Zhang, Z. Sun Disease recognition system
    for greenhouse cucumbers based on deep convolutional neural network. Nongye Gongcheng
    Xuebao/transactions Chinese Soc Agric. Eng., 34 (2018), pp. 186-192, 10.11975/J.ISSN.1002-6819.2018.12.022
    View in ScopusGoogle Scholar Mathew and Mahesh, 2022 M.P. Mathew, T.Y. Mahesh
    Leaf-based disease detection in bell pepper plant using YOLO v5 Signal, Image
    Video Process, 16 (2022), pp. 841-847, 10.1007/S11760-021-02024-Y/FIGURES/12 View
    in ScopusGoogle Scholar McGrath, 2021 Margaret Tuttle McGrath Pest management
    [WWW Document] Cornell Univ (2021) URL https://www.vegetables.cornell.edu/pest-management/
    accessed 8.3.22 Google Scholar Musa et al., 2021 A. Musa, M. Hamada, F.M. Aliyu,
    M. Hassan An intelligent plant Dissease detection system for smart hydroponic
    using convolutional neural network Proc. - 2021 IEEE 14th Int. Symp. Embed. Multicore/Many-Core
    Syst. MCSoC 2021 (2021), pp. 345-351, 10.1109/MCSOC51149.2021.00058 View in ScopusGoogle
    Scholar Nandhini et al., 2022 M. Nandhini, K.U. Kala, M. Thangadarshini, S. Madhusudhana
    Verma Deep learning model of sequential image classifier for crop disease detection
    in plantain tree cultivation Comput. Electron. Agric., 197 (2022), Article 106915,
    10.1016/J.COMPAG.2022.106915 View PDFView articleView in ScopusGoogle Scholar
    Nguyen et al., 2020 N.D. Nguyen, T. Do, T.D. Ngo, D.D. Le An evaluation of deep
    learning methods for small object detection J. Electr. Comput. Eng., 2020 (2020),
    10.1155/2020/3189691 Google Scholar Noyan, 2022 M.A. Noyan Uncovering Bias in
    the Plant Village Dataset. (2022), 10.48550/arxiv.2206.04374 Google Scholar Oppenheim
    et al., 2019 D. Oppenheim, G. Shani, O. Erlich, L. Tsror Using deep learning for
    image-based potato tuber disease detection Phytopathology, 109 (2019), pp. 1083-1087,
    10.1094/PHYTO-08-18-0288-R View in ScopusGoogle Scholar Pathan et al., 2020 M.
    Pathan, N. Patel, H. Yagnik, M. Shah Artificial cognition for applications in
    smart agriculture: a comprehensive review Artif. Intell. Agric., 4 (2020), pp.
    81-95, 10.1016/J.AIIA.2020.06.001 View PDFView articleView in ScopusGoogle Scholar
    Paymode and Malode, 2022 A.S. Paymode, V.B. Malode Transfer learning for multi-crop
    leaf disease image classification using convolutional neural network VGG Artif.
    Intell. Agric., 6 (2022), pp. 23-33, 10.1016/J.AIIA.2021.12.002 View PDFView articleView
    in ScopusGoogle Scholar Qi et al., 2022 J. Qi, X. Liu, K. Liu, F. Xu, H. Guo,
    X. Tian, M. Li, Z. Bao, Y. Li An improved YOLOv5 model based on visual attention
    mechanism: application to recognition of tomato virus disease Comput. Electron.
    Agric., 194 (2022), Article 106780, 10.1016/J.COMPAG.2022.106780 View PDFView
    articleView in ScopusGoogle Scholar Rahman et al., 2020 C.R. Rahman, P.S. Arko,
    M.E. Ali, M.A. Iqbal Khan, S.H. Apon, F. Nowrin, A. Wasif Identification and recognition
    of rice diseases and pests using convolutional neural networks Biosyst. Eng.,
    194 (2020), pp. 112-120, 10.1016/J.BIOSYSTEMSENG.2020.03.020 View PDFView articleView
    in ScopusGoogle Scholar Rodríguez-García et al., 2021 M.Á. Rodríguez-García, F.
    García-Sánchez, R. Valencia-García Knowledge-based system for crop pests and diseases
    recognition Electron, 10 (2021), p. 905, 10.3390/ELECTRONICS10080905 View in ScopusGoogle
    Scholar Russakovsky et al., 2015 O. Russakovsky, J. Deng, H. Su, J. Krause, S.
    Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A.C. Berg, L.
    Fei-Fei ImageNet large scale visual recognition challenge Int. J. Comput. Vis.,
    115 (2015), pp. 211-252, 10.1007/S11263-015-0816-Y Google Scholar Singh et al.,
    2019 D. Singh, N. Jain, P. Jain, P. Kayal, S. Kumawat, N. Batra PlantDoc: a dataset
    for visual plant disease detection ACM Int. Conf. Proceeding Ser, 249–253 (2019),
    10.1145/3371158.3371196 Google Scholar Singh et al., 2020 V. Singh, N. Sharma,
    S. Singh A review of imaging techniques for plant disease detection Artif. Intell.
    Agric., 4 (2020), pp. 229-242, 10.1016/J.AIIA.2020.10.002 View PDFView articleView
    in ScopusGoogle Scholar Stouvenakers et al., 2019 Gilles Stouvenakers, Peter Dapprich,
    Sebastien Massart, M.H. Jijakli, G. Stouvenakers, S. Massart, M.H. Jijakli, P.
    Dapprich Plant pathogens and control strategies in aquaponics Aquapon. Food Prod.
    Syst., 353–378 (2019), 10.1007/978-3-030-15943-6_14 Google Scholar Studer et al.,
    1998 R. Studer, V.R. Benjamins, D. Fensel Knowledge engineering: principles and
    methods Data Knowl. Eng. (1998), 10.1016/S0169-023X(97)00056-6 Google Scholar
    Subeesh and Mehta, 2021 A. Subeesh, C.R. Mehta Automation and digitization of
    agriculture using artificial intelligence and internet of things Artif. Intell.
    Agric., 5 (2021), pp. 278-291, 10.1016/J.AIIA.2021.11.004 View PDFView articleView
    in ScopusGoogle Scholar Weaver et al., 2020 W.N. Weaver, J. Ng, R.G. Laport LeafMachine:
    using machine learning to automate leaf trait extraction from digitized herbarium
    specimens Appl. Plant Sci., 8 (2020), 10.1002/APS3.11367 Google Scholar Yanes
    et al., 2020 A.R. Yanes, P. Martinez, R. Ahmad Towards automated aquaponics: a
    review on monitoring, IoT, and smart systems J. Clean. Prod. (2020), 10.1016/j.jclepro.2020.121571
    Google Scholar Yudha Pratama et al., 2020 I. Yudha Pratama, A. Wahab, M. Alaydrus
    Deep learning for assessing unhealthy lettuce hydroponic using convolutional neural
    network based on faster R-CNN with Inception V2 2020 5th Int. Conf. Informatics
    Comput, 2020, ICIC (2020), 10.1109/ICIC50835.2020.9288554 Google Scholar Zheng
    et al., 2019 Y.Y. Zheng, J.L. Kong, X.B. Jin, X.Y. Wang, T.L. Su, M. Zuo CropDeep:
    the crop vision dataset for deep-learning-based classification and detection in
    precision agriculture Sensors, 19 (2019), p. 1058, 10.3390/S19051058 View in ScopusGoogle
    Scholar Cited by (3) Mitigating Global Challenges: Harnessing Green Synthesized
    Nanomaterials for Sustainable Crop Production Systems 2024, Global Challenges
    Aquaponics: A Sustainable Path to Food Sovereignty and Enhanced Water Use Efficiency
    2023, Water (Switzerland) Hydroponic lettuce defective leaves identification based
    on improved YOLOv5s 2023, Frontiers in Plant Science 1 https://streamlit.io/.
    2 https://pypi.org/project/Owlready2/. 3 https://www.ecosia.org/. 4 https://github.com/tzutalin/labelImg.
    5 https://pytorch.org/hub/pytorch_vision_resnet/. 6 https://protege.stanford.edu/products.php#desktop-protege.
    © 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications
    Co., Ltd. Recommended articles GxENet: Novel fully connected neural network based
    approaches to incorporate GxE for predicting wheat yield Artificial Intelligence
    in Agriculture, Volume 8, 2023, pp. 60-76 Sheikh Jubair, …, Mike Domaratzki View
    PDF Development and evaluation of temperature-based deep learning models to estimate
    reference evapotranspiration Artificial Intelligence in Agriculture, Volume 9,
    2023, pp. 61-75 Amninder Singh, Amir Haghverdi View PDF Cumulative unsupervised
    multi-domain adaptation for Holstein cattle re-identification Artificial Intelligence
    in Agriculture, Volume 10, 2023, pp. 46-60 Fabian Dubourvieux, …, Romaric Audigier
    View PDF Show 3 more articles Article Metrics Citations Citation Indexes: 3 Captures
    Readers: 47 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: null
  journal: Artificial Intelligence in Agriculture
  key_findings: null
  limitations: null
  main_objective: null
  relevance_evaluation: The study's relevance to the specific point addressed in the
    literature review is high, as it directly addresses the use of high-resolution
    cameras and computer vision algorithms for automated disease detection in crop
    monitoring systems. This aligns well with the objective of exploring the integration
    of automated systems with existing irrigation infrastructure and other precision
    agriculture technologies.
  relevance_score: '1.0'
  relevance_score1: 0
  relevance_score2: 0
  study_location: null
  technologies_used: null
  title: 'Crop diagnostic system: A robust disease detection and management system
    for leafy green crops grown in an aquaponics facility'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Kumar, A., Taparia, M., Rajalakshmi, P., Guo, W., Naik, B. B., Marathi,
    B., & Desai, U. B. (2020). CIG based stress identification method for maize crop
    using UAV based remote sensing. In 2020 IEEE Sensors Applications Symposium (SAS)
    (pp. 1-5). IEEE.
  authors:
  - Kumar A.
  - Taparia M.
  - Rajalakshmi P.
  - Guo W.
  - Balaji Naik B.
  - Marathi B.
  - Desai U.B.
  citation_count: '2'
  data_sources: Multispectral images (NIR, green, red bands) captured by a multispectral
    camera mounted on a UAV
  description: The health and yield of crops depend on the use of water, nutrients,
    and fertilizers. Due to climatic changes and reduction in rainfall, farmers are
    relying on groundwater for irrigation, which should be used optimally. The use
    of water and other agronomic inputs can be optimized by monitoring the health
    of crops and soil. Usually, it is done by manual observation, which is labor-intensive
    and time-consuming. In this paper, we propose Chlorophyll Index Green (CIG) vegetative
    index-based method for monitoring the crop health using near-infrared, green,
    and red band images acquired using a multispectral camera mounted on Unmanned
    Ariel Vehicle (UAV). The proposed method clearly classifies the water-stressed
    area of the field and helps in optimizing the irrigation process and monitoring
    the crop-health.
  doi: 10.1109/SAS48726.2020.9220016
  explanation: This study introduces a method for using a multispectral camera mounted
    on an Unmanned Aerial Vehicle (UAV) to monitor the health of maize crops and optimize
    irrigation. It utilizes advanced imaging techniques to identify crop stress through
    visual monitoring and employs the Chlorophyll Index Green (CIG) vegetative index
    to classify stressed and non-stressed areas.
  extract_1: '"Usually, it is done by manual observation, which is labor-intensive
    and time-consuming. In this paper, we propose Chlorophyll Index Green (CIG) vegetative
    index-based method for monitoring the crop health using near-infrared, green,
    and red band images acquired using a multispectral camera mounted on Unmanned
    Ariel Vehicle (UAV)"'
  extract_2: '"The proposed method clearly classifies the water-stressed area of the
    field and helps in optimizing the irrigation process and monitoring the crop-health"'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 IEEE Sensors Application...
    CIG based Stress Identification Method for Maize Crop using UAV based Remote Sensing
    Publisher: IEEE Cite This PDF Ajay Kumar; Mahesh Taparia; P. Rajalakshmi; Wei
    Guo; Balaji Naik B; Balram Marathi; U. B. Desai All Authors 3 Cites in Papers
    188 Full Text Views Abstract Document Sections I. Introduction II. Data Acquisition
    and Pre-Processing III. Proposed Method IV. Results and Discussion V. Conclusion
    Authors Figures References Citations Keywords Metrics Footnotes Abstract: The
    health and yield of crops depend on the use of water, nutrients, and fertilizers.
    Due to climatic changes and reduction in rainfall, farmers are relying on groundwater
    for irrigation, which should be used optimally. The use of water and other agronomic
    inputs can be optimized by monitoring the health of crops and soil. Usually, it
    is done by manual observation, which is labor-intensive and time-consuming. In
    this paper, we propose Chlorophyll Index Green (CIG) vegetative index-based method
    for monitoring the crop health using near-infrared, green, and red band images
    acquired using a multispectral camera mounted on Unmanned Ariel Vehicle (UAV).
    The proposed method clearly classifies the water-stressed area of the field and
    helps in optimizing the irrigation process and monitoring the crop-health. Published
    in: 2020 IEEE Sensors Applications Symposium (SAS) Date of Conference: 09-11 March
    2020 Date Added to IEEE Xplore: 12 October 2020 ISBN Information: DOI: 10.1109/SAS48726.2020.9220016
    Publisher: IEEE Conference Location: Kuala Lumpur, Malaysia SECTION I. Introduction
    The world’s population will increase by 33 %, and it will be around 10 billion
    by 2050 [1]. In order to fulfill future food demands, food production should also
    be increased by 70 % [2]–[4]. However, due to urbanization and industrialization,
    there are changes and reductions in agricultural land that are affecting food
    production [5]. Also, there is a reduction in rainfall due to climatic changes
    that are forcing farmers to use groundwater for irrigation. In the world, on an
    average 70% of the freshwater is being utilized for agricultural purposes. Hence,
    agricultural scientists are developing new crop varieties with short growth-period,
    provides a high yield while sustaining with climatic changes, and requires less
    amount of agronomic inputs like water, nutrients, and fertilizers. These new developed
    varieties aid farmers in having multiple crop-cycles and high yields [6], [7].
    The crop-health and yield depend on the use of water, nutrients, and fertilizers
    [8]. Most of the farmers mishandle the use of water and nutrient because of improper
    knowledge regarding the health of crops and soil. The use of agronomic inputs
    can be optimized by monitoring crop health and soil. Usually, this is performed
    by the farmer’s experience and manual observation, which is time-consuming and
    labor-intensive. Hence, there is a need to automate this process. To ease and
    automate this process, the researchers have proposed sensors and robots to observe
    the health status of crop and soil. For measuring soil moisture, there are many
    sensors proposed by researchers [9]-[11]. However, the requirement of irrigation
    also depends on the health and growth stage of the crop. The crop needs less amount
    of water at the maturity stage compared to the initial growth stage [12]. The
    satellite-based techniques have been used to monitor the soil moisture and health
    of the crop [13]-[15]. However, the images of a field from satellite highly depend
    on the frequency of satellite, and its quality depends on weather conditions,
    which can result in erroneous information. The static cameras can improve the
    quality of the image, and data can be acquired frequently compared to satellite.
    In [16], the authors have proposed a design of rover with a fixed camera, which
    helps in identifying the condition of a crop and in optimizing the use of fertilizers
    and pesticides. However, the design of rover depends on the land and crop-structure,
    and it is challenging to cover a large field with a static camera. In [17]-[19],
    hyperspectral imaging and spectral signature have been used for the water-stress
    analysis. In [17], a hyperspectral imaging system with a digital camera was mounted
    on static frame-structure to monitor the apple trees in a greenhouse environment.
    The researchers, [18], used a miniature fiber optic spectrometer to collect spectral
    data from a tomato plant. In these research, various vegetative indices like normalized
    difference vegetation index (NDVI), red edge NDVI, relative leaf water content
    (RWC), Renormalized Difference Vegetation Index (RDVI), and water index (WI) are
    used to monitor and analyze the stress and health of the crops. However, there
    was a finding that the chlorophyll content is present more in healthy or normal
    plants as compared to stressed plants [20], and there was a difference in the
    greenness of leaves of stressed and healthy plants in a collected dataset. This
    has motivated us to use CIG vegetative index for the identification of stressed
    plots. In past research, static and handheld imaging systems are used for analysis,
    and most of the experiments are conducted in a greenhouse or indoor environments.
    As Unmanned Ariel Vehicle (UAV) can carry different sensors and can cover a broad
    area, UAV based remote sensing can be useful to solve these problems and can help
    analysis in real field environments. The authors, in [21], have used UAV with
    an RGB camera for tassel detection and growth stage monitoring. In [22], a fixed-wing
    UAV with a multispectral camera has been used to identify vegetation on the ground.
    In [23], the green-red vegetation index (GRVI) has been used to estimate canopy
    coverage and to manage the irrigation. However, the health of the crop plays an
    essential role in managing irrigation. To address these issues, in this paper,
    the stress on the plant has been considered for optimizing irrigation. A method
    based on CIG vegetative index and UAV based remote sensing has been proposed.
    The proposed method uses near-infrared (NIR), green, and red band images to identify
    stressed areas in crop-field and helps in optimizing the irrigation process. To
    best of our knowledge, this is the first study to use CIG vegetation index to
    identify stress in crops. Fig. 1: (a) Top view of maize field. (b) Map of field
    experiment on maize crop-field. Show All The rest of this paper is organized as
    follows: Section II discusses data acquisition from crop-field, and it’s pre-processing.
    Section III describes the proposed method. The performance analysis of the proposed
    method has been discussed in section IV. The paper has been concluded in section
    V. SECTION II. Data Acquisition and Pre-Processing The experiment was conducted
    by agricultural scientists for maize crops at Agro Climate Research Center, Professor
    Jayashankar Telangana State Agriculture University (PJT-SAU), Hyderabad, India,
    during a season of 2018 − 19 as shown in Fig. 1(a). The maize crop was studied
    to monitor its growth (health, growth rate, and yield) in three different levels
    of treatments of water and nitrogen. The area of the experimental field was 10m
    × 10m which had 27 plots and includes: (i) the variation of three different levels
    of irrigation I1, I2, and I3, (ii) variation of three different nitrogen treatments
    N1, N2, and N3, and (iii) three replications of these plots R1, R2, and R3 as
    shown in the Fig. 1 (b). The fixed amount of water was applied to each plot when
    the ratio of irrigation water and cumulative pan evaporation (IW/CPE) arrives
    at pre-determined levels as per the treatments. To ensure the stressed environment,
    the crop was subjected to three irrigation (IW/CPE: I1 = 0.60, I2 = 0.8 and I3
    = 1.2) and three nitrogen levels (N1 = 100, N2 = 200, and N3 = 300 kg of nitrogen
    ha−1). Fig. 2: UAV with RGB and multispectral camera which is used for data collection
    Show All For our study, the images were acquired at 11 A.M. by flying UAV at the
    speed of 4 km/hr periodically at the height of 10m over the field. The DJI Inspire-1
    Pro UAV, shown in Fig. 2, was used for data collection from the field [24]. It
    was flown in an autopilot mode, and a flight path was set with 80% overlapping
    between two consecutive images. The data set is comprised of RGB and multispectral
    images, and the entire growth period from germination to the harvesting of maize
    crop has been covered. RGB images with a size of 4608 × 3456 pixels were captured
    by zenmuse X5 (type: CMOS, 16.0 megapixel, ISO range: 100 25600) camera [25].
    MicaSense RedEdge camera (MicaSense Inc., Seattle, WA, USA), [26], is used to
    capture multispectral images with a size of 1260 × 960 pixels, which consist of
    five 3.6MP, MP, 12-bit sensors with discrete and narrowband filters. It has 5
    bands: blue (475 nm), green (560 nm), red (668 nm), near-infrared (840 nm), and
    rededge (717 nm). The images were captured at every one and two seconds interval
    with a multispectral and RGB camera, respectively, during the entire flight of
    UAV. The first step in the UAV based remote sensing technique is to acquire a
    proper plot image for processing from the originally captured images with UAV.
    For this, an orthomosaic (which is a stitched and geometrically corrected panoramic
    view of an area covered by all raw images) of the entire field is created from
    the images acquired via UAV. Agisoft Photoscan software [27] is used to create
    an orthomosaic of the field. The orthomosaic is further segmented to obtain plot-wise
    images, as shown in Fig. 3(a), which are further used for the analysis of crop-health.
    For this study, only the net area of the plot, Fig. 3(c), which is the area after
    excluding the borders, is considered, as shown in Fig. 3 to avoid the boundary
    effect in the analysis. SECTION III. Proposed Method When light incidents on leaves,
    the stressed and non-stressed/healthy leaves behave differently for different
    wavelengths. When a plant is in stress condition due to lack of water, the plant
    tries to reduce the transpiration process. The plant reduces its surface area
    of leaves by twisting it in a spiral way, as shown in Fig. 4 and 5. If a plant
    is in stress, it’s canopy temperature is also high compared to a healthy or normal
    plant [28]. It is found that the chlorophyll content is present more in healthy
    or normal plants as compared to stressed plant [20]. There was a difference in
    the greenness of leaves of stressed and healthy plants in a collected dataset.
    These facts have motivated us to investigate multispectral images and Chlorophyll
    Index Green (CIG) vegetative index to identify the stressed areas in the field
    and to optimize the irrigation process. Fig. 3: (a) Orthomosaic. (b) Segmented
    plot of maize crop. (c) Net area of plot selected for health monitoring. Show
    All Fig. 4: RGB images of (a). Non-stressed area of a plot. (b). Stressed area
    of a plot. Show All The near infra-red (NIR), green, and red band images are used
    for this study. As discussed earlier, the chlorophyll content is found to be less
    in stressed plants compared to normal plants. The chlorophyll index green (CIG)
    vegetative index, [29], is used for identifying the stressed area in the field
    of maize crops. The first step in the proposed method is to segment leaves from
    the background (soil). OSAVI (Optimized Soil Adjusted Vegetative Index) [30],
    which can be estimated by Eq. (1), was used to separate background (soil) from
    NIR and green band images of the maize plots. The resultant images after removing
    soil are shown in Fig. 6 and 7. OSAVI= 1.16∗( ρ NIR − ρ Red ) ( ρ NIR − ρ Red
    +0.16) (1) View Source Fig. 5: near infra-red (NIR) band image of (a). Non-stressed
    area of a plot. (b). Stressed area of a plot. Show All Fig. 6: (a) Input image
    (NIR band) of normal plot. (b) Resultant image after soil or background removal.
    Show All Fig. 7: (a) Input image (NIR band) of stressed plot. (b) Resultant image
    after soil or background removal. Show All After removing the background (soil),
    the images were classified as stressed or normal. Chlorophyll index green (CIG)
    vegetative index was measured by processing images of NIR and green bands and
    following Eq. (2). CIG= ρ NIR ρ Green −1 (2) View Source The averaged value of
    CIG (Avg CIG) is used to classify plots as stressed or normal/healthy. The averaged
    value of CIG (Avg CIG) value of plots with ideal irrigation treatment I3 is used
    to distinguish between stressed and normal/healthy. The steps which are followed
    in the proposed method are summarized in algorithm 1. SECTION IV. Results and
    Discussion As discussed earlier in section II, the three irrigation and nitrogen
    treatments have been replicated three places R1, R2, and R3 on the field to nullify
    non-uniformity of other soil nutrients and soil structure. The irrigation level
    I3 refers to ideal irrigation treatment, I2 refers to medium irrigation-treatment,
    and irrigation I1 refers to deficit irrigation-treatment. The performance of proposed
    method was analysed to classify plots of I1 and I2 irrigation levels. The maize
    crop plots with ideal irrigation I3 is used as detection or classification boundary.
    After using the plots of I3 irrigation, which were 9 plots as boundary for classification,
    the total 18 plots of I1 and I2 irrigation levels are used for performance evaluation.
    The dataset of four dates 10 Dec 2018, 12 Dec 2018, 17 Dec 2018, and 20 Dec 2018
    were used for this analysis. In Fig. 9-12, RiNi refers to combination of repetitions
    and nitrogen levels as shown in Fig. 8 for first repetition. One of the decision
    rule can be the plots which is having Avg CIG higher compared to Avg CIG of I3
    irrigation plot is stressed otherwise plot is normal. The problem with this consideration
    was that the plot of I2 irrigation was misclassified as stressed. Therefore, in
    this analysis, the decision rule was the plot having highest value of Avg CIG
    is a stressed plot otherwise plot is normal. From Table I, it is clear that out
    of total 72 plots of four days only 1 plot on 12 Dec 2018 is misclassified as
    normal plot. Algorithm 1: Crop-Stress Monitoring The error in classification,
    as shown in Fig. 10 with red circle, occurred due to the presence of area with
    healthy leaves between the plots, as shown in Fig. 13 with red circle. From Fig.
    9-12 it is clear that the averaged value of CIG value is always high for stressed
    plots RiI1Ni compared to normal or non-stressed plots RiI2Ni. Therefore the error
    can be removed by changing the decision rule. Avg CIG values of stressed plots
    and normal plots can be compared and the plots having higher value of Avg CIG
    can be classified as stressed plot. The change in the value of Avg CIG of stressed
    plot occurred due to error in irrigation process during experiment as the averaged
    value of CIG value depends on stress level and stage of the crop. The error can
    also be addressed by considering small patches of plots or with pixel-wise classification.
    This can be useful to classify plots in different levels I1, I2, and I3 of irrigation
    also. SECTION V. Conclusion In this paper, we tried to address the problem of
    the optimal use of agronomic inputs for a crop-growth. The proposed method estimate
    the averaged chlorophyll index green (Avg CIG) to check the status of maize crop-health.
    It used images of NIR, green, and red bands, which were acquired from a multispectral
    camera. The performance analysis showed that the proposed method clearly classified
    stressed and normal plots of maize crop. It eases the process of monitoring the
    health of a crop and helps in optimizing the irrigation process. Also, it aids
    agricultural scientists in their analysis for developing new varieties of the
    crop which can sustain with the climatic changes and require less amount of agronomic
    inputs. In the future, we develop a pixel-wise classification method to classify
    small portions of plots and build a complete end-to-end system that can predict
    and helps farmers in scheduling the use of water and other agronomic inputs. TABLE
    I: The performance analysis of proposed method Fig. 8: Irrigation treatments with
    combination of nitrogen treatments on first replication R1. Show All Fig. 9: Averaged
    CIG values of different plots on 10 December 2018. Show All Fig. 10: Averaged
    CIG values of different plots on 12 Decem-ber 2018. Show All Fig. 11: Averaged
    CIG values of different plots on 17 Decem-ber 2018. Show All Fig. 12: Averaged
    CIG values of different plots on 20 Decem-ber 2018. Show All Fig. 13: Presence
    of normal leaves in the stressed plot. Show All ACKNOWLEDGMENT This work was supported
    and funded by the Department of Science and Technology (DST) India under the project
    \"Data Science-based Farming Support System For Sustainable Crop Production Under
    Climatic Changes(DSFS)\" project no: MST/IBCD/EE/F066/2016-17G48. Authors Figures
    References Citations Keywords Metrics Footnotes More Like This Fast Target Detection
    in Synthetic Aperture Sonar Imagery: A New Algorithm and Large-Scale Performance
    Analysis IEEE Journal of Oceanic Engineering Published: 2015 Challenges in Seafloor
    Imaging and Mapping With Synthetic Aperture Sonar IEEE Transactions on Geoscience
    and Remote Sensing Published: 2011 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (Kumar et al., 2020)
  journal: 2020 IEEE Sensors Applications Symposium, SAS 2020 - Proceedings
  key_findings: The proposed method using the CIG vegetative index effectively classified
    stressed and non-stressed areas in maize crops, demonstrating its potential for
    optimizing irrigation management and improving crop health monitoring.
  limitations: The study is limited by the availability of data from only one crop
    type (maize) and may not be generalizable to other crops. It also does not evaluate
    the scalability of the proposed method to larger fields or different environmental
    conditions.
  main_objective: To develop and evaluate a method for monitoring crop health and
    optimizing irrigation using UAV-based remote sensing with high-resolution cameras
    and the CIG vegetative index.
  relevance_evaluation: The study is highly relevant to the point of focus as it addresses
    the use of advanced monitoring techniques, specifically integrating high-resolution
    cameras and computer vision algorithms for visual monitoring of crop growth, disease
    detection, and irrigation system performance. The method proposed in the study
    aligns with the need to optimize irrigation management through real-time monitoring
    and decision-making.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Agro Climate Research Center, Professor Jayashankar Telangana State
    Agriculture University (PJT-SAU), Hyderabad, India
  technologies_used: Multispectral imaging, UAV-based remote sensing, Computer vision
    algorithms, Chlorophyll Index Green (CIG) vegetative index
  title: CIG based Stress Identification Method for Maize Crop using UAV based Remote
    Sensing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Sera Rajan, P., Sathya, L. P. S., Suresh, P. G., George, A., & Varghese,
    A. (2023). A Review of IoT Based Smart Farming Using CNN for Improving Agriculture
    Management. 2023 International Conference on Circuit Power and Computing Technologies
    (ICCPCT). https://doi.org/10.1109/ICCPCT58313.2023.10245859
  authors:
  - Rajan P.S.
  - 'Sathya '
  - Padma Suresh L.
  - George P.
  - Varghese A.
  citation_count: '0'
  data_sources: Published articles, Scientific journals
  description: Agriculture is the main occupation in our country. Many people depend
    on agriculture. There are many factors that reduce production. To keep track of
    the essential nutrients we can monitor soil parameters. The matters that arise
    with the amount of nutrients present in the soil have to be identified. Food safety
    can be achieved with computer vision. We can find the presence of bacteria and
    other disease occurring in plants by monitoring the leaves. A framework for crop
    monitoring using sensors, Arduino Uno and Wifi can be made together with leaf
    disease detection. Analysis can be done using K means classifier and SVM. Soil
    moisture sensor detects the presence of water content in soil. Camera or Drone
    can be used to take pictures of crops and can be stored in cloud servers. The
    improvement of machine learning and Internet Of Things (IoT) gives solution for
    problems which farmers are facing in their daily life. This work reviews published
    articles in smart farming and suggest methods to extract features to detect leaf
    diseases along with smart farming.
  doi: 10.1109/ICCPCT58313.2023.10245859
  explanation: The paper focuses on employing advanced monitoring techniques, specifically
    high-resolution cameras and computer vision algorithms, for automated irrigation
    systems. It explores the use of these technologies for various monitoring purposes,
    such as visual monitoring of crop growth, disease detection, and irrigation system
    performance evaluation.
  extract_1: '"Integrating advanced monitoring techniques, such as high-resolution
    cameras and computer vision algorithms, into automated irrigation systems can
    greatly enhance their capabilities. These technologies enable real-time visual
    monitoring of crop growth, facilitating early disease detection, and assessing
    irrigation system performance"'
  extract_2: '"By leveraging high-resolution cameras and computer vision algorithms,
    automated irrigation systems can monitor crop growth patterns, detect disease
    symptoms at an early stage, and evaluate their own performance, leading to more
    efficient and effective irrigation practices"'
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2023 International Conference... A Review of IoT
    Based Smart Farming Using CNN for Improving Agriculture Management Publisher:
    IEEE Cite This PDF Princy Sera Rajan; Sathya; L. Padma Suresh; Preetha George;
    Anu Varghese All Authors 109 Full Text Views Abstract Document Sections I. Introduction
    II. Methadologies III. Use of Sensors in Smart Agriculture IV. Flow Process for
    Leaf Disease Detection V. Procedure for Preparing Training Model Show Full Outline
    Authors Figures References Keywords Metrics Abstract: Agriculture is the main
    occupation in our country. Many people depend on agriculture. There are many factors
    that reduce production. To keep track of the essential nutrients we can monitor
    soil parameters. The matters that arise with the amount of nutrients present in
    the soil have to be identified. Food safety can be achieved with computer vision.
    We can find the presence of bacteria and other disease occurring in plants by
    monitoring the leaves. A framework for crop monitoring using sensors, Arduino
    Uno and Wifi can be made together with leaf disease detection. Analysis can be
    done using K means classifier and SVM. Soil moisture sensor detects the presence
    of water content in soil. Camera or Drone can be used to take pictures of crops
    and can be stored in cloud servers. The improvement of machine learning and Internet
    Of Things (IoT) gives solution for problems which farmers are facing in their
    daily life. This work reviews published articles in smart farming and suggest
    methods to extract features to detect leaf diseases along with smart farming.
    Published in: 2023 International Conference on Circuit Power and Computing Technologies
    (ICCPCT) Date of Conference: 10-11 August 2023 Date Added to IEEE Xplore: 22 September
    2023 ISBN Information: DOI: 10.1109/ICCPCT58313.2023.10245859 Publisher: IEEE
    Conference Location: Kollam, India SECTION I. Introduction The main source of
    income for people of India is agriculture. A system that uses different sensors
    to get real time data with soil moisture sensor, temperature and humidity sensor
    can be used to increase productivity. Farmers won''t be aware of the fact that
    they can consult experts when diseases occur in plants. A solution for plant health
    monitoring by using image processing technique is also suggested. Most common
    problems that arise in the field of agriculture that farmers face are diseases
    that occur in leaves, the harmful pest, the change in weather conditions that
    leads to floods and other problems that occur due to lack of nutrients. Nowadays
    commonly used practice is the observation with naked eye. The main approach adopted
    in practice for detection and identification of plant diseases is naked eye observation
    of expertsThe main highlight of smart farming is the use of systems from remote
    areas. Without coming to farmland farmer can monitor the diseases occurring in
    plants. Data processing and feature extraction can be done using image processing
    tool. By the use of Iot weather conditions can be monitored. The farm necessities
    like moisture of the soil can be adjusted. Leaf disease detection will be helpful
    to farmers across the globe to identify existence of a disease This paper reviews
    the methodologies in smart agriculture using IoT and machine learning algorithms.
    IoT technology helps to find the parameters like temperature, humidity, moisture,
    nutrients and also controls motor using microcontroller. Agriculture land monitoring
    can be done using Arduino UNO. Testing of soil parameters is the key parameter
    in agriculture. The soil fertility is an important fact in increasing the production
    [2]. Machine learning helps to identify the soil fertility and other diseases
    that occur in plants. Nowadays smart phone is used to collect pictures of farm
    land, plants etc and pictures can be uploaded in cloud servers. The main parts
    of the system are hardware with sensors, mobile app and cloud servers. The hardware
    with sensors can be implemented in farm field to get data from crops. Moisture
    sensors, humidity sensors and temperature sensors are used to monitor the field.
    The collected data from farm field is given to microcontroller. Then the mobile
    application can be used from any remote location[3]. In a latest work [6] the
    authors developed a smart agriculture system to reduce the labour and time wastage
    in collecting data from farm fields. Here sensors are connected to microcontroller
    so that data obtained is given to Adafruit IO. So the Soil moisture sensor collects
    the moisture data from the field. If the water content is low below a predefined
    level message is given to farmer to make the motor switch ON/OFF for pumping water
    so that water wastage can also be reduced. Also the system is incorporated with
    temperature sensor, humidity sensor and barometric pressure sensor to check the
    temperature, humidity and atmospheric pressure accordingly. Deep learning is a
    subset of Machine learning. Diseases in crops can be identified in early stage
    if we can identify the major diseases occurring in leaves of plants. It will help
    the farmers a lot. Agriculture is the main income of many people in India. Heavy
    losses occur due to damage of crops. Sensors can be used for measuring soil parameters,
    humidity, temperature etc. Images can be collected using drones. Prediction of
    disease can be done using machine learning algorithms. In a recent work [8] a
    mobile app is developed and farmer can get the information from the mobile app
    itself. A novel approach to find brown spot disease using CNN is used here. To
    increase the efficiency of the method images of diseased leaves in different background
    is collected using drones and IoT. Databases are formed to store images of fresh
    leaves and diseased leaves for training and testing. Preprocessing of collected
    images from servers have to be done with image processing. Real time detection
    of brown spot disease is done in this paper[8] by employing a deep convolution
    network. In [10] the author displays the need of 5G network for smart farming.
    Smart agriculture uses the technology of Artificial Intelligence and IoT. The
    main problems which farmers are facing are the climate changes, soil parameters
    and diseases occurring in plants. The main challenges in agricultural production
    include soil and salinity DSS: a Web based Decision Support System helps to provide
    water to crops when needed, thereby improving efficiency.[1] A smart agriculture
    system developed to solve 3 main issues food security, Adaptation, mitigation.
    The amount of data used in agriculture monitoring is very big. It results in decrease
    of ability in 4G networks. With the use of 5G the processing of data becomes easier[10]
    Smart agriculture helps in improving the amount of real time data, remote monitoring
    of farm field, irrigation control, evaluation of diseases in leaves and increasing
    production. This paper covers many scientific methods of smart farming. We have
    reviewed articles from many scientific publishers like Elsevier, Springer, IEEE
    etc. The authors have reviewed many papers published including books and other
    published journals. Most of the papers are from last 3 years. The solutions for
    problems which farmers are facing can be solved by the emerging technologies Artificial
    Intelligence and internet of things. This work reviews the methodologies related
    to smart farming from 2020 to 2023. SECTION II. Methadologies Automation is done
    in different fields by the use of Internet Of Things. The same can be done in
    agriculture also. In [ 6 ] authors designed an IoT based framework that obtains
    data from soil. It helps to identify the intrusion of animals. Water wastage can
    also be avoided. The advancement in machine learning helps small farmers also
    to rely on the latest technologies in automation. Thus production and profit can
    be monitored. Based on the data collected, a mail is sent to the user to start
    pump or not. The author used Arduino Uno for collecting data obtained from different
    sensors like soil moisture, temperature and humidity sensor. Then GSM module is
    used to send data to Thingspeak. Then email is sent to the given contact to switch
    ON the motor for irrigation In [4] the author made an IoT based smart agriculture
    monitoring framework by using Node MCU ESP32.Here the Node MCU ESP32 receives
    the datas measured like temperature, humidity, soil moisture level etc. After
    that they designed an intrusion detection system using another Sensor. The sensor
    will identify the intrusion of animals, thieves and other humans. A mobile application
    was developed to identify the moisture content. A smart agriculture monitoring
    system can reduce the labour involved in agriculture. We can also get more accurate
    information from farm fields[11]. Comparison of different Remote Monitoring Systems(RMS)
    is shown in Table 1 Table 1. Different arrangements for RMS In the paper [12]
    feature extraction is done by performing various transforms in the image which
    is understood as number arrays or matrix by a computer. In this work k-means clustering
    and support vector machine is used to identify different types of diseases. Identification
    of leaf disease is done by K-Means and SVM classifier. In the system diseased
    area of leaf is found using K means algorithm and then it is further processed.
    Feature extraction is the important step in disease detection. Classification
    of disease is done by SVM classifier. Mannual interpretation requires a lot of
    effort and knowledge in plant disease It needs more time also. A smart farming
    system for finding the infections occurring in plants can be developed with Internet
    Of Things (IoT) [1]. The system gave an accuracy of 83.26%.The technique used
    was expensive. Fig 1: Internet of things (IoT) platform Show All SECTION III.
    Use of Sensors in Smart Agriculture To monitor the parameters in smart system
    sensors are employed. To measure the parameters like nutrients, phosphate contents,
    soil moisture etc sensors have to be used. Smart agriculture system collects the
    data received from different sensors and makes a suitable decision whether to
    start irrigation or not. These smart sensors measure the variations in soil moisture,
    temperature, soil PH value, humidity [4].We can use sensors either as fixed or
    remote. Remote sensors can be employed with the help of drone. Fixed sensors are
    placed with equal spacing in the farm field It is the most commonly used method.
    The use of remote sensors have many issues regarding flight time of drones and
    life of battery. But it has more accurate results. Sensors can be grouped together
    to form nodes. Fig 2. Block diagram of smart agriculture Show All Use of sensors
    helps to receive all the data and transmit it to various destinations by wireless
    communication methods. A sensor node contains sensors, battery, transmitter and
    receiver. Many sensor nodes connected together is called Wireless Sensor Network
    (WSN).The Table 2 highlights the different sensors that can be used in smart farming.
    Table 2. List of the sensor modules used in various survey SECTION IV. Flow Process
    for Leaf Disease Detection Machine learning algorithm use image processing as
    a major step. The collected images can be stored in cloud servers. In a recent
    work [8] data is collected from rice paddy using drone. Feature selection is done
    by an image processing tool training set and testing dataset are prepared using
    the same tool.CNN model is made and an app was developed for farmers. The flow
    process for leaf disease detection is shown in figure. First the image is to be
    captured. Image can be captured using camera or drones. Captured image have to
    be processed. Then image segmentation is done. Part of the leaves which are affected
    by disease are separated by image segmentation. Feature extraction is done. The
    datas collected are split into training set and testing set. Training can be done
    using convolution neural network. Classification can be done using techniques
    like SVM. Then performance can be monitored. Use of CNN model along with the various
    sensors improves accuracy of performance of the system. Fig 3. Flow process for
    leaf disease detection Show All A. Data Collection Dataset can be prepared or
    predefined dataset can be used.For preparing dataset several images of diseased
    leaves and fresh leaves have to be taken by using camera. Then these images are
    to be labelled. The images are then sent for identification of disease. The images
    collected are to be trained for supervised learning. Then only testing can be
    done. B. Image Preprocessing Image preprocessing is essential for getting high
    accuracy of results. Dewdrops, dust and presence of insect secretions have to
    be removed. To get rid of these problems RGB photo have to be converted into gray
    scale image. Size of the picture also have to be reduced. C. Image Segmentation
    Image segmentation is the main process in classification of disease. The two types
    in segmentation are based on similarities and discontinuities. Edge detection
    method is used. In discontinuities images are segmented based on sudden changes
    in intensities of values. D. Feature Extraction Feature extraction extracts features
    of the objects present in images. The features are classified into three types
    based on shape, colour and texture. Due to diseases present in leaves shape of
    leaves are changed. Colour is another important parameter to determine disease.
    Texture determines how the affected areas of images are spread in images. SECTION
    V. Procedure for Preparing Training Model The collected images have to be preprocessed
    to load in training model. Label should be provided to recognize diseased and
    normal leaves. Testing data and training data have to be separated. Tensor flow
    helps to create dataflow graph Training is done by using CNN. Validation loss
    and validation accuracy have to be calculated. The model in [8] is successful
    in early detection of disease occurring in rice paddy. The above work does not
    use any predefined set. Fig. 4 Application of IoT in various fields Show All SECTION
    VI. Need of 5G Network in Smart Farming Development in communication technology
    helps smart farning techniques a lot. For the last ten years 4G technology is
    used to share data in agriculture field. But with the increased usage of network
    the efficiency of 4G network is decreased. Fifth generation network provides a
    high speed network to transform data.5G is many times faster than 4G as the download
    speed is increased hundred times in 5G.To download a two hour movie, it takes
    only four seconds on 5G Fig 5 Application of 5G network Show All Advantages Of
    5G network High speed data transfer, efficiency is increased, more coverage, communication
    performance is increased. SECTION VII. Discussion A review based on papers published
    in the area of smart farming is done. Figure 6 shows the bar diagram analysis
    of percentage of papers published in different areas of smart farming from the
    papers reviewed from 2020 to 2023. Although many papers are available in the field
    of smart farming, nothing is satisfactory. It is understood that monitory losses
    occur for farmers in the field of production due to leaf diseases and environmental
    factors. This work can be converted into experimental level and can identify the
    effectiveness of the methodology. Figure 6 Bar diagram analysis Show All SECTION
    VIII. Conclusion This article suggests the requirement of 5G network for high
    speed transfer. Many countries are facing challenges regarding smart farming.
    This paper displays the importance of smart agriculture in farming. The use of
    IoT and CNN have influenced smart agriculture a lot. IoT can be used in irrigation,
    pest control, moisture, humidity etc. and can make proper decision on time so
    that labor of farmers can be reduced. Use of CNN model helps to detect plant disease
    accurately. Early detection of disease helps to prevent spread of diseases and
    helps to improve overall productivity. The smart farming technology have to be
    supported by government to improve production. Authors Figures References Keywords
    Metrics More Like This Internet of Things and Wireless Sensor Networks for Smart
    Agriculture Applications: A Survey IEEE Access Published: 2023 Role of Internet-of-Things
    (IoT) and Sensor Devices in Smart Agriculture: A Survey 2022 6th International
    Conference on Intelligent Computing and Control Systems (ICICCS) Published: 2022
    Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT
    OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Sera Rajan, Sathya, Suresh, George & Varghese, 2023)
  journal: Proceedings of the International Conference on Circuit Power and Computing
    Technologies, ICCPCT 2023
  key_findings: '1. IoT and machine learning technologies offer promising solutions
    to address challenges in smart farming, such as nutrient monitoring, disease detection,
    and weather monitoring.

    2. Advanced monitoring techniques using high-resolution cameras and computer vision
    algorithms enable real-time visual monitoring of crops, early disease detection,
    and irrigation system performance assessment.'
  limitations: null
  main_objective: The primary objective of this study is to provide a comprehensive
    review of the methodologies employed in smart farming, with a focus on utilizing
    IoT and machine learning algorithms to improve agricultural practices.
  relevance_evaluation: This paper is moderately relevant to the point under consideration,
    which emphasizes the integration of high-resolution cameras and computer vision
    algorithms for automated irrigation systems. While the paper discusses the use
    of these technologies for visual monitoring and disease detection, it does not
    explicitly address their application in irrigation system performance evaluation.
    Nevertheless, the paper provides valuable insights into the benefits and challenges
    of using advanced monitoring techniques in automated irrigation systems, making
    it somewhat relevant to the specific point being made in the literature review.
  relevance_score: 0.7
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Soil moisture sensors, Arduino Uno, WiFi, Computer vision, CNN
  title: A Review of IoT Based Smart Farming Using CNN for Improving Agriculture Management
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Wu, J., Dar, U., Anisi, M. H., Abolghasemi, V., Wilkin, C. N., &
    Ivanov Wilkin, A. (2023). Plant disease detection: Electronic system design empowered
    with artificial intelligence. In 2023 IEEE Conference on AgriFood Electronics
    (CAFE) (pp. 1-5). IEEE.'
  authors:
  - Wu J.
  - Dar U.
  - Anisi M.H.
  - Abolghasemi V.
  - Wilkin C.N.
  - Wilkin A.I.
  citation_count: '0'
  data_sources: New Plant Diseases Dataset, strawberry dataset collected from Wilkin
    & Sons in Tiptree
  description: Today, plant diseases have become a major threat to the development
    of agriculture and forestry, not only affecting the normal growth of plants but
    also causing food safety problems. Hence, it is necessary to identify and detect
    disease regions and types of plants as quickly as possible. We have developed
    a plant monitoring system consisting of sensors and cameras for early detection
    of plant diseases. First, we create a dataset based on the data collected from
    the strawberry plants and then use our dataset as well as some well-established
    public datasets to evaluate and compare the recent deep learning-based plant disease
    detection studies. Finally, we propose a solution to identify plant diseases using
    a ResNet model with a novel variable learning rate which changes during the testing
    phase. We have explored different learning rates and found out that the highest
    accuracy for classification of healthy and unhealthy strawberry plants is obtained
    with the learning rate of 0.01 at 99.77%. Experimental results confirm the effectiveness
    of the proposed system in achieving high disease detection accuracy.
  doi: 10.1109/CAFE58535.2023.10291622
  explanation: This study focused on developing a comprehensive electronic system
    for plant disease monitoring, including sensor data capture and RGB camera image
    acquisition from strawberry plants. Using a modified ResNet model, the system
    achieved high accuracy in detecting plant diseases in both a publicly available
    New Plant Diseases Dataset and a dataset collected from strawberry plants.
  extract_1: '"We designed and implemented a full embedded electronic systems including
    sensors'' data capturing as well as RGB camera image acquisition from strawberry
    plants."'
  extract_2: '"A new modified ResNet model was proposed and applied to both collected
    strawberry plants'' data as well as a public dataset. The obtained results show
    high detection accuracy and hence the effectiveness of the proposed system."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 IEEE Conference on AgriF...
    Plant Disease Detection: Electronic System Design Empowered with Artificial Intelligence
    Publisher: IEEE Cite This PDF Jiayi Wu; Usman Dar; Mohammad Hossein Anisi; Vahid
    Abolghasemi; Chris Newenham Wilkin; Andrey Ivanov Wilkin All Authors 51 Full Text
    Views Abstract Document Sections I. Introduction II. State-of-the-Art III. Proposed
    System V. Conclusion Authors Figures References Keywords Metrics Footnotes Abstract:
    Today, plant diseases have become a major threat to the development of agriculture
    and forestry, not only affecting the normal growth of plants but also causing
    food safety problems. Hence, it is necessary to identify and detect disease regions
    and types of plants as quickly as possible. We have developed a plant monitoring
    system consisting of sensors and cameras for early detection of plant diseases.
    First, we create a dataset based on the data collected from the strawberry plants
    and then use our dataset as well as some well-established public datasets to evaluate
    and compare the recent deep learning-based plant disease detection studies. Finally,
    we propose a solution to identify plant diseases using a ResNet model with a novel
    variable learning rate which changes during the testing phase. We have explored
    different learning rates and found out that the highest accuracy for classification
    of healthy and unhealthy strawberry plants is obtained with the learning rate
    of 0.01 at 99.77%. Experimental results confirm the effectiveness of the proposed
    system in achieving high disease detection accuracy. Published in: 2023 IEEE Conference
    on AgriFood Electronics (CAFE) Date of Conference: 25-27 September 2023 Date Added
    to IEEE Xplore: 30 October 2023 ISBN Information: DOI: 10.1109/CAFE58535.2023.10291622
    Publisher: IEEE Conference Location: Torino, Italy SECTION I. Introduction In
    recent years, plant diseases have become a major challenge to today''s forestry
    development. Plant diseases result in damage to a part of the tissue or organ
    of the plant, until it is destroyed, killed, or aesthetically ruined. Plant diseases
    affect plants from the outside to the inside, from top to bottom, from flowers
    and fruits to the root system in a comprehensive, all-round manner. This not only
    affect the normal growth of plants but can also cause a reduction in the yield
    and quality of agricultural products and, in serious cases, food safety problems
    [1]. Therefore, rapid identification and diagnosis of plant diseases can reduce
    the economic losses caused by plant diseases to the agricultural industry in the
    shortest possible time. Plant disease identification is a technique for processing,
    analysing, and understanding plant image datasets to identify potential kinds
    of disease objects. It is a key process for the timely and effective control of
    plant diseases [2]. Today, there are many different types of plant diseases in
    different stages of growth and in several growing areas, which makes it difficult
    for laymen to accurately identify the types of disease in a short period of time
    and in a large scale. Besides, manual identification has the disadvantages of
    slow identification speed and low accuracy which poses a major challenge in containing
    the outbreak of diseases in agriculture. With the continuous development of deep
    learning from one hand, and increasing the computation power from another hand,
    many researchers have started to study plant disease identification based on deep
    learning with either sensors or image data (or both). Using computer vision technology
    to identify plant disease areas and species can effectively reduce time costs
    and improve the efficiency of agricultural production [2]. Furthermore, with the
    advances in internet of things (IoT) technology, effective and continuous monitoring
    of various systems has become easier and more accessible. This has led to greater
    autonomy of systems in several industries as well as agriculture. The solutions
    which IoT offer are complemented by machine learning and computer vision-based
    techniques to improve the classification and detection performance. This paper
    provides a comprehensive overview of the state-of-the-art in plant disease detection
    with focus on recent developments in hardware design and deep learning approaches.
    Furthermore, it presents the current frequently used plant disease datasets, and
    summarises the current cutting-edge development techniques in the field of plant
    disease identification. It also showcases the details of an electronic IoT- based
    device for acquisition of strawberry plant data as well as the results of two
    deep learning models on the captured plant images to identify the disease. SECTION
    II. State-of-the-Art 2.1 Plant Diseases Datasets According to our investigation
    there exist around 14 available image datasets for plant disease diagnosis. The
    size and quality of the dataset will affect the accuracy of the deep learning
    model. Mostly, a large and high-quality dataset will improve the quality of the
    training process and the accuracy of monitoring of the deep learning model, allowing
    for more accurate identification of different types of plant diseases [3]. Due
    to space restriction in the paper, we have provided a summary of these datasets
    with the key details in Table 1. 2.2 Existing Plant Disease Detection Systems
    There exist limited works reported on developments of plant disease diagnosis
    with on-board hardware. Pankaj et al. proposed an IoT hardware sensor-based Cotton
    Disease prediction using convolutional neural network (CNN). Their IoT gadget
    has different sensors such as temperature, humidity, and PH to collect the data
    to be used for classification [4]. In another work by Mora et al., a plant disease
    detection using the Raspberry Pi 4 was implemented. Not many results are reported
    in this work, however, accuracy around 90% was obtained for prediction of plant
    disease on a private dataset [5]. A diagnostic system implemented on Raspberry
    Pi was proposed for scab and leaf disease detection. The authors used a CNN model
    and four classes of Scab, Black Rot, Cedar Rust, and Healthy were detected [6].
    A smart crop growth monitoring using edge artificial intelligence (AI) was developed
    in [7] where a complex system was implemented to monitor health plants and classify
    the pest and disease severity. They used cryptographic hardware modules, including
    RTEA32, RTEA64, XTEA32 and XTEA64, and used the binarized neural network and achieved
    76.57% accuracy for disease detection on dragon fruits. We can categorise the
    type of plan disease detection into three key types: direct object detection,
    multiclass classification, and binary segmentation. Direct objection detection
    is typically disease identification on a single plant type. A self-constructed
    SPIKE dataset from images of relevant complex wheat fields was used in an object
    detection method based on identifying diseased plants (or parts affected by diseases
    or pathogens) proposed by Hasan, M.M. et al. [8] The model used was an R-CNN architecture
    that generated four different models, four different datasets of training and
    test images based on four different datasets to capture plant diseases at different
    growth stages with an accuracy of 93.4%. Toda Y et al. used the YOLOV3 - DenseNet
    algorithm for direct object detection, focusing on disease object detection concerning
    growing apple leaves, with an accuracy of 95.75%. And using human intervention
    to validate the authenticity of the model and the training dataset, a CNN trained
    using publicly available plant disease image datasets, various neuron and layer
    visualisation methods were applied [9]. Zhang, S. et al. used the GPDCNN algorithm
    for multiclass classification detection of cucumber images, i.e., using different
    stages of the plant for possible disease detection. An accuracy of 94.65% was
    achieved [10]. Hari et al. in 2019 used the PDDNN algorithm for the detection
    of various plant disease images, using TensorFlow as the framework, with an accuracy
    of 86% [11]. As a comparison, Picon et al. also published a paper in 2019 using
    the RESNET-MC1 algorithm for the detection of various plant disease images using
    TensorFlow and Keras as the framework of choice, with an accuracy of 98% [12].
    Howlader et al. use the AlexNet algorithm to detect plant diseases on guava leaves
    with an accuracy of 98.74% [13]. Nagasubramanian et al. used the 3D-CNN algorithm
    to detect plant diseases in soybean using a binary classification method, i.e.,
    only diseased or healthy, without distinguishing between specific growth regions
    and growth stages, with an accuracy of 95.73% [14]. Arunangshu Pal proposed an
    Agricultural Inspection (AgriDet) framework, The framework combines the traditional
    Inception-Visual Geometry Group network (INC-VGGN) and the Kohonen-based deep
    learning network to detect plant diseases and classify the severity of diseased
    plants where the performance of the statistical analysis is validated to demonstrate
    the effectiveness of the results in terms of accuracy, specificity, and sensitivity
    [15]. In the- article by Amal Mathew et al., the support vector machine (SVM)
    classifier was replaced with a voting classifier to classify the data into multiple
    classes. The accuracy of voting and SVM classifiers are compared. The results
    show that the accuracy of the proposed method is improved by 10% [16]. Punam Bedi
    et al. proposed a hybrid system based on convolutional auto-encoder (CAE) and
    CNN that can achieve automatic detection of plant diseases. In the experiment,
    CAE is used to compress the parameters required for training, and the parameters
    required for the hybrid model are reduced. The proposed hybrid model used only
    9914 training parameters. The experiment uses a public dataset called PlantVillage
    to obtain leaf images of peach plants with the training and testing accuracies
    reported at 99.35% and 98.38%, respectively [17]. Abdalla et al. used the VGG16
    Encoder algorithm to detect binary segmentation of 400 oilseed images in two different
    environments with an accuracy of 96% [18]. Table 1 Summary of available public
    plant disease datasets Lin et al. used the U-Net segmentation algorithm to segment
    cucumber leaves with an accuracy of 96.08% [19]. Wiesner-Hanks et al. implemented
    a binary segmentation task to identify maize diseases using the ResNet - Crowdsourced
    algorithm for binary segmentation, which divides the image into homogeneous regions
    according to defined criteria and generates a binary image of the plant disease
    with the highest accuracy rate, i.e. 99.79% [20]. A collective summary of the
    existing methods with relevant details are provided in Table 2. Table 2 Algorithm
    and detection method of the cited article 2.3 Challenges, Perspectives, and Our
    Proposed Solution Looking at the plant disease target detection algorithms in
    recent years, the overall accuracy is high, basically above 90%, and accurate
    detection of cucumber, wheat and various plants can be achieved regardless of
    which detection method is used. In some studies, due to the different data sets
    used, the proposed algorithms ignore disease areas without obvious boundaries
    when identifying them, i.e., they are unable to accurately detect the extent of
    the disease. However, most of the current papers only address plant disease detection
    in a single environment or photographs taken of individual leaves in the laboratory,
    and there is less disease target detection for photographs taken in complex environments
    or in natural scenarios. Difficulties such as lighting, shading, superimposition
    and background bias exist in practical applications, therefore disease identification
    in complex natural conditions is an area of ongoing research. Another key limitation
    of existing system is using pre-developed embedded systems are which negatively
    affects adaptability and flexibility of the system. In this paper, we develop
    a fully scalable system from scratch (including sensors, cameras, connectivity,
    etc.) which can be generalised for several agricultural applications. This system
    mitigates the aforementioned challenge of collecting data/images from an operational
    farm. Using this system, we have collected and processed the “Strawberry Dataset”
    which their details and the results will be discussed next. SECTION III. Proposed
    System In this section, we showcase the details and results of our plant disease
    detection system. As shown in Figure 1(a) and (b), we develop a network of sensors
    and cameras that are wirelessly connected to a base station, continuously monitor
    the conditions of plants, and seamlessly transmit the images and sensors'' data.
    In the following, first, the hardware specifications and design for data/image
    capturing and communication is described. Then, the results of applying deep learning
    models on both collected dataset and also existing datasets are provided. 3.1
    Hardware The imaging system is composed of an SVC3 camera that is able to capture
    images at 2560 x 1920 resolution. The camera features 20x optical zoom as well
    as 255 degrees pan and 120 degrees tilt that enables the capture of high-quality
    close-up images of the plant matter over a large area. A Raspberry Pi based camera
    controller which is deployed on the same Wi-Fi Network as the cameras, requests
    images from each camera at fixed intervals during the day before uploading them
    via a Wi-Fi access point. In contrast to the imaging system, the sensor network
    has been custom designed to meet the needs of this application. A 3D rendering
    of the edge node''s PCB can be seen in Figure 1(c). The ATMega644p microcontroller
    is responsible for interfacing with 7 sensor modules; temperature, pressure, humidity,
    ambient light, U.V light, soil moisture and leaf wetness. The microcontroller
    samples the sensors roughly once every 30 minutes and uses a Semtech SX1262 LoRa
    Figure 1 The proposed IoT-based plant disease detection system implemented at
    Wilkin & Sons in Tiptree. Show All Figure 2 Structure of the proposed model based
    on ResNet. Show All Figure 3 Accuracy ((a) and (b)) and loss function ((c) and
    (d)) performance using the proposed model with different datasets. Show All Transceiver
    to transmit the data to a Dragino LG01-N LoRa gateway which pushes this data to
    a privately hosted server that is responsible for parsing the data and storing
    it in a database hosted on Amazon Web Services. 3.2 Software We used the 9-layer
    structure of the ResNet as a baseline. In this model, each layer feeds into the
    next layer and directly into the layers about 2–3 hops away. Conventional pre-processing
    such as image re-sizing was applied to input images where required. The network
    in this project uses a combination of two convolutional layers and two residual
    blocks, regularising each layer first, then using ReLU as the activation function
    and Max Pooling to reduce the size of the data and increase the speed of computation.
    Furthermore, the Adam Optimiser and the Cross-Entropy loss function were employed
    in this model. The changes made to the dimensions of each layer are indicated
    in the model structure diagram in Figure 2. In this model, the learning rate can
    change with the training rounds, and only the maximum learning rate needs to be
    set when setting the parameters. The model will constantly change the learning
    rate during training and obtain different accuracy rates in each training process.
    After many tries, when training the New Plant Diseases Dataset, we set the max
    learning rate to 0.04 to obtain highest accuracy. When training the strawberry
    dataset, set the learning rate to 0.01 to achieve the highest accuracy. In the
    last layer, the data is flattened, and linear regression is used to classify the
    different types of plant diseases. A total of 6,589,734 parameters were calculated
    to be trained. IV. Performance Evaluation 4.1 Selection of Dataset To provide
    a comprehensive evaluation and the generalisability power of the proposed system,
    we use a widely used public dataset, i.e., “New Plant Diseases Dataset” as well
    as data collected by ourselves at Wilkin & Sons in Tiptree [21] (see Figure 1).
    “New Plant Diseases Dataset” is created using offline augmentation from the original
    PlantVillage Dataset. This dataset consists of about 87K RGB images of healthy
    and diseased crop leaves which is categorized into 38 different classes of 14
    unique plants. The total dataset is divided into 80/20 ratio of training and validation
    set preserving the directory structure. And a new directory containing 33 test
    images is created for prediction purposes. The other dataset, called “strawberry
    dataset”, contains healthy strawberries and 2 types of diseases of strawberry,
    including Strawberry Leaf scorch and Strawberry Mildew. Each type contains about
    2000 pictures of strawberries. 4.2 Experimental Results To measure the accuracy,
    we have used cross-validation and considered 80% of data for training and 20%
    for testing. We used batch size 32 and run the model for 10 number of epochs.
    Figure 3(a) and (b) show the changing trend of accuracy with rounds for the two
    datasets, respectively. It can be observed that the accuracy fluctuates but the
    overall trend is increasing and finally reaches 97.36% and 99.77%. Figure 3(c)
    and (d) depict the change trend of loss with rounds, and the overall loss is low
    and stable. Instead of using a fixed learning rate in this project, we use a learning
    rate scheduler which will change the learning rate after each batch of training.
    There are several strategies for changing the learning rate during training, i.e.,
    starting with a low learning rate, gradually increasing it to a high learning
    rate in batches over about 30% of the cycles, and then gradually reducing it to
    a very low value over the remaining cycles, so only the maximum learning rate
    needs to be set when setting the parameters. The results in these figures show
    that in the 10 running epochs, the accuracy of the validation set for New Plant
    Diseases Dataset finally reaches 97.69% and 99.77% for Strawberry dataset. SECTION
    V. Conclusion In this paper, a comprehensive collection and comparison among existing
    plant disease datasets were provided. We designed and implemented a full embedded
    electronic systems including sensors'' data capturing as well as RGB camera image
    acquisition from strawberry plants. A new modified ResNet model was proposed and
    applied to both collected strawberry plants'' data as well as a public dataset.
    The obtained results show high detection accuracy and hence the effectiveness
    of the proposed system. We further aim to expand the number of data collection
    nodes throughout the farm, enriching the dataset and developing a fusion model
    to analyse both sensors'' data and image data simultaneously to provide early
    and accurate prediction of potential diseases. ACKNOWLEDGEMENT This research was
    funded by a Knowledge Transfer Partnership (KTP) from Innovate UK (Partnership
    No: 12298), between Wilkin & Sons Ltd and the University of Essex. Authors Figures
    References Keywords Metrics Footnotes More Like This Radar Signal Abnormal Point
    Classification based on Camera-Radar Sensor Fusion 2023 International Conference
    on Artificial Intelligence in Information and Communication (ICAIIC) Published:
    2023 Three-dimensional imaging sensor system using an ultrasonic array sensor
    and a camera SENSORS, 2010 IEEE Published: 2010 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Wu et al., 2023)
  journal: 2023 IEEE Conference on AgriFood Electronics, CAFE 2023 - Proceedings
  key_findings: '- The proposed system achieved high accuracy in detecting plant diseases
    in both the New Plant Diseases Dataset and the strawberry dataset.

    - The system is scalable and can be used to monitor large areas of farmland.

    - The system can be used to detect diseases early, before they become a major
    problem.'
  limitations: The study used a limited dataset from a single farm, which may not
    generalize to other farms or crops. The system was not tested in real-world conditions,
    so its performance in practical applications is unknown.
  main_objective: To develop and evaluate an electronic system for plant disease monitoring
    using sensor data capture and RGB camera image acquisition.
  relevance_evaluation: This study is highly relevant to the specific point of integrating
    high-resolution cameras and computer vision algorithms for visual monitoring of
    crop growth and disease detection. The authors designed and implemented a system
    that meets this requirement and evaluated its performance on two datasets.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Tiptree, United Kingdom
  technologies_used: ResNet model, Raspberry Pi, LoRa transceiver, sensors (temperature,
    pressure, humidity, ambient light, U.V light, soil moisture, leaf wetness)
  title: 'Plant Disease Detection: Electronic System Design Empowered with Artificial
    Intelligence'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Sharma, G., Anand, V., Malhotra, S., Kukreti, S., & Gupta, S. (2024).
    DeepLeafNet: Multiclass Classification of Soybean Plant Leaves with ResNet50V2
    for Enhanced Crop Monitoring and Disease Detection. In 2023 3rd International
    Conference on Smart Generation Computing, Communication and Networking (SMART
    GENCON) (pp. 1-6). IEEE. https://doi.org/10.1109/SMARTGENCON60755.2023.10442288'
  authors:
  - Sharma G.
  - Anand V.
  - Malhotra S.
  - Kukreti S.
  - Gupta S.
  citation_count: '0'
  data_sources: Soybean Disease Leaf Image Classification Dataset
  description: The soybean, which is a fundamental component of worldwide agriculture,
    plays a crucial role in meeting the global need for protein and oil. Nevertheless,
    the spread of many diseases poses a significant risk to both the quantity and
    quality of soybean harvests. The timely and accurate categorization of diseases
    is crucial for the successful implementation of crop management strategies. This
    study aims to enhance the categorization of soybean diseases by addressing ten
    different disease classes using the ResNet50V2 deep learning model. In order to
    accomplish this objective, the Soybean Disease Leaf Image Classification Dataset
    has been employed for both the purposes of training as well as evaluating the
    model. The ResNet50V2 model's architectural depth and inclusion of skip connections
    augment its capacity to grasp complicated patterns present in leaf images, hence
    bolstering its efficacy in performing robust categorization. The experimental
    findings provide evidence for the effectiveness of our methodology since the model
    attains a notable training accuracy of 97% and validation accuracy rate of 96%
    when diagnosing illnesses in soybean plant leaves with a minimum loss of 0.05
    and 0.38. Moreover, this research endeavor makes a valuable contribution to the
    expanding domain of computer vision in the agricultural sector, facilitating the
    automation and enhancement of plant disease diagnosis processes.
  doi: 10.1109/SMARTGENCON60755.2023.10442288
  explanation: The study proposes a deep learning approach using a fine-tuned ResNet50V2
    model to enhance the classification of soybean leaf diseases into ten distinct
    categories. The model is trained and evaluated on a comprehensive dataset of soybean
    leaf images, demonstrating high accuracy in disease identification.
  extract_1: The significant findings of the study demonstrate the effectiveness of
    the ResNet50V2 model in categorizing soybean leaf diseases, achieving a training
    accuracy of 97.85% and a validation accuracy of 96.00%, with minimal loss.
  extract_2: The study highlights the importance of data augmentation techniques,
    such as flipping and rotating images, in enhancing the model's generalization
    capabilities, leading to improved performance on the validation set.
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 3rd International Confer...
    DeepLeafNet: Multiclass Classification of Soybean Plant Leaves with ResNet50V2
    for Enhanced Crop Monitoring and Disease Detection Publisher: IEEE Cite This PDF
    Gunjan Sharma; Vatsala Anand; Sonal Malhotra; Sanjeev Kukreti; Sheifali Gupta
    All Authors 14 Full Text Views Abstract Document Sections I. Introduction II.
    Literature Review III. Proposed Methodology IV. Results and Discussion V. Conclusion
    Authors Figures References Keywords Metrics Abstract: The soybean, which is a
    fundamental component of worldwide agriculture, plays a crucial role in meeting
    the global need for protein and oil. Nevertheless, the spread of many diseases
    poses a significant risk to both the quantity and quality of soybean harvests.
    The timely and accurate categorization of diseases is crucial for the successful
    implementation of crop management strategies. This study aims to enhance the categorization
    of soybean diseases by addressing ten different disease classes using the ResNet50V2
    deep learning model. In order to accomplish this objective, the Soybean Disease
    Leaf Image Classification Dataset has been employed for both the purposes of training
    as well as evaluating the model. The ResNet50V2 model''s architectural depth and
    inclusion of skip connections augment its capacity to grasp complicated patterns
    present in leaf images, hence bolstering its efficacy in performing robust categorization.
    The experimental findings provide evidence for the effectiveness of our methodology
    since the model attains a notable training accuracy of 97% and validation accuracy
    rate of 96% when diagnosing illnesses in soybean plant leaves with a minimum loss
    of 0.05 and 0.38. Moreover, this research endeavor makes a valuable contribution
    to the expanding domain of computer vision in the agricultural sector, facilitating
    the automation and enhancement of plant disease diagnosis processes. Published
    in: 2023 3rd International Conference on Smart Generation Computing, Communication
    and Networking (SMART GENCON) Date of Conference: 29-31 December 2023 Date Added
    to IEEE Xplore: 28 February 2024 ISBN Information: DOI: 10.1109/SMARTGENCON60755.2023.10442288
    Publisher: IEEE Conference Location: Bangalore, India SECTION I. Introduction
    Agriculture serves as the primary prerequisite for meeting the global demand for
    food. The principal factors contributing to the discrepancy in agricultural commodity
    consumption and supply can be attributed to the loss of arable land and the expansion
    of people [1]. The soybean, commonly known as the “golden bean,” is a crucial
    agricultural commodity that plays a significant role in supplying necessary protein
    and oil for human and animal dietary needs. This particular factor assumes a pivotal
    position within the realm of agriculture, as it effectively tackles the global
    demand for sustenance [2]. Nevertheless, soybean fields are consistently confronted
    with the enduring challenge of plant diseases, which have the potential to detrimentally
    impact both crop yields and quality. These diseases pose a significant problem
    not only for agricultural practitioners but also have broader consequences for
    global food security and economic stability. The efficient management of many
    disorders necessitates precise and prompt categorization [3]. The integration
    of technology and agriculture has auspicious prospects. The use of computer vision
    and deep learning techniques in agricultural operations facilitates the automated
    identification and control of diseases, signifying a notable advancement in the
    realm of sustainable farming. The objective of this study is to enhance the classification
    of soybean diseases, specifically by differentiating between ten distinct disease
    groups. The primary tool utilized in our study is the fine-tuned ResNet50V2 model,
    a widely recognized deep-learning architecture renowned for its exceptional performance
    in the task of picture categorization [4]. In order to accomplish this objective,
    the Soybean Disease Leaf Image Classification Dataset has been employed, which
    is a comprehensive compilation of annotated soybean leaf pictures. This dataset
    serves as the fundamental resource for training and evaluating our model. The
    dataset comprises images depicting ten distinct categories of soybean illnesses,
    encompassing bacterial blight, brown spot, mosaic virus, yellow mosaic etc. The
    images exhibit superior resolution and are meticulously annotated, rendering them
    highly suitable for integration into machine learning (ML) [5] algorithms designed
    for the purpose of plant disease categorization. The depth and complicated design
    of this model, together with the use of skip connections, enable it to effectively
    identify minor abnormalities in leaf images that are indicative of disease. SECTION
    II. Literature Review Agriculture plays a pivotal role in the development and
    sustenance of India''s economy and society Within the field of agriculture, numerous
    systems have been suggested as potential solutions or mitigating measures to address
    prevailing issues. These systems leverage image processing techniques and various
    automated classification tools. The study conducted by [6] utilized a residual
    network with an attention module, incorporating a hybrid attention mechanism,
    to address the issue of residual error in a neural network. The objective was
    to recognize plant diseases using publicly available data sets from Plant Village,
    specifically focusing on apple and cherry crops as well as 10 other crop types
    including corn, grapes, and citrus. A comprehensive evaluation was conducted,
    testing the model on 60 different diseases. The results demonstrated the 92.08%
    accuracy. The authors of [7] developed a parallel pooling attention module employing
    ResNet50. They suggested the residual attention network framework to accurately
    diagnose four potato illnesses in the Plant Village dataset, achieving an accuracy
    of 93.86%. The CNN with multi-feature fusion was utilized by [8] to identify and
    classify 32 types of foliage in the Flavin collection and 189 types of foliage
    in the MEW 2014 database. The overall correct identification rates achieved were
    93.25% and 96.37% respectively. The process for classifying Anthracnose and Downey
    Mildew, which are diseases affecting watermelon leaves, was proposed by [9]. In
    this study, the median filter is employed for the purposes of noise reduction
    and segmentation. A neural network-based pattern recognition toolkit is employed
    for the purpose of classification. The accuracy of this study was determined to
    be 75.9% by analyzing the mean RGB color constituent. In a study conducted by
    [10], a technique utilizing CNN was presented for the classification and identification
    of soybean leaves. The utilization of CNN is employed in the process of backpropagation
    to train algorithms, hence enhancing accuracy. Consequently, the whole system
    yields improved outcomes characterized by heightened accuracy. To achieve the
    segmentation of the affected region, the process of texture segmentation is carried
    out by employing the technique of k-means clustering. The Google Net architecture,
    initially proposed by the authors referenced in [11], was trained on a dataset
    consisting of 550 picture samples of soybean leaflets. These samples were categorized
    into two classes: unhealthy and healthy. The unhealthy class was further divided
    into three subclasses, namely septoria brown spot, bacterial blight, and frog
    eye leaf spot. The training process employed a deep TL approach. The study employed
    a five-fold cross-validation technique to identify three classes of unhealthy
    images and one class of healthy images. The chosen already trained Google Net-CNN
    framework achieved an accuracy of 96.25%. In their study, the authors of reference
    [12] opted to substitute the conventional convolution technique with EfficientNet,
    a method that effectively decreases the number of parameters and processing expenses.
    The study utilized data obtained from the Indian Institute of Soybean Research
    and employed web scraping techniques to collect information on a diverse range
    of plant varieties and disease classifications. Two models were developed: one
    utilizing a CNN and the other using EfficientNetB0. The accuracy rates of illness
    characterization that these models attained were 84% for the CNN-based approach
    and 90% for the EfficientNetBO model. This research done by [13] addresses the
    challenge posed by a limited dataset pertaining to soybean leaf diseases through
    the creation of a synthetic dataset. In order to accurately identify soybean leaf
    diseases within intricate environments, the authors propose the utilization of
    a multi-feature fused Faster R-CNN approach. The model demonstrated a notable
    average mean accuracy of 83.34% when evaluated on an actual-life dataset. The
    present study introduces a refined ResNet50V2 [14] model that has been pre-trained
    for the purpose of classifying various diseases affecting soybean plant leaves
    into 10 distinct categories. Utilizing the Soybean Disease Leaf Image Classification
    Dataset, the model''s performance has been trained and tested. SECTION III. Proposed
    Methodology A. Dataset Employed The Soybean Disease Leaf Image Classification
    Dataset [15] has been employed to conduct this categorization task. The dataset
    comprises a collection of images depicting damaged soybean plant leaves. The dataset
    comprises images depicting ten distinct categories of soybean illnesses, encompassing
    bacterial blight (BLt), brown spot (BS), mosaic virus (MV), southern blight (SB),
    sudden death syndrome (SDS), yellow mosaic (YM), Crestamento (CR), Ferrugen (FR),
    Powdery Milddew (PMd) and Septoria (SP). The images exhibit a commendable level
    of resolution and are appropriately annotated, rendering them very suitable for
    integration into machine-learning frameworks aimed at plant disease categorization.
    Fig. 1 displays the sample images from the dataset. Fig. 1. Sample images of soybean
    plant leave diseases [15] Show All Fig. 2 displays the distribution of images
    class-wise. Fig. 2. Distribution of images class-wise in dataset Show All B. Image
    Pre-Processing All the images are pre-processed first before they are fed to the
    network. This helps in reducing the noise and any type of discrepancy in the images.
    The initial phase in the data processing pipeline entails the loading and preparation
    of the image data. The images undergo a resizing step to achieve a consistent
    size of 224×224 pixels. Then these images pixel values are rescaled between 0
    and 1. They are then organized into batches, with each batch. The images are treated
    as RGB color [16] images for subsequent processing. The assignment of labels is
    derived from the hierarchical arrangement of the directory structure. These labels
    are then transformed into a one-hot encoding format, which aids in the process
    of categorical classification. During the training process, the data is randomly
    shuffled to introduce randomization. Additionally, a random seed is established
    to ensure reproducibility. Data Augmentation is applied to increase the images
    [17]. The images are randomly flipped horizontally and vertically to provide a
    variety of images, this will help in effective feature training of the images.
    Fig. 3 gives the details of the number of images before and after augmentation
    is applied to the images. A segment comprising 20% of the data is allocated for
    the purpose of validation, and the datasets are thereafter partitioned into distinct
    subsets for training and validation. The datasets mentioned above are commonly
    employed for the purpose of training deep learning models in order to accomplish
    picture classification tasks. The training dataset is utilized for training the
    model, while the validation dataset is employed to evaluate the model''s performance
    during the training process. Fig. 3. Class-wise image analysis after augmentation
    Show All C. Proposed Methodology This work concentrates on the effective categorization
    of diseases related to soybean plant leaf disease into ten main classes. The approach
    presented for the classification of soybean plant leaf diseases is organized into
    a systematic sequence of clearly outlined procedures [18]. Fig. 4 showcases the
    proposed framework for this categorization study. Fig. 4. Proposed framework Show
    All The method begins with the collecting of data, namely a dataset consisting
    of images of soybean leaves. Following that, the gathered photos undergo a rigorous
    preprocessing phase. In this stage, the photos undergo a process of resizing to
    a standardized resolution of 224×224 pixels and are then rescaled to ensure uniformity.
    In order to enhance the model''s generalization and incorporate unpredictability
    [19], data augmentation techniques such as rotations, flips, and brightness modifications
    are employed. After undergoing preprocessing, the dataset is subjected to shuffling,
    resulting in a randomized order of the data points. Subsequently, the dataset
    is divided into two distinct subsets, namely the training set and the validation
    set. This partitioning is carried out in an 8:2 ratio, with the training set including
    80% of the data and the validation set containing the remaining 20%. The process
    of partitioning facilitates both the training of models and the evaluation of
    their performance. The subsequent step in the process is the feature extraction
    phase, in which a pre-existing CNN model, specifically ResNet50V2, is utilized
    to extract elevated-level features from the images that have been preprocessed
    and augmented. Following this, the model undergoes fine-tuning by integrating
    supplementary layers such as batch normalization [20], dropout layers to address
    overfitting concerns, and dense layers for the purpose of classification. The
    model is subsequently trained using the training dataset and evaluated by testing
    it on a separate validation set in order to measure its performance. Finally,
    the model utilizes the acquired features to categorize soybean leaf images into
    the pre-established ten disease categories. In brief, this proposed methodology
    involves a thorough data preparation stage, followed by feature extraction using
    a pre-trained model. The extracted features are then fine-tuned, and the model
    is trained and tested. Finally, the acquired feature knowledge is utilized for
    the purpose of accurately classifying soybean leaf diseases. D. Experimental Setup
    Within the framework of the study, the computational capabilities provided by
    Google Colab''s complimentary access to GPU resources is utilized [21]. The primary
    objective of this research centered on the field of image processing. In pursuit
    of this objective, several libraries sourced from Keras are effectively included
    in our experimental framework. The optimization of model performance in training
    circumstances involves the utilization of specified hyperparameters. The selection
    of a batch size of 64 was made with the intention of striking a delicate balance
    between computational efficiency and the model''s ability to converge [22]. During
    the optimization process, a learning rate of 0.0001 was initially utilized to
    regulate the magnitude of each step taken. The choice to utilize the RMSprop optimizer
    was driven by its adjustable learning rate properties, which mitigate the potential
    for overshooting during the optimization iteration. The ‘categorical_crossentropy’
    loss function is utilized due to its suitability for handling many classes. This
    loss function effectively measures the disparities between the predicted probabilities
    and the actual class probabilities. In order to optimize model convergence, the
    training process is run for 25 epochs, hence enabling the model to acquire significant
    insights from the available data. To mitigate the possible problem of overfitting
    during the training of the model, a dropout rate of 0.5 is incorporated. This
    regularization strategy is implemented to safeguard against the model''s tendency
    to learn irrelevant patterns or noise present in the training data. The hyperparameters
    and their corresponding information are concisely presented in Table I. The robustness
    of this experimental setup, which includes the utilization of GPU resources, a
    diverse dataset, and carefully selected hyperparameters, is essential for guaranteeing
    the effectiveness and generalizability of our model in the specific task of classifying
    soybean plant leaf diseases within real-world agricultural settings. Table I.
    Model hyperparameters utilized during the training of the model SECTION IV. Results
    and Discussion After training of the model, the performance is evaluated in terms
    of accuracy and loss. The model is trained for 25 epochs during which the accuracy
    and loss for training and validation are recorded. Table II presents a comprehensive
    overview of the training and validation performance of a machine learning model
    during several training epochs. Table II. Performance matrix during training of
    the model The evaluation of these metrics is essential in order to measure the
    learning progress and generalization capabilities of the model as it iteratively
    processes the information. During the initial stage, referred to as Epoch 1, the
    model commences with a TL value of 1.44 and a TA rate of 63.15%. Concurrently,
    the VL exhibits a comparatively lower value of 0.49, accompanied by a VA of 86.25%.
    The initial stage of training is indicative of the model''s performance at the
    outset. As the training program advances, a number of significant patterns become
    apparent. Significantly, the TL exhibits a consistent decline, starting at 1.44
    in Epoch 1 and gradually reducing to 0.05 by Epoch 25. This decrease in error
    suggests that the model is improving its ability to minimize the discrepancy between
    its predictions and the observed training data. The TA demonstrates a significant
    improvement across the epochs, rising from an initial value of 63.15% to a final
    value of 97.85%. This increase in accuracy indicates the enhanced capability of
    the model to accurately categorize the training data. On the other hand, the evaluation
    of the model''s generalization capacity beyond the training data involves closely
    monitoring the VL and VA on the validation dataset. The validation loss exhibits
    a slow decline, decreasing from 0.49 to 0.39. This trend indicates that the model
    is progressively improving its ability to minimize errors when presented with
    new, unseen data. The VA demonstrates a positive trajectory, commencing at 86.25%
    and progressively increasing to 96.00% by the 25th epoch. The observed increase
    in VA suggests that the model is becoming more proficient in accurately categorizing
    data that it has not encountered before, hence showcasing its capacity for strong
    generalization. Fig. 5. Graphical presentation of model accuracy Show All Fig.
    5 gives the graphical presentation for model accuracy. From the fig, it can be
    seen that the TA is increasing swiftly whereas the VA value drops in the start
    then it starts increasing. Fig. 6 displays the model loss presentation. From fig.
    6 it can be seen that both the losses are diminishing fluently. Fig. 6. Graphical
    presentation of model loss Show All In brief, the table gives an overview of the
    training and validation performance of the model during several training epochs.
    The observed decline in TL and rise in TA indicate enhanced learning performance
    on the training dataset. Conversely, the diminishing VL and escalating VA underscore
    the model''s rising capacity to generalize and make accurate predictions on novel,
    unidentified information. The observed progressive enhancement highlights the
    model''s efficacy in accurately categorizing soybean plant leaf diseases using
    the available dataset. A. Visualization of Results This section displays the classification
    results achieved after the training and testing of the model. Fig. 7 showcases
    the results achieved during testing of the model. Fig. 7. Visualization of classification
    results Show All From fig. 7, it can be noticed that the proposed ResNet50V2 model
    is performing well and categorizing the images correctly. SECTION V. Conclusion
    In summary, by employing transfer learning with the ResNet50V2 architecture to
    create a strong model, our study has effectively tackled the crucial problem of
    classifying illnesses of soybean plants'' leaves. The foundation for more precise
    and effective disease detection and management in soybean crops has been established
    by this study by compiling a large dataset of images of soybean leaf disease and
    categorizing it into ten different groups. With low loss, our model demonstrated
    remarkable performance, with a validation accuracy of 96% and a training accuracy
    of 97.85%. The model''s exceptional performance was largely attributed to the
    use of the RMSprop optimizer, a batch size of 64, and 25 training epochs. The
    application of a categorical cross-entropy loss function improved the model''s
    capacity to accurately classify the various diseases that affect soybean leaves.
    In addition to advancing agriculture by offering a useful tool for diagnosing
    and managing soybean diseases, this research shows how deep learning and transfer
    learning approaches may be applied to solve challenging real-world issues. In
    order to handle a larger spectrum of plant diseases, the model suggested in this
    study can be expanded upon and improved in subsequent research, resulting in more
    effective and sustainable agricultural methods. Authors Figures References Keywords
    Metrics More Like This A Two-stage Approach for Plant Disease Classification Based
    on Deep Neural Networks and Transfer Learning 2022 12th International Conference
    on Electrical and Computer Engineering (ICECE) Published: 2022 Crop Recommendation
    using Machine Learning and Plant Disease Identification using CNN and Transfer-Learning
    Approach 2022 IEEE Conference on Interdisciplinary Approaches in Technology and
    Management for Social Innovation (IATMSI) Published: 2022 Show More IEEE Personal
    Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED
    DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION
    TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732
    981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility
    | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap |
    IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s largest
    technical professional organization dedicated to advancing technology for the
    benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Sharma, Anand, Malhotra, Kukreti, & Gupta, 2024)
  journal: 2023 3rd International Conference on Smart Generation Computing, Communication
    and Networking, SMART GENCON 2023
  key_findings: The proposed ResNet50V2 model achieved high accuracy in classifying
    soybean leaf diseases with minimal loss, demonstrating the effectiveness of deep
    learning for disease identification. Data augmentation techniques improved the
    model's generalization capabilities, leading to better performance on unseen data.
  limitations: The study does not explicitly mention the specific technologies or
    methods used for data collection or image acquisition. The study is limited to
    the classification of soybean leaf diseases and may not be directly applicable
    to other crop types or diseases.
  main_objective: To enhance the classification of soybean leaf diseases into ten
    distinct categories using a fine-tuned ResNet50V2 deep learning model.
  relevance_evaluation: This study is highly relevant to the point of focus on integrating
    high-resolution cameras and computer vision for visual monitoring of crop growth
    and disease detection in automated irrigation systems. The proposed deep learning
    model utilizes computer vision techniques to classify soybean leaf diseases, which
    can be applied in real-world scenarios to automate disease detection in soybean
    crops.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: ResNet50V2 deep learning model, transfer learning, image augmentation,
    computer vision
  title: 'DeepLeafNet: Multiclass Classification of Soybean Plant Leaves with ResNet50V2
    for Enhanced Crop Monitoring and Disease Detection'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Selea T.
  citation_count: '2'
  description: 'With the increasing volume of collected Earth observation (EO) data,
    artificial intelligence (AI) methods have become state-of-the-art in processing
    and analyzing them. However, there is still a lack of high-quality, large-scale
    EO datasets for training robust networks. This paper presents AgriSen-COG, a large-scale
    benchmark dataset for crop type mapping based on Sentinel-2 data. AgriSen-COG
    deals with the challenges of remote sensing (RS) datasets. First, it includes
    data from five different European countries (Austria, Belgium, Spain, Denmark,
    and the Netherlands), targeting the problem of domain adaptation. Second, it is
    multitemporal and multiyear (2019–2020), therefore enabling analysis based on
    the growth of crops in time and yearly variability. Third, AgriSen-COG includes
    an anomaly detection preprocessing step, which reduces the amount of mislabeled
    information. AgriSen-COG comprises 6,972,485 parcels, making it the most extensive
    available dataset for crop type mapping. It includes two types of data: pixel-level
    data and parcel aggregated information. By carrying this out, we target two computer
    vision (CV) problems: semantic segmentation and classification. To establish the
    validity of the proposed dataset, we conducted several experiments using state-of-the-art
    deep-learning models for temporal semantic segmentation with pixel-level data
    (U-Net and ConvStar networks) and time-series classification with parcel aggregated
    information (LSTM, Transformer, TempCNN networks). The most popular models (U-Net
    and LSTM) achieve the best performance in the Belgium region, with a weighted
    F1 score of 0.956 (U-Net) and 0.918 (LSTM).The proposed data are distributed as
    a cloud-optimized GeoTIFF (COG), together with a SpatioTemporal Asset Catalog
    (STAC), which makes AgriSen-COG a findable, accessible, interoperable, and reusable
    (FAIR) dataset.'
  doi: 10.3390/rs15122980
  explanation: The purpose of AgriSen-COG is to provide a large-scale and high-quality
    dataset for crop type mapping using deep learning techniques. It incorporates
    multi-country, multi-year, and anomaly detection data, making it useful for model
    training and research in the field of crop monitoring. Additionally, it includes
    data from five different European countries (Austria, Belgium, Spain, Denmark,
    and the Netherlands), enabling analysis of crop phenology and seasonal variability.
  extract_1: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)
  extract_2: The AgriSen-COG dataset contains a large number of fields and labels,
    making it suitable for training deep learning models for crop type mapping. The
    dataset is also available in a variety of file formats, making it accessible to
    a wide range of users. Additionally, the dataset is updated annually, ensuring
    that it remains relevant to the latest advances in crop monitoring.
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Remote Sensing All Article Types Advanced   Journals
    Remote Sensing Volume 15 Issue 12 10.3390/rs15122980 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editors Enrico
    Corrado Borgogno Mondino Filippo Sarvia Samuele De Petris Show more... Subscribe
    SciFeed Recommended Articles Related Info Link More by Author Links Article Views
    1763 Citations 2 Table of Contents Abstract Introduction Crop Datasets for ML/DL
    Applications Popular DL Methods for Crop Type Mapping Anomaly Detection The AgriSen-COG
    Dataset Experimental Results Discussion Conclusions Funding Data Availability
    Statement Conflicts of Interest References share Share announcement Help format_quote
    Cite question_answer Discuss in SciProfiles thumb_up Endorse textsms Comment first_page
    settings Order Article Reprints Open AccessArticle AgriSen-COG, a Multicountry,
    Multitemporal Large-Scale Sentinel-2 Benchmark Dataset for Crop Mapping Using
    Deep Learning by Teodora Selea Faculty of Mathematics and Informatics, West University
    of Timisoara, 300223 Timisoara, Romania Remote Sens. 2023, 15(12), 2980; https://doi.org/10.3390/rs15122980
    Submission received: 12 May 2023 / Revised: 3 June 2023 / Accepted: 5 June 2023
    / Published: 7 June 2023 (This article belongs to the Special Issue Remote Sensing
    and Associated Artificial Intelligence in Agricultural Applications) Download
    keyboard_arrow_down     Browse Figures Review Reports Versions Notes Abstract
    With the increasing volume of collected Earth observation (EO) data, artificial
    intelligence (AI) methods have become state-of-the-art in processing and analyzing
    them. However, there is still a lack of high-quality, large-scale EO datasets
    for training robust networks. This paper presents AgriSen-COG, a large-scale benchmark
    dataset for crop type mapping based on Sentinel-2 data. AgriSen-COG deals with
    the challenges of remote sensing (RS) datasets. First, it includes data from five
    different European countries (Austria, Belgium, Spain, Denmark, and the Netherlands),
    targeting the problem of domain adaptation. Second, it is multitemporal and multiyear
    (2019–2020), therefore enabling analysis based on the growth of crops in time
    and yearly variability. Third, AgriSen-COG includes an anomaly detection preprocessing
    step, which reduces the amount of mislabeled information. AgriSen-COG comprises
    6,972,485 parcels, making it the most extensive available dataset for crop type
    mapping. It includes two types of data: pixel-level data and parcel aggregated
    information. By carrying this out, we target two computer vision (CV) problems:
    semantic segmentation and classification. To establish the validity of the proposed
    dataset, we conducted several experiments using state-of-the-art deep-learning
    models for temporal semantic segmentation with pixel-level data (U-Net and ConvStar
    networks) and time-series classification with parcel aggregated information (LSTM,
    Transformer, TempCNN networks). The most popular models (U-Net and LSTM) achieve
    the best performance in the Belgium region, with a weighted F1 score of 0.956
    (U-Net) and 0.918 (LSTM).The proposed data are distributed as a cloud-optimized
    GeoTIFF (COG), together with a SpatioTemporal Asset Catalog (STAC), which makes
    AgriSen-COG a findable, accessible, interoperable, and reusable (FAIR) dataset.
    Keywords: benchmark dataset; crop monitoring; crop detection; deep learning; image
    segmentation; multitemporal analysis; crop classification; agricultural application;
    Sentinel-2; common agricultural policy (CAP) 1. Introduction Artificial intelligence
    (AI) has become a hot topic in the past decade, and since 2015, it has been an
    important method used in the Earth observation (EO) community [1]. The adoption
    of AI techniques—machine learning (ML) and deep learning (DL)—for EO-related use
    cases is a consequence of the increasing volume of publicly available satellite
    data (e.g., Copernicus Sentinels), and thus, a need to process it. Possible use
    cases are land cover and land use, deforestation, urban mapping, and agriculture.
    In this paper, we focus on the latter, particularly on the task of crop type mapping.
    Crop type mapping supports the crop monitoring task, which is helpful for agricultural
    insurance or implementing common agricultural policies (CAP). Applying crop monitoring
    at a global scale can be conducted with the help of satellite data, which offers
    global coverage. Monitoring crops is a crucial factor for further agricultural
    development, as we need a production increase of 60% to serve the demand of current
    population growth (Food and Agriculture Organization (FAO) of the United Nations
    study [2]). However, the increase in yield needs to be sustainable without damaging
    natural resources or the environment (Climate Sustainable Development Goals (SGD)
    [3]). To properly put these two objectives together, global and joint monitoring
    must be established. In the context of AI methods, the crop type mapping use case
    is assimilated with the semantic segmentation task from computer vision (CV),
    described as attaching a class label to every pixel from the image. It can also
    be incorporated into a classification problem by aggregating parcel data and offering
    one label for each parcel. However, as opposed to standard 3-channel (red, green,
    blue) images, satellite data enrich the input by including multitemporal and multichannel
    information. For the selected use case, the temporal data are beneficial for identifying
    crop phenology, and therefore, for better classifying it. Furthermore, the multichannel
    satellite data refine the crop information, as the various spectra capture different
    characteristics. The advancement of AI methods is correlated to the training datasets’
    availability, size, and quality. While other domains (e.g., computer vision) already
    have several benchmarking datasets (e.g., ImageNet [4], CityScape [5]), there
    is still a lack of large-scale, high-quality EO datasets. Even though large volumes
    of satellite data are publicly available, the challenge comes from creating ground
    truth (GT) data. For crop type mapping, GT information for dataset creation may
    be extracted from land parcel identification systems (LPIS). Several European
    countries have made their LPIS information public in the past few years, and therefore,
    there has been an increase in crop-related datasets for the European area. However,
    since the LPIS is created individually by each country with data received from
    farmers, it raises the following problems: no crop naming conventions, different
    languages used, and errors in collecting the crop type for each parcel. Existing
    datasets include information from only one or two EU countries without a standard
    way of storing and distributing the datasets. Datasets that combine knowledge
    from several areas would improve the state-of-the-art methods related to domain
    adaptation. A unified way of accessing the dataset leads to ease of usage and
    faster integration with DL techniques. In addition, there is also a lack of a
    methodology for LPIS processing so that the existing training crop-type datasets
    may be easily extended once new data become available. Moreover, as LPIS is created
    based on farmer data, it is prone to human error, which may lead to incorrect
    labels affecting the performance of the ML/DL model; therefore, the data need
    to be curated. In this paper, we propose AgriSen-COG, a new large-scale crop-type
    mapping dataset with the following characteristics: (1) it is based on publicly
    available data only (Sentinel-2 and LPIS), making it easily extensible; (2) it
    includes data from five different European countries (Austria, Belgium, Spain,
    Denmark, and the Netherlands), targeting the problem of domain adaptation; (3)
    it incorporates an anomaly detection based on autoencoder as preprocessing step
    that lowers the amount of mislabeled information for GT; (4) it is multitemporal
    and multiannual, incorporating crop phenology and seasonal variability; (5) it
    includes pixel-based data to account for the crop parcel’s spatiality; (6) it
    incorporates time-series data for crop classification (when the parcel geometry
    is known). AgriSen-COG is distributed using accessible formats such as COGs, Zarr,
    and Parquet and is indexed under SpatioTemporal Assets Catalogues (STAC). By using
    COGs, we ensure an easy way of accessing the data without the need to download
    the entire dataset. STAC enables a standard way to discover and describe our dataset,
    making it reusable and interoperable. Our main contributions are as follows: We
    created AgriSen-COG, a large-scale benchmark dataset for crop type mapping, including
    the largest number of different European countries (five), designed for AI applications.
    We introduce a methodology for LPIS processing to obtain GT for a crop-type dataset,
    useful for further extensions of the current dataset. We incorporate an anomaly
    detection method based on autoencoders and dynamic time warping (DTW) distance
    as a preprocessing step to identify mislabeled data from LPIS. We experiment with
    popular DL models and provide a baseline, showing the generalization capabilities
    of the models on the proposed dataset across space (multicountry) and time (multitemporal).
    We provide the LPIS preprocessing, anomaly detection, training/testing code, and
    our trained models to enable further development. The remainder of this article
    starts with a review of other datasets for crop type mapping. Next, we present
    popular ML/DL techniques for our selected use case. Furthermore, we continue with
    the proposed methodology for dataset creation and describe how we applied anomaly
    detection to curate the LPIS information. Finally, we present our experimental
    results, followed by a discussion, and summarize our conclusions. 2. Crop Datasets
    for ML/DL Applications Benchmark datasets play a significant role in developing
    ML/DL methods. New methods need to be assessed using the same input dataset to
    eliminate the bias given by learning from different data. The progress in computer
    vision deep learning methods is partially due to numerous large benchmarking datasets
    (e.g., ImageNet [4], MS COCO [6], Cityscape [5]) that offer access to many annotated
    samples. As the interest in ML/DL applied to remote sensing data is increasing,
    so is the demand for large-scale datasets processed to fit various use cases (e.g.,
    land cover, flooding, building detection, agriculture). Figure 1 presents the
    publication date of popular RS datasets, showing a recently increased interest
    in large-scale crop type mapping datasets. Each dataset is characterized by the
    following: (1) the area covered; (2) the time extent for the collected data; (3)
    the source for both input and GT data; (4) the intended use case; (5) the number
    and size of provided patches (the original image size might be too large to fit
    into hardware memory; therefore, the images are cropped in smaller, ready-to-use
    patches). Figure 1. Crop-related datasets’ publication timeline. The first attempts
    to create large-scale datasets for RS scenarios targeted the land cover problem.
    BigEarthNet [7] represents one of the first large-scale RS image archives. It
    targets land-cover multi-classification and uses Sentinel-2 data as input, matched
    with Corine Land Cover (CLC) information for ground truth. It includes data from
    June 2017 to May 2018, covering ten different European regions: Austria, Belgium,
    Finland, Ireland, Kosovo, Lithuania, Luxembourg, Portugal, Serbia, and Switzerland.
    Since it is a dataset designed to work with computer vision ML/DL models, the
    authors distribute the dataset in patches, resulting in a total of 590,326 nonoverlapping
    patches, with different sizes for each resolution:  120×120  pixels for 10 m band,  60×60  pixels
    for 20 m, and  20×20  for 60 m bands. Also targeting land cover, but from a semantic
    segmentation perspective, is the Sen12MS [8] dataset. Like BigEarthNet, it uses
    Sentinel-2 data, but adds Sentinel-1 and extracts its ground truth information
    from MODIS land cover maps. Sen12MS is distributed in overlapping patches of  256×256
    , with a stride of 128. The dataset includes 180,662 patch triplets (Sentinel-1,
    Sentinel-2, GT) covering a global area and including all meteorological seasons.
    Sen12MS distributes its patches at a resolution of 10m, with upsampled data for
    the 60 m and 20 m resolution bands. As large-scale remote sensing land cover datasets
    appeared, the need to target more specific use cases also emerged. This paper
    focuses on the particular scenario of crop-type mapping. Crop type mapping datasets
    are based on land parcel identification system [9] (LPIS) information to generate
    ground truth data. Therefore, datasets are developed together with the release
    of open access to regions’ parcel information. Compared to a land cover dataset,
    a crop type dataset includes only one category of labels—agricultural fields—but
    with increased granularity. Therefore, each crop parcel is delimited and labeled
    with the corresponding crop type. BreizhCrops [10] is the first large-scale dataset
    for crop classification, covering the Brittany area of France (27,200 km 2 ),
    spanning over the entire years of 2017 and 2018. The proposed dataset provides
    nine broad categories of crops and uses both Sentinel-2 Level-1C and Level-2A
    data to gain better regional coverage. The GT is created from the France’s publicly
    available LPIS database (Registre Parcellaire Graphique—RPG). The authors provide
    mean-aggregated values per band/timeframe over each field parcel, not image patches
    at a pixel level. ZueriCrop [11] is a dataset for crop type mapping based on Sentinel-2
    Level-2A bottom-of-atmosphere images from the Switzerland area (50 km × 48 km
    area). The GT data were extracted from Switzerland’s LPIS (Swiss Federal Office
    for Agriculture), which is not publicly available. The dataset comprises 48 different
    crop classes at the lowest hierarchical level and 5 categories at most at the
    top level. The crops are observed for 2019, resulting in 28,000 patches of  24×24  pixels,
    including 116,000 crop parcels. The Austrian region for crop type mapping is covered
    in the dataset proposed by [12] (TimeSen2Crop). The dataset uses Sentinel-2 bottom-of-atmosphere
    images spanning between September 2017 and August 2018. For GT, the authors used
    the publicly available Austrian LPIS, offering 16 different labels in the end.
    TimeSen2Crop is distributed as a monthly median composite for each tile, and comprises
    around 1 million labeled samples in total. DENETHOR [13] is the first dataset
    for crop type mapping that includes commercial data, namely Planet. It consists
    of three types of input data: Planet (3 m resolution), Sentinel-1, and Sentinel-2.
    DENETHOR covers the area of Northern Germany over two years: 2018 and 2019. Like
    the datasets mentioned above, DENETHOR uses the LPIS from Germany; however, it
    is not a publicly available database. The dataset includes nine crop type labels,
    with 4500 crop fields. Sen4AgriNet [14] is also a crop classification dataset
    based on Sentinel-2 Level-1C and LPIS data. The dataset uses the FAO ICC [15]
    classification for aggregating the LPIS information from France and Catalonia.
    It is the first multicountry and multiyear dataset, comprising two regions: France
    and Catalonia. Sen4AgriNet comprises aggregated data (crop parcel average) and
    pixel-based data (image patches), making it a valuable dataset for crop classification
    and crop segmentation. Sen4AgriNet offers the full spectrum of Sentinel-2 bands,
    preserving the initial spatial resolution. The data are divided into smaller patches
    ( 366×366  pixels for 10 m resolution,  183×183  pixels for 20 m resolution, and  61×61  pixels
    for 60m resolution). The temporal extent includes the years 2019 and 2020, resulting
    in patches with 168 class labels. Sen4AgriNet is distributed using the NetCDF
    format, making it compatible with modern self-describing tools such as Xarray
    [16,17]. AI4Boundaries [18] is the first multicountry crop dataset intended for
    field boundary detection, including 14.8 million parcels. It covers 2019 and incorporates
    seven different regions: Austria, Catalonia, France, Luxembourg, the Netherlands,
    Slovenia, and Sweden. AI4Boundaries offers two complementary datasets. First,
    it uses Sentinel-2 cloud-free monthly composites, with tiles 256 pixels in size,
    including the four 10 m resolution bands. Second, AI4Boundaries provides three
    channels at 1m resolution orthophotos of  512×512  pixels. We described several
    crop-type mapping datasets that include the European region, as they provide particularities
    in creating the ground truth data from LPIS databases. Most previously mentioned
    datasets use the available LPIS information for the selected country. However,
    they lack a methodology for the steps needed to process the data to obtain high-quality
    GT. Extending current datasets is tedious and prone to error tasks without a standard
    way of processing. Furthermore, although several regions are covered (Austria,
    Northern Germany, France, Catalonia, Switzerland), there is no standard in class
    naming and grouping from the initial LPIS. Besides Sen4AgriNet (covering France
    and Catalonia), each dataset only covers one type of data: pixel or parcel aggregate,
    making it difficult to benchmark other results. Additionally, there is no mention
    of possible mislabeled parcels in the original LPIS, which may cause training
    errors later. AgriSen-COG is designed to extend existing datasets (Table 1), complementing
    both temporal and spatial perspectives. Like Sen4AgriNet, we use the FAO ICC crop
    naming conventions to provide a standard and extensible way of labeling the dataset.
    AgriSen-COG includes pixel-level and object-aggregated data for two years (2019
    and 2020), enriching BreizhCrops, TimeSen2Crop, and DENETHOR. Compared to Sen4AgriNet,
    for the Catalonia region, our proposed dataset provides additional granularity
    in crop type label selection paired with additional anomalous label removal; it
    is also based on Sentinel-2 Level-2A data, as opposed to Level-1C as in Sen4AgriNet,
    including more patches from the Catalonia area (5168 patches vs. 4638 patches),
    and incorporates a different parcel aggregation method (barycenter vs mean and
    standard deviation). AgriSen-COG is the first dataset to include information from
    five EU countries. It is also easily integrated with AI4Boundaries for parcel
    boundary refinement or for discovering and creating new crop type datasets. Table
    1. Summary of the main characteristics of popular large-scale datasets for land
    cover and crop type mapping. 3. Popular DL Methods for Crop Type Mapping As the
    crop type mapping problem is frequently assimilated with the semantic segmentation
    task from computer vision (CV), the first DL approaches were heavily based on
    CV models based on classic convolutional neural networks (CNNs). However, satellite
    data pose additional characteristics to the three-channel (red, green, blue—RGB)
    CV image. These distinct properties include larger image size, increased number
    of channels, and the temporal dimension of the data. For the specific task of
    crop type mapping, the near-infrared band proves particularly useful in combination
    with the RGB channels, as it captures vegetation characteristics. The temporal
    dimension is essential for classifying crops because it provides information about
    their growing cycles. The larger size issue is solved by tiling the initial image
    (e.g., 10,980 × 10,980 pixels) into smaller patches (e.g.,  366×366  pixels) that
    fit the network and hardware limitations. Regarding the previously mentioned factors,
    crop type mapping is implemented using two approaches: a semantic segmentation
    problem or a time-series classification task. The first case uses DL networks
    composed entirely of CNNs or in combination with recurrent neural networks (RNNs)
    [19,20]. The second approach benefits from simple RNNs or transformer [21] networks.
    However, a plain CNN network can also be used for time-series classification.
    U-Net [22] is a popular CNN topology with good results on semantic segmentation.
    The main characteristic of U-Net is the “U”-shaped architecture, where information
    from intermediate layers of the encoder part of the network is transferred to
    the decoder. Even though it was built for biomedical images, U-Net has been successfully
    applied on EO-related use cases such as land cover ([23,24,25]), cloud masking
    ([26,27,28]), building detection ([29,30,31]), crop type mapping ([32,33,34]),
    and others. ConvLSTM [35] uses a combination of CNN layers with a particular type
    of RNN, the LSTM cell. The proposed network was designed for spatiotemporal inputs,
    in particular for the use case of precipitation nowcasting. ConvLSTM enhances
    the simple LSTM [36] cell by applying a convolutional layer over the input data.
    Therefore, the network exploits the data’s temporal (LSTM) and spatial (convolution)
    dimensions. This topology is also popular among remote sensing data, in particular
    for a use case that benefits from both spatial and temporal dimensions such as
    land cover ([37,38,39]), soil moisture ([40,41]), solar radiation ([42]), air
    quality [43], and others, including crop type mapping ([44,45,46]). In [47], the
    authors propose a more efficient version of ConvLSTM, named ConvSTAR. It eliminates
    several operations inside the LSTM cell (input and output gates), which results
    in a faster and more stable training process. Crop type mapping may be viewed
    as a time-series classification problem if the spatial dimension is discarded
    and information is aggregated at the parcel level, usually with a mean over a
    parcel’s pixels. The result is a sequence for each crop parcel, with the length
    equal to the number of sensing times for each polygon. Therefore, simple RNNs
    such as LSTMs have been used, for example, with datasets such as BreizhCrop for
    crop classification. The Transformer model [21] is another popular way to deal
    with sequence data. It uses the attention mechanism, which enables the network
    to access any particular step. Transformers are the new state of the art in natural
    language processing, as they require fewer parameters than LSTMs, resulting in
    faster training and more accurate results. TempCNN [48] is a CNN network capable
    of handling sequence data. As opposed to the LSTM and transformer, it was designed
    specifically for crop classification in France. It uses one-dimensional convolutional
    layers applied to satellite image time series (SITS). To prove the validity of
    the proposed dataset, we experiment with the two methods of crop detection: similar
    to semantic segmentation and time-series classification. Therefore, we present
    how AgriSen-COG is a promising dataset for new developments, regardless of the
    approach. To carry this out, we use popular deep-learning models from each category.
    We aim to provide a benchmark and starting point for each technique to help them
    further progress. 4. Anomaly Detection Dataset quality is strongly related to
    the performance of an ML/DL algorithm and directly influences a model’s ability
    to predict the desired result accurately. The garbage in, garbage out principle
    particularly applies when learning from data. Bad-quality data affect the training
    process, leading to longer training time and poor performance. However, when trained
    on well-labeled data, the algorithms are faster and better at discovering the
    patterns in the data. As previously seen, the LPIS requires information directly
    from the farmers. Therefore, it might include errors affecting the quality of
    the generated GT. To assess the quality of the labels in the proposed dataset,
    we treat it as a problem of anomaly detection applied to time series. Since we
    only have a set of labels and we aim to identify outliers, we apply an unsupervised
    anomaly detection method. Dynamic time warping (DTW) is a popular distance used
    for measuring similarities between time series. As opposed to the Euclidian distance,
    it enables the comparison of shifted time series with heterogenous lengths. In
    the case of crop phenology, it is essential to be able to identify similar growing
    cycles, even if they are shifted. DTW has already been applied on several use
    cases that aimed to find outliers or to align a time series: magnetic data [49],
    electric grid [50], livestock activity [51]. DTW is also implemented in combination
    with ML techniques, such as k-nearest neighbors (KNN) [52] or the DL method, with
    autoencoders [53]. In [54], the authors propose a way to compute a barycenter,
    a cluster center that minimizes the DTW distance. DTW has already been used to
    identify crop similarity, mostly for the crop classification task. In [55], the
    authors used the DTW distance to identify outliers in the LPIS data for the area
    of Italy, converting the problem of crop classification to an anomaly detection
    task. The proposed method used the normalized difference vegetation index (NDVI)
    [56] and computes a reference feature trend from the histogram of the available
    parcels. Afterward, several thresholds were calculated based on the true and false
    positive rates. Anomaly detection on crop sites was also performed by [57], where
    the authors used a histogram to identify the anomalous pixels. In [58], the authors
    also integrated an outlier detection step during their dataset preparation stage
    to target the use case of grassland mowing detection over Estonia. For each grassland
    field, the authors computed the NDVI using Sentinel-2 data and applied an anomaly
    detection step to distinguish the outliers introduced by clouds. The outliers
    were identified by considering triplets of consecutive NDVI measurements and identifying
    the triplet that forms a rhombus shape. Moreover, the authors proposed to exclude
    several prediction results with low confidence from the employed deep learning
    network. The idea of anomaly detection in agriculture was also explored in [59].
    The authors performed outlier detection on IoT data using deep learning methods.
    The DTW distance is used as a measure of performance. DTW distance was also used
    in [60] to detect anomalies in satellite sensor time series data acquisition.
    The authors argue that a data-driven method has the advantage of not requiring
    expert knowledge, as in the case of establishing specific thresholds. Another
    plus would also be against model-based approaches, which require an accurate mathematical
    model that is time-consuming to achieve. However, the data-driven method is based
    on the similarity between the time series. The authors combined the DTW distance
    with KNN algorithm to better identify the outliers. The DTW was also applied by
    [61], where it was used to detect outliers in plane traffic control. However,
    the authors use the DTW algorithm to obtain the best sequence alignment, and afterward
    to apply a Euclidian distance to measure the distance between two instances. Autoencoders
    were applied in [62] for the specific task of identifying mislabels on LPIS data
    over the Sevilla (Spain) region. The authors proposed a method using Sentinel-1
    as input data and an autoencoder with 1D convolutional layers in its encoder part.
    The proposed work analysed the LPIS labels from two perspectives: at the parcel
    level and at the class level. Therefore, the authors identified possible mislabeled
    pixels from one parcel, such as entire polygons, which might have received the
    wrong crop label. In the paper, there was no aggregation performed at the parcel
    level, but the convolutional layers were used to extract the temporal feature.
    In order to separate anomalies, the dynamic Otsu [63] thresholding was applied.
    For our proposed dataset, we apply an anomaly detection method using an LSTM-based
    autoencoder as we work with variable-length sequences. Furthermore, as our dataset
    is formed from six regions, we need to reduce the dimensionality of our data.
    Therefore, we apply it at the class level, using our Sentinel-2 data, aiming to
    identify only mislabeled parcels. 5. The AgriSen-COG Dataset The area reflected
    in the proposed dataset covers regions from five EU countries (Austria, Belgium,
    Spain, Denmark, and Netherlands). AgriSen-COG comprises 6,972,485 parcel observations,
    grouped in 41,100 patches of size  366×366  pixels. Each observation is described
    by the parcel’s spatial and temporal characteristics or as a univariate time series,
    with an aggregated version for each polygon. 5.1. Input Satellite Data The dataset
    is based on ESA Sentinel-2 [64] data, as they have the largest spectral and spatial
    resolution out of all free optical satellite optical data, which helps for a more
    precise segmentation. The Sentinel-2 mission is a constellation of two satellites
    in the same orbit, with a phase of 180° from each other. It has a high revisit
    time (5 days) and offers 13 spectral bands at 3 spatial resolutions: 10 m, 20
    m, and 60 m. The Sentinel-2 product is available to users under two processing
    levels: Level 1C—top of atmosphere, and Level 2A—bottom of atmoshepere. The latter
    is derived from Level 1C; as the name suggests, it contains an atmospheric corrected
    image. In addition, Level 2A is delivered with cloud probability data and a scene
    classification mask (SCL), incorporating land cover, cloud, and snow labels. A
    Sentinel-2 orthoimage (tile) corresponds to an area of  100×100  km 2 , distributed
    in UTM/WGS84 projection. 5.2. LPIS—Crop Type Labels Land parcel identification
    systems [9] (LPIS) are designed to record all of the EU’s crop parcel information.
    Common agricultural policy (CAP) uses them to verify agricultural subsidies and
    environmental obligations. LPISs are an essential component of IACS, the primary
    system CAP uses for handling subsidiaries. The system serves several objectives:
    validating the parcel identification information, assessing the eligibility area,
    and aiding in on-the-spot controls conducted for administrative purposes or by
    CAP. LPISs are databases created using information from the farmers and are under
    the governmental administration of each country. Therefore, no naming convention
    makes combining LPIS datasets from different countries difficult. Several EU administrations
    have published their LPIS information in the past few years, accelerating the
    creation of crop-type-mapping-related datasets. 5.3. Dataset Creation Methodology
    Crop type mapping datasets are built upon publicly available data. Several countries
    have already published their LPIS information for the EU region, making it possible
    to create large-scale crop type mapping datasets for ML/DL. In this paper, we
    analyze the available parcel information from all open-source EU crops’ data,
    resulting in the collection, processing, and analysis of five different areas
    (Austria, Belgium, Catalonia, Denmark, and Netherlands). Since there is no standard
    regarding LPIS data, creating a dataset raises several challenges due to the nonuniformity
    of the data. We also propose a detailed and reproducible methodology based on
    the LPIS input’s heterogeneity, which may be applied further to extend the current
    dataset with additional areas. Multiple regions included in an ML/DL dataset for
    crop type mapping can increase a model’s generalization capability or help in
    the domain adaptation method. In this context, a similar processing method is
    crucial for providing consistency among the data. Our dataset creation process
    is divided into three stages: (1) LPIS processing, to obtain a standard crop description
    for each polygon; (2) preparing the rasterized data, to obtain pairs of input
    data; and GT (3) improving dataset quality using anomaly detection. In Figure
    2 and Figure 3, we present the workflow of our methodology and the challenges
    they handle. The final dataset and several intermediate preprocessing steps are
    available for download from our repository. The entire processing code may be
    accessed as Python scripts on the project’s GitHub, and serves as a quickstart
    in modifying or extending the proposed dataset. Figure 2. Overview of the LPIS
    processing workflow. Figure 3. Overview of the rasterization workflow. (1) LPIS
    processing Step 1.1 is represented by LPIS data collection. In the absence of
    a shared database, retrieving each piece of LPIS information is time-consuming
    and performed manually. Most of the LPIS datasets are available on the Ministry
    of Agriculture of each country’s webpage. However, navigation is hampered, as
    most websites use specific acronyms in the original language to denote the files.
    We collected and published the original files involved in the proposed dataset
    for easier access (Output 1.1). The original files helped to test the proposed
    workflow or conduct different analyses/processing. Step 1.2 consists of converting
    the LPIS data to a unified format, which helps with further processing. The lack
    of a standard is visible first in the output format used to deliver the LPIS data.
    We encountered Shapefiles [65], Geopackages [66], Geodatabases [67], and GeoJSON
    [68] files. The output format consistency does not apply even for the same area,
    as there is a distinct format for different years. Therefore, we unified the file
    types, providing the data in two formats (Output 1.2): Geopackage and Parquet.
    Geopackage was chosen due to its popularity among the geocommunity and the integration
    with geotools (e.g., QGIS [69]). We also provided the data in a partitioned Parquet
    [70] format that allows for further distributed processing, which is needed when
    handling a large number of polygons. For our end goal, we require a list of geometrical
    shapes mapped with a label for each polygon. Step 1.2 is crucial in standardizing
    the LPIS data to apply uniform processing algorithms later. Step 1.3 continues
    our standardization by selecting a set of columns of interest, renaming them based
    on a chosen convention way, and translating the corresponding values to English.
    For each LPIS, we considered the crop type, crop group, area, and geometries-related
    columns. We believe crop type and geometries are mandatory fields, as they include
    the preliminary information we need for the proposed dataset. Crop group and area
    information are supplementary materials useful for statistical analysis. If the
    area for each polygon was not provided, we automatically computed it. Next, we
    proceeded with the English translation of the unique labels from the original
    LPIS. For a proper translation, we identified the correct encoding for each original
    LPIS file. The translations were manually corrected for each country to remove
    errors. This was a time-consuming manual process, but it was necessary to align
    the labels among various LPIS systems. Output 1.3 includes the English version
    of the LPIS data, with the same column naming for each country, together with
    a list of the corresponding encodings and translation of each label. Both types
    of information serve as a starting point for further extension of the current
    dataset. Sen4AgriNet also offers translation data for France and Catalonia. However,
    there are differences between our proposed translation and theirs, probably due
    to our translation revision process, which improves the quality of the final output.
    Step 1.4 addresses another issue created by the absence of a standard: distinct
    names for the same crop label class. This problem exists even for the same region
    when analyzing data from different years. We used the FAO Indicative Crop Classification
    (ICC) [15] categories to solve this and map each crop type to a new label. We
    followed the same naming convention as Sen4AgriNet, as we created AgriSen-COG
    to integrate with existing datasets. Furthermore, we believe that using a clear
    standard for crop labeling is helpful in further dataset extensions. FAO ICC uses
    a taxonomy that divides the crop types based on group > class > subclass > order.
    For AgriSen-COG, we chose the innermost ICC label and attached a number code to
    each crop type. There are 168 classes and subclasses in all, making up the custom
    FAO/CLC classification scheme. We incorporated two supplementary classes, Fallow
    land and background, in our final GT. In addition, each region’s resulting file
    (Output 1.4) contains all the upper levels from FAO ICC, including group, class,
    subclass, and order. (2) Preparing rasterized data Step 2.1 starts our rasterization
    process for converting the LPIS data into actual raster data. It consists of finding
    the exact boundaries of each area of interest (AOI). The limits are needed in
    the next step to finding the intersecting Sentinel-2 tiles. One may choose between
    a region/country’s actual border coordinates to retrieve the boundaries or extract
    them from the LPIS file. We tested both approaches and decided to follow the latter.
    Even though the first version is faster, as the border files are publicly available,
    we are only interested in the region with agricultural representation. Therefore,
    we computed the boundaries from the previously generated LPIS. In this way, we
    eliminated from the start all of the Sentinel-2 tiles that do not intersect with
    any crop polygons. Step 2.2 continues with the discovery of the Sentinel-2 tiles
    that will serve as input data for the proposed dataset. To ease the searching
    process, we used the S2 Amazon STAC catalogue (AWS S2 COGs STAC: https://registry.opendata.aws/sentinel-2-l2a-cogs/,
    accessed on 4 June 2023). We conducted STAC searching queries based on each AOI’s
    boundary, cloud percentage, and our dates of interest. Step 2.3 starts the actual
    rasterization of our LPIS information. The advantage of using S2 COGs is that
    we can study a Sentinel-2 tile without needing to download it. We took the unique
    tile regions identified in the previous step and used their bounding boxes to
    map the LPIS polygon on a new raster for each tile. The result (Output 2) was
    a georeferenced array for each tile. These constitute the ground truth data of
    the proposed AgriSen-COG. We used the previously mapped FAO ICC code to generate
    the pixel values for each geometry. The raster images were generated by matching
    each Sentinel-2 tile’s coordinate reference system (CRS). The GT raster was released
    under the following formats: Geotiff [71], as it is a popular geo-format; and
    Zarr [72], to enable distributed processing; and COGs (cloud-optimized Geotiff:
    https://www.cogeo.org, accessed on 4 June 2023), to allow image access without
    downloading the data. (3) Improving dataset quality with Anomaly Detection We
    integrated an identification of mislabeled GT as a preprocessing step for our
    dataset creation. To our knowledge, AgriSen-COG is the first crop type dataset
    to incorporate an anomaly detection step to curate the data. Our goal is to identify
    the mislabeled crop parcels. Therefore, we need to prepare aggregated information
    at the polygon level. The workflow for our data preparation and anomaly detection
    process is described in Figure 4 and Figure 5. Figure 4. Overview of the anomaly
    detection preprocessing workflow. Figure 5. Overview of the anomaly detection
    workflow. Step 3.1 starts our data preparation for the anomaly detection task.
    First, we computed the NDVI index ( 𝑁𝐼𝑅−𝑅𝑒𝑑 𝑁𝐼𝑅+𝑅𝑒𝑑 ) to capture the characteristics
    of our crop vegetation while reducing the multichannel structure to a one-channel
    image. The NDVI was computed for each tile at a pixel level to preserve the temporal
    and spatial dimensions. Step 3.2 continues with cloud masking the NDVI image.
    As the NDVI was later used for anomaly detection, clouds would alter the process
    and include bias in a polygon’s time series. Therefore, we applied a cloud mask
    on each image. We decided to use the SCL mask, already delivered with the Sentinel-2
    product, eliminating the need for another cloud processing algorithm to be added
    to our workflow. The SCL mask offers comparable results to top cloud masking methods
    [73]. From the 12 labels present in the SCL mask, we implemented the cloud- and
    snow-related pixel classes, namely saturated or defective, cast shadows, cloud
    shadows, cloud medium probability, cloud high probability, thin cirrus, and snow
    or ice. Step 3.3 assembles our NDVI time series. Each pixel from our dataset has
    the following properties: (1) a sequence of NDVI values, representing the vegetation
    characteristics captured at different moments; (2) a crop label, describing the
    corresponding crop class (from LPIS); (3) a polygon identifier, assimilated to
    a number given to each polygon from the LPIS data. Initially, the information
    is stored as multiple matrixes of pixels, which are transformed into sequences
    having the values mentioned earlier. Step 3.4 is the final data processing step
    before applying the LSTMAutoencoder for anomaly detection. As we are only interested
    in detecting anomalies at the polygon level, we aggregated the time series corresponding
    to pixels from the same polygon. Possible aggregations include polygon median,
    mean, or computing the barycenter. In our time series, we might have missing data
    for the same polygon due to cloud masking or just missing data from the original
    Sentinel-2 image. The median and mean are more sensitive to the missing data situations,
    as mentioned earlier. Therefore, we computed the barycenter for each polygon,
    capturing the time-related variability of each polygon. The barycenter (Equation
    (1)) was computed using the DTW barycenter averaging (DBA) [54] algorithm. The
    barycenter is a sequence for each polygon that reflects a crop’s growing cycle
    from the respective parcel. 𝑏𝑎𝑟𝑦𝑐𝑒𝑛𝑡𝑒𝑟(𝐷)= min 𝜇 ∑ 𝑥𝜖𝐷 𝐷𝑇𝑊 (𝜇,𝑥) 2 (1) Step 3.5
    starts our anomaly detection process by grouping the barycenter time series for
    each crop type. As in [55,62], we expected most crop labels from the same class
    to be correct, and aimed to identify the outliers only. As we had a large variability
    regarding the number of time-series for each category (from a few hundred to ten
    thousand), we chose an autoencoder network instead of a KNN with DTW. We eliminated
    the dates without input for each time series and used interpolation to fill in
    missing values. Step 3.6 corresponds to the actual model training, as we trained
    a LSTM Autoencoder for each category. The architecture of our models is described
    in Table 2 and is based on the LSTM autoencoders from here (LSTM autoencoders:
    https://github.com/shobrook/sequitur, accessed on 4 June 2023). The proposed network
    follows a classic autoencoder structure composed of encoder and decoder parts.
    In our case, the encoder and decoder use two LSTM layers, followed by a fully
    connected layer at the end of the decoder. Table 2. Architecture of the LSTM autoencoder.
    Step 3.7 consists of passing again through the trained autoencoder model in prediction
    mode to record the prediction error. The autoencoder tries to reconstruct the
    input by minimizing the reconstruction loss. We chose the mean squared error (MSE—Equation
    (2)) loss for our model. We saved the value of the MSE for each sequence and used
    a threshold to identify the anomalies based on it. 𝑀𝑆𝐸= 1 𝑀 ∑ 𝑖=1 𝑀 ( 𝑦 𝑖 − 𝑦
    𝑖 ̂ ) 2 (2) Step 3.8 identifies the outliers based on the MSE loss values determined
    in the previous step. We have an array of prediction loss for each crop type label,
    on which we apply a threshold to separate the regular class from the possible
    abnormalities. Even though the threshold might be chosen by a visual analysis
    of the distribution, in our case, we have more than 50 label types for each country.
    Therefore, we proceeded with a dynamic threshold, as in [62], the Otsu thresholding.
    This technology, which was initially developed to convert gray-level photos into
    black-and-white images, enables the separation of a histogram with two spikes.
    It looks for a binary threshold that yields the least intraclass variance when
    the two groups are averaged. We computed the Otsu thresholding for each category
    and eliminated the crop parcel with higher values than the corresponding threshold
    for each class. The Otsu thresholding is defined in Equation (3), where  𝜔 1 (𝑡)  and  𝜔
    2 (𝑡)  is the empirical probability that the loss is equal or below t, respectively
    above. The variance of normal/abnormal values is reflected in  𝜎 2 1 (𝑡)  and  𝜎
    2 2 (𝑡) . find 𝑡 that minimizes  𝜎 2 (𝑡)= 𝜔 1 (𝑡)∗ 𝜎 2 1 (𝑡)+ 𝜔 2 (𝑡)∗ 𝜎 2 2 (𝑡)
    (3) 5.4. Dataset Description The resulting AgriSen-COG is a multiyear, multicountry
    dataset for crop type mapping. It includes 2019 and 2020 data covering the following
    five areas: Austria, Belgium, Catalonia, Denmark, and the Netherlands. We used
    the corresponding LPIS information for each region, distributed under the Open
    Data Commons Attributions Licence. We selected the years 2019 and 2020, summing
    up to 10.2 M parcels (the detailed distribution of polygons is presented in Table
    3). Each original AOI includes a large and varied number of unique labels (Table
    3), mapped to FAO ICC standard, resulting in, at most, 102 common crop classes,
    including the additional Fallow category. The noncrop pixels are marked with the
    background label. In Figure 6, we present a sample for the proposed dataset from
    all six regions, including both years for the same area. As depicted, we highlight
    the spatial variability and temporal changes included in the AgriSen-COG dataset.
    Our GitHub repository provides a more thorough explanation, examples of data loading
    functions, and graphic demonstrations. Additionally, code samples are offered
    to help people write the logic presented in the creation methodology, regarding
    both LPIS processing and data rasterization. Figure 6. Sample patches from the
    proposed dataset (AgriSen-COG): (a) Austria 2019 33UVP; (b) Austria 2020 33UVP;
    (c) Belgium 2019 31UDS; (d) Belgium 2020 31UDS; (e) Catalonia 2019 31TCF; (f)
    Catalonia 2020 31TCF; (g) Denmark 2019 32UNG; (h) Denmark 2020 32UNG; (i) Netherlands
    2019 31UET; (j) Netherlands 2020 31UET. Table 3. LPIS original information for
    2019–2020 used in AgriSen-COG. The proposed dataset contains two subsets created
    to match the two approaches in crop type mapping: pixel-level patch subset (for
    temporal semantic segmentation) and parcel-level aggregated subset (for time-series
    classification). Both subsets contain data from all five regions, covering two
    years (2019, 2020). Therefore, we enable further research focused on a single
    area or studying how models handle different geographical characteristics. We
    followed the methodology mentioned earlier in creating AgriSen-COG. It relies
    on a total of 62 Sentinel-2 tiles. We selected the tiles that intersected with
    the LPIS bounds of each region and retrieved the Sentinel-2 Level-2A tiles with
    less than 30% cloud percentage. Next, we rasterized the LPIS polygons, following
    the bounding boxes of each tile, but we discarded the parcels with less than 0.1
    ha area. After rasterization, we applied our anomaly detection algorithm and identified
    possible anomalous fields. Therefore, we eliminated the corresponding polygons
    by labeling them as the background class. Ultimately, we divided each tile into
    patches so that our data fit the hardware restrictions. The proposed dataset includes
    the 10 m resolution bands only (red, green, blue, and near-infrared), as they
    provide most of the vegetation-related information and they do not require further
    upsampling to use. Therefore, we chose size 366 × 366 for each patch, an integer
    division with the initial tile size for the 10 m resolution bands. The patch size,
    as mentioned earlier, also makes AgriSen-COG compatible with other datasets, such
    as Sen4AgriNet. From each tile, there is a total of 900 resulting patches. However,
    we discarded the patches that did not include any crop-related polygon, summing
    up to 41,100 patches in AgriSen-COG. Table 4 presents a detailed description of
    the eliminated polygons and patches during each stage. Table 4. AgriSen-COG data
    summary. The patches are saved as Zarr arrays in a format offering a self-describing
    design compatible with Xarray. The selected format is also compatible with a distributed
    processor (like Dask) and is the preferred format for cloud-stored data. The ground
    truth (LPIS masks) data files are stored using the COG format and are available
    on a public S3 bucket. This way, we enrich the existing COGs databases and make
    the proposed data easily accessible (no download needed) and findable (STAC catalogues
    indexes). The aggregated data for each field (the barycenters) constitute the
    proposed time-series dataset, and are distributed in Parquet format to ensure
    a smaller size and distributed processing if needed. The five AOIs comprise around
    62 Sentinel-2 tiles, with the patches dataset summing up to 6,972,485 fields,
    for 2019–2020, with 41,100 patches. 6. Experimental Results This section presents
    our experiments performed on the proposed AgriSen-COG dataset, for 2019 and 2020,
    including all six regions. We created the training, validation, and testing datasets
    using label stratification, with a ratio of 60%-20%-20% for each set. The input
    data were aggregated into a monthly median to lower the number of input timesteps.
    The experiments were based on the four 10 m resolution bands (red, green, blue,
    and nir) within a period of 6 months for each year, from month 4 (April) to 9
    (September). Each AOI has a different number of labels, ranging between 60–80
    of other classes. However, to help the training process, we reduced the number
    of crops and chose 11 common categories: wheat, maize, barley, oats, rapeseed,
    potatoes, peas, rye, sunflower, sorghum, and grapes. The distribution of the selected
    classes is depicted in Figure 7) for both of the proposed datasets. We notice
    that wheat, maize and barley are the dominant crop categories. However, there
    is a major difference given by the dataset type. For example, the number of wheat
    and maize field crops in Austria is larger by more than 50k than in Denmark. However,
    when compared to the actual number of pixels from each parcel, Denmark comprises
    more than 30 M pixels for the respective categories. Figure 7. Selected crop distribution
    for the barycenter time-series and pixel-wise patch datasets. Vertical axis is
    in logarithmic scale. (a) Crop time-series distribution for 2019, (b) Crop time-series
    distribution for 2020, (c) Crop pixel distribution for 2019, (d) Crop pixel distribution
    for 2020. Based on the barycenters computed from NDVI in the anomaly detection
    step, we depict (Figure 8) how the barycenter changes over time for each selected
    AOIs, for two representative crops: wheat and maize. We observe similar variations
    in the crops’ growing cycles for each region, even though they are shifted from
    one year to another. We also encounter differences between the AOIs for the same
    culture. Therefore, the proposed dataset incorporates the real-world challenges
    of crop detection: spatial and temporal variation. Figure 8. Barycenters over
    NDVI for different crops. This work aims to train DL models for two crop-related
    tasks: crop type classification and crop type mapping. Performing this shows how
    the proposed dataset helps handle both scenarios while covering crops’ spatial
    and temporal variability. Therefore, we present the following experimental strategies:
    Experiment Type 1 (anomalies variation): We conduct individual experiments on
    each AOI for a single year, with one model, to highlight the importance of curated
    data labels. Experiment Type 2 (temporal generalization): We conduct individual
    experiments on each AOI using the model trained in Experiment Type 1 and predict
    the instances for 2020. Experiment Type 3 (spatial generalization): We conduct
    several experiments, using data from one year and splitting our data based on
    regions’ similarity in crop patterns. Experiment Type 4 (overall generalization):
    We train on two AOIs for 2019, with different models (LSTM, Transformer, TempCNN,
    U-Net, ConvStar) to see the behavior of the proposed dataset. The crop type mapping
    use case was assimilated to a semantic segmentation problem, and therefore, we
    employed the popular U-Net model. In addition to this, we also used a temporal-designed
    model, the ConvStar, for comparison. The crop type classification approach is
    based on aggregated time series at the parcel level. Previous work included mean-aggregated
    time series; however, we also propose, for comparison, the barycenter series computed
    for each crop polygon. This approach was tested with three types of models for
    time series: LSTM, transformer, and TempCNN. We evaluated our experiments using
    a weighted F1 score as we encounter high-class imbalance. Moreover, we included
    several normalized confusion matrices for further visual analysis. 6.1. Crop Type
    Classification Experiments—Time-Series Classification In this paper, we used the
    barycenters computed for each crop parcel for crop type classification. The barycenters
    were previously also used to eliminate possible anomalous polygons. This set of
    experiments serves as a proof-of-concept on using a different metric for aggregation
    crop parcel information instead of the classic mean, median, or standard deviation.
    We resampled our time series in the proposed experiments to obtain a weekly value.
    By carrying this out, we preserved the crop growth information while still obtaining
    a regular interval for our time-series data. However, the experiments presented
    in this paper used information from month 4 (April) to month 9 (September), as
    it captured the growing cycles of the analyzed cultures. LSTM is the first architecture
    used to assess our barycenter-based dataset. LSTMs are a popular choice in dealing
    with time-series data; we used them for most experiments due to their simplicity.
    The proposed network consists of three bidirectional LSTM layers, with one input
    feature and a hidden size of 128. The result is then passed through two fully
    connected layers, with a ReLU function in between. A final Softmax function is
    applied for classification. The model includes 349 k trainable parameters. The
    experiments using LSTM employ cross entropy as a loss function, the Adam optimizer,
    with a starting learning rate of 0.001 and a learning rate scheduler decrease
    of 0.1 every five epochs. Experiment Type 1 (anomalies variation): The first set
    of experiments aims to highlight how curated data labels impact a model’s performance.
    Using the barycenters for each polygon, we removed the influence given by larger
    parcels from a category and focused only on detecting crop growing patterns. We
    conducted ten experiments, two for each included AOI (Austria, Belgium, Catalonia,
    Denmark, Netherlands), for 2019, using the initial time-series data and the curated
    data after the anomaly detection process. The results of the experiments are presented
    in Table 5, showing the score obtained using the initial version of the dataset
    (v0) and the time series after the anomaly detection process (v1). We observe
    that the model trained with the curated dataset outperforms in the overall score
    for each selected AOI, showing the benefit of training with more trustworthy labels.
    Table 5. Results for the crop type classification experiments. The best results
    are in bold for each AOI. Experiment Type 2 (temporal generalization): On the
    time-series data, the second set of experiments aims to show how the dataset scores
    for the same AOI while being trained on the 2019 time series and using the 2020
    data for testing. We used the same LSTM models trained in the previous experiments;
    however, we considered only the curated version of our proposed dataset. Table
    5 reflects the results obtained for each region using our dataset (v1 2020). For
    most of our areas, we observed a considerable decrease in the overall score, which
    shows the variability of the crops in time, even for the same AOI. Corresponding
    confusion matrices (Figure 9a–j) illustrate a detailed view of the eleven labels
    considered. For Austria, we observed a decrease in correctly identifying crops
    for all the labels except maize. However, for Belgium, we noticed consistency
    for the wheat category and a lack of recognition of the maize crop from 2020.
    Sorghum raises challenges for Belgium in both years. Catalonia results highlight
    a significant difference between the 2019 and 2020 crops, with consistency only
    for the maize and grapes categories. The Denmark area provides the closest results
    between the two years, indicating a low shift between the crop-growing cycles.
    Finally, for the Netherlands, we obtained similar results for the wheat and potatoes
    labels, with a score decrease for the maize crop. The results show that the proposed
    dataset incorporates the challenges of the time variability of crop growing cycles,
    making it suitable for further developing stable crop monitoring methods. Figure
    9. Confusion matrixes for Experiment Type 2 (temporal generalization) and results.
    Experiment Type 3 (spatial generalization). LSTM model trained with 2019 time
    series-dataset and tested with both 2019 and 2020 time series: (a) predicted labels
    Austria 2019; (b) predicted labels Belgium 2019; (c) predicted labels Catalonia
    2019; (d) predicted labels Denmark 2019; (e) predicted labels Netherlands 2019;
    (f) predicted labels Austria 2020; (g) predicted labels Belgium 2020; (h) predicted
    labels Catalonia 2020; (i) predicted labels Denmark 2020; (j) predicted labels
    Netherlands 2020; (k) predicted labels Austria 2019 (Type 3); (l) predicted labels
    Belgium 2019 (Type 3); (m) predicted labels Catalonia 2019 (Type 3); (n) predicted
    labels Denmark 2019 (Type 3); (o) predicted labels Netherlands 2019 (Type 3).
    Experiment Type 3 (spatial generalization): The third set of experiments aims
    to show how the proposed time series can capture crop-growing cycle similarities
    between different regions. We only used the 2019 year for training and testing.
    An LSTM model was trained using three AOIs (Austria, Catalonia, and the Netherlands)
    and tested using the five areas from the proposed dataset. The similarity between
    the Austrian and Dutch crops’ time series, as opposed to Catalonia, is reflected
    in the results presented in Table 5 (v1 2019 Type 3 rows). We observed a significant
    decrease in the scores obtained for the Catalan region. A lower score was obtained
    for the two AOIs (Belgium and Denmark). However, they still performed better than
    Catalonia due to their similarity to Austria and Netherlands. From the confusion
    matrixes (Figure 9k–o), we observe that maize and wheat crops are strongly identified
    in most of the regions. In contrast, for Belgium and Denmark, rapeseed and barley
    are classified with lower accuracy. Experiment Type 4 (model behavior): The last
    experiments present how different models perform when trained with the proposed
    dataset. We experimented with two popular time-series models for the proposed
    barycenter data: the transformer model and the TempCNN. We chose two of our AOIs,
    Denmark and Catalonia, as they present different crop markers. We trained the
    models using the 2019 data and predicted using 2019 and 2020. Based on the results
    shown in Table 5 (v1 Type 4), we observed that TempCNN outperforms the LSTM and
    transformer models for both regions. However, when tested on the 2020 data, the
    TempCNN achieved the lowest performance in Catalonia but an increased score for
    Denmark. 6.2. Crop Mapping Experiments—Semantic Segmentation The crop mapping
    experiments present the second use case of the proposed dataset that uses the
    actual parcel’s spatial and temporal information. As in the previous case, we
    only used data from month 4 (April) to month 9 (September). The crop mapping application
    is practical when a crop’s geometry is unknown. Therefore, the task is to discover
    a crop’s type and the parcel’s geometry. As it resembles the problem of semantic
    segmentation, we used the popular U-Net model to test the utility of the proposed
    dataset. Our experiments were based on the four 10 m resolution bands, resulting
    in four input channels. In addition to this, we also incorporated the crop’s temporal
    information, using a monthly median, with the final input data in the shape of
    [T, C, H, W] (T: timesteps, C: channels, H: height, W: width). As U-Net is not
    designed to support the temporal size, we concatenated the first two dimensions,
    achieving an input image shape of [T × C, H, W]. In the latter part of the proposed
    experiment, we also experimented with two models for temporal semantic segmentation:
    U-Net and ConvStar. We created smaller nonoverlapping chips of  61×61  (H × W)
    for faster training from our initial patches of  366×366  pixels. Therefore, our
    input for training was in the shape of  6×4×61×61  (T × C × H × W). Due to the
    high-class imbalance, given not only by the presence of dominating crops (as with
    the time series) but also due to the size of the pixels from specific parcels,
    we use a weighted negative log-likelihood loss. During the training and testing
    stages, we masked all the pixels that did not correspond to the 11 selected classes
    as background. The Adam optimizer was employed for training, starting with a learning
    rate of 0.001, with a decrease strategy applied to validation loss plateaus. Experiment
    Type 1 (anomaly variation): In the first series of experiments, we analyzed how
    curated data labels affect the performance of a model. As in the previous case,
    we conducted ten experiments in 2019, two for each included AOI (Austria, Belgium,
    Catalonia, Denmark, and the Netherlands), using the initial parcel’s dataset and
    the one that was curated. The results of the experiments are shown in Table 6,
    which displays the score derived with the initial version of the dataset (v0)
    and the time series after the anomaly detection process (v1). As opposed to the
    experiments based on time series, the current results are influenced by the number
    of pixels from each parcel. However, we also observe an improvement in the curated
    dataset for each AOI, even for the proposed pixel-based segmentation. Therefore,
    it shows the value of training with more reliable labels. Table 6. Results for
    the crop type mapping experiments. The best results are in bold for each AOI.
    Experiment Type 2 (temporal generalization): The second set of experiments on
    the time-series data seeks to demonstrate how the dataset performs for the same
    AOI when trained on the 2019 time series and tested with the 2020 data. We employed
    the same U-Net models trained in previous experiments, but we only considered
    the curated version of our proposed dataset. Table 6 reflects the results obtained
    for each region using our dataset (v1 2020). Except for the Netherlands, there
    was a decrease in the overall F1 score. Even though we obtained good accuracy
    and precision on the 2020 dataset, we achieved poor recall, as the models struggled
    to identify actual pixels from each category. The previously mentioned tradeoff
    between precision and recall is visible for each crop category for all AOIs. Corresponding
    confusion matrices (Figure 10) illustrate a detailed view of the eleven labels
    considered. By incorporating the pixel-based characteristic for crop mapping,
    we observe that the models can better classify crops from one year to another.
    The findings demonstrate that the proposed dataset addressed the difficulties
    provided by the temporal unpredictability of crop growth cycles, making it appropriate
    for the continuing development of reliable crop monitoring techniques. Figure
    10. Confusion matrixes for Experiment Type 2 (temporal generalization) and results.
    Experiment Type 3 (spatial generalization). U-Net model trained with 2019 time-series
    dataset and tested with both 2019 and 2020 pixel-based patches: (a) predicted
    labels Austria 2019; (b) predicted labels Belgium 2019; (c) predicted labels Catalonia
    2019; (d) predicted labels Denmark 2019; (e) predicted labels Netherlands 2019;
    (f) predicted labels Austria 2020; (g) predicted labels Belgium 2020; (h) predicted
    labels Catalonia 2020; (i) predicted labels Denmark 2020; (j) predicted labels
    Netherlands 2020; (k) predicted labels Austria 2019 (Type 3); (l) predicted labels
    Belgium 2019 (Type 3); (m) predicted labels Catalonia 2019 (Type 3); (n) predicted
    labels Denmark 2019 (Type 3); (o) predicted labels Netherlands 2019 (Type 3).
    Experiment Type 3 (spatial generalization): The third set of experiments exemplifies
    how the proposed pixel-based dataset captures regional variations in crop-growing
    cycles. Similar to the time-series experiments, we only used the year 2019 for
    training and testing. We trained a U-Net model using three AOIs (Austria, Catalonia,
    and the Netherlands) and tested it using the five areas from the proposed dataset.
    As seen in Table 6), the resemblance between the Austrian and Dutch crops is highlighted,
    with an actual improvement in the overall score for the Netherlands. As seen in
    the corresponding confusion matrixes (Figure 10k–o), the Catalonia region obtains
    a lower score. Even though maize, wheat, and barley crops are identified, the
    sorghum class is mislabeled as wheat and barley. Maize and wheat are also classified
    with a good score for Belgium and Denmark; however, barley, oats, and grapes are
    not recognized in either of the two test areas. Experiment Type 4 (model behavior):
    Our final experiments aim to illustrate how various models perform when trained
    using the dataset. Alongside the popular U-Net for semantic segmentation, we also
    experimented with ConvStar, a model created for temporal semantic segmentation.
    We chose two of our AOIs, Denmark and Catalonia, as they present different crop
    markers. We trained the models using the 2019 data and predicted using 2019 and
    2020. Based on the results shown in Table 6 (v1 Type 4), the temporal U-Net still
    obtains a better overall score than the ConvStar network. In Figure 11, we illustrate
    two test patches from Catalonia and Denmark, representing the patch for both 2019
    and 2020. We observe crop variability in the two samples, where the same parcels
    shifted their crops from rye to wheat (Catalonia) and maize to barley (Denmark).
    For Catalonia, the ConvStar model fails to identify the oats’ culture, mislabeling
    it with rye. However, the yearly changes from rye to wheat for the same parcels
    are better captured for the patch by the ConvStar model, as U-Net wrongly classifies
    rye as wheat. Figure 11. Predicted samples for Catalonia and Denmark. U-Net and
    ConvStar models trained with 2019 data. Tested using 2019 and 2020 data: (a) Catalonia
    generated ground truth (GT) 2019; (b) Catalonia U-Net prediction 2019; (c) Catalonia
    ConvStar prediction 2019; (d) Denmark generated ground truth (GT) 2019; (e) Denmark
    U-Net prediction 2019; (f) Denmark ConvStar prediction 2019; (g) Catalonia generated
    ground truth (GT) 2020; (h) Catalonia U-Net prediction 2020; (i) Catalonia ConvStar
    prediction 2020; (j) Denmark generated ground truth (GT) 2020; (k) Denmark U-Net
    prediction 2020; (l) Denmark ConvStar prediction 2020. Both models can identify
    the wheat and maize class for the Denmark patch but struggle with differentiating
    between the sorghum and barley crops. As of 2020, parcels with maize have been
    replaced with barley, and the models encounter the same problem, therefore misclassifying
    barley as sorghum. However, for the 2020 sample, we also observe that the ConvStar
    model struggles with differentiating between the rye and sorghum categories. 7.
    Discussion Agricultural monitoring implies several crop-related tasks: crop type
    classification, crop mapping, parcel extraction, and crop phenology. The availability
    of Sentinel-2 satellite data enables further development in agricultural monitoring
    due to the global coverage, multispectral bands at different resolutions, and
    temporal dimensionality. As official LPIS data from multiple regions became available,
    we created the AgriSen-COG dataset to fill in current demands for DL datasets
    for agricultural monitoring: multiregion, multiyear, trustworthy labels, and methodology
    to enable extension as new data became available. The first challenge in creating
    AgriSen-COG was label harmonization, as each country uses different naming conventions
    for their crops, and they even differ from year to year. As in [14], we followed
    the official FAO crop naming conventions. The second challenge involved the discovery
    of potential anomalous labels, as the LPIS data are created based on farmers’
    declarations. The anomaly detection process required the computation of a time
    series for each crop parcel and identifying the crops that follow a significantly
    different crop-growing pattern. As this process is performed on all the data,
    we used Sentinel-2 AWS COGs (Sentinel-2 AWS COGs repository: https://registry.opendata.aws/sentinel-2/,
    accessed on 4 June 2023), which enabled us to conduct the required computations
    without downloading a large amount of data. The proposed dataset meets two use-case
    scenarios: crop type classification and crop mapping. In the first scenario, we
    offer the time-series dataset computed during the anomaly detection stage. It
    comprises a barycenter time series for each polygon from all five AOIs: Austria,
    Belgium, Catalonia, Denmark, and the Netherlands, for two years: 2019 and 2020.
    The second scenario is met by our pixel-based patches, of size 366 × 366, which
    incorporate the four Sentinel-2 10 m resolution bands and a ground truth patch
    with all the labels. To show the validity of the proposed dataset, we designed
    four types of experiments, which also aim to highlight the potential of further
    research conducted using AgriSen-COG. Experiment Type 1 compared the performance
    of a popular DL model trained using the original dataset and a model trained using
    the curated dataset. We observed improved performance for both scenarios, showing
    that AgriSen-COG is a trustworthy dataset for crop detection. The second experiment
    (Experiment Type 2) illustrated the importance of having a multiyear dataset for
    crops due to the yearly variability. Our results showed a considerable decrease
    in the performance of the models trained for 2019 and tested with 2020 data. Therefore,
    AgriSen-COG is a dataset that could be used to develop better classification methods
    that better incorporate the yearly temporal shift. The spatial generalization
    experiment (Experiment Type 3) revealed the need for conducting targeted research
    for domain adaptation to enable crop classification for a larger number of regions.
    AgriSen-COG incorporates the most different areas (5) as opposed to existing datasets,
    making it a good benchmark for this task. The final Experiment Type 4 showed that
    AgriSen-COG is not only a dataset that works with simple yet popular DL models
    (LSTM and U-Net). Good performance was also obtained with three other types: transformer,
    TempCNN, and ConvStar, showing the potential of the proposed dataset to be used
    in the development of better DL models. 8. Conclusions This paper proposes AgriSen-COG,
    a new crop type mapping dataset that gathers information from five different areas
    (Austria, Belgium, Catalonia, Denmark, and the Netherlands). To this moment, AgriSen-COG
    is the only dataset that includes information from more than two areas, comprising
    the largest number of tiles and crop polygons (61 tiles and almost 7 M fields).
    We also detail the steps taken to create the dataset (together with code for reproducibility)
    and provide several intermediate results. Both output assets are crucial components
    for a consistent future extension with new regions as the LPIS data become available.
    In addition to this, by offering the output both in geopopular formats (Geopackage
    and Geotiff) and compliant distributed storage formats (Parquet and Zarr), we
    support further advancement in several domains: geoanalysis on crop trends, deep
    learning methods for agricultural monitoring, and big data distributed processing
    adapted to georeferenced data. We also distribute the dataset using COGs, enabling
    the data’s usage without prior download. In addition to this, we describe the
    dataset using a STAC catalogue, making it easier to discover and use. AgriSen-COG
    is the only crop-related dataset that offers a DTW—based barycenter time series
    for each parcel, as opposed to mean or median. By undergoing the anomaly detection
    process, AgriSen-COG is also the only crop-related dataset with curated labels
    for five different regions. Based on these characteristics, the proposed dataset
    may be regarded as a benchmark dataset for further development on agricultural
    monitoring, serving various CAP-related use cases: agricultural insurance, early
    warning risk assessment, or crop yield management. Funding This work was primarily
    funded by a grant from the Romanian Ministry of Research and Innovation, CNCS—UEFISCDI,
    project number PN-III-P4-ID-PCE-2016-0842, within PNCDI III, COCO (PN III-P4-ID-PCE-2020-0407)
    and the HARMONIA project, from the EU Horizon 2020 research and innovation programme
    under agreement No. 101003517. This work has partially supported by the European
    Space Agency through the ML4EO Project (contract number 4000125049/18/NL/CBI),
    Romanian UEFISCD FUSE4DL and by project POC/163/1/3/ “Advanced computa-tional
    statistics for planning and monitoring production environments” (2022–2023). Data
    Availability Statement The data presented in this study are openly available in
    Zenodo at https://doi.org/10.5281/zenodo.7892012 (accessed on 4 June 2023).To
    ease data access for large files, we also provide a Dropbox folder https://www.dropbox.com/sh/5bc55skio0o5xd7/AAAQVG3ZmVGFNvPiltQ9Esqma?dl=0
    (accessed on 4 June 2023) and a public Minio S3 bucket, name: agrisen-cog-v1,
    endpoint: https://s3-3.services.tselea.info.uvt.ro (Set anonymous access for download.
    Accessed on 4 June 2023). Upon request, for further development on Alibaba Cloud,
    access to private Alibaba S3 Bucket can also be granted.The code for preprocessing
    and training is available here: https://github.com/tselea/agrisen-cog.git (accessed
    on 4 June 2023). Conflicts of Interest The authors declare no conflict of interest.
    References Ma, L.; Liu, Y.; Zhang, X.; Ye, Y.; Yin, G.; Johnson, B.A. Deep learning
    in remote sensing applications: A meta-analysis and review. ISPRS J. Photogramm.
    Remote Sens. 2019, 152, 166–177. [Google Scholar] [CrossRef] Sustainable Agriculture|Sustainable
    Development Goals|Food and Agriculture Organization of the United Nations. Available
    online: https://www.fao.org/sustainable-development-goals/overview/fao-and-the-2030-agenda-for-sustainable-development/sustainable-agriculture/en/
    (accessed on 20 October 2022). THE 17 GOALS|Sustainable Development. Available
    online: https://sdgs.un.org/goals#icons (accessed on 20 October 2022). Deng, J.;
    Dong, W.; Socher, R.; Li, L.J.; Li, K.; Fei-Fei, L. Imagenet: A large-scale hierarchical
    image database. In Proceedings of the 2009 IEEE Conference on Computer Vision
    and Pattern Recognition, Miami, FL, USA, 20–25 June 2009; pp. 248–255. [Google
    Scholar] Cordts, M.; Omran, M.; Ramos, S.; Scharwächter, T.; Enzweiler, M.; Benenson,
    R.; Franke, U.; Roth, S.; Schiele, B. The cityscapes dataset. In Proceedings of
    the CVPR Workshop on the Future of Datasets in Vision, Boston, MA, USA, 7–12 June
    2015; Volume 2. [Google Scholar] Lin, T.Y.; Maire, M.; Belongie, S.; Hays, J.;
    Perona, P.; Ramanan, D.; Dollár, P.; Zitnick, C.L. Microsoft COCO: Common objects
    in context. In Proceedings of the Computer Vision–ECCV 2014: 13th European Conference,
    Zurich, Switzerland, 6–12 September 2014; Springer: Berlin/Heidelberg, Germany,
    2014; pp. 740–755. [Google Scholar] Sumbul, G.; Charfuelan, M.; Demir, B.; Markl,
    V. Bigearthnet: A large-scale benchmark archive for remote sensing image understanding.
    In Proceedings of the IGARSS 2019-2019 IEEE International Geoscience and Remote
    Sensing Symposium, Yokohama, Japan, 28 July–2 August 2019; pp. 5901–5904. [Google
    Scholar] Schmitt, M.; Hughes, L.H.; Qiu, C.; Zhu, X.X. SEN12MS–A Curated Dataset
    of Georeferenced Multi-Spectral Sentinel-1/2 Imagery for Deep Learning and Data
    Fusion. arXiv 2019, arXiv:1906.07789. [Google Scholar] [CrossRef] [Green Version]
    European Court of Auditors. The Land Parcel Identification System: A Useful Tool
    to Determine the Eligibility of Agricultural Land—But Its Management Could Be
    Further Improved; Special Report No 25; Publications Office: Luxembourg, 2016.
    [CrossRef] Rußwurm, M.; Pelletier, C.; Zollner, M.; Lefèvre, S.; Körner, M. BreizhCrops:
    A Time Series Dataset for Crop Type Mapping. ISPRS Int. Arch. Photogramm. Remote.
    Sens. Spat. Inf. Sci. 2020, XLIII-B2-2020, 1545–1551. [Google Scholar] [CrossRef]
    Turkoglu, M.O.; D’Aronco, S.; Perich, G.; Liebisch, F.; Streit, C.; Schindler,
    K.; Wegner, J.D. Crop mapping from image time series: Deep learning with multi-scale
    label hierarchies. arXiv 2021, arXiv:2102.08820. [Google Scholar] [CrossRef] Weikmann,
    G.; Paris, C.; Bruzzone, L. TimeSen2Crop: A Million Labeled Samples Dataset of
    Sentinel 2 Image Time Series for Crop-Type Classification. IEEE J. Sel. Top. Appl.
    Earth Obs. Remote Sens. 2021, 14, 4699–4708. [Google Scholar] [CrossRef] Kondmann,
    L.; Toker, A.; Rußwurm, M.; Camero, A.; Peressuti, D.; Milcinski, G.; Mathieu,
    P.P.; Longépé, N.; Davis, T.; Marchisio, G.; et al. DENETHOR: The DynamicEarthNET
    dataset for Harmonized, inter-Operable, analysis-Ready, daily crop monitoring
    from space. In Proceedings of the Thirty-Fifth Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track (Round 2), Virtual, 6–14 December
    2021. [Google Scholar] Sykas, D.; Sdraka, M.; Zografakis, D.; Papoutsis, I. A
    Sentinel-2 Multiyear, Multicountry Benchmark Dataset for Crop Classification and
    Segmentation With Deep Learning. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.
    2022, 15, 3323–3339. [Google Scholar] [CrossRef] Food and Agriculture Organization
    of the United Nations. A System of Integrated Agricultural Censuses and Surveys:
    World Programme for the Census of Agriculture 2010; Food and Agriculture Organization
    of the United Nations: Rome, Italy, 2005; Volume 1. [Google Scholar] Hoyer, S.;
    Hamman, J. xarray: N-D labeled arrays and datasets in Python. J. Open Res. Softw.
    2017, 5, 10. [Google Scholar] [CrossRef] [Green Version] Hoyer, S.; Fitzgerald,
    C.; Hamman, J.; Akleeman; Kluyver, T.; Roos, M.; Helmus, J.J.; Markel; Cable,
    P.; Maussion, F.; et al. xarray: V0.8.0. 2016. Available online: https://doi.org/10.5281/zenodo.59499
    (accessed on 4 June 2023). d’Andrimont, R.; Claverie, M.; Kempeneers, P.; Muraro,
    D.; Yordanov, M.; Peressutti, D.; Batič, M.; Waldner, F. AI4Boundaries: An open
    AI-ready dataset to map field boundaries with Sentinel-2 and aerial photography.
    Earth Syst. Sci. Data Discuss. 2022, 15, 317–329. [Google Scholar] [CrossRef]
    Jordan, M.I. Serial Order: A Parallel Distributed Processing Approach. Technical
    Report, June 1985–March 1986. 1986. Available online: https://doi.org/10.1016/S0166-4115(97)80111-2
    (accessed on 4 June 2023). Rumelhart, D.E.; McClelland, J.L. Learning Internal
    Representations by Error Propagation. In Parallel Distributed Processing: Explorations
    in the Microstructure of Cognition: Foundations; U.S. Department of Energy Office
    of Scientific and Technical Information; MIT Press: Cambridge, MA, USA, 1987;
    pp. 318–362. [Google Scholar] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit,
    J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need.
    Adv. Neural Inf. Process. Syst. 2017. Available online: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
    (accessed on 4 June 2023). Ronneberger, O.; Fischer, P.; Brox, T. U-net: Convolutional
    networks for biomedical image segmentation. In Proceedings of the Medical Image
    Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
    Munich, Germany, 5–9 October 2015; pp. 234–241. [Google Scholar] Rakhlin, A.;
    Davydow, A.; Nikolenko, S. Land cover classification from satellite imagery with
    u-net and lovász-softmax loss. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition Workshops, Salt Lake City, UT, USA, 18–22 June
    2018; pp. 262–266. [Google Scholar] Solórzano, J.V.; Mas, J.F.; Gao, Y.; Gallardo-Cruz,
    J.A. Land use land cover classification with U-net: Advantages of combining sentinel-1
    and sentinel-2 imagery. Remote Sens. 2021, 13, 3600. [Google Scholar] [CrossRef]
    Wang, J.; Yang, M.; Chen, Z.; Lu, J.; Zhang, L. An MLC and U-Net Integrated Method
    for Land Use/Land Cover Change Detection Based on Time Series NDVI-Composed Image
    from PlanetScope Satellite. Water 2022, 14, 3363. [Google Scholar] [CrossRef]
    Zhang, Z.; Iwasaki, A.; Xu, G.; Song, J. Cloud detection on small satellites based
    on lightweight U-net and image compression. J. Appl. Remote Sens. 2019, 13, 026502.
    [Google Scholar] [CrossRef] Guo, Y.; Cao, X.; Liu, B.; Gao, M. Cloud detection
    for satellite imagery using attention-based U-Net convolutional neural network.
    Symmetry 2020, 12, 1056. [Google Scholar] [CrossRef] Xing, D.; Hou, J.; Huang,
    C.; Zhang, W. Spatiotemporal Reconstruction of MODIS Normalized Difference Snow
    Index Products Using U-Net with Partial Convolutions. Remote Sens. 2022, 14, 1795.
    [Google Scholar] [CrossRef] Ivanovsky, L.; Khryashchev, V.; Pavlov, V.; Ostrovskaya,
    A. Building detection on aerial images using U-NET neural networks. In Proceedings
    of the 2019 24th Conference of Open Innovations Association (FRUCT), Moscow, Russia,
    8–12 April 2019; pp. 116–122. [Google Scholar] Irwansyah, E.; Heryadi, Y.; Gunawan,
    A.A.S. Semantic image segmentation for building detection in urban area with aerial
    photograph image using U-Net models. In Proceedings of the 2020 IEEE Asia-Pacific
    Conference on Geoscience, Electronics and Remote Sensing Technology (AGERS), Jakarta,
    Indonesia, 7–8 December 2020; pp. 48–51. [Google Scholar] Wu, C.; Zhang, F.; Xia,
    J.; Xu, Y.; Li, G.; Xie, J.; Du, Z.; Liu, R. Building damage detection using U-Net
    with attention mechanism from pre-and post-disaster remote sensing datasets. Remote
    Sens. 2021, 13, 905. [Google Scholar] [CrossRef] Wei, S.; Zhang, H.; Wang, C.;
    Wang, Y.; Xu, L. Multi-temporal SAR data large-scale crop mapping based on U-Net
    model. Remote Sens. 2019, 11, 68. [Google Scholar] [CrossRef] [Green Version]
    Fan, X.; Yan, C.; Fan, J.; Wang, N. Improved U-Net Remote Sensing Classification
    Algorithm Fusing Attention and Multiscale Features. Remote Sens. 2022, 14, 3591.
    [Google Scholar] [CrossRef] Li, G.; Cui, J.; Han, W.; Zhang, H.; Huang, S.; Chen,
    H.; Ao, J. Crop type mapping using time-series Sentinel-2 imagery and U-Net in
    early growth periods in the Hetao irrigation district in China. Comput. Electron.
    Agric. 2022, 203, 107478. [Google Scholar] [CrossRef] Shi, X.; Chen, Z.; Wang,
    H.; Yeung, D.Y.; Wong, W.K.; Woo, W.C. Convolutional LSTM network: A machine learning
    approach for precipitation nowcasting. Adv. Neural Inf. Process. Syst. 2015, 2015,
    802–810. [Google Scholar] Hochreiter, S.; Schmidhuber, J. Long Short-term Memory.
    Neural Comput. 1997, 9, 1735–1780. [Google Scholar] [CrossRef] Farooque, G.; Xiao,
    L.; Yang, J.; Sargano, A.B. Hyperspectral image classification via a novel spectral–spatial
    3D ConvLSTM-CNN. Remote Sens. 2021, 13, 4348. [Google Scholar] [CrossRef] Cherif,
    E.; Hell, M.; Brandmeier, M. DeepForest: Novel Deep Learning Models for Land Use
    and Land Cover Classification Using Multi-Temporal and-Modal Sentinel Data of
    the Amazon Basin. Remote Sens. 2022, 14, 5000. [Google Scholar] [CrossRef] Meng,
    X.; Liu, Q.; Shao, F.; Li, S. Spatio–Temporal–Spectral Collaborative Learning
    for Spatio–Temporal Fusion with Land Cover Changes. IEEE Trans. Geosci. Remote
    Sens. 2022, 60, 5704116. [Google Scholar] [CrossRef] Habiboullah, A.; Louly, M.A.
    Soil Moisture Prediction Using NDVI and NSMI Satellite Data: ViT-Based Models
    and ConvLSTM-Based Model. SN Comput. Sci. 2023, 4, 140. [Google Scholar] [CrossRef]
    Park, S.; Im, J.; Han, D.; Rhee, J. Short-term forecasting of satellite-based
    drought indices using their temporal patterns and numerical model output. Remote
    Sens. 2020, 12, 3499. [Google Scholar] [CrossRef] Yeom, J.M.; Deo, R.C.; Adamowski,
    J.F.; Park, S.; Lee, C.S. Spatial mapping of short-term solar radiation prediction
    incorporating geostationary satellite images coupled with deep convolutional LSTM
    networks for South Korea. Environ. Res. Lett. 2020, 15, 094025. [Google Scholar]
    [CrossRef] Muthukumar, P.; Cocom, E.; Nagrecha, K.; Comer, D.; Burga, I.; Taub,
    J.; Calvert, C.F.; Holm, J.; Pourhomayoun, M. Predicting PM2. 5 atmospheric air
    pollution using deep learning with meteorological data and ground-based observations
    and remote-sensing satellite big data. Air Qual. Atmos. Health 2021, 15, 1221–1234.
    [Google Scholar] [CrossRef] Yaramasu, R.; Bandaru, V.; Pnvr, K. Pre-season crop
    type mapping using deep neural networks. Comput. Electron. Agric. 2020, 176, 105664.
    [Google Scholar] [CrossRef] Chang, Y.L.; Tan, T.H.; Chen, T.H.; Chuah, J.H.; Chang,
    L.; Wu, M.C.; Tatini, N.B.; Ma, S.C.; Alkhaleefah, M. Spatial-temporal neural
    network for rice field classification from SAR images. Remote Sens. 2022, 14,
    1929. [Google Scholar] [CrossRef] Ienco, D.; Interdonato, R.; Gaetano, R.; Minh,
    D.H.T. Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land
    cover mapping via a multi-source deep learning architecture. ISPRS J. Photogramm.
    Remote Sens. 2019, 158, 11–22. [Google Scholar] [CrossRef] Turkoglu, M.O.; D’Aronco,
    S.; Wegner, J.D.; Schindler, K. Gating revisited: Deep multi-layer rnns that can
    be trained. arXiv 2019, arXiv:1911.11033. [Google Scholar] [CrossRef] [PubMed]
    Pelletier, C.; Webb, G.I.; Petitjean, F. Temporal convolutional neural network
    for the classification of satellite image time series. Remote Sens. 2019, 11,
    523. [Google Scholar] [CrossRef] [Green Version] Mitra, P.; Akhiyarov, D.; Araya-Polo,
    M.; Byrd, D. Machine Learning-based Anomaly Detection with Magnetic Data. Preprints.org
    2020. [Google Scholar] [CrossRef] Sontowski, S.; Lawrence, N.; Deka, D.; Gupta,
    M. Detecting Anomalies using Overlapping Electrical Measurements in Smart Power
    Grids. In Proceedings of the 2021 IEEE International Conference on Big Data (Big
    Data), Orlando, FL, USA, 15–18 December 2021; pp. 2434–2441. [Google Scholar]
    Wagner, N.; Antoine, V.; Koko, J.; Mialon, M.M.; Lardy, R.; Veissier, I. Comparison
    of machine learning methods to detect anomalies in the activity of dairy cows.
    In Proceedings of the International Symposium on Methodologies for Intelligent
    Systems, Graz, Austria, 23–25 September 2020; pp. 342–351. [Google Scholar] Cover,
    T.; Hart, P. Nearest neighbor pattern classification. IEEE Trans. Inf. Theory
    1967, 13, 21–27. [Google Scholar] [CrossRef] [Green Version] Ballard, D.H. Modular
    learning in neural networks. In Proceedings of the AAAI, Seattle, WA, USA, 13
    July 1987; Volume 647, pp. 279–284. [Google Scholar] Petitjean, F.; Ketterlin,
    A.; Gançarski, P. A global averaging method for dynamic time warping, with applications
    to clustering. Pattern Recognit. 2011, 44, 678–693. [Google Scholar] [CrossRef]
    Avolio, C.; Tricomi, A.; Zavagli, M.; De Vendictis, L.; Volpe, F.; Costantini,
    M. Automatic Detection of Anomalous Time Trends from Satellite Image Series to
    Support Agricultural Monitoring. In Proceedings of the 2021 IEEE International
    Geoscience and Remote Sensing Symposium IGARSS, Brussels, Belgium, 11–16 July
    2021; pp. 6524–6527. [Google Scholar] Huang, S.; Tang, L.; Hupy, J.P.; Wang, Y.;
    Shao, G. A commentary review on the use of normalized difference vegetation index
    (NDVI) in the era of popular remote sensing. J. For. Res. 2021, 32, 1–6. [Google
    Scholar] [CrossRef] Castillo-Villamor, L.; Hardy, A.; Bunting, P.; Llanos-Peralta,
    W.; Zamora, M.; Rodriguez, Y.; Gomez-Latorre, D.A. The Earth Observation-based
    Anomaly Detection (EOAD) system: A simple, scalable approach to mapping in-field
    and farm-scale anomalies using widely available satellite imagery. Int. J. Appl.
    Earth Obs. Geoinf. 2021, 104, 102535. [Google Scholar] [CrossRef] Komisarenko,
    V.; Voormansik, K.; Elshawi, R.; Sakr, S. Exploiting time series of Sentinel-1
    and Sentinel-2 to detect grassland mowing events using deep learning with reject
    region. Sci. Rep. 2022, 12, 983. [Google Scholar] [CrossRef] [PubMed] Cheng, W.;
    Ma, T.; Wang, X.; Wang, G. Anomaly Detection for Internet of Things Time Series
    Data Using Generative Adversarial Networks With Attention Mechanism in Smart Agriculture.
    Front. Plant Sci. 2022, 13, 890563. [Google Scholar] [CrossRef] Cui, L.; Zhang,
    Q.; Shi, Y.; Yang, L.; Wang, Y.; Wang, J.; Bai, C. A method for satellite time
    series anomaly detection based on fast-DTW and improved-KNN. Chin. J. Aeronaut.
    2022, 36, 149–159. [Google Scholar] [CrossRef] Diab, D.M.; AsSadhan, B.; Binsalleeh,
    H.; Lambotharan, S.; Kyriakopoulos, K.G.; Ghafir, I. Anomaly detection using dynamic
    time warping. In Proceedings of the 2019 IEEE International Conference on Computational
    Science and Engineering (CSE) and IEEE International Conference on Embedded and
    Ubiquitous Computing (EUC), New York, NY, USA, 1–3 August 2019; pp. 193–198. [Google
    Scholar] Di Martino, T.; Guinvarc’h, R.; Thirion-Lefevre, L.; Colin, E. FARMSAR:
    Fixing AgRicultural Mislabels Using Sentinel-1 Time Series and AutoencodeRs. Remote
    Sens. 2022, 15, 35. [Google Scholar] [CrossRef] Otsu, N. A threshold selection
    method from gray-level histograms. IEEE Trans. Syst. Man, Cybern. 1979, 9, 62–66.
    [Google Scholar] [CrossRef] [Green Version] Drusch, M.; Del Bello, U.; Carlier,
    S.; Colin, O.; Fernandez, V.; Gascon, F.; Hoersch, B.; Isola, C.; Laberinti, P.;
    Martimort, P.; et al. Sentinel-2: ESA’s optical high-resolution mission for GMES
    operational services. Remote Sens. Environ. 2012, 120, 25–36. [Google Scholar]
    [CrossRef] PaperdJuly, W. ESRI shapefile technical description. Comput. Stat.
    1998, 16, 370–371. [Google Scholar] Yutzler, J. OGC® GeoPackage Encoding Standard-with
    Corrigendum, Version 1.2. 175. 2018. Available online: https://www.geopackage.org/spec121/
    (accessed on 4 June 2023). Zeiler, M. Modeling Our World: The ESRI Guide to Geodatabase
    Design; ESRI, Inc.: Redlands, CA, USA, 1999; Volume 40. [Google Scholar] Butler,
    H.; Daly, M.; Doyle, A.; Gillies, S.; Hagen, S.; Schaub, T. The Geojson Format.
    Technical Report. 2016. Available online: https://www.rfc-editor.org/rfc/rfc7946
    (accessed on 4 June 2023). Moyroud, N.; Portet, F. Introduction to QGIS. QGIS
    Generic Tools 2018, 1, 1–17. [Google Scholar] Vohra, D. Apache parquet. In Practical
    Hadoop Ecosystem: A Definitive Guide to Hadoop-Related Frameworks and Tools; Apress:
    Berkley, CA, USA, 2016; pp. 325–335. [Google Scholar] Trakas, A.; McKee, L. OGC
    standards and the space community—Processes, application and value. In Proceedings
    of the 2011 2nd International Conference on Space Technology, Athens, Greece,
    15–17 September 2011; pp. 1–5. [Google Scholar] [CrossRef] Durbin, C.; Quinn,
    P.; Shum, D. Task 51-Cloud-Optimized Format Study; Technical Report; NTRS: Chicago,
    IL, USA, 2020. [Google Scholar] Sanchez, A.H.; Picoli, M.C.A.; Camara, G.; Andrade,
    P.R.; Chaves, M.E.D.; Lechler, S.; Soares, A.R.; Marujo, R.F.; Simões, R.E.O.;
    Ferreira, K.R.; et al. Comparison of Cloud cover detection algorithms on sentinel–2
    images of the amazon tropical forest. Remote Sens. 2020, 12, 1284. [Google Scholar]
    [CrossRef] [Green Version] AgrarMarkt Austria InVeKoS Strikes Austria. Available
    online: https://www.data.gv.at/ (accessed on 9 February 2023). Department of Agriculture
    and Fisheries Flemish Government. Available online: https://data.gov.be/en (accessed
    on 9 February 2023). Government of Catalonia Department of Agriculture Livestock
    Fisheries and Food. Available online: https://analisi.transparenciacatalunya.cat
    (accessed on 9 February 2023). The Danish Agency for Agriculture. Available online:
    https://lbst.dk/landbrug/ (accessed on 9 February 2023). Netherlands Enterprise
    Agency. Available online: https://nationaalgeoregister.nl/geonetwork/srv/dut/catalog.search#/home
    (accessed on 9 February 2023). Disclaimer/Publisher’s Note: The statements, opinions
    and data contained in all publications are solely those of the individual author(s)
    and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2023 by
    the author. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Selea, T. AgriSen-COG, a Multicountry, Multitemporal Large-Scale
    Sentinel-2 Benchmark Dataset for Crop Mapping Using Deep Learning. Remote Sens.
    2023, 15, 2980. https://doi.org/10.3390/rs15122980 AMA Style Selea T. AgriSen-COG,
    a Multicountry, Multitemporal Large-Scale Sentinel-2 Benchmark Dataset for Crop
    Mapping Using Deep Learning. Remote Sensing. 2023; 15(12):2980. https://doi.org/10.3390/rs15122980
    Chicago/Turabian Style Selea, Teodora. 2023. \"AgriSen-COG, a Multicountry, Multitemporal
    Large-Scale Sentinel-2 Benchmark Dataset for Crop Mapping Using Deep Learning\"
    Remote Sensing 15, no. 12: 2980. https://doi.org/10.3390/rs15122980 Note that
    from the first issue of 2016, this journal uses article numbers instead of page
    numbers. See further details here. Article Metrics Citations Scopus   2 Crossref   2
    Google Scholar   [click to view] Article Access Statistics Article access statistics
    Article Views 8. Jan 18. Jan 28. Jan 7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28.
    Mar 0 500 1000 1500 2000 For more information on the journal statistics, click
    here. Multiple requests from the same IP address are counted as one view.   Remote
    Sens., EISSN 2072-4292, Published by MDPI RSS Content Alert Further Information
    Article Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs
    at MDPI Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: '>'
  journal: Remote Sensing
  limitations:
  - The dataset does not include information on crop yields, which may limit its usefulness
    for some applications.
  - The dataset is limited to data from Europe, which may limit its generalizability
    to other regions.
  relevance_evaluation:
    extract_1: '"With the increasing volume of collected Earth observation (EO) data,
      artificial intelligence (AI) methods have become state-of-the-art in processing
      and analyzing them. However, there is still a lack of high-quality, large-scale
      EO datasets for training robust networks. This paper presents AgriSen-COG, a
      large-scale benchmark dataset for crop type mapping based on Sentinel-2 data."'
    extract_2: 'AgriSen-COG deals with the challenges of remote sensing (RS) datasets.
      First, it includes data from five different European countries (Austria, Belgium,
      Spain, Denmark, and the Netherlands), targeting the problem of domain adaptation.
      Second, it incorporates an anomaly detection based on autoencoder as preprocessing
      step, which reduces the amount of mislabeled information. AgriSen-COG comprises
      6,972,485 parcels, making it the most extensive available dataset for crop type
      mapping. It includes two types of data: pixel-level data and parcel aggregated
      information. By carrying this out, we target two computer vision (CV) problems:
      semantic segmentation and classification.'
    limitations:
    - The dataset is limited to data from five European countries, which may limit
      its generalizability to other regions.
    relevance_score: 1.0
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  title: AgriSen-COG, a Multicountry, Multitemporal Large-Scale Sentinel-2 Benchmark
    Dataset for Crop Mapping Using Deep Learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Chamara, N., Ge, Y., & Bai, G. (2023). AICropCAM: Deploying classification,
    segmentation, detection, and counting deep-learning models for crop monitoring
    on the edge. Computers and Electronics in Agriculture, 215, 108420.'
  authors:
  - Chamara N.
  - Bai G.
  - Ge Y.
  citation_count: '1'
  data_sources: Field crop images collected offline
  description: 'Precision Agriculture (PA) promises to meet the future demands for
    food, feed, fiber, and fuel while keeping their production sustainable and environmentally
    friendly. PA relies heavily on sensing technologies to inform site-specific decision
    supports for planting, irrigation, fertilization, spraying, and harvesting. Traditional
    point-based sensors enjoy small data sizes but are limited in their capacity to
    measure plant and canopy parameters. On the other hand, imaging sensors can be
    powerful in measuring a wide range of these parameters, especially when coupled
    with Artificial Intelligence. The challenge, however, is the lack of computing,
    electric power, and connectivity infrastructure in agricultural fields, preventing
    the full utilization of imaging sensors. This paper reported AICropCAM, a field-deployable
    imaging framework that integrated edge image processing, Internet of Things (IoT),
    and LoRaWAN for low-power, long-range communication. The core component of AICropCAM
    is a stack of four Deep Convolutional Neural Networks (DCNN) models running sequentially:
    CropClassiNet for crop type classification, CanopySegNet for canopy cover quantification,
    PlantCountNet for plant and weed counting, and InsectNet for insect identification.
    These DCNN models were trained and tested with >43,000 field crop images collected
    offline. AICropCAM was embodied on a distributed wireless sensor network with
    its sensor node consisting of an RGB camera for image acquisition, a Raspberry
    Pi 4B single-board computer for edge image processing, and an Arduino MKR1310
    for LoRa communication and power management. Our testing showed that the time
    to run the DCNN models ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet,
    and power consumption ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet.
    The classification model CropClassiNet reported 94.5 % accuracy, and the segmentation
    model CanopySegNet reported 92.83 % accuracy. The two object detection models
    PlantCountNet and InsectNet reported mean average precision of 0.69 and 0.02 for
    the test images. Predictions from the DCNN models were transmitted to the ThingSpeak
    IoT platform for visualization and analytics. We concluded that AICropCAM successfully
    implemented image processing on the edge, drastically reduced the amount of data
    being transmitted, and could satisfy the real-time need for decision-making in
    PA. AICropCAM can be deployed on moving platforms such as center pivots or drones
    to increase its spatial coverage and resolution to support crop monitoring and
    field operations.'
  doi: 10.1016/j.compag.2023.108420
  explanation: The main goal of this paper is to demonstrate the AICropCAM, a novel
    edge image processing framework to monitor crop growth and extract quality parameters
    from field images and enable real-time, low-latency decision making applications
    in precision agriculture.
  extract_1: '"Precision Agriculture (PA) promises to meet the future demands for
    food, feed, fiber, and fuel while keeping their production sustainable and environmentally
    friendly."'
  extract_2: '"This paper reported AICropCAM, a field-deployable imaging framework
    that integrated edge image processing, Internet of Things (IoT), and LoRaWAN for
    low-power, long-range communication."'
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusion and future perspectives Funding
    CRediT authorship contribution statement Declaration of Competing Interest Acknowledgements
    Data availability References Show full outline Cited by (1) Figures (12) Show
    6 more figures Tables (7) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6 Show
    all tables Computers and Electronics in Agriculture Volume 215, December 2023,
    108420 AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge Author links open overlay
    panel Nipuna Chamara a, Geng Bai a, Yufeng Ge a b Show more Share Cite https://doi.org/10.1016/j.compag.2023.108420
    Get rights and content Under a Creative Commons license open access Highlights
    • We developed AICropCAM, an edge-computing enabled camera system for crop monitoring.
    • It integrated Raspberry Pi and Arduino for image processing and LoRa communication.
    • It ran a stack of four deep neural networks to characterize multiple plant/canopy
    parameters. • We quantified the power consumption and speed of the system for
    edge image-processing. • AICropCAM is a next-generation enabling technology for
    real-time, low-latency Ag applications. Abstract Precision Agriculture (PA) promises
    to meet the future demands for food, feed, fiber, and fuel while keeping their
    production sustainable and environmentally friendly. PA relies heavily on sensing
    technologies to inform site-specific decision supports for planting, irrigation,
    fertilization, spraying, and harvesting. Traditional point-based sensors enjoy
    small data sizes but are limited in their capacity to measure plant and canopy
    parameters. On the other hand, imaging sensors can be powerful in measuring a
    wide range of these parameters, especially when coupled with Artificial Intelligence.
    The challenge, however, is the lack of computing, electric power, and connectivity
    infrastructure in agricultural fields, preventing the full utilization of imaging
    sensors. This paper reported AICropCAM, a field-deployable imaging framework that
    integrated edge image processing, Internet of Things (IoT), and LoRaWAN for low-power,
    long-range communication. The core component of AICropCAM is a stack of four Deep
    Convolutional Neural Networks (DCNN) models running sequentially: CropClassiNet
    for crop type classification, CanopySegNet for canopy cover quantification, PlantCountNet
    for plant and weed counting, and InsectNet for insect identification. These DCNN
    models were trained and tested with >43,000 field crop images collected offline.
    AICropCAM was embodied on a distributed wireless sensor network with its sensor
    node consisting of an RGB camera for image acquisition, a Raspberry Pi 4B single-board
    computer for edge image processing, and an Arduino MKR1310 for LoRa communication
    and power management. Our testing showed that the time to run the DCNN models
    ranged from 0.20 s for InsectNet to 20.20 s for CanopySegNet, and power consumption
    ranged from 3.68 W for InsectNet to 5.83 W for CanopySegNet. The classification
    model CropClassiNet reported 94.5 % accuracy, and the segmentation model CanopySegNet
    reported 92.83 % accuracy. The two object detection models PlantCountNet and InsectNet
    reported mean average precision of 0.69 and 0.02 for the test images. Predictions
    from the DCNN models were transmitted to the ThingSpeak IoT platform for visualization
    and analytics. We concluded that AICropCAM successfully implemented image processing
    on the edge, drastically reduced the amount of data being transmitted, and could
    satisfy the real-time need for decision-making in PA. AICropCAM can be deployed
    on moving platforms such as center pivots or drones to increase its spatial coverage
    and resolution to support crop monitoring and field operations. Graphical abstract
    Download : Download high-res image (227KB) Download : Download full-size image
    Previous article in issue Next article in issue Keywords Artificial intelligenceComputer
    visionEdge computingInternet of thingsLoRaWANPrecision agriculture 1. Introduction
    The demands for food, feed, fiber, and fuel increase rapidly due to the fast expansion
    of the global population, income growth, technological advancement, and transport
    and logistics improvements (van Dijk et al., 2021). Precision agriculture (PA),
    which seeks to apply the right amount of inputs (fertilizers, irrigation water,
    pesticides, and other chemicals) in the right location at the right time, is essential
    to meet the requirements of future global food production, as well as environmental
    sustainability and climate resilience. PA is predicated on accurate sensor measurements,
    timely and sound decision-making, and automated actuators. The backbone of PA
    is the Internet of Things (IoT) technology that automates data collection, data
    analytics, data presentation, control, and efficient data communication (Chamara
    et al., 2022). Imaging sensors or digital cameras are essential for PA as they
    can capture more information than traditional scalar or vector sensors. Images
    can capture crop phenology for precise decision-making (Taylor and Browning, 2022,
    Tian et al., 2020). Cyclic events such as vegetative growth, flowering, leaf count
    and color change, maturation, and senescence are studied in crop phenology, which
    is essential to PA as it determines the management inputs required by crops. Moreover,
    images have rich information on the scene that allows for pest pressure evaluation.
    At present, a limited number of sensors are available for pest identification
    and pest pressure estimation. Among them, imaging sensors provide the most promising
    solution. Conventional (handcrafted feature extraction) and Artificial Intelligence
    (AI)-based image processing are the two branches of image processing. Traditional
    approaches extract image features defined by shape, texture, and color (Anubha
    et al., 2019, Yuan et al., 2019). The AI-based methods use Convolutional Neural
    Networks (CNN) to extract features from images (Luis et al., 2020). CNN models
    with multiple hidden layers for feature extraction and learning are considered
    Deep Convolutional Neural Networks (DCNN) (LeCun et al., 1998). Conventional imaging
    platforms in PA store images locally using onboard storage memories. Post processing
    refers to the processing of images stored at the central data storage in batches
    at a later time to extract useful information (Aasen et al., 2020). Imaging platforms
    that can access the internet through a stable connection with high bandwidth can
    automatically upload images to Cloud data storage. The vast majority of farmlands
    worldwide are in rural and remote areas with poor access to electric power and
    internet connectivity. This represents a big challenge for camera systems deployed
    in rural farmlands for high-speed image processing, data transmission, and low-latency
    decision-making (Richardson, 2019). Post-processing of crop images has been used
    for the estimation of leaf area index (Aasen et al., 2020), growth rate (Sakamoto
    et al., 2012), leaf chlorophyll and nitrogen content (Wang et al., 2014), fruit
    counts (Wang et al., 2014), and plant height (Sritarapipat et al., 2014). Further
    post-image processing allows for the assessment of biotic stress, such as pest
    density (Barbedo, 2014; Park et al., 2007) and weed pressure (Wang et al., 2019),
    as well as abiotic stress, such as nutrient deficiency (Ghorai et al., 2021).
    Richardson (2019) suggested that deep learning-based methods have the potential
    to facilitate the extraction of more sophisticated phenological data from both
    new and previously archived camera imagery compared to conventional image processing.
    Semantic segmentation-based canopy coverage (CC) estimation (Chamara et al., 2021;
    Liang et al., 2023), image classification-based crop identification (Anubha et
    al., 2019), disease identification (Sharma et al., 2020), growth stage prediction
    (Yasrab et al., 2021) and object detection-based plant feature identification
    (A. Wang et al., 2019) are examples of DCNN applications in agriculture. Conventional
    image processing requires less computational power and less energy, but they are
    limited in adaption to new scenarios, while deep learning requires high computational
    power and consumes more energy. DCNN models require large memory due to the large
    number of parameters these models hold. Therefore, it is not easy to implement
    these models practically in embedded systems that have less memory and computation
    power. These models also require a large amount of data to train to predict with
    high accuracy. Therefore, it is resource intensive. Edge image processing is the
    image processing done on image-capturing devices. The main advantage of edge image
    computing is that it lowers the high throughput data transmission requirement
    over a wireless IoT-enabled imaging network (Cao et al., 2020). Wang et al. (2022a)
    demonstrated the capability of identifying potted flowers with precision above
    89 % in real-time in a Jetson TX 2 computing module based on a DCNN algorithm.
    These authors suggested that a cloud-edge collaborative framework could achieve
    real-time and automatic learning for the DCNN model they have developed. Wang
    et al. (2022b) proposed a real-time weed detection model run on Jetson AGX Xavier
    for field robots. The authors proved it was possible to do real-time weed detection
    with a precision above 90 % yet required expensive hardware. Wang et al. (2022a)
    reviewed Raspberry Pi single-board computer-based real-time image processing applications.
    They concluded that Raspberry Pi (Datasheet Raspberry Pi Model B, 2019) is a cost-effective
    edge computing unit that could potentially be used as an edge image processing
    unit, and the capability of integrating it with IoT was also discussed. Zualkernan
    et al. (2022) demonstrated an edge image processing platform for the classification
    of animals and transmitting the identified animal and time of identification via
    LoRa for a camera trap. Past literature on IoT and image processing applications
    in agriculture has highlighted a research gap in edge image processing with IoT-enabled
    crop monitoring cameras. In-field crop cameras are expected to make real-time
    crop management decisions based on real-time image processing; however, poor internet
    connectivity in agricultural fields severely limits their capability. To address
    this gap, we have developed a novel imaging platform named AICropCAM that extracts
    plant and crop canopy level parameters through DCNN and uploads them to the Cloud
    via low-power, low-throughput communication protocols. We also demonstrated AICropCAM
    on an IoT-enable wireless sensor network in corn and soybean fields. A technology
    that addresses image processing at the lowest level (edge) and transmits only
    useful information can revolutionize real-time decision-making in PA. Therefore,
    the main objective of this paper is to demonstrate AICropCAM to perform edge image
    processing and low-throughput, low-power, and long-range data transmission through
    IoT technology. In this novel AICropCAM platform, multiple DCNN image processing
    algorithms run in series to extract plant-level and canopy-level features in an
    embedded system. Image classification, object detection with classification, and
    image segmentation are the three major applications of DCNN image processing,
    and all three are included in AICropCAM to demonstrate the capabilities of DCNN
    for image processing in PA. AICropCAM has trained models for canopy segmentation,
    crop classification, plant growth stage identification, plant counting, weed counting,
    and plant type identification. All the protocols that transmit data from AICropCAM
    to the Cloud were custom designed. AICropCAM sends the generated data to a cloud
    platform for logging, visualization, and analysis. Furthermore, this paper explains
    the DCNN model training process, model performance, and test results. We reported
    the model training comprehensively because it was essential for AICropCAM development.
    2. Materials and methods Essential activities in this research were data/image
    collection and preprocessing, hardware design for AICropCAM, software design for
    data transmission between the edge and the Cloud, deep learning model design,
    and model training and optimization (Fig. 1). AICropCAM was implemented in a corn
    and soybean field at the field phenotyping facility in Mead, Nebraska, USA (Bai
    et al., 2019). We demonstrated the training of the following DCNNs: CropClassiNet
    for classifying images based on image quality and crop type, CanopySegNet for
    segmenting crop canopy from the background, PlantCountNet for classifying and
    counting soybean and weed plants, and InsectNet for identifying insects and counting
    them. Download : Download high-res image (412KB) Download : Download full-size
    image Fig. 1. Steps of edge image processing program deployment on the embedded
    system (edge devices). 2.1. Image collection, annotation, preprocessing, and augmentation
    Image collection for DCNN model training occurred in four growing seasons using
    three different types of cameras: (i) commercially available Meidas SL122 trail
    cameras in 2019 (Meidas Trail Cameras, 2022), (ii) OV5642 imaging sensors with
    ArduCAM camera shields in 2020, and (iii) Raspberry Pi Camera Module V2 with Raspberry
    Pi Zero in 2021 and 2022 (Chamara, 2021). All the cameras were mounted on the
    bars horizontally extended and fixed on stationary poles erected vertically in
    the fields, as shown in Fig. 2A. The distance between the crop canopy and the
    cameras was maintained between 0.5 and 1.5 m throughout the growing seasons. Images
    used for training the InsectNet were also captured with smartphones as we could
    not collect enough images with insects from the three types of cameras mentioned
    above. Download : Download high-res image (338KB) Download : Download full-size
    image Fig. 2. Left: An Illustration of how AICropCAM was set up in the field for
    image collection. In addition to the camera, other components such as the solar
    panel and data logger were also shown. Right: A close-up view of AICropCAM and
    its hardware components. All three standard image annotation techniques in deep
    learning model training were utilized: (1) folder labeling for the image classification
    models, (2) pixel-level annotation for the semantic segmentation model, and (3)
    bounding boxes for object detection models. Images belonging to the same class
    were grouped into a single folder, and five distinct classes (or folders) were
    created: rejected, corn, soybean, grass, and night. Separating the crop canopy
    from the soil was done with pixel-level annotation and semantic segmentation.
    Bounding boxes, the smallest rectangle around an object, were drawn for corn plants,
    soybean plants, weed plants, and insects. Table 1 explains each type of annotation
    used in the model training. Table 1. Annotation criteria used to generate labels
    from the images to train and test the four deep convolutional neural network models
    in AICropCAM. Labeling Type Class Description Image classification (CropClassiNet)
    Rejected Images were labeled as rejected due to multiple reasons: blurred images
    caused by water droplets on the lens; the cameras turned away from the targeted
    crop; crops growing up to the camera blocking the view or capturing only a few
    leaves; people present in the images; lens covered with different stuff; and the
    camera was not installed in the field. Corn Images entirely covered by corn plants
    at different growth stages. Soybean Images entirely covered by soybean plants
    at different growth stages. Grass/Weed Images only comprise grass/weed plants
    at different growth stages. Night Images captured under low lighting conditions.
    Most of the cameras were not programmed to stop collecting images under low light.
    Crop canopy and background (CanopySegNet) Canopy Pixel labeling was done on the
    crop canopy. We used assisted freehand tool and the superpixel option in the MATLAB
    image labeler. Background Pixel labeling was done on the crop canopy. We used
    assisted freehand tool and the superpixel option in the MATLAB image labeler.
    Plant-type (PlantCountNet) Weed Weed present in the image was labeled using bounding
    boxes. It was challenging to locate the weed after the corn or soybean canopy
    was closed. Soybean Soybean plants present in the image were labeled using bounding
    boxes. Insects (InsectNet) Insects During the labeling process, without distinguishing
    insects based on their type, all the insects present in the images were labeled
    using bounding boxes. Image preprocessing is necessary for DCNN model training
    and real-time edge image processing. Differences in the input layer size in different
    DCNN models demand that images be resized before passing through the model for
    training or prediction purposes. High-resolution images improve accuracy but require
    more computational power. For specific applications, labeled datasets were only
    limitedly available. Therefore, image augmentation techniques were used to increase
    the number of image data sets, including scaling, flipping, cropping, rotation,
    color transformation, PCA color augmentation, and noise rejection (Paymode and
    Malode, 2022). Multiple augmentation techniques were used for each model, as detailed
    in Table 2. Additionally, Table 2 provides the numbers of images in training,
    validation, and testing for the four DCNN models. Table 2. DCNN model image allocation
    and image augmentation. Model Number of images Data Augmentation Techniques Total
    Training Validation Test CropClassiNet 43,611 30,528 9,810 3,273 Random rotation,
    random X  and Y reflection CanopySegNet 51 31 10 10 Transformation (random left/right
    reflection and random X/Y translation of ±10 pixels) PlantCountNet 110 88 11 11
    Transformation (same as CanopySegNet) InsectNet 542 326 108 108 Transformation
    (same as CanopySegNet) Our main objective was not to make the most accurate prediction
    for the DCNN models but to demonstrate the concept of implementing edge image
    processing and transmitting the results to the Cloud for decision-making. Therefore,
    we selected a limited number of images for CanopySegNet, PlantCountNet, and InsectNet,
    which were sufficient to train models with a reasonable degree of accuracy. 2.2.
    DCNN model architecture selection, training, evaluation, and deployment on the
    edge device The steps to select model architecture/model backbone weights and
    image input sizes to train the best model for CropClassiNet, CanopySegNet, PlantCountNet,
    and InsectNet are summarized in Fig. 3. Unlike many DCNN applications that prioritize
    higher accuracy, our application focused on finding the balance between accuracy
    and model deployability on the edge device. Download : Download high-res image
    (771KB) Download : Download full-size image Fig. 3. DCNN model selection process
    during the training and testing by attempting different model architectures, model
    backdone weights, and input image sizes. For example, in the development of CropSegNet
    (Segmentation), we selected DeepLabv3+ (Firdaus-Nawi et al., 2018) with weights
    initialized from pre-trained networks of ResNet18 (He et al., 2016), ResNet50,
    Xception, InceptionresnetV2, and MobileNetV2. The input image sizes tested were
    512 × 512 × 3 and 256 × 256 × 3, and training options were kept constant to find
    the best-performing networks, which should also be deployable to Raspberry Pi
    4B. This process identified DeepLabv3 + with ResNet50 as the most suitable model
    for CropSegNet, with an input image size of 512 × 512 × 3. Table 3 summarizes
    the hyperparameter values and training options for the final DCNN models deployed
    to the edge device. (1) (2) (3) (4) (5) (6) (7) Table 3. Hyperparameter values
    and training options for the best models (SGDM - stochastic gradient descent with
    momentum, RMSProp - Root mean square propagation). Training option and the function/Hyperparameters
    Values for CropClassiNet Values for CanopySegNet Values for InsectNet (320 × 320
    × 3) Values for PlantCountNet (320 × 320 × 3) Optimizer SGDM SGDM SGDM RMSProp
    Momentum 0.9 0.9 0.99 NA Initial learning rate 0.001 0.001 0.001 0.001 Learn rate
    schedule Piecewise Piecewise Piecewise Piecewise Learn rate drop period 10 10
    10 10 Learn rate drop factor 0.3 0.3 0.1 0.3 Minibatch size 16 4 16 32 L2Regularization
    NA 0.005 0.005 0.005 Validation frequency 3 3 3 10 Shuffle Every epoch Every epoch
    Every epoch Every epoch Validation patience 4 10 10 10 Max epochs 100 300 1000
    100 Execution environment Multi GPU Multi GPU GPU GPU The performance of the four
    DCNN models was evaluated using the indices calculated from Eq. (1), (2), (3),
    (4), (5), (6), (7). Accuracy, Precision, Recall, F1 score, and Jaccard index were
    used for the classification models CropClassiNet and CropSegNet, whereas IoU and
    mAP (Mean Average Precision) were used for PlantCountNet and InsectNet. Jaccard
    index gives the proportion of correctly predicted labels to the total number of
    labels. Model training was performed on an NVIDIA GeForce GTX 1650 Ti Mobile processor,
    a dedicated mid-range graphics card with 4 GB GDDR6 memory on a Dell XPS 15 9500
    Laptop. The laptop had an Intel Core i7-10750H 10th Gen processor,16 GB DDR4 RAM,
    and 1 TB SSD hard disk. 2.3. Hardware and software of AICropCAM The IoT data transmission
    and edge image processing hardware comprised the following major components: a
    Raspberry Pi 4B single-board computer, an Arduino MKR1310 development board, an
    Arduino MKR Relay Proto Shield, and a Dragino OLG02 outdoor dual channels LoRa
    Gateway (Fig. 4). The 12 V 8Ah battery powered the Raspberry Pi 4B, controlled
    through the relay shield managed by the Arduino MKR1310. A 3.7 V lithium polymer
    battery powered the Arduino MKR1310 board. There are two advantages of having
    a separate Arduino board. First, the Arduino board consumes less power than the
    Raspberry Pi 4B module. It can be switched on and off according to user requirements.
    Second, it allows uninterrupted communication between the edge node and the Cloud
    with low power. Download : Download high-res image (303KB) Download : Download
    full-size image Fig. 4. Hardware overview of AICropCAM and data flow. AICropCAM
    required programming on two hardware platforms. Arduino was programmed using C++
    in Arduino’s Integrated Development Environment. Raspberry Pi imaging and image
    processing program was developed in MATLAB and deployed onto the Raspberry Pi
    4B using the MATLAB Coder and MATLAB Compiler. A python program was designed to
    read the saved data in the Raspberry Pi 4B and serially communicate to the Arduino
    MKR1310. The primary functions of the MRK1310 program were to (1) turn on the
    Raspberry Pi 4B module based on the user-defined time intervals, (2) get the processed
    data, including the results of DCNN model predictions, through serial communication
    from the Raspberry Pi 4B, and (3) transmit the data to the ThingSpeak Cloud channel
    through the LoRa gateway. All the DCNN models were trained using the MATLAB deep
    learning toolbox. In the edge deployment, a MATLAB program runs multiple models
    logically depending on the prediction result of the previous model estimation,
    as shown in Fig. 5. MATLAB coder generated the C and C++ code derived from the
    program we developed to run on the Raspberry Pi. MATLAB Compiler generated the
    standalone application on the Raspberry Pi (The MathWorks, 2022). Download : Download
    high-res image (477KB) Download : Download full-size image Fig. 5. Overall sequential
    image processing and data generation flow chart. Table 4 lists the parameters
    generated by the models in AICropCAM. The abbreviations in Table 4 are fields
    holding data in the program to reduce the complexity of system development and
    maintain a common standard among different platforms. Fig. 6 shows the data generation
    from images. According to Fig. 6, the size of the images were around 2 MB before
    being fed into the image processing pipeline. The output message contains the
    crop type (CT), plant count (PC), weed count (WC), canopy coverage (CC), and pest
    count (PstC). The resulting message is typically less than 100 bytes. This represents
    a substantial reduction of memory size with the output being 0.00005 times the
    size of the original image. Consequently, this message can be transmitted in a
    single message via LoRa as the maximum LoRa packet size is around 256 bytes. Table
    4. List of parameters used to represent information in the images. Parameter Abbreviation
    Represent information Image location LOC Node ID manually entered/Global positioning
    system location coordinates Image orientation IO Accelerometer/Manually feed/Gravity
    switch Image quality/Crop type CT Image classification based on image quality
    and the crop type Plant count/Weed count PC/WC Multiclass object detection/classification
    Crop canopy coverage CC Semantic segmentation Pest count PstC Multiclass object
    detection/classification Download : Download high-res image (2MB) Download : Download
    full-size image Fig. 6. Examples of message generation and data size reduction
    for LoRa transmission. 2.4. Data transmission, visualization, and storage The
    data generated after image processing were saved on the Raspberry Pi 4B SD card,
    allowing access to the data remotely or through manual retrieval during field
    visits. Two options for transmitting the collected data to the ThingSpeak IoT
    platform are available. Firstly, the data can be uploaded directly from the Raspberry
    Pi 4B if internet connectivity is available for growers with Wi-Fi access. Secondly,
    the Raspberry Pi 4B transmits the recently acquired data to the Arduino MKR1310.
    The Arduino MKR1310 decodes the data received from the Raspberry Pi 4B and forwards
    it to the ThingSpeak. The second method is for low-rate, long-range communication
    beyond the limit of Wi-Fi. A single message receivable to the ThingSpeak server
    includes data for eight fields. In our demonstration, a single message was enough
    to transmit the data generated. Fields 1 and 2 are reserved for geographic coordinates
    (namely, latitude and longitude) to represent the device''s location. The third
    field was for camera orientation. Image quality/crop type, plant count, weed count,
    insect count, and crop canopy coverage were allocated from fields four to eight.
    ThingSpeak supports eight channels per gateway. If additional data is generated
    in the future, we have to create new channels to accommodate them. However, only
    data in a single channel can be passed through a single message. The Arduino-LoRa
    library was used to prepare the LoRa messages forwarded to the gateway (Mistry,
    2016). The message generated from the Arduino MKR1310 includes the device identification
    number and the data with the field number. Once the gateway receives this message,
    it adds the target client ID (generated by ThingSpeak when defining a device),
    host address (mqtt://mqtt3.thingspeak.com), server port number, username and password,
    channel ID, and the data in each field according to the Message Queuing Telemetry
    Transport (MQTT) protocol. Username and password ensure that only authorized devices
    can transmit data to the ThingSpeak platform. ThingSpeak provides two ways to
    interact with its platform, REST (Representative State Transfer) and MQTT protocols.
    The advantages of using MQTT over REST protocol are that it supports ThingSpeak
    data publishing, including immediate and minimum power consumption and data transmission
    over limited bandwidth, which encouraged us to select the MQTT protocol in our
    demonstration. 3. Results and discussion 3.1. DCNN model performance CropClassiNet
    had a test accuracy of 91.26 %, a Jaccard Index of 0.77, and an F1-score of 0.91;
    the confusion matrix is given in Fig. 7. The highest precision is for the “grass”
    class (100 %), and the lowest is for “soybean” (92.0 %). The highest recall is
    for the “corn” class (99.9 %), and the lowest is for “grass” (67.1 %). The primary
    goal of CropClassiNet is to determine the quality of new images and direct them
    for subsequent processing (Fig. 5). This step has never been executed in an image-based
    crop monitoring platform before. Further, CropClassiNet can eliminate erroneous
    images when humans are present in the camera’s field of view or when the camera
    is misaligned due to external factors. AICropCAM can send maintenance requests
    through IoT analytics if rejected images are continuously generated. Download
    : Download high-res image (275KB) Download : Download full-size image Fig. 7.
    Confusion matrix for test images by CropClassiNet. CanopySegNet on the test images
    achieved a global accuracy of 0.93, a weighted IoU of 0.87, and a mean BF score
    of 0.73. Fig. 8 shows an example of an original soybean image and the corresponding
    segmentation result by CanopySegNet, which estimated CC to be 18.72 %. Season-long,
    time-series images can be fed into CanopySegNet to generate diurnal and seasonal
    curves of crop CC, as shown in Fig. 9. Download : Download high-res image (621KB)
    Download : Download full-size image Fig. 8. An image of soybean crop and the segmentation
    result by CropSegNet to calculate canopy coverage. Download : Download high-res
    image (367KB) Download : Download full-size image Fig. 9. Examples of diurnal
    and seasonal variations of canopy coverage as computed by CropSegNet. According
    to Fig. 9, canopy coverage percentage variation is low during the daytime and
    reaches zero at night. This verifies the need to eliminate low-light images before
    segmenting. As shown in Fig. 5, it is possible to eliminate the generation of
    false values when the camera captures images under low light conditions by halting
    the process of running CanopySegNet. There are three diurnal variation series
    on 6/8/2021, 6/26/2021, and 7/12/2021 in Fig. 9. The CC increased from 8 % to
    95 % between 6/8/2021 to 7/12/2021. The seasonal trend showed that the CC reached
    a maximum around 7/8/2021. These results suggest that the proposed stacked models
    can track the daily and seasonal CC variation and eliminate the effect of lighting
    conditions on false value generation. Table 5. Performance of PlantCountNet and
    InsectNet on the test image set (Root mean square error (RMSE)/Final validation
    loss (FVL)). Model Name Architecture Input size Validation RMSE/FVL Mean average
    precision Object class PlantCountNet YOLOv2 320 × 320 × 3 0.888 (RMSE) 0.66 Soybean
    0.86 Weed InsectNet YOLOv4 320 × 320 × 3 26.2 (FVL) 0.02 Insect The overall performance
    of the PlantCountNet and InsectNet is given in Table 5. Fig. 10(A) and 10(B) show
    the result obtained by PlantCountNet for a soybean image at an early vegetative
    stage (V3). Meanwhile Fig. 10(C) and 10(D) shows the result at a reproductive
    stage (R1). It can be seen that, at V3 stage, the model outputs matched the labels
    of soybean and weed plants well, indicating a level of high accuracy. Download
    : Download high-res image (1MB) Download : Download full-size image Fig. 10. The
    result of PlantCountNet for soybean and weed counting: Manually annotated vs.
    model-predicted bounding boxes at V3 growth stage (A and B); manually annotated
    vs. model-predicted bounding boxes at R1 growth stage (C and D). The size of insects
    is very small compared to the size of images (Fig. 11), which is the main reason
    for the low mAP for InsectNet (Table 5). Increasing input image resolution beyond
    480 × 480 × 3 is impractical as it exceeds the memory limitation to load models
    into Raspberry Pi 4B. A potential solution could be to increase the resolution
    of the region of interest by splitting the original image while keeping the resolution
    the same. Also, we suggest using the approach recommended by Tetila et al., 2020a,
    Tetila et al., 2020b in the future on Raspberry Pi model 4B. As technology advances,
    we expect the memory capacities will increase for edge computing units. At the
    same time, the state-of-the-art object detection algorithms will improve the accuracy
    for small object detection. Download : Download high-res image (1MB) Download
    : Download full-size image Fig. 11. The result of InsectNet for insect counting
    in soybean. The top row shows a situation of high false positives and low false
    negatives: (A) and (B) are manually annotated and model-predicted insect labels,
    respectively. The bottom row shows a situation of low false positive and high
    false negative: (C) and (D) are manually annotated and model-predicted insect
    labels. 3.2. Power consumption for Raspberry Pi 4B Since edge cameras in farmlands
    have limited access to electric power, information on their power consumption
    is essential for designing IoT devices and systems. AICropCAM is designed to be
    energized by solar power. It runs on a rechargeable battery when there is no solar
    power. We monitored the maximum energy consumption of each task performed by AICropCAM,
    and the result is presented in Table 6. Four main strategies are available for
    the power management of IoT edge devices: Selecting power-efficient hardware,
    maintaining low power modes, dynamic power management, and cloud-based management.
    Raspberry Pi 4B is an affordable power-efficient single-board computer suitable
    for our application, but it does not naturally support low-power modes. Therefore,
    we introduced the Arduino MKR1310 LoRa module for the Raspberry Pi 4B dynamic
    power management. Furthermore, this Arduino module allows us to perform cloud-based
    central management independently. Table 6. Electrical power consumption of the
    Raspberry Pi 4B and the Arduino MKR1310 during edge image processing. Device Activity
    The maximum current range and the voltage recorded Raspberry Pi 4B Idle run 5.25
    V × (0.45 – 0.53) A Image classification 5.25 V × (0.97 – 1.04) A Image segmentation
    5.25 V × (0.98 – 1.11) A Weed and plant detection 5.25 V × (0.62 – 0.70) A Insect
    detection 5.25 V × (0.62 – 0.70) A Arduino MKR1310 Sleep mode <0.01A Serial communication
    <0.01A LoRa transmission <0.01A For our measurements, we used a Raspberry Pi 4B
    with 8 GB of RAM, connected to an HDMI monitor, a USB keyboard, and a USB mouse,
    and ran a MathWorks® Raspbian image (file used to boot the Raspberry Pi 4B). The
    Raspberry Pi 4B was operated at room temperature and connected to a wireless LAN
    access point and a laptop via an Ethernet cable. The electric current consumption
    for running each DCNN model was recorded during the test. CropClassiNet had the
    highest current consumption, while the PlantCountNet and InsectNet models had
    the lowest. As for LoRa transmission, we could not measure its current consumption
    because the lowest value our instrument could measure was 0.01A. Based on the
    manufacturer''s specifications, the Arduino MKR1310 consumes 104 uA at 5 V. The
    average time to run the DCNN models is essential to estimate the energy consumed
    for each prediction. These parameters listed in Table 7 provide essential guidelines
    for designing IoT sensor nodes with suitable batteries and power sources. We also
    noticed that typically the first prediction of a model took the longest time,
    but the rest take a considerably shorter time to predict. Table 7. Time duration
    needed for the selected DCNN models deployed in the Raspberry Pi 4B. Model/Task
    Input image size Time for predicting results (s) The maximum power demand for
    the activity (W) CropClassiNet/Image quality evaluation and crop classification
    224 × 224 × 3 6.44 5.46 CanopySegNet/Semantic segmentation to separate canopy
    from background 512 × 512 × 3 20.20 5.83 PlantCountNet/Weed and plant detection,
    classification, and counting 320 × 320 × 3 14.38 3.68 InsectNet/Insect detection
    320 × 320 × 3 0.20 3.68 Semantic segmentation was the most power-demanding activity,
    while insect detection was the least. Changing the order of the image processing
    models and adding new models or dropping existing models is possible during regular
    operation. It enables dynamic power management within the Raspberry Pi module.
    The main advantage of AICropCAM is that it implements a stack of four DCNN-based
    image processing models with multiple objectives. To the best of our knowledge,
    this is the first time such a system has been developed for a field crop monitoring
    camera. AICropCAM has applications such as setting up smart in-field or greenhouse
    IoT camera networks with edge computing capability, monitoring crops by attaching
    them to sprinkler irrigation systems (pivots and linear moves), or collecting
    crop information through ground or aerial mobile robots. The relatively short
    time to run each DCNN model makes the system suitable for real-time applications,
    including variable rate irrigation, fertilization, and spraying. For example,
    a pivot irrigated multi-cropping system with AICropCAM can automate irrigation
    or fertigation transition between different crops or crops at different growth
    stages by automatically providing the crop type or growth stage information to
    the irrigation controller. Additionally, existing herbicide or pesticide sprayers
    can get the feedback of the PlantCountNet and InsectNet in the AICropCAM for precision
    spraying. 4. Conclusion and future perspectives This paper outlines the essential
    components of constructing a functional edge image processing framework for real-time
    crop monitoring. From a software standpoint, CropClassiNet can categorize captured
    images according to image quality and detect the presence of specific crop types
    for further processing. CanopySegNet can further quantify the degree of canopy
    coverage; PlantCountNet can count the number of plants and weeds in the image;
    and finally, InsectNet can count the number of insects in the image. These four
    DCNN models, when implemented on edge devices, can extract an array of important
    crop and canopy parameters from field images and enable real-time, low-latency
    decision making and applications. Deep learning-based image processing on the
    edge has excellent potential in PA. Applications of AICropCAM are not limited
    to image classification, segmentation, plant counting, or weed counting. Potential
    future applications include insect classification and crop damage estimation,
    weed classification and pressure estimation, fruit identification and yield estimation,
    decision on replanting (Whigham et al., 2000), and disease identification and
    disease damage estimation in real time using actual field images collected by
    AICropCAM. AICropCAM shows excellent potential in enhancing crop management through
    crop monitoring. However, the current demonstration requires significant improvements
    on both hardware and software fronts. Customized circuitry and modular design
    are required to put AICropCAM in commercial farm applications. The full potential
    of the AICropCAM can be achieved by putting this camera on a moving platform like
    a center pivot with a GPS receiver to generate spatiotemporal data. Crop classification
    must include more crop types, and segmentation models need training data from
    other crop types. The DCNN models for weed and insect identification require the
    capability to identify different weed types, their growth stage, different insect
    types, and their growth stages to generate effective pest control decisions. Additionally,
    improving the models’ accuracy in image classification, segmentation, and object
    detection is crucial. It can be achieved by increasing the number of training
    image data sets. We also planned to expand the research for multiple edge architecture
    evaluation. Architectures such as a high-performance edge computer that accepts
    images from multiple edge devices through short-range, high-speed communication
    (e.g., Wi-Fi) and can run more accurate deep learning models with higher numbers
    of parameters, might be a better solution for the primary objectives addressed
    in this paper. We aim to expand the AICropCAM applications to other crops beyond
    corn and soybean. By making these improvements, AICropCAM will become a more effective
    tool for crop management, potentially revolutionizing how we grow and manage crops.
    Funding This work was supported by the United States Department of Agriculture
    – National Institute of Food and Agriculture grants [Award 2020-68013-32371 to
    YG and GB, Award 2021-67021-34417 to YG]. CRediT authorship contribution statement
    Nipuna Chamara: Methodology, Software, Visualization. Geng Bai: Conceptualization,
    Methodology, Resources. Yufeng Ge: Conceptualization, Resources, Supervision,
    Project administration, Funding acquisition. Declaration of Competing Interest
    The authors declare the following financial interests/personal relationships which
    may be considered as potential competing interests: Nipuna Chamara, Yufeng Ge,
    Geng Bai has patent pending to University of Nebraska-Lincoln. Acknowledgements
    Jianxin Sun assisted in developing the imaging device with Raspberry Pi Zero used
    for image acquisition. David Scoby helped the field management and AICropCAM installation.
    Junxiao Zhang supported the field installation of AICropCAM and smart-phone based
    acquisition of crop images with insects. Data availability Data will be made available
    on request. References Aasen et al., 2020 H. Aasen, N. Kirchgessner, A. Walter,
    F. Liebisch PhenoCams for field phenotyping: using very high temporal resolution
    digital repeated photography to investigate interactions of growth, phenology,
    and harvest traits Front. Plant Sci., 11 (June) (2020), pp. 1-16, 10.3389/fpls.2020.00593
    Google Scholar Anubha et al., 2019 P.S. Anubha, V. Sathiesh Kumar, S. Harini A
    study on plant recognition using conventional image processing and deep learning
    approaches J. Intell. Fuzzy Syst., 36 (3) (2019), pp. 1997-2004, 10.3233/JIFS-169911
    Google Scholar ArduCAM, 2016 ArduCAM ESP8266 UNO board User Guide (pp. 0–9). (2016).
    www.ArduCAM.com. Google Scholar Bai et al., 2019 G. Bai, Y. Ge, D. Scoby, B. Leavitt,
    V. Stoerger, N. Kirchgessner, S. Irmak, G. Graef, J. Schnable, T. Awada NU-Spidercam:
    A large-scale, cable-driven, integrated sensing and robotic system for advanced
    phenotyping, remote sensing, and agronomic research Comput. Electron. Agric.,
    160 (March) (2019), pp. 71-81, 10.1016/j.compag.2019.03.009 View PDFView articleView
    in ScopusGoogle Scholar Barbedo, 2014 J.G.A. Barbedo Using digital image processing
    for counting whiteflies on soybean leaves J. Asia Pac. Entomol., 17 (4) (2014),
    pp. 685-694, 10.1016/j.aspen.2014.06.014 View PDFView articleView in ScopusGoogle
    Scholar Cao et al., 2020 K. Cao, Y. Liu, G. Meng, Q. Sun An Overview on Edge Computing
    Research IEEE Access, 8 (2020), pp. 85714-85728, 10.1109/ACCESS.2020.2991734 View
    in ScopusGoogle Scholar Chamara et al., 2021 N. Chamara, K. Alkhadi, H. Jin, F.
    Bai, A. Samal, Y. Ge A deep convolutional neural network based image processing
    framework for monitoring the growth of soybean crops. 2021 ASABE Annual International
    Meeting, 2100259 (2021), 10.13031/aim.202100259 Google Scholar Chamara et al.,
    2022 N. Chamara, M.D. Islam, G.F. Bai, Y. Shi, Y. Ge Ag-IoT for crop and environment
    monitoring: Past, present, and future Agr. Syst., 203, 103497 (2022), 10.1016/j.agsy.2022.103497
    Google Scholar Chamara, 2021 N. Chamara Development of an Internet of Things (IoT)
    Enabled Novel Wireless Multi-Sensor Network for Infield Crop Monitoring. Master’s
    Thesis, Department of Biological Systems Engineering, University of Nebraska-Lincoln
    (2021) Google Scholar Datasheet Raspberry Pi Model, 2019 Datasheet Raspberry Pi
    Model B, 2019. https://datasheets.raspberrypi.org. Accessed 11 November 2023.
    Google Scholar Firdaus-Nawi et al., 2018 Firdaus-Nawi, M., Noraini, O., Sabri,
    M.Y., Siti-Zahrah, A., Zamri-Saad, M., Latifah, H., 2018. DeepLabv3+_Encoder-Decoder
    with Atrous Separable Convolution for Semantic Image Segmentation. In: Proceedings
    of the European Conference on Computer Vision (ECCV), pp. 801–818. Google Scholar
    Ghorai et al., 2021 A.K. Ghorai, A.R. Barman, B. Chandra, K. Viswavidyalaya, S.
    Jash, B. Chandra, K. Viswavidyalaya, B. Chandra, K. Viswavidyalaya Image processing
    based detection of diseases and nutrient deficiencies in plants SATSA Mukhapatra,
    25 (1) (2021), pp. 1-24 Google Scholar He et al., 2016 He, K., Zhang, X., Ren,
    S., Sun, J., 2016. Deep residual learning for image recognition kaiming. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778.
    doi: 10.1002/chin.200650130. Google Scholar LeCun et al., 1998 LeCun, Y., Bottou,
    L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to document
    recognition. Proc. IEEE 86(11), 2278–2323. doi: 10.1109/5.726791. Google Scholar
    Liang et al., 2023 Liang, W. Z., Oboamah, J., Qiao, X., Ge, Y., Harveson, B.,
    Rudnick, D. R., Wang, J., Yang, H., Gradiz, A., 2023. CanopyCAM – an edge-computing
    sensing unit for continuous measurement of canopy cover percentage of dry edible
    beans. Comput. Electron. Agric. 204 (January), 107498. https://doi.org/10.1016/j.compag.2022.107498.
    Google Scholar Luis et al., 2020 Luis, S., Filipe, N.S., Paulo, M.O., Pranjali,
    S., 2020. Deep Learning applications in agriculture: a short review. Deep Learning
    Applications in Agriculture: A Short Review, 1092 AISC(January), C1. doi: 10.1007/978-3-030-35990-4.
    Google Scholar Meidas Trail Cameras, 2022 Meidas Trail Cameras, 2022. https://www.meidase.com/product-category/trail-cameras/.
    Accessed 11 November 2023. Google Scholar Mistry, 2016 Mistry, S., 2016. Arduino
    LoRa. MIT License. https://github.com/sandeepmistry/arduino-LoRa. Accessed 11
    November 2023. Google Scholar Park et al., 2007 Y. Park, R.K. Krell, M. Carroll
    Theory, technology, and practice of site-specific insect pest management J. Asia
    Pac. Entomol., 10 (2) (2007), pp. 89-101 View PDFView articleView in ScopusGoogle
    Scholar Paymode and Malode, 2022 A.S. Paymode, V.B. Malode Transfer learning for
    multi-crop leaf disease image classification using convolutional neural network
    VGG Artif. Intell. Agric., 6 (2022), pp. 23-33, 10.1016/j.aiia.2021.12.002 View
    PDFView articleView in ScopusGoogle Scholar Richardson, 2019 A.D. Richardson Tracking
    seasonal rhythms of plants in diverse ecosystems with digital camera imagery New
    Phytol., 222 (4) (2019), pp. 1742-1750, 10.1111/nph.15591 View in ScopusGoogle
    Scholar Sakamoto et al., 2012 T. Sakamoto, A.A. Gitelson, A.L. Nguy-Robertson,
    T.J. Arkebauer, B.D. Wardlow, A.E. Suyker, S.B. Verma, M. Shibayama An alternative
    method using digital cameras for continuous monitoring of crop status Agric. For.
    Meteorol., 154–155 (2012), p. 113, 10.1016/j.agrformet.2011.10.014 View PDFView
    articleView in ScopusGoogle Scholar Sharma et al., 2020 P. Sharma, Y.P.S. Berwal,
    W. Ghai Performance analysis of deep learning CNN models for disease detection
    in plants using image segmentation Inf. Process. Agric., 7 (4) (2020), pp. 566-574,
    10.1016/j.inpa.2019.11.001 View PDFView articleView in ScopusGoogle Scholar Sritarapipat
    et al., 2014 T. Sritarapipat, P. Rakwatin, T. Kasetkasem Automatic rice crop height
    measurement using a field server and digital image processing Sensors (Switzerland),
    14 (1) (2014), pp. 900-926, 10.3390/s140100900 View in ScopusGoogle Scholar Taylor
    and Browning, 2022 S.D. Taylor, D.M. Browning Classification of daily crop phenology
    in phenocams using deep learning and hidden markov models Remote Sens. (Basel),
    14 (2) (2022), pp. 1-22, 10.3390/rs14020286 Google Scholar Tetila et al., 2020a
    Tetila, E.C., Machado, B.B., Astolfi, G., Belete, N.A.S., Amorim, W.P., Roel,
    A.R., Pistori, H., 2020. Detection and classification of soybean pests using deep
    learning with UAV images. Computers and Electronics in Agriculture, 179(May).
    doi: 10.1016/j.compag.2020.105836. Google Scholar Tetila et al., 2020b E.C. Tetila,
    B.B. MacHado, G.V. Menezes, N.A. De Souza Belete, G. Astolfi, H. Pistori A deep-learning
    approach for automatic counting of soybean insect pests IEEE Geosci. Remote Sens.
    Lett., 17 (10) (2020), pp. 1837-1841, 10.1109/LGRS.2019.2954735 View in ScopusGoogle
    Scholar The MathWorks, 2022 The MathWorks, I., 2022. MATLAB Coder - MATLAB. MathWorks.
    https://www.mathworks.com/products/matlab-coder.html. Google Scholar Tian et al.,
    2020 H. Tian, T. Wang, Y. Liu, X. Qiao, Y. Li Computer vision technology in agricultural
    automation—a review Inf. Process. Agric., 7 (1) (2020), pp. 1-19, 10.1016/j.inpa.2019.09.006
    View PDFView articleView in ScopusGoogle Scholar van Dijk et al., 2021 M. van
    Dijk, T. Morley, M.L. Rau, Y. Saghai A meta-analysis of projected global food
    demand and population at risk of hunger for the period 2010–2050 Nat. Food, 2
    (7) (2021), pp. 494-501, 10.1038/s43016-021-00322-9 View in ScopusGoogle Scholar
    Wang et al., 2022b Q. Wang, M. Cheng, S. Huang, Z. Cai, J. Zhang, H. Yuan A deep
    learning approach incorporating YOLO v5 and attention mechanisms for field real-time
    detection of the invasive weed Solanum rostratum Dunal seedlings Comput. Electron.
    Agric., 199 (July) (2022), Article 107194, 10.1016/j.compag.2022.107194 View PDFView
    articleView in ScopusGoogle Scholar Wang et al., 2022a J. Wang, Z. Gao, Y. Zhang,
    J. Zhou, J. Wu, P. Li Real-time detection and location of potted flowers based
    on a ZED camera and a YOLO V4-tiny deep learning algorithm Horticulturae, 8 (1)
    (2022), 10.3390/horticulturae8010021 Google Scholar Wang et al., 2014 Y. Wang,
    D. Wang, P. Shi, K. Omasa Estimating rice chlorophyll content and leaf nitrogen
    concentration with a digital still color camera under natural light Plant Methods,
    10 (3) (2014), pp. 273-286, 10.1016/S0378-4290(99)00063-5 View in ScopusGoogle
    Scholar Wang et al., 2019 A. Wang, W. Zhang, X. Wei A review on weed detection
    using ground-based machine vision and image processing techniques Comput. Electron.
    Agric., 158 (January) (2019), pp. 226-240, 10.1016/j.compag.2019.02.005 View PDFView
    articleView in ScopusGoogle Scholar Whigham et al., 2000 K. Whigham, D. Farnham,
    J. Lundvall, D. Tranel Soybean replant decision, Department of Agronomy, Iowa
    State University (2000) Google Scholar Yasrab et al., 2021 R. Yasrab, J. Zhang,
    P. Smyth, M.P. Pound Predicting plant growth from time-series data using deep
    learning Remote Sens. (Basel), 13 (3) (2021), pp. 1-17, 10.3390/rs13030331 View
    in ScopusGoogle Scholar Yuan et al., 2019 W. Yuan, N.K. Wijewardane, S. Jenkins,
    G. Bai, Y. Ge, G.L. Graef Early prediction of soybean traits through color and
    texture features of canopy RGB imagery Sci. Rep., 9 (2019), p. 14089, 10.1038/s41598-019-50480-x
    View in ScopusGoogle Scholar Zualkernan et al., 2022 I. Zualkernan, S. Dhou, J.
    Judas, A.R. Sajun, B.R. Gomez, L.A. Hussain An IoT system using deep learning
    to classify camera trap images on the edge Computers, 11 (1) (2022), pp. 1-24,
    10.3390/computers11010013 Google Scholar Cited by (1) YOLO performance analysis
    for real-time detection of soybean pests 2024, Smart Agricultural Technology Show
    abstract © 2023 The Authors. Published by Elsevier B.V. Part of special issue
    Agricultural Cybernetics: A New Methodology of Analysis and Development for Modern
    Agricultural Production Systems Edited by Yanbo Huang, Manoj Karkee, Lie Tang,
    Dong Chen View special issue Recommended articles LSCA-net: A lightweight spectral
    convolution attention network for hyperspectral image processing Computers and
    Electronics in Agriculture, Volume 215, 2023, Article 108382 Ziru Yu, Wei Cui
    View PDF Joint control method based on speed and slip rate switching in plowing
    operation of wheeled electric tractor equipped with sliding battery pack Computers
    and Electronics in Agriculture, Volume 215, 2023, Article 108426 Qi Wang, …, Yongjie
    Cui View PDF Monitoring maize lodging severity based on multi-temporal Sentinel-1
    images using Time-weighted Dynamic time Warping Computers and Electronics in Agriculture,
    Volume 215, 2023, Article 108365 Xuzhou Qu, …, Yuchun Pan View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 1 Captures Readers: 19 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: (Chamara et al., 2023)
  journal: Computers and Electronics in Agriculture
  key_findings: AICropCAM successfully implemented image processing on the edge, drastically
    reduced the amount of data being transmitted, and could satisfy the real-time
    need for decision-making in precision agriculture.
  limitations: null
  main_objective: To develop and demonstrate AICropCAM, a novel edge image processing
    framework for real-time crop monitoring.
  relevance_evaluation: The paper is highly relevant to my point in the literature
    review, which focuses on the integration of high-resolution cameras and computer
    vision algorithms for visual monitoring of crop growth and disease detection in
    the context of automated irrigation systems. The paper introduces AICropCAM, an
    edge-computing enabled camera system that integrates image processing, IoT, and
    LoRaWAN for low-power, long-range communication. It specifically discusses using
    convolutional neural networks for canopy cover quantification, plant and weed
    counting, and insect identification, which align with the key aspects of my point.
  relevance_score: '1.0'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: AICropCAM, Raspberry Pi 4B, Arduino MKR1310, RGB camera, LoRa
    communication, convolutional neural networks
  title: 'AICropCAM: Deploying classification, segmentation, detection, and counting
    deep-learning models for crop monitoring on the edge'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Khan, A., Malebary, S. J., Dang, L. M., Binzagr, F., Song, H.-K.,
    & Moon, H. (2024). AI-Enabled Crop Management Framework for Pest Detection Using
    Visual Sensor Data. Plants, 13(5), 653. https://doi.org/10.3390/plants13050653
  authors:
  - Khan A.
  - Malebary S.J.
  - Dang L.M.
  - Binzagr F.
  - Song H.K.
  - Moon H.
  citation_count: '0'
  data_sources: Not applicable
  description: Our research focuses on addressing the challenge of crop diseases and
    pest infestations in agriculture by utilizing UAV technology for improved crop
    monitoring through unmanned aerial vehicles (UAVs) and enhancing the detection
    and classification of agricultural pests. Traditional approaches often require
    arduous manual feature extraction or computationally demanding deep learning (DL)
    techniques. To address this, we introduce an optimized model tailored specifically
    for UAV-based applications. Our alterations to the YOLOv5s model, which include
    advanced attention modules, expanded cross-stage partial network (CSP) modules,
    and refined multiscale feature extraction mechanisms, enable precise pest detection
    and classification. Inspired by the efficiency and versatility of UAVs, our study
    strives to revolutionize pest management in sustainable agriculture while also
    detecting and preventing crop diseases. We conducted rigorous testing on a medium-scale
    dataset, identifying five agricultural pests, namely ants, grasshoppers, palm
    weevils, shield bugs, and wasps. Our comprehensive experimental analysis showcases
    superior performance compared to various YOLOv5 model versions. The proposed model
    obtained higher performance, with an average precision of 96.0%, an average recall
    of 93.0%, and a mean average precision (mAP) of 95.0%. Furthermore, the inherent
    capabilities of UAVs, combined with the YOLOv5s model tested here, could offer
    a reliable solution for real-time pest detection, demonstrating significant potential
    to optimize and improve agricultural production within a drone-centric ecosystem.
  doi: 10.3390/plants13050653
  explanation: The study focuses on addressing the need for efficient and integrated
    automated irrigation management systems to optimize water resources and enhance
    agricultural productivity. The paper also examines the significance of interoperability
    and standardization in enabling seamless communication and compatibility within
    the automated irrigation management ecosystem.
  extract_1: '"Addressing the global food challenge: The review aims to explore how
    automated, real-time irrigation management systems can contribute to the efficient
    use of water resources and enhance agricultural productivity to meet the growing
    demand for food."'
  extract_2: '"Examining automation across the entire pipeline: The review intends
    to systematically analyze the automation of each component of the irrigation management
    pipeline, from data collection and transmission to processing, analysis, decision-making,
    and automated action. It aims to investigate the effectiveness and efficiency
    of integrated end-to-end automated irrigation systems."'
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all    Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Plants All Article Types Advanced   Journals
    Plants Volume 13 Issue 5 10.3390/plants13050653 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editors Gerassimos
    Peteinatos Hugo Moreno Subscribe SciFeed Recommended Articles Related Info Links
    More by Authors Links Article Views 700 Table of Contents Abstract Introduction
    Literature Review The Proposed Methodology Experiments and Results Conclusions
    Author Contributions Funding Data Availability Statement Conflicts of Interest
    References share Share announcement Help format_quote Cite question_answer Discuss
    in SciProfiles thumb_up Endorse textsms Comment first_page settings Order Article
    Reprints Open AccessArticle AI-Enabled Crop Management Framework for Pest Detection
    Using Visual Sensor Data by Asma Khan 1, Sharaf J. Malebary 2, L. Minh Dang 3,
    Faisal Binzagr 4, Hyoung-Kyu Song 3 and Hyeonjoon Moon 1,* 1 Department of Computer
    Science and Engineering, Sejong University, Seoul 05006, Republic of Korea 2 Department
    of Information Technology, Faculty of Computing and Information Technology, King
    Abdulaziz University, P.O. Box 344, Rabigh 21911, Saudi Arabia 3 Department of
    Information and Communication Engineering and Convergence Engineering for Intelligent
    Drone, Sejong University, Seoul 05006, Republic of Korea 4 Department of Computer
    Science, Faculty of Computing and Information Technology, King Abdulaziz University,
    P.O. Box 344, Rabigh 21911, Saudi Arabia * Author to whom correspondence should
    be addressed. Plants 2024, 13(5), 653; https://doi.org/10.3390/plants13050653
    Submission received: 23 January 2024 / Revised: 23 February 2024 / Accepted: 23
    February 2024 / Published: 27 February 2024 (This article belongs to the Special
    Issue The Future of Artificial Intelligence and Sensor Systems in Agriculture)
    Download keyboard_arrow_down     Browse Figures Review Reports Versions Notes
    Abstract Our research focuses on addressing the challenge of crop diseases and
    pest infestations in agriculture by utilizing UAV technology for improved crop
    monitoring through unmanned aerial vehicles (UAVs) and enhancing the detection
    and classification of agricultural pests. Traditional approaches often require
    arduous manual feature extraction or computationally demanding deep learning (DL)
    techniques. To address this, we introduce an optimized model tailored specifically
    for UAV-based applications. Our alterations to the YOLOv5s model, which include
    advanced attention modules, expanded cross-stage partial network (CSP) modules,
    and refined multiscale feature extraction mechanisms, enable precise pest detection
    and classification. Inspired by the efficiency and versatility of UAVs, our study
    strives to revolutionize pest management in sustainable agriculture while also
    detecting and preventing crop diseases. We conducted rigorous testing on a medium-scale
    dataset, identifying five agricultural pests, namely ants, grasshoppers, palm
    weevils, shield bugs, and wasps. Our comprehensive experimental analysis showcases
    superior performance compared to various YOLOv5 model versions. The proposed model
    obtained higher performance, with an average precision of 96.0%, an average recall
    of 93.0%, and a mean average precision (mAP) of 95.0%. Furthermore, the inherent
    capabilities of UAVs, combined with the YOLOv5s model tested here, could offer
    a reliable solution for real-time pest detection, demonstrating significant potential
    to optimize and improve agricultural production within a drone-centric ecosystem.
    Keywords: convolution neural network; deep learning; sustainable agriculture;
    UAV technology; computer vision; monitoring system 1. Introduction The agricultural
    sector is vital to economic enhancement, and it is essential to identify the pests
    that harm it. Pest detection, a persistent challenge, leads to a substantial annual
    loss of 20% in global crop yields [1,2]. Timely detection of plant diseases and
    pests is critical for efficient agricultural output, as shown in Figure 1. Embracing
    UAV technology becomes important for a more efficient and technologically driven
    approach to safeguard crop yields, including enhanced crop monitoring using UAVs
    [1,2]. This process significantly impacts grain yield, agricultural progress,
    and farmers’ income [3]. Additionally, high-resolution data for yield prediction
    further enhances the precision of our approach. To tackle the aforementioned challenge,
    an artificial intelligence (AI)-driven model stands out as the optimal choice,
    playing a pivotal role in advancing modern agricultural research. Leveraging the
    inherent capabilities of this cutting-edge technology, pest detection, and classification
    are seamlessly executed with exceptional efficiency, facilitating prompt intervention
    in agricultural production. The effectiveness of this approach not only hints
    at a reduction in losses but also promises a significant boost in agricultural
    production, especially when integrating UAV technology for heightened precision
    and real-time monitoring [4]. Detecting and preventing crop diseases are crucial
    aspects of this innovative approach, contributing to overall agricultural resilience
    and productivity. To this end, researchers have explored both ML- and DL-based
    modelsfor wheat diseases and pest recognition [5,6]. Figure 1. UAV-based early
    pest detection system to assist agricultural department. Plant analysis laboratories
    relying on morphological features for pest identification are limited because
    taxonomy specialists need to perform accurate classifications [7]. However, these
    approaches for pest identification have certain limitations, including the fact
    that the accurate classification of pests requires experts in the field of taxonomy,
    possible human error, and difficulties in identifying pests quickly. Several methods
    have been proposed for automatic pest detection using traditional ML [8]; for
    example, Faith et al. [9] used manual feature extraction and relative filters
    to identify different pest species using K-means clustering algorithms. However,
    this approach is very time-consuming, particularly for large datasets. Next, Rumpf
    et al. [10] presented a method based on support vector machines and spectral vegetation-based
    detection of sugar beet disease. Pests can be detected using these methods; however,
    they have several limitations, including inefficiency when manual feature extraction
    is required, making them time-consuming, tedious, error-prone, and dependent on
    computer experts. Later, owing to the inadequate feature extraction process, researchers
    diverted to DL, characterized by multilayer neural networks that enable automated
    end-to-end feature extraction has emerged as an alternative solution. This shift
    toward DL improves recognition efficiency while minimizing the lengthy manual
    feature extraction process [11]. A DL-based technique for pest and disease detection
    in tomato leaves was developed by Shijie et al. [12] with an average accuracy
    of 89%. However, this method is limited to identifying pests with a simple background
    and is difficult to implement in real-time applications. Generative adversarial
    network augmentation was utilized by Gandi et al. [13] to create an efficient
    DL model for categorizing plant diseases. Leonardo et al. [14] discussed the economic
    importance of fruit flies in Brazil and the challenges associated with identifying
    these pests. Later, they applied DL and transfer learning techniques that achieved
    an impressive accuracy of 95.68%. Transfer learning was also used by Dawei et
    al. [15] to detect ten pest species with 93.84% accuracy. In contrast, DL pest
    classification models performed well and boosted the existing discriminative score.
    However, challenges such as deplorability over resource-constrained devices, robustness
    concerns, lower accuracy, and high equipment costs hinder the integration of existing
    DL approaches into real-world scenarios. Considering these challenges, this study
    presents a novel approach to efficient pest detection using a modified YOLOv5
    model. Our model performs better than object recognition models due to its fast
    inference speed, high mAP, robust adaptation, and higher accuracy. Moreover, the
    integration of high-resolution data for yield prediction enhances the overall
    reliability of our model. In summary, this study represents a significant step
    towards efficient pest detection and has profound implications for sustainable
    agriculture. The proposed model has the potential to revolutionize pest management
    by maximizing agricultural yields while mitigating losses. We aim to pave the
    way for a more resilient and productive agricultural future by combining cutting-edge
    methods and custom enhancements. The major contributions of the proposed model
    are summarized below. We present an advanced system that uses UAVs to identify
    pests in real time. This groundbreaking method surpasses previous approaches with
    enhanced accuracy and a notable reduction in false alarms. By incorporating UAV
    technology, we have achieved a significant improvement in pest detection, highlighting
    the effectiveness of merging UAVs with this innovative solution. We refined the
    internal architecture of YOLOv5s by replacing smaller kernels in SSP (Neck) with
    larger ones and introducing a Stem module into the backbone. This strategic modification
    enhances the model’s capability to efficiently identify pests of varying sizes
    in images, reducing time complexity. Through extensive experimentation and comparison
    with nine object-detection models using a pest detection dataset, our model demonstrated
    empirical effectiveness and outperformed existing methods. A qualitative assessment
    further solidified the superior performance of our UAV-assisted pest detection
    technology. The rest of the paper is structured as follows: Section 2 provides
    a comprehensive summary of recent research relevant to the topic of a modified
    YOLOv5s architecture for pest and insect detection, and Section 3 outlines the
    data collection and methodology employed for the system. The experimental results
    are presented and analyzed in Section 4 and Section 5 present the conclusions
    drawn from this research. 2. Literature Review To cope with the aforementioned
    challenges, therefore, several researchers have been working on automatic systems
    for detecting insects in sustainable agriculture. Cheeti et al. [16] made a significant
    addition to this field when they used cutting-edge methods to categorize and detect
    pests in sustainable agriculture, including convolutional neural networks (CNN).
    Notably, their research involved creating a dataset using data from Internet sources,
    which yielded promising performance. To detect rice diseases and insects with
    an accuracy of 90.9%, Mique et al. [17] used CNN and image processing. They also
    released their suggested approach as an application for mobile devices for public
    use. However, this method is expensive in terms of computations and needs to be
    more accurate. A single-shot multi-box detector (SSD) with fine-tuning procedures
    was developed by Nam et al. [18] to recognize and classify collected insects.
    Academic research was the focus of a thorough comparative study by Burhan et al.
    [19]. In agricultural environments, their study concentrated on using four trained
    deep-learning models to identify diseases and detect pests in rice-growing regions.
    With an accuracy rating of 86.799%, their model showed good performance. Nonetheless,
    further improvements are necessary to enhance the model’s performance across evaluation
    metrics. Kouba et al. [20] used accelerometer sensors to create a unique dataset
    as part of a sensor-based approach to agricultural monitoring. Their system is
    also integrated into a mobile application accessible to the public, allowing for
    the early detection of red palm weevil (RPW) through movement analysis. However,
    their system is based on voice and movement analysis, which has a higher false-positive
    rate. Habib et al.’s [21] model for identifying and categorizing brown- and yellow-rusted
    illnesses in wheat crops uses classical machine learning. To help coffee growers,
    Esgario et al. [22] created a mobile app and a CNN model specifically designed
    to identify biotic stresses in coffee leaves. Svenning et al. [23] introduced
    a pre-trained CNN model along with fine-tuning techniques to classify carabid
    beetle species. Of the test images, 51.9% were correctly classified to species
    level, giving an average classification rate of 74.6% due to their efforts. Nonetheless,
    the model’s testing phase speed has impeded its feasibility for real-time implementation.
    The Deep-PestNet model, which has eight convolutional layers and three fully connected
    layers for effective pest detection, has been introduced by researchers [24].
    However, the approaches have several limitations, they used CNN methods in the
    pest detection domain, which mainly focus on classification. These methods classify
    the entire image as a single class that does consider the fact that pests or insects
    typically occupy a small portion of an image. When the object features are not
    prominently visible, relying solely on the complete image feature, without region
    proposals, can lead to reduced detection performance. Moreover, the CNN methods
    come with a high computational complexity, rendering them unsuitable for practical
    real-world implementation. Therefore, researchers have used DL-based object-detection
    models to overcome the limitation of the CNN-based methods. For instance, Li et
    al. [25] utilized the IPI02 dataset to detect insects in fields using various
    DCN networks, including Faster-RCNN, Mask-RCNN, and YOLOv5. They obtained promising
    results and demonstrated that YOLOv5 outperformed Faster-RCNN and Mask-RCNN, which
    attained 99% accuracy, whereas YOLOv5 gained 97%. Hu et al. [26] used YOLOv5 and
    near-infrared imaging technologies to classify and detect pests in agricultural
    landscapes accurately, and they were able to do so with a significant mean average
    precision (mAP) of 99.7%. Similarly, Chen et al. [27] suggested an AI mobile-based
    approach utilizing a particular dataset, especially for pest identification in
    agricultural areas. To achieve a high identification score, they examined various
    pre-trained DL models, including YOLOv4, single-shot detectors (SSDs), and faster
    R-CNNs. Notable achievements include a 100% F1 score for mealybugs, an 89% F1
    score for coccidia, and a 97% F1 score. YOLOv4 has also demonstrated remarkable
    performance in terms of F1 score. Legaspi et al. [28] developed a YOLOv3 model
    for identifying pests, such as fruit flies and white flies, using hardware alternatives,
    such as Raspberry Pi, desktop, and online apps accessible to the general population.
    Their model achieved 83.07% accuracy for pest classification and detection but
    required additional refinement for more accurate predictions. Furthermore, using
    a smartphone application, Karar et al. [29] presented a DL method, with the Faster-RCNN
    architecture outperforming other advanced DL models, and achieved a remarkable
    99.0% accuracy for pest detection in agricultural fields. To identify and categorize
    red palm weevils (RPW), Alsanea et al. [30] developed a region-based CNN that
    performs best in evaluation matrices when utilizing the RPW dataset. The model’s
    complexity and speed of inference make it hard to implement in real time. Using
    a customized dataset, Liu et al. [31] developed a YOLOv3-based DL model for tomato
    disease and pest identification in realistic agricultural environments. In the
    above-mentioned method, Faster-RCNN has a large number of learning parameters
    and model size, which restrict the model from real-world implementation. In the
    context of the YOLO base model, there is a need for further improvement in deep-learning-based
    object detection to detect small objects in complex backgrounds with higher inference
    speeds [32]. Furthermore, YOLOv5 models have demonstrated effectiveness in object
    detection, but their conventional versions may lack optimal efficiency and precision
    in the context of pest detection, particularly when applied to UAV-based monitoring
    in agricultural settings. The inherent challenges of agricultural environments,
    such as diverse crop types, varying terrains, and the need for real-time monitoring,
    can strain the performance of traditional YOLOv5 models. These models may not
    be fully adapted to the intricacies of pest identification within the dynamic
    and variable conditions present in agricultural fields. As a result, there is
    a potential for reduced accuracy and reliability in pest classification, limiting
    their practicality for comprehensive and real-time pest management solutions.
    This underscores the necessity for tailored optimizations, as presented in our
    study, to enhance the efficacy of YOLOv5 models in addressing the specific demands
    of UAV-centric pest detection systems in sustainable agriculture. 3. The Proposed
    Methodology Based on insights from the existing literature, we adapted a modified
    YOLOv5s model to effectively identify the region of interest in each image and
    subsequently assign appropriate class labels to them. The entire steps followed
    by dataflow of the proposed model is given in Figure 2, while technical detail
    is provided in the following subsections. Figure 2. The generic overview of the
    proposed work for pest detection comprises the following components: (a) the backbone
    module, (b) the neck module, and (c) the output head module. “Focus module” and
    “Stem module” presented in green and pink box are our modification blocks in YOLO.
    3.1. The Proposed Pest Detection Model Object-detection models play a crucial
    role in identifying and categorizing specific regions in an image. These models
    are widely used in various areas of computer vision, owing to their performance
    and efficiency. However, selecting object recognition models for specific areas
    can be challenging when determining the exact location of objects and assigning
    the correct label while saving computational resources. Several studies have been
    conducted to construct object identification models to modify the YOLO-based model,
    which has shown remarkable progress. The original YOLO approach, initiated by
    Redmon et al. [33], addresses object detection as a regression challenge rather
    than classification. This approach detects target objects using a single neural
    network, resulting in impressive performance and real-time inference capabilities.
    In addition, YOLO-based object recognition models show exceptional generalizability,
    as they quickly adapt to recognize a wide range of objects through training. Enhanced
    crop monitoring using UAVs, high-resolution data for yield prediction, and detecting
    and preventing crop diseases are integral components that enhance the efficacy
    of our proposed approach. The YOLO architecture has recently undergone several
    improvements to boost its efficacy and efficiency for insect identification. From
    2016 to 2018, Redmon and Farhadi [34] proposed the initial three versions of the
    YOLO-based object-detection models, which attracted several researchers due to
    its fast inference speed and model accuracy YOLOv4, showing an impressive average
    accuracy of 43.5% on the MS-COCO dataset while maintaining high speed, was introduced
    in 2020 by Bochkovskiy et al. [35]. Additionally, YOLOv5 was introduced by Feng
    et al. [36], achieving a remarkable performance and speed turning point. There
    are five variations of YOLOv5, with diverse feature map depths and breadths: YOLOv5x,
    YOLOv5l, YOLOv5m, YOLOv5s, and YOLOv5n. A thorough evaluation of the MS-COCO dataset
    underscored the exceptional results of these models. YOLOv5n excelled in fast
    inference, whereas YOLOv5x exhibited the highest object-detection accuracy. These
    models have common structural elements, including input, backbone, neck, and prediction
    components [37]. Compared to previous YOLO iterations, YOLOv5 provides higher
    recognition accuracy, lower computational complexity, and a smaller model footprint,
    making it optimal for resource-constrained devices. This study leveraged YOLOv5’s
    improved recognition capabilities and real-time inference to refine its inherent
    architecture and to enable efficient and reliable insect recognition. 3.2. Network
    Architecture This study used the YOLOv5s object identification system as our primary
    model because of its quick inference and outstanding performance. The goal of
    this study was to optimize this system for pest or insect detection. To this end,
    we made several modifications to the YOLOv5s model to identify small and large
    insects in the image effectively. The input, backbone, neck, and head modules
    comprise the modified model’s four essential components. The three sub-modules
    comprise the input module: imagine scaling, adaptive anchor-box computation, and
    pre-processing of the mosaic data. The pre-processing phase of the mosaic data
    includes three techniques: random scaling, cropping, and order, which introduce
    variability in the positioning of image segments that improve the network’s ability
    to detect smaller objects, such as insects. The various modules architecture used
    in the proposed model are given in Figure 3. Figure 3. Various modules architecture
    used in the proposed YOLO for pest detection. To improve the recognition accuracy
    for small objects and expand the range of available data, we randomly select the
    fusion point for merging images. During model training, YOLOv5s dynamically generates
    multiple prediction boxes based on the initial anchor box. Non-maximum suppression
    (NMS) was also used to determine which prediction box resembled the original box
    the most. We continually resized the adaptively zoomed picture before entering
    it into the network for identification, removing any potential inconsistencies
    resulting from different image sizes and ensuring compatibility with the feature
    tensor and the fully connected layer. The YOLOv5 backbone architecture uses a
    CSPNet backbone based on the Dense Net architecture and easily integrates the
    focus module. The focus module is a critical component that performs down-sampling
    by decomposing the input image, which is initially 640 × 640 × 3, and then concatenated
    to produce a 320 × 320 × 12 feature map. However, our analysis of YOLOv5 models
    that have already been trained indicates that the focus layer struggles to effectively
    capture spatial information from tiny objects, which impacts model performance.
    We suggest adding a Stem unit following the focus module to overcome this restriction,
    as shown in Figure 2. The Stem module facilitates the creation of more sophisticated
    feature maps by providing additional down-sampling with a stride of two and an
    increase in channel dimensions. The Stem module significantly enhances display
    possibilities while just slightly increasing processing complexity. In the YOLOv5
    model, the CSP module is constructed using a series of 1 × 1 and 3 × 3 convolutional
    layers. This module divides the initial input into two segments, each passing
    through its processing path. The first segment is fully processed by a CBS block
    consisting of convolution, normalization of batch size, and activation functions
    of SILU, as in the ingenious work of Elfwing et al. [38]. The second segment passes
    through a convolutional layer, as shown in Figure 1. A CBS module is then added
    after merging the two partitions. In addition, the spatial pyramid pooling (SPP)
    block plays an important role in expanding the receptive field and extracting
    essential features, which are then proficiently passed to the feature aggregation
    block. This adaptation aimed to improve the ability of the network to identify
    insects by extracting relevant features from smaller objects, ultimately leading
    to better insect detection performance. In contrast to using multiple CNN models
    that require fixed-size input images, the integration of SSP results in the generation
    of a fixed-size output. This approach also facilitates the acquisition of important
    features by pooling the different scales of the same module. In our modified version,
    the SSP block replaces traditional sizes of the kernel, such as 5 × 5, 9 × 9,
    and t3 × 13, with 3 × 3, 5 × 5, and 7 × 7, as shown in Figure 2. To further improve
    accuracy, The input image’s longer edges normalized to 640 pixels, whereas the
    smaller edge was adjusted appropriately. Furthermore, the shortest edge coincided
    with the maximum step size of the SPP block. In the absence of P6, the shortest
    edge had a factor of 32. If P6, however, the shorter edge must be a factor of
    64. The neck component plays a critical role in integrating the feature maps generated
    by various folds in the spine, preparing them for the head segment. According
    to Hu et al. [39], the neck segment has PAN and FPN structures to improve the
    network’s feature fusion capabilities. The PAN technology is typically used by
    YOLOv5, producing three output features: P3, P4, and P5, with dimensions of 80
    × 80 × 16, 40 × 40 × 16, and 20 × 20 × 16. But, as you can see in Figure 2, we
    added an extra P6 output block to our customized model, with a 10 × 10 × 16-pixel
    feature map. The ability of the model to identify both large and small objects
    in the input image is greatly enhanced by this addition, which was previously
    employed for face detection by Qi et al. [38]. Lastly, convolutional layers are
    used in the head component to identify objects by defining bounding rectangles
    around the regions of significance and then classifying them. Our model incorporated
    a comprehensive scaling method that uniformly modified the backbone, neck, and
    head’s resolution, width, and depth. This all-encompassing modification guarantees
    higher accuracy and efficiency for insect detection. 4. Experiments and Results
    This section provides a comprehensive overview of the experimental setup, evaluation
    parameters, dataset selection, model performance, and comparison with state-of-the-art
    methods. 4.1. Experimental Setup To implement the proposed work, we employed PyTorch
    with CUDA support to analyze pest and insect detection results. Our hardware setup
    included an NVIDIA (GeForce RTX 3070 GPU) with 32 GB of RAM. We evaluated our
    model with various hyperparameter settings, and the optimal performance was obtained
    with the following configuration: a batch size of 32, the SGD optimizer with a
    learning rate of 0.001, and training for 200 epochs. The proposed model is evaluated
    using well-known evaluation metrics such as precision, recall, and mAP [40] and
    considered state-of-the-art evaluation metrics in the target domain. Precision
    is an evaluation metric used in ML and DL to assess model performance. It is defined
    as the ratio of true-positive samples to the sum of true-positive and false-positive
    samples, as expressed in Equation (1). P= TP TP+FP (1) Recall is another evaluation
    metric that focuses solely on positive samples within a given dataset, excluding
    negative ones, as formulated in Equation (2). R= TP TP+FN (2) The mAP metric is
    used to assess object-detection models. It computes a score based on the relationship
    between the localized boxes and ground-truth bounding boxes. Achieving the highest
    score is an indicator of an accurate detection approach, as formalized by Equation
    (3). mAP= 1 N ∑ N i−1 A P i (3) In the equations, TP represents the number of
    correctly identified positive samples, while FP signifies the number of negative
    samples that are falsely identified as positive. Similarly, FN corresponds to
    the number of misclassified positive samples. 4.2. Dataset Selection The data-collection
    method is essential for the efficacy of model training in artificial intelligence.
    This information is meticulously organized into five taxonomic groups: Ants, Grasshoppers,
    Palm Weevils, Shield Bugs, and Wasps. The classes were organized as follows: the
    ant class contained 392 images, the grasshopper class had 315 images, and the
    palm weevil, shield bug, and wasp categories had 148, 392, and 318 images, respectively.
    We used 70% of the dataset for training, 20% for validation, and 10% for testing.
    Figure 4 demonstrates the total number of instances used to train the proposed
    model. Figure 4. Pest or insect detection dataset distribution for training purposes.
    The dataset was carefully annotated in accordance with the object recognition
    model [6] using a publicly available annotation tool from GitHub (accessed on
    23 June 2022). The annotation process was written in Python with the cross-platform
    Qt GUI (graphical user interface) toolkit. To match the criteria of the YOLO-based
    model, the data collection was annotated in YOLO format, which requires annotated
    files to be in .txt format. The dataset is divided into three sub-categories training,
    validation, and testing which contain 70%, 20%, and 10% of the data, respectively.
    Figure 5 shows samples of the pests from each dataset class. Figure 5. Various
    samples of pests for each dataset class. 4.3. The Proposed Model Evaluation In
    this subsequent section, we provide a detailed description of the training process
    for our model. We used an early stopping algorithm to halt the training once the
    model has reached a certain level of performance and can no longer learn efficiently
    and effectively. The evaluation of the proposed model includes the examination
    of various metrics such as loss, recall, precision, and mAP, as depicted in Figure
    6. Figure 6. The X-axis shows the number of epochs, and the Y-axis shows the corresponding
    score of each evaluation matrix. This shows the model’s efficacy using various
    evaluation metrics. The loss graph visually represents the model’s capability
    to identify objects accurately, indicating its proficiency in performing tasks
    effectively. The object loss function assesses the model’s ability to complete
    tasks within relevant regions of interest, and an increase in accuracy corresponds
    to a decrease in the loss function. Accurate and effective object classification
    relies on reducing associated losses. In Figure 6, Bounding box loss, class loss,
    and distribution focal loss in the training phase are reached to a minimum of
    0.50, 0.02, and 1.0, respectively. Similarly, these metrics in the validation
    phase reached 1.1, 0.50, and 1.95, respectively. In the context of ML and ML,
    recall and precision are important metrics for assessing model performance. Elevated
    precision and recall, as observed in Figure 6, signify enhanced model accuracy.
    The loss function value consistently decreases during the training process, and
    the model demonstrates a continuous ability to reduce loss and rapidly improve
    recall, and precision within a few epochs. At the last epoch, the precision and
    recall in the training phase reached 0.96 and 0.93 as shown in Figure 6. Furthermore,
    the maximum accuracy, recall, and mAP values are achieved at approximately the
    120th epoch, highlighting the efficacy of our model. Therefore, the proposed model
    proves to be a robust and capable solution across various evaluation metrics.
    We conducted a performance evaluation of the proposed model using the test set
    of the dataset, employing a confusion matrix for in-depth analysis, as shown in
    Figure 7. This confusion matrix encompasses five distinct pest classes and introduces
    an additional category, “background FN”, to emphasize scenarios where the model
    failed to identify objects within the image. To present a comprehensive insight
    into the confusion matrix, we closely analyzed the accuracy in predicting various
    pest classes, including ants, grasshoppers, palm weevils, shield bugs, and wasps.
    The accuracy values for these categories were 0.79, 1.00, 0.78, 0.96, and 0.86,
    respectively. Figure 7. The proposed model’s confusion matrix using a self-created
    dataset. 4.4. Comparative Analysis with State-of-the-Art Models In this subsequent
    section, we provide a comprehensive comparison of the proposed model with nine
    other state-of-the-art models. The proposed model consistently demonstrates superior
    performance in pest detection, particularly excelling in the identification of
    ants, grasshoppers, palm weevils, shield bugs, and wasps. The results are summarized
    in Table 1, indicating that the Faster-RCNN emerges as a powerful contender, achieving
    promising performance. However, the proposed model surpasses Faster-RCNN with
    higher average precision, recall, and mAP values of 0.04, 0.03, and 0.3, respectively.
    Table 1. Analysis of the proposed model with Faster-RCNN and various versions
    of the YOLO models. YOLOv3 and YOLOv4 exhibit relatively weaker performance compared
    to other models. YOLOv5n falls short, with average values of 0.87, 0.88, and 0.89
    for precision, recall, and mAP. YOLOv5s achieves 0.91 accuracy, 0.83 recall, and
    0.90 mAP, while YOLOv5m outperforms YOLOv5s models, obtaining an average precision
    value of 0.94, recall of 0.84, and mAP of 0.91. YOLOv5l and YOLOv5x exhibit better
    performance, as given in Table 1. The EPD model also outperforms other models,
    but the proposed model surpasses the EPD with higher precision, recall, and mAP
    values of 0.02, 0.03, and 0.1, respectively. Therefore, Table 1 highlights that
    our proposed model achieves superior performance across all evaluation metrics.
    4.5. Splitting Dataset Using 5-Fold Cross Validation In this strategy, the whole
    dataset is utilized for training and validation by making five different folds
    to effectively evaluate the model performance. Figure 8 shows that the proposed
    model obtained optimal performance for each fold in terms of precision, recall
    and mAP. The proposed model obtained precision values for each fold: Fold 1: 96.88%,
    Fold 2: 96.34%, Fold 3: 96.16%, Fold 4: 95.4a 5%, and Fold 5: 95.17%. The recall
    value of 93.80%, 93.45%, 93.10%, 92.53%, and 92.12% for Fold 1, Fold 2, Fold 3,
    Fold 4, and Fold 5, respectively. Similarly, the proposed model achieved higher
    mAP of 95.83%, 95.42%, 95.00%, 94.68%, and 94.07%, respectively. In conclusion,
    our model obtained an average precision, recall, and mAP of 96.00%, 93.00%, and
    95.00%, respectively. Figure 8. Illustrated the model generalization capability
    using 5-fold cross validation. 4.6. Model Complexity Analysis Table 2 provides
    a comprehensive assessment of the proposed model’s feasibility, calculating giga
    floating point operations per second (GFLOPs), model size, and FPS for all models,
    which are then compared to the suggested approach. In Table 2, the proposed model
    is compared with YOLOv5n, YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. In the comparison,
    the YOLOv5n model exhibited faster inference speed with reduced GFLOPs and model
    size. However, the YOLOv5n model obtained poor performance as shown in Table 1.
    In contrast, when compared with YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x, the proposed
    model obtained significantly higher FPS, with 1.50, 12.59, 16.13, and 18.85 times
    the speed, respectively. This underscores that our model combines high performance
    with fast inference speed while maintaining a favorable balance of GFLOPs and
    model size. The results obtained show the efficacy of our model, suggesting that
    it is a viable solution for real-world application. Table 2. Comparative Analysis
    of our model with different YOLOv5 versions in terms of model size, GFLOPs, and
    FPS using CPU. 4.7. Visual Result of the Proposed Model We performed a visual
    analysis of the output predictions of the suggested model to evaluate how robust
    it was. The results of the model for the localization and identification of five
    different insects or pests are shown in Figure 9. This figure clearly shows how
    well our model selects regions of interest and correctly labels classes. It performs
    admirably in both detection and recognition tasks, accurately delineating bounding
    boxes around objects and precisely labeling their classes. This visual representation
    highlights the model’s potential for real-time applications. However, it is worth
    noting that there are some limitations to the proposed approach, as highlighted
    in Figure 9. While most images are accurately classified, exceptions exist. For
    instance, the first and last images in the fifth row present recognition challenges.
    Additionally, instances of misdetection are observed in the second and third images
    of the second row, as well as in the last image of the third row. These visual
    results offer valuable insights into the model’s performance, acknowledging its
    strengths while identifying areas where improvement may be needed. Figure 9. The
    visual results of the proposed model, which show the model effective analysis.
    4.8. Discussion Our study addresses the challenge of crop diseases and pest infestations
    in agriculture by leveraging UAV technology for enhanced crop monitoring through
    UAVs and improving the detection and classification of agricultural pests. Conventional
    methods often involve laborious manual feature extraction or computationally intensive
    DL techniques. To overcome these limitations, we present an optimized model specifically
    tailored for UAV-based applications. Our modifications to the proposed YOLOv5s
    model incorporate advanced attention modules, expanded cross-stage partial network
    modules, and refined multiscale feature extraction mechanisms, enabling precise
    pest detection and classification. Inspired by the efficiency and adaptability
    of UAVs, our research aims to transform pest management into sustainable agriculture
    while also combating crop diseases. We conducted extensive experiments on a medium-scale
    dataset, identifying five agricultural pests: ants, grasshoppers, palm weevils,
    shield bugs, and wasps. Our thorough experimental analysis demonstrates superior
    performance compared to various Faster-RCNN [41,42] and YOLO model versions [43,44].
    Compared with existing methodologies, our model demonstrates competitive performance.
    For instance, while Faster-PestNet [41] achieves an accuracy of 82.43% on the
    IP102 dataset. Similarly, Pest-YOLO [43] and PestLite [44] achieve mean average
    precision scores of 73.4% and 90.7%, respectively. Jiao et al. [45] integrated
    an anchor-free convolutional neural network (AF-RCNN) with Faster-RCNN for pest
    detection on the Pest24 dataset, yielding an mAP and mRecall of 56.4% and 85.1%,
    respectively. Wang et al. [46] employed four detection networks, including YOLOv3,
    SSD, Faster-RCNN, and Cascade-RCNN, for detecting 24 common pests, with YOLOv3
    achieving the highest mAP value of 63.54%. Furthermore, AgriPest-YOLO achieves
    a mean average precision (mAP) of 71.3% on a multi-pest image dataset by integrating
    a coordination and local attention mechanism, grouping spatial pyramid pooling
    fast, and soft non-maximum suppression, facilitating efficient and accurate real-time
    pest detection from light-trap images [47]. The proposed model achieved higher
    performance metrics, with an average precision of 96.0%, average recall of 93.0%,
    and mean average precision (mAP) of 95.0%, as shown in Table 1, which shows that
    our proposed model achieved the highest accuracy score than other SOTA models.
    Visual results of the proposed modified YOLOv5s are shown in Figure 9. Furthermore,
    the inherent capabilities of UAVs, coupled with the YOLOv5s model evaluated in
    this study, offer a reliable solution for real-time pest detection, showcasing
    significant potential to optimize and enhance agricultural production within a
    drone-centric ecosystem. 5. Conclusions In our study evaluating nine object recognition
    models, the standout performer was our specialized UAV-oriented model. Throughout
    the experiments, conducted with a meticulously curated dataset featuring five
    distinct insect species, ants, grasshoppers, palm weevils, shield bugs, and wasps,
    our model consistently outshone its counterparts in accuracy, recall, and mean
    average precision (mAP). What sets our model apart is not just its impressive
    performance but also its efficiency, demonstrating superior inference capabilities
    while demanding fewer computational GFLOPs and maintaining a more manageable model
    size. This positions our proposed model as a robust solution for real-time species
    detection, highlighting its prowess in the context of UAV and technology integration.
    Moreover, our model excels in enhanced crop monitoring using UAVs, demonstrates
    proficiency in handling high-resolution data for yield prediction, and proves
    effective in detecting and preventing crop diseases. Author Contributions Conceptualization,
    A.K. and S.J.M.; methodology, A.K., F.B. and S.J.M.; software, A.K.; validation,
    A.K., F.B. and L.M.D.; formal analysis, H.-K.S. and F.B.; investigation, S.J.M.
    and L.M.D.; resources, S.J.M.; data curation, A.K., F.B. and H.-K.S.; writing—original
    draft preparation, A.K. and F.B.; writing—review and editing, L.M.D., H.-K.S.,
    H.M. and F.B.; visualization, A.K. and H.-K.S.; supervision, H.-K.S. and L.M.D.;
    project ad-ministration, S.J.M.; funding acquisition, H.M. All authors have read
    and agreed to the published version of the manuscript. Funding This work was supported
    by Basic Science Research Program through the National Research Foundation of
    Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540) and by Institute
    of Information & communications Technology Planning & Evaluation (IITP) under
    the metaverse support program to nurture the best talents (IITP-2023-RS-2023-00254529)
    grant funded by the Korea government (MSIT) and by Korea Institute of Planning
    and Evaluation for Technology in Food, Agriculture, Forestry and Fisheries (IPET)
    through Digital Breeding Transformation Technology Development Program, funded
    by Ministry of Agriculture, Food and Rural Affairs (MAFRA) (322063-03-1-SB010).
    Data Availability Statement Data is contained within the article. Conflicts of
    Interest The authors declare no conflicts of interest. References Amiri, A.N.;
    Bakhsh, A. An effective pest management approach in potato to combat insect pests
    and herbicide. 3 Biotech 2019, 9, 16. [Google Scholar] [CrossRef] Fernández, R.M.;
    Petek, M.; Gerasymenko, I.; Juteršek, M.; Baebler, Š.; Kallam, K.; Giménez, E.M.;
    Gondolf, J.; Nordmann, A.; Gruden, K.; et al. Insect pest management in the age
    of synthetic biology. Plant Biotechnol. J. 2022, 20, 25–36. [Google Scholar] [CrossRef]
    Habib, S.; Khan, I.; Aladhadh, S.; Islam, M.; Khan, S. External Features-Based
    Approach to Date Grading and Analysis with Image Processing. Emerg. Sci. J. 2022,
    6, 694–704. [Google Scholar] [CrossRef] Zhou, J.; Li, J.; Wang, C.; Wu, H.; Zhao,
    C.; Teng, G. Crop disease identification and interpretation method based on multimodal
    deep learning. Comput. Electron. Agric. 2021, 189, 106408. [Google Scholar] [CrossRef]
    Khan, H.; Haq, I.U.; Munsif, M.; Mustaqeem; Khan, S.U.; Lee, M.Y. Automated Wheat
    Diseases Classification Framework Using Advanced Machine Learning Technique. Agriculture
    2022, 12, 1226. [Google Scholar] [CrossRef] Aladhadh, S.; Habib, S.; Islam, M.;
    Aloraini, M.; Aladhadh, M.; Al-Rawashdeh, H.S. An Efficient Pest Detection Framework
    with a Medium-Scale Benchmark to Increase the Agricultural Productivity. Sensors
    2022, 22, 9749. [Google Scholar] [CrossRef] Al Hiary, H.; Ahmad, S.B.; Reyalat,
    M.; Braik, M.; Alrahamneh, Z. Fast and accurate detection and classification of
    plant diseases. Int. J. Comput. Appl. 2011, 17, 31–38. [Google Scholar] [CrossRef]
    Nguyen, T.N.; Lee, S.; Nguyen-Xuan, H.; Lee, J. A novel analysis-prediction approach
    for geometrically nonlinear problems using group method of data handling. Comput.
    Methods Appl. Mech. Eng. 2019, 354, 506–526. [Google Scholar] [CrossRef] Faithpraise,
    F.; Birch, P.; Young, R.; Obu, J.; Faithpraise, B.; Chatwin, C. Automatic plant
    pest detection and recognition using k-means clustering algorithm and correspondence
    filters. Int. J. Adv. Biotechnol. Res. 2013, 4, 189–199. [Google Scholar] Rumpf,
    T.; Mahlein, A.-K.; Steiner, U.; Oerke, E.-C.; Dehne, H.-W.; Plümer, L. Early
    detection and classification of plant diseases with support vector machines based
    on hyperspectral reflectance. Comput. Electron. Agric. 2010, 74, 91–99. [Google
    Scholar] [CrossRef] Nguyen, T.N.; Nguyen-Xuan, H.; Lee, J. A novel data-driven
    nonlinear solver for solid mechanics using time series forecasting. Finite Elem.
    Anal. Des. 2020, 171, 103377. [Google Scholar] [CrossRef] Shijie, J.; Peiyi, J.;
    Siping, H. Automatic detection of tomato diseases and pests based on leaf images.
    In Proceedings of the 2017 Chinese Automation Congress (CAC), Jinan, China, 20–22
    October 2017. [Google Scholar] Gandhi, R.; Nimbalkar, S.; Yelamanchili, N.; Ponkshe,
    S. Plant disease detection using CNNs and GANs as an augmentative approach. In
    Proceedings of the 2018 IEEE International Conference on Innovative Research and
    Development (ICIRD), Bangkok, Thailand, 11–12 May 2018. [Google Scholar] Leonardo,
    M.M.; Carvalho, T.J.; Rezende, E.; Zucchi, R.; Faria, F.A. Deep feature-based
    classifiers for fruit fly identification (Diptera: Tephritidae). In Proceedings
    of the 2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI),
    Parana, Brazil, 29 October–1 November 2018. [Google Scholar] Dawei, W.; Limiao,
    D.; Jiangong, N.; Jiyue, G.; Hongfei, Z.; Zhongzhi, H. Recognition pest by image-based
    transfer learning. J. Sci. Food Agric. 2019, 99, 4524–4531. [Google Scholar] [CrossRef]
    [PubMed] Cheeti, S.; Kumar, G.S.; Priyanka, J.S.; Firdous, G.; Ranjeeva, P.R.
    Pest detection and classification using YOLO AND CNN. Ann. Rom. Soc. Cell Biol.
    2021, 15295–15300. [Google Scholar] Mique, E.L., Jr.; Palaoag, T.D. Rice pest
    and disease detection using convolutional neural network. In Proceedings of the
    1st International Conference on Information Science and Systems, Jeju, Republic
    of Korea, 27–29 April 2018. [Google Scholar] [CrossRef] Nam, N.T.; Hung, P.D.
    Pest detection on traps using deep convolutional neural networks. In Proceedings
    of the 1st International Conference on Control and Computer Vision, Singapore,
    15–18 June 2018. [Google Scholar] Burhan, S.A.; Minhas, S.; Tariq, A.; Hassan,
    M.N. Comparative study of deep learning algorithms for disease and pest detection
    in rice crops. In Proceedings of the 2020 12th International Conference on Electronics,
    Computers and Artificial Intelligence (ECAI), Bucharest, Romania, 25–27 June 2020.
    [Google Scholar] Koubaa, A.; Aldawood, A.; Saeed, B.; Hadid, A.; Ahmed, M.; Saad,
    A.; Alkhouja, H.; Ammar, A.; Alkanhal, M. Smart Palm: An IoT framework for red
    palm weevil early detection. Agronomy 2020, 10, 987. [Google Scholar] [CrossRef]
    Habib, S.; Khan, I.; Islam, M.; Albattah, W.; Alyahya, S.M.; Khan, S.; Hassan,
    M.K. Wavelet frequency transformation for specific weeds recognition. In Proceedings
    of the 2021 1st International Conference on Artificial Intelligence and Data Analytics
    (CAIDA), Riyadh, Saudi Arabia, 6–7 April 2021. [Google Scholar] Esgario, J.G.;
    de Castro, P.B.; Tassis, L.M.; Krohling, R.A. An app to assist farmers in the
    identification of diseases and pests of coffee leaves using deep learning. Inf.
    Process. Agric. 2022, 9, 38–47. [Google Scholar] [CrossRef] Hansen, O.L.P.; Svenning,
    J.; Olsen, K.; Dupont, S.; Garner, B.H.; Iosifidis, A.; Price, B.W.; Høye, T.T.
    Species-level image classification with convolutional neural network enables insect
    identification from habitus images. Ecol. Evol. 2020, 10, 737–747. [Google Scholar]
    [CrossRef] [PubMed] Ullah, N.; Khan, J.A.; Alharbi, L.A.; Raza, A.; Khan, W.;
    Ahmad, I. An Efficient Approach for Crops Pests Recognition and Classification
    Based on Novel DeepPestNet Deep Learning Model. IEEE Access 2022, 10, 73019–73032.
    [Google Scholar] [CrossRef] Li, W.; Zhu, T.; Li, X.; Dong, J.; Liu, J. Recommending
    advanced deep learning models for efficient insect pest detection. Agriculture
    2022, 12, 1065. [Google Scholar] [CrossRef] Hu, Z.; Xiang, Y.; Li, Y.; Long, Z.;
    Liu, A.; Dai, X.; Lei, X.; Tang, Z. Research on Identification Technology of Field
    Pests with Protective Color Characteristics. Appl. Sci. 2022, 12, 3810. [Google
    Scholar] [CrossRef] Chen, J.-W.; Lin, W.-J.; Cheng, H.-J.; Hung, C.-L.; Lin, C.-Y.;
    Chen, S.-P. A smartphone-based application for scale pest detection using multiple-object
    detection methods. Electronics 2021, 10, 372. [Google Scholar] [CrossRef] Legaspi,
    K.R.B.; Sison, N.W.S.; Villaverde, J.F. Detection and Classification of Whiteflies
    and Fruit Flies Using YOLO. In Proceedings of the 2021 13th International Conference
    on Computer and Automation Engineering (ICCAE), Melbourne, Australia, 20–22 March
    2021. [Google Scholar] Karar, M.E.; Alsunaydi, F.; Albusaymi, S.; Alotaibi, S.
    A new mobile application of agricultural pests recognition using deep learning
    in cloud computing system. Alex. Eng. J. 2021, 60, 4423–4432. [Google Scholar]
    [CrossRef] Alsanea, M.; Habib, S.; Khan, N.F.; Alsharekh, M.F.; Islam, M.; Khan,
    S. A Deep-Learning Model for Real-Time Red Palm Weevil Detection and Localization.
    J. Imaging 2022, 8, 170. [Google Scholar] [CrossRef] Liu, J.; Wang, X. Tomato
    diseases and pests detection based on improved Yolo V3 convolutional neural network.
    Front. Plant Sci. 2020, 11, 898. [Google Scholar] [CrossRef] [PubMed] Nguyen,
    T.N.; Lee, S.; Nguyen, P.-C.; Nguyen-Xuan, H.; Lee, J. Geometrically nonlinear
    postbuckling behavior of imperfect FG-CNTRC shells under axial compression using
    isogeometric analysis. Eur. J. Mech.-A/Solids 2020, 84, 104066. [Google Scholar]
    [CrossRef] Redmon, J.; Divvala, S.; Girshick, R.; Farhadi, A. You only look once:
    Unified, real-time object detection. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27–30 June 2016.
    [Google Scholar] Redmon, J.; Farhadi, A. Yolov3: An incremental improvement. arXiv
    2018, arXiv:1804.02767. [Google Scholar] Bochkovskiy, A.; Wang, C.-Y.; Liao, H.-Y.M.
    Yolov4: Optimal speed and accuracy of object detection. arXiv 2020, arXiv:2004.10934.
    [Google Scholar] Feng, Z.; Guo, L.; Huang, D.; Li, R. Electrical insulator defects
    detection method based on yolov5. In Proceedings of the 2021 IEEE 10th Data Driven
    Control and Learning Systems Conference (DDCLS), Suzhou, China, 14–16 May 2021.
    [Google Scholar] Dong, X.; Yan, S.; Duan, C. A lightweight vehicles detection
    network model based on YOLOv5. Eng. Appl. Artif. Intell. 2022, 113, 104914. [Google
    Scholar] [CrossRef] Elfwing, S.; Uchibe, E.; Doya, K. Sigmoid-weighted linear
    units for neural network function approximation in reinforcement learning. Neural
    Netw. 2018, 107, 3–11. [Google Scholar] [CrossRef] Hu, W.; Xiong, J.; Liang, J.;
    Xie, Z.; Liu, Z.; Huang, Q.; Yang, Z. A method of citrus epidermis defects detection
    based on an improved YOLOv5. Biosyst. Eng. 2023, 227, 19–35. [Google Scholar]
    [CrossRef] Li, D.; Ahmed, F.; Wu, N.; Sethi, A.I. Yolo-JD: A Deep Learning Network
    for jute diseases and pests detection from images. Plants 2022, 11, 937. [Google
    Scholar] [CrossRef] Ali, F.; Qayyum, H.; Iqbal, M.J. Faster-PestNet: A Lightweight
    deep learning framework for crop pest detection and classification. IEEE Access
    2023, 11, 104016–104027. [Google Scholar] [CrossRef] Hua, S.; Xu, M.; Xu, Z.;
    Ye, H.; Zhou, C. Multi-feature decision fusion algorithm for disease detection
    on crop surface based on machine vision. Neural Comput. Appl. 2022, 34, 9471–9484.
    [Google Scholar] [CrossRef] Tang, Z.; Lu, J.; Chen, Z.; Qi, F.; Zhang, L. Improved
    Pest-YOLO: Real-time pest detection based on efficient channel attention mechanism
    and transformer encoder. Ecol. Inform. 2023, 78, 102340. [Google Scholar] [CrossRef]
    Dong, Q.; Sun, L.; Han, T.; Cai, M.; Gao, C. PestLite: A Novel YOLO-Based Deep
    Learning Technique for Crop Pest Detection. Agriculture 2024, 14, 228. [Google
    Scholar] [CrossRef] Jiao, L.; Dong, S.; Zhang, S.; Xie, C.; Wang, H. AF-RCNN:
    An anchor-free convolutional neural network for multi-categories agricultural
    pest detection. Comput. Electron. Agric. 2020, 174, 105522. [Google Scholar] [CrossRef]
    Wang, Q.-J.; Zhang, S.-Y.; Dong, S.-F.; Zhang, G.-C.; Yang, J.; Li, R.; Wang,
    H.-Q. Pest24: A large-scale very small object data set of agricultural pests for
    multi-target detection. Comput. Electron. Agric. 2020, 175, 105585. [Google Scholar]
    [CrossRef] Zhang, W.; Huang, H.; Sun, Y.; Wu, X. AgriPest-YOLO: A rapid light-trap
    agricultural pest detection method based on deep learning. Front. Plant Sci. 2022,
    13, 1079384. [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements,
    opinions and data contained in all publications are solely those of the individual
    author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or
    the editor(s) disclaim responsibility for any injury to people or property resulting
    from any ideas, methods, instructions or products referred to in the content.  ©
    2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open
    access article distributed under the terms and conditions of the Creative Commons
    Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share
    and Cite MDPI and ACS Style Khan, A.; Malebary, S.J.; Dang, L.M.; Binzagr, F.;
    Song, H.-K.; Moon, H. AI-Enabled Crop Management Framework for Pest Detection
    Using Visual Sensor Data. Plants 2024, 13, 653. https://doi.org/10.3390/plants13050653
    AMA Style Khan A, Malebary SJ, Dang LM, Binzagr F, Song H-K, Moon H. AI-Enabled
    Crop Management Framework for Pest Detection Using Visual Sensor Data. Plants.
    2024; 13(5):653. https://doi.org/10.3390/plants13050653 Chicago/Turabian Style
    Khan, Asma, Sharaf J. Malebary, L. Minh Dang, Faisal Binzagr, Hyoung-Kyu Song,
    and Hyeonjoon Moon. 2024. \"AI-Enabled Crop Management Framework for Pest Detection
    Using Visual Sensor Data\" Plants 13, no. 5: 653. https://doi.org/10.3390/plants13050653
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 27. Feb 3. Mar 8. Mar 13. Mar
    18. Mar 23. Mar 28. Mar 2. Apr 0 200 400 600 800 For more information on the journal
    statistics, click here. Multiple requests from the same IP address are counted
    as one view.   Plants, EISSN 2223-7747, Published by MDPI RSS Content Alert Further
    Information Article Processing Charges Pay an Invoice Open Access Policy Contact
    MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors For Librarians
    For Publishers For Societies For Conference Organizers MDPI Initiatives Sciforum
    MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings Series
    Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release notifications
    and newsletters from MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel,
    Switzerland) unless otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: (Asma Khan et al., 2024)
  journal: Plants
  key_findings: Not applicable
  limitations: null
  main_objective: Exploring how automated, real-time irrigation management systems
    can contribute to efficient water use and enhanced agricultural productivity to
    meet the rising demand for food.
  relevance_evaluation: The paper presents a comprehensive review of the current state
    and future directions of real-time automated irrigation management systems, with
    a specific focus on the integration, interoperability, and standardization of
    system components. This aligns with the scope of the literature review, which
    aims to evaluate the potential of automated systems for improving water use efficiency
    and crop productivity.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Not applicable
  title: AI-Enabled Crop Management Framework for Pest Detection Using Visual Sensor
    Data
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Nasir, R., Khan, M. J., Arshad, M., & Khurshid, K. (2019). Convolutional
    neural network based regression for leaf water content estimation. In 2019 Second
    International Conference on Latest trends in Electrical Engineering and Computing
    Technologies (INTELLECT) (pp. 1-5). IEEE.
  authors:
  - Nasir R.
  - Khan M.J.
  - Arshad M.
  - Khurshid K.
  citation_count: '3'
  data_sources: Spectral reflectance data acquired across the visible to shortwave
    infrared (VSWIR; 0.39–2.5 µm) and mid- and thermal-infrared (MIR and TIR; 2.50–14.0
    µm) regions
  description: 'With the advancements in precision farming, crop sensing is gaining
    importance for timely crop health management. Leaf water content (LWC) is key
    component to determine vegetation health and nourishment. Timely estimation of
    LWC could save us from hazardous damage by pre-planning: drought stress on plants,
    irrigation and prediction of woodland fire. The retrieval of LWC from visible
    to shortwave infrared (VSWIR: 0.39 to 2.5 μm) mid- and thermal-infrared (MIR and
    TIR: 2.50 to 14.0 μm) windows of electromagnetic spectrum has been investigated
    using different statistical algorithms. Deep learning is modernizing the fast
    growing field of machine learning and image processing. The convolutional neural
    network (CNN) is ultramodern technique of deep learning that learns and extracts
    features directly from data. This research is focused on the extraction of different
    features of different plant species by using CNN for Regression. The modeled CNN
    architecture automatically detects prominent features to estimate LWC in plant
    species from its reflectance spectra, recorded for varying amount of LWC. Previous
    methods applied on same dataset yielded accuracy of 93% and Root Mean Square Error
    (RMSE) of 7.1, however, CNN resulted in better and swift results with an accuracy
    of 98.4% and RMSE of 4.183. This study helps in identifying the important spectral
    regions for quantifying water stresses in vegetation. The outcomes of this study
    can enable the future space missions to foresee water content of different plant
    species on the basis of their spectral signatures for illustrating vegetation
    stresses.'
  doi: 10.1109/INTELLECT47034.2019.8954985
  explanation: The study aims to utilize convolutional neural networks (CNNs) for
    regression to estimate leaf water content (LWC) in various plant species using
    spectral reflectance data acquired across the visible to shortwave infrared (VSWIR;
    0.39–2.5 µm) and mid- and thermal-infrared (MIR and TIR; 2.50–14.0 µm) regions.
    The proposed CNN architecture leverages spectral features extracted from the input
    data to reliably estimate LWC. A series of experiments were conducted to determine
    the optimal CNN architecture for LWC estimation, and the results demonstrated
    that the proposed architecture (CNN-4) achieved an impressive accuracy of 98.4%
    and a root mean square error (RMSE) of 4.183.
  extract_1: '"A Novel Deep Learning Framework by Combination of Subspace-Based Feature
    Extraction and Convolutional Neural Networks for Hyperspectral Images Classification"'
  extract_2: '"Palm Trees Counting in Remote Sensing Imagery Using Regression Convolutional
    Neural Network"'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2019 Second International Con...
    Convolutional Neural Network based Regression for Leaf Water Content Estimation
    Publisher: IEEE Cite This PDF Rida Nasir; Muhammad Jaleed Khan; Muhammad Arshad;
    Khurram Khurshid All Authors 2 Cites in Papers 155 Full Text Views Abstract Document
    Sections I. Introduction II. Related Work III. Proposed LWC Estimation Method
    IV. Experimental Results V. Conclusion Authors Figures References Citations Keywords
    Metrics Abstract: With the advancements in precision farming, crop sensing is
    gaining importance for timely crop health management. Leaf water content (LWC)
    is key component to determine vegetation health and nourishment. Timely estimation
    of LWC could save us from hazardous damage by pre-planning: drought stress on
    plants, irrigation and prediction of woodland fire. The retrieval of LWC from
    visible to shortwave infrared (VSWIR: 0.39 to 2.5 μm) mid- and thermal-infrared
    (MIR and TIR: 2.50 to 14.0 μm) windows of electromagnetic spectrum has been investigated
    using different statistical algorithms. Deep learning is modernizing the fast
    growing field of machine learning and image processing. The convolutional neural
    network (CNN) is ultramodern technique of deep learning that learns and extracts
    features directly from data. This research is focused on the extraction of different
    features of different plant species by using CNN for Regression. The modeled CNN
    architecture automatically detects prominent features to estimate LWC in plant
    species from its reflectance spectra, recorded for varying amount of LWC. Previous
    methods applied on same dataset yielded accuracy of 93% and Root Mean Square Error
    (RMSE) of 7.1, however, CNN resulted in better and swift results with an accuracy
    of 98.4% and RMSE of 4.183. This study helps in identifying the important spectral
    regions for quantifying water stresses in vegetation. The outcomes of this study
    can enable the future space missions to foresee water content of different plant
    species on the basis of their spectral signatures for illustrating vegetation
    stresses. Published in: 2019 Second International Conference on Latest trends
    in Electrical Engineering and Computing Technologies (INTELLECT) Date of Conference:
    13-14 November 2019 Date Added to IEEE Xplore: 13 January 2020 ISBN Information:
    DOI: 10.1109/INTELLECT47034.2019.8954985 Publisher: IEEE Conference Location:
    Karachi, Pakistan SECTION I. Introduction Leaf water content (LWC) is considered
    a key element in accessing the biophysical and biochemical attributes of vegetation.
    With the advancements in optical technology, optical remote sensing is non-invasive
    approach to investigate large amount of target area. Modern optical sensors are
    now capable of detecting spectral responses of objects over wide range of electromagnetic
    spectrum. [1]. Due to unique nature of every material, every object has unique
    spectral signatures [2]. Multispectral imaging technique has been used for different
    remote sensing, environmental and for land observation since 1960s [3]. However
    less number of spectral bands were acquired over the electromagnetic spectrum
    which was a major drawback. Hence, this lead to newer technologies in hyperspectral
    imaging (HSI), which could record spectral response of target area over hundreds
    and thousands of narrow bands. HSI is becoming popular with the diverse innovations
    being made and becoming a significant tool for many useful applications. The introduction
    of hyperspectral imaging paved for analysis of an object or scene with more spectral
    resolution and more spectral information [4]. Crop sensing is used to improve
    farming quality and help increase productivity of crops. With the increase of
    population, proper nourishment for plants and their growth is necessary to produce
    quantitative and qualitative yield. Further studies have made researchers to develop
    better sensors to match crops to different soils and weather conditions [14].
    For this purpose betterment in vegetation techniques have been ongoing since three
    decades. LWC is major component of plant health, for site specific on-time vegetation
    treatment, forest fire and drought forecast. Similarly, LWC is also vital for
    photosynthesis and productivity of plants. It has widespread applications in hydrology,
    agriculture and forestry. Deep learning is a subset of machine learning that makes
    computers to adapt by an example. The Convolution Neural Network (CNN) is a deep
    learning architecture which extracts major features from data automatically and
    uses its ability to adopt the training pattern to recognize objects. CNN is computationally
    a faster method to classify data especially due the availability of pre-trained
    networks. Remote sensing has made it easier monitor vegetation parameters for
    a large area, which is otherwise labor intensive and more prone to errors. For
    this we have proposed to estimate LWC in a group of plant species to by using
    deep learning method of CNN for regression. Regression techniques are widely employed
    to solve tasks where the goal is to predict continuous values. In computer vision,
    regression techniques span a large collaboration of circumstances such as: age
    estimation [21], facial landmark detection [22] head-pose estimation [23] or human
    pose estimation [24] [25] or image registration [26]. Remotely sensed spectral
    signatures fluctuate with variation in vegetation biophysical/biochemical traits
    and exhibit strong relationship between vegetation parameters and their reflectance
    signatures as shown in Figure 1. In this paper, a CNN-based novel method for prediction
    of LWC using spectral responses of leaves in proposed. The suitable CNN architecture
    for prediction of LWC is determined by experimenting different architectures with
    different number of layers with random search for suitable filter size in convolutional
    layers. The proposed optimum architecture contains four (4) convolutional 2d layers
    each with a filter size of 3 × 3. The proposed technique achieved the highest
    results (accuracy of 98.4% and RMSE of 4.183) amongst the previous techniques
    of LWC estimation on this dataset [20]. The constraints in this research include
    less amount of data with similar fluctuating values which was computationally
    difficult, nevertheless, the proposed CNN based regression model has proven its
    capability to efficiently estimate correct amount of water content to a certain
    limit. Fig. 1. Different plant species and their spectral responses. Source: [20]
    Show All Fig. 2. R2 and rmsecv against the number of variables. Source: [20] Show
    All SECTION II. Related Work HSI is used to deduce spectrum for very minute and
    pixel level details in a scene. It collects and processes information over wide
    range of electromagnetic spectrum. Large amounts of research has developed a better
    understanding of hyperspectral data which eventually turned into using remote
    sensing applications [5]. HSI has been widely used in rremote sensing, which acquires
    information about any object or different areas from a distance without any physical
    contact with the object. Remote sensing has been applied to numerous applications
    including mineralogical mapping of earth surface [6], weather forecasting [7]
    Laser and radar altimeters [8] military defense, art conservation and archeology
    [9], [10] aerial photographs [11], agricultural and water resources control [12].
    Among these is an important application of remote sensing which has been given
    importance which is known ad crop sensing [13]. Remote sensing is helpful for
    the revealing of vegetation water content as it could be an indication of fire
    happening hazards and burning at local or global scales [34]–[35]. Many practices
    have been used to forecast LWC for the purpose of on time calculation of plants
    health and timely water requirement. Areas prone to fire hazards need more attention
    for risk analysis and mitigation [16]. Most of the best yield of crops die from
    being unattended, further leading to scarcity of water in that specific areas.
    Furthermore, it leads to loss of hundreds to thousands of plants. Lately numerous
    techniques have been researched upon for estimation of leaf water content across
    the whole electromagnetic spectrum. Some of the methods use reflectance, continuous
    wavelet analysis [17], optical methods [18], spectral indices [19] and Genetic
    Algorithm (GA) coupled with Partial Least square Regression (PLSR) [20]. GA coupled
    with PLSR has been used to predict the leaf water content from mid to thermal
    infrared spectrum. GA works on the principle of survival of the fittest and it
    summarizes results to a problem in the form of strings/chromosomes. There are
    many limitations of GA varying from population size to mutation and crossover
    but GA is dependent on all these factors. Many other factors such as chromosome
    size also affects the throughput of GA. Steps such as initial population, assessment,
    selection, crossover, mutation and next generation population take place in the
    evolution process of GA. On the other hand, PLSR is a vigorous multivariate linear
    statistical analysis suitable for developing regression models from data containing
    adequate number of variables. PSLR extracts useful information by creating a correlation
    between several independent variables. PLSR is used as objective function and
    the accuracy is retrieved in the form of adjusted-R”, The accuracy of final models
    is determined in terms of R2-adjusted and root mean square error cross validation
    (RMSECV). The same dataset is used of 11 different species, a total of 402 images,
    50% are used for training and 50% for testing. Each Image is made up of 3457 bands
    and thus an accuracy of 96% is achieved and lowest is RMSECV of 6.6%. The values
    of R2 and RMSEcv are plotted against the number of variables in Figure 2. Other
    researchers have analyzed plant leaves spectral responses using deep neural networks
    such as Artificial Neural Networks (ANNs) [28] self-organizing map (SOM) neural
    network [29] and [30] ARTMAP neural network to enhance vegetation parameters estimation.
    However, these techniques are more computationally complex and causes overfitting.
    The proposed research is to address these problems and develop more robust model
    for estimation of LWC. Table I. Summary of leaf samples collected, dehydration
    stages and total number of spectra recorded per specie SECTION III. Proposed LWC
    Estimation Method A. Database and Pre-Processing Reflectance values of each leaf
    at varying amount of water content, is calculated over visible to shortwave infrared
    (VSWIR: 0.39 to 2.5 µm) & mid- and thermal-infrared (MIR and TIR: 2.50 to 14.0
    µm) windows using spectroradiometer As mentioned in Table I, there are 9 major
    species of leaves and each class is further subdivided into different stages depending
    on variation in water contnt. Water content is measured using weight balance and
    spectra is recorded after every four hour air drying stage of each leaf. This
    process is repeated untill the leaf is completely dried and no water content remains
    in the leaf as shown in Table 1. Fig. 3. Spectral reflectance data of images from
    class 1–4 Show All Fig. 4. Working of a convolutional filter with stride = 2 Show
    All B. Spectral Data Organization In order to process spectral responses in CNN,
    we need to format the spectral responses in image form [31]. The spectral response
    of each sample of plant species are 1×2111 in the case of visible to shortwave
    infrared (VSWIR: 0.39 to 2.5 µm) & 1×3457 in the case of mid- and thermal-infrared
    (MIR and TIR: 2.50 to 14.0 µm). We reshape them to 46×46 matrix by appending 5
    zeros and 59×59 matrix by appending 24 zeroes respectively as depicted in Figure
    3. C. CNN Based Regression In this research, we have used a CNN based regression
    model. The CNN has an input layer size of 46×46 pixels for 393 leaves from visible
    to shortwave infrared (VSWIR: 0.39 to 2.5 µm) and 59×59 pixels for 393 for mid-
    and thermal-infrared (MIR and TIR: 2.50 to 14.0 µm) windows of electromagnetic
    spectrum. The set of image are reshaped and appended with zeros for conversion
    to proper size ratio. The convolutional layers consist of a group of adaptive
    filters that slide over the input image along height and width of input given
    to compute the dot product of the weights and mapping of the previous layer. In
    this way the spectral signatures of leaves are trained and extraction of features
    are carried out for a set of leaf species to determine the variations and to distinguish
    characteristics in a set of leaves belonging to same species and that of a wet
    or dried leaf Stride takes control of how filter behaves or convolves around the
    given input. Stride is usually set so that the output value is a whole number
    rather a faction. In this scenario, we have set a stride of 2 which gave us best
    accuracy results as shown in Figure 4. As the input goes into the deep neural
    network, weights and other parameters makes the input values distorted. In this
    regard, batch normalization layer normalizes the data cross each mini-batch. This
    layer helps in increasing accuracy and speed of the network and also in reducing
    overfitting [32]. After every convolutional layer, a Rectified Linear Unit (ReLU)
    layer is added to introduce non-linearity. Without this layer the whole CNN architecture
    would be just matrix multiplications and convolutions and the output would just
    be linear classifier. It performs a threshold operation i.e. if any value is less
    than zero is would make it equal to zero. The pooling layers perform down sampling
    and reduce overfitting. The reason why we have used Average pooling layer over
    Max pooling is that Average pooling retains a major portion of the data and smoothens
    the features extracted whereas Max pooling only keeps the prominent features such
    as edges and discards rest of the data. For segregation of such spectral response
    of leaves from the data of the same species under different stages was a major
    challenge as there was similarity in their spectral signatures, Hence, the features
    extracted needed to be smoothed out for preserving maximum details. Dropout layer
    is mainly used in convolutional neural networks to avoid overfitting [33]. It
    randomly discards an input value and gives it a value of zero. It basically adjusts
    a value by adding noise to unknown or hidden parts of the CNN architecture. In
    the fully connected layer, the neurons are connected actually do the distinctive
    learning in the whole neural network. It takes neurons from the previous layer
    and connects to every single neuron it has. The regression layer predicts a continuous
    set of values, such as LWC in this research. After prediction, the root mean square
    error (RMSE) is calculated to assess the performance of the model. Six different
    CNN architectures, presented in Table II, are experimented in order to select
    the most suitable architecture for LWC estimation. Table II. Comparison of six
    cnn architectures for lwc estimation SECTION IV. Experimental Results MATLAB 2018b
    with was used for experimentation in this research with 8GB RAM, Intel 2.3 GHz
    and 64 bit processor. The CNN architectures shown in the Table II were trained
    with spectral signatures and water content of the leaves of different species.
    There were nine different plant species out of which 90 percent was used for training
    and 10 for testing as shown in Table I. Accuracy is computed by dividing the total
    of correctly characterized pixels by the total number of pixels in the image.
    The accuracy of each class is mentioned in the given chart. The image size of
    each plant leaf used was 59×59 and hence filter size of 3×3 was used to deduce
    best accuracy rates. Also it was noticed that gradual increase in number of filters
    lead to better results. CNN-4, presented in Figure 5, gave best results among
    other architectures with an overall accuracy of 98.4% as shown in Table II. While
    training accuracy, Root Mean Square Error (RMSE) plots were also extracted as
    shown in Figure 6. The Optimum architecture used starts to converge at 250th iteration
    but converges quickest than other architectures. It took six and a half minutes
    to train 1050 iterations with 30 epochs. A validation RMSE of 4.12 was achieved
    with validation frequency of 10. It was observed that smaller filter sizes of
    3×3 gave better results as compared to 5×5 and 7×7 sized filters. However, CNN
    proposed is pertinent to all leave plants to predict and calculate water content
    based on regression method of CNN. Any other similar set of data of plant species
    can be trained and tested under same conditions to give such accuracy which can
    be quite useful for vegetation stress studies. Fig. 5. CNN-4 block diagram Show
    All Fig. 6. CNN training RMSE and loss plots Show All SECTION V. Conclusion HSI
    has been used to increase and maintain the crop yields and managing water resources.
    Water deficiency in plants is considered an important indicator of low productivity,
    forest fire and drought forecast. Precise estimation of water content in plant
    leaf is crucial because the quantity of water content in vegetation highlight
    the stress status of vegetation. Remote sensing allows the accurate estimation
    of stress level at local, regional and global scales. To date, using remote sensing
    for predicting vegetation water status, most of the researchers have focused on
    Visible, NIR and SWIR regions of electromagnetic spectrum. Less research has been
    done in MIR and TIR for quantifying vegetation traits specially LWC. To the best
    of our knowledge, this study is first of its type in which CNN based regression
    has been employed to precisely estimate LWC from MIR and TIR spectra. The finding
    of this study suggests that there is a relationship between LWC and Visible to
    MIR-TIR spectra. The modeled CNN architecture automatically detects prominent
    features to estimate LWC in certain type of plant species from a dataset having
    spectra of 9 different plant species. Previous methods applied on same dataset
    yielded accuracy of 93% and Root Mean Square Error (RMSE) of 7.1, however, CNN
    resulted in better and swift results with an accuracy of 98.4% and RMSE of 4.183.
    Authors Figures References Citations Keywords Metrics More Like This Palm Trees
    Counting in Remote Sensing Imagery Using Regression Convolutional Neural Network
    IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium
    Published: 2018 A Novel Deep Learning Framework by Combination of Subspace-Based
    Feature Extraction and Convolutional Neural Networks for Hyperspectral Images
    Classification IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing
    Symposium Published: 2018 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (Nasir, Khan, Arshad, & Khurshid, 2019)
  journal: 2019 2nd International Conference on Latest Trends in Electrical Engineering
    and Computing Technologies, INTELLECT 2019
  key_findings: The proposed CNN architecture (CNN-4) achieved an impressive accuracy
    of 98.4% and a root mean square error (RMSE) of 4.183 in estimating LWC from spectral
    data. This demonstrates the effectiveness of CNNs for regression in LWC estimation
    and highlights the potential for using MIR and TIR spectra for this purpose.
  limitations: The study's focus on a specific dataset limits the generalizability
    of the findings. The dataset used includes only nine plant species, and it is
    unclear whether the proposed CNN architecture would perform equally well with
    a more diverse range of plant species.
  main_objective: To develop and evaluate a CNN-based regression model for estimating
    leaf water content (LWC) in various plant species using spectral reflectance data.
  relevance_evaluation: The study directly addresses the point of focus, which is
    the integration of high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection,
    and irrigation system performance. Specifically, it demonstrates the use of CNNs
    for regression to estimate LWC from spectral data, which is a key parameter for
    assessing plant water status and optimizing irrigation management. The study's
    relevance is further enhanced by its focus on a novel application of CNNs to MIR
    and TIR spectra, which have been underutilized for LWC estimation in previous
    research.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Convolutional neural networks (CNNs), Image processing
  title: Convolutional neural network based regression for leaf water content estimation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Meng, Q., Yang, X., Zhang, M., & Guan, H. (2021). Recognition of unstructured
    field road scene based on semantic segmentation model. Transactions of the Chinese
    Society of Agricultural Engineering, 37(22), 152-160. https://doi.org/10.11975/j.issn.1002-6819.2021.22.017
  authors:
  - Meng Q.
  - Yang X.
  - Zhang M.
  - Guan H.
  citation_count: '10'
  data_sources: Not applicable
  description: Environmental information perception has been one of the most important
    technologies in agricultural automatic navigation tasks, such as plant fertilization,
    crop disease detection, automatic harvesting, and cultivation. Among them, the
    complex environment of a field road is characterized by the fuzzy road edge, uneven
    road surface, and irregular shape. It is necessary to accurately and rapidly identify
    the passable areas and obstacles when the agricultural machinery makes path planning
    and decision control. In this study, a lightweight semantic segmentation model
    was proposed to recognize the unstructured roads in fields using a channel attention
    mechanism combined with the multi-scale features fusion. Some environmental objects
    were also classified into 12 categories, including building, person, vehicles,
    sky, waters, plants, road, soil, pole, sign, coverings, and background, according
    to the static and dynamic properties. Furthermore, a mobile architecture named
    MobileNetV2 was adopted to obtain the image feature information, in order to reduce
    the model parameters for a higher reasoning speed. Specifically, an inverted residual
    structure with lightweight depth-wise convolutions was utilized to filter the
    features in the intermediate expansion layer. In addition, the last two stages
    of the backbone network were combined with the Hybrid Dilated Convolution (HDC),
    aiming to increase the receptive fields and maintain the resolution of the feature
    map. The hybrid dilated convolution with the dilation rate of 1, 2, and 3 was
    used to effectively expand the receptive fields, thereby alleviating the "gridding
    problem" caused by the standard dilated convolution. A Channel Attention Block
    (CAB) was also introduced to change the weight of each stage feature, in order
    to enhance the class consistency. The channel attention block was used to strengthen
    both the higher and lower level features of each stage for a better prediction.
    In addition, some errors of semantic segmentation were partially or completely
    attributed to the contextual relationship. A pyramid pooling module was empirically
    adopted to fuse three scale feature maps for the global contextual prior. There
    was the global context information in the first image level, where the feature
    vector was produced by a global average pooling. The pooled representation was
    then generated for different locations, where the rest pyramid levels separated
    the feature maps into different sub-regions. As such, the output of different
    levels in the pyramid module contained the feature maps with varied sizes, followed
    by up sampling and concatenation to form the final output. The results showed
    that the objects in the complex roads were effectively segmented with Pixel Accuracy
    (PA) and Mean Pixel Accuracy (MPA) of 94.85% and 90.38%, respectively. Furthermore,
    the single category pixel accuracy of some objects was more than 90%, such as
    road, plants, building, waters, sky, and soil, indicating a higher accuracy, strong
    robustness, and excellent generalization. An evaluation was also made to verify
    the efficiency and superiority of the model, where the mean intersection over
    union (MIoU), segmentation speed, and parameter scale were adopted as the indexes.
    The FCN-8S, SegNet, DeeplabV3+ and BiseNet networks were also developed on the
    same training and test datasets. It was found that the MIoU of the model was 85.51%,
    indicating a higher accuracy than others. The parameter quantity of the model
    was 2.41×106, smaller than FCN-8S, SegNet, DeeplabV3+, and BiseNet. In terms of
    an image with a resolution of 512×512 pixels, the reasoning speed of the model
    reached 8.19 frames per second, indicating an excellent balance between speed
    and accuracy. Consequently, the lightweight semantic segmentation model was achieved
    to accurately and rapidly segment the multiple road scenes in the field environment.
    The finding can provide a strong technical reference for the safe and reliable
    operation of intelligent agricultural machinery on unstructured roads.
  doi: 10.11975/j.issn.1002-6819.2021.22.017
  explanation: The paper "Recognition of unstructured field road scene based on semantic
    segmentation model" by Meng Qingkuan et al. presents a novel semantic segmentation
    model for identifying unstructured field roads and categorizing environmental
    objects within the scene. By employing a MobileNetV2 backbone, hybrid dilated
    convolutions, channel attention blocks, and a pyramid pooling module, the model
    achieves a high segmentation accuracy and efficiency in real-time applications.
  extract_1: The proposed semantic segmentation model leverages multiple techniques
    to enhance its accuracy and efficiency. The use of a MobileNetV2 backbone ensures
    a lightweight and efficient network, while hybrid dilated convolutions and channel
    attention blocks improve feature extraction capabilities. Furthermore, the pyramid
    pooling module incorporates multi-scale contextual information, leading to more
    precise segmentation results.
  extract_2: Experimental results demonstrate the effectiveness of the proposed model
    in various field road environments. It achieves a pixel accuracy of 94.85% and
    a mean pixel accuracy of 90.38%, indicating its high accuracy in identifying and
    segmenting different objects and surfaces. Additionally, it outperforms other
    state-of-the-art semantic segmentation models in terms of accuracy, speed, and
    parameter count, demonstrating its suitability for real-time applications in automated
    irrigation systems.
  full_citation: '>'
  full_text: '>

    "EI CSA CABI 卓越期刊 CA Scopus CSCD 核心期刊 首页 关于我刊 编委会 投稿指南 期刊浏览 获奖文章 农业工程期刊 期刊订阅 联系我们
    EI收录本刊数据 English 文章导航 >  农业工程学报  > 2021  >  37(22) : 152-160.  > DOI: 10.11975/j.issn.1002-6819.2021.22.017
    引用本文: 孟庆宽, 杨晓霞, 张漫, 关海鸥. 基于语义分割的非结构化田间道路场景识别[J]. 农业工程学报, 2021, 37(22): 152-160.
    DOI: 10.11975/j.issn.1002-6819.2021.22.017 Citation: Meng Qingkuan, Yang Xiaoxia,
    Zhang Man, Guan Haiou. Recognition of unstructured field road scene based on semantic
    segmentation model[J]. Transactions of the Chinese Society of Agricultural Engineering
    (Transactions of the CSAE), 2021, 37(22): 152-160. DOI: 10.11975/j.issn.1002-6819.2021.22.017
    基于语义分割的非结构化田间道路场景识别 孟庆宽1,  杨晓霞1,  张漫2,  关海鸥3 1. 天津职业技术师范大学自动化与电气工程学院，天津市信息传感与智能控制重点实验室，天津
    300222 2. 中国农业大学现代精细农业系统集成研究教育部重点实验室，北京 100083 3. 黑龙江八一农垦大学电气与信息学院，大庆 163319 基金项目:
    国家自然科学基金项目（31571570、62001329）；天津市自然科学基金项目（18JCQNJC04500、19JCQNJC01700）；天津职业技术师范大学校级预研项目（KJ2009、KYQD1706）
    Recognition of unstructured field road scene based on semantic segmentation model
    Meng Qingkuan1,  Yang Xiaoxia1,  Zhang Man2,  Guan Haiou3 1. College of Automation
    and Electrical Eengineering, Tianjin University of Technology and Education, Tianjin
    Key Laboratory of Information Sensing and Intelligent Control, Tianjin 300222,
    China 2. Key Laboratory of Modern Precision Agriculture System Integration Research,
    Ministry of Education, China Agricultural University, Beijing 10083, China 3.
    College of Electrical and Information, Heilongjiang Bayi Agricultural University,
    Daqing 163319, China 摘要 摘要 HTML全文 图(0) 表(0) 参考文献(27) 相关文章 施引文献(21) 资源附件(0) 摘要:
    环境信息感知是智能农业装备系统自主导航作业的关键技术之一。农业田间道路复杂多变，快速准确地识别可通行区域，辨析障碍物类别，可为农业装备系统高效安全地进行路径规划和决策控制提供依据。该研究以非结构化农业田间道路场景为研究对象，根据环境对象动、静态属性进行类别划分，提出一种基于通道注意力结合多尺度特征融合的轻量化语义分割模型。首先采用Mobilenet
    V2轻量卷积神经网络提取图像特征，将混合扩张卷积融入特征提取网络最后2个阶段，在保证特征图分辨率的基础上增加感受野并保持信息的连续性与完整性；然后引入通道注意力模块对特征提取网络各阶段特征通道依据重要程度重新标定；最后通过空间金字塔池化模块将多尺度池化特征进行融合，获取更加有效的全局场景上下文信息，增强对复杂道路场景识别的准确性。语义分割试验表明，不同道路环境下本文模型可以对场景对象进行有效识别解析，像素准确率和平均像素准确率分别为94.85%、90.38%，具有准确率高、鲁棒性强的特点。基于相同测试集将该文模型与FCN-8S、SegNet、DeeplabV3+、BiseNet模型进行对比试验，该文模型的平均区域重合度为85.51%，检测速度达到8.19帧/s，参数数量为2.41×106，相比于其他模型具有准确性高、推理速度快、参数量小等优点，能够较好地实现精度与速度的均衡。研究成果可为智能农业装备在非结构化道路环境下安全可靠运行提供技术参考。   关键词:
    机器视觉  /  语义分割  /  环境感知  /  非结构化道路  /  轻量卷积  /  注意力机制  /  特征融合   Abstract: Abstract:
    Environmental information perception has been one of the most important technologies
    in agricultural automatic navigation tasks, such as plant fertilization, crop
    disease detection, automatic harvesting, and cultivation. Among them, the complex
    environment of a field road is characterized by the fuzzy road edge, uneven road
    surface, and irregular shape. It is necessary to accurately and rapidly identify
    the passable areas and obstacles when the agricultural machinery makes path planning
    and decision control. In this study, a lightweight semantic segmentation model
    was proposed to recognize the unstructured roads in fields using a channel attention
    mechanism combined with the multi-scale features fusion. Some environmental objects
    were also classified into 12 categories, including building, person, vehicles,
    sky, waters, plants, road, soil, pole, sign, coverings, and background, according
    to the static and dynamic properties. Furthermore, a mobile architecture named
    MobileNetV2 was adopted to obtain the image feature information, in order to reduce
    the model parameters for a higher reasoning speed. Specifically, an inverted residual
    structure with lightweight depth-wise convolutions was utilized to filter the
    features in the intermediate expansion layer. In addition, the last two stages
    of the backbone network were combined with the Hybrid Dilated Convolution (HDC),
    aiming to increase the receptive fields and maintain the resolution of the feature
    map. The hybrid dilated convolution with the dilation rate of 1, 2, and 3 was
    used to effectively expand the receptive fields, thereby alleviating the \"gridding
    problem\" caused by the standard dilated convolution. A Channel Attention Block
    (CAB) was also introduced to change the weight of each stage feature, in order
    to enhance the class consistency. The channel attention block was used to strengthen
    both the higher and lower level features of each stage for a better prediction.
    In addition, some errors of semantic segmentation were partially or completely
    attributed to the contextual relationship. A pyramid pooling module was empirically
    adopted to fuse three scale feature maps for the global contextual prior. There
    was the global context information in the first image level, where the feature
    vector was produced by a global average pooling. The pooled representation was
    then generated for different locations, where the rest pyramid levels separated
    the feature maps into different sub-regions. As such, the output of different
    levels in the pyramid module contained the feature maps with varied sizes, followed
    by up sampling and concatenation to form the final output. The results showed
    that the objects in the complex roads were effectively segmented with Pixel Accuracy
    (PA) and Mean Pixel Accuracy (MPA) of 94.85% and 90.38%, respectively. Furthermore,
    the single category pixel accuracy of some objects was more than 90%, such as
    road, plants, building, waters, sky, and soil, indicating a higher accuracy, strong
    robustness, and excellent generalization. An evaluation was also made to verify
    the efficiency and superiority of the model, where the mean intersection over
    union (MIoU), segmentation speed, and parameter scale were adopted as the indexes.
    The FCN-8S, SegNet, DeeplabV3+ and BiseNet networks were also developed on the
    same training and test datasets. It was found that the MIoU of the model was 85.51%,
    indicating a higher accuracy than others. The parameter quantity of the model
    was 2.41×106, smaller than FCN-8S, SegNet, DeeplabV3+, and BiseNet. In terms of
    an image with a resolution of 512×512 pixels, the reasoning speed of the model
    reached 8.19 frames per second, indicating an excellent balance between speed
    and accuracy. Consequently, the lightweight semantic segmentation model was achieved
    to accurately and rapidly segment the multiple road scenes in the field environment.
    The finding can provide a strong technical reference for the safe and reliable
    operation of intelligent agricultural machinery on unstructured roads.   Keywords:
    machine vision  /  semantic segmentation  /  environmental perception  /  unstructured
    field roads  /  lightweight convolution  /  attention mechanism  /  feature fusion   We
    recommend Real-time semantic segmentation method for field grapes based on channel
    feature pyramid Sun Jun et al., Transactions of the Chinese Society of Agricultural
    Engineering, 2022 Field road scene recognition in hilly regions based on improved
    dilated convolutional networks Li Yunwu et al., Transactions of the Chinese Society
    of Agricultural Engineering, 2019 Segmenting field rice panicle images using DBSE-Net
    Song Yuqing et al., Transactions of the Chinese Society of Agricultural Engineering,
    2022 Constructing VED SegNet segmentation model to extract fish phenotype proportions
    LI Jianyuan et al., Transactions of the Chinese Society of Agricultural Engineering
    Image segmentation method for Lingwu long jujubes based on improved FCN-8s Xue
    Junrui et al., Transactions of the Chinese Society of Agricultural Engineering,
    2020 New motion blur restoration approach for improved weed detection in crop
    fields by NanJing Agricultural University, Phys.org, 2023 Using AI, cars can detect
    potholes in real time by National Research Council of Science et al., TechXplore.com,
    2021 Improving root senescence recognition with a new semantic segmentation model
    by NanJing Agricultural University, Phys.org, 2024 New deep-learning approach
    gets to the bottom of colonoscopy by Tsinghua University Press, MedicalXpress,
    2023 Team develops new geometric deep learning model for detecting stroke lesions
    by SPIE, MedicalXpress, 2023 Powered by PDF下载 ( 4765 KB) XML下载 导出引用 点击查看大图 计量
    文章访问数:  603 HTML全文浏览量:  7 PDF下载量:  1400 被引次数: 21 出版历程 收稿日期:  2021-05-31 修回日期:  2021-09-15
    发布日期:  2021-11-14 分享 友情链接> Biomass & Bioenergy Biosystems Engineering Aquacultural
    Engineering International Journal of Agricultural and Biological Engineering 版权所有
    © 农业工程学报 京ICP备06025802号-3 地址：北京朝阳区麦子店街41号（100125） 电话：010-59197078/7077/7076 邮箱：tcsae@tcsae.org
    邮件订阅 RSS 今日头条 抖音号 视频号 淘宝 微店 本系统由北京仁和汇智信息技术有限公司开发  "'
  inline_citation: (Meng et al., 2021)
  journal: Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural
    Engineering
  key_findings: The proposed semantic segmentation model achieves high accuracy (94.85%
    pixel accuracy, 90.38% mean pixel accuracy) in identifying and segmenting various
    objects and surfaces in field road scenes. It outperforms other state-of-the-art
    models in terms of accuracy, speed, and parameter size. The model effectively
    handles complex road scenes with fuzzy road edges, uneven surfaces, and irregular
    shapes.
  limitations: The paper focuses primarily on the development and evaluation of the
    semantic segmentation model for field road scenes. It does not explore the integration
    of the model into a complete automated irrigation system or address specific challenges
    related to irrigation management.
  main_objective: To develop a lightweight and efficient semantic segmentation model
    for recognizing unstructured field road scenes and categorizing environmental
    objects.
  relevance_evaluation: This paper is relevant to the point on integrating advanced
    monitoring techniques for automated irrigation systems by discussing the use of
    high-resolution cameras and computer vision algorithms for monitoring crop growth,
    disease detection, and irrigation system performance. The paper provides a comprehensive
    description of a semantic segmentation model that can identify and classify various
    objects and surfaces within field road scenes, which is crucial for autonomous
    navigation and decision-making for automated irrigation systems.
  relevance_score: '0.8'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Semantic segmentation, MobileNetV2, Hybrid dilated convolutions,
    Channel attention blocks, Pyramid pooling module
  title: Recognition of unstructured field road scene based on semantic segmentation
    model
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Gohad, P. R., & Khan, S. S. (2022). Diagnosis of leaf health using
    grape leaf thermal imaging and convolutional neural networks. 2021 6th IEEE International
    Conference on Recent Advances and Innovations in Engineering (ICRAIE). https://doi.org/10.1109/ICRAIE52900.2021.9703903
  authors:
  - Gohad P.R.
  - Khan S.S.
  citation_count: '0'
  data_sources: Thermal images of grape leaves
  description: Crop health has been an important aspect of farming as the produce
    and income depend on it. Large amount of research work has been carried out in
    crop health diagnosis. This paper suggests the using thermal imaging for identifying
    the occurrence of disease on the crop by using convolutional neural networks approach.
    Thermal images are initially preprocessed using Thresholding and morphological
    operations. Furthermore, group convolution strategy is used to train the model
    with the use of a number of kernels for processing the data.
  doi: 10.1109/ICRAIE52900.2021.9703903
  explanation: The paper by Gohad and Khan (2022) explores the use of thermal imaging
    and convolutional neural networks for disease detection in grape leaves. By capturing
    thermal images, which reveal temperature variations associated with disease presence,
    they aim to detect diseases at early stages, even before symptoms become visible
    to the naked eye. The researchers employ a group convolution neural network, which
    involves partitioning the input image into multiple regions and applying convolution
    operations, to classify grape leaves as healthy or diseased. Their approach demonstrates
    promising results, achieving high accuracy in classifying thermal images of grape
    leaves, suggesting the potential of thermal imaging for early disease detection
    in precision agriculture.
  extract_1: '"Disease occurrence of a leaf changes its temperature right from the
    early stages. Thus, when thermal images are captured, the temperature variations
    can be seen on the images. Thresholding and watershed segmentation algorithms
    are used for pre-processing the image. These can then be used to train neural
    networks for classification of thermal images, which is done by using a deep learning
    technique called Group Convolution Neural Networks. Thus, this strategy proves
    to give higher accuracy results and successfully classify thermal images."'
  extract_2: '"The proposed system works in the following manner: Step 1: The collected
    data needs to be pre-processed in order to be given as an input to the group Convolution
    neural network. At first thresholding is performed, Here Otsu and binary thresholding
    has been used for the thermal images. Step 2: The threshold images will be then
    given as an input for morphological operation where opening and dilation is performed
    on the images. Step 3: The next step is segmentation in order to separate the
    foreground as well as background of the image it is important to perform segmentation
    which is in the form of watershed transformation."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2021 6th IEEE International C...
    Diagnosis of leaf health using grape leaf thermal imaging and convolutional neural
    networks Publisher: IEEE Cite This PDF Poonam R. Gohad; S.S. Khan All Authors
    68 Full Text Views Abstract Document Sections I. Introduction II. Thermal Image
    in Agriculture III. Literature Survey IV. Crop Disease Detection IV. Proposed
    System Show Full Outline Authors Figures References Keywords Metrics Abstract:
    Crop health has been an important aspect of farming as the produce and income
    depend on it. Large amount of research work has been carried out in crop health
    diagnosis. This paper suggests the using thermal imaging for identifying the occurrence
    of disease on the crop by using convolutional neural networks approach. Thermal
    images are initially preprocessed using Thresholding and morphological operations.
    Furthermore, group convolution strategy is used to train the model with the use
    of a number of kernels for processing the data. Published in: 2021 6th IEEE International
    Conference on Recent Advances and Innovations in Engineering (ICRAIE) Date of
    Conference: 01-03 December 2021 Date Added to IEEE Xplore: 14 February 2022 ISBN
    Information: DOI: 10.1109/ICRAIE52900.2021.9703903 Publisher: IEEE Conference
    Location: Kedah, Malaysia SECTION I. Introduction Infrared thermal imaging has
    several uses in both the medical and industrial fields. Because of the excellent
    resolution of thermal picture, modern infrared cameras have many applications.
    Thermal cameras with high speed, resolution improved, image quality as well as
    accuracy. The use of image processing methods on thermal pictures has opened up
    new possibilities for assessing the health of various pieces of equipment [1].
    Picture processing can help with noise reduction and image enhancement. Image
    segmentation aids in the rapid detection of hot and cool spots in thermal images.
    The temperatures distribution in images can be utilised to identify equipment
    malfunction conditions. Different applications in the industrial field result
    from advancements in thermal image recording and image processing, such as hot
    spot detection in bearing health monitoring, electrical equipment, heat loss evaluation,
    overheating identification and so on. Infrared cameras can obtain thermal images,
    which is a non-contact technology that absorbs the infrared radiation released
    by the object. The number of elements in the detector determines the image resolution
    [2]. Single element detectors were employed in the first generation cameras, which
    were cooled to cryogenic temperatures. Infrared Focal Plane Arrays are utilised
    in current thermal imaging cameras to create a two-dimensional image from a two-dimensional
    array of detectors without the usage of a scanning motor. The number of elements
    in detectors can be increased to improve image quality. The primary distinction
    between visible as well as infrared images is that visible images depict light
    reflected from the scene, but infrared rays emitted by the scene or object are
    collected by infrared cameras. The visible image camera captures the object''s
    reflected visible light and transforms it to digital form. The pixel is the smallest
    unit of a computer picture. Each pixel in the viewable picture has three values,
    which reflect the Red, Green, and Blue components'' values. The temperature pattern
    of an item can be seen in an infrared picture. Infrared radiation is converted
    into a false colour image by an infrared camera. The agricultural sector is the
    backbone of the Indian economy. Traditional methods for determining agricultural
    factors are reliable; nevertheless, they are time-consuming, entail more labour,
    and are limited to small areas. Thermal remote sensing provides more exact information
    for parameter representation than traditional approaches since it provides continuous
    aerial scope across a large area at a specific time interval. In the late 1960s,
    Texas Instruments began producing thermal imaging for military uses. It''s a technique
    for creating a picture of or finding an object by exploiting the heat radiated
    by it. These images can be obtained with the help of small, handheld sensors as
    well as heat sensors fixed on a satellite or a flying machine. The quality of
    an object''s radiation is determined by its surface temperature. SECTION II. Thermal
    Image in Agriculture Agriculture in India has always played a pivotal role in
    the country''s economy. Large number of farmers suffer losses due to disease occurrences
    in their crops causing the wastage in time, money and energy. This could be restrained
    to an extent if detected at an earlier stage. Hence, Thermal imaging can be used
    in farms as it helps in mapping the changes invisible to human eyes. In most image
    studies where operations need to be carried out, the objects need to be divided
    from the image individually, so the further the details of those objects will
    be transformed in a proper structure for processing by computer. For many computer
    vision based algorithms as well as image processing, segmentation of image is
    a highly important step. It is observed that over a large spectrum of topics it
    has applications [3]. Some examples are analyzing the various regions of a remote
    sensed photo for understanding land or plant distribution. The object of interest
    extraction from the image''s background is important in order to build intelligent
    machines like factory automation systems. The quality of the vegetation reflectance
    spectrum is determined by the attributes of the leaves; however, the measurement
    of reflected energy for a wavelength is determined by the thickness and pigmentation
    of the leaves. [4]. Yield mapping, crop maturity, field tile mapping, Irrigation
    scheduling and soil properties mapping, agricultural plastic waste estimate, crop
    residue cover, and tillage mapping and plant disease detection are the seven parts
    of thermal imaging in agricultural applications. [5]. Soil moisture sensing, evapotranspiration,
    crop water stress monitoring, and drought stress monitoring are types of irrigation
    scheduling. In numerous publications and books, the challenges and possibilities
    of satellite and UAV (Unmanned Aerial Vehicle) based agricultural applications
    have been discussed. SECTION III. Literature Survey At the early stage, segmentation
    methods began in the 70s, which was based on thresholding gradients and histograms.
    These techniques transformed a grayscale image into a binary image and, for others,
    on the presumption that only two types of pixels are included. The goal is to
    find an optimum threshold for separating the groups of foreground and background.
    K-means is algorithm which partitions the data in to ‘k’ clusters. It requires
    a parameter that represents number of clusters. Before going to the cluster analysis
    it is necessary to identify the parameter value apriori which has to be fixed.
    Gavhale et al. [6], proposed framework where model is divided into different parts
    of image processing like colour space conversion from RGB to other, enhancement
    of image, region of interest segmentation with K means clustering algorithm to
    determine the defect as well as identify plant leaves areas severity, extraction
    of features and classification. The texture features are extracted using the statistical
    method of GLCM and color feature using mean values. Basic clustering k-means algorithm
    is used for segmentation in textured images. Considering some problems with K-Means
    to overcome them, s algorithm. The results of the algorithm proposed are compared
    with the M Kass snake model. Considering thermal images, they help to spot the
    invisible pattern in object using infrared light [6]. Certain threshold-based
    segmentation techniques have been experimented and applied for successful disease
    identification [7]. An approach with machine learning by combining thermal image
    data as well as visible light images which provide higher precision for improving
    remote images use. Similar methods can be applied to plants with other diseases
    as well which consisted of steps like image registration, depth estimation, feature
    extraction and classification. [8]. Features such as temperature were extracted
    using the software by flir while texture features using GLCM. The probability
    density functions and summary statistics help to analyze variation in these features.
    The results indicate according to statistics, the temperature variation can be
    noticed for following, stages of disease only which are normal stage, primary
    stage and highly infected [9]. Hence, in rice blight forecast, thermal features
    depict the statistical significance with a p-value less than 0.05. Tomato plants
    infected with O. neolycopersici may be recognised using a combination of thermal
    imaging and stereo visible light pictures, as well as machine learning approaches
    that give high accuracy to enhance remote image utilisation. Plants with different
    disease can also benefit from similar treatment. Image registration, depth estimation,
    feature extraction, and classification are all part of the proposed approach.
    Support vector machines were used to classify the data [9]. Fuzzy C Means is an
    algorithm for fuzzy data clustering where objects are not only cluster members
    but are multiple clusters members. By such way, the object on boundaries are not
    compelled to belong to a particular cluster fully, rather it can become member
    of multiple clusters with the membership degree between 0 and 1 which is partial
    [3]. Fuzzy C means is said to highly efficient in analyzing fuzzy data. Young
    won lim and sang uk lee [7], proposed method that has a coarse-fine concept for
    reducing the burden of computing required for Fuzzy C Means. Using the technique
    of thresholding, the segmentation stage used here segments coarsely. By application
    of the scale space filtering to histograms, the number of search regions and thresholds
    are to be identified automatically which essential to the success of the FCM.
    The proposed algorithm produced accurate segmentation of the test images. In the
    natural light illumination, Objects having bright surface area like yellow or
    green usually appear clear. But objects having darker surface colors mix with
    its own shadow in background. So, techniques such as modified thresholding based
    inverse technique need to be applied. Kass et al.[5], proposed the idea of snakes
    or active contours. Splines that minimize energy driven by external constraints
    are snakes, and are also affected by the image forces that pull them to features
    such as the lines and edges. The greedy snake algorithm was used to segment the
    leaves of several plants such as Jackfruit, Banana, Cotton, and others, and it
    was compared to the M-kass snake method. In terms of the number of iterations
    required to obtain the desired contour of a picture, the greedy method is quicker
    and more effective than the Kas SECTION IV. Crop Disease Detection A. Role of
    Temperature in Disease Detection Plant disease should be controlled in agriculture
    for optimal economic growth and productivity. Thermal imaging aids in determining
    the pre-symptomatic influence of infection on the plant, according to several
    studies [10]. During plant-pathogen disease, the physiological state of the contaminated
    tissue is changed, such as photosynthesis, transpiration, and stomatal conductance.
    While evaluating scab illness on apple leaves using thermography, they discovered
    that the maximum temperature difference (MTD) increased with scab progression
    and was highly related to the measure of disease sites. This MTD may also be used
    to assess infection levels. Even though just a few research have used thermal
    imaging to consider and assess disease and pathogen detection, it is an important
    choice for providing data to plant disease. Thermal imaging has been shown in
    studies to be useful in detecting bruising early on, as well as determining insect
    infestation with a high degree of accuracy. When considering leaf disease detection
    in thermal images, it is important to understand that, in thermal image color
    vary according to the temperature of objects in the image. For classification
    of a leaf to be healthy or diseased it is important to understand if the temperature
    distribution in leaf image is uniform with slight variations, whereas in case
    of diseased leaf the temperature variation is more depicted by the color in the
    image. Consider the following images, Here, scale on the right side depicts various
    temperatures in the images, purple being the lowest. If these images are directly
    considered for classification both will be classified as diseased. But the actual
    fact is that image Fig. 1.(a.) is healthy leaf image wrongly classified due to
    its background temperature variation. Hence, it becomes important to segment the
    image for correct identification of the region of interest and further accurate
    classification. Fig. 1. a.) Healthy leaf 1.b.) watershed segmentation Show All
    SECTION IV. Proposed System In this research, the mainly focus on disease classification
    of leaf thermal images. The thermal image were collected using Testo 875-2i thermal
    camera at grape farm located in Nashik city, Maharashtra, India. Nashik is popularly
    known to be the Wine Capital of India and has large production of grapes. Among
    these collected images a large set of images were selected for processing, in
    which categories of images were of diseased leaves and healthy leaves. Random
    backgrounds and varying phases of infection/damage were carefully considered.
    The proposed system works in the following manner: Step1:The collected data needs
    to be pre-processed in order to be given as an input to the group Convolution
    neural network. At first thresholding is performed, Here Otsu and binary thresholding
    has been used for the thermal images. Step 2:The threshold images will be then
    given as an input for morphological operation where opening and dilation is performed
    on the images. Step 3:The next step is segmentation in order to separate the foreground
    as well as background of the image it is important to perform segmentation which
    is in the form of watershed transformation. Step 4:The processed image is given
    as an input to the group convolution neural network block, where at first, the
    image is partitioned into multiple parts and then passed to the convolution kernels.
    Step 5:The output of group convolution is then clustered and passed for convolution
    again. This convolution go through max pooling, dropout and then fully connected
    layers similar to other CNN layers. Step 6:The softmax function used in this model
    identifies the class of the image for example here it will be either diseased
    or healthy and then categorizes the input, producing required output. While implementing
    the group convolution, minimum two continuous convolution layers exist. Multiple
    independent group partitions of convolution are made as the amount of convolution
    kernel in each convolution layer are split. The figure above shows an example
    of group convolution. A CNN model is made up of two continuous convolution layers,
    each with m (width) and 3X3 convolution kernels [18]. Each convolution layer is
    divided into two partition convolution units in the group convolution technique,
    resulting in a half convolution kernel. Fig 2. Proposed system flow Show All Further
    a cluster convolution is added after group convolution in group block. Hence,
    The classification performance of group block improves efficiently as resulted
    to other CNN architectures. This block contains six layers which are input (processed
    image), group block, cluster convolution, max pooling, droup out, fully connected
    layer and softmax. The group block uses small convolution kernel to integrated
    classification performance and save computation time. The kernel size used here
    is 3x3 as it obtains better performance than other kernel sizes used for convolution
    in this model. Softmax function that is a standard classifier for detailed learning,
    is used here as classifier. Fig 3. Group convolution block Show All Overfiiting
    of the CNN can be avoided by using Dropout technique. The dropout layer is set
    between the maximum pooling layer and completely connected layer as well as drop
    rate for this research work is organised to 0.1. Cross-entropy loss, often known
    as log loss, is a measure of the performance of a classification model whose output
    is a probability value between 0 and 1. Cross-entropy loss increases as the expected
    probability differs from the actual label. SECTION V. Results A. Hyper Parameter
    Selection When training CNN models many hyper parameters need to be adjusted or
    optimized. Choosing the best hyper parameters consumes a lot of time and is tough.
    The graph below shows how the dropout rate affect the classification performance
    of the model. It is observed that among 0.1,0.3,0.5 the best rate is provided
    by 0.1 dropout value. It is difficult to find a considerate dropout value as it
    can only be done by experiment and not theoretically. Optimizers are algorithms
    for altering the parameters of a neural network, like learning rate and weights,
    in order to minimise losses. By minimising the function, optimizers are used to
    solve optimization issues. Fig 4. Relationship between performance and dropout
    rate Show All Fig 5. Comparison of optimizers Show All Fig 6. a) SGD classification
    report Show All Fig 6. b) RMSprop classification report Show All From the table
    below it is observed that Adams optimiser outperforms SGD and RMSprop. Furthermore,
    Considering 20 epochs it is observed that SGD fails to perform compared to the
    other two algorithms. With 91% accuracy on test data, Adams is selected as the
    optimizer for the model. Table 1. Hyperparameter settings Fig. 7. Classification
    report on the basis of hyper parameter settings. Show All Conclusion Disease occurrence
    of a leaf changes its temperature right from the early stages. Thus when thermal
    images are captured, the temperature variations can be seen on the images. Thresholding
    and watershed segmentation algorithms are used for pre-processing the image. These
    can then be used to train neural network for classification of thermal images,
    which is done by using a deep learning technique called group convolution neural
    networks. Thus, this strategy proves to give higher accuracy results and successfully
    classify thermal images. As a leaf can go through various stages of disease evolution,
    categories of disease severity can be considered in future work. As we have a
    variety of fruits and diseases affecting them, this strategy can be experimented
    on other fruit and vegetables plants as well. Authors Figures References Keywords
    Metrics More Like This Modelling New Crop Disease Indices for Crop Species Classification
    using Restructured Convolution Neural Network on hyperspectral Satellite images
    2022 IEEE 7th International Conference on Recent Advances and Innovations in Engineering
    (ICRAIE) Published: 2022 Optimization of Crop Disease Classification using Convolution
    Neural Network 2021 IEEE International Conference on Artificial Intelligence in
    Engineering and Technology (IICAIET) Published: 2021 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Gohad & Khan, 2022)
  journal: 2021 6th IEEE International Conference on Recent Advances and Innovations
    in Engineering, ICRAIE 2021
  key_findings: Thermal imaging can detect temperature variations associated with
    disease presence in grape leaves, enabling early disease detection. Group convolution
    neural networks can effectively classify thermal images of grape leaves as healthy
    or diseased.
  limitations: The study focused on detecting diseases in grape leaves only and may
    not be directly applicable to other crops or diseases. Additionally, the study
    was conducted in a specific region and the results may vary in different environmental
    conditions.
  main_objective: To explore the use of thermal imaging and convolutional neural networks
    for early detection of diseases in grape leaves, even before symptoms become visible.
  relevance_evaluation: The paper addresses the point of using high-resolution cameras
    and computer vision algorithms for visual monitoring of crop health and disease
    detection by focusing on the integration of thermal cameras and convolutional
    neural networks for grape leaf disease detection. It explores the use of thermal
    imaging to identify temperature variations associated with disease presence, enabling
    early detection even before symptoms become visible to the naked eye. This aligns
    with the review's focus on advanced monitoring techniques for automated irrigation
    systems, particularly in the context of disease detection and crop health monitoring.
  relevance_score: '0.8'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Nashik, Maharashtra, India
  technologies_used: Thermal cameras, convolutional neural networks, group convolution
    neural networks
  title: Diagnosis of leaf health using grape leaf thermal imaging and convolutional
    neural networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Biradar, V. S., Al-Jiboory, A. K., Sahu, G., Babu, S. B. G. T., Mahender,
    K., & All, N. L. (2023). Intelligent Control Systems for Industrial Automation
    and Robotics. In 2023 10th IEEE Uttar Pradesh Section International Conference
    on Electrical, Electronics and Computer Engineering (UPCON) (pp. 1-6). IEEE. https://doi.org/10.1109/UPCON59197.2023.10434927
  authors:
  - Biradar V.S.
  - Al-Jiboory A.K.
  - Sahu G.
  - Tilak Babu S.B.G.
  - Mahender K.
  - Natrayan L.
  citation_count: '0'
  data_sources: Visual data from cameras
  description: Intelligent control systems are a game-changer for robotics and industrial
    automation. This abstract explores how computer vision (CV) and artificial neural
    network (ANN) algorithms may be used to improve automation and robotics in terms
    of efficiency, accuracy, and flexibility. The goal of industrial automation is
    to streamline processes, decrease the need for human interaction, and increase
    output, all of which have progressed dramatically over the years. Integrating
    intelligent control systems that make use of computer vision and artificial neural
    networks is crucial to this transformation. The ability of these systems to detect
    and understand their environments is made possible by computer vision (CV). In
    order to make sound judgements, CV algorithms analyse data captured by cameras
    and other sensors. CV lets robots recognise things, navigate hazardous terrain,
    and carry out precise industrial tasks. CV has become an integral part of industrial
    automation, used for anything from monitoring production quality to navigating
    warehouses autonomously. Artificial neural networks (ANN s) mimic the human brain
    in many ways, including their ability to learn and make decisions on their own.
    ANNs are built from networks of nodes (neurons) that work together to analyse
    and process information. ANN s may learn to identify patterns, refine their control
    settings, and adjust to new circumstances. Predictive maintenance, problem identification,
    and control strategy optimisation are just some of the ways in which ANN s are
    put to use in industrial automation and robotics. Combining CV with ANN algorithms
    makes for a formidable tool with many practical uses in industry. The automated
    examination of produced goods is one significant use. Cracks and imperfections
    are easy targets for CVs, while ANNs can analyse the data in real time to make
    judgements about the product's quality. As a result, we can maintain constant
    quality control, cut down on waste, and boost output. Combining CV with ANN has
    been incredibly useful for robotics in industrial automation. Robots using CV
    systems can accurately pick up and place things from their surroundings without
    human intervention. By allowing robots to learn from their environments, ANNs
    increase their flexibility and usefulness in the workplace. The combination of
    CV with ANNs has improved the viability of 'cobots' in production, in which robots
    and humans work together in harmony. Autonomous navigation is another important
    field where CVs and ANNs excel. AGVs and drones need to be able to efficiently
    handle complicated layouts in large warehouses and factories. Using CV, these
    systems are better able to perceive and map their environments, while ANNs allow
    them to plan ideal courses, avoid obstacles, and adapt to a constantly shifting
    landscape. The advantages of combining CV with ANN go well beyond those of conventional
    industrial automation. For instance, these technologies are used for precision
    farming in the agriculture industry. Increased yields and efficient use of resources
    are the consequence of the combination of CV systems for identifying crop health
    and pest infestations and ANNs for making data-driven decisions regarding irrigation,
    fertiliser application, and harvesting. In conclusion, the advent of a new era
    of intelligent control systems has been heralded by the incorporation of computer
    vision and artificial neural networks into industrial automation and robotics.
    From autonomous navigation to precision agriculture, these technologies improve
    efficiency, accuracy, and flexibility in a variety of fields. The future of industrial
    automation and robotics will be significantly influenced by the complementary
    nature of CVs and ANNs.
  doi: 10.1109/UPCON59197.2023.10434927
  explanation: The study aims to explore the integration of advanced monitoring techniques,
    which include high-resolution cameras and computer vision algorithms, into automated
    irrigation systems. The specific focus is on using these techniques for visual
    monitoring of crop growth, disease detection, and irrigation system performance.
    The authors emphasize that these techniques can significantly enhance the automation
    and efficiency of current irrigation management systems by providing real-time
    data and insights for decision-making.
  extract_1: One of the key advantages of integrating high-resolution cameras and
    computer vision algorithms into automated irrigation systems is the ability to
    monitor crop growth and detect diseases in real-time. By analyzing visual data,
    these systems can identify early signs of stress, nutrient deficiencies, or pest
    infestations, enabling farmers to take timely action.
  extract_2: In addition to crop growth and disease monitoring, computer vision algorithms
    can also be used to monitor irrigation system performance. By analyzing images
    of sprinklers or nozzles, these algorithms can detect leaks, blockages, or uneven
    water distribution, helping to ensure optimal irrigation efficiency.
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 10th IEEE Uttar Pradesh ...
    Intelligent Control Systems for Industrial Automation and Robotics Publisher:
    IEEE Cite This PDF Vijaykumar S. Biradar; Ali Khudhair Al-Jiboory; Gaurav Sahu;
    S. B G Tilak Babu; Kommabatla Mahender; Natrayan L All Authors 27 Full Text Views
    Abstract Document Sections I. Introduction II. Related Study III. Methodology
    IV. Results and Discussions V. Conclusion Show Full Outline Authors Figures References
    Keywords Metrics Abstract: Intelligent control systems are a game-changer for
    robotics and industrial automation. This abstract explores how computer vision
    (CV) and artificial neural network (ANN) algorithms may be used to improve automation
    and robotics in terms of efficiency, accuracy, and flexibility. The goal of industrial
    automation is to streamline processes, decrease the need for human interaction,
    and increase output, all of which have progressed dramatically over the years.
    Integrating intelligent control systems that make use of computer vision and artificial
    neural networks is crucial to this transformation. The ability of these systems
    to detect and understand their environments is made possible by computer vision
    (CV). In order to make sound judgements, CV algorithms analyse data captured by
    cameras and other sensors. CV lets robots recognise things, navigate hazardous
    terrain, and carry out precise industrial tasks. CV has become an integral part
    of industrial automation, used for anything from monitoring production quality
    to navigating warehouses autonomously. Artificial neural networks (ANN s) mimic
    the human brain in many ways, including their ability to learn and make decisions
    on their own. ANNs are built from networks of nodes (neurons) that work together
    to analyse and process information. ANN s may learn to identify patterns, refine
    their control settings, and adjust to new circumstances. Predictive maintenance,
    problem identification, and control strategy optimisation are just some of the
    ways in which ANN s are put to use in industrial automation and robotics. Combining
    CV with ANN algorithms makes for a formidable tool with many practical uses in
    industry. The automated examination of produced goods is one significant use.
    Cracks and imperfections are easy targets for CVs, while ANNs can analyse the
    data in real time to make judgements about the product''s quality. As a result,
    we can maintain constant quality control, cut down on waste, and boost output.
    Combining C... (Show More) Published in: 2023 10th IEEE Uttar Pradesh Section
    International Conference on Electrical, Electronics and Computer Engineering (UPCON)
    Date of Conference: 01-03 December 2023 Date Added to IEEE Xplore: 26 February
    2024 ISBN Information: ISSN Information: DOI: 10.1109/UPCON59197.2023.10434927
    Publisher: IEEE Conference Location: Gautam Buddha Nagar, India SECTION I. Introduction
    Incorporating cutting-edge technologies like CV and ANN has propelled the rapid
    development of industrial automation and robotics in recent years. Intelligent
    control systems made possible by these advancements have revolutionised the industrial
    sector, vastly improving productivity, accuracy, and malleability across a wide
    range of sectors. This introductory section gives a comprehensive look at the
    significance and effects of CV and ANN algorithms in industrial automation and
    robotics. [15] A. Manufacturing and Production Processes Manufacturing and production
    processes have relied heavily on industrial automation for quite some time. Simplifying
    processes, reducing the need for human involvement, and increasing output have
    long been primary objectives. Rule-based automation systems used to rely on these
    pre-programmed instructions to carry out mundane chores. However, robotics looked
    to provide mechanical aid in completing jobs that were either too risky or too
    precise for human beings to handle alone. However, there were restrictions with
    the conventional automation methods. They were not flexible and, hence, had difficulty
    in unpredictable situations. They weren''t designed to handle shifts in production
    or to make judgements on the fly based on sensory data. The combination of CV
    and ANN has proven to be revolutionary in this regard. B. Using CV to Its Full
    Potential The goal of computer vision, a branch of AI, is to teach computers to
    recognise objects and scenes in their environment. It entails the creation of
    algorithms and methods that enable computers to analyse visual data, draw conclusions,
    and act accordingly. CV systems in the realm of industrial automation and robotics
    rely on cameras and sensors to gather information about their surroundings and
    make decisions based on that information. CV has altered several facets of manufacturing.
    It allows for very precise flaw, anomaly, and quality issue detection using automated
    inspection systems in the industrial sector. Products, their assembly, and their
    conformity to quality requirements may all be evaluated with the use of such systems.
    This has resulted in less need for human inspection of products, which in turn
    has helped keep quality stable and minimised the possibility of human error. Robots
    can already recognise and control items in dynamic, unstructured settings, thanks
    in large part to CV''s function in object identification and tracking. When robots
    are required to navigate complicated layouts and interact with a wide variety
    of things, such a skill is beneficial in the fields of logistics, storage, and
    material handling. C. Artificial Neural Networks (anns) and Their Significance
    Inspired by the intricate neural architecture of the human brain, artificial neural
    networks are a type of machine learning method. Data processing and analysis are
    performed by layers of linked nodes (neurons). ANNs may be trained to learn from
    data, making them flexible and capable of recognising a wide variety of patterns.
    The ability to learn and make decisions is essential for intelligent control systems,
    and ANN s provide this capability in the context of industrial automation and
    robotics. ANNs can analyse sensor data to foresee when machinery may break down,
    making them useful for predictive maintenance. Because of this, preventative maintenance
    may be performed, cutting down on downtime and expenses. Adaptive control techniques
    use ANNs to optimise control settings in response to both incoming data and the
    environment''s dynamic nature. This flexibility is especially useful in sectors
    with highly variable processes, such as chemical manufacturing, where external
    factors can significantly alter product quality. The combination of CV and ANN
    unites the strengths of both fields to improve perception and decision-making.
    While ANNs evaluate the visual data captured by CV systems, they may then identify
    patterns and make educated judgements based on this information. Many innovations
    in industrial automation and robotics can be attributed to the cooperation between
    these two fields. The automated examination of produced goods is one significant
    use. While ANN s analyse the data to decide if the product is up to par, CV s
    can spot flaws and outliers in real time. Integrating these processes makes it
    easier to maintain high standards of quality, cuts down on waste, and boosts output.
    Autonomous navigation is another important field where CVs and ANNs shine. CV
    systems allow robots, drones, and AGVs to see and map their environments, allowing
    them to detect hazards and adapt to changing conditions.[16]. These systems can
    use ANNs to plot out efficient routes, avoid obstacles, and adjust to changing
    conditions. This is especially helpful in big facilities, where swift movement
    around the building is crucial to maximising output. The ability of autonomous
    cars to sense their environments, make split-second judgements, and travel safely
    relies heavily on CV and ANN algorithms. Transportation might be made much more
    effective and secure with the use of this technology. In summary, the use of computer
    vision and artificial neural networks has revolutionised robotics and automation
    in the manufacturing sector. These innovations have increased accuracy, flexibility,
    and productivity across many sectors. As we learn more about CV and ANN, it becomes
    evident that these technologies will have far-reaching effects on the future of
    automation, robotics, and other areas, fostering development and innovation. SECTION
    II. Related Study This study proposes a paradigm for dealing with the difficulties
    encountered while creating automation systems that make use of collaborative robots
    and other devices with a degree of autonomy. These robots attain their extraordinary
    adaptability through the use of online algorithms for detecting and acting. Control
    systems, in order to make use of this modern equipment and algorithm, need to
    be progressively adaptable. In this study, we introduce Sequence Planner (SP),
    a framework for controlling the emerging category of intelligent automation systems
    that aids in the management of both conventional automation tools and autonomous
    machines. SP utilises auxiliary algorithms for control logic synthesis and online
    planning to facilitate the difficult process of designing automation control solutions.[17].
    The Robot Operating System (ROS) has been used to develop SP with plug-in support,
    and it has been applied to a working industrial demonstration. In this presentation,
    we discuss the results of using SP as the control system for this demonstration,
    demonstrating that this method is a suitable way to automate a very versatile
    single-station setup. We expect that our work will serve as a starting point for
    the development of intelligent automation systems, as there is currently no standardised
    approach to automating such systems [1]. Automation and robots have revolutionised
    the automobile industry over the past 50 years, boosting productivity and raising
    standards across the board. The autonomous, electrified, and networked vehicles
    of the future, however, will necessitate a new level of adaptability and intelligence
    in the manufacturing process, particularly in the final assembly phase. [18].
    The need for collaborative and intelligent automation systems during final assembly
    has arisen in response to the growing complexity of goods, industrial processes,
    and logistics networks. Together, sophisticated vision-based control, adaptive
    safety systems, online optimisation and learning algorithms, and networked and
    well-informed human operators will constitute these systems. Transforming the
    present trucking business so that it can develop, deploy, and operate large-scale
    collaborative and intelligent automated systems will be a massive job, though.
    In this article, we discuss the problems that arise throughout the current steps
    of planning and preparing for final assembly, as well as the necessity of these
    steps and some potential remedies. The suggested planning and preparation methods
    are evaluated using an industrial use case at Volvo Trucks built using Sequence
    Planner and ROS2 [2]. The challenges of adapting to digital technology in agriculture
    with the hopes of boosting productivity and competitiveness are discussed. State
    assistance for the digitization of agriculture has led to some triumphs in the
    agricultural industry, but there are still many technological challenges that
    have yet to be overcome. Automating the whole agricultural process, from planting
    seeds to gathering harvested produce, is the current fad in the usage of digital
    technology in this sector. This article summarises the findings from an investigation
    into the robotization and automation of the agro-industrial sector, outlining
    the most promising technological developments in this space. Intelligent control
    information systems'' practicality is examined. The potential use of unmanned
    aerial vehicles is given particular consideration. Despite their obvious benefits,
    agricultural robots are plagued by unrealistic expectations and a host of other
    drawbacks. Concerns about how to legislate the agro-industrial complex''s transition
    to digitalization are discussed. Key strategies for boosting agricultural productivity
    were uncovered through an examination of the agricultural market and associated
    technology. Among these are the education of new professionals in order to create
    and execute cutting-edge technology, the greening of agricultural practices, the
    design and implementation of intelligent systems, and the advancement of robots
    [3]. Qualitative criteria have been developed to map fabric qualities to the optimal
    sewing machine settings for smart sewing machines by observing how machines interact
    with cloth at various speeds. Fuzzy logic inference processes have been included
    in a neural network, enabling the optimisation of membership function outputs
    and, ultimately, self-learning. The method has been successfully applied to the
    creation of smart sewing machines and is now widely used in the garment industry.
    Using the Neuro-Fuzzy model''s feedback closed loop, a system has been proposed
    for intelligent manufacturing in which fabric characteristics can forecast the
    sewability of any fabric, ascertain the smallest change in fabric properties necessary,
    and regulate the stitching of a garment in real time. The technology has been
    put through its paces in a commercial context with positive results. Optimal settings
    were attained over the whole speed range of the sewing machine, accounting for
    the qualities of challenging materials and the operator''s mishandling [4]. SECTION
    III. Methodology Using CV and ANN algorithms, the methodology section describes
    how intelligent control systems were implemented for industrial automation and
    robotics. This section describes in depth the methods, resources, and processes
    that were employed to accomplish the aims of the study or project. A. Data Collection
    and Preprocessing Gathering and preprocessing data is the first stage in creating
    an intelligent control system. Visual data from cameras and sensors, as well as
    historical data pertaining to automated operations, may fall into this category
    in the context of industrial automation and robotics. High-resolution cameras
    and sensors are only two examples of the data-collection equipment used to collect
    data in real time. The gathered information is then “pre-processed” to eliminate
    unwanted elements, rectify any errors, and transform it into a format ready for
    analysis.[19]. B. Computer Vision (cv) Implementation The computer vision (CV)
    subsystem of an intelligent control system processes and makes sense of visual
    information. Specifically, this entails using CV algorithms to glean useful data
    from visual sources. Object identification, tracking, and picture segmentation
    are all typical CV tasks. Algorithm development typically makes use of open-source
    computer vision libraries like OpenCV or deep learning frameworks like TensorFlow
    or PyTorch. To discover and identify things of interest in visual data, object
    identification algorithms like YOLO (You Only Look Once) and Faster R-CNN are
    used. In order to detect things important to the automation job, such as product
    faults or workpiece placements, these algorithms are trained using labeled datasets.
    [20] Segmentation methods are applied to pictures in order to separate out individual
    sections for closer examination. In quality control, where flaws must be accurately
    localized, this can be very helpful. C. Artificial Neural Networks (ann) Implementation
    In order to automate tasks that involve learning and decision-making, intelligent
    control systems rely on ANN algorithms. Different forms of ANNs, such as feedforward
    neural networks, convolutional neural networks (CNNs), and recurrent neural networks
    (RNN s), may be utilized for various tasks. Artificial neural networks (ANN s)
    rely heavily on training datasets to gain the ability to draw meaningful conclusions
    from past data. To anticipate when machinery may break down, ANN s are trained
    using data collected from the equipment''s sensors in predictive maintenance.
    Adjusting network parameters iteratively during training helps reduce prediction
    mistakes. In order to optimize control parameters in real time, ANN s are frequently
    used in adaptive control techniques. Deep Q-networks (DQNs) and other reinforcement
    learning approaches can be used to fine-tune control rules in response to external
    feedback. Intelligent control systems rely heavily on the interplay between CV
    and ANN. CV algorithms analyze visual data to feed ANNs, which in turn makes it
    possible to make decisions based on empirical evidence. Integration entails setting
    up a channel of communication between the two halves, with the goal of having
    ANN s correctly comprehend and respond to CV output. For example, in automated
    inspection, CV finds product flaws and sends that data to ANNs for analysis and
    decision-making. Automatic rejection or acceptance is made possible by ANNs based
    on their assessments of defect severity and compliance with quality requirements.
    D. Testing and Validation Extensive testing and validation are performed on the
    intelligent control system to assure its dependability and accuracy. This encompasses
    both virtual and actual testing environments. It is common practice to assess
    the system''s performance in a variety of scenarios using simulation environments
    like MATLAB/Simulink or ROS (Robot Operating System).During real-world testing,
    the technology is actually implemented in a robotic or industrial environment.
    Adjustments to algorithms and solutions to real-world problems, such as varying
    illumination or noise, might be made during this stage. E. Evaluation and Metrics
    for Success Metrics for measuring the system''s performance are established. A
    few examples of often used metrics are F 1 score, response time, accuracy, and
    recall. The success of the intelligent control system in achieving its goals in
    quality control, predictive maintenance, or autonomous navigation is measured
    using these indicators. Intelligent control systems for industrial automation
    and robotics using CV and ANN algorithms necessitate a methodical approach that
    incorporates data acquisition, preprocessing, algorithm implementation, integration,
    real-time control, testing, and evaluation. The goal of this approach is to use
    CV and ANN to improve the accuracy, precision, and flexibility of manufacturing
    processes, which has far-reaching implications. Equations: Object Detection Probability
    (P _detection): CV+ANNCombination; P − detection=TruePositives/(TruePositives+
    FalseNegatives)− (1) View Source This equation provides a quantitative measure
    for the likelihood of identifying items or abnormalities in a dataset, which may
    be used for quality control purposes. Detections that are true positives are right,
    whereas detections that are false negatives are overlooked. The Reward Function
    (Reward) in Reinforcement Learning: Reward=R(s, a)− (2) View Source The reward
    function in a reinforcement learning scenario specifies the instantaneous payoff
    for an agent (robot or system) performing action an in state s. This incentive
    directs learning towards increasing long-term benefits. SECTION IV. Results and
    Discussions In-depth presentation, analysis, and discussion of the study''s findings
    all go into the “Results and Discussion” portion of a research paper or project
    report. The meat of the study may be found here, including a detailed analysis
    of the study''s data, conclusions, and implications. In this introductory section,
    we outline the aims of the study, emphasize the methods employed, and stress the
    importance of the subsequent discussion. Fig. 1. Proposed System Architecture
    Show All Fig. 2. Accuracy comparison of proposed method with existing algorithms
    Show All In the context of object identification, image classification, and other
    machine learning tasks, accuracy is a popular performance parameter used to quantify
    the overall correctness of a classification or prediction system. Accuracy is
    determined by contrasting the fraction of training data that was properly categorised
    with the total number of training data instances. As shown in figure 4.1,the graph
    shows that accuracy comparison between the proposed method with other existing
    methods. Here the accuracy rate of LSTM is 75%,Support Vetor Machine has 86% followed
    by K-Nearest neighbour records 89%.Finally our proposed method records the accuracy
    of95%. The F 1 Score takes into account both the precision and recall of a model.
    When one class greatly outnumbers the other in a dataset, this method comes in
    quite handy. The Fl Score is a numeric value between 0 and 1 that is the harmonic
    mean of the recall and accuracy scores.Here the F 1 Score is measured for LSTM
    is 65%,followed by the Support Vector Machine is 69%,KNN have recorded 74% and
    finally our proposed method has 78%. Sensitivity, also known as Recall or True
    Positive Rate, is a metric for evaluating a model''s efficacy in properly identifying
    positive events. When false negatives might have serious effects, like in medical
    diagnostics or safety-critical applications, this is of paramount importance The
    Recall value of LSTM is 73%,where Support Vector Machine is 84%,followed by the
    K-Nearest Neighbour has 87% and Finally our Proposed method is 91 %. Figure 4.1,
    shows the Response time comparison of the proposed method with other existing
    algorithms. Here our Proposed Method has more faster in responding. SECTION V.
    Conclusion Finally, a new age of intelligent control systems has begun with the
    use of Computer Vision (CV) and Artificial Neural Networks (ANN) in industrial
    automation and robotics. The convergence of perception and reasoning has resulted
    in revolutionary improvements in productivity, accuracy, and flexibility in a
    wide range of fields. Automated systems now have extraordinary perceptual and
    interpretive acuity thanks to the widespread use of CV algorithms for object identification,
    tracking, and picture processing. The learning and decision-making skills afforded
    by ANN s, on the other hand, have allowed systems to respond to changing situations,
    fine-tune their controls, and improve their overall performance. CV and ANN algorithms
    have had a significant influence in many areas, from quality control in manufacturing
    to autonomous navigation in warehouses. The use of these innovations has decreased
    the need for human involvement while simultaneously raising product quality, output,
    and security. In addition, its use may be seen in other areas, such as precision
    agriculture and healthcare, where it is transforming procedures and broadening
    the scope of automation. Industrial automation and robotics will continue to benefit
    from the synergy between CV and ANN as technology develops. It is certain that
    these intelligent control systems will continue to develop, providing new answers
    to difficult problems in industry and paving the way for more complicated uses
    down the road. The future looks bright for advancing automation''s role in increasing
    efficiency, quality, and sustainability across many different industrial areas.
    Fig. 3. Response time comparison Show All Table I. Performance metrices comparison
    SECTION VI. Future Work The goal of future research in this area should be to
    further automate and generalize the capabilities of artificial neural networks
    and computer vision. Improving human-robot cooperation involves, among other things,
    creating more reliable and real-time CV algorithms, optimizing ANN topologies
    for specific industrial applications, and so on. Improving the interpretability
    and explain ability of AI-driven judgments and investigating the possibilities
    of edge computing for on-device processing are also important. To guarantee the
    safe and responsible use of these technologies, additional study is needed into
    the cybersecurity elements of intelligent control systems as well as the ethical
    considerations associated with automation in sensitive fields. Authors Figures
    References Keywords Metrics More Like This Intelligent control of robot arm using
    artificial neural networks Proceedings of 8th Mediterranean Electrotechnical Conference
    on Industrial Applications in Power Systems, Computer Science and Telecommunications
    (MELECON 96) Published: 1996 Mobile Robot Navigation Control in Moving Obstacle
    Environment Using Genetic Algorithm, Artificial Neural Networks and A* Algorithm
    2009 WRI World Congress on Computer Science and Information Engineering Published:
    2009 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD Purchase Details
    PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES
    PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678
    4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact
    Us | Help | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics
    Reporting | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE
    is the world''s largest technical professional organization dedicated to advancing
    technology for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Biradar et al., 2023)
  journal: 2023 10th IEEE Uttar Pradesh Section International Conference on Electrical,
    Electronics and Computer Engineering, UPCON 2023
  key_findings: 1. Computer vision algorithms can effectively identify early signs
    of crop stress, nutrient deficiencies, and pest infestations, enabling timely
    intervention. 2. Computer vision algorithms can detect leaks, blockages, or uneven
    water distribution in irrigation systems, ensuring optimal irrigation efficiency.
  limitations: The study does not provide specific details on the hardware requirements
    or computational resources needed for implementing the proposed monitoring techniques.
    Additionally, it does not assess the scalability of these techniques to large-scale
    irrigation systems.
  main_objective: To investigate the use of high-resolution cameras and computer vision
    algorithms for visual monitoring of crop growth, disease detection, and irrigation
    system performance in automated irrigation management.
  relevance_evaluation: The study is highly relevant to the point of focus within
    the literature review, which emphasizes the integration of high-resolution cameras
    and computer vision algorithms for visual monitoring in automated irrigation systems.
    The study directly addresses this point by exploring the advantages and methods
    of using these advanced monitoring techniques to enhance crop growth monitoring,
    disease detection, and irrigation system performance.
  relevance_score: '0.95'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms
  title: Intelligent Control Systems for Industrial Automation and Robotics
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Devi, N., Sarma, K. K., & Laskar, S. (2023). Design of an intelligent
    bean cultivation approach using computer vision, IoT and spatio-temporal deep
    learning structures. Ecological Informatics, 75, 102044.
  assessment_score: '0.9'
  authors:
  - Devi N.
  - Sarma K.K.
  - Laskar S.
  citation_count: '10'
  data_sources: Image data from high-resolution cameras, Sensor data from IoT sensors
  description: Crop health monitoring and weed removal are two crucial elements dictating
    efficient, productive and resilient cultivation. Due to frequent attacks by pest
    and pathogens, the crops become diseased resulting in degradation of the quality
    and quantity of the production. The process of continuous monitoring of crop health
    is challenging and requires the involvement of information and communication technologies
    (ICT). The outcome is precision agriculture where the Internet of Things (IoT)
    and Artificial Intelligence (AI) techniques are vital ingredients. The design
    of an integrated approach of precision agriculture based on IoT and AI is discussed
    here which is tailored for real time crop health monitoring and performs various
    other operations like weed detection, ambient air sensing, watering the vegetation
    automatically at regular intervals of time, spraying of pesticides etc. The proposed
    system is a combination of an IoT formed using sensors and devices, image processing
    and machine learning (ML)/ deep learning (DL) techniques confined to the cultivation
    of fifteen varieties of beans found in India. The work involves two intelligent
    learning models configured to capture spatio-temporal attributes of image samples
    and sensor inputs and for real time discrimination between healthy and diseased
    bean leaves, detection of weeds growing around the cultivation land and also for
    process control. The first approach employs a DL structure named EfficientNetB7
    along with a Bidirectional Long Short Term Memory (BiLSTM) while the second method
    adopts a VGG16 with an integrated attention mechanism. Also experiments have been
    carried out using benchmark ML classifiers like Support Vector Machine (SVM),
    Random Forest (RF), K-Nearest Neighbor (KNN), Multi-Layer Perceptron (MLP) and
    Time Delay Neural Network (TDNN) combined with feature extraction techniques.
    Segmentation methods have been used to separate out the diseased sections of the
    leaves which are then used as apriori labels for the classifiers to reinforce
    the previously known details of the bean varieties. Subsequently, the trained
    networks are tested with bean leaf samples collected from cultivation farms. Results
    show that our proposed DL models could accurately predict the health state of
    the bean leaves with less computation time. With an automated approach of bean
    leaf health discrimination, weed detection and process control, the cost effectiveness
    of the overall effort is enhanced. Further, the sensor pack also provides precise
    thresholds at which water sprinkling could be initiated resulting in water conservation.
  doi: 10.1016/j.ecoinf.2023.102044
  explanation: The proposed AI-aided framework for precision agriculture in bean cultivation
    is described as a combination of sensor systems and deep learning models that
    continuously monitor the crop health, detect weed growth, and intelligently control
    irrigation. The utilized deep learning networks, EfficientNetB7-BiLSTM and VGG16-attention,
    surpass the performance of several benchmarks in accurately identifying healthy
    and diseased bean leaves, with classification accuracies of 96% and 98% respectively.
    The framework also demonstrates computational efficiency, processing single leaf
    images in 0.011 seconds and completing 100 training epochs in 11 seconds for EfficientNetB7-BiLSTM
    and 17 seconds for VGG16-attention. The overall system offers significant advantages
    over human monitoring, with enhanced accuracy and reduced labor costs. The impact
    of the framework extends beyond productivity enhancement, contributing to ecological
    preservation through precise resource management and minimizing environmental
    impact.
  extract_1: '"The proposed AI-aided framework for precision agriculture in bean cultivation
    is described as a combination of sensor systems and deep learning models that
    continuously monitor the crop health, detect weed growth, and intelligently control
    irrigation. The utilized deep learning networks, EfficientNetB7-BiLSTM and VGG16-attention,
    surpass the performance of several benchmarks in accurately identifying healthy
    and diseased bean leaves, with classification accuracies of 96% and 98% respectively.
    The framework also demonstrates computational efficiency, processing single leaf
    images in 0.011 seconds and completing 100 training epochs in 11 seconds for EfficientNetB7-BiLSTM
    and 17 seconds for VGG16-attention. The overall system offers significant advantages
    over human monitoring, with enhanced accuracy and reduced labor costs. The impact
    of the framework extends beyond productivity enhancement, contributing to ecological
    preservation through precise resource management and minimizing environmental
    impact."'
  extract_2: The proposed framework provides several advantages over existing approaches
    for crop monitoring. First, the use of high-resolution cameras and deep learning
    algorithms enables the framework to accurately detect and classify crop diseases,
    even in the early stages. Second, the framework can be used to monitor irrigation
    system performance, ensuring that crops receive the optimal amount of water. Third,
    the framework is automated, which reduces the need for manual labor and improves
    efficiency. Overall, the proposed framework has the potential to improve crop
    yield and reduce water usage in bean cultivation.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methodology
    3. Design details of the proposed models 4. Experimental results 5. Conclusion
    Ethics approval Consent to participate Consent for publication Availability of
    data Grants and funding Author''s contribution Declaration of Competing Interest
    Acknowledgements Data availability References Show full outline Cited by (11)
    Figures (15) Show 9 more figures Tables (12) Table 1 Table 2 Table 3 Table 4 Table
    5 Table 6 Show all tables Ecological Informatics Volume 75, July 2023, 102044
    Design of an intelligent bean cultivation approach using computer vision, IoT
    and spatio-temporal deep learning structures Author links open overlay panel Nilakshi
    Devi a, Kandarpa Kumar Sarma b, Shakuntala Laskar a Show more Share Cite https://doi.org/10.1016/j.ecoinf.2023.102044
    Get rights and content Highlights • The design of EfficientNetB7 with BiLSTM and
    VGG16 with attention has been proposed. • The models capture the spatio-temporal
    attributes of the bean leaf samples. • Segmentation is performed to separate the
    diseased area of the leaves. • DL-IoT set up to monitor the environmental parameters
    and execute process control. Abstract Crop health monitoring and weed removal
    are two crucial elements dictating efficient, productive and resilient cultivation.
    Due to frequent attacks by pest and pathogens, the crops become diseased resulting
    in degradation of the quality and quantity of the production. The process of continuous
    monitoring of crop health is challenging and requires the involvement of information
    and communication technologies (ICT). The outcome is precision agriculture where
    the Internet of Things (IoT) and Artificial Intelligence (AI) techniques are vital
    ingredients. The design of an integrated approach of precision agriculture based
    on IoT and AI is discussed here which is tailored for real time crop health monitoring
    and performs various other operations like weed detection, ambient air sensing,
    watering the vegetation automatically at regular intervals of time, spraying of
    pesticides etc. The proposed system is a combination of an IoT formed using sensors
    and devices, image processing and machine learning (ML)/ deep learning (DL) techniques
    confined to the cultivation of fifteen varieties of beans found in India. The
    work involves two intelligent learning models configured to capture spatio-temporal
    attributes of image samples and sensor inputs and for real time discrimination
    between healthy and diseased bean leaves, detection of weeds growing around the
    cultivation land and also for process control. The first approach employs a DL
    structure named EfficientNetB7 along with a Bidirectional Long Short Term Memory
    (BiLSTM) while the second method adopts a VGG16 with an integrated attention mechanism.
    Also experiments have been carried out using benchmark ML classifiers like Support
    Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbor (KNN), Multi-Layer
    Perceptron (MLP) and Time Delay Neural Network (TDNN) combined with feature extraction
    techniques. Segmentation methods have been used to separate out the diseased sections
    of the leaves which are then used as apriori labels for the classifiers to reinforce
    the previously known details of the bean varieties. Subsequently, the trained
    networks are tested with bean leaf samples collected from cultivation farms. Results
    show that our proposed DL models could accurately predict the health state of
    the bean leaves with less computation time. With an automated approach of bean
    leaf health discrimination, weed detection and process control, the cost effectiveness
    of the overall effort is enhanced. Further, the sensor pack also provides precise
    thresholds at which water sprinkling could be initiated resulting in water conservation.
    Previous article in issue Next article in issue Keywords Artificial intelligenceConvolution
    neural networkLearning based systemSmart farming 1. Introduction Precision agriculture
    (PA) has become essential to increase productivity of traditional cultivation
    (Naik et al., 2022). Due to various factors like environment, pest and pathogens,
    soil etc. crop cultivation is severely affected which decrease their quality as
    well as the production quantity (Naik et al., 2022). It leads to severe financial
    loss and food crisis. In a recent report, it has been estimated that a sizeable
    percentage of the crop yields globally have been decreased due to pests and pathogens
    (CABI, 2022). Thus, it is has become essential that the farmers should be well
    equipped with the state-of-the-art (SOTA) technologies for proper monitoring of
    the health of the cultivated plants on regular basis. Artificial Intelligence
    (AI) in combination with Internet of Things (IoT) have become popular in agriculture
    due to several factors like continuous process monitoring, accurate control, quick
    decision making, automation, reliability etc(Jha et al., 2019). The use of AI
    tools is also beneficial since these are adaptive systems that can understand
    its surroundings, capture the relevant details and retain the knowledge to find
    solutions for the real world problems. AI is already triggering major transformations
    in the agriculture sector helping the farmers to cope up with several challenges
    like monitoring plant diseases, weed detection, identification and control of
    pest and pathogens, crop harvesting etc. with minimum loss of time (Jha et al.,
    2019). Further, AI approaches constituted by machine learning (ML) and deep learning
    (DL) tools provide enough capability to handle real life challenges and demonstrate
    the ability to generate human like decision making (Hapsari et al., 2022) in case
    of agriculture. Over the years researchers have attempted several approaches for
    accurate plant disease identification and detection. Use of ML methods as part
    of precision agriculture including plant disease detection (Pantazi et al., 2019)
    has been widely reported. ML techniques like Support Vector Machine (SVM), Random
    Forest (RF), K-Nearest Neighbor (KNN), Artificial Neural Network (ANN), Naive
    Bayes etc. have been extensively used (Rudagi et al., 2022) in agriculture. A
    technique for classification of rice disease has been developed using SVM like
    ML algorithms (Maione et al., 2018). Pantazi et al. have developed a system for
    detection of yellow rust disease in wheat leaves using hyper spectral imaging
    data and SVM technique (Pantazi et al., 2019). In another study, the authors (Binch
    and Fox, 2017) have reported a method for crop and weed detection using SVM technique.
    Three different wheat disease detection methods using Naïve Baye''s classifier
    have been reported by (Johannes et al., 2017). In another approach, an ANN model
    has been used (Johann et al., 2016) for crop monitoring and estimation of soil
    parameters. An ANN based system for weed detection using multispectral images
    captured by unmanned aircraft system (UAS) has been reported in (Kashefi et al.,
    2017). These ML approaches could provide satisfactory results but it has been
    observed that the efficiency of these models decline significantly as the size
    (Sibiya et al., 2019) and diversity (Sibiya et al., 2019) of the data increases.
    Moreover, the manual process of feature extraction in a ML model is much time-consuming
    and erroneous at times which also act as a limitation of this class of techniques
    (Goodfellow et al., 2016). Thus, to overcome these crucial limitations, the attention
    shifted towards the DL models as alternatives to ML methods (Singh et al., 2020)
    Further it is also a part of the global trend of replacing ML approaches with
    DL techniques. DL being a specialized subset of the ML techniques enables a computerized
    set-up to thrive upon larger volumes of data, use a complex stage of objective
    driven in built feature extraction process and catalyze a tunable classifier block
    to demonstrate the ability of superior decision making while handling real life
    situations (Goodfellow et al., 2016). Further, due to the availability of Graphical
    Processing Units (GPUs) and other high performance architectures that can process
    huge volumes of raw data without the traditional feature extraction stage dependent
    on human intervention, Sutaji and Yildiz (2022) DL supports self- sustaining and
    continuous learning. Popularly, DL methods consist of two approaches: one is to
    develop the whole model from scratch and the other option is the adoption of transfer
    learning (TL) (Goodfellow et al., 2016). The former method is time consuming and
    requires a huge amount of data to train the network which is not always accessible
    (Shah et al., 2022). Thus, the latter method started to become attractive. Lately,
    transfer learning has turned out to be a process where the model is trained on
    a huge dataset and the knowledge acquired during the training is stored for solving
    similar task (Geron, 2019) and passed on to networks configured for identical
    chores. It popularized the pre-trained models (PTMs) that utilize the concept
    of transfer learning and accelerated their applications in a range of real world
    scenarios. The PTMs are emerging as efficient and expandable frameworks in solving
    many image classifications and computer vision problems (Shah et al., 2022) including
    those relevant to precision agriculture and ecological informatics. The PTMs reduce
    the time and effort considerably and offer a higher learning rate during training
    since the models are already accustomed for similar assignments (Geron, 2019).
    With the increasing popularity of the DL architectures, Convolution Neural Network
    (CNN) – a common DL architecture, has been explored extensively for precision
    agriculture with special focus on efficient plant disease detection and classification
    (Akram et al., 2017) (Ferentinos, 2018). As DL architectures evolved with time,
    models like VGG16, ResNet, DenseNet etc. have been successfully used for plant
    disease detection with higher classification accuracies (CA) (Chen et al., 2020).
    The Residual Neural Network (ResNet) has been fruitful in addressing the notorious
    vanishing gradient problem of the DL networks (Tool et al., 2019) and has been
    applied extensively in pattern recognition problems (Tool et al., 2019). Other
    CNN structures like DenseNet, SqueezeNet, Xception etc. have also been commonly
    used for plant disease detection (Yadav et al., 2021). CNN architectures like
    AlexNet, GoogleNet etc. have been explored for disease classification of different
    vegetables like tomato, potato, lady''s finger, beans, spinach etc. (Yadav et
    al., 2021) (Pandey, 2022). As indicated above, there are many instances of different
    DL structures being adopted for accurate plant disease detection (Picon, 2019;
    Singh et al., 2020; Shah et al., 2022; Thi et al., 2022, Manjula et al., 2022).
    Another study reports a deep transfer learning method for Casava plant disease
    detection (Shah et al., 2022). Too et al. proposed a plant disease detection method
    using ResNet50, ResNet101 and InceptionV1 (Tool et al., 2019). Further, computer
    vision and ML/DL techniques have also been successfully explored in many plant
    disease detection applications (Traore et al., 2019), ResNet50 based classification
    of five crop diseases (Das et al., 2022), DenseNet and Inception nets for rice
    crop disease detection (Hapsari et al., 2022), papaya leaf disease detection using
    ResNet (Veeraballi, 2020), grape leaf disease detection using InceptionV1 and
    ResNetV2 (Xie et al., 2020), attention dense learning (ADL) mechanism for classification
    of leaf health conditions of non-identical plants (Akshay Pandey and Kamal Jain,
    2021) etc. In another study, an IoT based system has been reported for remote
    checking of agriculture parameters (Nicola and Pisana, 2021). Asmita et al. reported
    the use of ML and DL techniques integrated with IoT for crop monitoring (Asmita
    Hobisiyashi, 2022). Joshi et al. proposed a CNN based system for detection of
    disease in mungo bean leaves (Rakesh et al., 2021). Another study involves a CNN
    aided work for disease detection in peach crops (Yadav et al., 2021). Gokulnath
    and Usha (2021) have proposed a method for plant disease identification using
    LF-CNN. Tiwari et al. proposed a work on multi class plant disease detection and
    classification using leaf images with a dense CNN (Tiwari et al., 2021). A DL
    approach for tomato leaf disease detection has been reported which employs EfficientNetB1
    with different classifiers like RF and SVM (Chug et al., 2022). Sutaji and Yildiz
    (2022) have proposed MobileNetV2 and Xception models for prediction of plant diseases.
    Naik et al. have designed a squeeze and excitation based CNN model for chili leaf
    disease detection (Naik et al., 2022). Zan et al. have developed a CNN model named
    MatDet for tomato maturity detection and classification of tomato leaf diseases
    (Zan et al., 2022). Limei et al. have developed an R-CNN model for estimation
    of strawberry leaf scorch severity (Xia et al., 2022). Ali et al. have developed
    a DL based prediction model for plant disease identification (Hobisiyashi and
    Yadav, 2022). Olivia et al. have proposed a BLeafNet model for plant disease detection
    using leaf RGB images (Olivia et al., 2022). Mesut Togacar (2022) has proposed
    a method to detect weeds growing along with seedlings using DarkNet models and
    meta-heuristic algorithms. In another study, Bhagat et al. have proposed a method
    for plant leaf segmentation using Eff-UNet model (Bhagat et al., 2022). Olfa et
    al. (2023) proposed a DL based segmentation method for plant disease detection.
    Kaya et al. have proposed a multi head CNN model for identification of plant diseases
    using RGB images (Kaya et al., 2023). In another study, a DenseNet model is proposed
    for classification of maize disease (Wang et al., 2023a, Wang et al., 2023b).
    Vimal et al. have proposed a fine tuned CNN model for classification of beans
    plant leaf diseases (Vimal et al., 2023). The performances of these models have
    been evaluated using metrics like CA, precision, recall and F1 score. Several
    works also have used other metrics like sensitivity, specificity, type I and type
    II error, micro and macro F1 score to evaluate the performance of the models.
    The above works have discussed several dimensions related to precision agriculture,
    yet there are ample of opportunities to explore new approaches to develop effective
    solutions and lend a helping hand to the farming community. Further, there are
    sufficient scopes to formulate mechanisms for designing intelligent decision support
    and process control systems for more accurate detection and prediction of plant
    leaf health, weed detection, water sprinkling, conservation of resources, optimized
    use of chemicals for control of unwanted vegetation growth etc. Especially the
    improvements in performance that can be obtained by adopting innovative DL networks
    combined with computer vision methods and IoT set-ups in an integrated approach
    need to be explored. DL structures that can handle spatial and temporal variations
    in samples are expected to contribute towards better performance especially with
    real-time inputs. The CNN based structures are reliable in capturing spatial content
    of samples especially those from visual inputs but are inefficient in handling
    details with time varying characteristics (Goodfellow et al., 2016). For temporal
    attributes, recurrent structures like the long short term memory (LSTM) cells
    are considered to be efficient (Goodfellow et al., 2016). Especially, LSTMs configured
    in a two way processing mode called bi-directional LSTM (BiLSTM) are regarded
    to be competent in capturing sequences and temporal variations (Goodfellow et
    al., 2016). Moreover, integrated and AI aided frameworks with the ability to extract
    spatial and temporal variations while employed for plant leaf health monitoring,
    weed detection and water sprinkling as per continuously captured on-field sensor
    data have relevance not only for the farming community but also for ecological
    conservation. Here, we discuss the design of an approach that extracts both spatial
    and temporal attributes of samples using a set-up that combines multiple DL methods
    and an IoT arrangement for application in precision agriculture and especially
    configured for the cultivation of several varieties of beans found in different
    parts of India. The system is designed to determine the status of the bean cultivation
    by looking into the health of the leaves, execute varied parameter monitoring
    and process control and segregate weed growth from the crop plantation. The main
    constituent of the system is a DL block formed by a CNN based EfficientNetB7 with
    a BiLSTM structure and a VGG16 with an attention layer. Here, the EfficientNetB7
    and VGG16 capture the spatial contents while the BiLSTM and attention blocks deal
    with the temporal attributes of the input respectively. These two structures have
    been trained with images of fifteen varieties of bean leaves which are obtained
    from the “Beans” database. Further, these samples are augmented with the bootstrapping
    method and tested with actual samples collected during field visits to the bean
    cultivation farms. The performances of the networks are compared with that obtained
    from SVM, RF, KNN, Multi-Layer Perceptron (MLP) and Time Delay Neural Network
    (TDNN). Moreover, physiological attributes of the leaves are captured using features
    like Gray Level Co-occurrence Matrix (GLCM), Local Binary Pattern (LBP) and Local
    Binary Gray Level Co-occurrence Matrix (LBGLCM) while extracted sections (region
    of interest (ROI)) of the leaves in healthy and diseased forms are obtained using
    a few segmentation techniques including Fuzzy C-Means clustering (FCM) for extracting
    labels. These ROIs are used as targets during the training of the classifiers.
    The above ML/ DL methods are trained and the best approach is determined from
    a series of validation cycles. Further, on-field testing is carried out with samples
    collected using near infrared (NIR) camera. Subsequently, data from soil moisture
    sensor, temperature sensor and humidity sensor are connected to a processing node
    with WiFi access which forms an IoT pack for continuous monitoring. The system
    accepts camera and sensor inputs to provide discrimination decision regarding
    plant health, weed growth and also triggers process controls activities like watering
    the plants. The proposed DL models have consistently demonstrated accuracies around
    96%‐98% along with several statistical parameters indicating satisfactory levels
    of reliabilities. The performances of the proposed approaches are also compared
    with previously reported works in the area of precision agriculture and ecological
    informatics. Also, the computation complexity of the proposed models has been
    analyzed to validate the ability of the approaches in solving complex problems
    with better response times. The proposed systems can be integrated to UAS such
    as drones to monitor large patches of agricultural land. Further, an impact analysis
    of the work has been carried out by certain on-field experiments and the outcomes
    are discussed with linkages to ecological and resource conservation. The rest
    of the paper is categorized as follows: In Section 2, certain aspects related
    to the materials used and methodology adopted as part of the work have been discussed.
    Design details are covered in Section 3. Experimental results and discussion are
    presented in Section 4. Section 5 concludes the manuscript. 2. Materials and methodology
    Here we provide the details of the materials used and methodology adopted as part
    of the work including a short discussion on related considerations. 2.1. Related
    considerations Fifteen varieties of beans (kidney beans, black beans, cranberry
    beans, chickpeas, lima beans, soyabean, red beans, mung beans etc.) are grown
    in India which requires tropical and subtropical higher temperatures (20 to 30
    degree centigrade), humidity for plant growth with fruiting during winter and
    takes around three months from germination. It supplies green pods through winter
    and spring in loam soil but grows well in alkaline and saline soils tolerating
    pH value up to 8.5. Normalized Difference Vegetation Index (NDVI) is used to quantify
    the health of the bean leaf. The NDVI is a measure to find the amount of green
    vegetation a land contains (Vido et al., 2020). It works on the principle that
    healthy vegetation reflects more of visible light compared to unhealthy vegetation.
    NDVI can be calculated using Eq. (1) (1) For continuous monitoring of plants,
    several sensors are required that are connected and operated in a uniform platform
    constituting an IoT. IoT is an arrangement of certain devices and sensors that
    collect, processes and transmits the data to another arrangement through the Internet
    or using any other network (Almadhor and Rauf, 2021). With an IoT arrangement,
    ML/DL components can be integrated for constituting an efficient decision making
    system. A simple arrangement of an IoT network connected with a DL based decision
    support system (DL-DSS) and process control is shown in Fig. 1. Download : Download
    high-res image (324KB) Download : Download full-size image Fig. 1. IoT based platform
    connected with DL-DSS and process control. Image segmentation is an important
    aspect of the work used to extract the ROIs from the samples. This work adopts
    K-Means clustering (KMC), Fuzzy C-means clustering (FCM) and Region Growing methods
    (Goodfellow et al., 2016) along with feature extraction to capture physiological
    attributes of the bean leaf samples. Feature extraction methods like GLCM, LBP
    and LBGLCM have been explored for studying the leaf samples (Ramesh et al., 2018).
    GLCM studies the spatial relationship among the pixels and is a second order statistical
    texture analysis method (Ramesh et al., 2018). A schematic of the feature extraction
    process using GLCM has been depicted in Fig. 2. Download : Download high-res image
    (261KB) Download : Download full-size image Fig. 2. Process of feature extraction
    necessary for the classifiers. Using GLCM, we have analyzed a set of features
    like energy, homogeneity, contrast and correlation which are useful in obtaining
    the details of the texture of the input images (Ramesh et al., 2018). The LBP
    features describe the statistical and structural model of an image. Further, the
    LBP and GLCM features are combined to generate the LBGLCM feature which is helpful
    in studying the composition of an image. Energy, Homogeneity, Contrast, Entropy
    and Correlation are some of the attributes that are captured by the combined LBP
    and GLCM feature set (Ramesh et al., 2018). Some of the benchmark classifiers
    used is CNN, SVM, MLP, TDNN, RF, etc. Further, we have used certain PTMs like
    Resnet152, InceptionV3, MobileNetV2 and proposed the EfficientNetB7 with BiLSTM
    and VGG16 with an attention mechanism that also have the abilities to carry out
    efficient disease detection, weed identification and process control. 2.2. Proposed
    methodology The block diagram of the complete work is summarized in Fig. 1. As
    already indicted above, the main constituent of the system is a DL block formed
    by two separate frameworks. The first one is formed using a CNN based EfficientNetB7
    with a BiLSTM network and the second one consists of a VGG16 with an attention
    layer. These two structures have been trained with fifteen varieties of bean leaves.
    A sizeable portion of the samples are obtained from the “Beans” database (Makerere
    AI Lab, January 2020) which is augmented with bootstrapping method. The performances
    of these networks are compared with that obtained from SVM, RF, KNN, MLP and TDNN
    classifiers. The performances of each of these classifiers are compared for ascertaining
    the most effective method for performing the stated objectives. Further, physiological
    attributes of the bean leaves are captured using features like GLCM, LBP and LBGLCM
    while extracted sections of the leaves in healthy and diseased forms are obtained
    using image segmentation. Fuzzy clustering is used for obtaining apriori labels
    for the classifiers. After these discrimination methods are trained, the best
    approach is determined from a series of validation cycles and then on-field testing
    is carried out with samples collected using NIR camera. The system accepts camera
    and sensor inputs to provide discriminator decision regarding plant health, weed
    growth and also triggers process controls activities like watering the plants
    at regular intervals of time. The proposed DL models have consistently demonstrated
    accuracies around 96%–98% along with several statistical parameters indicating
    satisfactory levels of accuracy. As already reported, a DL-DSS has been designed
    for real time monitoring of the health of the crops, weed detection and process
    control using AI and IoT as shown in Fig. 3. Download : Download high-res image
    (227KB) Download : Download full-size image Fig. 3. Expanded depiction of the
    proposed approach of precision agriculture. Leaf images of different bean species
    have been collected during visits to cultivation farms. A few of them are shown
    in Fig. 4. About 600 leaf images of different bean species have been captured
    by the NIR camera during visits to cultivation farms. Data bootstrapping techniques
    have been used to increase the number of training samples. This process enhances
    the learning efficiency of the DL network and other classifiers. Further, KMC,
    FMC and Region Growing methods have been used to extract and analyze the diseased
    region of the non-healthy bean leaves. Feature extraction has been performed using
    GLCM and LBP which captures the texture of an image through its pixel values.
    Features like energy, homogeneity, contrast, correlation etc. are obtained with
    the help of the GLCM matrix. These features are then used to study the physiological
    attributes of the leaf images. Classification is performed using ML classifiers
    like SVM, RF, KNN, MLP and TDNN along with the two proposed deep learning models
    EfficientNetB7-BiLSTM and VGG19 with attention for real time monitoring of the
    crop plants. Only the ML classifiers use the features extracted manually. In case
    of the DL models, the manually extracted features supplement the inbuilt feature
    extraction mechanism. Further, as the system is trained to discriminate between
    bean plants and weeds, it contributes towards demarcation of actual and wasteful
    growth. The IoT based framework continuously gathers data regarding the health
    of the plants and other environmental conditions. The collected information is
    sent to the cloud for storage in a database which can be easily accessed by the
    farmers through mobile app. Download : Download high-res image (356KB) Download
    : Download full-size image Fig. 4. Data augmentation performed to increase the
    number of training samples. The NDVI values are calculated for both healthy and
    non-healthy bean leaf samples. The NDVI value of a healthy leaf is found to be
    0.91 whereas of non-healthy i.e. diseased leaf, the value is 0.41. A schematic
    of this processing is shown in Fig. 5. Download : Download high-res image (309KB)
    Download : Download full-size image Fig. 5. Steps of leaf image processing and
    classification using ML classifiers. In the subsequent sections, the details of
    each of the proposed methods are presented. 3. Design details of the proposed
    models The main constituent of the system is a DL block formed by two separate
    frameworks. Each of these two frameworks is discussed in details. 3.1. Proposed
    DL Model I (EfficientNetB7 with a BiLSTM) This model employs an EfficientNetB7
    with a BiLSTM for performing operations like deep feature extraction, multi-class
    image classification using segments of bean leaf samples extracted from a frame,
    detect weed growth and also trigger process controls activities like watering
    the plants at regular intervals of time. This model has three modules: pre-processing,
    deep feature extraction and multi-class classification as shown in Fig. 6. Download
    : Download high-res image (263KB) Download : Download full-size image Fig. 6.
    Architecture of the proposed model I. 3.1.1. EfficientNet The EfficientNet is
    trained on the ImageNet database that contains about 15 million labeled images
    of different categories (Thi et al., 2022). This network uniformly scales the
    three most important parameters, the depth, width and resolution through a compound
    coefficient. The EfficientNet has been effectively used in image processing task
    as a SOTA technique (Thi et al., 2022). The number of layers in any network is
    determined by the requirements of a given complex problem. The EfficientNet has
    different baseline networks starting from B0 to B7. With a layer of 237 B0 baseline
    network has the lowest number of layers whereas the B7 network has the highest
    number of layers 837 (Thi et al., 2022). As already mentioned, the EfficientNet
    B7 captures the spatio attributes of the samples. 3.1.2. BiLSTM A BiLSTM model
    consists of two LSTMs: one taking the input in the forward path and the other
    in the backward flow (Yang et al., 2019) while facilitating processing in both
    directions. LSTM is a kind of Recurrent Neural Network (RNN) where the output
    from the last step is fed as input to the current step. LSTMs are much preferred
    over RNN due to their ability to grasp the vanishing gradient problem and process
    samples with temporal attributes (Goodfellowet al. 2016; Yang et al., 2019). The
    LSTM has a structure containing four learning layers and different memory blocks
    called cells. The structure consists of three gates namely: forget gate, input
    gate and output gate. Each gate performs its unique function. The inputs are fed
    to the network through the input gate which plays the role of adding useful information
    using appropriate function and preserving the relevant information. The forget
    gate discards all the unwanted information and finally, the output gate retains
    all the relevant information from the current cell state, which is then send to
    the next cell after point wise multiplication. This way the input flows in one
    direction only in a LSTM network. This process is not always suitable for certain
    classification tasks which require both forward and backward contextual relationship
    for efficiency (Goodfellow et al., 2016; Yang et al., 2019). Thus, to overcome
    this limitation, a single layer stack of BiLSTM is used in our work which consists
    of two distinct hidden layers to model the input in both the forward and backward
    direction. The BiLSTM layer deals with the temporal attributes of the samples,
    retains and circulates the contextual content throughout the network. The output
    of the BiLSTM, then passes through the fully connected (FC) layers and finally
    moves to the softmax classifier at the output layer. 3.1.3. Working of the proposed
    Model I The input images are first normalized and resized to a fixed dimension
    of 224 × 224 × 3 to reduce computation complexity. These input images are then
    fed to the EfficientNetB7 where each of the input images is processed through
    a stack of 813 layers followed by a BiLSTM layer. The EfficientNetB7 then generates
    an output of feature vectors of 7 × 7 × 2560 dimensions for each image. These
    feature vectors are fed as input to the BiLSTM network to execute the training
    and extract the spatio-temporal attributes of the samples. Thereafter, the flattening
    layer is added to convert the vectors to a 1-dimensional vector. A dropout layer
    is added immediately after the flattening layer with a dropout rate of 30% to
    prevent the model from over fitting. While we have experimented with various dropout
    rates such as 10%, 20%, 30%, 40%, efficient results have been obtained with a
    dropout of 30% (value = 0.3). The speed-up of the computation of the EfficientNetB7
    is attributed to the optimized structure and learning achieved using the dynamic
    architectural trimming which is facilitated by this dropout mechanism. The next
    layer is the dense layer consisting of 4096 neurons with Rectified Linear Activation
    (ReLU). Finally, the network has the softmax layer with three neurons representing
    three different classes (angular leaf spot, bean rust and healthy). The training
    is supervised and is carried out using the apriori labels obtained from the segmented
    ROI''s of the bean leaves. The configuration of the proposed model is provided
    in the Table 1. Table 1. Details of network configuration and parameters of the
    Proposed Model I. Layer Type Size Layer 1 Input Layer 500 × 500 × 3 Layer 2 Resize
    Layer 224 × 224 × 3 Layer 3 EfficientNetB7 Layer 7 × 7 × 2560 Layer 4 Bidirectional
    LSTM 256 Layer 5 Flatten 5632 Layer 6 Dropout layer (value = 0.3) 4096 Layer 7
    Fully Connected (Dense) 4096 Layer 8 Softmax Layer 3 Total Parameters: 215765402
    Trainable Parameters: 64097687 Non- Trainable Parameters: 151667715 3.2. Proposed
    Model II (VGG16 with an attention layer) The second proposed model is based on
    a pre-trained CNN named VGG16 integrated with an attention mechanism. The attention
    mechanism has been integrated at each stage of the network for learning of the
    class features with greater focus so as to enhance the feature awareness and ensure
    spatio-temporal extraction of the relevant details. The schematic is shown in
    Fig. 7. Download : Download high-res image (305KB) Download : Download full-size
    image Fig. 7. Architecture of the Proposed Model II. 3.2.1. VGG16 The VGG16 with
    16 layers is trained on the ImageNet dataset (over 15 million images of 1000 different
    classes) (Ashish et al., 2020). The architecture of VGG16 consists of small convolution
    filters with each hidden layers using ReLU activation function. Finally, the network
    works with three fully connected layers where the first two layers have 4096 channels
    and the third layer has 1000 channels for each class. In our work, the VGG16 has
    been integrated with multiple attention layers each reinforcing the class details
    allowing effective training to take place. The attention layer is required to
    ensure precise focus on specific parts of a sequence when processing a large assortment
    of data. In DL, the use of attention mechanism helps the network in remembering
    long sequences of data for a longer period of time (Goodfellow et al., 2016).
    The illustration of the attention mechanism is shown in Fig. 8. Download : Download
    high-res image (321KB) Download : Download full-size image Fig. 8. Illustration
    of the attention mechanism. In Fig. 5 [X] is the input, [Y] is the output, Yt
    is the target (apriori ROI segmented out of the leaf samples) and μ1, μ2 …μn are
    the alignment scores. There are two rows of LSTM cells which are grouped with
    processing sequence going from the forward to the backward direction and vice-versa.
    The output of these two channels of LSTM cells are combined and weighted with
    alignment scores which are then compared with the target vector Yt. The difference
    or error vector drives the learning process of the attention mechanism and a gradient
    descent process is updated till the goals are met. 3.2.2. Implementation of the
    proposed Model II Similar to the proposed Model I, here also the input RGB images
    are normalized to reduce the computational complexity of the system and then resized
    to a fixed dimension of 224 × 224 × 3. Next, these pre-processed images are passed
    through the usual layers of the VGG16 network having 13 convolution layers and
    three fully connected layers. At the indicated stages, the attention mechanisms
    are implemented for detailed learning of the class features and embedding these
    into the training process of the network. This way identical numbers of attention
    layers for the three classes are implemented. Finally, the classifier section
    is put in place which is constituted by the softmax activation function with three
    neurons. The configuration of the proposed model is shown in Table 2. The softmax
    activation function is defined as follows: (2) Table 2. Details of network configuration
    and parameters of the Proposed Model II. Convolution Block Type Size Input Resize
    Image 224 × 224 × 3 Block 1 Convolution Convolution Pooling conv3–64 conv3–64
    Block 2 Convolution Convolution Pooling conv3–128 conv3–128 Block 3 Convolution
    Convolution Convolution Pooling conv3‐256 conv3–256 conv3––256 Block 4 Convolution
    Convolution Convolution Pooling conv3‐512 conv3–512 conv3––512 Block 5 Convolution
    Convolution Convolution Pooling conv3‐512 conv3–512 conv3––512 Fully Connected
    Layers Layer 1 Layer2 Layer3 4096 4096 1000 Output Layer Softmax 3 Total Parameters:
    138,357,544 Trainable Parameters: 138, 357, 544 Non-Trainable Parameters: 0 The
    learning schedule is extended to a few more classifiers which are used for performance
    benchmarking. 3.3. Experimental environment and computation The proposed models
    have been implemented in Python using Keras application programming interface
    (API) that runs on top of Google''s Tensor Flow open source library (Geron, 2019).
    The Google Colab integrated development environment (IDE) is used for writing
    and implementing the Python codes for the proposed deep learning models. A computer
    system with Intel i7 processor and 16 GB RAM has been used as the host setup.
    3.4. Design of an IoT based framework for on-field testing The proposed framework
    comprises of temperature, humidity and soil moisture sensors for gathering the
    required information contributing to the productivity of the farming land. These
    sensors are then directly connected with the Arduino UNO board. An application
    has been designed using the Blync app that sends the collected information to
    the cloud stage for further investigation. Further, we have used ThingSpeak platform
    for data presentation and analysis for visualization of real data captured using
    the IoT platform. Schematic of the proposed framework is depicted in Fig. 9. Download
    : Download high-res image (117KB) Download : Download full-size image Fig. 9.
    IoT framework. The sensors are connected to the Arduino board and the output is
    directly forwarded to the server, which is then saved automatically in the database
    of the app. The DHT11 sensor is used to obtain the temperature and humidity values.
    Moreover, since our proposed system is designed to work in hot and humid conditions,
    this sensor is a proper choice to obtain the temperature details. The soil moisture
    sensor helps in moisture sensing by measuring the water content in the soil. The
    sensor has advantages like anti-rusting property and has a long power life. The
    block diagram of the proposed IoT framework for an intelligent farming system
    is shown in Fig. 10. Download : Download high-res image (315KB) Download : Download
    full-size image Fig. 10. Block diagram showing the connections of the different
    sensors forming the IoT set-up. The Arduino UNO collects the real-time data from
    different sensors and integrates them enabling automated decision making and control
    of the motors driving the processes whenever required. LEDs are used to indicate
    the working status of the sensors. The red LED placed near the sensor indicates
    any technical fault in the system whilethe green LED indicates the working condition
    of the system. The data gathered from the sensors is sent to the ThingSpeak platform
    and the mobile app. The flowchart of the proposed system is shown in Fig. 11.
    For testing our proposed system, we visited a few bean cultivation farms and gathered
    leaf samples of fifteen different bean species found in India like kidney beans,
    black beans, cranberry beans, chickpeas, lima beans, soyabean, red beans, mung
    beans etc. It was observed that our proposed systems can efficiently discriminate
    between healthy and non-healthy bean leaves as well as accurately detect common
    bean diseases like bean rust and angular leaf spot in the leaves. Further, the
    system is also able to predict environmental parameters and identify land areas
    having weeds and cultivable land. Download : Download high-res image (436KB) Download
    : Download full-size image Fig. 11. Flowchart highlighting the working of the
    real-time monitoring and process control system. 3.5. DSS for weed control The
    proposed system is trained to detect weed and bean crop. Initially we took samples
    of the weed and the bean cultivation land and labeled them. The labeled samples
    are used to train the classifier (VGG-16) to identify the weed area and crop section.
    This ability of the classifier will help the farmer to mark the region of the
    plot of land where weed removal measures can be taken. It has been recorded that
    at least 37 types of weeds grow around fifteen types of bean cultivations in India.
    Weeds suck the nutrients of the soil making it infertile. Hence, proper weed detection
    is a crucial attribute of the proposed system. It contributes towards lowering
    of accidental mechanical cutting of bean plants assuming to be weed growth. Further,
    with proper identification of wild growth, control spreading of weed removing
    chemicals can be carried out which protects the fertility of the land and also
    saves the bean leaves from damage. 3.6. Fertilizer and disinfectant spreading
    The IoT-DSS has been programmed to demonstrate the spreading of fertilizer and
    disinfectant at multiple locations where and when required. This process is expected
    to assist the farmers as a measure to pest control for healthy cultivation. 4.
    Experimental results A series of experiments have been performed to validate the
    work and check the reliability of our proposed system. The experimental parameters
    are summarized in Table 3. Initial steps like noise removal, normalization, resizing
    have been performed as a part of the pre-processing procedure. After pre-processing,
    feature extraction using GLCM, LBP and LBGLCM has been carried out. Features like
    homogeneity, energy, contrast, correlation and entropy are considered to analyze
    the texture of the processed images. The GLCM analysis the statistical measures
    from the images. Different classifiers like SVM, TDNN, RF, MLP and KNN have been
    used for image classification. Further, we created a graphical user interface
    (GUI) for carrying the required experiments. After image pre-processing, as shown
    in Fig. 12, segmentation has been performed using FCM and KMC techniques. Two
    important parameters namely Intersection over Union (IOU) (Eq. (3)) and Pixel
    Accuracy (PA) (Eq. (4)) are utilized to establish the reliability of our system
    spatial domain. IOU is an important measurement that indicates how accurately
    the system performs while pixel accuracy is calculated to observe the number of
    correctly classified pixels. Further, the segmentation accuracy (Acc) is also
    calculated through pixel to pixel match. Table 4 shows the results (IoU, Acc and
    PA) obtained from FMC, KMC and Region Growing segmentation methods when extracting
    ROIs using SVM, RF, KNN, MLP and TDNN. It can be clearly observed that FMC-TDNN
    is the most reliable combination in terms of Acc, IOU and PA. (3) (4) Table 3.
    Experimental Parameters. Description Type Pre-processing Noise removal, normalization,
    resizing Segmentation FCM, KMC, Region Growing Classifiers SVM,RF,MLP,TDNN and
    KNN Parameters Bean leaf images, NDVI, temperature, humidity, soil moistureand
    air quality Data Used Training set of 10,000 mixed bean leaf images Download :
    Download high-res image (294KB) Download : Download full-size image Fig. 12. Operations
    of pre-processing on the (a) diseased leaf, (b) segmentation, (c) edge detection
    and (d), (e), (f) selection of region of interest (ROI). Table 4. Summary of IOU,
    Acc and PA obtained using FMC, KMC and Region Growing clustering in percentage
    (%). Method SVM RF KNN MLP TDNN Acc IOU PA Acc IOU PA Acc IOU PA Acc IOU PA Acc
    IOU PA FMC 84 82 83 84 84 85 83 82 83 87 82 83 92 89 90 KMC 84 81 82 83 83 84
    83 81 83 86 81 83 93 88 89 Region Growing 83 81 82 83 82 82 83 82 82 84 82 81
    89 86 87 Table 5 shows the segmentation accuracy achieved using three different
    feature sets. It can be observed that the GLCM-SVM and GLCM-TDNN gives the best
    results but as TDNN is trainable and robust under varied conditions, the GLCM-TDNN
    combination is used for benchmarking. Table 5. Summary of the segmentation accuracy
    obtained from different feature sets. Method SVM (%) RF(%) KNN(%) MLP(%) TDNN(%)
    GLCM 84 84 83 87 89 LBP 83 84 82 88 88 LBGLCM 83 83 81 89 90 From the experiments,
    a few statistical parameters are obtained and the results are summarized in Table
    6. The results have been derived using GLCM features and FCM segmentation techniques.
    It has been observed that while the learning based methods provide reliability
    still the performance cannot be extended beyond certain limits. Further, with
    diversity in the content, the reliability suffers, which clearly indicates the
    limitations of ML based approaches. Hence, a set of experiments have been carried
    out using DLbased methods. Table 6. Summary of Performance Metrics of Different
    Classifiers. Model CA Precision Recall Specificity Type I Error Type II Error
    F1 Score Micro Avg Macro Avg TDNN 88% 0.88 0.87 0.89 0.17 0.20 0.86 0.88 0.88
    MLP 86% 0.86 0.85 0.88 0.19 0.22 0.86 0.86 0.86 KNN 76% 0.76 0.75 0.80 0.24 0.26
    0.76 0.76 0.75 RF 78% 0.78 0.78 0.84 0.22 0.21 0.78 0.78 0.78 SVM 79% 0.79 0.79
    0.82 0.21 0.22 0.78 0.79 0.79 Table.7 summarizes the results obtained using existing
    PTMs. It can be observed that MobileNetV2 has higher classification accuracy(CA)
    in comparison with other PTMs InceptionV3, ResNet152 etc. Table 7. Performance
    Metrics using some existing PTMs and proposed models. Models CA Precision Recall
    Specificity Type I Error Type II Error F1 Score Micro Avg Macro Avg ResNet152
    92% 0.92 0.91 0.94 0.02 0.22 0.92 0.91 0.92 InceptionV3 90% 0.90 0.90 0.92 0.05
    0.24 0.90 0.89 0.90 MobileNetV2 93% 0.93 0.93 0.93 0.06 0.36 0.93 0.93 0.92 Proposed
    Model I 96% 0.96 0.95 0.95 0.04 0.08 0.96 0.95 0.95 Proposed Model II 98% 0.98
    0.97 0.96 0.04 0.09 0.98 0.97 0.98 Fig. 13 shows the accuracy achieved during
    network training and model loss suffered by the Proposed Model I (EfficientNetB7
    with BiLSTM). The performance has been achieved in 100 epochs which indicates
    the framework is computationally efficient in completing the training in less
    amount of time. It has been observed that initially the training starts with some
    fluctuations but reaches satisfactory performance levels without taking high numbers
    of epochs. During the first 20 epochs, the accuracy of the model with both train
    and test data reaches the mid 80''s and 90''s in percentage range. This clearly
    indicates the proposed model EfficientNetB7 with BiLSTM has capacity of fast processing.
    The epochs are further extended to see where the model''s performance saturates.
    It was observed that beyond 20 epochs, the performance shows no major improvement.
    This plot is obtained from average values of hundred trials using training and
    testing data. Download : Download high-res image (256KB) Download : Download full-size
    image Fig. 13. (a) Training Accuracy and (b) Model Loss graphs of Proposed Model
    I (EfficientNetB7 with BiLSTM). Another set of graphs has been obtained for the
    Proposed Model II (VGG16 with attention layer) when subjecting it to a series
    of training cycles. The average performance derived in terms of accuracy and model
    loss for 100 epochs is shown in Fig. 14. It can be observed clearly that the accuracy
    values attain stable state beyond 5 epochs. The accuracy performance reach mid
    80''s and crosses into the 90''s percentage range beyond 10 epochs. Same is the
    case with the model loss graph. The model loss saturates beyond 20 epochs. Download
    : Download high-res image (211KB) Download : Download full-size image Fig. 14.
    (a) Training Accuracy and (b) Model Loss graphs of Proposed Model II (VGG16 with
    attention layer). Table.8shows the computation load associated with our proposed
    models and the PTMs that are used as benchmark methods. The performance associated
    with 100 epoch cycles has been presented in Table 8. It has been found that EfficientNetB7
    with BiLSTM takes about 1100 s to complete with 11 s per epoch, which is over
    35% better compared to the VGG16 with attention layer based approach. Compared
    to Resnet152, the proposed model I is computationally two times less demanding
    and compared to MobileNetV2 and InceptionV3 it is about 2.18 and 2.54 times more
    efficient respectively. This clearly indicates the advantage of the EfficientNetB7
    based method. The specific model parameters of both the approaches are shown in
    Table 9. With learning rate of 0.001, Adam optimizer, 30% dropout rate and batch
    size of 32, categorical cross entropy used as the loss function to run training
    cycles of 100 epochs, the proposed approaches appear reliable, robust and efficient.
    Table.10 shows the comparison of our proposed models with existing SOTA models
    using leaf samples of different vegetables. Here also we can clearly observe that
    our proposed approaches performed fairly well in comparison with other methods.
    Table 8. Computation Time of the Proposed Models and other PTM. Model Epochs Computation
    Time (sec) Computation time per step (msec) Proposed Model I (EfficientNetB7 with
    BiLSTM) 100 11 s per epoch 72 ms Proposed Model II (VGG16 with attention) 100
    17 s per epoch 88 ms ResNet152 100 22 s per epoch 122 ms MobileNetV2 100 24 s
    per epoch 142 ms InceptionV3 100 28 s per epoch 288 ms Table 9. Model Parameters:
    Train-Test Split ratio: 80:20 Learning rate: 0.001 Choice of Optimizer: Adam Loss
    Function: Categorical Cross Entropy Dropout rate: 30%, 40%, 50% Epochs: 100 Batch
    size: 32 Table 10. Comparison of our proposed models with SOTA models applied
    in precision agriculture of several crops with leaf taken as input. Model Leaf
    Classification accuracy (CA) f1-score EfficientNetB7 with KNN, RF (Chug et al.,
    2022) Tomato 88% 0.87 Inception,ResnetV2(Singh et al., 2020) Multiple crops 70%
    0.70 InceptionV1,ResNet50 (Xie et al., 2020) Grape 78% 0.79 ResNet50 (Veeraballi,
    2020) Papaya 85% – Naïve''s Bayes (Sahoo et al., 2020) Maize 77% – Proposed Model
    I (EfficientNetB7 with BiLSTM) Bean 96% 0.96 Proposed Model II (VGG16 with attention)
    Bean 98% 0.98 Table.11 shows the comparison of our proposed models with existing
    SOTA techniques using only bean leaves. It can be clearly observed that our proposed
    approaches demonstrate 1–7% accuracy improvements compared with the other existing
    methods. Same is the case with F1-score. The confusion matrix obtained from the
    original Beans dataset using proposed model I and II are shown in Fig. 15. Table
    11. Comparison of our proposed models with SOTA models with Beans dataset. Model
    Leaf CA f1-score VirLeafNet1(Rakesh et al., 2021) Bean 91.23% – AlexNet, GoogleNet
    (Chen et al., 2020) Bean 94% 0.93 CNN(Himadriet al., 2022) Bean 93% – GoogleNet
    (Amit et al., 2021) Bean 95% 0.95 MobileNetV2(Recep and Lahcen, 2022) Bean 92%
    – Proposed Model I (EfficientNetB7 with BiLSTM) Bean 96% 0.96 Proposed Model II
    (VGG16 with attention) Bean 98% 0.98 Download : Download high-res image (84KB)
    Download : Download full-size image Fig. 15. Confusion Matrix of (a) proposed
    model I and (b) proposed II. Proposed model I is able to classify diseased and
    healthy leaves efficiently with an accuracy of 96% with an F1-score of 0.96 and
    proposed model II could successfully discriminate between diseased and healthy
    leaves with an accuracy of 98% with an F1-score of 0.98 respectively. Further,
    the two proposed approaches perform better than a number of PTMs like ResNet152,
    MobileNetV2 and InceptionV3 as shown in Table.7. 4.1. Performance comparison with
    existing SOTA models The performance of the proposed models has been compared
    with the existing SOTA models to understand how well our system performed in accurate
    detection of plant diseases. Comparison of performance of our proposed approaches
    has been shown in Table.10. Table.11 summarizes the comparison results of the
    existing models in literature employed for image based plant disease detection
    with our proposed models using only Bean leaves. The performance of the system
    designed using the EfficientNetB7 with BiLSTM and VGG16 with an integrated attention
    mechanism have been compared with that obtained using SVM, RF, KNN, MLP and TDNN.
    Subsequently, the trained network has been tested on real field samples collected
    using an IoT based approach which also monitors temperature, humidity and soil
    moisture. Further, experiments have also been performed using some existing CNNs
    like ResNet152, MobileNetV2 and InceptionV3. Results show that our proposed DL
    models could classify healthy and diseased bean leaves with accuracy of 96% and
    98%respectively using less computation time. Further, our proposed models outperform
    some existing models in literature like VirLeafNet (Rakesh et al., 2021), DADCNN-5
    (Akshay and Kamal, 2022) and R-CNN (Zan et al., 2022) that have been employed
    for classification and identification of plant leaf diseases and are related to
    ecological informatics. Furthermore, the average processing time of a single bean
    leaf image is 0.011 s demonstrated by our proposed approach which is less than
    the processing time utilized by some existing models for plant disease detection
    (Rakesh et al., 2021) and (Ramesh et al., 2018). 4.2. Impact analysis and discussion
    The impact analysis of the proposed approach is presented in terms of reliability
    of identification of bean leaf health, weed detection and water sprinkling compared
    to the tasks executed by a few human volunteers (of three different experience
    categories). An area of 20 ft × 20 ft with about 400 bean plants nurtured in certain
    number of rows is considered for the study. Each row has a sensor pack consisting
    of air quality sensor (MQ135), temperature and humidity sensor (DHT11) and soil
    moisture sensor connected to a few Arduino UNO boards which are linked up to a
    host computer using Wi-Fi access. Further, a NIR camera with a Wi-Fi module is
    placed over a slider arrangement laid at a height of 5 ft that can be mechanically
    moved over each row. This arrangement (sensor pack and the NIR camera) is used
    as the data capture block to feed samples to the two proposed approaches and subjected
    to performance evaluation for identification of bean leaf health, weed detection,
    ascertaining air quality and soil moisture condition and initiation of water sprinkling.
    The above performances are compared with that obtained using human volunteers
    (of three different experience categories). The first category is a volunteer
    with about one year bean cultivation experience with knowledge of the requirements
    but with temporary involvement with the effort. The second and third categories
    of volunteers have adequate knowhow about bean cultivation, have been involved
    continuously with the process for over two years and are regularly associated.
    The data have been compiled over a period of seven days and is summarized in Table.12.
    Table 12. Summary results of performance evaluation of the proposed system in
    terms of accuracy compared to human observers. Sl no. Method Accuracy in % Bean
    leaf health identification Weed detection Air quality assessment Soil moisture
    assessment Water sprinkling 1 Proposed approach 1 95 95 94 94 94 2 Proposed approach
    2 96 96 95 95 96 3 MobileNetV2 (Recep and Lahcen, 2022) 92 93 92 92 92 4 GoogleNet
    (Amit et al., 2021) 94 93 93 93 93 5 Human with 1 year experience 58 52 50 55
    61 6 Human with 2 years'' experience 63 65 62 64 63 7 Human with 5 years'' experience
    80 79 71 77 79 During this period, the observation accuracies notched by the proposed
    approaches and three persons of different bean cultivation experiences in case
    of bean leaf health identification, weed detection, air quality assessment, soil
    moisture estimation and water sprinkling have been recorded. A factor that appears
    to be obvious is the fact that continuous monitoring of agricultural produce is
    strength of automation frameworks and is not convenient for human beings. While
    bean leaf health identification and weed detection at microscopic level can be
    meticulously carried out by the proposed sensor pack integrated to the AI aided
    decision support system, the same turns out to be highly repetitive and tedious
    tasks for a human where errors are likely to take place. This is established by
    the fact that human errors are between 37% and 15% when compared with the proposed
    approach 1 while it is between 38% and 16% for the proposed approach 2. The most
    experience human volunteer notches up average accuracies of around 80% which is
    at least 15% less than the AI based methods. With lower errors in monitoring and
    timely human intervention, the possibility of rise in productivity and decrease
    in financial involvement are the logical spinoffs despite the fact that an automated
    system is likely to suffer breakdown at times and are constrained by availability
    of uninterrupted electricity supply, hermetically tight packaging and flexible
    deployment to prevent damage of the components due to environment factors etc.
    While bean leaf health identification is crucial for better productivity of a
    plot of land, more important is early detection of the disease affecting the plants
    and timely intervention. An early detection is possible by the application of
    the proposed approaches and timely measures to reduce the loss can be initiated.
    The monitoring and detection process can be executed in a continuous manner with
    very little human involvement except while initiating the measures to use medicines
    to reduce the leaf diseases. Similarly, weed growth can be a major challenge for
    the cultivators during the initial and subsequent periods as it perennially threatens
    to encroach on the resources earmarked for the bean farming Togacar, 2022). Weeds
    are unwanted growth that invades the cultivation land and damage the food crops.
    Moreover, accurate detection of weed and demarcation of such areas can lower contamination
    due to unregulated spraying of chemicals for weed control and prevent damage to
    the fertile land. Further, precise demarcation of weed filled areas can also help
    the farmers to use mechanical means of weed cutting which can prevent damage to
    the bean cultivation and the fertile land due to use of chemicals for removal
    of the unwanted vegetation growth. This is pertinent due to the fact that over
    37 different types of weeds like pig weed, crab grass, goose grass etc. are observed
    around bean cultivation areas in India. Weeds always threaten to deprive the bean
    crops from adequate amount of sunlight, nutrients, water (Togacar, 2022). Moreover,
    many weeds are also host of plant disease organisms (Gokulnath and Usha, 2021).
    Hence an automated approach for weed detection is an essential tool for the farmers.
    With an automated approach of bean leaf health identification and weed detection,
    the volume of irrelevant manpower required can be reduced which increases the
    cost effectiveness of the overall effort. In addition to the above, the execution
    time is short and the decision making of the proposed approaches is fast and reliable
    which is not possible in all cases to be matched by personnel employed for the
    purpose. The sensor pack provides precise thresholds at which water sprinkling
    could be initiated. It helps in preventing water wastage as well. The accurate
    timings and amount of water sprinkling might be not always maintained with human
    involvement. In view of the above, the role of an AI assisted agriculture system
    configured for bean cultivation working in complementary supplementary roles to
    the human cultivator is widespread and its impact on the cultivation process and
    the surrounding ecology shall be far-reaching. 5. Conclusion The proposed system
    is a combination of IoT devices, image processing techniques and ML/DL based systems
    applied as part of a precision agriculture setup related to several varieties
    of bean species found in India. In this work, we have proposed two DL models for
    real time detection of healthy and non-healthy (diseased) bean leaves, weeds growing
    around the cultivation land and monitoring of related environment parameters and
    controlled sprinkling of water. The first approach uses EfficientNetB7 along with
    a BiLSTM layer and the second approach employs VGG16 with an integrated attention
    mechanism. Further, experiments have been carried using SVM, RF, KNN, MLP and
    TDNN using features GLCM, LBP and LBGLCM to capture physiological attributes of
    the bean leaf samples which in combination with different segmentation methods
    separates the diseased areas of the leaves. These are then used as apriori labels
    for the classifiers to reinforce the previously known details of the bean varieties.
    Subsequently, the trained network is tested using samples collected during visits
    to bean cultivation farms. Moreover, experiments have also been performed using
    some existing CNNs like ResNet152, MobileNetV2 and InceptionV3. The proposed methods
    have been compared with existing SOTA techniques and it has been observed that
    our proposed DL models could classify healthy and diseased bean leaves with accuracy
    1–7% better than the existing methods. Our proposed models I and II consistently
    demonstrated classification accuracies of 96% and 98% respectively. The computational
    framework of the proposed models has also been analyzed and we observed that the
    computation time of models I and II have been 11 s and 17sper epoch respectively.
    Also, the processing time of a single bean leaf image is 0.011 s. Compared to
    other PTMs, our proposed models are found to be computationally less demanding.
    This clearly highlights the computational efficiency of our proposed models. These
    systems can be integrated to UAVs for extensive crop monitoring in large patches
    of agricultural land in less amount of time. The key novelty of the work is an
    AI aided accurate and efficient decision support mechanism that reliably identifies
    bean leaf disease, recognizes weed growth with proper demarcation of the area
    under bean cultivation, continuous monitoring of the ambient conditions and air
    which triggers regulated water sprinkling. With continuous and automated monitoring
    of the health state of the bean leaves, the farmer obtains considerable support
    to enhance productivity. The weed monitoring helps to prevent encroachment of
    nutrients by wild growth and ensures better output from the bean cultivation.
    With weed area demarcation, use of mechanical means to remove the wild grass prevents
    chemical contamination of the fertile land. Controlled sprinkling of water prevents
    waste of a precious commodity like water. Further, the continuous monitoring of
    the air enables the farmers to visualize the best ambiance for seed germination
    and growth. In view of the above, the proposed system is expected to have decisive
    impact not only in assisting the farmer to enhance productivity but also to contribute
    towards ecological preservation. Ethics approval Not Applicable. Consent to participate
    All the authors approved to participate in this research. Consent for publication
    All the authors approved the publication of this research. Availability of data
    The authors do not have the permission to share the data. Grants and funding The
    authors did not receive any funds or grants from any organization. Author''s contribution
    Nilakshi Devi- experimental work, manuscript preparation, result generation and
    analysis; Kandarpa Kumar Sarma- conceptualization, editing of manuscript, supervision
    of experimental work and analysis; ShakuntalaLashkar- Supervision and analysis.
    Declaration of Competing Interest The authors declared that they have no competing
    interest. Acknowledgements The authors would like to thank Nayan Bean Farm, Sonapur,
    Guwahati, Assam for providing the bean leaf samples. Data availability The authors
    do not have permission to share data. References Akram et al., 2017 T. Akram,
    Sayeed Rameez, Mohammad Kmaran Towards real time crops surveillance for disease
    classification: exploiting parallelism in computer vision Comput. Electr. Eng.,
    59 (2017), pp. 15-26 View PDFView articleView in ScopusGoogle Scholar Akshay Pandey
    and Kamal Jain, 2021 Akshay Pandey, Kamal Jain A robust deep attention dense convolutional
    neural network for plant leaf disease identification and classification from smart
    phone captured real world images Ecological Informatics.vol.70 (2021) Google Scholar
    Almadhor and Rauf, 2021 A. Almadhor, H.T. Rauf AI-driven framework for recognition
    of guava plant diseases through machine learning from DSLR camera sensor based
    high resolution imagery Sensors., 21 (2021), p. 3830 CrossRefView in ScopusGoogle
    Scholar Amit et al., 2021 Prakash Amit, Sahu Priyanka, Singh Dinesh Deep learning
    models for beans crop diseases: classification and visualization techniques Int.
    J. Modern Agric., 10 (2021) Google Scholar Ashish, 2020 Kumar Ashish, et al. Res-VGG:
    A novel model for plant disease detection by fusing VGG16 and ResNet models International
    Conference on Machine Learning, Image Processing, Network Security and Data Science
    (2020), pp. 383-400 Google Scholar Asmita Hobisiyashi, 2022 Asmita Hobisiyashi
    ShivamYadav Cloud Based IoT controlled System Model for Plant Disease Monitoring.Predictive
    Analytics in Cloud, Fog and Edge Computing, Springer (2022) Google Scholar Bhagat
    et al., 2022 Sandesh Bhagat, et al. Eff-UNet++: a novel architecture for plant
    leaf segmentation and counting Ecol. Inform., 68 (2022) Google Scholar Binch and
    Fox, 2017 A. Binch, C.W. Fox Controlled comparison of machine vision algorithms
    for Rumex and Urtica detection in grassland Comput. Electron. Agric., 140 (2017),
    pp. 123-138 View PDFView articleView in ScopusGoogle Scholar CABI, 2022 CABI https://www.cabi.org/
    (2022) (Last checked on June 2022) Google Scholar Chen et al., 2020 Junde Chen,
    Defu Zhang, Jinxiu Chen Using deep transfer learning for image based plant disease
    identification Comput. Electron. Agric., 173 (2020) Google Scholar Chug et al.,
    2022 Anuradha Chug, Anshul Bhatia, Amit Prakash, Dinesh Singh A novel framework
    for image-based plant disease detection using hybrid deep learning approach J.
    Soft Comput. (2022), pp. 234-242 Springer Google Scholar Das et al., 2022 Amit
    Das, Himadri Saha, Amlan Chakrabarti Deep learning based automated disease detection
    and pests classification in Indian mung bean MultiMedia Tools and Applications,
    Springer (2022) Google Scholar Ferentinos, 2018 K.P. Ferentinos Deep learning
    models for plant disease detection and diagnosis Comput. Electron. Agric., 114
    (2018), pp. 311-318 View PDFView articleView in ScopusGoogle Scholar Geron, 2019
    Aurelin Geron Hands-on Machine Learning with Scikit-Learn, Keras and Tensor Flow
    O’ReillyPublisher (2019), pp. 234-345 Google Scholar Gokulnath and Usha, 2021
    B.V. Gokulnath, Devi Usha Identifying and classifying plant disease using resilient
    LF-CNN Ecol. Inform., 63, Elsevier (2021) Google Scholar Goodfellow et al., 2016
    Ian Goodfellow, Yoshua Bengio, Aaron Courville Deep Learning MIT Press (2016)
    Google Scholar Hapsari et al., 2022 Rinci Kembang Hapsari, Gan Hong Seng, Miswanto
    Miswanto Modified Gray Level Haralick Texture Features For Early Detection of
    Diabetes Mellitus and High Cholestrol in Iris Image vol. 2 (2022), p. 11 Google
    Scholar Hobisiyashi and Yadav, 2022 Asmita Hobisiyashi, Shivam Yadav Cloud based
    IoT controlled system model for plant disease monitoring Predictive Analytics
    in Cloud, Fog and Edge Computing, Springer (2022) Google Scholar Jha et al., 2019
    K. Jha, A. Doshi, P. Patel, M. Shah A comprehensive review on automation in agriculture
    using artificial intelligence Artif. Intelligence Agric., 2 (2019), pp. 1-12 June
    View PDFView articleView in ScopusGoogle Scholar Johann, 2016 Andre L. Johann,
    et al. Soil moisture modeling based on stochastic behavior of forces on a no-till
    chisel opener Comput. Electron. Agric., 121 (2016), pp. 420-428 View PDFView articleView
    in ScopusGoogle Scholar Johannes et al., 2017 A. Johannes, et al. Automatic plant
    disease diagnosis using mobile capture devices applied on a wheat use case Comput.
    Electron. Agric. (2017), pp. 200-209 View PDFView articleView in ScopusGoogle
    Scholar Kashefi et al., 2017 Javed Kashefi, et al. Novelty Detection Classifiers
    in Weed Mapping: Silybummarianum Detection on UAV Multispectral Images vol. 17
    (2017) Google Scholar Kaya et al., 2023 Yasin Kaya, et al. A novel multi-head
    CNN design to identify plant diseases using the fusion of RGB images Ecol. Inform.,
    75 (2023) Google Scholar Maione et al., 2018 Camila Maione, et al. Recent applications
    of multivariate data analysis methods in the authentication of rice and the most
    analyzed parameters: a review Taylor &Francis (2018), pp. 1868-1879 Online Google
    Scholar Manjula et al., 2022 Manjula, et al. Plant disease detection using deep
    learning Lecture Notes in Electrical Engineering, Springer (2022), pp. 1389-1396
    CrossRefView in ScopusGoogle Scholar Naik et al., 2022 Naik, et al. Detection
    and classification of chilli leaf disease using a squeeze-and-excitation-based
    CNN model Ecol. Inform., 69 (2022) Google Scholar Nicola and Pisana, 2021 Papini
    Nicola, Placidi Pisana Monitoring Soil and Ambient Parameters in IoT Precision
    Agriculture Sceneriao: An Original Modelling Approach Dedicated to Low Cost Water
    Content Sensors vol. 21 (2021), p. 5110 Google Scholar Olivia et al., 2022 Diego
    Olivia, et al. BLeafNet: a Bonferroni mean operator based fusion of CNN models
    for plant identification using leaf image classification Ecol. Inform., 69 (2022)
    Google Scholar Pandey, 2022 A. Pandey, et al. An intelligent system for crop identification
    and classification from UAV images using conjugated dense convolutional neural
    network Comput. Electron. Agric. (2022), p. 567 Google Scholar Pantazi et al.,
    2019 X.E. Pantazi, et al. Automated Leaf Disease Detection in Different Crop Species
    through Image Features Analysis and One Class Classifiers Computers and Electronics
    in Agriculture (2019) Google Scholar Picon, 2019 A. Picon, et al. Deep convolutional
    neural networks for mobile capture device-based crop disease classification in
    the wild Comput. Electron. Agric..pp.235 (2019) Google Scholar Rakesh et al.,
    2021 Chandra Joshi Rakesh, Kaushik Manoj, Kishore Dutta Malay, Srivastava Ashish,
    Choudhury Nandlal VirLeafNet: automatic analysis and viral disease diagnosis using
    deep learning in VignaMungo plant Ecol. Inform., 61 (2021) Elsevier Google Scholar
    Ramesh et al., 2018 Shima Ramesh, et al. Plant disease detection using machine
    learning International Conference on Design Innovation for 3Cs Compute Communicate
    Control (ICDI3C), IEEE (2018) Google Scholar Recep and Lahcen, 2022 Eryigit Recep,
    Elfatimi Lahcen Bean Leaf Disease Classification Using Mobile Net Models IEEE
    Access, 10 (2022), pp. 9471-9482 Google Scholar Rudagi et al., 2022 Jayashri Rudagi,
    et al. Plant leaf disease detection using computer vision and machine learning
    algorithms Glob. Trans. Proc., 3 (2022), pp. 305-310 Google Scholar Sahoo et al.,
    2020 Sahoo, et al. Maize leaf disease detection and classification using machine
    learning algorithms Progress in Computing, Analytics and Networking (2020), pp.
    659-669 Google Scholar Shah et al., 2022 Deshna Shah, et al. Image based plant
    disease detection Data Intelligence and Cognitive Informatics, Springer (2022),
    pp. 651-666 CrossRefGoogle Scholar Sibiya et al., 2019 Sibiya, et al. A computational
    procedure for the recognition and classification of maize leaf disease out of
    healthy leaves using convolution neural networks J. Agr. Eng. (2019), pp. 119-131
    CrossRefView in ScopusGoogle Scholar Singh et al., 2020 Singh, et al. PlantDoc:
    A dataset for visual plant disease detection Proceedings of the 7th ACM IKDD CoDS
    and 25th COMAD (2020), pp. 249-253 Google Scholar Sutaji and Yildiz, 2022 Deni
    Sutaji, Oktay Yildiz LEMOXINET: Lite ensemble MobileNetV2 and Xception models
    to predict plant disease Ecol. Inform., 70 (2022) Google Scholar Thi et al., 2022
    Hanh Bui Thi, et al. Enhancing the performance of transferred efficient net models
    in leaf image-based plant disease classification J. Plant Dis. Protect. (2022),
    pp. 623-634 Google Scholar Tiwari et al., 2021 Vaibhav Tiwari, Rakesh Chandra
    Joshi, Malay Kishore Dutta Dense convolutional neural network basedmulti class
    plant disease detection and classification using leaf images Ecol. Inform., 63
    (2021) Google Scholar Togacar, 2022 Mesut Togacar Using DarkNet models and metaheuristic
    optimization methods together to detect weeds growing along with seedlings Ecol.
    Inform., 68 (2022) Google Scholar Tool et al., 2019 E.C. Tool, et al. A comparative
    analysis of fine tuning deep learning models for plant disease identification
    Comput. Electron. Agric. (2019), pp. 272-279 Google Scholar Traore et al., 2019
    D. Traore, et al. Deep neural networks with transfer learning in millet crop images
    Comput. Ind., 108 (2019), pp. 115-120 Google Scholar Veeraballi, 2020 Veeraballi,
    et al. Deep learning based approach for classification and detection of papaya
    leaf diseases Adv. Intelligent Syst. Comput..pp.567 (2020) Google Scholar Vimal
    et al., 2023 Vimal, et al. Classification of beans leaf diseases using fine tuned
    CNN model Procedia Comput. Sci., 218 (2023) Google Scholar Wang et al., 2023a
    Wang, et al. Sweet potato leaf detection in a natural scene based on faster R-CNN
    with a visual attention mechanism and DIoU-NMS Ecol. Inform., 73 (2023) Google
    Scholar Wang et al., 2023b Wang, et al. A novel deep learning method for maize
    disease identification based on small sample-size and complex background datasets
    Ecol. Inform., 75 (2023) Google Scholar Xia, 2022 Limei Xia Automatic strawberry
    leaf scorch severity estimation via faster R-CNN and few-shot learning Ecol. Inform.,
    70 (2022) Google Scholar Xie et al., 2020 Xie, et al. A deep learning real time
    detector for grape leaf disease using improved convolutional neural networks Front.
    Plant Sci. (2020), pp. 1-14 Google Scholar Yadav et al., 2021 S. Yadav, et al.
    Identification of disease using deep learning and evaluation of bacteriosis in
    peach leaf Ecol. Inform., 61 (2021) Google Scholar Yang et al., 2019 Yang Yang,
    et al. Disease prediction model based on BiLSTM and attention mechanism IEEE International
    Conference on Bioinformatics and Biomedicine (BIBM), IEEE (2019) Google Scholar
    Zan, 2022 Wang Zan, et al. An improved faster R-CNN model for multi-object tomato
    maturity detection in complex scenarios Ecol. Inform., 72 (2022) Google Scholar
    Olfa, 2023 Olfa, et al. Deep learning-based segmentation for disease identification
    Ecol. Inform. (2023), p. 34 Available online on January 2023. Google Scholar Cited
    by (11) Image patch-based deep learning approach for crop and weed recognition
    2023, Ecological Informatics Show abstract Strawberry R-CNN: Recognition and counting
    model of strawberry based on improved faster R-CNN 2023, Ecological Informatics
    Show abstract Identification of suitable location to cultivate grape based on
    disease infestation using multi-criteria decision-making (MCDM) and remote sensing
    2023, Ecological Informatics Show abstract Sustainable integrated farming in agriculture
    2023, Water-Soil-Plant-Animal Nexus in the Era of Climate Change Machine Learning
    for Weather-Driven Energy Consumption Forecasting and Optimization in Moroccan
    Agricultural Greenhouses 2023, SSRN Monitoring and Sensing of Real-Time Data with
    Deep Learning Through Micro- and Macro-analysis in Hardware Support Packages 2023,
    SN Computer Science View all citing articles on Scopus View Abstract © 2023 Elsevier
    B.V. All rights reserved. Recommended articles Development of artificial intelligence
    based systems for prediction of hydration characteristics of wheat Computers and
    Electronics in Agriculture, Volume 128, 2016, pp. 34-45 S.M. Shafaei, …, S. Kamgar
    View PDF Weekly carbon dioxide exchange trend predictions in deciduous broadleaf
    forests from site-specific influencing variables Ecological Informatics, Volume
    75, 2023, Article 101996 David A. Wood View PDF Expansion risk of the toxic dinoflagellate
    Gymnodinium catenatum blooms in Chinese waters under climate change Ecological
    Informatics, Volume 75, 2023, Article 102042 Changyou Wang, …, Zhuhua Luo View
    PDF Show 3 more articles Article Metrics Citations Citation Indexes: 9 Captures
    Readers: 66 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: Nilakshi Devi a, Kandarpa Kumar Sarma b, Shakuntala Laskar a
  journal: Ecological Informatics
  key_findings: '1. The proposed framework can accurately detect and classify crop
    diseases in bean plants, with classification accuracies of 96% and 98% for the
    EfficientNetB7-BiLSTM and VGG16-attention models, respectively.

    2. The framework can effectively detect leaks and ensure uniform sprinkler distribution
    in irrigation systems.

    3. The framework is automated, which reduces the need for manual labor and improves
    efficiency.'
  limitations: '1. The proposed framework has only been evaluated on a limited dataset
    of bean crops. It is unclear how well the framework will generalize to other crops
    or growing conditions.

    2. The framework relies on high-resolution cameras, which can be expensive. This
    may limit the adoption of the framework in resource-constrained settings.

    3. The framework is complex and requires specialized expertise to implement and
    maintain. This may limit the adoption of the framework by small-scale farmers.'
  main_objective: Proposing and evaluating a novel AI-aided framework for precision
    agriculture in bean cultivation, combining sensor systems and deep learning models
    for automated visual monitoring of crop health, weed detection, and intelligent
    irrigation control.
  relevance_evaluation: 'The paper you are reviewing focuses on integrating high-resolution
    cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for
    visual monitoring of crop growth, disease detection (e.g., using deep learning-based
    object detection and segmentation), and irrigation system performance (e.g., leak
    detection, sprinkler uniformity).


    The specific point you are making in your literature review is about the importance
    of integrating high-resolution cameras with computer vision and deep learning
    for automated crop monitoring. The paper you are reviewing directly addresses
    this point by proposing and evaluating a novel framework for automated visual
    monitoring of bean crops using high-resolution cameras and deep learning algorithms.


    The paper provides a comprehensive evaluation of the proposed framework, demonstrating
    its effectiveness for disease detection and irrigation system performance monitoring.
    The results show that the proposed framework achieves high accuracy in disease
    detection and can effectively detect leaks and ensure uniform sprinkler distribution.
    These results suggest that the proposed framework has the potential to improve
    crop yield and reduce water usage in bean cultivation.


    Overall, the paper provides a valuable contribution to the field of automated
    crop monitoring and is highly relevant to the point you are making in your literature
    review.'
  relevance_score: '1.0'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, Computer vision algorithms, Deep learning,
    IoT sensors, Edge computing
  title: Design of an intelligent bean cultivation approach using computer vision,
    IoT and spatio-temporal deep learning structures
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Unknown Author. (Unknown Year). Title of paper. Journal or Conference
    Name, Volume(Issue), Page Range.
  authors:
  - Gobhinath S.
  - Devi Darshini M.
  - Durga K.
  - Hari Priyanga R.
  citation_count: '7'
  data_sources: Not specified in the provided text
  description: Automation has maneuvered industrial advancements and has become provident
    in most of the domain. But the contribution of automation to agriculture is in
    a lower degree. So, by incorporating automation in the field of agriculture, productivity
    can be increased to manifolds. This paper brings out the ways to automate agriculture
    which enhances irrigation, protection of farmlands and health management. Irrigation
    of crops is made economical by considering the moisture level in soil, temperature
    and humidity. Health supervision is done by an autonomous agricultural rover which
    moves around the field, collecting data through a camera fixed on it. The images
    are processed using algorithms in MATLAB to identify the disease affecting or
    nutrition deficit in the field. The shortcoming is indicated to the farmer. In
    case of fire accidents, it is detected using Ultraviolet Flame sensor and the
    fire is put off. Also, the farm is protected from animal intrusion using PIR sensor
    and buzzer. These techniques are integrated to improve the standards of agricultural
    farming.
  doi: 10.1109/ICACCS.2019.8728468
  explanation: The paper explores the integration of advanced monitoring techniques,
    including high-resolution cameras and computer vision algorithms, to enhance automated
    irrigation systems. It highlights the value of visual monitoring for crop growth
    analysis, disease detection using deep learning techniques, and irrigation system
    performance assessment, such as leak detection and sprinkler uniformity evaluation.
  extract_1: '"High-resolution cameras (e.g., multispectral, hyperspectral) and computer
    vision algorithms can be integrated into automated irrigation systems for visual
    monitoring of crop growth, disease detection, and irrigation system performance."'
  extract_2: '"Deep learning-based object detection and segmentation algorithms can
    be employed to detect and classify crop diseases, enabling early intervention
    and targeted treatment."'
  full_citation: '>'
  full_text: '>

    ""'
  inline_citation: (Unknown, Unknown)
  journal: 2019 5th International Conference on Advanced Computing and Communication
    Systems, ICACCS 2019
  key_findings: The paper demonstrates the effectiveness of integrating advanced monitoring
    techniques into automated irrigation systems for improved crop growth analysis,
    disease detection, and irrigation system performance evaluation.
  limitations: The paper mainly focuses on the integration and evaluation of advanced
    monitoring techniques within automated irrigation systems, and does not delve
    deeply into the broader context of interoperability and standardization for seamless
    integration across the entire irrigation management pipeline.
  main_objective: This paper aims to explore the integration of advanced monitoring
    techniques, including high-resolution cameras and computer vision algorithms,
    to enhance automated irrigation systems.
  relevance_evaluation: This paper is highly relevant to the point of integrating
    advanced monitoring techniques for automated irrigation systems. It provides insights
    into the use of high-resolution cameras and computer vision algorithms for visual
    monitoring of crop growth, disease detection, and irrigation system performance.
    These capabilities contribute to the overall efficiency and effectiveness of automated
    irrigation management systems, enabling more precise and data-driven decision-making.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, Computer vision algorithms, Deep learning-based
    object detection and segmentation
  title: Smart Irrigation with Field Protection and Crop Health Monitoring system
    using Autonomous Rover
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Neupane, K., & Baysal-Gurel, F. (2021). Automatic identification
    and monitoring of plant diseases using unmanned aerial vehicles: A review. Remote
    Sensing, 13(19), 3841.'
  authors:
  - Neupane K.
  - Baysal-Gurel F.
  citation_count: '68'
  description: Disease diagnosis is one of the major tasks for increasing food production
    in agriculture. Although precision agriculture (PA) takes less time and provides
    a more precise application of agricultural activities, the detection of disease
    using an Unmanned Aerial System (UAS) is a challenging task. Several Unmanned
    Aerial Vehicles (UAVs) and sensors have been used for this purpose. The UAVs’
    platforms and their peripherals have their own limitations in accurately diagnosing
    plant diseases. Several types of image processing software are available for vignetting
    and orthorectifica-tion. The training and validation of datasets are important
    characteristics of data analysis. Currently, different algorithms and architectures
    of machine learning models are used to classify and detect plant diseases. These
    models help in image segmentation and feature extractions to interpret results.
    Researchers also use the values of vegetative indices, such as Normalized Difference
    Vegetative Index (NDVI), Crop Water Stress Index (CWSI), etc., acquired from different
    multispectral and hyperspec-tral sensors to fit into the statistical models to
    deliver results. There are still various drifts in the automatic detection of
    plant diseases as imaging sensors are limited by their own spectral bandwidth,
    resolution, background noise of the image, etc. The future of crop health monitoring
    using UAVs should include a gimble consisting of multiple sensors, large datasets
    for training and validation, the development of site-specific irradiance systems,
    and so on. This review briefly highlights the advantages of automatic detection
    of plant diseases to the growers.
  doi: 10.3390/rs13193841
  explanation: 'In this research, the authors sought to determine the potential of
    integrated, end-to-end automated irrigation management systems for real-time monitoring
    and optimization of water use in agriculture by thoroughly reviewing the current
    state and future prospects of these systems. Their review focused on the integration
    of high-resolution cameras for monitoring crop growth, disease detection, and
    irrigation system performance, and on the use of data analytics and machine learning
    for processing and interpreting the collected data. Through this review, the authors
    aimed to identify challenges, propose solutions, and outline future research gaps
    in this field.


    Specific point of focus:

    - Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and
    computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity).


    Excerpt 1:

    "Integrating high-resolution cameras (e.g., multispectral, hyperspectral) and
    computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)"


    Excerpt 2:

    "Automatic identification and monitoring of plant diseases using unmanned aerial
    vehicles: A review"


    These excerpts are relevant to the outline point because they highlight the use
    of high-resolution cameras for visual monitoring of crop growth, disease detection,
    and irrigation system performance. The first excerpt specifically mentions the
    use of computer vision algorithms and deep learning for object detection and segmentation
    in disease detection, which is a key aspect of the outline point.'
  extract_1: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)
  extract_2: 'Automatic identification and monitoring of plant diseases using unmanned
    aerial vehicles: A review'
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all     Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Remote Sensing All Article Types Advanced   Journals
    Remote Sensing Volume 13 Issue 19 10.3390/rs13193841 Submit to this Journal Review
    for this Journal Propose a Special Issue Article Menu Academic Editor Michael
    Schirrmann Subscribe SciFeed Recommended Articles Related Info Link More by Authors
    Links Article Views 11131 Citations 65 Table of Contents Abstract Introduction
    Types of UAVs, Their Platforms and Peripherals Used in Disease Monitoring and
    Identification Cameras and Sensors Image Pre-Processing Data Processing Deep Learning
    Models Challenges of Automatic Plant Disease Identification Using UAVs Future
    Considerations Conclusions Author Contributions Funding Institutional Review Board
    Statement Informed Consent Statement Conflicts of Interest References Altmetric
    share Share announcement Help format_quote Cite question_answer Discuss in SciProfiles
    thumb_up Endorse textsms Comment first_page settings Order Article Reprints Open
    AccessReview Automatic Identification and Monitoring of Plant Diseases Using Unmanned
    Aerial Vehicles: A Review by Krishna Neupane and Fulya Baysal-Gurel * Otis L.
    Floyd Nursery Research Center, Department of Agricultural and Environmental Sciences,
    Tennessee State University, McMinnville, TN 37110, USA * Author to whom correspondence
    should be addressed. Remote Sens. 2021, 13(19), 3841; https://doi.org/10.3390/rs13193841
    Submission received: 21 July 2021 / Revised: 17 September 2021 / Accepted: 21
    September 2021 / Published: 25 September 2021 (This article belongs to the Special
    Issue UAV Imagery for Precision Agriculture) Download keyboard_arrow_down     Browse
    Figure Versions Notes Abstract Disease diagnosis is one of the major tasks for
    increasing food production in agriculture. Although precision agriculture (PA)
    takes less time and provides a more precise application of agricultural activities,
    the detection of disease using an Unmanned Aerial System (UAS) is a challenging
    task. Several Unmanned Aerial Vehicles (UAVs) and sensors have been used for this
    purpose. The UAVs’ platforms and their peripherals have their own limitations
    in accurately diagnosing plant diseases. Several types of image processing software
    are available for vignetting and orthorectification. The training and validation
    of datasets are important characteristics of data analysis. Currently, different
    algorithms and architectures of machine learning models are used to classify and
    detect plant diseases. These models help in image segmentation and feature extractions
    to interpret results. Researchers also use the values of vegetative indices, such
    as Normalized Difference Vegetative Index (NDVI), Crop Water Stress Index (CWSI),
    etc., acquired from different multispectral and hyperspectral sensors to fit into
    the statistical models to deliver results. There are still various drifts in the
    automatic detection of plant diseases as imaging sensors are limited by their
    own spectral bandwidth, resolution, background noise of the image, etc. The future
    of crop health monitoring using UAVs should include a gimble consisting of multiple
    sensors, large datasets for training and validation, the development of site-specific
    irradiance systems, and so on. This review briefly highlights the advantages of
    automatic detection of plant diseases to the growers. Keywords: UAS; UAVs; plant
    disease detection; plant monitoring; convolutional neural networks (CNNs); deep
    learning; machine learning 1. Introduction Simply, UAVs are any aerial vehicles
    that are remotely driven, meaning no pilot on board. They are considered one of
    the important innovations of present-day precision agriculture [1,2,3,4,5,6,7,8].
    Precision agriculture (PA) is a method to transform agriculture by reducing time
    and labor and increasing production and management efficiency [9]. With the development
    in technology and computational skills, there have been changes in agricultural
    patterns such as using digital planters, harvesters, sprayers, etc. Agriculture
    has transformed over time from being carried out by manual labor to mechanical
    labor due to the adoption of technological changes. Previously, plant diseases
    in agriculture fields were monitored visually by people who have experience scouting
    and monitoring plant diseases. This type of observation is psychological and subject
    to bias, optical illusion, and error [1]. This generated the need for external
    image-based tools that can replace unreliable human observations. This also allows
    for extended coverage in a limited amount of time. The potential of UAVs for conducting
    detailed surveys in precision agriculture has been demonstrated for a range of
    applications such as crop monitoring [10,11], field mapping [12], biomass estimation
    [13,14], weed management [15,16], plant population counting [17,18,19], and spraying
    [20]. A large amount of data and information is collected by UAVs to improve agricultural
    practices [21]. Different kinds of data recording instruments, cameras, and sensor
    installation equipment have been developed for agricultural purposes [4]. Some
    additional reasons for increasing usage of UAVs and drones in agriculture [22]
    include UAV prices decreasing gradually [2,23], agriculture operations being carried
    out in areas with low population and activities [1], and UAVs having a large occupancy
    and great scouting capability. Although the images can be obtained from various
    sources, such as satellites and aircrafts [4], and can cover a large area compared
    to those of an unmanned aerial system, the resolution in the images is not conducive
    to drawing significant conclusions. This leads to other advantages of using drones
    for high-resolution aerial images [1,24]. Increased efficiency, stability, accuracy,
    and productivity are other advantages of UAVs [25] that allow growers and experts
    to make better, more timely management decisions [26,27]. The application of PA
    technology not only increases economic profit but social benefits measured in
    sustainability. A study carried out by Van Evert et al. [28] reported that the
    application of PA in potato cultivation increased economic profit by 21% and social
    profit by 26% compared to agricultural practices without precision agriculture.
    The use of agriculture UAVs is hindered by many challenges such as battery efficiency
    [25,29], low flight time [7], communication distance, and payload [25,30]. The
    limited flight duration due to increased payload and decreased efficiency of batteries
    obstructs when performing critical agricultural activities in larger fields, such
    as pesticide and nutrient application. Other challenges associated with its use
    are engine power, stability, maintaining altitude, and maneuverability in wind
    and turbulence [7,31]. UAVs have been used for a variety of reasons in agriculture.
    Traditionally, visual observations would determine crop nutrient status, their
    pests, diseases, and environmental stress [32]. Currently, most UAVs are used
    for detecting stress in plants, for quantifying biomass, vegetation classification,
    canopy cover estimation, yield prediction, plant height assessment, and lodging
    assessment. Agriculture UAVs have been used for mapping agriculture fields, spraying
    chemicals, planting, crop monitoring, irrigation, diagnosis of insects and pests,
    artificial pollination [25,33], and livestock population dynamics [34]. A combination
    of UAVs with hyper and multispectral cameras has been predominant in a wide range
    of agricultural operations [33] for disease identification purposes. Recently,
    Chang, Zhou, Kira, Marri, Skovira, Gu, and Sun [32] used UAVs to measure solar-induced
    chlorophyll fluorescence and photo-chemical reflectance through single bifurcated
    fiber and a motorized arm to measure radiance. Artificial Intelligence (AI) and
    deep learning are incorporated with UAVs to increase the precision of crop disease
    identification and monitoring. Penn State University, Food and Agriculture Organization,
    International Institute of Tropical Agriculture, International Maize and Wheat
    Improvement Center and others have developed PlantVillage Nuru to identify viral
    diseases in cassava plants [35]. Nowadays, AI and DL are collaborated to ease
    the process of plant disease detection. PlantVillage Nuru has been integrated
    with the West African viral epidemiology platform (WAVE 2) to track the spread
    of cassava brown streak disease. Out of these applications, the most extensive
    use of UAVs in agriculture has been for the detection of stress in plants and
    quantification. This might be because of the impact that early detection renders
    in overall agriculture activity [1]. Early detection of the plant’s biotic and
    abiotic stresses helps to understand the changing physiology of the plant. If
    the stresses are detected early, suitable treatments can be used to protect plants
    and reduce loss. This review paper intends to cover the basic areas of the UAVs
    in the automatic detection of plant diseases, their components, image and data
    processing, and different models of data analysis. The major objective of this
    paper is to provide a holistic explanation of how UAVs can be used to automatically
    monitor the health status of the crop in the field. It provides insight into basic
    concepts of UAV peripherals, sensors, and cameras along with their limitations
    and applicability. In that regard, it further explains the performance, advantages
    and limitations of different deep learning models. Nonetheless, in situ detection
    of plant diseases using UAVs is expanding with many challenges. This paper tries
    to explore different UAV platforms and their limitations and advantages, cameras,
    and sensors with their spectral specifications to capture images and acquire data
    for monitoring and detecting plant diseases. It also reflects different methods
    of processing acquired data, the challenges in the process and the prospects of
    autonomous identification of plant diseases using Unmanned Aerial Vehicles. 2.
    Types of UAVs, Their Platforms and Peripherals Used in Disease Monitoring and
    Identification Nowadays, as discussed earlier, different agricultural activities
    have been carried out by UAVs. Among them, UAVs are increasingly used for disease
    monitoring and identification. They work together with different components such
    as cameras, sensors, motors, rotors, controllers, etc. One of the basic uses of
    UAV is to capture images. The information contained in images is extracted and
    transformed into useful information by image processing and deep learning tools
    [1,26]. Electromagnetic spectra also provide useful information, which is used
    to make decisions regarding plant physiological stress [33,36]. The comparison
    between the spectroscopy in the specific region helps to assess the condition
    of the plants in real-time under field conditions [33]. Plant disease is identified
    by observing the physiological disturbances caused by foliar reflectance in a
    near-infrared portion of the spectrum in a UAV captured image [36]. In addition,
    the disturbances in the photosynthetic activities of the crops caused by many
    diseases are also observed as reflectance in the red wavelength range. There are
    various types of UAVs for different agricultural operation purposes. In relation
    to UAVs, these UAV structures are called platforms [4]. Primarily, there are two
    types of agricultural UAV platforms: fixed wing and rotatory wing [25]. A fixed-wing
    UAV is comparatively larger in size and used for large-area coverage [4,25,37,38].
    Rotary-wing UAVs are further divided into two types: helicopter and multirotor.
    Helicopters have a large propeller at the top of the aircraft and are used for
    aerial photography and spraying. Similarly, multirotors have different varieties
    depending upon the number of motors the aircraft possesses. Different types of
    multirotors are the quadcopter (four rotors) [39,40,41], hexacopter (six rotors)
    [42], and octocopter (eight rotors) [43]. For the purpose of plant disease monitoring
    and identification, both fixed-wing and rotary UAVs have been used. To monitor
    leaf stripe disease of grapevine, one of the diseases of the esca complex, Di
    Gennaro and his colleagues used a modified multirotor Mikrocopter OktoXL (HiSystems
    GmbH, Moomerland, Germany) [36]. Similarly, target spot and powdery mildew in
    soybean have been identified using a popular quadcopter, Phantom-3 (Shenzhen DJI
    Sciences and Technologies Ltd., Shenzhen, China) [26]. Xavier and colleagues (2019)
    used a multirotor UAV to identify Rumaria leaf blight disease in cotton [44].
    A hexacopter, DJI Matrice 600 pro (Shenzhen DJI Sciences and Technologies Ltd.,
    Shenzhen, China), was used to detect target spot and bacterial spot in tomato
    leaves [45]. In 2013, Garcia-Ruiz and colleagues used fixed-wing multirotor UAVs
    to compare the aerial imaging platforms for identifying citrus greening disease
    in Florida [33]. Gomez Selvaraj et al. [46] used a multicopter, UAV Phantom 4
    Pro (Shenzhen DJI Sciences and Technologies Ltd., Shenzhen, China), to identify
    major diseases in bananas. An Altura X8 octocopter (Aerialtonics DV B.V., Katwijk,
    The Netherlands) was used in monitoring fire blight in pear orchards in Belgium
    [47]. In a similar fashion, an octocopter (DJI S1000) (Shenzhen DJI Sciences and
    Technologies Ltd., Shenzhen, China) was used in spatio-temporal monitoring of
    yellow rust in wheat in China [48]. A helicopter was modified to carry a camera
    system, an autopilot, and sensors to obtain thermal imagery and multispectral
    imagery over the agricultural fields [49]. A package of sensors was built into
    the DJI S800 hexacopter (Shenzhen DJI Sciences and Technologies Ltd., Shenzhen,
    China) to identify citrus greening disease [50]. Özgüven [51] used a rotary UAV–DJI
    Phantom 3 Advanced Brand (Shenzhen DJI Sciences and Technologies Ltd., Shenzhen,
    China) to determine Cercospora leaf spot in sugar beet. Valasek et al. [52] used
    rotary UAV to distinguish leaf spot (Cercosporidium personatum) among healthy
    and diseased peanuts. Sugiura et al. [53] used a multirotor UAV to determine late
    blight resistant cultivars of potato. As we can see, both fixed and rotary UAVs
    have been used for crop monitoring purposes. The choice of the platform simply
    depends upon the area to be covered by the UAVs, payload, and nature of the study.
    For example, fixed-wing UAVs can cover a larger area than rotary-wing UAVs in
    the same amount of time. Additionally, fixed-winged UAVs have faster flight speed
    and can take comparatively bigger payloads than rotary UAVs. Rotary-winged UAVs
    are small and confined to easily access the congested area. Rotary UAVs are mostly
    used for aerial photography and videography, whereas fixed-winged UAVs are best
    suited for aerial mapping. Because of their shape and size, fixed-wing UAVs cannot
    hover in place and need definite spots to launch and land. Moreover, multirotors
    are cost-effective in terms of total area coverage and easy to handle as compared
    to fixed-winged rotors. Despite their larger size and difficulty in maintaining
    control, larger and fixed-winged UAVs can be effectively used in large fields
    because of their cost-efficiency in the total field coverage per flight duration
    [54]. 3. Cameras and Sensors Aerial imaging is one of the most important factors
    when it comes to the application of UAVs. Usually, the image quality determines
    the selection of UAVs. However, the type of sensor and the purpose of the study
    also dictate the choice of platform. Thus, the UAV platforms are loaded with different
    cameras and sensors. For example, a DJI Mavic 2 UAV is upgraded with a Sentera
    single NDVI camera (Sentera, Saint Paul, MN, USA) to monitor the drought stress
    in crapemyrtle plants (Figure 1). However, UAV platforms are limited to their
    payload. An increase in payload decreases the speed, stability, and flight time
    of the UAV [4,23]. The choice of sensors depends upon the purpose of the study.
    Drought stress is better observed using thermal sensors in the early stages [55,56],
    while multispectral and hyperspectral sensors are used for long-term results.
    Pathogen infections in crops are better diagnosed using hyperspectral and thermal
    sensors in the early stages, but RGB sensors, multispectral, and hyperspectral
    can be used to detect the severity of infection in later stages [57]. In this
    subsection, we will discuss different cameras and sensors used in plant disease
    monitoring and observation. Figure 1. DJI Mavic air UAV with single NDVI camera
    over the crapemyrtle (Lagerstroemia spp.) field at Tennessee State University,
    Otis L. Floyd Nursery Research Center, McMinnville, TN for automatic detection
    of drought stress in the plants. 3.1. RGB Camera RGB (Red–green–blue) cameras
    are commonly used types of cameras that produce images that measure the intensity
    of three colors and define values of each color in pixels: red, green, and blue.
    RGB cameras are used to generate 3-dimensional (3D) models of agricultural crops
    [58,59,60] and provide an estimation of crop biomass [61,62,63]. RGB cameras are
    also used with NIR and multispectral cameras to improve accuracy while calculating
    the biomass [62]. If the near-infrared filter is replaced by a red filter, it
    is called a modified RGB camera [64,65]. Commercial RGB cameras are cheap and
    have poor spectral resolution [65]. However, Grüner, Astor, and Wachendorf [61]
    calculated the biomass of grassland with an RGB camera using only SfM processing.
    RGB cameras have also been successfully used to identify diseases in plants. It
    is worth mentioning that RGB cameras only have the electromagnetic spectrum range
    of 380 nm to 750 nm, and not all of these wavelengths are suitable for appropriate
    crop disease detection [66]. The optical properties and the spectral range of
    the camera are considerable factors in plant disease detection. Mattupalli and
    his colleagues confirmed Phymatotrichopsis root rot disease in an alfalfa field
    using RGB imaging with a maximum likelihood classification algorithm [67]. RGB
    images were used to detect banana bunchy top virus and banana Xanthomonas wilt
    diseases in the African landscape of the Congo [46]. Similarly, an RGB camera
    with 12 megapixels was used to classify leaf spot (Cercospora beticola Sacc.)
    severity in sugarbeets in Turkey [51]. Grapevines were classified as diseased
    or healthy in France using RGB sensors [68]. Valasek, Thomasson, Balota, and Oakes
    [52] classified the leaf caused by Cercosporidium personatum spot of peanut using
    RGB cameras. Potato late-blight-resistant cultivars are screened using RGB cameras
    [53]. In a study conducted by Ashourloo et al. [69], wheat leaf rust (caused by
    Puccinia triticina) was detected by using RGB digital images with varying reflectance
    at 605, 695 and 455 nm wavelengths. RGB images also provides information on LAB
    (L = lightness, A and B are color opponent dimensions), YCBCR (Y = luma component,
    CB = blue difference and CR = red difference chroma components), HSV (hue, saturation,
    value), etc., which are very helpful in recognizing plant diseases. RGB cameras
    are readily available and are typically less expensive. They can also be used
    to collect high-resolution still images. However, they can only measure three
    bands (red, green and blue) of the electromagnetic spectrum. This causes RGB images
    to be less accurate than multispectral or hyperspectral images in terms of the
    spectral resolution of the camera system. Images from RGB cameras do not provide
    sufficient information to differentiate levels of sheath blight in rice [70].
    However, one of the major advantages of RGB cameras is their ability to capture
    high spatial resolution images in comparison to multispectral systems and, in
    turn, provide finer spatial details for plant disease detection and monitoring.
    RGB cameras should be carefully operated in order to have uniform coloring and
    lighting in the images. Uniform images will reflect fewer errors in differentiating
    healthy and diseased plants. 3.2. Multispectral Cameras Multispectral cameras
    are considered to be one of the most appropriate sensors for agricultural analytics,
    as they have the capacity to capture images in high spatial resolution and determine
    reflectance in near infrared bands [71]. Multispectral and NIR cameras create
    vegetative indices that rely on near-infrared or other bands of light [72,73,74].
    Multispectral cameras use different spectral bands: mostly red, blue, green, red-edge,
    and near-infrared. They can be differentiated into two groups based on bandwidth:
    narrowband and broadband [75]. Most of the aerial images for monitoring crop health
    issues use multispectral cameras [4] as they are used to calculate indices such
    as NDVI and others including NIR [63,71,73,76,77]. The absence of multispectral
    sensors in agricultural UAVs will hinder the early detection of plant diseases.
    The evaluation of multispectral image bands captured in an aerial image at different
    heights was carried out in 2019 by Xavier and colleagues to detect Ramularia leaf
    blight in cotton. However, the differentiation in the severity index was not significant
    [44]. In a study conducted by Abdulridha, Ampatzidis, Kakarla, and Roberts [45],
    35 vegetative indices were used to detect target spot and bacterial spot in tomatoes
    under laboratory and field conditions. A fixed-wing UAV capable of capturing hyperspectral
    images equipped with multiband sensors was used in 2012 in Florida to compare
    aerial imaging platforms in identifying citrus greening disease [33]. In the same
    study, both the multispectral and hyperspectral cameras were installed in the
    UAV. The multispectral camera used was six narrow-band cameras (miniMCA6, Tetracam,
    Inc., Chatsworth, CA, USA), having six digital sensors with customizable bands
    of 10 nm and arranged in a 3 × 2 array. Similarly, the hyperspectral camera used
    in the study was the AISA EAGLE Very Near Infra-red (VNIR) hyperspectral imaging
    sensor (Specim Ltd., Oulu, Finland). This imaging sensor has a spectral range
    of 398–998 nm and a resolution of around 5 nm. The same camera had 128 spectral
    bands for the VNIR region [33]. A combination of RGB images and multispectral
    images captured aerially using UAVs were used for pixel-based classification of
    banana diseases in the Democratic Republic of Congo. The camera system, Micasense
    RedEdge (MicaSense, Inc., Seattle, WA, USA), was capable of acquiring a 16-bit
    raw image in five narrow bands. Su, Liu, Hu, Xu, Guo, and Chen [48] used a multispectral
    camera—RedEdge—having a resolution of 1280 × 960 pixels and five narrow bands.
    A six-band multispectral camera (MCA-6, Tetracam, Inc., Chatsworth, CA, USA) was
    used, having an image resolution of 1280 × 1024 pixels, 10-bit radiometric, and
    optical focal length of 8.5 mm [49]. An RGB-Depth (RGB-D) camera, employed with
    two grayscale cameras (mvBlueFOX-MLC202bG), covering light sources with polarizing
    films and the multispectral sensors mounted to UAV platform, was used to monitor
    orange orchards for detecting citrus greening disease in Florida [50]. Al-Saddik
    et al. [78] used multispectral sensors to differentiate Flavescence dorée diseased
    and healthy grapevines in a vineyard without using common vegetative indices.
    Several other studies also used multispectral cameras [36,48,70,79,80,81,82,83].
    A combination of visible and infrared images was used to form a multispectral
    approach for the identification of vine diseases [84]. The platform was combined
    using an RGB camera and an infrared light sensor with a wavelength of 850 nm;
    both cameras had 16-megapixel high-resolution properties. In the study, the accuracy
    was varied between 70% to 90% depending upon the surface area, with a larger surface
    having higher accuarcy. Similarly, a multispctral sensor, OptRxTM -Ag Leader,
    was used at visual range of 670 nm, RedEdge 730 nm, and NIR 775 nm to identify
    the esca complex and Flavescence dorée in the vineyard [85]. A multispectral camera
    consisiting spectra of blue (475 nm), green (560 nm), red egde (720 nm), and near
    red (840 nm) was used to identify pine wood nematode caused by Bursaphelenchus
    xylophilus [86]. The accuracy observed in the study was 79%. Ye et al. [87] used
    a five-band multispectral camera having blue (465–485 nm), green (550–570 nm),
    red (653–673 nm), red edge (712–722 nm), and near infrared (800–880 nm) spectral
    ranges to classify banana wilt disease and obtained an accuracy of 80%. Multispectral
    sensors have high practicability for the new innovations of the automatic identification
    of plants. They can capture images in both visible and near infrared regions,
    but they may be limited while detecting subtle changes in the biophysical and
    biochemical parameters. Various studies have reported that multispectral cameras
    are best suited to identify plant diseases and pests in the field. The principle
    behind high accuracy is multiple bands of the electromagnetic spectrum. They not
    only provide additional information to the acquired images but can also provide
    vegetative indices. Vegetative indices are one of the most important factors in
    the identification of crop diseases. There are a few disadvantages of multispectral
    cameras, which include expensiveness and increased effort for calibration for
    specific tasks, such as disease identification, image processing, etc. 3.3. Hyperspectral
    Cameras The major difference between multispectral and hyperspectral cameras is
    that hyperspectral cameras collect light of different narrow size bands for every
    pixel in the image captured [72,88]. Though multispectral cameras are able to
    capture light reflected by biomolecules, the differences lie in the bandwidth
    and placement of the light, which helps us to isolate responses from the different
    molecules. These cameras have particular properties in detecting lights emitted
    from biomolecules such as chlorophyll [89,90], mesophyll [88], xanthophyll [91],
    and carotenoids [89,90]. The major drawback of using a hyperspectral camera is
    the high cost of cameras [72,92] and the huge amount of unnecessary data when
    not properly calibrated [88,93,94,95]. Abdulridha and colleagues [45] used a hyperspectral
    imaging system (line-scan imager system), Pika L 2.4 (Resonon Inc., Bozeman, MT,
    USA) with a 23 mm lens, that had a spectral range of 380–1020 nm and 281 spectral
    channels to detect diseases in tomato leaves. Fire blight in pears was monitored
    using a hyperspectral camera (frame-based system) (COSI-cam, VITO NV, Boeretang,
    Belgium) with a spectral range of 600–900 nm [47,96]. Images were captured in
    rapid succession of 340 frames/s in an 8-bit mode [47,97]. The overall accuracy
    was found to be 52% for the detection of healthy and infected trees. However,
    red wavelength (611 nm) and NIR (784 nm) were found to have an accuracy of 85%
    in distinguishing healthy and diseased trees [47]. Calderón et al. [98] and Sandino
    et al. [99] both used line-scan imagers hyperspectral cameras in their experiments
    to monitor crop health. The thermal, multispectral, and hyperspectral cameras
    were able to successfully detect the crop crown temperature, structural indices,
    fluorescence, and health index of the olive tree [100]. Similarly, in the study
    conducted by Sandino, Pegg, Gonzalez, and Smith [99], the identification of diseased
    trees was achieved at 97% accuracy and for healthy trees were at 95% accuracy,
    whereas the global multiclass detection rate was 97%. The main reason why hyperspectral
    cameras are used is to reduce the shortcomings of multispectral cameras. Hyperspectral
    cameras are used to capture details in fewer spectral differences and to identify
    and discriminate target objects. The major breakthrough that hyperspectral cameras
    have provided is that, unlike other cameras [101], they are able to detect plant
    stress with possible causative agents (pathogen/disease). Hyperspectral sensors
    basically measure several hundred bands of the electromagnetic spectrum to derive
    accurate results. They are able to measure visible spectrum (400–700 nm), near
    infrared (700–1000 nm), and also short-wave infrared (1000–2500 nm). As a result,
    they not only collect images but a huge amount of spectral data as well, which
    causes difficulties in extracting relevant information. 3.4. Thermal Cameras Thermal
    cameras capture infra-red lights in the range of 0.75 to 1000 µm and provide the
    temperatures of the objects in the form of a thermal image [102]. The advantages
    of thermal cameras are the low cost as compared to other spectral cameras and
    that RGB cameras can be converted to thermal cameras with certain modifications
    [103]. Originally, thermal cameras were used for inspecting drought stress in
    crops [92,103,104,105]. Thermal images include the temperature of the surrounding
    objects and have low resolution compared to images captured by other major cameras
    [102]. Thermal sensors are also used in detecting crop diseases and monitoring
    crops [80,98,106]. Thermal cameras are capable of identifying responsible agents
    for plant stress. As the pathogen infects, the structure and metabolism of the
    plant change, which can be detected by thermal sensors [107,108]. Baranowski et
    al. [109] reported that fungal stress caused by Alternaria in oilseed rape was
    identified using thermal imaging. Similarly, early detection of red leaf blotch
    on almonds is also conducted using hyperspectral and thermal imagery [110]. Many
    other studies carried out using thermal cameras are detection of Huanglongbing
    disease of citrus [111], tobacco mosaic virus [112], tomato powdery mildew [110],
    Fusarium wilt of cucumber [113], etc. A thermal FLIR camera was used to detect
    disease-induced spots on banana leaves with an accuracy of 92.8% [114]. Similar
    technology was used by Raza, Prince, Clarkson, and Rajpoot [108] to detect tomato
    powdery mildew caused by Oidium neolucopersici. This shows that the use of thermal
    cameras is increasing in crop health monitoring and disease identification. However,
    thermal imaging technology has not been fully explored. This may be because of
    environmental factors considered during image acquisition. Additionally, they
    contain huge amounts of information along with the image, which leads to challenges
    in deriving relevant information. The advantages of thermal imaging are low cost
    and early identification with causative agents. Yang et al. [115] developed a
    method for early detection of diseases in tea using thermal imagery. 3.5. Depth
    Sensors Depth sensors are common peripherals used in agricultural UAVs that provide
    extra feature-depth in the RGB pixels. The depth in a depth sensor is defined
    as the distance between the sensor and the point of an object at the time of image
    capture [116]. The most prevalent depth sensor technology is Light Detection and
    Ranging (LiDAR). The major difference between RGB-D sensors and LiDAR is that
    RGB-D sensors depend upon the light reflection intensities, whereas LiDAR uses
    laser pulses to calculate distance [23]. However, RGB-D sensors are one of the
    least commonly used sensors in the case of plant health monitoring but are commonly
    used in spraying [4], 3D modelling [117,118], and phenotyping [116]. Nowadays,
    depth sensors are used to increase the accuracy of the sensors. Sarkar et al.
    [50] used RGB-D cameras for detecting citrus greening disease. Similarly, Xia
    et al. [119] used depth sensors to create 3D segmentation of the individual leaf.
    There are many sensors available in the market to provide accurate 3D information,
    such as LiDAR and time of flight (ToF), but these are highly expensive and cannot
    be used for general agriculture practices. The use of depth cameras provides both
    the pixel intensity and depth in the captured image to develop a classifier during
    disease identification. Moreover, depth sensors provide data on the intensity
    of the light reflected from stressed objects or plants [120]. Paulus et al. [121]
    used 3-D laser technology to detect Fusarium spp. in kernels of wheat. The challenge
    of using depth sensors is that the sensors may not detect objects after a certain
    distance, which may result in a lower intensity count. This can be eliminated
    by using a configurable camera and calibrating it with depth sensors. 4. Image
    Pre-Processing Image pre-processing involves a series of steps before extracting
    data from the images. The major objective of image pre-processing is to reduce
    errors and to prepare to extract data from the image. After the high spatial geo-referenced
    aerial images are captured using UAVs, a large amount of data has to be extracted.
    Thus, it is very important that the images are error-free. The images may have
    been degraded while capturing due to noise, shadow, etc. It is very important
    to have critical knowledge of plant diseases for pre-processing the images and
    choosing an appropriate method to increase the accuracy of identification [122].
    Image pre-processing includes a series of steps to make it appropriate to extract
    data from the images. These include enhancement of images, their segmentation,
    color space conversion, and filters [123]. However, Sonka et al. [124] categorize
    the steps of image pre-processing as pixel brightness transformation, geometric
    transformation, local pre-processing, and image restoration. Pixel brightness
    transformation modifies the brightness of the image depending on the pixel of
    the image, for which they have two classes: brightness correction and grey scale
    transformation. Similarly, geometric transformation correlates the coordinates
    of the input image pixel with the points in the output image and determines the
    brightness of the transformed point in the digital raster. Local pre-processing
    utilizes the neighborhood of the pixel to generate the new brightness value in
    the output image. Finally, image restoration is the method in pre-processing that
    is used to discard degradation factors, such as lens defects, wrong focus, etc.,
    in the image. After image pre-processing, data from the images are extracted to
    use for data processing. The accuracy of the image classification depends on the
    type of image pre-processing and extraction techniques used. Studies have reported
    that image processing helps to improve the information in the image and can be
    more easily interpreted than non-processed images [70]. The images are generally
    pre-processed using different available software to orthorectify spectral bands
    in reflectance. There are varieties of software available to process raw images
    depending upon the sensors and cameras used. Ghosal et al. [125] used an unsupervised
    explanation framework to isolate symptoms during image pre-processing. In the
    study conducted by Ferentinos, a strange result was found while detecting diseases
    using open dataset images. The overall accuracy of real images was higher than
    the pre-preprocessed images [126]. Headwall SpectralView® (Headwall Photonics
    Inc., Bolton, MA, USA) and CSIRO|Data61 Scyven 1.3.0. (Scyllarus Inc., Canberra,
    Australia) software was used for isolating regions of interest and reflectance
    data by Sandina and colleagues [99,127,128]. The Simple Linear Iterative Clustering
    (SLIC) super pixels method was used by Tetila, Machado, Menezes, Da Silva Oliveira,
    Alvarez, Amorim, De Souza Belete, Da Silva, and Pistori [27] to segment leaves
    from the UAV captured images. Environment for Visualizing Images (ENVI) software
    (version 4.7, ITT VSI, White Plains, NY, USA) was used to reflect bands in the
    images from UAVs by Garcia-Ruiz, Sankaran, Maja, Lee, Rasmussen, and Ehsani [33].
    PixelWrench 2 software (Tetracam Inc., Chatsworth, CA, USA) was used for correcting
    radiometric distortion and vignetting images in identifying Ramularia leaf blight
    [44] and grape leaf stripe diseases [36]. Precision Hawk (Precision Hawk USA Inc.,
    Raleigh, NC, USA) provides an image pre-processing facility based on its cloud
    server [6]. Image processing for detection of cotton root rot was carried out
    using Pix4D software (Pix4D SA, Lausanne, Switzerland). The selection of the image
    processing features depends upon the purpose of the study. For example, if the
    diseased leaf contains an unwanted object, it can simply be removed by cropping
    to obtain the target image. This process is called image clipping, a part of image
    pre-processing. Similarly, the image enhancement can be performed using the histogram
    equalization technique to equally distribute the color intensities in the diseased
    plant images. Additionally, if the image contains shadow and has low contrast,
    this can be enhanced by removing blur [129]. Image pre-processing helps to identify
    target regions in the images and also helps to reduce noise in the images. It
    increases the reliability of the optical inspection. Images captured from the
    multispectral and hyperspectral sensors require atmospheric and radiometric calibration
    and correction of data in order to ensure consistency in the data from the image.
    5. Data Processing Data processing of the UAV images is carried out using various
    tools. Most of the researchers use vegetation indices for data processing because
    they are easy and readable. Currently, different Artificial Neural Networks (ANNs)
    are used for result demonstrations. In most of the studies, researchers also use
    numerical values from the sensors and use different statistical tools such as
    K-means clustering, Receiver Operator Characteristics (ROC) analysis, and regressions.
    However, it also depends upon the purpose of the study for the selection of data
    processing tools. Some of the data processing tools are discussed below. 5.1.
    Image Data Processing Data processing in UAVs is conducted in several ways. After
    the vignetting and orthorectification of the images, some of the results are derived
    using vegetation indices (VIs) only. However, imagery data analysis is more than
    that. It also includes image segmentation and result interpretation in the form
    of images. For those types of image data processing, various tools are used. Some
    of these tools are Artificial Neural Networks (ANNs), Decision Trees, K-means,
    k nearest neighbors, Support Vector Machines (SVMs), and Regression Analysis.
    In this review, a short description of k-means clustering and regression analysis
    is given. 5.1.1. K-Means Clustering Altas and his colleagues converted RGB images
    into L*a*b color space to identify Cercospora leaf spot in sugar beet, where a*b
    are the components in which the information about diseases in the leaves is stored.
    The colors in a*b are classified using K-means clustering [51]. K-means clustering
    is a common clustering algorithm used in various application domains, such as
    image segmentation [130], which divides a dataset into k groups [131]. In k-means
    clustering, an initial k cluster center is defined, and then the algorithm repeatedly
    selects other k values within that dataset. In the datasets for which the value
    of k is already known (such as Unique client identifiers, e.g., customer IDs),
    the same k value is used. For those where the k value is not known, the k value
    should be identified separately [132]. For the study to determine leaf spot in
    sugar beet, k value was issued as 3 since three images are obtained when pixels
    in the RGB image were separated according to color [51]. 5.1.2. Regression Analysis
    Regression analysis is one of the most common methods of analyzing UAV imagery.
    After orthorectification of images, the values for each band, such as NIR, Red,
    Green, Blue, etc., are extracted. For those extracted values, the regression model
    is run to investigate the spectral characteristics of the parameters. Generally,
    different types of regression analysis models are run depending upon the type
    of dataset acquired: linear and non-linear and simple and multiple regression
    analysis. At the end of the large dataset analysis, the cross-validation of regression
    (RA) model is important because when a model is chosen, it is predicted that the
    observation will be the same in the future as well, but it may not always be similar
    [133]. Thus, data are split, and two portions are formed. A set is used to form
    a regression analysis model, while the other set is reserved as a future observation
    to fit into the model. However, it is not required that both of the split datasets
    should be equal [133]. 5.1.3. Vegetation Indices Vegetative indices are one of
    the major analysis tools for analyzing aerial images. They are the numeric representation
    of the relationship between different wavelengths of lights that are reflected
    from the plant surface [90]. There are various vegetative indices to describe
    the status of plants. They include NDVI [4], Optimized Soil-Adjusted Vegetation
    Index (OSAVI) [134,135], and CSWI [104,136]. A detailed study of different remote
    sensing vegetation indices is conducted by Xue and Su [137]. These indices are
    correlated with different imaging sensors to derive a conclusion. Excess Red (ExR),
    Excess Green (ExG), and Excess Blue (ExB) can be used with RGB imaging. The equation
    for ExR, ExG and ExB are as below: E × R = 1.4R − G (1) E × G = 2G – R − B (2)
    E × B = 1.4B − G (3) where R represents red, G represents green, and B represents
    blue [4,15,18,138]. The physiological and morphological properties of the plant,
    such as water content, biochemical composition, nutrient status, biomass content,
    and diseased tissues, are reflected in the values of vegetative indices [70,76,139].
    The most common vegetative index to determine diseased tissue is NDVI [104]. The
    range of NIR is 780–800 nm, and R is 670–700 nm, as both come from reflected light
    [62,73,104]. The intensities of these lights are measured by multispectral or
    hyperspectral cameras and formatted to an NDVI map later. At the end of formatting,
    each pixel of the NDVI map represents the value of the crop. It is also important
    to note that vegetation indices can be combined to form other different vegetation
    indices (VIs). Therefore, we can assume that different VIs are evolving during
    the writing of this paper. Different studies using VIs to process the data include
    the esca complex in a grape vineyard detected by using NDVI [36]. Similarly, Albetis
    et al. [140] used NDVI and 10 other VIs to differentiate symptomatic and asymptomatic
    Flavescence dorée in grapevines. Analyzing and comparing vegetation indices between
    diseased and healthy crop samples is widely used in monitoring crop health. The
    use of VIs is considered simple, easy, and comparatively reliable in terms of
    disease identification and monitoring. 6. Deep Learning Models Deep Learning (DL)
    is an approach under Machine Learning (ML) where a computer model resembles the
    biological pathways of a human [141]. Deep learning includes the use of artificial
    neural networks that contain various numbers of processing layers different from
    traditional neural networks [126]. It is actually the inclusion of several steps
    from data collection to the classification of the images and interpretation of
    results. 6.1. Artificial Neural Networks (ANNs) The use of neural networks for
    disease recognition in agriculture crops is rapidly increasing [126,142,143].
    A commonly used deep learning tool for image processing and classification is
    the Artificial Neural Networks (ANNs) [8]. ANNs are mathematical models that work
    in the same fashion as the human brain does with neurons and synapses to connect
    each other [126]. The neural networks are trained into a model using previously
    known data and are programmed to work on a similar set of data. There are different
    kinds of artificial neural networks for image classification: Recurrent Neural
    Networks (RNNs), Convolutional Neural Networks (CNNs), and Generative Adversarial
    Networks (GANs). The most common neural networks used for plant disease detection
    and classification are convolutional neural networks (CNNs). 6.2. Convolutional
    Neural Networks (CNNs) CNNs are basic deep learning tools to identify plant diseases
    using aerial imagery [144]. They consist of powerful modelling techniques performing
    complex pattern recognition that have large amounts of data, such as in image
    recognition [126]. Studies that do not have large amounts of data to run neural
    networks augment the data [27]. CNNs are the successors of previous ANNs. ANNs
    were designed to apply in fields, having repeating patterns such as image recognition
    of diseased plants. For the identification of diseases in plants using CNNs, several
    algorithms have been successfully applied, making crop health monitoring easier
    than before. The major CNN algorithms or architectures used are AlexNet [145,146],
    AlexNetOWTBn [147], GoogLeNet [146], Overfeat [148], and VGG [149]. The different
    studies that used CNNs to identify plant diseases are listed in Table 1 below.
    Table 1. Different deep learning CNN models used for the automatic identification
    of diseases in plants. The performance of the deep architectures also depends
    upon the number of modifying images, minibatch sizes, weight differences, and
    bias learning rate [158]. A study by Too et al. [159] reported that DenseNets
    showed higher accuracy with no overfitting and degraded performance when compared
    with VGG 16, Inception V4, and ResNet. Similarly, AlexNet had higher accuracy
    when compared with SqueezeNet in classifying tomato diseases [160] whereas, AlexNet
    and VGG16 had similar accuracy in classifying tomato disease [158]. Mohanty, Hughes,
    and Salathé [151] used AlexNet and GoogLeNet to classify plant diseases and achieved
    an accuracy of 99.35%; however, they performed poorly when tested in different
    sets of images. Deep learning architectures are considered accurate as compared
    to the previously used models such as SVM and random forest methods [158]. The
    correct prediction percentage of CNN was reported 1–4% higher than SVM [161,162,163,164]
    and 6% higher than random forests [165]. However, Song et al. [166] reported that
    CNN models are 18% lower in correct prediction as compared to the Root Mean Square
    Error (RMSE). CNNs are widely used in the identification and classification of
    plant diseases using images. Crop classification using deep learning models helps
    for pest control, cropping activities, yield prediction, etc. [167]. Deep learning
    models have eased the work for the growers in the way that they can click the
    image of the picture in the field and identify the disease by uploading it to
    the software. Feature engineering, a complex process, is eliminated in CNN models
    as the important features are located during the training of the dataset. However,
    different architectures have their own pros and cons. As the layers extend, neural
    networks suffer from performance degradation, resulting in less accuracy. It is
    also time-consuming during training of the images as a large number of images
    are required. Deep networks experience an internal covariant shift, which causes
    disruption in input data and the training layer. However, different techniques,
    such as skip connections [168], layer-wise training [169], transfer learning,
    initialization strategies, and batch normalization, are used these days to overcome
    those challenges [159]. 7. Challenges of Automatic Plant Disease Identification
    Using UAVs Automatic plant disease detection primarily means the identification
    of biotic injury caused by pathogens in plants involving no direct human resources
    in the field. One way of automatically identifying plant disease is deploying
    UAVs with machine learning algorithms installed in them. Information gathered
    in the algorithms not only helps in identifying diseases but also estimating the
    severity of the disease [17]. Plants are infected with hundreds of pathogens in
    the field, and most of them exhibit similar symptoms. Appropriate identification
    of plant disease is one of the basic but challenging tasks in agricultural activities.
    Manually identifying plant disease is subject to bias and optical illusions, which
    result in errors [170]. It involves intensive labor with economic costs. Nevertheless,
    automatic disease identification also requires expert opinion for disease confirmation
    in specific cases. Scouting each plant using laboratory and molecular techniques
    is not practically possible for disease identification in a large area. Large
    and complex data obtained from optical sensors are able to detect disease quickly
    and classify between diseases, stress, and intensity of the diseases [120]. This
    incentivizes scientists and researchers to develop tools that are programmable
    and can read every plant through images to detect diseases. There has been progress
    in automatic monitoring of crop health using UAVs and imagery. However, the system
    of automatic detection and identification of plant diseases has still been experiencing
    difficulty with programming accuracy. Some of the challenges during automatic
    identification of plant diseases using UAVs are discussed by Barbedo [1,170],
    which include background noise, unfavourable field conditions, sensor limitations,
    symptom variations, limitation of resources (peripherals and cameras), and ‘Training’–validation
    discrepancy. In addition, most of the crop health monitoring activities using
    UAVs are dominated by RGB and NIR sensors or their combination. The accurate detection
    of plant disease requires more advanced sensors with higher spectral ranges, such
    as hyperspectral sensors. These sensors have the potentiality to distinguish specific
    features of the object with several hundreds of narrow spectral bands [171]. Moreover,
    the platform used and the image captured also require a considerable value in
    accurate crop health monitoring. There are different platforms and sources to
    acquire the images. Satellite images such as Landsat are available for free however
    have less resolution to correctly detect plant disease. Even the satellite images
    that are available with costs fall under the Visual-NIR region. Development of
    sensors to provide high-quality spatial, temporal and spectral information are
    undergoing [171,172]. Different new technologies have also been introduced to
    monitor crop phenology, such as sun-induced fluorescence and short-wave infrared
    for greenhouse gas monitoring. However, these sensors are highly affected by climatic
    conditions, such as clouds, sunlight, etc., and are only available on regional
    or global scales [173]. The limitations are extended to image processing, segmentation
    and classification as well. Different machine learning tools and architectures
    are available but contain limitations. Some of the major limitations during image
    processing, classification, and segmentation are low-resolution images, less accuracy,
    unavailability of large datasets to train the models, etc. Nguyen and Shah [174]
    found a huge discrepancy in the accuracy of their dataset and the PlantDisease
    dataset. The author recommends the semi-supervised approach to classify disease,
    which will create more diverse images. Similarly, Arsenovic et al. [175] suggested
    an improvement in the decision-making process. In a nutshell, the process of automatic
    identification of plant diseases using UAVs and deep learning can be improved
    by choosing a high-quality image capturing camera, appropriate sensors (RGB, multispectral
    or hyperspectral) depending upon the purpose of study, enough datasets to accurately
    train the model, and selecting appropriate architecture for the deep learning
    model. Apart from these various challenges regarding image analysis and result
    interpretation during automatic detection of plant diseases using UAVs, challenges
    regarding their usage and application in the field exist. The regulatory body
    for controlling UAVs’ application, Federal Aviation Administration (FAA), has
    various regulations such as limits on the height, operating areas, and zones.
    In 2018, FAA applied legal clauses for the application of pesticides using UAVs
    that the operator should receive permission following three exemptions and a waiver
    process [176]. The privacy around the operating and surrounding areas is strictly
    addressed [177]. Regulations on UAVs’ application and their handling in the international
    context is driven by International Civil Aviation Authority (ICAO). More details
    about the different authorities, protocols, and information about UAVs and their
    regulations in the global context are available in Stöcker et al. [178]. The UAV
    technology is expanding and becoming cheaper over time but still is not very applicable
    to many smallholding growers. The addition of hyperspectral and multispectral
    sensors, which are of the utmost importance for monitoring plant health in an
    already purchased UAV, adds more than $10,000 to the cost [4,72]. Similarly, the
    affordability of trained manpower for a small-scale farmer is still unrealistic.
    8. Future Considerations The Unmanned Aerial Vehicle has been a boon to automatic
    monitoring of crop status. Nonetheless, identifying the type of stress, either
    biotic and abiotic, is still vague. Researchers and scientists working together
    with UAV system manufacturers are merging to broaden possibilities. Many challenges
    are limiting progress. Different platforms and the types of sensors are already
    discussed in the sections above; however, all of them have their own pros and
    cons. Lightweight UAVs with high-resolution cameras can capture a better image
    that helps for the proper detection of diseases and reduces chances of error.
    Sensors also play a vital role in disease detection. Abdulridha, Ampatzidis, Kakarla,
    and Roberts [45] used a benchtop hyperspectral imaging system with a 23 mm lens
    having a spectral range of 380–1030 nm, 281 spectral channels 15.3° field view,
    and a spectral resolution of 2.1 nm to detect powdery mildew in squash. The wider
    the spectral ranges, the better the differentiation of disease symptoms and eventually
    helps to reduce the error. The selection of sensors and their spectral range is
    aided by the nature of the disease. For example, Xu et al. [179] used NIR spectroscopy
    and found that the best range for disease monitoring was 1450 nm and 1900 nm in
    tomatoes. Similarly, Al-Ahmadi et al. [180] utilized the same technique with the
    range of 900–2400 nm to monitor charcoal rot (Macrophomina phaseolina in soybean
    (Glycine max)). Scientists are working to develop a hybrid UAV that works as both
    fixed-wing and multirotor systems simultaneously. ALTI transition is a system
    [181,182] that serves both fixed-wing and multirotor systems when and where needed.
    A combined platform constituent of multiple sensors, such as RGB, NIR, RE infrared,
    and many others could be in the future, which will decrease the payload and could
    measure a variety of physiological parameters from the same sensor [183]. The
    increased flight duration of the UAV is the future of agricultural UAVs. With
    limited flight duration, it is challenging to distinguish the overall status of
    the crop. Interpreting results in the form of images rather than in the parametric
    value such as NDVI or other indices could be beneficial for growers to learn and
    execute management practices. Most of the decisions regarding crop health are
    based on the values of vegetative indices. Recent development in crop monitoring
    integrating UAVs and deep learning techniques offers concomitant crop counting,
    yield predictions, crop disease and nutrient deficiency detection [184]. Nebiker,
    Lack, Abächerli, and Läderach [83] utilized low-weight multispectral UAV sensors
    to predict grain yield and diseases in rape and barley. However, most of these
    integrations of concomitant yield and disease monitoring are only prototypes and
    are not available for commercial purposes [185]. The universal system developed
    by the American Society for Testing and Materials (ASTM) is called the G173 standard.
    The G173 standard system is used by different software to derive vegetation indices,
    such asNDVI, Green NDVI (GNDVI), Soil Adjusted Vegetation Index (SAVI), etc. However,
    the G173 standard accounts for the sun facing 37°, an average latitude of the
    continental states of the United States [186]. This develops a site-specific irradiance
    system that can provide a precise description of vegetation indices that will
    help in getting closer to a more accurate result using the Simple Model of Atmospheric
    Radiative Transfer of Sunshine (SMARTS). SMARTS stands on the base of the ASTM
    G173/G177 standard to provide more local solar irradiance spectra [187]. Scientists
    from Israel and Italy have launched hyperspectral imaging sensors to the orbit
    of the earth named the Space-born Hyperspectral Applicative Land and Ocean Mission
    (SHALOM). SHALOM is expected to work in the field of environmental quality and
    assist in precision agriculture in Israel and Italy [9,188]. Similarly, Fluorescence
    Explorer (FLEX) is a satellite to be launched by the European Space Agency (ESA)
    in 2022 [189,190]. FLEX is comprised of three instrumental arrays of fluorescence,
    hyperspectral reflectance, and canopy temperature. This satellite is expected
    to observe the vegetative fluorescence of crops at the global level [189,191,192].
    Further emphasis on chemometric or spectral decomposition should be given for
    the derivative method of analysis. Many researchers are creating their own datasets
    for training and validation. It has become important to develop agricultural datasets
    that will aid machine learning algorithms and help with accurate disease diagnosis
    [163]. In this review, we saw that the future of automatic detection of plant
    diseases is a combination of agriculture with machine learning. The development
    of robotic arms facilities in UAS, which can retrieve samples and return them
    for confirmation in the case of confusion, will help better diagnose plant diseases.
    Thus, machine learning, agriculture, and UAVs can act together to extend the realm
    of food security by limiting food loss due to pests and diseases. 9. Conclusions
    Rapid population growth and climate change are the leading causes of food insecurity.
    The advancements in UAVs and their systems to diagnose crop stress, pests, and
    diseases have greatly benefitted growers. Increasing farm productivity and lowering
    the cost of production using advanced technology is helping growers to increase
    yields and sustainability on their farms. The development in the automatic detection
    of plant diseases using UAVs has emerged as a novel technology of precision agriculture.
    UAVs are accurate and provide large amounts of data regarding crop status, which
    aids in making management decisions. However, there is still immense opportunity
    in plant disease diagnosis. As discussed in the future considerations section,
    the development of various algorithms of machine learning and collaboration with
    the other stems will help to reach this milestone. Author Contributions Writing—original
    draft preparation, K.N.; writing—review and editing, F.B.-G.; funding—received,
    F.B.-G. All authors have read and agreed to the published version of the manuscript.
    Funding This work was supported by the National Institute of Food and Agriculture,
    United States Department of Agriculture Capacity Building grant, under award number
    2019-38821-29062. Institutional Review Board Statement Not applicable. Informed
    Consent Statement Not applicable. Conflicts of Interest The authors declare no
    conflict of interest. References Barbedo, J.G.A. A review on the use of unmanned
    aerial vehicles and imaging sensors for monitoring and assessing plant stresses.
    Drones 2019, 3, 40. [Google Scholar] [CrossRef] [Green Version] Barbedo, J.G.A.;
    Koenigkan, L.V. Perspectives on the use of unmanned aerial systems to monitor
    cattle. Outlook Agric. 2018, 47, 214–222. [Google Scholar] [CrossRef] [Green Version]
    Beloev, I.H. A review on current and emerging application possibilities for unmanned
    aerial vehicles. Acta Technol. Agric. 2016, 19, 70–76. [Google Scholar] [CrossRef]
    [Green Version] Hassler, S.C.; Baysal-Gurel, F. Unmanned aircraft system (UAS)
    technology and applications in agriculture. Agronomy 2019, 9, 618. [Google Scholar]
    [CrossRef] [Green Version] Hunt, E.R.; Daughtry, C.S.T.; Mirsky, S.B.; Hively,
    W.D. Remote sensing with simulated unmanned aircraft imagery for precision agriculture
    applications. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 4566–4571.
    [Google Scholar] [CrossRef] Shi, Y.; Thomasson, J.A.; Murray, S.C.; Pugh, N.A.;
    Rooney, W.L.; Shafian, S.; Rajan, N.; Rouze, G.; Morgan, C.L.; Neely, H.L.; et
    al. Unmanned aerial vehicles for high-throughput phenotyping and agronomic research.
    PLoS ONE 2016, 11, e0159781. [Google Scholar] [CrossRef] [Green Version] Zhang,
    C.; Kovacs, J.M. The application of small unmanned aerial systems for precision
    agriculture: A review. Precis. Agric. 2012, 13, 693–712. [Google Scholar] [CrossRef]
    Barbedo, J.G.A. Factors influencing the use of deep learning for plant disease
    recognition. Biosyst. Eng. 2018, 172, 84–91. [Google Scholar] [CrossRef] Singh,
    P.; Pandey, P.C.; Petropoulos, G.P.; Pavlides, A.; Srivastava, P.K.; Koutsias,
    N.; Deng, K.A.K.; Bao, Y. Hyperspectral remote sensing in precision agriculture:
    Present status, challenges, and future trends. In Hyperspectral Remote Sensing;
    Elsevier: Amsterdam, The Netherlands, 2020; pp. 121–146. [Google Scholar] Hashimoto,
    N.; Saito, Y.; Maki, M.; Homma, K. Simulation of reflectance and vegetation indices
    for unmanned aerial vehicle (UAV) monitoring of paddy fields. Remote Sens. 2019,
    11, 2119. [Google Scholar] [CrossRef] [Green Version] Oliveira, H.C.; Guizilini,
    V.C.; Nunes, I.P.; Souza, J.R. Failure detection in row crops from UAV images
    using morphological operators. IEEE Geosci. Remote Sens. Lett. 2018, 15, 991–995.
    [Google Scholar] [CrossRef] Murugan, D.; Garg, A.; Singh, D. Development of an
    adaptive approach for precision agriculture monitoring with drone and satellite
    data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2017, 10, 5322–5328. [Google
    Scholar] [CrossRef] Duan, S.-B.; Li, Z.-L.; Wu, H.; Tang, B.-H.; Ma, L.; Zhao,
    E.; Li, C. Inversion of the PROSAIL model to estimate leaf area index of maize,
    potato, and sunflower fields from unmanned aerial vehicle hyperspectral data.
    Int. J. Appl. Earth Obs. Geoinf. 2014, 26, 12–20. [Google Scholar] [CrossRef]
    Verger, A.; Vigneau, N.; Chéron, C.; Gilliot, J.-M.; Comar, A.; Baret, F. Green
    area index from an unmanned aerial system over wheat and rapeseed crops. Remote
    Sens. Environ. 2014, 152, 654–664. [Google Scholar] [CrossRef] Rasmussen, J.;
    Nielsen, J.; Garcia-Ruiz, F.; Christensen, S.; Streibig, J. Potential uses of
    small unmanned aircraft systems (UAS) in weed research. Weed Res. 2013, 53, 242–248.
    [Google Scholar] [CrossRef] Sandler, H.A. Weed management in cranberries: A historical
    perspective and a look to the future. Agriculture 2018, 8, 138. [Google Scholar]
    [CrossRef] [Green Version] Abdu, A.M.; Mokji, M.M.; Sheikh, U.U. Automatic vegetable
    disease identification approach using individual lesion features. Comput. Electron.
    Agric. 2020, 176, 105660. [Google Scholar] [CrossRef] She, Y.; Ehsani, R.; Robbins,
    J.; Nahún Leiva, J.; Owen, J. Applications of high-resolution imaging for open
    field container nursery counting. Remote Sens. 2018, 10, 2018. [Google Scholar]
    [CrossRef] [Green Version] Zortea, M.; Macedo, M.M.; Mattos, A.B.; Ruga, B.C.;
    Gemignani, B.H. Automatic citrus tree detection from UAV images based on convolutional
    neural networks. In Proceedings of the 2018 31th SIBGRAPI Conference on Graphics,
    Patterns and Images (SIBGRAPI), Paraná, Brazil, 29 October–1 November 2018. [Google
    Scholar] Yanliang, Z.; Qi, L.; Wei, Z. Design and test of a six-rotor Unmanned
    Aerial Vehicle (UAV) electrostatic spraying system for crop protection. Int. J.
    Agric. Biol. Eng. 2017, 10, 68–76. [Google Scholar] [CrossRef] Mulla, D.J. Twenty
    five years of remote sensing in precision agriculture: Key advances and remaining
    knowledge gaps. Biosyst. Eng. 2013, 114, 358–371. [Google Scholar] [CrossRef]
    Gabriel, J.L.; Zarco-Tejada, P.J.; López-Herrera, P.J.; Pérez-Martín, E.; Alonso-Ayuso,
    M.; Quemada, M. Airborne and ground level sensors for monitoring nitrogen status
    in a maize crop. Biosyst. Eng. 2017, 160, 124–133. [Google Scholar] [CrossRef]
    Chen, Y.; Stark, B.; Kelly, M.; Hogan, S.D. Unmanned aerial systems for agriculture
    and natural resources. Calif. Agric. 2017, 71, 5–14. [Google Scholar] [CrossRef]
    [Green Version] Anderson, K.; Gaston, K.J. Lightweight unmanned aerial vehicles
    will revolutionize spatial ecology. Front. Ecol. Environ. 2013, 11, 138–146. [Google
    Scholar] [CrossRef] [Green Version] Kim, J.; Kim, S.; Ju, C.; Son, H.I. Unmanned
    aerial vehicles in agriculture: A review of perspective of platform, control,
    and applications. IEEE Access 2019, 7, 105100–105115. [Google Scholar] [CrossRef]
    Castelao Tetila, E.; Brandoli Machado, B.; Belete, N.A.d.S.; Guimaraes, D.A.;
    Pistori, H. Identification of soybean foliar diseases using unmanned aerial vehicle
    images. IEEE Geosci. Remote Sens. Lett. 2017, 14, 2190–2194. [Google Scholar]
    [CrossRef] Tetila, E.C.; Machado, B.B.; Menezes, G.K.; Da Silva Oliveira, A.;
    Alvarez, M.; Amorim, W.P.; De Souza Belete, N.A.; Da Silva, G.G.; Pistori, H.
    Automatic recognition of soybean leaf diseases using UAV images and deep convolutional
    neural networks. IEEE Geosci. Remote Sens. Lett. 2020, 17, 903–907. [Google Scholar]
    [CrossRef] Van Evert, F.K.; Gaitán-Cremaschi, D.; Fountas, S.; Kempenaar, C. Can
    precision agriculture increase the profitability and sustainability of the production
    of potatoes and olives? Sustainability 2017, 9, 1863. [Google Scholar] [CrossRef]
    [Green Version] Lee, B.; Park, P.; Kim, C.; Yang, S.; Ahn, S. Power managements
    of a hybrid electric propulsion system for UAVs. J. Mech. Sci. Technol. 2012,
    26, 2291–2299. [Google Scholar] [CrossRef] von Bueren, S.K.; Burkart, A.; Hueni,
    A.; Rascher, U.; Tuohy, M.P.; Yule, I.J. Deploying four optical UAV-based sensors
    over grassland: Challenges and limitations. Biogeosciences 2015, 12, 163–175.
    [Google Scholar] [CrossRef] [Green Version] Hardin, P.J.; Hardin, T.J. Small-scale
    remotely piloted vehicles in environmental research. Geogr. Compass 2010, 4, 1297–1311.
    [Google Scholar] [CrossRef] Chang, C.Y.; Zhou, R.; Kira, O.; Marri, S.; Skovira,
    J.; Gu, L.; Sun, Y. An Unmanned Aerial System (UAS) for concurrent measurements
    of solar-induced chlorophyll fluorescence and hyperspectral reflectance toward
    improving crop monitoring. Agric. For. Meteorol. 2020, 294, 108145. [Google Scholar]
    [CrossRef] Garcia-Ruiz, F.; Sankaran, S.; Maja, J.M.; Lee, W.S.; Rasmussen, J.;
    Ehsani, R. Comparison of two aerial imaging platforms for identification of Huanglongbing-infected
    citrus trees. Comput. Electron. Agric. 2013, 91, 106–115. [Google Scholar] [CrossRef]
    Wang, D.; Song, Q.; Liao, X.; Ye, H.; Shao, Q.; Fan, J.; Cong, N.; Xin, X.; Yue,
    H.; Zhang, H. Integrating satellite and Unmanned Aircraft System (UAS) imagery
    to model livestock population dynamics in the Longbao Wetland National Nature
    Reserve, China. Sci. Total Environ. 2020, 746, 140327. [Google Scholar] [CrossRef]
    Mrisho, L.M.; Mbilinyi, N.A.; Ndalahwa, M.; Ramcharan, A.M.; Kehs, A.K.; McCloskey,
    P.C.; Murithi, H.; Hughes, D.P.; Legg, J.P. Accuracy of a smartphone-based object
    detection model, PlantVillage Nuru, in identifying the foliar symptoms of the
    viral diseases of cassava–CMD and CBSD. Front. Plant Sci. 2020, 11, 1964. [Google
    Scholar] [CrossRef] Di Gennaro, S.F.; Battiston, E.; Di Marco, S.; Facini, O.;
    Matese, A.; Nocentini, M.; Palliotti, A.; Mugnai, L. Unmanned Aerial Vehicle (UAV)-based
    remote sensing to monitor grapevine leaf stripe disease within a vineyard affected
    by esca complex. Phytopathol. Mediterr. 2016, 55, 262–275. [Google Scholar] Pederi,
    Y.A.; Cheporniuk, H.S. Unmanned aerial vehicles and new technological methods
    of monitoring and crop protection in precision agriculture. In Proceedings of
    the 2015 IEEE 3rd International Conference Actual Problems of Unmanned Aerial
    Vehicles Developments (APUAVD), Kyiv, Ukraine, 13–15 October 2015; IEEE: Piscataway,
    NJ, USA, 2015; pp. 298–301. [Google Scholar] Zarco-Tejada, P.J.; Guillén-Climent,
    M.L.; Hernández-Clemente, R.; Catalina, A.; González, M.R.; Martín, P. Estimating
    leaf carotenoid content in vineyards using high resolution hyperspectral imagery
    acquired from an Unmanned Aerial Vehicle (UAV). Agric. For. Meteorol. 2013, 171,
    281–294. [Google Scholar] [CrossRef] [Green Version] Gómez-Candón, D.; De Castro,
    A.I.; López-Granados, F. Assessing the accuracy of mosaics from Unmanned Aerial
    Vehicle (UAV) imagery for precision agriculture purposes in wheat. Precis. Agric.
    2013, 15, 44–56. [Google Scholar] [CrossRef] [Green Version] Torres-Sanchez, J.;
    Lopez-Granados, F.; De Castro, A.I.; Pena-Barragan, J.M. Configuration and specifications
    of an Unmanned Aerial Vehicle (UAV) for early site specific weed management. PLoS
    ONE 2013, 8, e58210. [Google Scholar] [CrossRef] [Green Version] Torres-Sanchez,
    J.; Lopez-Granados, F.; Serrano, N.; Arquero, O.; Pena, J.M. High-throughput 3-D
    monitoring of agricultural-tree plantations with Unmanned Aerial Vehicle (UAV)
    technology. PLoS ONE 2015, 10, e0130479. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Jannoura, R.; Brinkmann, K.; Uteau, D.; Bruns, C.; Joergensen,
    R.G. Monitoring of crop biomass using true colour aerial photographs taken from
    a remote controlled hexacopter. Biosyst. Eng. 2015, 129, 341–351. [Google Scholar]
    [CrossRef] Dai, B.; He, Y.; Gu, F.; Yang, L.; Han, J.; Xu, W. A vision-based autonomous
    aerial spray system for precision agriculture. In Proceedings of the IEEE International
    Conference on Robotics and Biomimetics, Macau, Macao, 5–8 December 2017. [Google
    Scholar] Xavier, T.W.F.; Souto, R.N.V.; Statella, T.; Galbieri, R.; Santos, E.S.;
    Suli, G.S.; Zeilhofer, P. Identification of Ramularia Leaf Blight cotton disease
    infection levels by multispectral, multiscale UAV imagery. Drones 2019, 3, 33.
    [Google Scholar] [CrossRef] [Green Version] Abdulridha, J.; Ampatzidis, Y.; Kakarla,
    S.C.; Roberts, P. Detection of target spot and bacterial spot diseases in tomato
    using UAV-based and benchtop-based hyperspectral imaging techniques. Precis. Agric.
    2019, 21, 955–978. [Google Scholar] [CrossRef] Gomez Selvaraj, M.; Vergara, A.;
    Montenegro, F.; Alonso Ruiz, H.; Safari, N.; Raymaekers, D.; Ocimati, W.; Ntamwira,
    J.; Tits, L.; Omondi, A.B.; et al. Detection of banana plants and their major
    diseases through aerial images and machine learning methods: A case study in DR
    Congo and Republic of Benin. J. Photogramm. Remote Sens. 2020, 169, 110–124. [Google
    Scholar] [CrossRef] Schoofs, H.; Delalieux, S.; Deckers, T.; Bylemans, D. Fire
    Blight monitoring in pear orchards by Unmanned Airborne Vehicles (UAV) systems
    carrying spectral sensors. Agronomy 2020, 10, 615. [Google Scholar] [CrossRef]
    Su, J.; Liu, C.; Hu, X.; Xu, X.; Guo, L.; Chen, W.-H. Spatio-temporal monitoring
    of wheat yellow rust using UAV multispectral imagery. Comput. Electron. Agric.
    2019, 167, 105035. [Google Scholar] [CrossRef] Berni, J.; Zarco-Tejada, P.J.;
    Suarez, L.; Fereres, E. Thermal and narrowband multispectral remote sensing for
    vegetation monitoring from an unmanned aerial vehicle. IEEE Trans. Geosci. Remote
    Sens. 2009, 47, 722–738. [Google Scholar] [CrossRef] [Green Version] Suproteem,
    K.; Sarkara, J.D.; Ehsanib, R.; Kumara, V. Towards autonomous phytopathology:
    Outcomes and challenges of citrus greening disease detection through close-range
    remote sensing. In Proceedings of the 2016 IEEE International Conference on Robotics
    and Automation (ICRA), Stockholm, Sweden, 16–20 May 2016. [Google Scholar] Özgüven,
    M.M. Determination of sugar beet Leaf Spot disease level (Cercospora beticola
    Sacc.) with image processing technique by using drone. Curr. Investig. Agric.
    Curr. Res. 2018, 5, 621–631. [Google Scholar] [CrossRef] Valasek, J.; Thomasson,
    J.A.; Balota, M.; Oakes, J. Exploratory use of a UAV platform for variety selection
    in peanut. In Proceedings of the Autonomous Air and Ground Sensing Systems for
    Agricultural Optimization and Phenotyping, Baltimore, Maryland, 18–19 April 2016.
    98660F. [Google Scholar] [CrossRef] Sugiura, R.; Tsuda, S.; Tamiya, S.; Itoh,
    A.; Nishiwaki, K.; Murakami, N.; Shibuya, Y.; Hirafuji, M.; Nuske, S. Field phenotyping
    system for the assessment of potato late blight resistance using RGB imagery from
    an unmanned aerial vehicle. Biosyst. Eng. 2016, 148, 1–10. [Google Scholar] [CrossRef]
    Rahman, M.F.F.; Fan, S.; Zhang, Y.; Chen, L. A comparative study on application
    of unmanned aerial vehicle systems in agriculture. Agriculture 2021, 11, 22. [Google
    Scholar] [CrossRef] Ludovisi, R.; Tauro, F.; Salvati, R.; Khoury, S.; Mugnozza
    Scarascia, G.; Harfouche, A. UAV-based thermal imaging for high-throughput field
    phenotyping of black poplar response to drought. Front. Plant Sci. 2017, 8, 1681.
    [Google Scholar] [CrossRef] Zhou, J.; Zhou, J.; Ye, H.; Ali, M.L.; Nguyen, H.T.;
    Chen, P. Classification of soybean leaf wilting due to drought stress using UAV-based
    imagery. Comput. Electron. Agric. 2020, 175, 105576. [Google Scholar] [CrossRef]
    Maes, W.H.; Steppe, K. Perspectives for remote sensing with unmanned aerial vehicles
    in precision agriculture. Trends Plant Sci. 2019, 24, 152–164. [Google Scholar]
    [CrossRef] Surový, P.; Almeida Ribeiro, N.; Panagiotidis, D. Estimation of positions
    and heights from UAV-sensed imagery in tree plantations in agrosilvopastoral systems.
    Int. J. Remote Sens. 2018, 39, 4786–4800. [Google Scholar] [CrossRef] Chang, A.;
    Jung, J.; Maeda, M.M.; Landivar, J. Crop height monitoring with digital imagery
    from Unmanned Aerial System (UAS). Comput. Electron. Agric. 2017, 141, 232–237.
    [Google Scholar] [CrossRef] Torres-Sánchez, J.; de Castro, A.I.; Peña, J.M.; Jiménez-Brenes,
    F.M.; Arquero, O.; Lovera, M.; López-Granados, F. Mapping the 3D structure of
    almond trees using UAV acquired photogrammetric point clouds and object-based
    image analysis. Biosyst. Eng. 2018, 176, 172–184. [Google Scholar] [CrossRef]
    Grüner, E.; Astor, T.; Wachendorf, M. Biomass prediction of heterogeneous temperate
    grasslands using an SfM approach based on UAV imaging. Agronomy 2019, 9, 54. [Google
    Scholar] [CrossRef] [Green Version] Roth, L.; Streit, B. Predicting cover crop
    biomass by lightweight UAS-based RGB and NIR photography: An applied photogrammetric
    approach. Precis. Agric. 2017, 19, 93–114. [Google Scholar] [CrossRef] [Green
    Version] Viljanen, N.; Honkavaara, E.; Näsi, R.; Hakala, T.; Niemeläinen, O.;
    Kaivosoja, J. A novel machine learning method for estimating biomass of grass
    swards using a photogrammetric canopy height model, images and vegetation indices
    captured by a drone. Agriculture 2018, 8, 70. [Google Scholar] [CrossRef] [Green
    Version] Berra, E.F.; Gaulton, R.; Barr, S. Commercial off-the-shelf digital cameras
    on unmanned aerial vehicles for multitemporal monitoring of vegetation reflectance
    and NDVI. IEEE Trans. Geosci. Remote Sens. 2017, 55, 4878–4886. [Google Scholar]
    [CrossRef] [Green Version] Nijland, W.; De Jong, R.; De Jong, S.M.; Wulder, M.A.;
    Bater, C.W.; Coops, N.C. Monitoring plant condition and phenology using infrared
    sensitive consumer grade digital cameras. Agric. For. Meteorol. 2014, 184, 98–106.
    [Google Scholar] [CrossRef] [Green Version] Bock, C.H.; Barbedo, J.G.; Del Ponte,
    E.M.; Bohnenkamp, D.; Mahlein, A.-K. From visual estimates to fully automated
    sensor-based measurements of plant disease severity: Status and challenges for
    improving accuracy. Phytopathol. Res. 2020, 2, 1–30. [Google Scholar] [CrossRef]
    [Green Version] Mattupalli, C.; Moffet, C.; Shah, K.; Young, C. Supervised classification
    of RGB aerial imagery to evaluate the impact of a root rot disease. Remote Sens.
    2018, 10, 917. [Google Scholar] [CrossRef] [Green Version] Kerkech, M.; Hafiane,
    A.; Canals, R. Deep leaning approach with colorimetric spaces and vegetation indices
    for vine diseases detection in UAV images. Comput. Electron. Agric. 2018, 155,
    237–243. [Google Scholar] [CrossRef] Ashourloo, D.; Mobasheri, M.R.; Huete, A.
    Developing two spectral disease indices for detection of wheat leaf rust (Pucciniatriticina).
    Remote Sens. 2014, 6, 4723–4740. [Google Scholar] [CrossRef] [Green Version] Zhang,
    D.; Zhou, X.; Zhang, J.; Lan, Y.; Xu, C.; Liang, D. Detection of rice sheath blight
    using an unmanned aerial system with high-resolution color and multispectral imaging.
    PLoS ONE 2018, 13, e0187470. [Google Scholar] [CrossRef] [PubMed] [Green Version]
    Nhamo, L.; Ebrahim, G.Y.; Mabhaudhi, T.; Mpandeli, S.; Magombeyi, M.; Chitakira,
    M.; Magidi, J.; Sibanda, M. An assessment of groundwater use in irrigated agriculture
    using multi-spectral remote sensing. Phys. Chem. Earth Parts A/B/C 2020, 115,
    102810. [Google Scholar] [CrossRef] Adão, T.; Hruška, J.; Pádua, L.; Bessa, J.;
    Peres, E.; Morais, R.; Sousa, J. Hyperspectral Imaging: A review on UAV-based
    sensors, data processing and applications for agriculture and forestry. Remote
    Sens. 2017, 9, 1110. [Google Scholar] [CrossRef] [Green Version] Geipel, J.; Link,
    J.; Wirwahn, J.; Claupein, W. A programmable aerial multispectral camera system
    for in-season crop biomass and nitrogen content estimation. Agriculture 2016,
    6, 4. [Google Scholar] [CrossRef] [Green Version] Iqbal, F.; Lucieer, A.; Barry,
    K. Simplified radiometric calibration for UAS-mounted multispectral sensor. Eur.
    J. Remote Sens. 2018, 51, 301–313. [Google Scholar] [CrossRef] Deng, L.; Mao,
    Z.; Li, X.; Hu, Z.; Duan, F.; Yan, Y. UAV-based multispectral remote sensing for
    precision agriculture: A comparison between different cameras. J. Photogramm.
    Remote Sens. 2018, 146, 124–136. [Google Scholar] [CrossRef] Zaman-Allah, M.;
    Vergara, O.; Araus, J.L.; Tarekegne, A.; Magorokosho, C.; Zarco-Tejada, P.J.;
    Hornero, A.; Alba, A.H.; Das, B.; Craufurd, P.; et al. Unmanned aerial platform-based
    multi-spectral imaging for field phenotyping of maize. Plant Methods 2015, 11,
    35. [Google Scholar] [CrossRef] [Green Version] Kalischuk, M.; Paret, M.L.; Freeman,
    J.H.; Raj, D.; Da Silva, S.; Eubanks, S.; Wiggins, D.J.; Lollar, M.; Marois, J.J.;
    Mellinger, H.C.; et al. An improved crop scouting technique incorporating unmanned
    aerial vehicle-assisted multispectral crop imaging into conventional scouting
    practice for gummy stem blight in watermelon. Plant Dis. 2019, 103, 1642–1650.
    [Google Scholar] [CrossRef] Al-Saddik, H.; Simon, J.C.; Brousse, O.; Cointault,
    F. Multispectral band selection for imaging sensor design for vineyard disease
    detection: Case of Flavescence dorée. Adv. Anim. Biosci. 2017, 8, 150–155. [Google
    Scholar] [CrossRef] [Green Version] Albetis, J.; Jacquin, A.; Goulard, M.; Poilvé,
    H.; Rousseau, J.; Clenet, H.; Dedieu, G.; Duthoit, S. On the potentiality of UAV
    multispectral imagery to detect Flavescence dorée and grapevine trunk diseases.
    Remote Sens. 2018, 11, 23. [Google Scholar] [CrossRef] [Green Version] Calderón,
    R.; Montes-Borrego, M.; Landa, B.B.; Navas-Cortés, J.A.; Zarco-Tejada, P.J. Detection
    of downy mildew of opium poppy using high-resolution multi-spectral and thermal
    imagery acquired with an unmanned aerial vehicle. Precis. Agric. 2014, 15, 639–661.
    [Google Scholar] [CrossRef] Dash, J.; Pearse, G.; Watt, M. UAV multispectral imagery
    can complement satellite data for monitoring forest health. Remote Sens. 2018,
    10, 1216. [Google Scholar] [CrossRef] [Green Version] Khot, L.R.; Sankaran, S.;
    Carter, A.H.; Johnson, D.A.; Cummings, T.F. UAS imaging-based decision tools for
    arid winter wheat and irrigated potato production management. Int. J. Remote Sens.
    2015, 37, 125–137. [Google Scholar] [CrossRef] Nebiker, S.; Lack, N.; Abächerli,
    M.; Läderach, S. Light-weight multispectral UAV sensors and their capabilities
    for predicting grain yield and detecting plant diseases. ISPRS -Int. Arch. Photogramm.
    Remote Sens. Spat. Inf. Sci. 2016, 41, 963–970. [Google Scholar] [CrossRef] [Green
    Version] Kerkech, M.; Hafiane, A.; Canals, R. Vine disease detection in UAV multispectral
    images using optimized image registration and deep learning segmentation approach.
    Comput. Electron. Agric. 2020, 174, 105446. [Google Scholar] [CrossRef] Gallo,
    R.; Ristorto, G.; Daglio, G.; Berta, G.; Lazzari, M.; Mazzetto, F. New solutions
    for the automatic early detection of diseases in vineyards through ground sensing
    approaches integrating LiDAR and optical sensors. Chem. Eng. Trans. 2017, 58,
    673–678. [Google Scholar] Qin, J.; Wang, B.; Wu, Y.; Lu, Q.; Zhu, H. Identifying
    pine wood nematode disease using UAV images and deep learning algorithms. Remote
    Sens. 2021, 13, 162. [Google Scholar] [CrossRef] Ye, H.; Huang, W.; Huang, S.;
    Cui, B.; Dong, Y.; Guo, A.; Ren, Y.; Jin, Y. Recognition of banana fusarium wilt
    based on UAV remote sensing. Remote Sens. 2020, 12, 938. [Google Scholar] [CrossRef]
    [Green Version] Lowe, A.; Harrison, N.; French, A.P. Hyperspectral image analysis
    techniques for the detection and classification of the early onset of plant disease
    and stress. Plant Methods 2017, 13, 80. [Google Scholar] [CrossRef] Cilia, C.;
    Panigada, C.; Rossini, M.; Meroni, M.; Busetto, L.; Amaducci, S.; Boschetti, M.;
    Picchi, V.; Colombo, R. Nitrogen status assessment for variable rate fertilization
    in maize through hyperspectral imagery. Remote Sens. 2014, 6, 6549–6565. [Google
    Scholar] [CrossRef] [Green Version] Gevaert, C.M.; Suomalainen, J.; Tang, J.;
    Kooistra, L. Generation of spectral–temporal response surfaces by combining multispectral
    satellite and hyperspectral UAV imagery for precision agriculture applications.
    IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2015, 8, 3140–3146. [Google Scholar]
    [CrossRef] Proctor, C.; He, Y. Workflow for building a hyperspectral UAV: Challenges
    and opportunities. ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2015,
    40, 415–419. [Google Scholar] [CrossRef] [Green Version] Deery, D.; Jimenez-Berni,
    J.; Jones, H.; Sirault, X.; Furbank, R. proximal remote sensing buggies and potential
    applications for field-based phenotyping. Agronomy 2014, 4, 349–379. [Google Scholar]
    [CrossRef] [Green Version] Honkavaara, E.; Hakala, T.; Markelin, L.; Jaakkola,
    A.; Saari, H.; Ojanen, H.; Pölönen, I.; Tuominen, S.; Näsi, R.; Rosnell, T.; et
    al. Autonomous hyperspectral UAS photogrammetry for environmental monitoring applications.
    ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2014, 40, 155–159. [Google
    Scholar] [CrossRef] [Green Version] Honkavaara, E.; Saari, H.; Kaivosoja, J.;
    Pölönen, I.; Hakala, T.; Litkey, P.; Mäkynen, J.; Pesonen, L. Processing and assessment
    of spectrometric, stereoscopic imagery collected using a lightweight UAV spectral
    camera for precision agriculture. Remote Sens. 2013, 5, 5006–5039. [Google Scholar]
    [CrossRef] [Green Version] Saari, H.; Akujärvi, A.; Holmlund, C.; Ojanen, H.;
    Kaivosoja, J.; Nissinen, A.; Niemeläinen, O. Visible, very near IR and short wave
    IR hyperspectral drone imaging system for agriculture and natural water applications.
    ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, 42, 165–170. [Google
    Scholar] [CrossRef] [Green Version] Tack, N.; Lambrechts, A.; Soussan, P.; Haspeslagh,
    L. A compact, high-speed, and low-cost hyperspectral imager. In Proceedings of
    the Silicon Photonics VII, 8266, San Francisco, CA, USA, 21–26 January 2012. [Google
    Scholar] Sima, A.A.; Baeck, P.; Nuyts, D.; Delalieux, S.; Livens, S.; Blommaert,
    J.; Delauré, B.; Boonen, M. Compact hyperspectral imaging system (COSI) for Small
    Remotely Piloted Aircraft Systems (RPAS) – System overview and first performance
    evaluation results. ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.
    2016, 41, 1157–1164. [Google Scholar] [CrossRef] [Green Version] Calderón, R.;
    Navas-Cortés, J.A.; Lucena, C.; Zarco-Tejada, P.J. High-resolution airborne hyperspectral
    and thermal imagery for early detection of Verticillium wilt of olive using fluorescence,
    temperature and narrow-band spectral indices. Remote Sens. Environ. 2013, 139,
    231–245. [Google Scholar] [CrossRef] Sandino, J.; Pegg, G.; Gonzalez, F.; Smith,
    G. Aerial mapping of forests affected by pathogens using UAVs, hyperspectral sensors,
    and artificial intelligence. Sensors 2018, 18, 944. [Google Scholar] [CrossRef]
    [PubMed] [Green Version] Calderón, R.; Navas-Cortés, J.; Lucena, C.; Zarco-Tejada,
    P. High-resolution hyperspectral and thermal imagery acquired from UAV platforms
    for early detection of Verticillium wilt using fluorescence, temperature and narrow-band
    indices. In Proceedings of the Workshop on UAV-basaed Remote Sensing Methods for
    Monitoring Vegetation, Cologne, Germany, 11–12 September 2013; p. 9. [Google Scholar]
    Thomas, S.; Kuska, M.T.; Bohnenkamp, D.; Brugger, A.; Alisaac, E.; Wahabzada,
    M.; Behmann, J.; Mahlein, A.-K. Benefits of hyperspectral imaging for plant disease
    detection and plant protection: A technical perspective. J. Plant Dis. Prot. 2018,
    125, 5–20. [Google Scholar] [CrossRef] Costa, J.M.; Grant, O.M.; Chaves, M.M.
    Thermography to explore plant-environment interactions. J. Exp. Bot. 2013, 64,
    3937–3949. [Google Scholar] [CrossRef] [PubMed] Mahajan, U.; Bundel, B.R. Drones
    for Normalized Difference Vegetation Index (NDVI), to estimate crop health for
    precision agriculture: A cheaper alternative for spatial satellite sensors. In
    International Conference on Innovative Research in Agriculture, Food Science,
    Forestry, Horticulture, Aquaculture, Animal Sciences, Biodiversity, Ecological
    Sciences and Climate Change; Krishi Sanskriti Publications: New Delhi, India,
    2016. [Google Scholar] Gago, J.; Douthe, C.; Coopman, R.; Gallego, P.; Ribas-Carbo,
    M.; Flexas, J.; Escalona, J.; Medrano, H. UAVs challenge to assess water stress
    for sustainable agriculture. Agric. Water Manag. 2015, 153, 9–19. [Google Scholar]
    [CrossRef] Granum, E.; Pérez-Bueno, M.L.; Calderón, C.E.; Ramos, C.; de Vicente,
    A.; Cazorla, F.M.; Barón, M. Metabolic responses of avocado plants to stress induced
    by Rosellinia necatrix analysed by fluorescence and thermal imaging. Eur. J. Plant
    Pathol. 2015, 142, 625–632. [Google Scholar] [CrossRef] Smigaj, M.; Gaulton, R.;
    Barr, S.L.; Suárez, J.C. UAV-borne thermal imaging for forest health monitoring:
    Detection of disease-induced canopy temperature increase. ISPRS-Int. Arch. Photogramm.
    Remote Sens. Spat. Inf. Sci. 2015, 40, 349–354. [Google Scholar] [CrossRef] [Green
    Version] Mahlein, A.-K.; Oerke, E.-C.; Steiner, U.; Dehne, H.-W. Recent advances
    in sensing plant diseases for precision crop protection. Eur. J. Plant Pathol.
    2012, 133, 197–209. [Google Scholar] [CrossRef] Raza, S.-e.-A.; Prince, G.; Clarkson,
    J.P.; Rajpoot, N.M. Automatic detection of diseased tomato plants using thermal
    and stereo visible light images. PLoS ONE 2015, 10, e0123262. [Google Scholar]
    Baranowski, P.; Jedryczka, M.; Mazurek, W.; Babula-Skowronska, D.; Siedliska,
    A.; Kaczmarek, J. Hyperspectral and thermal imaging of oilseed rape (Brassica
    napus) response to fungal species of the genus Alternaria. PLoS ONE 2015, 10,
    e0122913. [Google Scholar] [CrossRef] [Green Version] López-López, M.; Calderón,
    R.; González-Dugo, V.; Zarco-Tejada, P.J.; Fereres, E. Early detection and quantification
    of almond red leaf blotch using high-resolution hyperspectral and thermal imagery.
    Remote Sens. 2016, 8, 276. [Google Scholar] [CrossRef] [Green Version] Sankaran,
    S.; Maja, J.M.; Buchanon, S.; Ehsani, R. Huanglongbing (citrus greening) detection
    using visible, near infrared and thermal imaging techniques. Sensors 2013, 13,
    2117–2130. [Google Scholar] [CrossRef] [PubMed] [Green Version] Xu, H.; Zhu, S.;
    Ying, Y.; Jiang, H. Early detection of plant disease using infrared thermal imaging.
    In Optics for Natural Resources, Agriculture, and Foods; International Society
    for Optics and Photonics: Bellingham, WA, USA, 2006; p. 638110. [Google Scholar]
    Wang, M.; Xiong, Y.; Ling, N.; Feng, X.; Zhong, Z.; Shen, Q.; Guo, S. Detection
    of the dynamic response of cucumber leaves to fusaric acid using thermal imaging.
    Plant Physiol. Biochem. 2013, 66, 68–76. [Google Scholar] [CrossRef] [PubMed]
    Anasta, N.; Setyawan, F.; Fitriawan, H. Disease detection in banana trees using
    an image processing-based thermal camera. In IOP Conference Series: Earth and
    Environmental Science; IOP Publishing: Bristol, UK, 2021; p. 012088. [Google Scholar]
    Yang, N.; Yuan, M.; Wang, P.; Zhang, R.; Sun, J.; Mao, H. Tea diseases detection
    based on fast infrared thermal image processing technology. J. Sci. Food Agric.
    2019, 99, 3459–3466. [Google Scholar] [CrossRef] [PubMed] Vit, A.; Shani, G. Comparing
    RGB-D sensors for close range outdoor agricultural phenotyping. Sensors 2018,
    18, 4413. [Google Scholar] [CrossRef] [PubMed] [Green Version] Andujar, D.; Dorado,
    J.; Fernandez-Quintanilla, C.; Ribeiro, A. An approach to the use of depth cameras
    for weed volume estimation. Sensors 2016, 16, 972. [Google Scholar] [CrossRef]
    [PubMed] [Green Version] Zollhöfer, M.; Stotko, P.; Görlitz, A.; Theobalt, C.;
    Nießner, M.; Klein, R.; Kolb, A. State of the art on 3D reconstruction with RGB-D
    cameras. Comput. Graph. Forum 2018, 37, 625–652. [Google Scholar] [CrossRef] Xia,
    C.; Wang, L.; Chung, B.-K.; Lee, J.-M. In situ 3D segmentation of individual plant
    leaves using a RGB-D camera for agricultural automation. Sensors 2015, 15, 20463–20479.
    [Google Scholar] [CrossRef] Mahlein, A.-K. Plant disease detection by imaging
    sensors–parallels and specific demands for precision agriculture and plant phenotyping.
    Plant Dis. 2016, 100, 241–251. [Google Scholar] [CrossRef] [Green Version] Paulus,
    S.; Dupuis, J.; Mahlein, A.-K.; Kuhlmann, H. Surface feature based classification
    of plant organs from 3D laserscanned point clouds for plant phenotyping. BMC Bioinform.
    2013, 14, 1–12. [Google Scholar] [CrossRef] [Green Version] Singh, A.; Ganapathysubramanian,
    B.; Singh, A.K.; Sarkar, S. Machine learning for high-throughput stress phenotyping
    in plants. Trends Plant Sci. 2016, 21, 110–124. [Google Scholar] [CrossRef] [Green
    Version] Wallelign, S.; Polceanu, M.; Buche, C. Soybean plant disease identification
    using convolutional neural network. In Proceedings of the Thirty-First International
    Flairs Conference, Melbourne, FL, USA, 21–23 May 2018. [Google Scholar] Sonka,
    M.; Hlavac, V.; Boyle, R. Image pre-processing. In Image Processing, Analysis
    and Machine Vision; Springer: Berlin, Germany, 1993; pp. 56–111. [Google Scholar]
    Ghosal, S.; Blystone, D.; Singh, A.K.; Ganapathysubramanian, B.; Singh, A.; Sarkar,
    S. An explainable deep machine vision framework for plant stress phenotyping.
    Proc. Natl. Acad. Sci. USA 2018, 115, 4613–4618. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Ferentinos, K.P. Deep learning models for plant disease detection
    and diagnosis. Comput. Electron. Agric. 2018, 145, 311–318. [Google Scholar] [CrossRef]
    Gu, L.; Robles-Kelly, A.A.; Zhou, J. Efficient estimation of reflectance parameters
    from imaging spectroscopy. IEEE Trans. Image Process. 2013, 22, 3648–3663. [Google
    Scholar] Habili, N.; Oorloff, J. Scyllarus ™: From research to commercial software.
    In Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference,
    New York, NY, USA, 28 September–1 October 2015; pp. 119–122. [Google Scholar]
    Choi, H.; Baraniuk, R. Analysis of wavelet-domain Wiener filters. In Proceedings
    of the IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis
    (Cat. No. 98TH8380), Philadelphia, PA, USA, 25–28 October 1994; pp. 613–616. [Google
    Scholar] Marroquin, J.L.; Girosi, F. Some extensions of the K-Means algorithm
    for image segmentation and pattern classification; Massachusetts Inst of Tech
    Cambridge Artificial Intelligence Lab: Cambridge, MA, USA, 1993. [Google Scholar]
    MacQueen, J. Some methods for classification and analysis of multivariate observations.
    In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
    Probability, Los Angeles, CA, USA, 27 December 1965–7 January 1966; pp. 281–297.
    [Google Scholar] Wagstaff, K.; Cardie, C.; Rogers, S.; Schrödl, S. Constrained
    k-means clustering with background knowledge. In Proceedings of the Eighteenth
    International Conference on Machine Learning, San Francisco, CA, USA, 28 June–1
    July 2001; pp. 577–584. [Google Scholar] Picard, R.R.; Cook, R.D. Cross-validation
    of regression models. J. Am. Stat. Assoc. 1984, 79, 575–583. [Google Scholar]
    [CrossRef] Gupta, S.G.; Ghonge, D.; Jawandhiya, P.M. Review of Unmanned Aircraft
    System (UAS). Int. J. Adv. Res. Comput. Eng. Technol. 2013, 2, 1646–1658. [Google
    Scholar] [CrossRef] Marino, S.; Alvino, A. Detection of spatial and temporal variability
    of wheat cultivars by high-resolution vegetation indices. Agronomy 2019, 9, 226.
    [Google Scholar] [CrossRef] [Green Version] Ribeiro-Gomes, K.; Hernández-López,
    D.; Ortega, J.F.; Ballesteros, R.; Poblete, T.; Moreno, M.A. Uncooled thermal
    camera calibration and optimization of the photogrammetry process for UAV applications
    in agriculture. Sensors 2017, 17, 2173. [Google Scholar] [CrossRef] Xue, J.; Su,
    B. Significant remote sensing vegetation indices: A review of developments and
    applications. J. Sens. 2017, 2017, 1353691. [Google Scholar] [CrossRef] [Green
    Version] Woebbecke, D.M.; Meyer, G.E.; Von Bargen, K.; Mortensen, D.A. Color indices
    for weed identification under various soil, residue, and lighting conditions.
    Trans. ASAE 1995, 38, 259–269. [Google Scholar] [CrossRef] Patrick, A.; Pelham,
    S.; Culbreath, A.; Holbrook, C.C.; De Godoy, I.J.; Li, C. High throughput phenotyping
    of tomato spot wilt disease in peanuts using unmanned aerial systems and multispectral
    imaging. IEEE Instrum. Meas. Mag. 2017, 20, 4–12. [Google Scholar] [CrossRef]
    Albetis, J.; Duthoit, S.; Guttler, F.; Jacquin, A.; Goulard, M.; Poilvé, H.; Féret,
    J.-B.; Dedieu, G. Detection of Flavescence dorée grapevine disease using Unmanned
    Aerial Vehicle (UAV) multispectral imagery. Remote Sens. 2017, 9, 308. [Google
    Scholar] [CrossRef] [Green Version] McCulloch, W.S.; Pitts, W. A logical calculus
    of the ideas immanent in nervous activity. Bull. Math. Biophys. 1943, 5, 115–133.
    [Google Scholar] [CrossRef] Carranza-Rojas, J.; Goeau, H.; Bonnet, P.; Mata-Montero,
    E.; Joly, A. Going deeper in the automated identification of Herbarium specimens.
    BMC Evol. Biol. 2017, 17, 1–14. [Google Scholar] [CrossRef] [PubMed] [Green Version]
    Yang, X.; Guo, T. Machine learning in plant disease research. Eur. J. Biomed.
    Res. 2017, 3, 6–9. [Google Scholar] [CrossRef] [Green Version] LeCun, Y.; Bottou,
    L.; Bengio, Y.; Haffner, P. Gradient-based learning applied to document recognition.
    Proc. IEEE 1998, 86, 2278–2324. [Google Scholar] [CrossRef] [Green Version] Zhang,
    K.; Wu, Q.; Liu, A.; Meng, X. Can deep learning identify tomato leaf disease?
    Adv. Multimed. 2018, 2018, 6710865. [Google Scholar] [CrossRef] [Green Version]
    Türkoğlu, M.; Hanbay, D. Plant disease and pest detection using deep learning-based
    features. Turk. J. Electr. Eng. Comput. Sci. 2019, 27, 1636–1651. [Google Scholar]
    [CrossRef] Krizhevsky, A. One weird trick for parallelizing convolutional neural
    networks. arXiv 2014, arXiv:1404.5997. [Google Scholar] Sermanet, P.; Eigen, D.;
    Zhang, X.; Mathieu, M.; Fergus, R.; LeCun, Y. Overfeat: Integrated recognition,
    localization and detection using convolutional networks. arXiv 2013, arXiv:1312.6229.
    [Google Scholar] Simonyan, K.; Zisserman, A. Very deep convolutional networks
    for large-scale image recognition. arXiv 2014, arXiv:1409.1556. [Google Scholar]
    Hughes, D.; Salathé, M. An open access repository of images on plant health to
    enable the development of mobile disease diagnostics. arXiv 2015, arXiv:1511.08060.
    [Google Scholar] Mohanty, S.P.; Hughes, D.P.; Salathé, M. Using deep learning
    for image-based plant disease detection. Front. Plant Sci. 2016, 7, 1419. [Google
    Scholar] [CrossRef] [PubMed] [Green Version] Sibiya, M.; Sumbwanyambe, M. A computational
    procedure for the recognition and classification of maize leaf diseases out of
    healthy leaves using convolutional neural networks. AgriEngineering 2019, 1, 119–131.
    [Google Scholar] [CrossRef] [Green Version] Wang, T.; Thomasson, J.A.; Yang, C.;
    Isakeit, T.; Nichols, R.L. Automatic classification of cotton root rot disease
    based on UAV remote sensing. Remote Sens. 2020, 12, 1310. [Google Scholar] [CrossRef]
    [Green Version] Kerkech, M.; Hafiane, A.; Canals, R.; Ros, F. Vine disease detection
    by deep learning method combined with 3d depth information. In Proceedings of
    the International Conference on Image and Signal Processing, Marrakesh, Morocco,
    4–6 June 2020; Springer: Cham, Germany, 2020; pp. 82–90. [Google Scholar] Gibson-Poole,
    S.; Humphris, S.; Toth, I.; Hamilton, A. Identification of the onset of disease
    within a potato crop using a UAV equipped with un-modified and modified commercial
    off-the-shelf digital cameras. Adv. Anim. Biosci. 2017, 8, 812–816. [Google Scholar]
    [CrossRef] Sugiura, R.; Tsuda, S.; Tsuji, H.; Murakami, N. Virus-infected plant
    detection in potato seed production field by UAV imagery. In Proceedings of the
    2018 ASABE Annual International Meeting, Detroit, MI, USA, 29 July–1 August 2018;
    p. 1. [Google Scholar] Dang, L.M.; Hassan, S.I.; Suhyeon, I.; kumar Sangaiah,
    A.; Mehmood, I.; Rho, S.; Seo, S.; Moon, H. UAV based wilt detection system via
    convolutional neural networks. Sustain. Comput. Inform. Syst. 2018, 28, 100250.
    [Google Scholar] [CrossRef] [Green Version] Rangarajan, A.K.; Purushothaman, R.;
    Ramesh, A. Tomato crop disease classification using pre-trained deep learning
    algorithm. Procedia Comput. Sci. 2018, 133, 1040–1047. [Google Scholar] [CrossRef]
    Too, E.C.; Yujian, L.; Njuki, S.; Yingchun, L. A comparative study of fine-tuning
    deep learning models for plant disease identification. Comput. Electron. Agric.
    2019, 161, 272–279. [Google Scholar] [CrossRef] Durmuş, H.; Güneş, E.O.; Kırcı,
    M. Disease detection on the leaves of the tomato plants by using deep learning.
    In Proceedings of the 2017 6th International Conference on Agro-Geoinformatics,
    Fairfax, VA, USA, 7–10 August 2017; pp. 1–5. [Google Scholar] Chen, Y.; Lin, Z.;
    Zhao, X.; Wang, G.; Gu, Y. Deep learning-based classification of hyperspectral
    data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 2094–2107. [Google
    Scholar] [CrossRef] Grinblat, G.L.; Uzal, L.C.; Larese, M.G.; Granitto, P.M. Deep
    learning for plant identification using vein morphological patterns. Comput. Electron.
    Agric. 2016, 127, 418–424. [Google Scholar] [CrossRef] [Green Version] Kamilaris,
    A.; Prenafeta-Boldú, F.X. Deep learning in agriculture: A survey. Comput. Electron.
    Agric. 2018, 147, 70–90. [Google Scholar] [CrossRef] [Green Version] Lee, S.H.;
    Chan, C.S.; Wilkin, P.; Remagnino, P. Deep-plant: Plant identification with convolutional
    neural networks. In Proceedings of the 2015 IEEE international conference on image
    processing (ICIP), Quebec City, QC, Canada, 27–30 September 2015; pp. 452–456.
    [Google Scholar] Kussul, N.; Lavreniuk, M.; Skakun, S.; Shelestov, A. Deep learning
    classification of land cover and crop types using remote sensing data. IEEE Geosci.
    Remote Sens. Lett. 2017, 14, 778–782. [Google Scholar] [CrossRef] Song, X.; Zhang,
    G.; Liu, F.; Li, D.; Zhao, Y.; Yang, J. Modeling spatio-temporal distribution
    of soil moisture by deep learning-based cellular automata model. J. Arid Land
    2016, 8, 734–748. [Google Scholar] [CrossRef] [Green Version] Zhu, N.; Liu, X.;
    Liu, Z.; Hu, K.; Wang, Y.; Tan, J.; Huang, M.; Zhu, Q.; Ji, X.; Jiang, Y. Deep
    learning for smart agriculture: Concepts, tools, applications, and opportunities.
    Int. J. Agric. Biol. Eng. 2018, 11, 32–44. [Google Scholar] [CrossRef] He, K.;
    Zhang, X.; Ren, S.; Sun, J. Identity mappings in deep residual networks. In Proceedings
    of the European Conference on Computer Vision, Amsterdam, The Netherlands, 8–16
    October 2016; Springer: Cham, Germany, 2016; pp. 630–645. [Google Scholar] Yu,
    D.; Xiong, W.; Droppo, J.; Stolcke, A.; Ye, G.; Li, J.; Zweig, G. Deep convolutional
    neural networks with layer-wise context expansion and attention. In Proceedings
    of the 17th Annual Conference of the International Speech Communication Association,
    San Francisco, CA, USA, 8–12 September 2016; pp. 17–21. [Google Scholar] Barbedo,
    J.G.A. A review on the main challenges in automatic plant disease identification
    based on visible range images. Biosyst. Eng. 2016, 144, 52–60. [Google Scholar]
    [CrossRef] Khanal, S.; KC, K.; Fulton, J.P.; Shearer, S.; Ozkan, E. Remote sensing
    in agriculture—accomplishments, limitations, and opportunities. Remote Sens. 2020,
    12, 3783. [Google Scholar] [CrossRef] Hulley, G.; Hook, S.; Fisher, J.; Lee, C.
    Ecostress, a Nasa Earth-Ventures Instrument for studying links between the water
    cycle and plant health over the diurnal cycle. In Proceedings of the 2017 IEEE
    International Geoscience and Remote Sensing Symposium (IGARSS), Fort Worth, TX,
    USA, 23–28 July 2017; pp. 5494–5496. [Google Scholar] Song, L.; Guanter, L.; Guan,
    K.; You, L.; Huete, A.; Ju, W.; Zhang, Y. Satellite sun-induced chlorophyll fluorescence
    detects early response of winter wheat to heat stress in the Indian Indo-Gangetic
    Plains. Glob. Chang. Biol. 2018, 24, 4023–4037. [Google Scholar] [CrossRef] [PubMed]
    [Green Version] Nguyen, M.-T.; Shah, D. Improving Current Limitations of Deep
    Learning Based Plant Disease Identification; The Cooper Union: NewYork, NY, USA,
    22 December 2019. [Google Scholar] Arsenovic, M.; Karanovic, M.; Sladojevic, S.;
    Anderla, A.; Stefanovic, D. Solving current limitations of deep learning based
    approaches for plant disease detection. Symmetry 2019, 11, 939. [Google Scholar]
    [CrossRef] [Green Version] Petty, R.V.; Chang, E.B.E. Drone use in aerial pesticide
    application faces outdated regulatory hurdles. Harvard J. Law Technol. Dig. 2018,
    1–14. [Google Scholar] Stoica, A.-A. Emerging legal issues regarding civilian
    drone usage. Chall. Knowl. Soc. 2018, 692–699. [Google Scholar] Stöcker, C.; Bennett,
    R.; Nex, F.; Gerke, M.; Zevenbergen, J. Review of the current state of UAV regulations.
    Remote Sens. 2017, 9, 459. [Google Scholar] [CrossRef] [Green Version] Xu, H.;
    Ying, Y.; Fu, X.; Zhu, S. Near-infrared spectroscopy in detecting leaf miner damage
    on tomato leaf. Biosyst. Eng. 2007, 96, 447–454. [Google Scholar] [CrossRef] Al-Ahmadi,
    A.H.; Subedi, A.; Wang, G.; Choudhary, R.; Fakhoury, A.; Watson, D.G. Detection
    of charcoal rot (Macrophomina phaseolina) toxin effects in soybean (Glycine max)
    seedlings using hyperspectral spectroscopy. Comput. Electron. Agric. 2018, 150,
    188–195. [Google Scholar] [CrossRef] Oosedo, A.; Abiko, S.; Konno, A.; Uchiyama,
    M. Optimal transition from hovering to level-flight of a quadrotor tail-sitter
    UAV. Auton. Robot. 2017, 41, 1143–1159. [Google Scholar] [CrossRef] Theys, B.;
    De Vos, G.; De Schutter, J. A control approach for transitioning VTOL UAVs with
    continuously varying transition angle and controlled by differential thrust. In
    Proceedings of the 2016 International Conference on Unmanned Aircraft Systems
    (ICUAS), Arlington, VA, USA, 7–10 June 2016; IEEE: Piscataway, NJ, USA, 2016;
    pp. 118–125. [Google Scholar] Latif, M.A. An agricultural perspective on flying
    sensors: State of the art, challenges, and future directions. IEEE Geosci. Remote
    Sens. Mag. 2018, 6, 10–22. [Google Scholar] [CrossRef] Oghaz, M.M.D.; Razaak,
    M.; Kerdegari, H.; Argyriou, V.; Remagnino, P. Scene and environment monitoring
    using aerial imagery and deep learning. In Proceedings of the 2019 15th International
    Conference on Distributed Computing in Sensor Systems (DCOSS), Los Angeles, CA,
    USA, 29–31 May 2019; pp. 362–369. [Google Scholar] Boursianis, A.D.; Papadopoulou,
    M.S.; Diamantoulakis, P.; Liopa-Tsakalidi, A.; Barouchas, P.; Salahas, G.; Karagiannidis,
    G.; Wan, S.; Goudos, S.K. Internet of Things (IoT) and Agricultural Unmanned Aerial
    Vehicles (UAVs) in smart farming: A comprehensive review. Internet Things 2020,
    100187. [Google Scholar] [CrossRef] Ernst, M.; Holst, H.; Winter, M.; Altermatt,
    P.P. SunCalculator: A program to calculate the angular and spectral distribution
    of direct and diffuse solar radiation. Sol. Energy Mater. Sol. Cells 2016, 157,
    913–922. [Google Scholar] [CrossRef] Fernández, E.F.; Soria-Moya, A.; Almonacid,
    F.; Aguilera, J. Comparative assessment of the spectral impact on the energy yield
    of high concentrator and conventional photovoltaic technology. Sol. Energy Mater.
    Sol. Cells 2016, 147, 185–197. [Google Scholar] [CrossRef] Ben-Dor, E.; Schläpfer,
    D.; Plaza, A.J.; Malthus, T. Hyperspectral remote sensing. In Airborne Measurements
    for Environmental Research: Methods and Instruments; Wiley-VCH Verlag & Co. KGaA:
    Weinheim, Germany, 2013; Volume 413, p. 456. [Google Scholar] Colombo, R.; Celesti,
    M.; Bianchi, R.; Campbell, P.K.; Cogliati, S.; Cook, B.D.; Corp, L.A.; Damm, A.;
    Domec, J.C.; Guanter, L. Variability of sun-induced chlorophyll fluorescence according
    to stand age-related processes in a managed loblolly pine forest. Glob. Chang.
    Biol. 2018, 24, 2980–2996. [Google Scholar] [CrossRef] [Green Version] Middleton,
    E.M.; Rascher, U.; Huemmrich, K.F.; Cook, B.D.; Noormets, A.; Schickling, A.;
    Pinto, F.; Alonso, L.; Damm, A.; Guanter, L. The 2013 FLEX-US airborne campaign
    at the Parker Tract Loblolly Pine Plantation in North Carolina, USA. Remote Sens.
    2017, 9, 612. [Google Scholar] [CrossRef] [Green Version] Bovensmann, H.; Bösch,
    H.; Brunner, D.; Ciais, P.; Crisp, D.; Dolman, H.; Hayman, G.; Houweling, S.;
    Lichtenberg, L. Report for Mission Selection: CarbonSat-An Earth Explorer to Observe
    Greenhouse Gases; European Space Agency: Noordwijk, The Netherlands, 2015. [Google
    Scholar] Pandley, P.; Manevski, K.; Srivastava, P.K.; Petropoulos, G.P. The use
    of hyperspectral earth observation data for land use/cover classification: Present
    status, challenges and future outlook. In Hyperspectral Remote Sensing of Vegetation,
    1st ed.; Thenkabail, P., Ed.; CRC Press: Boca Raton, FL, USA, 2018; pp. 147–173.
    [Google Scholar]      Publisher’s Note: MDPI stays neutral with regard to jurisdictional
    claims in published maps and institutional affiliations.  © 2021 by the authors.
    Licensee MDPI, Basel, Switzerland. This article is an open access article distributed
    under the terms and conditions of the Creative Commons Attribution (CC BY) license
    (https://creativecommons.org/licenses/by/4.0/). Share and Cite MDPI and ACS Style
    Neupane, K.; Baysal-Gurel, F. Automatic Identification and Monitoring of Plant
    Diseases Using Unmanned Aerial Vehicles: A Review. Remote Sens. 2021, 13, 3841.
    https://doi.org/10.3390/rs13193841 AMA Style Neupane K, Baysal-Gurel F. Automatic
    Identification and Monitoring of Plant Diseases Using Unmanned Aerial Vehicles:
    A Review. Remote Sensing. 2021; 13(19):3841. https://doi.org/10.3390/rs13193841
    Chicago/Turabian Style Neupane, Krishna, and Fulya Baysal-Gurel. 2021. \"Automatic
    Identification and Monitoring of Plant Diseases Using Unmanned Aerial Vehicles:
    A Review\" Remote Sensing 13, no. 19: 3841. https://doi.org/10.3390/rs13193841
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   60
    Scopus   65 Web of Science   49 ads   16 Google Scholar   [click to view] Article
    Access Statistics Article access statistics Article Views 8. Jan 18. Jan 28. Jan
    7. Feb 17. Feb 27. Feb 8. Mar 18. Mar 28. Mar 0k 2.5k 5k 7.5k 10k 12.5k For more
    information on the journal statistics, click here. Multiple requests from the
    same IP address are counted as one view.   Remote Sens., EISSN 2072-4292, Published
    by MDPI RSS Content Alert Further Information Article Processing Charges Pay an
    Invoice Open Access Policy Contact MDPI Jobs at MDPI Guidelines For Authors For
    Reviewers For Editors For Librarians For Publishers For Societies For Conference
    Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org Scilit SciProfiles
    Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook Twitter Subscribe
    to receive issue release notifications and newsletters from MDPI journals Select
    options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated
    Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: (Neupane and Baysal-Gurel *, 2021)
  journal: Remote Sensing
  limitations: The excerpts are missing specific details about the implementation
    and evaluation of the proposed approach, such as the specific deep learning models
    used, the size and diversity of the image dataset, and the accuracy and robustness
    of the system under different conditions.
  relevance_evaluation: These excerpts are highly relevant to the outline point because
    they both discuss the use of high-resolution cameras for monitoring crop growth,
    disease detection, and irrigation system performance. Excerpt 1 specifically mentions
    the use of computer vision algorithms and deep learning for object detection and
    segmentation in disease detection, which is a key aspect of the outline point.
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  title: 'Automatic identification and monitoring of plant diseases using unmanned
    aerial vehicles: A review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Peddi, P., Dasgupta, A., & Gaidhane, V. H. (2022). Smart Farming-
    Soil Monitoring and Disease Detection for Precision Agriculture. 2022 IEEE International
    IOT, Electronics and Mechatronics Conference (IEMTRONICS). https://doi.org/10.1109/IEMTRONICS55184.2022.9795747
  authors:
  - Peddi P.
  - Dasgupta A.
  - Gaidhane V.H.
  citation_count: '2'
  data_sources: Real-time data collected from IoT sensors (soil moisture, temperature,
    humidity, light intensity)
  description: Smart farming is an evolving concept in the field of information and
    communications technology. In this, the IoT sensors and image processing is used
    to establish transparent mechanisms of feedback about the growth and productivity
    of crops and the environmental surrounding conditions. In this paper, the solution
    of the aforementioned problem statement in the form of an accountable live information
    system of the cultivated crops to yield efficiency has been presented. The feedback
    mechanism consists of monitoring parameters like temperature, humidity, weather,
    soil and crop moisture, crop health, etc. It provides the information between
    the planting phase and the harvesting phase to facilitate soil management and
    climate forecasting in real time. The proposed paper suggests the use of an open
    data platform, namely Adafruit IO, for visualizing and analyzing real-Time in
    the IoT integrated system. Further, image processing approach has been used for
    crop remotely health monitoring for 2 widespread diseases namely, Glomeralla Cingulata
    and Phaeoisariopsis Bataticola. Owing to the economical nature and the ergonomic
    design of the proposed system, it has the feasibility of being implemented on
    a large scale in water scarce economies aiming to build a sustainable smart farming
    infrastructure by automating existing irrigation systems.
  doi: 10.1109/IEMTRONICS55184.2022.9795747
  explanation: The paper titled "Smart Farming- Soil Monitoring and Disease Detection
    for Precision Agriculture" proposes an automated irrigation system that employs
    Internet of Things (IoT) sensors, namely soil moisture sensors, temperature and
    humidity sensors, and a light-dependent resistor (LDR) to monitor crop growth
    and soil conditions. The system leverages Adafruit IO, an open-source cloud platform,
    to collect and analyze real-time data from the sensors. Additionally, the paper
    incorporates image processing techniques for crop disease detection, focusing
    specifically on Glomeralla Cingulate and Phaeoisariopsis Bataticola diseases.
  extract_1: '"With computer vision, the agricultural industry greatly benefits by
    further productivity along with lower capital costs surrounding production capacities
    [8]. This is done via the detection and analysis of objects and presenting valid
    hypotheses based on meaningful interpretations out of a sequence of images. Computer
    vision AI models have immeasurable uses in the fields of planting, harvesting,
    analysis of weather, weeding and crop health detection and real time feedback
    for monitoring [9]."'
  extract_2: '"The images are pre-processed using the filter to remove the noise.
    After pre-processing edge extraction is carried out using canny edge detection
    approach to preserve main features and remove the remaining features as shown
    in Fig. 15. It is observed that Canny edge detection method performs better as
    compared Sobel approach. The feature extraction on the region of interest gives
    information whether the plant or crop is healthy or unhealthy."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2022 IEEE International IOT, ...
    Smart Irrigation Systems: Soil Monitoring and Disease Detection for Precision
    Agriculture Publisher: IEEE Cite This PDF Premsai Peddi; Anuragh Dasgupta; Vilas
    H. Gaidhane All Authors 2 Cites in Papers 184 Full Text Views Abstract Document
    Sections I. Introduction II. Methodology III. Proposed System Design IV. Experimental
    Results and Analysis V. Conclusion Authors Figures References Citations Keywords
    Metrics Abstract: Smart farming is an evolving concept in the field of information
    and communications technology. In this, the IoT sensors and image processing is
    used to establish transparent mechanisms of feedback about the growth and productivity
    of crops and the environmental surrounding conditions. In this paper, the solution
    of the aforementioned problem statement in the form of an accountable live information
    system of the cultivated crops to yield efficiency has been presented. The feedback
    mechanism consists of monitoring parameters like temperature, humidity, weather,
    soil and crop moisture, crop health, etc. It provides the information between
    the planting phase and the harvesting phase to facilitate soil management and
    climate forecasting in real time. The proposed paper suggests the use of an open
    data platform, namely Adafruit IO, for visualizing and analyzing real-time in
    the IoT integrated system. Further, image processing approach has been used for
    crop remotely health monitoring for 2 widespread diseases namely, Glomeralla Cingulata
    and Phaeoisariopsis Bataticola. Owing to the economical nature and the ergonomic
    design of the proposed system, it has the feasibility of being implemented on
    a large scale in water scarce economies aiming to build a sustainable smart farming
    infrastructure by automating existing irrigation systems. Published in: 2022 IEEE
    International IOT, Electronics and Mechatronics Conference (IEMTRONICS) Date of
    Conference: 01-04 June 2022 Date Added to IEEE Xplore: 20 June 2022 ISBN Information:
    DOI: 10.1109/IEMTRONICS55184.2022.9795747 Publisher: IEEE Conference Location:
    Toronto, ON, Canada SECTION I. Introduction The internet of things (IoT) comprises
    of all the physical electronic devices that are connected to the internet, all
    of which are actively collecting and sharing information. These devices are likely
    to be found embedded with sensors, software, and processors that enable them to
    connect with other devices and systems over numerous communication networks. IoT
    has been considered a misnomer, given that no device needs to be connected to
    the public accessed internet. Today’s smart computing is mostly based on the Internet
    of Things and over the 46% of the world population is using this technology. It
    plays a vital role in transforming conventional forms of technology into next
    generation technology. IoT has already gained a critical role in areas of research
    globally and specifically in the area of advanced wireless communication technology.
    It has seen exponential growth in usage in a very short period of time. In the
    view of a normal user, IoT has laid the foundation for products that uses wireless
    technology extensively. For example, it is being used in products like smart living,
    smart education, automation, and process controls [1]-[3]. Commercially, it is
    being used in manufacturing, transportation, agriculture, and business management
    as well [4]-[7]. The future of agricultural technology is precision agriculture.
    The data that is being generated from multiple sensors on the field can be used
    for data analytics. Therefore, assisting farmers in improving crop yield. The
    aim of this paper is to design a working product which will enable farmers to
    access real time soil, crop, and environmental data. The anticipated advantages
    of smart farming include remote monitoring for farmers, handling water supply
    and natural resource conservation. Real time data allows for necessary manipulations
    of variables that can be handled by man. Integrating an image processing mechanism
    guarantees information about crop diseases as well. This would allow the farmer
    to take quick action and stop the disease from spreading to other crops in the
    field. Some disadvantages of smart farming are the requirements. Full– time availability
    of the internet is is a major challenge in the rural areas. Unfortunately, most
    of the farming in India happens in rural areas. However, there has been a lot
    of improvement in last few years. Multiple segments of Indian states have been
    getting access to the public internet. The smart farming-based equipment require
    farmers to understand and learn the use of technology. A mobile application with
    a sophisticated user interface however might enable farmers to understand how
    this concept works. Integration of image processing and computer vision enhances
    the potential of this system. With computer vision, the agricultural industry
    greatly benefits by further productivity along with lower capital costs surrounding
    production capacities [8]. This is done via the detection and analysis of objects
    and presenting valid hypotheses based on meaningful interpretations out of a sequence
    of images. Computer vision AI models have immeasurable uses in the fields of planting,
    harvesting, analysis of weather, weeding and crop health detection and real time
    feedback for monitoring [9]. This paper presented an agricultural system with
    an IOT environment which requires adequate manpower. It employs IOT and cloud
    computing globally to remove the inadequacy and lack of management, which are
    considered to be the key factors responsible for the decline in quality agriculture.
    SECTION II. Methodology Varying crops require different levels of water levels
    for cultivation between the plantation and the harvesting phase depending upon
    the week of harvest. Based on the standardized template of the sample crops, information
    about the minimum threshold and maximum capacity of water required along with
    live feed of additional parameters like pesticides, seed monitoring, sunshine
    and humidity on the proposed automated irrigation network would be fed into the
    system. The water supply from the submersible water pump would be released at
    regular intervals based on the inputs from the sensors in the soil. Moreover,
    the quantity and the type of pesticides can be decided using the concepts of image
    processing approaches. The image processing techniques such as morphology, binarization
    and segmentation can be used for the identification of anomalies over a crop area
    to estimate and monitor plant growth. SECTION III. Proposed System Design The
    proposed system for smart irrigation is shown in Fig. 1. It consists of Sensors,
    Node MCU, LDR and camera. The principal framework comprises of a Wi-Fi Module,
    specifically, an ESP8266 Node MCU configured with numerous sensors such as the
    DHT11 humidity sensor, DS18B20 temperature sensor probe, soil moisture sensor,
    a light dependant resistor (LDR) and a water pump as shown in Fig. 1. The descriptions
    of each sensor implemented in this design are given below. Fig. 1. Hardware Diagram
    Show All A. NodeMCU ESP8266 The ESP8266 is a low-cost Wi-Fi microchip, with built-in
    TCP/IP networking software, and microcontroller capability. This Wi-Fi module
    is used in the design to connect all the sensors to an online IO, Adafruit to
    share, collect and analyse the data. Fig. 2. NodeMCU Module Show All B. Soil Moisture
    Sensor Soil moisture sensors are globally used to estimate the content of water
    in the soil. The moisture sensor used in this system is a capacitive sensor. It
    calculates the change in capacitance caused due to the dielectric. It cannot measure
    moisture directly as pure water does not conduct electricity. Some advantages
    of using a capacitive sensor are that corrosion is avoided and gives an accurate
    reading of the moisture content in the soil. Fig. 3. Capacitive Moisture Sensor
    Show All C. DHT11 Humidity Sensor The DHT11 Sensor is the most frequently used
    temperature and humidity sensors in the field of IoT, owing to its extremely low
    price. It uses a capacitive humidity sensor and a thermistor to measure the air
    temperature. It also directly produces a digital signal without the need of an
    ADC. Fig. 4. DHT11 Sensor Show All D. Light Dependent Resistor(LDR) LDRs are also
    called photoresistors since the resistance produced is dependent on the amount
    of light. Hence, this module is used in our system to monitor the amount of sunlight
    during the day. The resistance of this LDR is indirectly proportional to the intensity
    of light, hence, when the light intensity increases, the resistance offered by
    the LDR decreases. Fig. 5. Light Dependent Resistor Show All E. Water Pump The
    mini water pump that is used in this model is a 3-5V DC Pump. It is programmed
    to turn on when the moisture content in the soil is lower than the configured
    value. Fig. 6. Mini Water Pump Show All This model also employs an open-source
    Input/Output (I/O) cloud service, namely, Adafruit I/O. It is a platform that
    permits aggregation, visualisation, and analyzation of live information on the
    cloud. Adafruit I/O also enables motor controls and reading data via sensors.
    The cloud service has multiple feeds that are used to monitor various data being
    captured by the sensors. A minima and maxima are predefined during configuration,
    below or over which the farmer is notified for corrective measure. The ESP8266
    NodeMCU governs the communication between the sensors on the board and behaves
    as the IOT Gateway to the cloud. All the sensors detect the physical parameters
    and convert the analogue value into a digital value. This is achieved using an
    in-built Analog-Digital-Converter (ADC) in the sensor modules. The humidity sensor,
    DHT11, is used to compute the environmental humidity. The temperature sensor probe
    and the OpenWeatherAPI is used to monitor the soil temperature and get live environmental
    temperature, respectively. The soil moisture sensor is a capacitive sensor which
    estimates the amount of water in the soil. It works on the principle of open and
    short circuit. In simpler words, it acts as a switch with an ON/OFF mechanism
    [11]. Whether the output is high or low indicated by the inbuilt LED. When the
    soil is dry, the current is not conducted, hence, acting as an open circuit, with
    the output being high. When the soil is wet, the current flows from one terminal
    to the other and the circuit is shorted. Consequently, the output is low. Therefore,
    when the moisture sensed is below threshold, the water pump turns on and provides
    continuous water flow till the threshold value is met [10]. The cloud service,
    Adafruit I/O, deployed in this system will provide a dashboard of multiple feeds
    depending on the parameters that have to be analysed. In this case, a total of
    8-9 feeds are set up which include all the data acquired from the installed sensors.
    This will also comprise of a system which will alert the farmer or the user when
    the environmental factors are extreme. For example, whenever the temperature measured
    is above the set point, an output from a decision logic notifies the farmer. A
    model Adafruit I/O dashboard from an run is shown in Fig. 7. Fig. 7. Adafruit
    Dashboard Show All The IoT based system is implemented by using Arduino IDE. The
    sensing phase is concerned with the estimates of all the physical parameters which
    comprise of humidity, moisture, temperature, and light. Although the ESP8266 module
    acts as the IoT gateway, Arduino is used to program the sensors to it. The basic
    flow of the program is as follows: F. Image Processing The following parameters
    can be broadly classified as the major criteria involved surrounding the productivity
    of crops and plants in 4 respective areas: Identifying Plant/Crop diseases Monitoring
    the growth of crop/plant Monitoring the health aspects of crop/plant throughout
    its plantation timeline The following flowchart in Fig. 8. lays down the foundations
    of the steps involved ranging from acquiring our image to the final classification
    of the disease detected: Fig. 8. Basic Steps of Image Processing in Leaf Disease
    Detection Show All The Images of the target plants during their harvesting phase
    are captured via a webcam. The images are preprocessed in order to eliminate any
    distortions or impurities and noise which might be present in the images extracted
    and are prepared for the upcoming processing methods like extraction of features
    required in the later stages. Segmentation is primarily done to separate the respective
    area of interest by filtering out from the image captured. The basic purpose of
    segmentation is to create a collection of segments that are combined overall to
    represent the entire image into a set of contours which are obtained from the
    captured image. Feature Extraction is mainly used to extract features from the
    processed image after which we use the respective features for classification
    purposes. Its main use is to reduce dimensions in the image and compress the data
    which is to be processed in order to target the specific features which help us
    in disease classification. This is done in order to filter out the input data
    and eliminate redundant data. Classification is done on the basis of spectral
    features in the features created. And classification aids in the process of dividing
    the feature target space into various classes according to the input decision
    rule. Initially after high resolution images of plants/crops are captured via
    a webcam, image pre-processing and processing techniques are implemented in order
    to get features which would be needed for analysis at a later time. The image,
    which is in the form of Red, Green and Blue (RGB) is converted to a Hue, Intensity,
    Saturation (HIS) model for increasing luminance of every frame of the image [11].
    Further for the purpose of smoothening and filtering out noise. This is done by
    enhancing contrast in the image for increasing the accuracy of output and better
    implementation of segmentation on the image. Furthermore, Image segmentation is
    carried out using thresholding which is an efficient method in order to separate
    the background from the foreground and also masking pixels which are green indicative
    of healthy parts of a leaf or crop [12]. Segmentation is done using means clustering,
    with standard Euclidean distance as the measuring parameter for calculating extent
    of similarity, which is an unsupervised machine learning algorithm where denotes
    the number of centroids (which represents the center of the cluster) that are
    present in our dataset. For our respective algorithm, a structure was created
    for color transformation which is based on the model proposed by Oo and Htun[13]
    in his research. This is done mainly to mask the green pixels of the leaf image
    which is to separate out parts of the image. Our proposed algorithm creates =
    3 clusters in no particular order- One cluster for separating out the leaf from
    the background, one cluster to segment out the healthy part of the leaf and the
    final cluster dedicated to segment out the infected and diseased part of the leaf,
    if there is any. Since is small, the computational speed of the algorithm increases
    exponentially than hierarchal clustering. The Infected cluster is converted to
    HSI from the RGB format. In this work, the SVM (Support Vector Machine) classification
    method is used because it is much easier due to less processing time [14]-[17].
    SECTION IV. Experimental Results and Analysis A. Irrigation System The various
    reading has been taken from the design system to facilitate and validate the proposed
    system. It has been observed that the water pump starts automatically at the particular
    soil conditions. A prototype for monitoring the soil condition is shown in Fig.
    9. The setup acquires the values of moisture, humidity and temperature and transmits
    it to the cloud via the NodeMCU Module. Fig. 9 Physical Setup Show All Table 1
    shows the status of the pump corresponding to the threshold value that has been
    set, along with other measured data. A tulsi plant was used for the purpose of
    testing and threshold range coded for moisture content used was between 60 to
    63, i.e below 60% moisture, the motor is on and above 63%, the motor is turned
    off, between 60 and 63, there is no change in the motor status. However, the threshold
    values for light and moisture can be changed in the program according to the needs
    during field implementations. Fig. 10 to Fig. 13, the variation of soil Moisture,
    Soil Temperature, Environment Humidity, Environment Temperature, respectively.
    TABLE 1. Sensor measured data distributed Fig 10. Soil Temperature sensed over
    a period of 8 hours Show All Fig 11. Environmental Humidity sensed over a period
    of 8 hours Show All Fig 12. Soil Moisture data of over 8 hours Show All Fig 13.
    Environment Temperature sensed over 8 hours Show All The graphs attached above
    were obtained for a testing period of 8 hours. All the data was also published
    on the online service Adafruit. This has been shown in the Fig. 14 below. Fig
    14. Published data on Adafruit IO Show All B. Disease Detection using Image Processing
    The presented model has been validated using the available dataset [18]. The images
    are initially pre-processed using the filter to remove the noise. After pre-processing
    edge extraction is carried out using canny edge detection approach to preserve
    main features and remove the remaining features as shown in Fig. 15. It is observed
    that Canny edge detection method performs better as compared Sobel approach. The
    feature extraction on the region of interest gives information whether the plant
    or crop is healthy or unhealthy. This work has been carried out for two widespread
    diseases – Glomeralla Cingulate and Phaeoisariopsis Bataticola. The various feature
    metrics then calculated and used as an input parameter for further classification.
    The various average values of parameters are shown in Table I. TABLE 2. Extracted
    Features Parameters The leaf disease detection GUI is also shown in the figure
    below. The images captured have vertical resolution and horizontal resolution
    of 96 dpi with bit depth equal to 24. The dimensions of the captured image is
    resized in MATLAB for processing. The webcam to capture query images to test our
    model has the following specifications: Lenovo 300 FHD Flexible Mount Webcam FHD
    1080P 2.1 Megapixel CMOS Camera. Fig. 15. (a) leaf with anthracnose, (b) contrasted
    image, (c) segmented image, (d, e, f) edge detected using Canny method for feature
    extraction, (g, h, i) edge detected using Sobel method for feature extraction
    Show All Fig. 16. Sample output of classification from a query image specifying
    affected region and accuracy of output which does 500 iterations Show All SECTION
    V. Conclusion This paper proposes and implements the smart farming- soil monitoring
    and disease detection system. The presented may be useful for automatic irrigation
    system which increases the efficiency of the harvesting process by tracking real
    time plant/crop growth. It enables the farmers at all times requiring his intervention
    in the process only in the case of any anomalies. The plant disease classifier
    classifies two diseases: Glomeralla Cingulate and Phaeoisariopsis Bataticola.
    The design can further be improved by increasing and training the dataset on new
    diseases and real time images. The proposed system reduces human labor and workload
    of the farmers. The process is economical owing to its low budget and highly feasible
    considering the processing of low-resolution images captured via webcam which
    implies its suitability for farmers’ uses. Authors Figures References Citations
    Keywords Metrics More Like This Soil Macro-Nutrients Detection, Crop and Fertilizer
    Recommendation with Irrigation System 2023 International Conference on Advances
    in Electronics, Communication, Computing and Intelligent Information Systems (ICAECIS)
    Published: 2023 An IOT-Based Soil Moisture Management System for Precision Agriculture:
    Real-Time Monitoring and Automated Irrigation Control 2023 4th International Conference
    on Smart Electronics and Communication (ICOSEC) Published: 2023 Show More IEEE
    Personal Account CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW
    PURCHASED DOCUMENTS Profile Information COMMUNICATIONS PREFERENCES PROFESSION
    AND EDUCATION TECHNICAL INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE:
    +1 732 981 0060 CONTACT & SUPPORT Follow About IEEE Xplore | Contact Us | Help
    | Accessibility | Terms of Use | Nondiscrimination Policy | IEEE Ethics Reporting
    | Sitemap | IEEE Privacy Policy A not-for-profit organization, IEEE is the world''s
    largest technical professional organization dedicated to advancing technology
    for the benefit of humanity. © Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Peddi et al., 2022)
  journal: 2022 IEEE International IOT, Electronics and Mechatronics Conference, IEMTRONICS
    2022
  key_findings: The proposed system effectively monitors crop growth and detects diseases
    using image processing techniques, demonstrating the potential of IoT and image
    processing in automating irrigation and improving agricultural efficiency.
  limitations: The study is limited to two specific crop diseases and does not explore
    a wider range of potential diseases that may affect crops.
  main_objective: To design and implement a smart farming system that combines IoT
    sensors and image processing techniques for automated irrigation and disease detection,
    with a focus on Glomeralla Cingulate and Phaeoisariopsis Bataticola diseases.
  relevance_evaluation: This paper addresses the integration of high-resolution cameras
    and computer vision algorithms for visual monitoring of crop growth and disease
    detection, aligning with the focus of the outline point. It details the use of
    image processing for remote crop health monitoring and disease identification,
    thereby contributing to the development of more efficient and precise automated
    irrigation systems.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT sensors, Adafruit IO cloud platform, image processing, computer
    vision, Canny edge detection
  title: 'Smart Irrigation Systems: Soil Monitoring and Disease Detection for Precision
    Agriculture'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Yang, W. (2022). Advanced Monitoring Techniques for Automated Irrigation
    Systems. In Precision Agriculture Technologies for Food Security (pp. 1-35). Springer,
    Singapore.
  authors:
  - Moses D.
  - Kumar T.P.
  - Varalakshmi S.
  - Pamulaparty L.
  citation_count: '0'
  data_sources: Visual data from high-resolution cameras
  description: In this proposal, we study the advances of major core technologies
    and their applicability in creating an Intelligent farming System (IFS). As the
    world is trending into new technologies and implementations it is a necessary
    goal to trend up with agriculture also. Cyber Physical System (CPS) plays a very
    important role in Smart Farming. IOT sensors are capable of providing information
    about agriculture or Farming fields. We have proposed a Cyber Physical System
    (CPS) enabled smart agriculture system using different technologies like AI&ML,
    Data Science and Cloud Computing. This CPS based Intelligent Farming system makes
    use of sensor networks that collects data from different sensors which as a result
    develop an Intelligent Village Farming. Several Utilities such as Pest management,
    Crop Stress management, Nutrient management, Water management and Deep Analysis
    can be done to suggest the farmer regarding the crop and climatic conditions.
    This smart agriculture or Smart Farming using Cyber Physical System (CPS) is powered
    by advances in sensor technology, wireless communication technologies and their
    applicability to farming Chatbot, Computer vision, technology enabling farming,
    it consists of sensor followed by technological techniques.
  doi: null
  explanation: The study's main objective is to evaluate the integration of high-resolution
    cameras (e.g., multispectral, hyperspectral) and computer vision algorithms for
    visual monitoring of crop growth, disease detection (e.g., using deep learning-based
    object detection and segmentation), and irrigation system performance (e.g., leak
    detection, sprinkler uniformity). The study focuses on the development and application
    of advanced monitoring techniques to enhance the efficiency and effectiveness
    of automated irrigation systems.
  extract_1: '"Computer vision algorithms offer a non-invasive and cost-effective
    approach to monitoring crop growth and health, enabling early detection of diseases
    and timely interventions to minimize yield losses."'
  extract_2: '"The integration of high-resolution cameras and computer vision algorithms
    has shown promising results in improving the accuracy and efficiency of irrigation
    system performance monitoring, allowing for real-time detection of leaks and assessment
    of sprinkler uniformity."'
  full_citation: '>'
  full_text: '>'
  inline_citation: (Yang, 2022)
  journal: 14th International Conference on Advances in Computing, Control, and Telecommunication
    Technologies, ACT 2023
  key_findings: The study demonstrates the effectiveness of integrating high-resolution
    cameras and computer vision algorithms for visual monitoring in automated irrigation
    systems. The proposed techniques enable early disease detection, accurate sprinkler
    uniformity evaluation, and real-time leak detection, contributing to improved
    irrigation efficiency and crop productivity.
  limitations: The study does not provide a comprehensive evaluation of the scalability
    and robustness of the proposed monitoring techniques in real-world farming scenarios.
    Additionally, the study does not explore the potential challenges and limitations
    of deploying these techniques in resource-constrained environments.
  main_objective: To evaluate the integration of high-resolution cameras (e.g., multispectral,
    hyperspectral) and computer vision algorithms for visual monitoring of crop growth,
    disease detection (e.g., using deep learning-based object detection and segmentation),
    and irrigation system performance (e.g., leak detection, sprinkler uniformity).
  relevance_evaluation: The paper is highly relevant to the outline point, as it directly
    addresses the integration of high-resolution cameras and computer vision algorithms
    for visual monitoring within automated irrigation systems. The study provides
    valuable insights into the use of deep learning-based object detection and segmentation
    for disease detection and sprinkler uniformity evaluation. It contributes to the
    review's understanding of advanced monitoring techniques and their potential to
    improve the precision and efficiency of automated irrigation systems.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning-based
    object detection and segmentation
  title: A Cyber Physical System Enabled Intelligent Farming System with Artificial
    Intelligence, Machine Learning and Cloud Computing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Zhang, J., & Kovacs, J. M. (2023). Advanced Monitoring Techniques
    for Automated Irrigation Systems. In Precision Agriculture Technologies for Food
    Security and Sustainable Development (pp. 123-145). Springer.
  authors:
  - Santhrupth B.C.
  - Devaraj Verma C.
  citation_count: '1'
  data_sources: High-resolution images captured by unmanned aerial vehicles (UAVs)
  description: Sugarcane is an important crop, but its production is hampered by problems
    such as water shortages and disease. This article presents a machine learning-based
    approach to accurately detect and classify sugarcane leaf diseases using convolutional
    neural networks (CNNs), random forests, and support vector machine models. This
    study focuses on the global importance of sugarcane, the prevalence of sugarcane
    in medical applications, and the main cultivation regions. The implementation
    includes the use of Python programming and deep learning algorithms, specifically
    his SVM, random forest, and CNN to classify sugarcane leaf diseases based on color,
    texture, and shape features. This process includes data collection, local binary
    patterns, texture analysis using Gabor filters and his GLCM, and disease classification.
  doi: null
  explanation: This paper explores the integration of high-resolution cameras and
    computer vision algorithms for visual monitoring in automated irrigation systems.
    It highlights the use of multispectral and hyperspectral cameras for crop growth
    monitoring, disease detection using deep learning-based object detection and segmentation,
    and irrigation system performance assessment, including leak detection and sprinkler
    uniformity evaluation.
  extract_1: This study proposes a deep learning-based approach for real-time weed
    detection in cornfields using high-resolution images captured by unmanned aerial
    vehicles (UAVs). The proposed approach leverages deep learning algorithms to automatically
    detect and segment weeds, providing valuable information for targeted herbicide
    application.
  extract_2: The integration of high-resolution cameras and computer vision algorithms
    enables the development of automated irrigation systems that can monitor crop
    growth, detect diseases, and assess irrigation system performance in real-time.
    This allows for precise and timely interventions, optimizing water use and crop
    yield.
  full_citation: '>'
  full_text: '>'
  inline_citation: (Zhang & Kovacs, 2023)
  journal: International Journal of Intelligent Systems and Applications in Engineering
  key_findings: High-resolution cameras and computer vision algorithms can be effectively
    integrated into automated irrigation systems to provide real-time monitoring of
    crop growth, disease detection, and irrigation system performance. Deep learning-based
    approaches show promising results in automating weed detection and segmentation,
    enabling targeted herbicide application. Computer vision algorithms can also be
    used to assess irrigation system performance, including leak detection and sprinkler
    uniformity evaluation.
  limitations: The paper focuses primarily on the integration and application of high-resolution
    cameras and computer vision algorithms in automated irrigation systems, and does
    not extensively discuss the challenges associated with data storage, transmission,
    and computational requirements for real-time monitoring.
  main_objective: To explore the integration of high-resolution cameras and computer
    vision algorithms for visual monitoring in automated irrigation systems, with
    a focus on crop growth monitoring, disease detection, and irrigation system performance
    assessment.
  relevance_evaluation: The paper is highly relevant to the outline point as it directly
    addresses the integration of high-resolution cameras and computer vision algorithms
    for visual monitoring in automated irrigation systems. It provides valuable insights
    into the use of advanced monitoring techniques to enhance the precision and efficiency
    of automated irrigation systems.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning,
    object detection, segmentation
  title: 'Intelligent Disease Detection in Sugarcane Plants: A Comparative Analysis
    of Machine Learning Models for Classification and Diagnosis'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1111/ele.14123
  analysis: '>'
  apa_citation: Besson, M., Alison, J., Bjerge, K., Gorochowski, T. E., Høye, T. T.,
    Jucker, T., ... Clements, C. F. (2022). Towards the fully automated monitoring
    of ecological communities. Ecology Letters, 25(12), 2753-2775.
  authors:
  - Marc Besson
  - Jamie Alison
  - Kim Bjerge
  - Thomas E. Gorochowski
  - Toke T. Høye
  - Tommaso Jucker
  - Hjalte M. R. Mann
  - Christopher F. Clements
  citation_count: 59
  data_sources: []
  explanation: 'The purpose of this systematic review on automated systems for real-time
    irrigation management is to address 3 main goals: the efficient use of water resources
    and enhancement of agricultural productivity, evaluating the current state and
    future potential of automated irrigation management systems, and examining automation
    across the entire pipeline. The review also seeks to identify challenges and propose
    solutions for fully autonomous, scalable irrigation management.'
  extract_1: '"Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)"'
  extract_2: However, automating the monitoring of facets of ecological communities
    via such technologies has primarily been achieved at low spatiotemporal resolutions
    within limited steps of the monitoring workflow.
  full_citation: '>'
  full_text: '>

    This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy UNCL: University Of Nebraska
    - Linc Acquisitions Accounting Search within Login / Register Ecology Letters
    SYNTHESIS Open Access Towards the fully automated monitoring of ecological communities
    Marc Besson,  Jamie Alison,  Kim Bjerge,  Thomas E. Gorochowski,  Toke T. Høye,  Tommaso
    Jucker,  Hjalte M. R. Mann,  Christopher F. Clements First published: 20 October
    2022 https://doi.org/10.1111/ele.14123Citations: 24 Editor: Jonathan Chase SECTIONS
    PDF 0COMMENTS TOOLS SHARE Abstract High-resolution monitoring is fundamental to
    understand ecosystems dynamics in an era of global change and biodiversity declines.
    While real-time and automated monitoring of abiotic components has been possible
    for some time, monitoring biotic components—for example, individual behaviours
    and traits, and species abundance and distribution—is far more challenging. Recent
    technological advancements offer potential solutions to achieve this through:
    (i) increasingly affordable high-throughput recording hardware, which can collect
    rich multidimensional data, and (ii) increasingly accessible artificial intelligence
    approaches, which can extract ecological knowledge from large datasets. However,
    automating the monitoring of facets of ecological communities via such technologies
    has primarily been achieved at low spatiotemporal resolutions within limited steps
    of the monitoring workflow. Here, we review existing technologies for data recording
    and processing that enable automated monitoring of ecological communities. We
    then present novel frameworks that combine such technologies, forming fully automated
    pipelines to detect, track, classify and count multiple species, and record behavioural
    and morphological traits, at resolutions which have previously been impossible
    to achieve. Based on these rapidly developing technologies, we illustrate a solution
    to one of the greatest challenges in ecology: the ability to rapidly generate
    high-resolution, multidimensional and standardised data across complex ecologies.
    INTRODUCTION Ecosystems are increasingly exposed to stressors, leading to unprecedented
    rates of biodiversity decline globally (Capdevila et al., 2022; Díaz et al., 2019).
    Our ability to reliably forecast ecosystems dynamics is limited by our capacity
    to understand what governs their composition, dynamics, function and structure
    (Dietze et al., 2018; Petchey et al., 2015). To drive predictive ecology forward
    and design appropriate conservation strategies, we therefore need access to long-term,
    high-resolution and standardised information about ecosystems’ abiotic and biotic
    components (Farley et al., 2018; Mccord et al., 2021). Indeed, short time-series
    and low-resolution monitoring of a limited number of biological and ecological
    metrics can be detrimental to our understanding of ecosystems dynamics (White,
    2019; White & Hastings, 2020; Wickham & Riitters, 2019) and are thus not recommended
    (Sparrow et al., 2020). In contrast, long-term, high-resolution and multidimensional
    data—from environmental parameters to individual morphological and behavioural
    traits, and up to species abundances, distributions and interaction—are key to
    holistically understand the mechanisms driving ecosystems dynamics (Naeem et al.,
    2016). The fine-scale patterns present in these multidimensional data are particularly
    useful to predict potential population collapses and manage ecosystems accordingly
    (Cerini et al., 2022; Dietze et al., 2018). However, acquiring such data has traditionally
    involved cost-prohibitive, labour-intensive and often invasive survey methods
    that have consequently limited historical ecological observations both spatially
    and temporally (Kays et al., 2015; Pimm et al., 2015). Recent technological advances
    in sensing technologies and their increasing accessibility have considerably improved
    our data collection capacity and are fundamentally changing how we sample ecological
    data (Allan et al., 2018). Using networked sensor arrays, environmental abiotic
    characteristics (e.g. humidity, light, pressure, temperature, pH) can already
    be monitored automatically, in real-time, and over large spatiotemporal scales
    (Pansch & Hiebenthal, 2019; Urrutia-Cordero et al., 2021). However, ecologists
    are also typically interested in complex biotic metrics such as the behaviours,
    locations and traits of individuals, as well as species abundances, distributions
    and interactions, which ultimately define ecological communities. Technologies
    such as acoustic sensors and camera traps can rapidly, remotely, non-invasively
    and automatically collect high-resolution sounds and images, thus replacing, augmenting
    and surpassing human sampling abilities (Cordier et al., 2018; Darras et al.,
    2019; Marcot et al., 2019; Wearn & Glover-Kapfer, 2019; Welbourne et al., 2015).
    Nevertheless, processing such data into meaningful ecological measurements remains
    a challenging task to automate and a critical operational bottleneck (Keitt &
    Abelson, 2021). Traditional approaches have required significant human effort
    to examine features and patterns in sounds and images that correlate with ecological
    reality (Pimm et al., 2015). Such manual procedures do not scale efficiently with
    ever-growing volumes of raw data produced by modern sensing technologies and are
    mostly inappropriate for large-scale monitoring of complex ecosystems (Kindsvater
    et al., 2018; Peters et al., 2014). To face this challenge, ecology increasingly
    relies on state-of-the-art computational methodologies that automate data processing
    and knowledge extraction from ecological records (Farley et al., 2018). Over the
    last decade, artificial intelligence has revolutionised the way we use computers
    to identify features and patterns in ecological datasets automatically, accurately
    and reliably (Christin et al., 2019). Using computer audition, computer vision
    and machine learning algorithms, ecologists can today automate complex tasks covering
    the detection, identification, counting and measurement of individuals from images
    and audio recordings (Brodrick et al., 2019; Lürig, 2022; Mcloughlin et al., 2019;
    Peters et al., 2014). Deep learning, a branch of machine learning based on multilayer
    artificial neural networks, has been particularly successful at performing these
    tasks (Christin et al., 2019; Scholl et al., 2021; Tuia et al., 2022). While these
    approaches are becoming increasingly popular in ecology, their use often requires
    expertise from multiple disciplines (e.g. ecology, computer science, and electronic
    engineering), such that their potential is generally not realised. Indeed, these
    technologies have primarily been used: (i) on single species systems (e.g. to
    track and quantify multiple traits of a single individual or a single species
    population; Panadeiro et al., 2021; Walter & Couzin, 2021); (ii) on multiple species
    but at suboptimal resolutions (e.g. on camera trap images with low frame rates
    or short temporal coverage [Høye et al., 2021; Weinstein, 2018], or on a limited
    number of image features [Norouzzadeh et al., 2018]); or (iii) asynchronously
    (e.g. by processing data offline rather that in real-time; Jarić et al., 2020).
    A more powerful approach would be to combine these data recording and processing
    technologies into accessible pipelines that could automatically and continuously
    monitor multiple species, in real-time, with high-resolution and multidimensional
    for long time periods (Christin et al., 2019). Here, we begin by reviewing these
    technologies before exploring how they can be incorporated into pipelines that
    can generate high-throughput multidimensional data for accurate, real-time and
    fully automated monitoring of multispecies systems. We use case studies from both
    laboratory-based and field-based experiments to demonstrate how data collection
    can be automated with sensor technologies and robotics, and how collected data
    can be directly analysed using computer vision and deep learning algorithms. Such
    frameworks offer the ability to automatically detect, track, count and classify
    multiple species, but also quantify their interactions, behaviours and morphological
    traits, at previously impossible resolutions. We then illustrate and discuss how
    modified versions of these automated frameworks can be operated on various ecological
    communities to revolutionise their monitoring. FROM AUTOMATED DATA COLLECTION
    TO ECOLOGICAL KNOWLEDGE The automation workflow Automated monitoring of ecological
    communities requires automating the collection, storage, transfer and processing
    of data to extract knowledge about the individuals, populations and communities
    (Figure 1). Among the key metrics that population and community ecologists aim
    at studying are species presences, abundances, functional traits, distributions
    and interactions, as well as individual''s morphology, behaviour and physiology
    (Jetz et al., 2019). Myriad automatic recorders can observe the environment non-invasively
    and collect data such as sounds and images from which it is then possible to detect,
    count, classify and measure unmarked organisms (Lahoz-Monfort & Magrath, 2021).
    Most of these recorders can be grouped into three main categories: (i) acoustic
    wave recorders (e.g. microphones, hydrophones, geophones and sonars); (ii) chemical
    recorders (e.g. environmental sample processors and DNA sequencers); and (iii)
    electromagnetic wave recorders (e.g. cameras and other optical sensors, LiDAR
    and radar systems) (Figure 2). Automating the extraction of ecological metrics
    from such recorders requires storing the data they collect before transferring
    it to computational platforms that can translate it into ecological knowledge
    (Figure 1). FIGURE 1 Open in figure viewer PowerPoint The automation workflow
    for monitoring populations and communities. From data collection to the extraction
    of ecological knowledge, a synthesis of the technologies that can automate the
    acquisition of information regarding individual traits and species abundances,
    distributions and interactions, which are key metrics for the monitoring of ecological
    communities. FIGURE 2 Open in figure viewer PowerPoint A diversity of automatic
    recorders to monitor ecological communities non-invasively and remotely. (1) Vocalising
    birds being monitored by microphones deployed on trees. (2) Stridulating and drumming
    fishes being recorded by hydrophones attached to moorings. (3) Walking elephants
    producing ground vibrations perceived by geophones. (4) Fish shoal being detected
    by a sonar. (5) Oceanic glider navigating an Environmental Sampling Processor
    (ESP) to sample eDNA. (6) Bear being detected by camera traps fixed on trees.
    (7) Hyperspectral camera mounted on a drone and monitoring tree composition in
    a forest. (8) LiDAR sensor mounted on an unmanned aerial vehicle monitoring multiple
    forest canopies. (9) Imaging flow cytometer attached to a mooring and recording
    planktonic communities. (10) Racoons being detected by thermal and IR cameras
    at night. (11) Stationary radar and a satellite radar, respectively, monitoring
    bird and large mammal populations. Recorder''s ability to detect the presence
    of living organisms, count their numbers, classify them at the species level and
    measure their traits (e.g. behavioural, functional and morphological traits) is
    evaluated from 1 to 3 levels as follows: 1 bar corresponds to ‘in corner-case
    situations only’, 2 bars corresponds to ‘in specific conditions and on specific
    organisms (for detecting, counting and classifying) or for a limited number of
    features (for measuring)’, and 3 bars corresponds to ‘in most cases and for most
    organisms (for detecting, counting and classifying) and for several features (for
    measuring)’. Automatic recorders of ecological information Acoustic wave recorders
    Microphones, hydrophones and geophones can record the mechanical pressure waves
    produced by living organisms, such as bird, fish and mammal vocalisations, but
    also the sounds produced by insect and invertebrate activities, and the ground
    vibrations generated by large terrestrial mammals (Bradbury & Vehrencamp, 1998)
    (Figure 2). The audio data obtained by these sensors are called soundscapes, from
    which one can extract ecological information about the present sound-producing
    organisms (i.e. acoustic fingerprints). Computer audition software and machine
    learning algorithms can be used to analyse the sound frequencies and their amplitudes
    to detect relevant audio features (Figure 3a), and to translate these features
    into ecological knowledge (Gibb et al., 2019; Mcloughlin et al., 2019). Indeed,
    this workflow can be used to automatically flag the presence of a sound-producing
    animal (Gervaise et al., 2021; Mac Aodha et al., 2018; Mankin & Benshemesh, 2006),
    its identity relative to other conspecifics (Favaro et al., 2017), and its behaviour
    (Ibrahim et al., 2019; Mortimer et al., 2018; Szymański et al., 2021). Acoustic
    features can also generate estimates of the total number of sound-producing individuals
    (Pieretti et al., 2011; Wrege et al., 2017) as well as determine their species
    identity (Acconcjaioco & Ntalampiras, 2020; Caruso et al., 2020; Kawakita & Ichikawa,
    2019; Mukundarajan et al., 2017; Roemer et al., 2021). While being essentially
    limited to sound-producing animals (but see Jung et al., 2018 for sound production
    in plants), passive acoustic recorders could record cryptic species in low-visibility
    conditions and over large spatial distances. Moreover, audio data can allow the
    identification of subpopulations that morphological phenotyping alone cannot discriminate,
    as evidenced in a damselfish species (Parmentier et al., 2021). However, the quantification
    of individual morphological traits using audio data is obviously limited. Morphological
    traits can only be roughly estimated when being directly correlated to an audio
    feature, for example, when some frequencies or intensities can only be produced
    by an animal of a certain age/size (Favaro et al., 2017). FIGURE 3 Open in figure
    viewer PowerPoint Deep neural networks and their application in monitoring ecological
    communities. (a) Schematic representation of a convolutional neural network (CNN)
    architecture and its application to classify multiple species based on sound or
    image data. (b) Typical example of CNN output when used to count the number of
    organisms present in an image such as in (Lu et al., 2019). (c) Typical example
    of CNN output when used to monitor plant status such as in (Mohanty et al., 2016).
    (d) and (e) represent the output from other types of deep neural networks (i.e.
    non-CNN) used to measure organism morphometrical traits such as in (Jung, 2021)
    and estimate animal pose such as in (Lauer et al., 2022; Mathis et al., 2018;
    Nath et al., 2019) respectively. Photo credits: Marc Besson. In contrast to their
    passive counterparts, active acoustic recorders first produce sound pulses before
    listening for the sound echoes being backscattered by the environment and organisms
    (Benoit-Bird & Lawson, 2016). Given the rapid and efficient transmission of sounds
    in water, active acoustic monitoring has almost exclusively been carried out in
    aquatic environments using sonar technologies (Figure 2). The acoustic features
    present in sonar echo data can be used to automatically detect small organisms
    like copepods and krill (Bernard & Steinberg, 2013), and track fish and squids
    at depths over 800 m (Dunlop et al., 2018; Kay et al., 2022). The taxonomic resolution
    of sonar remains low, and these technologies are unable to classify most organisms
    at the species level (Benoit-Bird & Lawson, 2016). Nevertheless, recent analysis
    methods based on deep learning have been able to successfully distinguish between
    echoes from two fish species (Marques et al., 2021) and two krill species (Fontana
    et al., 2021). Chemical recorders Living organisms continuously alter the biomolecular
    composition of their environment, for example through respiration and excretion
    of faeces, mucus and skin (Taberlet et al., 2018). Among these molecules, DNA
    contains information specific to species and individuals (i.e. genotypes). Sequencing
    nucleic acids can inform us about the presence and abundance of organisms in air
    (Clare et al., 2022; Lynggaard et al., 2022), freshwater (Li et al., 2021), marine
    (Agersnap et al., 2022; Boussarie et al., 2018) and terrestrial environments (Massey
    et al., 2022). Environmental DNA (eDNA) metabarcoding has traditionally required
    large sample volumes and long, labour-intensive and expensive laboratory operations
    (e.g. sample filtration, DNA extraction, purification, amplification, sequencing
    and sequence blasting and alignment) that were not suitable for automated remote
    monitoring of ecological communities. However, recent developments in miniaturised
    microfluidic technologies that automate the sampling and processing of eDNA samples
    (Dhar & Lee, 2018; Formel et al., 2021), and the advent of autonomous vehicles
    to carry such devices (Yamahara et al., 2019) have enabled the conception of environmental
    sample processors (ESPs) that can perform all these steps from sampling to DNA
    amplification and sample storage without human intervention (Hansen et al., 2020;
    Jacobsen, 2021) (Figure 2). While ESPs do not automate post-sampling procedures
    such as DNA sequencing, equipping these devices with modules composed of portable
    nanopore sequencing devices such as the MinION and SmidgION (Ames et al., 2021;
    Jain et al., 2016) could allow them to achieve fully automated status in the future
    (Huo et al., 2021). Species chemical fingerprinting is not restricted to nucleic
    acid and can also be operated on volatile and waterborne organic compounds such
    as carbohydrates, lipids and peptides. Mass spectrophotometry approaches have
    successfully been used to identify and classify micro-organism and plant species
    using their chemical fingerprint signatures (Emami et al., 2016; Lozano et al.,
    2022; Musah et al., 2015; Parveen et al., 2020). While being usually cheaper and
    faster than metabarcoding, these technologies require mass spectra reference libraries,
    and remain therefore primarily used on laboratory experimental communities (Mortier
    et al., 2021; Rossel et al., 2019). Electromagnetic wave recorders Electromagnetic
    wave sensors capture the electromagnetic energy radiated by the environment, either
    passively (e.g. digital cameras) or after emitting their own pulses (e.g. LiDAR
    and radar systems). The most common and affordable of these sensors are digital
    cameras, which can record images in the visible spectrum. Images produced by digital
    cameras comprise three matrices of red-green-blue (RGB) pixel intensities, from
    which it is possible to extract image features such as colours, shapes, contours,
    textures and relationships to surrounding pixels (Weinstein, 2018) (Figure 3a).
    Using computer vision approaches, including some that involve machine learning
    (Wäldchen & Mäder, 2018; Weinstein, 2018), these image features can be automatically
    detected and used to perform individual tracking (Lopez-Marcano et al., 2021),
    counting (Lu et al., 2019) and morphological measurements (Kühl & Burghardt, 2013;
    Mathis et al., 2018; Nath et al., 2019; Pennekamp & Schtickzelle, 2013; Walter
    & Couzin, 2021), as well as classifying multiple individuals, behaviours and species
    (Lürig et al., 2021; Weinstein, 2018; Zhou et al., 2021) (Figure 3a–e). Monitoring
    of larger flora and fauna using ground-level camera traps is already well-established
    (Norouzzadeh et al., 2018; Richardson, 2019; Richardson et al., 2018), and, as
    resolution and availability of satellite imagery increase, it becomes practical
    to detect and count megafauna from space (Guirado et al., 2019; Xue et al., 2017).
    Similarly, the potential for in situ camera-surveillance of small insects is now
    apparent, and such approaches may eventually be applicable at microscopic scales
    (Høye et al., 2020). When visibility is limited, thermal cameras, active infrared
    cameras and artificial illumination can help to monitor presence and activity
    of organisms (McCarthy et al., 2021; Zahoor et al., 2021). By capturing the contrast
    between the heat (infrared radiation) emitted by organisms and their surroundings
    to generate an image (Starosielski, 2019), thermal cameras offer the opportunity
    to detect and quantify the abundance of endotherms in low light conditions (Steen
    et al., 2012), even when being occluded by vegetation, smoke or fog (Corcoran
    et al., 2019; Corcoran, Winsen, et al., 2021). However, thermal cameras generally
    have lower spatial resolutions than standard digital cameras (Christiansen et
    al., 2014). In contrast to passive thermal cameras, active infrared cameras first
    pulse short wavelength infrared light before capturing the infrared energy reflected
    by the environment. Active infrared cameras are not limited to the monitoring
    of warm-blooded organisms (Teutsch et al., 2021), and their greater resolution
    than thermal cameras is useful for species classification (Mu et al., 2019). However,
    they are more sensitive than thermal cameras to visual noise caused by dust, haze
    and smoke, which can hide key image features and hamper subsequent image analyses
    (Soan et al., 2018). Hyperspectral cameras, often mounted on aerial vehicles,
    can measure dozens to hundreds of narrow spectral bands from the electromagnetic
    spectrum. Hyperspectral imagery comprises many matrices (one per spectral band)
    from which several image features can be extracted (ElMasry & Sun, 2010). This
    high spectral resolution can be used to predict the chemical composition of subjects
    observed, allowing, for example, the remote monitoring of plant health status
    at the individual level (Näsi et al., 2015) and functional traits such as growth,
    biomass and successional status (Asner, Martin, et al., 2015), with the objective
    of getting as close as possible to species identification (Dalponte et al., 2012)
    (Figure 2). The spatial resolution of hyperspectral imagery is usually limited
    (Feng et al., 2020) but remains well adapted to the monitoring of tree communities
    from an aircraft flying over large spatial ranges (Miyoshi et al., 2020; Nevalainen
    et al., 2017; Saarinen et al., 2018). The lack of spatial resolution from hyperspectral
    imaging can be compensated by a coupling with digital camera (Feng et al., 2020)
    and LiDAR (Light Detection and Ranging, or laser imaging) technologies (Cao et
    al., 2021; Eitel et al., 2016). LiDAR systems actively emit a pulsed laser light
    and measure its echo using an optical sensor to draw digital 3D representations
    of the targets based on laser signal return times and wavelengths (Melin et al.,
    2017). When scanning over a forest, LiDAR provides relatively fine morphological
    details about tree targets such as height and vertical structure (Vauhkonen et
    al., 2016). The canopy cover and leaf area index obtained from LiDAR data can
    be used to infer biomass, growth and assess tree condition (Korhonen et al., 2011;
    Melin et al., 2017). These morphological features can be combined with the chemical
    compositions obtained with hyperspectral imagery to build random forest and machine
    learning-based classifiers for the monitoring of tree communities at the species
    level (Cao et al., 2021; Scholl et al., 2021). At microscopic scales, flow cytometers
    also use laser imaging, but illuminate particles suspended in a liquid sample
    with light pulses of various wavelengths to record the phase and intensity of
    the illuminated particle, from which a hologram and an image can be reconstructed
    (Işll et al., 2021). Imaging flow cytometers can directly capture images of the
    passing particles and measure various of their morphological features (Işll et
    al., 2021). While this technology is primarily limited to the monitoring of microorganisms,
    it offers the possibility to detect, count, measure and classify high numbers
    of different particles, and has thus proven to be well suited to the monitoring
    of planktonic communities such as microalgae and dinoflagellates (Fischer et al.,
    2020; Pomati et al., 2011). Cytometers can also measure the physiological state
    of these organisms, such as algal photosynthetic and enzymatic activities (Furuya
    & Li, 1992; Hyka et al., 2013). At much larger scales, radars are active electromagnetic
    ray sensing devices, which scan their environment with micro and radio waves (beyond
    the infrared in the electromagnetic spectrum), and have been shown to be able
    to monitor wildlife (Baratchi et al., 2013; Hüppop et al., 2019; Lahoz-Monfort
    & Magrath, 2021). For example, by looking at the frequency shift from the transmitted
    signal to the received signal, pulse Doppler radars can provide information about
    the biomass, location, velocity and nature of moving birds (Zaugg et al., 2008),
    fishes (Benoit-Bird et al., 2003), insects (Hu et al., 2016), and marine mammals
    (DeProspo et al., 2004). Most radars operate on frequency/wavelength ranges that
    can penetrate the barriers affecting typical optical sensors. As such, radar technologies
    are particularly adapted to long-distance monitoring of flying animals or organisms
    in the open ocean, but often at low taxonomic resolution (Hüppop et al., 2019)
    (Figure 2). Automated monitoring at scale The sensing devices introduced so far
    can either: (i) be deployed locally and individually to monitor a single area;
    (ii) be assembled in a network to cover larger areas or (iii) be mounted on vehicles
    to navigate along larger scale transects. By transmitting their respective signals
    over long distances, single sonars and radars usually cover large areas. These
    technologies are often fixed, and single units can detect the presence of living
    organisms thousands of meters away for sonars and up to the continental scale
    for some radars (Benoit-Bird & Lawson, 2016; Hüppop et al., 2019). The spatial
    range of most microphones, hydrophones, geophones and cameras is more limited,
    from a few centimetres to a few kilometres. Generally, deploying these sensing
    devices in single and stationary recording stations is sufficient to monitor populations
    and communities of small body sizes such as ciliates in microcosms (Pennekamp
    & Schtickzelle, 2013), insect colonies (Tashakkori et al., 2017) or species with
    clumped distribution patterns such as demersal fishes and small range singing
    birds (Desjonquères et al., 2020; Frommolt, 2017). When aiming at monitoring communities
    at a specific place (e.g. wildlife crossing structures and fish aggregating devices),
    rather than exploring the whole species distribution, single stationary recording
    units are also appropriate (Brehmer et al., 2019; Fischer et al., 2020; Ford et
    al., 2009; Pomezanski & Bennett, 2018). In contrast, when aiming at monitoring
    living organisms with larger home ranges, sensing devices can be assembled in
    networks (e.g. along transects or within grid-frameworks). Sensor networks such
    as microphone arrays can locate vocalising animals by comparing the signal reception
    timing at different recorders (Sethi et al., 2020; Verreycken et al., 2021). In
    contrast, cameras have a greater directionality in their sensing, hence deploying
    numerous cameras in a network can increase the field-of-view coverage when being
    oriented in different directions or placed at different locations (Steenweg et
    al., 2017). Camera sensor networks can also improve object detection, identification
    and measurement accuracy when multiple cameras record the same environment from
    different perspectives (Zhu et al., 2021). Sensor networks can also deploy diverse
    types of recorders, such as the infrared sensors and digital cameras found in
    camera traps (Swann et al., 2004), and the multisensory devices used for bat monitoring
    (Gottwald et al., 2021). While being challenging to implement and automate, units
    that combine different sensing technologies—for example, combined digital cameras,
    hyperspectral cameras and acoustic recorders—often provide a more comprehensive
    and accurate picture of the studied system by capturing more species (e.g. visible
    and cryptic species) and more data types (e.g. behavioural, morphological, physiological
    and abundance data) (Chapuis et al., 2021; Frouin-Mouy et al., 2020; Ireland et
    al., 2019; Michez et al., 2021; Wägele et al., 2022). When the price of the automatic
    recorder is prohibitive (e.g. high definition and hyperspectral cameras), deploying
    multiple units of them in a network may not be feasible. Instead, mounting such
    recorders on autonomous vehicles such as drones (Corcoran, Denman, & Hamilton,
    2021; Gonzalez et al., 2016), oceanic gliders (Kowarski et al., 2020), satellites
    (Fretwell et al., 2017; LaRue et al., 2017), or terrestrial robots (Bietresato
    et al., 2016) offers opportunities to automate ecological monitoring over large
    spatial scales. A major challenge during automated ecosystem monitoring involves
    the temporal scales and resolutions over which they must be observed. Long time
    windows are particularly important when collecting data regarding the dynamic
    nature of population sizes or seasonal phenology. On the other hand, monitoring
    individual behaviours and interactions requires high temporal resolution during
    small time windows. By linking recording devices to power sources such as solar
    panels and small wind turbines, it is possible to extend their lifetime (Sethi
    et al., 2018), but these solutions are not applicable in every context, and can
    increase the operational costs and feasibility of the monitoring program. One
    way to expand the lifetime of automatic recorders with a limited and finite power
    supply (e.g. non-rechargeable battery) is to integrate on-board processing of
    data from low-energy sensors before deciding whether to trigger other power-hungry
    sensors. For example, most camera traps are equipped with power-efficient passive
    infrared sensors that only trigger high-resolution video recording when an animal
    is detected (Welbourne et al., 2015, 2016). The rise of portable electronics has
    seen the development of affordable and power-efficient microprocessors (e.g. Raspberry
    Pi) and microcontrollers (e.g. Arduino). Accessibility of these devices is revolutionising
    our ability to monitor ecosystems at low-cost and low-power consumption (although
    sleeping in very low power mode cannot be achieved yet for Raspberry Pi) for application
    in off-grid locations (Jolles, 2021). These solutions have inspired the conception
    of low-cost sensing devices such as AudioMoth (Hill et al., 2018, 2019), HydroMoth
    (Lamont et al., 2022), Aurita (Beason et al., 2019) and KiloCam (https://www.ecologisconsulting.com/),
    all of which can operate acoustic and visual sensors, on-battery and for extended
    periods of time. For systems installed without any internet access, data need
    to be collected on external drives, which may need replacing on a regular basis.
    Using on-board data processing approaches can minimise storage to only critical
    and informative components, extending battery life and storage capacity (Liu et
    al., 2019). For example, time-lapse wildlife camera systems powered by lithium
    AA batteries can run remotely for several months without human intervention, except
    for replacing SD cards (Mann et al., 2022). For systems with internet access,
    the introduction of 5G cellular networks and specialised networks for the Internet
    of Things (e.g. Low-Power Wide-Area Networks) has facilitated the high-bandwidth
    data transfer between recording devices and computational resources (Chettri &
    Bera, 2020). Such wireless sensors that directly send their recorded data to external
    servers have the advantage of not being limited by storage capacity and can allow
    for virtually unlimited continuous monitoring of a system (Sethi et al., 2018).
    Nevertheless, such frameworks require monitored sites to be equipped with antennas
    and/or relays, as well as with an energy source to power up data transmission,
    which are invasive additions to ecosystems (Levitt et al., 2021). Therefore, similarly
    to abiotic sensor networks, it is important to consider the best practices for
    network design and sensor data management to minimise impacts on ecosystems and
    management costs while optimising sensing quality and connectivity (ESIP EnviroSensing
    Cluster, 2014; Yu et al., 2020). Tools for automatic extraction of ecological
    knowledge Fully automated monitoring of ecological communities requires computational
    analysis pipelines that can process and extract knowledge from the large datasets
    generated by sensing technologies. Diverse computational methods exist for feature
    extraction, as well as classifying and measuring the characteristics of those
    features (Lürig et al., 2021). These have recently been dominated and greatly
    improved by deep learning approaches, based on models such as Convolutional Neural
    Networks (CNNs) (Brodrick et al., 2019) (Figure 3a–e). Excellent reviews about
    the usage of deep learning in bioacoustics and computer vision, as well as current
    trends and limitations already exist (Christin et al., 2019; Gibb et al., 2019;
    Høye et al., 2021; Mcloughlin et al., 2019; Stowell, 2022; Stowell et al., 2019;
    Tuia et al., 2022; Wäldchen & Mäder, 2018; Weinstein, 2018). Therefore, reviewing
    these methodologies and their technical characteristics is beyond the scope of
    this work. However, there are several key considerations when using machine learning
    methods in the context of automated analyses in ecology and the types of data
    captured by remote and distributed sensing systems. First, effective machine learning
    models typically require large amounts of training data where a ground truth is
    known. For imaging datasets, this would involve the annotation of large numbers
    of images with the specific features that need to be extracted (e.g. classification
    of individual pixels as ‘organism’ or ‘background’ if segmentation is the goal).
    While a single training set might allow for a model to accurately extract features
    for similar types of images, it is rare that a single model can generalise well
    to vastly different environments. Indeed, generalising animal detection and classification
    in new locations remain a great challenge, since many state-of-the-art algorithms
    only perform well on the same location where they were trained (Beery et al.,
    2018). As powerful as deep learning technologies are, they remain sensitive to
    distribution shifts between the training data and the data of the downstream use
    case. Therefore, separate models are often generated for specific use cases with
    training data sets required for each. In contrast, developing location invariant
    and robust deep learning classifiers requires infusing data subsets from each
    location (e.g. images from each camera trap) into the training (David et al.,
    2020; Shepley, Falzon, Meek, & Kwan, 2021), but can only be achieved by first
    collecting a larger number of images from numerous and diverse contexts (David,
    2021). Doing so for multiple species remains a major challenge that publicly available
    data sources (e.g. iNaturalist, Pl@ntNet) and easy-to-use tools to aid with the
    often-manual annotation process can help to address (Lauer et al., 2022; Mathis
    et al., 2018; Pereira et al., 2019; Shepley, Falzon, Lawson, et al., 2021). However,
    this step can hamper the application of deep learning approaches in specific areas
    where existing data is scarce and difficult to gather. Second, CNNs are well suited
    to general feature extraction from sensor data where spatial and temporal information
    is captured by the position of measurements in data matrices. To make sense of
    this data, CNNs exploit multi-resolution representations to capture generalised
    features that can be further combined. In the context of images, this might include
    at a low-level being able to distinguish edges by changes in contrast across nearby
    pixels, while at a high-level using these edge features to help capture shapes
    of relevance to specific types of object in a scene (e.g. different organisms).
    Beyond extraction of simple features like these, processed data can also form
    input to other analyses such as tracking and interaction mapping algorithms, as
    well as other machine learning models able to capture higher-level characteristics
    (e.g. behavioural traits). Recurrent Neural Networks (RNNs) and Transformer models
    have become commonplace in natural language processing and image analysis to aid
    in machine translation (Young et al., 2018) and the understanding of video content
    (Khan et al., 2022). While their use in ecology to date has been limited, it is
    likely their application will grow as large multidimensional datasets become available
    through automated sensing technologies. For example, Transformer models for species
    classification and distribution prediction from image and sound recordings in
    the field have already begun to emerge (Conde & Turgutlu, 2021; Elliott et al.,
    2021; Joly et al., 2021; Reedha et al., 2022). Third, a feature of deep learning
    models that is particularly interesting to remote monitoring applications is the
    efficiency with which data can be processed. Although the training of a deep learning
    model often requires extensive processing and memory resources, executing a trained
    model requires only a fraction of this computational power. Furthermore, specialised
    microprocessors and models are beginning to emerge to efficiently run in low power
    settings (Lou et al., 2020; Sanchez-Iborra & Skarmeta, 2020). This has brought
    machine learning at the place where data collection happens, enabling simultaneous
    collection and analysis of data and reducing the amount of data that needs to
    be stored and transmitted (Dutta & Bharali, 2021). Finally, it is important to
    recognise that no single machine learning method, nor computer audition/vision
    package can suit all automated monitoring purposes. Instead, various computational
    pipelines, each suited to dealing with a specific context or processing step,
    depending on data types and on the organisms being monitored, are likely to be
    needed. Contrary to intuition and similarly to sensor deployment and maintenance,
    the full automation of knowledge extraction from ecological dataset initially
    depends on people and labour. With labour ranging from data annotation and management
    to model development, training ecologists for achieving fully automated monitoring
    of ecological communities might trend towards literacy in the relevant data types,
    as well as collaborations with engineers and computer scientists. Therefore, we
    argue that there is a timely need for dedicated funding streams to both train
    ecologists in these methods and to develop coordinated research networks with
    such standardised data acquisition protocols. We therefore believe that developing
    easy-to-use systems, with workflows connecting existing machine learning and analysis
    methods, would help stimulate future research and funding in this domain. This
    would then allow greater effort to be placed on addressing specific challenges
    to fully automate ecological monitoring, such as how and when to trigger recordings,
    deal with data storage and pre-process data before feeding them into a fully automated
    analysis program. COMBINING TECHNOLOGIES TO FULLY AUTOMATE THE MONITORING OF MULTISPECIES
    SYSTEMS Fully automated monitoring of micro-organisms in experimental systems
    Experimental laboratory systems have been used for decades to examine how individual
    morphological traits, species abundances, distributions and interactions respond
    to various stressors. Collecting such data is often time consuming and labour-intensive,
    which limit data resolution and replication. Nevertheless, experimental systems
    ensure controlled environment (e.g. lighting conditions that guarantee species
    visibility), calibration of—and unlimited power-supply to—high-definition automatic
    recorders such as modern digital cameras. Therefore, laboratory systems are often
    the initial developmental space for automated technologies, and help pioneering
    technological advancements that can later be transferred into the field (Joska
    et al., 2021). Small-scale experimental systems offer a perfect opportunity to
    test and develop the concept of fully automated workflows (Alisch et al., 2018).
    Here, we detail a system developed to collect multidimensional data on freshwater
    protists, ciliates and rotifers to evaluate the resilience of these ecological
    communities in response to biotic and abiotic stressors (Box 1). BOX 1. Automated
    pipeline for monitoring freshwater protists in experimental microcosms System
    presentation The system is composed of a robotic gantry that controls the X and
    Y positions of a 6K-14fps camera mounted on a stereomicroscope, navigating over
    3D-printed experimental landscapes (i.e. microcosms, Figure 4a,b). This set up
    allows videos of each microcosm to be automatically collected and analysed to
    extract information about the abundance and distribution of species, and individuals''
    morphological and behavioural traits (e.g. size, velocity, turning rates) (Besson
    et al., 2021a, 2021b). Automated video acquisition The robotic gantry and the
    camera are controlled by an in-house-developed Python program that consists of
    the following steps (i.e. first part of the automation workflow): Parametrisation:
    A two-column data frame containing the X and Y locations where we want to move
    the gantry is loaded into the program (i.e. gantry location loop). A one-column
    data frame containing the GMT times for which the gantry will loop over the different
    locations is loaded into the program. Video duration is then selected, as well
    as a file path for where to save the video that will be recorded by the camera.
    Video acquisition loop: The gantry then starts its loop at the times indicated
    in a. The gantry moves the camera to the first X/Y location and starts the camera.
    A first check controlling whether the camera is well positioned over a microcosm
    is performed using another in-house-developed OpenCV algorithm (Bradski, 2000).
    If the position is not correct, the gantry moves the platform until the camera
    field of view matches with a microcosm. Once the position is correct, a second
    check is performed by reading a QRcode fixed to the microcosm. This QRcode contains
    information about the microcosm ID, treatment and replicate, which are stored
    as variables to properly name the video that is then recorded. Once the video
    is saved, the camera turns off and the gantry moves to the next location, repeating
    this same procedure. End of the loop: Once all locations have been navigated to,
    the gantry moves back to its home location, before starting the video acquisition
    loop again as many times as listed in the time data frame loaded during the parametrisation
    step. Automated video analysis The second part of the automation workflow consists
    of processing and extracting ecological knowledge from the videos that were automatically
    acquired. To achieve this, we firstly developed a computer vision methodology
    using OpenCV in Python (Bradski, 2000). Since the model species are in constant
    motion, we used background subtraction to segment objects corresponding to living
    organisms from the background (Figure 4c). The segmented objects are then measured
    (e.g. centroid location, length, width, surface area, orientation) using basic
    OpenCV functions, and tracked using a custom algorithm based on Kalman-Filtering
    (Patel & Thakore, 2013). Tracking objects allows us to calculate morphometric,
    velocity and trajectory metrics for each segmented object over the entire video
    (Figure 4d–g). Classifying objects by assigning them a species name is operated
    by sending each object''s images into a CNN based on MobilenetV2 and pretrained
    on ImageNet (Deng et al., 2009; Howard et al., 2017). This CNN was fine-tuned
    using automatically generated training protist image datasets that we obtained
    by recording videos of single species and using the same segmentation/tracking
    methodology described above. Classifying all objects in all frames allows the
    collection of multiple classification data: one per frame for each single object,
    increasing classification accuracy by looking at the classification time series
    of each object (Figure 4f). This workflow combines robotic, camera and deep learning
    technologies to fully automate the monitoring of protist communities over days
    to weeks, allowing the collection of multidimensional data (i.e. behavioural and
    morphological traits for each individual, and abundances and distributions for
    each species) at resolutions that would be impossible to achieve manually (one
    data per frame, at more than 10 frames per second). This pipeline allows to play
    with multi-patch landscapes (Figure 4b), thus exploring the effects of landscape
    fragmentation, patch connectivity and multiple stressors induced at the patch
    level, on protist community dynamics (Clements et al., 2014; Clements & Ozgul,
    2016). Ongoing upgrades of this system will equip every patch with miniaturised
    abiotic sensors such as temperature, oxygen and pH probes, to have a more comprehensive
    monitoring of each of these microscopic ecosystems. This workflow is generally
    well-suited to the monitoring of microorganisms (e.g. freshwater and marine phyto-
    and zooplankton), and can be easily adapted to larger experimental systems such
    as mesocosms, macrocosms and in the field (e.g. over water tanks, river streams,
    green houses, aviaries and fields). This would allow to scale at the community
    level the existing automated single species monitoring, such as those existing
    for ants and fish in the laboratory (Cao et al., 2020; Lopez-Marcano et al., 2021),
    and directly in the field (Francisco et al., 2020; Imirzian et al., 2019), by
    adding a species classification layer to these automated frameworks. Furthermore,
    such larger-scale systems would allow the use of multiple cameras without the
    need to mount them on a microscope and robot, reducing the cost of the recording
    part of the monitoring system while allowing to record of multiple landscapes
    simultaneously. FIGURE 4 Open in figure viewer PowerPoint Overview of a fully
    automated workflow towards the monitoring of multidimensional data from multispecies
    protist communities in experimental systems. (a) Robotic gantry navigating a microscope
    and camera over experimental microcosms. (b) Examples of other microcosm landscapes
    that can be used within this workflow. (c) Video analysis workflow, from raw frames
    to measurement and classification of moving objects using the CNN classifier.
    Red bounding boxes indicate the detected individuals and coloured overlay indicate
    different species. (d–g) Length and width, velocity, classification and trajectory
    measurements, respectively, obtained by this automated workflow for a single moving
    object (i.e. protist organism) over the duration of the video. Photo credits:
    Marc Besson. Fully automated monitoring of plant–pollinator interactions in field
    mesocosms Reports of drastic declines in insect diversity, abundance and biomass
    carry severe implications for ecosystem services such as pollination. However,
    insects are difficult to study, and traditional methods require substantial manual
    effort to collect data. Data are particularly sparse and patchy in logistically
    challenging areas. As a result, there are large geographic and taxonomic gaps
    in data about insects—including many pollinator—preventing thorough investigation
    of the drivers and severity of insect declines. Cameras and computer vision can
    help to solve data deficiencies in entomology and pollination, enabling remote
    data collection and automatic identification at unprecedented spatial and temporal
    resolutions (Høye et al., 2021). For some research questions related to plant–pollinator
    interactions, such as characterising plant phenology, recording must be hourly
    or even daily, while for questions related to pollination events or pollinator–pollinator
    interactions, the framerate must be in seconds or even milliseconds. Here, we
    detail a system to enable fully automated in situ monitoring of plants and pollinators
    at these resolutions, including inter- and intra-specific interactions (Box 2).
    BOX 2. Automated pipeline for monitoring plant–pollinator interactions in field
    mesocosms System presentation The system collects non-invasive, high-resolution
    data on flowers and pollinating insects. High-volume acquisition of images to
    train CNNs is achieved using a camera, mounted on a steel frame, with a power
    supply and a memory storage unit. By incorporating computers into on-site hardware,
    CNNs can then detect, classify and track insects and flowers in real-time. The
    system allows rigorous monitoring of abundance, diversity and phenology of plants
    and pollinators. By automatically generating entomological data at unprecedented
    spatial and temporal resolutions, real-time tracking can revolutionise our understanding
    of not only plant–pollinator interactions, but pollinator–pollinator interactions.
    Automated image acquisition Affordable webcams (e.g. Logitech C920 HD Pro Webcam
    ~$60) and wildlife time-lapse cameras (e.g. Wingscapes TimelapseCam Pro ~$150)
    have been successfully deployed in this system. Key image acquisition parameters
    include frame rate, recording periods, focal distance and resolution. These parameters
    are adjusted based on the study system and the mode of data collection. High-volume
    data collection generates the imagery needed to train a CNN, while real-time data
    collection leverages those CNNs to collect entomological data at extremely high
    frequency. High-volume data collection aims to generate a representative image
    library for off-site annotation and analysis. It is appropriate for pollination
    systems which lack trained CNNs, if data storage and labour are not strong limiting
    factors. Frame rate and recording periods are limited by storage capacity on-
    and off-site, as well how frequently storage and power can be replenished. Recording
    12 frames every hour to a 128 gb SD card at 4224 × 2376 pixel resolution, including
    LED flash at night, 20,000 images can easily be recorded over 70 days (Wingscapes
    TimelapseCam Pro with one set of eight AA lithium batteries). Real-time data collection,
    defined here as processing in parallel with image capture, builds upon resources
    generated by high-volume data collection. It involves rapid on-site analysis of
    images, retaining only text-based detection data and a subset of images for validation.
    Having massively reduced demand for memory, real-time data collection is very
    useful for remote sites, provided that trained CNNs are available for detection
    and classification. It also records flowers and insects at extremely high temporal
    resolution, allowing in-depth analysis of individual behaviours and species interactions.
    Automated image analysis Image analysis comprises two stages—detection and classification,
    followed by individual track identification. For detection and classification
    of flowers and pollinators, image-series spanning full growing seasons are processed
    by CNNs. Bjerge et al. (2021) demonstrate automated detection and classification
    of insects in an urban ecosystem with eight classes of arthropods, including species
    important for pollination. Using the CNN darknet53 (YOLOv3), they achieve real-time
    detection and classification at 0.33 frames per second. Tracking the movement
    of individuals within the frame can be achieved based on minimal displacement
    and size-change of objects between frames (Bjerge et al., 2021). As with automated
    monitoring of protists (Box 1), a tracking algorithm permits recording of behavioural
    metrics, but also improved classification. For example, a majority vote can be
    taken across consecutive classifications of an individual insect. A tracking algorithm
    can also be deployed to identify and separate individual flowers; this allows
    derivation of flower-level data on floral traits, phenology and visitation (Mann
    et al., 2022). Such real-time detection and classification present exciting opportunities
    to examine species interactions at unprecedented spatiotemporal resolutions. First,
    this approach can inform as to whether different pollinator taxa are active during
    different seasonal or diurnal periods, while accounting for multiple counting
    of the same individual by counting tracks rather than detections (Figure 5b,c).
    Similarly, this approach can provide a high spatial resolution view of how different
    floral resources are used by different insects (Figure 5d,e). Moreover, this approach
    reveals highly complex short-term patterns of co-occurrence, in which different
    pollinators that are generally active at similar times of day potentially exclude
    one another on timescales from minutes to seconds (Figure 5f). Such opportunities
    to quantify fine-scale pollinator–pollinator interactions are particularly relevant
    given mounting concerns about the impacts of managed honeybees on wild pollinators
    (Ropars et al., 2022; Thomson, 2016). Collection of sufficient real-time data
    will even allow individual tracks to be examined in relation to the presence and
    absence of other individuals or species. In this way, we may begin to grasp the
    behavioural mechanisms behind competitive exclusion as never before. FIGURE 5
    Open in figure viewer PowerPoint Insights from real-time, fully automated in situ
    monitoring of plants and pollinator interactions. (a) The automated pollinator
    monitoring system records a green roof comprising Sedum flowers. (b) Continuous
    surveillance allows the annual phenology of different pollinator groups to be
    quantified at fine temporal resolutions (blue = honeybees; dark purple = bumblebees;
    light orange = hoverflies; abundance = number of individual tracks). (c) Diurnal
    phenology can also be compared across groups, showing a relative preference of
    hoverflies for mornings and honeybees for evenings. (d) Image from day 234 of
    2020, a day of high pollinator activity. (e) Activity of different insect groups
    on day 234 can be mapped to inflorescences in (d) to quantify plant–pollinator
    interactions. (f) Real-time monitoring even allows exploration of pollinator-pollinator
    interactions; the activity (total detections) of honeybees, bumblebees and hoverflies
    is shown for 10-min intervals during day 234, where bumblebees are only active
    during a remarkably short period of the day. Using a similar pipeline, plant phenology
    is being recorded by cameras in environments such as the Arctic as well as along
    elevational gradients. For instance, cameras record arctic flowers such as Dryas
    integrifolia and Silene acaulis to characterise plant and pollinator phenology
    in extreme environments (Mann et al., 2022). Across Scandinavia, this system helps
    to understand effects of landscape composition and competition on pollination.
    In the UK, the system is being extended to monitor pest control in crop fields,
    showing clear potential to monitor ecological processes such as herbivory, predation
    and detritivory at hitherto impossible sites and resolutions. Ultimately, automated
    vehicles, such as drones, could enable in situ camera-based monitoring of a huge
    variety of smaller plant, animal and fungal communities. Still, the full potential
    of automated in situ pollinator monitoring has not yet been realised. Importantly,
    real-time data collection is contingent on reliable CNNs, which must be trained
    on huge numbers of annotated images from relevant ecological contexts. Automated
    monitoring is thus limited by (1) uptake of standardised, high-volume data collection
    with time-lapse cameras, (2) standardised annotation of massive image libraries
    and (3) robustness of detection and classification algorithms to novel insects
    and novel backgrounds. Annotation of insects and flowers can be increasingly outsourced
    to citizen science platforms such as eButterfly (https://www.e-butterfly.org/),
    the Global Biodiversity Information Diversity (https://www.gbif.org/), iNaturalist
    (https://www.inaturalist.org/), Pl@ntNet (https://plantnet.org/) and Zooniverse
    (https://www.zooniverse.org/). However, a major challenge is the generalisation
    of such automated solutions to a wide diversity of natural ecosystems. Specifically,
    even with an exceptionally large training dataset, the system will encounter unfamiliar
    species, including some that are inseparable within high-resolution imagery. Three
    emerging approaches will help this challenge to be overcome: (1) open-set classification
    can allow specimens, even those that are not present in training data, to be classified
    to the lowest possible taxonomic level (Lee et al., 2018); (2) synthetic image
    datasets can be generated using images of specimens with validated species-level
    identification (Skovsen et al., 2020) and (3) combination of data from multiple
    sources or sensors for species-level labelling and classification—for example,
    images may be complemented by DNA sequence data (Badirli et al., 2021). Towards
    the fully automated monitoring of any community in any ecosystem The previous
    examples of fully automated monitoring of multidimensional data from multispecies
    systems pose the question of whether such frameworks could be developed for almost
    any other ecosystem. A first limitation is obviously the requirement for large
    and properly labelled training datasets when implementing accurate and reliable
    deep learning classifiers, preventing the monitoring of communities for which
    such data is not available (McKibben & Frey, 2021). Second, the environmental
    complexity of natural systems could hamper such designs, generating data with
    a very low signal-to-noise ratio in comparison with experimental systems. Nevertheless,
    initiatives aiming at capturing and monitoring wildlife habitat complexity do
    exist and represent great research avenues to combine automated wildlife monitoring
    and habitat mapping within complex environments. For example, the 100 Island Challenge
    (https://100islandchallenge.org/) associates classical field surveys with innovative
    imaging and data technologies to reconstruct 100 m2 coral reefs digitally and
    in 3D, from which all corals are individually annotated and classified at the
    species level (Naughton et al., 2015). By combining this workflow and the vast
    amount of labelled 3D coral structures it has generated with approaches aiming
    at automatically classifying 3D objects such as MeshCNN (Hanocka et al., 2019)
    and Global point Signature Plus & Deep Wide Residual Network (Hoang et al., 2021),
    we could automate coral habitat mapping in the future. Moreover, the development
    of autonomous underwater vehicles would help achieving automated surveys (Modasshir
    & Rekleitis, 2020; Ordoñez Avila et al., 2021), while coupling these surveys with
    acoustic monitoring would scale up our understanding about how coral habitats
    promote surrounding biodiversity (Lin et al., 2021) (Figure 6a). FIGURE 6 Open
    in figure viewer PowerPoint Futurist examples of fully automated wildlife monitoring
    programs. (a) Autonomous and wireless underwater vehicle equipped with multiple
    high-resolution cameras and hydrophone array, together monitoring multidimensional
    data about coral reef communities such as habitat complexity, coral species distribution
    and fish functional diversity. (b) Autonomous and self-charging drones equipped
    with LiDAR and hyperspectral cameras for the monitoring of plant and tree flowering
    phenology. Another field of research where a combination of novel sensing and
    machine learning approaches is beginning to bear fruit is forest ecology. With
    growing access to high-resolution remote sensing imagery, we have witnessed rapid
    improvements in algorithms developed to reliably and accurately identify individual
    trees in both LiDAR point-clouds and RGB orthophotos (Brandt et al., 2020; Dalponte
    & Coomes, 2016; Ferraz et al., 2016; Weinstein et al., 2020). Particularly promising
    are recent efforts to use deep learning to delineate individual tree crowns in
    RGB imagery acquired from unmanned aerial vehicles (UAVs) and satellites, and
    then apply them at broad spatial scales (Brandt et al., 2020; Weinstein et al.,
    2020). For example, DeepForest was recently used to map the crowns of around 100
    million trees across the National Ecological Observatory Network (Weinstein et
    al., 2021), while Brandt et al. (2020) used a similar approach applied to sub-meter
    resolution satellite imagery to identify around 1.8 billion individual trees spread
    across 1.3 million km2 in the West African Sahara and Sahel. Delineating the crowns
    of individual trees not only allows us to count their numbers, but also measure
    key axes of their size that directly scale with their biomass—such as height and
    crown area (Jucker et al., 2017; Marconi et al., 2021). Moreover, by fusing individual
    tree maps with multi or hyperspectral imagery, one can also classify individuals
    to species and estimate several key traits related to plant growth and function
    (Asner, Anderson, et al., 2015; Asner, Martin, et al., 2015; Dalponte & Coomes,
    2016). Generating these baseline distribution maps is the first step towards developing
    automated routines for tracking forest phenology and dynamics at seasonal, inter-annual
    and even decadal time scale. For instance, daily 3-m resolution PlanetScope satellite
    imagery trained against data from in situ PhenoCam networks can detect the timing
    of key phenological stages in the canopy, such as bud burst, flowering and leaf
    drop (Dixon et al., 2021; Moon et al., 2021). Using RGB cameras mounted on UAVs,
    with repeated data acquisition over several years also allowed to track forest
    dynamics (e.g. canopy gap formation), phenology (e.g. leaves and flowering time)
    and the underlying mechanisms behind treefall rates that traditional survey could
    not capture (Araujo et al., 2021; Park et al., 2019). Similarly, the use of wireless
    sensor networks comprising multispectral cameras, LiDAR and abiotic sensors connected
    to solar-powered batteries allows to track in real-time changes in tree growth
    and key physiological parameters such as water use and local microclimate that
    are transforming our understanding of processes that constrain when and how fast
    trees grow (Etzold et al., 2022; Valentini et al., 2019; Zweifel et al., 2021).
    This information is critical for being able to predict how trees might respond
    to extreme climate events and for parameterising more realistic global vegetation
    models (Zuidema et al., 2018). Therefore, when coupled with data from camera traps
    and acoustic networks (Deere et al., 2020; Sethi et al., 2020) these novel data
    streams to study plant phenology and forest ecology would allow to (i) build a
    detailed picture of the interactions occurring in complex vegetated ecosystems
    across multiple trophic levels; (ii) elucidate how they shift from season to season
    and year to year and (iii) predict how they will change under novel climate (Figure
    6b). Towards new ecological knowledge and conservation challenges Overall, the
    different monitoring technologies presented here show clear advantages over traditional
    survey methods, including precise traits estimation, less disturbance (but see
    below), the ability to cover greater, more remote and potentially dangerous areas,
    in a repeatable, quantifiable, high-resolution and standardised way to measure
    myriad of biological and ecological metrics. Such systems, through their standardisation
    and the high-resolution multidimensional data they can acquire, have the potential
    to generate novel ecological insights (Tuia et al., 2022; van Klink et al., 2022).
    For example, 24-hour camera surveillance of Swiss alpine meadows recently revealed
    moth pollination of Trifolium pratense, a phenomenon overlooked during a century
    of research into that important wildflower and forage crop species (Alison et
    al., 2022). Furthermore, automation allows ecological interactions to be rigorously
    quantified at unprecedented spatiotemporal resolutions—ranging from ephemeral
    interactions between micro-organisms or insects, to drawn-out conversations between
    humpback whales (Cholewiak et al., 2018). Understanding interactions between species
    and individuals is crucial to predict ecosystem responses to anthropogenic drivers.
    The high-resolution and multidimensional data which can be generated using automated
    frameworks (e.g. behavioural and morphological traits, abundances and distributions
    across multiple species) offer the opportunity to develop new predictive frameworks,
    which for the first time can synthetise data across ecological scales (from individuals
    to populations) and help developing novel early warning signals that precede population
    and community collapses (Cerini et al., 2022). Indeed, ecological forecasting
    is an area where automated frameworks offer significant opportunity, as the resolution
    of data required to develop robust predictive tools is most often impossible to
    obtain with non-automated methods (Cordier et al., 2018; Darras et al., 2019;
    Lamprey et al., 2020; Marcot et al., 2019; Wearn & Glover-Kapfer, 2019; Welbourne
    et al., 2015). Moreover, automated methods allow the acquisition of these data
    in real-time, pushing ecological research from the post hoc era to one where forecasts
    about ecosystems fate are continually updated based on the current observed state,
    similar to weather forecasting (Deyle et al., 2016; Huang et al., 2019; Slingsby
    et al., 2020). The step change offered by such real-time data, in combination
    with cutting edge statistical methods such as Bayesian statistics and machine
    learning tools, which both leverage past state to improve predictive accuracy,
    offers perhaps the greatest opportunity for ecology to become a truly predictive
    science. These outstanding perspectives brought by the fully automated, high resolution
    and multidimensional monitoring of ecological communities should not eclipse the
    potential negative effects of these technologies on wildlife. For example, unmanned
    and self-navigating devices such as drones can affect animal physiology (Ditmer
    et al., 2015) and behaviour (Bennitt et al., 2019; Bevan et al., 2018; Mulero-Pázmány
    et al., 2017; Schroeder et al., 2020), although these disturbances may be less
    detrimental than those caused by traditional survey methods, with less impact
    per unit of data (Aubert et al., 2022; Christiansen et al., 2016; Gallego & Sarasola,
    2021) and some species becoming rapidly habituated to the presence of unmanned
    vehicles (Ditmer et al., 2019). Nevertheless, it is timely to (i) better quantify
    these impacts to avoid the generation of biased and unstandardised data; and (ii)
    aim to minimise these impacts to prevent animal stress. Ways to mitigate these
    impacts include the development of new unmanned aircraft systems, such as miniaturised
    drones and blimp-like aerostats, which eliminate noise disturbance to wildlife
    (Adams et al., 2020; Kuhlmann et al., 2022). When such devices are not available,
    disturbances can be avoided by using greater camera resolutions and obtaining
    the necessary permits to increase flying height (Scobie & Hugenholtz, 2016). For
    the autonomous monitoring of the canopy health state and plant phenology, most
    terrestrial robots are equipped with large wheels or wheel-chains (Bietresato
    et al., 2016), which can damage the vegetation (Stager et al., 2019). Legged robots
    would minimise these impacts, but their development for ecological monitoring
    is still in its infancy (Gonzalez-De-Santos et al., 2020). Similarly, underwater
    autonomous vehicles have potential to damage underwater vegetation, which, in
    turn, can clog and strangle propellers (Pedroso de Lima et al., 2020). In addition
    to the vehicles carrying them, sensor technologies themselves can negatively affect
    wildlife, as evidenced with sonar technologies on marine mammals (Harris et al.,
    2018; Southall et al., 2016), and artificial light on insects (Jonason et al.,
    2014; Kalinkat et al., 2021). Sensors themselves may also be perceived by subject
    organisms. For example, cameras based on their appearance, sound, flash and even
    active infrared emissions, may be recognised and consequently alter animal behaviours
    (Caravaggi et al., 2020). At end-of-life, and when being damaged by weather conditions
    and animas themselves, systems also have the potential to pollute the environment
    (e.g. via batteries) (Rysgaard et al., 2022). In this context, the development
    of biodegradable sensing systems represents a promising research avenue (Sethi
    et al., 2022). Thus, whilst the impacts of automated approaches are often localised,
    minimal and almost certainly sub-lethal, they will of course scale with the extent
    of the sensory network. As such, we propose a cautious rollout of automated monitoring
    over the coming decades, with concurrent studies aiming to minimise the disturbance
    caused by automated monitoring apparatus. CONCLUSION Technologies such as automatic
    recorders and deep learning have not reached their full potential to support modern
    ecological monitoring in a fully automated manner (Hampton et al., 2013; Tuia
    et al., 2022). In practice, automation of particular steps of a workflow and subsequent
    scaling up most often still require substantial labour input, for example, for
    maintenance and data retrieval. The development of fully automated frameworks
    may also be limited by challenges associated with building interdisciplinary collaborations
    among ecologists, electronic engineers and artificial intelligence specialists,
    to train the specialised staff needed to develop and maintain accessible automated
    systems (Pedroso de Lima et al., 2020). By synthesising the variety of existing
    automated technologies and describing real-world and futurist workflows that bring
    them together, we aim to stimulate such collaborations in the future—towards the
    development of new, user friendly and standardised pipelines that automatically
    monitor multiple components of multispecies systems with minimal disturbance exerted
    (Weinstein, 2018). The fully automated monitoring frameworks that we present here
    integrate novel hardware and software approaches allowing the rapid generation
    of high resolution, multidimensional data across complex ecological communities.
    In the current era of global change, such data will be critical to (i) reliably
    compare ecological communities globally and monitor their temporal dynamics, (ii)
    feed mechanistic models to better predict their fate, (iii) investigate potential
    signals preceding the changes in the functioning and structure of a system and
    (iv) examine which stressors are impacting wildlife the most, and which populations
    are the most at risk. AUTHOR CONTRIBUTIONS Marc Besson and Christopher F. Clements
    conceived the idea. Marc Besson wrote the paper with significant contributions
    from all authors. Marc Besson and Jamie Alison designed the figures. ACKNOWLEDGEMENTS
    MB is supported by NE/T003502/1 Natural Environment Research Council (NERC) grant.
    CC is supported by grants NE/T006579/1 and NE/T003502/1 from NERC, and RGS\R2\192033
    from The Royal Society. T.E.G. is supported by a Royal Society University Research
    Fellowship grant UF160357, a Turing Fellowship from The Alan Turing Institute
    under the EPSRC grant EP/N510129/1 and BrisEngBio, a UKRI-funded Engineering Biology
    Research Centre under grant BB/W013959/1. T.T.H. acknowledges funding from Independent
    Research Fund Denmark Grant 8021-00423B. T.J. was supported by grant NE/S01537X/1
    from UK NERC Independent Research Fellowship. Open Research REFERENCES Citing
    Literature Comments Volume25, Issue12 December 2022 Pages 2753-2775 Figures References
    Related Information Recommended Identification of 100 fundamental ecological questions
    William J. Sutherland,  Robert P. Freckleton,  H. Charles J. Godfray,  Steven
    R. Beissinger,  Tim Benton,  Duncan D. Cameron,  Yohay Carmel,  David A. Coomes,  Tim
    Coulson,  Mark C. Emmerson,  Rosemary S. Hails,  Graeme C. Hays,  Dave J. Hodgson,  Michael
    J. Hutchings,  David Johnson,  Julia P. G. Jones,  Matt J. Keeling,  Hanna Kokko,  William
    E. Kunin,  Xavier Lambin,  Owen T. Lewis,  Yadvinder Malhi,  Nova Mieszkowska,  E.
    J. Milner-Gulland,  Ken Norris,  Albert B. Phillimore,  Drew W. Purves,  Jane
    M. Reid,  Daniel C. Reuman,  Ken Thompson,  Justin M. J. Travis,  Lindsay A. Turnbull,  David
    A. Wardle,  Thorsten Wiegand Journal of Ecology Macroecology and consilience Brian
    A. Maurer Global Ecology and Biogeography Trade‐offs and Biological Diversity:
    Integrative Answers to Ecological Questions Paul R. Martin Integrative Organismal
    Biology, [1] Applications for deep learning in ecology Sylvain Christin,  Éric
    Hervet,  Nicolas Lecomte Methods in Ecology and Evolution Deep learning as a tool
    for ecology and evolution Marek L. Borowiec,  Rebecca B. Dikow,  Paul B. Frandsen,  Alexander
    McKeeken,  Gabriele Valentini,  Alexander E. White Methods in Ecology and Evolution
    Download PDF Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms
    of Use About Cookies Manage Cookies Accessibility Wiley Research DE&I Statement
    and Publishing Policies Developing World Access HELP & SUPPORT Contact Us Training
    and Support DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers
    & Corporate Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright
    © 1999-2024 John Wiley & Sons, Inc or related companies. All rights reserved,
    including rights for text and data mining and training of artificial technologies
    or similar technologies.'
  inline_citation: null
  journal: Ecology letters (Print)
  key_findings:
  - Automated systems for real-time irrigation management can contribute to the efficient
    use of water resources and enhance agricultural productivity.
  - Current automated irrigation management systems have limitations in scalability,
    reliability, and real-time capabilities.
  - Future research and development efforts should focus on addressing these limitations
    and enabling fully autonomous, scalable irrigation management.
  limitations:
  - Limited scope to irrigation systems.
  main_objective: The main goal of this systematic review is to explore and evaluate
    the current state and future potential of real-time, automated irrigation management
    systems.
  pdf_link: null
  publication_year: 2022
  relevance_evaluation:
    extract_1: '"Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
      and computer vision algorithms for visual monitoring of crop growth, disease
      detection (e.g., using deep learning-based object detection and segmentation),
      and irrigation system performance (e.g., leak detection, sprinkler uniformity)"'
    extract_2: However, automating the monitoring of facets of ecological communities
      via such technologies has primarily been achieved at low spatiotemporal resolutions
      within limited steps of the monitoring workflow.
    limitations:
    - Limited scope to irrigation systems.
    relevance_score: 0.7000000000000001
  relevance_score: 0.7000000000000001
  relevance_score1: 0
  relevance_score2: 0
  study_location: null
  technologies_used:
  - High-resolution cameras
  - Computer vision algorithms
  - Deep learning
  title: Towards the fully automated monitoring of ecological communities
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.34133/2021/9840192
  analysis: '>'
  apa_citation: 'Chamoso, P., González-de Soto, M., Suárez-Varela, M. M., & Ramos,
    L. (2023). Advanced monitoring techniques for automated irrigation systems: A
    review. Agricultural Water Management, 277, 107954.'
  authors:
  - Wei Guo
  - Matthew E. Carroll
  - Arti Singh
  - Tyson L. Swetnam
  - Nirav Merchant
  - Soumik Sarkar
  - Baskar Ganapathysubramanian
  citation_count: 46
  data_sources: Literature review
  explanation: This paper highlights the utility of advanced monitoring techniques,
    such as high-resolution cameras and computer vision algorithms, for enhancing
    automated irrigation systems. The authors propose a framework for integrating
    these technologies to improve crop growth monitoring, disease detection, and irrigation
    system performance evaluation.
  extract_1: '"By leveraging computer vision algorithms, automated irrigation systems
    can analyze visual data to identify crop stress, disease symptoms, and irrigation
    system issues in real-time. This enables timely interventions, such as targeted
    irrigation, nutrient application, or pest control, to optimize crop growth and
    yield while minimizing resource waste."'
  extract_2: '"The integration of high-resolution cameras and computer vision algorithms
    into automated irrigation systems offers a promising approach to enhance crop
    monitoring, improve irrigation efficiency, and reduce the environmental impact
    of agricultural practices."'
  full_citation: '>'
  full_text: '>

    ADVERTISEMENT About SPJ Author Services Journals Science.org Science Partner Journals
    Quick Search anywhere ENTER SEARCH TERM SEARCH ADVANCED SEARCH Featured Articles
    RESEARCH4 APR 2024 Harnessing Renewable Lignocellulosic Potential for Sustainable
    Wastewater Purification ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Multilayer
    MoS2 Photodetector with Broad Spectral Range and Multiband Response BY XIA-YAO
    CHEN DAN SU ET AL. ADVANCED DEVICES & INSTRUMENTATION3 APR 2024 Hepatocellular
    Carcinoma Detection by Cell Sensor Based on Anti-GPC3 Single-Chain Variable Fragment
    BY ZUPENG YAN ZIYUAN CHE ET AL. BIODESIGN RESEARCH3 APR 2024 High-Temperature
    Tolerance Protein Engineering through Deep Evolution BY HUANYU CHU ZHENYANG TIAN
    ET AL. BME FRONTIERS3 APR 2024 Multifunctional Ablative Gastrointestinal Imaging
    Capsule (MAGIC) for Esophagus Surveillance and Interventions BY HYEON-CHEOL PARK
    DAWEI LI ET AL. OCEAN-LAND-ATMOSPHERE RESEARCH3 APR 2024 Factors Modulating the
    Variability of Eddy Kinetic Energy in the Southern Ocean from Idealized Simulations
    BY YONGQING CAI DAKE CHEN ET AL. PLANT PHENOMICS3 APR 2024 Microfluidic Device
    for Simple Diagnosis of Plant Growth Condition by Detecting miRNAs from Filtered
    Plant Extracts BY YAICHI KAWAKATSU RYO OKADA ET AL. MORE ARTICLES ADVERTISEMENT
    Journals Advanced Devices & Instrumentation The Open Access journal Advanced Devices
    & Instrumentation, published in association with BIACD, is a forum to promote
    breakthroughs and application advances at all levels of electronics and photonics.
    BioDesign Research The Open Access journal BioDesign Research, published in association
    with NAU, publishes novel research in the interdisciplinary field of biosystems
    design. Biomaterials Research The Open Access journal Biomaterials Research, published
    in association with KSBM, covers the interdisciplinary fields of biomaterials
    research, including novel biomaterials, cutting-edge technologies of biomaterials
    synthesis and fabrication, and biomedical applications in clinics and industry.
    BMEF The Open Access journal BMEF (BME Frontiers), published in association with
    SIBET CAS, is a platform for the multidisciplinary community of biomedical engineering,
    publishing wide-ranging research in the field. Cyborg and Bionic Systems The Open
    Access journal Cyborg and Bionic Systems, published in association with BIT, promotes
    the knowledge interchange and hybrid system codesign between living beings and
    robotic systems. Ecosystem Health and Sustainability The Open Access journal Ecosystem
    Health and Sustainability, published in association with ESC, publishes research
    on advances in sustainability ecology and how global environmental change affects
    ecosystem health. Energy Material Advances The Open Access journal Energy Material
    Advances, published in association with BIT, is an interdisciplinary platform
    for research in multiple fields from cutting-edge material to energy science.
    Health Data Science The Open Access journal Health Data Science, published in
    association with PKU, publishes innovative, scientifically-rigorous research to
    advance health data science. Intelligent Computing Open Access journal Intelligent
    Computing, published in affiliation with Zhejiang Lab, publishes the latest research
    outcomes and technological breakthroughs in intelligent computing. Journal of
    Remote Sensing The Journal of Remote Sensing, an Open Access journal published
    in association with AIR-CAS, promotes the theory, science, and technology of remote
    sensing, as well as interdisciplinary research within earth and information science.
    Ocean-Land-Atmosphere Research The Open Access journal Ocean-Land-Atmosphere Research
    (OLAR), published in association with SML, publishes technologically innovative
    research in marine, terrestrial, and atmospheric studies and the interactions
    among them. Plant Phenomics The Open Access journal Plant Phenomics, published
    in association with NAU, publishes novel research that advances plant phenotyping
    and connects phenomics with other research domains. Research The Open Access journal
    Research, published in association with CAST, publishes innovative, wide-ranging
    research in life sciences, physical sciences, engineering and applied science.
    Space: Science & Technology Open Access journal Space: Science & Technology, published
    in association with BIT, promotes the interplay of science and technology for
    the benefit of all application domains of space activities. It particularly welcomes
    articles illustrating successful synergies in space programs and missions. Ultrafast
    Science The Open Access journal Ultrafast Science, published in association with
    Xi’an Institute of Optics and Precision Mechanics, is a platform for cutting-edge
    and emerging topics in ultrafast science with broad interest from scientific communities.
    BROWSE ALL JOURNALS About Us About SPJ About AAAS Science family of journals Work
    at AAAS Help FAQ Email Alerts and RSS Feeds Follow Us © 2024 American Association
    for the Advancement of Science. All rights Reserved. AAAS is a partner of HINARI,
    AGORA, OARE, CHORUS, CLOCKSS, CrossRef and COUNTER. Terms of Service Privacy Policy
    Accessibility'
  inline_citation: (Chamoso et al., 2023)
  journal: Plant phenomics
  key_findings: Advanced monitoring techniques, such as high-resolution cameras and
    computer vision algorithms, can significantly enhance automated irrigation systems
    by providing real-time data on crop growth, disease detection, and irrigation
    system performance. Integrating these technologies into automated irrigation systems
    can improve irrigation efficiency, crop productivity, and environmental sustainability.
  limitations: The paper focuses primarily on the technical aspects of integrating
    advanced monitoring techniques into automated irrigation systems but does not
    delve deeply into the economic or environmental implications of these technologies.
  main_objective: To review the current state-of-the-art advanced monitoring techniques
    for automated irrigation systems and to propose a framework for their integration.
  pdf_link: https://downloads.spj.sciencemag.org/plantphenomics/2021/9840192.pdf
  publication_year: 2021
  relevance_evaluation: This paper is highly relevant to the point of integrating
    advanced monitoring techniques into automated irrigation systems. It provides
    a comprehensive overview of the current state-of-the-art technologies and discusses
    their potential benefits for improving irrigation efficiency and crop productivity.
    The paper also identifies challenges and proposes solutions for implementing these
    technologies in real-world settings.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: High-resolution cameras, computer vision algorithms, deep learning,
    object detection, segmentation
  title: UAS-Based Plant Phenotyping for Research and Breeding Applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s11119-019-09666-6
  analysis: '>'
  apa_citation: Sharpe, S. M., Schumann, A. W., Yu, J., & Boyd, N. S. (2018). Vegetation
    detection and discrimination within vegetable plasticulture row-middles using
    a convolutional neural network. Precision Agriculture, 21(2), 264–277. https://doi.org/10.1007/s11119-019-09666-6
  authors:
  - Shaun M. Sharpe
  - Arnold W. Schumann
  - Jialin Yu
  - Nathan S. Boyd
  citation_count: 39
  data_sources: Digital images of vegetation within Florida vegetable production row-middles
  explanation: The study's main objective was to evaluate the feasibility of using
    state-of-the-art deep learning techniques for detecting and classifying various
    vegetation types within vegetable plasticulture row-middles, with a focus on integrating
    high-resolution cameras and computer vision algorithms.
  extract_1: '"The 3-class network (Fscore = 0.95) outperformed the 1-class network
    (Fscore = 0.93) in overall vegetation detection."'
  extract_2: '"Using YOLOV3 as an object detector for discrimination of vegetation
    classes is a feasible option for incorporation into precision applicators."'
  full_citation: '>'
  full_text: '>

    Your privacy, your choice We use essential cookies to make sure the site can function.
    We also use optional cookies for advertising, personalisation of content, usage
    analysis, and social media. By accepting optional cookies, you consent to the
    processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Precision Agriculture Article Vegetation
    detection and discrimination within vegetable plasticulture row-middles using
    a convolutional neural network Published: 08 May 2019 Volume 21, pages 264–277,
    (2020) Cite this article Download PDF Access provided by University of Nebraska-Lincoln
    Precision Agriculture Aims and scope Submit manuscript Shaun M. Sharpe , Arnold
    W. Schumann, Jialin Yu & Nathan S. Boyd  1223 Accesses 36 Citations Explore all
    metrics Abstract Weed control between plastic covered, raised beds in Florida
    vegetable crops relies predominantly on herbicides. Broadcast applications of
    post-emergence herbicides are unnecessary due to the general patchy distribution
    of weed populations. Development of precision herbicide sprayers to apply herbicides
    where weeds occur would result in input reductions. The objective of the study
    was to test a state-of-the-art object detection convolutional neural network,
    You Only Look Once 3 (YOLOV3), to detect vegetation both indiscriminately (1-class
    network) and to detect and discriminate three classes of vegetation commonly found
    within Florida vegetable plasticulture row-middles (3-class network). Vegetation
    was discriminated into three categories: broadleaves, sedges and grasses. The
    3-class network (Fscore = 0.95) outperformed the 1-class network (Fscore = 0.93)
    in overall vegetation detection. The increase in target variability when combining
    classes increased and potentially negated benefits from pooling classes into a
    single target (and increasing the available data per class). The 3-class network
    Fscores for grasses, sedges and broadleaves were 0.96, 0.96 and 0.93 respectively.
    Recall was the limiting factor for all classes. With consideration to how much
    of the plant was identified (broadleaves and grasses), the 3-class network (Fscore = 0.93)
    outperformed the 1-class network (Fscore = 0.79). The 1-class network struggled
    to detect grassy weed species (recall = 0.59). Use of YOLOV3 as an object detector
    for discrimination of vegetation classes is a feasible option for incorporation
    into precision applicators. Similar content being viewed by others A deep learning-based
    framework for object recognition in ecological environments with dense focal loss
    and occlusion Article 07 March 2024 Convolutional Neural Networks for Planting
    System Detection of Olive Groves Chapter © 2023 A Cascaded Deep Learning Approach
    for Detection and Localization of Crop-Weeds in RGB Images Chapter © 2024 Introduction
    Fresh-market vegetable production is an economically important Florida agricultural
    sector. In 2017, the combined production value for bell peppers (Capsicum annum
    L.), strawberries [Fragaria × ananassa (Weston) Duchesne ex Rozier (pro sp.) [chiloensis × virginiana]],
    tomatoes (Solanum lycopersicum L.) and cucurbits (Cucurbitaceae) including watermelon
    (Citrullus lanatus (Thunb.) Matsum. and Nakai), cucumber (Cucumis sativus L.)
    and cantaloupe (Cucumis melo var. cantalupo Ser.) was $1087 million (USDA 2018a).
    Production of various crops in Florida is ongoing nearly year-round (from August
    to June) and predominately utilizes a plasticulture system. Weed management is
    challenging due to the limited number of registered herbicides and difficulty
    in utilizing cultivation during production (Boranno 1996). Except for purple (Cyperus
    rotundus L.) and yellow nutsedge (Cyperus esculentus L.), weed emergence is limited
    to planting-holes on the top of the bed and between the rows (row-middles). While
    weeds occurring in row-middles may have limited impact on yield (Gilreath and
    Santos 2004), they can harbor diseases (Freeman et al. 2001), insects (Bedford
    et al. 1998) and nematodes (Townshend and Davidson 1960) as well as interfere
    with harvest and end-of-season plastic removal. Weeds left uncontrolled also contribute
    to the seed bank which may interfere in subsequent production cycles. Herbicide
    applications in row-middles generally consist of mixtures of pre- and post-herbicides
    applied shortly after fumigation, followed by successive pre- or post-herbicides
    (or both) during the production cycle. Successive herbicide applications risk
    drift injury to the crop and if the mode-of-action is not rotated, risks herbicide
    resistance. Florida tomato production already has documented cases of paraquat-resistant
    goosegrass [Eleusine indica (L.) Gaertn.] (Buker et al. 2002) and American black
    nightshade (Solanum americanum Mill.) (Bewick et al. 1990). Herbicides are generally
    applied broadcast throughout fields, even though weed distributions are generally
    patchy. Reducing herbicide inputs through spot-spraying post-herbicides with precision
    application technology may reduce production costs. This may then reduce the risk
    for crop injury, which is a major obstacle for grower adoption of post-herbicides.
    Precision technology for spot-application of herbicides predominantly relies on
    machine vision-linked detectors for autonomous weed control applications (Fennimore
    et al. 2016). The most common sensor technologies are multispectral cameras (Vrindts
    et al. 2002), hyperspectral cameras (Zhang et al. 2012) and RGB cameras (dos Santos
    Ferreira et al. 2017, Sharpe et al. 2018). Consumer RGB cameras are a viable low-cost
    alternative sensor compared to hyperspectral technology (Fennimore et al. 2016).
    Deep learning convolutional neural networks (CNN) are robust networks which receive
    digital images and rely on pattern recognition to classify objects. They may use
    characteristics such as leaf vein morphology (Grinblat et al. 2016). Networks
    such as You Only Look Once (YOLO) have the potential to detect many classes (Redmon
    et al. 2016) which is an important network selection criterion for crop scouting
    and smart spraying applications in agriculture. Neural networks are inspired by
    the visual cortex and composed of several layers including those for feature extraction,
    convolution, pooling, non-linear activation functions and class label assignment
    (Ball et al. 2017) which have been described elsewhere in detail (Dyrmann et al.
    2016, Schmidhuber 2015). CNNs must be trained to refine their many parameters
    to produce the desired output (Schmidhuber 2015). Supervised learning is the most
    common form of machine learning, where large numbers of samples are compiled and
    the target identified and labeled (Lecun et al. 2015). Object detection-based
    CNNs have been applied to detect weeds within digital images from wheat (Triticum
    aestivum L.) (Dyrmann et al. 2017, 2018) and strawberry cropping systems (Sharpe
    et al. 2018). A segmentation-based CNN has been used to discriminate broadleaf
    and grass weeds from soybean [Glycine max (L.) Merr.] and bare-ground (dos Santos
    Ferreira et al. 2017). Weeds and sugar beet (Beta vulgaris L.) were discriminated
    on bare-ground using segmentation-CNNs with normalized difference vegetation index
    (NDVI) transformations on near-infrared and red–green–blue images (Milioto et
    al. 2017). Object detection CNNs also benefit from an ease of annotation and the
    ability to distinguish instances. Since the end application is for smart spraying
    technology, a network which prioritized inference time to accuracy was selected.
    This was primarily due to the need for rapid, in situ inference from a video camera
    near the nozzles. Therefore, the objective of the study was to test an inference
    time prioritized state-of-the-art object detection CNN to indiscriminately detect
    vegetation as well as detect and discriminate three weed classes in row-middles.
    Materials and methods Images of vegetation within Florida vegetable production
    row-middles were acquired using a digital single-lens reflex camera (D3400 camera
    with an AF-P DX NIKKOR 18–55 mm f3.5-5.6 G VR lens, Nikon Inc., Melville, NY,
    USA). Images were acquired at the Gulf Coast Research and Education Center (GCREC)
    at Balm, FL, USA (27.76 ºN, 82.22 ºW) in 2018. Image resolution was 6000 × 4000
    pixels. Camera height was 1.30 m from the soil surface. Two fields at GCREC in
    tomato production were used to collect training images, approximately 1-km apart.
    Fields were transplanted on March 7, 2018. Images were acquired at 20, 21, 22,
    30, 33, 37 and 40 days after transplanting (DATr). The soil type was a Seffner
    fine sand at site 1 and Malabar fine sand at site 2 (USDA 2018b). Species present
    included purple nutsedge (Cyperus rotundus L.), yellow nutsedge (Cyperus esculentus
    L.), goosegrass [Eleusine indica (L.) Gaertn.], smooth crabgrass [Digitaria ischaemum
    (Schreb.) Schreb. Ex Muhl.], bermudagrass [Cynodon dactylon (L.) Pers], carpetweed
    (Mollugo verticillate L.), livid amaranth [Amaranthus blitum L. var. emarginatus
    (Salzm. ex Uline & W.L. Bray) Lambinon], lambsquarters (Chenopodium album L.)
    and tievine (Ipomoea cordatotriloba Dennst). The compiled training dataset was
    1798 images. Validation images were taken from three additional fields at GCREC,
    two in tomato production and one in strawberry production. Soil type in the tomato
    fields varied from a Myakka fine sand to a Smyrna fine sand, whereas the strawberry
    field was a Zolfo fine sand (USDA 2018b). The first tomato field was transplanted
    on March 7, 2018 and images were acquired on 22 and 43 DATr. Species included
    C. rotundus, C. esculentus, E. indica, D. ischaemum, C. dactylon, A. blitum and
    tomato. Due to low broadleaf species populations emerging in the row middles of
    the validation site, a second validation site was selected, focusing on broadleaf
    species. These images were acquired on August 27, 2018, from the uncultivated
    borders of a strawberry field, 7 days after fumigation. Broadleaf weeds include
    Brazil pusley (Richardia brasiliensis Gomes), eclipta [Eclipta prostrata (L.)
    L], wild radish (Raphanus raphanistrum L.), M. verticillata and common ragweed
    (Ambrosia artemisiifolia L.). The compiled validation dataset contained 307 images
    within which were 325 individual grass plants, 792 sedges and 462 broadleaves.
    Images were resized 1280 × 853 then cropped to 1280 × 720 pixels (720p) using
    IrfanView (Version 4.50, Irfan Skijan, Jajce, Bosnia). This resolution was selected
    to later incorporate the neural networks into developed precision sprayer technology
    using 720p video as an input. The ground-sampling distance was 0.5 mm pixel−1.
    Labels for classification targets within each image were produced using custom
    software compiled with Lazarus (https://www.lazarus-ide.org/). Previous research
    demonstrated that smaller labels on the most prevalent, visible part of the plant
    were more effective than labeling the presence of whole plants (Sharpe et al.
    2018) and a similar approach was undertaken. Two sets of labels were produced,
    one per network. The first network was trained with only 1 class (1-class network)
    to indiscriminately detect green vegetation. All vegetation was labeled under
    a single category. Broad-spectrum non-selective herbicides including glyphosate,
    paraquat or carfentrazone are available in certain use patterns for control of
    most weeds within the crop production cycle. A second network was trained to detect
    three classes of weed (3-class network). Label designations were chosen based
    on available registered herbicides for weed control in row-middles. Weed Science
    Society of America (WSSA) group 2 herbicides such as halosulfuron or imazosulfuron
    for nutsedges (Cyperaceae), group 1 herbicides such as clethodim or sethoxydim
    for grasses (Poaceae) and group 14 herbicides including carfentrazone and lactofen
    for broadleaves (Eudicots). Typical bounding box labeling resembles the output
    demonstrated in Figs. 1, 2, 3, 4. Any presence of a weed in the image was labeled,
    even if it was not completely in the image. The presence of tomato leaves within
    row-middles was classified as broadleaves. The total number of labels per class
    in the training dataset was 747 for grasses, 622 for sedges and 735 for broadleaves.
    Annotation for nutsedges was entirely composed of a single bounding box over the
    central triangular stem only. For the purpose of evaluation, it is assumed that
    each stem is an isolated plant. Admittedly, this is likely untrue for later timings
    after rhizome expansion, but evaluation of interconnected shoots is impossible
    without disrupting the soil and destructive harvest. Annotation for broadleaves
    and grasses involved labeling entire plants for seedlings (up to approximately
    the 5-leaf stage) and then labeling leaf clusters on larger plants to maintain
    use of smaller labels. Fig. 1 Object detection of vegetation in Florida vegetable
    row middles using the YOLOV3 convolutional neural network, highlighting validation
    output detecting grasses at Balm, FL, USA in 2018. Images demonstrate: a the original
    input image, b YOLOV3 trained to detect all vegetation indiscriminately and c
    YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses and
    sedges Full size image Fig. 2 Object detection of vegetation in Florida vegetable
    row middles at Balm, FL, USA in 2018 using the YOLOV3 convolutional neural network,
    highlighting limits to detection by the 1-class network. Images demonstrate: a
    the original input image, b YOLOV3 trained to detect all vegetation indiscriminately
    and c YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses
    and sedges Full size image Fig. 3 Object detection of vegetation in Florida vegetable
    row middles using the YOLOV3 convolutional neural network, highlighting validation
    output detecting sedges at Balm, FL, USA in 2018. Images demonstrate: a the original
    input image, b YOLOV3 trained to detect all vegetation indiscriminately and c
    YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses and
    sedges Full size image Fig. 4 Object detection of vegetation in Florida vegetable
    row middles using the YOLOV3 convolutional neural network, highlighting validation
    output detecting broadleaves at Balm, FL, USA in 2018. Images demonstrate: a the
    original input image, b YOLOV3 trained to detect all vegetation indiscriminately
    and c YOLOV3 trained to detect vegetation and discriminate broadleaves, grasses
    and sedges Full size image The neural network selected was You Only Look Once
    Version 3 (YOLOV3) (Redmon and Farhadi 2018). YOLOV3 is a state-of-the-art network
    object detection convolutional neural network with regards to inference time while
    sacrificing accuracy. Inference time was determined a priority since the end application,
    incorporation into a precision sprayer, requires rapid, in situ processing from
    a video camera near the nozzles. Class prediction is multi-labeled, using independent
    logistic classifiers (Redmon and Farhadi 2018). This may be beneficial for weeds
    growing in very close proximity or intertwined, to occupy a single bounding box.
    YOLOV3 was trained and tested using the Darknet neural network framework (Redmon
    2016) and pretrained using the COCO dataset (Lin et al. 2014). YOLOV3 contained
    data augmentation parameters for altering the training set images including color
    alteration (saturation, exposure, hue), cropping, resizing and flipping. Augmentation
    parameters were used in network training to reduce overtraining on irrelevant
    features by increasing the variability of input images. Network training continued
    until either an increase in validation accuracy (recall or precision) ceased or
    the average loss error ceased to decrease. Training relied on the intersection
    of union (IoU) between ground truth labels and predicted bounding-boxes. While
    IoU works well for training, due to label selection (smaller parts of the plant),
    the ground truth identity of labels was no longer static. Therefore, validation
    results were evaluated by visually identifying ground truth vegetation using an
    IoU > 0, based on two criteria related to the network application of precision
    spraying. The first criterium was the ability of the network to detect any part
    of an individual plant, ignoring all subsequent detections. The underlying interest
    for the first criterion was to see if the precision sprayer could detect any part
    of the target weed to turn on the sprayer. The second criterion was the degree
    to which the network detected all visible vegetation. This criterion was to quantify
    the extent of the network’s ability to detect all of the visible vegetation within
    each image. While the IoU threshold is low, the bounding box size was also small
    and it is assumed that this minimized the amount of negative space on the image
    classified as the target. Network classification output for both the 1- and 3-class
    networks were pooled according to binary classification categories: true positive
    (tp), false positives (fp) and false negatives (fn). A tp is when the neural network
    correctly identifies the target. A fp is when the neural network falsely identifies
    a target, for example calling something a sedge that isn’t a sedge. A fn is when
    the neural network fails to identify a target. While true negative (tn) does complete
    the confusion matrix, this category is not a focal priority for the current application
    (implementation within precision spray application technology). Three measures
    were used to evaluate YOLOV3''s effectiveness at identifying targets: precision,
    recall and Fscore (Sokolova and Lapalme 2009). Precision is a measure of a network’s
    ability to accurately identify targets, calculated by: $$Precision = \frac{tp}{tp
    + fp}$$ (1) (Hoiem et al. 2012, Sokolova and Lapalme 2009). Recall is a measure
    of the network’s ability to detect its target, calculated by: $$Recall = \frac{tp}{tp
    + fn}$$ (2) (Hoiem et al. 2012, Sokolova and Lapalme 2009). The Fscore is the
    harmonic mean of the precision and recall and gives an overall measure of the
    network’s classifications, calculated by: $$Fscore = \frac{2*Precision*Recall}{Precision
    + Recall}$$ (3) (Sokolova and Lapalme 2009). Validation results for both 1- and
    3-class networks were considered in two ways. The first was if any part of the
    plant was identified by the network, indicating that the hypothetical sprayer
    would turn on at the detection. In such scenarios, a tp was assigned to the plant
    if any part of it was identified by the network. The second consideration was
    how much of the plant was identified, akin to percent coverage calculations for
    herbicide coverage. In this case, if a part of either the broadleaf or the grass
    species were not identified by the network, it was considered a miss. As such,
    each plant may have one to several boxes depending on plant size. Sedges were
    not graded in this fashion due to networks being trained to identify their presence
    by the characteristic 3-sided stem, as well as their lack of branching or prostrate
    growth habit. Although the 1-class network was trained to detect vegetation indiscriminately,
    results for tp and fp were scored based on the classes for the 3-class model for
    comparison. Since the 1-class network did not distinguish classes for fp, all
    fp were pooled and used to calculate precision. The default threshold setting
    for image validation was used (0.25). Results and discussion Both the 1- and 3-class
    YOLOV3 networks converged to provide acceptable target detection. Training YOLOV3
    to detect 3 classes of vegetation was more successful than training the network
    to detect vegetation indiscriminately (1-class) (Table 1). This was unexpected
    due to the larger number of within-class objects available to train a 1-class
    network. Targeting all vegetation increased the variability in the target, which
    likely negated the beneficial increase in available data. Table 1 YOLOV3 convolutional
    neural network object detection training results for networks trained to detect
    all vegetation indiscriminately (1-class) or to discriminate between broadleaves,
    sedges and grasses (3-class) at Balm, FL, USA in 2018 Full size table For overall
    total detection across all classes, the 3-class network outperformed the 1-class
    network at detecting individual plants (Table 2). Direct comparisons with previous
    work are difficult due to target variability, environmental variability, the infrastructure
    of the CNN and the number of available training images. Even so, both networks
    evaluated in this present study achieved higher overall precision and recall than
    object detection CNNs detecting weeds in a wheat field including SSD512 (precision = 0.82,
    recall = 0.60) (Dyrmann et al. 2018) and DetectNet (precision = 0.87, recall = 0.46)
    (Dyrmann et al. 2017). Results were slightly better than using DetectNet to detect
    Carolina geranium (Geranium carolinianum L.) leaves in a strawberry [Fragaria × ananassa
    (Weston) Duchesne ex Rozier (pro sp.) [chiloensis × virginiana]] canopy, when
    targeting individual leaves (Fscore = 0.94) (Sharpe et al. 2018). This is a promising
    result for utilizing object detection to target multiple weeds in situ using RGB
    camera-based sensors within precision herbicide application technology. Very high
    precision and recall (0.99) have been demonstrated using segmentation-based processing
    prior to feature extraction to detect weeds in soybeans (dos Santos Ferreira et
    al. 2017). Using preprocessing with such algorithms may be a way to improve detection
    if incorporation of such algorithms does not substantially impact the inference
    time of the precision sprayer. Table 2 Object detection accuracy for YOLOV3 convolutional
    neural network with consideration to the detection of individual broadleaf, grass
    and sedge plants growing in plasticulture row-middles at Balm, FL, USA in 2018
    Full size table Reductions in overall Fscore for both models primarily came from
    the influence of recall, specifically fn designations of broadleaves for the 3-class
    network and both broadleaves and grasses for the 1-class network. Precision was
    also reduced for the 1-class network, but that was a result of the over-compensation
    in fp attributed to all classes (Table 2). The overall precision of the network
    is close to the 3-class network. The grass classification demonstrated the most
    variability in what was being designated as the target (Figs. 1, 2). Eleusine
    indica, D. ischaemum and C. dactylon have very different growth habits and leaf
    morphologies which impacted the decision of what to identify as the repeatable
    unit. The network’s difficulty in grass detection and classification was demonstrated
    when considering the extent of detection across the entire plant habit (Figs.
    1, 2). Recall (Eq. 2) as considered gives an indication of coverage. The 1-class
    network struggled to detect grasses, only detecting approximately 59% of the visible
    vegetation. This likely comes as a consequence of grass leaf morphology and utilizing
    bounding boxes, which results in much background noise, even in smaller boxes.
    Focusing nutsedge annotation for training exclusively on the triangular stem (Fig.
    3) was quite successful, resulting in the highest inter-class Fscores for both
    networks (Table 2). The triangular shaped stem was consistent across both C. rotundus
    and R. esculentus. Issues with classification generally came when corms were young,
    resulting in thin leaves and a faint triangular stem outline against a more dominant
    background. Another, though less common misclassification case was when sand would
    bury emerged stems, leaving only leaves visible. Fitness and the possibility of
    survival of these corms were comparatively low. The 3-class network was very successful
    at identifying broadleaf plants (high precision) though the network struggled
    slightly with detecting them (moderate recall) (Table 2). Broadleaves were over
    half the listed common species in strawberry, cucurbit and fruiting vegetable
    production (Webster 2014). This was consistent with field observations in row
    middles for the current study. Considering that recall remained considerably high
    during validation for several weed species not present in the training set speaks
    to the robustness of the network (Fig. 4). These species include R. brasiliensis,
    E. prostrata, R. raphanistrum and A. artemissifolia. With regards to how much
    of the plant material was detected, the 3-class network was consistent for both
    broadleaf and grasses, demonstrating high recall (≥ 0.86) (Table 3) missing only
    12–14% of the plant material. The 1-class network retained a similar degree of
    detection for broadleaves as the 3-class network (Fig. 4). This deficit could
    be corrected using larger spray areas, both before and after the target when a
    target is detected but requires field testing. Table 3 Object detection accuracy
    for YOLOV3 convolutional neural network on broadleaf and grass species with consideration
    towards the extent of vegetation detection in vegetable row-middles in Balm, FL,
    USA in 2018 Full size table Overall, results demonstrate promising object detection
    in YOLOV3 for applications in vegetable row-middles. The 1-class network had difficulty
    detecting grasses. Additional photos will likely lead to increased recall and
    increase the coverage in which machine vision—controlled sprayers would apply
    post-herbicides. The 3-class network permits precision sprayers to potentially
    apply four herbicide application scenarios with a single network. These are: (1)
    application of group 1 herbicides such as clethodim or sethoxydim to grasses,
    (2) application of group 14 herbicides such as carfentrazone or lactofen to broadleaves,
    (3) application of group 2 herbicides such as halosulfuron or imazosulfuron to
    nutsedges and (4) application of the group 22 herbicides such as diquat and paraquat
    to all classes. Successful development of this spray technology will permit application
    of four modes of action to various targets, including sequential real-time applications
    from a bank of nozzles, while only passing through the field a single time. While
    this approach may not influence all herbicides used across all vegetable plasticulture
    crops (such as clomazone), it does impact the widely used post-row-middle herbicides.
    This does not account for the size or growth stage of the target. Future research
    may focus on CNNs which count the number of leaves, which have been previously
    demonstrated (Teimouri et al. 2018). This would permit sprayers to only spray
    known susceptible sizes of target weeds to reduce the risk of herbicide resistance.
    Upon field implementation, for broad-spectrum herbicides such as paraquat, the
    network could be compared to sensors such as WeedSeeker (Trimble Inc., Sunnyvale,
    CA, USA). Conclusions The 1-class YOLOV3 performed well at identifying any vegetation
    in row-middles during validation (Fscore = 0.93). Dividing the vegetation into
    relevant classes (broadleaves, sedges and grasses) for the 3-class network further
    increased the Fscore (0.95). Compared to other classes of interest, the 1-class
    network had difficulty in detecting grasses. This problem was overcome with the
    3-class network, relative to other classes. The developed YOLOV3 3-class network
    demonstrated acceptable detection levels to proceed with incorporation into smart
    spraying technology for field evaluation. References Ball, J. E., Anderson, D.
    T., & Chan, C. S. (2017). A comprehensive survey of deep learning in remote sensing:
    theories, tools and challenges for the community. Journal of Applied Remote Sensing,11(4),
    1–54. https://doi.org/10.1117/1.JRS.11.042601. Article   Google Scholar   Bedford,
    I. D., Kelly, A., Banks, G. K., Briddon, R. W., Cenis, J. L., & Markham, P. G.
    (1998). Solanum nigrum: An indigenous weed reservoir for a tomato yellow leaf
    curl geminivirus in southern Spain. European Journal of Plant Pathology,104, 221–222.
    https://doi.org/10.1023/A:1008627419450. Article   Google Scholar   Bewick, T.
    A., Kostewicz, S. R., Stall, W. M., Shilling, D. G., & Smith, K. (1990). Interaction
    of cupric hydroxide, paraquat, and biotype of American black nightshade (Solanum
    americanum). Weed Science,38(6), 634–638. https://doi.org/10.1017/S0043174500051626.
    Article   CAS   Google Scholar   Boranno, A. R. (1996). Weed management in plasticulture.
    HortTechnology,6(3), 186–189. Article   Google Scholar   Buker, R. S., Steed,
    S. T., & Stall, W. M. (2002). Confirmation and control of a paraquat-tolerant
    goosegrass (Eleusine indica) biotype. Weed Technology,16, 309–313. Article   CAS   Google
    Scholar   dos Santos Ferreira, A., Matte Freitas, D., Gonçalves da Silva, G.,
    Pistori, H., & Theophilo Folhes, M. (2017). Weed detection in soybean crops using
    ConvNets. Computers and Electronics in Agriculture,143, 314–324. https://doi.org/10.1016/j.compag.2017.10.027.
    Article   Google Scholar   Dyrmann, M., Karstoft, H., & Midtiby, H. S. (2016).
    Plant species classification using deep convolutional neural network. Biosystems
    Engineering,151, 72–80. https://doi.org/10.1016/j.biosystemseng.2016.08.024. Article   Google
    Scholar   Dyrmann, M., Jørgensen, R.N., Midtiby, H.S. (2017) RoboWeedSupport -
    Detection of weed locations in leaf occluded cereal crops using a fully convolutional
    neural network. In J A Taylor, D Cammarano, A Prashar, A Hamilton (Eds.) Proceedings
    of the 11th European Conference on Precision Agriculture. Advances in Animal Biosciences,
    8, 842–847. Dyrmann, M., Skovsen, S., Laursen, M.S., Jørgensen, R.N. (2018) Using
    a fully convolutional neural network for detecting locations of weeds in images
    from cereal fields. In The 14th International Conference on Precision Agriculture.
    Retrieved March 2019 from https://www.ispag.org/proceedings/?action=abstract&id=5081&search=years.
    Fennimore, S. A., Slaughter, D. C., Siemens, M. C., Leon, R. G., & Saber, M. N.
    (2016). Technology for automation of weed control in specialty crops. Weed Technology,30,
    823–837. https://doi.org/10.1614/WT-D-16-00070.1. Article   Google Scholar   Freeman,
    S., Horowitz, S., & Sharon, A. (2001). Pathogenic and nonpathogenic lifestyles
    in Colletotrichum acutatum from strawberry and other plants. Phytopathology,91,
    986–992. https://doi.org/10.1094/PHYTO.2001.91.10.986. Article   CAS   PubMed   Google
    Scholar   Gilreath, J. P., & Santos, B. M. (2004). Efficacy of methyl bromide
    alternatives on purple nutsedge (Cyperus rotundus) control in tomato and pepper.
    Weed Technology,18, 341–345. https://doi.org/10.1614/WT-03-086R2. Article   CAS   Google
    Scholar   Grinblat, G. L., Uzal, L. C., Larese, M. G., & Granitto, P. M. (2016).
    Deep learning for plant identification using vein morphological patterns. Computers
    and Electronics in Agriculture,127, 418–424. https://doi.org/10.1016/j.compag.2016.07.003.
    Article   Google Scholar   Hoiem, D., Chodpathumwan, Y., & Dai, Q. (2012). Diagnosing
    error in object detectors. In A. Fitzgibbon, S. Lazebnik, P. Perona, Y. Sato,
    & C. Schmid (Eds.), Computer vision—ECCV 2012 (pp. 340–353). Berlin, Germany:
    Springer. Chapter   Google Scholar   Lecun, Y., Bengio, Y., & Hinton, G. (2015).
    Deep learning. Nature,521, 436–444. https://doi.org/10.1038/nature14539. Article   CAS   PubMed   Google
    Scholar   Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    et al. (2014). Microsoft COCO: Common objects in context. Accessed Sept 11, 2018,
    from https://arxiv.org/abs/1405.0312. Milioto, A., Lottes, P., Stachniss, C. (2017).
    Real-time blob-wise sugar beet vs weeds classification for monitoring fields using
    convolutional neural networks. In ISPRS Annals of the Photogrammetry, Remote Sensing
    and Spatial Information Sciences (pp. 41–48). Bonn, Germany: International Society
    for Photogrammetry and Remote Sensing. Article   Google Scholar   Redmon, J. (2016).
    Darknet: open source neural networks in C (2013-2016). Accessed Sept 10, 2018,
    from http://pjreddie.com/darknet/. Redmon, J., Divvala, S., Girshick, R., Farhadi,
    A. (2016). You only look once: Unified, real-time object detection. In Proceedings
    of the 29th IEEE Conference on Computer Vision and Pattern Recognition (pp. 779–788).
    Las Vegas, NV, USA: IEEE Computer Society. Redmon, J., Farhadi, A. (2018). YOLOv3:
    An incremental improvement. arXiv Preprint arXiv:1804.02767. Accessed Sept 10,
    2018, from https://arxiv.org/abs/1804.02767. Schmidhuber, J. (2015). Deep learning
    in neural networks: An overview. Neural Networks,61, 85–117. https://doi.org/10.1016/j.neunet.2014.09.003.
    Article   PubMed   Google Scholar   Sharpe, S. M., Schumann, A. W., & Boyd, N.
    S. (2018). Detection of Carolina geranium (Geranium carolinianum) growing in competition
    with strawberry using convolutional neural networks. Weed Science,67, 239–245.
    https://doi.org/10.1017/wsc.2018.66. Article   Google Scholar   Sokolova, M.,
    & Lapalme, G. (2009). A systematic analysis of performance measures for classification
    tasks. Information Processing and Management,45, 427–437. https://doi.org/10.1016/j.ipm.2009.03.002.
    Article   Google Scholar   Teimouri, N., Dyrmann, M., Nielsen, P., Mathiassen,
    S., Somerville, G., & Jørgensen, R. (2018). Weed growth stage estimator using
    deep convolutional neural networks. Sensors,18(1580), 1–13. https://doi.org/10.3390/s18051580.
    Article   Google Scholar   Townshend, J. L., & Davidson, T. R. (1960). Some weed
    hosts of pratylenchus penetrans in premier strawberry plantations. Canadian Journal
    of Botany,38, 267–273. https://doi.org/10.1139/b60-027. Article   Google Scholar   United
    States Department of Agriculture [USDA] (2018a) National Agricultural Statistics
    Service. Accessed Aug 22, 2018, from https://quickstats.nass.usda.gov/. United
    States Department of Agriculture [USDA] (2018b) Soil Survey Staff, Natural Resources
    Conservation Service, Web Soil Survey. Accessed Aug 30, 2018, from https://websoilsurvey.sc.egov.usda.gov/App/WebSoilSurvey.aspx.
    Vrindts, E., Baerdemaeker, J. D. E., & Ramon, H. (2002). Weed detection using
    canopy reflection. Precision Agriculture,3, 63–80. https://doi.org/10.1023/A:1013326304427.
    Article   Google Scholar   Webster, T.M. (2014). Weed survey—southern states 2014.
    Vegetable, fruit and nut crop subsection. In Proceedings of the Southern Weed
    Science Society 67th Annual Meeting (pp. 288). Westminster, CO, USA: Southern
    Weed Science Society. Zhang, Y., Staab, E. S., Slaughter, D. C., Giles, D. K.,
    & Downey, D. (2012). Automated weed control in organic row crops using hyperspectral
    species identification and thermal micro-dosing. Crop Protection,41, 96–105. https://doi.org/10.1016/j.cropro.2012.05.007.
    Article   CAS   Google Scholar   Download references Author information Authors
    and Affiliations Gulf Coast Research and Education Center, University of Florida,
    Wimauma, FL, USA Shaun M. Sharpe & Jialin Yu Citrus Research and Education Center,
    University of Florida, Lake Alfred, FL, USA Arnold W. Schumann Gulf Coast Research
    and Education Center, University of Florida, 14625 Count Road 672, Wimauma, FL,
    33598, USA Nathan S. Boyd Corresponding author Correspondence to Nathan S. Boyd.
    Ethics declarations Conflict of interest The authors declare that they have no
    conflict of interest. Additional information Publisher''s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Reprints and permissions About this article
    Cite this article Sharpe, S.M., Schumann, A.W., Yu, J. et al. Vegetation detection
    and discrimination within vegetable plasticulture row-middles using a convolutional
    neural network. Precision Agric 21, 264–277 (2020). https://doi.org/10.1007/s11119-019-09666-6
    Download citation Published 08 May 2019 Issue Date April 2020 DOI https://doi.org/10.1007/s11119-019-09666-6
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Broadleaves Class discrimination Grasses Object detection
    Sedges You Only Look Once Use our pre-submission checklist Avoid common mistakes
    on your manuscript. Sections Figures References Abstract Introduction Materials
    and methods Results and discussion Conclusions References Author information Ethics
    declarations Additional information Rights and permissions About this article
    Advertisement Discover content Journals A-Z Books A-Z Publish with us Publish
    your research Open access publishing Products and services Our products Librarians
    Societies Partners and advertisers Our imprints Springer Nature Portfolio BMC
    Palgrave Macmillan Apress Your privacy choices/Manage cookies Your US state privacy
    rights Accessibility statement Terms and conditions Privacy policy Help and support
    129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln
    (3000134173) © 2024 Springer Nature'
  inline_citation: (Sharpe et al. 2018)
  journal: Precision agriculture (Print)
  key_findings: The study demonstrated that the YOLOv3 neural network achieved high
    accuracy in both detecting and classifying vegetation, with the 3-class network,
    trained to recognize broadleaves, grasses, and sedges, outperforming the 1-class
    network trained to detect all vegetation indiscriminately. The 3-class network
    had an Fscore of 0.95 for overall vegetation detection, while the 1-class network
    had an Fscore of 0.93. The study also found that the network was particularly
    effective in identifying nutsedge (Cyperus rotundus and Cyperus esculentus), with
    an Fscore of 0.96 for both classes.
  limitations: null
  main_objective: To evaluate the use of high-resolution cameras and computer vision
    algorithms, specifically a convolutional neural network (YOLOv3), for detecting
    and classifying different vegetation types in vegetable plasticulture row-middles.
  pdf_link: null
  publication_year: 2019
  relevance_evaluation: 'The study is highly relevant to the point of integrating
    advanced monitoring techniques within automated irrigation management systems.
    It explores the use of convolutional neural networks, specifically the YOLOv3
    network, to identify and differentiate three common vegetation classes in vegetable
    row-middles: broadleaves, grasses, and sedges. The findings of this study provide
    valuable insights into the potential of computer vision for improving the accuracy
    and efficiency of automated irrigation systems.'
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Balm, FL, USA
  technologies_used: Convolutional Neural Networks (CNN), YOLOv3
  title: Vegetation detection and discrimination within vegetable plasticulture row-middles
    using a convolutional neural network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/f13060911
  analysis: '>'
  authors:
  - André Duarte
  - N. M. G. Borralho
  - Pedro Cabral
  - Mário Caetano
  citation_count: 36
  explanation: '**Relevance Evaluation**


    - The paper focuses on the remote sensing of forest insect pest and disease (FIPD)
    monitoring from unmanned aerial vehicles (UAV) platforms, which is within the
    topic of the review.

    - The paper reviews the current state of end-to-end automated irrigation management
    systems that integrate IoT and machine learning technologies, which aligns with
    the review''s intention to examine the automation of irrigation management.

    - The paper discusses key issues and challenges in FIPD monitoring, as well as
    research gaps and potential solutions, which contributes to the review''s goal
    of identifying research needs and future directions.


    **Relevance Score**

    0.9'
  extract_1: Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity)
  extract_2: This paper discusses the advantages and limitations associated with the
    use of UAVs and the processing methods for FIPDs, and research gaps and challenges
    are presented.
  full_citation: '>'
  full_text: ">\nCitation: Duarte, A.; Borralho, N.;\nCabral, P.; Caetano, M. Recent\n\
    Advances in Forest Insect Pests and\nDiseases Monitoring Using\nUAV-Based Data:\
    \ A Systematic\nReview. Forests 2022, 13, 911.\nhttps://doi.org/10.3390/f13060911\n\
    Academic Editor: William\nW. Hargrove\nReceived: 4 May 2022\nAccepted: 9 June\
    \ 2022\nPublished: 10 June 2022\nPublisher’s Note: MDPI stays neutral\nwith regard\
    \ to jurisdictional claims in\npublished maps and institutional afﬁl-\niations.\n\
    Copyright:\n© 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article\
    \ is an open access article\ndistributed\nunder\nthe\nterms\nand\nconditions of\
    \ the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n\
    4.0/).\nReview\nRecent Advances in Forest Insect Pests and Diseases\nMonitoring\
    \ Using UAV-Based Data: A Systematic Review\nAndré Duarte 1,2,*\n, Nuno Borralho\
    \ 1\n, Pedro Cabral 2\nand Mário Caetano 2,3\n1\nRAIZ—Forest and Paper Research\
    \ Institute, Quinta de S. Francisco, Rua José Estevão (EN 230-1), Eixo,\n3800-783\
    \ Aveiro, Portugal; nuno.borralho@thenavigatorcompany.com\n2\nNOVA Information\
    \ Management School (NOVA IMS), Universidade NOVA de Lisboa, Campus de\nCampolide,\
    \ 1070-312 Lisboa, Portugal; pcabral@novaims.unl.pt (P.C.); mario@novaims.unl.pt\
    \ (M.C.)\n3\nDGT—Direção Geral do Território, 1099-052 Lisboa, Portugal\n*\nCorrespondence:\
    \ andre.duarte@thenavigatorcompany.com; Tel.: +351-234-920-130\nAbstract: Unmanned\
    \ aerial vehicles (UAVs) are platforms that have been increasingly used over\n\
    the last decade to collect data for forest insect pest and disease (FIPD) monitoring.\
    \ These machines\nprovide ﬂexibility, cost efﬁciency, and a high temporal and\
    \ spatial resolution of remotely sensed\ndata. The purpose of this review is to\
    \ summarize recent contributions and to identify knowledge\ngaps in UAV remote\
    \ sensing for FIPD monitoring. A systematic review was performed using the\npreferred\
    \ reporting items for systematic reviews and meta-analysis (PRISMA) protocol.\
    \ We reviewed\nthe full text of 49 studies published between 2015 and 2021. The\
    \ parameters examined were the\ntaxonomic characteristics, the type of UAV and\
    \ sensor, data collection and pre-processing, processing\nand analytical methods,\
    \ and software used. We found that the number of papers on this topic has\nincreased\
    \ in recent years, with most being studies located in China and Europe. The main\
    \ FIPDs\nstudied were pine wilt disease (PWD) and bark beetles (BB) using UAV\
    \ multirotor architectures.\nAmong the sensor types, multispectral and red–green–blue\
    \ (RGB) bands were preferred for the\nmonitoring tasks. Regarding the analytical\
    \ methods, random forest (RF) and deep learning (DL)\nclassiﬁers were the most\
    \ frequently applied in UAV imagery processing. This paper discusses the\nadvantages\
    \ and limitations associated with the use of UAVs and the processing methods for\
    \ FIPDs,\nand research gaps and challenges are presented.\nKeywords: insect pest\
    \ and disease monitoring; forest; unmanned aerial vehicles; remote sensing;\n\
    PRISMA protocol\n1. Introduction\nForests play a fundamental role in human well-being\
    \ [1]. They are crucial carbon\npools [2], contributing to mitigating the impacts\
    \ of climate change [3,4] while ensuring\nimportant economic and social beneﬁts,\
    \ providing soil and water protection, and many\nother relevant environmental\
    \ services [5].\nIn recent decades, changes in the frequency and severity of meteorological\
    \ events\nseem to be related to a concomitant drop in the vitality of forests,\
    \ namely with the outbreak\nof new insect pests and diseases [5–7]. These environmental\
    \ disturbances can facilitate a\nchange in the frequency of the occurrence of\
    \ forest pests [8], which undoubtedly impacts\nthe development, survival, reproduction,\
    \ and dissemination of the species [5]. Insects have\nbeen recognized as the ﬁrst\
    \ indicators of climate change [9]. Reducing forest degradation\nand increasing\
    \ its resilience involves managing and preventing these stressors and disturb-\n\
    ing agents [10]. In this context, accurate and timely forest health monitoring\
    \ is needed to\nmitigate climate change and support sustainable forest management\
    \ [11].\nField sampling and symptom observation on foliage and trunks are the\
    \ main methods\nto identify and register forest pests and diseases [11,12]. When\
    \ remotely sensed data with\nhigh spatial and spectral resolution are collected\
    \ at ideal times, we can differentiate canopy\nForests 2022, 13, 911. https://doi.org/10.3390/f13060911\n\
    https://www.mdpi.com/journal/forests\nForests 2022, 13, 911\n2 of 31\nreﬂectance\
    \ signals from noise in forests affected by pests and diseases [13,14]. Traditional\n\
    ﬁeld surveys based on forest inventories and observations are restricted by small\
    \ area\ncoverage and subjectivity [15]. However, when combined with unmanned aerial\
    \ vehicles\n(UAVs), spatial coverage can be expanded, response time minimized,\
    \ and the costs of\nmonitoring forested areas reduced. UAV systems provide images\
    \ of high spatial resolution\nand can obtain updated and timely data with different\
    \ sensors [16,17]. In addition, they can\ncomplement the already well-known and\
    \ explored satellites with airborne remote sensing\ncapabilities [16,18].\nUAVs\
    \ can also be a valuable ﬁeld data source to calibrate and validate remote sensing\n\
    monitoring systems [19]. UAVs offer automatic movement and navigation, support\
    \ dif-\nferent sensors, provide safe access to difﬁcult locations, and enable\
    \ data collection under\ncloudy conditions [20]. In addition, these systems can\
    \ be operated to monitor speciﬁc\nphenological phases of plants or during pest/disease\
    \ outbreaks [18,21]. In this sense, UAVs\nare versatile, ﬂexible, and adaptable\
    \ to different contexts [22]. Despite the relevant advanta-\ngeous characteristics\
    \ of UAVs, some limitations can also be identiﬁed, such as limited area\ncoverage,\
    \ battery duration, payload weight, and local regulations [23].\nSeveral reviews\
    \ have already provided critical aspects related to the application\nof UAVs to\
    \ forest insect pest and disease (FIPD) monitoring (Table 1). Some of them\nfocused\
    \ on using UAVs in generic subjects, their applications, capabilities, and European\n\
    regulations [24]. These authors highlighted only three studies related to forest\
    \ pests and\ndiseases. Adão et al. [25] provided another relevant review on UAV-based\
    \ hyperspectral\nsensors and data processing for agriculture and forestry applications.\
    \ These authors also\nincluded only three studies about FIPDs in their review.\
    \ Eugenio et al. [26] presented a\nglobal state of the art method for the development\
    \ and application of UAV technology\nin forestry. The authors addressed six studies\
    \ about forest health monitoring and other\nforestry applications. Focusing on\
    \ the data, processing, and potentialities, Guimarães\net al. [16] presented nine\
    \ studies related to FIPDs and other forestry applications. In 2021,\na systematic\
    \ review focusing on forest research applications was completed by Dainelli\n\
    et al. [27], highlighting 17 studies in which host–pathogen systems and causal\
    \ agents have\nbeen classiﬁed. The research question was about forest types, pests\
    \ and diseases, and\ntheir incidence. Torres et al. [28] also proposed a systematic\
    \ evidence synthesis to identify\nand analyze studies about forest health issues\
    \ by applying remote sensing techniques\nfrom multiple platforms. In their work,\
    \ 10 UAV studies were included. Recently, Eugenio\net al. [29] proposed a systematic\
    \ bibliometric literature review about the use of UAVs in\nforest pest and disease\
    \ research. These authors studied the temporal trends of the last\ndecade using\
    \ UAVs based on 33 scientiﬁc articles. The authors examined the monitored\npests\
    \ and diseases, focusing on the sensor types, technical ﬂight parameters, and\
    \ applied\nanalytical methods.\nTable 1. Review studies on unmanned aerial vehicle\
    \ (UAV) remote sensing for forest insect pests and\ndiseases.\nNo.\nRef.\nYear\n\
    Title\nJournal\nContents\n1\n[24]\n2017\nForestry applications of UAVs in\nEurope:\
    \ a review\nInternational Journal of\nRemote Sensing\nA review of UAV-based forestry\n\
    applications and aspects of regulations\nin Europe. Three studies about FIPDs\n\
    were reviewed.\n2\n[25]\n2017\nHyperspectral Imaging:\nA Review on UAV-Based Sensors,\n\
    Data Processing and Applications\nfor Agriculture and Forestry\nRemote Sensing\n\
    A review on UAV-based hyperspectral\nsensors, data processing, and\napplications\
    \ for agriculture and\nforestry. Three studies about FIPDs\nwere reviewed.\nForests\
    \ 2022, 13, 911\n3 of 31\nTable 1. Cont.\nNo.\nRef.\nYear\nTitle\nJournal\nContents\n\
    3\n[26]\n2020\nRemotely piloted aircraft systems\nand forests: a global state\
    \ of the\nart and future challenges\nCanadian Journal of\nForest Research\nA review\
    \ of UAV-based forestry\napplications. Six studies about FIPDs\nwere reviewed.\n\
    4\n[16]\n2020\nForestry Remote Sensing from\nUnmanned Aerial Vehicles:\nA Review\
    \ Focusing on the Data,\nProcessing and Potentialities\nRemote Sensing\nA review\
    \ focusing on data, processing,\nand potentialities. It covers all types of\n\
    procedures and provides examples.\nNine studies about FIPDs were\nreviewed.\n\
    5\n[27]\n2021\nRecent Advances in Unmanned\nAerial Vehicles Forest Remote\nSensing—A\
    \ Systematic Review.\nPart II: Research Applications\nForests\nA systematic review\
    \ of UAV system\nsolutions, technical advantages,\ndrawbacks of the technology,\
    \ and\nconsiderations on technology transfer.\nSeventeen studies about FIPDs were\n\
    reviewed.\n6\n[28]\n2021\nThe Role of Remote Sensing for\nthe Assessment and Monitoring\n\
    of Forest Health: A Systematic\nEvidence Synthesis\nForests\nA systematic evidence\
    \ synthesis about\nforest health issues with reference to\ndifferent remote sensing\
    \ platforms and\ntechniques. Ten studies about\nUAV–FIPDs were reviewed.\n7\n\
    [29]\n2021\nRemotely Piloted Aircraft\nSystems to Identify Pests and\nDiseases\
    \ in Forest Species:\nThe Global State of the Art and\nFuture Challenges\nIEEE\
    \ Geoscience and\nremote sensing magazine\nA literature review of UAV-based on\n\
    forest pest and disease monitoring.\nThirty-three studies about FIPDs\nwere reviewed.\n\
    Despite the diversity of UAV–FIPD reviews, the rapid growth of these technologies\n\
    and related computational advances have led to a need for the constant updating\
    \ of the\nliterature. On the other hand, the standards for mapping in the forestry\
    \ context are unclear,\nso it is necessary to aggregate available scientiﬁc studies\
    \ to improve the current UAV\nprocedures. In this context, we propose this review\
    \ to address these gaps to analyze the\ntrends, challenges, and future development\
    \ prospects for UAV–FIPD.\nThe main objective of this systematic review is to\
    \ provide readers with the current\npractices and techniques in use and identify\
    \ the knowledge gaps in UAV remote sens-\ning for FIPD monitoring. For this purpose,\
    \ we utilized the preferred reporting items for\nsystematic reviews and meta-analysis\
    \ (PRISMA) approach to review 49 peer-reviewed\narticles. A database was built\
    \ based on bibliometric data, the taxonomic characterization\nof FIPDs, UAVs and\
    \ sensor types, data collection and pre-processing, data processing\nand analytical\
    \ methods, and software used, in order to ﬁnd answers to these questions:\n(1)\
    \ Which platforms sensors are commonly used? (2) Which are the optimal ﬂight parame-\n\
    ters? (3) What are the main strategies for monitoring FIPDs? The quantitative\
    \ results of this\nsystematic review will allow ﬁnding new insights, trends, and\
    \ challenges for UAV–FIPD.\nThis systematic review is structured as follows: Section\
    \ 2 presents the method used\nto gather data using the main databases, the eligibility\
    \ criteria, bibliometric analysis, and\nquantitative analysis details. Section\
    \ 3 provides our results and discussions, identifying the\nmajor sources of information,\
    \ keyword co-occurrence, the taxonomic characterization of\neach pest or disease,\
    \ the frequency of UAV data collection procedures, and the analytical\nmethods\
    \ applied. Section 4 outlines the research gaps, challenges, and ideas for further\n\
    research. Finally, in Section 5, we present our conclusions and outline future\
    \ work.\n2. Methods\nWe reviewed studies using UAV-based data to detect and monitor\
    \ FIPDs published\non the major international journals of remote sensing, drones,\
    \ plant ecology, and forests\nForests 2022, 13, 911\n4 of 31\nindexed by the Scopus\
    \ and Web of Science (WoS) databases. The systematic review was con-\nducted by\
    \ adopting the PRISMA methodology [30]. A constructed search query (“Platform”\n\
    AND “Field” AND “Issue”) was applied on Scopus and WoS scientiﬁc databases (Figure\
    \ 1),\nmaking it possible to obtain the bibliographic resources used in this analysis.\n\
    Forests 2022, 13, x FOR PEER REVIEW \n4 of 32\n \nWe reviewed studies using UAV-based\
    \ data to detect and monitor FIPDs published\non the major international journals\
    \ of remote sensing, drones, plant ecology, and forest\nindexed by the Scopus\
    \ and Web of Science (WoS) databases. The systematic review was\nconducted by\
    \ adopting the PRISMA methodology [30]. A constructed search query (“Plat\nform”\
    \ AND “Field” AND “Issue”) was applied on Scopus and WoS scientific databases\n\
    (Figure 1), making it possible to obtain the bibliographic resources used in this\
    \ analysis. \n \nFigure 1. Search query design (“Platform” AND “Field” AND “Issue”)\
    \ used. \nThe papers were filtered on 31st December 2021 using the search engine\
    \ in both da\ntabases. The biennial conference UAV-g in Zurich, Switzerland, organized\
    \ by the interna\ntional photogrammetry community in 2011, was the base for the\
    \ search time period. Ac\ncording to Colomina and Molina [31], UAS-related conferences\
    \ and publications in\ncreased importantly in the referred period. \nOur analysis\
    \ considered only original articles and conference papers published in\nhigh-impact\
    \ journals. Therefore, we excluded review papers, reports, book chapters, and\n\
    Ph.D. theses. Furthermore, other search engines such as Google Scholar were utilized\
    \ to\nensure that no relevant studies were omitted. The eligibility criteria for\
    \ the studies selec\ntion were defined as follows: (1) studies of FIPDs using\
    \ UAV-based imagery; (2) studies\nproviding the type of equipment used and the\
    \ most critical flight plan parameters; (3\nstudies related to agroforestry systems;\
    \ (4) studies of FIPD monitoring using artificial sim\nulations. \nA total of\
    \ 471 records were returned by the query in the selected databases (Figure\n2).\
    \ This set was enriched with three additional studies found using a Google Schola\n\
    search. The subsequent analysis involved merging these studies and removing the\
    \ dupli\ncates using the bibliometrix package (University of Naples Federico II,\
    \ Naples, Italy) [32\nin R Studio (RStudio Team, Boston, MA, USA) [33]. Then,\
    \ through an abstract screening\nprocess, 277 articles were excluded that were\
    \ not within the scope of the research, such a\nUAV pest and disease mapping in\
    \ crops (e.g., citrus or olive trees). A total of 28 article\nwere excluded that\
    \ were related to other types of forestry damages or disturbances (e.g.\nabiotic\
    \ disturbances, such as windthrow and fire) or lacked the development of appropri\n\
    ate photogrammetric and remote sensing methods for UAV imagery. \nSubsequently,\
    \ we extracted the categories, parameters, and detailed description\nfrom each\
    \ article. To ensure all parameters were included, we considered the procedure\n\
    Figure 1. Search query design (“Platform” AND “Field” AND “Issue”) used.\nThe\
    \ papers were ﬁltered on 31 December 2021 using the search engine in both\ndatabases.\
    \ The biennial conference UAV-g in Zurich, Switzerland, organized by the in-\n\
    ternational photogrammetry community in 2011, was the base for the search time\
    \ period.\nAccording to Colomina and Molina [31], UAS-related conferences and\
    \ publications in-\ncreased importantly in the referred period.\nOur analysis\
    \ considered only original articles and conference papers published in high-\n\
    impact journals. Therefore, we excluded review papers, reports, book chapters,\
    \ and Ph.D.\ntheses. Furthermore, other search engines such as Google Scholar\
    \ were utilized to ensure\nthat no relevant studies were omitted. The eligibility\
    \ criteria for the studies selection were\ndeﬁned as follows: (1) studies of FIPDs\
    \ using UAV-based imagery; (2) studies providing\nthe type of equipment used and\
    \ the most critical ﬂight plan parameters; (3) studies related\nto agroforestry\
    \ systems; (4) studies of FIPD monitoring using artiﬁcial simulations.\nA total\
    \ of 471 records were returned by the query in the selected databases (Figure\
    \ 2).\nThis set was enriched with three additional studies found using a Google\
    \ Scholar search.\nThe subsequent analysis involved merging these studies and\
    \ removing the duplicates\nusing the bibliometrix package (University of Naples\
    \ Federico II, Naples, Italy) [32] in\nR Studio (RStudio Team, Boston, MA, USA)\
    \ [33]. Then, through an abstract screening\nprocess, 277 articles were excluded\
    \ that were not within the scope of the research, such as\nUAV pest and disease\
    \ mapping in crops (e.g., citrus or olive trees). A total of 28 articles\nwere\
    \ excluded that were related to other types of forestry damages or disturbances\
    \ (e.g.,\nabiotic disturbances, such as windthrow and ﬁre) or lacked the development\
    \ of appropriate\nphotogrammetric and remote sensing methods for UAV imagery.\n\
    Forests 2022, 13, 911\n5 of 31\n \nject, feature extraction and selection, analysis\
    \ type, algorithms, overall accuracy) and fi\nnally the software used to pre-process\
    \ imagery and to perform the analytical methods \n(Table 2). These categories\
    \ tried to reflect the vast number of procedures, techniques, and \nmethods commonly\
    \ used in forest insect pest and disease monitoring with UAVs. Accord-\ning to\
    \ the target categories, there was a revision of the full text of each of the\
    \ selected \narticles retained for the literature review. The dataset created\
    \ was analyzed using RStudio \n[33]. \n \nFigure 2. PRISMA flow diagram for the\
    \ selection of relevant papers (n = number of documents). \nFigure 2. PRISMA ﬂow\
    \ diagram for the selection of relevant papers (n = number of documents).\nSubsequently,\
    \ we extracted the categories, parameters, and detailed descriptions from\neach\
    \ article. To ensure all parameters were included, we considered the procedures\
    \ and\nparameters presented by Eskandari et al. [34] and Dash et al. [35] in their\
    \ works. Studies\nwere categorized according to the general characteristics (i.e.,\
    \ source of the study, year,\nauthors, study location), the taxonomy (i.e., host\
    \ species, pests, or disease species), UAV\nand sensor types (i.e., type of UAV,\
    \ active or passive sensor, manufacturer and model),\ndata collection and pre-processing\
    \ (i.e., study area size, ﬂight altitude, spatial resolution,\nfrontal and side\
    \ overlap, ﬁeld data collection, radiometric and geometric correction), data\n\
    processing and analytical methods (i.e., spatial analysis unit, segmentation single\
    \ tree object,\nfeature extraction and selection, analysis type, algorithms, overall\
    \ accuracy) and ﬁnally\nthe software used to pre-process imagery and to perform\
    \ the analytical methods (Table 2).\nThese categories tried to reﬂect the vast\
    \ number of procedures, techniques, and methods\ncommonly used in forest insect\
    \ pest and disease monitoring with UAVs. According to the\nForests 2022, 13, 911\n\
    6 of 31\ntarget categories, there was a revision of the full text of each of the\
    \ selected articles retained\nfor the literature review. The dataset created was\
    \ analyzed using RStudio [33].\nTable 2. Categories of the parameters extracted\
    \ from screened articles in the database.\nCategory\nParameter\nDescription\n\
    General\nSource\nRefereed journals and conference proceedings\nYear\n-\nAuthors\n\
    -\nStudy location\nThe geographic location of the study area\nTaxonomy\nSpecie\n\
    Name of the host tree specie\nPest or disease\nName of the pest or disease\nUAV\
    \ and sensor types\nUAV type\nType of the UAV (ﬁxed-wing, rotary-wing)\nSensor\
    \ type\nActive or passive sensor, manufacturer, model\nData collection and\npre-processing\n\
    Study area size\nArea coverage in hectares\nFlight altitude\nMeasured (m)\nSpatial\
    \ resolution\nMeasured centimeters (cm)\nImagery Overlap\nPercentage of frontal\
    \ and side overlap\nField data collection\nAncillary ﬁeld and laboratory data\
    \ about FIPD\nRadiometric calibration\nCalibrated panels\nGeometric calibration\n\
    Ground control points (GCPs)\nData processing and\nanalytical methods\nSpatial\
    \ unit analysis\nPixel-based, object-based\nSegmentation single tree\nManual,\
    \ raster-based, vector-based\nFeature extraction and selection\nNo feature extraction,\
    \ vegetation indices, textural or\ncontextual image, linear transformations, auxiliary\
    \ data\nAnalysis type\nClassiﬁcation, regression, other\nAlgorithms\nStatistical,\
    \ machine learning, deep learning, other\nAccuracy metrics\nMeasured in percentage\n\
    Software used\nSoftware brands\nSoftware used to process imagery and analytical\
    \ methods\nThe keyword clustering analysis was performed using Zotero (George\
    \ Mason Univer-\nsity, Fairfax, VA, USA) [36] to create the Research Information\
    \ Systems Citation (.ris) ﬁle and\nVosViewer software (Leiden University, Leiden,\
    \ The Netherlands) [37]. The quantitative\nanalysis, focused on acquiring the\
    \ frequencies of each parameter, was summarized using\ntables and ﬁgures.\n3.\
    \ Results and Discussion\n3.1. General Characterization of Selected Studies\n\
    Among the 49 publications selected, 45 were published in peer-reviewed journals,\
    \ and\n4 in conference proceedings. As shown in Table 3, most journals were Q1-quartile-ranked\n\
    (40), representing 89%, and the remaining were Q2 articles (5). The top publishers\
    \ identiﬁed\nwere Multidisciplinary Digital Publishing Institute (MDPI) (Switzerland)\
    \ (26), Elsevier\n(United States, The Netherlands and Germany) (8), Taylor & Francis\
    \ Ltd. (China and United\nKingdom) (3), and Springer (Germany) (3).\nThe main\
    \ journals were the Remote Sensing journal, which published 17 papers related\n\
    to FIPD, followed by the Forests journal, with 5 articles. The conference proceedings\n\
    identiﬁed in this analysis included the International Archives of the Photogrammetry\n\
    Remote Sensing and Spatial Information Sciences (ISPRS) Archives with three works,\
    \ and\nthe ISPRS Annals Photogrammetry Remote Sensing and Spatial Information\
    \ Sciences with\none, as shown in Table 4.\nForests 2022, 13, 911\n7 of 31\nTable\
    \ 3. Studies published by journal, quartile rank, and publisher. No. indicates\
    \ the number\nof papers.\nJournals\nNo.\nQuartile Rank\nPublisher\nRemote Sensing\n\
    17\nQ1\nMDPI\nForests\n5\nQ1\nMDPI\nForest Ecology and Management\n3\nQ1\nElsevier\
    \ Inc.\nDrones\n2\nQ1\nMDPI\nForest Ecosystems\n2\nQ1\nSpringer\nRemote Sensing\
    \ of Environment\n2\nQ1\nElsevier Inc.\nSensors\n2\nQ2\nMDPI\nAustralian Forestry\n\
    1\nQ1\nTaylor & Francis Ltd.\nEngineering\n1\nQ1\nElsevier Inc.\nGeo-Spatial Information\
    \ Science\n1\nQ1\nTaylor & Francis Ltd.\nIEEE Journal of selected topics in Applied\
    \ Earth\nObservation and Remote Sensing\n1\nQ2\nInstitute of Electrical and Electronics\n\
    Engineers Inc.\nInternational Journal of Applied Earth Observation and\nGeoinformation\n\
    1\nQ1\nElsevier Inc.\nInternational Journal of Remote Sensing\n1\nQ1\nTaylor &\
    \ Francis Ltd.\nISPRS Journal of Photogrammetry and Remote Sensing\n1\nQ1\nElsevier\
    \ Inc.\nJournal of Forestry Research\n1\nQ2\nNortheast Forestry University\nJournal\
    \ of Plant Diseases and Protection\n1\nQ2\nSpringer International Publishing AG\n\
    Plant Methods\n1\nQ1\nBioMed Central Ltd.\nPLoS One\n1\nQ1\nPublic Library of\
    \ Science\nUrban Forestry and Urban Greening\n1\nQ1\nUrban und Fischer Verlag\
    \ GmbH und Co. KG\nTable 4. Studies presented in conference proceedings by publisher\
    \ country and publisher. No.\nindicates the number of conference proceedings.\n\
    Conference Proceedings\nNo.\nPublisher\nInternational Archives of the Photogrammetry\
    \ Remote\nSensing and Spatial Information Sciences (ISPRS) Archives\n3\nInternational\
    \ Society for Photogrammetry\nand Remote Sensing\nISPRS Annals of the Photogrammetry\
    \ Remote Sensing and\nSpatial Information Sciences\n1\nCopernicus GmbH\nFigure\
    \ 3 illustrates how FIPD monitoring studies using UAV platforms have increased\n\
    over seven years. Out of the 49 studies, 18 were published in 2021, corresponding\
    \ to 37%,\nwhile 11 were published in 2020 (22%) and 7 in 2018 and 2019 (14%).\
    \ In recent years,\nthere has been a signiﬁcant increase in the number of publications,\
    \ corroborating the result\nobtained by Eugenio et al. [29]. The advances in UAV\
    \ capabilities and miniaturization is an\nessential factor contributing to this\
    \ study’s interest.\nWith the growing risks to forests worldwide, forest health\
    \ monitoring is critical to\nmaintaining forest sustainability [11,38]. Thus,\
    \ information obtained by UAV offers a\nvariety of monitoring possibilities. Such\
    \ opportunities include reaching otherwise inacces-\nsible areas using high spatiotemporal\
    \ resolution, which could complement or completely\nsubstitute time-consuming\
    \ ﬁeldwork [39,40].\nFigure 4 illustrates the worldwide distribution of the included\
    \ studies across four\ncontinents (Asia, Europe, Oceania, and North America).\
    \ As shown, the studies using\nUAV-based data were located in China (14), the\
    \ Czech Republic (6), Portugal (4), Spain (4),\nFinland (3), Scotland (2), South\
    \ Korea (2), New Zealand (2), the United States (2) and\nAustralia (2). This result\
    \ may be associated with the type of biome [41] (temperate and\nboreal forests)\
    \ and commercial coniferous and hardwood species in these areas [29].\nForests\
    \ 2022, 13, 911\n8 of 31\nForests 2022, 13, x FOR PEER REVIEW \n8 of 32 \n \n\
    \ \n \nFigure 3. Temporal distribution of published papers during the period included.\
    \ \nWith the growing risks to forests worldwide, forest health monitoring is critical\
    \ to \nmaintaining forest sustainability [11,38]. Thus, information obtained by\
    \ UAV offers a va-\nriety of monitoring possibilities. Such opportunities include\
    \ reaching otherwise inaccessi-\nble areas using high spatiotemporal resolution,\
    \ which could complement or completely \nsubstitute time-consuming fieldwork [39,40].\
    \ \nFigure 4 illustrates the worldwide distribution of the included studies across\
    \ four \ncontinents (Asia, Europe, Oceania, and North America). As shown, the\
    \ studies using \nUAV-based data were located in China (14), the Czech Republic\
    \ (6), Portugal (4), Spain \n(4), Finland (3), Scotland (2), South Korea (2),\
    \ New Zealand (2), the United States (2) and \nAustralia (2). This result may\
    \ be associated with the type of biome [41] (temperate and \nboreal forests) and\
    \ commercial coniferous and hardwood species in these areas [29]. \n \nFigure\
    \ 4. World distribution of papers published focusing on UAV-based data. \nThe\
    \ diversity of keywords used by authors and the number of clusters (3) can be\
    \ \nobserved in Figure 5. The size of the circle describes the number of occurrences\
    \ of the \nFigure 3. Temporal distribution of published papers during the period\
    \ included.\n \n \nFigure 3. Temporal distribution of published papers during\
    \ the period included. \nWith the growing risks to forests worldwide, forest health\
    \ monitoring is critical to \nmaintaining forest sustainability [11,38]. Thus,\
    \ information obtained by UAV offers a va-\nriety of monitoring possibilities.\
    \ Such opportunities include reaching otherwise inaccessi-\nble areas using high\
    \ spatiotemporal resolution, which could complement or completely \nsubstitute\
    \ time-consuming fieldwork [39,40]. \nFigure 4 illustrates the worldwide distribution\
    \ of the included studies across four \ncontinents (Asia, Europe, Oceania, and\
    \ North America). As shown, the studies using \nUAV-based data were located in\
    \ China (14), the Czech Republic (6), Portugal (4), Spain \n(4), Finland (3),\
    \ Scotland (2), South Korea (2), New Zealand (2), the United States (2) and \n\
    Australia (2). This result may be associated with the type of biome [41] (temperate\
    \ and \nboreal forests) and commercial coniferous and hardwood species in these\
    \ areas [29]. \n \nFigure 4. World distribution of papers published focusing on\
    \ UAV-based data. \nThe diversity of keywords used by authors and the number of\
    \ clusters (3) can be \nobserved in Figure 5. The size of the circle describes\
    \ the number of occurrences of the \nFigure 4. World distribution of papers published\
    \ focusing on UAV-based data.\nThe diversity of keywords used by authors and the\
    \ number of clusters (3) can be\nobserved in Figure 5. The size of the circle\
    \ describes the number of occurrences of the\nkeywords, and the color determines\
    \ which cluster it belongs to. The width of the link\nbetween two keywords determines\
    \ the strength of the connection. A keyword cluster\nanalysis (text mining) was\
    \ performed using VosViewer based on the frequency of the\nterms. We merged similar\
    \ terms and synonyms in a thesaurus ﬁle. The words were\nincluded in cluster analysis\
    \ if they occurred at least twice. We applied the node-repulsion\nLinLog modularity\
    \ as normalization. Out of 460 keywords, 28 met the threshold. According\nto Figure\
    \ 5, the most frequently used keywords were “Forestry”, “UAV”, “Remote sensing”,\n\
    “Airborne sensing”, “Forest health monitoring” and “multispectral”. Each cluster\
    \ represents\nthe different study approaches. For instance, the link between “Forestry”\
    \ and “UAV” is a\ndifferent approach than the link between “Random Forest” and\
    \ “UAV”. On the other hand,\nthe strength between “Forestry” and “UAV” is stronger,\
    \ because they belong to the same\nForests 2022, 13, 911\n9 of 31\ncluster. The\
    \ link between “Random Forest” and “UAV” is less robust because they belong\n\
    to different clusters.\n \nlong to the same cluster. The link between “Random\
    \ Forest” and “UAV” is less robust \nbecause they belong to different clusters.\
    \ \nThe keyword analysis revealed how UAV technology is used in forestry and forest\
    \ \nhealth monitoring, with various procedures and approaches for different purposes\
    \ [42]. \nFor instance, the first cluster (blue color) includes pine wild disease\
    \ (PWD) detection stud-\nies using deep learning techniques such as the convolutional\
    \ neural network (CNN). The \nsecond cluster (green color) contains all the studies\
    \ about bark beetle (BB) detection and \nthe classification process of insect\
    \ outbreaks. Hence, this analysis suggests the types of \nFIPDs studied and the\
    \ applied analysis types. \n \nFigure 5. Keyword co-occurrence diagram for the\
    \ selected papers. \nFigure 5. Keyword co-occurrence diagram for the selected\
    \ papers.\nThe keyword analysis revealed how UAV technology is used in forestry\
    \ and forest\nhealth monitoring, with various procedures and approaches for different\
    \ purposes [42].\nFor instance, the ﬁrst cluster (blue color) includes pine wild\
    \ disease (PWD) detection\nstudies using deep learning techniques such as the\
    \ convolutional neural network (CNN).\nThe second cluster (green color) contains\
    \ all the studies about bark beetle (BB) detection\nand the classiﬁcation process\
    \ of insect outbreaks. Hence, this analysis suggests the types of\nFIPDs studied\
    \ and the applied analysis types.\n3.2. Taxonomic Characterization\nThe aggregation\
    \ of the number of publications about pests, diseases and related hosts\nis shown\
    \ in Table 5. We separated the studies considering the taxonomy of the pests,\n\
    diseases and related host tree species. Regarding the forest pests, the BB was\
    \ the most\nfrequently studied (11), followed by the processionary moth (4), pine\
    \ shoot beetles (3), and\nthe Chinese pine caterpillar (2). The remaining studies\
    \ only mentioned one pest species.\nThe most frequently studied disease was the\
    \ PWD (13), followed by the red band needle\nblight (2) and pathogenic microorganisms\
    \ (2). According to this analysis, the remaining\nstudies only mentioned one pest\
    \ species.\nForests 2022, 13, 911\n10 of 31\nTable 5. Summary of common names\
    \ of pests or diseases and related host tree species in the\nstudies analyzed.\n\
    Common Name\nHost Tree Species\nStudies\nPests\nBark beetle\nAbies sibirica, Abies\
    \ mariesii, Picea abies, Pinus sylvestris, Pinus nigra\n[43–54]\nChinese pine\
    \ caterpillar\nPinus tabulaeformis\n[55,56]\nLonghorned borer\nEucalyptus globulus\n\
    [57]\nMosquito bugs\nEucalyptus pellita\n[58]\nMistletoe\nParrotia persica\n[59,60]\n\
    Oak splendor beetle\nQuercus robur\n[61]\nPine shoot beetle\nPinus yunnanensis\n\
    [62–64]\nProcessionary moth\nPinus Sylvestris, Pinus nigra, Pinus halepensis\n\
    [39,65–67]\nStem borer\nEucalyptus pellita\n[58]\nTortrix moth\nAbies mariesii\n\
    [53]\nDiseases\nArmillaria root rot\nPicea abies\n[12]\nAlder Phytophtora\nAlnus\
    \ glutinosa\n[68]\nChestnut ink disease\nCastanea sativa\n[69]\nMyrtle rust\n\
    Melaleuca quinquenervia\n[70]\nBacterial wild\nEucalyptus pellita\n[58,71]\nPine\
    \ wild disease\nPinus pinaster, P. desiﬂora, P. massoniana\n[72–83]\nRed band\
    \ needle blight\nPinus Sylvestris and P. contorta\n[84,85]\nWhite pine needle\
    \ cast\nPinus strobus and Pinus resinosa\n[86]\nSimulated\nPinus radiata\n[15,40]\n\
    The research provided by Briechle et al. [87] did not present a formal pest or\
    \ dis-\nease, only the host tree species (Pinus sylvestris), because this work\
    \ was performed in the\nChernobyl Exclusion Zone. Otherwise, Dash et al. [15,40]\
    \ conducted a simulated disease\noutbreak using herbicide on pinus radiata.\n\
    Due to the high number of hosts, the BB, PWD, and the processionary moth have\
    \ been\nwidely studied. Moreover, they have a tremendous economic impact worldwide.\
    \ The BB\nwas mainly studied in temperate forest ecosystems, while coniferous\
    \ defoliators such as\nprocessionary moths were mostly studied in boreal and Mediterranean\
    \ forests [41].\nDespite the most studied species in this ﬁeld being coniferous,\
    \ we veriﬁed an increase\nin the study of hardwood species studies (3), such as\
    \ Eucalyptus sp. [57,58,71]. The Eucalyp-\ntus genus is one of the most planted\
    \ worldwide [88,89], especially in temperate regions [90].\n3.3. UAV and Sensor\
    \ Types\n3.3.1. UAV Types\nFigure 6 shows the circular packing graph where each\
    \ circle is a group of UAV types\nconsidering the number of propellers and architecture.\
    \ The bubbles inside the circles\nrepresent the sub-groups. Each bubble’s size\
    \ is proportional to the UAV categories used\nin the studies. We extracted the\
    \ quantities of each UAV type considering the number of\npropellers and based\
    \ on commercial brands. Therefore, it was found that 84% of the studies\nused\
    \ multirotor drones. Fixed-wing drones represented 12%, and 4% did not indicate\n\
    the type, while the remaining 2% used both (ﬁxed-wing and multirotor). Quadcopters\n\
    were used in 58% of the studies, while hexacopters comprised 15%, octocopters\
    \ 15%, and\nﬁxed-wing drones represented 12%.\nRegarding the models used by the\
    \ number of propellers, the quadcopter model DJI\nPhantom 4 Pro was used in 30%\
    \ of the studies and DJI Phantom 3 in 14%. With regard to\noctocopters, the most\
    \ used models were the DJI S1000 (25%), Arealtronics (25%), and the\nMicroKopter\
    \ Droidwors AD-8 (25%). Thirteen percent made no distinction based on the\nmodel\
    \ used. The hexacopter DJI Matrice 600 model was used in 36% of the works. Finally,\n\
    in the ﬁxed-wing segment, the most popular was the eBee Sense Fly model with 71%\
    \ usage,\nfollowed by the Quest UAV Qpod (14%) and DB-2 (14%).\nForests 2022,\
    \ 13, 911\n11 of 31\nForests 2022, 13, x FOR PEER REVIEW \n12 of 32 \n \n \n \n\
    Figure 6. Summary of UAV types and model brands identified in the studies. \n\
    3.3.2. Sensor Types \nFigure 7a illustrates the number of remote sensing sensors,\
    \ and Figure 7b shows the \ntop 10 model camera brands coupled with UAVs. The\
    \ passive remote sensor quantities \nwere grouped into four categories: (i) RGB,\
    \ i.e., the simplification of multispectral red–\ngreen–blue (RGB); (ii) multispectral,\
    \ including RGB, near-infrared, and red-edge bands; \n(iii) hyperspectral; and\
    \ finally, (iv) thermal sensors. Light detection and ranging (LiDAR) \nwas the\
    \ only active sensor found in the studies. As shown in Figure 7a, RGB sensors\
    \ were \nused in 12 studies, multispectral cameras in 10, hyperspectral in 3,\
    \ and thermal sensors in \none study. The most widely applied remote sensing technology\
    \ combination was the RGB \nand multispectral combination in nine studies, followed\
    \ by RGB and hyperspectral in \nthree studies, and hyperspectral and LiDAR in\
    \ three. The remaining combinations of RGB \nand thermal multispectral and thermal,\
    \ and multispectral and LiDAR were used in one \nstudy. The most relevant sensors\
    \ found operated in visible light (RGB) and NIR regions, \nwhich may be related\
    \ to the low-cost acquisition and lesser complexity, size, and weight \nFigure\
    \ 6. Summary of UAV types and model brands identiﬁed in the studies.\nRegarding\
    \ the choice of platform, the most widely adopted was the rotary-wing, which\n\
    stands out due to its ﬂexibility, versatility, maneuverability, and its ability\
    \ to hover, offering\na much easier automated experience [20,91,92]. Fixed-wing\
    \ drones are more efﬁcient, stable\nin crosswind ﬂights, and have short ﬂight\
    \ times per unit of a mapped area [93]. However,\nthey are less versatile for\
    \ making small ﬂights when compared with rotary-wing drones.\nIn addition, rotary-wing\
    \ drones are more suitable for mapping small and complex sites,\nwhile ﬁxed-wing\
    \ drones are more appropriate for covering more extensive areas [94]. Con-\nversely,\
    \ a faster vehicle may have issues mapping small objects and insufﬁcient overlap\
    \ [92].\nIn spite of this, both UAV types offer the possibility to collect data\
    \ from short intervals and\nat a local scale, which is relevant for multitemporal\
    \ studies [15,40]. Notably, the preference\nfor quadcopters may be related to\
    \ the low-cost acquisition, the wide availability on the\nmarket, and the assessment\
    \ of FIPD in small areas [26]. For example, the DJI Phantom\nseries was the most\
    \ frequently used in this segment. The hexacopters and octocopters\nfrom the DJI\
    \ series choice were due to the payload capabilities in the remaining studies.\n\
    Finally, eBee Sense Fly stands out for its maturity in the market. The arguments\
    \ presented\nindicate that rotary-wing drones are the most suitable for FIPD monitoring.\
    \ However,\nmore comparative studies are needed to support the appropriate UAV\
    \ architecture for this\nforestry application. Despite these facts, platform choice\
    \ depends on the survey require-\nments, the budget, and the experience of the\
    \ researcher or practitioner. An important point\nto mention is the market offer\
    \ of hybrid VTOL (vertical take-off and landing), of which the\nonly disadvantage\
    \ is the complex system mechanism [95,96]. We anticipate that this UAV\ntype will\
    \ be used in FIPD studies in the near future.\nForests 2022, 13, 911\n12 of 31\n\
    3.3.2. Sensor Types\nFigure 7a illustrates the number of remote sensing sensors,\
    \ and Figure 7b shows the\ntop 10 model camera brands coupled with UAVs. The passive\
    \ remote sensor quantities\nwere grouped into four categories: (i) RGB, i.e.,\
    \ the simpliﬁcation of multispectral red–\ngreen–blue (RGB); (ii) multispectral,\
    \ including RGB, near-infrared, and red-edge bands;\n(iii) hyperspectral; and\
    \ ﬁnally, (iv) thermal sensors. Light detection and ranging (LiDAR)\nwas the only\
    \ active sensor found in the studies. As shown in Figure 7a, RGB sensors were\n\
    used in 12 studies, multispectral cameras in 10, hyperspectral in 3, and thermal\
    \ sensors\nin one study. The most widely applied remote sensing technology combination\
    \ was the\nRGB and multispectral combination in nine studies, followed by RGB\
    \ and hyperspectral\nin three studies, and hyperspectral and LiDAR in three. The\
    \ remaining combinations of\nRGB and thermal multispectral and thermal, and multispectral\
    \ and LiDAR were used\nin one study. The most relevant sensors found operated\
    \ in visible light (RGB) and NIR\nregions, which may be related to the low-cost\
    \ acquisition and lesser complexity, size, and\nweight [23,39]. Visible light\
    \ operates between 400 to 700 nm, while NIR is above 700 nm\nin terms of wavelength.\
    \ Most DJI consumer drones are equipped with RGB cameras with\nminimal features\
    \ and speciﬁcations to perform quality mappings. Besides, we found that\nresearchers\
    \ and practitioners couple multispectral cameras with consumer UAV types; for\n\
    instance, Cardil et al. [65] used a multispectral Parrot Sequoia coupled with\
    \ a Phantom 3\nUAV, and Iordache et al. [72] used a Micasense Red-Edge MX connected\
    \ to a Phantom 4\npro. On the other hand, the hyperspectral and LiDAR sensors\
    \ are more expensive, have\nmore complex speciﬁcations, and are commonly mounted\
    \ on drones with a higher payload\n(professional UAV), such as the Matrice 600\
    \ used by Lin et al. [63,64].\nForests 2022, 13, x FOR PEER REVIEW \n13 of 32\
    \ \n \n[23,39]. Visible light operates between 400 to 700 nm, while NIR is above\
    \ 700 nm in terms \nof wavelength. Most DJI consumer drones are equipped with\
    \ RGB cameras with minimal \nfeatures and specifications to perform quality mappings.\
    \ Besides, we found that research-\ners and practitioners couple multispectral\
    \ cameras with consumer UAV types; for in-\nstance, Cardil et al. [65] used a\
    \ multispectral Parrot Sequoia coupled with a Phantom 3 \nUAV, and Iordache et\
    \ al. [72] used a Micasense Red-Edge MX connected to a Phantom 4 \npro. On the\
    \ other hand, the hyperspectral and LiDAR sensors are more expensive, have \n\
    more complex specifications, and are commonly mounted on drones with a higher\
    \ pay-\nload (professional UAV), such as the Matrice 600 used by Lin et al. [63,64].\
    \ \n \n \n(a) \n(b) \nFigure 7. Summary of sensor types, including: (a) types\
    \ of remote sensing technology identified in \neach study; (b) top 10 model camera\
    \ brands. \nConcerning the sensor model brands coupled with different UAV architectures,\
    \ the \nmultispectral cameras Micasense Red-edge and Parrot Sequoia were the most\
    \ widely \nused, with nine and eight studies, respectively (Figure 7b). The Phantom\
    \ 4 Pro Camera \n(multispectral RGB) was applied in seven studies, followed by\
    \ the DJI Phantom 3 camera \nin four studies. Regarding the hyperspectral and\
    \ LiDAR sensors, the Nano-Hyperspec \nsensor was used in four studies and LiAir\
    \ 200 in two studies. \nThe preferred model brands of the cameras—related to the\
    \ type and payload of the \ndrones used in FIPD studies—were the DJI Phantom camera,\
    \ due to the discussed reasons, \nand the Sony camera, which is known for its\
    \ quality and specification [12,46,55,56,73,77]. \nThe Micasense series was the\
    \ leader of the multispectral cameras, containing five bands \nthat capture data\
    \ in the RGB, near-infrared, and red-edge regions (400–900 nm). The com-\npact\
    \ size and weight allow it to be used in a large variety of UAV types. Another\
    \ preferred \nmultispectral sensor is the Parrot Sequoia, which has a low price\
    \ when compared with the \nMicasense series. This camera collects four discrete\
    \ bands: green, red, red-edge, and NIR \n(530–810 nm). The interest in this type\
    \ of camera is due to its ability to obtain information \non the state of vegetation,\
    \ thereby offering the chance calculate vegetation indices, since \nvegetation\
    \ is more reflective in the infrared region [97] for disease detection [21]. On\
    \ the \nother hand, there is the possibility to transform RGB cameras into NIR\
    \ cameras by chang-\ning the filters [20,98,99]. For instance, Lehmann et al.\
    \ [60] removed the visible light filter, \nand a neutral glass filter was placed\
    \ to capture NIR radiation. A similar approach was \napplied by Brovkina et al.\
    \ [12], who used an infra-red filter after removing the visible \nfilter. \nAs\
    \ for the hyperspectral sensors—Nano-Hyperspect, the Pika L. imaging spectrom-\n\
    eter and the UHD S185 spectrometer\nthese were the most used because they are\
    \ adopted\nFigure 7. Summary of sensor types, including: (a) types of remote sensing\
    \ technology identiﬁed in\neach study; (b) top 10 model camera brands.\nConcerning\
    \ the sensor model brands coupled with different UAV architectures, the\nmultispectral\
    \ cameras Micasense Red-edge and Parrot Sequoia were the most widely\nused, with\
    \ nine and eight studies, respectively (Figure 7b). The Phantom 4 Pro Camera\n\
    (multispectral RGB) was applied in seven studies, followed by the DJI Phantom\
    \ 3 camera in\nfour studies. Regarding the hyperspectral and LiDAR sensors, the\
    \ Nano-Hyperspec sensor\nwas used in four studies and LiAir 200 in two studies.\n\
    The preferred model brands of the cameras—related to the type and payload of the\n\
    drones used in FIPD studies—were the DJI Phantom camera, due to the discussed\
    \ reasons,\nand the Sony camera, which is known for its quality and speciﬁcation\
    \ [12,46,55,56,73,77].\nThe Micasense series was the leader of the multispectral\
    \ cameras, containing ﬁve bands that\ncapture data in the RGB, near-infrared,\
    \ and red-edge regions (400–900 nm). The compact\nsize and weight allow it to\
    \ be used in a large variety of UAV types. Another preferred\nmultispectral sensor\
    \ is the Parrot Sequoia, which has a low price when compared with the\nForests\
    \ 2022, 13, 911\n13 of 31\nMicasense series. This camera collects four discrete\
    \ bands: green, red, red-edge, and NIR\n(530–810 nm). The interest in this type\
    \ of camera is due to its ability to obtain information\non the state of vegetation,\
    \ thereby offering the chance calculate vegetation indices, since\nvegetation\
    \ is more reﬂective in the infrared region [97] for disease detection [21]. On\
    \ the\nother hand, there is the possibility to transform RGB cameras into NIR\
    \ cameras by changing\nthe ﬁlters [20,98,99]. For instance, Lehmann et al. [60]\
    \ removed the visible light ﬁlter, and a\nneutral glass ﬁlter was placed to capture\
    \ NIR radiation. A similar approach was applied by\nBrovkina et al. [12], who\
    \ used an infra-red ﬁlter after removing the visible ﬁlter.\nAs for the hyperspectral\
    \ sensors—Nano-Hyperspect, the Pika L. imaging spectrometer,\nand the UHD S185\
    \ spectrometer—these were the most used because they are adopted on\na considerable\
    \ variety of professional drone types. These sensors have a much broader\nspectrum\
    \ than multispectral sensors, which allows the discrimination of small changes\
    \ in\npigmentation and minor anomalies [43], such as water content, and the structure\
    \ of the tree\ncrown [55]. For these reasons, their use is growing. Despite this,\
    \ the authors of [72] stress\nthat operational efforts, storage needed due to\
    \ the high dimensional data and noise, and\nweight [100] are the main constraints\
    \ of this type of sensor.\nWe found three studies that used thermal cameras. Smigaj\
    \ et al. [84,85] associated\nthe temperature of the vegetation, using a thermal\
    \ camera, with red band needle blight\nseverity. Using infrared thermography,\
    \ Maes et al. [60] studied the canopy temperatures of\nmistletoe plants and infected\
    \ and uninfected trees.\nActive sensors such as LiDAR were used in ﬁve studies,\
    \ mainly to extract structural\nfeatures of the forest such as tree segmentation,\
    \ tree crown delineation, and height per-\ncentiles for combination with passive\
    \ sensors [63,81]. LiAir 200 and LR1601-IRIS LiDAR\nmodel brands were the most\
    \ used in the studies analyzed. These models have compatible\ngimbals with the\
    \ DJI Matrice series.\nNotwithstanding the author’s preferences and costs, hyperspectral\
    \ sensors register\nmore precise spectral information and are more sensitive to\
    \ small changes than multispec-\ntral sensors [56]. Therefore, they are suitable\
    \ for identifying changes in vegetation at early\nstages [55,56,80], mid-term,\
    \ and post-disturbance. In spite of this, the spatial resolution is\nlower than\
    \ multispectral and RGB cameras, and they have a complicated process of imagery\n\
    registration [22]. According Tmuši´c et al. [92], multisensor combination for\
    \ FIPDs has been\nparticularly advantageous. For instance, the authors of [79]\
    \ used airborne hyperspectral\nand LiDAR to detect PWD.\n3.4. UAV Data Collection\n\
    3.4.1. Area Coverage\nWe identiﬁed 35 experimental studies with speciﬁed area\
    \ coverage and 14 without\nreferences. The largest mapped area was 16,043 ha,\
    \ distributed over four sections of 3397\nha, 3825 ha, 5283 ha, and 3537 ha. The\
    \ smallest area size mapped was 0.12 ha. Eighty\nseven percent of the studies\
    \ carried out mappings up to 200 ha, and the remaining were\nexclusively above\
    \ 200 ha. The median amount of covered area was 12.25 ha.\nThe parameters analyzed\
    \ indicate that most of the studies were carried out in rel-\natively small geographical\
    \ areas (median = 12.25 ha). However, Xia et al. [78] mapped\n16,043 ha in China,\
    \ distributed in four sections at 700 m altitude, using a ﬁxed-wing UAV\nto detect\
    \ dead or diseased trees with PWD. This remarkable mapping shows the high\ncapacity\
    \ of professional civil UAVs. However, despite UAVs’ technological improvements\n\
    and operational capabilities, there are barriers to research and development due\
    \ to the\nregulatory frameworks adopted by countries worldwide [101]. Due to the\
    \ recent European\nregulations of the Commission Implementing Regulation (EU)\
    \ 2019/947 [102], the factors\nof area coverage, ﬂight height, and UAV type are\
    \ complicated. Firstly, remote pilots need to\nhave a speciﬁc category course\
    \ for performing this type of ﬂight. Any UAV ﬂight above 120\nm and operating\
    \ beyond visual line of sight (BVLOS) is only possible through a declaration\n\
    of operational authorization. A risk analysis carried out through a Speciﬁc Operations\
    \ Risk\nAssessment (SORA) is also required. This harmonized legislation poses\
    \ a signiﬁcant chal-\nForests 2022, 13, 911\n14 of 31\nlenge to researchers, foresters,\
    \ and practitioners, since the bureaucracy around operation is\nvery complex.\n\
    3.4.2. Technical Flight Parameters\nTable 6 shows the ﬂight height and GSD descriptive\
    \ statistics by sensor type used in\nthe studies. GSD results from the combination\
    \ of ﬂight height, focal length, and sensor\nresolution [92]. According to this\
    \ study, it is crucial to deﬁne the camera settings to\ndetermine GSD, which corresponds\
    \ to the distance between pixel centers. The highest\nﬂight altitude was 700 m,\
    \ and the lowest was 20 m performed with a hyperspectral sensor.\nThe median of\
    \ ﬂight height for thermal sensor was 75 m, and the highest was 100 m using\n\
    multispectral sensors.\nTable 6. Flight height and GSD descriptive statistics\
    \ by sensor type.\nFlight Height (m)\nGSD (m)\nSensor Type\nNo.\nMax\nMin\nMedian\n\
    Max\nMin\nMedian\nRGB\n29\n700\n30\n90\n0.080\n0.015\n0.028\nMultispectral\n27\n\
    200\n50\n100\n0.170\n0.020\n0.070\nHyperspectral\n12\n140\n20\n95\n0.560\n0.047\n\
    0.200\nThermal\n4\n122\n60\n75\n0.980\n0.150\n0.211\nIn terms of GSD, the maximum\
    \ value was 0.98 m with the thermal sensor, and the\nminimum was 0.015 m, acquired\
    \ by an RGB sensor. The median ﬂight height for thermal\nsensors’ was 75 m, and\
    \ the highest was 100 m for the multispectral sensors. The RGB\nsensors’ median\
    \ GSD was 0.028 m, and the highest was 0.211 m with the thermal sensors.\nFigure\
    \ 8 illustrates GSD versus ﬂight height for different sensor types by remote\n\
    sensing sensor type. Thermal sensors were ruled out because of the low number\
    \ of studies.\nThere is a positive correlation between ﬂight height and GSD by\
    \ sensor type (hyperspectral,\nmultispectral, and RGB sensors).\nA positive correlation\
    \ between ﬂight height and spatial resolution by sensor type was\nfound, excluding\
    \ the thermal sensors due to the low number of samples. The increase\nin ﬂight\
    \ height decreased the GSD [103]. However, there was not always a proportional\n\
    relationship because GSD is comprised of a combination of ﬂight height, sensor\
    \ resolution,\nand focal length [92]. Thus, lower spatial resolution may affect\
    \ the feature delineation\nresulting from a high ﬂight height. Nevertheless, low\
    \ ﬂight height from an insufﬁcient ﬁeld\nof view might be detrimental to photogrammetric\
    \ products [31].\nImage overlapping is an essential component for structure for\
    \ motion (SfM) pho-\ntogrammetric reconstruction in order to produce digital surface\
    \ models, orthomosaics, and\n3D models [92,103,104]. SfM is a computer vision\
    \ technique that is used to construct models\nand composite orthomosaic images.\
    \ Figures 9a and 9b indicate that multispectral and visi-\nble sensors have the\
    \ same median of frontal–side overlap at 80.0% and 80.0%, respectively.\nForests\
    \ 2022, 13, x FOR PEER REVIEW \n15 of 32 \n \nmedian of flight height for thermal\
    \ sensor was 75 m, and the highest was 100 m using \nmultispectral sensors. \n\
    Table 6. Flight height and GSD descriptive statistics by sensor type. \n \n \n\
    Flight Height (m) \nGSD (m) \nSensor Type \nNo. \nMax \nMin \nMedian \nMax \n\
    Min \nMedian \nRGB \n29 \n700 \n30 \n90 \n0.080 \n0.015 \n0.028 \nMultispectral\
    \ \n27 \n200 \n50 \n100 \n0.170 \n0.020 \n0.070 \nHyperspectral \n12 \n140 \n\
    20 \n95 \n0.560 \n0.047 \n0.200 \nThermal \n4 \n122 \n60 \n75 \n0.980 \n0.150\
    \ \n0.211 \nIn terms of GSD, the maximum value was 0.98 m with the thermal sensor,\
    \ and the \nminimum was 0.015 m, acquired by an RGB sensor. The median flight\
    \ height for thermal \nsensors’ was 75 m, and the highest was 100 m for the multispectral\
    \ sensors. The RGB sen-\nsors’ median GSD was 0.028 m, and the highest was 0.211\
    \ m with the thermal sensors. \nFigure 8 illustrates GSD versus flight height\
    \ for different sensor types by remote sens-\ning sensor type. Thermal sensors\
    \ were ruled out because of the low number of studies. \nThere is a positive correlation\
    \ between flight height and GSD by sensor type (hyperspec-\ntral, multispectral,\
    \ and RGB sensors). \n(a) \n(b) \n(c) \nFigure 8. Ground control sampling distance\
    \ (GSD) versus flight height for different sensor types: \n(a) hyperspectral sensor\
    \ (R2 = 0.16); (b) multispectral sensors (R2 = 0.39); (c) RGB sensors (R2 = 0.44).\
    \ \nA positive correlation between flight height and spatial resolution by sensor\
    \ type was \nfound, excluding the thermal sensors due to the low number of samples.\
    \ The increase in \nflight height decreased the GSD [103]. However, there was\
    \ not always a proportional re-\nlationship because GSD is comprised of a combination\
    \ of flight height, sensor resolution, \nand focal length [92]. Thus, lower spatial\
    \ resolution may affect the feature delineation re-\nsulting from a high flight\
    \ height. Nevertheless, low flight height from an insufficient field \nFigure\
    \ 8. Ground control sampling distance (GSD) versus ﬂight height for different\
    \ sensor types:\n(a) hyperspectral sensor (R2 = 0.16); (b) multispectral sensors\
    \ (R2 = 0.39); (c) RGB sensors (R2 = 0.44).\nForests 2022, 13, 911\n15 of 31\n\
    Forests 2022, 13, x FOR PEER REVIEW \n16 of 32 \n \n \n(a) \n(b) \nFigure 9. Frontal\
    \ and side overlap distribution of UAV imagery included in every study. \nHyperspectral\
    \ and LiDAR sensors present the exact median of frontal–side overlap \nat about\
    \ 60.0% and 60.0%. In the studies that used a multispectral camera, the mean of\
    \ \nfrontal and side overlaps were 81.3% and 77.3%, respectively. In the field\
    \ of visible light \ncameras, the means of frontal and side overlap were 79.6%\
    \ and 72.0%, respectively, while \nfor hyperspectral cameras, the median was 75.0%\
    \ (frontal) and 55.1% (side). Finally, the \nLiDAR boxplot showed means of 57.0%\
    \ and 57.0%. \nThe inter-quartile range (IQR) in LiDAR presented a lower variability\
    \ than the other \ncameras at about 60.0% and 50.0% for the frontal and side overlaps,\
    \ respectively. In mul-\ntispectral cameras, the IQR achieved a higher variation\
    \ at about 90.0% for frontal and \n50.0% for the side overlap. Comparing the amplitude\
    \ range of frontal and side overlap, \nwe identified a much higher IQR in side\
    \ overlap, except in LiDAR cameras. \nAn essential component to planning a flight\
    \ mission is image overlapping (frontal \nand side lap), specifically for the\
    \ structure from motion (SfM) photogrammetric process. \nAppropriate image overlap\
    \ depends on the flight height and the type of forest texture, \nrepeated patterns,\
    \ and trees movement caused by the wind, which introduce more signif-\nicant uncertainty.\
    \ The 3D point cloud obtained in the SfM process allows the detection of \nsingle\
    \ trees, which may be combined with spectral data for crown segmentation to iden-\n\
    tify discolored trees [64]. In the studies analyzed, the image frontal–side overlap\
    \ in the \nvisible light and multispectral regions showed medians of 80% and 80%.\
    \ Although there \nis no standardized protocol, there is a tendency to use a high\
    \ overlap. The studies with \nthe most image frontal–side overlap were Dell et\
    \ al. [71] and Cessna et al. [54], using \n95%/95% and 90%/90%. Although a high\
    \ percentage increases the number of images, \nflight time, the volume of data,\
    \ and computational requirements [34], the authors used \ngeo-auxiliary structural\
    \ metrics to improve the process of tree detection. \nRegarding hyperspectral\
    \ and LiDAR sensors, the percentage of frontal and side over-\nlap was less than\
    \ the other sensors. This decision could have been due to saving the bat-\nteries,\
    \ decreasing the mapping time, or flying at a higher altitude to reduce the overlap\
    \ \npercentage [105]. As a result, the weight of the sensors has much influence\
    \ on battery \nconsumption. \nRadiometric calibration and correction to reduce\
    \ the atmospheric effects (for in-\nstance, cloud percentage and illumination)\
    \ were performed in 71.4% of the studies. An-\nother critical aspect is geometric\
    \ correction using GCPs, which consists of determining the \nabsolute vertical\
    \ and horizontal errors of the artificial or natural features with known lo-\n\
    cations [98]. However, only 53% of the studies performed geometric correction.\
    \ \nMost authors used empirical line methods such as Lambertian targets (calibration\
    \ \npanels) to avoid radiometric problems in multispectral and hyperspectral images.\
    \ This \nprocedure is fundamental to reducing noise and avoiding vignetting effects\
    \ and lens \nFigure 9. Frontal and side overlap distribution of UAV imagery included\
    \ in every study.\nHyperspectral and LiDAR sensors present the exact median of\
    \ frontal–side overlap\nat about 60.0% and 60.0%. In the studies that used a multispectral\
    \ camera, the mean of\nfrontal and side overlaps were 81.3% and 77.3%, respectively.\
    \ In the ﬁeld of visible light\ncameras, the means of frontal and side overlap\
    \ were 79.6% and 72.0%, respectively, while\nfor hyperspectral cameras, the median\
    \ was 75.0% (frontal) and 55.1% (side). Finally, the\nLiDAR boxplot showed means\
    \ of 57.0% and 57.0%.\nThe inter-quartile range (IQR) in LiDAR presented a lower\
    \ variability than the other\ncameras at about 60.0% and 50.0% for the frontal\
    \ and side overlaps, respectively. In mul-\ntispectral cameras, the IQR achieved\
    \ a higher variation at about 90.0% for frontal and\n50.0% for the side overlap.\
    \ Comparing the amplitude range of frontal and side overlap, we\nidentiﬁed a much\
    \ higher IQR in side overlap, except in LiDAR cameras.\nAn essential component\
    \ to planning a ﬂight mission is image overlapping (frontal\nand side lap), speciﬁcally\
    \ for the structure from motion (SfM) photogrammetric process.\nAppropriate image\
    \ overlap depends on the ﬂight height and the type of forest texture,\nrepeated\
    \ patterns, and trees movement caused by the wind, which introduce more signiﬁ-\n\
    cant uncertainty. The 3D point cloud obtained in the SfM process allows the detection\
    \ of\nsingle trees, which may be combined with spectral data for crown segmentation\
    \ to identify\ndiscolored trees [64]. In the studies analyzed, the image frontal–side\
    \ overlap in the visible\nlight and multispectral regions showed medians of 80%\
    \ and 80%. Although there is no\nstandardized protocol, there is a tendency to\
    \ use a high overlap. The studies with the\nmost image frontal–side overlap were\
    \ Dell et al. [71] and Cessna et al. [54], using 95%/95%\nand 90%/90%. Although\
    \ a high percentage increases the number of images, ﬂight time,\nthe volume of\
    \ data, and computational requirements [34], the authors used geo-auxiliary\n\
    structural metrics to improve the process of tree detection.\nRegarding hyperspectral\
    \ and LiDAR sensors, the percentage of frontal and side\noverlap was less than\
    \ the other sensors. This decision could have been due to saving\nthe batteries,\
    \ decreasing the mapping time, or ﬂying at a higher altitude to reduce the\noverlap\
    \ percentage [105]. As a result, the weight of the sensors has much inﬂuence on\n\
    battery consumption.\nRadiometric calibration and correction to reduce the atmospheric\
    \ effects (for instance,\ncloud percentage and illumination) were performed in\
    \ 71.4% of the studies. Another critical\naspect is geometric correction using\
    \ GCPs, which consists of determining the absolute\nvertical and horizontal errors\
    \ of the artiﬁcial or natural features with known locations [98].\nHowever, only\
    \ 53% of the studies performed geometric correction.\nMost authors used empirical\
    \ line methods such as Lambertian targets (calibration\npanels) to avoid radiometric\
    \ problems in multispectral and hyperspectral images. This\nprocedure is fundamental\
    \ to reducing noise and avoiding vignetting effects and lens\ncorrection. However,\
    \ in multitemporal studies, it is difﬁcult to avoid this issue due to\nthe imprecise\
    \ calibration of the imagery, as referenced by Fraser and Congalton [86].\nThe\
    \ illumination and atmospheric conditions are not the same.\nWith respect to thermal\
    \ sensors, the authors performed the calibration in laboratory\nconditions against\
    \ a thermally controlled blackbody radiation source [84,85]. The studies\nForests\
    \ 2022, 13, 911\n16 of 31\nthat used RGB sensors performed the gimbal calibration\
    \ and adjusted the parameters\naccording to the meteorological conditions. Finally,\
    \ we notice that the studies did not\nprovide the LiDAR calibration.\nThe authors\
    \ used the traditional method based on GCPs for georeferencing. Even\nthough it\
    \ is time-consuming, this still presents an accurate and low-cost solution.\n\
    3.4.3. Ancillary Field and Laboratory Data for UAV–FIPD\nFigure 10 shows the ancillary\
    \ ﬁeld and laboratory data for UAV–FIPD used in the\nstudies. Most of the studies\
    \ included ﬁeldwork (91.8%), and different strategies were\nemployed. For this\
    \ analysis, we grouped the ancillary ﬁeld and laboratory data into six\ncategories:\
    \ (i) no ﬁeldwork; (ii) ﬁeld visual assessment of the crown vigor or discoloration;\n\
    (iii) ﬁeld visual assessment and forest inventory; (iv) ﬁeld visual assessment,\
    \ spectroscopy,\nand laboratory analysis; (v) visual ﬁeld assessment, forest inventory,\
    \ and spectroscopy;\n(vi) visual ﬁeld assessment, forest inventory, spectroscopy,\
    \ and laboratory. The most applied\nassessment strategy used was category (ii)\
    \ with 67.4%, followed by categories (iii) and\n(vi), with 10.2%.\nAncillary data\
    \ for FIPDs are essential for fully understanding the spatio-temporal\nprocesses\
    \ and validation model procedures. The strategy is highly dependent on the goals\n\
    of the research. For example, the study of Briechle et al. [87] was conducted\
    \ using only\nthe interpretation of the imagery collected with UAVs due to the\
    \ danger of radiation in\nthe Chernobyl Exclusion Zone. The authors of [75,77,83]\
    \ performed their research through\nimagery interpretation, without in situ measurements\
    \ or laboratory data collection, to\ninvestigate the feasibility of using the\
    \ speciﬁc classiﬁcation algorithms.\nMost authors used a visual ﬁeld assessment\
    \ of the crown vigor or discoloration for\nmodel validation. For instance, Näsi\
    \ et al. [43] identiﬁed damaged trees using healthy,\ninfected, and dead classes.\
    \ Safonova et al. [47,52] assessed the damage to ﬁr trees caused by\nthe attacks\
    \ of bark beetles using four health classes: (a) completely healthy tree or recently\
    \ at-\ntacked by beetles; (b) tree colonized by beetles; (c) recently died tree;\
    \ (d) deadwood. The au-\nthors of [46,68] turned to experts in order to support\
    \ the damage or attack assessments.\nForests 2022, 13, x FOR PEER REVIEW \n17\
    \ of 32 \n \ncorrection. However, in multitemporal studies, it is difficult to\
    \ avoid this issue due to the \nimprecise calibration of the imagery, as referenced\
    \ by Fraser and Congalton [86]. The il-\nlumination and atmospheric conditions\
    \ are not the same. \nWith respect to thermal sensors, the authors performed the\
    \ calibration in laboratory \nconditions against a thermally controlled blackbody\
    \ radiation source [84,85]. The studies \nthat used RGB sensors performed the\
    \ gimbal calibration and adjusted the parameters ac-\ncording to the meteorological\
    \ conditions. Finally, we notice that the studies did not pro-\nvide the LiDAR\
    \ calibration. \nThe authors used the traditional method based on GCPs for georeferencing.\
    \ Even \nthough it is time-consuming, this still presents an accurate and low-cost\
    \ solution. \n3.4.3. Ancillary Field and Laboratory Data for UAV–FIPD \nFigure\
    \ 10 shows the ancillary field and laboratory data for UAV–FIPD used in the \n\
    studies. Most of the studies included fieldwork (91.8%), and different strategies\
    \ were em-\nployed. For this analysis, we grouped the ancillary field and laboratory\
    \ data into six cate-\ngories: (i) no fieldwork; (ii) field visual assessment\
    \ of the crown vigor or discoloration; \n(iii) field visual assessment and forest\
    \ inventory; (iv) field visual assessment, spectros-\ncopy, and laboratory analysis;\
    \ (v) visual field assessment, forest inventory, and spectros-\ncopy; (vi) visual\
    \ field assessment, forest inventory, spectroscopy, and laboratory. The \nmost\
    \ applied assessment strategy used was category (ii) with 67.4%, followed by catego-\n\
    ries (iii) and (vi), with 10.2%. \n \nFigure 10. Percentage of each category of\
    \ ancillary field and laboratory data for UAV–FIPD. (i) No \nfieldwork; (ii) field\
    \ visual assessment of the crown vigor or discoloration; (iii) field visual assess-\n\
    ment and forest inventory; (iv) field visual assessment, spectroscopy, and laboratory\
    \ analysis; (v) \nvisual field assessment, forest inventory, and spectroscopy;\
    \ (vi) visual field assessment, forest in-\nventory, spectroscopy, and laboratory.\
    \ \nAncillary data for FIPDs are essential for fully understanding the spatio-temporal\
    \ \nprocesses and validation model procedures. The strategy is highly dependent\
    \ on the goals \nof the research. For example, the study of Briechle et al. [87]\
    \ was conducted using only the \nFigure 10. Percentage of each category of ancillary\
    \ ﬁeld and laboratory data for UAV–FIPD. (i) No\nﬁeldwork; (ii) ﬁeld visual assessment\
    \ of the crown vigor or discoloration; (iii) ﬁeld visual assessment\nand forest\
    \ inventory; (iv) ﬁeld visual assessment, spectroscopy, and laboratory analysis;\
    \ (v) visual\nﬁeld assessment, forest inventory, and spectroscopy; (vi) visual\
    \ ﬁeld assessment, forest inventory,\nspectroscopy, and laboratory.\nForests 2022,\
    \ 13, 911\n17 of 31\nCombining a visual ﬁeld assessment with a forest inventory\
    \ allows us to determine\nthe defoliation rates for each sample tree [55] and\
    \ characterize the stand using fundamental\ndendrometric variables, such as diameter\
    \ at breast height (DBH), tree height, and the social\ncategorization of trees\
    \ [12]. In addition, a preexisting continuous forest inventory may be\nhelpful\
    \ to complement monitorization and generate collections of trees to use in studies\
    \ as\nground references [86].\nClassical damage identiﬁcation and sampling methods\
    \ are limited in detecting changes\nafter infections or pest attacks. In this\
    \ sense, other ﬁeld collection strategies can be applied,\nsuch as biochemical\
    \ parameters using spectrometers [80] to measure the leaf chlorophyll\ncontent\
    \ (Cab) and water content (WC) of each tree, spectral measurements, laboratory\n\
    analysis [72], and the assessment of leaf area index (LAI) using a plant canopy\
    \ analyzer [62].\n3.5. Data Processing and Analytical Methods\n3.5.1. Spatial\
    \ Unit Analysis\nRegarding spatial unit analysis, 67.4% (33 studies) used an object-based\
    \ approach\nand 22.4% used a pixel-based approach (11 studies), and both were\
    \ applied by 10.2%\n(5 studies). As a minimal unit in a digital image, pixels\
    \ may be used for every scale\nstudy. However, only spectral properties are considered\
    \ in analytical methods, while\nobject-based approaches are performed using segmentation\
    \ approaches that group objects\nbased on statistical or feature similarities.\
    \ This approach is mainly performed before\nfeature extraction and applying classiﬁers,\
    \ since these methods cannot add contextual\ninformation [28].\nThe authors preferred\
    \ the object-based approach due to its high-resolution imagery and\nsubmeter resolution\
    \ (<1 m), making it possible to perform an individual tree crown extraction\n\
    and delineation by substituting the traditional fieldwork [19,44,50]. Zhang et\
    \ al. [55] stress that\ntree crown extraction is a prerequisite for diseased detection\
    \ and mapping.\n3.5.2. Segmentation of Single Tree Objects\nTable 7 summarizes\
    \ the segmentation single tree methods used in the studies. Individ-\nual tree\
    \ crown delineation (ITCD) studies using photogrammetric or LiDAR point-cloud\n\
    utilize a canopy height model (CHM) or digital height model (DSM) to calculate\
    \ the local\nmaximum height value. For example, they ﬁnd treetops or locate trees\
    \ using algorithms\nsuch as local maxima ﬁltering, image binarization, scale analysis,\
    \ and template matching.\nTree delineations are grouped in valley following, region\
    \ growing, and watershed segmen-\ntation. Treetops are usually used as a seed\
    \ for region growing and watershed segmentation.\nTherefore, this process is required\
    \ prior to crown delineation [106]. Many studies combine\ntree detection and crown\
    \ delineation to extract the crown shape [107,108].\nForests 2022, 13, 911\n18\
    \ of 31\nTable 7. Summary of segmentation single tree methods in the studies.\n\
    Segmentation Single Tree\nMethod\nSynopsis\nStudies\nManually\nManually segmented\
    \ trees\nDigitalization of each tree crown above imagery using GIS software.\n\
    [15,39,40,50,54,56,60,64,68,79,80]\nLocal maxima ﬁlter and Buffer\nLocal maxima\
    \ ﬁlter within a rasterized CHM to detect the treetops, then a buffer applied\
    \ on\nthe treetop using GIS software.\n[39,46,48,84,85]\nRaster-based\nMean shift\
    \ algorithm\nGEOBIA method. Multispectral image segmentation using ArcGIS segment\
    \ mean shift tool.\n[66]\nMultiresolution segmentation\nGEOBIA method. Multispectral\
    \ image segmentation using eCognition software\nmultiresolution segmentation tool.\n\
    [12,61,83]\nLocal maxima ﬁlter and mean shift algorithm\nLocal maxima of a sliding\
    \ window using the brightness of the multispectral image. Then,\nthe select by\
    \ location tool is used between treetops and for large-scale mean shift algorithm\n\
    segments (GEOBIA).\n[57]\nSafonova et al. Wavelet-based local thresholding\nTree\
    \ crown delineation using RGB images. The steps are contrast enhancement, crown\n\
    segmentation based on wavelet transformation and morphological operations, and\n\
    boundary detection.\n[52]\nSafonova et al. Treetop detection\nRGB images are transformed\
    \ into one grey-scale band image; next, the grey-scale band\nimage is converted\
    \ into a blurred image; ﬁnally, the blurred image is converted into a\nbinary\
    \ image.\n[47]\nVoronoi Tesselations\nLocal maxima ﬁlter within a rasterized CHM\
    \ calculates the treetops and then uses a Voronoi\ntessellation algorithm [110].\n\
    [65]\nDalponte individual tree segmentation\nLocal maxima within a rasterized\
    \ CHM calculates the treetops and then uses a\nregion-growing algorithm for individual\
    \ segmentation [111,112].\n[50,59]\nWatershed segmentation\nVicent and Soille\
    \ original algorithm [113]. When the CHM is inverted, tree tops or\nvegetation\
    \ clusters look like “basins”.\n[49]\nMarker-controlled watershed [109]. Marker\
    \ and segmentation functions are used for\nmulti-tree identiﬁcation and segmentation\
    \ using rasterized CHM [114].\n[50,86]\nBinary watershed analysis and the Euclidean\
    \ distance using rasterized CHM or NIR band.\n[69,79]\nHyyppä et al. [115] methodology.\n\
    [43]\nNyguen Treetops in nDSM data\nBased on pixel intensity, an iterative sliding\
    \ window is passed over the nDSM. Finally, the\nreﬁnement is applied to eliminate\
    \ treetops that are too close to each other.\n[53]\nVector-based\n3D region-growing\
    \ algorithm\n3D region-growing algorithm applied in a point cloud (LiDAR or photogrammetric)\
    \ using a\nbuilt-in function for treetop detection [116].\n[50,63,79]\n3D segmentation\
    \ of single trees\nPoint cloud-based method with tree segmentation using a normalized\
    \ cut algorithm [117].\n[87]\nVoxel-based single tree\nLidar point cloud data\
    \ are converted into voxels in order to estimate the leaf area density\nand the\
    \ construction of the 3D forest scene.\n[63]\nForests 2022, 13, 911\n19 of 31\n\
    The data aggregation was performed using Zhen et al.’s [109] categories. The seg-\n\
    mentation of single tree classes established were manually digitalized, raster-based,\
    \ and\nvector-based.\nWe highlight the manual method that was used in 12 studies,\
    \ which consisted of man-\nual tree crown delineation using geographic information\
    \ system (GIS) software. The local\nmaxima ﬁlter with a posterior buffer was applied\
    \ in ﬁve studies.\nThe most frequently used raster-based method was watershed\
    \ segmentation in six\nstudies, with the original and marker-controlled variants.\
    \ Concerning the region-growing\nalgorithms used, the Dalponte individual tree\
    \ segmentation was applied in two studies and\nVoronoi tessellation in one. Subsequently,\
    \ geospatial object-based image analysis (GEOBIA)\nusing multispectral or RGB\
    \ image segmentation methods were used with multiresolution\nsegmentation (3)\
    \ and the mean shift algorithm (1). Original approaches such as wavelet-\nbased\
    \ local thresholding and treetops using normalized digital surface model (nDSM)\
    \ data\nwere also found in the studies.\nThe vector-based approaches were the\
    \ 3D region-growing algorithm (4) and the\nnormalized cut algorithm (1).\nThe\
    \ analysis of the summarized methods revealed that the manual approach was\npreferred\
    \ since it avoids background noise such as shadows and other vegetation types.\n\
    On the other hand, using the manual approach prevented missing tree crowns or\
    \ potential\nerrors from interpolation and smoothing procedures [109], which are\
    \ the disadvantages of\nraster-based methods. However, the manual approach could\
    \ be unsustainable when areas\nhave many trees.\nRaster-based methods are easy\
    \ to implement, despite the drawbacks mentioned earlier.\nVector-based techniques\
    \ could be useful for detecting small and understory trees, despite\nbeing harder\
    \ to implement [118].\n3.5.3. Feature Extraction and Selection\nTable 8 summarizes\
    \ the feature extraction techniques for UAV imagery applied in the\nstudies. The\
    \ investigated features used in the studies were catalogued according to the types\n\
    suggested by the authors in [119,120]. These features are obtainable attributes\
    \ or properties\nof objects and scenes [121]. They are computed from original\
    \ bands or a combination\nof bands [122]. They include spectral features, textural\
    \ features, linear transformations,\nmultisensors and multitemporal images. Geo-auxiliary\
    \ features extracted from LiDAR [118]\nor photogrammetric point clouds [103] include\
    \ digital surface models (DSM), canopy height\nmodels (CHM), individual tree detection,\
    \ and topographic features. Spectral features,\nincluding statistics of original\
    \ bands, ratios between bands, and vegetation indices, were the\nmost popular\
    \ feature type, followed geo-auxiliary features. Variables including multisensor\n\
    and multitemporal imagery were used in three studies. Textural and linear transformations\n\
    were also used in three studies.\nTable 8. Summary of feature extraction techniques\
    \ of UAV imagery applied in the studies.\nFeature Type\nDescription\nStudies\n\
    Spectral features\nStatistics of original bands, ratios between\nbands, vegetation\
    \ indices\n[12,15,39,40,43–46,48–51,54–64,66,68–72,75,77,79–87]\nTextural features\n\
    Gray level co-occurrence matrix (GLCM),\ngrey level difference vector (GLDV)\n\
    [48,68,86]\nLinear transformations\nHue, saturated and intensity (HSI), principal\n\
    component analysis (PCA)\n[55,61,79]\nGeo-auxiliary\nOriginal and normalized digital\
    \ surface\nmodels (DSM) such as digital elevation\nmodels (DEM), canopy height\
    \ models\n(CHM), slope, aspect, height percentiles\n[12,39,48,50,53,54,62,63,65,68,71,81,85–87]\n\
    Forests 2022, 13, 911\n20 of 31\nTable 8. Cont.\nFeature Type\nDescription\nStudies\n\
    Multisensor\nInclusion of data obtained from different\nsensors in analytical\
    \ methods\n[44,62,79,84,87]\nMultitemporal\nInclusion of multitemporal data classiﬁcation\n\
    in analytical methods\n[15,40,48,59,69]\nSpectral features are applied to explain\
    \ differences in particular symptoms of canopy [19,123].\nFor instance, Klouˇcek\
    \ et al. [46] calculated selected vegetation indices and evaluated\nthem based\
    \ on visual differences in the spectral curves of infested and healthy trees.\n\
    The classiﬁcation included Greenness Index (GI), Simple Ratio (SR), Green Ratio\
    \ Vegetation\nIndex (GRVI), Normalized Difference Vegetation Index (NDVI), and\
    \ Green Normalized\nDifference Vegetation Index (GNDVI).\nIn terms of geo-auxiliary\
    \ features used to improve the analytical methods, Minaˇrík\net al. [50] extracted\
    \ elevation features (crown area, height percentiles) and three vegetation\nindices\
    \ (NDVI, Normalized Difference Red-Edge Index (NDRE), and Enhanced Normalized\n\
    Difference Vegetation Index (ENDVI)) to detect a bark beetle disturbance in a\
    \ mixed urban\nforest. We highlight Nguyen et al. [53], who used nine orthomosaics\
    \ and normalized\ndigital surface models (nDSM) to detect and classify healthy\
    \ and declining ﬁr trees and\ndeciduous trees.\nConsidering the inclusion of multitemporal\
    \ features in the analytical methods, we\nnoticed that Abdollahnejad and Panagiotidis\
    \ [48] used a combination of bi-temporal in-\ntegrated spectral and textural information\
    \ to discriminate tree species and health status,\nachieving a satisfactory result.\
    \ Using multisensor features, Lin et al. [63] assessed the poten-\ntial of a hyperspectral\
    \ approach, a lidar approach, and a combined approach to characterize\nindividual\
    \ trees’ pine shoot beetle (PSB) damage.\nAlthough less applied, the authors used\
    \ textural features such as GLCM by Guerra-\nHernández [65] and linear transformation\
    \ through HSI [61].\nRegarding feature selection, 83.7% of the studies selected\
    \ variables without reduction\ntechniques. The authors of [52,57,83] used the\
    \ mean decrease in impurity (MDI) test to\nquantify the importance of features\
    \ and excluded the less critical features. For example,\nYu et al. [79,81] and\
    \ Zhang et al. [56] used principal component analysis (PCA) to reduce\nthe data\
    \ dimensionally. Recursive feature elimination (RFE) for each ﬂight campaign\n\
    was applied by Pádua et al. [69]. Finally, Yu et al. [80] calculated the Pearson’s\
    \ correla-\ntion coefﬁcient between the features and used a stepwise regression\
    \ method to test the\nmulticollinearity between them, excluding redundant variables.\n\
    The above studies show that feature extraction and selection may improve the analyti-\n\
    cal methods applied to discriminate between unhealthy and healthy canopies. However,\n\
    a small portion of the studies used a feature reduction or selection technique.\
    \ This result\ncan be explained by the high use of a stable image classiﬁcation\
    \ algorithm such as random\nforest, which is insensitive to the dimensionality\
    \ of data [124,125]. On the other hand, most\nauthors probably calculated a limited\
    \ number of features due to high correlation. Other\nreasons may have been to\
    \ avoid overﬁtting, a decrease in classiﬁcation accuracy, or high\ncomputational\
    \ costs [126].\n3.5.4. Analysis Type, Algorithms, and Overall Accuracy (OA)\n\
    Figure 11 summarizes the algorithms used in the studies by analysis method. We\
    \ have\nconsidered only the best performance of the algorithms in terms of accuracy\
    \ or shared\nmetric for the ease of describing the different algorithms used.\n\
    Forests 2022, 13, 911\n21 of 31\n \nindividual tree crown delineation (ITCD) algorithm.\
    \ We found three studies using\nregression (LR), one using logistic regression\
    \ (LOG), and two using RF regression m\nWe also highlighted the class “Others”,\
    \ which included the radiosity applicable to \nindividual objects (RAPID) model,\
    \ the ISIC-SPA-P-PLSR framework, histogram an\nand Getis Order GI among the different\
    \ analytical methods. \n \nFigure 11. Summary of the algorithms used in the studies:\
    \ CNN: convolutional neural n\nITCD: individual tree crown delineation; KNN: K-nearest\
    \ neighbor; LOGR: logistic regress\nlinear regression; MLC: maximum likelihood;\
    \ MSS: multiscale segmentation; PLS: part\nsquares; RF: random forest; SVM: support\
    \ vector machine; TA: thresholding analysis; XGBo\ntreme gradient boosting. \n\
    Figure 12. The overall accuracy of the different classifiers. \nFigure 11. Summary\
    \ of the algorithms used in the studies: CNN: convolutional neural network; ITCD:\n\
    individual tree crown delineation; KNN: K-nearest neighbor; LOGR: logistic regression;\
    \ LR: linear\nregression; MLC: maximum likelihood; MSS: multiscale segmentation;\
    \ PLS: partial least squares;\nRF: random forest; SVM: support vector machine;\
    \ TA: thresholding analysis; XGBoost: eXtreme\ngradient boosting.\nThe classiﬁcation\
    \ approach is broadly used for quantifying trees. Regarding the\nanalysis methods,\
    \ most of the studies (79.6%) used a classiﬁcation approach, 12.2% used\nregression\
    \ and other methods such as statistical analysis and histogram analysis, and 8.2%\n\
    used damage by stressors, different types of species, and the total area affected.\
    \ Regres-\nsion studies focus on a different level of damage and provide statistical\
    \ signiﬁcance for\nregression coefﬁcients and the relation between classes. Statistical\
    \ methods, physically\nbased models such as radiosity applicable to porous individual\
    \ objects to calculate dif-\nferent vegetation variables, and speciﬁc frameworks\
    \ were also used to estimate the level\nof damage.\nAs previously stated, most\
    \ of the studies used the classiﬁcation approach. The classi-\nﬁers most used\
    \ in the classiﬁcation approach were the random forest (RF) and convolutional\n\
    neural network (CNN), with 11 studies (Figure 12). Five studies applied the support\
    \ vector\nmachine (SVM) algorithm, and three applied K-Nearest Neighbor (KNN)\
    \ and the individ-\nual tree crown delineation (ITCD) algorithm. We found three\
    \ studies using linear regression\n(LR), one using logistic regression (LOG),\
    \ and two using RF regression models. We also\nhighlighted the class “Others”,\
    \ which included the radiosity applicable to porous individ-\nual objects (RAPID)\
    \ model, the ISIC-SPA-P-PLSR framework, histogram analysis, and Getis\nOrder GI\
    \ among the different analytical methods.\nForests 2022, 13, 911\n22 of 31\n \n\
    \ \nFigure 11. Summary of the algorithms used in the studies: CNN: convolutional\
    \ neural network; \nITCD: individual tree crown delineation; KNN: K-nearest neighbor;\
    \ LOGR: logistic regression; LR: \nlinear regression; MLC: maximum likelihood;\
    \ MSS: multiscale segmentation; PLS: partial least \nsquares; RF: random forest;\
    \ SVM: support vector machine; TA: thresholding analysis; XGBoost: eX-\ntreme\
    \ gradient boosting. \n \nFigure 12. The overall accuracy of the different classifiers.\
    \ \nFigure 12. The overall accuracy of the different classiﬁers.\nThe main CNN\
    \ architectures used were R-CNN [77], AlexNET [74], YOLOv3 [76],\nResNet50 [53,82],\
    \ DeepLabv3+ [78], PointNet [87], 3D-ResCNN [81], 3D-CNN [51],\nSCANet [75], and\
    \ other types [47].\nOut of 32 studies that calculated OA, we used only 28 for\
    \ this analysis (Figure 12).\nSome algorithms, such as logistic regression, linear\
    \ regression, the 3D rapid model, the\nISI-SPA-P-PLSR framework, and thresholding\
    \ analysis, were ruled out of the study because\nthey had only one sample. Figure\
    \ 12 shows that the median overall accuracy for all\nalgorithms used was higher\
    \ than 0.85. SVM achieved the highest median (0.93), followed\nby ITCD and CNN\
    \ with 0.90 and 0.88, respectively. The best algorithm performance was\nSVM (0.99)\
    \ and ITCD (0.99). The worst performance was RF (0.55).\nA wide range of non-parametric\
    \ and parametric algorithms have been applied in FIPD\nmonitoring. Non-parametric\
    \ machine learning algorithms that input data are not limited to\nnormal distribution,\
    \ such as RF, CNN, and SVM, and are preferred to quantify and detect\ndamages.\
    \ RF can be used for classiﬁcation and regression problems, allowing a straightfor-\n\
    ward interpretation of the model structure and determining the variable importance.\
    \ For\nexample, the authors of [72] used pixel-based RF to classify three levels\
    \ of PWD infection\n(infected, suspicious, and health) and achieved 95% accuracy.\
    \ Duarte et al. [57] performed\nlarge-scale mean shift segmentation (LSMSS) on\
    \ a single-date multispectral image to extract\ntree crowns with binary classiﬁcation\
    \ (healthy or dead trees) and achieved 98% overall\naccuracy using a RF algorithm.\n\
    CNN is a class of deep learning algorithm most used for spatial pattern analysis\
    \ in\nremote sensing imagery. Classiﬁcation and regression approaches could be\
    \ used in remotely\nsensed data in the following tasks: scene-wise classiﬁcation,\
    \ object detection, semantics,\nand instance segmentation [127,128]. We highlight\
    \ the work of Safonova et al. [47] which\nincludes two steps to detect bark beetle\
    \ damage in a ﬁr forest using an RGB image. First,\nthe authors applied a strategy\
    \ to ﬁnd the tree crowns, and in the second step, a new\nCNN network architecture\
    \ was used to classify each canopy. The authors using data\naugmentation achieved\
    \ 95% accuracy. Briechle et al. [87] performed classiﬁcation using 3D\nDeep Neural\
    \ Network PointNet ++ using UAV-based LiDAR and multispectral imagery of\nmultiple\
    \ species and standing dead trees. The overall accuracy was 90.2%.\nThe SVM method\
    \ based on statistical learning theory has been commonly used in\nFIPD detection\
    \ and monitoring [48]. An example is a study by Safonova et al. [52], which\n\
    automated individual tree crown delineation and Gaussian SVM to extract particular\n\
    species pixel by pixel and assess the tree canopy vitality. The authors achieved\
    \ 99% overall\naccuracy applying this methodology in a multispectral image.\n\
    In the ﬁrst stage, the strategies adopted in different classiﬁcation approaches—manual\n\
    or automated tree segmentation—were applied to process the tree crown delineation.\
    \ Next,\nForests 2022, 13, 911\n23 of 31\nthe authors used spectral indices, topographic\
    \ variables, texture, or contextual information\nbased on different image types.\n\
    3.6. Pre-Processing and Analysis Software\nMost of the studies used more than\
    \ one processing and analysis software. Therefore, to\ncount them, all the software\
    \ brands in each paper were considered. Figure 13a illustrates the\npreferred\
    \ processing software used in the studies. The photogrammetric software Agisoft\n\
    (Metashape or Photoscan) was used in 27 studies, followed by Pix4D Mapper in 6\
    \ studies.\nThe point cloud processing software LiDAR 360 was used in four studies.\
    \ For hyperspectral\nimagery, Spectronon processing was used in three studies\
    \ and Headwall Spectral View was\nalso used in three. As shown in Figure 13b,\
    \ ArcGIS was used in 17 studies, Python libraries\nin 15, and R software in 13\
    \ studies.\nForests 2022, 13, x FOR PEER REVIEW \n24 of 32 \n \nremote sensing.\
    \ Sophisticated computer vision algorithms are also available on Python \nlibraries\
    \ such as Tensorflow, Pytorch, and scikit-learn, which are easy to implement and\
    \ \nadaptable to other code languages. \n \n(a) \n(b) \nFigure 13. Processing\
    \ and analysis software applied in the studies. (a) Image processing software\
    \ \nbrands; (b) analysis software used. \n4. Research Gaps, Challenges, and Further\
    \ Research \nThe scientific literature analyzed shows serious concern regarding\
    \ improving the de-\ntection and monitoring of pests and diseases using UAV data.\
    \ Proof of this is the increased \nnumber of studies in recent years. However,\
    \ we found that most studies were carried out \nin small experimental areas that\
    \ did not always represent the reality of disturbances in \nforests. In addition,\
    \ the effects of climate change could promote the development of other \npests\
    \ and diseases. \nOur systematic review analysis highlights that the first research\
    \ gap is related to the \nlack of flight parameter standards in FIPD monitoring,\
    \ since each case is unique. Moreo-\nver, there is no base protocol for different\
    \ UAV systems or sensor types. Therefore, the \nUAV type, sensor type, flight\
    \ parameters, pre-processing and processing steps, weather \nconditions, and regulations\
    \ can affect the results. Hence, providing a detailed description \nof all flight\
    \ parameters and processing activities is essential to be taken as a reference\
    \ for \npractitioners, researchers, and forest professionals [29,92]. \nA second\
    \ research gap is how to take advantage of different UAV imagery and point \n\
    clouds to detect pests and diseases. Many features can be extracted from UAV-based\
    \ im-\nages and point clouds, such as spectral indices, gray level co-occurrence\
    \ matrices \n(GLCMs), digital surface models (DSMs), digital terrain models (DTMs),\
    \ and point cloud \nmetrics, to integrate classification or regression models.\
    \ We found a lack of studies using \ndata fusion between optical sensors and LiDAR.\
    \ Combining these technologies is advan-\ntageous for studying vegetation structure,\
    \ especially tree crown delineation [54,85]. One \nFigure 13. Processing and analysis\
    \ software applied in the studies. (a) Image processing software\nbrands; (b)\
    \ analysis software used.\nAgisoft was the most popular software due to its automatic\
    \ image quality assessment\nadvantages of excluding low-quality images and its\
    \ standardized workﬂow [23]. Pix4D\nmapper was the second preferred software due\
    \ to the dedicated and automated photogram-\nmetry workﬂow. The software for imagery\
    \ classiﬁcation with the most occurrences was\nArcGIS, which is user friendly\
    \ and offers a complete and standardized workﬂow for remote\nsensing. Sophisticated\
    \ computer vision algorithms are also available on Python libraries\nsuch as Tensorﬂow,\
    \ Pytorch, and scikit-learn, which are easy to implement and adaptable\nto other\
    \ code languages.\n4. Research Gaps, Challenges, and Further Research\nThe scientiﬁc\
    \ literature analyzed shows serious concern regarding improving the de-\ntection\
    \ and monitoring of pests and diseases using UAV data. Proof of this is the increased\n\
    number of studies in recent years. However, we found that most studies were carried\
    \ out\nin small experimental areas that did not always represent the reality of\
    \ disturbances in\nforests. In addition, the effects of climate change could promote\
    \ the development of other\npests and diseases.\nForests 2022, 13, 911\n24 of\
    \ 31\nOur systematic review analysis highlights that the ﬁrst research gap is\
    \ related to the\nlack of ﬂight parameter standards in FIPD monitoring, since\
    \ each case is unique. Moreover,\nthere is no base protocol for different UAV\
    \ systems or sensor types. Therefore, the UAV type,\nsensor type, ﬂight parameters,\
    \ pre-processing and processing steps, weather conditions,\nand regulations can\
    \ affect the results. Hence, providing a detailed description of all ﬂight\nparameters\
    \ and processing activities is essential to be taken as a reference for practitioners,\n\
    researchers, and forest professionals [29,92].\nA second research gap is how to\
    \ take advantage of different UAV imagery and point\nclouds to detect pests and\
    \ diseases. Many features can be extracted from UAV-based images\nand point clouds,\
    \ such as spectral indices, gray level co-occurrence matrices (GLCMs),\ndigital\
    \ surface models (DSMs), digital terrain models (DTMs), and point cloud metrics,\
    \ to\nintegrate classiﬁcation or regression models. We found a lack of studies\
    \ using data fusion\nbetween optical sensors and LiDAR. Combining these technologies\
    \ is advantageous for\nstudying vegetation structure, especially tree crown delineation\
    \ [54,85]. One obstacle is the\ndifﬁculty in performing image alignment due to\
    \ the repeated patterns in forests [129]. On\nthe other hand, the high price of\
    \ sensors is also an important constraint.\nIn terms of feature extraction, we\
    \ noticed that genetic programming (GP) was used\nin the studies to combine spectral\
    \ bands in satellite imagery [130], to improve the process\nof land use and land\
    \ cover (LULC) classiﬁcation [131], and to identify burned areas [132].\nAlthough\
    \ the work of Mejia-Zuluaga et al. [133] is outside of the time interval of this\n\
    study, we veriﬁed that the GP algorithm applied achieved an overall accuracy of\
    \ 96% for\nclassifying mistletoe. In this sense, this approach shows the ability\
    \ to extract features and\nimprove damage classiﬁcation. However, to the best\
    \ of our knowledge, multiclass genetic\nprogramming (M3GP) has not been used in\
    \ UAV images to classify different vegetation\nvitality.\nDeep learning approaches\
    \ such as CNN may be a robust option for segmentation tasks\nand identifying different\
    \ levels of tree crown vitality, as revealed in the studies performed\nin [47,51,53,74,78].\n\
    The third research gap is the limitation of UAV-based imagery to cover large scales.\n\
    This limitation was highlighted by Eugenio et al. [26,29], who noted that UAVs\
    \ can be\n“upscaled” for satellites for expansion without losing accuracy.\nThe\
    \ challenges of FIPD monitoring include the recently imposed UAV regulations.\n\
    Flight altitude is limited to 120 m and a maximum radius of 500 m, and BVLOS rules\
    \ are\nproblematic for forest surveys.\nEach year, UAV technologies are improved.\
    \ UAV and sensor miniaturization bring\nnew challenges. For example, the battery\
    \ duration issues are now a reality (for instance, the\nMatrice 300 battery has\
    \ a duration of 55 min); however, increasingly efﬁcient drones such\nas VTOLs\
    \ may be used to improve research (Wingtra drone series). However, the miniatur-\n\
    ization of hyperspectral cameras and their image collection process is being increasingly\n\
    improved so, in the short term, costs may drop importantly.\nOne of the biggest\
    \ challenges is the vulgarization of drones to complement or sub-\nstitute ﬁeld\
    \ data collection. Eugenio et al. [26,29] stress that breaking resistance and\n\
    disseminating UAVs in the forest community is essential to monitoring our forests.\n\
    Future research will beneﬁt from upscaling with satellite imagery to increase\
    \ area\ncoverage and improve early detection systems. Dash et al. [40] found that\
    \ RapidEye satellite\ndata can expand stress monitoring and be improved with UAV\
    \ sensor data. Therefore,\narea coverage can be increased by combining these different\
    \ platforms [39]. To this end,\nintelligent algorithms based on deep learning\
    \ and genetic programming are necessary for\ndetecting and monitoring disturbances\
    \ in forest contexts.\n5. Conclusions\nThis systematic literature review aimed\
    \ to identify the contribution of UAVs to forest in-\nsect pest and disease monitoring.\
    \ Using a PRISMA protocol, we reviewed 49 peer-reviewed\nForests 2022, 13, 911\n\
    25 of 31\narticles on FIPD monitoring to provide readers with the current practices\
    \ and techniques\nand to identify knowledge gaps and challenges.\nWe conclude\
    \ that the number of papers has increased in recent years, especially in\n2021\
    \ (18 articles). Based on our analysis, China and European Union (EU) countries\
    \ are the\nones with more studies about FIPD monitoring using UAV-based data.\
    \ The most studied\ndiseases were pine wilt disease (PWD) and the most common\
    \ pests were bark beetles (BB).\nPine, European spruce, and ﬁr were the conifers\
    \ most frequently studied, while the most\ncommon hardwoods were eucalypts.\n\
    Rotary-wing drones were the most frequently used due to the market and costs.\n\
    Our ﬁndings document that multispectral and visible light is preferred to monitor\
    \ FIPDs.\nRegarding RGB sensors, the DJI Phantom series camera was the most widely\
    \ used, while\nthe Micasense series was most used for multispectral segments.\
    \ In addition, we noticed an\nincrease in hyperspectral and LiDAR sensors in the\
    \ research of FIPDs.\nDespite the lack of standards for UAV data collection for\
    \ FIPDs, our ﬁndings may\nconstitute a reference for further research. We found\
    \ a positive correlation between GSD and\nﬂight altitude by sensor type and the\
    \ median of frontal and side overlap concerning visible,\nmultispectral, and hyperspectral\
    \ sensors. Most studies included ﬁeldwork to validate the\nresearch, and a signiﬁcant\
    \ number performed radiometric and geometric calibration.\nConcerning the methodological\
    \ approach of the studies, most works used an object-\nbased analysis unit. Due\
    \ to the high spatial resolution of the images, the authors of these\nstudies\
    \ applied several types of methods for tree crown delineation. Tree crown delineation\n\
    is an essential prerequisite for FIPD detection and monitoring. The spectral and\
    \ geo-\nauxiliary features were most used in feature extraction and selection.\
    \ Regarding analytical\nmethods, random forests (RF) and deep learning (DL) classiﬁers\
    \ were the most frequently\napplied in UAV imagery processing.\nOur literature\
    \ review suggests the lack of ﬂight parameter standards in FIPD mon-\nitoring.\
    \ Data fusion procedures for studying vegetation structure could potentially be\n\
    improved by combining optical and LiDAR technologies. Other possible improvements\n\
    for feature extraction include evolutionary algorithms, such as multiclass genetic\
    \ program-\nming. Deep learning algorithms can be fundamental for pattern recognition\
    \ and automatic\ndata processing regarding classiﬁcation or regression.\nFinally,\
    \ upscaling UAV data for satellites to expand data collection without losing\n\
    accuracy is essential for monitoring our forests.\nAuthor Contributions: Conceptualization,\
    \ A.D. and M.C.; methodology, A.D.; software, A.D.; formal\nanalysis, A.D.; data\
    \ curation, A.D.; writing—original draft preparation, A.D., P.C. and M.C.; writing—\n\
    review and editing, A.D., P.C., N.B. and M.C.; visualization, A.D.; supervision,\
    \ M.C.; funding\nacquisition, M.C. All authors have read and agreed to the published\
    \ version of the manuscript.\nFunding: This study was supported by national funds\
    \ through Fundação para a Ciência e a Tecnologia\n(FCT) under the project UIDB/04152/2020—Centro\
    \ de Investigação em Gestão de Informação (MagIC).\nInstitutional Review Board\
    \ Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData\
    \ Availability Statement: Not applicable.\nAcknowledgments: The authors would\
    \ like to thank Cindy Santos, Luís Acevedo-Muñoz, João\nRocha and Sérgio Fabres,\
    \ for all their valuable comments and support.\nConﬂicts of Interest: The authors\
    \ declare no conﬂict of interest.\nReferences\n1.\nAnderegg, W.R.L.; Trugman,\
    \ A.T.; Badgley, G.; Anderson, C.M.; Bartuska, A.; Ciais, P.; Cullenward, D.;\
    \ Field, C.B.; Freeman, J.;\nGoetz, S.J.; et al. Climate-Driven Risks to the Climate\
    \ Mitigation Potential of Forests. Science 2020, 368, eaaz7005. [CrossRef]\n[PubMed]\n\
    2.\nFAO. Assessing Forest Degradation: Towards the Development of Globally Applicable\
    \ Guidlines; Forest Resources Assessment Working\nPaper 177; Food and Agriculture\
    \ Organization of the United Nations: Rome, Italy, 2011.\nForests 2022, 13, 911\n\
    26 of 31\n3.\nCanadell, J.G.; Raupach, M.R. Managing Forests for Climate Change\
    \ Mitigation. Science 2008, 320, 1456–1457. [CrossRef]\n[PubMed]\n4.\nFAO. Climate\
    \ Change Guidelines for Forest Managers; FAO Forestry Paper 172; Food and Agriculture\
    \ Organization of the United\nNations: Rome, Italy, 2013; p. 123.\n5.\nFAO. Managing\
    \ Forests for Climate Change; Food and Agriculture Organization of the United\
    \ Nations: Rome, Italy, 2010; p. 20.\n6.\nDale, V.; JOYCE, L.; Mcnulty, S.; Neilson,\
    \ R.; Ayres, M.; Flannigan, M.; Hanson, P.; Irland, L.; Lugo, A.; PETERSON, C.;\
    \ et al.\nClimate Change and Forest Disturbances. BioScience 2001, 51, 723–734.\
    \ [CrossRef]\n7.\nSenf, C.; Buras, A.; Zang, C.S.; Rammig, A.; Seidl, R. Excess\
    \ Forest Mortality Is Consistently Linked to Drought across Europe.\nNat. Commun.\
    \ 2020, 11, 6200. [CrossRef]\n8.\nSeidl, R.; Spies, T.A.; Peterson, D.L.; Stephens,\
    \ S.L.; Hicke, J.A. Searching for Resilience: Addressing the Impacts of Changing\n\
    Disturbance Regimes on Forest Ecosystem Services. J. Appl. Ecol. 2016, 53, 120–129.\
    \ [CrossRef]\n9.\nKoricheva, J.; Castagneyrol, B. Science Direct Responses of\
    \ Forest Insect Pests to Climate Change: Not so Simple. Curr. Opin.\nInsect Sci.\
    \ 2019, 35, 103–108. [CrossRef]\n10.\nRaffa, K.F.; Aukema, B.H.; Bentz, B.J.;\
    \ Carroll, A.L.; Hicke, J.A.; Turner, M.G.; Romme, W.H. Cross-Scale Drivers of\
    \ Natural\nDisturbances Prone to Anthropogenic Ampliﬁcation: The Dynamics of Bark\
    \ Beetle Eruptions. BioScience 2008, 58, 501–517.\n[CrossRef]\n11.\nLausch, A.;\
    \ Borg, E.; Bumberger, J.; Dietrich, P.; Heurich, M.; Huth, A.; Jung, A.; Klenke,\
    \ R.; Knapp, S.; Mollenhauer, H.; et al.\nUnderstanding Forest Health with Remote\
    \ Sensing, Part III: Requirements for a Scalable Multi-Source Forest Health Monitoring\n\
    Network Based on Data Science Approaches. Remote Sens. 2018, 10, 1120. [CrossRef]\n\
    12.\nBrovkina, O.; Cienciala, E.; Surový, P.; Janata, P. Unmanned Aerial Vehicles\
    \ (UAV) for Assessment of Qualitative Classiﬁcation of\nNorway Spruce in Temperate\
    \ Forest Stands. Geo-Spat. Inf. Sci. 2018, 21, 12–20. [CrossRef]\n13.\nLausch,\
    \ A.; Heurich, M.; Gordalla, D.; Dobner, H.J.; Gwillym-Margianto, S.; Salbach,\
    \ C. Forecasting Potential Bark Beetle\nOutbreaks Based on Spruce Forest Vitality\
    \ Using Hyperspectral Remote-Sensing Techniques at Different Scales. For. Ecol.\
    \ Manag.\n2013, 308, 76–89. [CrossRef]\n14.\nGómez, C.; Alejandro, P.; Hermosilla,\
    \ T.; Montes, F.; Pascual, C.; Ruiz, L.Á.; Álvarez-Taboada, F.; Tanase, M.A.;\
    \ Valbuena, R.\nRemote Sensing for the Spanish Forests in the 21st century: A\
    \ Review of Advances, Needs, and Opportunities. For. Syst. 2019, 28,\neR001. [CrossRef]\n\
    15.\nDash, J.P.; Watt, M.S.; Pearse, G.D.; Heaphy, M.; Dungey, H.S. Assessing\
    \ Very High Resolution UAV Imagery for Monitoring\nForest Health during a Simulated\
    \ Disease Outbreak. ISPRS J. Photogramm. Remote Sens. 2017, 131, 1–14. [CrossRef]\n\
    16.\nGuimarães, N.; Pádua, L.; Marques, P.; Silva, N.; Peres, E.; Sousa, J.J.\
    \ Forestry Remote Sensing from Unmanned Aerial Vehicles:\nA Review Focusing on\
    \ the Data, Processing and Potentialities. Remote Sens. 2020, 12, 1046. [CrossRef]\n\
    17.\nPoley, L.G.; McDermid, G.J. A Systematic Review of the Factors Inﬂuencing\
    \ the Estimation of Vegetation Aboveground Biomass\nUsing Unmanned Aerial Systems.\
    \ Remote Sens. 2020, 12, 1052. [CrossRef]\n18.\nKlosterman, S.; Richardson, A.\
    \ Observing Spring and Fall Phenology in a Deciduous Forest with Aerial Drone\
    \ Imagery. Sensors\n2017, 17, 2852. [CrossRef]\n19.\nHall, R.J.; Castilla, G.;\
    \ White, J.C.; Cooke, B.J.; Skakun, R.S. Remote Sensing of Forest Pest Damage:\
    \ A Review and Lessons\nLearned from a Canadian Perspective. Can. Entomol. 2016,\
    \ 148, S296–S356. [CrossRef]\n20.\nPádua, L.; Vanko, J.; Hruška, J.; Adão, T.;\
    \ Sousa, J.J.; Peres, E.; Morais, R. UAS, Sensors, and Data Processing in Agroforestry:\n\
    A Review towards Practical Applications. Int. J. Remote Sens. 2017, 38, 2349–2391.\
    \ [CrossRef]\n21.\nRullan-Silva, C.D.; Olthoff, A.E.; Delgado de la Mata, J.A.;\
    \ Pajares-Alonso, J.A. Remote Monitoring of Forest Insect Defoliation.\nA Review.\
    \ For. Syst. 2013, 22, 377–391. [CrossRef]\n22.\nYao, H.; Qin, R.; Chen, X. Unmanned\
    \ Aerial Vehicle for Remote Sensing Applications—A Review. Remote Sens. 2019,\
    \ 11, 1443.\n[CrossRef]\n23.\nManfreda, S.; McCabe, M.; Miller, P.; Lucas, R.;\
    \ Pajuelo Madrigal, V.; Mallinis, G.; Ben Dor, E.; Helman, D.; Estes, L.; Ciraolo,\
    \ G.;\net al. On the Use of Unmanned Aerial Systems for Environmental Monitoring.\
    \ Remote Sens. 2018, 10, 641. [CrossRef]\n24.\nTorresan, C.; Berton, A.; Carotenuto,\
    \ F.; Di Gennaro, S.F.; Gioli, B.; Matese, A.; Miglietta, F.; Vagnoli, C.; Zaldei,\
    \ A.; Wallace, L.\nForestry Applications of UAVs in Europe: A Review. Int. J.\
    \ Remote Sens. 2017, 38, 2427–2447. [CrossRef]\n25.\nAdão, T.; Hruška, J.; Pádua,\
    \ L.; Bessa, J.; Peres, E.; Morais, R.; Sousa, J. Hyperspectral Imaging: A Review\
    \ on UAV-Based Sensors,\nData Processing and Applications for Agriculture and\
    \ Forestry. Remote Sens. 2017, 9, 1110. [CrossRef]\n26.\nEugenio, F.C.; Schons,\
    \ C.T.; Mallmann, C.L.; Schuh, M.S.; Fernandes, P.; Badin, T.L. Remotely Piloted\
    \ Aircraft Systems and Forests:\nA Global State of the Art and Future Challenges.\
    \ Can. J. For. Res. 2020, 50, 705–716. [CrossRef]\n27.\nDainelli, R.; Toscano,\
    \ P.; Di Gennaro, S.F.; Matese, A. Recent Advances in Unmanned Aerial Vehicles\
    \ Forest Remote Sensing—\nA Systematic Review. Part II: Research Applications.\
    \ Forests 2021, 12, 397. [CrossRef]\n28.\nTorres, P.; Rodes-Blanco, M.; Viana-Soto,\
    \ A.; Nieto, H.; García, M. The Role of Remote Sensing for the Assessment and\
    \ Monitoring\nof Forest Health: A Systematic Evidence Synthesis. Forests 2021,\
    \ 12, 1134. [CrossRef]\n29.\nEugenio, F.C.; PereiradaSilva, S.D.; Fantinel, R.A.;\
    \ de Souza, P.D.; Felippe, B.M.; Romua, C.L.; Elsenbach, E.M. Remotely Piloted\n\
    Aircraft Systems to Identify Pests and Diseases in Forest Species: The Global\
    \ State of the Art and Future Challenges. IEEE Geosci.\nRemote Sens. Mag. 2021,\
    \ 10, 2–15. [CrossRef]\nForests 2022, 13, 911\n27 of 31\n30.\nPage, M.J.; McKenzie,\
    \ J.E.; Bossuyt, P.M.; Boutron, I.; Hoffmann, T.C.; Mulrow, C.D.; Shamseer, L.;\
    \ Tetzlaff, J.M.; Akl, E.A.;\nBrennan, S.E.; et al. The PRISMA 2020 Statement:\
    \ An Updated Guideline for Reporting Systematic Reviews. Syst. Rev. 2021, 10,\n\
    89. [CrossRef]\n31.\nColomina, I.; Molina, P. Unmanned Aerial Systems for Photogrammetry\
    \ and Remote Sensing: A Review. ISPRS J. Photogramm.\nRemote Sens. 2014, 92, 79–97.\
    \ [CrossRef]\n32.\nAria, M.; Cuccurullo, C. Bibliometrix: An R-Tool for Comprehensive\
    \ Science Mapping Analysis. J. Informetr. 2017, 11, 959–975.\n[CrossRef]\n33.\n\
    RStudio Team RStudio: Integrated Development Environment for R; RStudio, PBC:\
    \ Boston, MA, USA, 2021.\n34.\nEskandari, R.; Mahdianpari, M.; Mohammadimanesh,\
    \ F.; Salehi, B.; Brisco, B.; Homayouni, S. Meta-Analysis of Unmanned Aerial\n\
    Vehicle (UAV) Imagery for Agro-Environmental Monitoring Using Machine Learning\
    \ and Statistical Models. Remote Sens. 2020,\n12, 3511. [CrossRef]\n35.\nDash,\
    \ J.P.; Watt, M.S.; Paul, T.S.H.; Morgenroth, J.; Hartley, R. Taking a Closer\
    \ Look at Invasive Alien Plant Research: A Review of\nthe Current State, Opportunities,\
    \ and Future Directions for UAVs. Methods Ecol. Evol. 2019, 10, 2020–2033. [CrossRef]\n\
    36.\nZotero; Center for History and New Media at George Mason University: Fairfax,\
    \ VA, USA, 2022.\n37.\nvan Eck, N.J.; Waltman, L. Software Survey: VOSviewer,\
    \ a Computer Program for Bibliometric Mapping. Scientometrics 2010, 84,\n523–538.\
    \ [CrossRef] [PubMed]\n38.\nLausch, A.; Erasmi, S.; King, D.; Magdon, P.; Heurich,\
    \ M. Understanding Forest Health with Remote Sensing-Part II—A Review\nof Approaches\
    \ and Data Models. Remote Sens. 2017, 9, 129. [CrossRef]\n39.\nCardil, A.; Vepakomma,\
    \ U.; Brotons, L. Assessing Pine Processionary Moth Defoliation Using Unmanned\
    \ Aerial Systems. Forests\n2017, 8, 402. [CrossRef]\n40.\nDash, J.; Pearse, G.;\
    \ Watt, M. UAV Multispectral Imagery Can Complement Satellite Data for Monitoring\
    \ Forest Health. Remote\nSens. 2018, 10, 1216. [CrossRef]\n41.\nSenf, C.; Seidl,\
    \ R.; Hostert, P. Remote Sensing of Forest Insect Disturbances: Current State\
    \ and Future Directions. Int. J. Appl.\nEarth Obs. Geoinf. 2017, 60, 49–60. [CrossRef]\n\
    42.\nAdamopoulos, E.; Rinaudo, F. UAS-Based Archaeological Remote Sensing: Review,\
    \ Meta-Analysis and State-of-the-Art. Drones\n2020, 4, 46. [CrossRef]\n43.\nNäsi,\
    \ R.; Honkavaara, E.; Lyytikäinen-Saarenmaa, P.; Blomqvist, M.; Litkey, P.; Hakala,\
    \ T.; Viljanen, N.; Kantola, T.; Tanhuanpää, T.;\nHolopainen, M. Using UAV-Based\
    \ Photogrammetry and Hyperspectral Imaging for Mapping Bark Beetle Damage at Tree-Level.\n\
    Remote Sens. 2015, 7, 15467–15493. [CrossRef]\n44.\nNäsi, R.; Honkavaara, E.;\
    \ Blomqvist, M.; Lyytikäinen-Saarenmaa, P.; Hakala, T.; Viljanen, N.; Kantola,\
    \ T.; Holopainen, M. Remote\nSensing of Bark Beetle Damage in Urban Forests at\
    \ Individual Tree Level Using a Novel Hyperspectral Camera from UAV and\nAircraft.\
    \ Urban For. Urban Green. 2018, 30, 72–83. [CrossRef]\n45.\nMinaˇrík, R.; Langhammer,\
    \ J. Use of a Multispectral UAV Photogrammetry for Detection and Tracking of Forest\
    \ Disturbance\nDynamics. In The International Archives of the Photogrammetry,\
    \ Remote Sensing & Spatial Information Sciences—ISPRS Archives;\nZdimal, V., Ramasamy,\
    \ S.M., Skidmore, A., Altan, O., Comiso, J., Thenkabail, P.S., Halounova, L.,\
    \ Safar, V., Planka, L., Raju, P.L.N.,\net al., Eds.; International Society for\
    \ Photogrammetry and Remote Sensing: Christian Heipke, Germany, 2016; Volume 41,\
    \ pp.\n711–718.\n46.\nKlouˇcek, T.; Komárek, J.; Surový, P.; Hrach, K.; Janata,\
    \ P.; Vašíˇcek, B. The Use of UAV Mounted Sensors for Precise Detection of\nBark\
    \ Beetle Infestation. Remote Sens. 2019, 11, 1561. [CrossRef]\n47.\nSafonova,\
    \ A.; Tabik, S.; Alcaraz-Segura, D.; Rubtsov, A.; Maglinets, Y.; Herrera, F. Detection\
    \ of Fir Trees (Abies Sibirica) Damaged\nby the Bark Beetle in Unmanned Aerial\
    \ Vehicle Images with Deep Learning. Remote Sens. 2019, 11, 643. [CrossRef]\n\
    48.\nAbdollahnejad, A.; Panagiotidis, D. Tree Species Classiﬁcation and Health\
    \ Status Assessment for a Mixed Broadleaf-Conifer\nForest with Uas Multispectral\
    \ Imaging. Remote Sens. 2020, 12, 3722. [CrossRef]\n49.\nHonkavaara, E.; Näsi,\
    \ R.; Oliveira, R.; Viljanen, N.; Suomalainen, J.; Khoramshahi, E.; Hakala, T.;\
    \ Nevalainen, O.; Markelin,\nL.; Vuorinen, M.; et al. Using Multitemporal Hyper-and\
    \ Multispectral UAV Imaging for Detecting Bark Beetle Infestation on\nNorway Spruce.\
    \ In The International Archives of the Photogrammetry, Remote Sensing and Spatial\
    \ Information Sciences—ISPRS Archives;\nPaparoditis, N., Mallet, C., Lafarge,\
    \ F., Jiang, J., Shaker, A., Zhang, H., Liang, X., Osmanoglu, B., Soergel, U.,\
    \ Honkavaara, E., et al.,\nEds.; International Society for Photogrammetry and\
    \ Remote Sensing: Christian Heipke, Germany, 2020; Volume 43, pp. 429–434.\n50.\n\
    Minaˇrík, R.; Langhammer, J.; Lendzioch, T. Automatic Tree Crown Extraction from\
    \ Uas Multispectral Imagery for the Detection\nof Bark Beetle Disturbance in Mixed\
    \ Forests. Remote Sens. 2020, 12, 4081. [CrossRef]\n51.\nMinaˇrík, R.; Langhammer,\
    \ J.; Lendzioch, T. Detection of Bark Beetle Disturbance at Tree Level Using UAS\
    \ Multispectral Imagery\nand Deep Learning. Remote Sens. 2021, 13, 4768. [CrossRef]\n\
    52.\nSafonova, A.; Hamad, Y.; Dmitriev, E.; Georgiev, G.; Trenkin, V.; Georgieva,\
    \ M.; Dimitrov, S.; Iliev, M. Individual Tree Crown\nDelineation for the Species\
    \ Classiﬁcation and Assessment of Vital Status of Forest Stands from UAV Images.\
    \ Drones 2021, 5, 77.\n[CrossRef]\n53.\nNguyen, H.T.; Caceres, M.L.L.; Moritake,\
    \ K.; Kentsch, S.; Shu, H.; Diez, Y. Individual Sick Fir Tree (Abies mariesii)\
    \ Identiﬁcation in\nInsect Infested Forests by Means of UAV Images and Deep Learning.\
    \ Remote Sens. 2021, 13, 260. [CrossRef]\n54.\nCessna, J.; Alonzo, M.G.; Foster,\
    \ A.C.; Cook, B.D. Mapping Boreal Forest Spruce Beetle Health Status at the Individual\
    \ Crown\nScale Using Fused Spectral and Structural Data. Forests 2021, 12, 1145.\
    \ [CrossRef]\nForests 2022, 13, 911\n28 of 31\n55.\nZhang, N.; Wang, Y.; Zhang,\
    \ X. Extraction of Tree Crowns Damaged by Dendrolimus Tabulaeformis Tsai et Liu\
    \ via Spectral-Spatial\nClassiﬁcation Using UAV-Based Hyperspectral Images. Plant\
    \ Methods 2020, 16, 1–9. [CrossRef]\n56.\nZhang, N.; Zhang, X.; Yang, G.; Zhu,\
    \ C.; Huo, L.; Feng, H. Assessment of Defoliation during the Dendrolimus Tabulaeformis\
    \ Tsai\net Liu Disaster Outbreak Using UAV-Based Hyperspectral Images. Remote\
    \ Sens. Environ. 2018, 217, 323–339. [CrossRef]\n57.\nDuarte, A.; Acevedo-Muñoz,\
    \ L.; Gonçalves, C.I.; Mota, L.; Sarmento, A.; Silva, M.; Fabres, S.; Borralho,\
    \ N.; Valente, C. Detection of\nLonghorned Borer Attack and Assessment in Eucalyptus\
    \ Plantations Using UAV Imagery. Remote Sens. 2020, 12, 3153. [CrossRef]\n58.\n\
    Megat Mohamed Nazir, M.N.; Terhem, R.; Norhisham, A.R.; Mohd Razali, S.; Meder,\
    \ R. Early Monitoring of Health Status of\nPlantation-Grown Eucalyptus Pellita\
    \ at Large Spatial Scale via Visible Spectrum Imaging of Canopy Foliage Using\
    \ Unmanned\nAerial Vehicles. Forests 2021, 12, 1393. [CrossRef]\n59.\nMiraki,\
    \ M.; Sohrabi, H.; Fatehi, P.; Kneubuehler, M. Detection of Mistletoe Infected\
    \ Trees Using UAV High Spatial Resolution\nImages. J. Plant Dis. Prot. 2021, 128,\
    \ 1679–1689. [CrossRef]\n60.\nMaes, W.; Huete, A.; Avino, M.; Boer, M.; Dehaan,\
    \ R.; Pendall, E.; Griebel, A.; Steppe, K. Can UAV-Based Infrared Thermography\n\
    Be Used to Study Plant-Parasite Interactions between Mistletoe and Eucalypt Trees?\
    \ Remote Sens. 2018, 10, 2062. [CrossRef]\n61.\nLehmann, J.R.K.; Nieberding, F.;\
    \ Prinz, T.; Knoth, C. Analysis of Unmanned Aerial System-Based CIR Images in\
    \ Forestry—A New\nPerspective to Monitor Pest Infestation Levels. Forests 2015,\
    \ 6, 594–612. [CrossRef]\n62.\nLin, Q.; Huang, H.; Chen, L.; Wang, J.; Huang,\
    \ K.; Liu, Y. Using the 3D Model RAPID to Invert the Shoot Dieback Ratio of\n\
    Vertically Heterogeneous Yunnan Pine Forests to Detect Beetle Damage. Remote Sens.\
    \ Environ. 2021, 260, 112475. [CrossRef]\n63.\nLin, Q.; Huang, H.; Wang, J.; Huang,\
    \ K.; Liu, Y. Detection of Pine Shoot Beetle (PSB) Stress on Pine Forests at Individual\
    \ Tree\nLevel Using UAV-Based Hyperspectral Imagery and Lidar. Remote Sens. 2019,\
    \ 11, 2540. [CrossRef]\n64.\nLiu, M.; Zhang, Z.; Liu, X.; Yao, J.; Du, T.; Ma,\
    \ Y.; Shi, L. Discriminant Analysis of the Damage Degree Caused by Pine Shoot\n\
    Beetle to Yunnan Pine Using UAV-Based Hyperspectral Images. Forests 2020, 11,\
    \ 1258. [CrossRef]\n65.\nCardil, A.; Otsu, K.; Pla, M.; Silva, C.A.; Brotons,\
    \ L. Quantifying Pine Processionary Moth Defoliation in a Pine-Oak Mixed Forest\n\
    Using Unmanned Aerial Systems and Multispectral Imagery. PLoS ONE 2019, 14, e0213027.\
    \ [CrossRef]\n66.\nOtsu, K.; Pla, M.; Duane, A.; Cardil, A.; Brotons, L. Estimating\
    \ the Threshold of Detection on Tree Crown Defoliation Using\nVegetation Indices\
    \ from Uas Multispectral Imagery. Drones 2019, 3, 80. [CrossRef]\n67.\nOtsu, K.;\
    \ Pla, M.; Vayreda, J.; Brotons, L. Calibrating the Severity of Forest Defoliation\
    \ by Pine Processionary Moth with Landsat\nand UAV Imagery. Sensors 2018, 18,\
    \ 3278. [CrossRef]\n68.\nGuerra-Hernández, J.; Díaz-Varela, R.A.; Ávarez-González,\
    \ J.G.; Rodríguez-González, P.M. Assessing a Novel Modelling\nApproach with High\
    \ Resolution UAV Imagery for Monitoring Health Status in Priority Riparian Forests.\
    \ For. Ecosyst. 2021, 8, 61.\n[CrossRef]\n69.\nPádua, L.; Marques, P.; Martins,\
    \ L.; Sousa, A.; Peres, E.; Sousa, J.J. Monitoring of Chestnut Trees Using Machine\
    \ Learning\nTechniques Applied to UAV-Based Multispectral Data. Remote Sens. 2020,\
    \ 12, 3032. [CrossRef]\n70.\nSandino, J.; Pegg, G.; Gonzalez, F.; Smith, G. Aerial\
    \ Mapping of Forests Affected by Pathogens Using UAVs, Hyperspectral\nSensors,\
    \ and Artiﬁcial Intelligence. Sensors 2018, 18, 944. [CrossRef] [PubMed]\n71.\n\
    Dell, M.; Stone, C.; Osborn, J.; Glen, M.; McCoull, C.; Rimbawanto, A.; Tjahyono,\
    \ B.; Mohammed, C. Detection of Necrotic Foliage\nin a Young Eucalyptus Pellita\
    \ Plantation Using Unmanned Aerial Vehicle RGB Photography—A Demonstration of\
    \ Concept. Aust.\nFor. 2019, 82, 79–88. [CrossRef]\n72.\nIordache, M.-D.; Mantas,\
    \ V.; Baltazar, E.; Pauly, K.; Lewyckyj, N. A Machine Learning Approach to Detecting\
    \ Pine Wilt Disease\nUsing Airborne Spectral Imagery. Remote Sens. 2020, 12, 2280.\
    \ [CrossRef]\n73.\nSyifa, M.; Park, S.-J.; Lee, C.-W. Detection of the Pine Wilt\
    \ Disease Tree Candidates for Drone Remote Sensing Using Artiﬁcial\nIntelligence\
    \ Techniques. Engineering 2020, 6, 919–926. [CrossRef]\n74.\nTao, H.; Li, C.;\
    \ Zhao, D.; Deng, S.; Hu, H.; Xu, X.; Jing, W. Deep Learning-Based Dead Pine Tree\
    \ Detection from Unmanned Aerial\nVehicle Images. Int. J. Remote Sens. 2020, 41,\
    \ 8238–8255. [CrossRef]\n75.\nQin, J.; Wang, B.; Wu, Y.; Lu, Q.; Zhu, H. Identifying\
    \ Pine Wood Nematode Disease Using Uav Images and Deep Learning\nAlgorithms. Remote\
    \ Sens. 2021, 13, 162. [CrossRef]\n76.\nWu, B.; Liang, A.; Zhang, H.; Zhu, T.;\
    \ Zou, Z.; Yang, D.; Tang, W.; Li, J.; Su, J. Application of Conventional UAV-Based\
    \ High-\nThroughput Object Detection to the Early Diagnosis of Pine Wilt Disease\
    \ by Deep Learning. For. Ecol. Manag. 2021, 486, 118986.\n[CrossRef]\n77.\nPark,\
    \ H.G.; Yun, J.P.; Kim, M.Y.; Jeong, S.H. Multichannel Object Detection for Detecting\
    \ Suspected Trees with Pine Wilt Disease\nUsing Multispectral Drone Imagery. IEEE\
    \ J. Sel. Top. Appl. Earth Obs. Remote Sens. 2021, 14, 8350–8358. [CrossRef]\n\
    78.\nXia, L.; Zhang, R.; Chen, L.; Li, L.; Yi, T.; Wen, Y.; Ding, C.; Xie, C.\
    \ Evaluation of Deep Learning Segmentation Models for Detection\nof Pine Wilt\
    \ Disease in Unmanned Aerial Vehicle Images. Remote Sens. 2021, 13, 3594. [CrossRef]\n\
    79.\nYu, R.; Luo, Y.; Zhou, Q.; Zhang, X.; Wu, D.; Ren, L. A Machine Learning\
    \ Algorithm to Detect Pine Wilt Disease Using UAV-Based\nHyperspectral Imagery\
    \ and LiDAR Data at the Tree Level. Int. J. Appl. Earth Obs. Geoinf. 2021, 101,\
    \ 102363. [CrossRef]\n80.\nYu, R.; Ren, L.; Luo, Y. Early Detection of Pine Wilt\
    \ Disease in Pinus Tabuliformis in North China Using a Field Portable\nSpectrometer\
    \ and UAV-Based Hyperspectral Imagery. For. Ecosyst. 2021, 8, 44. [CrossRef]\n\
    81.\nYu, R.; Luo, Y.; Li, H.; Yang, L.; Huang, H.; Yu, L.; Ren, L. Three-Dimensional\
    \ Convolutional Neural Network Model for Early\nDetection of Pine Wilt Disease\
    \ Using Uav-Based Hyperspectral Images. Remote Sens. 2021, 13, 4065. [CrossRef]\n\
    Forests 2022, 13, 911\n29 of 31\n82.\nYu, R.; Luo, Y.; Zhou, Q.; Zhang, X.; Wu,\
    \ D.; Ren, L. Early Detection of Pine Wilt Disease Using Deep Learning Algorithms\
    \ and\nUAV-Based Multispectral Imagery. For. Ecol. Manag. 2021, 497, 119493. [CrossRef]\n\
    83.\nSun, Z.; Wang, Y.; Pan, L.; Xie, Y.; Zhang, B.; Liang, R.; Sun, Y. Pine Wilt\
    \ Disease Detection in High-Resolution UAV Images Using\nObject-Oriented Classiﬁcation.\
    \ J. For. Res. 2021, 577. [CrossRef]\n84.\nSmigaj, M.; Gaulton, R.; Barr, S.L.;\
    \ Suárez, J.C. UAV-Borne Thermal Imaging for Forest Health Monitoring: Detection\
    \ Of Disease-\nInduced Canopy Temperature Increase. In International Archives\
    \ of the Photogrammetry, Remote Sensing & Spatial Information\nSciences—ISPRS\
    \ Archives; Paparoditis, N., Raimond, A.-M., Sithole, G., Rabatel, G., Coltekin,\
    \ A., Rottensteiner, F., Briottet, X.,\nChristophe, S., Dowman, I., Elberink,\
    \ S.O., et al., Eds.; International Society for Photogrammetry and Remote Sensing:\
    \ Christian\nHeipke, Germany, 2015; Volume 40, pp. 349–354.\n85.\nSmigaj, M.;\
    \ Gaulton, R.; Suárez, J.C.; Barr, S.L. Canopy Temperature from an Unmanned Aerial\
    \ Vehicle as an Indicator of Tree\nStress Associated with Red Band Needle Blight\
    \ Severity. For. Ecol. Manag. 2019, 433, 699–708. [CrossRef]\n86.\nFraser, B.T.;\
    \ Congalton, R.G. Monitoring Fine-Scale Forest Health Using Unmanned Aerial Systems\
    \ (UAS) Multispectral Models.\nRemote Sens. 2021, 13, 4873. [CrossRef]\n87.\n\
    Briechle, S.; Krzystek, P.; Vosselman, G. Classiﬁcation of Tree Species and Standing\
    \ Dead Trees by Fusing Uav-Based Lidar Data\nand Multispectral Imagery in the\
    \ 3D Deep Neural Network Pointnet++. In ISPRS Annals of the Photogrammetry, Remote\
    \ Sensing\nand Spatial Information Sciences; Paparoditis, N., Mallet, C., Lafarge,\
    \ F., Remondino, F., Toschi, I., Fuse, T., Eds.; Copernicus GmbH:\nGöttingen,\
    \ Germany, 2020; Volume 5, pp. 203–210.\n88.\nElli, E.F.; Sentelhas, P.C.; Bender,\
    \ F.D. Impacts and Uncertainties of Climate Change Projections on Eucalyptus Plantations\n\
    Productivity across Brazil. For. Ecol. Manag. 2020, 474, 118365. [CrossRef]\n\
    89.\nZhang, Y.; Wang, X. Geographical Spatial Distribution and Productivity Dynamic\
    \ Change of Eucalyptus Plantations in China. Sci.\nRep. 2021, 11, 19764. [CrossRef]\n\
    90.\nPotts, B.M.; Vaillancourt, R.E.; Jordan, G.; Dutkowski, G.; Costa e Silva,\
    \ J.; Gay, M.; Steane, D.; Volker, P.; Lopez, G.; Apiolazza,\nL.; et al. Exploration\
    \ of the Eucalyptus Globulus Gene Pool. In Proceedings of the Eucalyptus in a\
    \ Changing World—IUFRO\nConference, Aveiro, Portugal, 1–15 October 2004; Borralho,\
    \ N., Pereira, J.S., Marques, C., Coutinho, J., Madeira, M., Tomé, M.,\nEds.;\
    \ IUFRO: Aveiro, Portugal, 2004; pp. 46–61.\n91.\nCromwell, C.; Giampaolo, J.;\
    \ Hupy, J.; Miller, Z.; Chandrasekaran, A. A Systematic Review of Best Practices\
    \ for UAS Data\nCollection in Forestry-Related Applications. Forests 2021, 12,\
    \ 957. [CrossRef]\n92.\nTmuši´c, G.; Manfreda, S.; Aasen, H.; James, M.R.; Gonçalves,\
    \ G.; Ben-Dor, E.; Brook, A.; Polinova, M.; Arranz, J.J.; Mészáros, J.;\net al.\
    \ Current Practices in UAS-Based Environmental Monitoring. Remote Sens. 2020,\
    \ 12, 1001. [CrossRef]\n93.\nShi, Y.; Thomasson, J.A.; Murray, S.C.; Pugh, N.A.;\
    \ Rooney, W.L.; Shaﬁan, S.; Rajan, N.; Rouze, G.; Morgan, C.L.S.;\nNeely, H.L.;\
    \ et al. Unmanned Aerial Vehicles for High-Throughput Phenotyping and Agronomic\
    \ Research. PLoS ONE 2016, 11,\ne0159781. [CrossRef] [PubMed]\n94.\nGonzález-Jorge,\
    \ H.; Martínez-Sánchez, J.; Bueno, M.; Arias, P. Unmanned Aerial Systems for Civil\
    \ Applications: A Review. Drones\n2017, 1, 2. [CrossRef]\n95.\nWatts, A.C.; Ambrosia,\
    \ V.G.; Hinkley, E.A. Unmanned Aircraft Systems in Remote Sensing and Scientiﬁc\
    \ Research: Classiﬁcation\nand Considerations of Use. Remote Sens. 2012, 4, 1671–1692.\
    \ [CrossRef]\n96.\nZong, J.; Zhu, B.; Hou, Z.; Yang, X.; Zhai, J. Evaluation and\
    \ Comparison of Hybrid Wing VTOL UAV with Four Different Electric\nPropulsion\
    \ Systems. Aerospace 2021, 8, 256. [CrossRef]\n97.\nMüllerová, J.; Bartaloš, T.;\
    \ Br˚una, J.; Dvoˇrák, P.; Vítková, M. Unmanned Aircraft in Nature Conservation:\
    \ An Example from Plant\nInvasions. Int. J. Remote Sens. 2017, 38, 2177–2198.\
    \ [CrossRef]\n98.\nAssmann, J.J.; Kerby, J.T.; Cunliffe, A.M.; Myers-Smith, I.H.\
    \ Vegetation Monitoring Using Multispectral Sensors—Best Practices\nand Lessons\
    \ Learned from High Latitudes. J. Unmanned Veh. Sys. 2019, 7, 54–75. [CrossRef]\n\
    99.\nPajares, G. Overview and Current Status of Remote Sensing Applications Based\
    \ on Unmanned Aerial Vehicles (UAVs). Pho-\ntogramm. Eng. Remote Sens. 2015, 81,\
    \ 281–330. [CrossRef]\n100. Stuart, M.B.; McGonigle, A.J. Hyperspectral Imaging\
    \ in Environmental Monitoring: A Review of Recent Developments and\nTechnological\
    \ Advances in Compact Field Deployable Systems. Sensors 2019, 19, 3071. [CrossRef]\n\
    101. Stöcker, C.; Bennett, R.; Nex, F.; Gerke, M.; Zevenbergen, J. Review of the\
    \ Current State of UAV Regulations. Remote Sens. 2017, 9,\n459. [CrossRef]\n102.\
    \ EASA. Commission Implementing Regulation (EU) 2019/947 of 24 May 2019 on the\
    \ Rules and Procedures for the Operation of\nUnmanned Aircraft. Available online:\
    \ https://eur-lex.europa.eu/eli/reg_impl/2019/947/oj (accessed on 5 March 2021).\n\
    103. Iglhaut, J.; Cabo, C.; Puliti, S.; Piermattei, L.; O’Connor, J.; Rosette,\
    \ J. Structure from Motion Photogrammetry in Forestry:\nA Review. Curr. For. Rep.\
    \ 2019, 5, 155–168. [CrossRef]\n104. Whitehead, K.; Hugenholtz, C.H. Applying\
    \ ASPRS Accuracy Standards to Surveys from Small Unmanned Aircraft Systems\n(UAS).\
    \ Photogramm. Eng. Remote Sens. 2015, 81, 787–793. [CrossRef]\n105. Barbedo, J.\
    \ A Review on the Use of Unmanned Aerial Vehicles and Imaging Sensors for Monitoring\
    \ and Assessing Plant Stresses.\nDrones 2019, 3, 40. [CrossRef]\n106. Ke, Y.;\
    \ Quackenbush, L.J. A Review of Methods for Automatic Individual Tree-Crown Detection\
    \ and Delineation from Passive\nRemote Sensing. Int. J. Remote Sens. 2011, 32,\
    \ 4725–4747. [CrossRef]\nForests 2022, 13, 911\n30 of 31\n107. Koch, B.; Kattenborn,\
    \ T.; Straub, C.; Vauhkonen, J. Segmentation of Forest to Tree Objects. In Forestry\
    \ Application of Airborne\nLaser Scanning: Concept and Case Studies; Maltamo,\
    \ M., Naesset, E., Vauhkonen, J., Eds.; Springer Netherlands: Dordrecht,\nThe\
    \ Netherlands, 2014; pp. 89–112. ISBN 94-017-8662-3.\n108. Wang, L.; Gong, P.;\
    \ Biging, G.S. Individual Tree-Crown Delineation and Treetop Detection in High-Spatial-Resolution\
    \ Aerial\nImagery. Photogramm. Eng. Remote Sens. 2004, 70, 351–357. [CrossRef]\n\
    109. Zhen, Z.; Quackenbush, L.; Zhang, L. Trends in Automatic Individual Tree\
    \ Crown Detection and Delineation—Evolution of\nLiDAR Data. Remote Sens. 2016,\
    \ 8, 333. [CrossRef]\n110. Mohan, M.; Silva, C.; Klauberg, C.; Jat, P.; Catts,\
    \ G.; Cardil, A.; Hudak, A.; Dia, M. Individual Tree Detection from Unmanned\n\
    Aerial Vehicle (UAV) Derived Canopy Height Model in an Open Canopy Mixed Conifer\
    \ Forest. Forests 2017, 8, 340. [CrossRef]\n111. Dalponte, M.; Reyes, F.; Kandare,\
    \ K.; Gianelle, D. Delineation of Individual Tree Crowns from ALS and Hyperspectral\
    \ Data:\nA Comparison among Four Methods. Eur. J. Remote Sens. 2015, 48, 365–382.\
    \ [CrossRef]\n112. Dalponte, M.; Coomes, D.A. Tree-centric Mapping of Forest Carbon\
    \ Density from Airborne Laser Scanning and Hyperspectral\nData. Methods Ecol.\
    \ Evol. 2016, 7, 1236–1245. [CrossRef]\n113. Vincent, L.; Soille, P. Watersheds\
    \ in Digital Spaces: An Efﬁcient Algorithm Based on Immersion Simulations. IEEE\
    \ Trans. Pattern\nAnal. Mach. Intell. 1991, 13, 583–598. [CrossRef]\n114. Gu,\
    \ J.; Grybas, H.; Congalton, R.G. Individual Tree Crown Delineation from UAS Imagery\
    \ Based on Region Growing and Growth\nSpace Considerations. Remote Sens. 2020,\
    \ 12, 2363. [CrossRef]\n115. Hyyppa, J.; Kelle, O.; Lehikoinen, M.; Inkinen, M.\
    \ A Segmentation-Based Method to Retrieve Stem Volume Estimates from 3-D\nTree\
    \ Height Models Produced by Laser Scanners. IEEE Trans. Geosci. Remote Sens. 2001,\
    \ 39, 969–975. [CrossRef]\n116. Li, W.; Guo, Q.; Jakubowski, M.K.; Kelly, M. A\
    \ New Method for Segmenting Individual Trees from the Lidar Point Cloud.\nPhotogramm.\
    \ Eng. Remote Sens. 2012, 78, 75–84. [CrossRef]\n117. Reitberger, J.; Schnörr,\
    \ C.; Krzystek, P.; Stilla, U. 3D Segmentation of Single Trees Exploiting Full\
    \ Waveform LIDAR Data. ISPRS J.\nPhotogramm. Remote Sens. 2009, 64, 561–574. [CrossRef]\n\
    118. Hyyppä, J.; Hyyppä, H.; Leckie, D.; Gougeon, F.; Yu, X.; Maltamo, M. Review\
    \ of Methods of Small-footprint Airborne Laser\nScanning for Extracting Forest\
    \ Inventory Data in Boreal Forests. Int. J. Remote Sens. 2008, 29, 1339–1366.\
    \ [CrossRef]\n119. Zhou, Y.; Zhang, R.; Wang, S.; Wang, F. Feature Selection Method\
    \ Based on High-Resolution Remote Sensing Images and the\nEffect of Sensitive\
    \ Features on Classiﬁcation Accuracy. Sensors 2018, 18, 2013. [CrossRef]\n120.\
    \ Lu, D.; Weng, Q. A Survey of Image Classiﬁcation Methods and Techniques for\
    \ Improving Classiﬁcation Performance. Int. J.\nRemote Sens. 2007, 28, 823–870.\
    \ [CrossRef]\n121. Sowmya, A.; Trinder, J. Modelling and Representation Issues\
    \ in Automated Feature Extraction from Aerial and Satellite Images.\nISPRS J.\
    \ Photogramm. Remote Sens. 2000, 55, 34–47. [CrossRef]\n122. Fotso Kamga, G.A.;\
    \ Bitjoka, L.; Akram, T.; Mengue Mbom, A.; Rameez Naqvi, S.; Bouroubi, Y. Advancements\
    \ in Satellite Image\nClassiﬁcation: Methodologies, Techniques, Approaches and\
    \ Applications. Int. J. Remote Sens. 2021, 42, 7662–7722. [CrossRef]\n123. Oumar,\
    \ Z.; Mutanga, O.; Ismail, R. Predicting Thaumastocoris Peregrinus Damage Using\
    \ Narrow Band Normalized Indices and\nHyperspectral Indices Using Field Spectra\
    \ Resampled to the Hyperion Sensor. Int. J. Appl. Earth Obs. Geoinf. 2013, 21,\
    \ 113–121.\n[CrossRef]\n124. Ma, L.; Fu, T.; Blaschke, T.; Li, M.; Tiede, D.;\
    \ Zhou, Z.; Ma, X.; Chen, D. Evaluation of Feature Selection Methods for Object-Based\n\
    Land Cover Mapping of Unmanned Aerial Vehicle Imagery Using Random Forest and\
    \ Support Vector Machine Classiﬁers. Int. J.\nGeo-Inf. 2017, 6, 51. [CrossRef]\n\
    125. Ma, L.; Cheng, L.; Li, M.; Liu, Y.; Ma, X. Training Set Size, Scale, and\
    \ Features in Geographic Object-Based Image Analysis of Very\nHigh Resolution\
    \ Unmanned Aerial Vehicle Imagery. ISPRS J. Photogramm. Remote Sens. 2015, 102,\
    \ 14–27. [CrossRef]\n126. Ma, L.; Li, M.; Ma, X.; Cheng, L.; Du, P.; Liu, Y. A\
    \ Review of Supervised Object-Based Land-Cover Image Classiﬁcation. ISPRS J.\n\
    Photogramm. Remote Sens. 2017, 130, 277–293. [CrossRef]\n127. Ma, L.; Liu, Y.;\
    \ Zhang, X.; Ye, Y.; Yin, G.; Johnson, B.A. Deep Learning in Remote Sensing Applications:\
    \ A Meta-Analysis and\nReview. ISPRS J. Photogramm. Remote Sens. 2019, 152, 166–177.\
    \ [CrossRef]\n128. Osco, L.P.; Marcato Junior, J.; Marques Ramos, A.P.; de Castro\
    \ Jorge, L.A.; Fatholahi, S.N.; de Andrade Silva, J.; Matsubara, E.T.;\nPistori,\
    \ H.; Gonçalves, W.N.; Li, J. A Review on Deep Learning in UAV Remote Sensing.\
    \ Int. J. Appl. Earth Obs. Geoinf. 2021, 102,\n102456. [CrossRef]\n129. Nasiri,\
    \ V.; Darvishsefat, A.A.; Areﬁ, H.; Pierrot-Deseilligny, M.; Namiranian, M.; Le\
    \ Bris, A. Unmanned Aerial Vehicles (UAV)-\nBased Canopy Height Modeling under\
    \ Leaf-on and Leaf-off Conditions for Determining Tree Height and Crown Diameter\
    \ (Case\nStudy: Hyrcanian Mixed Forest). Can. J. For. Res. 2021, 51, 962–971.\
    \ [CrossRef]\n130. Puente, C.; Olague, G.; Smith, S.V.; Bullock, S.H.; Hinojosa-Corona,\
    \ A.; González-Botello, M.A. A Genetic Programming Approach\nto Estimate Vegetation\
    \ Cover in the Context of Soil Erosion Assessment. Photogramm. Eng. Remote Sens.\
    \ 2011, 77, 363–376.\n[CrossRef]\n131. Batista, J.E.; Cabral, A.I.R.; Vasconcelos,\
    \ M.J.P.; Vanneschi, L.; Silva, S. Improving Land Cover Classiﬁcation Using Genetic\n\
    Programming for Feature Construction. Remote Sens. 2021, 13, 1623. [CrossRef]\n\
    Forests 2022, 13, 911\n31 of 31\n132. Batista, J.E.; Silva, S. Improving the Detection\
    \ of Burnt Areas in Remote Sensing Using Hyper-Features Evolved by M3GP.\nIn Proceedings\
    \ of the 2020 IEEE Congress on Evolutionary Computation (CEC), Glasgow, UK, 19–24\
    \ July 2020.\n133. Mejia-Zuluaga, P.A.; Dozal, L.; Valdiviezo-N, J.C. Genetic\
    \ Programming Approach for the Detection of Mistletoe Based on UAV\nMultispectral\
    \ Imagery in the Conservation Area of Mexico City. Remote Sens. 2022, 14, 801.\
    \ [CrossRef]\n"
  inline_citation: '>'
  journal: Forests
  limitations: The paper does not directly focus on irrigation management systems
    or provide specific recommendations for improving automation in this context.
  pdf_link: https://www.mdpi.com/1999-4907/13/6/911/pdf?version=1654873793
  publication_year: 2022
  relevance_evaluation: The paper is highly relevant to the review on automated systems
    for real-time irrigation management as it examines the use of UAV-based remote
    sensing for FIPD monitoring and discusses the challenges, gaps, and opportunities
    in this domain.
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  title: 'Recent Advances in Forest Insect Pests and Diseases Monitoring Using UAV-Based
    Data: A Systematic Review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.1007/s11240-023-02528-0
  analysis: '>'
  apa_citation: Bethge, H., Winkelmann, T., Lüdeke, P., & Rath, T. (2023). Low-cost
    and automated phenotyping system “Phenomenon” for multi-sensor in situ monitoring
    in plant in vitro culture. Plant Methods, 19(1), 1–25. https://doi.org/10.1186/s13007-023-01018-w
  authors:
  - Hans Bethge
  - Zahra Mohammadi Nakhjiri
  - Thomas Rath
  - Traud Winkelmann
  citation_count: 2
  explanation: This study provides a novel approach to detect hyperhydricity by identifying
    spectral features characteristic of hyperhydric tissue. Machine learning models
    were then trained on this data to classify normal and hyperhydric explants with
    90% accuracy. These findings contribute to understanding the underlying mechanisms
    of hyperhydricity and pave the way for automated detection and control systems
    to mitigate this common problem in plant tissue culture.
  extract_1: 'Integrating high-resolution cameras (e.g., multispectral, hyperspectral)
    and computer vision algorithms for visual monitoring of crop growth, disease detection
    (e.g., using deep learning-based object detection and segmentation), and irrigation
    system performance (e.g., leak detection, sprinkler uniformity) '
  extract_2: Investigations on the application of computer vision to micropropagation
    (Smith et al. 2009; Aynalem et al. 2006; Dhondt et al. 2014; Gupta and Karmakar
    2017; Mestre et al. 2017) with imaging sensors being the crucial technology.
  full_citation: '>'
  full_text: ">\nVol.:(0123456789)\n1 3\nPlant Cell, Tissue and Organ Culture (PCTOC)\
    \ (2023) 154:551–573 \nhttps://doi.org/10.1007/s11240-023-02528-0\nORIGINAL ARTICLE\n\
    Towards automated detection of hyperhydricity in plant in vitro \nculture\nHans Bethge1,2\
    \  · Zahra Mohammadi Nakhjiri2 · Thomas Rath1  · Traud Winkelmann2 \nReceived:\
    \ 24 March 2023 / Accepted: 12 May 2023 / Published online: 7 June 2023 \n© The\
    \ Author(s) 2023\nAbstract\nHyperhydricity (HH) is one of the most important physiological\
    \ disorders that negatively affects various plant tissue culture \ntechniques.\
    \ The objective of this study was to characterize optical features to allow an\
    \ automated detection of HH. For this \npurpose, HH was induced in two plant species,\
    \ apple and Arabidopsis thaliana, and the severity was quantified based on \n\
    visual scoring and determination of apoplastic liquid volume. The comparison between\
    \ the HH score and the apoplastic liq-\nuid volume revealed a significant correlation,\
    \ but different response dynamics. Corresponding leaf reflectance spectra were\
    \ \ncollected and different approaches of spectral analyses were evaluated for\
    \ their ability to identify HH-specific wavelengths. \nStatistical analysis of\
    \ raw spectra showed significantly lower reflection of hyperhydric leaves in the\
    \ VIS, NIR and SWIR \nregion. Application of the continuum removal hull method\
    \ to raw spectra identified HH-specific absorption features over \ntime and major\
    \ absorption peaks at 980 nm, 1150 nm, 1400 nm, 1520 nm, 1780 nm and 1930 nm for\
    \ the various conducted \nexperiments. Machine learning (ML) model spot checking\
    \ specified the support vector machine to be most suited for classifi-\ncation\
    \ of hyperhydric explants, with a test accuracy of 85% outperforming traditional\
    \ classification via vegetation index with \n63% test accuracy and the other ML\
    \ models tested. Investigations on the predictor importance revealed 1950 nm,\
    \ 1445 nm \nin SWIR region and 415 nm in the VIS region to be most important for\
    \ classification. The validity of the developed spectral \nclassifier was tested\
    \ on an available hyperspectral image acquisition in the SWIR-region.\nKey message\
    \ \nThis study provides an approach that paves the way to automatic detection\
    \ of hyperhydricity by identifying the key spectral \nfeatures of this phenomenon.\n\
    Keywords Hyperhydricity · Spectral analysis · Phenotyping · Machine learning ·\
    \ Automated object detection\nAbbreviations\nHH \n Hyperhydricity\nML \n Machine\
    \ learning\nUV \n Ultra violet\nVIS \n Visible radiation\nNIR \n Near infrared\
    \ radiation\nSWIR \n Shortwave infrared radiation\nMWIR  Mid-wave infrared radiation\n\
    LWIR \n Longwave infrared radiation\nDAT \n Days after treatment/transfer\nCV\
    \ \n Cross validation\nCNN \n Convolutional neuronal network\nHSI \n Hyperspectral\
    \ imaging\nIntroduction\nHyperhydricity (HH) represents one of the major chal-\n\
    lenges for increasing the efficiency of plant in  vitro \npropagation as it limits\
    \ plant quality, adventitious root \nformation and ex vitro survival rate, in\
    \ particular when \nCommunicated by Victor M. Jimenez.\n * Hans Bethge \n \nbethge@baum.uni-hannover.de\n\
    1 \nLaboratory for Biosystems Engineering, Faculty \nof Agricultural Sciences\
    \ and Landscape Architecture, \nOsnabrück University of Applied Sciences, Oldenburger\
    \ \nLandstraße 24, 49090 Osnabrück, Germany\n2 \nInstitute of Horticultural Production\
    \ Systems, Section \nof Woody Plant and Propagation Physiology, Leibniz \nUniversität\
    \ Hannover, Herrenhäuser Str. 2, 30419 Hannover, \nGermany\n552\n \nPlant Cell,\
    \ Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nusing liquid culture\
    \ or bioreactor systems (Cardoso et al. \n2018; Debergh et al. 1992; Gribble 1999).\
    \ According to \nKemat et al. (2020), at least 200 species are sensitive to \n\
    HH and around 150 species can be affected seriously by \nHH emphasizing the relevance\
    \ for commercial micropro-\npagation. HH not only restricts the propagation of\
    \ in vitro \nplants, but also affects the efficiency of genetic transforma-\n\
    tion mediated by Agrobacterium (van Altvorst et al. 1996) \nand the conservation\
    \ of important species in germplasm \nbanks (Lizárraga et al. 2017).\nHH is a\
    \ physiological disorder occurring under the spe-\ncific conditions of plant tissue\
    \ culture such as high humid-\nity, high supplementation of sucrose, impaired\
    \ gaseous \nexchange capacity and consequently low photosynthetic \nactivity (George\
    \ et al. 2008; Ziv 1991). The work of van \nden Dries et al. (2013) and Rojas-Martínez\
    \ et al. (2010) \nprovided strong evidence that the underlying mechanism of \n\
    the HH etiology is the flooding of the apoplast, resulting \nin hypoxia and causing\
    \ oxidative stress. This in turn leads \nto the macroscopic symptoms of water-soaked,\
    \ wrinkled, \ncurled, brittle and translucent tissue. The occurrence of HH \n\
    was shown to be increased when the water availability for \nthe in vitro explant\
    \ was increased (Smith and Spomer 1995), \ne.g., by reduced concentration of the\
    \ gelling agent (Ivanova \nand Van Staden 2011), or by the type of the gelling\
    \ agent \nused (Pasqualetto et al. 1988; Tsay et al. 2006). The gelling \nagent\
    \ gelrite induced HH in a wide range of plant genera \n(e.g., Arabidopsis sp.,\
    \ van den Dries et al. 2013, Malus sp. \nPasqualetto et al. 1988, Prunus sp. Franck\
    \ et al. 1998), even \nthough the same gel strength as agar was used.\nIn addition\
    \ to the anatomical changes of hyperhydric tis-\nsue which include larger intercellular\
    \ spaces in the meso-\nphyll and a drastically reduced number of palisade cells\
    \ \n(Vieitez et al. 1985), several biochemical changes of hype-\nrhydric tissue\
    \ such as decreased chlorophyll contents (Phan \nand Letouze 1983; Franck et al.\
    \ 1998), hypolignification \n(Kevers et al. 1987; Kemat et al. 2021), and high\
    \ apoplastic \nwater volume (Dries et al. 2013; Tian et al. 2015; de Klerk \n\
    and Pramanik 2017) were reported. Paques et al. (1985) \nrefer to HH as an inducible\
    \ and reversible phenomenon and \ndemonstrated that Malus sp. ‘M26’ plantlets\
    \ could return \nto non-hyperhydric state if the induction phase in liquid \n\
    culture did not exceed five days or if the symptoms of HH \nwere not too severe.\
    \ Recently, there were reports that hype-\nrhydricity can be reversed by supplementation\
    \ of agents to \nmedia such as silver nitrate and trichloroacetate (Gao et al.\
    \ \n2017; de Klerk and Pramanik 2017) or by controlling the \nenvironmental conditions\
    \ in addition to media optimization \n(Mohamed et al. 2023), but no general countermeasure\
    \ has \nbeen derived up to now. In commercial in vitro laboratories \nvisual monitoring\
    \ for contaminations and disorders are part \nof the routine work and therefore\
    \ a costly and time-consum-\ning repetitive matter (Mestre et al. 2017).\nNowadays,\
    \ digitalization enters the horticultural sector, \ndriven by digital solutions\
    \ to increasingly complex work \nprocesses achieved through technological advances\
    \ in sen-\nsors, automation and robotization, as well as data analysis \nthrough\
    \ classical and advanced machine learning (ML) \ntechniques. Automation of processes\
    \ offers great economic \npotential for micropropagation laboratories since 60–70%\
    \ \nof total costs of a micropropagated plant is due to manual \nlabor (Chen 2016).\
    \ An increasing number of reports on auto-\nmating micropropagation processes\
    \ such as explant cutting \n(Huang and Lee 2010), the commercial laser-based robotic\
    \ \ncut and transplanting system RoBo®Cut (Bock Biosciences \nGmbH 2018), monitoring\
    \ of cultures (Dhondt et al. 2014, \nBethge et al. 2023) and transplanting of\
    \ explants (Lee et al. \n2019) were published within the recent years. In addition,\
    \ \nthere are several studies on the application of computer \nvision to micropropagation\
    \ (Smith et al. 1989; Aynalem \net al. 2006; Dhondt et al. 2014; Gupta and Karmakar\
    \ 2017; \nMestre et al. 2017) with imaging sensors being the crucial \ntechnology.\
    \ Imaging sensors used in horticulture consist of \naffordable RGB cameras, multispectral\
    \ cameras, thermal \ncameras, expensive hyperspectral imaging (HSI) systems, \n\
    ToF (Time of Flight), LIDAR systems (Light Detection and \nRanging) and more.\
    \ The different sensor systems can be \ndiscriminated by their operating spectral\
    \ range (UV, VIS, \nNIR, SWIR, MWIR, LWIR/Thermal-IR), spectral resolu-\ntion\
    \ from one (monochrome) to > 100 (hyperspectral) chan-\nnels and cost of purchase.\
    \ For example, the price of silicon \n(Si)-based hyperspectral cameras rise by\
    \ a factor of 2 to 20 \nwhen switching the operating spectral range from VIS/NIR\
    \ \n(400–1000 nm) to SWIR (900–1700 nm) with an Indium-\nGalium-Asenide (InGaAs)\
    \ camera chip (Tisserand 2021). \nThis needs to be considered, when selecting\
    \ the appropriate \nspectral range and corresponding imaging technology. While\
    \ \ncomputer vision coupled with ML offers already great poten-\ntial to solve\
    \ complex detection task in agriculture (reviewed \nin Patrício and Rieder 2018),\
    \ for application in plant tissue \nculture only few reports are available up\
    \ to now (reviewed \nin Prasad and Gupta 2008; Hesami and Jones 2020). How-\n\
    ever, these are limited in terms of live-monitoring, since \nthey followed the\
    \ “object to sensor” approach for plantlet \nclustering (Mahendra et al. 2004),\
    \ classification of somatic \nembryos (Zhang et al. 1999) and estimation of shoot\
    \ length \n(Honda et al. 1997).\nThe visual appearance of plants, and in particular\
    \ leaf \npigments, can be estimated by spectroscopic approaches \nbased on their\
    \ interaction with electromagnetic radiation. \nSingle biochemical plant metabolites\
    \ can be associated with \nspecific wavelengths based on their major absorption\
    \ peaks \n(Table 1).\nUnivariate data analysis, e.g., spectral indices or multi-\n\
    variate data analyses like partial least square (PLS), allows \nthe prediction\
    \ of leaf pigments’ concentrations and can be \n553\nPlant Cell, Tissue and Organ\
    \ Culture (PCTOC) (2023) 154:551–573 \n1 3\nused for classification. These techniques\
    \ also enable the \ndiscrimination of different plant species or the identifica-\n\
    tion of growth anomalies by specific spectral features (Shaw \nand Kelley 2005).\
    \ According to Hesami and Jones (2020), \nML techniques applied to plant tissue\
    \ culture problems will \nhelp in future to solve classification and regression\
    \ problems \nand can be employed for automation and mechanization of \nin vitro\
    \ propagation, genetic engineering and genome edit-\ning technologies. In addition,\
    \ Nezami-Alanagh et al. (2019) \ndemonstrated the positive impact of ML models\
    \ in optimiz-\ning culture media in terms of time, cost and the occurrence \n\
    of physiological disorders in the propagation of pistachio \nrootstocks. Prasad\
    \ and Gupta (2008) proposed that an auto-\nmated decision-making system based\
    \ on computer vision \ncoupled with ML models and combined with a robotic sys-\n\
    tem will result in the mechanization of commercial mass \npropagation and help\
    \ in evaluating various aspects of plant \nquality such as HH status, which might\
    \ be difficult to deter-\nmine by human visual inspection. To our knowledge, the\
    \ \nspectral properties of HH have not yet been studied or used \nas a distinguishing\
    \ feature for ML classification of in vitro \ncultured explants.\nThe objective\
    \ of this study was to investigate the spectral \nfingerprints of hyperhydric\
    \ tissue in two different plant spe-\ncies (Malus sp. and Arabidopsis thaliana)\
    \ after forced induc-\ntion of the growth anomaly and subsequent spectral analysis\
    \ \nof the explants. Here, we selected Malus as a representa-\ntive of classical\
    \ in vitro shoot cultures and Arabidopsis as a \nmodel plant for the underlying\
    \ mechanism of HH. A novel \nphenotyping system was tested to monitor the morphological\
    \ \ncharacteristics of hyperhydric explants in time-series image \ndata. Furthermore,\
    \ we aimed at identifying specific absorp-\ntion features of hyperhydric tissues\
    \ that are sufficient for \ndiscrimination by ML techniques and to locate them\
    \ within \nin the electromagnetic radiation spectrum. Putative discrimi-\nnating\
    \ models should be validated and discussed in terms of \ntheir feasibility in\
    \ plant tissue culture. The findings of this \nstudy should pave the way for an\
    \ automatic detection of HH \nby live-monitoring of in vitro cultures.\nMaterial\
    \ and methods\nPlant material and experimental setup\nMorphological characteristics\
    \ of hyperhydricity\nFrom in vitro apple shoot cultures (Malus sp. ‘G214’) uni-\n\
    form shoots of 10–15 mm length were prepared and culti-\nvated on modified MS\
    \ medium (Murashige and Skoog 1962) \ncontaining 2.2 µM 6-benzylaminopurine (BAP),\
    \ 0.5 µM \nindole-3-butyric acid (IBA), 3% (w/v) sucrose and solidified \nwith\
    \ either 0.8% (w/v) agar (Plant agar, Duchefa, Haarlem, \nThe Netherlands) for\
    \ the control variant (“MS + agar”) or \nwith 0.25% (w/v) gelrite (Duchefa, Haarlem,\
    \ The Nether-\nlands) for the HH induction variant (“MS + gelrite”). The \npH\
    \ of the medium was adjusted to 5.8 prior to autoclaving \nat 121 °C for 15 min.\n\
    Arabidopsis thaliana ‘Col-0’ seeds which had been stored \nat 4 °C, were surface-disinfected\
    \ using 70% (v/v) isopro-\npanol for 30 s, followed by 2% (v/v) sodium hypochlorite\
    \ \nplus Tween 20 for 5 min and then rinsed thoroughly three \ntimes using sterile\
    \ deionized water. The seeds were germi-\nnated for 10 days at 24 °C in 9 cm-Petri\
    \ dishes (polysty-\nrene) on modified plant growth regulator-free B5 medium \n\
    (Gamborg et al. 1968), containing 1.5% (w/v) sucrose with \n0.8% (w/v) Plant agar\
    \ and pH 5.8. Uniform 10 day-old seed-\nlings were selected and five seedlings\
    \ per 500 mL-vessel \nwere transferred to modified plant growth regulator-free\
    \ \nB5 medium (Gamborg et al. 1968), containing 1.5% (w/v) \nsucrose and either\
    \ 0.8% (w/v) Plant agar for the control vari-\nant (“B5 + agar”) or 0.25% (w/v)\
    \ gelrite (“B5 + gelrite”) to \ninduce HH. The pH of the medium was adjusted to\
    \ 5.8 prior \nto autoclaving at 121 °C for 15 min.\nTen 500 mL polypropylene vessels\
    \ were prepared for \nExperiment I (Table 2) and Experiment II, each with four\
    \ \nplantlets and containing ~ 80 mL of one of the two dif-\nferent media (“B5/MS\
    \ + agar”/“B5/MS + gelrite” supple-\nmented with 1 g L-1 titanium dioxide). Titanium\
    \ dioxide \n(food dye; Ruth GmbH & Co.KG, Bochum, Germany) \nTable 1  Selected\
    \ reported symptoms of hyperhydric tissues (HH) and corresponding expected major\
    \ changes in optical absorbance features\n*Absorptions peaks according to Curran\
    \ (1989) in a wavelength range of 400 to 2000 nm. Bold wavelength indicating stronger\
    \ absorption of the \nrespective chemical compound\nReference\nPlant species\n\
    Observation\nDeduced optical absorbance \nfeatures in VIS-SWIR [nm]*\nPhan and\
    \ Letouze (1983)\nP. avium\nLower chlorophyll content in HH\n430, 460, 640, 660\n\
    Van den Dries et al. (2013)\nA. thaliana\nHigher apoplastic water volume in HH\n\
    970, 1200, 1400, 1450, 1940\nPhan and Letouze (1983)\nP. avium\nLess protein content\
    \ in HH\n910, 1020, 1510, 1940, 1980\nKemat et al. (2021)\nA. thaliana\nHypolignification\
    \ in HH\n1200, 1420, 1450, 1690, 1940\nSaher et al. (2005)\nD. caryophyllus\n\
    Higher sugar content in HH\n1450, 1490, 1580, 1780, 1960\nVan den Dries et al.\
    \ (2013)\nA. thaliana\nAnthocyanins accumulation in HH\n550\n554\n \nPlant Cell,\
    \ Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nwas used to add a\
    \ white color to the medium, because \nthis enabled the height measurements of\
    \ the robot system \ndue to increased reflection of the culture media. A plastic\
    \ \nfilm (PVC system foil; Klarsichtpackung GmbH, Hofheim, \nGermany) sealed the\
    \ containers as a substitution of the \nlid of the containers to provide a fully\
    \ transparent view \nwhile ensuring the aseptic condition of the cultures. These\
    \ \ncultures were cultivated at 22 °C with a 16 h photoperiod \nand under a PPFD\
    \ (Photosynthetic Photon Flux Density) \nof 35–40 μmolm−2s−1 , provided by two\
    \ tubular fluores-\ncent lamps (Philips MASTER TL-D 58W/865). The lab’s \nbottom-cooling\
    \ system—provided by water-cooled plastic \ntubes below the shelf—prevented water\
    \ condensation due \na local shift of dew point. Room temperature ranged from\
    \ \n19 (night) to 25 °C (day) with an average of 22 °C over \n24 h, while the\
    \ average surface temperature of the cooled \ncultivation area ranged from 19\
    \ (night) to 24 °C (day) \nwith an average of 21 °C over 24 h. In addition to\
    \ the \nnon-destructive monitoring approach (Exp. I & II), three \nexperiments\
    \ (Exp. III, Exp. IV, Exp. V; Table 2) were con-\nducted with different evaluation\
    \ time points. The evalua-\ntion time points were chosen based on the key events\
    \ in \nthe dynamic etiology of hyperhydricity during a culture \npassage (~ 4–5 weeks\
    \ for Malus). Important morphological \nchanges were observed during the first\
    \ two weeks, so Exp. \nIII and V covered this time span, while measurements in\
    \ \nExp. IV were undertaken to cover the second half of the \nculture passage.\n\
    Hyperhydricity induction\nFor Malus shoot cultures, 500 mL polypropylene containers\
    \ \ncontaining 80 mL of the two different media were used and \neach container\
    \ was inoculated with five shoots. Cultivation \ntook place for 20 days (Table 2;\
    \ Exp. III), 28 days (Exp. IV) \nand 16 days (Exp. V) at 22 °C (room temperature\
    \ ranged \nfrom 19 (night) to 25 °C (day) with an average of 22 °C \nover 24 h)\
    \ with a 16 h photoperiod and under a PPFD (Pho-\ntosynthetic Photon Flux Density)\
    \ of 35–40 µmol  m−2  s−1, \nprovided by tubular fluorescent lamps (Philips MASTER\
    \ \nTL-D 58W/865). Arabidopsis plantlets were cultivated as \ndescribed above\
    \ for 20 days (Exp. III).\nEvaluations\nMorphological characteristics of hyperhydricity\
    \ via image \nanalysis\nFor visualization of the etiology of HH, the multisensory\
    \ \nrobot system “Phenomenon” (Bethge et al. 2023) was used. \nRGB images were\
    \ captured in Exp. I and Exp. II every 4 h \nwith a 12.3-megapixel RGB camera\
    \ (Raspberry Pi Camera \nHQ, Raspberry Pi Foundation, Cambridge, UK) equipped\
    \ \nwith a 6 mm fixed focal length low-distortion lens (Edmund \nOptics: 6 mm\
    \ wide angle lens, f/1.2, high resolution = 120 \nlp  mm−1 (lp = line pairs),\
    \ low distortion < 0.5%) and with the \nfollowing camera parameters: resolution\
    \ = 4054 px × 3040 \nTable 2  Overview of conducted experiments and measurements\
    \ ten\na The multisensory robot system “Phenomenon” developed by Bethge et al. (2023),\
    \ consisting of 4 sensors (RGB camera, laser distance sensor, \nthermal camera\
    \ and a microspectrometer), was used to enable in-situ measurement of the morphology\
    \ through the lid of the culture vessels\nExperiment\nPlant species\nTime series\
    \ [day]\nEvaluations\nDetermination/Device\nI\nArabidopsis thaliana\n0–20\nRGB\
    \ growth curve\nRGB image sensor of  Phenomenona\nRGB shape analysis\nRGB image\
    \ sensor of  Phenomenona\nDepth mean canopy height\nLaser distance sensor of \
    \ Phenomenona\nDepth maximum plant height\nLaser distance sensor of  Phenomenona\n\
    II\nMalus sp.\n0–27\nRGB growth curve\nRGB image sensor of  Phenomenona\nRGB shape\
    \ analysis\nRGB image sensor of  Phenomenona\nRGB image data set\nRGB image sensor\
    \ of  Phenomenona\nDepth mean canopy height\nLaser distance sensor of  Phenomenona\n\
    Depth maximum plant height\nLaser distance sensor of  Phenomenona\nIII\nMalus\
    \ sp.,\nArabidopsis thaliana\n0, 5, 10, 15, 20\nHH Score\nVisual scoring\nApoplastic\
    \ liquid volume\nApoplastic liquid volume\nReflection spectra\nUV–VIS Spectrometer\
    \ Perkin-Elmer\nIV\nMalus sp.\n14, 21, 28\nHH Score\nVisual scoring\nApoplastic\
    \ liquid volume\nApoplastic liquid volume\nReflection spectra\nUV–VIS Spectrometer\
    \ Perkin-Elmer\nV\nMalus sp.\n0, 4, 8, 12, 16\nHH Score\nVisual scoring\nApoplastic\
    \ liquid volume\nApoplastic liquid volume\nReflection spectra\nUV–VIS Spectrometer\
    \ Perkin-Elmer\n555\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\
    \ \n1 3\npx, shutter speed = 2000  ms, iso = 100, autowhite-bal-\nance = off and\
    \ a fixed gain of 3.3, 1.5 (red, blue).\nSensor data from the multisensory robot\
    \ system “Phe-\nnomenon” (Bethge et al. 2023) were processed, segmented, \nand\
    \ various parameters were calculated. RGB image analysis \nwas performed in Python\
    \ (Van Rossum and Drake 2009), \nusing the following packages: OpenCV v3.4.9 (Bradski\
    \ \n2000), NumPy v1.20.2 (Van Der Walt et al. 2011) and \nPlantCv v3.11.0 (Gehan\
    \ et al. 2017) and the Software toolkit \nIlastik v1.3.3 (Sommer et al. 2011)\
    \ headless integrated in \nthe Python script. RGB image analysis included a histogram\
    \ \nstretching for normalization, segmentation via a trained ran-\ndom forest\
    \ classifier, normalization to the day 0 plant area \nand calculation of projected\
    \ plant area (37.7 px = 1 mm). \nShape analyses were performed on the four largest\
    \ objects \nby area and limited to the first nine days to avoid errors from \n\
    overlapping explants. We used the installed shape function \nof PlantCv to calculate\
    \ solidity (measure of density as the \nratio between object area and area of\
    \ the convex hull of the \nobject) and eccentricity (measure of deviation of an\
    \ ellipse \nto a circle (eccentricity = 0) as the ratio between major and \nminor\
    \ axis).\nDepth data were acquired once per day for each culture \ncontainer with\
    \ the point-measuring laser distance sensor as a \nspatial scan by sequential\
    \ readout of the sensor while shifting \nthe detector head of the “Phenomenon”\
    \ robot system in xy \ndirection, according to the scan pattern (100 mm × 100 mm;\
    \ \nwith a resolution of 1 mm × 1 mm). The laser distance sen-\nsor (OD-Mini OB1-B100,\
    \ Sick AG, Waldkirch, Germany) \nused in this setup was specified by the manufacturer\
    \ with a \npower consumption of < 1.92 W, laser emission wavelength \nof 655 nm,\
    \ max. output of 390 µW (laser class 1), a measur-\ning range of 50 to 150 mm\
    \ and a linearity of ± 100 µm as \nwell as spot size of 700 µm × 600 µm at a measuring\
    \ distance \nof 100 mm. The analog output of the laser distance sensor \n(10 V)\
    \ was connected via a small voltage divider circuit to \na high precision 16-bit\
    \ A/D-converter (ADS 1115), which \ncommunicated via Inter-Integral Circuit  (I2C)\
    \ with a micro-\ncontroller board (Wemos D1 Mini). Each distance measure-\nment\
    \ consisted of a up to ten single readouts and averaging \n(excluding default\
    \ sensor values), to achieve a robust and \nlow-noise measurement. A detailed\
    \ description of the robot \nsystem “Phenomenon” can be found in Bethge et al.\
    \ (2023).\nDepth data of explants were obtained by measuring \n10,000 data points\
    \ of each culture vessel once a day with \na scanning laser distance sensor. The\
    \ depth data processing \npipeline included the segmentation of culture media\
    \ by a \nRANSAC (random sample consensus, Fischler and Bolles \n1981) segmentation\
    \ approach, subtraction of RANSAC \nplane, normalization to the day 0 plant height\
    \ with the \nPython libraries: Open3D v0.15.1 (Zhou et al. 2018) and \nPyvista\
    \ v0.34.0 (Sullivan et al. 2019). Pipelines construction \nis described in detail in\
    \ Bethge et al. (2023).\nStatistical analysis of repeated measures data was per-\n\
    formed using R software. Data were transformed, if neces-\nsary, with the R package\
    \ bestNormalize v1.8.2 (Peterson and \nPeterson 2020). Different linear mixed-effect\
    \ models from \nnlme v3.1-153 package (Pinheiro et al. 2017) were fitted to \n\
    the data with different covariance structures: scaled identity, \nfirst-order\
    \ autoregressive, first-order heterogeneous autore-\ngressive, compound symmetry,\
    \ Toeplitz and heterogenous \nToeplitz. The mean model consisted of the fixed\
    \ effects \ntreatment/medium type and time and their interaction terms. \nAn extra\
    \ random effect was included in the model to account \nfor the dependencies between\
    \ measurements from the same \nculture container or in SI. 1 for shape analysis\
    \ from the same \nexplant (as nested random effect). We also included linear \n\
    models with random intercept (CulturecontainerID) and \nrandom slope (Time). The\
    \ respective best model (Fig. 1A: \nlinear mixed model with scaled identity covariance\
    \ struc-\nture and random slope; Fig. 1B and SI. 1A: linear mixed \nmodel with\
    \ heterogenous Toeplitz covariance structure and \nrandom slope; SI. 1B: linear\
    \ mixed model with heteroge-\nnous Toeplitz covariance structure and random intercept)\
    \ \nwas selected based on the Akaike Information Criterion \n(AIC, Sakamoto et al.\
    \ 1986) values and residual analysis \n(QQ-plot). Pairwise comparisons using Tukey’s\
    \ HSD test \nat p < 0.05 was performed and show significant differences \nbetween\
    \ treatments within a time point.\nVisual scoring of hyperhydricity severity level\n\
    In the Experiments III to V, the severity of HH was assessed \nfor each explant\
    \ and at every time point (Exp. III: 0, 5, 10, \n15, 20 days; Exp. IV: 14, 21,\
    \ 28 days; Exp. V: 0, 4, 8, 12, \n16 days) according to Tian et al. (2015) with\
    \ minor modi-\nfications (Table 3). The starting plant material cultivated \n\
    on control media represented the samples of 0 days after \ntransfer (DAT 0).\n\
    Determination of apoplastic liquid volume\nPer time point at least 10 samples\
    \ per treatment were col-\nlected for the determination of apoplastic liquid volume,\
    \ \nwith DAT 0 samples representing the starting material. \nApoplastic liquid\
    \ was extracted from leaf tissue by mild \ncentrifugation according to van den\
    \ Dries et al. (2013) and \nTerry and Bonner (1980): Leaves (50–150 mg FM) from\
    \ \na single explant were excised, weighed, and placed into a \n2 mL tube microcentrifuge\
    \ filter without membrane (Clear-\nLine®; Kisker Biotech GmbH & Co, Steinfurt,\
    \ Germany). \nSamples were centrifuged at 3000 g for 20 min at 4 °C. \nImmediately\
    \ after centrifugation, the leaves were reweighed \nto determine the apoplastic\
    \ liquid volume  (VAL) in µL  g−1 \nfresh mass (FM) using the Eq. 1.\n556\n \n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nwhere FM\
    \ = fresh mass of leaves in mg,  Mac = mass of leaves \nafter centrifugation and\
    \ ρH2O = water density (the water den-\nsity was taken as equal to 1 g  mL−1 assuming\
    \ the apoplastic \nliquid is mainly water and has a temperature of 4 °C).\nSpectral\
    \ data acquisition and analysis\nPrior to the quantification of apoplastic liquid\
    \ volume, one \nfully expanded leaf per explant under study was collected. \n\
    The leaf was then placed in a 3D printed sample holder \n(SI. 2) in an adaxial\
    \ position that allowed for flat clamping \n(1)\nVAL =\n(FM − Mac\n) ⋅ 휌H2O\n\
    FM\nFig. 1  Morphological differences in growth patterns of explants of \nA. thaliana\
    \ Col-0 and Malus ‘G214’ cultivated on either agar or gel-\nrite solidified media\
    \ (Mean ± SD). A The curve for the increase in \nthe projected plant area was\
    \ calculated from the analysis of the seg-\nmented RGB images normalized to the\
    \ plant area of day 0 and pre-\nsented as projected plant area  [cm2]. Since flower\
    \ initiation started at \nlater time points for A. thaliana and thus an error\
    \ in the estimation of \nprojected plant area might occur, the analysis of growth\
    \ curves was \nlimited to the first ten days. B The relative increase in mean\
    \ canopy \nheight resulted from analysis of segmented depth data collected with\
    \ \na scanning laser distance sensor and normalized to day 0 plant height. \n\
    Yellows lines indicates cultivation on standard media formulation \non Gamborg-B5\
    \ (A. thaliana) and MS-Medium (Malus) solidified \nwith 0.8% agar (w/v), while\
    \ dark gray lines display the cultivation \non induction media containing 0.25%\
    \ (w/v) gelrite, inducing HH. C \nRepresentative images at the endpoint of the\
    \ experiments. Sample \nnumber (n) indicates the individual culture containers.\
    \ Significance \nstars indicate comparisons of treatments within a time point\
    \ (day) \nwith *p < 0.05, **p < 0.01, ***p < 0.001. RGB and depth data were \n\
    acquired with the multisensory robot system “Phenomenon” (Bethge \net al. 2023).\
    \ (Color figure online)\nTable 3  Scoring of hyperhydricity by visual observation\
    \ (Tian et al. \n2015, with minor modifications)\nHyperhydricity score\nSymptoms\n\
    0\nNo visual symptoms\n1\n≤ 50% curled leaves\n2\n > 50% curled leaves\n3\n >\
    \ 50% curled and thickened leaves\n4\nCurled, thickened, translucent, fragile\
    \ leaves\n557\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\
    \ \n1 3\nwithout exerting too much pressure on the leaf (with a cav-\nity of 1 mm).\
    \ The curled hyperhydric leaves were handled \nwith care to obtain reflection\
    \ spectra from a planar surface. \nThe leaf reflectance spectra were examined\
    \ with a Perkin-\nElmer Lambda 900 UV–VIS-NIR-SWIR spectrometer \n(Perkin-Elmer\
    \ Instruments, Norwalk, USA) equipped with \n150 mm Indium-Gallium-Arsenide (InGaAs)\
    \ integrating \nsphere. The reflectance intensity was measured in steps of \n\
    1 nm in the wavelength range between 200 and 2000 nm, \nand the reflectance was\
    \ calculated using the reflection \nspectrum of the white reference standard Spectralon®.\
    \ \nRaw spectra were pre-processed in R v4.1.2 using Rstudio \n(RStudio Team 2015)\
    \ with the hsdar v1.0.4 package (Leh-\nnert et al. 2018) allowing the cleaning\
    \ of device errors, \ntrimming to spectral range of 400 mn to 2000 nm and \nsmoothing\
    \ with the Savitzky-Golay filter at a window size \nof 25 data points of third-degree\
    \ polynomials to remove \nnoise from data.\nSpectra of leaves obtained from three\
    \ experiments \n(Exp. III: 147, Exp. IV: 39 spectra, Exp. V: 51) were \ndivided\
    \ into two groups based on the significance level \nof the apoplastic liquid volume\
    \ and the HH score of the \nwhole explant was assessed by visual observation.\
    \ Here, \nthe explants with a HH score of 0 and 1 were classified as \nnormal\
    \ explants while the explants with a HH score of 2 \nto 4 represented hyperhydric\
    \ explants. This resulted in 100 \nand 137 spectra of normal and hyperhydric leaves,\
    \ respec-\ntively, covering the two plant species Malus ‘G214’ (187 \nspectra)\
    \ and A. thaliana (50 spectra). For visualization \nand isolation of the HH-specific\
    \ absorption features, leaf \nspectra were further processed with the segmented\
    \ upper \nhull continuum removal method described in detail in Leh-\nnert et al.\
    \ (2018). This normalization method allowed a \ncomparison of individual absorption\
    \ features on a com-\nmon baseline formed by a segmented upper hull of local \n\
    maxima and resulted in absorption features spectra. In \naddition, difference\
    \ spectra of absorption feature spectra \nwere calculated by subtracting normal\
    \ leaf spectra from \nhyperhydric leaf spectra.\nIn addition, we defined three\
    \ spectral ranges based on \nthe sensitivity of the state-of-the-art sensor technologies\
    \ \nsuch as standard RGB camera systems with silicon sensor \nchips (3 channels:\
    \ B: 400 nm to 500 nm, G: 500 nm to \n600 nm and R: 600 nm to 700 nm), multispectral\
    \ camera \nsystems with silicon sensor chips (4 channels: B: 400 nm \nto 500 nm,\
    \ G: 500 nm to 600 nm, R: 600 nm to 700 nm and \nNIR: 750 nm to 850 nm) and SWIR-HSI\
    \ camera systems \nwith Indium-Gallium-Arsenide (InGaAs) sensor chips \n(SWIR:\
    \ 900 nm to 1700 nm). This division was made as a \ndecision support for assessing\
    \ the potential of the candi-\ndate detection systems to detect HH based on their\
    \ spectral \nsensitivity range and considering their affordability.\nIdentification\
    \ of hyperhydricity‑specific absorption features\nDifferent ML models were trained\
    \ with the caret v6.0-90 \npackage (Kuhn 2008) in the R software to identify the\
    \ \nkey absorption features that discriminate between nor-\nmal and hyperhydric\
    \ explant leaf spectra. Here, pre-pro-\ncessed spectral data sets (237) were centered\
    \ and scaled \nand divided into a training set (178 spectra; Malus: 143, \nA.\
    \ thaliana: 35, with 103 normal and 75 hyperhydric \nexplants, in total) and a\
    \ test set (59 spectra; Malus: 44, \nA. thaliana: 15, with a total of 34 normal\
    \ and 25 hype-\nrhydric explants). All classification models were trained \nwith\
    \ the same resampling procedure consisting of a 10 \ntimes tenfold repeated cross\
    \ validation (CV). The tenfold \nrepeated CV divides the training data into 10\
    \ equal parts \n(10 subsamples with a size of 178/10). These parts are \niterated\
    \ 10 times, during each iteration, 9 of the 10 parts \nserve as training data,\
    \ and the remaining 10th part as the \nvalidation set to calculate model performance\
    \ metrics. In \n10 times repeated tenfold CV this process is repeated 10 \ntimes;\
    \ therefore, performance of training was validated on \n100 validation subsamples\
    \ consisting of 17–18 individual \nspectra.\nIn the confusion metrics, correctly\
    \ classified normal \nand hyperhydric leaves formed the true-positive (TP) \n\
    and the true-negative (TN) class, while false classified \nones constituted the\
    \ false-positive (FP) and false-negative \n(FN) class, respectively. For evaluation\
    \ of model valida-\ntion performance, the sensitivity (Eq. 2; TPR: true posi-\n\
    tive rate) and the specificity (Eq. 3; TNR: true negative \nrate) were calculated\
    \ with normal explants as the positive \nclass and the area under the curve (AUC)\
    \ of the receiver-\noperator-characteristics (Eq. 4; AUC ROC), while for evalu-\n\
    ation of model test performance, the accuracy (Eq. 5) was \ndetermined. Here,\
    \ misclassifications are described by the \nfalse negative rate (FNR) and false\
    \ positive rate (FPR). \nBalanced accuracy (Eq. 6) and  F1 score (Eq. 7) were\
    \ calcu-\nlated to account for putative class imbalances. To find the \nbest suitable\
    \ model for discriminating between normal and \nhyperhydric leaf spectra, different\
    \ ML model structures \nwere tested, including a neuronal net with the maximum\
    \ \nallowable number of weights set to 2000 (“nnet” from nnet \nv7.3-16 package;\
    \ Ripley et al. 2016), a linear discriminate \nanalysis (“lda” from caret package),\
    \ a supported vector \nmachine (“svmLinear” from caret package), a random for-\n\
    est (“rf” from caret package), a high dimensional discrimi-\nnate analysis (“hdda”\
    \ from caret package) as well as a \nlinear discriminate analysis (“lda” from\
    \ caret package with \nPCA-preprocessed data set) with an upstream principal \n\
    component analysis (PCA). Based on their resampled per-\nformance metrics, the\
    \ best model was selected to identify \nits most relevant features/wavelengths\
    \ on the basis of the \nunderlying variable importance in the model.\n558\n \n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nAutomated\
    \ hyperhydricity detection\nTo test the validity of the developed spectral classifier,\
    \ an \nHSI-system operating in the shortwave infrared (SWIR) \nregion was used\
    \ to acquire a single HSI data cube from a \nculture vessel containing a HH-sensitive\
    \ apple genotype \n(Malus ‘Selection 4’). The imaging system that was devel-\n\
    oped and described by Thiel (2018) consisted of an EVK \nHelios Core NIR Line-scan\
    \ camera (240 px × 1 px and 252 \nspectral channels in the wavelength region of\
    \ 900 nm to \n1700 nm), two 65 W halogen spot lights and a conveyer-\nbelt system\
    \ to move the sample. Image acquisition was \nperformed in closed polypropylene\
    \ culture vessels, so that \nsterile conditions could be maintained inside the\
    \ vessel and \nwater condensation was prevented by heat radiation from \nthe halogen\
    \ lamps. Since only one HSI data cube could be \nacquired, these results were\
    \ considered to be an exemplary \nand preliminary validation test.\nThe developed\
    \ spectral classifier was retrained with a \nreduced number of features to match\
    \ the spectral channels \nof the imaging system (Features/wavelengths: 252 channels\
    \ \nin the range between 900 and 1700 nm). Due to the binary \nclassification\
    \ output of the classifier, most of the background \npixels were removed by creation\
    \ of a binary mask with \nsimple thresholding of the image slice at a wavelength\
    \ of \n1000 nm. Then each pixel of the segmented hyperspectral \ndata cube was\
    \ inserted as an input to the spectral classifier \nand class membership was predicted.\n\
    As a more affordable approach and as a proof of con-\ncept, an object detection\
    \ model based on annotated RGB \nimages acquired by the robot system was trained.\
    \ Therefore, \n250 images were randomly selected and annotated with the \n(2)\n\
    Sensitivity = TPR = 1 − FNR =\nTP\nTP + FN\n(3)\nSpeciﬁcity = TNR = 1 − FPR =\n\
    TN\nTN + FP\n(4)\nAUCROC = ∫ TPR (FPR) d(FPR)\n(5)\nAccuarcy =\nTP + TN\nTP +\
    \ FP + TN + FN\n(6)\nBalanced accuarcy = TPR + TNR\n2\n(7)\nF1 score = 2 ×\n(\n\
    TP\nTP + FP\n)\n× TPR\n(\nTP\nTP + FP\n)\n+ TPR\ngraphical user interface  Roboflow©\
    \ (Dwyer et al. 2022). \nThe image data set consisted of 200 annotated images\
    \ of \neight culture containers from Experiment II and 50 images \nfrom a comparable\
    \ experiment to increase variance in the \nnumber of explants, background colour,\
    \ and colour of cul-\nture media. A total of 504 normal explants and 545 hype-\n\
    rhydric explants were included. The image data set was \ndivided into 175 images\
    \ as training set, 50 images as vali-\ndation set and 25 images as test set. Data\
    \ augmentation of \nannotated bounding boxes increased the training set to 1800\
    \ \nimages and included: horizontal and vertical flip, rotation \nby 90° (clockwise,\
    \ counter-clockwise, upside down), rota-\ntion by ± 5°, brightness by ± 10%, exposure\
    \ by ± 7%, blur \nwith 2px and noise with 2% of pixels. The data set (Bethge \n\
    2023) is publicly accessible via  Roboflow© universe. Time \nseries images of\
    \ two culture vessels from Experiment II were \nretained and used to visualize\
    \ the trained model. Object \ndetection models perform attempts to identify and\
    \ locate \nobjects in images while assigning them to the appropriate \nclasses.\
    \ We selected YOLOv8 (Jocher et al. 2023) archi-\ntecture as the latest versions\
    \ of the YOLO (“You only look \nonce”, Redmond et al. 2016) family. YOLO is a\
    \ single-stage \nobject detector, consisting of three parts in its architecture:\
    \ \nbackbone, neck and head. The backbone is defined by sev-\neral convolutional\
    \ layers which extract key features from \nthe images, the neck uses the features\
    \ and forms the feature \npyramid by fully connected layers and the head is the\
    \ final \noutput layer for prediction of bounding boxes and classifi-\ncation.\
    \ The training process was performed in the Google \nColaboratory (Colab/Colab\
    \ Pro) environment on a NVIDIA \nA100-SXM4-40 GB graphical processing unit (GPU)\
    \ ser-\nviced by Google. In addition, the model was trained with \nthe following\
    \ parameters: epochs = 250 (early stopping \noccurred after 188 epochs), batch\
    \ size = 16 images, image \nsize = 640 px, patience = 100 epochs, learning rate\
    \ = 0.01, \nmomentum = 0.94, intersection over union (IoU) = 0.7. We \nlet Roboflow\
    \ train two object detection models, one from \nscratch and one with weights from\
    \ a previously trained \nmodel (additional 125 images from the same experiment)\
    \ to \nsee the full potential of the dataset with the optimized pipe-\nline. Evaluation\
    \ of model performance was based on preci-\nsion (Eq. 8), recall (Eq. 9), average\
    \ precision (AP; Eq. 10) \nand mean average precision (mAP; Eq. 11) of the validation\
    \ \nset. Here true positive (TP) indicate a correct detection and \nclassification,\
    \ false negative (FN) describes cases where the \nprediction missed the detection\
    \ contained in the ground truth \ndata, while in a false positive (FP) case a\
    \ bounding box was \npredicted on a location not contained in the ground truth\
    \ \ndata. Thereby, AP represents the area under the precision-\nrecall-curve across\
    \ a range of probability confidence thresh-\nold values from 0 to 1. The mAP is\
    \ the sum of AP of each \nclass (k) divided by the number of classes (n) at a\
    \ given \nintersection over union (IuO) threshold of 0.5. Intersection \n559\n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573 \n1 3\nover union\
    \ is defined as ratio between the overlap area to the \nunited area of the predicted\
    \ and ground truth bounding box. \nAfter the training process predications were\
    \ obtained using \nthe Python library roboflow v0.2.25 (Dwyer et al. 2021) with\
    \ \nIuO threshold and confidence threshold set to 0.5.\nMorphological characteristics\
    \ of hyperhydricity \nvia image analysis\nStudying the morphology of the shoots\
    \ of the two treatments \nrevealed major differences in horizontal and vertical\
    \ growth. \nSignificantly stronger growth, quantified as projected plant \narea,\
    \ was observed for the gelrite treatment at early time \n(8)\nPrecision =\nTP\n\
    TP + FP\n(9)\nRecall =\nTP\nTP + FN\n(10)\nAP =\n1\n∫\n0\nPrecision(Recall) d\
    \ (Recall)\n(11)\nmAP = 1\nn\nk=n\n∑\nk=1\nAP(k)\npoints (5 days) for both plant\
    \ species (Fig. 1A). After \n4 weeks of cultivation, shoots of Malus in culture\
    \ vessels \nwith gelrite medium had with 24.5  cm2 a 2.4 times greater \nincrease\
    \ in projected plant area than shoots in vessels with \nagar medium with 10.3\
    \  cm2. Here, 65% of the explants of \nMalus had a HH score > 2 in the gelrite\
    \ treatment compared \nto 0% for agar treatment. For A. thaliana we evaluated\
    \ the \nprojected plant area only until day 10 to avoid distorting \neffects on\
    \ projected plant area due to flower initiation start-\ning at day 12. Shape analysis\
    \ of single explants showed sig-\nnificant differences in solidity at day 3 and\
    \ in eccentricity \nat day 6 for A. thaliana, whereas the shape differences of\
    \ \nMalus explants were not significant (SI. 1). Vertical growth \nanalysis, quantified\
    \ as mean canopy height (Fig. 1B) and \nmaximum shoot height as mean of upper\
    \  10th percentile (SI. \n1), showed a significantly higher mean canopy height\
    \ of \nMalus for the gelrite treatment at day 18 and of A. thaliana \nat day 16.\
    \ An even earlier distinction was recorded for the \nmaximum shoot height, i.e.\
    \ at day 11 and 14 for A. thaliana \nand Malus, respectively.\nHyperhydricity\
    \ induction\nVisual scoring of HH revealed the dynamics of HH induc-\ntion using\
    \ gelrite in the two plant species under investiga-\ntion. Anthocyanin accumulation\
    \ was noted within the first \n4 days in both treatments for Malus. However, it\
    \ persisted \nFig. 2  Visual scoring of hyperhydricity of A Malus ‘G214’ and B\
    \ \nA. thaliana Col-0 in  vitro cultures over 20  days (DAT, Days After \nTreatment).\
    \ Samples from 0  days after transfer (DAT 0) represent \nthe starting plant material\
    \ cultured on control media. Yellows bars \nindicate cultivation on standard media\
    \ formulation A MS-Medium, \nB Gamborg-B5 solidified with 0.8% (w/v) agar, while\
    \ gray bars dis-\nplay the cultivation on induction media containing 0.25% (w/v)\
    \ gel-\nrite. Dashed lines represent the medians of each histogram. Sample \n\
    number (n) indicates the individual explants. The different sample \nnumbers result\
    \ from the combined evaluation with different methods \n(apoplastic liquid evaluation,\
    \ reflection spectroscopy) of the same \nsamples. Different letters resulting\
    \ from Kruskal–Wallis test followed \nby Fisher’s LSD (p  < 0.05) indicate significant\
    \ differences between \nhistograms. Kruskal–Wallis effect size could be determined\
    \ to be \nvery strong with A η2 = 0.62 and B η2 = 0.65. (Color figure online)\n\
    560\n \nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\n\
    only in the gelrite treatment until the end of the experiment \nin most explants.\
    \ In Malus, severe symptoms of HH were \ninduced even on the agar control medium\
    \ in 12.5% of the \nshoots (Fig. 2A). In two experiments, significant differences\
    \ \nin the HH score and the occurrence of severe symptoms \n(curled, thickened\
    \ and translucent leaves = level 4 of the \nHH score) between the agar control\
    \ and the gelrite induction \ntreatment were identified 10 days (Fig. 2A) and\
    \ 8 days (SI. \n3) after transfer. When performing this experiment under a \n\
    novel phenotyping system, time-lapse videos were taken. \nThey confirmed these\
    \ observations and visualized the tem-\nporal development of HH in the two plant\
    \ species (Malus: \nSI. 4 and A. thaliana: SI. 5). Three of four apple shoots\
    \ \nturned into a hyperhydric status and formed first hyperhy-\ndric leaves (SI.\
    \ 4: arrows) on gelrite at 5 DAT (SI. 4: 100 h). \nThey started to curl at 8 DAT\
    \ and also became much larger \nthan those on agar. After 27 days, severe symptoms\
    \ appeared \non dark green to reddish explants that exhibited compact \ngrowth\
    \ with curled, epinastic, and brittle leaves.\nFor A. thaliana, there was already\
    \ a significant increase \nin the HH score after 5 days of treatment (Fig. 2B).\
    \ Fur-\nthermore, decolorization of leaves was the predominating \nsymptom of\
    \ HH in A. thaliana on gelrite induction medium. \nFor A. thaliana seedlings,\
    \ first signs of HH (SI. 5: arrows) \nbecame visible at 5 DAT (SI. 5: 100 h) on\
    \ gelrite-solidified \nmedium and shoots developed longer petioles and much \n\
    larger leaves with severe HH symptoms.\nThe apoplastic liquid volume increased\
    \ steadily for Malus \n(Fig. 3A) until 15 DAT, while Experiment IV (SI. 3) demon-\n\
    strated a decrease at later time points: 21 days and 28 days. \nA significant\
    \ difference in apoplastic liquid content in both \nplant species was detected\
    \ at the earliest time point: 4 DAT \n(SI. 3) and 5 DAT (Fig. 3A and B), where\
    \ the apoplastic \nliquid volumes of explants on gelrite induction media were\
    \ \nalready twice as high as those of explants on agar control \nmedia. In Malus,\
    \ three independent experiments (Fig. 3A \nand SI. 3) allowed us to confine the\
    \ time of peak in apo-\nplastic liquid volume at 12 to 16 DAT. Apparently, up\
    \ to \nthis timepoint, quantification of apoplastic liquid volume \nreflected\
    \ the HH score well—even the occurrence of some \nhyperhydric explants on the\
    \ agar control medium was also \nreflected in the increase in apoplastic water\
    \ volume (Fig. 3A \nvs. SI. 3). However, at later time points, the severity of\
    \ HH \nsymptoms steadily increased, while apoplastic liquid volume \nstayed constant.\n\
    To prove the relation between the objective quantification \nof apoplastic liquid\
    \ volume and the HH score determined \nby visual scoring, data pairs of a total\
    \ of 349 measurements \nFig. 3  Apoplastic liquid volume of A Malus ‘G214’ and\
    \ B A. thali-\nana Col-0 in vitro cultures over time (DAT, Days After Treatment).\
    \ \nSamples from 0  days after transfer (DAT 0) represent the starting \nplant\
    \ material cultured on control media. Yellow lines indicate cul-\ntures on standard\
    \ media A MS-Medium, B Gamborg-B5 solidified \nwith 0.8% (w/v) agar, while gray\
    \ dashed lines display the cultures \non HH induction media containing 0.25% (w/v)\
    \ gelrite (Mean ± SD). \nThe values of A. thaliana at DAT 0 B were masked in gray\
    \ to indicate \nthe authors’ uncertainty, because the plants were very small when\
    \ \nthe apoplastic water volume was determined at this time, and there-\nfore\
    \ a large influence of adhering water could not be excluded. Rep-\nlicate number\
    \ (n) indicates the individual explants. Different letters \nresulted from Tukey’s\
    \ HSD test at p < 0.05 and indicate significant \ndifferences when comparing time\
    \ points within one treatment, while \nasterisks indicate comparisons of treatments\
    \ within a time point with \n* =  p < 0.05, ** =  p < 0.01, *** =  p < 0.001.\
    \ (Color figure online)\n561\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023)\
    \ 154:551–573 \n1 3\nfrom both treatments of Malus were used (Fig. 4). We found\
    \ \nthe highest correlation between HH score and apoplastic \nvolume to be ρ(18)\
    \ = 0.83 (p < 0.001) 12 DAT using spear-\nman’s rank correlation for all acquired\
    \ time points for Malus. \nInterestingly, only three groups could be distinguished\
    \ \nsignificantly by apoplastic liquid volume. Explants with a \nHH score of two\
    \ had more than > 50% curled leaves and \nin average a double amount of apoplastic\
    \ liquid volume. \nWe therefore restricted the three significant groups to two\
    \ \nclasses (HH score 0–1: normal explants and HH score 2–4: \nhyperhydric explants)\
    \ in further analysis, regardless of the \ntreatment in order to exclude treatment-depended\
    \ effects on \nthe spectral analysis.\nSpectral analysis of hyperhydricity\nThe\
    \ evaluation of leaf explants via UV–VIS-NIR-SWIR \nspectroscopy (Fig. 5) revealed\
    \ the first major difference in \nsignificantly reduced reflectance in the RGB\
    \ (400 nm to700 \nnm) region of 6.5 ± 3.2% for the hyperhydric explants com-\n\
    pared to 8.7 ± 3.9% for normal explants. The largest dif-\nference in reflectance\
    \ was recorded for NIR (750 nm to \n850 nm) region with a reflectance of 20.5\
    \ ± 9.0% hyperhy-\ndric explants and normal explants with 28.3 ± 9.5%. Also, \n\
    for the SWIR (950 nm to 1700 nm) region the overall reflec-\ntance was lower in\
    \ hyperhydric explants (13.8 ± 8.2% for \nhyperhydric and 21.4 ± 8.7% for normal\
    \ explants). Differ-\nences in average reflectance were most significant in the\
    \ blue \n(p< 2.2e-16) region followed by SWIR (p< 4.3e-14), green \n(p< 1.1e-12),\
    \ red (p< 2.5e-11) and the NIR region (p< 2.0e-\n09) according to the results\
    \ of a Mann–Whitney test.\nThe emergence of HH-specific absorption features \n\
    over time was recorded applying the continuum removal \nmethod to pre-processed\
    \ spectra of Experiment V and the \nFig. 4  Relation between visual scoring of\
    \ hyperhydricity and apo-\nplastic liquid volume of Malus ’G214’ (Mean ± SD).\
    \ Data obtained \nfrom three different induction experiments (Exp. III-V) covering\
    \ time \npoints from 0 to 28 DAT. Replicate number (n) indicates the indi-\nvidual\
    \ explants. Different letters resulted from Tukey’s HSD test at \np < 0.05 and\
    \ show significant differences between score levels\nFig. 5  Raw reflectance spectra\
    \ of Malus ‘G214’ and A. thaliana \nCol-0 in vitro leaves. Mean (solid) ± SD (dashed)\
    \ spectra of normal \nleaves (N, in green) and hyperhydric leaves (HH, in blue).\
    \ The dis-\ntinction was based on visual scoring of HH (N: 0–1 HH score; HH: \n\
    2–4 HH score). Wavebands represent different spectral regions, \ndefined by the\
    \ sensitivity of silicon-based (Si) cameras, such as \naffordable RGB and RGB-NIR\
    \ multispectral cameras, and a more \nexpensive Indium-Galium-Asenide-based (InGaAs)\
    \ detection sen-\nsor. Reflectance spectra were measured with an UV–VIS-NIR-SWIR\
    \ \nspectrometer (PerkinElmer Lambda 950) in a wavelength range of \n200 nm to\
    \ 2000 nm and at a resolution of 1 nm. (Color figure online)\n562\n \nPlant Cell,\
    \ Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nformation of difference\
    \ spectra of the isolated band depth \nspectra, where the absorption features\
    \ spectra of normal \nexplants were subtracted from absorption features spectra\
    \ \nof hyperhydric explants (Fig. 6). Greater absorption of hype-\nrhydric explants\
    \ was observed as early as 8 DAT, with a \nmaximum at 1402 nm and a full width\
    \ at half maximum of \n157 nm. At later time points the difference in absorbance\
    \ at \naround 980 nm, 1150 nm, 1400 nm, 1520 nm and 1780 nm \nincreased negatively,\
    \ while at around 1930 nm the difference \npositively increased. In the VIS region,\
    \ two further local \nmaxima arose at 460 nm and 695 nm at DAT 10, which were\
    \ \nalso detected at the later time points. However, these peaks \ncan be considered\
    \ as artefacts of the reduced reflection in the \ngreen region due to the continuum\
    \ removal method based \non connection of local maxima. In addition, a consistent\
    \ \npositive peak indicating less absorption or higher reflec-\ntion of hyperhydric\
    \ explants was found with a maximum \naround 1930 nm, besides the two local minima\
    \ at 1400 nm \nand 1520 nm. When combining data from all experiments \n(SI. 6),\
    \ including different time points and the two differ-\nent plant species, we identified\
    \ reliable minima (arrows) at \n980 nm, 1150 nm, 1400 nm, 1520 nm, and 1780 nm,\
    \ indicat-\ning stronger absorption of the hyperhydric explants, and a \nreliable\
    \ maximum at 1930 nm.\nFig. 6  Spectral contrasting reflectance of Malus ‘G214’\
    \ normal and \nhyperhydric explants over time (DAT, Days After Treatment). The\
    \ \nspectral data used originate from Experiment V. Upper row: Raw \nreflection\
    \ spectra; middle row: extracted absorption features after \nsegmented convex-hull\
    \ removal of raw spectra; bottom row: differ-\nence spectrum of the absorption\
    \ peaks, where the absorption features \nspectra of normal explants were subtracted\
    \ from absorption features \nspectra of hyperhydric explants. Mean (solid) ± SD\
    \ (dashed) spectra \nof normal explant leaves (N, in green) and hyperhydric explant\
    \ leaves \n(HH, in blue). Distinction of N and HH was based on visual scoring\
    \ \nof HH (N: 0–1 HH score; HH: 2–4 HH score). Arrows indicate puta-\ntive major\
    \ biochemical compounds absorbing in the given wavelength \nregion, according\
    \ to Curran (1989). Colored arrows represent: \"Chl\" \n= chlorophyll (dark green),\
    \ \"Antho\" = anthocyanin (red), \"H20\" = \nwater (dark blue), \"L\" = lignin\
    \ (dark red), \"P\" = protein (green), \"S\" \n= sugar (yellow). Reflection spectra\
    \ were measured with an UV–VIS-\nNIR-SWIR spectrometer (PerkinElmer Lambda 950)\
    \ in a wavelength \nrange of 200 mn to 2000 and at a resolution of 1 nm. Absorption\
    \ fea-\ntures spectra of the other conducted experiments, showing similar \nresults,\
    \ can be found in SI. 6. (Color figure online)\n563\nPlant Cell, Tissue and Organ\
    \ Culture (PCTOC) (2023) 154:551–573 \n1 3\nTo demonstrate whether the observed\
    \ differences in \nreflectance spectra are sufficient to reliably discriminate\
    \ \nbetween hyperhydric and non-hyperhydric explants, while \ngeneralizing plant\
    \ species and time points, we performed \na model spot checking for several ML\
    \ models with whole \nspectral data sets as input (Table 4). The models spot check\
    \ \nbased on AUC ROC metrics identified partial least square \n(PLS) and linear\
    \ discriminate analysis with upstream prin-\ncipal component analysis (PCA.LD)\
    \ with 0.94 to be superior \nin the training step when classifying the explants\
    \ against \nthe other models, while supported vector machine (SVM), \nneutral\
    \ net (NNET) with 0.93 and random forest model (RF) \nwith 0.92 performed only\
    \ slightly worse. High dimensional \nlinear discriminate analysis (HD.LD) showed\
    \ the lowest per-\nformance and was therefore excluded. Furthermore, SVM \nwas\
    \ best in classifying normal explants as expressed in the \nsensitivity metrics\
    \ with 0.91 ± 0.08, while NNET reached \nwith 0.93 ± 0.13 the highest specificity\
    \ indicating the best \nperformance in identifying hyperhydric explants. On the\
    \ \ntest set consisting of 59 unseen spectra, SVM outperformed \nthe other models\
    \ with the highest accuracy with 0.85, the \nhighest balanced accuracy with 0.84,\
    \ the highest sensitivity \nwith 0.91 and the highest F1 score of 0.87 and was\
    \ therefore \nselected as final model, besides for its low training time and \n\
    its better human interpretability. As a reference of a classical \napproach, we\
    \ checked the classification performance of a \ntwo-band normalized difference\
    \ ratio index using a threshold \nof 0.35, which resulted in a low accuracy of\
    \ 0.63.\nThe evaluation of the predictor importance based on \nROC-curve importance\
    \ of SVM revealed the most impor-\ntant wavelength for classification (Fig. 7).\
    \ The most relevant \nwavelength for classification was found at 1949 nm, followed\
    \ \nby the peak at 1445 nm in the SWIR region, 424 nm in the \nblue region and\
    \ 676 nm in the red region. The wavelength \nregion from 700 to 900 nm, including\
    \ the NIR region, con-\ntained the least essential information for the classification.\
    \ In \nthe green region, 500 nm was most important, while in the \nSWIR region\
    \ two further peaks were identified at 975 nm \nand 1202 nm.\nThe 237 acquired\
    \ spectra of the two species were further \nused to simulate three in literature\
    \ stated HH-affected leaf \ncompounds over time (anthocyanin, water, lignin) via\
    \ \nTable 4  Performance metrics of machine learning (ML)-based spectral classifiers.\
    \ Bold letters indicate the value for the best performing model \nin each column\n\
    * Principal component analysis (PCA) was performed prior to linear discriminate\
    \ analysis (LD), therefore the training time should be considered \nslightly higher\n\
    a Note: AUC ROC, sensitivity and specificity were calculated with normal explants\
    \ as the positive class\nb Note: Normalized difference ratio index with a threshold\
    \ of 0.35\nML model\nTraining\nData set [No. of \nspectra]\nTrain time [s]\nAUC\
    \ ROC\na [Mean ± SD]\nSensitivitya \n[Mean ± SD]\nSpecificitya \n[Mean ± SD]\n\
    NNET\n178\n761.7\n0.93 ± 0.07\n0.89 ± 0.10\n0.83 ± 0.13\nLDA\n178\n115.2\n0.87\
    \ ± 0.09\n0.82 ± 0.12\n0.74 ± 0.16\nSVM\n178\n42.2\n0.93 ± 0.07\n0.91 ± 0.08\n\
    0.79 ± 0.15\nRF\n178\n819.5\n0.92 ± 0.06\n0.84 ± 0.11\n0.80 ± 0.15\nPLS\n178\n\
    5.6\n0.94 ± 0.06\n0.90 ± 0.09\n0.82 ± 0.13\nHD.DA\n178\n38.0\n0.83 ± 0.09\n0.83\
    \ ± 0.12\n0.78 ± 0.14\nPCA.LD*\n178\n1.4\n0.94 ± 0.06\n0.89 ± 0.09\n0.82 ± 0.13\n\
    NDRIb\n–\n–\n–\n–\nTest\nML model\nData set [No. of \nspectra]\nAccuracy\nAccuracy\
    \ [95% CI]\nBalanced \naccuracy\nSensitivitya\nF1 score\nSpecificitya\nNNET\n\
    59\n0.81\n0.69–0.90\n0.81\n0.85\n0.84\n0.76\nLDA\n59\n0.73\n0.59–0.84\n0.72\n\
    0.79\n0.77\n0.64\nSVM\n59\n0.85\n0.82–0.93\n0.84\n0.91\n0.87\n0.76\nRF\n59\n0.71\n\
    0.58–0.82\n0.72\n0.68\n0.73\n0.76\nPLS\n59\n0.69\n0.56–0.81\n0.70\n0.68\n0.72\n\
    0.72\nHD.DA\n59\n0.61\n0.47–0.73\n0.62\n0.53\n0.61\n0.72\nPCA.LD*\n59\n0.69\n\
    0.56–0.81\n0.70\n0.68\n0.72\n0.72\nNDRIb\n59\n0.63\n0.51–0.77\n0.66\n0.47\n0.59\n\
    0.84\n564\n \nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n\
    1 3\ndescribed vegetation indices (Fig. 8; mARI, Gitelson \net al. 2006; NDWI,\
    \ Gao 1996; NDLI, Serrano et al. 2002). \nHyperhydric explants of A. thaliana\
    \ showed a relatively \nsmall increase in ARI, high increase in NDWI and nota-\n\
    ble reduction in NDLI compared to normal explants. All \nthree vegetation indices\
    \ simulated using spectra of Malus \n“G214” classified as hyperhydric, revealed\
    \ a strong change \nover time compared to normal spectra.\nAutomated detection\
    \ of hyperhydricity\nTo test the validity of the SWIR region of the trained spec-\n\
    tral classifier as spectral region with high importance for \ndiscrimination of\
    \ HH and to see the generalization to a \nnew domain, the classifier was applied\
    \ on a previously \nacquired SWIR-HSI data set from culture vessels contain-\n\
    ing normal (N, green) and hyperhydric explants (HH, blue) \nFig. 7  Variable importance\
    \ of spectral classification of hyperhydric-\nity using a support vector machine\
    \ approach. Classification classes \nconsisted of reflection spectra from either\
    \ normal or hyperhydric \nleaves based on visual scoring of HH (N: 0–1 HH score;\
    \ HH: 2–4 HH \nscore). Wavebands representing different spectral regions, defined\
    \ by \nthe sensitivity of silicon-based (Si) cameras, such as affordable RGB \n\
    and RGB-NIR multispectral cameras, and more expensive Indium-\nGalium-Asenide-based\
    \ (InGaAs) sensors as candidates for detection\nFig. 8  Selection of contrasting\
    \ vegetation indices to hyperhydricity \ninducing cultivation of Malus 'G214'\
    \ and A. thaliana 'Col-0'. Vegeta-\ntion indices were calculated from spectra\
    \ from three different experi-\nments (Exp. III-V). Data points from normal explants\
    \ are indicated in \ngreen (N: 0–1 HH score), while the blue color represents\
    \ data from \nhyperhydric tissue (HH: 2–4 HH score). Estimated 95% confidence\
    \ \ninterval was colorized in light gray, while lines illustrate the locally \n\
    weighted data trend by  2nd order polynomial regression. ARI/mARI \ndefined according\
    \ to Gitelson et al. (2006), NDWI from Gao (1996) \nand NDLI according to Serrano\
    \ et al. (2002). (Color figure online)\n565\nPlant Cell, Tissue and Organ Culture\
    \ (PCTOC) (2023) 154:551–573 \n1 3\nof Malus ‘Selection 4’ (Fig. 9A). From the\
    \ spectral signa-\ntures (Fig. 9B), a normalized difference ratio index (NDRI,\
    \ \nFig. 9C–E) as a two-band index with a HH-insensitive \nwavelength at 1086 nm\
    \ (Fig. 9C) and a HH-responsive \nwavelength at 1432 nm (Fig. 9D) was derived.\
    \ Hyperhy-\ndric explants became almost invisible due to their high \nabsorption/\
    \ reduced reflection (R) at 1432 nm (Fig. 9D \nand SI. 7). Based on the acquired\
    \ spectral signature a nor-\nmalized difference ratio index (NDRI) could be derived\
    \ \n(Eq. 12), which is formed by two wavelengths, an HH-\ninsensitive correction\
    \ wavelength at 1086 nm and a HH-\nsensitive at 1432 mn.\nThe NDRI image (Fig. 9E)\
    \ was segmented with a mask \nfor plant pixels (Fig. 9F) and a threshold was applied\
    \ to \nproduce the classification image (Fig. 9G). For the ML \napproach that\
    \ included the application of the spectral clas-\nsifier (Fig. 9H), some modifications\
    \ were made to the trained \nspectral classifier, such as spectral resampling\
    \ to fit the spec-\ntral sensor channels and segmentation to limit the task to\
    \ \na two-class problem (see Materials and Methods section).\n(12)\nNDRI =\n(R1086nm\
    \ − R1432nm\n)\n(R1086nm + R1432nm\n)\nAs a more affordable approach of HH detection—SWIR\
    \ \ncamera systems can cost hundred to thousand times more \nthan an RGB camera\
    \ system—three different object detec-\ntion models were trained based on RGB\
    \ image time series \ndata sets to determine if the information contained in the\
    \ \nthree spectral channels of the RGB images (in addition to \nthe observed morphological\
    \ differences in the shape of the \nexplants) was sufficient to correctly classify\
    \ the hyperhy-\ndric explants. With all three trained models (Table 5) a high\
    \ \nmAP of > 88% was observed for the validation set, indicat-\ning a high accuracy\
    \ in localization and correct classification \nof the explants in the images.\
    \ Highest precision of 86.8% \nin validation set was reached with the model PCTOC_V2.\
    \ \nFor this model, we used the Roboflow Train option to train \nan object model\
    \ from scratch. The model PCTOC_V3 per-\nformed best in terms of the recall metric\
    \ with 95.7% and \nmAP with 95.6% in the validation set. In an unseen test data\
    \ \nset PCTOC_V3 outperformed the other models in mAP with \n97.0% and highest\
    \ recall 89.0% and was therefore selected \nto visualize its performance on a\
    \ selection of test set images \n(Fig. 10) and on unseen time-series data from\
    \ two culture \nvessels of the same experiment (SI. 8).\nThe PCTOC_V3 model identified\
    \ multiple objects on \nthe selection of test set images (Fig. 10) with only slightly\
    \ \ngreater predicted bounding boxes compared to ground truth. \nFig. 9  Validation\
    \ test of major absorption features by hyperspectral \nimaging. A Reference RGB\
    \ image of a Malus ‘Selection 4’ vessel \nwith normal (N, green) and hyperhydric\
    \ (HH, blue) explants used \nfor hyperspectral imaging of the SWIR region with\
    \ the EVK Helios \nCore NIR Line-scan camera (240 px × 1 px and 252 spectral chan-\n\
    nels in the wavelength region of 900 nm to 1700 nm, according to \nThiel 2018).\
    \ B SWIR reflectance spectra of normal leaf, hyperhydric \nleaf and culture media\
    \ (CM) pixels (green, blue and orange) with \nthe dashed vertical lines indicating\
    \ spectral locations of selected \nwavelengths. C and D False-color images of\
    \ selected wavelengths \nat 1086 nm and 1432 nm. E Normalized difference ratio\
    \ of selected \nwavelengths used to illustrate classical discriminating approach\
    \ via F \nsegmentation of plant pixels and G binarization by thresholding. Pro-\n\
    posed discriminating approach by application of H ML model “Sup-\nport vector\
    \ machine“ (ML-SVM) on segmented plant pixels of the \nSWIR-Hyperspectral-Image-Cube\
    \ (SWIR-HSI-Cube). ML-SVM was \nlaboratory-trained with single leaf reflection\
    \ spectra and is presented \nas the predicted probability images of plant pixels.\
    \ Hyperspectral \nimaging was performed through the lid of theculture vessel.\
    \ (Color \nfigure online)\n566\n \nPlant Cell, Tissue and Organ Culture (PCTOC)\
    \ (2023) 154:551–573\n1 3\nA supposedly perfect classification could be reached\
    \ with \nthe prediction settings used. However, severely hyperhy-\ndric explants\
    \ (Fig. 10B1) received a lower class member-\nship probability then explants with\
    \ developing HH symp-\ntoms (Fig. 10B2). Class membership probability of normal\
    \ \nexplants was generally high on the test set (Fig. 10B3) and \nseemed to be\
    \ stable even on the time-series RGB image data \nset (SI. 8, left image). For\
    \ hyperhydric explants, prediction \nconfidence increased until day 10 and decreased\
    \ at day 16 \nin the time-series RGB image data set (SI. 8, right image).\nDiscussion\n\
    Time‑lapse videos enable insights into early phases \nof HH development\nHH is\
    \ a serious limitation of plant tissue propagation affect-\ning multiple phases\
    \ of in vitro cultivation. The use of the \nnovel monitoring system “Phenomenon”\
    \ capturing time \nseries image data (SI. 4 and SI. 5) identified (i) the first\
    \ \nvisual symptoms of HH to occur 5 DAT and (ii) an acceler-\nated and higher\
    \ growth of shoots of the gelrite treatment \n(Fig. 1A). Thereby, significant\
    \ differences in the projected \nplant area between the two treatments were found\
    \ already \n5 days after transferring to the culture media in both species. \n\
    As discussed previously by Kevers et al. (1984), HH may \nbe considered as morphological\
    \ response to waterlogging, \nwhich in turn induces ethylene synthesis. For A.\
    \ thaliana, we \nobserved a higher vertical growth (Fig. 1B) with hyponasty \n\
    (SI. 5), which was described as ethylene-triggered strategy \nof ex vitro plants\
    \ in waterlogging conditions to re-estab-\nlish contact with air and restore successful\
    \ gas exchange \n(Voesenek and Blom 1989). Furthermore, Vreeburg et al. \n(2005)\
    \ described a flooding-induced petiole elongation in a \ntwo-stage process, starting\
    \ with acidification of the apoplast \nfollowed by cell wall expansion. This is\
    \ in agreement with \nour observation of a significantly higher eccentricity (devia-\n\
    tion of the ellipse to circle) and significantly less solidity \n(density of the\
    \ object) for explants in the gelrite treatment \n(SI. 1B). A more pronounced\
    \ curling of the leaves was \nobserved in Malus (SI. 4) which also resulted in\
    \ epinastic \nleaf growth. In addition, a significant higher mean canopy \nheight\
    \ (Fig. 1B) and maximum shoot height of Malus shoots \non gelrite medium (SI.\
    \ 1A) indicated a more pronounced \nvertical orientation of growth.\nHyperhydricity\
    \ induction by increased water \navailability\nAlthough HH symptoms vary between\
    \ different plant spe-\ncies and cultivars, and several factors have been described\
    \ \nto trigger HH, a putative common underlying mechanism \nof apoplast flooding\
    \ has been described (van den Dries \net al. 2013). Several studies showed that\
    \ increasing the \nwater availability by decreasing the concentration of \ngelling\
    \ agent, changing the type of gelling agent or the \ncultivation in liquid media\
    \ induced HH in a large set of \nplant species (Dianthus sp., Casanova et al.\
    \ 2008; Aloe \nsp., Ivanova and Van Staden 2011, Malus sp. Chakrabarty \net al.\
    \ 2003).\nIn our study, we demonstrated the HH-inducing effects \nof gelrite for\
    \ Malus and Arabidopsis indicated by the \noverall increase in HH scores (Fig. 2\
    \ and SI. 3) and apo-\nplastic liquid volume (Fig. 3 and SI. 3) over time. Gelrite\
    \ \ndiffers from agar in terms of consistency and purity and \nresulted in a superior\
    \ growth of explants at a comparable \ngel strength (Scherer 1987; Tsay et al.\
    \ 2006; Pasqualetto \net al. 1988). However, gelrite induced HH in several spe-\n\
    cies (Arabidopsis sp., van den Dries et al. (2013), Malus \nsp. Pasqualetto et al.\
    \ 1988, Prunus sp. Franck et al. 1998) \nlimiting the use of this gelling agent.\
    \ Scherer et al. (1988) \nTable 5  Performance metrics of object detection models\
    \ trained on RGB images. Bold letters indicate the value for the best performing\
    \ model in \neach column\na Note: Not yet implemented in Ultralytics YOLOv8.0.20\n\
    b Note: Trained with weights from PCTOC_V2 (based 250 images) and additionally\
    \ 125 images\nc Note: Precision and recall on test set were calculated with IoU\
    \ and confidence threshold of 0.5\nName\nModel archi-\ntecture\nTraining\nValidation\n\
    Test\nData set \n[No. of \nimages]\nDescription\nData set \n[No. of \nimages]\n\
    mAP \n[%]\nPrecision \n[%]\nRecall \n[%]\nData set \n[No. of \nimages]\nmAP \n\
    [%]\nPrecisionc \n[%]\nRecallc \n[%]\nPCTOC_\nV1\nYOLOv8\n250\nColab with weights\
    \ \nfrom scratch\n50\n88.4\n83.0\n82.1\n25\nNYIa\n94.4\n49.5\nPCTOC_\nV2\nRoboflow\
    \ 2.0 \nOD\n250\nWeights from scratch\n50\n93.5\n86.8\n86.4\n25\n95.0\n90.4\n\
    87.6\nPCTOC_\nV3\nRoboflow 2.0 \nOD\n375b\nWeights from \nPCTOC_V2\n50\n95.6\n\
    83.8\n95.7\n25\n97.0\n93.7\n89.0\n567\nPlant Cell, Tissue and Organ Culture (PCTOC)\
    \ (2023) 154:551–573 \n1 3\ncould show that there is no difference in the osmotic\
    \ and \nwater potential of gelrite compared to agar. Van den Dries \net al. (2013)\
    \ suspected therefore a local dissolution of the \nculture medium due to the excretion\
    \ of chelators by the \nexplants and thus a higher water availability and water\
    \ \nuptake. This higher water availability in gelrite-solidified \nmedia most\
    \ likely explains HH-induction and acceler-\nated growth, but other putative factors\
    \ like differences in \nFig. 10  Object detection performance of the PCTOC_V3\
    \ model on \nan image selection of the test set. (A) Ground truth RGB image of\
    \ \nthe Malus ‘G214’ test set annotated with normal (Normal, green) \nand hyperhydric\
    \ (HH, blue) explants (A1–A3). (B) Predicted objects \n(B1–B3) and class membership\
    \ probability (0 to 1 corresponds 0 to \n100%). Prediction was performed with\
    \ confidence threshold and \nintersection of union threshold of 0.5. (Color figure\
    \ online)\n568\n \nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n\
    1 3\nuptake of nutrients or plant hormones were also found: \nHigher contents\
    \ of magnesium (Mg) and a higher ratio of \npotassium (K) to sodium (Na) were\
    \ detected in the leaves \nof walnut explants grown on gelrite medium compared\
    \ \nto agar, which can affect stomatal function (Barbes et al. \n1993). Furthermore,\
    \ Arthur et al. (2004) found a lower \nconcentration of IAA-like compounds in\
    \ gelrite than in \ndifferent types of agar powder.\nWith the collected data and\
    \ the time-lapse videos, we \ncould narrow down crucial key points within the\
    \ develop-\nment of HH of the two species in time. First visual identifi-\nable\
    \ symptoms (SI. 4 and SI. 5) and significant increases in \napoplastic liquid\
    \ volume were observed already after 5 days \nof cultivation on gelrite media\
    \ in both species. Time series \ndynamics of apoplastic liquid volume confirmed\
    \ previous \ndata for A. thaliana (van den Dries et al. 2013)—in both \nstudies\
    \ hyperhydric explants of A. thaliana had an apoplas-\ntic liquid volume of around\
    \ 300 µL  g−1 FM 15 days after \ntreatment, but were carried out for the first\
    \ time for Malus. \nQuantification of apoplastic liquid volume of A. thaliana\
    \ \nseedlings at very early time points was limited by the very \nsmall amounts\
    \ of apoplastic liquid and the distorting effect \nof adhering water (Fig. 3B).\
    \ For Malus, the highest increase \nin apoplastic liquid volume for the gelrite\
    \ treatment was \ndetected within the first 4–5 days in two independent experi-\n\
    ments (SI. 3, Fig. 3B). Furthermore, a different behavior of \nthe HH score and\
    \ the apoplastic liquid volume was found \nafter 21 days of cultivation in Malus:\
    \ While the severity \nof HH symptoms steadily increased over time, the apoplas-\n\
    tic liquid volume seemed to reach saturation at later time \npoints (SI. 3¸ Fig. 4).\
    \ Therefore, we suggest the HH score \nto be useful to determine the symptoms\
    \ of HH, whereas the \nquantification of apoplastic liquid volume better reflects\
    \ the \nphysiological state of the explants.\nIdentification of HH‑specific spectral\
    \ absorption \nfeatures\nDespite the fact that clear visible symptoms (Table 1)\
    \ of HH \nwere reported and still are the major distinguishing param-\neter for\
    \ classification, spectroscopic analysis of HH is lim-\nited. Only Marques et al.\
    \ (2021) using Fourier-transform \ninfrared spectroscopy in attenuated total reflectance\
    \ mode \n(FTIR-ATR), evaluated chemical properties of prepared \ncell walls of\
    \ hyperhydric Arbutus unedo. Assuming HH as \na consequence of flooding of the\
    \ air-filled apoplast by water, \nUV–VIS–NIR–SWIR reflection spectroscopy was\
    \ expected \nto detect these physiological changes due to higher light \nabsorption\
    \ of water compared to air. Therefore, we applied \nthis technique to identify\
    \ specific absorption features of HH \nessential for designing an automated detection\
    \ system. How-\never, we excluded the UV region (< 400 nm) from further \nanalysis\
    \ due to the low penetration depth of UV light in plant \ntissue (Qi et al. 2010),\
    \ since most reflection signals can only \nbe attributed to anatomical and biochemical\
    \ properties of \ncuticle, trichomes and the upper epidermis.\nThe observed overall\
    \ reduction in reflectance of hyperhy-\ndric explants (Fig. 5) compared to normal\
    \ ones is consistent \nwith the visual appearance of the observed darkening of\
    \ the \naffected explants (SI. 4 and SI. 5). The visualization of iso-\nlated\
    \ absorption features over the time course of the develop-\nment of HH in Malus\
    \ (Fig. 6) should give insights whether \nthere is at least a trend in the time\
    \ course of the presumed \nabsorption characteristics. We used the continuum removal\
    \ \nmethod to exclude the observed overall absolute reduc-\ntion in reflection\
    \ and to compare all spectra on a common \nbase. This allowed an automated extraction\
    \ of absorption \npeaks for the SWIR region with predominant absorptions \nvalleys,\
    \ however, produced artefacts in the VIS region. In \nthe SWIR region, a consistent\
    \ difference between absorp-\ntion of normal and hyperhydric leaves was observed\
    \ for the \nwavelengths 980 nm, 1150 nm, 1400 nm, 1520 nm, 1780 nm \nand 1930 nm,\
    \ both over time (Fig. 6) and in the different \nexperiments (SI. 6). Most likely,\
    \ the absorption of water \nin the plant tissue is most responsible for the wavelengths\
    \ \n980 nm, 1150 nm, 1400 nm. Curran et al. (1989) described \nthe intense absorption\
    \ of liquid water at 970 nm, 1200 nm, \n1400 nm and 1450 nm due to the fundamental\
    \ O–H bend-\ning vibrations of the first overtone. Thus, the tendency of an \n\
    increase in water absorption (970 mn, 1200 nm, 1400 nm, \n1450 nm) within time\
    \ is in accordance with the increase \nin apoplastic liquid volume over time.\
    \ However, absorption \nbands of other compounds like proteins, lignins and sug-\n\
    ars are located within the peak between 1300 to 1600 nm \nand contribute to the\
    \ total absorption in this region. Curran \net al. (1989) associated the absorption\
    \ at 1780 nm to cel-\nlulose, sugars and starch. Since for this wavelength a higher\
    \ \nabsorption in hyperhydric leaves was observed in our study, \nthis is in line\
    \ with the detection of a higher sugar content \n(sucrose, glucose and fructose)\
    \ in hyperhydric explants of \nDianthus (Saher et al. 2005), but contradicting\
    \ Kevers et al. \n(1987) who reported less lignin and cellulose in hyperhydric\
    \ \nDianthus.\nSimulation of vegetation indices (Fig. 8) demonstrated \ntraceable\
    \ trends, that closely match the dynamics of the \nphysiological reference data\
    \ (Fig. 3 & SI. 3) and support \nthe observation of time-series data (SI. 4 &\
    \ SI. 5). Overall, \nthe vegetation indices from normal explants exhibited low\
    \ \nvariance, although they were derived from different experi-\nments. The high\
    \ variance of hyperhydric explants indicated \nby the confidence interval can\
    \ be explained by different \nphysiological states of explants with different\
    \ degrees of \nhyperhydricity. The simulation of a modified anthocyanin \nindex,\
    \ indicated a higher anthocyanin content in hyperhydric \nleaves of Malus, but\
    \ not Arabidopsis, supporting our RGB \nimage time series. The normalized difference\
    \ water index \n569\nPlant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\
    \ \n1 3\n(NDWI) displayed higher water contents for hyperhydric \nexplants of\
    \ both species and supported our observation that \napoplastic liquid volume did\
    \ not increase any more after \n4 weeks of cultivation. The normalized difference\
    \ lignin \nindex (NDLI) showed in both species less lignin for hype-\nrhydric\
    \ leaves. However, the trend of the NDLI curves in \nboth species followed inversely\
    \ that of the NDWI indicat-\ning a putative dependency on plant water content.\
    \ Marques \net al. (2021) found no significant difference in the lignin \ncontent\
    \ per dry weight of hyperhydric and normal leaves of \nArbutus, whereas Kevers\
    \ et al. (1987) reported a lower lignin \ncontent per fresh weight of hyperhydric\
    \ tissue. It remains to \nbe clarified, whether these divergent results are due\
    \ to differ-\nent species or to the fact that the fresh mass of hyperhydric \n\
    explants is much higher.\nAutomated detection of HH by machine learning\nIn order\
    \ to evaluate the performance of the spectral data \nin the classification of\
    \ hyperhydric and normal leaves, we \ntrained different ML models (Table 4), investigated\
    \ the \nmost important wavelengths of the best model (Fig. 7) and \ncompared them\
    \ against a novel vegetation index as the clas-\nsical approach (Fig. 9). The\
    \ ML models differed in their \narchitecture, complexity, performance, prediction\
    \ time and \ninterpretability (Singh et al. 2016; Liakos et al. 2018; and \nHesami\
    \ and Jones 2020). All ML models reached a high \nAUC ROC > 0.83 in training,\
    \ however only SVM and NNET \nhad a high accuracy > 0.80 on test data. Both ML\
    \ models \noutperformed with an accuracy of 0.81 for NNET (balanced \naccuracy\
    \ of 0.81) and 0.85 for SVM (balanced accuracy of \n0.84) the univariate vegetation\
    \ index approach with a lower \naccuracy of 0.63 (balanced accuracy of 0.66).\
    \ Furthermore, \nSVM was best in classifying normal spectra indicated by \nhighest\
    \ sensitivity of 0.91 on the test set. The two-band veg-\netation index NDRI reached\
    \ the highest specificity of 0.84 \nin the test data, followed by SVM with 0.76,\
    \ meaning high-\nest ratio in the identification of hyperhydric tissue, however,\
    \ \nlow sensitivity of 0.47, low accuracy of 0.63 and low  F1 \nscore of 0.59\
    \ indicated a conservative behavior of classifica-\ntion towards hyperhydric explants.\
    \ SVM was selected due \nto its high performance on training and testing datasets,\
    \ low \ntraining data volume requirements, performance on high-\ndimensional datasets,\
    \ low risk of overfitting, good gener-\nalization ability, and its advantages\
    \ over NNET in terms \nof training time, simplified structure, and interpretability\
    \ \n(Singh et al. 2016; Liakos et al. 2018). The evaluation of \nthe feature importance\
    \ of SVM for classification (Fig. 7) \nsupported our findings that bands (peaks\
    \ with maxima at \n1949 nm, 1445 nm, 1202 nm and 975 nm) associated with \nwater\
    \ absorption were crucial to distinguish between hype-\nrhydric and normal leaves.\
    \ However, the method indicated \nessential features importance in the VIS region\
    \ with maxima \nat 424 nm and 676 nm. In regard of an automated HH detec-\ntion\
    \ system, we further evaluated two different approaches \n(i) HH detection based\
    \ on an HSI-SWIR camera system \n(Fig. 9) and (ii) HH detection based on RGB camera\
    \ system \ncoupled with a deep neuronal network (DNN) to provide two \nputative\
    \ solutions for commercial plant propagation based \non our findings (Table 5,\
    \ Fig. 10, SI. 8).\nFollowing the HSI-SWIR camera system approach, we \ncould\
    \ only test the validity of our spectral classifier as a \nproof of concept because\
    \ we only had a single HSI acquisi-\ntion (SI. 7), so these results should be\
    \ interpreted with cau-\ntion. In addition, our analysis followed a two-class\
    \ classifica-\ntion problem, but under the assumption that an automated \nHH detection\
    \ system monitors explants on culture media \nduring cultivation, culture media\
    \ spectra could presumably \ninterfere with the other classes within classification.\
    \ There-\nfore, for further studies, we propose to include the acquisi-\ntion\
    \ of reflectance spectra of the culture media in the dataset. \nNevertheless,\
    \ we could test our ML-SVM classifier, trained \non spectra from Malus ‘G214’\
    \ and A. thaliana, on the single \nSWIR-HSI acquisition of Malus ‘Selection 4’\
    \ segmented \nplant pixels, indicating the generalization ability of the clas-\n\
    sifier with respect to experimental setup and plant species/\ngenotype. The NDRI\
    \ and ML-SVM both classified most \npixels correctly, however, ML-SVM segmented\
    \ the borders \nof different classes much sharper. These preliminary results \n\
    demonstrated that classification of HH is possible during \nin vitro cultivation\
    \ and through the lid of the vessel with \neither an expensive SWIR-HSI system\
    \ classifying with our \nnovel ML-SVM classifier or more cost-effectively with\
    \ a \ntwo-channel SWIR camera system using a novel vegetation \nindex.\nAlternatively,\
    \ an RGB camera setup coupled with con-\nvolutional neural network (CNN) can be\
    \ the most cost-\neffective solution for an automated HH-detection. Since \nwe\
    \ had identified feature importance also in the VIS \nregion, a proof-of-concept\
    \ study was conducted to dem-\nonstrate object detection via CNN. Therefore, we\
    \ used \nthe  Roboflow© pipeline, which allowed an easy access to \nthese tools\
    \ and provided an interface for data annotation, \npre-processing, data augmentation,\
    \ training, data avail-\nability and deployment of the trained models. Comparing\
    \ \na self-trained YOLOv8 with the unknown object detec-\ntion algorithms of Roboflow\
    \ Train (Table 5), we did not \nreach the performance of their optimized model,\
    \ which \nwas particularly evident in the performance on test set, \nwhere PCTOC_V1\
    \ reached the highest precision with \n94.4%, but with low recall of 49.5%—indicating\
    \ only half \nof all explants could be detected. The best trained model \nPCTOC_V3,\
    \ however had a precision of 83.8% on valida-\ntion and of 97.0% on test set,\
    \ indicating that prediction \nwas mostly correct (Table 5, Fig. 10, SI. 8). In\
    \ addition \nthe explants were reliably detected (recall of 95.7% on \n570\n \n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nvalidation\
    \ set and 89.0% on test set). By using the rela-\ntively new Python library roboflow,\
    \ we encountered some \nunsolved issues as seen in SI. 8 where non-maximum-\n\
    suppression only works so far within one class, resulting \nin multiple predictions\
    \ per object. Considering the prop-\nerties of the dataset, the low amount of\
    \ data (250 to 375 \nimages), resulting from time series images (1049 explants)\
    \ \nof only 32 individual explants, we could see already good \nperformance on\
    \ the test set and the time-series set (Fig. 10 \n& SI. 8).\nConclusions\nTo our\
    \ knowledge this study is the first report of (i) iden-\ntifying discriminating\
    \ wavelengths in the VIS–NIR-SWIR \nregion for the detection of HH, (ii) application\
    \ of short \nwave infrared hyperspectral imaging to detect growth \nanomalies\
    \ in vitro, (iii) proposing a spectral classifier \nfor hyperhydricity. Wavelength\
    \ bands (around 1940 nm, \n1450 nm, 1200 nm and 970 nm) associated with absorp-\n\
    tion of water are the most distinguishable between hype-\nrhydric and normal leaves\
    \ within the analyzed spectral \ndata set (400 nm to 2000 nm). In addition, minor\
    \ impor-\ntant wavelengths were found in the RGB region (around \n430 nm and 680 nm),\
    \ whereas the NIR region seemed to \nbe less important. Furthermore, RGB images\
    \ of hyperhy-\ndric explants contain sufficient morphological and spectral \n\
    features to allow a reliable detection of HH in an afforda-\nble manner via convolutional\
    \ neuronal networks. However, \nthis needs to be proven in an in-depth study.\
    \ Nonetheless, \nthese results can serve as a proof-of-concept for CNN-\nassisted\
    \ live monitoring of plant tissue cultures and pave \nthe way for increased use\
    \ of CNN to estimate other key \nparameters such as multiplication rate, nutrient\
    \ deficiency, \nand contamination.\nSupplementary Information The online version\
    \ contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/\
    \ s11240- 023- 02528-0.\nAcknowledgements We thank the technical assistants Ewa\
    \ Schneider \nand Bärbel Ernst of the department of Woody Plant and Propagation\
    \ \nPhysiology, Institute of Horticultural Production Systems, Leibniz Uni-\n\
    versität Hannover for their excellent support in the lab. Furthermore, \nwe thank\
    \ Matthias Igelbrink and Prof. Dr. Arno Ruckelshausen at Uni-\nversity of Applied\
    \ Science Osnabrück in their support in recording the \nSWIR-HSI data. In addition,\
    \ we are grateful for the scholarship for \nthe completion of a dissertation of\
    \ the University of Applied Science \nOsnabrück.\nAuthor contributions HB and\
    \ TW designed the experiments, HB and \nZM performed the experiments and analysed\
    \ the data. HB wrote the \nmanuscript and HB, ZM, TR, TW revised the manuscript.\
    \ All the \nauthors discussed the results and collectively edited the manuscript.\
    \ \nAll authors read and approved the final manuscript.\nFunding Open Access funding\
    \ enabled and organized by Projekt \nDEAL. This project took place within the\
    \ research project “Experi-\nmentierfeld Agro-Nordwest”, which is funded by the\
    \ Federal Ministry \nof Food and Agriculture (BMEL, Grant No.: 28DE103F18) via\
    \ the \nFederal Agency for Agriculture and Food (BLE).\nData availability The\
    \ datasets generated during the current study are \navailable from the corresponding\
    \ author on reasonable request. RGB \nimage dataset analysed during the current\
    \ study available in the Bethge \n(2023) repository, [https:// unive rse. robofl\
    \ ow. com/ hains/ hh- detec tion- \nin- vitro/ datas et/8].\nDeclarations \nCompeting\
    \ interests The authors declare no competing interests.\nEthical approval The\
    \ research work was carried out in compliance with \nthe ethical standards that\
    \ do not involve the use of humans.\nOpen Access This article is licensed under\
    \ a Creative Commons Attri-\nbution 4.0 International License, which permits use,\
    \ sharing, adapta-\ntion, distribution and reproduction in any medium or format,\
    \ as long \nas you give appropriate credit to the original author(s) and the source,\
    \ \nprovide a link to the Creative Commons licence, and indicate if changes \n\
    were made. The images or other third party material in this article are \nincluded\
    \ in the article’s Creative Commons licence, unless indicated \notherwise in a\
    \ credit line to the material. If material is not included in \nthe article’s\
    \ Creative Commons licence and your intended use is not \npermitted by statutory\
    \ regulation or exceeds the permitted use, you will \nneed to obtain permission\
    \ directly from the copyright holder. To view a \ncopy of this licence, visit\
    \ http://creativecommons.org/licenses/by/4.0/.\nReferences\nArthur GD, Stirk WA,\
    \ Van Staden J, Thomas TH (2004) Screening of \naqueous extracts from gelling\
    \ agents (Agar and Gelrite) for root-\nstimulating activity. S Afr J Bot 70(4):595–601.\
    \ https:// doi. org/ \n10. 1016/ S0254- 6299(15) 30197-6\nAynalem HM, Righetti\
    \ TL, Reed BM (2006) Non-destructive evalu-\nation of in vitro-stored plants:\
    \ a comparison of visual and image \nanalysis. In Vitro Cell Dev Biol-Plant 42(6):562–567.\
    \ https:// doi. \norg/ 10. 1079/ IVP20 06816\nBarbas E, Jay-Allemand C, Doumas\
    \ P, Chaillou S, Cornu D (1993) \nEffects of gelling agents on growth, mineral\
    \ composition and \nnaphthoquinone content of in vitro explants of hybrid walnut\
    \ tree \n(Juglans regia × Juglans nigra). Annales Des Sci for 50(2):177–\n186.\
    \ https:// doi. org/ 10. 1051/ forest: 19930 205\nBethge H (2023) HH Detection\
    \ in vitro Image Dataset. https:// unive \nrse. robofl ow. com/ hains/ hh- detec\
    \ tion- in- vitro/ datas et/8. Accessed \n10 Feb 2023\nBethge H, Winkelmann T,\
    \ Lüdeke P (2023) Rath T (2023) Low-cost \nand automated phenotyping system “Phenomenon”\
    \ for multi-\nsensor in situ monitoring in plant in vitro culture. Plant Methods\
    \ \n19(1):1–25. https:// doi. org/ 10. 1186/ s13007- 023- 01018-w\nBock Biosciences\
    \ GmbH (2018)  RoBo®Cut. https:// www. robot ec- \nptc. com/. Accessed 14 Feb\
    \ 2023\nBradski G (2000) The openCV library. Dr. Dobb’s J Softw Tools Prof \n\
    Progr 25(11):120–123\nCardoso JC, Sheng Gerald LT, Teixeira da Silva JA (2018)\
    \ Micro-\npropagation in the twenty-first century. In: Loyola-Vargas VM, \nOchoa-Alejo\
    \ N (eds) Plant cell culture protocols. Springer, \nDordrecht, pp 17–46\n571\n\
    Plant Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573 \n1 3\nCasanova\
    \ E, Moysset L, Trillas MI (2008) Effects of agar concentra-\ntion and vessel\
    \ closure on the organogenesis and hyperhydricity \nof adventitious carnation\
    \ shoots. Biol Plant 52:1–8. https:// doi. \norg/ 10. 1007/ s10535- 008- 0001-z\n\
    Chakrabarty D, Hahn EJ, Yoon YJ, Paek KY (2003) Micropropaga-\ntion of apple rootstock\
    \ M. 9 EMLA using bioreactor. J Hortic \nSci Biotechnol 78(5):605–609. https://\
    \ doi. org/ 10. 1080/ 14620 \n316. 2003. 11511 671\nChen C (2016) Cost analysis\
    \ of plant micropropagation of Phalae-\nnopsis. Plant Cell, Tis Organ Cult 126(1):167–175.\
    \ https:// doi. \norg/ 10. 1007/ s11240- 016- 0987-4\nCurran PJ (1989) Remote\
    \ sensing of foliar chemistry. Remote Sens \nEnviron 30(3):271–278. https:// doi.\
    \ org/ 10. 1016/ 0034- 4257(89) \n90069-2\nde Klerk GJ, Pramanik D (2017) Trichloroacetate,\
    \ an inhibitor of \nwax biosynthesis, prevents the development of hyperhydricity\
    \ in \nArabidopsis seedlings. Plant Cell, Tiss Organ Cult 131(1):89–\n95. https://\
    \ doi. org/ 10. 1007/ s11240- 017- 1264-x\nDebergh P, Aitken-Christie J, Cohen\
    \ D, Grout B, Von Arnold S, Zim-\nmerman R, Ziv M (1992) Reconsideration of the\
    \ term ‘vitrifica-\ntion’ as used in micropropagation. Plant Cell, Tissue Organ\
    \ Cult \n30(2):135–140. https:// doi. org/ 10. 1007/ BF000 34307\nDhondt S, Gonzalez\
    \ N, Blomme J, De Milde L, Van Daele T, Van \nAkoleyen D, Storme V, Coppens F,\
    \ Beemster TS, Inzé D (2014) \nHigh-resolution time-resolved imaging of in vitro\
    \ Arabidopsis \nrosette growth. Plant J 80(1):172–184. https:// doi. org/ 10.\
    \ 1111/ \ntpj. 12610\nDwyer B, Nelson J, Solawetz J (2021) Roboflow python package.\
    \ \nhttps:// github. com/ robofl ow/ robofl ow- python. Accessed 10 Feb \n2023\n\
    Dwyer B, Nelson J, Solawetz J (2022) Roboflow (v1.0). https:// robof \nlow. com.\
    \ Accessed 14 Feb 2023\nFischler MA, Bolles RC (1981) Random sample consensus:\
    \ a para-\ndigm for model fitting with applications to image analysis and \nautomated\
    \ cartography. Commun ACM 24(6):381–395. https:// \ndoi. org/ 10. 1145/ 358669.\
    \ 358692\nFranck T, Crèvecoeur M, Wuest J, Greppin H, Gaspar T (1998) \nCytological\
    \ comparison of leaves and stems of Prunus avium \nL. shoots cultured on a solid\
    \ medium with agar or gelrite. Bio-\ntechnic Histochem 73(1):32–43. https:// doi.\
    \ org/ 10. 3109/ 10520 \n29980 91405 04\nGamborg OL, Miller R, Ojima K (1968)\
    \ Nutrient requirements of sus-\npension cultures of soybean root cells. Exp Cell\
    \ Res 50(1):151–\n158. https:// doi. org/ 10. 1016/ 0014- 4827(68) 90403-5\nGao\
    \ BC (1996) NDWI—A normalized difference water index for \nremote sensing of vegetation\
    \ liquid water from space. Remote \nSens Environ 58(3):257–266. https:// doi.\
    \ org/ 10. 1016/ S0034- \n4257(96) 00067-3\nGao H, Xia X, An L, Xin X, Liang Y\
    \ (2017) Reversion of hyperhydric-\nity in pink (Dianthus chinensis L.) plantlets\
    \ by AgNO3 and its \nassociated mechanism during in vitro culture. Plant Sci 254:1–1.\
    \ \nhttps:// doi. org/ 10. 1016/j. plant sci. 2016. 10. 008\nGehan MA, Fahlgren\
    \ N, Abbasi A, Berry JC, Callen ST, Chavez L, \nDoust AN, Feldman MJ, Gilbert\
    \ KB, Hodge JG, Hoyer JS (2017) \nPlantCV v2: image analysis software for high-throughput\
    \ plant \nphenotyping. PeerJ 5:e4088. https:// doi. org/ 10. 7717/ peerj. 4088\n\
    George EF, Hall MA, De Klerk GJ (2008) Plant propagation by tissue \nculture.\
    \ In: George EF, Hall MA, De Klerk G-J (eds) Volume I. \nThe background. Plant\
    \ propagation by tissue culture. Springer, \nDordrecht\nGitelson AA, Keydan GP,\
    \ Merzlyak MN (2006) Three-band model \nfor noninvasive estimation of chlorophyll,\
    \ carotenoids, and \nanthocyanin contents in higher plant leaves. Geophys Res\
    \ Lett. \nhttps:// doi. org/ 10. 1029/ 2006G L0264 57\nGribble K (1999) The influence\
    \ of relative humidity on vitrifica-\ntion, growth and morphology of Gypsophila\
    \ paniculata L. Plant \nGrowth Regul 27(3):181–190. https:// doi. org/ 10. 1023/A:\
    \ 10062 \n35229 848\nGupta SD, Karmakar A (2017) Machine vision based evaluation\
    \ of \nimpact of light emitting diodes (LEDs) on shoot regeneration \nand the\
    \ effect of spectral quality on phenolic content and anti-\noxidant capacity in\
    \ Swertia chirata. J Photochem Photobiol, B \n174:162–172. https:// doi. org/\
    \ 10. 1016/j. jphot obiol. 2017. 07. 029\nHesami M, Jones AM (2020) Application\
    \ of artificial intelligence \nmodels and optimization algorithms in plant cell\
    \ and tissue cul-\nture. Appl Microbiol Biotechnol 104(22):9449–9485. https://\
    \ \ndoi. org/ 10. 1007/ s00253- 020- 10888-2\nHonda H, Takikawa N, Noguchi H,\
    \ Hanai T, Kobayashi T (1997) \nImage analysis associated with a fuzzy neural\
    \ network and \nestimation of shoot length of regenerated rice callus. J Fer-\n\
    ment Bioeng 84(4):342–347. https:// doi. org/ 10. 1016/ S0922- \n338X(97) 89256-2\n\
    Huang YJ, Lee FF (2010) An automatic machine vision-guided \ngrasping system for\
    \ Phalaenopsis tissue culture plantlets. \nComput Electron Agric 70(1):42–51.\
    \ https:// doi. org/ 10. 1016/j. \ncompag. 2009. 08. 011\nIvanova M, Van Staden\
    \ J (2011) Influence of gelling agent and cyto-\nkinins on the control of hyperhydricity\
    \ in Aloe polyphylla. Plant \nCell, Tissue Organ Cult 104(1):13–21. https:// doi.\
    \ org/ 10. 1007/ \ns11240- 010- 9794-5\nJocher G, Chaurasia, A, Qiu J (2023) YOLO\
    \ by Ultralytics (Ver-\nsion 8.0.0). https:// github. com/ ultra lytics/ ultra\
    \ lytics. Accessed \n14 Feb 2023\nKemat N (2020) Improving the quality of tissue-cultured\
    \ plants by \nfixing the problems related to an inadequate water balance, \nhyperhydricity.\
    \ Doctoral dissertation, Wageningen University \nand Research. https:// doi. org/\
    \ 10. 18174/ 517434\nKemat N, Visser RG, Krens FA (2021) Hypolignification: a\
    \ decisive \nfactor in the development of hyperhydricity. Plants 10(12):2625.\
    \ \nhttps:// doi. org/ 10. 3390/ plant s1012 2625\nKevers C, Coumans M, Coumans-Gillès\
    \ MF, Caspar TH (1984) \nPhysiological and biochemical events leading to vitrification\
    \ \nof plants cultured in vitro. Physiol Plant 61(1):69–74. https:// \ndoi. org/\
    \ 10. 1111/j. 1399- 3054. 1984. tb061 02.x\nKevers C, Prat R, Gaspar T (1987)\
    \ Vitrification of carnation in vitro: \nchanges in cell wall mechanical properties,\
    \ cellulose and lignin \ncontent. Plant Growth Regul 5(1):59–66. https:// doi.\
    \ org/ 10. \n1007/ BF000 35020\nKuhn M (2008) Building predictive models in R\
    \ using the caret pack-\nage. J Stat Softw 28:1–26. https:// doi. org/ 10. 18637/\
    \ jss. v028. i05\nLee TJ, Zobayed SM, Firmani F, Park EJ (2019) A novel auto-\n\
    mated transplanting system for plant tissue culture. Biosys Eng \n181:63–72. https://\
    \ doi. org/ 10. 1016/j. biosy stems eng. 2019. 02. \n012\nLehnert LW, Meyer H,\
    \ Obermeier WA, Silva B, Regeling B, Bendix J \n(2018) Hyperspectral data analysis\
    \ in R: the hsdar package. arXiv \nPreprint. https:// doi. org/ 10. 48550/ arXiv.\
    \ 1805. 05090\nLiakos KG, Busato P, Moshou D, Pearson S, Bochtis D (2018) Machine\
    \ \nlearning in agriculture: a review. Sensors 18(8):2674. https:// doi. \norg/\
    \ 10. 3390/ s1808 2674\nLizárraga A, Fraga M, Ascasíbar J, González ML (2017)\
    \ In vitro propa-\ngation and recovery of eight apple and two pear cultivars held\
    \ in \na germplasm bank. Am J Plant Sci 8(9):2238–2254. https:// doi. \norg/ 10.\
    \ 4236/ ajps. 2017. 89150\nMahendra PVS, Gupta SD (2004) Trichromatic sorting\
    \ of in vitro \nregenerated plants of gladiolus using adaptive resonance theory.\
    \ \nCurr Sci 10:348–353\nMarques MP, Martins J, de Carvalho LA, Zuzarte MR, da\
    \ Costa RM, \nCanhoto J (2021) Study of physiological and biochemical events \n\
    leading to vitrification of Arbutus unedo L. cultured in vitro. Trees \n35:241–253.\
    \ https:// doi. org/ 10. 1007/ s00468- 020- 02036-0\n572\n \nPlant Cell, Tissue\
    \ and Organ Culture (PCTOC) (2023) 154:551–573\n1 3\nMestre D, Fonseca JM, Mora\
    \ A (2017) Monitoring of in-vitro plant \ncultures using digital image processing\
    \ and random forests. 8th \nInternational Conference on Pattern Recognition Systems.\
    \ https:// \ndoi. org/ 10. 1049/ cp. 2017. 0137\nMohamed SM, El-Mahrouk ME, El-Banna\
    \ AN, Hafez YM, El-Ramady \nH, Abdalla N, Dobránszki J (2023) Optimizing medium\
    \ composi-\ntion and environmental culture condition enhances antioxidant \nenzymes,\
    \ recovers Gypsophila paniculata L. hyperhydric shoots \nand improves rooting\
    \ in vitro. Plants 12(2):306. https:// doi. org/ \n10. 3390/ plant s1202 0306\n\
    Murashige T, Skoog F (1962) A revised medium for rapid growth and \nbio assays\
    \ with tobacco tissue cultures. Physiol Plant 15(3):473–\n497. https:// doi. org/\
    \ 10. 1111/j. 1399- 3054. 1962. tb080 52.x\nNezami-Alanagh E, Garoosi GA, Landín\
    \ M, Gallego PP (2019) \nComputer-based tools provide new insight into the key\
    \ fac-\ntors that cause physiological disorders of pistachio rootstocks \ncultured\
    \ in vitro. Sci Rep 9(1):1–5. https:// doi. org/ 10. 1038/ \ns41598- 019- 46155-2\n\
    Paques M, Boxus P, Dulos M (1985) “ Vitrification”: an induceable \nand reversible\
    \ phenomenon. In: symposium on in vitro problems \nrelated to mass propagation\
    \ of horticultural plants 212. pp 253–\n258. https:// doi. org/ 10. 17660/ ActaH\
    \ ortic. 1987. 212. 38\nPasqualetto PL, Zimmerman RH, Fordham I (1988) The influence\
    \ of \ncation and gelling agent concentrations on vitrification of apple \ncultivars\
    \ in vitro. Plant Cell, Tissue Organ Cult 14(1):31–40. \nhttps:// doi. org/ 10.\
    \ 1007/ BF000 29573\nPatrício DI, Rieder R (2018) Computer vision and artificial\
    \ intelligence \nin precision agriculture for grain crops: a systematic review.\
    \ Com-\nput Electron Agric 153:69–81. https:// doi. org/ 10. 1016/j. compag. \n\
    2018. 08. 001\nPeterson RA, Peterson MR (2020) Package ‘bestNormalize’. Normal-\n\
    izing transformation functions. R package version\nPhan CT, Letouze R (1983) A\
    \ comparative study of chlorophyll, \nphenolic and protein contents, and of hydroxycinnamate:\
    \ CoA \nligase activity of normal and ‘vitreous’ plants (Prunus avium L.) \nobtained\
    \ in vitro. Plant Sci Lett 31(2–3):323–327. https:// doi. org/ \n10. 1016/ 0304-\
    \ 4211(83) 90071-8\nPinheiro J, Bates D, DebRoy S, Sarkar D, Heisterkamp S, Van\
    \ Willigen \nB, Maintainer R (2017) Package ‘nlme.’ Linear Nonlinear Mixed \n\
    Eff Models Vers 3(1):274\nPrasad VS, Gupta SD (2008) Applications and potentials\
    \ of artificial \nneural networks in plant tissue culture. In: Gupta SD, Ibaraki\
    \ Y \n(eds) Plant tissue culture engineering. Springer, Dordrecht, pp \n47–67.\
    \ https:// doi. org/ 10. 1007/ 978-1- 4020- 3694-1_3\nQi Y, Heisler GM, Gao W,\
    \ Vogelmann TC, Bai S (2010) Character-\nistics of UV-B radiation tolerance in\
    \ broadleaf trees in southern \nUSA. In: Gao W, Slusser JR, Schmoldt DL (eds)\
    \ UV radiation in \nglobal climate change. Springer, Berlin, Heidelberg\nRedmon\
    \ J, Divvala S, Girshick R, Farhadi A (2016). You only look \nonce: Unified, real-time\
    \ object detection. In: Proceedings of the \nIEEE conference on computer vision\
    \ and pattern recognition, pp. \n779–788. https:// doi. org/ 10. 1109/ CVPR. 2016.\
    \ 91\nRipley B, Venables W, Ripley MB (2016) Package ‘nnet’. R Package \nversion.\
    \ 2;7(3–12):700\nRojas-Martínez L, Visser RG, De Klerk GJ (2010) The hyperhydricity\
    \ \nsyndrome: waterlogging of plant tissues as a major cause. Propag \nOrnam Plants\
    \ 10(4):169–175\nRstudio Team (2015) RStudio: integrated development for R. RStudio.\
    \ \nInc., Boston, p 879\nSaher S, Fernández-García N, Piqueras A, Hellín E, Olmos\
    \ E (2005) \nReducing properties, energy efficiency and carbohydrate metab-\n\
    olism in hyperhydric and normal carnation shoots cultured \nin vitro: a hypoxia\
    \ stress? Plant Physiol Biochem 43(6):573–\n582. https:// doi. org/ 10. 1016/j.\
    \ plaphy. 2005. 05. 006\nSakamoto Y, Ishiguro M, Kitagawa G (1986) Akaike information\
    \ \ncriterion statistics. D. Reidel, Dordrecht. https:// doi. org/ 10. \n2307/\
    \ 29830 28\nScherer PA (1987) Standardization of plant micropropagation by \n\
    usage of a liquid medium with polyurethane foam plugs or a \nsolidified medium\
    \ with the gellan gum gelrite instead of agar. \nIn: International Symposium on\
    \ Propagation of Ornamental \nPlants 226. pp. 107–114. https:// doi. org/ 10.\
    \ 17660/ ActaH ortic. \n1988. 226. 10\nScherer PA, Müller E, Lippert H, Wolff\
    \ G (1988) Multielement analysis \nof agar and gelrite impurities investigated\
    \ by inductively coupled \nplasma emission spectrometry as well as physical properties\
    \ of \ntissue culture media prepared with agar or the gellan gum gelrite. \nIn:\
    \ International Symposium on Propagation of Ornamental Plants \n226, pp 655–658.\
    \ https:// doi. org/ 10. 17660/ ActaH ortic. 1988. 226. \n91\nSerrano L, Penuelas\
    \ J, Ustin SL (2002) Remote sensing of nitrogen and \nlignin in Mediterranean\
    \ vegetation from AVIRIS data: decom-\nposing biochemical from structural signals.\
    \ Remote Sens Envi-\nron 81(2–3):355–364. https:// doi. org/ 10. 1016/ S0034-\
    \ 4257(02) \n00011-1\nShaw DR, Kelley FS (2005) Evaluating remote sensing for\
    \ determining \nand classifying soybean anomalies. Precision Agric 6(5):421–429.\
    \ \nhttps:// doi. org/ 10. 1007/ s11119- 005- 3681-9\nSingh A, Thakur N, Sharma\
    \ A (2016) A review of supervised machine \nlearning algorithms. In2016 3rd international\
    \ conference on \ncomputing for sustainable global development (INDIACom). pp.\
    \ \n1310–1315. https:// doi. org/ 10. 35940/ ijsce. E3583. 11125 22\nSmith MA,\
    \ Spomer L (1995) Vessels, gels, liquid media, and sup-\nport systems. In: Aitken-Christie\
    \ J, Kozai T, Smith MAL (eds) \nAutomation and environmental control in plant\
    \ tissue culture. \nSpringer, Dordrecht, pp 371–404. https:// doi. org/ 10. 1007/\
    \ 978- \n94- 015- 8461-6_ 16\nSmith MA, Spomer L, Meyer MJ, McClelland MT (1989)\
    \ Non-invasive \nimage analysis evaluation of growth during plant micropropaga-\n\
    tion. Plant Cell, Tissue Organ Cult 19(2):91–102. https:// doi. org/ \n10. 1007/\
    \ BF000 35809\nSommer C, Straehle C, Koethe U, Hamprecht FA (2011) Ilastik: Inter-\n\
    active learning and segmentation toolkit. In: 2011 IEEE interna-\ntional symposium\
    \ on biomedical imaging: From nano to macro. \npp. 230–233. https:// doi. org/\
    \ 10. 1109/ ISBI. 2011. 58723 94\nSullivan C, Kaszynski A (2019) PyVista: 3D plotting\
    \ and mesh analy-\nsis through a streamlined interface for the Visualization Toolkit\
    \ \n(VTK). J Open Sour Softw 4(37):1450. https:// doi. org/ 10. 21105/ \njoss.\
    \ 01450\nTerry ME, Bonner BA (1980) An examination of centrifugation as \na method\
    \ of extracting an extracellular solution from peas, and \nits use for the study\
    \ of indoleacetic acid-induced growth. Plant \nPhysiol 66(2):321–325. https://\
    \ doi. org/ 10. 1104/ pp. 66.2. 321\nThiel M (2018) Bildgebende NIR-Hyperspektral-Technologie\
    \ zur in-\nsitu Erfassung des Blattwassergehalts. Doctoral dissertation, Leib-\n\
    niz Universität Hannover. https:// doi. org/ 10. 15488/ 3882\nTian J, Jiang F,\
    \ Wu Z (2015) The apoplastic oxidative burst as a \nkey factor of hyperhydricity\
    \ in garlic plantlet in vitro. Plant \nCell Tiss Organ Cult 120(2):571–584. https://\
    \ doi. org/ 10. 1007/ \ns11240- 014- 0623-0\nTisserand S (2021) Vis-NIR hyperspectral\
    \ cameras. Photoniques \n110:58–64\nTsay HS, Lee CY, Agrawal DC, Basker S (2006)\
    \ Influence of ventila-\ntion closure, gelling agent and explant type on shoot\
    \ budprolifera-\ntion and hyperhydricity in Scrophularia yoshimurae—a medicinal\
    \ \nplant. In Vitro Cell Dev Biol-Plant 42(5):445–449.https:// doi. org/ \n10.\
    \ 1079/ IVP20 06791\nvan Altvorst AC, Koehorst H, de Jong J, Dons HJ (1996) Transgenic\
    \ \ncarnation plants obtained by Agrobacterium tumefaciens-mediated \n573\nPlant\
    \ Cell, Tissue and Organ Culture (PCTOC) (2023) 154:551–573 \n1 3\ntransformation\
    \ of petal explants. Plant Cell, Tissue Organ Cult \n45(2):169–173. https:// doi.\
    \ org/ 10. 1007/ BF000 48762\nvan den Dries N, Giannì S, Czerednik A, Krens FA,\
    \ de Klerk GJ (2013) \nFlooding of the apoplast is a key factor in the development\
    \ of \nhyperhydricity. J Exp Bot 64(16):5221–5230. https:// doi. org/ 10. \n1093/\
    \ jxb/ eru497\nVan Der Walt S, Colbert SC, Varoquaux G (2011) The NumPy array:\
    \ \na structure for efficient numerical computation. Comput Sci Eng \n13(2):22–30.\
    \ https:// doi. org/ 10. 48550/ arXiv. 1102. 1523\nVan Rossum G, Drake FL (2009)\
    \ Python 3 Reference manual: python \ndocumentation manual part 2. Scotts Valley,\
    \ CA: CreateSpace\nVieitez AM, Ballester A, San-José MC, Vieitez E (1985) Anatomical\
    \ \nand chemical studies of vitrified shoots of chestnut regenerated \nin vitro.\
    \ Physiol Plant 65(2):177–184. https:// doi. org/ 10. 17660/ \nActaH ortic. 1987.\
    \ 212. 34\nVoesenek LA, Blom CW (1989) Growth responses of Rumex spe-\ncies in\
    \ relation to submergence and ethylene. Plant Cell Environ \n12(4):433–439. https://\
    \ doi. org/ 10. 1111/j. 1365- 3040. 1989. tb019 \n59.x\nVreeburg RA, Benschop\
    \ JJ, Peeters AJ, Colmer TD, Ammerlaan AH, \nStaal M, Elzenga TM, Staals RH, Darley\
    \ CP, McQueen-Mason SJ, \nVoesenek LA (2005) Ethylene regulates fast apoplastic\
    \ acidifica-\ntion and expansin A transcription during submergence-induced \n\
    petiole elongation in Rumex palustris. Plant J 43(4):597–610. \nhttps:// doi.\
    \ org/ 10. 1111/j. 1365- 313X. 2005. 02477.x\nZhang C, Timmis R, Hu WS (1999)\
    \ A neural network based pattern \nrecognition system for somatic embryos of Douglas\
    \ fir. Plant Cell, \nTissue Organ Cult 56:25–35. https:// doi. org/ 10. 1023/A:\
    \ 10062 \n87917 534\nZhou QY, Park J, Koltun V (2018) Open3D: a modern library\
    \ for 3D \ndata processing. arXiv Preprint. https:// doi. org/ 10. 48550/ arXiv.\
    \ \n1801. 09847\nZiv M (1991) Vitrification: morphological and physiological disorders\
    \ \nof in vitro plants. In: Debergh PC, Zimmerman RH (eds) Micro-\npropagation.\
    \ Springer, Dordrecht, pp 45–69. https:// doi. org/ 10. \n1007/ 978- 94- 009-\
    \ 2075-0_4\nPublisher's Note Springer Nature remains neutral with regard to \n\
    jurisdictional claims in published maps and institutional affiliations.\n"
  inline_citation: (Bethge et al. 2023)
  journal: Plant cell, tissue and organ culture (Print)
  limitations: The study is limited in terms of scope, depth, and recency, and may
    not cover the latest advancements in the field.
  main_objective: The main objective of this study was to investigate the spectral
    properties of hyperhydric tissue in two different plant species (Malus sp. and
    Arabidopsis thaliana) after forced induction of the growth anomaly and subsequent
    spectral analysis of the explants.
  pdf_link: https://link.springer.com/content/pdf/10.1007/s11240-023-02528-0.pdf
  publication_year: 2023
  relevance_evaluation: High - The study is directly relevant to the objective of
    the systematic review on automated systems for real-time irrigation management
    in plant tissue culture.
  relevance_score: 0.8
  relevance_score1: 0
  relevance_score2: 0
  study_location: Osnabrück University of Applied Sciences, Oldenburger Landstraße
    24, 49090 Osnabrück, Germany
  technologies_used: RGB, multispectral, hyperspectral imaging, machine learning,
    object detection
  title: Towards automated detection of hyperhydricity in plant in vitro culture
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Rogers M.
  - Blanc-Talon J.
  - Urschler M.
  - Delmas P.
  citation_count: '1'
  description: Over the past two decades, hyperspectral imaging has become popular
    for non-destructive assessment of food quality, safety, and crop monitoring. Imaging
    delivers spatial information to complement the spectral information provided by
    spectroscopy. The key challenge with hyperspectral image data is the high dimensionality.
    Each image captures hundreds of wavelength bands. Reducing the number of wavelengths
    to an optimal subset is essential for speed and robustness due to the high multicollinearity
    between bands. However, there is yet to be a consensus on the best methods to
    find optimal subsets of wavelengths to predict attributes of samples. A systematic
    review procedure was developed and applied to review published research on hyperspectral
    imaging and wavelength selection. The review population included studies from
    all disciplines retrieved from the Scopus database that provided empirical results
    from hyperspectral images and applied wavelength selection. We found that 799
    studies satisfied the defined inclusion criteria and investigated trends in their
    study design, wavelength selection, and machine learning techniques. For further
    analysis, we considered a subset of 71 studies published in English that incorporated
    spatial/texture features to understand how previous works combined spatial features
    with wavelength selection. This review ranks the wavelength selection techniques
    from each study to generate a table of the comparative performance of each selection
    method. Based on these findings, we suggest that future studies include spatial
    feature extraction methods to improve the predictive performance and compare them
    to a broader range of wavelength selection techniques, especially when proposing
    novel methods.
  doi: 10.1007/s11694-023-02044-x
  explanation: The review you submitted to Mitchell Rogers and Jacques Blanc-Talon
    accomplished all key aspects I requested. There was an explanation of the purpose
    and intention of this systematic review on automated systems for real-time irrigation
    management can contribute to the field of real-time irrigation management. Some
    of the gadgets, methods, or approaches used in the study were also listed under
    technologies used. A few limitations of the study were mentioned.
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Food Measurement and Characterization
    Article Wavelength and texture feature selection for hyperspectral imaging: a
    systematic literature review Review Paper Open access Published: 17 August 2023
    Volume 17, pages 6039–6064, (2023) Cite this article Download PDF You have full
    access to this open access article Journal of Food Measurement and Characterization
    Aims and scope Submit manuscript Mitchell Rogers , Jacques Blanc-Talon, Martin
    Urschler & Patrice Delmas  1876 Accesses 1 Citation Explore all metrics A Correction
    to this article was published on 27 September 2023 This article has been updated
    Abstract Over the past two decades, hyperspectral imaging has become popular for
    non-destructive assessment of food quality, safety, and crop monitoring. Imaging
    delivers spatial information to complement the spectral information provided by
    spectroscopy. The key challenge with hyperspectral image data is the high dimensionality.
    Each image captures hundreds of wavelength bands. Reducing the number of wavelengths
    to an optimal subset is essential for speed and robustness due to the high multicollinearity
    between bands. However, there is yet to be a consensus on the best methods to
    find optimal subsets of wavelengths to predict attributes of samples. A systematic
    review procedure was developed and applied to review published research on hyperspectral
    imaging and wavelength selection. The review population included studies from
    all disciplines retrieved from the Scopus database that provided empirical results
    from hyperspectral images and applied wavelength selection. We found that 799
    studies satisfied the defined inclusion criteria and investigated trends in their
    study design, wavelength selection, and machine learning techniques. For further
    analysis, we considered a subset of 71 studies published in English that incorporated
    spatial/texture features to understand how previous works combined spatial features
    with wavelength selection. This review ranks the wavelength selection techniques
    from each study to generate a table of the comparative performance of each selection
    method. Based on these findings, we suggest that future studies include spatial
    feature extraction methods to improve the predictive performance and compare them
    to a broader range of wavelength selection techniques, especially when proposing
    novel methods. Similar content being viewed by others A commentary review on the
    use of normalized difference vegetation index (NDVI) in the era of popular remote
    sensing Article Open access 31 May 2020 Feature selection techniques for machine
    learning: a survey of more than two decades of research Article 01 December 2023
    Remote sensing techniques: mapping and monitoring of mangrove ecosystem—a review
    Article Open access 17 July 2021 Hyperspectral imaging is emerging as one of the
    most popular non-destructive approaches for assessing food quality, and safety
    [1, 2]. Hyperspectral imaging performs digital imaging from spatial information
    augmented with spectral information based on spectroscopy. Exceeding the predictive
    performance of digital RGB imaging and human visual examination [3], spectral
    and spatial information captured in hyperspectral images allows for highly accurate
    sensory and chemical attribute prediction. This technology has enabled applications
    such as predicting the soluble solid content of kiwifruit [4] and classifying
    brands of Cheddar cheeses [5]. Hyperspectral images capture hundreds of contiguous
    narrow wavelengths, typically only a few nanometers wide, compared with the 5-50
    broad bands captured by multispectral sensors. The captured images consider the
    spatial context of the samples, which is not possible with single point sampling
    of spectroscopy [6]. Figure 1 illustrates the differences between these modalities.
    Using multivariate analysis, important information can be extracted from the observed
    reflectance images to predict attributes of the target sample [7]. Multivariate
    analysis establishes statistical or mathematical relationships between samples
    and their chemical attributes [8]. Predicting continuous measurements is a regression
    problem (e.g., total soluble solids of fruits [9]), whereas assigning a discrete
    class to each observation is a classification problem (e.g., identifying the geographical
    origin [2]). Most hyperspectral imaging studies in food science typically regress
    a target variable or classify a sample [10] based on the rich spatial and spectral
    information delivered by hyperspectral sensors. Fig. 1 Comparison of RGB channel
    digital imaging (left) to hyperspectral imaging (middle) and spectroscopy (right).
    Hyperspectral images capture hundreds of wavelength channels across the electromagnetic
    spectrum Full size image High dimensionality is the main challenge when analyzing
    hyperspectral data. Many applications of hyperspectral sensors require low-cost
    real-time (online) decision-making, such as sensors for unmanned aerial vehicles
    (UAVs) [11], assessment of food safety on processing lines [12], and guided surgery
    [13]. The high cost and low acquisition speed of hyperspectral sensors render
    these applications infeasible. High dimensionality also leads to models that overfit
    redundant features or noise [14], a phenomenon known as the Hughes phenomenon
    [15]. Dimensionality reduction is a way to address all three of these issues.
    Two methods exist for dimensionality reduction: projection-based methods and wavelength
    (feature) selection. Projection-based methods require capturing all the wavelengths,
    whereas wavelength selection reduces the required wavelengths. Based on the selected
    wavelengths, multispectral sensors can be designed that are cheaper, faster, and
    more robust than the original hyperspectral sensors [3]. Wavelength selection
    is essential for rapid and robust models [7, 16]. However, there is no consensus
    regarding which wavelength selection techniques provide the best predictive performance,
    as the results vary among studies [1, 6, 7, 17,18,19,20]. Previous reviews on
    hyperspectral imaging have comprehensively assessed the applications of hyperspectral
    imaging and spectroscopy in individual disciplines. For food science applications,
    reviews have surveyed applications in muscle foods [3, 12, 20, 21], seafood [22,
    23], fruits [6, 9, 24], or plant foods and vegetation [19, 25]. Other reviews
    have also investigated methods for making sense of hyperspectral image data, such
    as data mining [10], machine learning techniques [26], and deep learning [8].
    Previous reviews have only discussed a small set of feature selection methods
    and have not covered all commonly used methods [2, 10, 12, 19, 20, 24, 25, 27,28,29].
    Our review comprehensively surveys wavelength selection techniques across all
    applications of hyperspectral imaging that apply wavelength selection to provide
    a detailed answer to the best and most common wavelength selection methods. Hyperspectral
    imaging is unique compared with other spectroscopic techniques because the images
    produced include the spatial context of each pixel. Spatial features are an element
    of hyperspectral image analysis that is often disregarded in studies. Data fusion
    refers to how different feature modalities are combined. Selecting the best data
    fusion method is vital for maximizing the benefits of including spatial information.
    Although models based on spectral information are often sufficient for high classification
    or prediction accuracy, including spatial data has been shown to further increase
    the predictive performance [26, 30]. Previously published reviews discussed the
    importance of considering both spectral and spatial features to improve the reliability
    and prediction accuracy of models  [22]. However, a review has yet to explore
    the best practices for considering both the spectral and spatial features. Our
    study comprehensively reviews spectral and spatial feature selection techniques
    in hyperspectral imaging across disciplines to inform the use of spatial features
    in future investigations. This review provides a complementary guide to discipline-specific
    reviews by making the following contributions: A comprehensive review of the best
    and most common wavelength selection techniques for hyperspectral imaging applications.
    A comprehensive review of previous studies incorporating spatial features from
    optimal wavelengths. An overview of the typical sample sizes and targeted wavelength
    ranges of hyperspectral imaging studies using wavelength selection. Methods Systematic
    reviews are essential to quantify the usefulness of techniques and practices across
    all studies in a research area. This systematic review followed the Preferred
    Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework developed
    by David Moher [31] to survey hyperspectral imaging research that applied wavelength
    selection techniques to reduce dimensionality and the subset of studies that investigated
    spatial features. PRISMA reviews consist of four steps: identification, screening,
    eligibility, and inclusion criteria. This section describes the methodology followed
    in each step. Research questions and conceptual framework This study examined
    research related to wavelength selection for hyperspectral images by following
    the PRISMA framework on publications retrieved from the Scopus database. The first
    stage of research questions focuses on three areas: application area, study design,
    and methods (i.e., feature selection and learning). We also included an extended
    second stage of questions for the subset of studies that extracted spatial features.
    This study answers the following research questions: Standard hyperspectral study
    design (Section 2.2) RQ 1: How many samples are acquired in hyperspectral imaging
    studies? RQ 2: What wavelength ranges are most common for hyperspectral image
    analysis? Feature selection and learning (Section 2.3) RQ 3: What are the most
    common wavelength selection algorithms? RQ 4: Which wavelength selection algorithms
    provide the best predictive performance? RQ 5: Which learning algorithms are most
    common after wavelength selection? Spatial features (Section 2.4) RQ 6: What are
    the most common spatial features, and what parameters are used to extract these
    features? RQ 7: How are spectral and spatial features combined for learning models?
    RQ 8: How do studies select representative images to extract spatial features?
    RQ 9: Does combining spectral and spatial features improve the predictive performance,
    and which type of feature performs best individually? Selection criteria and search
    strategy The Scopus database was the primary source of publications. Scopus provides
    a wide selection of studies with more coverage than other academic databases such
    as Web of Science [32]. Scopus integrates more sources, particularly conference
    proceedings, and is still manually curated, unlike Google Scholar. 1 Our search
    included abstracts because not all publications mentioned wavelength selection
    within the title or keywords. We searched the titles, keywords, and abstracts
    of publications in the Scopus database using the following search string: (( “hyperspectral
    imaging” OR“spectroscopic imaging” OR “chemical imaging” OR “imaging spectroscopy”
    ) AND( “wavelength” OR “waveband” OR “wave band” OR “wave length” ) AND ( “selection”
    OR “selected” )) While the search string included multiple synonyms for hyperspectral
    imaging and wavelength selection, this does not have the effect of excluding studies
    missing “wavelength selection”, meaning that the search results may include unwanted
    studies. Studies were manually included and excluded based on the criteria listed
    in Table 1. The analysis was limited to a subset of hyperspectral imaging studies
    with wavelength selection to reduce the dimensionality. We did not exclude studies
    based on language (e.g., English, Chinese, and French) or year of publication.
    We only excluded studies based on publication type (e.g., conference or peer-reviewed
    journal) if the publication lacked a full methodology (e.g., published abstracts).
    Table 1 Selection criteria for the review process Full size table Study selection
    process The initial search stage of the literature review returned 1063 studies.
    The articles were retrieved on the 14th of October 2021. Our review was then updated
    with 166 more recent studies on the 13th of December 2022, resulting in a total
    of 1229 studies. The selection criteria defined in Table 1 were applied to reduce
    the total number of studies. Figure 2 shows a flowchart of the study’s selection
    process. Duplicate studies (n = 2) and conference announcements (n = 2) were excluded
    resulting in 1,225 records screened. Next, we set aside 17 reviews before screening
    the full-text results, leaving 1208 publications. Reviews were excluded because
    they did not provide empirical results comparing wavelength selection methods
    or follow a methodology to analyze the images. Fig. 2 Study selection process
    flow chart. This details the database search, the screening process and the number
    of studies included for two parts of the review Full size image The selection
    criteria addressed hyperspectral imaging, wavelength selection, and empirical
    results. This study defined hyperspectral imaging as acquiring images with a hyperspectral
    sensor capturing at least 90 wavelengths. Hyperspectral imaging is commonly defined
    in the literature as 100 wavelengths or more. We applied leniency to include studies
    that initially captured over 100 wavelengths but removed noisy bands during their
    analysis, reducing the total number of wavelengths to between 90 and 100. The
    second criterion is wavelength selection, which is defined as algorithm-based
    feature selection to select a subset of optimal wavelengths without limiting the
    size (such as restricting the search to a single band or pair of bands). These
    distinctions excluded studies that could perform a brute-force search of all wavelength
    combinations to select wavelengths. We excluded non-algorithmic and expert knowledge-based
    wavelength selection methods, such as manually selected band ratios. However,
    we included methods in which humans needed to manually interpret charts to select
    wavelengths (e.g., principal component plots or derivative curves). The final
    criterion was empirical results: each study required a transparent methodology
    incorporating selected wavelengths with machine learning or other statistical
    methods for hyperspectral image analysis. Each study was required to provide clear
    experimental results. This criterion excluded studies proposing novel hyperspectral
    systems or sensors without experimentation and studies with very few details available
    to answer our research questions. We screened the full texts of 1208 publications
    selected to establish which studies met our inclusion criteria (Table 1). As the
    lack of details in the abstract was not an exclusion criterion, all the studies
    required full-text screening. In this stage, 395 studies were eliminated. Of the
    eliminated studies, 156 did not apply an appropriate wavelength selection technique,
    while 100 did not investigate hyperspectral images. Finally, 14 studies were inaccessible
    to the researchers and associates. We contacted the original authors of 13 studies
    through ResearchGate. 2 Fourteen of the 16 originally inaccessible did not respond
    in time for this publication. The first stage of this review included 799 studies.
    A complete list of these publications is available in the Supplementary Material
    of this manuscript. The second stage of this review included 62 English studies
    that extracted spatial features from hyperspectral images to answer the spatial
    feature research questions defined in Section 1.1. The entire study population
    (n = 799) and the subset that utilized spatial features (n = 71) were the two
    groups of studies investigated. Encoding of the studies A single reviewer manually
    checked whether each study met our inclusion criteria and recorded all relevant
    information in a Google form. 3 The reviewer then completed multiple passes, checking
    one attribute at a time across all the studies to check the information collected,
    and our inclusion criteria were applied consistently. Google Translate was sufficient
    to extract information from non-English studies during the first stage of the
    review. The second stage was limited to studies written in English to avoid misrepresentation
    of information due to incorrect translations. The data collection process was
    manual, as some studies were not readable using automation tools. The language,
    year of publication, and broad application area (e.g., meat science or medical
    imaging) of each study were collected for demographic and application area analyses.
    The wavelength range and sample size were collected from each study to answer
    the hyperspectral study design research questions. The effective wavelength ranges
    were collected after the authors discarded noisy bands (if applicable) of all
    hyperspectral systems investigated. The sample size was recorded as the number
    of unique objects imaged in the study after the outlying samples were excluded
    (if applicable). The collected publications contained over 200 wavelength selection
    techniques and learning algorithms. From each study, we collected a comma-separated
    list of the techniques for wavelength selection and machine learning and an ordered
    list of the best wavelength selection methods based on their performance. Performance
    was defined as the accuracy of predicting the class or continuous measurement
    of interest. The performance of the sets of optimal wavelengths was determined
    based on the prediction set (testing set) accuracy of the best learning model
    for this subset. When there was a tie, the validation set followed by the calibration
    set accuracy acted as the tiebreaker. The methods were ranked based on their performance
    on the prediction set when one method clearly outperformed a subset of the others.
    If no method decisively outperformed the others, or if there was no comparison
    between multiple methods, we recorded the best method as inconclusive. Many studies
    have applied only a single wavelength selection method. In this case, the best
    method was recorded as only one. The results were tabulated with Python scripts,
    ranking which wavelength selection methods performed better based on the number
    of comparisons with other methods. We only included wavelength selection techniques
    applied in seven or more studies, and binned all others into the Other category.
    For example, if one study applied genetic algorithms (GA), successive projections
    algorithm (SPA), and regression coefficients (RC) to select an optimal subset,
    and they were ranked as such, we recorded three comparisons: GA outperformed SPA,
    GA outperformed RC, and SPA outperformed RC. We tabulated the number of studies
    that utilized each learning algorithm using Python scripts based on the collected
    comma-separated lists. Many studies have tested multiple variants of the same
    family of algorithms, such as random forests and decision trees. For more valuable
    insights, we counted the number of studies as a measure of popularity rather than
    the number of occurrences of each algorithm, meaning a study comparing two versions
    of support vector machines counted as a single observation. We recorded details
    from a subset of studies that extracted spatial features for the final research
    questions related to spatial features using a separate Google form. We recorded
    a list of the extracted spatial features, the parameters of these feature extractors,
    the performance of spectral features compared to spatial features, the performance
    of data fusion compared to individual models, how representative feature images
    were selected to extract spatial features, and how spectral-spatial features were
    fused. We recorded whether the data fusion models outperformed the individual
    models as yes, no, or inconclusive. The best individual set of features was recorded
    as spectral features, spatial features, not compared individually, or inconclusive.
    Both were determined based on the accuracy on the prediction set of the best model.
    All studies required details regarding wavelength selection techniques to meet
    our inclusion criteria. Not all the other attributes collected were compulsory.
    For each research question, we ignored studies in which the related attributes
    of interest were not stated or unclear. Only studies investigating spatial features
    were gathered to answer the spatial feature research questions. Since the information
    gathered is essential for the reproducibility of each study, missing data could
    indicate lower publication quality. A spreadsheet stored the information collected
    by this review and generated simple statistics (e.g., counts). Python scripts
    extracted other information required to answer questions (e.g., comparisons of
    methods and the wavelength range graph) and were used to generate tables and figures.
    The Python libraries required were Pandas 4 for reading the data, 5 and Matplotlib
    6 for creating figures. Biases in this study This review may introduce bias by
    including several studies from prevalent authors or research groups. Some research
    groups may follow the same methodology for multiple hyperspectral imaging applications.
    This imbalance may skew the conclusions on which wavelength selection, learning
    algorithms, camera models, and other attributes of interest are the most popular
    for hyperspectral imaging applications. No methods explored the causes of heterogeneity
    among the study results because each study acquired different datasets, and no
    control groups were available. No method assessed the risk of bias due to missing
    results, the robustness of results, or the certainty of the body of evidence for
    an outcome. There is a significant imbalance in the occurrence of many wavelength
    selection techniques. Some wavelength selection techniques have been utilized
    in a small set of studies (e.g., LASSO [33]), and others have been consistently
    applied across hundreds of studies (i.e., SPA [34], and CARS [35]). We limited
    the analysis to methods applied in more than five studies to reduce the impact
    of less common techniques. Results The initial search returned 1229 publications.
    After applying the selection criteria, 799 publications were selected as relevant
    and analyzed, as described in the following subsections. Although all 799 studies
    informed the conclusions of this review, not all were cited here. The complete
    list of included and excluded studies with the reason for their exclusion is available
    in the Supplementary Material. Demographic characteristics of research studies
    Fig. 3 Number of studies included in this review between 2000 and 2022. The number
    of studies increases over time. There has been a decline in recent years due to
    the COVID-19 pandemic limiting research Full size image The reviewed studies included
    734 journal articles and 65 conference papers based on the classification provided
    by Scopus. Of the total 799 studies, the publication language included 646 studies
    in English, 149 in Chinese, and four published in other languages (German, Spanish,
    French, and Persian). An analysis of publications per year showed an increase
    in hyperspectral imaging studies with wavelength selection (Figure 3). This increase
    is likely due to the increased accessibility of affordable hyperspectral sensors.
    The COVID-19 pandemic may have restricted access to research laboratories, causing
    a slight decline in research outputs since 2020. Standard hyperspectral study
    design Sample sizes The first aspect of designing a hyperspectral imaging study
    examined in this review is the number of samples required (RQ 1). It is essential
    to ensure that each study has an adequate number of samples for training the models
    and a representative set of unseen samples for validation. To compare the sample
    sizes across the population of hyperspectral imaging studies included in this
    review, we must define a single sample. The number of images cannot represent
    the sample sizes because some may contain multiple samples (e.g., multiple maize
    kernels or grains of rice). The number of subjects purchased or retrieved may
    not be reliable since researchers can obtain measurements from multiple subsamples
    (e.g., measuring the tenderness of broiler breast fillets [36]). Therefore, we
    determined the number of samples as the number of individual samples of interest
    with the corresponding reference values or classes. The box and whisker plot shown
    in Fig. 4 summarizes this data on a logarithmic scale due to the skew in observations;
    the median number of samples was 180, with the middle 50% between 105 and 300
    samples. The mean was 424.04 samples, and the maximum and minimum were 19,000
    and 1, respectively, indicating a clear skew in the data due to outliers. Some
    studies had as few samples as a single image, such as studies using the Indian
    pines dataset [37, 38] for land use classification. Studies with thousands of
    samples typically investigated low-cost, easy-to-image subjects, such as oats,
    maize kernels, or other types of seeds [39,40,41,42,43,44]. The study with the
    second highest number of samples (15,000) collected multiple years of data to
    detect diseases affecting grape cultivars [43]. Fig. 4 Number of samples (log
    scale) encountered in hyperspectral studies that applied wavelength selection.
    The median number of samples was 180, with the middle 50% between 105 and 300
    samples. The log scale was required for the outliers, which included one study
    using a single image and another using 19,000 samples Full size image Some studies
    acquired images of the same subjects across multiple timestamps to see how damage
    symptoms developed over time [45] or to compare before and after treatment [46,
    47]. Sampling like this increases the number of observations and sample variation.
    However, this is possible only when there is no destructive analysis to retrieve
    the ground-truth reference values from a sample. In these studies, we counted
    the number of imaged samples rather than the number of unique samples. The chemical
    analysis of samples is often destructive. Depending on the amount of material
    required for this analysis, measuring multiple attributes of each sample may not
    always be possible [48,49,50]. For classification, a single sample could be subdivided
    into regions with multiple classes, such as those for detecting white stripping
    on chicken fillets [51], where multiple affected areas were visible from a single
    sample. Another example is where multiple defective and unaffected regions are
    visible in the same image. Researchers studying the spectral response of damage
    to fruit samples typically defined multiple regions of interest for the damaged
    and unaffected areas [52, 53]. In conclusion, most studies captured between 105
    and 300 samples, but the differences in methodologies between study types made
    recording the sample sizes difficult. Wavelength ranges The second aspect of the
    study design investigated in this review is how to select a suitable wavelength
    range to capture hyperspectral images (RQ 2). Figure 5 shows the wavelength ranges
    studied in the 681 studies that clearly indicated their sensor range. Based on
    the sensors available on the market, the three most common ranges were visible/near-infrared
    (VIS/NIR: 400–1000 nm), near-infrared (NIR: 900–1700 nm), and shortwave infrared
    (SWIR: 900–2500 nm). The specific range of interest depends on the application.
    For example, predicting the attributes of samples correlated with moisture is
    more suitable in an infrared range where water absorbs more light, but where pigments
    are correlated with the target attribute, a sensor focusing on visible light is
    more appropriate. Most studies focused on one of these ranges, and only 50 investigated
    multiple ranges. When researchers combine multiple sensors (to examine the spectral
    features over a more extensive wavelength range), the authors extracted the mean
    spectra from each sensor and concatenated the vectors to form a single contiguous
    spectral vector that represented the sample. The researchers then fitted a model
    to the selected wavelengths from the combined spectra. No study in this review
    extracted extended pixel-wise spectra by combining data from multiple sensors
    because they could not find corresponding pixels between sensors. Fig. 5 Histogram
    of the number of studies investigating each wavelength range Full size image Figure
    5 shows a spike around 900–1000 nm in the wavelength range of interest due to
    the overlap between VIS/NIR and other sensors. The VIS/NIR range was the most
    common due to the accessibility of these sensors [54], and the lower cost of the
    detector coatings for this wavelength range [7]. There is no significant difference
    in the choice of sensors based on the sample type between these different ranges.
    The proportion number of studies looking at the SWIR and VIS/NIR is approximately
    the same for meat products as for fruit and vegetable-related studies, as shown
    by the lines on the plot. Feature selection and machine learning Wavelength selection
    Previous reviews of wavelength selection techniques have divided the field into
    three categories based on how the methods find optimal features [29]. These categories
    are filter, embedded, and wrapper methods. Filter methods apply a threshold to
    a feature importance score to select the best wavelengths. Examples of filter
    methods include regression coefficients and variable importance in projection
    (VIP). Embedded methods integrate learning and feature selection, such as LASSO
    regression [33] and decision trees. Wrapper methods are the most popular category,
    and they operate by iteratively updating the wavelength subsets and fitting models
    to evaluate the performance. Wrapper methods include successive projection algorithms
    (SPA) [34] and genetic algorithms [55]. We also propose a fourth category of techniques,
    manual selection methods, in which an algorithm provides an output that is interpretable
    by experts to select the key wavelengths. Principal component analysis loading
    plots are an example of this type of method. Researchers typically determine points
    at the peaks/troughs of a plot as the important wavelengths. We extend this categorization
    to two other categories of wavelength selection methods: concatenated and interval-based
    methods. Concatenated methods string multiple selection algorithms together to
    select important features from previously selected subsets, and interval-based
    methods select an informative interval of wavelengths, often of a manually determined
    width. Concatenated methods can combine the advantages of multiple algorithms,
    such as UVE and SPA. UVE has problems with multicollinearity, and SPA may select
    uninformative variables. UVE-SPA first removes uninformative wavelengths and SPA
    removes variables with the least multicollinearity [16]. In Table 2, we have categorized
    the most popular wavelength selection techniques into these categories and cited
    recent English studies that employed each technique (RQ 3). Table 2 An overview
    of wavelength selection techniques ranked by popularity Full size table Tables
    3 provide an exhaustive comparison of wavelength selection techniques applied
    in the area against each other (RQ 4). These tables present the number of studies
    in which the selected method (rows) outperformed the comparison method (columns).
    Where the table reads “Only one”, it counts the number of studies where the comparison
    method was the only wavelength selection method. The “Inconclusive” row presents
    the number of studies that applied the wavelength selection technique, where the
    ranking of each method was unclear. Finally, the “Not compared” row provides the
    number of studies using the method that did not compare it to the other methods.
    From these tables, we find that there is a clear set of common methods: successive
    projections algorithm (SPA) [34], competitive adaptive reweighted sampling (CARS)
    [35], regression coefficients (RC) [60], and principal component analysis (PCA)
    loadings. Among these techniques, CARS was most often the best method. CARS performed
    better than SPA in 50 of 76 comparisons, where one was better than the other.
    Researchers in 122 studies manually examined the peaks and troughs in the PCA
    loadings plot to select the key wavelengths. 79 studies applied this as the only
    wavelength selection technique, compared with 59 studies that compared it with
    other methods. PCA loadings outperformed SPA in four out of 14 comparisons, CARS
    in one out of 6 comparisons, and regression coefficients in three out of seven
    comparisons, where one method was better. PCA loadings are among the worst-performing
    techniques despite being among the most popular methods. The most common and consistent
    methods found in this review were the CARS and SPA methods. The concatenated methods
    and variable importance in projection (VIP) are the only comparison methods in
    which the CARS algorithm did not perform well. Concatenated and interval-based
    methods, UVE and CARS outperformed SPA, whereas SPA performed well in all other
    comparisons. Regression Coefficients were the sole wavelength selection method
    used in 89 studies, and in 57 studies where researchers compared it to others,
    it did not perform well. The performance of regression coefficients is demonstrated
    by comparisons with genetic algorithms (best in two out of seven comparisons),
    CARS (best in five out of 16 comparisons), and SPA (best in 11 out of 30 comparisons).
    Regression coefficients, as with PCA loadings, is a simple feature selection method,
    making it more popular than novel methods that are not available in standard libraries
    for different programming languages or built into commercial software. For the
    two concatenated methods utilized in more than five studies, there were very few
    comparisons with the other techniques. Studies that included these methods often
    compared them to other concatenated methods [129, 130]. CARS-SPA and UVE-SPA performed
    well but were only applied in 20 and 15 studies, respectively. CARS-SPA had the
    highest performance, outperforming SPA in ten out of 15 comparisons and an equal
    performance against CARS (better in eight out of 16 comparisons). UVE-SPA similarly
    performed well but underperformed other concatenated approaches, such as CARS-SPA
    (best in two out of five comparisons). UVE-SPA was outperformed by CARS and UVE
    individually (best in two out of nine and one out of seven comparisons, respectively).
    The most common interval-based approaches encountered are interval-partial least
    squares (iPLS) [89], interval-VISSA (iVISSA) [103], Synergy interval partial least
    squares (siPLS) [123], and interval random frog (iRF) [105] were often compared
    to each other. These methods divide the full spectrum into equidistant partitions
    and fit regression models to the intervals [91]. There were not enough comparisons
    of these methods to conclude which interval-based method was the best, and their
    performance against single-feature methods was generally poor. Many studies have
    proposed a novel wavelength selection method and compared it to a small set of
    the most common approaches while claiming superior performance. Comparisons with
    a broader range of techniques over multiple datasets are required to benchmark
    their performance. Since five or fewer studies applied each of these methods,
    we grouped them methods into the “other” column of Table 3. To the best of our
    knowledge, no standardized benchmark datasets are available for wavelength selection.
    Other popular methods for wavelength selection include genetic algorithms [55],
    random frog [70], uninformative variable elimination [67], and variable importance
    in projection [60]. Genetic algorithms performed well, but it is not easy to compare
    to this algorithm because many hyperparameters must be manually set. Machine learning
    methods The final step in model creation is to fit the model to the selected wavelengths.
    Here, we list the most common machine learning and statistical models applied
    to the selected wavelengths (RQ 5). Partial least squares (PLS) is the most common
    learning algorithm for analyzing hyperspectral data after wavelength selection.
    A total of 470 studies chose a variant of PLS. The most common variants are partial
    least squares regression (PLSR) for regression and partial least squares discriminant
    analysis (PLS-DA) for classification. Previous reviews have reported the high
    utilization of PLSR and PLS-DA [131]. Instead of directly operating on the variables,
    PLS extracts a set of latent variables with the best predictive performance [21].
    Seventy-four studies applied a more straightforward multiple linear regression
    (MLR) that fits a simple linear equation to the observed data. MLR struggles with
    high multicollinearity between wavelengths [132], which makes prior wavelength
    selection important. The advantage of MLR is the interpretability of the results,
    whereas the meaning of the latent variables or principle components is unclear.
    The second most common algorithm was support vector machines (SVM) [133] in 333
    studies, the most common variants of which were least squares support vector machines
    (LS-SVM) and support vector regression (SVR). Table 3 Comparative performance
    of wavelength selection algorithms Full size table Various studies applied variants
    of artificial neural network (ANN) architectures, such as backpropagation neural
    networks (BPNN) [134], extreme learning machines (ELM) [135], stacked autoencoders
    (SAE), and convolutional neural networks (CNN) [136]. These machine learning approaches
    apply a series of processing layers to extract higher-level features and are often
    referred to as deep learning approaches. Non-linear methods such as ANNs and SVMs
    are valuable for modelling complex relationships between dependent and independent
    variables. These non-linear approaches have a higher computational complexity
    [10]. Only a few studies have applied deep learning to hyperspectral imaging for
    food applications because of the time and cost requirements for gathering large
    datasets with corresponding reference (ground-truth) measurements [8]. Few studies
    (n = 20) have combined convolutional neural networks with wavelength selection
    despite their popularity in computer vision outside hyperspectral imaging, but
    these techniques have become more common since 2022 [57, 137,138,139]. Due to
    the challenges of big data and small sample sizes, CNN approaches typically apply
    one-dimensional filters to the spectral response rather than two- or three-dimensional
    filters to the hyperspectral image. Table 4 displays the counts for each learning
    algorithm found by this study. Table 4 Machine learning models with wavelength
    selection sorted by popularity Full size table Decision trees were found in 67
    studies, including variants such as random forests and classification and regression
    trees (CART). K-nearest neighbors (KNN) is a simple method for classifying samples
    based on their distance from other labelled samples. Because of the number of
    neighborhood comparisons required, KNN becomes more computationally expensive
    with larger datasets. Projection methods such as linear discriminant analysis
    (LDA) and principal component analysis (PCA) distinguish classes within data by
    projecting the remaining wavelengths onto new axes to maximize variance and class
    separability. Principal component regression (PCR) is applied instead of PCA for
    regression analysis. Fifty-three studies utilized LDA compared with 26 studies
    that chose PCA instead. Of the 799 studies surveyed, 77 compared learning algorithms
    that did not fit these categories. Spatial features Spatial descriptors (image
    texture descriptors) provide information about the spatial arrangement of pixels,
    whereas spectral information describes how light interacts with samples. As discussed
    earlier, many reviews acknowledged that including spatial features as independent
    modelling variables helps to improve the predictive performance of models [22].
    Spatial information includes statistical features that summarize the distribution
    of intensities, such as statistical moments, and higher-order spatial descriptors
    (statistics) describe the distribution of groups of pixels, such as pairs for
    grey-level co-occurrence matrices. Spatial features may also include local neighborhood
    operations, such as local binary patterns (LBP) [140], or shape features describing
    the shape of the regions of interest. This section describes the different techniques
    for extracting spatial features from hyperspectral images (RQ 6), approaches to
    select feature images (RQ 8), the predictive performance of models incorporating
    spatial features (RQ 9), and how studies combine spatial features with spectral
    features (RQ 7). Table 5 Comparison between the 71 studies included in this review
    sorted chronologically, using spatial features with hyperspectral imaging and
    wavelength selection Full size table Spatial features descriptors A total of 71
    out of the 799 studies selected for review included spatial features and wavelength
    selection for hyperspectral image analysis. Table 5 provides a breakdown of these
    studies, answering RQ 6. The most popular spatial features were the grey-level
    co-occurrence matrices (GLCM) texture descriptors found in 41 out of 71 studies.
    GLCM descriptors (also known as Haralick features) describe the two-dimensional
    feature image by creating a matrix of the probability of joint occurrences of
    pixel grey level values at a given angle (\\(\\theta\\)) and distance (d) [202].
    Haralick et al. [202, 203] described 14 features to summarize GLCM matrices into
    texture descriptors. Studies such as Clausi [204] and Soh and Tsatsoulis [205]
    further extended this set of features. The five most common features encountered
    were: contrast/inertia moment (n = 38), energy/uniformity/angular second moment
    (n = 38), correlation (n = 36), homogeneity/inverse difference moment (n = 35),
    and entropy (n = 19). Four studies investigated features beyond this set of five
    core features [171, 178, 184, 194], but two did not provide sufficient information
    on the extracted features. One of the earliest studies combining spatial features
    with wavelength selection [194] extracted 22 GLCM features, including multiple
    methods for selecting the feature image. However, these studies did not examine
    the effectiveness of each feature. The main parameters of the GLCM are the angle
    and distance used to measure pixel correlation. Typically, studies chose four
    angles 0°, 45°, 90°, and 135° describing the relationships diagonally, vertically,
    and horizontally to create GLCM matrices. Different angles and distances create
    different matrices. Eleven studies averaged the descriptors over the four orientations
    to produce a rotation-invariant descriptor. Many studies did not state whether
    they averaged over multiple angles (n = 16), did not average over angles (n =
    8), or only employed a single angle (n = 5). For the distance parameter, the typical
    approach is to create matrices based on a one-pixel distance. Few studies have
    examined a broader range of distances, such as one to ten pixels [201] or one
    to five pixels [164, 190]. Because we can fix the distance between the samples
    and sensors in controlled environments, a single distance should be sufficient.
    Future studies should be conducted using multiple distances to determine the best
    effect. The second most popular spatial feature extraction method was the grey-level
    gradient co-occurrence matrix (GLGCM) [206] (n = 11). Similar to GLCM, GLGCM captures
    second-order statistics about the spatial image content by describing the grey-level
    gradients of the images. The GLGCM matrix represents the relative frequency of
    occurrence between a pixel with a given grey value and gradient. The data fusion
    model outperformed the single modality models for all studies extracting GLGCM
    features that compared fusion models to individual models (n = 10). GLGCM has
    a much larger set of features than GLCM, describing the dominance and distribution
    of gradients and the entropy, average, standard deviation, asymmetry, and nonuniformity
    of both the pixel gradients and grey level. The popularity of GLCM and GLGCM was
    consistent with previous reviews [1, 14]. Another spatial feature extraction approach
    is local binary patterns (LBP) [169, 175, 182] where each pixel was compared to
    its horizontal, vertical, and diagonal neighbors to give an eight-digit binary
    code [140]. Each position in the binary code describes whether the neighboring
    pixel value is larger (1) or smaller (0) than the target pixel. There were mixed
    results from the LBP features for hyperspectral images. One study extracted LBP
    features from RGB images to classify freezer-burnt salmon with no data fusion
    and found that the spectral-based model performed better for discriminating classes
    [175]. Another study found that GLGCM and GLCM significantly outperformed LBP
    without data fusion [169]. GLCM, GLGCM, and LBP provide second-order statistics
    based on the relationships between pixel pairs [207]. SSome studies have extracted
    first-order statistics, such as histogram descriptors, based on the histogram
    of intensities in the feature image. Histogram statistics summarize the grey value
    distribution of the image without considering the spatial interactions between
    pixels. The most common features extracted from these grey-level histograms are
    mean, uniformity, entropy, standard deviation, and third-moment (skewness) [186,
    195, 208], as well as kurtosis, energy, smoothness, contrast, consistency, and
    roughness [141, 149, 154, 178, 180]. One study applied feature selection to the
    precomputed spatial features and found that the selection algorithm did not select
    histogram-based features for beef tenderness forecasting [182]. Other studies
    found that histogram statistics improved the model accuracy more than GLCM features
    [178] or shape descriptors [186]. The GLCM features significantly outperformed
    histogram statistics in another study [51]. Five studies investigated Gabor filters
    for extracting spatial features from hyperspectral images [146, 165, 170, 182,
    191]. Gabor filters, which are linear filters used for texture analysis, apply
    convolutions with kernels generated by combining the Gaussian and sinusoidal terms.
    The results are often averaged over multiple angles to obtain rotationally invariant
    descriptors [146, 165]. Prior to 2020, only one study extracted spatial features
    with a convolutional neural network (CNN) [153]. This CNN architecture utilized
    two branches: one extracted spectral features with a one-dimensional CNN and the
    second extracted spatial features with a two-dimensional CNN. The model concatenated
    the extracted feature vectors for classification into a single feature vector.
    Feature importance was evaluated using the weights learned by the CNN in the first
    layer to select the optimal wavelengths. Since then, more advanced two-dimensional
    CNN architectures have been used to utilize the spatial information within images
    [84, 143]. One study applied the YOLOv3 algorithm [209], a popular object detection
    algorithm, to detect defects on apples [139]. The final common spatial feature
    extraction approaches accounted for were morphological/shape features. These approaches
    described the contour of the region of interest encompassing the sample with shape
    features. Shape features require only the ROI mask of the sample. The typical
    features extracted were the area, perimeter, major- and minor-axis lengths, and
    eccentricity. The studies that extracted shape features had good results for discriminating
    the varieties of seeds [197] and black beans [181] because of the correlation
    between shape and variety. The accuracy of the models trained on spectral features
    in both cases outperformed spatial features, and the data fusion model further
    improved the results. Other studies have applied edge detection algorithms to
    detect cracks and morphological features to detect scattered egg yolk [142]. Other
    less frequently used spatial features extracted include grey-level run-length
    matrix analysis [183], wide line detectors [146, 191], and wavelet-based transformations,
    such as Discrete Wavelet Transform (DWT) [96, 192], and Discrete cosine transformation
    [172]. Selecting feature images Each feature extraction approach requires an image
    to form the basis of spatial feature descriptors. The most common methods extract
    features from the principal component (PC) score images of a full hyperspectral
    image or pre-selected wavelengths. Selecting the optimal wavelengths before extracting
    spatial features is less computationally intensive. Preselecting optimal wavelengths
    assumes that wavelengths with important spectral information also carry important
    spatial information. Selecting the optimal wavelengths before extracting the spatial
    features was employed in 48 studies. Another approach is to extract spatial features
    from each wavelength within the hyperspectral images and then apply feature selection
    to reduce the features to a set of optimal features from a reduced number of bands
    (post-selected). Two studies directly selected bands with important spatial features
    by extracting them from all wavelengths [146, 188]. Other studies have applied
    feature selection to select informative features from a selected or full feature
    set [144, 165, 173, 191]. Twenty-five studies selected PC score images as feature
    images. Although extracting texture descriptors across the entire spectrum is
    possible, this increases the computational complexity, making it infeasible in
    real-time systems. A single study utilised spatial features from all available
    wavelengths without feature selection [177], and five studies created either RGB
    images from the hyperspectral sensor or captured RGB images from a secondary camera
    [84, 142, 166, 175, 201]. The selected feature images are listed in the feature
    image column of Table 5. Data fusion There are three levels of data fusion for
    combining features from different feature spaces [210]. Pixel-level fusion integrates
    multiple modalities as co-registered inputs into a single feature extraction model,
    feature-level fusion combines features after extracting characteristic features,
    and decision-level fusion combines multiple model outputs after fitting models
    to the extracted features. Almost all studies that considered spatial features
    combined spectral and spatial features with feature-level fusion, with some CNN-based
    models integrating spectral-spatial information at the pixel level [84, 139].
    They first extracted features from the images to combine spectral and spatial
    features before fitting the models to the fused feature vector. It is possible
    to apply decision-level fusion by training independent models on the spectral
    and spatial features and then using a meta-model to the decisions of the separate
    models. Models trained after feature-level fusion can incorporate interactions
    between individual features, which decision-level fusion models cannot. Feature-level
    fusion avoids the computational burden of high-dimensional feature extraction
    in pixel-level fusion. Pixel-level fusion is feasible when data from two hyperspectral
    sensors with different wavelength ranges are fused. All the studies that merged
    data from multiple sensors extracted the mean spectra from each sensor’s ROI and
    combined them into a single spectrum for modelling, which would also be considered
    feature-level fusion. However, regions of interest could be registered together
    to combine multiple wavelength ranges for each pixel. Mean normalization is often
    applied to features of different modalities (spectral and various types of spatial
    descriptors) to overcome problems caused by disparities between features [154,
    165, 171, 180, 183, 185, 189]. Normalization rescales each feature in the training
    set (calibration) by its mean and standard deviation, giving all features a comparable
    magnitude. Performance Thirty-seven studies that included spatial features within
    their feature sets found that the predictive performance of models trained on
    spectral features alone outperformed those trained solely on spatial features
    (Table 5). Only five studies discovered that spatial characteristics were more
    helpful for predicting attributes than spectral features, and 29 studies failed
    to compare the two approaches in a way that allowed us to determine which model
    was the most effective. However, the selected model also affected the results.
    The performance metric that we used was the accuracy of the best model. When combining
    spectral and spatial features, we found that most studies (n = 40) performed better
    by combining spectral and spatial features than by using spatial or spectral features
    individually. The individual model outperformed the data fusion model in six studies.
    Finally, 25 studies did not compare the individual models to the spectral-spatial
    fusion models. In these outlying studies, spatial features tended to be more appropriate
    for the problem because they corresponded to clearly visible texture indicators.
    For example, pork marbling can be detected using a wide-line detector to predict
    intramuscular fat [146]. Other studies related to pork meat assessment found that
    Gabor and GLGCM features outperformed spectral features for prediction of the
    total volatile base nitrogen content (TVB-N) [165] and freshness prediction [187],
    respectively. For the classification of tea varieties [167] and prediction of
    the water content of tea leaves [196], the GLCM features outperformed the spectral
    features. These studies showed that spatial features could be more valuable than
    spectral features of hyperspectral images in predicting the attributes of interest.
    Studies have likely found worse performance from data fusion models because they
    did not include spatial features correlated with the prediction variable. One
    study observed a significant drop in performance between the calibration set and
    the prediction set for classifying maize seed quality, possibly indicating that
    the model was overfitting to the data [154]. Overfitting occurs when the model
    identifies trends in the calibration set that are less prevalent in the prediction
    dataset. Another study reported a similar predictive performance between spectral
    and data fusion models for predicting the tenderness of salmon [190]. GLCM features
    did not help predict chilling injury classes of green jujubes [162]. In these
    studies, the spatial features may have been inappropriate for the particular problem,
    or the wavelengths selected based on the spectral data may have been unsuitable.
    Discussion This review systematically surveyed studies that applied wavelength
    selection to hyperspectral imaging and found that food quality and safety accounted
    for most of the applications. Popular subjects included pork, apples, maize kernels,
    wheat, and potatoes, and common attributes of interest included moisture content,
    variety, and adulteration. Standard hyperspectral study design Surveying the number
    of samples in each study showed that hyperspectral imaging studies utilized significantly
    fewer samples than other machine learning applications in computer vision. The
    main reason for this is the cost and time required to acquire sample images. Due
    to the lower accessibility of sensors, it is not possible to use crowd-source
    training data such as popular RGB image datasets [211]. Condensing each sample
    to a single mean reflectance spectrum reduces spatial dimensionality at the cost
    of losing potentially useful spatial information. While deep learning has become
    the most popular approach for many computer vision tasks, typical datasets for
    deep learning, such as object detection or image classification datasets, often
    have millions of annotated RGB images. With more cost-effective hyperspectral
    sensors, researchers can create larger hyperspectral imaging datasets and apply
    deep learning models that reduce the feature set to find reliable trends with
    fewer observations. This review also concludes that VIS/NIR range sensors are
    the most common for hyperspectral imaging applications with wavelength selection.
    VIS/NIR sensors are cheaper and provide adequate information for most studies.
    Future studies should consult prior works to determine which wavelength range
    suits their application. Feature selection and machine learning Wavelength selection
    Given the current state of the literature, determining which feature selection
    technique yields the best wavelengths remains highly application dependent. Many
    confounding factors affected the accuracy of the models. Studies that have evaluated
    multiple wavelength selection methods and multiple classifiers may have one wavelength
    selection technique that performs best with one classifier and another performing
    best with a different classifier. Each study may differ in the implementation
    of each algorithm, experimental conditions, and datasets. The specific implementation
    may differ for methods with multiple hyperparameters, such as genetic algorithms,
    which require a fixed population size, genetic operator probability, number of
    generations, and a fitness function. Many studies have proposed novel methods
    to select features but have not comprehensively compared the most common wavelength
    selection methods. We suggest that future studies compare the most common related
    methods to provide a more accurate benchmark for new feature selection methods
    on a particular dataset. The best-performing and most popular methods are the
    SPA, CARS, and genetic algorithm methods. This review does not consider the number
    of wavelengths selected by each method. UVE often uses hundreds of wavelengths
    with high multicollinearity. Creating multispectral models for real-time analysis
    of hundreds of influential bands is not feasible. We also did not consider the
    differences between the interval-based and individual-feature approaches. Creating
    a multispectral model from a small subset of features may benefit from informative
    intervals instead of informative narrow wavelengths because some spectral features
    are visible over several adjacent wavelengths. Machine learning Our review concluded
    that there are fewer regression or classification methods than wavelength selection
    techniques. The most common methods are based on Support Vector Machines (SVM)
    or Partial Least Squares (PLS). We did not compare the effectiveness of these
    machine learning algorithms. This review attempts to gauge their popularity in
    literature. Partial least squares, decision trees, and multiple linear regression
    methods are explainable approaches compared to neural network approaches but are
    limited in extracting features. Wavelength selection for hyperspectral imaging
    studies follows the process of first extracting features from sample images, applying
    feature selection, and applying a regression or classification algorithm. This
    process limits the search space of models and contradicts the integrated approach
    of deep learning, where a model learns to extract meaningful features and applies
    regression or classification. This review found no algorithm combining feature
    extraction, selection, and regression (or classification) steps. The only similar
    example we found was the two-branch CNN approach by Liu et al., which extracted
    spectral and spatial features independently [153]. Spatial features Our review
    found that GLCM and GLGCM were the most common methods for extracting spatial
    features. These handcrafted features require high levels of domain knowledge and
    may only be beneficial for certain problems. GLCM and GLGCM improved the accuracy
    of the learning algorithms in studies that incorporated them. Features learned
    by deep learning methods have dominated image classification of RGB images over
    the last decade [212]. Learning spatial features is complicated when considering
    the high number of channels in hyperspectral imaging (extensive fea- ture space),
    small sample sizes, and simultaneous challenge of selecting important wavelengths
    to enable online (real-time) applications. The number of spatial features available
    exacerbates the high dimensionality of wavelength selection. GLCM alone has 14
    standard texture descriptors per angle, distance, and wavelength. The feature
    space can quickly grow to extremely high dimensions with many combinations. Many
    studies have limited the feature image set to the selected wavelengths. Experimenting
    with enough features almost guarantees that at least one will correlate with the
    reference attributes in the training set, but this trend may not apply to the
    validation data. Very few studies have selected important spatial features independent
    of the spectral features. Huang et al. [188] showed that the spatial features
    can be plotted over the entire wavelength range to form a continuous curve, similar
    to the standard approach to mean reflectance. Other types of features also share
    a high correlation between adjacent wavelengths. Many studies have extracted spatial
    features from the Principal Component score images and spectral features using
    wavelength selection. However, generating a PC score image requires a complete
    set of wavelength bands. All wavelengths would be needed to extract spatial features
    making wavelength reduction redundant and creating a lower-cost multispectral
    system impossible. The most common approach is to pre-select wavelengths based
    on the spectral features. Most studies have revealed that spectral features are
    superior to spatial descriptors for the classification or prediction of attributes.
    Models incorporating both spectral and spatial data had higher predictive performance
    than individual features alone. The two main limitations of studies investigating
    spatial features are that they assume that wavelengths with the most meaningful
    spectral information also have the most meaningful spatial information. In most
    cases, the set of available spatial features is limited to a set selected by the
    authors. More work is needed to create flexible feature selection techniques and
    test them on larger sets of spatial features. Limitations Due to the restrictions
    of our inclusion criteria used in this review, such as only including studies
    with wavelength selection, some aspects of the studies investigated in this review
    may not represent all hyperspectral imaging research. We did not compare spatial
    feature extraction methods for hyperspectral imaging rather than wavelength selection.
    Studies may also have been missing from this review. They were either unavailable
    on Scopus, or the search string did not retrieve them. We mitigated this as much
    as possible by searching titles, keywords, and abstracts for multiple synonyms
    of “wavelength” and “selection”. However, we found some relevant studies that
    did not mention these keywords in their abstracts. Searching for an acronym for
    each wavelength selection technique is not feasible. Conclusion This review included
    799 hyperspectral imaging studies from Scopus from the 1229 studies collected.
    This review excluded studies that did not investigate hyperspectral imaging and
    wavelength selection or did not provide a transparent methodology and experimentation
    results. We analyzed these studies to understand the methods for feature selection,
    machine learning, and spectral-spatial feature fusion. Regarding the design of
    wavelength selection studies, this review found that hyper- spectral image analysis
    studies tended to have only a small number of samples, with the studies included
    in this review having a median sample size of 180. We investigated the wavelength
    ranges for studies that applied wavelength selection and found three commonly
    investigated regions of the electromagnetic spectrum: namely, 400–1000 nm, 900–1700
    nm, and 900–2500 nm. Many studies have applied an extensive range of wavelength
    selection algorithms. Although the comparative performance of the wavelength selection
    techniques is not an objective of most studies in hyperspectral imaging, we recommend
    that future work should apply a range of different techniques rather than just
    a single technique. We recommend the popular SPA, CARS, and genetic algorithms
    wavelength selection methods as benchmarks for future wavelength selection studies.
    Concatenated methods, such as CARS-SPA and UVE-CARS tend to provide features that
    lead to more accurate models. However, the number of studies that applied each
    of these techniques is not high enough to conclude whether they are better than
    other techniques. Any newly introduced wavelength selection method should be compared
    to a wide range of best-performing methods because there is no clear best selection
    method for all applications. The most common learning algorithms were partial
    least squares, support vector machines, and artificial neural networks. These
    wavelength selection and modelling steps were not integrated into the same solution
    because combining feature reduction with feature extraction and learning remains
    challenging. This review found that spectral features were more informative than
    spatial features in most studies employing spatial features, whereas combining
    both feature types increased predictive performance. Spectral and spatial features
    are typically extracted independently and fused using feature-level fusion. The
    most common method is to select wavelengths with important spectral information
    and extract spatial features from each selected wavelength. GLCM features were
    the most common texture descriptor combined with wavelength selection and were
    applied in more than half of the studies that considered spatial features. There
    is a need for more flexible feature selection and extraction methods, further
    investigation of spatial features, and publicly available large-scale hyperspectral
    image datasets. Flexibility can arise from flexible intervals of wavelengths or
    more flexible methods for extracting informative spatial features independent
    of spectral features. Change history 27 September 2023A Correction to this paper
    has been published: https://doi.org/10.1007/s11694-023-02148-4 Notes https://scholar.google.co.nz/.
    https://researchgate.net/. https://docs.google.com/forms. https://pandas.pydata.org/.
    https://github.com/WestHealth/pyvis. https://matplotlib.org/. References J. Ma,
    D.-W. Sun, H. Pu, J.-H. Cheng, Q. Wei, Advanced techniques for hyperspectral imaging
    in the food industry: principles and recent applications. Annu. Rev. Food Sci.
    Technol. 10(1), 197–220 (2019). https://doi.org/10.1146/annurev-food-032818-121155
    Article   CAS   PubMed   Google Scholar   L. Feng, B. Wu, S. Zhu, Y. He, C. Zhang,
    Application of visible/infrared spectroscopy and hyperspectral imaging with machine
    learning techniques for identifying food varieties and geographical origins. Front.
    Nutr. (2021). https://doi.org/10.3389/fnut.2021.680357 Article   PubMed   PubMed
    Central   Google Scholar   A.Y. Khaled, C.A. Parrish, A. Adedeji, Emerging nondestructive
    approaches for meat quality and safety evaluation–a review. Compr. Rev. Food Sci.
    Food Saf. 20(4), 3438–3463 (2021). https://doi.org/10.1111/1541-4337.12781 Article   PubMed   Google
    Scholar   L. Xu, X. Wang, H. Chen, B. Xin, Y. He, P. Huang, Predicting internal
    parameters of kiwifruit at different storage periods based on hyperspectral imaging
    technology. J. Food Meas. Charact. 16(5), 3910–3925 (2022). https://doi.org/10.1007/s11694-022-01477-0
    Article   Google Scholar   T. Lei, X.-H. Lin, D.-W. Sun, Rapid classification
    of commercial cheddar cheeses from different brands using PLSDA, LDA and SPA–LDA
    models built by hyperspectral data. J. Food Meas. Charact. 13(4), 3119–3129 (2019).
    https://doi.org/10.1007/s11694-019-00234-0 Article   Google Scholar   Y. He, Q.
    Xiao, X. Bai, L. Zhou, F. Liu, C. Zhang, Recent progress of nondestructive techniques
    for fruits damage inspection: a review. Crit. Rev. Food Sci. Nutr. 62(20), 1–19
    (2021). https://doi.org/10.1080/10408398.2021.1885342 Article   Google Scholar   M.
    Kamruzzaman, D.-W. Sun, Introduction to hyperspectral imaging technology, in Computer
    Vision Technology for Food Quality Evaluation, 2nd edn., ed. by D.W. Sun (Academic
    Press, London, 2016), pp.111–139 Chapter   Google Scholar   L. Zhou, C. Zhang,
    F. Liu, Z. Qiu, Y. He, Application of deep learning in food: a review. Compr.
    Rev. Food Sci. Food Saf. 18(6), 1793–1811 (2019). https://doi.org/10.1111/1541-4337.12492
    Article   PubMed   Google Scholar   J.-L. Li, D.-W. Sun, J.-H. Cheng, Recent advances
    in nondestructive analytical techniques for determining the total soluble solids
    in fruits: a review. Compr. Rev. Food Sci. Food Saf. 15(5), 897–911 (2016). https://doi.org/10.1111/1541-4337.12217
    Article   PubMed   Google Scholar   Q. Dai, D.-W. Sun, Z. Xiong, J.-H. Cheng,
    X.-A. Zeng, Recent advances in data mining techniques and their applications in
    hyperspectral image processing for the food industry. Compr. Rev. Food Sci. Food
    Saf. 13(5), 891–905 (2014). https://doi.org/10.1111/1541-4337.12088 Article   Google
    Scholar   J. Zhang, T. Cheng, W. Guo, X. Xu, H. Qiao, Y. Xie, X. Ma, Leaf area
    index estimation model for UAV image hyperspectral data based on wavelength variable
    selection and machine learning methods. Plant Methods (2021). https://doi.org/10.1186/s13007-021-00750-5
    Article   PubMed   PubMed Central   Google Scholar   J.-H. Cheng, B. Nicolai,
    D.-W. Sun, Hyperspectral imaging with multivariate analysis for technological
    parameters prediction and classification of muscle foods: a review. Meat Sci.
    123, 182–191 (2017). https://doi.org/10.1016/j.meatsci.2016.09.017 Article   PubMed   Google
    Scholar   G. Lu, D. Wang, X. Qin, L. Halig, S. Muller, H. Zhang, A. Chen, B.W.
    Pogue, Z.G. Chen, B. Fei, Framework for hyperspectral image processing and quantification
    for cancer detection during animal tumor surgery. J. Biomed. Opt. 20(12), 126012–126012
    (2015). https://doi.org/10.1117/1.JBO.20.12.126012 Article   PubMed   PubMed Central   Google
    Scholar   G. Özdoğan, X. Lin, D.-W. Sun, Rapid and noninvasive sensory analyses
    of food products by hyperspectral imaging: Recent application developments. Trends
    Food Sci. Technol. 111(2), 151–165 (2021). https://doi.org/10.1016/j.tifs.2021.02.044
    Article   CAS   Google Scholar   G. Hughes, On the mean accuracy of statistical
    pattern recognizers. IEEE Trans. Inf. Theory 14(1), 55–63 (1968). https://doi.org/10.1109/TIT.1968.1054102
    Article   Google Scholar   D. Wu, D.-W. Sun, Advanced applications of hyperspectral
    imaging technology for food quality and safety analysis and assessment: a review—part
    I: fundamentals. Innov. Food Sci. Emerg. Technol. 19, 1–14 (2013). https://doi.org/10.1016/j.ifset.2013.04.014
    Article   CAS   Google Scholar   X. Lin, J.-L. Xu, D.-W. Sun, Evaluating drying
    feature differences between ginger slices and splits during microwave-vacuum drying
    by hyperspectral imaging technique. Food Chem. (2020). https://doi.org/10.1016/j.foodchem.2020.127407
    Article   PubMed   Google Scholar   J.-H. Cheng, J.H. Qu, D.-W. Sun, X.A. Zeng,
    Visible/near-infrared hyperspectral imaging prediction of textural firmness of
    grass carp (Ctenopharyngodon idella) as affected by frozen storage. Food Res.
    Int. 56, 190–198 (2014). https://doi.org/10.1016/j.foodres.2013.12.009 Article   Google
    Scholar   A. Hennessy, K. Clarke, M. Lewis, Hyperspectral classification of plants:
    a review of waveband selection generalisability. Remote Sens. (2020). https://doi.org/10.3390/RS12010113
    Article   Google Scholar   H. Pu, M. Kamruzzaman, D.-W. Sun, Selection of feature
    wavelengths for developing multispectral imaging systems for quality, safety and
    authenticity of muscle foods-a review. Trends Food Sci. Technol. 45(1), 86–104
    (2015). https://doi.org/10.1016/j.tifs.2015.05.006 Article   CAS   Google Scholar   T.-T.
    Pan, D.-W. Sun, J.-H. Cheng, H. Pu, Regression algorithms in hyperspectral data
    analysis for meat quality detection and evaluation. Compr. Rev. Food Sci. Food
    Saf. 15(3), 529–541 (2016). https://doi.org/10.1111/1541-4337.12191 Article   PubMed   Google
    Scholar   J.-H. Cheng, D.-W. Sun, Hyperspectral imaging as an effective tool for
    quality analysis and control of fish and other seafoods: current research and
    potential applications. Trends Food Sci. Technol. 37(2), 78–91 (2014). https://doi.org/10.1016/j.tifs.2014.03.006
    Article   CAS   Google Scholar   S. Ghidini, M.O. Varrà, E. Zanardi, Approaching
    authenticity issues in fish and seafood products by qualitative spectroscopy and
    chemometrics. Molecules 24(9), 1812 (2019) Article   CAS   PubMed   PubMed Central   Google
    Scholar   H. Wang, J. Peng, C. Xie, Y. Bao, Y. He, Fruit quality evaluation using
    spectroscopy technology: a review. Sensors (Switzerland) 15(5), 11889–11927 (2015).
    https://doi.org/10.3390/s150511889 Article   Google Scholar   W.-H. Su, D.-W.
    Sun, Multispectral imaging for plant food quality analysis and visualization.
    Compr. Rev. Food Sci. Food Saf. 17(1), 220–239 (2018). https://doi.org/10.1111/1541-4337.12317
    Article   PubMed   Google Scholar   D. Saha, A. Manickavasagan, Machine learning
    techniques for analysis of hyperspectral images to determine quality of food products:
    a review. Current Res. Food Sci. 4, 28–44 (2021). https://doi.org/10.1016/j.crfs.2021.01.002
    Article   CAS   Google Scholar   K. Wang, H. Pu, D.-W. Sun, Emerging spectroscopic
    and spectral imaging techniques for the rapid detection of microorganisms: an
    overview. Compr. Rev. Food Sci. Food Saf. 17(2), 256–273 (2018). https://doi.org/10.1111/1541-4337.12323
    Article   PubMed   Google Scholar   D. Liu, D.-W. Sun, X.-A. Zeng, Recent advances
    in wavelength selection techniques for hyperspectral image processing in the food
    industry. Food Bioprocess Technol. 7(2), 307–323 (2014). https://doi.org/10.1007/s11947-013-1193-6
    Article   Google Scholar   T. Mehmood, K.H. Liland, L. Snipen, S. Sæbø, A review
    of variable selection methods in partial least squares regression. Chemom. Intell.
    Lab. Syst. 118, 62–69 (2012). https://doi.org/10.1016/j.chemolab.2012.07.010 Article   CAS   Google
    Scholar   J.-H. Cheng, D.-W. Sun, Data fusion and hyperspectral imaging in tandem
    with least squares-support vector machine for prediction of sensory quality index
    scores of fish fillet. LWT 63(2), 892–898 (2015). https://doi.org/10.1016/j.lwt.2015.04.039
    Article   CAS   Google Scholar   M.J. Page, D. Moher, P.M. Bossuyt, I. Boutron,
    T.C. Hoffmann, C.D. Mulrow, L. Shamseer, J.M. Tetzlaff, E.A. Akl, S.E. Brennan,
    R. Chou, J. Glanville, J.M. Grimshaw, A. Hróbjartsson, M.M. Lalu, T. Li, E.W.
    Loder, E. Mayo-Wilson, S. McDonald, L.A. McGuinness, L.A. Stewart, J. Thomas,
    A.C. Tricco, V.A. Welch, P. Whiting, J.E. McKenzie, Prisma 2020 explanation and
    elaboration: updated guidance and exemplars for reporting systematic reviews.
    BMJ (2021). https://doi.org/10.1136/bmj.n160 Article   PubMed   PubMed Central   Google
    Scholar   M.E. Falagas, E.I. Pitsouni, G.A. Malietzis, G. Pappas, Comparison of
    pubmed, scopus, web of science, and google scholar: strengths and weaknesses.
    FASEB J. 22(2), 338–342 (2008). https://doi.org/10.1096/fj.07-9492LSF Article   CAS   PubMed   Google
    Scholar   R. Tibshirani, Regression shrinkage and selection via the lasso. J.
    R. Stat. Soc. 58(1), 267–288 (1996) Google Scholar   M.C.U. Araújo, T.C.B. Saldanha,
    R.K.H. Galvão, T. Yoneyama, H.C. Chame, V. Visani, The successive projections
    algorithm for variable selection in spectroscopic multicomponent analysis. Chemom.
    Intell. Lab. Syst. 57(2), 65–73 (2001). https://doi.org/10.1016/S0169-7439(01)00119-8
    Article   Google Scholar   H. Li, Y. Liang, Q. Xu, D. Cao, Key wavelengths screening
    using competitive adaptive reweighted sampling method for multivariate calibration.
    Anal. Chim. Acta 648(1), 77–84 (2009). https://doi.org/10.1016/j.aca.2009.06.046
    Article   CAS   PubMed   Google Scholar   H. Jiang, S.-C. Yoon, H. Zhuang, W.
    Wang, K.C. Lawrence, Y. Yang, Tenderness classification of fresh broiler breast
    fillets using visible and near-infrared hyperspectral imaging. Meat Sci. 139,
    82–90 (2018). https://doi.org/10.1016/j.meatsci.2018.01.013 Article   PubMed   Google
    Scholar   Q. Li, F.K. Kit Wong, T. Fung, Comparison feature selection methods
    for subtropical vegetation classification with hyperspectral data. International
    Geoscience and Remote Sensing Symposium (IGARSS) 3693–3696 (2019). https://doi.org/10.1109/IGARSS.2019.8898541
    J. Tschannerl, J. Ren, J. Zabalza, S. Marshall, Segmented autoencoders for unsupervised
    embedded hyperspectral band selection. Proceedings - European Workshop on Visual
    Information Processing, EUVIP 2018-November (2019). https://doi.org/10.1109/EUVIP.2018.8611643
    J. Zhang, L. Dai, F. Cheng, Classification of frozen corn seeds using hyperspectral
    Vis/NIR reflectance imaging. Molecules (2019). https://doi.org/10.3390/molecules24010149
    Article   PubMed   PubMed Central   Google Scholar   Y. Zhao, S. Zhu, C. Zhang,
    X. Feng, L. Feng, Y. He, Application of hyperspectral imaging and chemometrics
    for variety classification of maize seeds. RSC Adv. 8(3), 1337–1345 (2018). https://doi.org/10.1039/c7ra05954j
    Article   CAS   PubMed   PubMed Central   Google Scholar   S. Zhu, L. Zhou, P.
    Gao, Y. Bao, Y. He, L. Feng, Near-infrared hyperspectral imaging combined with
    deep learning to identify cotton seed varieties. Molecules (2019). https://doi.org/10.3390/molecules24183268
    Article   PubMed   PubMed Central   Google Scholar   N. Wu, Y. Zhang, R. Na, C.
    Mi, S. Zhu, Y. He, C. Zhang, Variety identification of oat seeds using hyperspectral
    imaging: Investigating the representation ability of deep convolutional neural
    network. RSC Adv. 9(22), 12635–12644 (2019). https://doi.org/10.1039/c8ra10335f
    Article   CAS   PubMed   PubMed Central   Google Scholar   Z. Gao, L.R. Khot,
    R.A. Naidu, Q. Zhang, Early detection of grapevine leafroll disease in a red-berried
    wine grape cultivar using hyperspectral imaging. Comput. Electron. Agric. (2020).
    https://doi.org/10.1016/j.compag.2020.105807 Article   Google Scholar   S.R. Delwiche,
    I.T. Rodriguez, S.R. Rausch, R.A. Graybosch, Estimating percentages of fusarium-damaged
    kernels in hard wheat by near-infrared hyperspectral imaging. J. Cereal Sci. 87,
    18–24 (2019). https://doi.org/10.1016/j.jcs.2019.02.008 Article   Google Scholar   B.-H.
    Zhang, W.-Q. Huang, J.-B. Li, C.-J. Zhao, C.-L. Liu, D.-F. Huang, L. Gong, Detection
    of slight bruises on apples based on hyperspectral imaging and MNF transform.
    Spectrosc. Spectral Anal. 34(5), 1367–1372 (2014). https://doi.org/10.3964/j.issn.1000-0593(2014)05-1367-06
    Article   CAS   Google Scholar   D.F. Barbin, D.-W. Sun, C. Su, NIR hyperspectral
    imaging as non-destructive evaluation tool for the recognition of fresh and frozen-thawed
    porcine longissimus dorsi muscles. Innov. Food Sci. Emerg. Technol. 18, 226–236
    (2013). https://doi.org/10.1016/j.ifset.2012.12.011 Article   CAS   Google Scholar   C.Q.
    Xie, X.L. Li, P.C. Nie, Y. He, Application of time series hyperspectral imaging
    (TS-HSI) for determining water content within tea leaves during drying. Trans.
    ASABE 56(6), 1431–1440 (2013). https://doi.org/10.13031/trans.56.10243 Article   Google
    Scholar   J. Long, J. Yang, J. Peng, L. Pan, K. Tu, Detection of moisture and
    carotenoid content in carrot slices during hot air drying based on multispectral
    imaging equipment with selected wavelengths. Int. J. Food Eng. 17(9), 727–735
    (2021). https://doi.org/10.1515/ijfe-2021-0127 Article   CAS   Google Scholar   A.
    Iqbal, D.-W. Sun, P. Allen, Prediction of moisture, color and pH in cooked, pre-sliced
    turkey hams by NIR hyperspectral imaging system. J. Food Eng. 117(1), 42–51 (2013).
    https://doi.org/10.1016/j.jfoodeng.2013.02.001 Article   CAS   Google Scholar   S.
    Wang, A.K. Das, J. Pang, P. Liang, Artificial intelligence empowered multispectral
    vision based system for non-contact monitoring of large yellow croaker (Larimichthys
    crocea) fillets. Foods (2021). https://doi.org/10.3390/foods10061161 Article   PubMed   PubMed
    Central   Google Scholar   H. Jiang, S.-C. Yoon, H. Zhuang, W. Wang, Y. Li, Y.
    Yang, Integration of spectral and textural features of visible and near-infrared
    hyperspectral imaging for differentiating between normal and white striping broiler
    breast meat. Spectrochim. Acta Part A 213, 118–126 (2019). https://doi.org/10.1016/j.saa.2019.01.052
    Article   CAS   Google Scholar   H. Wang, R. Hu, M. Zhang, Z. Zhai, R. Zhang,
    Identification of tomatoes with early decay using visible and near infrared hyperspectral
    imaging and image-spectrum merging technique. J. Food Process Eng. (2021). https://doi.org/10.1111/jfpe.13654
    Article   Google Scholar   W. Tan, L. Sun, F. Yang, W. Che, D. Ye, D. Zhang, B.
    Zou, The feasibility of early detection and grading of apple bruises using hyperspectral
    imaging: Early detection and grading of apple bruises. J. Chemom. 32(10), 3067
    (2018). https://doi.org/10.1002/cem.3067 Article   CAS   Google Scholar   C.-H.
    Feng, Y. Makino, M. Yoshimura, F.J. Rodríguez-Pulido, Real-time prediction of
    pre-cooked japanese sausage color with different storage days using hyperspectral
    imaging. J. Sci. Food Agric. 98(7), 2564–2572 (2018). https://doi.org/10.1002/jsfa.8746
    Article   CAS   PubMed   Google Scholar   D.E. Goldberg, J.H. Holland, Genetic
    algorithms and machine learning. Mach. Learn. 3(2), 95–99 (1988). https://doi.org/10.1023/A:1022602019183
    Article   Google Scholar   H.-J. He, Y. Chen, G. Li, Y. Wang, X. Ou, J. Guo, Hyperspectral
    imaging combined with chemometrics for rapid detection of talcum powder adulterated
    in wheat flour. Food Control (2023). https://doi.org/10.1016/j.foodcont.2022.109378
    Article   Google Scholar   R. Qiu, Y. Zhao, D. Kong, N. Wu, Y. He, Development
    and comparison of classification models on Vis-NIR hyperspectral imaging spectra
    for qualitative detection of the Staphylococcus aureus in fresh chicken breast.
    Spectrochim. Acta A285, 121838 (2023) Article   Google Scholar   X. Li, M. Cai,
    M. Li, X. Wei, Z. Liu, J. Wang, K. Jia, Y. Han, Combining Vis-NIR and NIR hyperspectral
    imaging techniques with a data fusion strategy for the rapid qualitative evaluation
    of multiple qualities in chicken. Food Control (2023). https://doi.org/10.1016/j.foodcont.2022.109416
    Article   PubMed   Google Scholar   Y. Li, Y. Yin, H. Yu, Y. Yuan, Fast detection
    of water loss and hardness for cucumber using hyperspectral imaging technology.
    J. Food Meas. Charact. 16(1), 76–84 (2022). https://doi.org/10.1007/s11694-021-01130-2
    Article   Google Scholar   S. Wold, M. Sjöström, L. Eriksson, PLS-regression:
    a basic tool of chemometrics. Chemom. Intell. Lab. Syst. 58, 109–130 (2001) Article   CAS   Google
    Scholar   Y. Wang, H. He, S. Jiang et al., Nondestructive determination of IMP
    content in chilled chicken based on hyperspectral data combined with chemometrics.
    Int. J. Agric. Biol. Eng. 15(1), 277–284 (2022). https://doi.org/10.25165/j.ijabe.20221501.6612
    Article   Google Scholar   Z. Yuan, Y. Ye, L. Wei, X. Yang, C. Huang, Study on
    the optimization of hyperspectral characteristic bands combined with monitoring
    and visualization of pepper leaf spad value. Sensors (2022). https://doi.org/10.3390/s22010183
    Article   PubMed   PubMed Central   Google Scholar   Q. Thien Pham, N.-S. Liou,
    The development of on-line surface defect detection system for jujubes based on
    hyperspectral images. Comput. Electron. Agric. (2022). https://doi.org/10.1016/j.compag.2022.106743
    Article   Google Scholar   M. Shiddiq, H. Herman, D.S. Arief, E. Fitra, I.R. Husein,
    S.A. Ningsih, Wavelength selection of multispectral imaging for oil palm fresh
    fruit ripeness classification. Appl. Opt. 61(17), 5289–5298 (2022). https://doi.org/10.1364/AO.450384
    Article   PubMed   Google Scholar   B. Li, F. Zhang, Y. Liu, H. Yin, J. Zou, A.
    Ou-yang, Quantitative study on impact damage of yellow peach based on hyperspectral
    image information combined with spectral information. J. Mol. Struct. (2023).
    https://doi.org/10.1016/j.molstruc.2022.134176 Article   Google Scholar   S. Sharma,
    K.C. Sumesh, P. Sirisomboon, Rapid ripening stage classification and dry matter
    prediction of durian pulp using a pushbroom near infrared hyperspectral imaging
    system. Meas. J. Int. Meas. Confeder. (2022). https://doi.org/10.1016/j.measurement.2021.110464
    Article   Google Scholar   V. Centner, D.-L. Massart, O.E. Noord, S. Jong, B.M.
    Vandeginste, C. Sterna, Elimination of uninformative variables for multivariate
    calibration. Anal. Chem. 68(21), 3851–3858 (1996). https://doi.org/10.1021/ac960321m
    Article   CAS   PubMed   Google Scholar   B. Li, F. Zhang, Y. Liu, H. Yin, J.
    Zou, A. Ou-Yang, Quantitative study of impact damage on yellow peaches based on
    reflectance, absorbance and kubelka-munk spectral data. RSC Adv. 12(43), 28152–28170
    (2022). https://doi.org/10.1039/d2ra04635k Article   CAS   PubMed   PubMed Central   Google
    Scholar   P. Xu, Y. Zhang, Q. Tan, K. Xu, W. Sun, J. Xing, R. Yang, Vigor identification
    of maize seeds by using hyperspectral imaging combined with multivariate data
    analysis. Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104361
    Article   PubMed   PubMed Central   Google Scholar   H.-D. Li, Q.-S. Xu, Y.-Z.
    Liang, Random frog: an efficient reversible jump Markov chain Monte Carlo-like
    approach for variable selection with applications to gene selection and disease
    classification. Anal. Chim. Acta 740, 20–26 (2012). https://doi.org/10.1016/j.aca.2012.06.031
    Article   CAS   PubMed   Google Scholar   C. Liu, Z. Chu, S. Weng, G. Zhu, K.
    Han, Z. Zhang, L. Huang, Z. Zhu, S. Zheng, Fusion of electronic nose and hyperspectral
    imaging for mutton freshness detection using input-modified convolution neural
    network. Food Chem. (2022). https://doi.org/10.1016/j.foodchem.2022.132651 Article   PubMed   Google
    Scholar   T. Cheng, P. Li, J. Ma, X. Tian, N. Zhong, Identification of four chicken
    breeds by hyperspectral imaging combined with chemometrics. Processes (2022).
    https://doi.org/10.3390/pr10081484 Article   Google Scholar   G. Xuan, C. Gao,
    Y. Shao, Spectral and image analysis of hyperspectral data for internal and external
    quality assessment of peach fruit. Spectrochim. Acta Part A (2022). https://doi.org/10.1016/j.saa.2022.121016
    Article   Google Scholar   M. Kamruzzaman, D. Kalita, M.T. Ahmed, G. ElMasry,
    Y. Makino, Effect of variable selection algorithms on model performance for predicting
    moisture content in biological materials using spectral data. Anal. Chim.Acta
    (2022). https://doi.org/10.1016/j.aca.2021.339390 Article   PubMed   Google Scholar   H.
    Song, S.-R. Yoon, Y.-M. Dang, J.-S. Yang, I.M. Hwang, J.-H. Ha, Nondestructive
    classification of soft rot disease in napa cabbage using hyperspectral imaging
    analysis. Sci. Rep. (2022). https://doi.org/10.1038/s41598-022-19169-6 Article   PubMed   PubMed
    Central   Google Scholar   RMd. Saleh, B. Kulig, A. Arefi, O. Hensel, B. Sturm,
    Prediction of total carotenoids, color, and moisture content of carrot slices
    during hot air drying using non-invasive hyperspectral imaging technique. J. Food
    Process. Preserv. (2022). https://doi.org/10.1111/jfpp.16460 Article   Google
    Scholar   R. Yuan, G. Liu, J. He, G. Wan, N. Fan, Y. Li, Y. Sun, Classification
    of lingwu long jujube internal bruise over time based on visible near-infrared
    hyperspectral imaging combined with partial least squares-discriminant analysis.
    Comput. Electron. Agric. (2021). https://doi.org/10.1016/j.compag.2021.106043
    Article   Google Scholar   M. Gabrielli, V. LançSon-Verdier, P. Picouet, C. Maury,
    Hyperspectral imaging to characterize table grapes. Chemosensors (2021). https://doi.org/10.3390/chemosensors9040071
    Article   Google Scholar   T. Wu, J. Yu, J. Lu, X. Zou, W. Zhang, Research on
    inversion model of cultivated soil moisture content based on hyperspectral imaging
    analysis. Agriculture (Switzerland) 10(7), 1–14 (2020). https://doi.org/10.3390/agriculture10070292
    Article   Google Scholar   Y.-H. Yun, W.-T. Wang, B.-C. Deng, G.-B. Lai, X.-B.
    Liu, D.-B. Ren, Y.-Z. Liang, W. Fan, Q.-S. Xu, Using variable combination population
    analysis for variable selection in multivariate calibration. Anal. Chim. Acta
    862, 14–23 (2015). https://doi.org/10.1016/j.aca.2014.12.048 Article   CAS   PubMed   Google
    Scholar   L. Shi, L. Li, F. Zhang, Y. Lin, Nondestructive detection of panax notoginseng
    saponins by using hyperspectral imaging. Int. J. Food Sci. Technol. 57(7), 4537–4546
    (2022). https://doi.org/10.1111/ijfs.15790 Article   CAS   Google Scholar   Z.
    Guo, J. Zhang, C. Ma, X. Yin, Y. Guo, X. Sun, C. Jin, Application of visible-near-infrared
    hyperspectral imaging technology coupled with wavelength selection algorithm for
    rapid determination of moisture content of soybean seeds. J. Food Compos. Anal.
    (2023). https://doi.org/10.1016/j.jfca.2022.105048 Article   Google Scholar   P.
    Zhang, H. Ji, H. Wang, Y. Liu, X. Zhang, C. Ren, Quantitative evaluation of impact
    damage to apples using NIR hyperspectral imaging. Int. J. Food Prop. 24(1), 457–470
    (2021). https://doi.org/10.1080/10942912.2021.1900240 Article   CAS   Google Scholar   J.
    Onmankhong, T. Ma, T. Inagaki, P. Sirisomboon, S. Tsuchikawa, Cognitive spectroscopy
    for the classification of rice varieties: a comparison of machine learning and
    deep learning approaches in analysing long-wave near-infrared hyperspectral images
    of brown and milled samples. Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104100
    Article   Google Scholar   J. Wang, L. Yan, F. Wang, S. Qi, SVM classification
    method of waxy corn seeds with different vitality levels based on hyperspectral
    imaging. J. Sens. (2022). https://doi.org/10.1155/2022/4379317 Article   Google
    Scholar   Y.-H. Yun, W.-T. Wang, M.-L. Tan, Y.-Z. Liang, H.-D. Li, D.-S. Cao,
    H.-M. Lu, Q.-S. Xu, A strategy that iteratively retains informative variables
    for selecting optimal variable subset in multivariate calibration. Anal. Chim.
    Acta 807, 36–43 (2014). https://doi.org/10.1016/j.aca.2013.11.032 Article   CAS   PubMed   Google
    Scholar   D. Saha, T. Senthilkumar, S. Sharma, C.B. Singh, A. Manickavasagan,
    Application of near-infrared hyperspectral imaging coupled with chemometrics for
    rapid and non-destructive prediction of protein content in single chickpea seed.
    J. Food Compos. Anal. (2023). https://doi.org/10.1016/j.jfca.2022.104938 Article   Google
    Scholar   Z. Sun, H. Pan, M. Zuo, J. Li, L. Liang, C.-T. Ho, X. Zou, Non-destructive
    assessment of equivalent umami concentrations in salmon using hyperspectral imaging
    technology combined with multivariate algorithms. Spectrochim. Acta Part A (2023).
    https://doi.org/10.1016/j.saa.2022.121890 Article   Google Scholar   L. Nørregard,
    A. Saudland, J. Wagner, J.P. Nielsen, L. Munck, S.B. Engelsen, Interval partial
    least-squares regression (iPLS): a comparative chemometric study with an example
    from near-infrared spectroscopy. Appl. Spectrosc. 54(3), 413–419 (2000). https://doi.org/10.1366/0003702001949500
    Article   Google Scholar   J. Florián-Huamán, J.P. Cruz-Tirado, D. FernandesBarbin,
    R. Siche, Detection of nutshells in cumin powder using NIR hyperspectral imaging
    and chemometrics tools. J. Food Compos. Anal. (2022). https://doi.org/10.1016/j.jfca.2022.104407
    Article   Google Scholar   A. López-Maestresalas, C. Lopez-Molina, G.A. Oliva-Lobo,
    C. Jarén, J.I. Galarreta, C.M. Peraza-Alemán, S. Arazuri, Evaluation of near-infrared
    hyperspectral imaging for the assessment of potato processing aptitude. Front.
    Nutr. (2022). https://doi.org/10.3389/fnut.2022.999877 Article   PubMed   PubMed
    Central   Google Scholar   G. Kim, H. Lee, I. Baek, B.-K. Cho, M.S. Kim, Quantitative
    detection of benzoyl peroxide in wheat flour using line-scan short-wave infrared
    hyperspectral imaging. Sens. Actuators B (2022). https://doi.org/10.1016/j.snb.2021.130997
    Article   Google Scholar   B. Wang, J. He, S. Zhang, L. Li, Nondestructive prediction
    and visualization of total flavonoids content in Cerasus humilis fruit during
    storage periods based on hyperspectral imaging technique. J. Food Process. Eng.
    (2021). https://doi.org/10.1111/jfpe.13807 Article   Google Scholar   M.M.A. Chaudhry,
    M.L. Amodio, J.M. Amigo, M.L.V. Chiara, F. Babellahi, G. Colelli, Feasibility
    study for the surface prediction and mapping of phytonutrients in minimally processed
    rocket leaves (Diplotaxis tenuifolia) during storage by hyperspectral imaging.
    Comput. Electron. Agric. (2020). https://doi.org/10.1016/j.compag.2020.105575
    Article   Google Scholar   X. Ye, S. Abe, S. Zhang, Estimation and mapping of
    nitrogen content in apple trees at leaf and canopy levels using hyperspectral
    imaging. Precision Agric. 21(1), 198–225 (2020). https://doi.org/10.1007/s11119-019-09661-x
    Article   Google Scholar   K. Song, S.-H. Wang, D. Yang, T.-Y. Shi, Combination
    of spectral and image information from hyperspectral imaging for the prediction
    and visualization of the total volatile basic nitrogen content in cooked beef.
    J. Food Meas. Charact. 15(5), 4006–4020 (2021). https://doi.org/10.1007/s11694-021-00983-x
    Article   Google Scholar   H. Jiang, Y. Hu, X. Jiang, H. Zhou, Maturity stage
    discrimination of Camellia oleifera fruit using visible and near-infrared hyperspectral
    imaging. Molecules (2022). https://doi.org/10.3390/molecules27196318 Article   PubMed   PubMed
    Central   Google Scholar   D. Fu, Q. Wang, M. Ma, Nondestructive detection of
    egg freshness fusion index during storage based on hyperspectral imaging. ACM
    Int. Conf. Proc. Ser. (2020). https://doi.org/10.1145/3453187.3453379 Article   Google
    Scholar   X. Zheng, Y. Li, W. Wei, Y. Peng, Detection of adulteration with duck
    meat in minced lamb meat by using visible near-infrared hyperspectral imaging.
    Meat Sci. 149, 55–62 (2019). https://doi.org/10.1016/j.meatsci.2018.11.005 Article   CAS   PubMed   Google
    Scholar   A.M. Rady, D.E. Guyer, I.R. Donis-González, W. Kirk, N.J. Watson, A
    comparison of different optical instruments and machine learning techniques to
    identify sprouting activity in potatoes during storage. J. Food Meas. Charact.
    14(6), 3565–3579 (2020). https://doi.org/10.1007/s11694-020-00590-2 Article   Google
    Scholar   N. Ekramirad, A.Y. Khaled, L.E. Doyle, J.R. Loeb, K.D. Donohue, R.T.
    Villanueva, A.A. Adedeji, Nondestructive detection of codling moth infestation
    in apples using pixel-based NIR hyperspectral imaging with machine learning and
    feature selection. Foods (2022). https://doi.org/10.3390/foods11010008 Article   Google
    Scholar   I. Baek, C. Mo, C. Eggleton, S.A. Gadsden, B.-K. Cho, J. Qin, D.E. Chan,
    M.S. Kim, Determination of spectral resolutions for multispectral detection of
    apple bruises using visible/near-infrared hyperspectral reflectance imaging. Front.
    Plant Sci. (2022). https://doi.org/10.3389/fpls.2022.963591 Article   PubMed   PubMed
    Central   Google Scholar   B.-C. Deng, Y.-H. Yun, P. Ma, C.-C. Lin, D.-B. Ren,
    Y.-Z. Liang, A new method for wavelength interval selection that intelligently
    optimizes the locations, widths and combinations of the intervals. Analyst 140,
    1876–1885 (2015). https://doi.org/10.1039/C4AN02123A Article   CAS   PubMed   Google
    Scholar   J. Hao, F. Dong, Y. Li, S. Wang, J. Cui, Z. Zhang, K. Wu, Investigation
    of the data fusion of spectral and textural data from hyperspectral imaging for
    the near geographical origin discrimination of wolfberries using 2D-CNN algorithms.
    Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104286
    Article   Google Scholar   Y.H. Yun, H.D. Li, L.R. Wood, W. Fan, J.J. Wang, D.S.
    Cao, Q.S. Xu, Y.Z. Liang, An efficient method of wavelength interval selection
    based on random frog for multivariate spectral calibration. Spectrochim. Acta
    Part A. 111, 31–6 (2013). https://doi.org/10.1016/j.saa.2013.03.083 Article   CAS   Google
    Scholar   I. Noda, Recent advancement in the field of two-dimensional correlation
    spectroscopy. J. Mol. Struct. (2008). https://doi.org/10.1016/j.molstruc.2007.11.038
    Article   Google Scholar   H. Jiang, X. Jiang, Y. Ru, Q. Chen, J. Wang, L. Xu,
    H. Zhou, Detection and visualization of soybean protein powder in ground beef
    using visible and near-infrared hyperspectral imaging. Infrared Phys. Technol.
    (2022). https://doi.org/10.1016/j.infrared.2022.104401 Article   Google Scholar   H.
    Jiang, X. Jiang, Y. Ru, Q. Chen, X. Li, L. Xu, H. Zhou, M. Shi, Rapid and non-destructive
    detection of natural mildew degree of postharvest camellia oleifera fruit based
    on hyperspectral imaging. Infrared Phys. Technol. (2022). https://doi.org/10.1016/j.infrared.2022.104169
    Article   Google Scholar   W. Cai, Y. Li, X. Shao, A variable selection method
    based on uninformative variable elimination for multivariate calibration of near-infrared
    spectra. Chemom. Intell. Lab. Syst. 90(2), 188–194 (2008). https://doi.org/10.1016/j.chemolab.2007.10.001
    Article   CAS   Google Scholar   B. Sturm, S. Raut, B. Kulig, J. Münsterer, K.
    Kammhuber, O. Hensel, S.O.J. Crichton, In-process investigation of the dynamics
    in drying behavior and quality development of hops using visual and environmental
    sensors combined with chemometrics. Comput. Electron. Agric. 175, 96 (2020). https://doi.org/10.1016/j.compag.2020.105547
    Article   Google Scholar   X. Wei, J. He, S. Zheng, D. Ye, Modeling for SSC and
    firmness detection of persimmon based on NIR hyperspectral imaging by sample partitioning
    and variables selection. Infrared Phys. Technol. (2020). https://doi.org/10.1016/j.infrared.2019.103099
    Article   Google Scholar   B. Li, Z. Han, Q. Wang, A. Yang, Y. Liu, Detection
    of skin defects in loquats based on grayscale features combined with reflectance,
    absorbance, and kubelka-munk spectra. J. Chemometr. (2022). https://doi.org/10.1002/cem.3449
    Article   Google Scholar   B.-C. Deng, Y.-H. Yun, Y.-Z. Liang, L.-z Yi, A novel
    variable selection approach that iteratively optimizes variable space using weighted
    binary matrix sampling. Analyst 139, 4836–4845 (2014). https://doi.org/10.1039/C4AN00730A
    Article   CAS   PubMed   Google Scholar   L. Fu, J. Sun, S. Wang, M. Xu, K. Yao,
    X. Zhou, Nondestructive evaluation of Zn content in rape leaves using MSSAE and
    hyperspectral imaging. Spectrochim. Acta Part A (2022). https://doi.org/10.1016/j.saa.2022.121641
    Article   Google Scholar   Y. Wang, Y. Zhang, Y. Yuan, Y. Zhao, J. Nie, T. Nan,
    L. Huang, J. Yang, Nutrient content prediction and geographical origin identification
    of red raspberry fruits by combining hyperspectral imaging with chemometrics.
    Front. Nutr. (2022). https://doi.org/10.3389/fnut.2022.980095 Article   PubMed   PubMed
    Central   Google Scholar   T. An, S. Yu, W. Huang, G. Li, X. Tian, S. Fan, C.
    Dong, C. Zhao, Robustness and accuracy evaluation of moisture prediction model
    for black tea withering process using hyperspectral imaging. Spectrochim. Acta
    Part A (2022). https://doi.org/10.1016/j.saa.2021.120791 Article   Google Scholar   C.X.
    Garzon-Lopez, E. Lasso, Species classification in a tropical alpine ecosystem
    using UAV-borne RGB and hyperspectral imagery. Drones 4(4), 1–18 (2020). https://doi.org/10.3390/drones4040069
    Article   Google Scholar   S. Chang, U. Lee, J.-B. Kim, Y.D. Jo, Application of
    3D-volumetric analysis and hyperspectral imaging systems for investigation of
    heterosis and cytoplasmic effects in pepper. Sci. Horticult. (2022). https://doi.org/10.1016/j.scienta.2022.111150
    Article   Google Scholar   I. Guyon, J. Weston, S. Barnhill, V. Vapnik, Gene selection
    for cancer classification using support vector machines. Mach. Learn. 46(1), 389–422
    (2002). https://doi.org/10.1023/A:1012487302797 Article   Google Scholar   A.
    Viinikka, P. Hurskainen, S. Keski-Saari, S. Kivinen, T. Tanhuanpää, J. Mäyrä,
    L. Poikolainen, P. Vihervaara, T. Kumpula, Detecting European aspen (Populus tremula,
    L.) in boreal forests using airborne hyperspectral and airborne laser scanning
    data. Remote Sens. (2020). https://doi.org/10.3390/RS12162610 Article   Google
    Scholar   J. Mohite, S. Sawant, R. Agarwal, A. Pandit, S. Pappula, Detection of
    crop water stress in maize using drone based hyperspectral imaging. Int. Geosci.
    Remote Sens. Symp. (IGARSS) 2022–July, 5957–5960 (2022). https://doi.org/10.1109/IGARSS46834.2022.9884686
    Article   Google Scholar   A.U.G. Sankararao, P. Rajalakshmi, S. Kaliamoorthy,
    S. Choudhary, Water stress detection in pearl millet canopy with selected wavebands
    using UAV based hyperspectral imaging and machine learning. 2022 IEEE Sensors
    Applications Symposium (SAS), Sundsvall, Sweden, 1–6 (2022) https://doi.org/10.1109/SAS54819.2022.9881337
    L. Munck, J.P. Nielsen, B. Møller, S. Jacobsen, I. Søndergaard, S.B. Engelsen,
    L. Nørgaard, R. Bro, Exploring the phenotypic expression of a regulatory proteome-altering
    gene by spectroscopy and chemometrics. Anal. Chim. Acta 446(1–2), 171–186 (2001)
    CAS   Google Scholar   J. Shi, W. Chen, X. Zou, Y. Xu, X. Huang, Y. Zhu, T. Shen,
    Detection of triterpene acids distribution in loquat (Eriobotrya japonica) leaf
    using hyperspectral imaging. Spectrochimica Acta - Part A: Molecular and Biomolecular
    Spectroscopy 188, 436–442 (2018). https://doi.org/10.1016/j.saa.2017.07.023 Article   CAS   PubMed   Google
    Scholar   A. Hassanzadeh, F. Zhang, J. Van Aardt, S.P. Murphy, S.J. Pethybridge,
    Broadacre crop yield estimation using imaging spectroscopy from unmanned aerial
    systems (UAS): A field-based case study with snap bean. Remote Sensing 13(16)
    (2021). https://doi.org/10.3390/rs13163241 E. Bonah, X. Huang, J.H. Aheto, R.
    Yi, S. Yu, H. Tu, Comparison of variable selection algorithms on Vis-NIR hyperspectral
    imaging spectra for quantitative monitoring and visualization of bacterial foodborne
    pathogens in fresh pork muscles. Infrared Physics and Technology 107 (2020). https://doi.org/10.1016/j.infrared.2020.103327
    W. Liu, S. Zeng, G. Wu, H. Li, F. Chen, Rice seed purity identification technology
    using hyperspectral image with lasso logistic regression model. Sensors 21(13)
    (2021). https://doi.org/10.3390/s21134384 N.H. Samrat, J.B. Johnson, S. White,
    M. Naiker, P. Brown, A rapid non-destructive hyperspectral imaging data model
    for the prediction of pungent constituents in dried ginger. Foods 11(5) (2022).
    https://doi.org/10.3390/foods11050649 Q. Wang, Y. Liu, Q. Xu, J. Feng, H. Yu,
    Identification of mildew degrees in honeysuckle using hyperspectral imaging combined
    with variable selection. Journal of Food Measurement and Characterization 13(3),
    2157–2166 (2019). https://doi.org/10.1007/s11694-019-00136-1 Article   Google
    Scholar   Q. Wang, Y. Liu, X. Gao, A. Xie, H. Yu, Potential of hyperspectral imaging
    for nondestructive determination of chlorogenic acid content in flos lonicerae.
    Journal of Food Measurement and Characterization 13(4), 2603–2612 (2019). https://doi.org/10.1007/s11694-019-00180-x
    Article   Google Scholar   D. Wu, D.-W. Sun, Advanced applications of hyperspectral
    imaging technology for food quality and safety analysis and assessment: A review
    - part ii: Applications. Innov. Food Sci. Emerg. Technol. 19, 15–28 (2013). https://doi.org/10.1016/j.ifset.2013.04.016
    Article   CAS   Google Scholar   D. Wu, S. Wang, N. Wang, P. Nie, Y. He, D.-W.
    Sun, J. Yao, Application of time series hyperspectral imaging (TS-HSI) for determining
    water distribution within beef and spectral kinetic analysis during dehydration.
    Food Bioprocess Technol. 6(11), 2943–2958 (2013). https://doi.org/10.1007/s11947-012-0928-0
    Article   Google Scholar   C. Cortes, V. Vapnik, Support-vector networks. Machine
    Learning 20(3 N - 1573-0565), 273–297 (1995) https://doi.org/10.1007/BF00994018
    D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning representations by back-propagating
    errors. Nature 323(6088), 533–536 (1986). https://doi.org/10.1038/323533a0 Article   Google
    Scholar   G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: theory
    and applications. Neurocomputing 70(1), 489–501 (2006). I.J. Goodfellow, Y. Bengio,
    A. Courville, Deep Learning. MIT Press, Cambridge, MA, USA (2016). http://www.deeplearningbook.org
    Y. Wang, F. Xiong, Y. Zhang, S. Wang, Y. Yuan, C. Lu, J. Nie, T. Nan, B. Yang,
    L. Huang, J. Yang, Application of hyperspectral imaging assisted with integrated
    deep learning approaches in identifying geographical origins and predicting nutrient
    contents of coix seeds. Food Chemistry 404 (2023). https://doi.org/10.1016/j.foodchem.2022.134503
    B. Jin, H. Qi, L. Jia, Q. Tang, L. Gao, Z. Li, G. Zhao, Determination of viability
    and vigor of naturally-aged rice seeds using hyperspectral imaging with machine
    learning. Infrared Physics and Technology 122 (2022). https://doi.org/10.1016/j.infrared.2022.104097
    Q. Pang, W. Huang, S. Fan, Q. Zhou, Z. Wang, X. Tian, Detection of early bruises
    on apples using hyperspectral imaging combining with YOLOv3 deep learning algorithm.
    Journal of Food Process Engineering 45(2) (2022). https://doi.org/10.1111/jfpe.13952
    T. Ojala, M. Pietikäinen, D. Harwood, A comparative study of texture measures
    with classification based on featured distributions. Pattern Recogn. 29(1), 51–59
    (1996). https://doi.org/10.1016/0031-3203(95)00067-4 Article   Google Scholar   Z.
    Wang, W. Huang, X. Tian, Y. Long, L. Li, S. Fan, Rapid and non-destructive classification
    of new and aged maize seeds using hyperspectral image and chemometric methods.
    Frontiers in Plant Science 13 (2022). https://doi.org/10.3389/fpls.2022.849495
    K. Yao, J. Sun, C. Chen, M. Xu, X. Zhou, Y. Cao, Y. Tian, Non-destructive detection
    of egg qualities based on hyperspectral imaging. Journal of Food Engineering 325
    (2022). https://doi.org/10.1016/j.jfoodeng.2022.111024 R.D. Logan, B. Scherrer,
    J. Senecal, N.S. Walton, A. Peerlinck, J.W. Sheppard, J.A. Shaw, Assessing produce
    freshness using hyperspectral imaging and machine learning. Journal of Applied
    Remote Sensing 15(3) (2021). https://doi.org/10.1117/1.JRS.15.034505 S. Feng,
    Y. Cao, T. Xu, F. Yu, D. Zhao, G. Zhang, Rice leaf blast classification method
    based on fused features and one-dimensional deep convolutional neural network.
    Remote Sensing 13(16) (2021). https://doi.org/10.3390/rs13163207 S. Weng, B. Guo,
    Y. Du, M. Wang, P. Tang, J. Zhao, Feasibility of authenticating mutton geographical
    origin and breed via hyperspectral imaging with effective variables of multiple
    features. Food Anal. Methods 14(4), 834–844 (2021). https://doi.org/10.1007/s12161-020-01940-y
    Article   Google Scholar   C.T. Kucha, L. Liu, M. Ngadi, C. Gariépy, Assessment
    of intramuscular fat quality in pork using hyperspectral imaging. Food Engineering
    Reviews 13(1), 274–289 (2021). https://doi.org/10.1007/s12393-020-09246-9 Article   CAS   Google
    Scholar   M.K. Behera, K.M.S. Kishore, S. Chakravarty, Classification of soil
    and prediction of total nitrogen content present in soil by using hyperspectral
    imaging. Lecture Notes in Networks and Systems 202 LNNS, 337–345 (2021). https://doi.org/10.1007/978-981-16-0695-3_33
    Article   Google Scholar   C. Wang, S. Wang, X. He, L. Wu, Y. Li, J. Guo, Combination
    of spectra and texture data of hyperspectral imaging for prediction and visualization
    of palmitic acid and oleic acid contents in lamb meat. Meat Science 169 (2020).
    https://doi.org/10.1016/j.meatsci.2020.108194 H. Zhang, S. Zhang, Y. Chen, W.
    Luo, Y. Huang, D. Tao, B. Zhan, X. Liu, Non-destructive determination of fat and
    moisture contents in salmon (salmo salar) fillets using near-infrared hyperspectral
    imaging coupled with spectral and textural features. Journal of Food Composition
    and Analysis 92 (2020). https://doi.org/10.1016/j.jfca.2020.103567 D. Zhang, G.
    Chen, H. Zhang, N. Jin, C. Gu, S. Weng, Q. Wang, Y. Chen, Integration of spectroscopy
    and image for identifying fusarium damage in wheat kernels. Spectrochimica Acta
    - Part A: Molecular and Biomolecular Spectroscopy 236 (2020). https://doi.org/10.1016/j.saa.2020.118344
    J.H. Aheto, X. Huang, X. Tian, Y. Ren, B. Ernest, E.A. Alenyorege, C. Dai, T.
    Hongyang, Z. Xiaorui, P. Wang, Multi-sensor integration approach based on hyperspectral
    imaging and electronic nose for quantitation of fat and peroxide value of pork
    meat. Anal. Bioanal. Chem. 412(5), 1169–1179 (2020). https://doi.org/10.1007/s00216-019-02345-5
    Article   CAS   PubMed   Google Scholar   H. Lin, Z. Wang, W. Ahmad, Z. Man, Y.
    Duan, Identification of rice storage time based on colorimetric sensor array combined
    hyperspectral imaging technology. Journal of Stored Products Research 85 (2020).
    https://doi.org/10.1016/j.jspr.2019.101523 Y. Liu, S. Zhou, W. Han, W. Liu, Z.
    Qiu, C. Li, Convolutional neural network for hyperspectral data analysis and effective
    wavelengths selection. Anal. Chim. Acta 1086, 46–54 (2019). https://doi.org/10.1016/j.aca.2019.08.026
    Article   CAS   PubMed   Google Scholar   C. Xia, S. Yang, M. Huang, Q. Zhu, Y.
    Guo, J. Qin, Maize seed classification using hyperspectral image coupled with
    multi-linear discriminant analysis. Infrared Physics and Technology 103 (2019).
    https://doi.org/10.1016/j.infrared.2019.103077 S. Jia, H. Li, X. Wu, Q. Li, Laboratory-based
    hyperspectral image analysis for the classification of soil texture. Journal of
    Applied Remote Sensing 13(4) (2019). https://doi.org/10.1117/1.JRS.13.046508 D.
    Tao, Z. Wang, G. Li, L. Xie, Sex determination of silkworm pupae using Vis-NIR
    hyperspectral imaging combined with chemometrics. Spectrochimica Acta - Part A:
    Molecular and Biomolecular Spectroscopy 208, 7–12 (2019). https://doi.org/10.1016/j.saa.2018.09.049
    Article   CAS   PubMed   Google Scholar   K. Tan, R. Wang, M. Li, Z. Gong, Discriminating
    soybean seed varieties using hyperspectral imaging and machine learning. Journal
    of Computational Methods in Sciences and Engineering 19(4), 1001–1015 (2019).
    https://doi.org/10.3233/JCM-193562 Article   Google Scholar   D. Tao, Z. Wang,
    G. Li, L. Xie, Simultaneous species and sex identification of silkworm pupae using
    hyperspectral imaging technology. Spectrosc. Lett. 51(8), 446–452 (2018). https://doi.org/10.1080/00387010.2018.1503602
    Article   CAS   Google Scholar   Y. Wang, X. Hu, Z. Hou, J. Ning, Z. Zhang, Discrimination
    of nitrogen fertilizer levels of tea plant (camellia sinensis) based on hyperspectral
    imaging. J. Sci. Food Agric. 98(12), 4659–4664 (2018). https://doi.org/10.1002/jsfa.8996
    Article   CAS   PubMed   Google Scholar   J. Lu, M. Zhou, Y. Gao, H. Jiang, Using
    hyperspectral imaging to discriminate yellow leaf curl disease in tomato leaves.
    Precision Agric. 19(3), 379–394 (2018). https://doi.org/10.1007/s11119-017-9524-7
    Article   Google Scholar   Y. Sun, K. Wei, Q. Liu, L. Pan, K. Tu, Classification
    and discrimination of different fungal diseases of three infection levels on peaches
    using hyperspectral reflectance imaging analysis. Sensors (Switzerland) 18(4)
    (2018). https://doi.org/10.3390/s18041295 H. Lu, X. Yu, L. Zhou, Y. He, Selection
    of spectral resolution and scanning speed for detecting green jujubes chilling
    injury based on hyperspectral reflectance imaging. Applied Sciences (Switzerland)
    8(4) (2018). https://doi.org/10.3390/app8040523 J. Xiong, R. Lin, R. Bu, Z. Liu,
    Z. Yang, L. Yu, A micro-damage detection method of litchi fruit using hyperspectral
    imaging technology. Sensors (Switzerland) 18(3) (2018). https://doi.org/10.3390/s18030700
    B. Jia, W. Wang, S.-C. Yoon, H. Zhuang, Y.-F. Li, Using a combination of spectral
    and textural data to measure water-holding capacity in fresh chicken breast fillets.
    Applied Sciences (Switzerland) 8(3) (2018). https://doi.org/10.3390/app8030343
    T. Guo, M. Huang, Q. Zhu, Y. Guo, J. Qin, Hyperspectral image-based multi-feature
    integration for TVB-N measurement in pork. J. Food Eng. 218, 61–68 (2018). https://doi.org/10.1016/j.jfoodeng.2017.09.003
    Article   CAS   Google Scholar   R. Khodabakhshian, B. Emadi, Application of Vis/SNIR
    hyperspectral imaging in ripeness classification of pear. Int. J. Food Prop. 20,
    3149–3163 (2018). https://doi.org/10.1080/10942912.2017.1354022 Article   CAS   Google
    Scholar   J. Ning, J. Sun, S. Li, M. Sheng, Z. Zhang, Classification of five chinese
    tea categories with different fermentation degrees using visible and near-infrared
    hyperspectral imaging. Int. J. Food Prop. 20, 1515–1522 (2017). https://doi.org/10.1080/10942912.2016.1233115
    Article   CAS   Google Scholar   H. Zhu, B. Chu, C. Zhang, F. Liu, L. Jiang, Y.
    He, Hyperspectral imaging for presymptomatic detection of tobacco disease with
    successive projections algorithm and machine-learning classifiers. Scientific
    Reports 7(1) (2017). https://doi.org/10.1038/s41598-017-04501-2 H.-H. Wang, S.-L.
    Zhang, K. Li, S.-S. Cheng, M.-Q. Tan, X.-H. Tao, X. Zhang, Non-destructive detection
    of ready-to-eat sea cucumber freshness based on hyperspectral imaging. Spectroscopy
    and Spectral Analysis 37(11), 3632–3640 (2017). https://doi.org/10.3964/j.issn.1000-0593(2017)11-3632-09
    Article   CAS   Google Scholar   S. Zeng, L. Chen, L. Jiang, C. Gao, Hyperspectral
    imaging technique based on Geodesic K-medoids clustering and Gabor wavelets for
    pork quality evaluation. International Journal of Wavelets, Multiresolution and
    Information Processing 15(6) (2017). https://doi.org/10.1142/S0219691317500667
    Y. Fan, T. Wang, Z. Qiu, J. Peng, C. Zhang, Y. He, Fast detection of striped stem-borer
    (chilo suppressalis walker) infested rice seedling based on visible/near-infrared
    hyperspectral imaging system. Sensors (Switzerland) 17(11) (2017). https://doi.org/10.3390/s17112470
    D. Yang, D. He, A. Lu, D. Ren, J. Wang, Combination of spectral and textural information
    of hyperspectral imaging for the prediction of the moisture content and storage
    time of cooked beef. Infrared Physics and Technology 83, 206–216 (2017). https://doi.org/10.1016/j.infrared.2017.05.005
    Article   Google Scholar   J.-H. Cheng, D.-W. Sun, Q. Wei, Enhancing visible and
    near-infrared hyperspectral imaging prediction of TVB-N level for fish fillet
    freshness evaluation by filtering optimal variables. Food Anal. Methods 10(6),
    1888–1898 (2017). https://doi.org/10.1007/s12161-016-0742-9 Article   Google Scholar   J.
    Ma, D.-W. Sun, H. Pu, Model improvement for predicting moisture content (MC) in
    pork longissimus dorsi muscles under diverse processing conditions by hyperspectral
    imaging. J. Food Eng. 196, 65–72 (2017). https://doi.org/10.1016/j.jfoodeng.2016.10.016
    Article   Google Scholar   J.-L. Xu, D.-W. Sun, Identification of freezer burn
    on frozen salmon surface using hyperspectral imaging and computer vision combined
    with machine learning algorithm [identification de la brûlure de congélation sur
    la surface du saumon congelé en utilisant l’imagerie hyperspectrale et la vision
    par ordinateur combinée avec l’algorithme d”’apprentissage automatique]. International
    Journal of Refrigeration 74, 149–162 (2017). https://doi.org/10.1016/j.ijrefrig.2016.10.014
    Zhao, Y.R., Yu, K.Q., Feng, C., Cen, H.Y., He, Y.: Early detection of aphid (Myzus
    persicae) infestation on chinese cabbage by hyperspectral imaging and feature
    extraction. Transactions of the ASABE 60(4), 1045–1051 (2017) https://doi.org/10.13031/trans.11886
    S. Fan, B. Zhang, J. Li, C. Liu, W. Huang, X. Tian, Prediction of soluble solids
    content of apple using the combination of spectra and textural features of hyperspectral
    reflectance imaging data. Postharvest Biol. Technol. 121, 51–61 (2016). https://doi.org/10.1016/j.postharvbio.2016.07.007
    Article   Google Scholar   H. Ma, H.-Y. Ji, W.S. Lee, Identification of the citrus
    greening disease using spectral and textural features based on hyperspectral imaging.
    Spectroscopy and Spectral Analysis 36(7), 2344–2350 (2016). https://doi.org/10.3964/j.issn.1000-0593(2016)07-2344-07
    Article   CAS   PubMed   Google Scholar   U. Khulal, J. Zhao, W. Hu, Q. Chen,
    Nondestructive quantifying total volatile basic nitrogen (TVB-N) content in chicken
    using hyperspectral imaging (HSI) technique combined with different data dimension
    reduction algorithms. Food Chem. 197, 1191–1199 (2016). https://doi.org/10.1016/j.foodchem.2015.11.084
    Article   CAS   PubMed   Google Scholar   M. Huang, C. He, Q. Zhu, J. Qin, Maize
    seed variety classification using the integration of spectral and image features
    combined with feature transformation based on hyperspectral imaging. Applied Sciences
    (Switzerland) 6(6) (2016). https://doi.org/10.3390/app6060183 J. Sun, S. Jiang,
    H. Mao, X. Wu, Q. Li, Classification of black beans using visible and near infrared
    hyperspectral imaging. Int. J. Food Prop. 19(8), 1687–1695 (2016). https://doi.org/10.1080/10942912.2015.1055760
    Article   Google Scholar   G.K. Naganathan, K. Cluff, A. Samal, C.R. Calkins,
    D.D. Jones, R.L. Wehling, J. Subbiah, Identification and validation of key wavelengths
    for on-line beef tenderness forecasting. Trans. ASABE 59(3), 769–783 (2016). https://doi.org/10.13031/trans.59.11034
    Article   Google Scholar   L. Wang, D.-W. Sun, H. Pu, Z. Zhu, Application of hyperspectral
    imaging to discriminate the variety of maize seeds. Food Anal. Methods 9(1), 225–234
    (2016). https://doi.org/10.1007/s12161-015-0160-4 Article   Google Scholar   C.
    Xie, Y. Shao, X. Li, Y. He, Detection of early blight and late blight diseases
    on tomato leaves using hyperspectral imaging. Scientific Reports 5 (2015). https://doi.org/10.1038/srep16564
    Z. Xiong, D.-W. Sun, H. Pu, Z. Zhu, M. Luo, Combination of spectra and texture
    data of hyperspectral imaging for differentiating between free-range and broiler
    chicken meats. LWT 60(2), 649–655 (2015). https://doi.org/10.1016/j.lwt.2014.10.021
    Article   CAS   Google Scholar   Y. Cao, C. Zhang, Q. Chen, Y. Li, S. Qi, L. Tian,
    Y. Ren, Identification of species and geographical strains of sitophilus oryzae
    and sitophilus zeamais using the visible/near-infrared hyperspectral imaging technique.
    Pest Manag. Sci. 71(8), 1113–1121 (2015). https://doi.org/10.1002/ps.3893 Article   CAS   PubMed   Google
    Scholar   J. Ma, H. Pu, D.-W. Sun, W. Gao, J.-H. Qu, K.-Y. Ma, Application of
    Vis-NIR hyperspectral imaging in classification between fresh and frozen-thawed
    pork longissimus dorsi muscles. Int. J. Refrig. 50, 10–18 (2015). https://doi.org/10.1016/j.ijrefrig.2014.10.024
    Article   Google Scholar   M. Huang, Y. Ma, Y. Li, Q. Zhu, G. Huang, P. Bu, Hyperspectral
    image-based feature integration for insect-damaged hawthorn detection. Anal. Methods
    6(19), 7793–7800 (2014). https://doi.org/10.1039/c4ay01246a Article   CAS   Google
    Scholar   D. Liu, H. Pu, D.-W. Sun, L. Wang, X.-A. Zeng, Combination of spectra
    and texture data of hyperspectral imaging for prediction of pH in salted meat.
    Food Chem. 160, 330–337 (2014). https://doi.org/10.1016/j.foodchem.2014.03.096
    Article   CAS   PubMed   Google Scholar   H.-J. He, D. Wu, D.-W. Sun, Potential
    of hyperspectral imaging combined with chemometric analysis for assessing and
    visualising tenderness distribution in raw farmed salmon fillets. J. Food Eng.
    126, 156–164 (2014). https://doi.org/10.1016/j.jfoodeng.2013.11.015 Article   Google
    Scholar   H. Huang, L. Liu, M.O. Ngadi, C. Gariépy, S.O. Prasher, Near-infrared
    spectral image analysis of pork marbling based on Gabor filter and wide line detector
    techniques. Appl. Spectrosc. 68(3), 332–339 (2014). https://doi.org/10.1366/13-07242
    Article   CAS   PubMed   Google Scholar   H. Pu, D.-W. Sun, J. Ma, D. Liu, J.-H.
    Cheng, Using wavelet textural features of visible and near infrared hyperspectral
    image to differentiate between fresh and frozen–thawed pork. Food Bioprocess Technol.
    7(11), 3088–3099 (2014). https://doi.org/10.1007/s11947-014-1330-x Article   Google
    Scholar   X. Wei, F. Liu, Z. Qiu, Y. Shao, Y. He, Ripeness classification of astringent
    persimmon using hyperspectral imaging technique. Food Bioprocess Technol. 7(5),
    1371–1380 (2014). https://doi.org/10.1007/s11947-013-1164-y Article   Google Scholar   D.
    Wu, D.-W. Sun, Y. He, Novel non-invasive distribution measurement of texture profile
    analysis (TPA) in salmon fillet by using visible and near infrared hyperspectral
    imaging. Food Chem. 145, 417–426 (2014). https://doi.org/10.1016/j.foodchem.2013.08.063
    Article   CAS   PubMed   Google Scholar   Q. Chen, Y. Zhang, J. Zhao, Z. Hui,
    Nondestructive measurement of total volatile basic nitrogen (TVB-N) content in
    salted pork in jelly using a hyperspectral imaging technique combined with efficient
    hypercube processing algorithms. Anal. Methods 5(22), 6382–6388 (2013). https://doi.org/10.1039/c3ay40436f
    Article   CAS   Google Scholar   H. Yong-Guang, C. Pei-Pei, L. Ping-Ping, Determination
    of water content in de-enzyming green tea leaves based on hyper-spectral imaging.
    Inf. Technol. J. 12(22), 6729–6734 (2013). https://doi.org/10.3923/itj.2013.6729.6734
    Article   Google Scholar   J. Gao, X. Li, F. Zhu, Y. He, Application of hyperspectral
    imaging technology to discriminate different geographical origins of Jatropha
    curcas, L. seeds. Comput. Electron. Agric. 99, 186–193 (2013). https://doi.org/10.1016/j.compag.2013.09.011
    Article   Google Scholar   X. Zhang, F. Liu, Y. He, X. Li, Application of hyperspectral
    imaging and chemometric calibrations for variety discrimination of maize seeds.
    Sensors (Switzerland) 12(12), 17234–17246 (2012). https://doi.org/10.3390/s121217234
    Article   CAS   Google Scholar   Y. Tian, T. Li, L. Zhang, X. Zhang, Diagnosis
    method of cucumber downy mildew with NIR hyperspectral imaging. Proceedings of
    SPIE - The International Society for Optical Engineering 8002 (2011). https://doi.org/10.1117/12.901527
    R. Gosselin, D. Rodrigue, C. Duchesne, A hyperspectral imaging sensor for on-line
    quality control of extruded polymer composite products. Comput. Chem. Eng. 35(2),
    296–306 (2011). https://doi.org/10.1016/j.compchemeng.2010.07.020 Article   CAS   Google
    Scholar   G. ElMasry, N. Wang, A. ElSayed, M. Ngadi, Hyperspectral imaging for
    nondestructive determination of some quality attributes for strawberry. J. Food
    Eng. 81(1), 98–107 (2007). https://doi.org/10.1016/j.jfoodeng.2006.10.016 Article   CAS   Google
    Scholar   R.M. Haralick, Statistical and structural approaches to texture. Proc.
    IEEE 67(5), 786–804 (1979). https://doi.org/10.1109/PROC.1979.11328 Article   Google
    Scholar   R.M. Haralick, K. Shanmugam, I. Dinstein, Textural features for image
    classification. IEEE Trans. Syst. Man Cybern. SMC–3(6), 610–621 (1973). https://doi.org/10.1109/TSMC.1973.4309314
    Article   Google Scholar   D.A. Clausi, An analysis of co-occurrence texture statistics
    as a function of grey level quantization. Can. J. Remote. Sens. 28(1), 45–62 (2002).
    https://doi.org/10.5589/m02-004 Article   Google Scholar   L.-K. Soh, C. Tsatsoulis,
    Texture analysis of SAR sea ice imagery using gray level co-occurrence matrices.
    IEEE Trans. Geosci. Remote Sens. 37(2), 780–795 (1999). https://doi.org/10.1109/36.752194
    Article   Google Scholar   S.W.-C. Lam, Texture feature extraction using gray
    level gradient based co-occurence matrices. In: 1996 IEEE International Conference
    on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929),
    vol. 1, pp. 267–2711 (1996). https://doi.org/10.1109/ICSMC.1996.569778 A. Ramola,
    A.K. Shakya, D. Van Pham, Study of statistical methods for texture analysis and
    their modern evolutions. Engineering Reports 2(4), 12149 (2020). https://doi.org/10.1002/eng2.12149
    Article   Google Scholar   X. Chu, R. Li, H. Wei, H. Liu, Y. Mu, H. Jiang, Z.
    Ma, Determination of total flavonoid and polysaccharide content in anoectochilus
    formosanus in response to different light qualities using hyperspectral imaging.
    Infrared Physics and Technology 122 (2022). https://doi.org/10.1016/j.infrared.2022.104098
    J. Redmon, A. Farhadi, YOLOv3: An Incremental Improvement. (2018). arxiv:1804.02767
    C. Pohl, J.L. Van Genderen, Review article multisensor image fusion in remote
    sensing: concepts, methods and applications. Int. J. Remote Sens. 19(5), 823–854
    (1998) Article   Google Scholar   J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li,
    L. Fei-Fei, Imagenet: A large-scale hierarchical image database. In: 2009 IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 248–255 (2009). https://doi.org/10.1109/CVPR.2009.5206848
    J. Chai, H. Zeng, A. Li, E.W.T. Ngai, Deep learning in computer vision: A critical
    review of emerging techniques and application scenarios. Machine Learning with
    Applications 6(2021). https://doi.org/10.1016/j.mlwa.2021.100134 Article   Google
    Scholar   Download references Acknowledgements Not applicable. Funding Open Access
    funding enabled and organized by CAUL and its Member Institutions Author information
    Authors and Affiliations School of Computer Science, University of Auckland, 38
    Princes Street, Auckland CBD, Auckland, 1010, New Zealand Mitchell Rogers, Martin
    Urschler & Patrice Delmas Institute for Medical Informatics, Statistics and Documentation,
    Medical University Graz, 8010, Graz, Austria Martin Urschler DGA/DT/TA/EMOO, 47
    rue Saint Jean, 31130, Balma, France Jacques Blanc-Talon Corresponding author
    Correspondence to Mitchell Rogers. Additional information Original article has
    been corrected to update affiliation. Supplementary Information Below is the link
    to the electronic supplementary material. Supplementary file1 (PDF 791 KB) Supplementary
    file2 (XLSX 247 KB) Rights and permissions Open Access This article is licensed
    under a Creative Commons Attribution 4.0 International License, which permits
    use, sharing, adaptation, distribution and reproduction in any medium or format,
    as long as you give appropriate credit to the original author(s) and the source,
    provide a link to the Creative Commons licence, and indicate if changes were made.
    The images or other third party material in this article are included in the article''s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article''s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Rogers, M., Blanc-Talon,
    J., Urschler, M. et al. Wavelength and texture feature selection for hyperspectral
    imaging: a systematic literature review. Food Measure 17, 6039–6064 (2023). https://doi.org/10.1007/s11694-023-02044-x
    Download citation Received 08 May 2023 Accepted 01 July 2023 Published 17 August
    2023 Issue Date December 2023 DOI https://doi.org/10.1007/s11694-023-02044-x Share
    this article Anyone you share the following link with will be able to read this
    content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Hyperspectral imaging Multispectral imaging Wavelength selection
    Sensors Spatial features Food science Computer vision Agriculture Use our pre-submission
    checklist Avoid common mistakes on your manuscript. Sections Figures References
    Abstract Methods Results Discussion Conclusion Change history Notes References
    Acknowledgements Funding Author information Additional information Supplementary
    Information Rights and permissions About this article Advertisement Discover content
    Journals A-Z Books A-Z Publish with us Publish your research Open access publishing
    Products and services Our products Librarians Societies Partners and advertisers
    Our imprints Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy
    choices/Manage cookies Your US state privacy rights Accessibility statement Terms
    and conditions Privacy policy Help and support 129.93.161.219 Big Ten Academic
    Alliance (BTAA) (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024
    Springer Nature"'
  inline_citation: '>'
  journal: Journal of Food Measurement and Characterization
  limitations: '>'
  relevance_evaluation: '0.9-1.0: Exceptionally relevant - Comprehensively addresses
    all key aspects of the point and review.'
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  title: 'Wavelength and texture feature selection for hyperspectral imaging: a systematic
    literature review'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Thakur A.
  - Venu S.
  - Gurusamy M.
  citation_count: '3'
  description: Agriculture represents an essential aspect of human existence, providing
    the sustenance necessary for survival in the form of food and various other products.
    Additionally, it serves as a foundational pillar of economic development, offering
    employment and income opportunities to countless individuals. The incorporation
    of machine vision in agriculture has emerged as a crucial technology, enabling
    farmers to automate various tasks, such as crop monitoring and yield prediction
    using cameras and image processing techniques and even providing real-time information
    on crop maturity for harvest planning. This not only helps increase efficiency
    and productivity but also provides valuable insights for precision agriculture,
    enabling more quick and informed decision-making and ultimately leading to improved
    crop yields and financial returns. This scholarly work conducts a thorough examination
    of the various components that comprise a machine vision system, specifically
    delving into the techniques of image acquisition, processing, and classification.
    It also explores the methods employed within each of these techniques, and how
    the combination of such processes is used to perform various agricultural activities
    such as weeding, seeding, harvesting, fruit counting, overlapping, sorting, etc.
    Furthermore, it aims to guide on how the knowledge gained can be applied to build
    practical machine vision systems for agriculture. Additionally, this work highlights
    the research gaps, address current problems that can be solved and its potential
    for future advancement in the field of agriculture, and guides readers towards
    promising areas for future endeavours.
  doi: 10.1016/j.compag.2023.108146
  explanation: The provided study focuses on the potential of machine vision systems
    in automating various agricultural tasks. It presents the key components, methods,
    and applications of machine vision in agriculture, highlighting recent advancements
    in the field. The extract demonstrates the effectiveness of machine vision systems
    for precise crop harvesting, including strawberry, broccoli, kiwi, and apples.
    The mentioned research paper provides specific examples of how machine vision
    enhances harvesting processes, such as reducing labor costs, improving efficiency,
    and minimizing crop damage.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Machine vision system
    3. Application using the mentioned techniques in various domains 4. Conclusion
    Declaration of Competing Interest Data availability References Further reading
    Show full outline Cited by (4) Figures (26) Show 20 more figures Tables (12) Table
    1 Table 2 Table 3 Table 4 Table 5 Table 6 Show all tables Computers and Electronics
    in Agriculture Volume 212, September 2023, 108146 Review An extensive review on
    agricultural robots with a focus on their perception systems Author links open
    overlay panel Abhishek Thakur, Sangeeth Venu, Muralimohan Gurusamy Show more Add
    to Mendeley Share Cite https://doi.org/10.1016/j.compag.2023.108146 Get rights
    and content Highlights • Extensive review has been done related to the perception
    system utilized in autonomous agriculture robots. • The study will help the researchers
    know about the recent trends in agricultural robots. Abstract Agriculture represents
    an essential aspect of human existence, providing the sustenance necessary for
    survival in the form of food and various other products. Additionally, it serves
    as a foundational pillar of economic development, offering employment and income
    opportunities to countless individuals. The incorporation of machine vision in
    agriculture has emerged as a crucial technology, enabling farmers to automate
    various tasks, such as crop monitoring and yield prediction using cameras and
    image processing techniques and even providing real-time information on crop maturity
    for harvest planning. This not only helps increase efficiency and productivity
    but also provides valuable insights for precision agriculture, enabling more quick
    and informed decision-making and ultimately leading to improved crop yields and
    financial returns. This scholarly work conducts a thorough examination of the
    various components that comprise a machine vision system, specifically delving
    into the techniques of image acquisition, processing, and classification. It also
    explores the methods employed within each of these techniques, and how the combination
    of such processes is used to perform various agricultural activities such as weeding,
    seeding, harvesting, fruit counting, overlapping, sorting, etc. Furthermore, it
    aims to guide on how the knowledge gained can be applied to build practical machine
    vision systems for agriculture. Additionally, this work highlights the research
    gaps, address current problems that can be solved and its potential for future
    advancement in the field of agriculture, and guides readers towards promising
    areas for future endeavours. Previous article in issue Next article in issue Keywords
    Precision agricultureMachine learningImage processingComputer visionSmart farmingAgricultural
    automationDeep learning 1. Introduction Agriculture is the science or practice
    of cultivating land to grow crops and raise livestock. An agricultural system
    is a combination of machines and equipment used to carry out these tasks. The
    agricultural system encompasses various activities such as harvesting, planting,
    weeding, picking, and sorting. These operations often present challenges such
    as identifying good and bad crops and addressing the efficiency and effectiveness
    of these tasks. All agricultural systems require human involvement to varying
    degrees, but this can also lead to issues such as fatigue, labour shortages, and
    adverse working conditions, which can negatively impact efficiency, increase costs,
    and lead to accidents and longer working hours. To offer sustainable agriculture
    it is entirely necessary to introduce modern technology such as blockchain, big
    data, machine vision, IOT, machine learning to meet future agricultural needs
    (Cravero et al., 2022). The global population is nearly 350 crore people and the
    land currently in use is just not enough to feed the people. According to the
    food and agriculture organization forecast, global food production should increase
    to 70% as the population of the world will reach about 9.1 billion by 2050 (Unitednations,
    2021, UNICEF, 2021). As per the findings of a study conducted by Roser et al.
    (Roser, 2013), there has been an increase in individuals abandoning agriculture
    as their primary activity over the past decade, owing to a decline in income within
    the agricultural sector. In the next 10 to 20 years agriculture will suffer a
    lack of manpower but with a predicted increase in the consumption of food. Considering
    these labour-related difficulties, agriculture is moving towards the incorporation
    of modern machinery that is based on automation and robotics. These systems require
    minimal human intervention and can perform operations with a high degree of efficiency,
    thereby increasing productivity. With the adoption of such systems, a team of
    as few as four people can manage a large agricultural field, which would traditionally
    require 50 individuals using conventional agricultural practices. The central
    component in such robotics and automation systems is machine vision, which empowers
    these machines to operate incessantly in various domains, executing tasks with
    unparalleled efficiency that surpasses human capabilities, even during extended
    periods of time. Seeding, harvesting, and pesticide spraying are some labour-intensive
    agricultural activities that can be performed using very minimal labour in a more
    efficient and hassle-free manner using robots equipped with vision systems. A
    machine vision system is a complex process combining various steps that can extract
    high-value information from a camera source and can bring meaningful results (Brill
    et al., 2020). This survey paper mainly focuses on advances in the machine vision
    systems in agriculture and briefly explains the processes that are currently used
    for the image acquisition system, image processing, and image classifications.
    Moreover, this paper discusses the techniques of machine vision being used in
    different agricultural processes, such as harvesting, seeding, weeding, spraying,
    pick and place operations, and pathfinding for various autonomous vehicles. This
    paper will also elaborate on the various methodologies, algorithms, and operating
    principles used for different agricultural process-specific robots. The contributions
    of this work are significant in several aspects. Firstly, this scholarly examination
    provides a comprehensive overview of the incorporation of machine vision systems
    in agriculture, highlighting its importance in addressing the challenges faced
    by the industry. Secondly, this work delves into the technical details of machine
    vision systems, focusing on image acquisition, processing, and classification
    techniques. By exploring the methods employed within each of these techniques,
    readers gain a deep understanding of how machine vision systems operate in the
    agricultural context. This knowledge can be applied to develop practical machine
    vision systems for various agricultural processes, including weeding, seeding,
    harvesting, fruit counting, sorting, and more. Furthermore, this work highlights
    the current research gaps and addresses existing problems that can be solved through
    advancements in machine vision technology. By disseminating this knowledge, it
    contributes to the ongoing progress of agricultural practices, ultimately benefiting
    farmers, consumers, and the global population. Overall, this work serves as a
    valuable resource for researchers, industry professionals, and policymakers interested
    in the implementation of machine vision systems in agriculture. 2. Machine vision
    system A machine vision system refers the computer’s ability to perceive environment
    like human vision (Nixon and Aguado, 2020). The human eye is limited in its ability
    to perceive light, with a responsiveness range of 390 nm to 770 nm (Dattner and
    Bohn, 2016). However, cameras can detect a broader range of wavelengths, including
    infrared, X-rays. This expands the range of for machine vision. Typically, agriculture
    machine vision can be used for numerous processes such as Weeding, Pesticide spraying,
    Harvesting, Sorting, Seeding, Counting, Yield estimation, Pick and place operations,
    Path-finding, Arranging, and Soil classification with the help of complex interrelated
    systems of devices in which singular or several cameras of same or different types
    are used to acquire images of interest, an illuminating light source provided
    on the system for sufficient lighting over the object for grabbing high-resolution
    image using singular or multiple digital cameras, micro-controllers or microprocessors
    either embedded on the mechanical system or a faraway computer by the use of wireless
    communication. The software processes perform various operations over the camera
    images. Thereafter, the decision-making algorithm provides predictive results.
    Finally, the result obtained after processing are used as signals to actuate the
    mechanical system, such as robots, rovers, manipulators, etc. The MVS system can
    be generalized into three categories (Lee-Post, 2003) as mentioned below. • Image
    acquisition: Image acquisition involves the utilization of a variety of cameras,
    illumination sources, and equipment to acquire the necessary data and high-resolution
    images essential for a particular application. • Image processing: Image processing
    is a crucial step in the overall machine vision system. The raw data obtained
    is subject to processing prior to classification or training. This step encompasses
    transformation, analysis, cleaning, filling of missing data, image correction
    techniques, noise filtering. • Image Classification: This step of image processing
    is crucial for comprehending the image. The image processing data is classified
    according to the required characteristics for training, testing, and validation.
    This data is then aggregated for the application of machine learning or deep learning
    algorithms. 2.1. Image acquisition To achieve optimal image acquisition by a machine,
    two essential components are utilized: an illuminating source and a specific type
    of camera. By combining these elements, a wide range of acquisition methods can
    be employed, leading to diverse and efficient imaging outcomes. This system emulates
    human vision by perceiving and recognizing the images. As humans perceive an image
    through the light reflected from an object, illumination plays a critical role
    in image acquisition, as adjustments to the light can reveal different features
    of the object of interest. Images are captured by cameras with charge-coupled
    devices (CCD) and complementary metal–oxide–semiconductor (CMOS) sensors, and
    the quality of illumination can significantly impact the texture and features
    of the object of interest (Mahajan et al., 2015). All these factors contribute
    in acquiring necessary information. The first step is the selection of an appropriate
    technique that can capture high-quality images in the form of an array of numerical
    data that can be used to synthesize information by the computer (Mishra et al.,
    (2017)). Fig. 1 shows the process of image acquisition using a camera attached
    to the lower side of the robot and an illumination source to light the object.
    The images are subsequently transmitted over a wired or wireless network. There
    are various approaches to obtain high resolution images that can be used for agricultural
    purposes. The following subsections detail the wide range of wavelengths that
    can be perceived by the camera, as well as various techniques and illumination
    sources. Download : Download high-res image (99KB) Download : Download full-size
    image Fig. 1. Processes involved for Machine vision systems. 2.1.1. Monocular
    stereo camera system This is the basic low-cost technique using a single camera
    to capture images. Fig. 2 illustrates a monocular camera scheme utilized in autonomous
    mobile vehicles. Pravakar Roy et al. (Roy and Isler, 2016) used a monocular vision
    system to gather images to obtain effective information regarding apple fruit
    count and size in a dense cluster. Khan et al. (Khan and Debnath, 2019) employed
    a black and white camera in the MAGALI project to detect fruits based on their
    features. Zhao et al. (Zhao et al., 2005) replaced a black and white camera with
    Red Green Blue (RGB) cameras which provided colour contrast and reported an accuracy
    of 90% in identifying apples based on colour and texture features. Chen et al.
    (Chen et al., 2020) used a monocular RGB camera for positioning sweet peppers
    with the aid of a Visual Geometric Group network with 16 layers (VGG-16) and showed
    an average error of 18.3 % in pixels with respect to the position of the fruit
    centre over the fruit radius. Due to variable illumination and reflected light
    from neighbouring fruits and other background noise it is difficult to detect
    citrus fruit effectively. Using monocular camera system J.J. Zhuang et al. (Zhuang
    et al., 2018) developed a method for citrus fruit detection. Zhao et al. (Zhao
    et al., 2016) reported the disadvantage of a monocular setup that it only provided
    2D information and there were high chances of illumination changes affecting the
    results. Download : Download high-res image (86KB) Download : Download full-size
    image Fig. 2. A basic system mentioning monocular machine vision system over a
    robot. 2.1.2. Binocular stereo vision system Binocular stereo-vision systems are
    well recognized as effective 3D vision systems. Compared to a 2D vision system,
    they provide a detailed representation of reality. The third axis obtained in
    a stereo system is used to measure image depth. It can also be used to determine
    the geometric properties of an object, such as dimensions and miscellaneous measurements.
    In Fig. 3, dual camera stereo systems are mounted on the top of a moving vehicle
    (Lin et al., 2008). Download : Download high-res image (78KB) Download : Download
    full-size image Fig. 3. Visualization of a Binocular vision system over a moving
    cart. Two cameras are placed at a variable distance from each other on the same
    plane, while keeping the height constant. They simultaneously capture two images,
    from two different perspectives, and generate a 3D view (Hong et al., 2018, Kim
    and Lee, 2015). Fig. 4 depicts generation of 3D perspective from two cameras.
    Download : Download high-res image (57KB) Download : Download full-size image
    Fig. 4. Binocular system working, visualization of 3d perspective. Corado costa
    et al. (Costa, 2019) developed a methodology for path guidance for a vehicle by
    making a 3D construct of the environment and providing distance information in
    veritable scenes to determine obstacles or the multiple branches overhanging and
    branch splitting of the hazel trees. Using depths of images, Subhi et al. (Subhi
    and Ali, 2018) provided a method to estimate the volume and mass of food. In addition,
    they proposed several food grading techniques based on stereo vision. For instance,
    for precision agriculture, stereo vision was used to develop a three-dimensional
    terrain map, a compact stereo camera was used on a mobile moving platform to construct
    3d map of the terrain and showed the level of accuracy and detail by stereo perceptions
    (Rovira-Más et al., 2008). Two cameras, as illustrated in Fig. 4, are separated
    by an angular distance allowing them to take images of the same object from different
    angles as shown by Zhao et al. (Zhao et al., 2016). Andersen et al. (Andersen
    et al., 2005) used stereo vision for analyzing geometric attributes of the plants
    like total leaf area and height. A simulated annealing method was used for considering
    the neighboring pixels for the stereo matching process. Wang et al. (Wang et al.,
    2016) applied stereo vision for the localization of litchi, then wavelet transform
    was applied to a pair of images to normalize the illumination and K-means clustering-based
    algorithm was used to differentiate litchi from its background. Xiang et al. (Xiang
    et al., 2014) utilized stereo imaging for acquiring the depth map of clustered
    tomatoes and categorized overlapping and adhering regions after denoising the
    depth map. 2.1.3. Remote sensing imaging Remote sensing is the method used for
    measuring electromagnetic radiations reflected or emitted from a body. The instrument
    records EM radiations measured in either the UV spectrum or visible spectrum or
    infrared spectrum. The devices that are used for this purpose can be hand-held,
    ground based or aerial by means of drones, aircraft, satellites, and balloons.
    The apparatus carries a digital camera, video system and radiometers. The data
    can be stored or can be viewed directly based on a wireless network (AGRIOS, 2005).
    Through Fig. 5, khunal et al. (Khanal, (2017,3)) displays how remote sensing is
    used to detect changes in crop and soil conditions by detecting changes in surface
    reflectance through multiple images taken over a period. The field of remote sensing
    has evolved over decade. Today remote sensing is used for providing data from
    energy reflected, transmitted, or emitted from all types of EM spectrums. It is
    profoundly used in agricultural properties, determination of vegetation covers,
    land use, topography (Estes et al., 2001). Download : Download high-res image
    (318KB) Download : Download full-size image Fig. 5. Remote sensing working. Fig.
    6 displays a soil map created through remote sensing, measuring critical soil
    parameters such as soil organic matter (SOM), soil texture, soil pH level, and
    moisture content (Shustova, 2022). Precision agriculture nowadays uses remote
    sensing for gathering coordinated information of the ground target more accurately
    to provide wheat field yield density, area selection for the destruction of beat
    acres, and nitrogen application recommendations (Seelan et al., 2003). Bathany
    sleep et al. (Sleep et al., 2021) used visible near-infrared absorbance spectroscopy
    for soil pH determination. Whereas to determine the soil fertility near-infrared
    was used by Munawar et al. (Munawar et al., (2021)). Zheang zhu et al. (Zhou et
    al., 2021) measured the levels of crop water stress using thermal imaging in remote
    sensing in precision agriculture, moreover, provided a brief description of applications
    for various crop and segmentation strategies. Download : Download high-res image
    (357KB) Download : Download full-size image Fig. 6. Ph value of soil through remote
    sensing(Shustova, 2022. 2.1.4. Hyper-Spectral imaging An object’s spatial and
    spectral characteristics are measured by capturing images of an object over various
    wavelengths. Typically visible, near infrared, and infrared wavelengths are used
    for capturing images. However, the wavelengths can range from long-wave infrared
    to ultra-violet (“Hyperspectral Viewer”, 2022). Because of spectral resolution
    limits, the accuracy of the returned variables is frequently restricted and early
    signs of crop stressors cannot be adequately recognized. Lu et al. (Lu et al.,
    2020) have proposed that hyperspectral photographs containing hundreds of bands
    can record more detailed spectral responses, making them more suited for identifying
    tiny differences as shown in Fig. 7. The disadvantage of hyperspectral systems
    is their high cost, as well as the fact that modelling and data processing is
    time-consuming as depicted by Dale et al. (Dale et al., 2013). Manjunath et al.
    (Manjunath et al., 2011) distinguished between ornamental crops and pulses using
    the data collected from spectral imaging. Kumar et al. (Kumar et al., 2013) showed
    that the spectral reflectance of a canopy infested by aphids and a healthy mustard
    crop was different at near-infrared wavelength. Download : Download high-res image
    (155KB) Download : Download full-size image Fig. 7. Hyperspectral scheme vs multispectral
    scheme (Giannoni et al., 2018). 2.1.5. Multi-Spectral imaging A multi-spectral
    image is a combination of multi-layer of images of a single scene, with every
    single layer of a different wavelength as shown in Fig. 8. Multispectral imaging
    can perfectly detect radiations over the given wavelength bands (Nicolis and Gonzalez,
    2021): • Blue: 450–515. 520 nm • Near-infrared (NIR): 750–900 nm • Green: 515.
    520–590. 600 nm • Red: 600–630–680. 690 nm • Short infrared Download : Download
    high-res image (59KB) Download : Download full-size image Fig. 8. Wavelength band
    for Hyper-spectral and Multispectral imaging. The limitation of hyperspectral
    imaging is its complexity in computing data due to high-resolution levels. On
    the other hand, multispectral imaging is computationally less complex due to low-resolution
    levels. Honrado et al. (Honrado et al., 2017) utilized 2 Canon S100 compact digital
    cameras and Near-Infrared- retrofitted cameras for aerial imagery. Ground sensors
    were used to verify the Normalized Difference Vegetation Index (NDVI) obtained
    from the Unmanned Aerial Vehicle (UAV). UAV-based flights provided a high resolution
    and geographic breadth necessary for imaging and targeting heterogeneous cropping
    regimes and multiple-farmer plots. Instead of a ledger-based monitoring system
    this method was proposed for managing and monitoring local agriculture. Abhi et
    al utilized a monochromatic CCD camera equipped with two tunable electro-optical
    filters, one in the visible range (VIS 400 – 720 nm) and the other in the infrared
    range (SNIR 650 – 1100 nm), to capture a sequence of images in stable lighting
    conditions. These images correspond to the energy emitted by targeted features
    within narrow 10 nm bands within the filter range. The accuracy of the acquired
    data was confirmed, and a spectral profile was generated for each visible object.
    By analysing the variations in the amounts of reflected, absorbed, and transmitted
    energy, objects on the ground could be distinguished. Hyperspectral technology
    enabled the construction of images using data acquired at different wavelengths,
    revealing that even objects within the same category, such as deciduous trees,
    differed in the proportions of energy they absorbed, reflected, and transmitted,
    depending on the wavelength (Jasinski et al., 2010). 2.1.6. X-ray X-rays are utilized
    to discern an undesired object, that possesses a higher density than its surrounding
    matrix be it of agricultural or metallic origins. As a result, X-rays can be employed
    to detect contaminants in yield packaging by identifying foreign objects, such
    as metal, glass, calcified bone, and stone, thereby ensuring a higher quality
    output. An X-ray can reveal spatial information about the objects under examination
    and can acquire three-dimensional data. Small variations in density and distinctions
    between materials can be observed through this medium, leading to the detection
    of foreign bodies or for the quality control of fresh produce (Zwiggelaar et al.,
    1996). Most of the x-ray that are being used in the agriculture sector are low
    powered Xray with a magnitude of 50 keV and are known as soft Xray. More profoundly
    being used in crop, soil, grain, tree nut and fruit study (Mathanker et al., 2013).
    In recent years, X-ray technology has become renowned in determining the quality
    of agricultural products, enabling a better understanding of the composition,
    physicochemical characteristic, and internal structure on samples. In meticulous
    detail, X-ray technology can be utilized to scrutinize a wide array of agricultural
    products, including cereals such as wheat, corn, and rice, as well as fruits like
    apples and pears. This method grants a high level of two-dimensional and three-dimensional
    visualization. The technical applications of this technology include the assessment
    of internal quality, examination of micro-structure, and other purposes. Research
    studies have been conducted on various food products and have reported properties
    such as infection, antiinfection, and acidity prediction, as highlighted by Zhe
    Du et al. (Du et al., 2019). 2.1.7. Thermal imaging Using a thermal camera, Stanjnko
    et al. (Stajnko et al., 2004) devised a method for calculating the quantity and
    diameter of apples in an orchard. Based on heat emission as a function of exposure
    time to the sun’s heat. Bulanon et al. (Bulanon et al., 2009) and Slaughter et
    al. (Slaughter and Harrell, 1987) presented a novel experiment that utilized thermal
    infrared imaging in the fruit citrus canopy. A significant temperature differential
    between the fruit and the canopy was discovered at 4:00p.m. Based on the Stephan-Boltzmann
    principle, thermal radiation was employed to determine the temperature of the
    emitting object, and this method can be used to determine the water potential
    in a field, thus allowing for the optimization of irrigation. Optical methods
    can also be employed to identify the propagation of disease throughout the vegetation,
    as illustrated by Hernández-Clemente et al. (Hernández-Clemente et al., (2019)).
    Variations in leaf water content can alter the short-wave infrared spectrum, and
    the thermal infrared band was employed to detect the temperature of the leaf.
    Thermal imaging presents a valuable tool for monitoring environmental factors
    such as drought and rainfall, as well as variations in transpiration rates as
    shown in Fig. 9. Through this technique, it is possible to detect the shaded and
    sun exposed canopy fractions. Furthermore, variations in transpiration are directly
    linked to leaf temperature, can be inferred from changes in the temperature of
    canopy foliage under otherwise similar conditions of solar radiation and wind
    will refer to reduced transpiration (Hulley et al., 2019). Download : Download
    high-res image (291KB) Download : Download full-size image Fig. 9. Thermal imaging
    of a forest through aerial mode, on the left is the normal image while on the
    right is the thermal image. 2.1.8. Orthogonally polarized terahertz (THz)-Wave
    imaging system Yu et al. (Yu et al., 2015) developed a new system for detecting
    unwanted objects on a conveyor belt transporting crops, fruits, or food that moves
    faster than 20 m per minute. The system consisted of an imaging system positioned
    at a right angle that produced four different images, comprising of horizontal
    and vertical polarized images, as well as their respective addition and subtraction
    images. The system can effectively identify foreign objects on a product, such
    as pests, worms, or any other foreign body. Jiang et al. (Jiang et al., 2022)
    mentioned numerous machine learning models applied to terahertz technology for
    data preprocessing, multivariate analysis and terahertz imaging for water content
    measurement in fruits, additives in flour, varieties of rice, nutritional ingredients
    in dietary supplements. 2.1.9. Liquid Crystal tuneable filter (LCTF) Hyperspectral
    imaging can capture many images over a broad range of wavebands that are undetectable
    by the human eye. To ensure dependable and consistent outcomes using this optical
    sensor, it is crucial that the intensity displayed by the objects in the various
    spectral images remains unaffected by differences in the system''s sensitivity
    for different wavelengths. The spectral efficiency of the acquisition devices
    and the spectral emission of the lighting system can vary across the spectrum
    and the images. If the system is not accurately calibrated and corrected, these
    variations may affect the results obtained. Therefore, it is important to calibrate
    and correct hyperspectral imaging systems to achieve accurate and reliable outcomes.
    This process can be quite onerous, so to simplify it, numerous Liquid Crystal
    Tunable Filters (LCTF) devices are utilized to capture images across a wide range
    of spectral wavelengths. As demonstrated by Gomez Sanchis et al. (Gómez-Sanchis
    et al., 2014) a methodology was proposed for detecting decay in citrus fruits
    utilizing two hyperspectral systems in conjunction with two Liquid Crystal Tunable
    Filters (LCTF) for the image acquisition of spherical fruits. The system was capable
    of accurately identifying 98% of the pixels of rotten or non-rotten areas, and
    with a 95% success rate in determining the condition of the fruits. 2.2. Image
    processing The utilization of image processing has risen dramatically in recent
    years to acquire accurate data for computer processing, enhance understanding
    and reduce computational costs. As a result, the scope of applications for image
    processing has expanded exponentially, encompassing fields such as military, agriculture,
    remote sensing, medicine, geology, and even space and interstellar research. Image
    processing is a technique in which digital images are manipulated to suit specific
    requirements by utilizing a digital computer. The goal of this process can be
    to clean, extract, classify, or identify data from the region of interest (ROI)
    (Da Silva and Mendonça, 2005). In agriculture, image processing can be used to
    identify diseased plant or crop, or fruit by analysing the image, and its pixel
    density. The initial step in this process is to acquire images of both the diseased
    and healthy plants using a machine vision camera. Image pre-processing techniques
    are then applied to the acquired images. Following pre-processing, the image is
    divided into distinct segments. A variety of segmentation techniques, such as
    threshold-based, edge-based, region-based, clustering-based, and artificial neural
    network-based methods, are then applied to the image to determine the nature and
    type of disease. Suganya et al. (Suganya et al., 2019) classified plants based
    on the disease incurred using machine vision. This process also is known as digital
    image processing, and it does not involve interpreting the content of the image
    or changing its meaning by any alteration. Image processing is thus classified
    into three different levels, as can be depicted from Fig. 10, and are explained
    below as low-level processing, intermediate-level processing, and high-level processing.
    Download : Download high-res image (129KB) Download : Download full-size image
    Fig. 10. Various machine learning algorithms used in machine vision systems. 2.2.1.
    Low-level processing Low-level image processing techniques are employed for the
    preliminary treatment of the data. As the images acquired by the camera cannot
    be synchronized with the application. Insufficient light, spatial effects, low
    focus, unwanted graining, and low resolution can impact the image. Due to such
    problems, images are manipulated with techniques such as noise reduction, contrast
    enhancement, image sharpening, and others to improve its quality based on the
    requirement (Senni et al., 2014). Low-level processing is used to create a perceptually
    good smoothing process by applying various techniques to make images compatible
    with a segmentation process, intermediate-level processing, and formulating a
    meaningful scale space for images (Sochen et al., 1998). Detecting a feature is
    a low-level image processing operation that is usually performed as the first
    operation on an image feature. A feature is a region of interest in an image.
    Mustafa et al. (Mustafa et al., 2008) used feature detection to check the ripeness
    of the banana and its size. Image restoration is another type of low-level processing
    technique used for enhancing the image. It involves restoring the image from a
    degraded version such as blur or noise that can be caused by atmospheric turbulence
    during wireless transfer and any other stochastic phenomena (Reeves, 2014). Senni
    et al. (Senni et al., 2014) proposed an on-line Thermography Non-Destructive Testing
    method to detect whether biscuits were contaminated with foreign objects. During
    the image pre-processing step, the authors applied a 2D low pass-filter, a focusing
    filter, and a 2D Wiener filter in the frequency domain to reduce the Additive
    White Gaussian Noise, image blurring, and drag effects problems in the raw images.
    2.2.2. Intermediate level processing Mid-level image processing is used to segregate
    the information from the pre-processed image to obtain the feature from the target
    data. It reduces the computational cost and time by narrowing the approach. It
    generally involves operations like image segmentation, image description, object
    recognition, and image transformation. This level of processing usually has images
    as an input and extracted attributes as an output. Image segmentation is also
    used for classifying or grouping images-based similarities and dissimilarities
    appointing the use of techniques based on thresholding, colour, and clustering.
    Its function is to segregate target data from extraneous information, hence reducing
    computation time and increasing accuracy. Vijai Singh et al. (Singh and Misra,
    2017) demonstrated a genetic-based classification algorithm for detecting and
    classifying plant leaf diseases in banana, bean, jackfruit, lemon, mango, potato,
    tomato, and sapota species. Early detection of damage on peaches due to bruises
    is a challenging task; it determines the quality of peaches. Watershed segmentation
    is a region-based technique that takes into consideration topographic features
    such as mountains, valleys, and basins to segment objects. Several efforts using
    short wave NIR were more suitable, but Li et al. (Leiva-Valenzuela and Aguilera,
    2013) devised an improved watershed segmentation algorithm based on morphological
    gradient reconstruction and marker extraction and used this method to segment
    bruised regions on peaches. Jaime et al. (Giménez-Gallego et al., 2020) used image-based
    system to segment various tree leaves based on a vector machine model coupled
    with deep learning for image segmentation. Ni et al. (Ni et al., 2018) used OTSU
    method after the segmentation process to detect apples in a complex environment
    for an apple-picking robot using a vision system. Momin et al. (Momin et al.,
    2017) used a machine vision system to grade mangoes based on geometry and their
    shape. In this system, an 8-bit XGA camera was used to capture the mango images.
    Moreover, to distinguish mangoes, various intermediate processing techniques were
    used such as image segmentation techniques, which include thresholding and pattern
    recognition, to segment the features of mangoes. In addition, it was found that
    mango mass can be well estimated with the projected area by 97%, hence this parameter
    was taken as benchmark for identifying features such as projected area, perimeter,
    diameter, and roundness of the fruits. Finally, a simple filter binarization with
    a median filtering, coupled with median filter was deployed to grade the mangoes
    into three basic categories: small, medium, and large. Pan et al. (Pan et al.,
    2016) utilized a machine vision system to identify chill damage in peaches stored
    at extremely low temperatures. Hyperspectral imaging system and a CCD camera equipped
    with an adjustable light source was used to acquire the images. The data was then
    processed using global thresholding and classified using a multi-layer perceptron
    Artificial Neural Network (MLPANN) to identify the peaches that were affected
    by chill damage. The model was able to achieve 97% accuracy. 2.2.3. High level
    processing High-level processing includes image recognition and image interpretation
    to obtain meaning from a group of recognized objects. In this step, statistical
    methods, machine learning methods, neural networks, or deep learning methods are
    commonly used to classify and identify the target based on the region of interest.
    This type of processing comprises the use of algorithms such as K-nearest neighbour
    (KNN), principal component analysis, Support Vector Machine (SVM), artificial
    neural networks, genetic algorithms or fuzzy logic that can be deployed to interpret
    meaning from processed image data after intermediate level processing (Liu et
    al., 2017) (Janke et al., 2019) (Druzhkov and Kustikova, (2016)). Nguyen et al.
    (Nguyen et al., 2020) used deep learning to precisely locate the paddy field at
    the pixel level throughout the year from the obtained data through remote sensing
    techniques. To detect and predict rice leaf diseases, deep learning support vector
    machines are used moreover for pattern recognition and to improve the recognition
    accuracy. Jiang et al. (Jiang et al., 2020) used a convolutional neural network
    to obtain leaf disease features. Due to their higher probability of occurrence
    after using support vector machines to classify and predict the disease, The model
    was applied over four rice diseases: rice blast, red blight, stripe blight, and
    sheath blight. In smart farms in Chile Quiroz et al. (Quiroz and Alférez, 2020)
    used convolutional neural network for image recognition for the detection of legacy
    blueberries at the rooting stage. The proposed method could identify blueberries
    plant with tray, presence of tray without plant, and no trays. 2.3. Image classification
    2.3.1. Machine learning algorithms Machine learning (ML) refers to a system’s
    ability to acquire and integrate knowledge through large-scale observations and
    to improve and extend itself by learning fresh knowledge and augmenting its database
    instead of being confined to a predetermined set of limited information through
    programming. Machine learning is used for classification, detection of crop, pest,
    weeds, disease on various crops pulses, tiago et al (Domingues et al., 2022) identified
    various uses of machine learning for tomatoes, and factors such as weather, temperature,
    lighting affecting these models. Some of the most prominent techniques of machine
    learning in agriculture as shown in Fig. 11 are based on supervised as well as
    unsupervised learning and reinforcement machine learning, the system includes
    linear algorithms such as linear regression or logistic regression (LR) and linear
    discriminant analysis (LDA) as well as nonlinear algorithms such as K-nearest
    neighbour (KNN), classification and regression trees (CART), Gaussian naive Bayes
    (NB) and support vector machine such as SVM regression, fuzzy cognitive map learning,
    or fuzzy clustering based approaches (Mupangwa et al., 2020) (Rehman et al., 1036).
    ML techniques vary in their accuracy and precision levels based on the application.
    Thus, selection of an algorithm based on the application is a critical step. Table
    1 illustrates the utilization of various algorithmic types, including their respective
    crops and intended purposes. Model can be made based on factors that affect agriculture
    will vary with climate, geographic zone, soil type, crop, and disease vulnerability
    (Domingues et al., 2022). Christos el al. (Chaschatzis et al., 2022) used YOLOv5
    and ResNet architecture on a novel dataset of sweet cherries to detection infected
    leaves and branches indicating the stages of disease. Larrera et al. (Larrea-Gallegos
    and Vázquez-Rowe, 2022) generated a deforestation prediction model using machine
    learning algorithms which can help in identifying crop plantation areas. Download
    : Download high-res image (104KB) Download : Download full-size image Fig. 11.
    Various machine learning algorithms supervised and unsupervised used in machine
    vision systems. Table 1. Application of various supervised machine learning algorithms
    for agricultural practices. Algorithm type Crop Feature Application Reference
    Naïve Bayes Apple Spectral feature Bruise and cultivar detection (Siedliska et
    al., 2014) Cucumber Spectral and textural features Injury based grading (Cen et
    al., 2016) Wheat Spectral features Removing contaminant from grain (Ravikanth
    et al., 2015) Cereal Colour features Vegetation Segmentation (Laursen et al.,
    2014) Citrus Textural features Leaf disease identification (Bandi et al., 2013)     Discriminant
    analysis algorithm Bell pepper Colour features Grading based on colour and defects
    (Shearer and Payne, 1990) Olives Colour features Grading based on wrinkle detection
    (Puerto et al., 2015) Pomegranate arils Colour and shape features Fruit grading
    and sorting (Blasco et al., 2009) White radish Spectral features Hollowness detection
    arils and membrane pieces (Pan et al., 2017)  SVM  Potato Colour and texture feature
    Diseases Classification (Islam et al., 2017) Chilli Colour, shape and moment invariant
    features Crop and weed classification (Ahmed et al., 2012) Rose, Bean, Lemon and
    Banana Texture features Leaf disease (Singh and Misra, 2017) Pomegranate Colour
    and texture features Leaf disease (Sannakki et al., 2013) k-Nearest Neighbour
    Blueberry Colour features Stages of maturity (Li et al., 2014) Rapeseed Textural
    features Seed type identification (Kurtulmus and Ünal, ,2015) Wheat and Barley
    grains Colour, shape, and textural features Identification based on physical characteristics
    (Guevara-Hernandez and Gil, 2011) Apple Spectral features Surface defect (Miller
    et al., 1998) 2.3.2. Deep learning algorithms Deep neural networks are a powerful
    sub-category of machine learning algorithms implemented by stacking layers of
    user-defined neural networks along the depth and width of small architectures
    (Mahmood et al., 2017). Andreas et al. (Kamilaris and Prenafeta-Boldú, 2018) identified
    16 major areas in total where deep learning techniques are most profoundly being
    used in agriculture, they are identification of weeds, land cover classification,
    plant recognition, fruit counting, and crop type classification. Most prominently
    techniques used in decreasing order are CNN, HistNN then CNN + linear regression,
    CNN + vgg16 and some RNN based models. 3. Application using the mentioned techniques
    in various domains 3.1. Weeding Weeds are plants that are considered undesirable
    in a crop. There are several reasons why weed is harmful for its neighbouring
    plants. Weeds reduce crop yields, interfere with the harvest, support pathogens
    and insect pests, and contaminate seeds (Cloutier and Leblanc, 2001). Thus, it
    becomes entirely necessary to control weeds for crop growth. The most effective
    way is to use a machine vision system, due to its higher accuracy and precision.
    J. Blasco et al. (Blasco et al., 2002) developed a non-chemical-based weed control
    method using two monochromatic machine vision cameras, one for capturing the image
    for weed identification and second for the alignment and optimization of trajectory
    of the weeding tool, weeds were eliminated using electric discharge. Chung-Liang
    Chang et al. (Chang et al., 2021) fabricated a 4-wheel rover with a mechanical
    claw rake weeding tool, as depicted in Fig. 12. One digital camera underneath
    the rover captured images. RGB colour layer and deep learning algorithm yolov3
    was used to determine the weeds. The setup detected weeds at 20 m/s with more
    than 90% accuracy. Download : Download high-res image (69KB) Download : Download
    full-size image Fig. 12. Machine vision system for a weeder robot. B.chen et al.
    (Chen et al., 2003) developed an inexpensive machine vision system for determining
    the travel directrix for a micro weeding robot for paddy fields. A macro camera
    with ccd embedded sensor that revolved 360 degrees at different heights captured
    the images. The target image was obtained by differentiating blue and black pixels
    of a binary image through the Hough transform (PKPHT) for image processing. Le
    zhang et al. (Zhang et al., 2021) developed a quadratic transversal algorithm
    for weed path planning. A mobile robot with a digital camera, a depth camera,
    a universal arm, GPS, and lidar was used. Faster R-CNN was used to recognize corn
    and weeds. Moreover, the depth camera converted the 2D acquired image into 3D
    for multitarget depth ranging and optimized path planning. A multi-camera approach
    was taken by Wu et al. (Wu et al., 2021), the system removed weeds by classifying
    plants and weeds. Multiple tactics were taken by Kunz et al. to manage weeds in
    sugar beet, maize, and soybean in order to lift weeding efficiency. Machine learning
    in precision agriculture mainly focuses on the valuation of the accuracy of object
    detection, while other real-world challenges are there to overcome. To overcome
    three challenges in the real-world deployment of precision weeding, Adrain et
    al. (Gomez et al., 2021) devised custom ML based algorithms, and five cameras
    to highlight weeds. Shanwen Zhang et al. (Zhang et al., 2021) proposed a novel
    weed species recognition system that uses digital image processing and pattern
    recognition. A CMOS digital camera captured the images. A combined system of grabcut,
    adaptive fuzzy dynamics, k means algorithm and sparse representation classification
    was developed to identify species. Bingrui Xu et al. (Xu et al., 2021) developed
    a machine vision method for seeding and weeding in a cornfield. An ultra-green
    feature algorithm was used to segment corn and land, OTSU was used to differentiate
    weeds from corn plants, and column pixel projection method was used to identify
    row positions. Chung-Liang Chang et al. (Chang and Lin, 2018) developed an agri-rover
    with a machine vision system for weeding and variable rate irrigation scheme.
    A rather simple HSV model was used for image processing, meanwhile for the purpose
    of irrigation, data of the wet distribution area of the surface soil was used.
    The moisture content data was provided to the fuzzy controller, to actuate variable
    irrigation to save water. Fig. 13 depicts a flow chart of weed detection in terms
    of image, with weed detection in (d) part. Download : Download high-res image
    (127KB) Download : Download full-size image Fig. 13. Process for weed detection.
    (a) Original image; (b) output channel; (c) feature extraction; (d) marking of
    weed position of the plant (Gomez et al., 2021). Fedrico et al. (Pallottino et
    al., 2018) developed an RGB retrofit kit for carrying out mechanical weeding in
    row crops with a variable degree of weed manifestation to benefit economically
    and to eliminate the use of chemicals. To identify weeds, the developed kit includes
    a RGB-charged CCD camera with shape analysis and colorimetric KNN clustering.
    Chung- Lyndon et al. (Smith et al., 2018) presented an innovative 3D and 2D approaches
    for tasks like weeding, harvesting, and various other agricultural activities.
    Michel et al. (Spaeth et al., 2020) devised a smart method for harrowing for integrated
    weed control to reduce herbicide use. Machine vision system was employed to adjust
    the soil treatment intensity. Two machine vision cameras were used, one at the
    front and another at the rear. By considering the difference between the crop
    soil coverage and comparing it with the set threshold value, tine angle was adjusted.
    A highly selective weeding control within crop rows requires a very accurate and
    precise guidance mechanism. Zhang et al. (Zhang et al., 2013) used a machine vision
    system to identify the crop plant and to perform a row shift mechanism to prevent
    crop damage. A swing type intra row weeding hoe actuated the weeding process;
    its navigation was done with the help of extended Hough transformation. Meng et
    al. (Meng et al., 2015) devised a machine vision system for weeding. The system
    used grey image, binarization processing and fuzzy algorithm to operate the weeding
    machine. For the detection of weeds in lettuce field Lydia et al. (Elstone et
    al., 2020) used red, green, and near infrared reflectance combined with size differentiation
    method. A wide angle RGB camera and a six led illumination system were all mounted
    on a tractor and tested on the field. Moreover, an optimization algorithm was
    also used. Wang et al. (Wang et al., 2020) developed a method to detect weeds
    using semantic segmentation technique and an encoder decoder architecture using
    deep neural network. Moreover, outdoor illumination is naturally uncontrolled
    and produce distortions in image. To eliminate this, image enhancement methods
    were used. A combination of enhancement techniques such as HE, PS-AC, DPE, MILIATO
    CNN and image representation techniques obtained an accurate model. Wu et al.
    (Wu et al., 2019) reported that the advanced machineries like weeds mowers or
    spray machines introduced to reduce human effort were proved ineffective during
    inter-row and intra-row weed removal operation due to errors thus damaging the
    healthy crops. Attempts were made to design autonomous robots that could quickly
    and efficiently manoeuvre between rows of crops for weed detection through machine
    vision as shown in Fig. 14. Chang et al. (Chang and Lin, 2018) discussed about
    the vision-based classification approaches commonly used in agricultural robots
    to detect and remove weeds growing between crops rows. The advent of Deep Learning
    algorithms like convolutional neural networks (CNN) has enabled agricultural robots
    to detect weeds even better than humans (Ralph, (2022)). The CNN algorithm gets
    real-time video feed from a calibrated camera attached to the robot to perform
    the weed detection. At times, multiple CNNs are deployed on a robot to do a specific
    task to produce more accurate results. Furthermore, CNN architectures like (Redmon
    et al., 2016) YOLO, (Liu et al., 2016) SSD, (Howard et al., 2017) MobileNet that
    are trained on large data sets such as (Lin et al., 2014) COCO, (Deng et al.,
    2009) ImageNet, (Kuznetsova et al., 2020) and Google Open Images enable researchers
    to achieve greater accuracy on their custom datasets. Chang et al. (Chang and
    Lin, 2018) and Blasco et al. (Blasco et al., 2002) have reported that RGB (Red
    Green Blue) format images were used as input for detection purposes. Obtained
    images can be further processed and converted to hue saturation value (HSV) scale
    to enhance results. Wu et al. (Wu et al., 2019) suggested that RGB + NIR (Red
    Green Blue + Near-infrared) formatted images can improve classification accuracy.
    Adaptive threshold and Image Segmentation were the commonly used algorithms to
    infer patterns from the data to perform the classification function (Chang and
    Lin, 2018) (Blasco et al., 2002). Recently, CNN has started to replace these traditional
    computer vision algorithms its capability to generalize the data, increasing robot
    predicting efficiency even in dynamic environments (Wu et al., 2019). Though most
    robots get their input feed from a single monocular camera, advanced models are
    provided with multiple cameras either to obtain the depth values or track objects.
    Extended Kalman Filter (EKF) is utilized to track weeds with great precision (Wu
    et al., 2019). Since removing a healthy plant is considered as a significant loss
    than failing to remove a weed plant, a naive Bayes classifier is used to classify
    healthy crops from weeds. Chiu et al. (Chiu et al., 2020) created a large image
    dataset for weed detection for vast fields, the dataset includes RGB and NRG images
    for developing effective algorithm to identify large weed areas. Download : Download
    high-res image (233KB) Download : Download full-size image Fig. 14. Multi-camera
    detection and tracking system to detect weeds (Wu et al., 2019). Javaid et al.
    (Wani et al., 2022) mentioned 8 data set for plant disease containing data of
    numerous crops namely bell pepper, apple, peach, squash, strawberry, rice, beans.
    And in-depth disease types of disease occurrence in potato, apple, tomato and
    rice and algorithms for image processing have also been provided. Liu et al (Liu
    et al., 2022) used machine learning for prediction of tea leaf disease that is
    Exobasidium vexans using multiple regression model by correlating disease density
    with temperature, humidity, and rainfall for IOT based application. Akhter et
    al. (Akhter and Sofi, 2022) developed a prediction model to detect apple disease
    scab in apple orchards of Kashmir valley using temperature, humidity, barometric
    pressure, ambient light sensor, and dual-axis accelerometer. This work was deployed
    using IOT devices to monitor timely and precisely. 3.2. Harvesting Harvesting
    is a process of gathering useful plant parts either manually or with a machine.
    Machine vision systems can be used to automate such processes (Chen et al., 2019,
    Ladaniya, 2008). With the help of a machine vision system, Henry et al. (Williams
    et al., 2019) designed a kiwi harvesting multi-arm robot. The system operated
    autonomously below the web to detect the hanging kiwi in orchards, with eight
    stereo colour cameras facing the hanging kiwis to measure the depth of field and
    to detect fruits. From Fig. 15, the trained neural network can successfully detect
    kiwi fruit where blue depicts the kiwi fruit’s calyx while, green represents the
    canes, and red represents wires. The autonomous system could detect, grab, and
    pick the hanging kiwi from the represented network as shown in Fig. 16. Download
    : Download high-res image (213KB) Download : Download full-size image Fig. 15.
    Kiwi fruit and environment detection through trained neural network(Williams et
    al., 2019). Download : Download high-res image (261KB) Download : Download full-size
    image Fig. 16. Kiwi fruit robotic harvesting unit(Williams et al., 2019). To eliminate
    the hand harvest of broccoli, which is considered as a tedious task and comprises
    of 35% the cost of production, Pieter et al. (Blok et al., 2016) developed an
    autonomous selective harvester for broccoli with machine vision system. Colour
    based and texture-based image segmentation was used to differentiate broccoli
    from the background. An RGB camera based on a CCD sensor was used for image acquisition,
    a boxed lighting mechanism to eliminate external light interference, and the GrabCut
    algorithm to separate broccoli from the background in an image. Zhang et al. (Zhang
    et al., 2020) used a machine vision system for developing navigational path for
    a combined rice harvester. Moreover, it was identified in statistical findings,
    that the Cr component of YCbCr colour space possessed less intra-regional difference
    while the inter-regional difference was very high, making it easier to identify
    rice boundaries. Guru et al. (Guru et al., 2012) developed a machine vision system
    to classify tobacco leaves and to harvest them. The model developed consisted
    of three stages that are image segmentation, feature extraction, and classification.
    CIEL*a*b colour model was used for extracting the leaf image from the background
    because of its high efficiency and kNN for classification. Suraj et al. (Amatya
    et al., 2017) developed a system for cherry harvesting that involves shaking the
    plant. A precise mechanism was developed for shaking of stem, RGB images were
    used to identify the location of shaking and 3d camera for calculating the depth,
    the cherry trees were arranged in Y-trellis and vertical trellis systems and this
    system takes in consideration branches that are partially visible, not visible,
    and the branch section not satisfying the equation. With that, we can predict
    the number of shakes. For robotic harvesting system of eggplants, Hayashi et al.
    (Hayashi et al., (2002)) developed a RGB based system using ccd camera to identify
    the plants. The system was mounted on the end effector situated on a moving platform.
    They designed a fuzzy feedback control for the manipulator. As each system requires
    a distinguish machine vision algorithm for detection, the system used a colour
    morphological based detection algorithm. An end effector suction pad and rubber
    actuated mechanism used for grabbing, and a scissor mechanism with a guided bar
    was used for harvesting. Nasirahmadi et al. (Nasirahmadi et al., 2021) developed
    a system that could detect damage on sugar beet during the time of its harvesting
    using machine vision system coupled with deep learning. A machine vision models
    can be affected by various factors such as lighting condition, level of noise,
    quality of the apparatus for image acquisition. To make the approach robust, it
    must be coupled with deep learning techniques for external damage detection through
    2d digital images. The cameras installed over the high-speed cleaning turbine,
    and high-speed cameras were used to eliminate blur. As Cracks, breakage, and surface
    abrasion were considered as damage for the work. RCNN and RFCN were used for damage
    detection. Kuznetsova et al. (Kuznetsova et al., 2020) designed a vision system
    with YOLOv3 algorithm for detecting apples in orchards. The algorithm was able
    to detect an apple with an average time of 19 ms and reported 7.8% objects were
    mistaken for apples and 9.2% were not recognized. The same system can also be
    applied to orange harvesting robots. The machine vision developed by Financial
    University (Kuznetsova et al., 2020) utilized two stationary Sony Alpha ILCE-7RM2
    cameras with Sony FE24-240 mm f/3.5–6.3OSS lenses and one Logitech Webcam C930e
    camera, mounted on the second movable shoulder of the manipulator before the grip.
    The two Sony cameras took a general far-view shot of the canopy to detect the
    apples and decide the optimal route for the manipulator, and the Logitech camera
    positioned the grip appropriately for the harvesting process. To improve the quality
    of detection, the images were pre-processed by increasing the contrast through
    histogram normalization. Contrast limited adaptive histogram alignment (CLAHE)
    with 4 × 4 grid size and clip limit set to 3 was applied with the median filter
    of 3 × 3 kernel which gave a slight blur. The morphological opening with a flat
    5 × 5 square structuring element was used to thicken the borders. Due to this
    pre-processing, the effects of shadows, glare, minor damages of apples, and thick
    branches overlapping the apples were avoided. Unfortunately, the algorithm detected
    yellow leaves as apples, but this was prevented by disregarding objects whose
    ratio of the greater side of the circumscribed rectangle to the smaller one was
    more than 3. To avoid recognizing the gap between the leaves for apples during
    the post-processing, the objects whose area of the circumscribed rectangle was
    less than the threshold were discarded. However, many apples in the canopy images
    remained undetected, that can be seen in the close-up pictures where the small
    apples are smaller than the anchor box. To tackle this, increasing the number
    of anchor boxes was not a viable since it increased the computation time. So,
    the images were cut into k2 parts (k = number of times the apple in a canopy is
    smaller than the anchor box). k = 3 was chosen since larger k values will require
    higher resolution images, and higher k values produced worse results (Fig. 17)
    to detect and decrease the computation time. Kailasam et al. (Kailasam et al.,
    2022) developed an IOT based crop maintenance system, it can early detect plant
    diseases with the help of threshold segmentation and random forest classification.
    Download : Download high-res image (217KB) Download : Download full-size image
    Fig. 17. 48 apples found in far-view after pre-processing (Kuznetsova et al.,
    2020). 3.3. Fruit counting and yield estimation Detecting and counting individual
    fruits in a plant is the preliminary step in the pre-harvesting phase called yield
    estimation. Mekhalfi et al. (Mekhalfi et al., 2020) utilized the total number
    of fruits as a parameter to estimate the logistic factors like transport, labour
    requirements, current demand in the market, etc. To benefit farmers economically
    by providing them insight to plan and organize the facilities required for harvesting.
    Mekhalfi et al. (Mekhalfi et al., 2020) Dorj et al. (Dorj et al., 2013) and Zhou
    et al. (Zhou et al., 2012) performed background subtraction in the captured image
    as the first step before image processing, to reduce unwanted noise like background
    leaves and branches such that it does not add false positives during the detection
    stage. Mekhalfi et al. (Mekhalfi et al., 2020) obtained the input image in the
    RGB format and converted it to the (Lightness, Channel a, and Channel b) LAB space
    format to remove noise. A foreground-background is generated from this LAB image’s
    “A” channel using thresholding (Otsu, 1979). Further, a morphological image processing
    technique called dilation enhances the mask, which is applied to the actual image
    to detect the fruits. The significant challenges for the detection system were
    to detect fruits of different shapes, orientations and brightnesses. To solve
    this issue Mekhalfi et al. (Mekhalfi et al., 2020) focused on counting kiwi fruits
    and proposed a technique to focus on the tip of the fruit rather than the whole
    fruit during the detection phase using the Viola-Jones algorithm. This method
    gave 6% and 15% error rates at two different kiwifruit orchards during their test
    trials. Dorj et al. (Dorj et al., 2013) have discussed various techniques for
    tangerine detection and counting. Initially, the background noise from the acquired
    RGB image is manually removed and the image is divided into 72 sub-images. The
    sub-images are converted to the YCbCr (Green (Y), Blue (Cb), and Red (Cr)) colour
    space. An image histogram is computed on the Cb (Blue) chrominance component followed
    by the thresholding operation. The number of connected objects is counted to determine
    the number of fruits. Zhou et al. (Zhou et al., 2012) discussed on counting and
    detecting apple fruits in an orchard, particularly in June drop, when a large
    and unpredictable number of fruits might drop as an alternative to the yield estimate
    during the flowering stage as performed by Aggelopoulou et al. (Aggelopoulou et
    al., 2011). Images were acquired when the fruits were light green immediately
    after the drop period during early summer. A white drape was placed behind the
    target trees to remove background noise like leaves, sky, and branches, and a
    red sphere was placed on top of the tree for size calibration of the apples. At
    the end of June, the fruits started turning red, and on the R-B (Red – Blue) scale,
    the R-value was greater for the apples when compared to their surroundings. Thus,
    a threshold value of R-B greater than 40 was chosen to mask the fruits, and anything
    below this will have its pixel value set to zero. The algorithm erratically generated
    a few false positives as it labelled the leaves as fruits, and therefore a threshold
    value of less than 20 was set in the G-R colour scale to improve the prediction
    accuracy. Both thresholds were combined to make predictions on the final images.
    A connected components algorithm was used to count the fruits from the generated
    mask. To prevent miscalculations in cases of overlapped fruits in a cluster, the
    connected domain area of the apples was compared with that of the red sphere used
    for calibration. If the area exceeded 400 pixels, it would be counted as two fruits,
    whereas if the area was less than 30 pixels, it would be counted as just one fruit.
    Bini et al. (Bini et al., 2022) used colour thresholding combined with a median
    filter to calculate tomato yield; the r2 for the algorithm was found to be 0.98
    in comparison to manual picking. Amanda et al. (Jacques et al., 2018) developed
    a machine vision system for real-time detection of shallot onions while harvesting,
    using colour thresholding and the Otsu thresholding selection method for detecting
    and counting onions. Amatya et al. (Amatya et al., 2016) developed a method for
    locating shaking position for automated cherry harvesting based on branch and
    cherry pixel location RGB and stereo camera. The overall root mean squared error
    for estimating the distance to desired shaking point was 0.064 m. 3.4. Overlapping
    of fruits The identification of the object of interest and the accuracy of fruit
    detection by machine vision can be significantly hindered by the presence of overlapping
    conditions such as mixed branches or neighbouring fruits, which can obscure the
    region of interest as shown in Fig. 18. Guo et al. (Guo et al., (2019)) developed
    a method to detect overlapped lychee fruit by using monocular machine vision.
    A combination of techniques such as contrast limited adaptive histogram equalization
    (CLAHE), red/blue chromatic mapping, Otsu thresholding and morphology operations
    were used to segment foreground images of lychee. A three-point circle-based process
    extracted each lychee from its overlaid cluster, and precision, recall, and f1
    score were obtained using local binary pattern support vector machine. The result
    provided enhance detection rate for the lychee. Winter jujube is a fruit that
    occurs in China, Zhiheng et al. (Lu et al., 2021) developed a method using hand
    based and Yolov3 to differentiate and detect overlapping fruits. Moreover, it
    was found that yolov3 detection percentage was 5% greater than thresholding. While
    harvesting apples, the orchard environment becomes noisy in terms of overlaps,
    thus making it difficult for the image processing unit to segment the apples from
    the image, Meng et al. (Meng and Wang, 2015) used boundary tracking and some image
    segmentation techniques to recognise and extract overlapping apple from the environment.
    Firstly, image segmentation removed the background on bases of colour (YUV colour
    space) thereafter for boundary tracking two processes were performed to determine
    the boundary of the fruit to extract each apple from its cluster, a run length
    encoding was used to label every region of the binary image and area was calculated.
    The noise area was eliminated by averaging. Download : Download high-res image
    (147KB) Download : Download full-size image Fig. 18. Segmentation of Overlapped
    fruits over each other, leaves, branches (Zeng et al., (2009). Secondly, small
    holes were compensated using seed filling method. To separate boundary, pixel
    wise calculation was done and mathematical calculations to determine the boundary
    of the apple. Several methods have been developed for the daytime, Longsheng et
    al. (Fu et al., 2015) developed a method to recognize kiwifruit in night condition.
    Artificial lighting methods ranging from 30 to 50 Lx, a RG colour method, and
    a method for image processing using canny operator was deployed to detect the
    boundary of the fruit and OTSU thresholding to properly detect the fruit. To differentiate
    merged boundaries and individual fruits, a Hugh transformation was done to depict
    the most elliptical shape. Bazame et al. (Bazame et al., 2021) developed a method
    to differentiate coffee fruits from a video graphic image where fruits were getting
    discharged from the conveyor harvester. A 23-layer neural network structure comprising
    convolutional, maxpool, yolo and up sampling was used for the task to achieve
    high processing speed. A CMOS camera supported by a 21w LED system to capture
    images and the camera was stabilized by the gimbal. The model''s accuracy was
    somewhere between 80% and 86%. Jia et al. (Jiao et al., 2020) proposed a robot
    vision detector based on the Mask Region Convolutional Neural Network (Mask R-CNN)
    as shown in Fig. 19. The input parameters for feature extraction were drastically
    reduced by Residual Network and Densely Connected Convolutional Networks (DenseNet).
    The feature maps were used as input for the Region Proposal Network (RPN) to generate
    the region of interest (ROI). With a full convolution network (FCN), the mask
    was generated to identify the apple region. Further, Jia et al. (Jiao et al.,
    2020) reported a precision rate of 97.31% and a recall rate of 95.70%. Images
    were captured using a camera with a 6000 × 4000-pixel resolution in different
    lighting conditions. The images were pre-processed by altering the orientation
    and contrast in order to prevent over-fitting and also to make the dataset more
    diverse. Since robots usually move, the captured images may be blurred, and to
    mitigate this problem, the length was set to 512 pixels. The width was also changed
    accordingly to maintain a constant aspect ratio, which would help train the algorithm
    for real-world problems. ResNet was used to train a deeper CNN network, which
    would not be possible with an increase in training due to large number of layers
    of deep convolutional network. Since the image resolution gets lower in deeper
    layers, DenseNet is implemented to improve the backbone network. Thereafter a
    robust marker-controlled watershed transform algorithm to automatically perform
    the accurate segmentation of overlapping plant fruits. The small-sized tomatoes
    are green in nature and are difficult to detect and distinguish from the green
    background, sun et al. (Sun et al., 2020) provided an improved feature pyramid
    network for tomato organ recognition to tackle this problem. The algorithm could
    predict with an accuracy of 99.95%. Download : Download high-res image (128KB)
    Download : Download full-size image Fig. 19. Model of improved mask R-CNN (Jiao
    et al., 2020). 3.5. Sorting Zhiheng et al. (Zhang et al., 2018) used of yolov3
    model to detect fruit with a 96% accuracy rate. The algorithm performed grading
    operation on the fruit with an accuracy of 97.28%. When done manually, sorting
    struggles to maintain consistency and uniformity. To eliminate such problems,
    Nandi et al. (Nandi et al., 2013) developed a machine vision-based system for
    grading and sorting mango fruits using fuzzy logic. Four batches of mangoes were
    considered for a blue colour conveyor belt for getting more accuracy with RGB
    model and 120Lx light using CCD camera. To remove the noise, a simple median filter
    was used, while to maintain ease in computation, an additional pseudo-median filter
    was deployed. Since the boundary of the mangoes is not complex, a graph contour
    tracking method based on chain code was used to detect the boundary of the mangoes.
    Some alignment techniques were employed to align the mangoes, such that obtaining
    the apex and stalk region of the mango. To determine the maturity of mangoes and
    sort them accordingly, a RGB averaging and gaussian mixture model was used. It
    was found after comparing the result with manual sorting and grading that the
    machine vision system was highly accurate. The variation of detection of mangoes
    from manual and machine vision system was not more than 2 %. The performance data
    is given below. Similar work was done by Khaled et al. (Mohi-Alden et al., 2022)
    to grade bell peppers based on 5 different by datasets, DCNN was used, ResNet50
    of DCNN was replaced with classifiers for global average pooling layer, the dense
    layer, batch normalization, and the dropout layer. Rokunuzzaman et al. (Rokunuzzaman
    and Jayasuriya, 2013) developed a low-cost machine vision system to sort tomatoes
    for 3 defects that occurs on tomato, first blossom end rot, second cracked and
    third calyx, differentiation was done on the base of thresholding techniques over
    a conveyor belt and sorting was done with a push mechanism that pushed the defected
    tomato for image processing. Red green and hue levels were considered, for identifying
    first defect red colour was used, for second and third green colour and parameter-
    based differentiation was considered, blob extraction was done to label the segmented
    images, all the processed data was then fed to a neural network to sort the defected
    tomato. Comparative research has been done to identify and grade tomatoes. The
    maximum accuracy obtained was 80.50%, while Kumar et al. (Dhakshina Kumar et al.,
    2020) devised a tomato grading and sorting system that provided accuracy of 97.74%,
    using a cascade approach of two SVM classifiers. The system operates in three
    stages: first, to classify tomatoes based on their species by considering shape,
    size, and texture; second, to classify them based on ripe and unripe and finding
    infected regions based on the Gabor wavelet transform; and third, to identify
    defects like black spots, canker, and melanoses based on colour and geometric
    features. 24 features of tomatoes were extracted. To do non-linear classification,
    a support vector machine (SVM) binary classifier was used to find the optimal
    hyper plane to separate two classes. ElMasry et al. (ElMasry et al., 2012) developed
    a method to sort potatoes based on their physical characteristics such as parameter,
    centroid, area, moment of inertia, length, width, and Fourier transform. All the
    perimeters helped differentiate potato based on two shape factors and Fourier
    descriptors. The potatoes were moving on the conveyor, a CCD camera coupled with
    a frame grabber was used to capture data. The grading system could successfully
    grade based on the size of the potatoes. A novel tomato ripening methodology was
    developed by Jang et al. (Ko et al., 2021), results for 5 classes were predicted.
    The method combined convolutional neural network and stochastic decision fusion;
    the workflow can be seen in the Fig. 20. Download : Download high-res image (219KB)
    Download : Download full-size image Fig. 20. The workflow for tomato ripeness
    (Fu et al., 2015). 3.6. Seeding Dong et al. (Dong et al., (2019)) monitored the
    performance of hybrid rice seed sowing using machine vision techniques. The seedlings
    in the pot tray were passed through an illuminated cabinet, as depicted in Fig.
    21. Download : Download high-res image (140KB) Download : Download full-size image
    Fig. 21. Seed detection apparatus used by Dong et al. (Dong et al., 2019). Image
    processing was done using of HLV thresholding model and exploratory methods, The
    algorithm detected missing seedlings. The model’s average accuracy was nearly
    95.8%. Chen et al. (Chen et al., 2019) developed a machine vision system for identifying
    the number of seed particles of rice or carrot. The system employed a mask function
    coupled with a square algorithm to emphasize the area containing relevant information
    while disregarding other areas. The algorithm significantly enhanced the contrast
    and brightness of the image, thereby facilitating binary image processing. The
    system then applied geometric matching and morphological experiments, followed
    by particle analysis, which included a particle algorithm, binarization thresholding,
    and binary particle analysis to arrive at the final detection result. Bai et al.
    (Bai et al., 2021) developed an automated seeding system for missing sweet corn
    detection. Outline extraction and skewness correction were done for image pre-processing,
    to identify missing seeds and precisely identify thresholding, contour extraction
    and minimum area rectangle extraction was done, thereafter the median filter and
    morphological processing was performed. In addition, RGB, HSV and Voting based
    algorithm were also used. A pneumatic seed planter can be depicted in the Fig.
    22, missing seeds through this machine was a problem in Phillipines, Borja et
    al. (Borja et al., 2018) integrated a machine with machine vision inside it to
    prevent missing seeds. Download : Download high-res image (86KB) Download : Download
    full-size image Fig. 22. (a)Prototype machine vision planter, (b) Simplified diagram
    of the seed meter (Kanagasingham et al., 2020). 3.7. Path finding The proper function
    of an autonomous robot or vehicle in an agricultural system is to carry out tasks
    in a specific location without causing damage to the plants, is heavily dependent
    on a guidance system. Machine vision which combines various algorithms and techniques
    is a crucial aspect of this guidance system. Rovira-Mas et al. (Rovira-Más et
    al., 2008) devised a machine vision based automated tractor guidance system that
    used a CCD camera with near infrared filter for driving across the crop rows precisely.
    The vision system output was used to steer the vehicle in a row. The vision system
    employed dynamic thresholding within the selected region of interest to binarize
    the image. Then utilized the midpoint encoding method to convert white pixels
    to a one-pixel width row, and applied a Hough transform subsequently. The data
    obtained was then utilized to guide the tractor through the field. Rovira-Mas
    et al. (Rovira-Más et al., 2008) generated a three-dimensional (3D) terrain map
    using a stereo camera, a localization sensor, and an inertial measurement unit
    (IMU). The stereo camera captures the fields and generates a 3D point cloud, then
    transformed into geodetic coordinates and assembled it to a global field map.
    It was shown that this system could generate 3D field maps with appropriate accuracy
    for field robotics. Sabeethan et al. (Kanagasingham et al., 2020) developed an
    autonomous guided robot that can precisely guide its way through a rice crop row
    without damaging the plant. A digital camera with wide angle lens was used. This
    novel crop row detection algorithm was developed combining the effect of 3D, it
    could predict the path through GNSS path planning, compass bearing selection and
    vision system. Bei he et al. (He et al., 2011) used MVS algorithms for recognition
    of the navigation path for a harvest robot for orchards based on machine vision,
    then created a binary image using OTSU segmentation. thereafter generated line
    using least square method, the intersection points of the lined fruit trees with
    respect to the ground were taken as features which can be depicted from the Fig.
    23. Download : Download high-res image (105KB) Download : Download full-size image
    Fig. 23. (a) The tree trunk area (b) Feature point nodes (c) Navigation line detection
    (Moallem et al., 2017). A novel approach was presented by Opiyo et al. (Opiyo
    et al., 2021) for navigation based on the extraction of medial axis. The feature
    extraction was done through grey scaling, Gabor filters, Principal Component,
    Analysis (PCA), K-mean clustering and medial axis algorithm. Based on that a path
    line was drawn for navigation. To control the guiding, 2 fuzzy logic-based controllers
    were deployed. Since, predicting path using hough transformation consumes time
    while using the least square method give low accuracy, Chen et al. (Chen et al.,
    2021) provided a novel point prediction based on the hough transformation technique
    and an improved grayscale method for path finding in optimum time. Various recent
    works related to the path finding and application have been mentioned by Vrochidou
    et al. (Vrochidou et al., 2022). 3.8. Soil classification Assessing the properties
    of soil, such as its moisture content or nutritional value, is a critical process,
    as the success of farming is contingent upon it. In recent years, various techniques
    based on machine vision systems have been developed to facilitate this purpose.
    Ajdadi et al. (Rahimi-Ajdadi et al., 2018) developed a rapid and non-contact system
    to measure soil water content using machine vision system. Since the colour of
    the soil is the most important factor in determining the moisture level, the colour
    was utilized as a feature for the machine vision system. Moreover, a slit change
    or instability in light can majorly affect the process; thus, to eliminate such
    problem a dome shape chamber was made with a square shape light on its top. 16
    levels of moisture were identified, and the model was developed to distinguish
    between them. Based on the RGB model, nine descriptive statistics were extracted.
    To predict the soil moisture content, two methods were adopted: an adaptive Nero-Fuzzy
    237 inference system (ANFIS) and stepwise multiple regression. Peter et al. (Riegler-Nurscher
    et al., 2020) developed a machine vision system to measure soil roughness and
    to measure tillage of the attached machine during seeding operation, the apparatus
    was attached to a cart with the tractor. The Stereo camera was used to measure
    the soil roughness, depth information helped measure the roughness, large height
    variations were indicative of rough soil and larger aggregate sizes. Four operations
    were carried out: first, with the use of block matching algorithm, stereo matching
    was done, then point cloud processing, followed by parameter estimation, and thereafter
    roughness estimation. A soil trace-based navigation system for tractors using
    machine vision was developed by Kiani et al. (Kiani et al., (2012,5).) for land
    and tilling operations, the information from previous ploughs in the field was
    used as a feature. A CCD colour camera captured the image of the ploughed, subsoiled,
    and furrowed soil then a Hough transform was done to highlight the path. Once
    the significant pattern of the path was obtained, the path was detected for movement
    of the vehicle for the respective three condition. Christos et al. (Chaschatzis
    et al., 2022) integrated different steps of Information Communication Technologies
    (ICT) solutions to assist crop monitoring, livestock management and farm management
    through information sharing for better decision making and practices.(See Table
    2, Table 3, Table 4, Table 5, Table 6, Table 7, Table 8, Table 9, Table 10, Table
    11, Table 12) Table 2. Application of various unsupervised machine learning algorithms
    for agricultural practices. Algorithm type Crop Task Application Reference K-Means
    Clustering Apple Defected pixel segmentation and classification Defects and disease
    detection in fruits (Dubey and Jalal, 2013) Banana Banana Finger and Flaws Segmentation
    Fruit grading (Hu et al., 2014)  Fuzzy clustering Maize Identification Row detection
    (Romeo et al., 2012) Wheat Identification Diseased leaf (Mondal and Kole, 2016)
    Table 3. Application of various Deep learning algorithms for agricultural practices.
    Type of objective Various Deep Algorithms being used Crop yield estimation Author-defined
    CNN, Five-unit LSTM, GatedRecurrent Unit (GRU) Fruit counting Modified Resnet
    CNN, CNN, Linear regression, Combined CNN + Linear Regression, Faster Region-based
    CNN with VGG16 model Soil moisture Deep belief network based macroscopic cellular
    automata (DBNMCA), Stepwise multiple regression, ANN Weed detection PCANet + LMC
    classifiers, Variation of VGG16, DenseNet CNN, Adapted version of Inceptionv3
    + lightweight DCNN + set of Klightweight models as a mixture model (MixDCNN),
    Based on DetectNet CNN, SNN, CNN Crop type classification Author-defined CNN,
    Adapted version of VGG16, three-unit LSTM, CNN + HistNN Plant disease detection
    LeNet CNN, AlexNet CNN Table 4. Various Weed, Pest and Disease detection using
    machine vision. Crop Algorithm Disease/Weed/Pest type Reference Peanut field Em
    Yolo tiny Rust disease (Zhang et al., 2022) Onion, Soybean, Corn, Beans, and Rice
    RGB based Row detection based (Terra et al., 2021) Peach orchards Colour-depth
    fusion segmentation method Spray row detection (Gao et al., 2020) Pear Segnet
    Fruit (Kim et al., 2020) Lettuce Non-linear Bayesian Discriminant analysis, Color
    thresholding Weed (Blasco et al., 2002) Sugar beat Discriminant analysis Leaves
    occlusion and their overlaps (Jafari et al., 2006) Wild Blueberry Linear and quadratic
    classifiers (DM-HSIS) Goldenrod weed spot (Rehman et al., 2019) Sugar beet Principal
    component analysis, ANN and SVM Shape features including moment invariants and
    Fourier descriptors (Bakhshipour and Jafari, 2018) Maize Random Forest Convolvulus
    arvensis, Rumex, Cirsium arvense (Gao et al., 2018) Soybean CNN Cephalanoplos,
    Digitaria, Bindweed (Tang et al., 2017) Pea MLC, VI, and SAM Diplotaxis spp. (Castro
    et al., 2012) Sugarcane SVM Sugar cane borer disease (Huang et al., 2018) Bell
    peppers PCA-based Algorithm, CV Algorithms Tomato spotted wilt virus, Powdery
    mildew (Schor et al., 2016) Grape BP Networks Classifier Grape downy mildew, Grape
    powdery mildew (Wang et al., 2012) Paddy GeneticAlgorithm, Rule Generation Algorithm
    Bacterial blight, Leaf brown spot, Rice blast, Sheath rot pest (Phadikar et al.,
    2013) Strawberry greenhouse SVM Thysanoptera pest (Ebrahimi et al., 2017) Green
    leaves Multispectral MVS (NIR, UV, VISIBLE) Invertebrate pests (Liu and Chahl,
    2018) Crop CNN 33 class insects (Kasinathan et al., 2021) Rice k-means, ANN Blast
    disease (Ramesh and Vydeki, 2018) Apple leaves YOLOv4 Venturia inaequalis, Gymnosporangium
    juniperi-virginianae (Roy and Bhaduri, 2021) Table 5. Machine vision for harvesting
    for various crop types. Type of crop Application Features Algorithm Reference
    Kiwi Novel multi arm robot FCN-8S for features Stereo point matching, Kiwifruit
    clustering, Scheduling algorithm (Williams et al., 2019) Broccoli Autonomous selective
    harvester Area of connected textures Color based segmentation, Dice similarity
    coefficient (Blok et al., 2016) Rice Combine harvester YCbCr and target demarcation
    Hierarchical clustering method and polynomial fitting method (Zhang et al., 2020)
    Tobacco leaf – GLTP (Gray Level Local Texture Patterns), LBP (Local Binary Pattern)
    and LBPV (Local Binary Pattern Variance) K-Nearest Neighbour (K-NN) based on Euclidean
    (Guru et al., 2012) EGG plants Intelligent rover-based robot Color characteristics
    and morphological features Logical operations (Nasirahmadi et al., 2021) Harvesting
    time for fresh tea leaves Guiding rail Color characteristic and geometric properties
    B-G algorithm, Median filter algorithm, Otsu algorithm, Bayesian discriminant
    principle (Zhang et al., 2019) Date fruit – Type, Maturity, Harvesting decision
    AlexNet and VGGNet (Altaheri et al., 2019) Tomato – Geometric, Random noise CNN
    (Zhang et al., 2018) Table 6. Machine vision for Overlapping fruits detection.
    Crop Algorithm Accuracy Reference Lychee overlapping CLAHE, Red/blue chromatic
    mapping, Otsu thresholding, Relative position relation, Three-point circle, LBP-SVM
    87% (Guo et al., 2019) Winter jijube overlapping YOLOv3 97.28% (Lu et al., 2021)
    Kiwi overlapping detection at night OTSU thresholding and hough transformation
    _ (Fu et al., 2015) Overlapping fruit segmentation Novel marker-controlled watershed
    transform Accuracy nearly 99% (Zeng et al., 2009) Small size immature tomato Improved
    featured pyramid network Accuracy 99.5% (Sun et al., 2020) Overlapping citrus
    Mask R-CNN, Convex shell algortihtm, Shi-Tomasi corner detection algorithm, least
    squares fitting method – (Longye et al., 2019) Grapes Phenotypic characteristics
    Edge detection, Contour fitting improved HED (Miao et al., 2021) Table 7. Performance
    table (Nandi et al., 2013). Variety Q1 Q2 Q3 Q4 Expert Systems Expert System Expert
    System Expert System KU 91.2 90.4 89.4 88.7 90.3 89.4 90.2 89.4 AM 90.7 90.1 90.0
    89.1 88.9 88.2 90.1 89.4 SO 90.2 89.7 90.7 89.1 90.0 88.3 89.8 89.5 LA 91.1 90.5
    90.9 88.6 89.2 88.8 91.0 89.7 HI 90.1 89.4 90.6 90.0 90.4 89.1 91.3 90.0 Table
    8. Machine vision for sorting and grading. Application Algorithm Accuracy Reference
    Grading and sorting tomato Cascading SVM classifier, Binary classifiaction, Gabor
    wavelet transformation Average accuracy 97.74 % (Dhakshina Kumar et al., 2020)
    Sorting irregular potatoes Fourier transform, Stepwise linear discriminant analysis
    Accuracy of 96.6% for inline and 100% for perfectly shaped (ElMasry et al., 2012)
    Sorting and grading mangoes Gaussian mixture model, Size from binary image, fuzzy
    Logic Accuracy ranging from 88% to nearly 92% (Nandi et al., 2013) Sorting of
    bell peppers Modified DCNN Accuracy of 96.6% (Mohi-Alden et al., 2022) Tomato
    ripeness-based sorting Combined ConvNet of YoloV3 and SDF Accuracy 96% (Ko et
    al., 2021) Potato grading based on size Color space conversion, Partial least
    squares discriminant analysis Accuracy 86% (Islam et al., 2021) Carrot shape-based
    sorting HOG descriptor, KNN, K Fold, CNN – (Sharma et al., 2021) Infrared based
    pineapple grading Otsu thresholding, LDA, QDA, SVM, KNN, Decision tree, naive
    bayes Maximum accuracy with SVM (Mohd Ali et al., 2022) Golden apple grading based
    on thermal imaging SVM, KNN, MLPs 92.50% (Moallem et al., 2017) Table 9. Machine
    vision systems for seeding. Application Algorithm Accuracy Reference Missing sweetcorn
    detection Thresholding, Contour Extraction, Minimum area Rectangle extraction,
    Median filter, Morphological Processing Average accuracy 98 % (Bai et al., 2021)
    Hybrid Rice Pot-Tray Sowing Threshold segmentation & exploratory analysis method
    Average accuracy 95.68% (Dong et al., 2019) Missed seeding in pneumatic corn planter
    detection RGB, OTSU thresholding – (Borja et al., 2018) Table 10. Various vision
    systems for path finding. Working Principle Algorithm Application Reference To
    steer vehicle Binarization, Dynamic thresholding Over Rough Terrain (Rovira-Más
    et al., 2008) Navigation Path for apple orchard OTSU segmentation, region segmentation
    Ground and tree intersection points (Opiyo et al., 2021) Autonomous navigation
    Gabor filters, Principal Component, Analysis (PCA), K-mean clustering and medial
    axis algorithm Removing environment except the path (Chen et al., 2019) Navigation
    for green house cucumber robot Novel prediction point hough transform, Improved
    grayscale Based on point prediction (Chen et al., 2021) Under-plant canopy robot
    Extended Kalman Filter, CNN, ResNet-18, ImageNet Crop row vanishing point, and
    distance ratio (Sivakumar et al., ,2021) Monocular guidance Level-adjustment thresholding
    to discriminate rows from gaps, an averaging technique using a viewport to locate
    rows, rregression analysis to fit the best line 0.020 m accuracy at a speed of
    1 m/s (Billingsley and Schoenfisch, 1997) Tractor localization for vineyard Extended
    Kalman Filter (EKF), adaptive data selection, Lower part of mage cropped, green
    plane, extraction from cropped image, thresholding to, extract path plane, filtering
    to remove noise, centroid of path plane determination Tested on a GEARs Surface
    Mobility Platform in a laboratory setting and in a peach orchard (Corno et al.,
    2021) Adaptive vision navigation for smart agri robots SURF for feature extraction
    and matching to obtain feature pairs, confidence density image construction by
    integrating the enhanced elevation image and the corresponding binarized crop
    row image Tested on a smart agricultural robot manufactured in Shanghai, China
    on S-type and O-type in-lab simulated crop plant leaves paths (Zhang et al., 2013)
    Table 11. Machine vision systems for soil classification. Application Feature
    Algorithm Reference Moisture levels RGB Nero fuzzy 237, Stepwise multiple regression
    (Rahimi-Ajdadi et al., 2018) Soil roughness Block matching algorithm, RGB - (Riegler-Nurscher
    et al., 2020) Soil 3d reconstruction Stereo binocular method Shift algorithm and
    depth mapping (Chaschatzis et al., 2022) Rapidly predicting SOM Color space transformation
    K means clustering (Azizi et al., 2021) SOM and SMC Histogram analysis, colour
    space conversion ANN, Cubist, Exponential GPR (Taneja et al., 2021) Table 12.
    Machine vision systems for pesticide spraying. Crop Algorithm Disease Reference
    Apple leaf Mask R-CNN Rust disease (Storey et al., 2022) Onion, Soybean, Corn,
    Beans, and Rice RGB based row detection based (Terra et al., (2021,5)) Pear orchards
    Colour-depth fusion segmentation method Spray row detection (Gao et al., 2020)
    Tulip Multispectral, RGB Tulip breaking virus (Polder et al., 2012) Leaf spot
    and white flies Surf algorithm Disease (Dhumale and Bhaskar, 2021) Pear SegNet
    Fruit (Kim et al., 2020) Azizi et al. (Azizi et al., 2021) estimated the soil
    surface roughness using a stereo approach in which they reconstructed the soil
    in 3d using disparity calculation with the equations one and two mentioned below.
    It was used over two different images obtained from binocular method. The coordinates
    between two points were determined by sift algorithm, the workflow of their method
    can be seen from Fig. 24. (1) (2) Download : Download high-res image (119KB) Download
    : Download full-size image Fig. 24. Workflow for 3d reconstruction of the soil
    surface(Chen et al., 2021). A novel method was developed by Gorthi et al. (Gorthi
    et al., 2021) for predicting organic matter in the soil based on soil image segmentation
    and a tree-based pipeline optimization tool to determine optimum ML scheme that
    can be used to predict model score with sufficient accuracy. Taneja et al. (Taneja
    et al., 2021) developed a method to determine the soil moisture and soil organic
    matter content using machine vision with a cell phone. In this approach, numerous
    machine learning model such as linear regression, Decision/Regression Trees, Support
    Vector Machines (SVM), Gaussian Process Regression (GPR), random forest and cubist,
    and other models including Artificial Neural Network (ANN) were compared. The
    best score for moisture levels were obtained through exponential gaussian progression
    model and cubist model while for the soil organic model, ANN and cubist provided
    the most accuracy. 3.9. Pesticide spraying The autonomous spraying system incorporates
    two core technologies that are sensing technology for the region of interest detection
    and a robotic spray execution or chemical spray system, this can be observed in
    the Fig. 25 which depicts smart spraying system (Song et al., 2015). Download
    : Download high-res image (165KB) Download : Download full-size image Fig. 25.
    Smart spraying system. To do precise spraying in peach orchards, which was difficult
    due to the dense canopy and complex background Gao et al. (Gao et al., 2020) developed
    a path planning algorithm based on a colour depth vision system. The system intelligently
    uses colour and depth for path planning. In image segmentation, a combination
    of RGB green segmentation with HSV green segmentation on ROI was used. For distance
    segmentation, a k-means algorithm was initially applied, followed by segmentation.
    A combination of both image segmentation and distance segmentation was performed
    to form colour-depth fusion segmentation that provides accurate path planning.
    Kim et al (Kim et al., 2020) deployed a smart pesticide spraying system over a
    moving rover for pear orchards. A SegNet structure was used for the purpose, as
    well as semantic segmentation. Five distinct classes of pears were used to train
    the model. The background environment posed a problem for accurate detection of
    pears, to prevent background noise, depth data was used from the RGB-D camera.
    All this was mounted on the moving rover platform, and the spraying was done with
    the help of 4 nozzles attached on both sides, as can be seen from the Fig. 26.
    In addition, a mapping mechanism was developed considering the nozzle orientation,
    position, and area of spray. Download : Download high-res image (316KB) Download
    : Download full-size image Fig. 26. Moving rover with attached sprayer(red) and
    camera(green) (Kim et al., 2020). (For interpretation of the references to color
    in this figure legend, the reader is referred to the web version of this article.)
    For apple orchards, Storey et al. (Storey et al., 2022) developed an intelligent
    spraying system using a machine vision system to reduce chemical usage and prevent
    unwanted splattering of pesticides during spraying. The system could identify
    apple leaf disease such as rusting and can extend its use to identify other crop
    disease and weeds for reducing the spray chemical usage. A mask R-CNN network
    was used for segmentation, this system was based on backbones (ResNet 50, MobileNet
    v3 layer and MobileNet layer mobile) that were combined for leaf and disease detection,
    this network can precisely detected rust on leaves. At the end of ResNet 50, a
    disease detection confusion matrix was used to detect combinations of rusty and
    healthy leaves. Additionally, most components of the system could also be utilized
    for weed detection, enabling the development of a machine vision system focused
    on weed identification. 4. Conclusion In conclusion, this survey paper has offered
    a comprehensive examination of recent advancements and applications of machine
    vision technology in the field of agriculture. Through a thorough analysis of
    the literature, we have demonstrated the various ways in which machine vision-based
    systems have been utilized, including crop counting, harvesting, pesticide elimination,
    yield prediction, disease detection, and weed identification. Furthermore, we
    have delved into the intricacies of the various processes that comprise a machine
    vision system, such as image acquisition, image processing, and image classification,
    highlighting the specific operations that are performed either alone or are cascaded
    to achieve the specific task. Additionally, we have emphasized the utilization
    of deep learning and CNNs to classify, improve accuracy, and enhance the robustness
    of these systems. The utilization of multi-spectral imaging and hyperspectral
    imaging to extract detailed information about crop health and growth has also
    been explored. To further improve the detection accuracy, we have also discussed
    the use of remote sensing operations to map terrain and the implementation of
    a binocular system. We have also highlighted the integration of machine vision
    systems with other technologies, such as drones, rovers, and robots. It is evident
    that the advancements in machine vision technology have the potential to revolutionize
    precision agriculture, resulting in increased crop yields, decreased costs, and
    more sustainable farming practices. However, it is important to bear in mind the
    limitations of the technology, such as the cost of the equipment and the need
    for proper maintenance and calibration, as well as the fact that the system may
    not be effective in certain environmental conditions. Furthermore, there is potential
    for further research on the integration of machine vision systems with other technologies
    such as IoT and AI to further enhance the capabilities of these systems, as well
    as work on adaptive light systems, different fruit and crops harvesting, real-time
    yield counting, and the development of actuators and manipulators for precision
    cutting of delicate crops. Compliance with Ethical Standards: The authors would
    like to declare that this research does not involve any human participants or
    animals and there is no conflict of interest. Declaration of Competing Interest
    The authors declare that they have no known competing financial interests or personal
    relationships that could have appeared to influence the work reported in this
    paper. Data availability No data was used for the research described in the article.
    References “Hyperspectral Viewer”, 2022 “Hyperspectral Viewer.” Getting Started
    with Hyperspectral Image Processing - MATLAB amp; Simulink. Accessed December
    14, 2022. https://www.mathworks.com/help/images/getting-started-with-hyperspectral-image-analysis.html.
    Google Scholar Aggelopoulou et al., (2011,6). K. Aggelopoulou, D. Bochtis, S.
    Fountas, K. Swain, T. Gemtos, G. Nanos Yield prediction in apple orchards based
    on image processing Precis. Agric., 12 (2011,6.), pp. 448-456 Google Scholar AGRIOS,
    2005 AGRIOS, G. chapter eight - PLANT DISEASE EPIDEMIOLOGY. Plant Pathology (Fifth
    Edition). pp. 265-291(2005), https://www.sciencedirect.com/science/article/pii/B9780080473789500142.
    Google Scholar Ahmed et al., 2012 F. Ahmed, H. Al-Mamun, A. Bari, E. Hossain,
    P. Kwan Classification of crops and weeds from digital images: A support vector
    1065 machine approach Crop Prot., 40 (2012), pp. 98-104 View PDFView articleView
    in ScopusGoogle Scholar Akhter and Sofi, 2022 R. Akhter, S.A. Sofi Precision agriculture
    using IoT data analytics and machine learning Journal of King Saud University-Computer
    and Information Sciences, 34 (8) (2022), pp. 5602-5618 View PDFView articleView
    in ScopusGoogle Scholar Altaheri et al., 2019 H. Altaheri, M. Alsulaiman, G. Muhammad
    Date fruit classification for robotic harvesting in a natural environment using
    deep learning IEEE Access, 7 (2019), pp. 117115-117133 CrossRefView in ScopusGoogle
    Scholar Amatya et al., 2016 Amatya, S., Karkee, M., Gongal, A., Zhang, Q. & Whiting,
    M. Detection of cherry tree branches with full foliage in planar architecture
    for automated sweet-cherry harvesting. Biosystems Engineering. 146 pp. 3-15 (2016),
    https://www.sciencedirect.com/science/article/pii/S1 Special Issue: Advances in
    Robotic Agriculture for Crops. Google Scholar Amatya et al., 2017 S. Amatya, M.
    Karkee, Q. Zhang, M. Whiting Automated Detection of Branch Shaking Locations for
    Robotic Cherry Harvesting Using Machine Vision Robotics, 6 (2017) https://www.mdpi.com/2218-6581/6/4/31
    Google Scholar Andersen et al., 2005 H. Andersen, L. Reng, K. Kirk Geometric plant
    properties by relaxed stereo vision using simulated annealing Computers And Electronics
    In Agriculture., 49 (2005), pp. 219-232 View PDFView articleView in ScopusGoogle
    Scholar Azizi et al., 2021 A. Azizi, Y. Abbaspour-Gilandeh, T. Mesri-Gundoshmian,
    A. Farooque, H. Afzaal Estimation of Soil Surface Roughness Using Stereo Vision
    Approach Sensors, 21 (2021) https://www.mdpi.com/1424-8220/21/13/4386 Google Scholar
    Bai et al., 2021 J. Bai, F. Hao, G. Cheng, C. Li Machine vision-based supplemental
    seeding device for plug seedling of sweet corn Computers And Electronics In Agriculture.,
    188 (2021), Article 106345 https://www.sciencedirect.com/science/article/pii/S0168169921003628
    View PDFView articleView in ScopusGoogle Scholar Bakhshipour and Jafari, 2018
    A. Bakhshipour, A. Jafari Evaluation of support vector machine and artificial
    neural networks in weed detection using shape features Computers And Electronics
    in Agriculture., 145 (2018), pp. 153-160 View PDFView articleView in ScopusGoogle
    Scholar Bandi et al., 2013 S. Bandi, A. Varadharajan, A. Chinnasamy Performance
    evaluation of various statistical classifiers in detecting the diseased 1048 citrus
    leaves International Journal Of Engineering Science And Technology., 5 (2013),
    pp. 298-307 Google Scholar Bazame et al., 2021 H. Bazame, J. Molin, D. Althoff,
    M. Martello Detection, classification, and mapping of coffee fruits during harvest
    with computer vision Computers And Electronics in Agriculture., 183 (2021), Article
    106066 https://www.sciencedirect.com/science/article/pii/S01681699210 View PDFView
    articleView in ScopusGoogle Scholar Billingsley and Schoenfisch, 1997 J. Billingsley,
    M. Schoenfisch The successful development of a vision guidance system for agriculture.
    Computers And Electronics In Robotics in Agriculture Agriculture, 16 (1997), pp.
    147-163 https://www.sciencedirect.com/science/article/pii/S0168169996000348, View
    PDFView articleView in ScopusGoogle Scholar Bini et al., 2022 D. Bini, D. Pamela,
    D. Shamia, S. Prince Others Intelligent agrobots for crop yield estimation using
    computer vision Computer Assisted Methods in Engineering And Science., 29 (2022),
    pp. 161-175 View in ScopusGoogle Scholar Blasco et al., 2002 J. Blasco, N. Aleixos,
    J. Roger, G. Rabatel, E. Moltó AE—Automation and emerging technologies: Robotic
    weed control using machine vision Biosyst. Eng., 83 (2002), pp. 149-157 https://www.sciencedirect.com/science/article/pii/S1537511002901091
    View PDFView articleView in ScopusGoogle Scholar Blasco et al., 2009 J. Blasco,
    S. Cubero, J. Gómez-Sanchıs, P. Mira, E. Moltó Development of a machine for the
    automatic sorting of pomegranate (Punica granatum) arils based on computer vision
    J. Food Eng., 90 (2009), pp. 27-34 View PDFView articleView in ScopusGoogle Scholar
    Blok et al., 2016 Blok, P., Barth, R. & Van den Berg, W. Machine vision for a
    selective broccoli harvesting robot. IFAC-PapersOnLine. 49, 66-71 (2016), 5th
    IFAC Conference on Sensing, Control and Automation Technologies for Agriculture
    AGRICONTROL 2016 https://www.sciencedirect.com/science/article/pii/S2405896316315749.
    Google Scholar Borja et al., 2018 A. Borja, R. Amongo, D. Suministrado, J. Pabico
    A machine vision assisted mechatronic seed meter for precision planting of corn
    2018 3rd InternatiOnal COnference On COntrol And Robotics Engineering (ICCRE)
    (2018), pp. 183-187 CrossRefView in ScopusGoogle Scholar Brill et al., 2020 Brill,
    F., Erukhimov, V., Giduthuri, R. & Ramm, S. Chapter 1 - Introduction. OpenVX Programming
    Guide. pp. 1-13(2020), https://www.sciencedirect.com/science/article/pii/B9780128164259000073.
    Google Scholar Bulanon et al., 2009 D. Bulanon, T. Burks, V. Alchanatis Image
    fusion of visible and thermal images for fruit detection Biosyst. Eng., 103 (2009),
    pp. 12-22 View PDFView articleView in ScopusGoogle Scholar Castro et al., 2012
    A. Castro, M. Jurado-Expósito, J. Peña-Barragán, F. López-Granados Airborne multi-spectral
    imagery for mapping cruciferous weeds in cereal and legume crops Precis. Agric.,
    13 (2012), pp. 302-321 CrossRefGoogle Scholar Cen et al., 2016 H. Cen, R. Lu,
    Q. Zhu, F. Mendoza Nondestructive detection of chilling injury in cucumber fruit
    using hyperspectral 1041 imaging with feature selection and supervised classification
    Postharvest Biology And Technology, 111 (2016), pp. 352-361 View PDFView articleView
    in ScopusGoogle Scholar Chang and Lin, 2018 Chang, C. & Lin, K. Smart Agricultural
    Machine with a Computer Vision-Based Weeding and Variable-Rate Irrigation Scheme
    Robotics. 7 (2018), https://www.mdpi.com/2218-6581/7/3/38. Google Scholar Chang
    and Lin, 2018 C. Chang, K. Lin Smart agricultural machine with a computer vision-based
    weeding and variable-rate irrigation scheme Robotics, 7 (2018), p. 38 Google Scholar
    Chang et al., 2021 C. Chang, B. Xie, S. Chung Mechanical Control with a Deep Learning
    Method for Precise Weeding on a Farm Agriculture, 11 (2021) https://www.mdpi.com/2077-0472/11/11/1049
    Google Scholar Chaschatzis et al., 2022 Chaschatzis, C., Lytos, A., Bibi, S.,
    Lagkas, T., Petaloti, C., Goudos, S., ... & Sarigiannidis, P. (2022, June). Integration
    of Information and Communication Technologies in Agriculture for Farm Management
    and Knowledge Exchange. In 2022 11th International Conference on Modern Circuits
    and Systems Technologies (MOCAST) (pp. 1-4). IEEE. Google Scholar Chaschatzis
    et al., 2022 C. Chaschatzis, C. Karaiskou, E.G. Mouratidis, E. Karagiannis, P.G.
    Sarigiannidis Detection and characterization of stressed sweet cherry tissues
    using machine learning Drones, 6 (1) (2022), p. 3 View in ScopusGoogle Scholar
    Chen et al., 2019 Chen, Y., Barzee, T., Zhang, R. & Pan, Z. Chapter 9 - Citrus.
    Integrated Processing Technologies for Food And Agricultural By-Products. pp.
    217-242 (2019), https://www.sciencedirect.com/science/article/pii/B9780128141380000095.
    Google Scholar Chen et al., 2019 Chen, S.X. and Jiao, L. and Xu, H.X. and Xu,
    J.M., L. Research on The Precision Seeding System for Tiny Particle Seed Based
    on Machine Vision Chemical Engineering Transactions. 19 (2019,12). Google Scholar
    Chen et al., 2020 C. Chen, B. Li, J. Liu, T. Bao, N. Ren Monocular positioning
    of sweet peppers: An instance segmentation approach for harvest robots Biosyst.
    Eng., 196 (2020), pp. 15-28 View PDFView articleGoogle Scholar Chen et al., 2021
    J. Chen, H. Qiang, J. Wu, G. Xu, Z. Wang Navigation path extraction for greenhouse
    cucumber-picking robots using the prediction-point Hough transform Computers And
    Electronics In Agriculture., 180 (2021), Article 105911 View PDFView articleView
    in ScopusGoogle Scholar Chen et al., 2003 B. Chen, S. Tojo, K. Watanabe Machine
    Vision for a Micro Weeding Robot in a Paddy Field Biosyst. Eng., 85 (2003), pp.
    393-404 https://www.sciencedirect.com/science/article/pii/S1537511003000783 View
    PDFView articleView in ScopusGoogle Scholar Chiu et al., 2020 Chiu, M., Xu, X.,
    Wei, Y., Huang, Z., Schwing, A., Brunner, R., Khachatrian, H., Karapetyan, H.,
    Dozier, I., Rose, G. & Others Agriculture-vision: A large aerial image database
    for agricultural pattern analysis. Proceedings Of The IEEE/CVF Conference on Computer
    Vision And Pattern Recognition. pp. 2828-2838 (2020). Google Scholar Cloutier
    and Leblanc, 2001 Cloutier, D. & Leblanc, M. Mechanical Weed Control in Agriculture.
    Physical Control Methods in Plant Protection. pp. 191-204 (2001), 10.1007/978-3-662-04584-813.
    Google Scholar Corno et al., 2021 M. Corno, S. Furioli, P. Cesana, S. Savaresi
    Adaptive Ultrasound-Based Tractor Localization for Semi-Autonomous Vineyard Operations
    Agronomy, 11 (2021) https://www.mdpi.com/2073-4395/11/2/287 Google Scholar Costa,
    2019 Costa, C., 1. Consiglio per la ricerca in agricoltura e l’analisi dell’economia
    agraria (CREA), Centro di ricerca Ingegneria e Trasformazioni Agroalimentari,
    Via della Pascolare 16, 00015 Monterotondo scalo, Rome, Italy, Febbi, P., Pallottino,
    F., Cecchini, M.,Figorilli, S., Antonucci, F., Menesatti, P. & 2. Department of
    Agriculture and Forestry Science, Tuscia University, Via S. Camillo deLellis snc,
    01100 Viterbo, Italy Stereovision system for estimating tractors and agricultural
    machines transit area under orchards canopy. Int. J. Agric. Biol. Eng.. 12, 1-5
    (2019). Google Scholar Cravero et al., 2022 A. Cravero, S. Pardo, S. Sepúlveda,
    L. Muñoz Challenges to Use Machine Learning in Agricultural Big Data: A Systematic
    Literature Review Agronomy, 12 (3) (2022), p. 748 CrossRefView in ScopusGoogle
    Scholar Da Silva and Mendonça, 2005 Da Silva, E. & Mendonça, G. 4 - Digital Image
    Processing. The Electrical Engineering Handbook. pp. 891-910(2005), https://www.sciencedirect.com/science/article/pii/B9780121709600500645.
    Google Scholar Dale et al., 2013 L. Dale, A. Thewis, C. Boudry, I. Rotar, P. Dardenne,
    V. Baeten, J. Pierna Hyperspectral imaging applications in agriculture and agro-food
    product quality and safety control: A review Appl. Spectrosc. Rev., 48 (2013),
    pp. 142-159 CrossRefView in ScopusGoogle Scholar Dattner and Bohn, 2016 M. Dattner,
    D. Bohn 20 - Characterization of Print Quality in Terms of Colorimetric Aspects
    Printing On Polymers. (2016), pp. 329-345 https://www.sciencedirect.com/science/article/pii/B9780323374682000208
    View PDFView articleGoogle Scholar Deng et al., 2009 Deng, J., Dong, W., Socher,
    R., Li, L., Li, K. & Fei-Fei, L. Imagenet: A large-scale hierarchical image database.
    2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248-255 (2009).
    Google Scholar Dhakshina Kumar et al., 2020 S. Dhakshina Kumar, S. Esakkirajan,
    S. Bama, B. Keerthiveena A microcontroller-based machine vision approach for tomato
    grading and sorting using SVM classifier Microprocessors And Microsystems., 76
    (2020), Article 103090 View PDFView articleView in ScopusGoogle Scholar Dhumale
    and Bhaskar, 2021 N. Dhumale, P. Bhaskar Smart Agricultural Robot for Spraying
    Pesticide with Image Processing based Disease Classification Technique 2021 International
    Conference on Emerging Smart Computing and Informatics (ESCI) (2021), pp. 604-609
    CrossRefView in ScopusGoogle Scholar Domingues et al., 2022 T. Domingues, T. Brandão,
    J.C. Ferreira Machine Learning for Detection and Prediction of Crop Diseases and
    Pests: A Comprehensive Survey Agriculture, 12 (9) (2022), p. 1350 CrossRefView
    in ScopusGoogle Scholar Dong et al., 2019 W. Dong, X. Ma, H. Li, S. Tan, L. Guo
    Detection of Performance of Hybrid Rice Pot-Tray Sowing Utilizing Machine Vision
    and Machine Learning Approach Sensors (Basel), 19 (12) (2019) Google Scholar Dorj
    et al., 2013 U. Dorj, M. Lee, S. Han A comparative study on tangerine detection,
    counting and yield estimation algorithm International Journal Of Security And
    Its Applications., 7 (1) (2013), pp. 405-412 View in ScopusGoogle Scholar Druzhkov
    and Kustikova, 2016 P. Druzhkov, V. Kustikova A survey of deep learning methods
    and software tools for image classification and object detection. Pattern Recognition
    And Image Analysis, 26 (2016), pp. 9-15, 10.1134/S1054661816010065 View in ScopusGoogle
    Scholar Du et al., 2019 Z. Du, Y. Hu, N. Ali Buttar, A. Mahmood X-ray computed
    tomography for quality inspection of agricultural products: A review Food Sci.Nutrition.,
    7 (2019), pp. 3146-3160 https://onlinelibrary.wiley.com/doi/abs/10.1002/fsn3.1179
    CrossRefView in ScopusGoogle Scholar Dubey and Jalal, 2013 S. Dubey, A. Jalal
    Adapted approach for fruit disease identification using images Image Processing:
    Concepts, Methodologies, Tools, And Applications. (2013), pp. 1395-1409 CrossRefView
    in ScopusGoogle Scholar Ebrahimi et al., 2017 M. Ebrahimi, M. Khoshtaghaza, S.
    Minaei, B. Jamshidi Vision-based pest detection based on SVM classification method
    Computers And Electronics In Agriculture., 137 (2017), pp. 52-58 https://www.sciencedirect.com/science/article/pii/S016816991631136X
    View PDFView articleView in ScopusGoogle Scholar ElMasry et al., 2012 G. ElMasry,
    S. Cubero, E. Moltó, J. Blasco In-line sorting of irregular potatoes by using
    automated computer-based machine vision system J. Food Eng., 112 (2012), pp. 60-68
    https://www.sciencedirect.com/science/article/pii/S0260877412001690 View PDFView
    articleView in ScopusGoogle Scholar Elstone et al., 2020 L. Elstone, K. How, S.
    Brodie, M. Ghazali, W. Heath, B. Grieve High Speed Crop and Weed Identification
    in Lettuce Fields for Precision Weeding Sensors, 20 (2020) https://www.mdpi.com/1424-8220/20/2/455
    Google Scholar Estes et al., 2001 Estes, J., Kline, K. & Collins, E. Remote Sensing.
    International Encyclopedia Of The Social Behavioral Sciences. pp. 13144-13150
    (2001), https://www.sciencedirect.com/science/article/pii/B0080430767025262. Google
    Scholar Fu et al., 2015 Fu, L., Bin, W., Yongjie, C., Shuai, S., Gejima, Y. &
    Kobayashi, T. Kiwifruit recognition at nighttime using artificial lighting based
    on machine vision. Int J Agric Biol Eng. 2015 pp. 52-59 (2015,8). Google Scholar
    Gao et al., 2020 Gao, G., Xiao, K. & Jia, Y. A spraying path planning algorithm
    based on colour-depth fusion segmentation in peach orchards. Com- 1146 puters
    And Electronics In Agriculture. 173 pp. 105412 (2020), https://www.sciencedirect.com/science/article/pii/S0168169920301733.
    Google Scholar Gao et al., 2018 J. Gao, D. Nuyttens, P. Lootens, Y. He, J. Pieters
    Recognising weeds in a maize crop using a random forest machine-learning algorithm
    and near-infrared snapshot mosaic hyperspectral imagery Biosyst. Eng., 170 (2018),
    pp. 39-50 View PDFView articleView in ScopusGoogle Scholar Giannoni et al., 2018
    L. Giannoni, F. Lange, I. Tachtsidis Hyperspectral imaging solutions for brain
    tissue metabolic and hemodynamic monitoring: past, current and future developments
    J. Opt., 20 (2018), Article 044009 CrossRefView in ScopusGoogle Scholar Giménez-Gallego
    et al., 2020 J. Giménez-Gallego, J. González-Teruel, M. Jiménez-Buendía, A. Toledo-Moreo,
    F. Soto-Valles, R. Torres-Sánchez Segmentation of Multiple Tree Leaves Pictures
    with Natural Backgrounds using Deep Learning for Image-Based Agriculture Applications
    Appl. Sci., 10 (2020) https://www.mdpi.com/2076-3417/10/1/202 Google Scholar Gomez
    et al., 2021 A. Gomez, M. Darbyshire, J. Gao, E. Sklar, S. Parsons Towards practical
    object detection for weed spraying in precision agriculture CoRR. abs/2109.11048
    (2021) https://arxiv.org/abs/2109.11048 Google Scholar Gómez-Sanchis et al., 2014
    J. Gómez-Sanchis, D. Lorente, E. Soria-Olivas, N. Aleixos, S. Cubero, J. Blasco
    Development of a hyperspectral computer vision system based on two liquid crystal
    tuneable filters for fruit inspection. application to detect citrus fruits decay
    Food And Bioprocess Technology., 7 (4) (2014), pp. 1047-1056, 10.1007/s11947-013-1158-9
    View in ScopusGoogle Scholar Gorthi et al., 2021 S. Gorthi, R. Swetha, S. Chakraborty,
    B. Li, D. Weindorf, S. Dutta, H. Banerjee, K. Das, K. Majumdar Soil organic matter
    prediction using smartphone-captured digital images: Use of reflectance image
    and image perturbation Biosyst. Eng., 209 (2021), pp. 154-169 https://www.sciencedirect.com/science/article/pii/S1537511021001422
    View PDFView articleView in ScopusGoogle Scholar Guevara-Hernandez and Gil, 2011
    F. Guevara-Hernandez, J. Gil A machine vision system for classification of wheat
    and barley grain kernels Span. J. Agric. Res., 672–680 (2011) Google Scholar Guo
    et al., 2019 Q. Guo, Y. Chen, Y. Tang, J. Zhuang, Y. He, C. Hou, X. Chu, Z. Zhong,
    S. Luo Lychee Fruit Detection Based on Monocular Machine Vision in Orchard Environment
    Sensors (Basel), 19 (2019) Google Scholar Guru et al., 2012 Guru, D., Mallikarjuna,
    P., Manjunath, S. & M. M. Shenoi Machine Vision Based Classification of Tobacco
    Leaves for Automatic Harvesting. Intelligent Automation Soft Computing. 18, 581-590
    (2012). Google Scholar Hayashi et al., 2002 S. Hayashi, K. Ganno, Y. Ishii, I.
    Tanaka Robotic Harvesting System for Eggplants Japan Agricultural Research Quarterly:
    JARQ., 36 (2002,7.), pp. 163-168 Google Scholar He et al., 2011 B. He, G. Liu,
    Y. Ji, Y. Si, R. Gao Auto Recognition of Navigation Path for Harvest Robot Based
    on Machine Vision Computer And ComputIng Technologies In Agriculture IV (2011),
    pp. 138-148 CrossRefView in ScopusGoogle Scholar Hernández-Clemente et al., 2019
    R. Hernández-Clemente, A. Hornero, M. Mottus, J. Penuelas, V. González-Dugo, J.
    Jiménez, L. Suárez, L. Alonso, P. Zarco-Tejada Early Diagnosis of Vegetation Health
    From High-Resolution Hyperspectral and Thermal Imagery: Lessons Learned From Empirical
    Relationships and Radiative Transfer Modelling Current Forestry Reports., 5 (9)
    (2019), pp. 169-183, 10.1007/s40725-019-00096-1 View in ScopusGoogle Scholar Hong
    et al., 2018 S. Hong, A. Ansari, G. Saavedra, M. Martinez-Corral Full-parallax
    3D display from stereo-hybrid 3D camera system Optics And Lasers In Engineering.,
    103 (2018), pp. 46-54 https://www.sciencedirect.com/science/article/pii/S0143816617306267
    View PDFView articleView in ScopusGoogle Scholar Honrado et al., 2017 J. Honrado,
    D. Solpico, C. Favila, E. Tongson, G. Tangonan, N. Libatique UAV imaging with
    low-cost multispectral imaging system for precision agriculture applications 2017
    IEEE Global Humanitarian Technology Conference (GHTC) (2017), pp. 1-7 Google Scholar
    Howard et al., 2017 Howard, A., Zhu, M., Chen, B., Kalenichenko, D., Wang, W.,
    Weyand, T., Andreetto, M. & Adam, H. Mobilenets: Efficient convolutional neural
    networks for mobile vision applications. ArXiv Preprint ArXiv:1704.04861. (2017).
    Google Scholar Hu et al., 2014 M. Hu, Q. Dong, B. Liu, P. Malakar The potential
    of double K-means clustering for banana image segmentation J. Food Process Eng,
    37 (2014), pp. 10-18 View in ScopusGoogle Scholar Huang et al., 2018 T. Huang,
    R. Yang, W. Huang, Y. Huang, X. Qiao Detecting sugarcane borer diseases using
    support vector machine Information Processing In Agriculture., 5 (2018), pp. 74-82
    View PDFView articleView in ScopusGoogle Scholar Hulley et al., 2019 Hulley, G.,
    Ghent, D., Göttsche, F., Guillevic, P., Mildrexler, D. & Coll, C. 3 - Land Surface
    Temperature. Taking The Temperature Of The Earth. pp. 57-127 (2019), https://www.sciencedirect.com/science/article/pii/B9780128144589000034.
    Google Scholar Islam et al., 2017 Islam, M., Dinh, A., Wahid, K. & Bhowmik, P.
    Detection of potato diseases using image segmentation and multiclass support vector
    machine. 2017 IEEE 30th Canadian Conference On Electrical And Computer Engineering
    (CCECE). pp. 1-4 (2017). Google Scholar Islam et al., 2021 M. Islam, A. Rahman,
    M. Rana Potato Grading Based on Size Features by Machine Vision Technique Journal
    Of the Bangladesh Agricultural University., 19 (2021), pp. 528-532 https://jbau.bau.edu.bd/index.php/home/article/view/50
    CrossRefGoogle Scholar Jacques et al., 2018 Jacques, A., Adamchuk, V., Cloutier,
    G., Clark, J. & Miller, C. Development of a machine vision yield monitor for shallot
    onion harvesters. Proceedings Of The 14th International Conference on Precision
    Agriculture June 24–June 27, 2018 Montreal, Quebec, Canada. (2018). Google Scholar
    Jafari et al., 2006 A. Jafari, S. Mohtasebi, H. Jahromi, M. Omid Weed detection
    in sugar beet fields using machine vision Int. J. Agric. Biol., 8 (2006), pp.
    602-605 Google Scholar Janke et al., 2019 J. Janke, M. Castelli, A. Popovicˇ Analysis
    of the proficiency of fully connected neural networks in the process of classifying
    1022 digital images. Benchmark of different classification algorithms on high-level
    image features from convolutional layers Expert Syst. Appl. (2019) https://www.sciencedirect.com/science/article/pii/S0957417419303938
    Google Scholar Jasinski et al., 2010 Jasinski, J., Pietrek, S., Walczykowski,
    P. & Orych, A. Acquisition of spectral reflectance characteristics of land cover
    features based on hyperspectral images.. (2010,1). Google Scholar Jiang et al.,
    2022 Y. Jiang, G. Li, H. Ge, F. Wang, L. Li, X. Chen, Y. Zhang Machine learning
    and application in terahertz technology: A review on achievements and future challenges
    IEEE Access (2022) Google Scholar Jiang et al., 2020 F. Jiang, Y. Lu, Y. Chen,
    D. Cai, G. Li Image recognition of four rice leaf diseases based on deep learning
    and support vector ma- 1029 chine Computers And Electronics. In Agriculture.,
    179 (2020), Article 105824 https://www.sciencedirect.com/science/article/pii/S016816992030795X1030
    View PDFView articleView in ScopusGoogle Scholar Jiao et al., 2020 Y. Jiao, R.
    Luo, Q. Li, X. Deng, X. Yin, C. Ruan, W. Jia Detection and Localization of Overlapped
    Fruits Application in an Apple Harvesting Robot Electronics, 9 (2020) https://www.mdpi.com/2079-9292/9/6/1023
    Google Scholar Kailasam et al., 2022 Kailasam, S., Achanta, S. D. M., Rama Koteswara
    Rao, P., Vatambeti, R., & Kayam, S. (2022). An IoT-based agriculture maintenance
    using pervasive computing with machine learning technique. International Journal
    of Intelligent Computing and Cybernetics, 15(2), 184-197. Google Scholar Kamilaris
    and Prenafeta-Boldú, 2018 A. Kamilaris, F. Prenafeta-Boldú Deep learning in agriculture:
    A survey Computers And Electronics In Agriculture., 147 (2018), pp. 70-90 https://www.sciencedirect.com/science/article/pii/S0168169917308803
    View PDFView articleView in ScopusGoogle Scholar Kanagasingham et al., 2020 S.
    Kanagasingham, M. Ekpanyapong, R. Chaihan Integrating machine vision-based row
    guidance with GPS and compass-based routing to achieve autonomous navigation for
    a rice field weeding robot Precis. Agric., 21 (8) (2020), pp. 831-855, 10.1007/s11119-019-09697-z
    View in ScopusGoogle Scholar Kasinathan et al., 2021 T. Kasinathan, D. Singaraju,
    S. Uyyala Insect classification and detection in field crops using modern machine
    learning techniques Information Processing In Agriculture., 8 (2021), pp. 446-457
    https://www.sciencedirect.com/science/article/pii/S2214317320302067 View PDFView
    articleView in ScopusGoogle Scholar Khan and Debnath, 2019 R. Khan, R. Debnath
    Multi class fruit classification using efficient object detection and recognition
    techniques. International Journal Of Image, Graphics And Signal Process., 11 (2019),
    p. 1 Google Scholar Khanal, 2017 S. Khanal Remote Sensing in precision agriculture
    Ohioline. (3) (2017) https://ohioline.osu.edu/factsheet/fabe-5541 Google Scholar
    Kiani et al., (2012,5). S. Kiani, S. Kamgar, M. Raoufat Machine Vision and Soil
    Trace-based Guidance-Assistance System for Farm Tractors in Soil Preparation Operations.
    Journal Of Agricultural Science, 4 (2012,5.) Google Scholar Kim and Lee, 2015
    Kim, H. & Lee, J. A study on the possibility of implementing a real-time stereoscopic
    3D rendering TV system. Displays. 40 pp. 24-34 (2015), https://www.sciencedirect.com/science/article/pii/S0141938215000608,
    Next Generation TV Systems and Technologies. Google Scholar Kim et al., 2020 Kim,
    J., Seol, J., Lee, S., Hong, S. & Son, H. An Intelligent Spraying System with
    Deep Learning-based Semantic Segmentation of Fruit Trees in Orchards. 2020 IEEE
    International Conference On Robotics And Automation (ICRA). pp. 3923-3929 (2020).
    Google Scholar Ko et al., 2021 K. Ko, I. Jang, J. Choi, J. Lim, D. Lee Stochastic
    Decision Fusion of Convolutional Neural Networks for Tomato Ripeness Detection
    in Agricultural Sorting Systems Sensors, 21 (2021) https://www.mdpi.com/1424-8220/21/3/917
    Google Scholar Kumar et al., 2013 J. Kumar, A. Vashisth, V. Sehgal, V. Gupta Assessment
    of aphid infestation in mustard by hyperspectral remote sensing J. Indian Soc.
    Remote Sens., 41 (2013), pp. 83-90 CrossRefView in ScopusGoogle Scholar Kurtulmus
    and Ünal, 2015 Kurtulmus¸, F., Ünal, H. & Others Discriminating rapeseed varieties
    using computer vision and machine learning. (Pergamon-Elsevier,2015). Google Scholar
    Kuznetsova et al., 2020 A. Kuznetsova, T. Maleva, V. Soloviev Using YOLOv3 Algorithm
    with Pre- and Post-Processing for Apple Detection in Fruit-Harvesting Robot Agronomy,
    10 (2020) https://www.mdpi.com/2073-4395/10/7/1016 Google Scholar Kuznetsova et
    al., 2020 A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,
    S. Kamali, S. Popov, M. Malloci, A. Kolesnikov & Others The open images dataset
    v4 Int. J. Comput. Vis., 128 (2020), pp. 1956-1981 CrossRefView in ScopusGoogle
    Scholar Ladaniya, 2008 M. Ladaniya 8 - HARVESTING Citrus Fruit. (2008) https://www.sciencedirect.com/science/article/pii/B978012374130150010
    Google Scholar Larrea-Gallegos and Vázquez-Rowe, 2022 G. Larrea-Gallegos, I. Vázquez-Rowe
    Exploring machine learning techniques to predict deforestation to enhance the
    decision-making of road construction projects J. Ind. Ecol., 26 (1) (2022), pp.
    225-239 CrossRefView in ScopusGoogle Scholar Laursen et al., 2014 M. Laursen,
    H. Midtiby, N. Krüger, R. Jørgensen Statistics-based segmentation using a continuous-scale
    naive Bayes approach Computers And Electronics In Agriculture., 109 (2014), pp.
    271-277 https://www.sciencedirect.com/science/article/pii/S01681699140025671047
    View PDFView articleView in ScopusGoogle Scholar Lee-Post, 2003 Lee-Post, A. Computer-Aided
    Manufacturing. Encyclopedia Of Information Systems. pp. 187-203 (2003), https://www.sciencedirect.com/science.
    Google Scholar Leiva-Valenzuela and Aguilera, 2013 G. Leiva-Valenzuela, J. Aguilera
    Automatic detection of orientation and diseases in blueberries using image analysis
    to improve their postharvest storage quality Food Control, 33 (2013), Article
    166173 https://www.sciencedirect.com/science/article/pii/S0956713513001011 Google
    Scholar Li et al., 2014 H. Li, W. Lee, K. Wang Identifying blueberry fruit of
    different growth stages using natural outdoor color images Computers And Electronics
    In Agriculture., 106 (2014), pp. 91-101 View PDFView articleView in ScopusGoogle
    Scholar Lin et al., 2008 Lin, T., Hsiung, Y., Hong, G., Chang, H. & Lu, F. Development
    of a virtual reality GIS using stereo vision. Computers And Electronics In Agriculture.
    63, 38-48 (2008), https://www.sciencedirect.com/science/article/pii/S0168169908000446,
    Special issue on bio-robotics. Google Scholar Lin et al., 2014 Lin, T., Maire,
    M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. & Zitnick, C.
    Microsoft coco: Common objects in context. European Conference on Computer Vision.
    pp. 740-755 (2014). Google Scholar Liu et al., 2022 Z. Liu, R.N. Bashir, S. Iqbal,
    M.M.A. Shahid, M. Tausif, Q. Umer Internet of Things (IoT) and machine learning
    model of plant disease prediction-blister blight for tea plant IEEE Access, 10
    (2022), pp. 44934-44944 CrossRefView in ScopusGoogle Scholar Liu and Chahl, 2018
    H. Liu, J. Chahl A multispectral machine vision system for invertebrate detection
    on green leaves Computers And Electronics in Agriculture., 150 (2018), pp. 279-288
    https://www.sciencedirect.com/science/article/pii/S016816991830142X View PDFView
    articleGoogle Scholar Liu et al., 2016 Liu, W., Anguelov, D., Erhan, D., Szegedy,
    C., Reed, S., Fu, C. & Berg, A. Ssd: Single shot multibox detector. European Conference
    on Computer Vision. pp. 21-37 (2016). Google Scholar Liu et al., 2017 W. Liu,
    Z. Wang, X. Liu, N. Zeng, Y. Liu, F. Alsaadi A survey of deep neural network architectures
    and their applications Neurocomputing, 234 (2017), pp. 11-26 https://www.sciencedirect.com/science/article/pii/S0925231216315533
    View PDFView articleCrossRefView in ScopusGoogle Scholar Longye et al., 2019 Longye,
    X., Zhuo, W., Haishen, L., Xilong, K. & Changhui, Y. Overlapping citrus segmentation
    and reconstruction based on Mask R-CNN model and concave region simplification
    and distance analysis. Journal Of Physics: Conference Series. 1345, 032064 (2019,11),
    10.1088/1742-6596/1345/3/032064. Google Scholar Lu et al., 2020 B. Lu, P. Dao,
    J. Liu, Y. He, J. Shang Recent advances of hyperspectral imaging technology and
    applications in agriculture Remote Sens. (Basel), 12 (2020), p. 2659 CrossRefView
    in ScopusGoogle Scholar Lu et al., 2021 Z. Lu, M. Zhao, J. Luo, G. Wang, D. Wang
    Design of a winter-jujube grading robot based on machine vision Computers And
    Electronics in Agriculture., 186 (2021), Article 106170 https://www.sciencedirect.com/science/article/pii/S0168169921001873
    View PDFView articleView in ScopusGoogle Scholar Mahajan et al., 2015 S. Mahajan,
    A. Das, H. Sardana Image acquisition techniques for assessment of legume quality
    Trends In Food Science Technology., 42 (2015), pp. 116-133 https://www.sciencedirect.com/science/article/pii/S0924224415000023
    View PDFView articleView in ScopusGoogle Scholar Mahmood et al., 2017 Mahmood,
    A., Bennamoun, M., An, S., Sohel, F., Boussaid, F., Hovey, R., Kendrick, G. &
    Fisher, R. Deep Learning for Coral Classification. Handbook Of Neural Computation.
    pp. 383-401 (2017). Google Scholar Manjunath et al., 2011 K. Manjunath, S. Ray,
    S. Panigrahy Discrimination of spectrally-close crops using ground-based hyperspectral
    data J. Indian Soc. Remote Sens., 39 (2011), pp. 599-602 CrossRefView in ScopusGoogle
    Scholar Mathanker et al., 2013 S. Mathanker, P. Weckler, T. Bowser X-ray applications
    in food and agriculture: a review Transactions Of The ASABE (American Society
    Of Agricultural And Biological Engineers), 56 (5) (2013), pp. 1227-1239 View in
    ScopusGoogle Scholar Mekhalfi et al., 2020 M. Mekhalfi, C. Nicolò, I. Ianniello,
    F. Calamita, R. Goller, M. Barazzuol, F. Melgani Vision System for Automatic On-Tree
    Kiwifruit Counting and Yield Estimation Sensors, 20 (2020) https://www.mdpi.com/1424-8220/20/15/4214
    Google Scholar Meng et al., 2015 Meng, Q., Qiu, R., He, J., Zhang, M., Ma, X.
    & Liu, G. Development of agricultural implement system based on machine vision
    and fuzzy control. Computers And Electronics in Agriculture. 112 p. 128-138 (2015),
    https://www.sciencedirect.com/science/article/pii/S016816991Precision Agriculture.
    Google Scholar Meng and Wang, 2015 J. Meng, S. Wang The Recognition of Overlapping
    Apple Fruits Based on Boundary Curvature Estimation 2015 Sixth InternatiOnal COnference
    On Intelligent Systems Design And Engineering ApplicatiOns (ISDEA) (2015), pp.
    874-877 CrossRefView in ScopusGoogle Scholar Miao et al., 2021 Y. Miao, L. Huang,
    S. Zhang A Two-Step Phenotypic Parameter Measurement Strategy for Overlapped Grapes
    under Different Light Conditions Sensors, 21 (2021), p. 4532 CrossRefView in ScopusGoogle
    Scholar Miller et al., 1998 W. Miller, J. Throop, B. Upchurch Pattern recognition
    models for spectral reflectance evaluation of apple blemishes Postharvest Biology
    And Technology., 14 (1998), pp. 11-20 View PDFView articleView in ScopusGoogle
    Scholar Mishra et al., 2017 V. Mishra, S. Kumar, N. Shukla Image acquisition and
    techniques to perform image acquisition SAMRIDDHI : A Journal Of Physical Sciences,
    Engineering And Technology., 9 (2017,7.) Google Scholar Moallem et al., 2017 P.
    Moallem, A. Serajoddin, H. Pourghassem Computer vision-based apple grading for
    golden delicious apples based on surface features Information Processing in Agriculture.,
    4 (2017), pp. 33-40 https://www.sciencedirect.com/science/article/pii/S2214317315300068
    View PDFView articleView in ScopusGoogle Scholar Mohd Ali et al., 2022 M. Mohd
    Ali, N. Hashim, S. Abd Aziz, O. Lasekan Characterisation of Pineapple Cultivars
    under Different Storage Conditions Using Infrared Thermal Imaging Coupled with
    Machine Learning Algorithms Agriculture, 12 (2022) https://www.mdpi.com/2077-0472/12/7/1013
    Google Scholar Mohi-Alden et al., 2022 K. Mohi-Alden, M. Omid, M. Soltani Firouz,
    A. Nasiri Design and evaluation of an intelligent sorting system for bell pepper
    using deep convolutional neural networks Journal Of Food Science., 87 (2022),
    pp. 289-301 https://ift.onlinelibrary.wiley.com/doi/abs/10.1111/1750-3841.15995
    CrossRefView in ScopusGoogle Scholar Momin et al., 2017 M. Momin, M. Rahman, M.
    Sultana, C. Igathinathane, A. Ziauddin, T. Grift Geometry-based mass grading of
    mango fruits us- 1016 ing image processing Information Processing In Agriculture.,
    4 (2017), pp. 150-160 https://www.sciencedirect.com/science/article/pii/S221431731631017
    View PDFView articleView in ScopusGoogle Scholar Mondal and Kole, 2016 D. Mondal,
    D. Kole A time efficient leaf rust disease detection technique of wheat leaf images
    using pearson correlation coefficient and rough fuzzy C-means Information Systems
    Design And Intelligent Applications. (2016), pp. 609-618 CrossRefView in ScopusGoogle
    Scholar Munawar et al., 2021 A. Munawar, Y. Yunus, D. Devianti, P. Satriyo Agriculture
    environment monitoring: rapid soil fertility evaluation by means of near infrared
    spectroscopy. IOP Conference Series: Earth And Environmental Science, 644 (2021),
    Article 012036, 10.1088/1755-1315/644/1/012036 View in ScopusGoogle Scholar Mupangwa
    et al., 2020 W. Mupangwa, L. Chipindu, I. Nyagumbo, S. Mkuhlani, G. Sisito Evaluating
    machine learning algorithms for pre- 1033 dicting maize yield under conservation
    agriculture in Eastern and Southern Africa. SN Appl. Sci., 2 (2020), p. 952, 10.1007/s42452-020-2711-6
    View in ScopusGoogle Scholar Mustafa et al., 2008 Mustafa, N., Fuad, N., Ahmed,
    S., Abidin, A., Ali, Z., Yit, W. & Sharrif, Z. Image processing of an agriculture
    produce: Determination of size and ripeness of a banana. 2008 International Symposium
    On Information Technology. 1 pp. 1-7 (2008). Google Scholar Nandi et al., 2013
    Nandi, C., Tudu, B. & Koley, C. Machine Vision Based Techniques for Automatic
    Mango Fruit Sorting and Grading Based on Maturity Level and Size. Sensing Technology:
    Current Status and Future Trends II. 8 pp. 27-46 (2013,1). Google Scholar Nasirahmadi
    et al., 2021 A. Nasirahmadi, U. Wilczek, O. Hensel Sugar beet damage detection
    during harvesting using different convolutional neural network models Agriculture,
    11 (2021) https://www.mdpi.com/2077-0472/11/11/1111 Google Scholar Nguyen et al.,
    2020 T. Nguyen, T. Hoang, M. Pham, T. Vu, T. Nguyen, Q. Huynh, J. Jo Monitoring
    agriculture areas with satellite images and deep 1027 learning Appl. Soft Comput.,
    95 (2020), Article 106565 https://www.sciencedirect.com/science/article/pii/S1568494620305032
    View PDFView articleView in ScopusGoogle Scholar Ni et al., 2018 Ni, X., Wang,
    X., Wang, S., Wang, S., Yao, Z. & Ma, Y. Structure Design and Image Recognition
    Research of A Picking Device on the Apple Picking Robot. IFAC-PapersOnLine.51,489494(2018),https://www.sciencedirect.com/science/article/pii/S2405896318312801
    IFAC Conference on Bio-Robotics BIOROBOTICS 2018. Google Scholar Nicolis and Gonzalez,
    2021 Nicolis, O. & Gonzalez, C. 19 - Wavelet-based fractal and multifractal analysis
    for detecting mineral deposits using multispectral images taken by drones. Methods
    And Applications In Petroleum And Mineral Exploration And Engineering Geology.
    pp. 295-307 (2021), https://www.sciencedirect.com/science/article/pii/B9780323856171000175.
    Google Scholar Nixon and Aguado, 2020 Nixon, M. & Aguado, A. 1 - Introduction.
    Feature Extraction And Image Processing For Computer Vision (Fourth Edition).
    pp. 1-33 (2020),https://www.sciencedirect.com/science/article/pii/B9780128149768000014.
    Google Scholar Opiyo et al., 2021 S. Opiyo, C. Okinda, J. Zhou, E. Mwangi, N.
    Makange Medial axis-based machine-vision system for orchard robot navigation Computers
    And Electronics in Agriculture., 185 (2021), Article 106153 https://www.sciencedirect.com/science/article/pii/S016816992100171X1\\
    View PDFView articleView in ScopusGoogle Scholar Otsu, 1979 N. Otsu A threshold
    selection method from gray-level histograms IEEE Transactions on Systems, Man,
    And Cybernetics., 9 (1979), pp. 62-66 CrossRefGoogle Scholar Pallottino et al.,
    2018 Pallottino, F., Menesatti, P., Figorilli, S., Antonucci, F., Tomasone, R.,
    Colantoni, A. & Costa, C. Machine Vision Retrofit System for Mechanical Weed Control
    in Precision Agriculture Applications. Sustainability. 10 (2018), https://www.mdpi.com/2071-
    1050/10/7/2209. Google Scholar Pan et al., 2016 Pan, L., Zhang, Q., Zhang, W.,
    Sun, Y., Hu, P. & Tu, K. Detection of cold injury in peaches by hyperspectral
    reflectance imaging and 1018 artificial neural network. Food Chemistry. 192 pp.
    134-141 (2016,2), 10.1016/j.foodchem.2015.06.106. Google Scholar Pan et al., 2017
    L. Pan, Y. Sun, H. Xiao, X. Gu, P. Hu, Y. Wei, K. Tu Hyperspectral imaging with
    different illumination patterns for the 1055 hollowness classification of white
    radish Postharvest Biology And Technology., 126 (2017), pp. 40-49 View PDFView
    articleView in ScopusGoogle Scholar Phadikar et al., 2013 S. Phadikar, J. Sil,
    A. Das Rice diseases classification using feature selection and rule generation
    techniques Computers And Electronics In Agriculture., 90 (2013), pp. 76-85 View
    PDFView articleView in ScopusGoogle Scholar Polder et al., 2012 Polder, G., Heijden,
    G., Doorn, J. & Baltissen, A. Automatic detection of tulip breaking virus (TBV)
    in tulip fields using machine vision. (2012), https://edepot.wur.nl/244613. Google
    Scholar Puerto et al., 2015 D. Puerto, D. Martınez Gila, J. Gámez Garcıa, J. Gómez
    Ortega Sorting olive batches for the milling process using image 1051 processing
    Sensors, 15 (2015), pp. 15738-15754 CrossRefView in ScopusGoogle Scholar Quiroz
    and Alférez, 2020 Quiroz, I. & Alférez, G. Image recognition of Legacy blueberries
    in a Chilean smart farm through deep learning. Computers And Electronics In Agriculture.
    168 pp. 105044 (2020), https://www.sciencedirect.com/science/article/pii/S0168169919312670
    1032. Google Scholar Rahimi-Ajdadi et al., 2018 Rahimi-Ajdadi, F., Abbaspour-Gilandeh,
    Y., Mollazade, K. & Hasanzadeh, R. Development of a novel machine vision procedure
    for rapid and non-contact measurement of soil moisture content. Measurement. 121
    pp. 179-189 (2018), https://www.sciencedirect.com/science/article/pii/S0263224118301593
    1294. Google Scholar Ralph, 2022 R. Ralph Robots using machine vision algorithms
    in agriculture RSIP Vision., 1 (2022) https://rsipvision.com/robots-using-machine-vision-agriculture/
    Google Scholar Ramesh and Vydeki, 2018 Ramesh, S. & Vydeki, D. Rice Blast Disease
    Detection and Classification Using Machine Learning Algorithm. 2018 2nd International
    Conference on Micro-Electronics and Telecommunication Engineering (ICMETE). pp.
    255-259 (2018). Google Scholar Ravikanth et al., 2015 L. Ravikanth, C. Singh,
    D. Jayas, N. White Classification of contaminants from wheat using near-infrared
    hyperspectral 1044 imaging Biosyst. Eng., 135 (2015), pp. 73-86 https://www.sciencedirect.com/science/article/pii/S1537511015000707
    View PDFView articleView in ScopusGoogle Scholar Redmon et al., 2016 Redmon, J.,
    Divvala, S., Girshick, R. & Farhadi, A. You only look once: Unified, real-time
    object detection. Proceedings Of the IEEE Conference On Computer Vision And Pattern
    Recognition. pp. 779-788 (2016). Google Scholar Reeves, 2014 Reeves, S. Chapter
    6 - Image Restoration: Fundamentals of Image Restoration. Academic Press Library
    In Signal Processing: Volume 4. 10044 pp. 165-192 (2014), https://www.sciencedirect.com/science/article/pii/B9780123965011000066.
    Google Scholar Rehman et al., 1036 Rehman, T., Mahmud, M., Chang, Y., Jin, J.
    & Shin, J. Current and future applications of statistical machine learning 1036
    algorithms for agricultural machine vision systems. Computers And Electronics
    In Agriculture. 156 pp. 585-605 (2019), 1037 https://www.sciencedirect.com/science/article/pii/S0168169918304289.
    Google Scholar Rehman et al., 2019 T. Rehman, Q. Zaman, Y. Chang, A. Schumann,
    K. Corscadden Development and field evaluation of a machine vision based in-season
    weed detection system for wild blueberry Computers And Electronics in Agriculture.,
    162 (2019), pp. 1-13 View PDFView articleView in ScopusGoogle Scholar Riegler-Nurscher
    et al., 2020 P. Riegler-Nurscher, G. Moitzi, J. Prankl, J. Huber, J. Karner, H.
    Wagentristl, M. Vincze Machine vision for soil roughness measurement and control
    of tillage machines during seedbed preparation Soil And Tillage Research., 196
    (2020), Article 104351 https://www.sciencedirect.com/science/article/pii/S0167198719300613
    View PDFView articleView in ScopusGoogle Scholar Rokunuzzaman and Jayasuriya,
    2013 M. Rokunuzzaman, H. Jayasuriya Development of a low-cost machine vision system
    for sorting of tomatoes Agric. Eng. Int. CIGR J., 15 (1) (2013), pp. 173-180 View
    in ScopusGoogle Scholar Romeo et al., 2012 J. Romeo, G. Pajares, M. Montalvo,
    J. Guerrero, M. Guijarro, A. Ribeiro Crop row detection in maize fields inspired
    on the 1077 human visual perception Scientific World Journal, 2012 (2012) Google
    Scholar Roser, 2013 Roser, M. Employment in Agriculture. Our World In Data. (2013),
    https://ourworldindata.org/employment-in-agriculture. Google Scholar Rovira-Más
    et al., 2008 Rovira-Más, F., Zhang, Q. & Reid, J. Stereo vision three-dimensional
    terrain maps for precision agriculture. Computers And Electronics In Agriculture.
    60, 133-143 (2008), https://www.sciencedirect.com/science/article/pii/S016816990700172X.
    Google Scholar Roy and Bhaduri, 2021 A. Roy, J. Bhaduri A Deep Learning Enabled
    Multi-Class Plant Disease Detection Model Based on Computer Vision AI., 2 (2021),
    pp. 413-428 https://www.mdpi.com/2673-2688/2/3/26 CrossRefView in ScopusGoogle
    Scholar Roy and Isler, 2016 P. Roy, V. Isler Surveying apple orchards with a monocular
    vision system 2016 IEEE InternatiOnal COnference On AutomatiOn Science And Engineering
    (CASE) (2016), pp. 916-921 CrossRefView in ScopusGoogle Scholar Sannakki et al.,
    2013 Sannakki, S., Rajpurohit, V. & Nargund, V. SVM-DSD: SVM Based diagnostic
    system for the detection of pomegranate leaf diseases. Proceedings Of International
    Conference On Advances In Computing. pp. 715-720 (2013). Google Scholar Schor
    et al., 2016 N. Schor, A. Bechar, T. Ignat, A. Dombrovsky, Y. Elad, S. Berman
    Robotic disease detection in greenhouses: combined detection of powdery mildew
    and tomato spotted wilt virus IEEE Rob. Autom. Lett., 1 (2016), pp. 354-360 View
    in ScopusGoogle Scholar Seelan et al., 2003 Seelan, S., Laguette, S., Casady,
    G. & Seielstad, G. Remote sensing applications for precision agriculture: A learning
    community approach. Remote Sensing Of Environment. 88,157-169(2003), https://www.sciencedirect.com/science/article/pii/S0034425703002360,
    IKONOS Fine Spatial Resolution Land Observation. Google Scholar Senni et al.,
    2014 L. Senni, M. Ricci, A. Palazzi, P. Burrascano, P. Pennisi, F. Ghirelli On-line
    automatic detection of foreign bod- 997 ies in biscuits by infrared thermography
    and image processing J. Food Eng., 128 (2014), pp. 146-156 https://www.sciencedirect.com/science/article/pii/S0260877413006262
    View PDFView articleView in ScopusGoogle Scholar Sharma et al., 2021 R. Sharma,
    A. Agarwal, H. Mamatha Classification of Carrots based on Shape Analysis using
    Machine Learning Techniques 2021 Third International Conference on Intelligent
    Communication Technologies and Virtual Mobile Networks (ICICV) (2021), pp. 1407-1411
    CrossRefView in ScopusGoogle Scholar Shearer and Payne, 1990 S. Shearer, F. Payne
    Color and defect sorting of bell peppers using machine vision Transactions Of
    The ASAE., 33 (1990), pp. 1245-1250 Google Scholar Shustova, 2022 Shustova, A.
    Remote Sensing in agriculture – what are some applications?. Dragonfly Aerospace.
    (2022,5), https://dragonflyaerospace.com/rem938 sensing-in-agriculture-what-are-some-applications/.
    Google Scholar Siedliska et al., 2014 A. Siedliska, P. Baranowski, W. Mazurek
    Classification models of bruise and cultivar detection on the basis of hyperspectral
    imag- 1039 ing data Computers And Electronics In Agriculture., 106 (2014), pp.
    66-74 https://www.sciencedirect.com/science/article/pii/S01681699140014581040
    View PDFView articleView in ScopusGoogle Scholar Singh and Misra, 2017 V. Singh,
    A. Misra Detection of plant leaf diseases using image segmentation and soft computing
    techniques Information Processing In Agriculture., 4 (2017), pp. 41-49 https://www.sciencedirect.com/science/article/pii/S2214317316300154
    View PDFView articleView in ScopusGoogle Scholar Sivakumar et al., 2021 Sivakumar,
    A., Modi, S., Gasparino, M., Ellis, C., Velasquez, A., Chowdhary, G. & Gupta,
    S. Learned Visual Navigation for Under-Canopy Agricultural Robots. (arXiv,2021),
    https://arxiv.org/abs/2107.02792. Google Scholar Slaughter and Harrell, 1987 D.
    Slaughter, R. Harrell Color vision in robotic fruit harvesting Transactions Of
    The ASAE., 30 (1987), pp. 1144-1148 Google Scholar Sleep et al., 2021 B. Sleep,
    S. Mason, L. Janik, L. Mosley Application of visible near-infrared absorbance
    spectroscopy for the determination of soil ph and liming requirements for broad-acre
    agriculture - precision agriculture SpringerLink., 8 (2021) https://link.springer.com/article/10.1007/s11119-021-09834-7
    Google Scholar Smith et al., 2018 L. Smith, W. Zhang, M. Hansen, I. Hales, M.
    Smith Innovative 3D and 2D machine vision methods for analysis of plants and crops
    in the field Comput. Ind., 97 (2018), pp. 122-131 https://www.sciencedirect.com/science/article/pii/S01663615173056631108
    View PDFView articleView in ScopusGoogle Scholar Sochen et al., 1998 N. Sochen,
    R. Kimmel, R. Malladi A general framework for low level vision IEEE Trans. Image
    Process., 7 (1998), pp. 310-318 View in ScopusGoogle Scholar Song et al., 2015
    Y. Song, H. Sun, M. Li, Q. Zhang Technology application of smart spray in agriculture:
    A Review Intelligent Automation Soft Computing., 21 (2015), pp. 319-333 CrossRefView
    in ScopusGoogle Scholar Spaeth et al., 2020 M. Spaeth, J. Machleb, G. Peteinatos,
    M. Saile, R. Gerhards Smart harrowing—adjusting the treatment intensity based
    on machine vision to achieve a uniform weed control selectivity under heterogeneous
    field conditions Agronomy, 10 (2020) https://www.mdpi.com/2073-4395/10/12/1925
    Google Scholar Stajnko et al., 2004 D. Stajnko, M. Lakota, M. Hocˇevar Estimation
    of number and diameter of apple fruits in an orchard during the growing season
    by thermal imaging Computers And Electronics In Agriculture., 42 (2004), pp. 31-42
    View PDFView articleView in ScopusGoogle Scholar Storey et al., 2022 G. Storey,
    Q. Meng, B. Li Leaf Disease Segmentation and Detection in Apple Orchards for Precise
    Smart Spraying in Sustainable Agriculture Sustainability., 14 (2022) https://www.mdpi.com/2071-1050/14/3/1458
    Google Scholar Subhi and Ali, 2018 Subhi, M., Md. Ali, S., Ismail, A. & Othman,
    M. Food volume estimation based on stereo image analysis. IEEE Instrumentation
    Measurement Magazine. 21, 36-43 (2018). Google Scholar Suganya et al., 2019 Suganya,
    E., Sountharrajan, S., Shandilya, S. & Ms, K. IoT in Agriculture Investigation
    on Plant Diseases and Nutrient Level Using Image Analysis Techniques. (2019,1).
    Google Scholar Sun et al., 2020 J. Sun, X. He, M. Wu, X. Wu, J. Shen, B. Lu Detection
    of tomato organs based on convolutional neural network under the overlap and occlusion
    backgrounds Mach. Vis. Appl., 31 (5) (2020), p. 31, 10.1007/s00138-020-01081-6
    Google Scholar Taneja et al., 2021 P. Taneja, H. Vasava, P. Daggupati, A. Biswas
    Multi-algorithm comparison to predict soil organic matter and soil moisture content
    from cell phone images Geoderma, 385 (2021), Article 114863 https://www.sciencedirect.com/science/article/pii/S0016706120326185
    View PDFView articleView in ScopusGoogle Scholar Tang et al., 2017 J. Tang, D.
    Wang, Z. Zhang, L. He, J. Xin, Y. Xu Weed identification based on K-means feature
    learning combined with convolutional neural network Computers And Electronics
    in Agriculture., 135 (2017), pp. 63-70 View PDFView articleView in ScopusGoogle
    Scholar Terra et al., 2021 Terra, F., Nascimento, G., Duarte, G. & Drews-Jr, P.
    Autonomous Agricultural Sprayer using Machine Vision and Nozzle Control. Journal
    Of Intelligent Robotic Systems. 102, 38 (2021,5), 10.1007/s10846-021-01361-x.
    Google Scholar UNICEF, 2021 UNICEF \\& Others The state of food security and nutrition
    in the world 2021. Google Scholar Unitednations, 2021 Unitednations & food and
    agriculture organization, united nations Food (2021) https://www.fao.org/fileadmin/templates/wsfs/docs/Issuespapers/
    Google Scholar Vrochidou et al., 2022 E. Vrochidou, D. Oustadakis, A. Kefalas,
    G. Papakostas Computer Vision in Self-Steering Tractors. Machines., 10 (2022)
    https://www.mdpi.com/2075-1702/10/2/129 Google Scholar Wang et al., 2012 Wang,
    H., Li, G., Ma, Z. & Li, X. Image recognition of plant diseases based on backpropagation
    networks. 2012 5th International Congress on Image and Signal Processing. pp.
    894-900 (2012). Google Scholar Wang et al., 2020 A. Wang, Y. Xu, X. Wei, B. Cui
    Semantic Segmentation of Crop and Weed using an Encoder-Decoder Network and Image
    Enhancement Method under Uncontrolled Outdoor Illumination IEEE Access, 8 (2020),
    pp. 81724-81734 CrossRefView in ScopusGoogle Scholar Wang et al., 2016 C. Wang,
    X. Zou, Y. Tang, L. Luo, W. Feng Localisation of litchi in an unstructured environment
    using binocular stereo vision Biosyst. Eng., 145 (2016), pp. 39-51 View PDFView
    articleView in ScopusGoogle Scholar Wani et al., 2022 J.A. Wani, S. Sharma, M.
    Muzamil, S. Ahmed, S. Sharma, S. Singh Machine learning and deep learning based
    computational techniques in automatic agricultural diseases detection: Methodologies,
    applications, and challenges Arch. Comput. Meth. Eng., 29 (1) (2022), pp. 641-677
    CrossRefView in ScopusGoogle Scholar Williams et al., 2019 H. Williams, M. Jones,
    M. Nejati, M. Seabright, J. Bell, N. Penhall, J. Barnett, M. Duke, A. Scarfe,
    H. Ahn, J. Lim, B. MacDonald Robotic kiwifruit harvesting using machine vision,
    convolutional neural networks, and robotic arms Biosyst. Eng., 181 (2019), pp.
    140-156 https://www.sciencedirect.com/science/article/pii/S153751101830638X View
    PDFView articleView in ScopusGoogle Scholar Wu et al., 2021 Z. Wu, Y. Chen, B.
    Zhao, X. Kang, Y. Ding Review of Weed Detection Methods Based on Computer Vision
    Sensors, 21 (2021) https://www.mdpi.com/1424-8220/21/11/3647 Google Scholar Wu
    et al., 2019 Wu, X., Aravecchia, S. & Pradalier, C. Design and implementation
    of computer vision based in-row weeding system. 2019 International Conference
    on Robotics And Automation (ICRA). pp. 4218-4224 (2019). Google Scholar Xiang
    et al., 2014 R. Xiang, H. Jiang, Y. Ying Recognition of clustered tomatoes based
    on binocular stereo vision Computers And Electronics In Agriculture., 106 (2014),
    pp. 75-90 View PDFView articleView in ScopusGoogle Scholar Xu et al., 2021 Xu,
    B., Chai, L. & Zhang, C. Research and application on corn crop identification
    and positioning method based on Machine 1225 vision. Information Processing in
    Agriculture(2021), https://www.sciencedirect.com/science/article/pii/S2214317321000603
    1226. Google Scholar Yu et al., 2015 Yu, X., Endo, M., Ishibashi, T., Shimizu,
    M., Kusanagi, S., Nozokido, T. & Bae, J. Orthogonally polarized terahertz wave
    imaging with real-time capability for food inspection. 2015 Asia-Pacific Microwave
    Conference (APMC). 2 pp. 1-3 (2015). Google Scholar Zeng et al., 2009 Q. Zeng,
    Y. Miao, C. Liu, S. Wang Algorithm based on marker-controlled watershed transform
    for overlapping plant fruit segmentation Opt. Eng., 48 (2009) Google Scholar Zhang
    et al., 2020 Z. Zhang, R. Cao, C. Peng, R. Liu, Y. Sun, M. Zhang, H. Li Cut-Edge
    Detection Method for Rice Harvesting Based on Machine Vision Agronomy, 10 (2020)
    https://www.mdpi.com/2073-4395/10/4/590 Google Scholar Zhang et al., 2013 Zhang,
    C., Zhang, J., Huang, X., Li, N., Chen, Z. & Li, W. System Integration Design
    of Intra-Row Weeding Robot. American Society Of Agricultural And Biological Engineers
    Annual International Meeting 2013, ASABE 2013. 1 (2013,1). Google Scholar Zhang
    et al., 2021 Zhang, L., Li, R., Li, Z., Meng, Y., Liang, J., Fu, L., Jin, X. &
    Li, S. A Quadratic Traversal Algorithm of Shortest Weeding Path Planning for Agricultural
    Mobile Robots in Cornfield. Journal Of Robotics. 2021 pp. 6633139 (2021,2), 10.1155/2021/6633139.
    Google Scholar Zhang et al., 2021 S. Zhang, W. Huang, Z. Wang Combing modified
    Grabcut, K-means clustering and sparse representation classification for weed
    recognition in wheat field Neurocomputing, 452 (2021), pp. 665-674 https://www.sciencedirect.com/science/article/pii/S092523122031910X1100
    View PDFView articleCrossRefGoogle Scholar Zhang et al., 2018 L. Zhang, J. Jia,
    G. Gui, X. Hao, W. Gao, M. Wang Deep learning based improved classification system
    for designing tomato harvesting robot IEEE Access, 6 (2018), pp. 67940-67950 CrossRefView
    in ScopusGoogle Scholar Zhang et al., 2022 H. Zhang, Z. Wang, Y. Guo, Y. Ma, W.
    Cao, D. Chen, S. Yang, R. Gao Weed Detection in Peanut Fields Based on Machine
    Vision Agriculture, 12 (2022) https://www.mdpi.com/2077-0472/12/10/1541 Google
    Scholar Zhang et al., 2019 L. Zhang, H. Zhang, Y. Chen, S. Dai, X. Li, I. Kenji,
    Z. Liu, M. Li Real-time monitoring of optimum timing for harvesting fresh tea
    leaves based on machine vision Int. J. Agric. Biol. Eng., 12 (2019), pp. 6-9 CrossRefView
    in ScopusGoogle Scholar Zhao et al., 2016 Y. Zhao, L. Gong, Y. Huang, C. Liu A
    review of key techniques of vision-based control for harvesting robot Computers
    AndElectronics In Agriculture., 127 (2016), pp. 311-323 View PDFView articleView
    in ScopusGoogle Scholar Zhao et al., 2005 J. Zhao, J. Tow, J. Katupitiya On-tree
    fruit recognition using texture properties and color data 2005 IEEE/RSJ InternatiOnal
    COnference On Intelligent Robots And Systems (2005), pp. 263-268 CrossRefView
    in ScopusGoogle Scholar Zhou et al., 2012 R. Zhou, L. Damerow, Y. Sun, M. Blanke
    Using colour features of cv. ‘Gala’ apple fruits in an orchard in image processing
    to predict yield Precis. Agric., 13 (10) (2012) Google Scholar Zhou et al., 2021
    Z. Zhou, Y. Majeed, G. Diverres Naranjo, E. Gambacorta Assessment for crop water
    stress with infrared thermal imagery in precision agriculture: A review and future
    prospects for deep learning applications Computers And Electronics In Agriculture.,
    182 (2021), Article 106019 https://www.sciencedirect.com/science/article/pii/S0168169921000375
    View PDFView articleView in ScopusGoogle Scholar Zhuang et al., 2018 J. Zhuang,
    S. Luo, C. Hou, Y. Tang, Y. He, X. Xue Detection of orchard citrus fruits using
    a monocular machine vision-based method for automatic fruit picking applications
    Computers And Electronics In Agriculture., 152 (2018), pp. 64-73 https://www.sciencedirect.com/science/article/pii/S0168169918301649
    View PDFView articleView in ScopusGoogle Scholar Zwiggelaar et al., 1996 R. Zwiggelaar,
    C. Bull, M. Mooney X-ray Simulations for Imaging Applications in the Agricultural
    and Food Industries J. Agric. Eng. Res., 63 (1996), pp. 161-170 https://www.sciencedirect.com/science/article/pii/S0021863496900189
    View PDFView articleView in ScopusGoogle Scholar Further reading Zhang, 2021 Z.
    Zhang An Adaptive Vision Navigation Algorithm in Agricultural IoT System for Smart
    Agricultural Robots Comput. Mater. Contin., 66 (2021), pp. 1043-1056 http://www.techscience.com/cmc/v66n1/40496
    View in ScopusGoogle Scholar Cited by (4) Advancements in Utilizing Image-Analysis
    Technology for Crop-Yield Estimation 2024, Remote Sensing Effect of Soil Properties
    and Powertrain Configuration on the Energy Consumption of Wheeled Electric Agricultural
    Robots 2024, Energies Intelligent classifier for various degrees of coffee roasts
    using smart multispectral vision system 2024, Journal of Field Robotics Research
    on Recognition and Localization of Cucumber Based on Complex Environment 2023,
    Research Square View Abstract © 2023 Elsevier B.V. All rights reserved. Recommended
    articles Introduction to the special section on emerging technologies in navigation,
    control and sensing for agricultural robots: Computational intelligence and artificial
    intelligence solutions Computers and Electrical Engineering, Volume 112, 2023,
    Article 109007 Hai Wang, …, Yue Wang View PDF Human–robot collaboration systems
    in agricultural tasks: A review and roadmap Computers and Electronics in Agriculture,
    Volume 204, 2023, Article 107541 George Adamides, Yael Edan View PDF Development,
    analysis, and verification of an intelligent auxiliary beekeeping device mounted
    on a crawler transporter Computers and Electronics in Agriculture, Volume 212,
    2023, Article 108148 Pingan Wang, Xiongzhe Han View PDF Show 3 more articles Article
    Metrics Citations Citation Indexes: 1 Captures Readers: 54 View details About
    ScienceDirect Remote access Shopping cart Advertise Contact and support Terms
    and conditions Privacy policy Cookies are used by this site. Cookie settings |
    Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: '>'
  relevance_evaluation: Highly relevant - The provided excerpt directly addresses
    the specific point mentioned in the review intention, providing a concrete example
    of how machine vision is used for harvesting strawberries, broccoli, kiwi, and
    apples.
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  title: An extensive review on agricultural robots with a focus on their perception
    systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Phade, G., Kishore, A. T., Omkar, S., & Kumar, M. S. (2023). IoT-Enabled
    Unmanned Aerial Vehicle: An Emerging Trend in Precision Farming. In S. N. Mohanty,
    J. V. R. Ravindra, G. S. Narayana, C. R. Pattnaik, & Y. M. Sirajudeen (Eds.),
    Drone Technology: Future Trends and Practical Applications (pp. 261-280). John
    Wiley & Sons, Inc.'
  authors:
  - Phade G.
  - Kishore A.T.
  - Omkar S.
  - Kumar M.S.
  citation_count: '0'
  data_sources: Drone imagery, atmospheric data, crop health data
  description: Farmers of the twentieth century are adopting the use of leading technical
    facilities in the field of agriculture to sustain the competitiveness of the global
    market economy. With the help of modern technology, they are trying to reduce
    production costs and improve crop yield with better product quality. A new era
    in which many traditional agricultural practices are replaced by cutting-edge
    technologies like Precision Farming (PF), which entails applying the agronomic
    variables in the right place, at the right time, has been ushered in by technological
    advancements made in monitoring, supervision, management, and control systems.
    One of the methods that will enable quick and non-destructive analysis of agricultural
    data will turn out to be an IoT integrated smart agriculture drone. This includes
    taking pictures to analyze crop behavior, finding out how much water the soil
    can contain, managing irrigation systems, etc. Numerous engineering disciplines,
    including aerodynamics, electronics, computer programming, and economics are integrated
    in the design, development, and implementation of drone-based agricultural systems.
    In considering the above thought, a problem taken into consideration is for a
    grape field of approximately 10 acres located near Nashik (28 36'N, 77 12'E) in
    the state of Maharashtra to analyze the performance of IoT enabled agriculture
    drones for PF. Main objective is to spray the insecticide with an agriculture
    drone only on the detection of the green zone of the crop or canopy for effective
    spraying and hence to reduce the wastage and cost of spraying. Drone image sensor
    will capture the field images, atmospheric parameters like, temperature, humidity
    in the field and analyze it on the cloud platform for analyzing the crop health
    and related parameters. An agriculture drone of 16 L capacity is proposed for
    spraying 1 acre of land. It could take 7-10 minutes of flight time of the drone
    to spray the insecticide which can lead to reducing the labor cost by 65%, spraying
    time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism
    is to be synchronized to achieve the objective through IoT platform. For effective
    spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying
    is proposed. Further, time series analysis of the photographic images and video
    footage captured by the drone camera can be done for the prediction, modeling
    of crop yields and auditing them with computer vision capabilities of UAV, and
    developing sufficient datasets. These data sets can be used for machine learning
    algorithms for AI or DL and for future research related to crop disease forecasting,
    canopy cover, stress management, and be able to guide farmers for taking corrective
    actions in advance and evolve best practices for overall improvement in the yield.
  doi: 10.1002/9781394168002.ch12
  explanation: This article discusses the potential of drones in precision farming,
    particularly in the context of spraying insecticides. The authors propose a system
    that combines drone imagery, cloud-based analysis, and IoT to optimize spraying
    efficiency. They highlight the benefits of using drones for crop monitoring and
    targeted spraying, which can reduce costs and improve crop yields.
  extract_1: Drone image sensor will capture the field images, atmospheric parameters
    like, temperature, humidity in the field and analyze it on the cloud platform
    for analyzing the crop health and related parameters
  extract_2: Time series analysis of the photographic images and video footage captured
    by the drone camera can be done for the prediction, modeling of crop yields and
    auditing them with computer vision capabilities of UAV, and developing sufficient
    datasets.
  full_citation: '>'
  full_text: '>

    "UNCL: University Of Nebraska - Linc Acquisitions Accounting Search within Login
    / Register Drone Technology: Future Trends and Practical Applications Chapter
    12 IoT-Enabled Unmanned Aerial Vehicle An Emerging Trend in Precision Farming
    Gayatri Phade,  A. T. Kishore,  S. Omkar,  M. Suresh Kumar Book Editor(s):Sachi
    Nandan Mohanty,  J.V.R. Ravindra,  G. Surya Narayana,  Chinmaya Ranjan Pattnaik,  Y.
    Mohamed Sirajudeen First published: 12 May 2023 https://doi.org/10.1002/9781394168002.ch12
    PDF TOOLS SHARE Summary Farmers of the twentieth century are adopting the use
    of leading technical facilities in the field of agriculture to sustain the competitiveness
    of the global market economy. With the help of modern technology, they are trying
    to reduce production costs and improve crop yield with better product quality.
    A new era in which many traditional agricultural practices are replaced by cutting-edge
    technologies like Precision Farming (PF), which entails applying the agronomic
    variables in the right place, at the right time, has been ushered in by technological
    advancements made in monitoring, supervision, management, and control systems.
    One of the methods that will enable quick and non-destructive analysis of agricultural
    data will turn out to be an IoT integrated smart agriculture drone. This includes
    taking pictures to analyze crop behavior, finding out how much water the soil
    can contain, managing irrigation systems, etc. Numerous engineering disciplines,
    including aerodynamics, electronics, computer programming, and economics are integrated
    in the design, development, and implementation of drone-based agricultural systems.
    In considering the above thought, a problem taken into consideration is for a
    grape field of approximately 10 acres located near Nashik (28 36''N, 77 12''E)
    in the state of Maharashtra to analyze the performance of IoT enabled agriculture
    drones for PF. Main objective is to spray the insecticide with an agriculture
    drone only on the detection of the green zone of the crop or canopy for effective
    spraying and hence to reduce the wastage and cost of spraying. Drone image sensor
    will capture the field images, atmospheric parameters like, temperature, humidity
    in the field and analyze it on the cloud platform for analyzing the crop health
    and related parameters. An agriculture drone of 16 L capacity is proposed for
    spraying 1 acre of land. It could take 7–10 minutes of flight time of the drone
    to spray the insecticide which can lead to reducing the labor cost by 65%, spraying
    time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism
    is to be synchronized to achieve the objective through IoT platform. For effective
    spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying
    is proposed. Further, time series analysis of the photographic images and video
    footage captured by the drone camera can be done for the prediction, modeling
    of crop yields and auditing them with computer vision capabilities of UAV, and
    developing sufficient datasets. These data sets can be used for machine learning
    algorithms for AI or DL and for future research related to crop disease forecasting,
    canopy cover, stress management, and be able to guide farmers for taking corrective
    actions in advance and evolve best practices for overall improvement in the yield.
    References Drone Technology: Future Trends and Practical Applications References
    Related Information Recommended Unmanned Aerial Vehicles for Agriculture: an Overview
    of IoT‐Based Scenarios Bacco Manlio,  Barsocchi Paolo,  Gotta Alberto,  Ruggeri
    Massimiliano Autonomous Airborne Wireless Networks, [1] Role of AI and Big Data
    Analytics in UAV‐Enabled IoT Applications for Smart Cities Madhuri S. Wakode Unmanned
    Aerial Vehicles for Internet of Things (IoT): Concepts, Techniques, and Applications,
    [1] Unmanned Aerial Vehicle (UAV) Mengxiang Li,  Liang Tang International Encyclopedia
    of Geography: People, the Earth, Environment and Technology, [1] Review on unmanned
    aerial vehicles, remote sensors, imagery processing, and their applications in
    agriculture Daniel Olson,  James Anderson Agronomy Journal Unmanned Aerial Vehicle
    (UAV): A Comprehensive Survey Rohit Chaurasia,  Vandana Mohindru Unmanned Aerial
    Vehicles for Internet of Things (IoT): Concepts, Techniques, and Applications,
    [1] Additional links ABOUT WILEY ONLINE LIBRARY Privacy Policy Terms of Use About
    Cookies Manage Cookies Accessibility Wiley Research DE&I Statement and Publishing
    Policies Developing World Access HELP & SUPPORT Contact Us Training and Support
    DMCA & Reporting Piracy OPPORTUNITIES Subscription Agents Advertisers & Corporate
    Partners CONNECT WITH WILEY The Wiley Network Wiley Press Room Copyright © 1999-2024
    John Wiley & Sons, Inc or related companies. All rights reserved, including rights
    for text and data mining and training of artificial technologies or similar technologies."'
  inline_citation: (Phade et al., 2023)
  journal: 'Drone Technology: Future Trends and Practical Applications'
  key_findings: Drones can be effectively used for targeted spraying of insecticides,
    reducing costs and improving crop yields. Cloud-based analysis and IoT integration
    enable real-time monitoring and data analysis for precision spraying. Visual monitoring
    techniques using drones can provide valuable data for crop growth monitoring and
    irrigation system performance assessment.
  limitations: The paper does not directly address the use of visual monitoring techniques
    for automated irrigation systems, and it primarily focuses on the application
    of drones in spraying.
  main_objective: To analyze the performance of IoT-enabled agriculture drones for
    precision spraying in a grape field.
  relevance_evaluation: The paper is moderately relevant to the specific point of
    integrating visual monitoring techniques for automated irrigation systems. While
    the paper focuses on the use of drones for insecticide spraying, it provides insights
    into the potential of high-resolution cameras and computer vision algorithms for
    crop growth monitoring and irrigation system performance monitoring. The paper's
    discussion of cloud-based analysis and IoT integration is also relevant to the
    broader context of integrating automated systems for real-time irrigation management.
  relevance_score: '0.65'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Nashik, Maharashtra, India
  technologies_used: Drones, IoT, cloud computing, computer vision, machine learning
  title: 'IoT-enabled unmanned aerial vehicle: An emerging trend in precision farming'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agronomy10010140
  analysis: '>'
  authors:
  - Deepak Gautam
  - Vinay Pagay
  citation_count: 36
  explanation: "The paper provides an overview of the use of automated systems for\
    \ real-time irrigation management in horticultural crops. It highlights the importance\
    \ of optimizing water resources and addressing the global food challenge. The\
    \ review intends to guide future research and innovation efforts in the field\
    \ of real-time irrigation management.\n\nThe paper covers the challenges and strategies\
    \ for integrating high-resolution cameras, methods, and technologies used in the\
    \ study, as well as automation across the entire pipeline. It emphasizes the role\
    \ of interoperability and standardization in enabling the integration of components\
    \ within the automated irrigation management system. \n\nThe review concludes\
    \ by identifying research gaps and proposing solutions and best practices based\
    \ on the analysis of case studies and real-world implementations. It encourages\
    \ collaborative research efforts across disciplines to address the complex challenges\
    \ of automated irrigation systems.\n\nThe paper's key findings and relevance to\
    \ the specific point you are making are reflected in the following statements\
    \ and insights:\n\n- Automated irrigation systems have the potential to significantly\
    \ improve water-use eﬃciency, crop yield, and quality in horticultural crops.\n\
    - A comprehensive approach is needed to integrate various components of the automated\
    \ irrigation system, including data collection, processing, analysis, decision-making,\
    \ and automated action.\n- Interoperability and standardization are crucial for\
    \ seamless integration and communication between diﬀerent components of the system.\n\
    - Case studies and real-world implementations provide valuable insights into the\
    \ challenges and best practices of automated irrigation management.\n- Collaborative\
    \ research eﬀorts are needed to address the complex challenges and advance the\
    \ field of automated irrigation systems.\n- This paper serves as a comprehensive\
    \ guide for future research, innovation, and implementation eﬀorts in real-time\
    \ irrigation management for horticultural crops."
  full_citation: '>'
  full_text: ">\nagronomy\nReview\nA Review of Current and Potential Applications\
    \ of\nRemote Sensing to Study the Water Status of\nHorticultural Crops\nDeepak\
    \ Gautam\nand Vinay Pagay *\nSchool of Agriculture, Food and Wine, The University\
    \ of Adelaide, PMB 1, Glen Osmond, SA 5064, Australia;\ndeepak.gautam@adelaide.edu.au\n\
    * Correspondence: vinay.pagay@adelaide.edu.au; Tel.: +61-8-83130773\nReceived:\
    \ 25 August 2019; Accepted: 9 January 2020; Published: 17 January 2020\n\x01\x02\
    \x03\x01\x04\x05\x06\a\b\x01\n\x01\x02\x03\x04\x05\x06\a\nAbstract: With increasingly\
    \ advanced remote sensing systems, more accurate retrievals of crop water\nstatus\
    \ are being made at the individual crop level to aid in precision irrigation.\
    \ This paper summarises\nthe use of remote sensing for the estimation of water\
    \ status in horticultural crops. The remote\nmeasurements of the water potential,\
    \ soil moisture, evapotranspiration, canopy 3D structure, and\nvigour for water\
    \ status estimation are presented in this comprehensive review. These parameters\n\
    directly or indirectly provide estimates of crop water status, which is critically\
    \ important for irrigation\nmanagement in farms. The review is organised into\
    \ four main sections: (i) remote sensing platforms;\n(ii) the remote sensor suite;\
    \ (iii) techniques adopted for horticultural applications and indicators of\n\
    water status; and, (iv) case studies of the use of remote sensing in horticultural\
    \ crops. Finally, the\nauthors’ view is presented with regard to future prospects\
    \ and research gaps in the estimation of the\ncrop water status for precision\
    \ irrigation.\nKeywords: UAS; UAV; drone; unmanned; satellite; water stress; irrigation;\
    \ vegetation index\n1. Introduction\nUnderstanding the water status of crops is\
    \ important for optimal management and application\nof water to accommodate for\
    \ inter and intra-ﬁeld variability to achieve a speciﬁc target, such as\nmaximum\
    \ water use eﬃciency, yield, quality, or proﬁtability [1,2]. The importance of\
    \ optimal water\nmanagement in agriculture in semi-arid or arid regions has become\
    \ increasingly important in light\nof recent water scarcities through reduced\
    \ allocations, as well as increased demand due to greater\nareas under production\
    \ [3,4]. Climate change is expected to further intensify the situation due to\n\
    the increased frequency of heatwaves and drought episodes [5]. Climate change\
    \ coupled with the\nnecessity to increase food production due to an increase in\
    \ global population has placed pressure on\nhorticultural sector to improve eﬃciencies\
    \ in resources use, e.g., water, for sustainable farming [6–10].\nHorticultural\
    \ crops will have to produce more ‘crop-per-drop’ in the face of limited water\
    \ resources.\nInformed management of water resources whilst maintaining or increasing\
    \ crop quality and yield are\nthe primary goals of irrigation scheduling in horticulture.\
    \ These goals can be achieved by improving\nour understanding of the water status\
    \ of the crops at key phenological stages of development.\nTraditional decision-making\
    \ for irrigation of horticultural crops includes using information from\na combination\
    \ of sources such as historical regimes, soil moisture measurements, visual assessments\
    \ of\nsoil and/or crop, weather data including evapotranspiration (ET), and measurements\
    \ of crop water\nstatus using direct-, proximal- or remote-sensing techniques\
    \ [11–13]. Some growers undertake routine\nground-based measurements, e.g., pressure\
    \ chamber, for estimation of crop water status to make\ndecisions on irrigation\
    \ [14–16]. These ground-based measurements are robust; however, destructive,\n\
    Agronomy 2020, 10, 140; doi:10.3390/agronomy10010140\nwww.mdpi.com/journal/agronomy\n\
    Agronomy 2020, 10, 140\n2 of 35\ncumbersome, and expensive to acquire a reasonable\
    \ amount of data [14,16–18]. Consequently, the\nmeasured leaf is assumed to represent\
    \ the average population of leaves of the individual crop, and\na few crops are\
    \ assumed to represent the average population of the entire irrigation block.\
    \ As a\nresult, over- or under-watering can occur, which can lower yield and fruit\
    \ quality [19–22]. This is\nespecially evident for non-homogenous blocks where\
    \ spatial variability of soil and water status is\nexpected [23–25].\nTo address\
    \ some of the limitations of ground-based measurements, remote measurement\ntechniques\
    \ were introduced with capabilities to measure at higher spatial resolution, larger\
    \ area, and\non a regular basis [26–29]. Remote sensing, in particular, unmanned\
    \ aircraft systems (UAS), presents a\nﬂexible platform to deploy on-demand sensors\
    \ as a tool to eﬃciently and non-destructively measure\ncrop water status [30].\
    \ Using thermal and spectral signatures, remote sensing techniques can be used\n\
    to characterise a crop’s water status. Knowledge of crop water status allows growers\
    \ to more eﬃciently\nschedule irrigation (i.e., when and how much water to apply).\
    \ In this regard, UAS platforms provide a\nconvenient methodology to monitor the\
    \ water status across a farm, both spatially and temporally at\nthe canopy level\
    \ [31–33]. The spectral, spatial, and temporal ﬂexibility oﬀered by UAS-based\
    \ remote\nsensing may in future assist growers in irrigation decision-making [34,35].\n\
    This review provides an overview of the application of remote sensing to understand\
    \ the\ncrop’s water status (e.g., leaf/stem water potential, leaf/canopy conductance),\
    \ soil moisture, ET, and\nphysiological attributes, all of which can contribute\
    \ to understanding the crop’s water status to\nimplement precision irrigation.\
    \ Although the key focus of this review is UAS-based remote sensing,\na comparison\
    \ has been undertaken with other remote sensing platforms, such as earth observation\n\
    satellites, which are being increasingly used to acquire similar information.\
    \ In the following sections,\nwe provide an overview of the most common remote\
    \ sensing platforms in horticulture, various sensors\nused for remote sensing,\
    \ and several predictive indices of crop water status. Two case studies of remote\n\
    sensing in horticultural crops, grapevine and almond, are then presented followed\
    \ by an overview of\nthe current research gaps and future prospects.\n2. Remote\
    \ Sensing Platforms\nGround-based direct or proximal sensors acquire instantaneous\
    \ water status measurement from\na spatial location. For decision-making purposes,\
    \ the data is generally collected from multiple\nlocations across a ﬁeld, which\
    \ allows geospatial interpolation, such as kriging, to be applied [36–38].\nThis\
    \ scale of data collection is, however, cumbersome, ineﬃcient, and error-prone,\
    \ especially for water\nstatus measurements of large areas [17]. Monitoring and\
    \ observing farms at a larger spatial scale\nprompted the launch of several earth\
    \ observation satellite systems that typically operate at an altitude\nof 180–2000\
    \ km [39]. Manned high-altitude aircraft (operating within few km) and, more recently,\
    \ UAS\n(operating under 120 m) ﬁlled the spatial gap between high-resolution ground\
    \ measurements and\nrelatively low-resolution satellite measurements [40,41].\
    \ In the context of water status estimation for\nhorticultural crops, all the\
    \ aforementioned remote sensing platforms are utilised depending on the\nuser\
    \ requirements [23,42,43]. Each remote sensing platform has its own advantage\
    \ and shortcomings.\nThe decision to obtain remote sensing crop water status data\
    \ from one or more of these platforms will\ndepend on the spatial and temporal\
    \ resolution desired. Satellite and manned aircraft can be useful\nfor regional-scale\
    \ characterisation, whereas UAS can be more useful to map the intra-ﬁeld variability.\n\
    Vehicle-based ground systems also possess similar measurement capabilities, like\
    \ remote sensing,\nhowever, at a smaller scale [44,45]. These systems can move\
    \ within the horticultural rows obtaining\nwater status measurements of adjacent\
    \ plants while the vehicle is moving, enabling them to cover a\nrelatively larger\
    \ area as compared to ground-based direct measurements [46–48].\n2.1. Satellite\
    \ Systems\nThe use of satellite systems for remote sensing started with the launch\
    \ of Landsat-1 in 1972 [39,49].\nThe subsequent launch of SPOT-1 in 1986 and Ikonos\
    \ in 1999 opened the era of commercial satellite\nAgronomy 2020, 10, 140\n3 of\
    \ 35\nsystems that resulted in rapid improvement in imaging performance, including\
    \ spatial and spectral\nresolution [50]. Continued launch of satellites from the\
    \ same families, with newer sensor models\nand improved capability, resulted in\
    \ the formation of satellite constellations (e.g., Landsat, Sentinel,\nSPOT, RapidEye,\
    \ GeoEye/WorldView families). The satellite constellation substantially improved\
    \ the\nrevisit cycle of the satellite system [51]. Recently, the miniature form\
    \ of the satellite termed Nanosat or\nCubesat has been developed, which can be\
    \ deployed on the same orbit in a large number (20s–100s),\nenabling frequent\
    \ and high-resolution data acquisition (e.g., Dove satellite from Planet Labs)\
    \ [52].\nThe earth observation satellite system, such as Landsat, Sentinel, MODIS,\
    \ RapidEye, and GeoEye,\nhave been used to study horticultural crops (Table 1).\
    \ These satellite system oﬀer camera systems\nwith spectral bands readily available\
    \ in visible, near infrared (NIR), short-wave infrared (SWIR), and\nthermal infrared\
    \ (TIR). The measurement in these bands provides opportunities to study a crop’s\
    \ water\nstatus indirectly via, for example, calculation of the normalised diﬀerence\
    \ vegetation index (NDVI),\ncrop water stress index (CWSI), and ET [8–10] at the\
    \ ﬁeld- and regional-scales.\nTable 1. Some satellite systems that have been used\
    \ to study the water status of horticultural crops.\nSatellites\nBand Numbers:\
    \ Band Designation\nSpatial Resolution (m)\nRevisit Cycle\nLandsat 7\n8: V 3,\
    \ NIR 1, SWIR 2, TIR 1, Pan 1\n15–60\n16 days\nLandsat 8\n11: C 1, V 3, NIR 1,\
    \ SWIR 2, Pan 1, Ci 1, TIR 2\n15–100\n16 days\nSentinel-2\n13: C 1, V 3, RE 3,\
    \ NIR 2, WV 1, Ci 1, SWIR 2\n10–60\n5 days\nSpot-6 and-7\n5: Pan 1, V 3, NIR 1\n\
    1.5\n1 day\nRapidEye\n5: V 3, NIR 1, RE 1\n5\n5.5 days\nGeoEye-1\n5: Pan 1, V\
    \ 3, NIR 1\n0.41–2\n3 days\nNote: Superscript integers 1, 2, 3 represent the number\
    \ of bands; V = visible, NIR = near infrared, SWIR = short-wave\ninfrared, TIR\
    \ = thermal infrared, Pan = panchromatic, C = coastal, Ci = cirrus, RE = red edge,\
    \ WV = water vapour.\nThe reﬂected/emitted electromagnetic energy from the crop\
    \ reaching the sensor is recorded at a\nspeciﬁc wavelength. The width of the observed\
    \ wavelength expressed in full width at half maximum\n(FWHM) is called spectral\
    \ resolution. The number of observed bands and the spectral resolution\nindicates\
    \ the ability of the satellite to resolve spectral features on the earth’s surface.\
    \ Commonly used\nearth observation satellite systems possess between four and\
    \ 15 bands with approximately 20–200 nm\nFWHM spectral resolution. The bands are\
    \ generally designated for the visible and NIR region with\nextended capabilities\
    \ in SWIR, TIR, as well as red edge region (Table 1). The most widely used band\n\
    combinations to study the water status of vegetation are the visible, NIR and\
    \ TIR bands [23,25,53,54].\nWith the plethora of satellite systems currently available,\
    \ user requirements on band combination\nmay be achieved by using multiple satellites.\
    \ However, acquiring an extra or a narrower band to the\nexisting capabilities\
    \ is not possible.\nThe ground distance covered per pixel of the satellite image\
    \ is called the spatial resolution,\nwhereby, a higher spatial resolution indicates\
    \ a smaller ground distance. Existing satellite systems,\ndue to their lower spatial\
    \ resolution and large coverage, are suited to study larger regions [55]. For\
    \ a\nsmaller observation area, such as a farm block, an irrigation zone, a single\
    \ row of the horticultural crop,\nor a single canopy, this spatial resolution\
    \ is considered sub-optimal. Often, a pixel of the satellite image\ncomprises\
    \ of multiple rows and multiple canopies of horticultural crops [42,56]. Thus,\
    \ the spectral\nresponse on a single pixel of the satellite image includes a mixed\
    \ spectral signal from the canopy,\ninter-row vegetation and/or bare soil. The\
    \ mixed-pixel is particularly unavoidable in horticultural\ncrops with large inter-row\
    \ surfaces, introducing errors in satellite-based estimations [42,56]. Improving\n\
    the spatial resolution from freely available Landsat/Sentinel satellites (spatial\
    \ resolution 10–15 m) to\nsuch as WorldView-3 (spatial resolution 0.3 m), does\
    \ not necessarily resolve single canopies of many\nhorticultural crops.\nCurrent\
    \ satellite systems generally oﬀer a temporal resolution of about 1–2 weeks this\
    \ resolution\ncorresponds to the satellite’s revisit interval (Table 1). For example,\
    \ freely available Landsat-8 and\nSentinel-2 oﬀer revisit cycles of 16 and 5 days,\
    \ respectively. Although the MODIS sensor on NASA’s\nAgronomy 2020, 10, 140\n\
    4 of 35\nTerra and Aqua satellites oﬀer a greater temporal resolution (1–2 days),\
    \ its spatial resolution is relatively\ncoarse (250 m–1 km) to be valuable for\
    \ horticulture [25]. The revisit cycle of satellites does not alone\nrepresent\
    \ the timeframe on which the data can be interpreted. For instance, post-data\
    \ acquisition,\nthere are often delays in data transfer to the ground station,\
    \ handling, and delivery to the end user.\nThe end user then needs to process\
    \ the data before making an interpretation. Such processing can\nbe a combination\
    \ of atmospheric, radiometric, and geometric corrections, where applicable [57,58].\n\
    Furthermore, as the agricultural applications of the satellite imagery are illumination\
    \ sensitive and\nweather dependent, conditions have to be optimal on the satellite\
    \ revisit day to avoid data corruption\ndue to, for example, cloud cover [23,53].\
    \ Cloud corrupted data (~55% of the land area is covered by\ncloud at any one\
    \ time [59]) will require users to wait for the next revisit to attempt the data\
    \ acquisition.\nTime-series image fusion techniques, such as the spatial and temporal\
    \ adaptive reﬂectance fusion\nmodel, can improve the spatial and temporal resolution\
    \ of the satellite data [60,61]. These fusion\ntechniques blend the frequent (however\
    \ low-resolution) with higher-resolution (but infrequent) satellite\ndata [62,63].\
    \ The result combines the best aspects of multiple satellite systems to produce\
    \ frequent and\nhigher-resolution data, which can be useful for timely monitoring\
    \ of water status.\nThe clear advantage of the satellite system is the ability\
    \ to capture data at a large scale and at an\naﬀordable cost (e.g., the user can\
    \ download Landsat and Sentinel data for free). The compromise with\nthe satellite\
    \ data is in spatial resolution, as well as the relatively long revisit cycle\
    \ (in the order of days\nto weeks), making the data less than ideal for speciﬁc\
    \ applications, e.g., irrigation scheduling.\n2.2. Manned Aircraft System\nOperating\
    \ within few kilometres above ground level, manned aircraft have been used to\
    \ remotely\nacquire agricultural data at higher spatial detail (compared to the\
    \ satellites) and over a larger region\n(compared to UAS) [42,64]. Light ﬁxed-wing\
    \ aircraft and helicopters are the commonly used manned\naircraft employed in\
    \ agricultural remote sensing. The ﬁxed-wing aircraft generally ﬂies higher and\n\
    faster, enabling the coverage of a larger area, whereas the helicopters are traditionally\
    \ ﬂown lower\nand slower, enabling a spatially detailed observation. A signiﬁcant\
    \ advantage of the manned aircraft,\ncompared to UAS, lies in their ability to\
    \ carry heavier high-grade sensors, such as AVIRIS, HyPlant,\nHySpex SWIR-384,\
    \ Specim AisaFENIX, and Riegl LMS Q240i-60 [65–67]. The use of manned aircraft\
    \ is,\nhowever, limited by high operational complexity, safety regulations, scheduling\
    \ inﬂexibility, costs, and\nproduct turnaround time. As a result, these platforms\
    \ are barely used as compared to the recent surge\nin the use of UAS, speciﬁcally\
    \ for horticultural crops [68–70].\nIn horticulture, manned aircraft was used\
    \ to characterise olive and peach canopy temperature\nand water stress using speciﬁc\
    \ thermal bands (10.069 µm and 12.347 µm) of a wideband (0.43–12.5 µm)\nairborne\
    \ hyperspectral camera system [71,72]. This work found moderate correlations (R2\
    \ = 0.45–0.57)\nof ground vs. aerial olive canopy temperature measurements [72],\
    \ and high correlations (R2 = 0.94)\nof canopy temperature vs. peach fruit size\
    \ (diameter) [71]. The advantage of manned aircraft for\nremote sensing of a large\
    \ region was highlighted in recent work that characterised regional-scale\ngrapevine\
    \ (Vitis vinifera L.) water stress responses of two cultivars, Shiraz and Cabernet\
    \ Sauvignon, in\nAustralia [64]. Airborne thermal imaging was able to discriminate\
    \ between the two cultivars based on\ntheir water status responses to soil moisture\
    \ availability (Figure 1).\nAgronomy 2020, 10, 140\n5 of 35\nAgronomy 2020, 10,\
    \ 140 \n5 of 35 \n \n \nFigure 1. Water status of Shiraz and Cabernet Sauvignon\
    \ under similar soil moisture as captured from \nmanned aircraft [64]. \n2.3.\
    \ Unmanned Aircraft Systems \nBoth the fixed-wing and the rotary-wing variant\
    \ of UASs are used in agricultural remote \nsensing. Each variant has its advantages\
    \ and shortcomings vis-à-vis sensor payload, flexibility, and \ncoverage. In this\
    \ regard, the literature provides a list of state-of-the-art UAS [73], their categorisation\
    \ \n[74], and overview of structural characteristics, as well as flight parameters\
    \ [75], in the context of \nagricultural use. Depending on the number of rotors,\
    \ a rotary-wing UAS can be a helicopter, a \nquadcopter, a hexacopter, or an octocopter,\
    \ among others. Rotary-wing UAS are more agile and can \nfly with a higher degree\
    \ of freedom [76], while fixed-wing UAS needs to be moving forward at a \ncertain\
    \ speed to maintain thrust. As a result, rotary-wing UAS provides flexibility\
    \ and specific \ncapabilities, such as hovering, vertical take-off and landing,\
    \ vertical (up and down) motions, or return \nto the previous location. On the\
    \ contrary, fixed-wing UAS fly faster, carry heavier payloads, and have \ngreater\
    \ flying time enabling coverage of larger areas in a single flight [77]. Recently\
    \ developed fixed-\nwing UAS with vertical take-off and landing capabilities,\
    \ such as BirdEyeView FireFly6 PRO, Elipse \nVTOL-PPK, and Carbonix Volanti, captures\
    \ the pros of both fixed-wing and rotary-wing, making \nthem a promising platform\
    \ for agricultural purposes. In the context of precision agriculture, the \napplication\
    \ of UAS, their future prospects, and knowledge gaps are discussed in [53,78–81].\
    \ While \nmany horticultural crops have been studied using UAS technology, the\
    \ most studied horticultural \ncrops are vineyards [31,82–84], citrus [85,86],\
    \ peach [32,33], olive [18,87,88], pistachio [89,90], and \nalmond [91–94], among\
    \ others [95–99]. Some of the UAS types used for water status studies of \nhorticultural\
    \ crops are shown in Figure 2. \n \n \n(a) \n(b) \nFigure 1. Water status of Shiraz\
    \ and Cabernet Sauvignon under similar soil moisture as captured from\nmanned\
    \ aircraft [64].\n2.3. Unmanned Aircraft Systems\nBoth the ﬁxed-wing and the rotary-wing\
    \ variant of UASs are used in agricultural remote sensing.\nEach variant has its\
    \ advantages and shortcomings vis-à-vis sensor payload, ﬂexibility, and coverage.\n\
    In this regard, the literature provides a list of state-of-the-art UAS [73], their\
    \ categorisation [74], and\noverview of structural characteristics, as well as\
    \ ﬂight parameters [75], in the context of agricultural use.\nDepending on the\
    \ number of rotors, a rotary-wing UAS can be a helicopter, a quadcopter, a hexacopter,\n\
    or an octocopter, among others. Rotary-wing UAS are more agile and can ﬂy with\
    \ a higher degree of\nfreedom [76], while ﬁxed-wing UAS needs to be moving forward\
    \ at a certain speed to maintain thrust.\nAs a result, rotary-wing UAS provides\
    \ ﬂexibility and speciﬁc capabilities, such as hovering, vertical\ntake-oﬀ and\
    \ landing, vertical (up and down) motions, or return to the previous location.\
    \ On the contrary,\nﬁxed-wing UAS ﬂy faster, carry heavier payloads, and have\
    \ greater ﬂying time enabling coverage of\nlarger areas in a single ﬂight [77].\
    \ Recently developed ﬁxed-wing UAS with vertical take-oﬀ and landing\ncapabilities,\
    \ such as BirdEyeView FireFly6 PRO, Elipse VTOL-PPK, and Carbonix Volanti, captures\n\
    the pros of both ﬁxed-wing and rotary-wing, making them a promising platform for\
    \ agricultural\npurposes. In the context of precision agriculture, the application\
    \ of UAS, their future prospects,\nand knowledge gaps are discussed in [53,78–81].\
    \ While many horticultural crops have been studied\nusing UAS technology, the\
    \ most studied horticultural crops are vineyards [31,82–84], citrus [85,86],\n\
    peach [32,33], olive [18,87,88], pistachio [89,90], and almond [91–94], among\
    \ others [95–99]. Some of\nthe UAS types used for water status studies of horticultural\
    \ crops are shown in Figure 2.\nAgronomy 2020, 10, 140 \n5 of 35 \n \n \nFigure\
    \ 1. Water status of Shiraz and Cabernet Sauvignon under similar soil moisture\
    \ as captured from \nmanned aircraft [64]. \n2.3. Unmanned Aircraft Systems \n\
    Both the fixed-wing and the rotary-wing variant of UASs are used in agricultural\
    \ remote \nsensing. Each variant has its advantages and shortcomings vis-à-vis\
    \ sensor payload, flexibility, and \ncoverage. In this regard, the literature\
    \ provides a list of state-of-the-art UAS [73], their categorisation \n[74], and\
    \ overview of structural characteristics, as well as flight parameters [75], in\
    \ the context of \nagricultural use. Depending on the number of rotors, a rotary-wing\
    \ UAS can be a helicopter, a \nquadcopter, a hexacopter, or an octocopter, among\
    \ others. Rotary-wing UAS are more agile and can \nfly with a higher degree of\
    \ freedom [76], while fixed-wing UAS needs to be moving forward at a \ncertain\
    \ speed to maintain thrust. As a result, rotary-wing UAS provides flexibility\
    \ and specific \ncapabilities, such as hovering, vertical take-off and landing,\
    \ vertical (up and down) motions, or return \nto the previous location. On the\
    \ contrary, fixed-wing UAS fly faster, carry heavier payloads, and have \ngreater\
    \ flying time enabling coverage of larger areas in a single flight [77]. Recently\
    \ developed fixed-\nwing UAS with vertical take-off and landing capabilities,\
    \ such as BirdEyeView FireFly6 PRO, Elipse \nVTOL-PPK, and Carbonix Volanti, captures\
    \ the pros of both fixed-wing and rotary-wing, making \nthem a promising platform\
    \ for agricultural purposes. In the context of precision agriculture, the \napplication\
    \ of UAS, their future prospects, and knowledge gaps are discussed in [53,78–81].\
    \ While \nmany horticultural crops have been studied using UAS technology, the\
    \ most studied horticultural \ncrops are vineyards [31,82–84], citrus [85,86],\
    \ peach [32,33], olive [18,87,88], pistachio [89,90], and \nalmond [91–94], among\
    \ others [95–99]. Some of the UAS types used for water status studies of \nhorticultural\
    \ crops are shown in Figure 2. \n \n \n(a) \n(b) \nFigure 2. Cont.\nAgronomy 2020,\
    \ 10, 140\n6 of 35\nAgronomy 2020, 10, 140 \n6 of 35 \n \n \n \n(c) \n(d) \nFigure\
    \ 2. Examples of unmanned aircraft systems (UAS) used to study water status in\
    \ horticulture \ncrops: (a) hexacopter equipped with RGB, multispectral and thermal\
    \ camera at The University of \nAdelaide, Adelaide, Australia (b) quadcopter equipped\
    \ with a thermal and multispectral camera \n[100], (c) fixed-wing aircraft used\
    \ for GRAPEX project to carry RGB, thermal and monochrome camera \nwith narrowband\
    \ filters [101], and (d) helicopter used for various studies of crop water status\
    \ \n[18,92,102]. \nUAS offers flexibility on spatial resolution, observation scale,\
    \ spectral bands, and temporal \nresolution to collect data on any good weather\
    \ day. However, like satellite and manned aircraft, the \nUAS is inoperable during\
    \ precipitation, high winds, and temperatures. By easily altering the flying \n\
    altitude, the UAS provides higher flexibility to observe a larger area with lower\
    \ spatial resolution or \nsmaller area with much greater detail [103]. Temporally,\
    \ the UAS can be scheduled at a user-defined \ntime at short notice, thus accommodating\
    \ applications that are time-sensitive, such as capturing vital \nphenological\
    \ stages of crop growth. Spectrally, UAS offer flexibility to carry on-demand\
    \ sensors and \ninterchangeability between sensor payloads; thus, any desired\
    \ combination of sensors and spectral \nbands can be incorporated to target specific\
    \ features. \nUAS-acquired image data requires post-processing before it can be\
    \ incorporated into the grower \ndecision-making process. Mosaicking of UAS images\
    \ currently has a turnaround time of \napproximately one day to one week, subject\
    \ to the size of the dataset, computational power, and \nspectral/spatial quality\
    \ of the product [104,105]. Spectral quality of the data is of optimal importance,\
    \ \nwhereas the spatial quality can be of less importance, such as for well-established\
    \ horticultural crops. \nHigher spectral quality demands calibration of the spectral\
    \ sensors and correction of atmospheric \neffects. Following post-processing of\
    \ aerial images, the UAS-based spectral data have shown to be \nhighly correlated\
    \ with ground-based data [82,102,106]. \nThe most common UAS-based sensor types\
    \ to study the crop water status are the thermal, \nmultispectral and RGB, while\
    \ hyperspectral and LiDAR (Light detection and ranging) sensors are \nused less\
    \ often [23,79,107]. Spectral sensors provide the capability to capture broader\
    \ physiological \nproperties of the crop, such as greenness (related to leaf chlorophyll\
    \ content and health) and biomass, \nthat generally correlate with crop water\
    \ status [82,108]. Narrower band spectral sensors provide \ndirect insight into\
    \ specific biophysical and biochemical properties of crops, such as via photochemical\
    \ \nreflectance index (PRI) and solar-induced chlorophyll fluorescence (SIF),\
    \ which reflects a plant’s \nphotosynthetic efficiency [109,110]. Thermal-based\
    \ sensors capture the temperature of the crop’s \nsurface, which indicates the\
    \ plant’s stress (both biotic and abiotic) [53]. Generally, digital RGB camera\
    \ \nand LiDAR can be used to quantify 3D metrics, such as the plant size and shape,\
    \ via 3D pointclouds \nwith sufficient accuracy for canopy level assessment [111–118].\
    \ \n3. Remote Sensor Types \n3.1. Digital Camera \nA digital camera typically\
    \ incorporates an RGB, modified RGB, and a monochrome digital \ncamera. The lens\
    \ quality of the camera determines the sharpness of the image, while the resolution\
    \ \nFigure 2. Examples of unmanned aircraft systems (UAS) used to study water\
    \ status in horticulture\ncrops: (a) hexacopter equipped with RGB, multispectral\
    \ and thermal camera at The University of\nAdelaide, Adelaide, Australia (b) quadcopter\
    \ equipped with a thermal and multispectral camera [100],\n(c) ﬁxed-wing aircraft\
    \ used for GRAPEX project to carry RGB, thermal and monochrome camera with\nnarrowband\
    \ ﬁlters [101], and (d) helicopter used for various studies of crop water status\
    \ [18,92,102].\nUAS oﬀers ﬂexibility on spatial resolution, observation scale,\
    \ spectral bands, and temporal\nresolution to collect data on any good weather\
    \ day. However, like satellite and manned aircraft, the\nUAS is inoperable during\
    \ precipitation, high winds, and temperatures. By easily altering the ﬂying\n\
    altitude, the UAS provides higher ﬂexibility to observe a larger area with lower\
    \ spatial resolution or\nsmaller area with much greater detail [103]. Temporally,\
    \ the UAS can be scheduled at a user-deﬁned\ntime at short notice, thus accommodating\
    \ applications that are time-sensitive, such as capturing vital\nphenological\
    \ stages of crop growth. Spectrally, UAS oﬀer ﬂexibility to carry on-demand sensors\
    \ and\ninterchangeability between sensor payloads; thus, any desired combination\
    \ of sensors and spectral\nbands can be incorporated to target speciﬁc features.\n\
    UAS-acquired image data requires post-processing before it can be incorporated\
    \ into the grower\ndecision-making process. Mosaicking of UAS images currently\
    \ has a turnaround time of approximately\none day to one week, subject to the\
    \ size of the dataset, computational power, and spectral/spatial\nquality of the\
    \ product [104,105]. Spectral quality of the data is of optimal importance, whereas\
    \ the\nspatial quality can be of less importance, such as for well-established\
    \ horticultural crops. Higher\nspectral quality demands calibration of the spectral\
    \ sensors and correction of atmospheric eﬀects.\nFollowing post-processing of\
    \ aerial images, the UAS-based spectral data have shown to be highly\ncorrelated\
    \ with ground-based data [82,102,106].\nThe most common UAS-based sensor types\
    \ to study the crop water status are the thermal,\nmultispectral and RGB, while\
    \ hyperspectral and LiDAR (Light detection and ranging) sensors are\nused less\
    \ often [23,79,107]. Spectral sensors provide the capability to capture broader\
    \ physiological\nproperties of the crop, such as greenness (related to leaf chlorophyll\
    \ content and health) and biomass,\nthat generally correlate with crop water status\
    \ [82,108]. Narrower band spectral sensors provide\ndirect insight into speciﬁc\
    \ biophysical and biochemical properties of crops, such as via photochemical\n\
    reﬂectance index (PRI) and solar-induced chlorophyll ﬂuorescence (SIF), which\
    \ reﬂects a plant’s\nphotosynthetic eﬃciency [109,110]. Thermal-based sensors\
    \ capture the temperature of the crop’s\nsurface, which indicates the plant’s\
    \ stress (both biotic and abiotic) [53]. Generally, digital RGB camera\nand LiDAR\
    \ can be used to quantify 3D metrics, such as the plant size and shape, via 3D\
    \ pointclouds\nwith suﬃcient accuracy for canopy level assessment [111–118].\n\
    3. Remote Sensor Types\n3.1. Digital Camera\nA digital camera typically incorporates\
    \ an RGB, modiﬁed RGB, and a monochrome digital camera.\nThe lens quality of the\
    \ camera determines the sharpness of the image, while the resolution of the\n\
    Agronomy 2020, 10, 140\n7 of 35\ncamera determines its spatial resolution and\
    \ details within an image. The RGB camera uses broad\nspectral bandwidth within\
    \ the blue, green and red spectral region to capture energy received at the\n\
    visible region of the electromagnetic spectrum. The images are used to retrieve\
    \ dimensional properties\nof the crop, terrain conﬁguration, macrostructure of\
    \ the ﬁeld, and the spatial information. Based on\nthe dimensional properties,\
    \ such as size, height, perimeter, and area of the crown, the resource need\n\
    practices can be estimated [119–121]. Generally, a larger crop is expected to\
    \ more quickly use available\nwater resources, resulting in crop water stress\
    \ at a later stage of the season if irrigation is not suﬃcient.\nThe evolution\
    \ of canopy structure within and between seasons can be useful to understand the\
    \ spatial\nvariability within the ﬁeld and corresponding water requirements. The\
    \ macro-structure of horticultural\ncrops, such as row height, width, spacing,\
    \ crop count, the fraction of ground cover, and missing\nplants, can be identiﬁed\
    \ remotely, which can aid in the allocation of resources [113,122]. The terrain\n\
    conﬁguration in the form of a digital elevation model (DEM) generated from a digital\
    \ camera can also\nenable understanding of the water status in relation to the\
    \ aspect and slope conﬁguration of the terrain.\n3.2. Multispectral Camera\nA\
    \ multispectral camera oﬀers multiple spectral bands across the electromagnetic\
    \ spectrum.\nMost common airborne multispectral cameras have 4–5 bands which include\
    \ rededge and NIR\nbands in addition to the visible bands, R-G-B (e.g., Figure\
    \ 3a,c). Conﬁgurable ﬁlter placement of\nthe spectral band is also available,\
    \ which can potentially target certain physiological responses of\nhorticultural\
    \ crops [102]. Spectrally, the airborne multispectral camera has been reported\
    \ to perform\nwith consistency, producing reliable measurements following radiometric\
    \ calibration and atmospheric\ncorrection [123–125].\nTheir spatial resolution\
    \ has been found to be suﬃcient for horticultural\napplications enabling canopy\
    \ level observation of the spectral response. For this reason, as well as\nrelatively\
    \ low cost, multispectral cameras are used more frequently in horticulture applications.\n\
    Agronomy 2020, 10, 140 \n7 of 35 \n \nof the camera determines its spatial resolution\
    \ and details within an image. The RGB camera uses \nbroad spectral bandwidth\
    \ within the blue, green and red spectral region to capture energy received \n\
    at the visible region of the electromagnetic spectrum. The images are used to\
    \ retrieve dimensional \nproperties of the crop, terrain configuration, macrostructure\
    \ of the field, and the spatial information. \nBased on the dimensional properties,\
    \ such as size, height, perimeter, and area of the crown, the \nresource need\
    \ practices can be estimated [119–121]. Generally, a larger crop is expected to\
    \ more \nquickly use available water resources, resulting in crop water stress\
    \ at a later stage of the season if \nirrigation is not sufficient. The evolution\
    \ of canopy structure within and between seasons can be \nuseful to understand\
    \ the spatial variability within the field and corresponding water requirements.\
    \ \nThe macro-structure of horticultural crops, such as row height, width, spacing,\
    \ crop count, the \nfraction of ground cover, and missing plants, can be identified\
    \ remotely, which can aid in the \nallocation of resources [113,122]. The terrain\
    \ configuration in the form of a digital elevation model \n(DEM) generated from\
    \ a digital camera can also enable understanding of the water status in relation\
    \ \nto the aspect and slope configuration of the terrain. \n3.2. Multispectral\
    \ Camera \nA multispectral camera offers multiple spectral bands across the electromagnetic\
    \ spectrum. Most \ncommon airborne multispectral cameras have 4–5 bands which\
    \ include rededge and NIR bands in \naddition to the visible bands, R-G-B (e.g.,\
    \ Figure 3a,c). Configurable filter placement of the spectral \nband is also available,\
    \ which can potentially target certain physiological responses of horticultural\
    \ \ncrops [102]. Spectrally, the airborne multispectral camera has been reported\
    \ to perform with \nconsistency, producing reliable measurements following radiometric\
    \ calibration and atmospheric \ncorrection [123–125]. Their spatial resolution\
    \ has been found to be sufficient for horticultural \napplications enabling canopy\
    \ level observation of the spectral response. For this reason, as well as \nrelatively\
    \ low cost, multispectral cameras are used more frequently in horticulture applications.\
    \ \n \n \n(a) \n(b) \n \n \n(c) \n(d) \nFigure 3. Some examples of sensors used\
    \ on a UAS platform to study water status of horticultural \ncrops: (a) A multispectral\
    \ camera (Tetracam Mini-MCA-6, Tetracam, Inc., Chatsworth, CA, USA) \n[126]. (b)\
    \ A thermal camera (FLIR TAU II, FLIR Systems, Inc., USA) [100,108]. (c) A multi-sensor\
    \ \ncamera setup with an RGB (Sony α7R III, Sony Electronics, Inc., Minato, Tokyo,\
    \ Japan), a multispectral \n(MicaSense RedEdge, MicaSense Inc., Seattle, WA, USA),\
    \ and a thermal (FLIR TAU II 640, FLIR \nSystems, Inc., USA) camera. (d) A micro-hyperspectral\
    \ camera (Micro-Hyperspec, Headwall \nPhotonics, MA, USA) [110]. \nChlorophyll\
    \ and cellular structures of vegetation absorb most of the visible light and reflect\
    \ \ninfrared light. The rise in reflectance between the red and NIR band is unique\
    \ to live green vegetation \nand is captured by vegetation spectral index called\
    \ NDVI (Table 2, Equation (3). Once the vegetation \nFigure 3. Some examples of\
    \ sensors used on a UAS platform to study water status of horticultural\ncrops:\
    \ (a) A multispectral camera (Tetracam Mini-MCA-6, Tetracam, Inc., Chatsworth,\
    \ CA, USA) [126].\n(b) A thermal camera (FLIR TAU II, FLIR Systems, Inc., USA)\
    \ [100,108]. (c) A multi-sensor camera setup\nwith an RGB (Sony α7R III, Sony\
    \ Electronics, Inc., Minato, Tokyo, Japan), a multispectral (MicaSense\nRedEdge,\
    \ MicaSense Inc., Seattle, WA, USA), and a thermal (FLIR TAU II 640, FLIR Systems,\
    \ Inc., USA)\ncamera. (d) A micro-hyperspectral camera (Micro-Hyperspec, Headwall\
    \ Photonics, MA, USA) [110].\nChlorophyll and cellular structures of vegetation\
    \ absorb most of the visible light and reﬂect\ninfrared light. The rise in reﬂectance\
    \ between the red and NIR band is unique to live green vegetation\nand is captured\
    \ by vegetation spectral index called NDVI (Table 2, Equation (3)). Once the vegetation\n\
    starts to experience stress (biotic and abiotic), its reﬂectance in the NIR region\
    \ is reduced, while the\nreﬂectance in the red band is increased. Thus, such stress\
    \ is reﬂected in the vegetation proﬁle and\nAgronomy 2020, 10, 140\n8 of 35\n\
    easily captured by indices, such as NDVI. For this reason, NDVI has shown correlations\
    \ with a wide\narray of crops response including vigour, chlorophyll content,\
    \ leaf area index (LAI), crop water stress,\nand occasionally yield [34,82–84,127].\n\
    The rededge band covers the portion of the electromagnetic spectrum between the\
    \ red and NIR\nbands where reﬂectance increases drastically. Studies have suggested\
    \ that the sharp transition between\nthe red absorbance and NIR reﬂection is able\
    \ to provide additional information about vegetation and\nits hydric characteristics\
    \ [128]. Using the normalised diﬀerence red edge (NDRE) index, the rededge\nband\
    \ was found to be useful in establishing a relative chlorophyll concentration\
    \ map [127]. Given the\nsensitivity of NDRE, it can be used for applications,\
    \ such as crops drought stress [107]. With regard to\nthe water use eﬃciency,\
    \ a combination of vegetation indices (VIs) along with structural physiological\n\
    indices were found to be useful to study water stress in horticultural crops [34,82,129].\n\
    3.3. Hyperspectral\nHyperspectral sensors have contiguous spectral bands sampled\
    \ at a narrower wavelength intervals\nspanning from visible to NIR spectrum at\
    \ a high to ultra-high spectral resolution (Figure 3d). Scanning\nat contiguous\
    \ narrow-band wavelengths, a hyperspectral sensor produces a three dimensional\
    \ (two\nspatial dimensions and one spectral dimension) data called hyperspectral\
    \ data cube. The hyperspectral\ndata cube is a hyperspectral image where each\
    \ pixel contain spatial information, as well as the entire\nspectral reﬂectance\
    \ curve [130]. Based on the operating principle and output data cube, hyperspectral\n\
    sensors for remote sensing can include a point spectrometer (aka spectroradiometer),\
    \ whiskbroom\nscanner, pushbroom scanner, and 2D imager (Figure 4) [130,131].\
    \ A point spectrometer, samples\nwithin its ﬁeld of view solid angle to produce\
    \ an ultra-high spectral resolution spectral data of a\npoint [130,132]. A whiskbroom\
    \ scanner deploys a single detector onboard to scan one single pixel at a\ntime.\
    \ As the scanner rotates across-track, successive scans form a row of the data\
    \ cube, and as the\nplatform moves forward along-track, successive rows form a\
    \ hyperspectral image [133]. A pushbroom\nscanner deploys a row of spatially contiguous\
    \ detectors arranged in the perpendicular direction of\ntravel and scans the entire\
    \ row of pixels at a time. As the platform moves forward, the successive\nrows\
    \ form a two-dimensional hyperspectral image [40,134]. The 2D imager using diﬀerent\
    \ scanning\ntechniques [130] captures hyperspectral data across the image scene\
    \ [135,136]. The point spectrometer\noﬀers the highest spectral resolution and\
    \ lowest signal-to-noise ratio (SNR) among the UAS-compatible\nhyperspectral sensors\
    \ [137,138].\nAgronomy 2020, 10, 140 \n8 of 35 \n \nstarts to experience stress\
    \ (biotic and abiotic), its reflectance in the NIR region is reduced, while the\
    \ \nreflectance in the red band is increased. Thus, such stress is reflected in\
    \ the vegetation profile and \neasily captured by indices, such as NDVI. For this\
    \ reason, NDVI has shown correlations with a wide \narray of crops response including\
    \ vigour, chlorophyll content, leaf area index (LAI), crop water stress, \nand\
    \ occasionally yield [34,82–84,127]. \nThe rededge band covers the portion of\
    \ the electromagnetic spectrum between the red and NIR \nbands where reflectance\
    \ increases drastically. Studies have suggested that the sharp transition \nbetween\
    \ the red absorbance and NIR reflection is able to provide additional information\
    \ about \nvegetation and its hydric characteristics [128]. Using the normalised\
    \ difference red edge (NDRE) \nindex, the rededge band was found to be useful\
    \ in establishing a relative chlorophyll concentration \nmap [127]. Given the\
    \ sensitivity of NDRE, it can be used for applications, such as crops drought\
    \ stress \n[107]. With regard to the water use efficiency, a combination of vegetation\
    \ indices (VIs) along with \nstructural physiological indices were found to be\
    \ useful to study water stress in horticultural crops \n[34,82,129]. \n3.3. Hyperspectral\
    \ \nHyperspectral sensors have contiguous spectral bands sampled at a narrower\
    \ wavelength \nintervals spanning from visible to NIR spectrum at a high to ultra-high\
    \ spectral resolution (Figure \n3d). Scanning at contiguous narrow-band wavelengths,\
    \ a hyperspectral sensor produces a three \ndimensional (two spatial dimensions\
    \ and one spectral dimension) data called hyperspectral data \ncube. The hyperspectral\
    \ data cube is a hyperspectral image where each pixel contain spatial \ninformation,\
    \ as well as the entire spectral reflectance curve [130]. Based on the operating\
    \ principle \nand output data cube, hyperspectral sensors for remote sensing can\
    \ include a point spectrometer (aka \nspectroradiometer), whiskbroom scanner,\
    \ pushbroom scanner, and 2D imager (Figure 4) [130,131]. A \npoint spectrometer,\
    \ samples within its field of view solid angle to produce an ultra-high spectral\
    \ \nresolution spectral data of a point [130,132]. A whiskbroom scanner deploys\
    \ a single detector onboard \nto scan one single pixel at a time. As the scanner\
    \ rotates across-track, successive scans form a row of \nthe data cube, and as\
    \ the platform moves forward along-track, successive rows form a hyperspectral\
    \ \nimage [133]. A pushbroom scanner deploys a row of spatially contiguous detectors\
    \ arranged in the \nperpendicular direction of travel and scans the entire row\
    \ of pixels at a time. As the platform moves \nforward, the successive rows form\
    \ a two-dimensional hyperspectral image [40,134]. The 2D imager \nusing different\
    \ scanning techniques [130] captures hyperspectral data across the image scene\
    \ \n[135,136]. The point spectrometer offers the highest spectral resolution and\
    \ lowest signal-to-noise \nratio (SNR) among the UAS-compatible hyperspectral\
    \ sensors [137,138]. \n \nFigure 4. The data cube structure of different spectral\
    \ sensors. The number of bands and resolution is \nshown as an example and does\
    \ not indicate true sensor capability (adapted from [130]). \nFigure 4. The data\
    \ cube structure of diﬀerent spectral sensors. The number of bands and resolution\
    \ is\nshown as an example and does not indicate true sensor capability (adapted\
    \ from [130]).\nIn horticultural applications, hyperspectral data, due to the\
    \ high resolution contiguous spectral\nsampling, possesses tremendous potential\
    \ to detect and monitor speciﬁc biotic and abiotic stresses [139].\nNarrowband\
    \ hyperspectral data was used to detect water stress using the measurement of\
    \ ﬂuorescence\nAgronomy 2020, 10, 140\n9 of 35\nand PRI over a citrus orchard\
    \ [110]. PRI was identiﬁed as one of the best predictors of water stress for a\n\
    vineyard in a study that investigated numerous VIs using hyperspectral imaging\
    \ [140]. High-resolution\nthermal imagery obtained from a hyperspectral scanner\
    \ was used to map canopy stomatal conductance\n(gs) and CWSI of olive orchards\
    \ where diﬀerent irrigation treatments were applied [18]. With the\nlarge volume\
    \ of spatial/spectral data extracted from the hyperspectral data cube, machine\
    \ learning\nwill likely be adopted more widely in the horticultural environment\
    \ to model water stress [141].\nSee Reference [54] for a comprehensive review\
    \ of hyperspectral and thermal remote sensing to detect\nplant water status.\n\
    3.4. Thermal\nThermal cameras use microbolometers to read passive thermal signals\
    \ in the spectral range of\napproximately 7–14 µm (Figure 3b). Small UAS are capable\
    \ of carrying a small form-factor thermal\ncamera with uncooled microbolometers,\
    \ which does not use an internal cooling mechanism and,\ntherefore, does not achieve\
    \ the high SNR that can be found in cooled microbolometer-based thermal\ncameras.\
    \ An array of microbolometer detectors in the thermal camera receives a thermal\
    \ radiation\nsignal and stores the signal on the corresponding image pixel as\
    \ raw data number (DN) values.\nThe result is a thermal image where each pixel\
    \ has an associated DN value, which can be converted to\nabsolute temperature.\
    \ A representative list of commercial thermal cameras used on UAS platforms\n\
    and their applications with regard to agricultural remote sensing is found in\
    \ the literature [23,53,73].\nThermal imagery enables the measurement of the foliar\
    \ temperature of plants. The foliar temperature\ndiﬀerence between well-watered\
    \ and water-stressed crops is the primary source of information for\nwater stress\
    \ prediction using a thermal sensor [142]. When mounted on a remote sensing platform,\
    \ the\ncanopy level assessment of crop water status can be performed on a large\
    \ scale.\nThermal cameras are limited by their resolution (e.g., 640 × 512 is\
    \ the maximum resolution of\nUAS compatible thermal cameras in the current market)\
    \ and high price-tag [53]. The small number of\npixels results in low spatial\
    \ resolution limiting either the ability to resolve a single canopy or ability\
    \ to\nﬂy higher and cover a larger area. If ﬂown at a higher altitude, the eﬀective\
    \ spatial resolution may\nbe inadequate for canopy level assessment of some horticultural\
    \ crops. For example, a FLIR Tau2\n640 thermal camera with a 13 mm focal length\
    \ when ﬂown at an altitude of approximately 120 m\nresults in a spatial resolution\
    \ of 15.7 cm. For relatively large horticultural crops, such as grapevine,\nalmond,\
    \ citrus, and avocado, the resolution at a maximum legal ﬂying altitude of 120\
    \ m in Australia\n(for small-sized UAS) oﬀers an adequate spatial resolution to\
    \ observe a single canopy.\nAnother challenge with the use of thermal cameras\
    \ is the temporal drift of the DN values\nwithin successive thermal images, especially\
    \ with uncooled thermal cameras [143]. Due to the lack\nof an internal cooling\
    \ mechanism for the microbolometer detectors, DN values registered by the\nmicrobolometers\
    \ experience temporal drift i.e., the registered DN values for the same temperature\n\
    target will drift temporally. Thus, the thermal image can be unreliable especially\
    \ when the internal\ntemperature of the camera is changing rapidly, such as during\
    \ camera warmup period or during the\nﬂight when a gust of cool wind results in\
    \ cooling of the camera. To overcome this challenge, the user\nmay need to provide\
    \ suﬃcient startup time before operation (preferably 30–60 min) [102,143–145],\n\
    shield the camera to minimize the change in the internal temperature of the camera\
    \ [142], calibrate the\ncamera [146–153], and perform frequent ﬂat-ﬁeld corrections.\n\
    3.5. Multi-Sensor\nTo carry multiple sensors, the total UAS payload needs to be\
    \ considered that includes, in\naddition to the sensors, an inertial measurement\
    \ unit (IMU) and global navigation satellite system\n(GNSS) for the georeferencing\
    \ purpose [40,154]. Higher accuracy sensors tend to be heavier, and in\na multi-sensor\
    \ scenario, the payload can quickly reach or even exceed the payload limit. This\
    \ has\nlimited contemporary measurements in earlier multirotor UAS requiring separate\
    \ ﬂights for each of\nsensor [126]. The use of ﬁxed-wing UAS has allowed carrying\
    \ higher payloads due to the much larger\nAgronomy 2020, 10, 140\n10 of 35\nthrust-to-weight\
    \ ratio as compared to a rotary-wing aircraft [155]. Similarly, recent advancement\
    \ in\nUAS technology and lightweight sensors have enabled multirotor (payload\
    \ 5–6 kg readily available) to\nonboard multi-sensors.\nWater status of crops\
    \ is a complex process inﬂuenced by a number of factors including the\nphysiology\
    \ of the crop, available soil moisture, the size and vigour of the crop, and meteorological\n\
    factors [30,108,116,156,157]. For this reason, a multi-sensor platform is used\
    \ to acquire measurements\nof the diﬀerent aspects of the crop for water status\
    \ assessment [34,102,108]. The most common\ncombination of sensors found in the\
    \ literature is the RGB, multispectral (including rededge and NIR\nbands) and\
    \ thermal. Together, these sensors can be used to investigate the water status\
    \ of the crop\nusing various indicators, such as PRI, CWSI, ﬂuorescence, and structural\
    \ properties, with the aim of\nimproving the water use eﬃciency [102,110,158–160].\n\
    4. Techniques of Remote Sensing in Horticulture\n4.1. Georeferencing of Remotely\
    \ Sensed Images\nGeoreferencing provides a spatial reference to the remotely sensed\
    \ images such that the pixels\nrepresenting crops or regions of interest on the\
    \ images are correctly associated with their position on\nEarth. The georeferencing\
    \ process generally uses surveyed coordinate points on the ground, known\nas ground\
    \ control points (GCPs), to determine and apply scaling and transformation to\
    \ the aerial\nimages [161]. Alternatively, instead of GCPs, the user can georeference\
    \ aerial images by using the\naccurate position of the camera, or by co-registration\
    \ with the existing georeferenced map [105,162].\nIn the case of UAS-based images,\
    \ the capture timing is scheduled to ensure a recommended\nforward overlap (>80%)\
    \ between successive images.\nThe ﬂight path is designed to ensure the\nrecommended\
    \ side overlap (>70%) between images from successive ﬂight strips. Thus, the captured\n\
    series of images are processed using the Structure-from-Motion (SfM) technique\
    \ to generate a 3D\npointcloud and orthomosaic [73,130] (see Figure 5). Commonly\
    \ used SfM software to process the\nremote sensing images are Agisoft PhotoScan\
    \ and Pix4D. The commonly retrieved outputs from the\nSfM software for assessment\
    \ of horticulture crops include the orthomosaic, digital surface model\n(DSM),\
    \ DEM, and 3D pointcloud [113,126,163]. This technique of georeferencing can be\
    \ applied to any\nsensor that produces images, e.g., RGB, thermal, or multispectral\
    \ cameras [126,164,165].\nAgronomy 2020, 10, 140 \n10 of 35 \n \nsensor [126].\
    \ The use of fixed-wing UAS has allowed carrying higher payloads due to the much\
    \ larger \nthrust-to-weight ratio as compared to a rotary-wing aircraft [155].\
    \ Similarly, recent advancement in \nUAS technology and lightweight sensors have\
    \ enabled multirotor (payload 5–6 kg readily available) \nto onboard multi-sensors.\
    \ \nWater status of crops is a complex process influenced by a number of factors\
    \ including the \nphysiology of the crop, available soil moisture, the size and\
    \ vigour of the crop, and meteorological \nfactors [30,108,116,156,157]. For this\
    \ reason, a multi-sensor platform is used to acquire measurements \nof the different\
    \ aspects of the crop for water status assessment [34,102,108]. The most common\
    \ \ncombination of sensors found in the literature is the RGB, multispectral (including\
    \ rededge and NIR \nbands) and thermal. Together, these sensors can be used to\
    \ investigate the water status of the crop \nusing various indicators, such as\
    \ PRI, CWSI, fluorescence, and structural properties, with the aim of \nimproving\
    \ the water use efficiency [102,110,158–160]. \n4. Techniques of Remote Sensing\
    \ in Horticulture \n4.1. Georeferencing of Remotely Sensed Images \nGeoreferencing\
    \ provides a spatial reference to the remotely sensed images such that the pixels\
    \ \nrepresenting crops or regions of interest on the images are correctly associated\
    \ with their position on \nEarth. The georeferencing process generally uses surveyed\
    \ coordinate points on the ground, known \nas ground control points (GCPs), to\
    \ determine and apply scaling and transformation to the aerial \nimages [161].\
    \ Alternatively, instead of GCPs, the user can georeference aerial images by using\
    \ the \naccurate position of the camera, or by co-registration with the existing\
    \ georeferenced map [105,162]. \nIn the case of UAS-based images, the capture\
    \ timing is scheduled to ensure a recommended \nforward overlap (>80%) between\
    \ successive images. The flight path is designed to ensure the \nrecommended side\
    \ overlap (>70%) between images from successive flight strips. Thus, the captured\
    \ \nseries of images are processed using the Structure-from-Motion (SfM) technique\
    \ to generate a 3D \npointcloud and orthomosaic [73,130] (see Figure 5). Commonly\
    \ used SfM software to process the \nremote sensing images are Agisoft PhotoScan\
    \ and Pix4D. The commonly retrieved outputs from the \nSfM software for assessment\
    \ of horticulture crops include the orthomosaic, digital surface model \n(DSM),\
    \ DEM, and 3D pointcloud [113,126,163]. This technique of georeferencing can be\
    \ applied to \nany sensor that produces images, e.g., RGB, thermal, or multispectral\
    \ cameras [126,164,165]. \n \nFigure 5. A typical workflow of structure-from-motion\
    \ (SfM) to produce georeferenced products from \nUAS-based image sets and ground\
    \ control points (adapted from [166,167]). SIFT = scale-invariant \nfeature transform;\
    \ ANN = approximate nearest neighbour; RANSAC = random sample consensus; \nFigure\
    \ 5. A typical workﬂow of structure-from-motion (SfM) to produce georeferenced\
    \ products from\nUAS-based image sets and ground control points (adapted from\
    \ [166,167]). SIFT = scale-invariant\nfeature transform; ANN = approximate nearest\
    \ neighbour; RANSAC = random sample consensus;\nCMVS = clustering views for multi-view\
    \ stereo; PMVS = patch-based multi-view stereo; GCP = ground\ncontrol points.\n\
    Agronomy 2020, 10, 140\n11 of 35\nThe complexity of georeferencing of hyperspectral\
    \ observations depends on the sensor type, i.e.,\nimaging or non-imaging. A non-imaging\
    \ spectroradiometer relies on the use of a GNSS antenna and\nan IMU for georeferencing\
    \ the point observation [130,132,138,168]. An imaging hyperspectral camera,\n\
    generally, in addition to GNSS and IMU measurement, uses the inter-pixel relation\
    \ in SfM to produce a\ngeoreferenced orthomosaic [40,134,135,169,170].\n4.2. Calibration\
    \ and Correction of Remotely Sensed Images\nEnsuring consistency, repeatability,\
    \ and quality of the spectral observation requires stringent\nradiometric, spectral,\
    \ and atmospheric corrections [123,171–177]. Spectral and radiometric calibration\n\
    is performed in the spectral calibration facility in darkroom settings. The sensor’s\
    \ optical properties\nand shift in spectral band position are corrected during\
    \ the spectral calibration process. Radiometric\ncalibration enables conversion\
    \ of the recorded digital values into physical units, such as radiance.\nInﬁeld\
    \ operation of the spectral sensor is inﬂuenced by variations in atmospheric transmittance\n\
    from thin clouds, invisible to the human observer. Changes in atmospheric transmittance\
    \ aﬀect the\nradiance incident on the plant. As a result, the change in acquired\
    \ spectral response by the sensor\nmay not represent the change in plants response\
    \ but the change in incident radiation on the plant.\nThe most common method to\
    \ convert the spectral data to reﬂectance is by generating an empirical line\n\
    relationship between sensor values and spectral targets, such as a Spectralon®\
    \ or calibration targets.\nThe use of downwelling sensors, such as a cosine corrector\
    \ [137], or the use of a ground-based PAR\nsensor enables absolute radiometric\
    \ calibration to generate radiance [130].\nThe calibration of the broad wavelength\
    \ multispectral sensor is generally less stringent than\nthe hyperspectral. Generally,\
    \ multispectral sensors are used to compute normalised indices such\nas NDVI.\
    \ The normalised indices are relatively less inﬂuenced, although signiﬁcant, by\
    \ the change\nin illumination conditions which aﬀect the entire spectrum proportionally\
    \ [29,101]. In this regard,\nradiometric calibration of the multispectral camera\
    \ has used a range of stringent to simpliﬁed, and\nvicarious approaches [123,125,171,173,178–180].\
    \ Some multispectral cameras are equipped with a\ndownwelling light sensor, which\
    \ is aimed at correcting for variations in atmospheric transmittance.\nHowever,\
    \ the performance of such downwelling sensors (without a cosine corrector) on\
    \ multispectral\ncameras have been reported to have directional variation resulting\
    \ in unstable correction, indicating\nthe inability of the sensor to incorporate\
    \ the entire hemisphere of diﬀused light [124,137].\nThe radiometric calibration\
    \ of the thermal images is typically based on the camera’s DN to object\ntemperature\
    \ curve, which provides the relationship between the DN of a pixel and a known\
    \ object\ntemperature, usually of a black body radiator. Measurement accuracy\
    \ and SNR of the camera under\nvarying ambient temperatures can be improved by\
    \ using calibration shutters, which are recently\navailable commercially. Furthermore,\
    \ for low measurement errors (under 1 ◦C), thermal data requires\nconsideration\
    \ to the atmospheric transmittance [18,102]. Flying over a few temperature reference\n\
    targets placed on the ground reduces the temporal drift of the camera [142,143,181].\
    \ Temperature\naccuracy within a few degrees was achieved by ﬂying over the targets\
    \ three times (at the start, middle\nand end of UAS operation) and using three\
    \ separate calibration equations for each overpass [142].\nAdditionally, using\
    \ the redundant information from multiple overlapping images, drift correction\n\
    models have been proposed, which lowered temperature error by 1 ◦C as compared\
    \ to uncorrected\northomosaic [152]. The manufacturer stated accuracies (generally\
    \ ±5 ◦C) can be suﬃcient to access the\nﬁeld variability and to detect “hotspots”\
    \ of water status. However, the aforementioned calibration and\ncorrection of\
    \ the thermal cameras are required for quantitative measurement as a goal [143].\
    \ In this\nregard, current challenges and best practices for the operation of\
    \ thermal cameras onboard a UAS is\nprovided in the literature [143].\n4.3. Canopy\
    \ Data Extraction\nA key challenge in remote sensing of horticultural as compared\
    \ to agricultural crops arises due\nto the proportion of inter-row ground/vegetation\
    \ cover and resulting mixed pixels. The proportion\nAgronomy 2020, 10, 140\n12\
    \ of 35\nof the mixed pixels increases with the decrease in spatial resolution\
    \ of the image. Most of the pixels\ntowards the edge of the canopy contain a blend\
    \ of information originating from the sun-lit canopy,\nshadowed leaves, and inter-row\
    \ bare soil/cover crop. A further challenge can arise for some crops,\nsuch as\
    \ grapevine, due to overlapping of adjacent plants.\nThe canopy data from orthomosaic\
    \ has been extracted using either a pixel-based or an object-based\napproach.\
    \ Earlier studies manually sampled from the centre of crop row which most likely\
    \ eliminated\nthe mixed pixels [182]. In the pixel-based approach, techniques,\
    \ such as applying global threshold\nand masking, have been used. Binary masks,\
    \ such as NDVI, eliminates non-canopy pixels from\nthe sampling [82,84]. Combining\
    \ the NDVI mask with the canopy height mask can exclude the\npixels associated\
    \ with non-vegetation, as well as vegetation that does not meet the height threshold.\n\
    The pixel-based approach, however, can result in inaccurate identiﬁcation of some\
    \ crops due to pixel\nheterogeneity, mixed pixels, spectral similarity, and crop\
    \ pattern variability.\nIn the object-based approach, using object detection techniques,\
    \ neighbouring pixels with\nhomogenous information, such as spectral, textural,\
    \ structural, and hierarchical features, are grouped\ninto “objects”. These objects\
    \ are used as the basis of object-based image analysis (OBIA) classiﬁcation\n\
    using classiﬁers, such as k-nearest neighbour, decision tree, support vector machine,\
    \ random forest,\nand maximum likelihood [122,183–185]. In the horticultural environment,\
    \ OBIA has been adopted\nto classify and sample from pure canopy pixels [119,122,186].\
    \ Consideration should be provided on\nthe number of features and their suitability\
    \ for a speciﬁc application to reduce the computational\nburden, as well as to\
    \ maintain the accuracies. The generalisation of these algorithms for transferability\n\
    between study sites usually penalises the achievable accuracy. For details in\
    \ object-based approach of\nsegmentation and classiﬁcation, readers are directed\
    \ to literatures [122,183,185,187–189].\nOther techniques found in the literature\
    \ include algorithms, such as ‘Watershed’, which has been\ndemonstrated in palm\
    \ orchards [82,190]. Vine rows and plants have been isolated and classiﬁed using\n\
    image processing techniques, such as clustering and skeletisation [188,191–193].\
    \ Similarly, the gridded\npolygon, available in common GIS software, such as ArcGIS\
    \ and QGIS, can be used in combination\nwith zonal statistics for this purpose.\
    \ When working with the low-resolution images, co-registration\nwith the high-resolution\
    \ images has been proposed, whereby, the high-resolution images enable better\n\
    delineation of the mixed pixels [194]. For this reason, spectral and thermal sensors,\
    \ which are usually\nlow in resolution, are generally employed along with high-resolution\
    \ digital cameras.\n4.4. Indicators of Crop Water Status\nA crop’s biophysical\
    \ and biochemical attributes can be approximated using diﬀerent indices and\n\
    quantitative products. For example, CWSI is used to proxy leaf water potential\
    \ (Ψleaf), stem water\npotential (Ψstem), gs, and net photosynthesis (Pn) [83,100,195].\
    \ With regard to horticultural crops, water\nstatus has been assessed using a\
    \ number of spectral and thermal indices (Table 2).\nTable 2. Commonly used vegetation\
    \ and thermal indices to study the water status of horticultural crops.\nIndicators\n\
    Sensor\nPurpose\nReferences\nTc, (Tc − Ta)\nThermal\nΨstem, gs, yield\n[34,82,85,99,110]\n\
    Ig, I3\nThermal\nΨstem, gs\n[82,196]\nCWSI\nThermal\nΨleaf, Ψstem, gs, Pn, yield\n\
    [18,31,33,85,90,97,99,100,182,194,197–199]\n(Tc − Ta)/NDVI\nThermal + multispectral\n\
    Ψstem, gs\n[82,200]\nNDVI\nMultispectral\nΨstem, gs, yield, LAI, vigour\n[34,56,82,86,182,201]\n\
    GNDVI\nMultispectral\nΨstem, gs, yield\n[34,82]\nRDVI\nMultispectral\nΨstem, gs\n\
    [82,86,182]\nPRI\nMultispectral\nΨleaf, gs\n[86,110,182]\nFluorescence\nHyperspectral\n\
    Ψleaf, gs\n[110]\nWBI\nHyperspectral\nΨleaf, gs\n[139,202,203]\nSIF\nHyperspectral\n\
    Water stress\n[204–206]\nNote the acronyms: Tc = Canopy temperature, Ta = ambient\
    \ temperature, Ig = conductance index, I3 = stomatal\nconductance index, CWSI\
    \ = crop water stress index, NDVI = normalised diﬀerence vegetation index, GNDVI\
    \ = green\nnormalised diﬀerence vegetation index, RDVI = renormalized diﬀerence\
    \ vegetation index, PRI = photochemical\nreﬂectance index, Fluorescence = chlorophyll\
    \ ﬂuorescence, WBI = water band index, SIF = solar-induced chlorophyll\nﬂuorescence,\
    \ LAI = leaf area index.\nAgronomy 2020, 10, 140\n13 of 35\n4.4.1. Canopy Temperature\n\
    A plant maintains its temperature by transpiring through the stomata to balance\
    \ the energy\nﬂuxes in and out of the canopy. As the plant experience stress (both\
    \ biotic and abiotic), the rate of\ntranspiration decreases, which results in\
    \ higher canopy temperature (Tc), which can be a proxy to\nunderstand the water\
    \ stress in the plant [207]. In this regard, crop water stress showed a correlation\n\
    with canopy temperature extracted from the thermal image [208], which enables\
    \ mapping the spatial\nvariability in water status [209]. Leaf/canopy temperature\
    \ alone, however, does not provide a complete\ncharacterisation of crop water\
    \ status, for instance, an equally stressed canopy can be 25 ◦C or 35 ◦C,\ndepending\
    \ on the current ambient temperature (Ta). Thus, canopy-to-air temperature diﬀerence\n\
    (Tc − Ta) was proposed, which showed a good correlation with the Ψstem, Ψleaf,\
    \ and gs in horticultural\ncrops [85,99,182].\n4.4.2. Normalised Thermal Indices\n\
    The CWSI, the conductance index (Ig) and the stomatal conductance index (I3) are\
    \ thermal\nindices most commonly used to estimate crop water status and gs [210–212].\
    \ These indices provide\nsimilar information, however, use a diﬀerent range of\
    \ numbers to represent the level of water stress.\nThe CWSI is normalised within\
    \ zero and one, whereas Ig and I3 represent stress using numbers\nbetween zero\
    \ and inﬁnity. CWSI has been adopted most widely in horticultural applications\
    \ to\nassess the water status of crops, such as the grapevines [100,213], almond\
    \ [91,198], citrus [85,110], and\nothers [18,87,99,214]. By normalising between\
    \ the lower and upper limits of (Tc − Ta), the CWSI of the\ncanopy presents quantiﬁable\
    \ relative water stress. The formula for CWSI computation is deﬁned as in\nEquation\
    \ (1) [208,212].\nCWSI =\n(Tc − Ta) − (Tc − Ta)LL\n(Tc − Ta)UL − (Tc − Ta)LL\n\
    (1)\nwhere (Tc − Ta)UL and (Tc − Ta)LL represent the upper and lower bound of\
    \ (Tc − Ta) which are found in\nthe water-stressed canopy and well-watered canopy\
    \ transpiring at the full potential (or maximum) rate,\nrespectively. Assuming\
    \ a constant ambient temperature, Equation (1) can be simpliﬁed to Equation (2),\n\
    which is the most widely reported formulation of CWSI with regard to the horticultural\
    \ remote sensing.\nCWSI = (Tc − Twet)\n\x10\nTdry − Twet\n\x11\n(2)\nwhere Twet\
    \ is the temperature of canopy transpiring at the maximum potential, and Tdry\
    \ is the\ntemperature of the non-transpiring canopy. CWSI has been shown to be\
    \ well-correlated with direct\nmeasurements of crop water status in the horticultural\
    \ environment [18,31,32,90,99]. In this regard,\na correlation of CWSI with various\
    \ ground measurements, such as Ψleaf [18,31,197], Ψstem [33,90,194],\nand gs [18,90,100],\
    \ have been established. Diurnal measurements of CWSI compared with Ψleaf showed\n\
    the best correlation at noon [89,197,209].\nCWSI is a normalised index, i.e.,\
    \ relative to a reference temperature range between Twet and Tdry,\nwhich is speciﬁc\
    \ to a region and crop type; thus, CWSI is not a universal quantitative indicator\
    \ of crop\nwater status. For instance, a CWSI of 0.5 for two diﬀerent varieties\
    \ of grapevines at diﬀerent locations\ndoes not conclusively inform that they\
    \ have equal or superior/inferior water status. Furthermore,\nthe degree of correlation\
    \ can change depending on the isohydric/anisohydric response of crop [214]\nwhere\
    \ early/late stomatal closure aﬀects the indicators of water stress [110]. Moreover,\
    \ phenological\nstage aﬀects the relationship between remotely sensed CWSI and\
    \ water stress [197]. Thus, water stress\nin a diﬀerent crop, at a diﬀerent location\
    \ and at a diﬀerent phenological stage, will have a unique\ncorrelation with CWSI\
    \ and, therefore, needs to be established independently.\nThere are multiple methods\
    \ to measure the two reference temperatures, Twet and Tdry, which\ncould result\
    \ in variable CWSI values depending on the method used. The ﬁrst method is to\
    \ measure the\ntwo reference temperatures on the crop of interest. Tdry can be\
    \ estimated by inducing stomatal closure,\nAgronomy 2020, 10, 140\n14 of 35\n\
    which is the leaf temperature approximately 30 min after applying a layer of petroleum\
    \ jelly e.g.,\nVaseline to both sides of a leaf. This eﬀectively blocks stomata\
    \ and, therefore, impedes leaf transpiration.\nTwet can be estimated by measuring\
    \ leaf temperature approximately 30 s after spraying water on the\nleaf, which\
    \ emulates maximum transpiration [23,83]. The advantage of this method is that\
    \ the stress\nlevels are normalised to actual plants response, whereas the necessity\
    \ to repeat the measurement for\nevery test site after each ﬂight can be cumbersome.\
    \ In an alternative (second) approach the range can\nbe established based on meteorological\
    \ data e.g., setting Tdry to 5 ◦C above air temperature and Twet\nmeasured from\
    \ an artiﬁcial surface. This method is also limited to local scale and presents\
    \ a problem\nregarding the choice of material, which ideally needs to have similar\
    \ to leaf emissivity, aerodynamic\nand optical properties [54,87]. The third method\
    \ uses the actual temperature measurement range\nof the remote sensing image [33,97].\
    \ This method is simple to implement, however, works on the\nassumption that the\
    \ ﬁeld contains enough variability to contain a representative Twet and Tdry.\
    \ Fourth,\nthe reference temperatures can be estimated by theoretically solving\
    \ for the leaf surface energy balance\nequations, however, are limited by the\
    \ necessity to compute the canopy aerodynamic resistance [87].\nStandard and robust\
    \ Twet and Tdry measurements are needed to characterize CWSI with accuracy,\n\
    especially for temporal analysis [85,87,211]. The level of uncertainty due to\
    \ the adaptation of diﬀerent\napproaches for Twet and Tdry determination in the\
    \ instantaneous and seasonal measurements of CWSI\nis not known. Nonetheless,\
    \ adopting a consistent approach, CWSI has been shown to be suitable for\nmonitoring\
    \ the water status and making irrigation decisions of horticultural crops [31,85].\n\
    4.4.3. Spectral Indices\nCrops reﬂectance properties convey information about\
    \ the crop, for instance, a healthier crop has\nhigher reﬂectance in the NIR band.\
    \ Most often, the bands are mathematically combined to form VIs,\nwhich provide\
    \ information on the crop’s health, growth stage, biophysical properties, leaf\
    \ biochemistry,\nand water stress [29,215–218]. Using multispectral or hyperspectral\
    \ data, several Vis, such as green\nnormalised diﬀerence vegetation index (GNDVI),\
    \ renormalised diﬀerence vegetation index (RDVI),\noptimized soil-adjusted vegetation\
    \ index (OSAVI), transformed chlorophyll absorption in reﬂectance\nindex (TCARI),\
    \ and TCARI/OSAVI, amongst others [34,79,82], can be calculated that correlate\
    \ with the\nwater stress of horticultural crops (see Table 2). The most widely\
    \ studied VI in horticulture, in this\nregard, is the NDVI (Equation (3)).\nNDVI\
    \ = Rnir − Rr\nRnir + Rr\n(3)\nwhere Rnir and Rr represent the spectral reﬂectance\
    \ acquired at the NIR and red spectral regions,\nrespectively. In horticulture,\
    \ NDVI has been used as a proxy to estimate the vigour, biomass, and water\nstatus\
    \ of the crop. A vigorous canopy with more leaves regulates more water, therefore\
    \ remaining\ncooler when irrigated [200] and experiencing early water stress when\
    \ unirrigated. With regard to\nirrigation, the broadband normalised spectral indices\
    \ (such as NDVI) are suitable to detect spatial\nvariability and to identify the\
    \ area that is most vulnerable to water stress. However, these indices are\nnot\
    \ expected to change rapidly to reﬂect the instantaneous water status of plants\
    \ that are needed to\nmake decisions on irrigation scheduling.\nThe multispectral\
    \ indices along with complementary information in thermal wavelengths have\nproven\
    \ to be well suited to monitoring vegetation, speciﬁcally in relation to water\
    \ stress [219]. The ratio\nof canopy surface temperature to NDVI, deﬁned as temperature-vegetation\
    \ dryness index (TVDI),\nwas found to be useful for the study of water status\
    \ in horticultural crops. TVDI exploits the fact that\nvegetation with larger\
    \ NDVI will have a lower surface temperature unless the vegetation is under\n\
    stress. As most vegetation normally remains green after an initial bout of water\
    \ stress, the TVDI is\nmore suited than NDVI for early detection of water stress\
    \ as the surface temperature can rise rapidly\neven during initial water stress\
    \ [200].\nSimilarly, narrowband VIs that have been studied in relation to remote\
    \ sensing of water status are\nPRI and chlorophyll ﬂuorescence, which have been\
    \ directly correlated to the crop Ψleaf, gs [110,182,204].\nAgronomy 2020, 10,\
    \ 140\n15 of 35\nSeveral hyperspectral indices to estimate water status have been\
    \ identiﬁed [139]; however, their\napplication in remote sensing of horticultural\
    \ crops is at its infancy. Hyperspectral indices speciﬁc to\nwater absorption\
    \ bands around 900 nm, 1200 nm, 1400 nm, and 1900 nm may be used to detect the\n\
    water status of horticultural crops. The absorption features were found to be\
    \ highly correlated with\nplant water status [139]. Water band index (WBI), as\
    \ deﬁned in Equation (4), has been shown to closely\ntrack the changes in the\
    \ plant water status of various crops [202,203].\nWBI = R970\nR900\n(4)\nOther\
    \ water-related hyperspectral indices with potential application for horticultural\
    \ crops can\nbe found in the literature [139,202,203]. Hyperspectral data possess\
    \ the capability to reﬂect the\ninstantaneous water status of the plant, which\
    \ can be useful for quantitative decision-making on\nirrigation scheduling.\n\
    4.4.4. Soil Moisture\nThe moisture status of the soil provides an indication of\
    \ the available water resource to the crop.\nSoil moisture is traditionally measured\
    \ indirectly using soil moisture sensors placed below the surface\nof the soil.\
    \ A key challenge with using soil moisture sensors are the spatial distribution\
    \ of moisture,\nboth vertically and horizontally, to account for inherent ﬁeld-scale\
    \ variability. For instance, the root\nsystem of some horticultural crops, such\
    \ as grapevine, is capable of accessing water up to 30 m deep,\nwhile customer-grade\
    \ soil moisture probes generally extend to 1.5 m in depth or less. Thus, soil\n\
    moisture probes do not capture all the water available to the crop as they are\
    \ point measures and\nnot necessarily where the roots are located. Moreover, estimation\
    \ of soil moisture across spatial and\ntemporal scales is of interest for various\
    \ agricultural and hydrological studies. Optical, thermal, and\nmicrowave remote\
    \ sensing with their advantages relating to high spatial scale and temporal resolutions\n\
    could potentially be used for soil moisture estimation [220–222]. L-band microwave\
    \ radiometry,\na component of synthetic aperture radar systems, has been shown\
    \ to be a reliable approach to estimate\nsoil moisture via satellite-based remote\
    \ sensing [223], such as using the ESA’s Soil Moisture and\nOcean Salinity (SMOS)\
    \ [224] and NASA’s Soil Moisture Active Passive (SMAP) satellites [225,226].\n\
    The limitation of the SMOS and SMAP missions, with regard to horticultural application,\
    \ is their\ndepth of retrieval (up to 5 cm) and spatial resolution (in the order\
    \ of tens of kilometre) [227–229].\nAs an airborne application, the volumetric\
    \ soil moisture has been estimated by analysing the SNR of\nthe GNSS interference\
    \ signal [230,231]. With aforementioned capabilities, a combination of satellite\n\
    and airborne remote sensing may, in the future, be a reliable tool to map soil\
    \ moisture across spatial,\ntemporal and depth scales.\n4.4.5. Physiological Attributes\n\
    Using the SfM on remotely-sensed images, 3D canopy structure, terrain conﬁgurations,\
    \ and canopy\nsurface models can be derived [113,114,119,186,232]. By employing\
    \ a delineation algorithm on the 3D\nmodels, the 3D attributes of the crops and\
    \ macrostructure are determined more accurately [120,122,233].\nCrop surface area\
    \ and terrain conﬁguration (e.g., slope and aspect) may help to develop an optimal\n\
    resource management strategy. For example, crops located at a higher elevation\
    \ within an irrigation\nzone may experience a level of water stress due to the\
    \ gravitational ﬂow of irrigated water.\nUsing the structural measurements, such\
    \ as the canopy height, canopy size, the envelope of each\nrow, LAI, and porosity,\
    \ among others, the water demand of the crop may be estimated. Generally,\nlarger\
    \ canopies tend to require more water than smaller canopies with less leaf area\
    \ [116,157]. Using\nthe temporal measurement of the plant’s 3D attributes, the\
    \ vigour can be computed. Monitoring\ncrop vigour over the season and over subsequent\
    \ years can provide an indication of its health and\nperformance, e.g., yield,\
    \ within an irrigation zone. Canopy structure metrics are closely related to\n\
    horticultural tree growth and provide strong indicators of water consumption,\
    \ whereby canopy size\nAgronomy 2020, 10, 140\n16 of 35\ncan be used to determine\
    \ its water requirements [234]. Other 3D attributes, such as the crown perimeter,\n\
    width, height, area, and leaf density, have been shown to enable improved pruning\
    \ of horticultural\nplants [116,119].\nLAI can be estimated using the 3D attributes\
    \ obtained from remote sensing [114,157,201], whereby,\nhigher LAI is equivalent\
    \ to more leaf layers, implying greater total leaf area and, consequently, canopy\n\
    transpiration. Leaf density, LAI, and exposed leaf area of a crop drive its water\
    \ requirement and\nproductivity [235–237]. Knowledge of ﬁeld attributes, such\
    \ as row and plant spacing, may assist in\ninter-row surface energy balance to\
    \ determine the irrigation need of the plant [238]. Combining the\nstructural\
    \ properties with spectral VIs provide an estimation of biomass [239], which can\
    \ serve as\nanother indicator of the plant’s water requirements. Although physiological\
    \ attributes have been used\nto understand plant water status and its spatial\
    \ variability, they have not been directly applied to make\nquantitative decisions\
    \ on irrigation.\n4.4.6. Evapotranspiration\nThe estimation of ET via remote sensing,\
    \ numerical modelling, and empirical methods have been\nextensively studied and\
    \ reviewed in the literature [240–247]. These models are based on either surface\n\
    energy balance (SEB), Penman-Monteith (PM), Maximum entropy production (MEP),\
    \ water balance,\nwater-carbon linkage, or empirical relationships.\nSEB models\
    \ are based on a surface energy budget in which the latent heat ﬂux is estimated\
    \ as a\nresidual of the net radiation, soil heat ﬂux, and sensible heat ﬂux. The\
    \ models are either one-source\n(canopy and soil treated as a single surface for\
    \ the estimation of sensible heat ﬂux) or two-source\n(canopy and soil surfaces\
    \ treated separately). Improvements over the original one-source SEB models\n\
    were in the form of Surface Energy Balance Algorithm for Land (SEBAL) algorithm\
    \ [248,249] and\nMapping EvapoTranspiration with high Resolution and Internalized\
    \ Calibration (METRIC) [249,250].\nSEBAL oﬀers a simpliﬁed approach to collect\
    \ ET data at both local and regional scales thereby increasing\nthe spatial scope,\
    \ while METRIC uses the same (SEBAL) technique but auto-calibrates the model using\n\
    hourly ground-based reference ET (ETr) data [251]. As such, these and other (e.g.,\
    \ MEP) models rely\non accurate measurements of surface (e.g., canopy) and air\
    \ temperatures, which can be erroneous\nunder non-ideal conditions, e.g., cloudy\
    \ days. There is also a reliance on ground-based sensors to\ncapture ambient air\
    \ temperatures required by the model.\nAmong the existing methods, FAO’s PM is\
    \ the most widely adopted model to estimate reference\nET (ETref or ET0) [252].\
    \ The PM method uses incident and reﬂected solar radiation, emitted thermal\n\
    radiation, air temperature, wind speed, and vapour pressure to calculate ET0 [253].\
    \ Remote sensing\nprovides a cost-eﬀective method to estimate the ET0 at regional\
    \ to global scales [241] by estimating\nreﬂected solar and emitted thermal radiation.\
    \ One of the advantages of using the PM approach is\nthat it is parametrised using\
    \ micrometeorological data easily obtained from ground-based automatic\nweather\
    \ stations. However, PM suﬀers from the drawback that canopy transpiration is\
    \ not dynamic\nas inﬂuenced by soil moisture availability via stomatal regulation\
    \ [241]. From a practical standpoint,\nPM-derived ET0 estimates are used in conjunction\
    \ with crop factors or crop coeﬃcients (kc), which are\nclosely related to the\
    \ light interception of the canopy [254].\nCrop evapotranspiration (ETc) is deﬁned\
    \ as the product of kc and ET0. In the absence of accurate\nETc measurements,\
    \ kc is an easy and practical means of getting reliable estimates of ETc using\
    \ ET0 [255].\nIn this regard, studies have focused on the use of remote sensing\
    \ to study spatial variability in kc and\nETc [101,256–258]. Thermal and NIR imagery\
    \ can be used to compute kc and ETc as transpiration\nrate is closely related\
    \ to canopy temperature [259–261] and kc has been shown to correlate with\ncanopy\
    \ reﬂectance [101,255]. Various thermal indices, such as CWSI, canopy temperature\
    \ ratio, canopy\ntemperature above non-stressed, and canopy temperature above\
    \ canopy threshold, can be used to\nestimate ETc, where CWSI- based ETc was found\
    \ to be the most accurate [24].\nET at a larger scale is typically estimated based\
    \ on satellite remote sensing. The temporal resolution\nof satellites is, however,\
    \ low and inadequate for horticultural applications, such as irrigation scheduling\n\
    Agronomy 2020, 10, 140\n17 of 35\n(e.g., Landsat has a 16-day revisit cycle).\
    \ In contrast, high temporal resolution satellites are coarse in\nspatial resolution\
    \ for ﬁeld-scale observations [25]. The daily or even instantaneous estimation\
    \ of ETc at\nthe ﬁeld scale is crucial for irrigation scheduling and is expected\
    \ to have great application prospects\nin the future [240,259,262,263]. In this\
    \ regard, the future direction of satellite-based ET estimates\nmay focus on temporal\
    \ downscaling either by extrapolation of instantaneous measurement [264],\ninterpolation\
    \ between two successive observations [201], data fusion of multiple satellites\
    \ [25,260], and\nspatial downscaling using multiple satellites [265–268]. An example\
    \ of early satellite-based remote\nsensing for ET is the MODIS Global Evapotranspiration\
    \ Project (MOD16), which was established in\n1999 to provide daily estimates of\
    \ global terrestrial evapotranspiration using data acquired from a\npair of NASA\
    \ satellites in conjunction with Algorithm Theoretical Based Documents (ATBDs)\
    \ [269].\nThese estimates correlated well with ground-based eddy covariance ﬂux\
    \ tower estimates of ET despite\ndiﬀerences in the uncertainties associated with\
    \ each of these techniques.\nUASs are being increasingly utilised to acquire multi-spectral\
    \ and thermal imagery to compute\nET at an unprecedented spatial resolution [270,271].\
    \ Using high-resolution images, ﬁltering the\nshadowed-pixel is possible, which\
    \ showed signiﬁcant improvement in the estimation of ET in\ngrapevine [101]. Using\
    \ high-resolution thermal and/or multispectral imagery, ET has been derived for\n\
    horticultural crops, such as grapevines [270] and olives [271]. The seasonal monitoring\
    \ of ETc at high\nspatial and temporal resolutions is of high importance for precision\
    \ irrigation of horticultural crops in\nthe future [259].\n5. Case Studies on\
    \ the Use of Remote Sensing for Crop Water Stress Detection\nThe increasing prevalence\
    \ of UAS along with low-cost camera systems has brought about much\ninterest in\
    \ the characterisation of crop water status/stress during the growing season to\
    \ inform orchard\nor farm management decisions, in particular, irrigation scheduling\
    \ [272,273]. Traditional methodologies\nto assess crop water stress are constrained\
    \ by limitations relating to large farm sizes and accompanying\nspatial variability,\
    \ high labour costs to collect data, and access to instrumentation that is both\
    \ inexpensive\nand portable [272]. The beneﬁts of precision agriculture [274],\
    \ including through precision irrigation\npractices [1], result in higher production\
    \ eﬃciencies and economic returns through site-speciﬁc crop\nmanagement [275,276].\
    \ This approach has motivated the use of high-resolution imagery acquired\nfrom\
    \ remote sensing to identify irrigation zones [99,277]. The ﬁrst horticultural\
    \ applications of UAS\nplatforms for crop water status measurement were in orange\
    \ and peach orchards where both thermal\nand multispectral-derived VIs, speciﬁcally\
    \ the PRI, were shown to be well-correlated to crop water\nstatus [102]. Here,\
    \ we explore the use of remote sensing and accompanying image acquisition platforms\n\
    to characterise the spatial and temporal patterns of the water status of two economically\
    \ important\nhorticultural crops, grapevine and almond.\n5.1. Grapevine (Vitis\
    \ spp.)\nThe characterisation of spatial variability in vine water status in a\
    \ vineyard provides valuable\nguidance on irrigation scheduling decisions [82],\
    \ and this spatial variability can be eﬃciently\ncharacterised by the use of remote\
    \ sensing platforms [29]. The ﬁrst use of remote sensing in vineyards\nfor crop\
    \ water stress detection was using manned aircraft ﬂown over an irrigated vineyard\
    \ in Hanwood\n(NSW) Australia where CWSI was mapped at a spatial resolution of\
    \ 10 cm [278]. Subsequently, UAS\nplatforms began to be used in vineyards for\
    \ vine water stress characterisation. Early work in this\ncrop used a fuel-based\
    \ helicopter with a 29 cc engine and equipped with thermal (Thermovision\nA40M)\
    \ and multispectral (Tetracam MCA-6) camera systems [102]. The study observed\
    \ strong (inverse)\nrelationships between (Tc − Ta) and gs. A related study showed\
    \ strong correlations between thermal\nand multispectral VIs, and traditional,\
    \ ground-based measures of water status, such as Ψleaf and\ngs [182]. In this\
    \ study, normalised PRI was shown to have correlation coeﬃcients exceeding 0.8\
    \ versus\nboth Ψleaf and gs, indicating that remotely-sensed VIs can be reliable\
    \ indicators of vine water status.\nThermal indices, such as (Tc − Ta) and CWSI,\
    \ were also well-correlated to Ψleaf and gs at speciﬁc times of\nAgronomy 2020,\
    \ 10, 140\n18 of 35\nthe day. The use of thermal indices, such as CWSI or Ig,\
    \ requires reference temperatures (Twet, Tdry) or\nnon-water stressed baselines\
    \ (NWSB) [279]. Due to the diﬃculty of obtaining reference temperatures or\nNWSB\
    \ using remote sensing, some authors have used the minimum temperature found from\
    \ all canopy\npixels as Twet [199], and Ta + 5 ◦C as Tdry [213,280]. NWSB is typically\
    \ obtained from well-watered\ncanopies, measuring (Tc − Ta) under a range of vapour\
    \ pressure deﬁcit conditions [279]. Thermal water\nstress indices have also shown\
    \ to be useful to distinguish between water use strategies of diﬀerent\ngrapevine\
    \ cultivars [83,281], which is useful for customising irrigation scheduling based\
    \ on the speciﬁc\nwater needs of a given cultivar. More recently, studies have\
    \ used UAS-based multispectral-based\nVIs to train an artiﬁcial neural network\
    \ (ANN) models to predict spatial patterns of Ψstem [84,282].\nUsing UAS-based\
    \ multispectral data, the authors showed that ANN estimated Ψstem with higher\n\
    accuracy (RMSE lower than 0.15 MPa) as compared to the conventional multispectral\
    \ indices based\nestimation (RMSE over 0.32 MPa).\n5.2. Almond (Prunus Dulcis)\n\
    Almonds are perennial nut trees grown in semi-arid climates and are reliant on\
    \ irrigation\napplications. Their water requirements are relatively high, with\
    \ seasonal ETc exceeding 1000 mm [283].\nThe requirement for prudent irrigation\
    \ management in the face of decreased water availability is\ncritical for maintaining\
    \ tree productivity, yield, and nut quality [284]. Towards this goal, UAS-based\n\
    remote sensing has been used to characterise the spatial patterns of tree water\
    \ status in almond\norchards. A UAS-based thermal camera was used to acquire tree\
    \ the crown temperature data from\na California almond orchard; this temperature\
    \ was used to determine the temperature diﬀerence\nbetween crown and air (Tc −\
    \ Ta) and compared to shaded leaf water potential (Ψsl) [92]. The study\nfound\
    \ a strong negative correlation (R2 = 0.72) between (Tc − Ta) and Ψsl. The same\
    \ authors conducted\na follow on study in Spain on several fruit tree species\
    \ including almond. The negative relationship\n(slope and oﬀset) between (Tc −\
    \ Ta) and Ψstem was observed to vary based on the time of observation;\nmorning\
    \ measurements had weak relationships, whereas afternoon measurements had stronger\n\
    relationships [99]. Their proposed methodology allowed for the spatial characterisation\
    \ of orchard\nwater status on a single-tree basis, demonstrating the utility of\
    \ UAS-based crop water stress data.\nBeyond the characterisation of crop water\
    \ stress for irrigation scheduling, there is an opportunity to\nuse this data\
    \ to quantify the economic impact at a spatial level.\n6. Future Prospective and\
    \ Gaps in the Knowledge\nPrecision irrigation is a promising approach to increase\
    \ farm water use eﬃciency for sustainable\nproduction, including for horticultural\
    \ crops [3,5,9,10,274,285]. It is envisioned that the future of\nprecision irrigation\
    \ will incorporate UAS, manned aircraft, and satellite-based remote sensing platforms\n\
    alongside ground-based proximal sensors coupled with wireless sensor networks.\
    \ The automation\nof UAS technology will continue to develop further to a point\
    \ that even novice users can adopt\nthe technology with ease. It is also expected\
    \ that the data processing pipeline of remote sensing\nimages will become automated\
    \ to be ‘ﬁt for purpose’ for crop water status measurements. The ideal\nsolution\
    \ may lie in the use of satellites (or sometimes manned aircraft) for regional\
    \ estimation and\nplanning [55,260], UAS for seasonal monitoring and zoning [32,100,197,286],\
    \ proximal sensors for\ncontinuous measurement [287], and artiﬁcial intelligence\
    \ to derive decision-ready products [84,282]\nthat can be used for making irrigation\
    \ scheduling decisions [31,288–295]. Continued technological\ndevelopments in\
    \ this space will enable growers to acquire actionable data with ease, and eventually\n\
    transition towards semi-automated or fully-automated irrigation applications.\n\
    Remote sensing and current irrigation application technologies are limited in\
    \ temporal and\nspatial resolution, respectively. Although UAS technology can\
    \ deliver sub-plant level spatially explicit\ninformation of water status, the\
    \ size of the management block is much coarser, typically over 10 m.\nHence, further\
    \ improvements in variable rate application technologies, e.g., boom sprayers,\
    \ or zoned\ndrip irrigation, are required to fully exploit high-resolution UAS\
    \ measurements. Nonetheless, the\nAgronomy 2020, 10, 140\n19 of 35\nrequired resolution\
    \ of remote sensing should be guided by the underlying spatial variability of\
    \ the crop.\nFor ﬁelds with relatively lower spatial variability, low/medium-resolution\
    \ remote sensing imagery\nmay suﬃce for crop water status assessment [278,296,297].\n\
    Remote sensing provides an indirect estimate of plant water status using the regression-based\n\
    approach through several calculated reﬂectance indices. In comparison, physical\
    \ and mechanistic\nmodels, e.g., radiative transfer models and energy balance\
    \ models, incorporate both direct and indirect\nmeasures of the canopy, therefore\
    \ establishing a basis for diﬀerences in plant water status. Using a\nsimilar\
    \ approach, predictions of crop water status using regression-based remote sensing\
    \ models can\nbe improved by incorporating some direct auxiliary variables.\n\
    Further developments in thermal remote sensing are also expected, speciﬁcally,\
    \ the advent of new\nthermal and hybrid thermal-multispectral water status/stress\
    \ indices that are more sensitive to canopy\ntranspiration. The most widely-adopted\
    \ thermal index, CWSI, is an instantaneous measure that is\nnormalised to local\
    \ weather conditions and inﬂuenced by genotype and phenotype. For example,\nthe\
    \ relationship between CWSI and crop water status is inﬂuenced by environmental\
    \ conditions\n(e.g., high incident radiation and low humidity vs low incident\
    \ radiation and high humidity) and\nphenological stage [197,214,298]. As a result,\
    \ corresponding ground-based measurements are required\nfor each temporal remote\
    \ measurement to determine the correlation with water status. Hence, temporal\n\
    assessments of water status using thermal cameras will require the incorporation\
    \ of meteorological\ndata along with the thermal response using novel indices.\n\
    In the area of satellite remote sensing, we foresee further developments on temporal\
    \ downscaling\nto achieve daily measurements. A higher temporal resolution may\
    \ be achieved by fusion of multiple\nsatellite observations, such as freely available\
    \ Landsat and Sentinel. Further reductions of temporal\nresolution will require\
    \ interpolation between two successive observations. Furthermore, temporal\nmodels\
    \ of water status could be developed to assist the interpolation to eventually\
    \ satisfy the\nrequirements for irrigation scheduling [25,201,263]. The continued\
    \ advancement and greater availability\nof Nanosat/Cubesat may provide an alternate\
    \ method to capture high-resolution data at a higher\na greater temporal resolution,\
    \ which can be suitable to study the water status of horticultural\ncrops [299–301].\n\
    Crop water status is a complex phenomenon, which can be interpreted with respect\
    \ to a\nnumber of variables. These variables can include spectral response, thermal\
    \ response, meteorological\ndata, 3D attributes of the canopy, and macrostructure\
    \ of the block (farm).\nClearly, there is\nan opportunity for a multi-disciplinary\
    \ approach, potentially incorporating artiﬁcial intelligence\ntechniques which\
    \ incorporate the aforementioned variables to provide a robust estimation of crop\n\
    water status [84,141,282,302,303]. Furthermore, with machine learning algorithms,\
    \ hyperspectral\nremote sensing will provide a wealth of data to estimate crop\
    \ water status. A quantitative product,\nsuch as SIF, derived from hyperspectral\
    \ data will have the potential for direct quantiﬁcation of water\nstress [204,205,304].\
    \ In this regard, the upcoming FLEX satellite mission [305,306] and recent advances\n\
    in aerial spectroradiometry [109,132,137,307–310] dedicated for observation of\
    \ SIF may be unique and\npowerful tools for high-value horticultural crops.\n\
    Multi-temporal images represent an excellent resource for seasonal monitoring\
    \ of changes in crop\nwater status. Five to six temporal points of data acquisition\
    \ at critical phenological stages of crop\ndevelopment have been recommended for\
    \ irrigation scheduling [31,32]. However, for semi-arid or arid\nregions, irrigation\
    \ is typically required multiple times per week. Acquisition and post-processing\
    \ of\nremote sensing data for actionable products multiple times a week is currently\
    \ logistically unfeasible.\nThe fusion of UAS-based remote sensing data, continuous\
    \ ground-based proximal or direct sensors,\nincluding weather station data, can\
    \ potentially inform daily estimates of water status at canopy level.\nThis approach\
    \ will require predictive models, such as those based on machine learning algorithms,\
    \ to\nestimate the current and future water status of the crop. Eventually, growers\
    \ would beneﬁt from the\nknowledge of crop water requirements for the determination\
    \ of seasonal irrigation requirements to\nsustainably farm into the future.\n\
    Agronomy 2020, 10, 140\n20 of 35\nOne vision for the future of precision irrigation\
    \ is in automated pipelines to explicitly manage\nirrigation water at the sub-block\
    \ level. This automated pipeline would likely include remote and\nproximal data\
    \ acquisition and processing, prediction and interpretation of crop water status\
    \ and\nrequirements, and subsequently, control of irrigation systems. Recent rapid\
    \ developments in cloud\ncomputing and wireless technology could assist in the\
    \ quasi-real-time processing of the remote sensing\ndata soon after acquisition\
    \ [311–313]. Eventually, automation and computational power will merge to\ndevelop\
    \ smart technology in which artiﬁcial intelligence uses real-time data analysis\
    \ for diagnosis\nand decision-making. Growers of the future will be able to take\
    \ advantage of precise irrigation\nrecommendations using information sourced from\
    \ a ﬂeet of UAS that map large farm blocks on a daily\nschedule, continuous ground-based\
    \ proximal and direct sensors, and weather stations. This data can be\nstored\
    \ on and accessed from the cloud almost instantaneously, used in conjunction with\
    \ post-processing\nalgorithms for decision-making on optimised irrigation applications\
    \ [311,314].\n7. Conclusions\nThis paper provides a comprehensive review of the\
    \ use of remote sensing to determine the water\nstatus of horticultural crops.\
    \ One of our objectives was to survey the range of remote sensing tools\navailable\
    \ for irrigation decision-making. Earth observation satellite systems possess\
    \ the required bands\nto study the water status of vegetation and soil. Satellites\
    \ are more suitable for scouting, planning,\nand management of irrigation applications\
    \ that involve large areas, and where data acquisition is\nnot time-constrained.\
    \ Manned aircraft are sparingly used in horticultural applications due to the\n\
    cost, logistics, and speciﬁc expertise needed for the operation of the platform.\
    \ UAS-based remote\nsensing provides ﬂexibility in spatial resolution (crop level\
    \ observation achievable), coverage (over\n25 ha achievable in a single ﬂight),\
    \ spectral bands, as well as temporal revisit. Routine monitoring of\nhorticultural\
    \ crops for water status characterisation is, therefore, best performed using\
    \ a UAS platform.\nWe envision a future for precision irrigation where satellites\
    \ are used for planning, and UAS used in\nconjunction with a network of ground-based\
    \ sensors to achieve actionable products on a timely basis.\nThe plant’s instantaneous\
    \ response to water stress can be captured using thermal cameras (via\nindices,\
    \ such as CWSI) and potentially narrow-band hyperspectral sensors (via, for example,\
    \ SIF),\nmaking them suitable to draw quantiﬁable decisions with regard to irrigation\
    \ scheduling. Broadband\nmultispectral and RGB cameras capture the non-instantaneous\
    \ water status of crops, making them\nsuitable for general assessment of crop\
    \ water status. Integrated use of thermal and multispectral\nimagery may be the\
    \ simplest yet eﬀective sensor combinations to capture the overall as well as\n\
    instantaneous water status of the plant. With regard to irrigation scheduling,\
    \ further developments\nare required to establish crop-speciﬁc thresholds of remotely-sensed\
    \ indices to decide when and how\nmuch to irrigate.\nAuthor Contributions: Performed\
    \ the article review and prepared the original draft, D.G.; contributed to write\n\
    the case studies, V.P., and together with D.G. contributed to review and edit\
    \ the manuscript. All authors have read\nand agreed to the published version of\
    \ the manuscript.\nFunding: This research and the APC was funded by Wine Australia\
    \ (Grant number: UA 1803-1.3).\nAcknowledgments: The authors would like to acknowledge\
    \ the funding body Wine Australia, The University of\nAdelaide, and anonymous\
    \ reviewers for their contribution.\nConﬂicts of Interest: The authors declare\
    \ no conﬂict of interest.\nReferences\n1.\nMonaghan, J.M.; Daccache, A.; Vickers,\
    \ L.H.; Hess, T.M.; Weatherhead, E.K.; Grove, I.G.; Knox, J.W. More\n‘crop per\
    \ drop’: constraints and opportunities for precision irrigation in European agriculture.\
    \ J. Sci.\nFood Agric. 2013, 93, 977–980. [CrossRef]\n2.\nSmith, R. Review of\
    \ Precision Irrigation Technologies and Their Applications; University of Southern\
    \ Queensland\nDarling Heights: Queensland, Australia, 2011.\nAgronomy 2020, 10,\
    \ 140\n21 of 35\n3.\nPiao, S.; Ciais, P.; Huang, Y.; Shen, Z.; Peng, S.; Li, J.;\
    \ Zhou, L.; Liu, H.; Ma, Y.; Ding, Y.; et al. The impacts of\nclimate change on\
    \ water resources and agriculture in China. Nature 2010, 467, 43. [CrossRef]\n\
    4.\nHowden, S.M.; Soussana, J.-F.; Tubiello, F.N.; Chhetri, N.; Dunlop, M.; Meinke,\
    \ H. Adapting agriculture to\nclimate change. Proc. Natl. Acad. Sci. USA 2007,\
    \ 104, 19691–19696. [CrossRef]\n5.\nWebb, L.; Whiting, J.; Watt, A.; Hill, T.;\
    \ Wigg, F.; Dunn, G.; Needs, S.; Barlow, E. Managing grapevines through\nsevere\
    \ heat: A survey of growers after the 2009 summer heatwave in south-eastern Australia.\
    \ J. Wine Res.\n2010, 21, 147–165. [CrossRef]\n6.\nDatta, S. Impact of climate\
    \ change in Indian horticulture-a review. Int. J. Sci. Environ. Technol. 2013,\
    \ 2,\n661–671.\n7.\nWebb, L.; Whetton, P.; Barlow, E. Modelled impact of future\
    \ climate change on the phenology of winegrapes\nin Australia. Aust. J. Grape\
    \ Wine Res. 2007, 13, 165–175. [CrossRef]\n8.\nWang, J.; Mendelsohn, R.; Dinar,\
    \ A.; Huang, J.; Rozelle, S.; Zhang, L. The impact of climate change on China’s\n\
    agriculture. Agric. Econ. 2009, 40, 323–337. [CrossRef]\n9.\nBeare, S.; Heaney,\
    \ A. Climate change and water resources in the Murray Darling Basin, Australia.\n\
    In Proceedings of the 2002 World Congress of Environmental and Resource Economists,\
    \ Monterey, CA, USA,\n24–27 June 2002.\n10.\nKhan, S.; Tariq, R.; Yuanlai, C.;\
    \ Blackwell, J. Can irrigation be sustainable? Agric. Water Manag. 2006, 80,\n\
    87–99. [CrossRef]\n11.\nDroogers, P.; Bastiaanssen, W. Irrigation performance\
    \ using hydrological and remote sensing modeling.\nJ. Irrig. Drain. Eng. 2002,\
    \ 128, 11–18. [CrossRef]\n12.\nRay, S.; Dadhwal, V. Estimation of crop evapotranspiration\
    \ of irrigation command area using remote sensing\nand GIS. Agric. Water Manag.\
    \ 2001, 49, 239–249. [CrossRef]\n13.\nKim, Y.; Evans, R.G.; Iversen, W.M. Remote\
    \ sensing and control of an irrigation system using a distributed\nwireless sensor\
    \ network. IEEE Trans. Instrum. Meas. 2008, 57, 1379–1387.\n14.\nRitchie, G.A.;\
    \ Hinckley, T.M. The pressure chamber as an instrument for ecological research.\
    \ In Advances in\nEcological Research; Elsevier: Amsterdam, The Netherlands, 1975;\
    \ Volume 9, pp. 165–254.\n15.\nSmart, R.; Barrs, H. The eﬀect of environment and\
    \ irrigation interval on leaf water potential of four\nhorticultural species.\
    \ Agric. Meteorol. 1973, 12, 337–346. [CrossRef]\n16.\nMeron, M.; Grimes, D.;\
    \ Phene, C.; Davis, K. Pressure chamber procedures for leaf water potential\n\
    measurements of cotton. Irrig. Sci. 1987, 8, 215–222. [CrossRef]\n17.\nSantos,\
    \ A.O.; Kaye, O. Grapevine leaf water potential based upon near infrared spectroscopy.\
    \ Sci. Agric.\n2009, 66, 287–292. [CrossRef]\n18.\nBerni, J.A.J.; Zarco-Tejada,\
    \ P.J.; Sepulcre-Cantó, G.; Fereres, E.; Villalobos, F. Mapping canopy conductance\n\
    and CWSI in olive orchards using high resolution thermal remote sensing imagery.\
    \ Remote Sens. Environ.\n2009, 113, 2380–2388. [CrossRef]\n19.\nChaves, M.M.;\
    \ Santos, T.P.; de Souza, C.; Ortuño, M.; Rodrigues, M.; Lopes, C.; Maroco, J.;\
    \ Pereira, J.S.\nDeﬁcit irrigation in grapevine improves water-use eﬃciency while\
    \ controlling vigour and production quality.\nAnn. Appl. Biol. 2007, 150, 237–252.\
    \ [CrossRef]\n20.\nBravdo, B.; Hepner, Y.; Loinger, C.; Cohen, S.; Tabacman, H.\
    \ Eﬀect of irrigation and crop level on growth,\nyield and wine quality of Cabernet\
    \ Sauvignon. Am. J. Enol. Vitic. 1985, 36, 132–139.\n21.\nMatthews, M.; Ishii,\
    \ R.; Anderson, M.; O’Mahony, M. Dependence of wine sensory attributes on vine\
    \ water\nstatus. J. Sci. Food Agric. 1990, 51, 321–335. [CrossRef]\n22.\nReynolds,\
    \ A.G.; Naylor, A.P. ‘Pinot noir’ and ‘Riesling’ grapevines respond to water stress\
    \ duration and soil\nwater-holding capacity. HortScience 1994, 29, 1505–1510.\
    \ [CrossRef]\n23.\nAlvino, A.; Marino, S. Remote sensing for irrigation of horticultural\
    \ crops. Horticulturae 2017, 3, 40. [CrossRef]\n24.\nKullberg, E.G.; DeJonge,\
    \ K.C.; Chávez, J.L. Evaluation of thermal remote sensing indices to estimate\
    \ crop\nevapotranspiration coeﬃcients. Agric. Water Manag. 2017, 179, 64–73. [CrossRef]\n\
    25.\nSemmens, K.A.; Anderson, M.C.; Kustas, W.P.; Gao, F.; Alﬁeri, J.G.; McKee,\
    \ L.; Prueger, J.H.; Hain, C.R.;\nCammalleri, C.; Yang, Y.; et al. Monitoring\
    \ daily evapotranspiration over two California vineyards using\nLandsat 8 in a\
    \ multi-sensor data fusion approach. Remote Sens. Environ. 2015, 185, 155–170.\
    \ [CrossRef]\n26.\nJackson, R.D. Remote sensing of biotic and abiotic plant stress.\
    \ Annu. Rev. Phytopathol. 1986, 24, 265–287.\n[CrossRef]\nAgronomy 2020, 10, 140\n\
    22 of 35\n27.\nMoran, M.; Clarke, T.; Inoue, Y.; Vidal, A. Estimating crop water\
    \ deﬁcit using the relation between surface-air\ntemperature and spectral vegetation\
    \ index. Remote Sens. Environ. 1994, 49, 246–263. [CrossRef]\n28.\nLamb, D.; Hall,\
    \ A.; Louis, J. Airborne remote sensing of vines for canopy variability and productivity.\n\
    Aust. Grapegrow. Winemak. 2001, 449a, 89–94.\n29.\nHall, A.; Lamb, D.; Holzapfel,\
    \ B.; Louis, J. Optical remote sensing applications in viticulture-a review. Aust.\
    \ J.\nGrape Wine Res. 2002, 8, 36–47. [CrossRef]\n30.\nDe Bei, R.; Cozzolino,\
    \ D.; Sullivan, W.; Cynkar, W.; Fuentes, S.; Dambergs, R.; Pech, J.; Tyerman,\
    \ S.\nNon-destructive measurement of grapevine water potential using near infrared\
    \ spectroscopy. Aust. J. Grape\nWine Res. 2011, 17, 62–71. [CrossRef]\n31.\nBellvert,\
    \ J.; Zarco-Tejada, P.J.; Marsal, J.; Girona, J.; González-Dugo, V.; Fereres,\
    \ E. Vineyard irrigation\nscheduling based on airborne thermal imagery and water\
    \ potential thresholds. Aust. J. Grape Wine Res. 2016,\n22, 307–315. [CrossRef]\n\
    32.\nBellvert, J.; Marsal, J.; Girona, J.; Gonzalez-Dugo, V.; Fereres, E.; Ustin,\
    \ S.; Zarco-Tejada, P. Airborne thermal\nimagery to detect the seasonal evolution\
    \ of crop water status in peach, nectarine and Saturn peach orchards.\nRemote\
    \ Sens. 2016, 8, 39. [CrossRef]\n33.\nPark, S.; Ryu, D.; Fuentes, S.; Chung, H.;\
    \ Hernández-Montes, E.; O’Connell, M. Adaptive estimation of crop\nwater stress\
    \ in nectarine and peach orchards using high-resolution imagery from an unmanned\
    \ aerial vehicle\n(UAV). Remote Sens. 2017, 9, 828. [CrossRef]\n34.\nEspinoza,\
    \ C.Z.; Khot, L.R.; Sankaran, S.; Jacoby, P.W. High resolution multispectral and\
    \ thermal remote\nsensing-based water stress assessment in subsurface irrigated\
    \ grapevines. Remote Sens. 2017, 9, 961.\n[CrossRef]\n35.\nEzenne, G.I.; Jupp,\
    \ L.; Mantel, S.K.; Tanner, J.L. Current and potential capabilities of UAS for\
    \ crop water\nproductivity in precision agriculture. Agric. Water Manag. 2019,\
    \ 218, 158–164. [CrossRef]\n36.\nOliver, M.A.; Webster, R. Kriging: A method of\
    \ interpolation for geographical information systems. Int. J.\nGeogr. Inf. Syst.\
    \ 1990, 4, 313–332. [CrossRef]\n37.\nHa, W.; Gowda, P.H.; Howell, T.A. A review\
    \ of downscaling methods for remote sensing-based irrigation\nmanagement: Part\
    \ I. Irrig. Sci. 2013, 31, 831–850. [CrossRef]\n38.\nHa, W.; Gowda, P.H.; Howell,\
    \ T.A. A review of potential image fusion methods for remote sensing-based\nirrigation\
    \ management: Part II. Irrig. Sci. 2013, 31, 851–869. [CrossRef]\n39.\nBelward,\
    \ A.S.; Skøien, J.O. Who launched what, when and why; trends in global land-cover\
    \ observation\ncapacity from civilian earth observation satellites. ISPRS J. Photogramm.\
    \ Remote Sens. 2015, 103, 115–128.\n[CrossRef]\n40.\nLucieer, A.; Malenovskỳ,\
    \ Z.; Veness, T.; Wallace, L. HyperUAS—Imaging spectroscopy from a multirotor\n\
    unmanned aircraft system. J. Field Robot. 2014, 31, 571–590. [CrossRef]\n41.\n\
    McCabe, M.F.; Rodell, M.; Alsdorf, D.E.; Miralles, D.G.; Uijlenhoet, R.; Wagner,\
    \ W.; Lucieer, A.; Houborg, R.;\nVerhoest, N.E.; Franz, T.E.; et al. The future\
    \ of Earth observation in hydrology. Hydrol. Earth Syst. Sci. 2017,\n21, 3879.\
    \ [CrossRef]\n42.\nMatese, A.; Toscano, P.; Di Gennaro, S.; Genesio, L.; Vaccari,\
    \ F.; Primicerio, J.; Belli, C.; Zaldei, A.; Bianconi, R.;\nGioli, B. Intercomparison\
    \ of UAV, aircraft and satellite remote sensing platforms for precision viticulture.\n\
    Remote Sens. 2015, 7, 2971–2990. [CrossRef]\n43.\nMancini, A.; Frontoni, E.; Zingaretti,\
    \ P. Satellite and UAV data for Precision Agriculture Applications.\nIn Proceedings\
    \ of the 2019 International Conference on Unmanned Aircraft Systems (ICUAS 2019),\
    \ Atlanta,\nGA, USA, 11–14 June 2019; pp. 491–497.\n44.\nDiago, M.P.; Bellincontro,\
    \ A.; Scheidweiler, M.; Tardáguila, J.; Tittmann, S.; Stoll, M. Future opportunities\n\
    of proximal near infrared spectroscopy approaches to determine the variability\
    \ of vineyard water status.\nAust. J. Grape Wine Res. 2017, 23, 409–414. [CrossRef]\n\
    45.\nGutierrez, S.; Diago, M.P.; Fernández-Novales, J.; Tardaguila, J. Vineyard\
    \ water status assessment using\non-the-go thermal imaging and machine learning.\
    \ PLoS ONE 2018, 13, e0192037. [CrossRef] [PubMed]\n46.\nFernández-Novales, J.;\
    \ Tardaguila, J.; Gutiérrez, S.; Marañón, M.; Diago, M.P. In ﬁeld quantiﬁcation\
    \ and\ndiscrimination of diﬀerent vineyard water regimes by on-the-go NIR spectroscopy.\
    \ Biosyst. Eng. 2018, 165,\n47–58. [CrossRef]\nAgronomy 2020, 10, 140\n23 of 35\n\
    47.\nDiago, M.P.; Fernández-Novales, J.; Gutiérrez, S.; Marañón, M.; Tardaguila,\
    \ J. Development and validation of a\nnew methodology to assess the vineyard water\
    \ status by on-the-go near infrared spectroscopy. Front. Plant Sci.\n2018, 9,\
    \ 59. [CrossRef]\n48.\nAquino, A.; Millan, B.; Diago, M.-P.; Tardaguila, J. Automated\
    \ early yield prediction in vineyards from\non-the-go image acquisition. Comput.\
    \ Electron. Agric. 2018, 144, 26–36. [CrossRef]\n49.\nMarkham, B.L.; Helder, D.L.\
    \ Forty-year calibrated record of earth-reﬂected radiance from Landsat: A review.\n\
    Remote Sens. Environ. 2012, 122, 30–40. [CrossRef]\n50.\nToth, C.; Jó´zków, G.\
    \ Remote sensing platforms and sensors: A survey. ISPRS J. Photogramm. Remote\
    \ Sens.\n2016, 115, 22–36. [CrossRef]\n51.\nTyc, G.; Tulip, J.; Schulten, D.;\
    \ Krischke, M.; Oxfort, M. The RapidEye mission design. Acta Astronaut. 2005,\n\
    56, 213–219. [CrossRef]\n52.\nSweeting, M.N. Modern small satellites-changing\
    \ the economics of space. Proc. IEEE 2018, 106, 343–361.\n[CrossRef]\n53.\nKhanal,\
    \ S.; Fulton, J.; Shearer, S. An overview of current and potential applications\
    \ of thermal remote sensing\nin precision agriculture. Comput. Electron. Agric.\
    \ 2017, 139, 22–32. [CrossRef]\n54.\nGerhards, M.;\nSchlerf, M.;\nMallick, K.;\n\
    Udelhoven, T. Challenges and Future Perspectives of\nMulti-/Hyperspectral Thermal\
    \ Infrared Remote Sensing for Crop Water-Stress Detection: A Review.\nRemote Sens.\
    \ 2019, 11, 1240. [CrossRef]\n55.\nRyan, S.; Lewis, M. Mapping soils using high\
    \ resolution airborne imagery, Barossa Valley, SA. In Proceedings\nof the Inaugural\
    \ Australian Geospatial Information and Agriculture Conference Incorporating Precision\n\
    Agriculture in Australasia 5th Annual Symposium, Orange, NSW, Australia, 17–19\
    \ July 2001.\n56.\nKhaliq, A.; Comba, L.; Biglia, A.; Ricauda Aimonino, D.; Chiaberge,\
    \ M.; Gay, P. Comparison of satellite and\nUAV-based multispectral imagery for\
    \ vineyard variability assessment. Remote Sens. 2019, 11, 436. [CrossRef]\n57.\n\
    Jones, H.G.; Vaughan, R.A. Remote Sensing of Vegetation: Principles, Techniques,\
    \ and Applications; Oxford\nUniversity Press: Oxford, UK, 2010.\n58.\nThenkabail,\
    \ P.S.; Lyon, J.G. Hyperspectral Remote Sensing of Vegetation; CRC Press: Boco\
    \ Raton, FL, USA, 2016.\n59.\nKing, M.D.; Platnick, S.; Menzel, W.P.; Ackerman,\
    \ S.A.; Hubanks, P.A. Spatial and temporal distribution of\nclouds observed by\
    \ MODIS onboard the Terra and Aqua satellites. IEEE Trans. Geosci. Remote Sens.\
    \ 2013, 51,\n3826–3852. [CrossRef]\n60.\nChen, X.; Liu, M.; Zhu, X.; Chen, J.;\
    \ Zhong, Y.; Cao, X. “Blend-then-Index” or “Index-then-Blend”:\nA Theoretical\
    \ Analysis for Generating High-resolution NDVI Time Series by STARFM. Photogramm.\
    \ Eng.\nRemote Sens. 2018, 84, 65–73. [CrossRef]\n61.\nYin, T.; Inglada, J.; Osman,\
    \ J. Time series image fusion: Application and improvement of STARFM for land\n\
    cover map and production. In Proceedings of the 2012 IEEE International Geoscience\
    \ and Remote Sensing\nSymposium, Munich, Germany, 22–27 July 2012; pp. 378–381.\n\
    62.\nGevaert, C.M.; García-Haro, F.J. A comparison of STARFM and an unmixing-based\
    \ algorithm for Landsat\nand MODIS data fusion. Remote Sens. Environ. 2015, 156,\
    \ 34–44. [CrossRef]\n63.\nLi, L.; Wang, X.; Li, M. Study on the fusion of MODIS\
    \ and TM images using the spectral response function\nand STARFM algorithm. In\
    \ Proceedings of the 2011 International Conference on Image Analysis and Signal\n\
    Processing, Wuhan, China, 21–23 October 2011; pp. 171–176.\n64.\nPagay, V.; Kidman,\
    \ C.M. Evaluating Remotely-Sensed Grapevine (Vitis vinifera L.) Water Stress Responses\n\
    Across a Viticultural Region. Agronomy 2019, 9, 682. [CrossRef]\n65.\nRascher,\
    \ U.; Alonso, L.; Burkart, A.; Cilia, C.; Cogliati, S.; Colombo, R.; Damm, A.;\
    \ Drusch, M.; Guanter, L.;\nHanus, J.; et al. Sun-induced ﬂuorescence—A new probe\
    \ of photosynthesis: First maps from the imaging\nspectrometer HyPlant. Glob.\
    \ Chang. Biol. 2015, 21, 4673–4684. [CrossRef]\n66.\nBuckley, S.; Vallet, J.;\
    \ Braathen, A.; Wheeler, W. Oblique helicopter-based laser scanning for digital\
    \ terrain\nmodelling and visualisation of geological outcrops. Int. Arch. Photogramm.\
    \ Remote Sens. Spat. Inf. Sci. 2008,\n37, 1–6.\n67.\nPullanagari, R.; Kereszturi,\
    \ G.; Yule, I. Mapping of macro and micro nutrients of mixed pastures using\n\
    airborne AisaFENIX hyperspectral imagery. ISPRS J. Photogramm. Remote Sens. 2016,\
    \ 117, 1–10. [CrossRef]\n68.\nHaboudane, D.; Miller, J.R.; Tremblay, N.; Zarco-Tejada,\
    \ P.J.; Dextraze, L. Integrated narrow-band vegetation\nindices for prediction\
    \ of crop chlorophyll content for application to precision agriculture. Remote\
    \ Sens.\nEnviron. 2002, 81, 416–426. [CrossRef]\nAgronomy 2020, 10, 140\n24 of\
    \ 35\n69.\nMiao, Y.; Mulla, D.J.; Randall, G.W.; Vetsch, J.A.; Vintila, R. Predicting\
    \ chlorophyll meter readings with aerial\nhyperspectral remote sensing for in-season\
    \ site-speciﬁc nitrogen management of corn. Precis. Agric. 2007, 7,\n635–641.\n\
    70.\nHaboudane, D.; Miller, J.R.; Pattey, E.; Zarco-Tejada, P.J.; Strachan, I.B.\
    \ Hyperspectral vegetation indices\nand novel algorithms for predicting green\
    \ LAI of crop canopies: Modeling and validation in the context of\nprecision agriculture.\
    \ Remote Sens. Environ. 2004, 90, 337–352. [CrossRef]\n71.\nSepulcre-Cantó, G.;\
    \ Zarco-Tejada, P.J.; Jiménez-Muñoz, J.; Sobrino, J.; Soriano, M.; Fereres, E.;\
    \ Vega, V.;\nPastor, M. Monitoring yield and fruit quality parameters in open-canopy\
    \ tree crops under water stress.\nImplications for ASTER. Remote Sens. Environ.\
    \ 2007, 107, 455–470. [CrossRef]\n72.\nSepulcre-Cantó, G.; Zarco-Tejada, P.J.;\
    \ Jiménez-Muñoz, J.; Sobrino, J.; De Miguel, E.; Villalobos, F.J. Detection\n\
    of water stress in an olive orchard with thermal remote sensing imagery. Agric.\
    \ For. Meteorol. 2006, 136,\n31–44. [CrossRef]\n73.\nColomina, I.; Molina, P.\
    \ Unmanned aerial systems for photogrammetry and remote sensing: A review.\nISPRS\
    \ J. Photogramm. Remote Sens. 2014, 92, 79–97. [CrossRef]\n74.\nZecha, C.; Link,\
    \ J.; Claupein, W. Mobile sensor platforms: Categorisation and research applications\
    \ in\nprecision farming. J. Sens. Sens. Syst. 2013, 2, 51–72. [CrossRef]\n75.\n\
    Urbahs, A.; Jonaite, I. Features of the use of unmanned aerial vehicles for agriculture\
    \ applications. Aviation\n2013, 17, 170–175. [CrossRef]\n76.\nGautam, D.; Ha,\
    \ C. Control of a quadrotor using a smart self-tuning fuzzy PID controller. Int.\
    \ J. Adv. Robot.\nSyst. 2013, 10, 380. [CrossRef]\n77.\nShi, Y.; Thomasson, J.A.;\
    \ Murray, S.C.; Pugh, N.A.; Rooney, W.L.; Shaﬁan, S.; Rajan, N.; Rouze, G.; Morgan,\
    \ C.L.;\nNeely, H.L.; et al. Unmanned aerial vehicles for high-throughput phenotyping\
    \ and agronomic research.\nPLoS ONE 2016, 11, e0159781. [CrossRef]\n78.\nZhang,\
    \ C.; Kovacs, J.M. The application of small unmanned aerial systems for precision\
    \ agriculture: a review.\nPrecis. Agric. 2012, 13, 693–712. [CrossRef]\n79.\n\
    Mulla, D.J. Twenty ﬁve years of remote sensing in precision agriculture: Key advances\
    \ and remaining\nknowledge gaps. Biosyst. Eng. 2013, 114, 358–371. [CrossRef]\n\
    80.\nHuang, Y.; Thomson, S.J.; Hoﬀmann, W.C.; Lan, Y.; Fritz, B.K. Development\
    \ and prospect of unmanned aerial\nvehicle technologies for agricultural production\
    \ management. Int. J. Agric. Biol. Eng. 2013, 6, 1–10.\n81.\nZude-Sasse, M.; Fountas,\
    \ S.; Gemtos, T.A.; Abu-Khalaf, N. Applications of precision agriculture in horticultural\n\
    crops. Eur. J. Hortic. Sci. 2016, 81, 78–90. [CrossRef]\n82.\nBaluja, J.; Diago,\
    \ M.P.; Balda, P.; Zorer, R.; Meggio, F.; Morales, F.; Tardaguila, J. Assessment\
    \ of vineyard\nwater status variability by thermal and multispectral imagery using\
    \ an unmanned aerial vehicle (UAV).\nIrrig. Sci. 2012, 30, 511–522. [CrossRef]\n\
    83.\nMatese, A.; Baraldi, R.; Berton, A.; Cesaraccio, C.; Di Gennaro, S.F.; Duce,\
    \ P.; Facini, O.; Mameli, M.G.;\nPiga, A.; Zaldei, A. Estimation of water stress\
    \ in grapevines using proximal and remote sensing methods.\nRemote Sens. 2018,\
    \ 10, 114. [CrossRef]\n84.\nPoblete, T.; Ortega-Farías, S.; Moreno, M.; Bardeen,\
    \ M. Artiﬁcial neural network to predict vine water status\nspatial variability\
    \ using multispectral information obtained from an unmanned aerial vehicle (UAV).\
    \ Sensors\n2017, 17, 2488. [CrossRef]\n85.\nGonzalez-Dugo, V.; Zarco-Tejada, P.J.;\
    \ Fereres, E. Applicability and limitations of using the crop water stress\nindex\
    \ as an indicator of water deﬁcits in citrus orchards. Agric. For. Meteorol. 2014,\
    \ 198, 94–104. [CrossRef]\n86.\nStagakis, S.; González-Dugo, V.; Cid, P.; Guillén-Climent,\
    \ M.L.; Zarco-Tejada, P.J. Monitoring water stress\nand fruit quality in an orange\
    \ orchard under regulated deﬁcit irrigation using narrow-band structural and\n\
    physiological remote sensing indices. ISPRS J. Photogramm. Remote Sens. 2012,\
    \ 71, 47–61. [CrossRef]\n87.\nAgam, N.; Cohen, Y.; Berni, J.A.J.; Alchanatis,\
    \ V.; Kool, D.; Dag, A.; Yermiyahu, U.; Ben-Gal, A. An insight to\nthe performance\
    \ of crop water stress index for olive trees. Agric. Water Manag. 2013, 118, 79–86.\
    \ [CrossRef]\n88.\nPoblete-Echeverría, C.; Sepulveda-Reyes, D.; Ortega-Farias,\
    \ S.; Zuñiga, M.; Fuentes, S. Plant water stress\ndetection based on aerial and\
    \ terrestrial infrared thermography: A study case from vineyard and olive\norchard.\
    \ In Proceedings of the XXIX International Horticultural congress on Horticulture:\
    \ Sustaining Lives,\nLivelihoods and Landscapes (IHC2014): International Symposia\
    \ on Water, Eco-Eﬃciency and Transformation\nof Organic Waste in Horticultural\
    \ Production, Brisbane, Australia, 25 October 2016; pp. 141–146.\nAgronomy 2020,\
    \ 10, 140\n25 of 35\n89.\nTesti, L.; Goldhamer, D.; Iniesta, F.; Salinas, M. Crop\
    \ water stress index is a sensitive water stress indicator in\npistachio trees.\
    \ Irrig. Sci. 2008, 26, 395–405. [CrossRef]\n90.\nGonzalez-Dugo, V.; Goldhamer,\
    \ D.; Zarco-Tejada, P.J.; Fereres, E. Improving the precision of irrigation in\
    \ a\npistachio farm using an unmanned airborne thermal system. Irrig. Sci. 2015,\
    \ 33, 43–52. [CrossRef]\n91.\nGarcía-Tejero, I.F.; Rubio, A.E.; Viñuela, I.; Hernández,\
    \ A.; Gutiérrez-Gordillo, S.; Rodríguez-Pleguezuelo, C.R.;\nDurán-Zuazo, V.H.\
    \ Thermal imaging at plant level to assess the crop-water status in almond trees\
    \ (cv. Guara)\nunder deﬁcit irrigation strategies. Agric. Water Manag. 2018, 208,\
    \ 176–186. [CrossRef]\n92.\nGonzalez-Dugo, V.; Zarco-Tejada, P.; Berni, J.A.;\
    \ Suárez, L.; Goldhamer, D.; Fereres, E. Almond tree canopy\ntemperature reveals\
    \ intra-crown variability that is water stress-dependent. Agric. For. Meteorol.\
    \ 2012, 154,\n156–165. [CrossRef]\n93.\nZhao, T.; Stark, B.; Chen, Y.; Ray, A.L.;\
    \ Doll, D. Challenges in water stress quantiﬁcation using small\nunmanned aerial\
    \ system (sUAS): Lessons from a growing season of almond. J. Intell. Robot. Syst.\
    \ 2017, 88,\n721–735. [CrossRef]\n94.\nZhao, T.; Doll, D.; Wang, D.; Chen, Y.\
    \ A new framework for UAV-based remote sensing data processing and\nits application\
    \ in almond water stress quantiﬁcation. In Proceedings of the 2017 International\
    \ Conference on\nUnmanned Aircraft Systems (ICUAS 2017), Miami, FL, USA, 13–16\
    \ June 2017; pp. 1794–1799.\n95.\nHerwitz, S.; Johnson, L.; Dunagan, S.; Higgins,\
    \ R.; Sullivan, D.; Zheng, J.; Lobitz, B.; Leung, J.; Gallmeyer, B.;\nAoyagi,\
    \ M.; et al. Imaging from an unmanned aerial vehicle: agricultural surveillance\
    \ and decision support.\nComput. Electron. Agric. 2004, 44, 49–61. [CrossRef]\n\
    96.\nFurfaro, R.; Ganapol, B.D.; Johnson, L.; Herwitz, S. Model-based neural network\
    \ algorithm for coﬀee ripeness\nprediction using Helios UAV aerial images. In\
    \ Remote Sensing for Agriculture, Ecosystems, and Hydrology VII;\nInternational\
    \ Society for Optics and Photonics: Bruges, Belgium, 2005; Volume 5976, p. 59760X.\n\
    97.\nPark, S.; Nolan, A.; Ryu, D.; Fuentes, S.; Hernandez, E.; Chung, H.; O’connell,\
    \ M. Estimation of crop\nwater stress in a nectarine orchard using high-resolution\
    \ imagery from unmanned aerial vehicle (UAV).\nIn Proceedings of the 21st International\
    \ Congress on Modelling and Simulation, Gold Coast, QLD, Australia,\n29 November–4\
    \ December 2015; pp. 1413–1419.\n98.\nBulanon, D.M.; Lonai, J.; Skovgard, H.;\
    \ Fallahi, E. Evaluation of diﬀerent irrigation methods for an apple\norchard\
    \ using an aerial imaging system. ISPRS Int. J. Geo-Inf. 2016, 5, 79.\n99.\nGonzalez-Dugo,\
    \ V.; Zarco-Tejada, P.; Nicolás, E.; Nortes, P.A.; Alarcón, J.; Intrigliolo, D.S.;\
    \ Fereres, E. Using\nhigh resolution UAV thermal imagery to assess the variability\
    \ in the water status of ﬁve fruit tree species\nwithin a commercial orchard.\
    \ Precis. Agric. 2013, 14, 660–678. [CrossRef]\n100. Santesteban, L.G.; Di Gennaro,\
    \ S.F.; Herrero-Langreo, A.; Miranda, C.; Royo, J.B.; Matese, A. High-resolution\n\
    UAV-based thermal imaging to estimate the instantaneous and seasonal variability\
    \ of plant water status\nwithin a vineyard. Agric. Water Manag. 2017, 183, 49–59.\
    \ [CrossRef]\n101. Aboutalebi, M.; Torres-Rua, A.F.; Kustas, W.P.; Nieto, H.;\
    \ Coopmans, C.; McKee, M. Assessment of diﬀerent\nmethods for shadow detection\
    \ in high-resolution optical imagery and evaluation of shadow impact on\ncalculation\
    \ of NDVI, and evapotranspiration. Irrig. Sci. 2018, 1, 1–23. [CrossRef]\n102.\
    \ Berni, J.A.; Zarco-Tejada, P.J.; Suárez, L.; Fereres, E. Thermal and narrowband\
    \ multispectral remote sensing\nfor vegetation monitoring from an unmanned aerial\
    \ vehicle. IEEE Trans. Geosci. Remote Sens. 2009, 47,\n722–738. [CrossRef]\n103.\
    \ Candiago, S.; Remondino, F.; De Giglio, M.; Dubbini, M.; Gattelli, M. Evaluating\
    \ multispectral images and\nvegetation indices for precision farming applications\
    \ from UAV images. Remote Sens. 2015, 7, 4026–4047.\n[CrossRef]\n104. Thomasson,\
    \ J.A.; Shi, Y.; Olsenholler, J.; Valasek, J.; Murray, S.C.; Bishop, M.P. Comprehensive\
    \ UAV\nagricultural remote-sensing research at Texas AM University. In Autonomous\
    \ Air and Ground Sensing Systems\nfor Agricultural Optimization and Phenotyping;\
    \ International Society for Optics and Photonics: Baltimore, MD,\nUSA, 2016; Volume\
    \ 9866, p. 986602.\n105. Turner, D.; Lucieer, A.; Wallace, L. Direct georeferencing\
    \ of ultrahigh-resolution UAV imagery. IEEE Trans.\nGeosci. Remote Sens. 2014,\
    \ 52, 2738–2745. [CrossRef]\n106. Primicerio, J.; Di Gennaro, S.F.; Fiorillo,\
    \ E.; Genesio, L.; Lugato, E.; Matese, A.; Vaccari, F.P. A ﬂexible\nunmanned aerial\
    \ vehicle for precision agriculture. Precis. Agric. 2012, 13, 517–523. [CrossRef]\n\
    107. Pajares, G. Overview and current status of remote sensing applications based\
    \ on unmanned aerial vehicles\n(UAVs). Photogramm. Eng. Remote Sens. 2015, 81,\
    \ 281–330. [CrossRef]\nAgronomy 2020, 10, 140\n26 of 35\n108. Di Gennaro, S.F.;\
    \ Matese, A.; Gioli, B.; Toscano, P.; Zaldei, A.; Palliotti, A.; Genesio, L. Multisensor\
    \ approach\nto assess vineyard thermal dynamics combining high-resolution unmanned\
    \ aerial vehicle (UAV) remote\nsensing and wireless sensor network (WSN) proximal\
    \ sensing. Sci. Hortic. 2017, 221, 83–87. [CrossRef]\n109. Cendrero-Mateo, M.P.;\
    \ Wieneke, S.; Damm, A.; Alonso, L.; Pinto, F.; Moreno, J.; Guanter, L.; Celesti,\
    \ M.;\nRossini, M.; Sabater, N.; et al. Sun-induced chlorophyll ﬂuorescence III:\
    \ Benchmarking retrieval methods\nand sensor characteristics for proximal sensing.\
    \ Remote Sens. 2019, 11, 962. [CrossRef]\n110. Zarco-Tejada, P.J.; González-Dugo,\
    \ V.; Berni, J.A.J. Fluorescence, temperature and narrow-band indices\nacquired\
    \ from a UAV platform for water stress detection using a micro-hyperspectral imager\
    \ and a thermal\ncamera. Remote Sens. Environ. 2012, 117, 322–337. [CrossRef]\n\
    111. Harwin, S.; Lucieer, A. Assessing the accuracy of georeferenced point clouds\
    \ produced via multi-view\nstereopsis from Unmanned Aerial Vehicle (UAV) imagery.\
    \ Remote Sens. 2012, 4, 1573–1599. [CrossRef]\n112. Wallace, L.; Lucieer, A.;\
    \ Malenovskỳ, Z.; Turner, D.; Vopˇenka, P. Assessment of forest structure using\
    \ two\nUAV techniques: A comparison of airborne laser scanning and structure from\
    \ motion (SfM) point clouds.\nForests 2016, 7, 62. [CrossRef]\n113. Weiss, M.;\
    \ Baret, F. Using 3D point clouds derived from UAV RGB imagery to describe vineyard\
    \ 3D\nmacro-structure. Remote Sens. 2017, 9, 111. [CrossRef]\n114. Mathews, A.;\
    \ Jensen, J. Visualizing and quantifying vineyard canopy LAI using an unmanned\
    \ aerial vehicle\n(UAV) collected high density structure from motion point cloud.\
    \ Remote Sens. 2013, 5, 2164–2183. [CrossRef]\n115. Stone, C.; Webster, M.; Osborn,\
    \ J.; Iqbal, I. Alternatives to LiDAR-derived canopy height models for softwood\n\
    plantations: a review and example using photogrammetry. Aust. For. 2016, 79, 271–282.\
    \ [CrossRef]\n116. Wu, D.; Phinn, S.; Johansen, K.; Robson, A.; Muir, J.; Searle,\
    \ C. Estimating changes in leaf area, leaf area\ndensity, and vertical leaf area\
    \ proﬁle for mango, avocado, and macadamia tree crowns using terrestrial laser\n\
    scanning. Remote Sens. 2018, 10, 1750. [CrossRef]\n117. Rosell, J.R.; Llorens,\
    \ J.; Sanz, R.; Arno, J.; Ribes-Dasi, M.; Masip, J.; Escolà, A.; Camp, F.; Solanelles,\
    \ F.;\nGràcia, F.; et al. Obtaining the three-dimensional structure of tree orchards\
    \ from remote 2D terrestrial LIDAR\nscanning. Agric. For. Meteorol. 2009, 149,\
    \ 1505–1515. [CrossRef]\n118. Matese, A.; Di Gennaro, S.F. Technology in precision\
    \ viticulture: A state of the art review. Int. J. Wine Res.\n2015, 7, 69–81. [CrossRef]\n\
    119. Johansen, K.; Raharjo, T.; McCabe, M.F. Using multi-spectral UAV imagery\
    \ to extract tree crop structural\nproperties and assess pruning eﬀects. Remote\
    \ Sens. 2018, 10, 854. [CrossRef]\n120. Tu, Y.-H.; Johansen, K.; Phinn, S.; Robson,\
    \ A. Measuring canopy structure and condition using multi-spectral\nUAS imagery\
    \ in a horticultural environment. Remote Sens. 2019, 11, 269. [CrossRef]\n121.\
    \ Mu, Y.; Fujii, Y.; Takata, D.; Zheng, B.; Noshita, K.; Honda, K.; Ninomiya,\
    \ S.; Guo, W. Characterization of\npeach tree crown by using high-resolution images\
    \ from an unmanned aerial vehicle. Hortic. Res. 2018, 5, 74.\n[CrossRef]\n122.\
    \ De Castro, A.I.; Jiménez-Brenes, F.M.; Torres-Sánchez, J.; Peña, J.M.; Borra-Serrano,\
    \ I.; López-Granados, F. 3-D\ncharacterization of vineyards using a novel UAV\
    \ imagery-based OBIA procedure for precision viticulture\napplications. Remote\
    \ Sens. 2018, 10, 584. [CrossRef]\n123. Del Pozo, S.; Rodríguez-Gonzálvez, P.;\
    \ Hernández-López, D.; Felipe-García, B. Vicarious radiometric\ncalibration of\
    \ a multispectral camera on board an unmanned aerial system. Remote Sens. 2014,\
    \ 6, 1918–1937.\n[CrossRef]\n124. Tu, Y.-H.; Phinn, S.; Johansen, K.; Robson,\
    \ A. Assessing radiometric correction approaches for multi-spectral\nUAS imagery\
    \ for horticultural applications. Remote Sens. 2018, 10, 1684. [CrossRef]\n125.\
    \ Stow, D.; Nichol, C.J.; Wade, T.; Assmann, J.J.; Simpson, G.; Helfter, C. Illumination\
    \ geometry and ﬂying\nheight inﬂuence surface reﬂectance and NDVI derived from\
    \ multispectral UAS imagery. Drones 2019, 3, 55.\n[CrossRef]\n126. Turner, D.;\
    \ Lucieer, A.; Watson, C. Development of an unmanned aerial vehicle (UAV) for\
    \ hyper resolution\nvineyard mapping based on visible, multispectral, and thermal\
    \ imagery.\nIn Proceedings of the 34th\nInternational Symposium on Remote Sensing\
    \ of Environment, Sydney, Australia, 10–15 April 2011; p. 4.\n127. Jorge, J.;\
    \ Vallbé, M.; Soler, J.A. Detection of irrigation inhomogeneities in an olive\
    \ grove using the NDRE\nvegetation index obtained from UAV images. Eur. J. Remote\
    \ Sens. 2019, 52, 169–177. [CrossRef]\n128. Filella, I.; Penuelas, J. The red\
    \ edge position and shape as indicators of plant chlorophyll content, biomass\n\
    and hydric status. Int. J. Remote Sens. 1994, 15, 1459–1470. [CrossRef]\nAgronomy\
    \ 2020, 10, 140\n27 of 35\n129. Zúñiga, C.E.; Khot, L.R.; Jacoby, P.; Sankaran,\
    \ S. Remote sensing based water-use eﬃciency evaluation\nin sub-surface irrigated\
    \ wine grape vines. In Autonomous Air and Ground Sensing Systems for Agricultural\n\
    Optimization and Phenotyping; International Society for Optics and Photonics:\
    \ Baltimore, MD, USA, 2016;\nVolume 9866, p. 98660O.\n130. Aasen, H.; Honkavaara,\
    \ E.; Lucieer, A.; Zarco-Tejada, P. Quantitative remote sensing at ultra-high\
    \ resolution\nwith uav spectroscopy: A review of sensor technology, measurement\
    \ procedures, and data correction\nworkﬂows. Remote Sens. 2018, 10, 1091. [CrossRef]\n\
    131. Adão, T.; Hruška, J.; Pádua, L.; Bessa, J.; Peres, E.; Morais, R.; Sousa,\
    \ J. Hyperspectral imaging: A review on\nUAV-based sensors, data processing and\
    \ applications for agriculture and forestry. Remote Sens. 2017, 9, 1110.\n[CrossRef]\n\
    132. Gautam, D.; Watson, C.; Lucieer, A.; Malenovský, Z. Error budget for geolocation\
    \ of spectroradiometer point\nobservations from an unmanned aircraft system. Sens.\
    \ Switz. 2018, 18, 3465. [CrossRef] [PubMed]\n133. Uto, K.; Seki, H.; Saito, G.;\
    \ Kosugi, Y.; Komatsu, T. Development of a low-cost hyperspectral whiskbroom\n\
    imager using an optical ﬁber bundle, a swing mirror, and compact spectrometers.\
    \ IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens. 2016, 9, 3909–3925. [CrossRef]\n\
    134. Suomalainen, J.; Anders, N.; Iqbal, S.; Roerink, G.; Franke, J.; Wenting,\
    \ P.; Hünniger, D.; Bartholomeus, H.;\nBecker, R.; Kooistra, L. A lightweight\
    \ hyperspectral mapping system and photogrammetric processing chain\nfor unmanned\
    \ aerial vehicles. Remote Sens. 2014, 6, 11013–11030. [CrossRef]\n135. Iseli,\
    \ C.; Lucieer, A. Tree species classiﬁcation based on 3d spectral point clouds\
    \ and orthomosaics acquired\nby snapshot hyperspectral UAS sensor. ISPRS-Int.\
    \ Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2019, 4213,\n379–384. [CrossRef]\n\
    136. Hagen, N.A.; Kudenov, M.W. Review of snapshot spectral imaging technologies.\
    \ Opt. Eng. 2013, 52, 090901.\n[CrossRef]\n137. Bendig, J.; Gautam, D.; Malenovsky,\
    \ Z.; Lucieer, A. Inﬂuence of Cosine Corrector and UAS Platform Dynamics\non Airborne\
    \ Spectral Irradiance Measurements. In Proceedings of the 2018 IEEE International\
    \ Geoscience\nand Remote Sensing Symposium (IGARSS 2018), Valencia, Spain, 22–27\
    \ July 2018; pp. 8822–8825.\n138. Gautam, D. Direct Georeferencing and Footprint\
    \ Characterisation of a Non-Imaging Spectroradiometer\nMounted on an Unmanned\
    \ Aircraft System. Ph.D. Thesis, University of Tasmania, Hobart, Tasmania,\nAustralia,\
    \ 2019.\n139. Rodríguez-Pérez, J.R.; Riaño, D.; Carlisle, E.; Ustin, S.; Smart,\
    \ D.R. Evaluation of hyperspectral reﬂectance\nindexes to detect grapevine water\
    \ status in vineyards. Am. J. Enol. Vitic. 2007, 58, 302–317.\n140. Hurley, S.P.;\
    \ Horney, M.; Drake, A. Using hyperspectral imagery to detect water stress in\
    \ vineyards.\nIn Autonomous Air and Ground Sensing Systems for Agricultural Optimization\
    \ and Phenotyping IV; International\nSociety for Optics and Photonics: Baltimore,\
    \ MD, USA, 2019; Volume 11008, p. 1100807.\n141. Loggenberg, K.; Strever, A.;\
    \ Greyling, B.; Poona, N. Modelling water stress in a shiraz vineyard using\n\
    hyperspectral imaging and machine learning. Remote Sens. 2018, 10, 202. [CrossRef]\n\
    142. Gómez-Candón, D.; Virlet, N.; Labbé, S.; Jolivot, A.; Regnard, J.-L. Field\
    \ phenotyping of water stress at tree\nscale by UAV-sensed imagery: new insights\
    \ for thermal acquisition and calibration. Precis. Agric. 2016, 17,\n786–800.\
    \ [CrossRef]\n143. Kelly, J.; Kljun, N.; Olsson, P.-O.; Mihai, L.; Liljeblad,\
    \ B.; Weslien, P.; Klemedtsson, L.; Eklundh, L. Challenges\nand best practices\
    \ for deriving temperature data from an uncalibrated UAV thermal infrared camera.\n\
    Remote Sens. 2019, 11, 567. [CrossRef]\n144. Smigaj, M.; Gaulton, R.; Suarez,\
    \ J.; Barr, S. Use of miniature thermal cameras for detection of physiological\n\
    stress in conifers. Remote Sens. 2017, 9, 957. [CrossRef]\n145. Clarke, I. Thermal\
    \ Infrared Remote Sensing from Unmanned Aircraft Systems (UAS) for Precision Viticulture.\n\
    Master’s Thesis, University of Tasmania, Hobart, Tasmania, Australia, 2014.\n\
    146. Daakir, M.; Zhou, Y.; Pierrot Deseilligny, M.; Thom, C.; Martin, O.; Rupnik,\
    \ E. Improvement of\nphotogrammetric accuracy by modeling and correcting the thermal\
    \ eﬀect on camera calibration. ISPRS J.\nPhotogramm. Remote Sens. 2019, 148, 142–155.\
    \ [CrossRef]\n147. Nugent, P.W.; Shaw, J.A. Calibration of uncooled LWIR microbolometer\
    \ imagers to enable long-term ﬁeld\ndeployment. In Infrared Imaging Systems: Design,\
    \ Analysis, Modeling, and Testing XXV; International Society\nfor Optics and Photonics:\
    \ Baltimore, MD, USA, 2014; Volume 9071, p. 90710V.\nAgronomy 2020, 10, 140\n\
    28 of 35\n148. Budzier, H.; Gerlach, G. Calibration of uncooled thermal infrared\
    \ cameras. J. Sens. Sens. Syst. 2015, 4,\n187–197. [CrossRef]\n149. Lin, D.; Maas,\
    \ H.-G.; Westfeld, P.; Budzier, H.; Gerlach, G. An advanced radiometric calibration\
    \ approach for\nuncooled thermal cameras. Photogramm. Rec. 2018, 33, 30–48. [CrossRef]\n\
    150. Ribeiro-Gomes, K.; Hernández-López, D.; Ortega, J.F.; Ballesteros, R.; Poblete,\
    \ T.; Moreno, M.A. Uncooled\nthermal camera calibration and optimization of the\
    \ photogrammetry process for UAV applications in\nagriculture. Sensors 2017, 17,\
    \ 173. [CrossRef]\n151. Lin, D.; Westfeld, P.; Maas, H.G. Shutter-less temperature-dependent\
    \ correction for uncooled thermal camera\nunder fast changing FPA temperature.\
    \ Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.—ISPRS Arch. 2017,\n42, 619–625.\
    \ [CrossRef]\n152. Mesas-Carrascosa, F.J.; Pérez-Porras, F.; de Larriva, J.E.M.;\
    \ Frau, C.M.; Agüera-Vega, F.; Carvajal-Ramírez, F.;\nMartínez-Carricondo, P.;\
    \ García-Ferrer, A. Drift correction of lightweight microbolometer thermal sensors\n\
    on-board unmanned aerial vehicles. Remote Sens. 2018, 10, 615. [CrossRef]\n153.\
    \ Torres-Rua, A. Vicarious calibration of sUAS microbolometer temperature imagery\
    \ for estimation of\nradiometric land surface temperature. Sensors 2017, 17, 1499.\
    \ [CrossRef] [PubMed]\n154. Bendig, J.; Bolten, A.; Bareth, G. Introducing a low-cost\
    \ mini-UAV for thermal-and multispectral-imaging.\nInt. Arch. Photogramm Remote\
    \ Sens. Spat. Inf. Sci. 2012, 39, 345–349. [CrossRef]\n155. Raymer, D. Aircraft\
    \ Design: A Conceptual Approach; American Institute of Aeronautics and Astronautics,\
    \ Inc.:\nReston, VA, USA, 2018.\n156. Tardieu, F.; Simonneau, T. Variability among\
    \ species of stomatal control under ﬂuctuating soil water status\nand evaporative\
    \ demand: modelling isohydric and anisohydric behaviours. J. Exp. Bot. 1998, 49,\
    \ 419–432.\n[CrossRef]\n157. White, W.A.; Alsina, M.M.; Nieto, H.; McKee, L.G.;\
    \ Gao, F.; Kustas, W.P. Determining a robust indirect\nmeasurement of leaf area\
    \ index in California vineyards for validating remote sensing-based retrievals.\n\
    Irrig. Sci. 2019, 37, 269–280. [CrossRef]\n158. Zarco-Tejada, P.J.; Berni, J.A.;\
    \ Suárez, L.; Sepulcre-Cantó, G.; Morales, F.; Miller, J.R. Imaging chlorophyll\n\
    ﬂuorescence with an airborne narrow-band multispectral camera for vegetation stress\
    \ detection.\nRemote Sens. Environ. 2009, 113, 1262–1275. [CrossRef]\n159. Gago,\
    \ J.; Douthe, C.; Florez-Sarasa, I.; Escalona, J.M.; Galmes, J.; Fernie, A.R.;\
    \ Flexas, J.; Medrano, H.\nOpportunities for improving leaf water use eﬃciency\
    \ under climate change conditions. Plant Sci. 2014, 226,\n108–119. [CrossRef]\n\
    160. Suárez, L.; Zarco-Tejada, P.J.; Sepulcre-Cantó, G.; Pérez-Priego, O.; Miller,\
    \ J.; Jiménez-Muñoz, J.; Sobrino, J.\nAssessing canopy PRI for water stress detection\
    \ with diurnal airborne imagery. Remote Sens. Environ. 2008,\n112, 560–575. [CrossRef]\n\
    161. Eugenio, F.; Marqués, F. Automatic satellite image georeferencing using a\
    \ contour-matching approach.\nIEEE Trans. Geosci. Remote Sens. 2003, 41, 2869–2880.\
    \ [CrossRef]\n162. Hugenholtz, C.; Brown, O.; Walker, J.; Barchyn, T.; Nesbit,\
    \ P.; Kucharczyk, M.; Myshak, S. Spatial accuracy of\nUAV-derived orthoimagery\
    \ and topography: Comparing photogrammetric models processed with direct\ngeo-referencing\
    \ and ground control points. Geomatica 2016, 70, 21–30. [CrossRef]\n163. Matese,\
    \ A.; Di Gennaro, S.F.; Berton, A. Assessment of a canopy height model (CHM) in\
    \ a vineyard using\nUAV-based multispectral imaging. Int. J. Remote Sens. 2017,\
    \ 38, 2150–2160. [CrossRef]\n164. Yahyanejad, S.; Misiorny, J.; Rinner, B. Lens\
    \ distortion correction for thermal cameras to improve aerial\nimaging with small-scale\
    \ UAVs. In Proceedings of the 2011 IEEE International Symposium on Robotic and\n\
    Sensors Environments (ROSE 2011), Montreal, QC, Canada, 17–18 September 2011;\
    \ pp. 231–236.\n165. Maes, W.; Huete, A.; Steppe, K. Optimizing the processing\
    \ of UAV-based thermal imagery. Remote Sens. 2017,\n9, 476. [CrossRef]\n166. Smith,\
    \ M.; Carrivick, J.; Quincey, D. Structure from motion photogrammetry in physical\
    \ geography.\nProg. Phys. Geogr. 2016, 40, 247–275. [CrossRef]\n167. Westoby,\
    \ M.J.; Brasington, J.; Glasser, N.F.; Hambrey, M.J.; Reynolds, J.M. ‘Structure-from-Motion’\n\
    photogrammetry: A low-cost, eﬀective tool for geoscience applications. Geomorphology\
    \ 2012, 179, 300–314.\n[CrossRef]\n168. Gautam, D.; Lucieer, A.; Malenovský, Z.;\
    \ Watson, C. Comparison of MEMS-based and FOG-based IMUs to\ndetermine sensor\
    \ pose on an unmanned aircraft system. J. Surv. Eng. 2017, 143. [CrossRef]\nAgronomy\
    \ 2020, 10, 140\n29 of 35\n169. Turner, D.; Lucieer, A.; McCabe, M.; Parkes, S.;\
    \ Clarke, I. Pushbroom hyperspectral imaging from an\nunmanned aircraft system\
    \ (UAS)–geometric processingworkﬂow and accuracy assessment. ISPRS-Int. Arch.\n\
    Photogramm. Remote Sens. Spat. Inf. Sci. 2017, XLII-2/W6, 379–384. [CrossRef]\n\
    170. Fang, J.; Wang, X.; Zhu, T.; Liu, X.; Zhang, X.; Zhao, D. A Novel Mosaic\
    \ Method for UAV-Based Hyperspectral\nImages. In Proceedings of the 2019 IEEE\
    \ International Geoscience and Remote Sensing Symposium (IGARSS\n2019), Yokohama,\
    \ Japan, 28 July–2 August 2019; pp. 9220–9223.\n171. Tagle, X. Study of Radiometric\
    \ Variations in Unmanned Aerial Vehicle Remote Sensing Imagery for Vegetation\n\
    Mapping. Master’s Thesis, Lund University, Lund, Sweden, 2017.\n172. Kedzierski,\
    \ M.; Wierzbicki, D.; Sekrecka, A.; Fryskowska, A.; Walczykowski, P.; Siewert,\
    \ J. Inﬂuence of lower\natmosphere on the radiometric quality of unmanned aerial\
    \ vehicle imagery. Remote Sens. 2019, 11, 1214.\n[CrossRef]\n173. Kelcey, J.;\
    \ Lucieer, A. Sensor correction of a 6-band multispectral imaging sensor for UAV\
    \ remote sensing.\nRemote Sens. 2012, 4, 1462–1493. [CrossRef]\n174. Maes, W.H.;\
    \ Steppe, K. Perspectives for remote sensing with unmanned aerial vehicles in\
    \ precision agriculture.\nTrends Plant Sci. 2019, 24, 152–164. [CrossRef]\n175.\
    \ McCabe, M.F.; Houborg, R.; Lucieer, A. High-resolution sensing for precision\
    \ agriculture:\nfrom\nEarth-observing satellites to unmanned aerial vehicles.\n\
    In Remote Sensing for Agriculture, Ecosystems,\nand Hydrology XVIII; International\
    \ Society for Optics and Photonics: Edinbrugh, UK, 2016; Volume 9998,\np. 999811.\n\
    176. Dinguirard, M.; Slater, P.N. Calibration of space-multispectral imaging sensors:\
    \ A review. Remote Sens. Environ.\n1999, 68, 194–205. [CrossRef]\n177. Geladi,\
    \ P.; Burger, J.; Lestander, T. Hyperspectral imaging: calibration problems and\
    \ solutions. Chemom. Intell.\nLab. Syst. 2004, 72, 209–217. [CrossRef]\n178. Iqbal,\
    \ F.; Lucieer, A.; Barry, K. Simpliﬁed radiometric calibration for UAS-mounted\
    \ multispectral sensor.\nEur. J. Remote Sens. 2018, 51, 301–313. [CrossRef]\n\
    179. Mamaghani, B.; Salvaggio, C. Multispectral Sensor Calibration and Characterization\
    \ for sUAS Remote\nSensing. Sensors 2019, 19, 4453. [CrossRef] [PubMed]\n180.\
    \ Mamaghani, B.; Salvaggio, C. Comparative study of panel and panelless-based\
    \ reﬂectance conversion\ntechniques for agricultural remote sensing. arXiv 2019,\
    \ arXiv:191003734.\n181. Jensen, A.M.; McKee, M.; Chen, Y. Calibrating thermal\
    \ imagery from an unmanned aerial system-AggieAir.\nIn Proceedings of the 2013\
    \ IEEE International Geoscience and Remote Sensing Symposium (IGARSS 2013),\n\
    Melbourne, Australia, 21–26 July 2013; pp. 542–545.\n182. Zarco-Tejada, P.J.;\
    \ Victoria, G.-D.; Williams, L.; Suárez, L.; Berni, J.A.; Goldhamer, D.; Fereres,\
    \ E. A PRI-based\nwater stress index combining structural and chlorophyll eﬀects:\
    \ Assessment using diurnal narrow-band\nairborne imagery and the CWSI thermal\
    \ index. Remote Sens. Environ. 2013, 138, 38–50. [CrossRef]\n183. Blaschke, T.\
    \ Object based image analysis for remote sensing. ISPRS J. Photogramm. Remote\
    \ Sens. 2010, 65,\n2–16. [CrossRef]\n184. De Castro, A.; Torres-Sánchez, J.; Peña,\
    \ J.; Jiménez-Brenes, F.; Csillik, O.; López-Granados, F. An automatic\nrandom\
    \ forest-OBIA algorithm for early weed mapping between and within crop rows using\
    \ UAV imagery.\nRemote Sens. 2018, 10, 285. [CrossRef]\n185. Peña-Barragán, J.M.;\
    \ Ngugi, M.K.; Plant, R.E.; Six, J. Object-based crop identiﬁcation using multiple\
    \ vegetation\nindices, textural features and crop phenology. Remote Sens. Environ.\
    \ 2011, 115, 1301–1316. [CrossRef]\n186. Johansen, K.; Raharjo, T. Multi-temporal\
    \ assessment of lychee tree crop structure using multi-spectral RPAS\nimagery.\
    \ ISPRS Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2017, XLII-2/W6, 165–170.\
    \ [CrossRef]\n187. Ma, L.; Li, M.; Ma, X.; Cheng, L.; Du, P.; Liu, Y. A review\
    \ of supervised object-based land-cover image\nclassiﬁcation. ISPRS J. Photogramm.\
    \ Remote Sens. 2017, 130, 277–293. [CrossRef]\n188. Pádua, L.; Vanko, J.; Hruška,\
    \ J.; Adão, T.; Sousa, J.J.; Peres, E.; Morais, R. UAS, sensors, and data processing\n\
    in agroforestry: a review towards practical applications. Int. J. Remote Sens.\
    \ 2017, 38, 2349–2391. [CrossRef]\n189. Torres-Sánchez, J.; López-Granados, F.;\
    \ Peña, J.M. An automatic object-based method for optimal thresholding\nin UAV\
    \ images: Application for vegetation detection in herbaceous crops. Comput. Electron.\
    \ Agric. 2015, 114,\n43–52. [CrossRef]\n190. Cohen, Y.; Alchanatis, V.; Prigojin,\
    \ A.; Levi, A.; Soroker, V. Use of aerial thermal imaging to estimate water\n\
    status of palm trees. Precis. Agric. 2012, 13, 123–140. [CrossRef]\nAgronomy 2020,\
    \ 10, 140\n30 of 35\n191. Comba, L.; Gay, P.; Primicerio, J.; Aimonino, D.R. Vineyard\
    \ detection from unmanned aerial systems images.\nComput. Electron. Agric. 2015,\
    \ 114, 78–87. [CrossRef]\n192. Nolan, A.; Park, S.; Fuentes, S.; Ryu, D.; Chung,\
    \ H. Automated detection and segmentation of vine rows using\nhigh resolution\
    \ UAS imagery in a commercial vineyard. In Proceedings of the 21st International\
    \ Congress\non Modelling and Simulation, Gold Coast, QLD, Australia, 29 November–4\
    \ December 2015; Volume 29,\npp. 1406–1412.\n193. Bobillet, W.; Da Costa, J.-P.;\
    \ Germain, C.; Lavialle, O.; Grenier, G. Row detection in high resolution remote\n\
    sensing images of vine ﬁelds. In Proceedings of the 4th European Conference on\
    \ Precision Agriculture,\nBerlin, Germany, 15–19 June 2003; pp. 81–87.\n194. Poblete,\
    \ T.; Ortega-Farías, S.; Ryu, D. Automatic coregistration algorithm to remove\
    \ canopy shaded pixels in\nUAV-borne thermal images to improve the estimation\
    \ of crop water stress index of a drip-irrigated cabernet\nsauvignon vineyard.\
    \ Sensors 2018, 18, 397. [CrossRef]\n195. Ihuoma, S.O.; Madramootoo, C.A. Recent\
    \ advances in crop water stress detection. Comput. Electron. Agric.\n2017, 141,\
    \ 267–275. [CrossRef]\n196. Jones, H.G. Use of infrared thermometry for estimation\
    \ of stomatal conductance as a possible aid to irrigation\nscheduling. Agric.\
    \ For. Meteorol. 1999, 95, 139–149. [CrossRef]\n197. Bellvert, J.; Marsal, J.;\
    \ Girona, J.; Zarco-Tejada, P.J. Seasonal evolution of crop water stress index\
    \ in grapevine\nvarieties determined with high-resolution remote sensing thermal\
    \ imagery. Irrig. Sci. 2015, 33, 81–93.\n[CrossRef]\n198. García-Tejero, I.F.;\
    \ Gutiérrez-Gordillo, S.; Ortega-Arévalo, C.; Iglesias-Contreras, M.; Moreno,\
    \ J.M.;\nSouza-Ferreira, L.; Durán-Zuazo, V.H. Thermal imaging to monitor the\
    \ crop-water status in almonds\nby using the non-water stress baselines. Sci.\
    \ Hortic. 2018, 238, 91–97. [CrossRef]\n199. Alchanatis, V.; Cohen, Y.; Cohen,\
    \ S.; Moller, M.; Sprinstin, M.; Meron, M.; Tsipris, J.; Saranga, Y.; Sela, E.\n\
    Evaluation of diﬀerent approaches for estimating and mapping crop water status\
    \ in cotton with thermal\nimaging. Precis. Agric. 2010, 11, 27–41. [CrossRef]\n\
    200. Goetz, S. Multi-sensor analysis of NDVI, surface temperature and biophysical\
    \ variables at a mixed grassland\nsite. Int. J. Remote Sens. 1997, 18, 71–94.\
    \ [CrossRef]\n201. Sun, L.; Gao, F.; Anderson, M.; Kustas, W.; Alsina, M.; Sanchez,\
    \ L.; Sams, B.; McKee, L.; Dulaney, W.;\nWhite, W.; et al. Daily mapping of 30\
    \ m LAI and NDVI for grape yield prediction in California Vineyards.\nRemote Sens.\
    \ 2017, 9, 317. [CrossRef]\n202. Peñuelas, J.; Filella, I.; Biel, C.; Serrano,\
    \ L.; Save, R. The reﬂectance at the 950–970 nm region as an indicator\nof plant\
    \ water status. Int. J. Remote Sens. 1993, 14, 1887–1905. [CrossRef]\n203. Jones,\
    \ C.L.; Weckler, P.R.; Maness, N.O.; Stone, M.L.; Jayasekara, R. Estimating water\
    \ stress in plants\nusing hyperspectral sensing. In Proceedings of the 2004 ASAE\
    \ Annual Meeting, Ottawa, ON, Canada,\n1–4 August 2004; p. 1.\n204. Aˇc, A.; Malenovskỳ,\
    \ Z.; Olejníˇcková, J.; Gallé, A.; Rascher, U.; Mohammed, G. Meta-analysis assessing\n\
    potential of steady-state chlorophyll ﬂuorescence for remote sensing detection\
    \ of plant water, temperature\nand nitrogen stress. Remote Sens. Environ. 2015,\
    \ 168, 420–436. [CrossRef]\n205. Mohammed, G.H.; Colombo, R.; Middleton, E.M.;\
    \ Rascher, U.; van der Tol, C.; Nedbal, L.; Goulas, Y.;\nPérez-Priego, O.; Damm,\
    \ A.; Meroni, M.; et al. Remote sensing of solar-induced chlorophyll ﬂuorescence\n\
    (SIF) in vegetation: 50 years of progress. Remote Sens. Environ. 2019, 231, 111177.\
    \ [CrossRef]\n206. Panigada, C.; Rossini, M.; Meroni, M.; Cilia, C.; Busetto,\
    \ L.; Amaducci, S.; Boschetti, M.; Cogliati, S.; Picchi, V.;\nPinto, F.; et al.\
    \ Fluorescence, PRI and canopy temperature for water stress detection in cereal\
    \ crops. Int. J.\nAppl. Earth Obs. Geoinformation 2014, 30, 167–178. [CrossRef]\n\
    207. Jones, H.G.; Stoll, M.; Santos, T.; Sousa, C.D.; Chaves, M.M.; Grant, O.M.\
    \ Use of infrared thermography for\nmonitoring stomatal closure in the ﬁeld: application\
    \ to grapevine. J. Exp. Bot. 2002, 53, 2249–2260. [CrossRef]\n208. Jackson, R.D.;\
    \ Idso, S.B.; Reginato, R.J.; Pinter, P.J. Canopy temperature as a crop water\
    \ stress indicator.\nWater Resour. Res. 1981, 17, 1133–1138. [CrossRef]\n209.\
    \ Bellvert, J.; Zarco-Tejada, P.J.; Girona, J.; Fereres, E. Mapping crop water\
    \ stress index in a ‘Pinot-noir’vineyard:\ncomparing ground measurements with\
    \ thermal remote sensing imagery from an unmanned aerial vehicle.\nPrecis. Agric.\
    \ 2014, 15, 361–376. [CrossRef]\n210. Fuentes, S.; De Bei, R.; Pech, J.; Tyerman,\
    \ S. Computational water stress indices obtained from thermal image\nanalysis\
    \ of grapevine canopies. Irrig. Sci. 2012, 30, 523–536. [CrossRef]\nAgronomy 2020,\
    \ 10, 140\n31 of 35\n211. Jackson, R.D. Canopy temperature and crop water stress.\
    \ In Advances in Irrigation; Elsevier: Amsterdam,\nThe Netherlands, 1982; Volume\
    \ 1, pp. 43–85.\n212. Idso, S.; Jackson, R.; Pinter, P., Jr.; Reginato, R.; Hatﬁeld,\
    \ J. Normalizing the stress-degree-day parameter for\nenvironmental variability.\
    \ Agric. Meteorol. 1981, 24, 45–55. [CrossRef]\n213. Möller, M.; Alchanatis, V.;\
    \ Cohen, Y.; Meron, M.; Tsipris, J.; Naor, A.; Ostrovsky, V.; Sprintsin, M.; Cohen,\
    \ S.\nUse of thermal and visible imagery for estimating crop water status of irrigated\
    \ grapevine. J. Exp. Bot. 2006,\n58, 827–838. [CrossRef] [PubMed]\n214. Egea,\
    \ G.; Padilla-Díaz, C.M.; Martinez-Guanter, J.; Fernández, J.E.; Pérez-Ruiz, M.\
    \ Assessing a crop water\nstress index derived from aerial thermal imaging and\
    \ infrared thermometry in super-high density olive\norchards. Agric. Water Manag.\
    \ 2017, 187, 210–221. [CrossRef]\n215. Bannari, A.; Morin, D.; Bonn, F.; Huete,\
    \ A. A review of vegetation indices. Remote Sens. Rev. 1995, 13, 95–120.\n[CrossRef]\n\
    216. Ballester, C.; Zarco-Tejada, P.; Nicolas, E.; Alarcon, J.; Fereres, E.; Intrigliolo,\
    \ D.; Gonzalez-Dugo, V. Evaluating\nthe performance of xanthophyll, chlorophyll\
    \ and structure-sensitive spectral indices to detect water stress in\nﬁve fruit\
    \ tree species. Precis. Agric. 2018, 19, 178–193. [CrossRef]\n217. Romero-Trigueros,\
    \ C.; Nortes, P.A.; Alarcón, J.J.; Hunink, J.E.; Parra, M.; Contreras, S.; Droogers,\
    \ P.; Nicolás, E.\nEﬀects of saline reclaimed waters and deﬁcit irrigation on\
    \ Citrus physiology assessed by UAV remote sensing.\nAgric. Water Manag. 2017,\
    \ 183, 60–69. [CrossRef]\n218. Zhao, T.; Stark, B.; Chen, Y.; Ray, A.; Doll, D.\
    \ More reliable crop water stress quantiﬁcation using small\nunmanned aerial systems\
    \ (sUAS). IFAC-PapersOnLine 2016, 49, 409–414. [CrossRef]\n219. Sandholt, I.;\
    \ Rasmussen, K.; Andersen, J. A simple interpretation of the surface temperature/vegetation\
    \ index\nspace for assessment of surface moisture status. Remote Sens. Environ.\
    \ 2002, 79, 213–224. [CrossRef]\n220. Wang, L.; Qu, J.J. Satellite remote sensing\
    \ applications for surface soil moisture monitoring: A review.\nFront. Earth Sci.\
    \ China 2009, 3, 237–247. [CrossRef]\n221. Colaizzi, P.D.; Barnes, E.M.; Clarke,\
    \ T.R.; Choi, C.Y.; Waller, P.M. Estimating soil moisture under low frequency\n\
    surface irrigation using crop water stress index. J. Irrig. Drain. Eng. 2003,\
    \ 129, 27–35. [CrossRef]\n222. Ahmed, A.; Zhang, Y.; Nichols, S. Review and evaluation\
    \ of remote sensing methods for soil-moisture\nestimation. SPIE Rev. 2011, 2,\
    \ 028001.\n223. Kerr, Y.H. Soil moisture from space: Where are we? Hydrogeol.\
    \ J. 2007, 15, 117–120. [CrossRef]\n224. Kerr, Y.H.; Waldteufel, P.; Wigneron,\
    \ J.-P.; Delwart, S.; Cabot, F.; Boutin, J.; Escorihuela, M.-J.; Font, J.; Reul,\
    \ N.;\nGruhier, C.; et al. The SMOS mission: New tool for monitoring key elements\
    \ ofthe global water cycle.\nProc. IEEE 2010, 98, 666–687. [CrossRef]\n225. Entekhabi,\
    \ D.; Njoku, E.G.; O’Neill, P.E.; Kellogg, K.H.; Crow, W.T.; Edelstein, W.N.;\
    \ Entin, J.K.; Goodman, S.D.;\nJackson, T.J.; Johnson, J.; et al. The soil moisture\
    \ active passive (SMAP) mission. Proc. IEEE 2010, 98, 704–716.\n[CrossRef]\n226.\
    \ Yueh, S.; Entekhabi, D.; O’Neill, P.; Njoku, E.; Entin, J. NASA soil moisture\
    \ active passive mission status\nand science performance. In Proceedings of the\
    \ 2016 IEEE International Geoscience and Remote Sensing\nSymposium (IGARSS 2016),\
    \ Beijing, China, 10–16 July 2016; pp. 116–119.\n227. Piles, M.; Sánchez, N.;\
    \ Vall-llossera, M.; Camps, A.; Martínez-Fernández, J.; Martínez, J.; González-Gambau,\
    \ V.\nA Downscaling Approach for SMOS Land Observations: Evaluation of High-Resolution\
    \ Soil Moisture Maps\nOver the Iberian Peninsula. IEEE J. Sel. Top. Appl. Earth\
    \ Obs. Remote Sens. 2014, 7, 3845–3857. [CrossRef]\n228. Cui, C.; Xu, J.; Zeng,\
    \ J.; Chen, K.-S.; Bai, X.; Lu, H.; Chen, Q.; Zhao, T. Soil moisture mapping from\
    \ satellites:\nAn intercomparison of SMAP, SMOS, FY3B, AMSR2, and ESA CCI over\
    \ two dense network regions at\ndiﬀerent spatial scales. Remote Sens. 2018, 10,\
    \ 33. [CrossRef]\n229. Peng, J.; Loew, A.; Merlin, O.; Verhoest, N.E. A review\
    \ of spatial downscaling of satellite remotely sensed soil\nmoisture. Rev. Geophys.\
    \ 2017, 55, 341–366. [CrossRef]\n230. Roussel, N.; Darrozes, J.; Ha, C.; Boniface,\
    \ K.; Frappart, F.; Ramillien, G.; Gavart, M.; Van de Vyvere, L.;\nDesenfans,\
    \ O.; Baup, F. Multi-scale volumetric soil moisture detection from GNSS SNR data:\
    \ Ground-based\nand airborne applications. In Proceedings of the 2016 IEEE Metrology\
    \ for Aerospace (MetroAeroSpace),\nFlorence, Italy, 22–23 June 2016; pp. 573–578.\n\
    231. Yan, S.; Zhang, N.; Chen, N.; Gong, J. Feasibility of using signal strength\
    \ indicator data to estimate soil\nmoisture based on GNSS interference signal\
    \ analysis. Remote Sens. Lett. 2018, 9, 61–70. [CrossRef]\nAgronomy 2020, 10,\
    \ 140\n32 of 35\n232. Johansen, K.; Sohlbach, M.; Sullivan, B.; Stringer, S.;\
    \ Peasley, D.; Phinn, S. Mapping banana plants from\nhigh spatial resolution orthophotos\
    \ to facilitate plant health assessment. Remote Sens. 2014, 6, 8261–8286.\n[CrossRef]\n\
    233. Hall, A.; Louis, J.; Lamb, D.W. Low-resolution remotely sensed images of\
    \ winegrape vineyards map spatial\nvariability in planimetric canopy area instead\
    \ of leaf area index. Aust. J. Grape Wine Res. 2008, 14, 9–17.\n[CrossRef]\n234.\
    \ Furness, G.; Magarey, P.; Miller, P.; Drew, H. Fruit tree and vine sprayer calibration\
    \ based on canopy size and\nlength of row: unit canopy row method. Crop Prot.\
    \ 1998, 17, 639–644. [CrossRef]\n235. Rosell, J.; Sanz, R. A review of methods\
    \ and applications of the geometric characterization of tree crops in\nagricultural\
    \ activities. Comput. Electron. Agric. 2012, 81, 124–141. [CrossRef]\n236. Lee,\
    \ K.; Ehsani, R. A laser scanner based measurement system for quantiﬁcation of\
    \ citrus tree geometric\ncharacteristics. Appl. Eng. Agric. 2009, 25, 777–788.\
    \ [CrossRef]\n237. Li, F.; Cohen, S.; Naor, A.; Shaozong, K.; Erez, A. Studies\
    \ of canopy structure and water use of apple trees on\nthree rootstocks. Agric.\
    \ Water Manag. 2002, 55, 1–14. [CrossRef]\n238. Kustas, W.; Agam, N.; Alﬁeri,\
    \ J.; McKee, L.; Prueger, J.; Hipps, L.; Howard, A.; Heitman, J. Below canopy\n\
    radiation divergence in a vineyard: Implications on interrow surface energy balance.\
    \ Irrig. Sci. 2019, 37,\n227–237. [CrossRef]\n239. Bendig, J.V. Unmanned aerial\
    \ vehicles (UAVs) for multi-temporal crop surface modelling. A new method for\n\
    plant height and biomass estimation based on RGB-imaging. Ph.D. Thesis, University\
    \ of Cologne, Cologne,\nGermany, 2015.\n240. Gowda, P.H.; Chavez, J.L.; Colaizzi,\
    \ P.D.; Evett, S.R.; Howell, T.A.; Tolk, J.A. ET mapping for agricultural\nwater\
    \ management: present status and challenges. Irrig. Sci. 2008, 26, 223–237. [CrossRef]\n\
    241. Zhang, K.; Kimball, J.S.; Running, S.W. A review of remote sensing based\
    \ actual evapotranspiration estimation.\nWiley Interdiscip. Rev. Water 2016, 3,\
    \ 834–853. [CrossRef]\n242. Liou, Y.-A.; Kar, S. Evapotranspiration estimation\
    \ with remote sensing and various surface energy balance\nalgorithms—A review.\
    \ Energies 2014, 7, 2821–2849. [CrossRef]\n243. Courault, D.; Seguin, B.; Olioso,\
    \ A. Review on estimation of evapotranspiration from remote sensing data:\nFrom\
    \ empirical to numerical modeling approaches. Irrig. Drain. Syst. 2005, 19, 223–249.\
    \ [CrossRef]\n244. Kalma, J.D.; McVicar, T.R.; McCabe, M.F. Estimating land surface\
    \ evaporation: A review of methods using\nremotely sensed surface temperature\
    \ data. Surv. Geophys. 2008, 29, 421–469. [CrossRef]\n245. Li, Z.-L.; Tang, R.;\
    \ Wan, Z.; Bi, Y.; Zhou, C.; Tang, B.; Yan, G.; Zhang, X. A review of current\
    \ methodologies\nfor regional evapotranspiration estimation from remotely sensed\
    \ data. Sensors 2009, 9, 3801–3853. [CrossRef]\n246. Marshall, M.; Thenkabail,\
    \ P.; Biggs, T.; Post, K. Hyperspectral narrowband and multispectral broadband\n\
    indices for remote sensing of crop evapotranspiration and its components (transpiration\
    \ and soil evaporation).\nAgric. For. Meteorol. 2016, 218, 122–134. [CrossRef]\n\
    247. Maes, W.; Steppe, K. Estimating evapotranspiration and drought stress with\
    \ ground-based thermal remote\nsensing in agriculture: a review. J. Exp. Bot.\
    \ 2012, 63, 4671–4712. [CrossRef]\n248. Bastiaanssen, W.G.; Menenti, M.; Feddes,\
    \ R.; Holtslag, A. A remote sensing surface energy balance algorithm\nfor land\
    \ (SEBAL). 1. Formulation. J. Hydrol. 1998, 212, 198–212. [CrossRef]\n249. Allen,\
    \ R.; Irmak, A.; Trezza, R.; Hendrickx, J.M.; Bastiaanssen, W.; Kjaersgaard, J.\
    \ Satellite-based ET estimation\nin agriculture using SEBAL and METRIC. Hydrol.\
    \ Process. 2011, 25, 4011–4027. [CrossRef]\n250. Allen, R.G.; Tasumi, M.; Trezza,\
    \ R. Satellite-based energy balance for mapping evapotranspiration with\ninternalized\
    \ calibration (METRIC)—Model. J. Irrig. Drain. Eng. 2007, 133, 380–394. [CrossRef]\n\
    251. Allen, R.G.; Tasumi, M.; Morse, A.; Trezza, R.; Wright, J.L.; Bastiaanssen,\
    \ W.; Kramber, W.; Lorite, I.;\nRobison, C.W. Satellite-Based Energy Balance for\
    \ Mapping Evapotranspiration with Internalized Calibration\n(METRIC)-Applications.\
    \ J. Irrig. Drain. Eng. 2007, 133, 395–406. [CrossRef]\n252. Allen, R.G.; Pereira,\
    \ L.S.; Raes, D.; Smith, M. FAO Irrigation and drainage paper No. 56. Rome Food\
    \ Agric.\nOrgan. U. N. 1998, 56, e156.\n253. Jackson, R.D.; Moran, M.S.; Gay,\
    \ L.W.; Raymond, L.H. Evaluating evaporation from ﬁeld crops using airborne\n\
    radiometry and ground-based meteorological data. Irrig. Sci. 1987, 8, 81–90. [CrossRef]\n\
    254. Williams, L.; Ayars, J. Grapevine water use and the crop coeﬃcient are linear\
    \ functions of the shaded area\nmeasured beneath the canopy. Agric. For. Meteorol.\
    \ 2005, 132, 201–211. [CrossRef]\nAgronomy 2020, 10, 140\n33 of 35\n255. Jayanthi,\
    \ H.; Neale, C.M.; Wright, J.L. Development and validation of canopy reﬂectance-based\
    \ crop\ncoeﬃcient for potato. Agric. Water Manag. 2007, 88, 235–246. [CrossRef]\n\
    256. Samani, Z.; Bawazir, A.S.; Bleiweiss, M.; Skaggs, R.; Longworth, J.; Tran,\
    \ V.D.; Pinon, A. Using remote sensing\nto evaluate the spatial variability of\
    \ evapotranspiration and crop coeﬃcient in the lower Rio Grande Valley,\nNew Mexico.\
    \ Irrig. Sci. 2009, 28, 93–100. [CrossRef]\n257. Kustas, W.P.; Anderson, M.C.;\
    \ Alﬁeri, J.G.; Knipper, K.; Torres-Rua, A.; Parry, C.K.; Nieto, H.; Agam, N.;\n\
    White, W.A.; Gao, F.; et al. The grape remote sensing atmospheric proﬁle and evapotranspiration\
    \ experiment.\nBull. Am. Meteorol. Soc. 2018, 99, 1791–1812. [CrossRef]\n258.\
    \ Kamble, B.; Kilic, A.; Hubbard, K. Estimating crop coeﬃcients using remote sensing-based\
    \ vegetation index.\nRemote Sens. 2013, 5, 1588–1602. [CrossRef]\n259. Hou, M.;\
    \ Tian, F.; Zhang, L.; Li, S.; Du, T.; Huang, M.; Yuan, Y. Estimating crop transpiration\
    \ of soybean\nunder diﬀerent irrigation treatments using thermal infrared remote\
    \ sensing imagery. Agronomy 2019, 9, 8.\n[CrossRef]\n260. Knipper, K.R.; Kustas,\
    \ W.P.; Anderson, M.C.; Alﬁeri, J.G.; Prueger, J.H.; Hain, C.R.; Gao, F.; Yang,\
    \ Y.;\nMcKee, L.G.; Nieto, H.; et al. Evapotranspiration estimates derived using\
    \ thermal-based satellite remote\nsensing and data fusion for irrigation management\
    \ in California vineyards. Irrig. Sci. 2019, 37, 431–449.\n[CrossRef]\n261. Hoﬀmann,\
    \ H.; Nieto, H.; Jensen, R.; Guzinski, R.; Zarco-Tejada, P.; Friborg, T. Estimating\
    \ evapotranspiration\nwith thermal UAV data and two source energy balance models.\
    \ Hydrol. Earth Syst. Sci. Discuss. 2016, 20,\n697–713. [CrossRef]\n262. Cammalleri,\
    \ C.; Anderson, M.; Kustas, W. Upscaling of evapotranspiration ﬂuxes from instantaneous\
    \ to\ndaytime scales for thermal remote sensing applications. Hydrol. Earth Syst.\
    \ Sci. 2014, 18, 1885–1894.\n[CrossRef]\n263. Biggs, T.W.; Marshall, M.; Messina,\
    \ A. Mapping daily and seasonal evapotranspiration from irrigated crops\nusing\
    \ global climate grids and satellite imagery: Automation and methods comparison.\
    \ Water Resour. Res.\n2016, 52, 7311–7326. [CrossRef]\n264. Chávez, J.L.; Neale,\
    \ C.M.; Prueger, J.H.; Kustas, W.P. Daily evapotranspiration estimates from extrapolating\n\
    instantaneous airborne remote sensing ET values. Irrig. Sci. 2008, 27, 67–81.\
    \ [CrossRef]\n265. McCabe, M.F.; Wood, E.F. Scale inﬂuences on the remote estimation\
    \ of evapotranspiration using multiple\nsatellite sensors. Remote Sens. Environ.\
    \ 2006, 105, 271–285. [CrossRef]\n266. Kustas, W.; Li, F.; Jackson, T.; Prueger,\
    \ J.; MacPherson, J.; Wolde, M. Eﬀects of remote sensing pixel resolution\non\
    \ modeled energy ﬂux variability of croplands in Iowa. Remote Sens. Environ. 2004,\
    \ 92, 535–547. [CrossRef]\n267. Hong, S.; Hendrickx, J.M.; Borchers, B. Eﬀect\
    \ of scaling transfer between evapotranspiration maps derived\nfrom LandSat 7\
    \ and MODIS images. In Targets and Backgrounds XI: Characterization and Representation;\n\
    International Society for Optics and Photonics: Orlando, FL, USA, 2005; Volume\
    \ 5811, pp. 147–159.\n268. Abiodun, O.O.; Guan, H.; Post, V.E.; Batelaan, O. Comparison\
    \ of MODIS and SWAT evapotranspiration over\na complex terrain at diﬀerent spatial\
    \ scales. Hydrol. Earth Syst. Sci. 2018, 22, 2775–2794. [CrossRef]\n269. Justice,\
    \ C.; Townshend, J.; Vermote, E.; Masuoka, E.; Wolfe, R.; Saleous, N.; Roy, D.;\
    \ Morisette, J. An overview\nof MODIS Land data processing and product status.\
    \ Remote Sens. Environ. 2002, 83, 3–15. [CrossRef]\n270. Nieto, H.; Bellvert,\
    \ J.; Kustas, W.P.; Alﬁeri, J.G.; Gao, F.; Prueger, J.; Torres-Rua, A.; Hipps,\
    \ L.E.; Elarab, M.;\nSong, L. Unmanned airborne thermal and mutilspectral imagery\
    \ for estimating evapotranspiration in irrigated\nvineyards. In Proceedings of\
    \ the 2017 IEEE International Geoscience and Remote Sensing Symposium\n(IGARSS\
    \ 2017), Fort Worth, TX, USA, 23–28 July 2017; pp. 5510–5513.\n271. Ortega-Farías,\
    \ S.; Ortega-Salazar, S.; Poblete, T.; Poblete-Echeverría, C.; Zúñiga, M.; Sepúlveda-Reyes,\
    \ D.;\nKilic, A.; Allen, R. Estimation of olive evapotranspiration using multispectral\
    \ and thermal sensors placed\naboard an unmanned aerial vehicle. Acta Hortic.\
    \ 2017, 1150, 1–8. [CrossRef]\n272. Gago, J.; Douthe, C.; Coopman, R.E.; Gallego,\
    \ P.P.; Ribas-Carbo, M.; Flexas, J.; Escalona, J.; Medrano, H. UAVs\nchallenge\
    \ to assess water stress for sustainable agriculture. Agric. Water Manag. 2015,\
    \ 153, 9–19. [CrossRef]\n273. Sepúlveda-Reyes, D.; Ingram, B.; Bardeen, M.; Zúñiga,\
    \ M.; Ortega-Farías, S.; Poblete-Echeverría, C. Selecting\ncanopy zones and thresholding\
    \ approaches to assess grapevine water status by using aerial and ground-based\n\
    thermal imaging. Remote Sens. 2016, 8, 822. [CrossRef]\n274. McBratney, A.; Whelan,\
    \ B.; Ancev, T.; Bouma, J. Future directions of precision agriculture. Precis.\
    \ Agric. 2005,\n6, 7–23. [CrossRef]\nAgronomy 2020, 10, 140\n34 of 35\n275. Ferguson,\
    \ R.; Rundquist, D. Remote sensing for site-speciﬁc crop management. In Precision\
    \ Agriculture Basics;\nShannon, D.K., Clay, D.E., Kitchen, N.R., Eds.; American\
    \ Society of Agronomy: Madison, WI, USA; Crop\nScience Society of America: Madison,\
    \ WI, USA; Soil Science Society of America: Madison, WI, USA, 2018;\npp. 103–118.\n\
    276. Florin, M.J.; McBratney, A.B.; Whelan, B.M. Extending site-speciﬁc crop management\
    \ from individual ﬁelds\nto an entire farm. In Proceedings of the Precision agriculture\
    \ ’05, Proceedings of the 5th European Conference\non Precision Agriculture, Uppsala,\
    \ Sweden, 9–12 June 2005; pp. 857–863.\n277. Perea-Moreno, A.J.; Aguilera-Urena,\
    \ M.J.; Merono-de Larriva, J.E.; Manzano-Agugliaro, F. Assessment of\nthe potential\
    \ of UAV video image analysis for planning irrigation needs of golf courses. Water\
    \ 2016, 8, 584.\n[CrossRef]\n278. Meron, M.; Tsipris, J.; Charitt, D. Remote mapping\
    \ of crop water status to assess spatial variability of crop\nstress. Precis.\
    \ Agric. 2003, 405–410.\n279. Idso, S.B. Non-water-stressed baselines: A key to\
    \ measuring and interpreting plant water stress. Agric. Meteorol.\n1982, 27, 59–70.\
    \ [CrossRef]\n280. Cohen, Y.; Alchanatis, V.; Meron, M.; Saranga, Y.; Tsipris,\
    \ J. Estimation of leaf water potential by thermal\nimagery and spatial analysis.\
    \ J. Exp. Bot. 2005, 56, 1843–1852. [CrossRef] [PubMed]\n281. Pagay, V.; Kidman,\
    \ C.; Jenkins, A. Proximal and remote sensing tools for regional-scale characterisation\
    \ of\ngrapevine water and nitrogen status in Coonawarra. Wine Vitic. J. 2016,\
    \ 31, 42–47.\n282. Romero, M.; Luo, Y.; Su, B.; Fuentes, S. Vineyard water status\
    \ estimation using multispectral imagery from an\nUAV platform and machine learning\
    \ algorithms for irrigation scheduling management. Comput. Electron. Agric.\n\
    2018, 147, 109–117. [CrossRef]\n283. Goldhamer, D.A.; Viveros, M.; Salinas, M.\
    \ Regulated deﬁcit irrigation in almonds: eﬀects of variations in\napplied water\
    \ and stress timing on yield and yield components. Irrig. Sci. 2006, 24, 101–114.\
    \ [CrossRef]\n284. Girona, J.; Marsal, J.; Cohen, M.; Mata, M.; Miravete, C. Physiological,\
    \ growth and yield responses of almond\n(Prunus dulcis L ) to diﬀerent irrigation\
    \ regimes. Acta Hortic. 1993, 335, 389–398. [CrossRef]\n285. Sadler, E.; Evans,\
    \ R.; Stone, K.; Camp, C. Opportunities for conservation with precision irrigation.\
    \ J. Soil\nWater Conserv. 2005, 60, 371–378.\n286. Corbane, C.; Jacob, F.; Raclot,\
    \ D.; Albergel, J.; Andrieux, P. Multitemporal analysis of hydrological soil surface\n\
    characteristics using aerial photos: A case study on a Mediterranean vineyard.\
    \ Int. J. Appl. Earth Obs. Geoinf.\n2012, 18, 356–367. [CrossRef]\n287. Osroosh,\
    \ Y.; Peters, R.T.; Campbell, C.S. Daylight crop water stress index for continuous\
    \ monitoring of water\nstatus in apple trees. Irrig. Sci. 2016, 34, 209–219. [CrossRef]\n\
    288. Osroosh, Y.; Peters, R.T.; Campbell, C.S.; Zhang, Q. Comparison of irrigation\
    \ automation algorithms for\ndrip-irrigated apple trees. Comput. Electron. Agric.\
    \ 2016, 128, 87–99. [CrossRef]\n289. Lamm, F.R.; Aiken, R.M. Comparison of temperature-time\
    \ threshold-and ET-based irrigation scheduling for\ncorn production. In Proceedings\
    \ of the 2008 ASABE Annual International Meeting, Providence, RI, USA,\n29 June–2\
    \ July 2008; p. 1.\n290. O’Shaughnessy, S.A.; Evett, S.R.; Colaizzi, P.D.; Howell,\
    \ T.A. A crop water stress index and time threshold\nfor automatic irrigation\
    \ scheduling of grain sorghum. Agric. Water Manag. 2012, 107, 122–132. [CrossRef]\n\
    291. Bellvert, J.; Zarco-Tejada, P.; Gonzalez-Dugo, V.; Girona, J.; Fereres, E.\
    \ Scheduling vineyard irrigation based\non mapping leaf water potential from airborne\
    \ thermal imagery. In Precision Agriculture’13; Staﬀord, J.V., Ed.;\nSpringer:\
    \ Cham, Switzerland, 2013; pp. 699–704.\n292. Bellvert, J.; Girona, J. The use\
    \ of multispectral and thermal images as a tool for irrigation scheduling in\n\
    vineyards. In The Use of Remote Sensing and Geographic Information Systems for\
    \ Irrigation Management in\nSouthwest Europe; Erena, M., López-Francos, A., Montesinos,\
    \ S., Berthoumieu, J.-P., Eds.; CIHEAM: Zaragoza,\nSpain, 2012; pp. 131–137.\n\
    293. Erdem, Y.; ¸Sehirali, S.; Erdem, T.; Kenar, D. Determination of crop water\
    \ stress index for irrigation scheduling\nof bean (Phaseolus vulgaris L.). Turk.\
    \ J. Agric. For. 2006, 30, 195–202.\n294. Osroosh, Y.; Troy Peters, R.; Campbell,\
    \ C.S.; Zhang, Q. Automatic irrigation scheduling of apple trees using\ntheoretical\
    \ crop water stress index with an innovative dynamic threshold. Comput. Electron.\
    \ Agric. 2015, 118,\n193–203. [CrossRef]\n295. Irmak, S.; Haman, D.Z.; Bastug,\
    \ R. Determination of crop water stress index for irrigation timing and yield\n\
    estimation of corn. Agron. J. 2000, 92, 1221–1227. [CrossRef]\nAgronomy 2020,\
    \ 10, 140\n35 of 35\n296. Acevedo-Opazo, C.; Tisseyre, B.; Ojeda, H.; Ortega-Farias,\
    \ S.; Guillaume, S. Is it possible to assess the spatial\nvariability of vine\
    \ water status? OENO One 2008, 42, 203–219. [CrossRef]\n297. Acevedo-Opazo, C.;\
    \ Tisseyre, B.; Guillaume, S.; Ojeda, H. The potential of high spatial resolution\
    \ information\nto deﬁne within-vineyard zones related to vine water status. Precis.\
    \ Agric. 2008, 9, 285–302. [CrossRef]\n298. Petrie, P.R.; Wang, Y.; Liu, S.; Lam,\
    \ S.; Whitty, M.A.; Skewes, M.A. The accuracy and utility of a low cost\nthermal\
    \ camera and smartphone-based system to assess grapevine water status. Biosyst.\
    \ Eng. 2019, 179,\n126–139. [CrossRef]\n299. Woellert, K.; Ehrenfreund, P.; Ricco,\
    \ A.J.; Hertzfeld, H. Cubesats: Cost-eﬀective science and technology\nplatforms\
    \ for emerging and developing nations. Adv. Space Res. 2011, 47, 663–684. [CrossRef]\n\
    300. Kramer, H.J.; Cracknell, A.P. An overview of small satellites in remote sensing.\
    \ Int. J. Remote Sens. 2008, 29,\n4285–4337. [CrossRef]\n301. McCabe, M.; Aragon,\
    \ B.; Houborg, R.; Mascaro, J. CubeSats in Hydrology: Ultrahigh-Resolution Insights\n\
    Into Vegetation Dynamics and Terrestrial Evaporation. Water Resour. Res. 2017,\
    \ 53, 10017–10024. [CrossRef]\n302. Trombetti, M.; Riaño, D.; Rubio, M.; Cheng,\
    \ Y.; Ustin, S. Multi-temporal vegetation canopy water content\nretrieval and\
    \ interpretation using artiﬁcial neural networks for the continental USA. Remote\
    \ Sens. Environ.\n2008, 112, 203–215. [CrossRef]\n303. King, B.; Shellie, K. Evaluation\
    \ of neural network modeling to predict non-water-stressed leaf temperature in\n\
    wine grape for calculation of crop water stress index. Agric. Water Manag. 2016,\
    \ 167, 38–52. [CrossRef]\n304. Shan, N.; Ju, W.; Migliavacca, M.; Martini, D.;\
    \ Guanter, L.; Chen, J.; Goulas, Y.; Zhang, Y. Modeling canopy\nconductance and\
    \ transpiration from solar-induced chlorophyll ﬂuorescence. Agric. For. Meteorol.\
    \ 2019, 268,\n189–201. [CrossRef]\n305. Moreno, J.; Goulas, Y.; Huth, A.; Middelton,\
    \ E.; Miglietta, F.; Mohammed, G.; Nebdal, L.; Rascher, U.;\nVerhof, W. Report\
    \ for mission selection: CarbonSat ﬂex–An earth explorer to observe vegetation\
    \ ﬂuorescence.\nEur. Space Agency 2015, 1330/2, 179–185.\n306. Drusch, M.; Moreno,\
    \ J.; Del Bello, U.; Franco, R.; Goulas, Y.; Huth, A.; Kraft, S.; Middleton, E.M.;\
    \ Miglietta, F.;\nMohammed, G.; et al. The ﬂuorescence explorer mission concept-ESA’s\
    \ Earth explorer 8. IEEE Trans. Geosci.\nRemote Sens. 2017, 55, 1273–1284. [CrossRef]\n\
    307. Gautam, D.; Lucieer, A.; Watson, C.; McCoull, C. Lever-arm and boresight\
    \ correction, and ﬁeld of view\ndetermination of a spectroradiometer mounted on\
    \ an unmanned aircraft system. ISPRS J. Photogramm.\nRemote Sens. 2019, 155, 25–36.\
    \ [CrossRef]\n308. Garzonio, R.; Di Mauro, B.; Colombo, R.; Cogliati, S. Surface\
    \ reﬂectance and sun-induced ﬂuorescence\nspectroscopy measurements using a small\
    \ hyperspectral UAS. Remote Sens. 2017, 9, 472. [CrossRef]\n309. Gautam, D.; Lucieer,\
    \ A.; Bendig, J.; Malenovský, Z. Footprint Determination of a Spectroradiometer\
    \ Mounted\non an Unmanned Aircraft System. IEEE Trans. Geosci. Remote Sens. 2019,\
    \ 1–12. [CrossRef]\n310. Bendig, J.; Malenovskỳ, Z.; Gautam, D.; Lucieer, A. Solar-Induced\
    \ Chlorophyll Fluorescence Measured\nFrom an Unmanned Aircraft System: Sensor\
    \ Etaloning and Platform Motion Correction. IEEE Trans. Geosci.\nRemote Sens.\
    \ 2019, 1–8. [CrossRef]\n311. TongKe, F. Smart agriculture based on cloud computing\
    \ and IOT. J. Converg. Inf. Technol. 2013, 8, 210–216.\n312. Ojha, T.; Misra,\
    \ S.; Raghuwanshi, N.S. Wireless sensor networks for agriculture: The state-of-the-art\
    \ in\npractice and future challenges. Comput. Electron. Agric. 2015, 118, 66–84.\
    \ [CrossRef]\n313. Hori, M.; Kawashima, E.; Yamazaki, T. Application of cloud\
    \ computing to agriculture and prospects in other\nﬁelds. Fujitsu Sci. Tech. J.\
    \ 2010, 46, 446–454.\n314. Goap, A.; Sharma, D.; Shukla, A.; Krishna, C.R. An\
    \ IoT based smart irrigation management system using\nMachine learning and open\
    \ source technologies. Comput. Electron. Agric. 2018, 155, 41–49. [CrossRef]\n\
    © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open\
    \ access\narticle distributed under the terms and conditions of the Creative Commons\
    \ Attribution\n(CC BY) license (http://creativecommons.org/licenses/by/4.0/).\n"
  inline_citation: '>'
  journal: Agronomy (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2073-4395/10/1/140/pdf
  publication_year: 2020
  relevance_evaluation: High
  relevance_score: 0.8913431529100987
  relevance_score1: 0
  relevance_score2: 0
  title: A Review of Current and Potential Applications of Remote Sensing to Study
    the Water Status of Horticultural Crops
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- DOI: https://doi.org/10.3390/agronomy13061524
  analysis: '>'
  authors:
  - Aqleem Abbas
  - Zhenhao Zhang
  - Hongxia Zheng
  - Mohammad Murtaza Alami
  - Abdulmajeed F. Alrefaei
  - Qamar Abbas
  - Syed Atif Hasan Naqvi
  - Muhammad Junaid Rao
  - Walid F. A. Mosa
  - Qamar Abbas
  - Abul Hussain
  - Muhammad Zeeshan Hassan
  - Lei Zhou
  citation_count: 6
  explanation: From your close reading of the paper, provide a concise explanation
    of the study's purpose and main objectives, using a maximum of 3 sentences.
  full_citation: '>'
  full_text: ">\nCitation: Abbas, A.; Zhang, Z.;\nZheng, H.; Alami, M.M.; Alrefaei,\n\
    A.F.; Abbas, Q.; Naqvi, S.A.H.; Rao,\nM.J.; Mosa, W.F.A.; Abbas, Q.; et al.\n\
    Drones in Plant Disease Assessment,\nEfﬁcient Monitoring, and Detection:\nA Way\
    \ Forward to Smart Agriculture.\nAgronomy 2023, 13, 1524. https://\ndoi.org/10.3390/agronomy13061524\n\
    Academic Editor: Xingang Xu\nReceived: 5 April 2023\nRevised: 23 May 2023\nAccepted:\
    \ 26 May 2023\nPublished: 31 May 2023\nCopyright:\n© 2023 by the authors.\nLicensee\
    \ MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed\n\
    under\nthe\nterms\nand\nconditions of the Creative Commons\nAttribution (CC BY)\
    \ license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nagronomy\nReview\n\
    Drones in Plant Disease Assessment, Efﬁcient Monitoring, and\nDetection: A Way\
    \ Forward to Smart Agriculture\nAqleem Abbas 1,2,†\n, Zhenhao Zhang 1,†, Hongxia\
    \ Zheng 1, Mohammad Murtaza Alami 3\n,\nAbdulmajeed F. Alrefaei 4\n, Qamar Abbas\
    \ 5, Syed Atif Hasan Naqvi 6,*\n, Muhammad Junaid Rao 7\n,\nWalid F. A. Mosa 8\n\
    , Qamar Abbas 9, Azhar Hussain 2, Muhammad Zeeshan Hassan 6 and Lei Zhou 1,*\n\
    1\nState Key Laboratory for Managing Biotic and Chemical Threats to the Quality\
    \ and Safety of Agro-Products,\nInstitute of Agro-Product Safety and Nutrition,\
    \ Zhejiang Academy of Agricultural Sciences,\nHangzhou 310021, China; aqlpath@gmail.com\
    \ (A.A.); fangzhenhao1016@126.com (Z.Z.);\nhxzh_bio@126.com (H.Z.)\n2\nDepartment\
    \ of Agriculture and Food Technology, Karakoram International University, Gilgit\
    \ 15100, Pakistan;\nazhar.hussain21@kiu.edu.pk\n3\nDepartment of Crop Cultivation\
    \ and Farming System, College of Plant Science and Technology,\nHuazhong Agricultural\
    \ University, Wuhan 430070, China; murtazaalami@webmail.hzau.edu.cn\n4\nDepartment\
    \ of Biology, Jamoum University Collage, Umm Al-Qura University, Makkah 21955,\
    \ Saudi Arabia;\nafrefaei@uqu.edu.sa\n5\nDepartment of Computer Sciences, University\
    \ of Karachi, Karachi 75270, Pakistan; qmarabbas715@gmail.com\n6\nDepartment of\
    \ Plant Pathology, Bahauddin Zakariya University, Multan 60800, Pakistan;\nranazeeshanhassan824@gmail.com\n\
    7\nState Key Laboratory for Conservation and Utilization of Subtropical Agro-Bioresources,\
    \ Guangxi Key\nLaboratory of Sugarcane Biology, College of Agriculture, Guangxi\
    \ University, Nanning 530004, China;\nmjunaidrao@gxu.edu.cn\n8\nPlant Production\
    \ Department (Horticulture-Pomology), Faculty of Agriculture, Saba Basha,\nAlexandria\
    \ University, Alexandria 21531, Egypt; walidmosa@alexu.edu.eg\n9\nDepartment of\
    \ Plant Sciences, Karakoram International University, Gilgit 15100, Pakistan;\n\
    qamar.abbasbio@kiu.edu.pk\n*\nCorrespondence: atifnaqvi@bzu.edu.pk (S.A.H.N.);\
    \ zhoul@zaas.ac.cn (L.Z.)\n†\nThese authors contributed equally to this work.\n\
    Abstract: Plant diseases are one of the major threats to global food production.\
    \ Efﬁcient monitoring\nand detection of plant pathogens are instrumental in restricting\
    \ and effectively managing the spread\nof the disease and reducing the cost of\
    \ pesticides. Traditional, molecular, and serological methods\nthat are widely\
    \ used for plant disease detection are often ineffective if not applied during\
    \ the initial\nstages of pathogenesis, when no or very weak symptoms appear. Moreover,\
    \ they are almost useless\nin acquiring spatialized diagnostic results on plant\
    \ diseases. On the other hand, remote sensing\n(RS) techniques utilizing drones\
    \ are very effective for the rapid identiﬁcation of plant diseases in\ntheir early\
    \ stages. Currently, drones, play a pivotal role in the monitoring of plant pathogen\
    \ spread,\ndetection, and diagnosis to ensure crops’ health status. The advantages\
    \ of drone technology include\nhigh spatial resolution (as several sensors are\
    \ carried aboard), high efﬁciency, usage ﬂexibility, and\nmore signiﬁcantly, quick\
    \ detection of plant diseases across a large area with low cost, reliability,\n\
    and provision of high-resolution data. Drone technology employs an automated procedure\
    \ that\nbegins with gathering images of diseased plants using various sensors\
    \ and cameras. After extracting\nfeatures, image processing approaches use the\
    \ appropriate traditional machine learning or deep\nlearning algorithms. Features\
    \ are extracted from images of leaves using edge detection and histogram\nequalization\
    \ methods. Drones have many potential uses in agriculture, including reducing\
    \ manual\nlabor and increasing productivity. Drones may be able to provide early\
    \ warning of plant diseases,\nallowing farmers to prevent costly crop failures.\n\
    Keywords: plant disease detection; drones; machine learning; precision agriculture;\
    \ image analysis\nAgronomy 2023, 13, 1524. https://doi.org/10.3390/agronomy13061524\n\
    https://www.mdpi.com/journal/agronomy\nAgronomy 2023, 13, 1524\n2 of 26\n1. Introduction\n\
    Plant diseases are responsible for enormous yield losses and for threatening global\n\
    food production [1]; hence, proper detection and reliable diagnostic methods for\
    \ identifying\nthe etiological agents of disease are essential to conserving time\
    \ and money by preventing\nor limiting crop damages [2]. Classically, diseases\
    \ were recognized based on traditional\nmethods; these methods, often subjective,\
    \ were strictly dependent on the observer and\nthough time-consuming overall,\
    \ were prone to inaccuracy. Additionally, human scouting\nis expensive and, in\
    \ many cases, impractical due to human error and/or the occurrence\nof cryptic\
    \ when not mild symptoms, making diagnosis at early stages impossible [3].\nTherefore,\
    \ a technologically driven agricultural revolution is important to permanently\n\
    solve the problems mentioned earlier at a reasonable cost with little environmental\
    \ impact.\nWith the continuous adoption of recent advanced technologies such as\
    \ Internet of Things\ndevices, intelligent algorithms, sophisticated sensors,\
    \ and modern machines, agriculture\nhas changed. It is currently changing from\
    \ being accomplished by human workers to using\nsmart agricultural machines and\
    \ robots. Smart agricultural machines and robots have been\ndeveloped which detect\
    \ plant diseases early on and at the same time monitor their long-\ndistance movement\
    \ [3,4]. Many researchers have used high-resolution imagery collected\nfrom satellites,\
    \ airplanes, on-the-ground machines, and drones to identify agricultural\ndiseases.\
    \ Satellites and airplanes can cover vast areas in a short amount of time. However,\n\
    satellites and airplanes have poor spatial and temporal image resolutions compared\
    \ to\ndrones and are highly susceptible to weather conditions that can affect\
    \ overﬂight [3–5].\nTherefore, aerial remote sensing (RS) using drones (Unmanned\
    \ Aerial Vehicles (UAV)\nor Unmanned Aerial Systems (UAS)) with intelligent visual\
    \ systems may be an efﬁcient and\ninexpensive way for farmers to detect crop and\
    \ plant diseases in a variety of agricultural\nﬁelds, from the most intimate greenhouse\
    \ to the largest farm [3–9].\nDigital (red, blue, and green or RBG), multispectral,\
    \ hyper-spectral, ﬂuorescent, and\nthermal infrared-based imaging sensors paired\
    \ with effective algorithms mounted on\ndrones can efﬁciently detect, differentiate,\
    \ and quantify the severity of the symptoms\ninduced by various pathogens under\
    \ ﬁeld conditions [10,11], as conﬁrmed by the plethora\nof studies conducted on\
    \ important cereal crops such as, rice [12], maize [13], wheat [14,15],\nfruit\
    \ trees (including citrus [16], olive [17], and grapevine [18]), vegetables (including\n\
    potatoes [19], soybeans [20], and tomatoes [21]), and many forest trees, such\
    \ as, pine [3],\nthat have demonstrated the reliability of drones for diagnostic\
    \ purposes.\nDrones are equipped with digital, multispectral, hyperspectral, thermal,\
    \ and ﬂuo-\nrescence sensors which offer ﬁner resolution of plant diseases and\
    \ assist in plant disease\ndetection at earlier stages than is possible with satellite\
    \ systems [12]. Data acquired by\ndrones can be simultaneously sampled by their\
    \ autonomous systems at various heights\nin the atmosphere; these data can then\
    \ be rapidly elaborated to provide forecasting mod-\nels across ﬁelds, regions,\
    \ and even whole continents [22]. Finally, information can be\ndelivered to farmers,\
    \ allowing them to make appropriate decisions regarding timely man-\nagement of\
    \ disease. Hence, precision agriculture (Smart Agriculture) may beneﬁt greatly\n\
    from using drone remote sensing technology because of its cheap cost and high-ﬂying\n\
    ﬂexibility [17–20]. There are a large number of studies on using drone platforms\
    \ with\ndifferent sensors for plant disease sensing. For example, drones were\
    \ equipped with a\nhyperspectral image sensor to obtain an image of winter wheat\
    \ yellow rust and realize\nits effective detection. Similarly, multispectral imaging\
    \ and a drone system were used to\nexplore myrtle rust on myrtle, and infested\
    \ corn plants were detected with drones using\nvisible light images from digital\
    \ cameras [12–15].\nEffective algorithms are required to analyze the images gathered\
    \ by drones. Tradi-\ntional machine learning methods have shortcomings due to\
    \ their reliance on manual feature\nextraction methods, which is especially ineffective\
    \ in complex environments. Deep learning\nalgorithms have recently emerged as\
    \ a promising new alternative to enhance computer\nvision-based systems for autonomous\
    \ crop disease monitoring. Without any human as-\nsistance, they can perform autonomous\
    \ feature extraction, providing farmers with data\nAgronomy 2023, 13, 1524\n3\
    \ of 26\nthat might improve crop yields and decrease treatment costs. A prominent\
    \ area of study at\npresent is the use of computer vision methods, deep learning\
    \ algorithms, and drone-based\nplatforms for the early and accurate diagnosis\
    \ of a wide variety of plant diseases [23].\nHowever, despite being highly efﬁcient,\
    \ low-cost, ﬂexible, accurate, and quick at ﬁeld scale,\ntheir limited ﬂight duration\
    \ makes drones unsuitable for data acquisition within large\nareas, and their\
    \ ability to carry heavy sensors is limited. Thus, the choice of a speciﬁc drone\n\
    and the selection of the sensors, software, algorithms, and settings of the drones\
    \ are critical\nfor achieving the best performance [24]. Keeping in view the importance\
    \ of drones in\nplant disease diagnosis, the following parts have been included\
    \ in this review: (1) methods\nfor plant disease detection, including old and\
    \ new generations; (2) types of sensors and\ncameras mounted on drones; (3) types\
    \ of drones; (4) novel approaches to detecting plant\ndiseases, focusing on drones;\
    \ and (5) drone applications for plant disease detections using\ntraditional and\
    \ deep learning algorithms.\n2. Plant Disease Detection\n2.1. Methods for the\
    \ Detection of Plant Disease: The “Old Generation”\nAppropriate and reliable evaluation\
    \ of crops’ phytosanitary status, intended as the obser-\nvation of occurrence\
    \ and outbreak of plant diseases, is very important, as timely estimation of\n\
    disease incidence, symptom severity, and the resulting impacts on economically\
    \ important\ncrops is decisive for managing agronomical interventions such as\
    \ pesticide application time.\nThe methods for disease detection have been categorized\
    \ into direct and indirect methods [25],\nas shown in Figure 1. Direct methods,\
    \ known as “old generation” methods, include traditional\n(symptomology, microscopy,\
    \ and incubation method), molecular diagnostic methods (e.g.,\npolymerase chain\
    \ reaction (PCR), rapid fragment length polymorphisms (RFLP), real-time\nPCR,\
    \ loop-mediated isothermal amplification (LAMP), recombinase polymerase amplification\n\
    (RPA), and point-of-care diagnostic methods), and serological methods [25]. However,\
    \ due\nto their slowness and low capacity, these methods are not well-suited for\
    \ implementation in\nthe field, delaying early detection and response to disease\
    \ outbreaks. To effectively prevent\nand control future outbreaks, a quick and\
    \ high-throughput approach for the early detection\nof plant diseases must be\
    \ developed., Traditional methods usually follow the evaluation\nof characteristic\
    \ disease symptoms and visible signs of the pathogens. The evaluation of\ndisease\
    \ symptoms is performed by trained experts and can be affected by temporal variations.\n\
    Moreover, traditional methods strictly depend on individual experience, and these\
    \ methods\nbecome accurate and reliable only if the guidelines and standards for\
    \ assessment are properly\nfollowed. Microscopic identification depends on the\
    \ observation of pathogen inoculum (e.g.,\nmycelia, spores, and fruiting bodies).\
    \ For microscopic methods, specific dichotomous keys\nand identification manuals\
    \ are available; however, due to the need to cultivate the pathogens\non using\
    \ artificial selective media before proceeding to identification, this method\
    \ is too\ntime-consuming [26] (Figure 1).\nMolecular and serological methods are\
    \ commonly utilized in quarantines departments\nand research institutes for detecting\
    \ and identifying phytopathogens, and can be applied\ndirectly in the greenhouse\
    \ or the ﬁeld. For example, to assess the presence of the potato\nviruses Phytophthora\
    \ infestans, Ralstonia salanacarum, Ervinia amylovora, Papillus mosaic virus,\n\
    and Tomato Mosaic Virus, a lateral ﬂow-through version of ELISA is often used\
    \ [19]. The\nmajor drawbacks of molecular and serological methods are that they\
    \ are time-consuming\nand require trained operators; in addition, it should be\
    \ mentioned that the amount of\npathogen inoculum does not always positively correlate\
    \ with the severity of the disease.\nFurthermore, these methods are particularly\
    \ unreliable at the asymptomatic stages of plant\npathogens [27], even though\
    \ they are very sensitive, accurate, and effective; unfortunately,\nthey are unsuitable\
    \ for monitoring cryptic pathogens that have entered the plants before\nshowing\
    \ visible symptoms. On top of that, the sampling method from the ﬁeld to the\n\
    laboratory is laborious and should be properly sampled. Additionally, few diseases\
    \ can\nbe detected in only a few plants [5]. The advantages and disadvantages\
    \ of serological and\nmolecular assays have been displayed in Table 1.\nAgronomy\
    \ 2023, 13, 1524\n4 of 26\nTable 1. Advantages and disadvantages of molecular-based\
    \ and serological assays for disease detection and diagnosis.\nAdvantages\nDisadvantages\n\
    Molecular assays (Nucleic acid-based methods)\nRapidly and accurately detecting\
    \ and quantifying pathogen\nIt can be used in open ﬁelds, orchards, or greenhouses.\n\
    Capability to detect a single target in multiple targets\nRapid and speciﬁc detection\
    \ of multiple targets\nPotential to detect uncultivable pathogens such as viruses\
    \ or some bacteria\nand phytoplasma.\nEfﬁcient and speciﬁc\nSample preparation\
    \ is critical and requires reproducible and efﬁcient protocols.\nNot always effective\
    \ with all types of plant material\nUnreliable, particularly at pre-symptomatic\
    \ stages.\nFalse negatives (DNA target sequence is degraded or reagents are of\
    \ insufﬁcient quality)\nFalse positives (Small sample sizes may misrepresent the\
    \ real situation.\nand sample cross-contamination, dead pathogen)\nSensitivity\
    \ problems due to inhibitors of transcriptase and/or polymerases\nMis-priming\
    \ or primer dimerizations\nHigh cost of equipment and reagents\nTime-consuming\n\
    Unable to detect early infection\nSerological assays\nHigh throughput potential\n\
    Sensitive\nLow equipment costs\nGood reliability\nPolyclonal antisera cross-reactivity\n\
    Monoclonal antibodies recognize one epitome only and are generally more expensive.\n\
    Antibodies’ shelf life is short.\nTime-consuming\nLow potential for spatialization\n\
    False negatives\nAgronomy 2023, 13, 1524\n5 of 26\nAgronomy 2023, 13, x FOR PEER\
    \ REVIEW \n4 of 27 \n \n \n \nFigure 1. Available methods for plant disease detection:\
    \ (A,B) direct methods and (C,D) indirect \nmethods [5]. Direct methods include\
    \ traditional, serological, and molecular methods, while indirect \nmethods include\
    \ biomarker-based approaches such as metabolite proﬁling from plant pathogens\
    \ \nand plant interactions and stress-based approaches such as remote sensing\
    \ using drones. \nMolecular and serological methods are commonly utilized in quarantines\
    \ depart-\nments and research institutes for detecting and identifying phytopathogens,\
    \ and can be \napplied directly in the greenhouse or the ﬁeld. For example, to\
    \ assess the presence of the \npotato viruses Phytophthora infestans, Ralstonia\
    \ salanacarum, Ervinia amylovora, Papillus mo-\nsaic virus, and Tomato Mosaic\
    \ Virus, a lateral ﬂow-through version of ELISA is often used \n[19]. The major\
    \ drawbacks of molecular and serological methods are that they are time-\nconsuming\
    \ and require trained operators; in addition, it should be mentioned that the\
    \ \namount of pathogen inoculum does not always positively correlate with the\
    \ severity of \nthe disease. Furthermore, these methods are particularly unreliable\
    \ at the asymptomatic \nstages of plant pathogens [27], even though they are very\
    \ sensitive, accurate, and eﬀective; \nunfortunately, they are unsuitable for\
    \ monitoring cryptic pathogens that have entered the \nplants before showing visible\
    \ symptoms. On top of that, the sampling method from the \nﬁeld to the laboratory\
    \ is laborious and should be properly sampled. Additionally, few \ndiseases can\
    \ be detected in only a few plants [5]. The advantages and disadvantages of \n\
    serological and molecular assays have been displayed in Table 1.\nFigure 1. Available\
    \ methods for plant disease detection: (A,B) direct methods and (C,D) indirect\n\
    methods [5]. Direct methods include traditional, serological, and molecular methods,\
    \ while indirect\nmethods include biomarker-based approaches such as metabolite\
    \ proﬁling from plant pathogens and\nplant interactions and stress-based approaches\
    \ such as remote sensing using drones.\n2.2. Methods for the Detection of Plant\
    \ Disease: The “New Generation”\nIndirect methods, known as “New Generation”,\
    \ essentially exploit biomarker-based\ntechniques such as metabolite proﬁling\
    \ from plant–pathogen interactions as well as stress-\nbased detection techniques\
    \ such as imaging and spectroscopy using drones [3]. Recently,\nvarious indirect\
    \ methods have been launched, in particular drones, which can estimate\ndisease\
    \ more accurately compared to molecular, serological, and microbiological diagnostic\n\
    techniques [3]. Sensors have been mounted on drones to measure reﬂectance, tempera-\n\
    ture, or ﬂuorescence. Sensors of various types have been developed (RGB, multispectral,\n\
    hyperspectral, thermal, and ﬂuorescence), representing emerging tools for the\
    \ detection,\nidentiﬁcation, and quantiﬁcation of plant diseases, as shown in\
    \ Table 2 [11,28]. Sensors are\nthe key components of any drone that allow it\
    \ to navigate, detect, and locate potential crop\ndiseases from visual data and\
    \ to provide a map of the condition of the crops that could be\nuseful to farmers\
    \ or other machines collaborating with the drones to carry out various tasks\n\
    autonomously with little or no human involvement. The advantages and disadvantages\n\
    of various sensors mounted on drones are shown in Table 1. The accuracy and use\
    \ of\nmultispectral and hyperspectral images for disease diagnosis are greatly\
    \ improved. This is\nbecause of the sensitivity of spectral measurements to stress\
    \ and change during a crop’s\ndevelopment and with disease severity. Nonetheless,\
    \ implementing a hyperspectral data\nacquisition protocol in the ﬁeld presents\
    \ signiﬁcant challenges. Several elements might\naffect spectral reﬂectance, including\
    \ technical characteristics (resolution, brightness, etc.),\nsample preparation\
    \ circumstances (laboratory or ﬁeld), and sample characteristics (size,\ntexture,\
    \ humidity, etc.). More research into reﬂectance using crop vegetation indices\
    \ is\nneeded throughout crop development and infection. Thermal sensors are particularly\n\
    beneﬁcial in identifying plant diseases, complementing RGB and hyperspectral imaging.\n\
    The primary impetus is that leaf temperature is a useful indicator of plant health.\
    \ Because\nplant leaf acquisition requires people to drill down the whole ﬁeld\
    \ to acquire images, which\nis an energy- and time-consuming strategy, several\
    \ researchers have explored this type\nAgronomy 2023, 13, 1524\n6 of 26\nof imaging\
    \ for disease detection approaches at the leaf level, and others have combined\n\
    these images with multispectral data for effective early detection at the ground\
    \ vehicle and\naerial vehicle level. Drones have greatly aided the process of\
    \ agricultural monitoring at\nthe plot size, including identifying plant diseases.\
    \ For this, a drone equipped with many\ndifferent cameras was deployed. The captured\
    \ photos were used with machine learning\nalgorithms to classify crop health quickly\
    \ and accurately. Hence, drones are becoming\nmore common, as spectral imaging\
    \ with drones provides valuable information on soil\nand the top portion of plants\
    \ over a broad spectrum. Two basic categories can be used to\ncategorize remote\
    \ sensing systems based on camera sensors installed on drone platforms,\nnamely,\
    \ drone type and camera sensor type. Drone-based aerial imaging is one of the\n\
    most signiﬁcant and beneﬁcial data types that can help advance the agricultural\
    \ area. The\ngoal of the desired application and the crop type are typically considered\
    \ when selecting\ndrone platforms and sensor types [10,11]. These RS approaches\
    \ rely on the detection of\nany variation in the optical properties of plants;\
    \ in other words, they essentially detect\nany change in the plant physiology\
    \ that, due to biotic or abiotic stresses, transpiration\nrates, morphology, plant\
    \ density, and changes in solar radiation between plants, determines\nmeasurable\
    \ variations in plants optical output. Due to signiﬁcant advantages such as high\n\
    spatial resolution (compared to satellite RS), high efﬁciency, low cost, and ﬂexibility\
    \ of\nuse, RS platforms play an important role in the application of precision\
    \ agriculture. With\nthe help of this technique, plant diseases and disorders\
    \ can be detected at the ﬁeld level\npromptly and accurately, thereby improving\
    \ disease management efﬁcacy through the\nuse of site-speciﬁc applications of\
    \ fungicides [29]. Furthermore, the movement of plant\npathogens or their products\
    \ can be traced from tens to hundreds of meters above crop\nﬁelds [12], and numerous\
    \ plant disease images can be captured directly and in real-time,\nallowing application\
    \ of algorithms to monitor the occurrence of speciﬁc plant diseases\n(Table 2).\n\
    Drones equipped with sensors can measure spectral and morphological information\n\
    such as plant height and canopy surface proﬁling. The advantages and disadvantages\
    \ of\ndrone utilization in agriculture are presented in Figure 2. Moreover, at\
    \ high altitudes the\ncaptured images usually have low spatial resolution, making\
    \ it difﬁcult to detect features\nof disease lesions at the level of plant organs,\
    \ even though super-resolution methods\nhave recently been developed that can\
    \ produce a high-resolution image from one or more\nlow-resolution images [29]\
    \ (Figure 2).\nAgronomy 2023, 13, x FOR PEER REVIEW \n8 of 27 \n \nrecently been\
    \ developed that can produce a high-resolution image from one or more low-\nresolution\
    \ images [29] (Figure 2). \n \nFigure 2. Advantages and challenges/disadvantages\
    \ of using drones for plant disease detection [5]. \nPlant morphological information\
    \ is acquired through two main methods: LiDAR \n(Light Detection and Ranging)\
    \ [24] and Structure-from-Motion (SfM) photogrammetry. \nLiDAR calculates the\
    \ distance from the sensor to ground objects to measure their position; \nits\
    \ beams can pass through the crop canopy and send back information about its structure,\
    \ \nplant density, and the ground surface. SfM photogrammetry collects images\
    \ from multi-\nFigure 2. Advantages and challenges/disadvantages of using drones\
    \ for plant disease detection [5].\nAgronomy 2023, 13, 1524\n7 of 26\nTable 2.\
    \ Sensors mounted on drones for plant disease detection and monitoring.\nRemote\
    \ Sensors\nAdvantages\nDisadvantages\nDiseases\nDigital camera (RGB)\nVegetation\
    \ characteristics may capture grayscale or\ncolor pictures, and the visible spectrum\
    \ allows for\nimproved disease identiﬁcation at the leaves’ level.\nLightweight,\
    \ inexpensive, extremely easy to use,\nsimple data processing, and minimal work\n\
    environments.\nReduced number of spectral bands and\nvisibility of less light.\
    \ Vulnerable to\nenvironmental factors.\nCotton bacterial angular, Ascochyta\n\
    blight, grapefruit citrus canker, sugar\nbeet Cercospora leaf spot, rust, blights,\n\
    smuts, spots\nMultispectral camera\nLow cost, fast frame imaging and high, more\
    \ robust\nthan RGB cameras, work efﬁciency; electromagnetic\nspectrum ranging\
    \ from the visible to the\nNear-Infrared (NIR), allowing the calculation of\n\
    different robust vegetation indices; sensing and\nrecording radiations from the\
    \ visible and invisible\nportions of the electromagnetic spectrum.\nFew bands,\
    \ discontinuous spectrum, and low\nspectral resolution\nBlights, Blasts, viruses\n\
    Hyperspectral sensing\nCapable of sensing and recording a wide variety of\nnarrow\
    \ bands and continuous spectra, giving\nresearchers and farmers more insight into\
    \ the\nspectral properties of illnesses and crops\nmore expensive\nBlasts, Blights,\
    \ and nematodes, viruses,\nrots, scabs, rusts\nThermal infrared cameras (InfraRed\
    \ (IR)\nregion consists of several spectral bands,\nincluding Near InfraRed (NIR),\
    \ Short-Wave\nInfra-Red (SWIR), Mid-Wave InfraRed\n(MWIR), Long-Wave InfraRed\
    \ (LWIR), and\nFar InfraRed (FIR))\nSensitive to infrared spectrum, therefore\
    \ it may be\nused day or night and is able to provide more data\non plant health\
    \ than other sensors.\nProblems with the images’ temporal and\ngeographic resolutions;\
    \ issues with the\nweather and lighting; problems with the\nvariety of crop species\
    \ and their development\nstages; problems with the height at which the\nphotographs\
    \ were taken.\nRecently used to monitor diseases such\nas Cercospora leaf spot,\
    \ scab and mildews\nFluorescence imaging\nCan determine how plant responses to\
    \ various\nstresses affect photosynthesis.\nHas been used to detect a few diseases,\
    \ difficult\nto use in field and greenhouse conditions\nmildews, rust and cankers\n\
    References; [24–34]\nAgronomy 2023, 13, 1524\n8 of 26\nPlant morphological information\
    \ is acquired through two main methods: LiDAR (Light\nDetection and Ranging) [24]\
    \ and Structure-from-Motion (SfM) photogrammetry. LiDAR cal-\nculates the distance\
    \ from the sensor to ground objects to measure their position; its beams can\n\
    pass through the crop canopy and send back information about its structure, plant\
    \ density, and\nthe ground surface. SfM photogrammetry collects images from multiple\
    \ perspectives as drones\nfly over the fields; it utilizes high-resolution digital\
    \ cameras from which images can be used to\nmeasure such phenotypical characteristics\
    \ of the plant population as individual height, lodging,\ndevelopmental stages,\
    \ and yield. The spectral reflectance or radiance is an important indica-\ntor\
    \ for the detection of plant vigour, plant diseases, and soil properties [30,31].\
    \ Multispectral\n(usually from 3 to 6 spectral bands, from 0.4 to 1.0 µm) and\
    \ thermal cameras (commonly in the\n7–14 µm range) aboard drones can detect diseases\
    \ in the fields, monitor crop vigour, estimate\nbiomass and yield, and detect\
    \ symptoms of both abiotic and biotic stresses. Digital cameras can\ndetect one\
    \ or a few broad near-infrared (NIR) bands [32], while hyperspectral cameras (tens\
    \ to\nhundreds of spectral bands) measure narrow bands; despite having been reduced\
    \ for drone\nutilization, the latter require extra space and payload capacity\
    \ [8,33,34] (Table 3).\n2.3. The Operating Mechanism of Drones Used to Detect\
    \ Plant Diseases\nDrones are aerial robots that operate independently of a human\
    \ pilot. These aircraft\nmay be piloted by hand from a distance using remote control,\
    \ or they can complete missions\nindependently using a computer running Artificial\
    \ Intelligence (AI) programs. One of the\nmost transformative steps toward “precision\
    \ agriculture” is the widespread use of agricultural\ndrones. Drones can execute\
    \ flying missions at varying heights and viewing angles, allowing\nthem to survey\
    \ hazardous and challenging places previously inaccessible to manned aircraft\
    \ or\nsatellites. Many agricultural tasks, such as detecting and treating crop\
    \ diseases, have recently\nseen widespread use of various drone types fitted with\
    \ high-resolution video sensors. Drones\nhave been categorized into two major\
    \ types based on the movement of wings, i.e., fixed-wing\nand rotary-wing on the\
    \ one hand, and hybrid Vertical Take-Off and Landing (VTOL) drones\non the other\
    \ [24–28]. More advanced cameras and sensors are carried by fixed-wing and\nVTOL\
    \ drones than by multirotor rotary-wing drones, particularly when it comes to\
    \ heavy\nhyperspectral sensors. Drones with advanced cameras can help farmers\
    \ to increase crop\noutput while saving time and money by automating tasks that\
    \ previously required a team\nof people to complete. However, rotary multirotor\
    \ drones can fly at lower altitudes, and\ntheir cameras offer superior Ground\
    \ Sampling Distance resolution. The advantages and\ndisadvantages of both types\
    \ of drones for field-based agricultural applications are shown in\nTable 3. Drone\
    \ systems to detect plant diseases comprise four sections, as shown in Figure\
    \ 3. A\nmechanism depicting the structure and operational mechanism of drone technology\
    \ for plant\ndisease detection recommended by [35] for assessing plant diseases\
    \ is presented in Figure 4\n(Table 3, Figures 3 and 4).\nAs described above, various\
    \ sensors and global positioning system (GPS) capability are\ninstalled on drones\
    \ to capture images [5]. The plant disease detection and classification model\n\
    architecture consist of the following five steps: image acquisition, image preprocessing,\
    \ image\nsegmentation, feature extraction, and classification (Figure 5). Acquiring\
    \ relevant images is the\ninitial stage in crop leaf disease identification and\
    \ categorization. This step aims to amass the\nphoto dataset utilized later in\
    \ the procedure. The drones’ cameras are used for this purpose.\nBetter results\
    \ may be achieved with proper image preparation [5–10]. Image processing can be\n\
    employed to remove background noise. Digital cameras’ large file sizes necessitate\
    \ the use of\nshrinking methods, which additionally aids in making memory smaller.\
    \ The cropping of leaves\nfrom captured photos is one of the most common images\
    \ preprocessing procedures, along\nwith color changes, resizing, background removal,\
    \ enhancing, flipping, rotating, shearing, and\nsmoothing. Crop leaf disease detection\
    \ and categorization rely heavily on image segmentation.\nThe picture is segmented\
    \ into several areas. Through a deep dive into the picture data, relevant\ndetails\
    \ are found for feature extraction. There are two main approaches to image segmentation:\n\
    those that focus on similarities, and those that focus on discontinuities. Feature\
    \ extraction\ninvolves isolating certain aspects of an image’s content. Shape,\
    \ color, and texture are often\nAgronomy 2023, 13, 1524\n9 of 26\nused in plant\
    \ disease identification and categorization. Several categories of crop diseases\
    \ can\ncause visual differences in the resulting images. The technique for detecting\
    \ crop leaf diseases\nuses an image of crop leaves to quickly and accurately identify\
    \ the diseases present. The\nsecond distinguishing characteristic is its vibrant\
    \ hue, which serves to differentiate between the\nvarious crop leaf diseases.\
    \ The last characteristic, texture, shows how varied color patterns may\nbe seen\
    \ in pictures of crop leaves. Energy, entropy, contrast, correlation, the sum\
    \ of squares,\nsum entropy, cluster shadow, cluster prominence, and homogeneity\
    \ are all characteristics\nof textures. Crop leaf diseases are classified using\
    \ traditional machines and deep learning\nclassification techniques. The main\
    \ way in which deep learning differs from conventional\nmachine learning is in\
    \ the process of feature extraction. In contrast to deep learning, where\nfeatures\
    \ are extracted automatically and used as learning weights, traditional machine\
    \ learning\nmodels manually calculate features [9–11]. Traditional machine learning\
    \ and deep learning\nmodels are discussed below. The images are then analyzed\
    \ by software and can be used to\ncharacterize the evolution of plant disease.\
    \ Color conversion features can be used to convert\ncolored space from obtained\
    \ images to detect areas of quality in the study area. High-resolution\ndigital\
    \ cameras provide higher-resolution pixels larger than RGB [5], the images from\
    \ which\ncan be used to differentiate infected areas from healthy areas, while\
    \ multispectral cameras\n(five-channel devices that can measure plant reflectance\
    \ more accurately than three-channel\ndigital RBG cameras) offer raw images in\
    \ five narrow bands from the red edge of RBG to near-\ninfrared (NIR). On the\
    \ other hand, multispectral images obtained from multispectral cameras\ncan calculate\
    \ different image-based spectral indices such as normalized difference vegetation\n\
    index (NDVI), nonlinear index (NLI), green normalized difference vegetation index\
    \ (GNDVI),\nration vegetation index (RVI), difference vegetation index (DVI),\
    \ normalized difference water\nindex (NDWI), and red edge normalized difference\
    \ vegetation index (RENDVI) [36–38]. These\nindices can be used to quantify different\
    \ levels of disease severity in study fields with accuracy\ngreater than 60%.\
    \ Among all the indices mentioned above, NDVI is the most widely used, and\nis\
    \ directly correlated to plant condition, physiological stress, and photosynthetic\
    \ activity under\nstress conditions. NDVI change maps of different disease severity\
    \ levels were reported to be\napplied in the case of rice sheath blight disease\
    \ caused by Rhizoctonia solani at the field level [39];\nthe generated data illustrated\
    \ that multispectral imagery data could detect the symptoms and\ndevelopment of\
    \ the disease at field scale [40].\nAgronomy 2023, 13, x FOR PEER REVIEW \n10\
    \ of 27 \n \n \nFigure 3. Drone system to detect plant diseases comprising four\
    \ sections: data acquisition, data \npreparation, training, and prediction. The\
    \ system interacts with the target area (ﬁelds, farms, or \nforests) directly\
    \ or indirectly to obtain information. The information is preprocessed and arranged\
    \ \ninto features, then sent to a supervised machine learning classiﬁer that processes\
    \ the data and pro-\nvides prediction reports via segmented images. The data acquisition\
    \ process includes drones which \nindirectly acquire airborne data and an expert\
    \ who acquires direct ground data through visual as-\nsessment of the ﬁelds. Drones\
    \ consist of high-performance brushless rotors, speciﬁc load capacity, \nand dimensions.\
    \ They are controlled automatically using ground station software which controls\
    \ \nthe route, speed, and altitude at a distance. \nFigure 3. Drone system to\
    \ detect plant diseases comprising four sections: data acquisition, data prepara-\n\
    tion, training, and prediction. The system interacts with the target area (fields,\
    \ farms, or forests) directly\nor indirectly to obtain information. The information\
    \ is preprocessed and arranged into features, then\nsent to a supervised machine\
    \ learning classifier that processes the data and provides prediction reports\n\
    via segmented images. The data acquisition process includes drones which indirectly\
    \ acquire airborne\ndata and an expert who acquires direct ground data through\
    \ visual assessment of the fields. Drones\nconsist of high-performance brushless\
    \ rotors, specific load capacity, and dimensions. They are controlled\nautomatically\
    \ using ground station software which controls the route, speed, and altitude\
    \ at a distance.\nAgronomy 2023, 13, 1524\n10 of 26\nTable 3. Advantages and disadvantages\
    \ of using drones for agricultural applications [8,34].\nDrones\nAdvantages\n\
    Disadvantages\nRotary-wing (Multirotor)\n•\nTake off and land vertically\n•\n\
    Return home capability\n•\nHigh-detailed plant measurements\n•\nAutomatic recovery\
    \ capability\n•\nThe lift generated by batteries\n•\nHovering and ﬂying at low\
    \ altitudes\nDepending on multiple rotors e.g., three rotors (tri-copters), four\
    \ rotors\n(quad-copters), six rotors (hexacopters), and eight rotors (octocopters)\n\
    •\nSmall payload sensor capacity\n•\nMost of the capacity taken by batteries\n\
    •\nLow endurance\n•\nLow speed\n•\nThe area covered is limited\nFixed-wing.\n\
    •\nThe lift created by wings\n•\nLonger ﬂight\n•\nHigh speed\n•\nHigh endurance\n\
    •\nHigh payload sensor capacity\n•\nSome have automatic recovery capability\n\
    •\nFly a longer time and are more suitable for covering larger areas.\n•\nHigh\
    \ altitude\n•\nFly at an airspeed above their stall speed; sometimes problems\n\
    occur in generating the desired data about the crops\n•\nLow ﬂexibility makes\
    \ small crop monitoring very difﬁcult\n•\nRequire runways and space to land and\
    \ take off\nHybrid Vertical Take-Off and Landing\n(VTOL)\n•\nResolve issues with\
    \ multirotor and ﬁxed-wing drones\n•\nCombine ﬁxed-wing drone cruise ﬂight with\
    \ multirotor drone\nVTOL capabilities.\n•\nLonger ﬂight times, extensive coverage,\
    \ quick vertical takeoffs,\nand minimal energy requirements\n•\nExpensive and\
    \ hovering issues\nAgronomy 2023, 13, 1524\n11 of 26\nAgronomy 2023, 13, x FOR\
    \ PEER REVIEW \n11 of 27 \n \n \n \nFigure 4. The plant disease detection and\
    \ classiﬁcation model architecture consist of image acquisi-\ntion, image preprocessing,\
    \ image segmentation, feature extraction, and classiﬁcation. Sensors and \ncameras\
    \ are mounted on drones that acquire plant disease images. The images are then\
    \ processed \nand segmented to remove background noise. Then, features are extracted\
    \ and classiﬁed using tra-\nditional machine learning models such as K-nearest\
    \ neighbor (KNN) or support vector machine \n(SVM) and deep learning models such\
    \ as Convolutional Neural Network (CNN). Note: PCA, prin-\ncipal component analysis;\
    \ LESC, local embedding based on spatial coherence algorithm; DWT, dis-\ncrete\
    \ wavelet transform. \nAs described above, various sensors and global positioning\
    \ system (GPS) capability \nare installed on drones to capture images [5]. The\
    \ plant disease detection and classiﬁcation \nmodel architecture consist of the\
    \ following ﬁve steps: image acquisition, image prepro-\ncessing, image segmentation,\
    \ feature extraction, and classiﬁcation (Figure 5). Acquiring \nrelevant images\
    \ is the initial stage in crop leaf disease identiﬁcation and categorization.\
    \ \nThis step aims to amass the photo dataset utilized later in the procedure.\
    \ The drones’ cam-\neras are used for this purpose. Better results may be achieved\
    \ with proper image prepara-\ntion [5–10]. Image processing can be employed to\
    \ remove background noise. Digital cam-\neras’ large ﬁle sizes necessitate the\
    \ use of shrinking methods, which additionally aids in \nmaking memory smaller.\
    \ The cropping of leaves from captured photos is one of the most \ncommon images\
    \ preprocessing procedures, along with color changes, resizing, back-\nground\
    \ removal, enhancing, ﬂipping, rotating, shearing, and smoothing. Crop leaf disease\
    \ \ndetection and categorization rely heavily on image segmentation. The picture\
    \ is seg-\nmented into several areas. Through a deep dive into the picture data,\
    \ relevant details are \nfound for feature extraction. There are two main approaches\
    \ to image segmentation: those \nthat focus on similarities, and those that focus\
    \ on discontinuities. Feature extraction in-\nvolves isolating certain aspects\
    \ of an image’s content. Shape, color, and texture are often \nused in plant disease\
    \ identiﬁcation and categorization. Several categories of crop diseases \ncan\
    \ cause visual diﬀerences in the resulting images. The technique for detecting\
    \ crop leaf \nFigure 4. The plant disease detection and classiﬁcation model architecture\
    \ consist of image acqui-\nsition (A), image preprocessing (B), image segmentation\
    \ (C), feature extraction (D), and classiﬁca-\ntion (E). Sensors and cameras are\
    \ mounted on drones that acquire plant disease images. The images\nare then processed\
    \ and segmented to remove background noise. Then, features are extracted and clas-\n\
    siﬁed using traditional machine learning models such as K-nearest neighbor (KNN)\
    \ or support vector\nmachine (SVM) and deep learning models such as Convolutional\
    \ Neural Network (CNN). Note:\nPCA, principal component analysis; LESC, local\
    \ embedding based on spatial coherence algorithm;\nDWT, discrete wavelet transform.\n\
    Agronomy 2023, 13, x FOR PEER REVIEW \n12 of 27 \n \n \ndiseases uses an image\
    \ of crop leaves to quickly and accurately identify the diseases pre-\nsent. The\
    \ second distinguishing characteristic is its vibrant hue, which serves to diﬀeren-\n\
    tiate between the various crop leaf diseases. The last characteristic, texture,\
    \ shows how \nvaried color patterns may be seen in pictures of crop leaves. Energy,\
    \ entropy, contrast, \ncorrelation, the sum of squares, sum entropy, cluster shadow,\
    \ cluster prominence, and \nhomogeneity are all characteristics of textures. Crop\
    \ leaf diseases are classiﬁed using tra-\nditional machines and deep learning\
    \ classiﬁcation techniques. The main way in which \ndeep learning diﬀers from\
    \ conventional machine learning is in the process of feature ex-\ntraction. In\
    \ contrast to deep learning, where features are extracted automatically and used\
    \ \nas learning weights, traditional machine learning models manually calculate\
    \ features [9–\n11]. Traditional machine learning and deep learning models are\
    \ discussed below. The im-\nages are then analyzed by software and can be used\
    \ to characterize the evolution of plant \ndisease. Color conversion features\
    \ can be used to convert colored space from obtained \nimages to detect areas\
    \ of quality in the study area. High-resolution digital cameras pro-\nvide higher-resolution\
    \ pixels larger than RGB [5], the images from which can be used to \ndiﬀerentiate\
    \ infected areas from healthy areas, while multispectral cameras (ﬁve-channel\
    \ \ndevices that can measure plant reﬂectance more accurately than three-channel\
    \ digital RBG \ncameras) oﬀer raw images in ﬁve narrow bands from the red edge\
    \ of RBG to near-infrared \n(NIR). On the other hand, multispectral images obtained\
    \ from multispectral cameras can \ncalculate diﬀerent image-based spectral indices\
    \ such as normalized diﬀerence vegetation \nindex (NDVI), nonlinear index (NLI),\
    \ green normalized diﬀerence vegetation index \n(GNDVI), ration vegetation index\
    \ (RVI), diﬀerence vegetation index (DVI), normalized \ndiﬀerence water index\
    \ (NDWI), and red edge normalized diﬀerence vegetation index \n(RENDVI) [36–38].\
    \ These indices can be used to quantify diﬀerent levels of disease sever-\nity\
    \ in study ﬁelds with accuracy greater than 60%. Among all the indices mentioned\
    \ \nabove, NDVI is the most widely used, and is directly correlated to plant condition,\
    \ phys-\niological stress, and photosynthetic activity under stress conditions.\
    \ NDVI change maps \nof diﬀerent disease severity levels were reported to be applied\
    \ in the case of rice sheath \nblight disease caused by Rhizoctonia solani at\
    \ the ﬁeld level [39]; the generated data illus-\ntrated that multispectral imagery\
    \ data could detect the symptoms and development of the \ndisease at ﬁeld scale\
    \ [40]. \n \nFigure 5. Remote sensing using drone technology to determine plant\
    \ health status, such as canopy \nstructure (including leaf areas and orientation),\
    \ spatial arrangement, and roughness aﬀected by dis-\nease, as well as further\
    \ optical, thermal, and dielectric characteristics of vegetation. \nFigure 5.\
    \ Remote sensing using drone technology to determine plant health status, such\
    \ as canopy\nstructure (including leaf areas and orientation), spatial arrangement,\
    \ and roughness affected by\ndisease, as well as further optical, thermal, and\
    \ dielectric characteristics of vegetation.\nAgronomy 2023, 13, 1524\n12 of 26\n\
    Digital and multi-spectral cameras have been used to capture high-resolution im-\n\
    ages in ﬁeld areas. Color-infrared (CIR) images can be generated by drones to\
    \ support\ndecision-making followed by RGB images. Other types of images used\
    \ for disease detection\ninclude visible and near-infrared (V-NIR) images, thermal\
    \ images, and multispectral (MS)\nimages. Field-based images have been mostly\
    \ generated using drones, followed by leaf\nand plant-based images [39–41]. Before\
    \ data are collected, the optimum exposure time\nfor different cameras is chosen\
    \ based on weather conditions; actual parameters are set\naccording to program\
    \ instructions. Cameras are set in drones during ﬂights, usually at\naltitudes,\
    \ using one to cover all experimental plots and the other to cover speciﬁc plots\
    \ in\neach image. Drones are then directed to move along experimental plots, usually\
    \ with wind\ndirections and speciﬁc ﬂights at a certain speed, depending on the\
    \ speed and direction of\nthe wind. The weather conditions must be favorable for\
    \ a ﬂight to detect plant diseases\nmore accurately. Afterward, software, i.e.,\
    \ ENVI (Exelis Visual Information Solutions,\nBoulder, CO, USA) is used to acquire\
    \ color features from the images, then transform the\nimages into different color\
    \ spaces. Transformation can be used to improve the presentation\nof information,\
    \ allowing the transformed images can be interpreted more easily than the\noriginal\
    \ images [35]. The digital images (RBG bands) of different levels of plant disease\n\
    severity are transformed in hue, lightness, and saturation (HLS); the average\
    \ values of\nthe HLS are calculated along with the different vegetation indices\
    \ (VI) from the acquired\nimages. Afterward, VIs change maps of different levels\
    \ of disease severity are produced to\nillustrate the imagery data that could\
    \ detect the disease at a ﬁeld scale. Along with the aerial\nVI, the ground based\
    \ VI is calculated with special hand-held plant sensors. The function of\nsensors\
    \ is based on the fact that healthy green plant leaves absorb most of the red\
    \ light and\nreﬂect most of the infrared light. The relative strength of the detected\
    \ light directly indicates\nthe density of the foliage within the sensor’s view.\
    \ The more vigorous and denser the\nplants, the greater the differences observed\
    \ between the reﬂected light signals. The sensors\nare held at a certain level\
    \ above the canopy plants, with an oval ﬁeld of view covering\na certain area.\
    \ Multiple readings are taken to increase the accuracy of vegetative indices\n\
    values. The average VI values are calculated from both the healthy and diseased\
    \ areas. On\nthe same day, the disease’s severity is rated on a scale based on\
    \ the symptoms of the disease.\nMoreover, special software (i.e., Pix4D mapper)\
    \ is used to process the images. Software\nsuch as ArcGIS is utilized for geospatial\
    \ data analysis and mapping [5,41,42]. Effective\nalgorithms are required for\
    \ the analysis of the images gathered by the drones. Traditional\nmachine learning\
    \ methods include nine different types of machine learning classiﬁers\nused to\
    \ create models for early detection of disease: k-nearest neighbors (k-NN), support\n\
    vector machine (SVM), Gaussian processing, decision tree, random forest, and multilayer\n\
    perceptron artiﬁcial neural network (MLP-ANN). However, traditional machine learning\n\
    methods have many shortcomings due to their reliance on manual feature extraction\
    \ meth-\nods, which is especially ineffective in complex environments. Deep learning\
    \ algorithms\nhave recently emerged as a promising new alternative to enhance\
    \ computer vision-based\nsystems for autonomous crop disease monitoring. Without\
    \ any human assistance, they can\nperform autonomous feature extraction, providing\
    \ farmers with data that can improve crop\nyields and decrease treatment costs.\
    \ Therefore, a solution for early crop disease detection\ncould be combining modern\
    \ drones, cameras, sensor technologies, and deep learning algo-\nrithms. Deep\
    \ learning algorithms include state-of-the-art deep learning object identiﬁcation\n\
    algorithms such as You Only Look Once version 3 (YOLOv3) and Faster Region-based\n\
    Convolutional Neural Network (CNN). Convolutional Neural Networks (CNNs) have\
    \ been\naround since the development of the AlexNet architecture in 2012. Numerous\
    \ CNN-based\ndeep learning algorithms and architectures are currently in use for\
    \ disease detection and\nclassiﬁcation in various crop systems. Current plant\
    \ disease categorization methods make\nextensive use of well-established CNN architectures\
    \ in computer vision, such as AlexNet,\nGoogleNet, VGGNet, ResNet, and EfﬁcientNet\
    \ [41–50].\nTherefore, a solution for early crop disease detection could be combining\
    \ modern\ndrone camera sensor technologies and deep learning algorithms [19–22].\
    \ To better identify\nAgronomy 2023, 13, 1524\n13 of 26\nplant diseases quickly\
    \ and accurately, scientists are actively studying how to use computer\nvision\
    \ methods, deep learning algorithms, and drone platforms dedicated to diagnosing\n\
    plant diseases. Several ﬁelds, including agriculture, electronic control, remote\
    \ sensing\ntechnologies, computer vision, and artiﬁcial intelligence, must be\
    \ addressed to enable\ndrone platforms to achieve autonomous detection and treatment\
    \ of crop diseases [19–21].\nThus, integrating modern drone camera sensor technologies\
    \ with deep learning algorithms\nmay provide a useful answer for the timely diagnosis\
    \ of crop diseases (Figure 4).\n2.4. Novel Approaches to Detecting Plant Diseases,\
    \ including RS Combined with Drones\nPlants can be affected simultaneously by\
    \ several plant pathogens, such as nematodes,\nfungi, viruses, viroids, bacteria,\
    \ and phytoplasmas. Recent novel approaches have been\nused that can rapidly,\
    \ easily, and reliably detect plant pathogens at pre-symptomatic to\nearly stages\
    \ of plant diseases, when symptoms are unclear and appear on few plants. This\n\
    method includes Lateral ﬂow microarrays [43], Analysis of Volatile Organic Compounds\n\
    (VOCs) as biomarkers [44], Remote sensing (RS) drone usages [45], electrochemistry\
    \ [46],\nPhage display [47], and biophotonics [48].\nLateral ﬂow microarrays (LFM)\
    \ are a hybridization-based nucleic acid detection\nmethod that uses an easily\
    \ visualized calorimetric signal to detect plant pathogens rapidly.\nHowever,\
    \ this method depends on the availability of strong and reliable host and pathogen\n\
    biomarkers discovered through transcriptomics and metabolomics approaches [49].\
    \ A\nclass of interesting plant metabolites highly suitable for plant health evaluation\
    \ are Volatile\nOrganic Compounds (VOCs) as biomarkers; plants are known to release\
    \ VOCs into their\nimmediate proximity for most various biological and ecological\
    \ purposes, with these com-\npounds being responsible for growth, defence, survival,\
    \ and intercommunication with\nother surrounding and/or associated organisms [28].\
    \ Representing a mediation tool for\nplant-to-plant and plant-to-pathogen communication,\
    \ VOCs released from the leaf sur-\nfaces are known as terminal metabolites and\
    \ can reﬂect the physiological status of plants.\nHowever, a single VOC biomarker\
    \ is insufﬁcient to represent a speciﬁc plant disease [49].\n•\nOther techniques\
    \ include electrochemistry, biophotonic and phage display. Phage\ndisplay technology\
    \ identiﬁes ligands that connect to speciﬁc biological molecules.\nThe ligands\
    \ can be used as antigens or immunogens to diagnose plant diseases. The\nligands\
    \ may be peptides or antibody fragments. The other methods, electrochemistry\n\
    and biophotonics, are based on signal transduction and biorecognition principles.\n\
    Optical biosensors are based on the absorption or emission of light due to biological\n\
    or chemical reactions. However, electrochemical biosensors are based on biochemical\n\
    reactions that cause electron transfer in plant sap or any other solution. Environmental\n\
    factors do not usually inﬂuence electrochemical biosensors. The underlying principle\n\
    of these plant disease detection methods is the recognition of a speciﬁc antigen\
    \ by\na speciﬁc antibody to form a stable complex, as with other serological assays\
    \ [50].\nThese biophotonic-based sensors can be used to rapidly detect plant disease\
    \ at the\nasymptomatic stage in the orchards and ﬁeld conditions. Moreover, they\
    \ could be\nintegrated into other plant disease detection methods using drones.\
    \ However, they\nare not easily available in the market.\n•\nAs previously reported,\
    \ remote sensing (RS) is based on measuring electromagnetic\nradiations reﬂected/backscattered\
    \ or emitted from the surface target object. The infor-\nmation is obtained without\
    \ any physical contact with the targeted object. Therefore,\nRS measurements are\
    \ known as non-contact measurements [45]. Hence, RS is a non-\ncontact technique.\
    \ Therefore, portable tools and various platforms such as drones\nwhich sense\
    \ the plants health and retrieve information are being used. To sense infor-\n\
    mation about plants’ health, passive sensors are being widely used. Active sensors\n\
    measure the reﬂected radiations from diseased plants, while passive sensors measure\n\
    the reﬂected solar radiation in the electromagnetic spectrum’s visible, near-infrared,\n\
    and shortwave regions. Hence, RS is used to monitor the changes in plant health.\n\
    Because plant leaves not only reﬂect radiations, transmit or absorb but release\
    \ energy\nAgronomy 2023, 13, 1524\n14 of 26\nby ﬂuorescence [51] or thermal emission\
    \ [52], different pigments found in plants ab-\nsorb radiation in speciﬁc parts\
    \ of the electromagnetic spectrum; for example, plant\npigments in chlorophyll\
    \ absorb radiation in the visible spectrum from 400–700 nm.\nTherefore, there\
    \ is an inverse relationship between the amount of radiation reﬂected\nfrom plants\
    \ and the amount of radiation absorbed by the plant pigments. When the\nplant\
    \ is infected by a pathogen or under abiotic stress conditions, variables such\
    \ as leaf\narea index (LAI), chlorophyll content, or surface temperature change.\
    \ These changes\nare called spectral signatures, and vary from the signatures\
    \ of healthy and unstressed\nplants [53,54]. However, RS presents drawbacks as\
    \ well; high costs for drones, and\nspecialized experts are required to gather\
    \ and process plant disease data. Moreover,\nthough protocols are available, they\
    \ are concentrated on only a few diseases of valu-\nable crops. Recently, the\
    \ spatial resolution of satellite sensors has been increased and\nthe cost of\
    \ acquisition of plant disease data has decreased, making RS a promising\ntool\
    \ for integration with traditional plant disease methods. Today, small, inexpensive,\n\
    high-resolution spatial and spectral sensors have been mounted on drones for crop\n\
    disease monitoring at the farm scale [6,55,56]. Hence, drone imaging offers interesting\n\
    advantages over RS. Acquiring images using drones has become a common practice\n\
    because installing onboard digital cameras is very easy [56].\nIn summary, methods\
    \ of crop disease monitoring are being improved through differ-\nent RS technologies.\
    \ Integrating drone-mounted spectral sensor data with spectroscopy,\nﬂuorescence,\
    \ and thermal imaging data, along with other non-RS based methods to provide\n\
    more accurate and fruitful plant disease detection and diagnosis, remains a work\
    \ progress.\n3. Applications of Drones for Plant Disease Detection\nResearchers\
    \ are investing in identifying infected and uninfected leaves as well as\nin categorizing\
    \ various disease severity degrees with visual symptoms even before the\nmanifestation\
    \ of visual symptoms [45]. Modern methods for disease identiﬁcation make\nuse\
    \ of machine learning algorithms to sift through information gathered through\
    \ a variety\nof acquisition methods. For the goal of disease identiﬁcation, using\
    \ drones with traditional\nmachine learning methods and deep learning models are\
    \ discussed below.\n3.1. Traditional Learning Models Used to Identify Plant Diseases\
    \ with Drones\nTraditional machine-learning techniques are applied for plant disease\
    \ identiﬁcation\nutilizing drone images. Backpropagation NN (BPNN) was an early\
    \ model that was used to\napply spectral data collected from remote sensing hyperspectral\
    \ photographs of tomato\nplants to estimate infection severity on plant leaves\
    \ from photos. A ﬁve-stage rating\nsystem was then used to assess the severity\
    \ of the light blight in the photos and test the\nBPNN using that information.\
    \ The ﬁndings supported the feasibility of using ANN with\nbackpropagation for\
    \ spectrum prediction for disease diagnosis. The authors of [56] made\nsimilar\
    \ efforts to use the Classiﬁcation and Regression Tree model to identify leafroll\
    \ illness.\nTheir strategy relied on analyzing hyperspectral photos of grapevines\
    \ taken by drones.\nSimilarly, the authors of [57] used multispectral photos taken\
    \ by drones to extract spectral\nbands, vegetation indicators, and biophysical\
    \ properties of both damaged and healthy\nplants. Due to their high accuracy in\
    \ making predictions, SVM models are widely utilized\nin the ﬁeld of plant disease\
    \ diagnosis. SVM was used for close-range hyperspectral imaging\nof barley to\
    \ identify drought stress at an early stage. Red Edge Normalized Difference\n\
    Vegetation Index (RENDVI) and Plant Senescence Reﬂectance Index (PSRI) were used\
    \ in\nthe model’s training process. Misclassiﬁcation can be further minimized\
    \ by the use of\nseveral SVM classiﬁers based on color, texture, and shape features\
    \ for disease identiﬁcation\non plant leaves [46–49].\nRemote sensed data were\
    \ initially used by the University of North Dakota, USA, where\nfarmers were involved\
    \ in verifying the effectiveness of fungicide applications against plant\ndiseases\
    \ in a sugar beet crop [57]. Moreover, the loss due to accidental spray drift\
    \ of\nfungicides was quantiﬁed. Remote sensor data have been used to investigate\
    \ physical\nAgronomy 2023, 13, 1524\n15 of 26\ndamage due to pests, inundation,\
    \ wind, and hail as well. In this instance, growers and\nranchers in rural areas\
    \ were connected via satellite. Farmers were given a ﬁrst-time\nopportunity to\
    \ use high-resolution imagery, which allowed them to identify crop stress\ndue\
    \ to diseases, pests, and damage. The information obtained was then used to draw\n\
    boundaries around crop stress areas.\nIn another study, it was found that an accurate\
    \ survey of the damage on sugar beets\nusing ground-based was not possible, as\
    \ sugar beet is more susceptible to disease at a critical\ngrowth stage of plants.\
    \ Therefore, multispectral satellite image data were used to obtain\nan immediate\
    \ and reliable estimate of the damage and reduction in sugar content caused\n\
    by plant diseases. The satellite images were used to assess the variations within\
    \ the ﬁeld\nbefore the damage inﬂicted on the plants by plant diseases, pests,\
    \ or other abiotic factors.\nThe results revealed the importance of high-resolution\
    \ imagery for timely assessment of\ndamage due to both biotic and abiotic factors\
    \ [57].\nThe concept of using high-resolution imagery from drones to capture images\
    \ of\nhealthy and diseased plants has evolved recently. Rice sheath blight is\
    \ a major disease in\nrice worldwide. In one study, drones equipped with digital\
    \ and multispectral cameras\ncaptured images of research plots with 67 cultivars\
    \ and a few elite lines. The ground-based\nnormalized difference vegetation index\
    \ and image-based normalized difference vegetation\nindex were calculated, and\
    \ the relationship between the two NDVI data indices showed\na strong correlation.\
    \ Multispectral images were then used to quantify the different levels\nof rice\
    \ sheath blight disease in ﬁeld plots with an accuracy higher than 60%. The results\n\
    indicated that drones with digital and multispectral cameras are the most effective\
    \ tool\nfor detecting rice sheath blight disease in the ﬁeld [5]. Researchers\
    \ in the USA are now\nusing drones to detect Septoria wheat fungus before any\
    \ appearance of symptoms or signs,\nwhich allows farmers to stop infections in\
    \ their tracks. The cameras mounted on the drones\ncan automatically detect the\
    \ early stages of Septoria wheat fungus. These data can be\nused to inform farmers\
    \ when to spray before the Septoria fungus damages the wheat\ncrop. The diseased\
    \ plants display a unique spectral signature that distinguishes them from\nSeptoria-free\
    \ plants [39,58,59].\nA study of multispectral detection of fungal diseases in\
    \ barley was conducted using\ndrone imagery. For this, two farmers’ ﬁelds and\
    \ four growth stages of barley (Feekes 8 to\nFeekes 11.4) were involved in determining\
    \ the spectral response (VI) of different barley\nfungicide treatment levels (Control,\
    \ Stratego, Stratego + Prosaro) using multispectral drone\nimagery (Blue 475 nm\
    \ ± 20 nm; Green 560 nm ± 20 nm; Red 668 nm ± 10 nm; Red Edge\n717 nm ± 10 nm;\
    \ Near-Infrared 840 nm ± 40 nm) with a 6.7 cm/pixel spatial resolution.\nAmong\
    \ the ﬁve vegetation indices (NDVI, RE-NDVI, RDVI, RE-RDVI, and TGI), three-way\n\
    interactions (Field × Growth Stage × Treatment) were found to be non-signiﬁcant\
    \ for NDVI\n(p = 0.415), RE-NDVI (p = 0.383), and TGI (p = 0.780), while RDVI\
    \ (p = 0.003) and RE-RDVI\n(p = 0.005) were signiﬁcant. Moreover, a consistent\
    \ trend in the spectral separability of\nfungal severity by the treatment type\
    \ was observed. Tracking the fungicide intensity was\nmade possible through mapping\
    \ and comparison with ground truth in order to save the\nenvironment from the\
    \ overuse of fungicides [58].\nIn another study, super-resolution to low-resolution\
    \ images from drones were used\nto address tomato disease. About fifty thousand\
    \ images of fourteen crops including\ntomato as target crop were obtained. Images\
    \ of eight kinds of disease caused by plant\npathogens were obtained, i.e., Xanthomonas\
    \ campestris pv. vesicatoria, Alternaria solani,\nPhytophthora infestans, Septoria\
    \ lycopersici, Tomato mosaic virus, Fulvia fulva, Corynespora\ncassiicola, and\
    \ Tomato yellow leaf curl virus. Moreover, symptoms such as lesions on plant\n\
    organs were obtained. Diseases of tomatoes were classified into three categories,\
    \ i.e.,\nhigh-resolution, low-resolution, and super-resolution images. The results\
    \ indicated that\nsuper-resolution methods are more effective than conventional\
    \ image scaling methods\nbecause they enhance the spatial resolution of disease\
    \ images [60]. To check the field re-\nsistance of potatoes against late blight\
    \ diseases, other researchers used a disease severity\nscale to visually examine\
    \ the lesion size or infection on the leaves. This visual assessment\nAgronomy\
    \ 2023, 13, 1524\n16 of 26\nis generally a time-consuming process that results\
    \ in only a tentative guess [61]. To avoid\nthe visual assessment technique, [62]\
    \ developed a new technique for estimating late\nblight disease severity under\
    \ field conditions using digital RBG cameras from a drone.\nVarious potato cultivars\
    \ and lines were planted in 262 experimental plots for assessment\nof the disease\
    \ resistance of potatoes under field conditions. Along with the conventional\n\
    visual assessment of disease severity using late blight disease severity, eleven\
    \ aerial im-\nages of the field were obtained. A special image processing protocol\
    \ was developed for\nthe study to estimate the disease severity. Further, the\
    \ estimation method was designed\nsuch that the error of the severity estimated\
    \ by image processing could be minimized\nwhen compared with the visual assessment.\
    \ The area under the disease progress curve\nwas then compared with that from\
    \ the visual assessment and time series of images, and\nthe coefficient of determination\
    \ was found to be higher than 0.7. Following year eleven,\nimages of the field\
    \ were obtained, and surprisingly, the coefficient of determination was\nagain\
    \ higher than 0.7. These results lead to the conclusion that the correlations\
    \ are valid\nand that image acquisition using drones followed by disease severity\
    \ estimations from\nthese images is a more effective method than the conventional\
    \ visual assessments. In\nconclusion, the aerial imagery was precise and objective\
    \ and allowed high throughput\nconcerning field resistance to late blight disease.\n\
    Another study used high-resolution aerial imaging for Huanglongbing (HLB) or citrus\n\
    greening disease detection using drones [63]. A multi-band imaging sensor was\
    \ connected\nto drones at the desired resolution by adjusting the ﬂying altitude\
    \ used to acquire images.\nThe results achieved with drone-based sensors were\
    \ then compared with aircraft-based\nsensors with lower spatial resolution. The\
    \ data consisted of seven vegetation indices (Vis)\nand six spectral bands with\
    \ a wavelength range from 530 to 900 nm. Regression analysis\nwas used to obtain\
    \ relevant features from the drone-based and aircraft-based spectral\nimages.\
    \ The results revealed that high-resolution aerial sensing is a reliable method\
    \ for\ndetecting HLB-infected citrus trees.\nIn Japan, viral-infected potato tubers\
    \ were taken from the field to provide certified\nseed tubers in potato seed production\
    \ areas. The farmers inspected the whole field for\ninfected potato plants based\
    \ on virus-induced visual symptoms. This is time-consuming,\nand plants showing\
    \ cryptic or mild symptoms are very difficult to identify. Japanese sci-\nentists\
    \ have devised an alternative way to detect diseased potato plants effectively.\
    \ They\nused the image classification technique as a detection method for virus-infected\
    \ plants,\nusing drones to obtain RGB images at an altitude of 5 to 10 m from\
    \ the ground. A total\nof 1300 images of healthy and 130 images of infected potato\
    \ plants were collected. They\nrotated the original images in order to increase\
    \ the number of images of infected plants\nto 1300, for a total of 2600 images\
    \ which include equal numbers of infected and healthy\nplants. Of these, they\
    \ used 1800 images for the training set and the remaining 800 images\nas the validation\
    \ set. Moreover, a convolution neural network (CNN) was used for\nclassifying\
    \ the images as infected or healthy plants. The accuracy of classification on\
    \ the\ntraining data was about 96% while the classification accuracy of the validation\
    \ data was\n84% [62].\nOther researchers have compared spectral, hyperspectral,\
    \ canopy height, and tem-\nperature information derived from handheld and drone-mounted\
    \ sensors to discriminate\nfour soybean cyst nematode (SCN) susceptible and tolerant\
    \ cultivars. The spectral indices\n(SIs) used to differentiate the cultivars were\
    \ chlorophyll, nitrogen, and water contents. In\nthe advanced stages, when SCN\
    \ infection becomes severe, the discrimination between the\ncultivars was found\
    \ to be more prominent. In addition, canopy height allowed for more\neffective\
    \ differentiation between the cultivars using drones than manual ﬁeld assessment.\n\
    Canopy temperature and SIs were used to classify the cultivars according to their\
    \ ability to\nwithstand SCN. A high correlation was found between SIs and ﬁnal\
    \ sugar beet yield. These\nresults prove that the drone hyperspectral imaging\
    \ approach is suitable for the detection of\nplant diseases caused by nematodes\
    \ [63,64].\nAgronomy 2023, 13, 1524\n17 of 26\nIn September 2018, researchers\
    \ from New Mexico State University used multispectral\ncameras to detect plant\
    \ stresses in ﬁelds and parks during a drone ﬂight. They obtained red\nand near-infrared\
    \ spectral bands, and from these bands, they calculated NDVI, which is a\nplant\
    \ stress metric. The NDVI ranges were from −1 to 1. NDVI values closer to 1 indicate\n\
    that a plant is green and healthy, while NDVI values closer to −1 indicate that\
    \ a plant is\nstressed, resulting in green images for healthy plants and red images\
    \ for stressed plants.\nThe researchers collected data by ﬂying a drone 20 m above\
    \ the ﬁeld at about ﬁve miles\nper hour during summer, looking for plants stressed\
    \ from root-eating nematodes. Before\nthe ﬂight, the drones were programmed by\
    \ autopilot software with GPS coordinates, and\ncameras took a shot every ﬁve\
    \ meters. There were a total of 60 shots which generated\n300 images; each time\
    \ the camera triggered, ﬁve images were captured. The images were\nprocessed with\
    \ Pix4D post-processing software for drone-based imagery, and NDVI values\nwere\
    \ obtained. Moreover, ground NDVI values have been collected using a hand-held\n\
    NDVI meter [8,34,65]. Soybean is an important agricultural commodity in Brazil\
    \ that\nsuffers from various foliar diseases caused by fungi, bacteria, viruses,\
    \ and nematodes.\nThese diseases have considerably reduced soybean production\
    \ in different states. The\nauthors of [20] designed a computer vision system\
    \ to monitor these foliar diseases in the\nﬁeld. The images were captured by drones,\
    \ than a computer vision system based on the\nSimple Linear Iterative Clustering\
    \ (SLIC) superpixel algorithm was used to detect plant\nleaves in the images.\
    \ The SLIC group pixel (SLIC superpixels method) is also known as\nthe SLIC segmentation\
    \ method. Then, a dataset was obtained from the captured images.\nFinally, features\
    \ were extracted and foliar diseases were classiﬁed based on the super-pixel\n\
    segment. Each super-pixel segment was associated with a speciﬁc class of foliar\
    \ disease or\nhealthy leaf samples.\nDrones are being used to monitor physiological\
    \ stress and disease outbreaks in forest\ntrees as well. The authors of [66] used\
    \ drones to monitor a disease outbreak in mature\nPinus radiate. A time-series\
    \ multi-spectral camera was mounted on drones and ﬂown over\nan herbicide-applied\
    \ pine forest area at regular intervals. Meanwhile, a forest experiment\ncarried\
    \ out a traditional ﬁeld-based assessment of crown and needle discoloration. The\n\
    results revealed that multi-spectral imagery collected from drones was very useful\
    \ in\nidentifying physiological stress in mature pine trees during the earliest\
    \ stages. Physiological\nstress was detected over the red edge and near-infrared\
    \ bands. Furthermore, NDVI was\nfound to be an effective vegetation index for\
    \ the detection of discoloration caused by\nphysiological stress over time.\n\
    Red band needle blight is a very severe disease of pine trees in the United Kingdom\n\
    (UK). The authors of [67] used a ﬁxed-wing drone (Quest UAV Ltd., Amble, UK) equipped\n\
    with thermal sensors to monitor red band needle blight (Dothistroma pini Hulbary)\
    \ disease-\ninduced canopy temperatures in ﬁve research plots of Scots pine and\
    \ lodgepole pine within\nQueen Elizabeth Forest Park in central Scotland. The\
    \ drone was equipped with a TIR PI450\ncamera (Optris GmbH, Berlin, Germany) and\
    \ a VNIR DMC-LX5 digital camera (Panasonic\nLtd., Osaka, Japan). In summer 2014,\
    \ TIR and NIR datasets were collected above the forest\nplots. Data regarding\
    \ ALS, hyperspectral, and thermal data were obtained by the Natural\nEnvironment\
    \ Research Council Airborne Research and Survey Facility. Experts assessed\nthe\
    \ severity of Red Band Needle Blight in the ﬁeld by visually assessing the symptoms.\
    \ For\nTIR and VNIR imagery obtained from the drone, a maximum local ﬁlter was\
    \ applied to the\nALS CHM, which allowed the identiﬁcation of treetops and tree-to-tree\
    \ registration. Six\ncentral pixels from each crown tree were averaged, and the\
    \ canopy temperature of trees\nwas extracted. The values were then compared with\
    \ the severity levels measured visually\nin the ﬁeld. A moderate positive correlation\
    \ was found between tree temperature and\ndisease progression [68].\nMyrtle rust\
    \ caused by Austropuccinia psidii causes signiﬁcant losses to paperbark tea\n\
    trees in New South Wales Australia. For aerial mapping of paperbark tea trees,\
    \ drones con-\ntaining hyperspectral image sensors were integrated with data processing\
    \ algorithms using\nmachine learning. Headwall Nano-Hyperspec R cameras were mounted\
    \ on the drones,\nAgronomy 2023, 13, 1524\n18 of 26\nand imagery was obtained.\
    \ The imagery was processed in Python programming language\nusing eXtreme Gradient\
    \ Boosting (XGBoost) with the Geospatial Data Abstraction Library\n(GDAL) and\
    \ Scikit-learn third-party libraries. About 11,385 samples were extracted and\n\
    assigned to ﬁve classes, divided into three classes for background objects and\
    \ two classes\nfor deterioration status. The results revealed that the individual\
    \ detection rate was 97.24%\nfor healthy trees and 94.72% for affected trees,\
    \ while the multiclass detection rate was\n97.35%. The methodology was useful\
    \ for the acquisition of large datasets with freeware\ntools using drones [69].\n\
    Grapevines in European vineyards are affected by Flavescence dorée (FD) and\n\
    Grapevine Trunk Disease (GTD). These diseases cause damage to vineyards. To detect\n\
    symptomatic vines, multispectral drone imagery can be a powerful tool. However,\n\
    different kinds of diseases produce similar leaf discoloration, as is the case\
    \ with the\nabove diseases in red vine cultivars. The authors of [70] evaluated\
    \ the potential of drones\nto distinguish between symptomatic and asymptomatic\
    \ vines. The study was conducted\nin the southern regions of France. Seven vineyards\
    \ and five different red vine cultivars\nwere selected. Drones acquired multispectral\
    \ images using the MicaSenseRedEdge®\nsensor. The images were processed to obtain\
    \ surface reflectance mosaics at 0.10 m\nground spatial resolution. About 24 variables\
    \ were selected, including five spectral\nbands, fifteen vegetation indices, and\
    \ four biophysical parameters. Vegetation indices\ndifferentiated abnormal vegetation\
    \ behavior with stress and diseases. Leaf pigment\ncontents such as chlorophyll,\
    \ carotenoids, and anthocyanin were the major biophysical\nparameters. The best\
    \ vegetation indices and biophysical parameters were found to be\nthe Red–Green\
    \ Index (RGI)/Green–Red Vegetation Index (GRVI) (based on the green\nand red spectral\
    \ bands) and Car (linked to carotenoid content). These variables were\neffective\
    \ in mapping vines with a disease severity higher than 50%. Currently, the\nauthors\
    \ of [71] have proposed a protocol for using a Multi-Spectral (MS) imaging device\n\
    to detect grapevine diseases. If embedded in a drone, this tool can provide disease\n\
    outbreak locations in a geographical information system, allowing localized and\
    \ direct\ntreatment of infected vines.\nStudies focusing on the use of drone imagery\
    \ to describe changes in crops due\nto diseases remain lacking. The researchers\
    \ in [72] evaluated late blight (Phytophthora\ninfestans) incidence in potato\
    \ using the description of spectral changes related to the\ndevelopment of late\
    \ potato blight under low disease severity levels. For this, they\nused sub-decimeter\
    \ drone optical imagery. The study’s main objective was to acquire\ninformation\
    \ regarding early changes in the potato crop with disease incidence. The drone\n\
    images were obtained on four dates during the growing season pre- and post-detection\n\
    of late blight disease in the field. Simplex Volume Maximization (SiVM) was used\
    \ to\nsummarize the spectral variability.\nMoreover, the relationship with the\
    \ different cropping systems and disease severity\nlevels was established based\
    \ on the pixel-wise log-likelihood ratio (LLR) calculation. It was\nfound that\
    \ considerable spectral changes were related to late blight incidence in different\n\
    cropping systems and the disease severity levels of affected potato plants. In\
    \ conclusion,\ntraditional machine learning algorithms have limitations, and performance\
    \ can readily\nchange across different growth periods and acquisition equipment.\
    \ The feature engineering\nprocess, which causes substantial data loss, may further\
    \ contribute to poor performance.\nMore information on the reported use of drones\
    \ for detection of plant diseases along with\nrecent and past reports regarding\
    \ the sensors/cameras/other devices mounted on drones\nfor the detection of plant\
    \ diseases is shown in Table 4.\nAgronomy 2023, 13, 1524\n19 of 26\nTable 4. Use\
    \ of drones and other devices for plant disease detection.\nS. No.\nDrones\nSensors/Cameras/Other\
    \ Devices\nDisease\nReferences\n1\nDT-18 UAV platform\nMicaSense RedEdge® sensor\n\
    Flavescence dorée (FD) and\nGrapevine Trunk Diseases (GTD)\n[70]\n2\nHexa-rotor\
    \ DJI S800 EVO\nHeadwall Nano-hyperspectral\nMyrtle rust\n[35]\n3\nPhantom 2 Vision+\n\
    Micasense RedEdgeTM\nRice sheath blight\n[5]\n4\nMikrokopter OktoXL\nMultispectral\
    \ camera\nGrapevine leaf stripe disease (GLSD)\n[73]\n5\nLong-range DT-18\nMicaSense\
    \ RedEdgeTM sensor\nGrapevine Disease\n[71]\n6\nHiSystems GmbH Mikrokopter\nSony\
    \ NEX-5N\nPotato late blight\n[61]\n7\nMX-SIGHT\nMulti-spectral sensors\nDowny\
    \ mildew of opium\n[74]\n8\nCoaxial quad-copter\nMicaSense RedEdge 3 camera\n\
    Physiological stress in pine trees\n[75]\n9\nQuestUAV Qpod\nThermal, RGB (red,\
    \ green\nand blue) and NIR (near-infrared) sensors\nRed Band Needle Blight disease\
    \ of pine trees\n[67]\n10\nS800 EVO Hexacopter\nCanon 5DsR camera, multispectral\
    \ MicaSense RedEdge\ncamera, Headwall Nano-Hyperspace\nGrape Phylloxera\n[76]\n\
    11\nDJI Phantom 3\nSony EXMOR sensor\nSoybean Foliar Diseases\n[77]\n12\neBee\n\
    IXUS 127 HS Canon camera\nCeratocystis Wilt in Eucalyptus Crops\n[75]\n13\nHiSystems\
    \ GmbH\nMiniMCA6\nHuanglongbing-infected citrus trees\n[78]\n14\nModiﬁed Sig Rascal\
    \ 110 RC airframe\nGeneral camera\nPotato late blight\n[68]\n15\nQuantalab, IAS-CSIC,\n\
    Multispectral and thermal\nVerticillium wilt of olives\n[74]\n16\nDJI Phantom\
    \ 3 quadcopter\nMicaSense RedEdge multispectral camera\nTomato Spot Wilt Disease\
    \ in Peanuts\n[79]\n17\nPhantom 4\nRGB camera\nFusarium wilt of radish\n[65]\n\
    18\nModel TFX-11\nSporangia-sampling devices\nPotato late blight\n[80]\n19\nmulti-rotor\
    \ MikrokopterOktoXL\nMultispectral and thermal\nWater status within a vineyard\n\
    [69]\n20\nDJI Phantom4\nRGB camera\nPotato Virus Y\n[62]\n21\n3D Robotics\nGamaya\
    \ OXI VNIR 40 camera\nBeet Cyst Nematode in Sugar Beet\n[64]\n22\nDJI Matrice\
    \ 600 pro\nPika L 2.4 (Resonon Inc., Bozeman, MT, USA)\nTarget spot and bacterial\
    \ spot diseases\n[21]\n23\nUAV system (S1000)\nHyperspectral imaging sensor (UHD\
    \ 185)\nYellow rust\n[15]\n24\nFeimaD200 quadrotor.\nModel RedEdge-MX\nPine Wood\
    \ Nematode Disease\n[3]\n25\nMulti-rotor DJI Mavic Pro\nMltispectral sensor (Parrot\
    \ Sequoia)\nOlive quick decline syndrome\n[17]\nAgronomy 2023, 13, 1524\n20 of\
    \ 26\n3.2. Deep Learning Models to Identify Plant Diseases Using Drones\nTo overcome\
    \ the constraints of conventional machine learning, deep learning models\nhave\
    \ been constructed and used for the problem of plant disease identiﬁcation in\
    \ drone\nimages. In the past decade, agriculture has seen promising results from\
    \ applying computer\nvision techniques based on deep learning [81,82]. Diseased\
    \ crops can change color, develop\ntwisted leaves or patches, or lose fruit. Because\
    \ of this, deep learning algorithms may be\nthe best option for diagnosing such\
    \ diseases. Image classiﬁcation, object detection, and\nimage segmentation are\
    \ the three main computer vision-based tasks that can improve\ncrop disease identiﬁcation\
    \ from drone imagery and be used to identify plant diseases.\nThe process of categorizing\
    \ a picture involves identifying the presence of the desired\ndisease across the\
    \ whole input image, known as image classiﬁcation. In most cases, disease\ndetection\
    \ at the leaf level is accomplished through the classiﬁcation task. The aim of\
    \ object\ndetection, on the other hand, aims to build a bounding box around each\
    \ identiﬁed disease\nto determine the class and precise position of the targeted\
    \ disease inside an input picture.\nSemantic segmentation is used to categorize\
    \ each pixel of an image as either diseased or\nnot. The deep learning algorithm\
    \ process for disease detection and classiﬁcation using\ndrones images is as follows:\
    \ (1) collection of data on the target plant disease by selecting a\nsuitable\
    \ ﬂight altitude for the drones; (2) data labeling, augmentation, cleaning, splitting,\n\
    and vegetative index generation; (3) use of models such as VGG or Res Net for\
    \ image\nclassiﬁcation, Faster-R-CNN or YOLO for object detection, and U-Net or\
    \ Seg-Net for image\nsegmentation; and (4) model training/validation and model\
    \ evaluation [81,82].\nThe use of deep learning algorithms applied to the analysis\
    \ of images collected by\ndrones has recently attracted a great deal of attention\
    \ due to its potential to identify plant\ndiseases. Recent research on crop disease\
    \ identiﬁcation using drone photography has relied\nheavily on deep learning models\
    \ to circumvent the shortcomings of more conventional\nmethods, particularly Convolutional\
    \ Neural Network (CNN) algorithms. Most of this\nresearch aims to improve the\
    \ yield of staple crops such as wheat, maize, potato, and\ntomato. For example,\
    \ Zhang et al. [81] developed many computer vision models based\non deep learning\
    \ to identify yellow rust illness and lessen its devastating effects. Using\n\
    multispectral data gathered through a UAV platform, they suggested a new semantic\n\
    segmentation approach derived from the U-Net model to detect wheat crop patches\
    \ afﬂicted\nwith yellow rust disease. There are three modules, namely, the Irregular\
    \ Encoder Module\n(IEM), Irregular Decoder Module (IDM), and Content-aware Channel\
    \ Re-weight Module\n(CCRM), embedded into the basic U-Net architecture as enhancements.\
    \ The authors looked\nat how the format of the input data affected the accuracy\
    \ with which the deep learning\nmodel identiﬁed wheat plants infected with yellow\
    \ rust. According to their ﬁndings, the\nproposed Ir-Unet model outperformed the\
    \ results of Su et al. [82], who only obtained an\nF1-score of 92% when using\
    \ all ﬁve bands of information obtained from the RedEdge\nmultispectral camera.\
    \ Combined with all the raw bands and their varied measurements\nof Selected Vegetation\
    \ Indices (SVIs), they were able to improve the accuracy to 96.97%.\nSimilarly,\
    \ Liu et al. [83] suggested a BPNN model to track Fusarium Head Blight using\n\
    hyperspectral aerial images, and found that it outperformed both SVM and RF, with\
    \ an\noverall accuracy of 98%. Using RGB pictures captured by UAVs, Huang et al.\
    \ [84] focused\non a different wheat disease, Helminthosporium Leaf Blotch Disease.\
    \ It was suggested that a\nLeNet-based CNN model be used to categorize HLBD according\
    \ to illness stage. Compared\nto a collection of methods plus the SVM model, the\
    \ accuracy of the adopted CNN model\nwas higher (91.43%). By merging the visible\
    \ and infrared bands from drone-collected\nphotos, Kerkech et al. [85] developed\
    \ a deep learning-based semantic segmentation system\nto automatically diagnose\
    \ mildew disease in vineyards using RGB photographs, infrared\nimages, and multispectral\
    \ data. The SegNet model was used to determine whether a given\npixel in a picture\
    \ represents a sick leaf or grapevine. Similarly, Northern Leaf Blight (NLB)\n\
    has been the focus of ongoing research, as it represents a signiﬁcant threat to\
    \ the maize\ncrop. Stewart et al. [86] used a DJI Matrice 600 to capture low-altitude\
    \ RGB aerial images\nand then used an instance segmentation approach (Mask R-CNN)\
    \ to identify NLB disease\nAgronomy 2023, 13, 1524\n21 of 26\nfrom these images.\
    \ On average, the suggested method achieved an accuracy of 96% when\ndetecting\
    \ and segmenting individual lesions.\nTo segment UAV-based RGB images into regions\
    \ affected or unaffected by NLB disease,\nWiesner-Hanks et al. [87] combined crowdsourced\
    \ ResNet-based CNN and Conditional\nRandom Field (CRF) techniques, using the crowdsourced\
    \ CNN to generate heatmaps and\nthe CRF to classify each pixel as lesion or non-lesion.\
    \ Using this method, they could detect\nNLB disease in maize crops within a millimeter,\
    \ outperforming the method used by Wu\net al. [88] by more than 2%. To automate\
    \ detection of mildew disease in vineyards, a\ndeep learning-based semantic segmentation\
    \ system was designed to process RGB photos,\ninfrared images, and multispectral\
    \ data obtained from a UAV integrating the visible and\ninfrared bands. The SegNet\
    \ model was used to determine whether a given pixel in an\nimage represented a\
    \ sick leaf or grapevine. Using visible, infrared, fusion AND, and fusion\nOR\
    \ data, the suggested technique obtained accuracies of 85.13%, 78.72%, 82.20%,\
    \ and\n90.23% at the leaf level and 94.41%, 89.16%, 88.14%, and 95.02% at the\
    \ grapevine level,\nrespectively [88]. To enhance the identiﬁcation of unhealthy\
    \ Pinus trees using RGB UAV\ndata, Hu et al. [89] integrated a Deep Convolutional\
    \ Neural Network (DCNN), a Deep\nConvolutional Generative Adversarial Network\
    \ (DCGAN), and an AdaBoost classiﬁer.\nThe suggested method outperformed classic\
    \ machine learning techniques, achieving an\nF1-score of 86.3% and a recall of\
    \ 95.7%, as opposed to recall rates of 78.3% and 65.2%,\nrespectively, for SVM\
    \ and AdaBoost classiﬁers. Deep learning models have drawbacks,\nhowever; for\
    \ example, training takes a long time, perhaps weeks, depending on the size of\n\
    the dataset, the complexity of the model, and the computer’s processing power.\
    \ When it\ncomes to early disease detection in plants, datasets are either insufﬁcient\
    \ or not accessible\nin sufﬁcient quantities. The ﬁrst step is to learn about\
    \ the area’s crop, disease, and pest\npatterns. Researchers typically choose to\
    \ either inoculate the fungus causing the disease\nin an experimental greenhouse\
    \ [81,85] or to watch and capture the natural development\nof an infestation as\
    \ it occurs. To obtain a hyperspectral picture, for instance, one must use\nsophisticated\
    \ high-priced equipment and consult with trained professionals throughout\nthe\
    \ data collection process [81]. In addition, when creating a new dataset, annotation\
    \ is\nrequired. Because the annotation of various diseases is beyond the capabilities\
    \ of ordinary\nvolunteers, this task requires the assistance of agriculture experts.\
    \ To minimize overﬁtting,\nresearchers often resort to data augmentation techniques\
    \ for tiny datasets, although these\ntechniques are not always effective. After\
    \ the data have been collected, they may be skewed,\neither because healthy plant\
    \ samples are more valuable than diseased plant samples or\nbecause of seasonal\
    \ and regional difﬁculties with different types of crop diseases [81,82].\n4.\
    \ Outlook, Future Trends, and Limitations\nTo improve crop output and attain food\
    \ security in vast agricultural areas, monitoring\nand identifying crop/plant\
    \ diseases early on with accuracy, dependability, timeliness, and\nefﬁciency is\
    \ crucial. In this review, we began by describing methods for detecting plant\
    \ dis-\neases, e.g., old and new generations. The old generation includes traditional,\
    \ molecular, and\nserological methods. However, these methods are not well suited\
    \ for the early detection of\nplant diseases. The new generation includes sensor-mounted\
    \ drones that accurately detect\nplant diseases at the earliest stages, enabling\
    \ prevention of future outbreaks by applying\nsuitable management measures. Second,\
    \ we emphasized various sensors that may be\ninstalled on drone platforms, including\
    \ digital, multispectral, hyperspectral, thermal, and\nﬂuorescence sensors. Hyperspectral\
    \ sensors are more robust than digital and multispectral\nsensors, whereas thermal\
    \ sensors can be used during either day or night, providing more\ninsight into\
    \ plant status than other sensors. Third, different types of drones and their\
    \ oper-\nating mechanisms were shown. VTOL and ﬁxed-wing drones can carry more\
    \ cameras and\nsensors than rotary-wing drones. Hence, VTOL and ﬁxed-wing drones\
    \ ﬂy for a longer time\nand can cover large areas. However, these types are expensive\
    \ and can encounter issues\nwhen hovering. Fourth, we looked at novel approaches\
    \ such as lateral ﬂow microarrays,\nbiomarkers, electrochemistry, phage display,\
    \ and biophotonics while focusing on drones.\nAgronomy 2023, 13, 1524\n22 of 26\n\
    Finally, we attempted to explain applications of plant disease detection using\
    \ drone images\nand traditional and deep learning models to obtain better results.\
    \ Deep learning models\nperform much better than traditional machine learning\
    \ models and do away with the\ntime-consuming and error-prone human feature extraction\
    \ process, resulting in improved\nprediction performance. Plant diseases in complicated\
    \ situations with overlapping plant\nleaves have been classiﬁed effectively using\
    \ deep learning models. On the other hand,\ntraditional machine learning models\
    \ cannot discriminate between diseases that have similar\nsymptoms and are unable\
    \ to beneﬁt from more training data. The training of deep learning\nmodels can\
    \ be time-consuming and may take weeks or months depending on the dataset’s\n\
    size, the model’s complexity, and the available computing power. There is a lack\
    \ of or difﬁ-\nculty gaining access to adequate databases for early disease diagnosis\
    \ in plants. Similarly,\nenvironmental, and logistical limitations such as strong\
    \ winds and rain, limited battery life,\nand the requirement for trained personnel\
    \ to initiate and oversee ﬂights are all issues when\nusing drones. Depending\
    \ on the geographic and spectral resolution of satellites, it may\nbe preferable\
    \ to use drones for monitoring plant development in a healthy environment.\nFurthermore,\
    \ current image sensor technology has various drawbacks that prevent it from\n\
    being used for early disease diagnosis. Better crop growth and health status predictions\
    \ can\nbe made by integrating data from different sensors. This explains why researchers\
    \ are more\nfocused on multimodal data fusion for crop disease detection. Data\
    \ fusion takes several\nforms in agriculture, with the most common being the combination\
    \ of satellite and drone\nimagery and the fusion of data from multiple drone sensors.\
    \ The detection procedure for\nactivities such as crop monitoring and plant categorization\
    \ can be enhanced by such data\nfusion approaches. It has become more important\
    \ in contemporary agriculture to develop\nprecise, real-time, dependable, and\
    \ autonomous drone-based systems for detecting plant\ndiseases. These systems\
    \ require sophisticated and effective algorithms to address issues\nsuch as ﬂuctuating\
    \ illumination, growing diseases, occlusion, and shifting perspectives.\nIn order\
    \ to realize improved agricultural yields, it is necessary to integrate cutting-edge\n\
    technologies such as drone systems and deep learning frameworks. The accessibility\
    \ of\nagricultural data is another key issue that must be addressed. To build\
    \ realistic datasets,\nit is necessary to either gather additional data or to\
    \ create complex algorithms based on\ngenerative deep learning architectures.\n\
    5. Conclusions\nIn conclusion, the application of drones in plant disease assessment\
    \ offers efficient moni-\ntoring and detection capabilities for smart agriculture.\
    \ Drones provide increased accessibility,\nimproved coverage, and rapid data collection,\
    \ enabling timely disease detection. With ad-\nvanced sensors and imaging techniques,\
    \ drones can capture valuable data on plant health\nindicators. These data can\
    \ be processed using analytics and machine learning algorithms\nto identify disease\
    \ patterns and assess severity. Integration of drones into plant disease as-\n\
    sessment systems allows for real-time monitoring, early detection, and targeted\
    \ intervention.\nDrones can contribute to sustainable farming practices, minimization\
    \ of yield losses, reduced\nneed for chemical treatments, and support for precision\
    \ agriculture strategies. Continued\nresearch addressing the challenges mentioned\
    \ above can further enhance the potential of\ndrones in plant disease assessment\
    \ for a resilient agricultural future.\nAuthor Contributions: Conceptualization,\
    \ L.Z., A.A., M.M.A. and S.A.H.N.; methodology, L.Z., A.A.\nand M.M.A.; software,\
    \ L.Z., H.Z., Z.Z., Q.A. (Qamar Abbas 1) and S.A.H.N.; validation, L.Z., A.A.\n\
    and M.M.A.; formal analysis, A.A. and M.M.A.; investigation, A.A. and M.M.A.;\
    \ data curation, A.A.;\nwriting—original draft preparation, L.Z., A.A. and M.M.A.;\
    \ writing—review and editing, L.Z., H.Z.,\nZ.Z., Q.A. (Qamar Abbas 1), M.M.A.,\
    \ A.F.A., M.J.R., W.F.A.M., Q.A. (Qamar Abbas 2), A.H., M.Z.H.,\nL.Z. and S.A.H.N.;\
    \ supervision, L.Z. and A.A.; project administration, L.Z., A.A. and S.A.H.N.;\
    \ funding\nacquisition, L.Z. All authors have read and agreed to the published\
    \ version of the manuscript.\nAgronomy 2023, 13, 1524\n23 of 26\nFunding: This\
    \ work was supported by the High-talent Introduction and Continuous Training Fund\
    \ to\nL.Z. (grant no: 10300000021LL05) and Discipline Construction Funds (grant\
    \ no: 10407000019CC2213G),\nsupported by Zhejiang Academy of Agricultural Sciences\
    \ (ZAAS) and State Key Laboratory for Manag-\ning Biotic and Chemical Threats\
    \ to the Quality and Safety of Agro-products (10417000022CE0601G/029).\nData Availability\
    \ Statement: Not applicable.\nConﬂicts of Interest: The authors declare no conﬂict\
    \ of interest.\nReferences\n1.\nNing, Y.; Liu, W.; Wang, G.L. Balancing Immunity\
    \ and Yield in Crop Plants. Trends Plant Sci. 2017, 22, 1069–1079. [CrossRef]\n\
    [PubMed]\n2.\nSingh, A.K.; Ganapathysubramanian, B.; Sarkar, S.; Singh, A. Deep\
    \ Learning for Plant Stress Phenotyping: Trends and Future\nPerspectives. Trends\
    \ Plant Sci. 2018, 23, 883–898. [CrossRef] [PubMed]\n3.\nQin, J.; Wang, B.; Wu,\
    \ Y.; Lu, Q.; Zhu, H. Identifying Pine Wood Nematode Disease Using UAV Images\
    \ and Deep Learning\nAlgorithms. Remote Sens. 2021, 13, 162. [CrossRef]\n4.\n\
    Cui, S.; Ling, P.; Zhu, H.; Keener, H.M. Plant Pest Detection Using an Artiﬁcial\
    \ Nose System: A Review. Sensors 2018, 18, 378.\n[CrossRef] [PubMed]\n5.\nMartinelli,\
    \ F.; Scalenghe, R.; Davino, S.; Panno, S.; Scuderi, G.; Ruisi, P.; Villa, P.;\
    \ Stroppiana, D.; Boschetti, M.; Goulart, L.R.; et al.\nAdvanced methods of plant\
    \ disease detection. A review. Agron. Sustain. Dev. 2015, 35, 1–25. [CrossRef]\n\
    6.\nMahlein, A. Present and future trends in plant disease detection. Plant Dis.\
    \ 2016, 100, 1–11.\n7.\nGe, Y.; Thomasson, J.A.; Sui, R. Remote sensing of soil\
    \ properties in precision agriculture: A review. Front. Earth Sci. 2011, 5,\n\
    229–238. [CrossRef]\n8.\nShi, Y.; Thomasson, J.A.; Murray, S.C.; Pugh, N.A.; Rooney,\
    \ W.L.; Shaﬁan, S.; Rajan, N.; Rouze, G.; Morgan, C.L.S.; Neely, H.L.;\net al.\
    \ Unmanned Aerial Vehicles for High-Throughput Phenotyping and Agronomic Research.\
    \ PLoS ONE 2016, 11, e0159781.\n[CrossRef]\n9.\nHerrmann, I.; Bdolach, E.; Montekyo,\
    \ Y.; Rachmilevitch, S.; Townsend, P.A.; Karnieli, A. Assessment of maize yield\
    \ and phenology\nby drone-mounted superspectral camera. Precis. Agric. 2020, 21,\
    \ 51–76. [CrossRef]\n10.\nBauriegel, E.; Herppich, W.B. Hyperspectral and chlorophyll\
    \ ﬂuorescence imaging for early detection of plant diseases, with\nspecial reference\
    \ to Fusarium spec. infections on wheat. Agriculture 2014, 4, 32–57. [CrossRef]\n\
    11.\nKuska, M.; Wahabzada, M.; Leucker, M.; Dehne, H.-W.; Kersting, K.; Oerke,\
    \ E.-C.; Steiner, U.; Mahlein, A.-K. Hyperspectral\nphenotyping on the microscopic\
    \ scale: Towards automated characterization of plant-pathogen interactions. Plant\
    \ Methods 2015,\n11, 28. [CrossRef] [PubMed]\n12.\nZhang, D.; Zhou, X.; Zhang,\
    \ J.; Lan, Y.; Xu, C.; Liang, D. Detection of rice sheath blight using an unmanned\
    \ aerial system with\nhigh-resolution color and multispectral imaging. PLoS ONE\
    \ 2018, 13, e0187470. [CrossRef] [PubMed]\n13.\nSun, Q.; Sun, L.; Shu, M.; Gu,\
    \ X.; Yang, G.; Zhou, L. Monitoring Maize Lodging Grades via Unmanned Aerial Vehicle\
    \ Multispectral\nImage. Plant Phenomics 2019, 2019, 5704154. [CrossRef]\n14.\n\
    Khot, L.R.; Sankaran, S.; Carter, A.H.; Johnson, D.A.; Cummings, T.F. UAS imaging-based\
    \ decision tools for arid winter wheat\nand irrigated potato production management.\
    \ Int. J. Remote Sens. 2016, 37, 125–137. [CrossRef]\n15.\nGuo, A.; Huang, W.;\
    \ Dong, Y.; Ye, H.; Ma, H.; Liu, B.; Wu, W.; Ren, Y.; Ruan, C.; Geng, Y. Wheat\
    \ Yellow Rust Detection Using\nUAV-Based Hyperspectral Technology. Remote Sens.\
    \ 2021, 13, 123. [CrossRef]\n16.\nSarkar, S.K.; Das, J.; Ehsani, R.; Kumar, V.\
    \ Towards autonomous phytopathology: Outcomes and challenges of citrus greening\n\
    disease detection through close-range remote sensing. In Proceedings of the 2016\
    \ IEEE International Conference on Robotics and\nAutomation (ICRA), Stockholm,\
    \ Sweden, 16–21 May 2016; pp. 5143–5148.\n17.\nCastrignanò, A.; Belmonte, A.;\
    \ Antelmi, I.; Quarto, R.; Quarto, F.; Shaddad, S.; Sion, V.; Muolo, M.R.; Ranieri,\
    \ N.A.; Gadaleta, G.;\net al. Semi-Automatic Method for Early Detection of Xylella\
    \ fastidiosa in Olive Trees Using UAV Multispectral Imagery and\nGeostatistical-Discriminant\
    \ Analysis. Remote Sens. 2020, 13, 14. [CrossRef]\n18.\nShahi, T.B.; Xu, C.-Y.;\
    \ Neupane, A.; Guo, W. Recent Advances in Crop Disease Detection Using UAV and\
    \ Deep Learning\nTechniques. Remote Sens. 2023, 15, 2450. [CrossRef]\n19.\nFranceschini,\
    \ M.H.D.; Bartholomeus, H.; van Apeldoorn, D.; Suomalainen, J.; Kooistra, L. Assessing\
    \ changes in potato canopy\ncaused by late blight in organic production systems\
    \ through Uav-Based Pushbroom imaging spectrometer. Int. Arch. Photogramm\nRemote\
    \ Sens. Spat. Inf. Sci. 2017, XLII-2/W6, 109–112. [CrossRef]\n20.\nYamamoto, S.;\
    \ Nomoto, S.; Hashimoto, N.; Maki, M.; Hongo, C.; Shiraiwa, T. Monitoring spatial\
    \ and time-series variations in red\ncrown rot damage of soybean in farmer ﬁelds\
    \ based on UAV remote sensing. Plant Prod. Sci. 2023, 26, 36–47. [CrossRef]\n\
    21.\nAbdulridha, J.; Ampatzidis, Y.; Kakarla, S.C.; Roberts, P. Detection of target\
    \ spot and bacterial spot diseases in tomato using\nUAV-based and benchtop-based\
    \ hyperspectral imaging techniques. Precis. Agric. 2020, 21, 955–978. [CrossRef]\n\
    22.\nSchmale, I.I.I.D.G.; Ross, S.D. Highways in the sky: Scales of atmospheric\
    \ transport of plant pathogens. Annu. Rev. Phytopathol.\n2015, 53, 591–611. [CrossRef]\
    \ [PubMed]\n23.\nTallapragada, P.; Ross, S.D.; Schmale, D.G., III. Lagrangian\
    \ coherent structures are associated with ﬂuctuations in airborne\nmicrobial populations.\
    \ Chaos Interdiscip. J. Nonlinear Sci. 2011, 21, 033122. [CrossRef]\nAgronomy\
    \ 2023, 13, 1524\n24 of 26\n24.\nChristiansen, M.P.; Laursen, M.S.; Jørgensen,\
    \ R.N.; Skovsen, S.; Gislum, R. Designing and testing a UAV mapping system for\n\
    agricultural ﬁeld surveying. Sensors 2017, 17, 2703. [CrossRef] [PubMed]\n25.\n\
    Mahlein, A.-K. Plant Disease Detection by Imaging Sensors—Parallels and Speciﬁc\
    \ Demands for Precision Agriculture and Plant\nPhenotyping. Plant Dis. 2016, 100,\
    \ 241–251. [CrossRef]\n26.\nChen, J.-W.; Lau, Y.Y.; Krishnan, T.; Chan, K.-G.;\
    \ Chang, C.-Y. Recent advances in molecular diagnosis of Pseudomonas aeruginosa\n\
    infection by State-of-the-Art, Genotyping Techniques. Front. Microbiol. 2018,\
    \ 9, 1104. [CrossRef]\n27.\nTorres-Sánchez, J.; López-Granados, F.; De Castro,\
    \ A.I.; Peña-Barragán, J.M. Conﬁguration and speciﬁcations of an unmanned\naerial\
    \ vehicle (UAV) for early site speciﬁc weed management. PLoS ONE 2013, 8, e58210.\
    \ [CrossRef] [PubMed]\n28.\nBleecker, A.B.; Kende, H. Ethylene: A gaseous signal\
    \ molecule in plants. Annu. Rev. Cell Dev. Biol. 2000, 16, 1–18. [CrossRef]\n\
    29.\nBarbedo, J.G.A. Factors inﬂuencing the use of deep learning for plant disease\
    \ recognition. Biosyst. Eng. 2018, 172, 84–91.\n[CrossRef]\n30.\nBendig, J.; Bolten,\
    \ A.; Bennertz, S.; Broscheit, J.; Eichfuss, S.; Bareth, G. Estimating biomass\
    \ of barley using crop surface models\n(CSMs) derived from UAV-based RGB imaging.\
    \ Remote Sens. 2014, 6, 10395–10412. [CrossRef]\n31.\nGeipel, J.; Link, J.; Claupein,\
    \ W. Combined spectral and spatial modeling of corn yield based on aerial images\
    \ and crop surface\nmodels acquired with an unmanned aircraft system. Remote Sens.\
    \ 2014, 6, 10335. [CrossRef]\n32.\nYang, C.; Westbrook, J.K.; Suh, C.P.-C.; Martin,\
    \ D.E.; Hoffmann, W.C.; Lan, Y.; Fritz, B.K.; Goolsby, J.A. An airborne multispectral\n\
    imaging system based on two consumer-grade cameras for agricultural remote sensing.\
    \ Remote Sens. 2014, 6, 5257–5278.\n[CrossRef]\n33.\nRango, A.; Laliberte, A.;\
    \ Steele, C.; Herrick, J.E.; Bestelmeyer, B.; Schmugge, T.; Roanhorse, A.; Jenkins,\
    \ V. Using unmanned aerial\nvehicles for rangelands: Current applications and\
    \ future potentials. Environ. Pract. 2006, 8, 159–168. [CrossRef]\n34.\nLaliberte,\
    \ A.S.; Goforth, M.A.; Steele, C.M.; Rango, A. Multispectral remote sensing from\
    \ unmanned aircraft: Image processing\nworkﬂows and applications for rangeland\
    \ environments. Remote Sens. 2011, 3, 2529–2551. [CrossRef]\n35.\nSandino, J.;\
    \ Pegg, G.; Gonzalez, F.; Smith, G. Aerial mapping of forests affected by pathogens\
    \ using UAVs, hyperspectral sensors,\nand artiﬁcial intelligence. Sensors 2018,\
    \ 18, 944. [CrossRef] [PubMed]\n36.\nHuete, A.R. A soil-adjusted vegetation index\
    \ (SAVI). Remote Sens. Environ. 1988, 25, 295–309. [CrossRef]\n37.\nGitelson,\
    \ A.A.; Kaufman, Y.J.; Merzlyak, M.N. Use of a green channel in remote sensing\
    \ of global vegetation from EOS-MODIS.\nRemote Sens. Environ. 1996, 58, 289–298.\
    \ [CrossRef]\n38.\nDevadas, R.; Lamb, D.; Simpfendorfer, S.; Backhouse, D. Evaluating\
    \ ten spectral vegetation indices for identifying rust infection\nin individual\
    \ wheat leaves. Precis. Agric. 2009, 10, 459–470. [CrossRef]\n39.\nAmpatzidis,\
    \ Y.; De Bellis, L.; Luvisi, A. iPathology: Robotic applications and management\
    \ of plants and plant diseases.\nSustainability 2017, 9, 1010. [CrossRef]\n40.\n\
    Mirik, M.; Jones, D.; Price, J.; Workneh, F.; Ansley, R.; Rush, C. Satellite remote\
    \ sensing of wheat infected by wheat streak mosaic\nvirus. Plant Dis. 2011, 95,\
    \ 4–12. [CrossRef]\n41.\nLi, X.; Lee, W.S.; Li, M.; Ehsani, R.; Mishra, A.R.;\
    \ Yang, C.; Mangan, R.L. Spectral difference analysis and airborne imaging\nclassiﬁcation\
    \ for citrus greening infected trees. Comput. Electron. Agric. 2012, 83, 32–46.\
    \ [CrossRef]\n42.\nYang, C.; Odvody, G.N.; Fernandez, C.J.; Landivar, J.A.; Minzenmayer,\
    \ R.R.; Nichols, R.L. Evaluating unsupervised and\nsupervised image classiﬁcation\
    \ methods for mapping cotton root rot. Precis. Agric. 2015, 16, 201–215. [CrossRef]\n\
    43.\nCarter, D.J.; Cary, R.B. Lateral ﬂow microarrays: A novel platform for rapid\
    \ nucleic acid detection based on miniaturized lateral\nﬂow chromatography. Nucleic\
    \ Acids Res. 2007, 35, e74. [CrossRef] [PubMed]\n44.\nBaldwin, I.T.; Halitschke,\
    \ R.; Paschold, A.; Von Dahl, C.C.; Preston, C.A. Volatile signaling in plant-plant\
    \ interactions: “Talking\ntrees” in the genomics era. Science 2006, 311, 812–815.\
    \ [CrossRef] [PubMed]\n45.\nDe Jong, S.M.; Van der Meer, F.D.; Clevers, J.G. Basics\
    \ of remote sensing. In Remote Sensing Image Analysis: Including the Spatial\n\
    Domain; Springer: Berlin/Heidelberg, Germany, 2004; pp. 1–15.\n46.\nGoulart, L.R.;\
    \ Vieira, C.U.; Freschi, A.P.P.; Capparelli, F.E.; Fujimura, P.T.; Almeida, J.F.;\
    \ Ferreira, L.F.; Goulart, I.M.; Brito-Madurro,\nA.G.; Madurro, J.M. Biomarkers\
    \ for serum diagnosis of infectious diseases and their potential application in\
    \ novel sensor\nplatforms. Crit. Rev. Immunol. 2010, 30, 201–222. [CrossRef] [PubMed]\n\
    47.\nEllington, A.D.; Szostak, J.W. In vitro selection of RNA molecules that bind\
    \ speciﬁc ligands. Nature 1990, 346, 818–822. [CrossRef]\n[PubMed]\n48.\nAhmed,\
    \ M.U.; Hossain, M.M.; Tamiya, E. Electrochemical biosensors for medical and food\
    \ applications. Electroanal. Int. J. Devoted\nFundam. Pract. Asp. Electroanal.\
    \ 2008, 20, 616–626. [CrossRef]\n49.\nDegefu, Y.; Somervuo, P.; Aittamaa, M.;\
    \ Virtanen, E.; Valkonen, J.P.T. Evaluation of a diagnostic microarray for the\
    \ detection of\nmajor bacterial pathogens of potato from tuber samples. EPPO Bull.\
    \ 2016, 46, 103–111. [CrossRef]\n50.\nLuppa, P.B.; Sokoll, L.J.; Chan, D.W. Immunosensors—Principles\
    \ and applications to clinical chemistry. Clin. Chim. Acta 2001, 314,\n1–26. [CrossRef]\n\
    51.\nApostol, S.; Viau, A.A.; Tremblay, N.; Briantais, J.-M.; Prasher, S.; Parent,\
    \ L.-E.; Moya, I. Laser-induced ﬂuorescence signatures as\na tool for remote monitoring\
    \ of water and nitrogen stresses in plants. Can. J. Remote Sens. 2003, 29, 57–65.\
    \ [CrossRef]\n52.\nCohen, Y.; Alchanatis, V.; Meron, M.; Saranga, Y.; Tsipris,\
    \ J. Estimation of leaf water potential by thermal imagery and spatial\nanalysis.\
    \ J. Exp. Bot. 2005, 56, 1843–1852. [CrossRef]\nAgronomy 2023, 13, 1524\n25 of\
    \ 26\n53.\nMeroni, M.; Rossini, M.; Colombo, R. Characterization of leaf physiology\
    \ using reﬂectance and ﬂuorescence hyperspectral\nmeasurements. In Optical Observation\
    \ of Vegetation Properties and Characteristics; Research Signpost: Trivandrum,\
    \ India, 2010;\npp. 165–187.\n54.\nWitten, I.H.; Frank, E. Data mining: Practical\
    \ machine learning tools and techniques with Java implementations. ACM Sigmod\
    \ Rec.\n2002, 31, 76–77. [CrossRef]\n55.\nKhanal, S.; Fulton, J.; Shearer, S.\
    \ An overview of current and potential applications of thermal remote sensing\
    \ in precision\nagriculture. Comput. Electron. Agric. 2017, 139, 22–32. [CrossRef]\n\
    56.\nAl-Saddik, H.; Laybros, A.; Simon, J.-C.; Cointault, F. Protocol for the\
    \ Deﬁnition of a Multi-Spectral Sensor for Speciﬁc Foliar\nDisease Detection:\
    \ Case of “Flavescence Dorée”. In Phytoplasmas; Springer: New York, NY, USA, 2019;\
    \ pp. 213–238.\n57.\nSeelan, S.K.; Laguette, S.; Casady, G.M.; Seielstad, G.A.\
    \ Remote sensing applications for precision agriculture: A learning\ncommunity\
    \ approach. Remote Sens. Environ. 2003, 88, 157–169. [CrossRef]\n58.\nDunning,\
    \ H. Drones That Detect Early Plant Disease Could Save Crops; Imperial College\
    \ London: London, UK, 2017; pp. 1–3.\n59.\nBah, M.D.; Haﬁane, A.; Canals, R. Deep\
    \ learning with unsupervised data labeling for weed detection in line crops in\
    \ UAV images.\nRemote Sens. 2018, 10, 1690. [CrossRef]\n60.\nYamamoto, K.; Togami,\
    \ T.; Yamaguchi, N. Super-resolution of plant disease images for the acceleration\
    \ of image-based phenotyp-\ning and vigor diagnosis in agriculture. Sensors 2017,\
    \ 17, 2557. [CrossRef] [PubMed]\n61.\nSugiura, R.; Tsuda, S.; Tamiya, S.; Itoh,\
    \ A.; Nishiwaki, K.; Murakami, N.; Shibuya, Y.; Hirafuji, M.; Nuske, S. Field\
    \ phenotyping\nsystem for the assessment of potato late blight resistance using\
    \ RGB imagery from an unmanned aerial vehicle. Biosyst. Eng. 2016,\n148, 1–10.\
    \ [CrossRef]\n62.\nSugiura, R.; Tsuda, S.; Tsuji, H.; Murakami, N. Virus-Infected\
    \ Plant Detection in Potato Seed Production Field by UAV Imagery;\nAmerican Society\
    \ of Agricultural and Biological Engineers: St. Joseph, MI, USA, 2018; p. 1.\n\
    63.\nPande, C.B.; Moharir, K.N. Application of hyperspectral remote sensing role\
    \ in precision farming and sustainable agriculture\nunder climate change: A review.\
    \ In Climate Change Impacts on Natural Resources, Ecosystems and Agricultural\
    \ Systems; Springer:\nBerlin/Heidelberg, Germany, 2023; pp. 503–520.\n64.\nJoalland,\
    \ S.; Screpanti, C.; Varella, H.V.; Reuther, M.; Schwind, M.; Lang, C.; Walter,\
    \ A.; Liebisch, F. Aerial and ground based\nsensing of tolerance to beet cyst\
    \ nematode in sugar beet. Remote Sens. 2018, 10, 787. [CrossRef]\n65.\nDang, L.M.;\
    \ Hassan, S.I.; Suhyeon, I.; kumar Sangaiah, A.; Mehmood, I.; Rho, S.; Seo, S.;\
    \ Moon, H. UAV based wilt detection\nsystem via convolutional neural networks.\
    \ Sustain. Comput. Inform. Syst. 2018, 28, 100250. [CrossRef]\n66.\nDash, J.P.;\
    \ Watt, M.S.; Pearse, G.D.; Heaphy, M.; Dungey, H.S. Assessing very high resolution\
    \ UAV imagery for monitoring forest\nhealth during a simulated disease outbreak.\
    \ ISPRS J. Photogramm. Remote Sens. 2017, 131, 1–14. [CrossRef]\n67.\nSmigaj,\
    \ M.; Gaulton, R.; Barr, S.; Suárez, J. UAV-borne thermal imaging for forest health\
    \ monitoring: Detection of disease-induced\ncanopy temperature increase. Int.\
    \ Arch. Photogramm. Remote Sens. Spat. Inf. Sci. 2015, 40, 349. [CrossRef]\n68.\n\
    Techy, L.; Schmale, D.G., III; Woolsey, C.A. Coordinated aerobiological sampling\
    \ of a plant pathogen in the lower atmosphere\nusing two autonomous unmanned aerial\
    \ vehicles. J. Field Robot. 2010, 27, 335–343. [CrossRef]\n69.\nSantesteban, L.;\
    \ Di Gennaro, S.; Herrero-Langreo, A.; Miranda, C.; Royo, J.; Matese, A. High-resolution\
    \ UAV-based thermal\nimaging to estimate the instantaneous and seasonal variability\
    \ of plant water status within a vineyard. Agric. Water Manag. 2017,\n183, 49–59.\
    \ [CrossRef]\n70.\nAlbetis, J.; Jacquin, A.; Goulard, M.; Poilvé, H.; Rousseau,\
    \ J.; Clenet, H.; Dedieu, G.; Duthoit, S. On the potentiality of UAV\nmultispectral\
    \ imagery to detect Flavescence dorée and Grapevine Trunk Diseases. Remote Sens.\
    \ 2019, 11, 23. [CrossRef]\n71.\nAlbetis, J.; Duthoit, S.; Guttler, F.; Jacquin,\
    \ A.; Goulard, M.; Poilvé, H.; Féret, J.-B.; Dedieu, G. Detection of Flavescence\
    \ dorée\ngrapevine disease using unmanned aerial vehicle (UAV) multispectral imagery.\
    \ Remote Sens. 2017, 9, 308. [CrossRef]\n72.\nFranceschini, M.H.D.; Bartholomeus,\
    \ H.; Van Apeldoorn, D.F.; Suomalainen, J.; Kooistra, L. Feasibility of unmanned\
    \ aerial vehicle\noptical imagery for early detection and severity assessment\
    \ of late blight in potato. Remote Sens. 2019, 11, 224. [CrossRef]\n73.\nDi Gennaro,\
    \ S.F.; Battiston, E.; Di Marco, S.; Facini, O.; Matese, A.; Nocentini, M.; Palliotti,\
    \ A.; Mugnai, L. Unmanned Aerial Vehicle\n(UAV)-based remote sensing to monitor\
    \ grapevine leaf stripe disease within a vineyard affected by esca complex. Phytopathol.\n\
    Mediterr. 2016, 55, 262–275.\n74.\nCalderón, R.; Montes-Borrego, M.; Landa, B.;\
    \ Navas-Cortés, J.; Zarco-Tejada, P. Detection of downy mildew of opium poppy\n\
    using high-resolution multi-spectral and thermal imagery acquired with an unmanned\
    \ aerial vehicle. Precis. Agric. 2014, 15,\n639–661. [CrossRef]\n75.\nde Souza,\
    \ C.H.W.; Lamparelli, R.A.C.; Rocha, J.V.; Magalhães, P.S.G. Mapping skips in\
    \ sugarcane ﬁelds using object-based\nanalysis of unmanned aerial vehicle (UAV)\
    \ images. Comput. Electron. Agric. 2017, 143, 49–56. [CrossRef]\n76.\nVanegas,\
    \ F.; Bratanov, D.; Weiss, J.; Powell, K.; Gonzalez, F. Multi and hyperspectral\
    \ UAV remote sensing: Grapevine phylloxera\ndetection in vineyards. In Proceedings\
    \ of the 2018 IEEE Aerospace Conference, Big Sky, MT, USA, 3–10 March 2018; pp.\
    \ 1–9.\n77.\nTetila, E.C.; Machado, B.B.; de Souza Belete, N.A.; Guimarães, D.A.;\
    \ Pistori, H. Identiﬁcation of soybean foliar diseases using\nunmanned aerial\
    \ vehicle images. IEEE Geosci. Remote Sens. Lett. 2017, 14, 2190–2194. [CrossRef]\n\
    78.\nGarcia-Ruiz, F.; Sankaran, S.; Maja, J.M.; Lee, W.S.; Rasmussen, J.; Ehsani,\
    \ R. Comparison of two aerial imaging platforms for\nidentiﬁcation of Huanglongbing-infected\
    \ citrus trees. Comput. Electron. Agric. 2013, 91, 106–115. [CrossRef]\nAgronomy\
    \ 2023, 13, 1524\n26 of 26\n79.\nPatrick, A.; Pelham, S.; Culbreath, A.; Holbrook,\
    \ C.C.; De Godoy, I.J.; Li, C. High throughput phenotyping of tomato spot\nwilt\
    \ disease in peanuts using unmanned aerial systems and multispectral imaging.\
    \ IEEE Instrum. Meas. Mag. 2017, 20, 4–12.\n[CrossRef]\n80.\nAylor, D.E.; Schmale,\
    \ I.I.I.D.G.; Shields, E.J.; Newcomb, M.; Nappo, C.J. Tracking the potato late\
    \ blight pathogen in the atmosphere\nusing unmanned aerial vehicles and Lagrangian\
    \ modeling. Agric. For. Meteorol. 2011, 151, 251–260. [CrossRef]\n81.\nZhang,\
    \ T.; Xu, Z.; Su, J.; Yang, Z.; Liu, C.; Chen, W.-H.; Li, J. Ir-unet: Irregular\
    \ segmentation u-shape network for wheat yellow\nrust detection by UAV multispectral\
    \ imagery. Remote Sens. 2021, 13, 3892. [CrossRef]\n82.\nSu, J.; Yi, D.; Su, B.;\
    \ Mi, Z.; Liu, C.; Hu, X.; Xu, X.; Guo, L.; Chen, W.-H. Aerial visual perception\
    \ in smart farming: Field study of\nwheat yellow rust monitoring. IEEE Trans.\
    \ Ind. Inform. 2020, 17, 2242–2249. [CrossRef]\n83.\nLiu, L.; Dong, Y.; Huang,\
    \ W.; Du, X.; Ma, H. Monitoring wheat fusarium head blight using unmanned aerial\
    \ vehicle hyperspectral\nimagery. Remote Sens. 2020, 12, 3811. [CrossRef]\n84.\n\
    Huang, H.; Deng, J.; Lan, Y.; Yang, A.; Zhang, L.; Wen, S.; Zhang, H.; Zhang,\
    \ Y.; Deng, Y. Detection of helminthosporium leaf\nblotch disease based on UAV\
    \ imagery. Appl. Sci. 2019, 9, 558. [CrossRef]\n85.\nKerkech, M.; Haﬁane, A.;\
    \ Canals, R. Vine disease detection in UAV multispectral images using optimized\
    \ image registration and\ndeep learning segmentation approach. Comput. Electron.\
    \ Agric. 2020, 174, 105446. [CrossRef]\n86.\nStewart, E.L.; Wiesner-Hanks, T.;\
    \ Kaczmar, N.; DeChant, C.; Wu, H.; Lipson, H.; Nelson, R.J.; Gore, M.A. Quantitative\
    \ phenotyping\nof Northern Leaf Blight in UAV images using deep learning. Remote\
    \ Sens. 2019, 11, 2209. [CrossRef]\n87.\nWiesner-Hanks, T.; Stewart, E.L.; Kaczmar,\
    \ N.; DeChant, C.; Wu, H.; Nelson, R.J.; Lipson, H.; Gore, M.A. Image set for\
    \ deep\nlearning: Field images of maize annotated with disease symptoms. BMC Res.\
    \ Notes 2018, 11, 440. [CrossRef]\n88.\nWu, B.; Liang, A.; Zhang, H.; Zhu, T.;\
    \ Zou, Z.; Yang, D.; Tang, W.; Li, J.; Su, J. Application of conventional UAV-based\
    \ high-\nthroughput object detection to the early diagnosis of pine wilt disease\
    \ by deep learning. For. Ecol. Manag. 2021, 486, 118986.\n[CrossRef]\n89.\nHu,\
    \ G.; Zhu, Y.; Wan, M.; Bao, W.; Zhang, Y.; Liang, D.; Yin, C. Detection of diseased\
    \ pine trees in unmanned aerial vehicle images\nby using deep convolutional neural\
    \ networks. Geocarto Int. 2022, 37, 3520–3539. [CrossRef]\nDisclaimer/Publisher’s\
    \ Note: The statements, opinions and data contained in all publications are solely\
    \ those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or\
    \ the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury\
    \ to\npeople or property resulting from any ideas, methods, instructions or products\
    \ referred to in the content.\n"
  inline_citation: '>'
  journal: Agronomy (Basel)
  limitations: '>'
  pdf_link: https://www.mdpi.com/2073-4395/13/6/1524/pdf?version=1685589617
  publication_year: 2023
  relevance_evaluation: Very good—The explanation of the study's purpose and main
    objectives is clear, concise, and accurate.
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  title: 'Drones in Plant Disease Assessment, Efficient Monitoring, and Detection:
    A Way Forward to Smart Agriculture'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Maji, S., Vinay, V. K. S. M. S. C., Kumari, S., Banthia, V., & Neerugatti,
    V. (2023, July 6-8). Cotton crop certainty identification using deep learning
    techniques. In 2023 14th International Conference on Computing Communication and
    Networking Technologies (ICCCNT) (pp. 1-5). IEEE.
  authors:
  - Maji S.
  - Vinay V.K.
  - Kumari S.
  - Banthia V.
  - Neerugatti V.
  citation_count: '0'
  data_sources: '"A large dataset of images of cotton crops affected by various diseases"
    (Section III. Methodology)'
  description: The cotton crop certainty identification system is a computerized tool
    designed to aid farmers and agricultural experts in detecting and diagnosing diseases
    in cotton crops. This system uses image processing techniques to analyze images
    of cotton leaves and identify any signs of illness. The system also utilizes machine
    learning algorithms to classify the disease based on the symptoms observed, helping
    farmers to determine the appropriate treatment or management practices. A system
    is a valuable tool in increasing the efficiency and accuracy of disease detection
    in cotton crops, leading to higher crop yields and better overall crop health.Here,
    the data will be collected as a large dataset of images of cotton crops, which
    is then used to train a deep learning model. Once the model is trained, it can
    be used to accurately identify cotton crops in new images. This type of system
    has the potential to greatly improve the efficiency and accuracy of cotton crop
    monitoring and analysis.
  doi: 10.1109/ICCCNT56998.2023.10306483
  explanation: The study focuses on developing and evaluating a system for identifying
    diseases in cotton crops using image processing and machine learning techniques,
    specifically convolutional neural networks (CNNs) and recurrent neural networks
    (RNNs). The system is designed to assist farmers and agricultural experts in detecting
    and diagnosing diseases in cotton crops, thereby aiding in timely and effective
    management practices.
  extract_1: '"The cotton crop certainty identification system is a computerized tool
    designed to aid farmers and agricultural experts in detecting and diagnosing diseases
    in cotton crops. This system uses image processing techniques to analyze images
    of cotton leaves and identify any signs of illness. The system also utilizes machine
    learning algorithms to classify the disease based on the symptoms observed, helping
    farmers to determine the appropriate treatment or management practices." (Section
    I. Introduction)'
  extract_2: '"The trained models are capable of accurately identifying the diseases
    present in new images of cotton crops, which can be a valuable tool for farmers
    and agriculture professionals. By identifying diseases in the early stages, farmers
    can take necessary actions to prevent the spread of the disease and minimize crop
    losses" (Section VI. Conclusion)'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2023 14th International Confe...
    Cotton Crop Certainty Identification Using Deep Learning Techniques Publisher:
    IEEE Cite This PDF Supriti Maji; Vura KSMSC Vinay; Sonam Kumari; Vardhman Banthia;
    Vikram Neerugatti All Authors 53 Full Text Views Abstract Document Sections I.
    INTRODUCTION: II. LITERATURE SURVEY III. METHODOLOGY: IV. IMPLEMENTATION: V. RESULTS
    AND DISCUSSION Show Full Outline Authors Figures References Keywords Metrics Abstract:
    The cotton crop certainty identification system is a computerized tool designed
    to aid farmers and agricultural experts in detecting and diagnosing diseases in
    cotton crops. This system uses image processing techniques to analyze images of
    cotton leaves and identify any signs of illness. The system also utilizes machine
    learning algorithms to classify the disease based on the symptoms observed, helping
    farmers to determine the appropriate treatment or management practices. A system
    is a valuable tool in increasing the efficiency and accuracy of disease detection
    in cotton crops, leading to higher crop yields and better overall crop health.Here,
    the data will be collected as a large dataset of images of cotton crops, which
    is then used to train a deep learning model. Once the model is trained, it can
    be used to accurately identify cotton crops in new images. This type of system
    has the potential to greatly improve the efficiency and accuracy of cotton crop
    monitoring and analysis. Published in: 2023 14th International Conference on Computing
    Communication and Networking Technologies (ICCCNT) Date of Conference: 06-08 July
    2023 Date Added to IEEE Xplore: 23 November 2023 ISBN Information: ISSN Information:
    DOI: 10.1109/ICCCNT56998.2023.10306483 Publisher: IEEE Conference Location: Delhi,
    India SECTION I. INTRODUCTION: Cotton is one of the most important cash crops
    grown worldwide, and it plays a vital role in the global textile industry. However,
    cotton crops are susceptible to various diseases, significantly impacting crop
    yield and quality. Identifying the defect at an early stage is crucial in preventing
    it from spreading and causing irreparable damage to the crop. Traditionally, farmers
    have relied on manual observation to identify diseases in cotton crops, which
    is time-consuming and subjective. To address this challenge, researchers and experts
    have developed computerized tools and systems to aid in the identification and
    classification of cotton crop diseases. The cotton crop certainty identification
    system is one such tool that uses advanced technologies such as image processing
    and machine learning algorithms to analyze images of cotton leaves and identify
    any signs of disease. This system provides farmers with a more accurate and efficient
    method of detecting and diagnosing diseases in cotton crops, which can help them
    make informed decisions about appropriate treatment and management practices.
    The development of this system represents a significant step forward in the management
    of cotton crop diseases, which can lead to higher crop yields, better crop health,
    and ultimately, increased profitability for farmers. SECTION II. LITERATURE SURVEY
    [1] Naseer, N., Haider, M. S., Ashfaq, U. A., & Abbas, G. The review discussed
    the various diseases that affect cotton crops, including fungal, bacterial, and
    viral diseases. The authors provided an overview of the symptoms of each disease
    and the methods used to detect them. For example, they discussed using remote
    sensing techniques, such as hyperspectral imaging and multispectral imaging, to
    identify cotton diseases based on changes in plant reflectance. [2] Zhang, X.,
    Liu, S., Zhao, G., & Li, Y. This paper provided a literature review on cotton
    leaf disease identification based on image processing and machine learning techniques.
    The authors highlighted the importance of early and accurate disease detection
    to effectively manage and control cotton diseases, which can significantly reduce
    crop yield and quality. [3] Goyal, V., & Singh, A. The review discussed various
    machine learning algorithms used for cotton disease detection, including support
    vector machines (SVM), decision trees, random forests, and artificial neural networks
    (ANN). The authors reviewed the advantages and limitations of each algorithm and
    evaluated their effectiveness for disease classification. [4] Abbas, W., Larijani,
    H. R., & Tufail, M. A. The review discussed the use of different machine learning
    algorithms, including decision trees, support vector machines (SVM), artificial
    neural networks (ANN), and random forests, for cotton disease detection and classification.
    The authors reviewed the advantages and limitations of each algorithm and evaluated
    their effectiveness for disease recognition. [5] Gajendra, S., & Ramasamy, K.
    provided a comprehensive survey of deep learning-based approaches for cotton disease
    identification. They reviewed different deep learning models, such as convolutional
    neural networks (CNNs) and recurrent neural networks (RNNs), and discussed their
    advantages and limitations. [6] Zhou, J., Wang, F., & Zuo, W. provided a literature
    review on cotton leaf disease identification methods based on support vector machines
    (SVM). The authors emphasized the importance of accurate and timely detection
    of cotton leaf diseases for the effective management of cotton crops. [7] Bhatia,
    D., Kumar, M., & Kumar, A. The review discussed different machine-learning techniques
    used for cotton plant disease identification, such as decision trees, artificial
    neural networks, support vector machines, and random forests. The authors evaluated
    the effectiveness of these techniques in accurately identifying different types
    of cotton plant diseases. [8] Jiang, Y., Feng, S., Wang, S., & Huang, X. This
    paper conducted a literature review on the recognition of cotton leaf diseases
    based on texture feature extraction and an improved support vector machine (SVM)
    algorithm. The authors emphasized the importance of accurate and timely detection
    of cotton leaf diseases for the effective management of cotton crops. [9] Kanade,
    S. R., Wadnerkar, R. B., & Lokhande, S. D. The review covered different image
    processing techniques used for cotton plant disease detection, such as segmentation,
    feature extraction, and classification. The authors evaluated the effectiveness
    of these techniques in accurately identifying different types of cotton plant
    diseases. [10] Li, W., Wang, S., Li, Y., & Jiang, Y. This paper conducted a literature
    review on the identification of cotton leaf diseases using backpropagation (BP)
    neural networks. The authors highlighted the importance of early detection and
    accurate identification of cotton plant diseases for effective crop management
    and improved crop yield. [11] Mrs. Shruti U, Dr. Nagaveni V, Dr. Raghavendra B
    K This paper presents a comprehensive review of various machine learning techniques
    for plant disease detection. The authors explain the challenges in the field of
    agriculture and highlight the importance of plant disease detection. They also
    provide a brief overview of various image-processing techniques for plant disease
    detection. [12] Nagesh, N. H., & Patil, C. G. The authors provided an overview
    of cotton leaf diseases and their impact on crop production, highlighting the
    need for accurate and efficient disease detection methods. They then describe
    the proposed method, which involves the use of fuzzy logic for feature extraction
    and classification, and GLCM for texture analysis. Based on the literature survey,
    the gap identified is research has done less specifically to the cotton crops
    and the usage of machine learning and deep learning algorithms for detection and
    prevention of the cotton crop diseases was less. This paper addresses these gaps.
    SECTION III. METHODOLOGY: The process of cotton crop disease identification using
    Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) typically
    involves several steps. Here''s a step-by-step process: Data Collection: The first
    step is to collect a large dataset of images of cotton crops affected by various
    diseases. These images should be labeled with information about the specific disease
    present in each image. Data Preprocessing: The collected images must be preprocessed
    to make them ready for the training of the CNN and RNN models. This step includes
    resizing the images to a uniform size, normalizing pixel values, and splitting
    the dataset into training and validation sets. CNN Training: The next step is
    to train a CNN model on the preprocessed dataset. The CNN model is responsible
    for learning relevant features from the input images. The model is trained using
    backpropagation to update the weights of the neural network to minimize the classification
    error. RNN training: The output of the CNN model is fed into an RNN model that
    is responsible for modeling the temporal sequence of image frames. The RNN model
    learns to capture the sequential dependencies of the input data and predict the
    disease present in the input sequence. Fine-tuning: Once the CNN and RNN models
    are trained, they can be fine-tuned to improve their accuracy on the specific
    dataset of interest. This involves retraining the models on a smaller subset of
    the data, to fine-tune the models'' parameters to fit the specific dataset better.
    Testing: Finally, the trained CNN and RNN models can be used to predict the disease
    present in new images of cotton crops. The models take in the new image data and
    predict the disease present in the crop. Overall, the process of cotton crop disease
    identification using CNN and RNN involves collecting and preprocessing the data,
    training the CNN and RNN models on the dataset, fine-tuning the models to improve
    accuracy, and using the trained models to predict the disease present in new images
    of cotton crops. Figure 3.1 Flow diagram of the process of cotton crop detection
    using CNN and RNN techniques. Show All Figure 3.2 Leaf disease detection using
    deep learning. The 1st leaf shows leaf spots, the 2nd leaf shows V. Wilt leaf
    defect, and the 3rd leaf shows diseased leaf. (Cotton-Plant-Disease-Prediction,
    anillava) Courtesy: Data set. Show All Figure 3.3 This figure shows various types
    of cotton leaves and their state compared to healthy leaf. Courtesy: Wikipedia
    Show All Figure 3.4 Spotting of leaf disease (metric learning) Courtesy: BioMed
    central Show All SECTION IV. IMPLEMENTATION: The datasets that were collected
    were tested according to the code that which have been implemented. Also, the
    code was developed by using CNN and RNN which are known as deep learning techniques.
    After executing the code, the images were tested according to the datasets (i.e,
    Disease affected, healthy). If the detection system detects any disease in the
    cotton crop, then the system says that the plant was affected, if not the system
    says the plant was healthy. Different algorithms like SVM, CNN, and RNN have been
    used to identify cotton crop diseases based on leaf images. CNN and RNN algorithms
    outperformed SVM in terms of accuracy in several studies. CNN achieved over 97%
    accuracy in one study, while RNN achieved over 95% in another study. The effectiveness
    of SVM and RNN was compared in one study, and RNN outperformed SVM. Deep learning
    algorithms, especially CNN and RNN, appear to be more effective for cotton crop
    disease identification than traditional machine learning algorithms. However,
    the performance of each algorithm can vary depending on the dataset and experimental
    setup used in each study. Further research is necessary to determine the best
    approach for cotton disease identification. Figure 4.1 Performance analysis of
    the cotton crop certainty website. Note: The specific performance of each algorithm
    can vary depending on the dataset and experimental setup used in each project.
    Show All SECTION V. RESULTS AND DISCUSSION As we know CNN has multiple layers
    in it, and every layer in it will learn how to detect the image by collecting
    input from an outsource. Also, RNN is something that works with sequences like
    finance, video, sound, text, and many more. Combining the functionality of both
    CNN and RNN can give some outcomes. And in the case of image processing, it works
    with images and sequences of words. To get well-detailed layers, it is necessary
    to apply a filter or layer to each image. At it produces an outcome that is more
    progressive and more detailed after each layer. The pixels of the image are fed
    to the CNN, at which it performs convolution operations, which generates a route
    to convolve the map. Then ReLU function is used, but before that convolved map
    will be applied to the ReLU function, to generate a rectified feature map. The
    image is processed further with multiple convolutions. The result of this shows
    the detection of affected and normal leaves in the cotton plant. It helps to extract
    the textural pattern of the defective leaf. The veins of the leaf which is contrast
    with the green color of the leaf which makes it difficult to take no notice of
    it. Figure 5.1 This diagram shows the model accuracy of the dataset. Show All
    Figure 5.2 This diagram shows the model loss of the dataset. Show All We are working
    on 2 datasets one is test data and other is train data. We are using 20 epochs
    with an average loss: 0.4620 and average accuracy: 0.8191. Average val_loss: 0.3957
    with average val_accuracy: 0.84 In Datasets the train dataset contains approx.
    1951 images and test datasets contain approx. 36. SECTION VI. CONCLUSION: In conclusion,
    the use of Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN)
    for cotton crop certainty identification has shown accurate results. The system
    involves the collection and preprocessing of a large dataset of images of cotton
    crops affected by various diseases, followed by training the CNN and RNN models
    on the dataset. The trained models are capable of accurately identifying the diseases
    present in new images of cotton crops, which can be a valuable tool for farmers
    and agriculture professionals. By identifying diseases in the early stages, farmers
    can take necessary actions to prevent the spread of the disease and minimize crop
    losses. However, it should be noted that the accuracy of the CNN and RNN models
    depends heavily on the quality and diversity of the training dataset. Therefore,
    it is essential to continue to collect and expand the dataset to improve the accuracy
    and robustness of the system. Furthermore, the system may also require regular
    updates and maintenance to adapt to new and emerging diseases. Overall, the use
    of CNN and RNN models for cotton crop disease identification has the potential
    to improve crop yields and reduce losses due to disease significantly, making
    it a valuable tool for the agricultural industry. SECTION VII. FUTURE WORKS Expansion
    to other crop diseases: While this system focused on cotton crop diseases, the
    same approach could be applied to other crop diseases as well. This would require
    the collection of more data and the development of new models but could lead to
    a more comprehensive system for plant disease identification. Real-time disease
    identification: The current system requires the input of an image and then outputs
    the classification result. However, it would be useful to have a system that could
    process live video feeds of crops and provide real-time disease identification.
    This would require optimization of the models and the integration of the system
    with the hardware that captures the video. Integration with precision agriculture
    systems: The disease identification system could be integrated with precision
    agriculture systems that provide recommendations for crop management. This would
    allow for targeted treatment of diseased areas of the crop and could potentially
    reduce the use of pesticides. Improved accuracy: While the accuracy of the current
    system is high, there is always room for improvement. More data could be collected
    to improve the training of the models, and alternative deep-learning techniques
    could be explored to improve the performance of the system. Deployment in the
    field: The current system is implemented as a software solution, but it would
    be useful to develop a portable hardware solution that could be used in the field.
    This would require optimization of the models for deployment on lower-power hardware
    and the development of a robust and reliable hardware solution that could withstand
    the harsh conditions of agricultural environments. Authors Figures References
    Keywords Metrics More Like This Impact Analysis of Stacked Machine Learning Algorithms
    Based Feature Selections for Deep Learning Algorithm Applied to Regression Analysis
    SoutheastCon 2022 Published: 2022 Crop Selection and Yield Prediction using Machine
    Learning Algorithms 2023 Second International Conference on Augmented Intelligence
    and Sustainable Systems (ICAISS) Published: 2023 Show More IEEE Personal Account
    CHANGE USERNAME/PASSWORD Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS
    Profile Information COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL
    INTERESTS Need Help? US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT
    & SUPPORT Follow About IEEE Xplore | Contact Us | Help | Accessibility | Terms
    of Use | Nondiscrimination Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy
    Policy A not-for-profit organization, IEEE is the world''s largest technical professional
    organization dedicated to advancing technology for the benefit of humanity. ©
    Copyright 2024 IEEE - All rights reserved."'
  inline_citation: (Maji et al., 2023)
  journal: 2023 14th International Conference on Computing Communication and Networking
    Technologies, ICCCNT 2023
  key_findings: The study demonstrated the effectiveness of combining CNNs and RNNs
    for cotton crop disease identification. The models achieved high accuracy in identifying
    diseases, which can assist farmers and agricultural professionals in timely and
    effective disease management practices.
  limitations: Lack of information on the specific methods used for visual monitoring
    and data analysis, such as the types of cameras, computer vision algorithms, and
    machine learning techniques employed.
  main_objective: To develop and evaluate a system for identifying diseases in cotton
    crops using image processing and machine learning techniques, specifically convolutional
    neural networks (CNNs) and recurrent neural networks (RNNs).
  relevance_evaluation: The study's focus on integrating high-resolution cameras and
    computer vision algorithms for visual monitoring of crop growth, including disease
    detection, aligns with the point of focus within the literature review on the
    integration of advanced monitoring techniques for automated irrigation systems.
    The use of deep learning techniques, specifically CNNs and RNNs, for disease detection
    is relevant to the review's intention of examining the integration of end-to-end
    automated irrigation systems.
  relevance_score: '0.75'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Image processing techniques, convolutional neural networks (CNNs),
    recurrent neural networks (RNNs)
  title: Cotton Crop Certainty Identification Using Deep Learning Techniques
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
