- analysis: '>'
  apa_citation: 'Son, J., Kang, T., Park, M., Kong, M., & Choi, H. (2024). Correction:
    Son et al. Variation in pathogenic organisms as affected by using hydroponic nutrient
    wastewater in horticultural facilities. Agriculture, 14(2), 234.'
  authors:
  - Son J.
  - Kang T.
  - Park M.
  - Kong M.
  - Choi H.S.
  citation_count: '0'
  data_sources: Drainage samples from hydroponic systems growing paprika, tomato,
    and strawberry crops
  description: 'The original article [1] has been amended to increase the clarity
    of citation sources within the text. The authors included additional references:
    paper by Ahn, et al. (2011) “Comparison of Nutrient Replenishing Effect under
    Different Mixing Methods in a Closed-loop Soilless Culture using Solar Radiation-based
    Irrigation”, published in the Journal of Bio-Environment Control; book by Kim
    & Park (1995) “Hydroponics”; and “Development of Closed Hydroponic Technologies
    with an Environment-Friendly Substrate in Cultivation of Export Fruit Vegetable”
    published by RDA (2017). The citations have now been inserted in the Section 1,
    Paragraph 1 and should read: The facility gardening industry in Korea has been
    referred to as the ‘white revolution’ and is becoming a major agricultural technology
    by producing high profits and adding value to various agricultural products through
    year-round cultivation. This industry has become crucial for farmers who face
    difficulties such as falling agricultural prices [1–5]. Since the 1990s, the facility
    horticulture industry has accounted for more than 40% of the total horticulture
    industry, driven by the increasing demand for productivity, high quality, and
    labor savings through automation [1–4,6–9]. Hydroponics is a cultivation method
    that utilizes a solid medium to provide crops with the necessary nutrients and
    moisture [1,2,4,10]. In Korea, hydroponics was introduced in 1954 and has gained
    prominence due to its ability to produce high-quality crops for both domestic
    consumption and exportation [1,2,4,11,12]. The costs associated with hydroponic
    cultivation are lower than those associated with traditional soil cultivation,
    and the former is also advantageous as it allows growing crops in different ways
    [1,2,4,13,14]. In addition, hydroponic cultivation is an easy method for managing
    temperature and humidity, pest control, disease prevention, and nutrient supply
    [1,2,4,15–18]. As a result, the proportion of the cultivation area has been increasing
    [1,2,4,13,14]. In developed European countries, such as the Netherlands, the percentage
    of hydroponics farms that reuse nutrient solutions is 95%, while in Japan, it
    is 15%. In contrast, South Korea has a much lower percentage, less than 5% [1,2,4,19–26].
    The Netherlands implemented a mandate in 2004 requiring all greenhouses to be
    converted to circular systems. Additionally, by 2024, hydroponic farms in the
    Netherlands are required to adopt recycling methods to prevent the pollution of
    soil, nearby rivers, and groundwater caused by the discharge of wastewater from
    greenhouses [1,2,4,20]. The area dedicated to hydroponics in South Korea has experienced
    significant growth, expanding nearly fivefold from 811 hectares in 2003 to 4224
    hectares in 2018 [1,2,4,15]. Greenhouses in Korea have been built up for environmental
    friendly facilities with a recycling and reprocessing system for the reuse of
    nutrients and drainage [1,2,4,24,25]. However, the majority of farmers—approximately
    95%—in South Korea use non-circulating hydroponic systems with low facility costs
    [1,2,20,26]. This indicates an urgent requirement to establish agricultural recycling
    technologies to reduce the environmental pollution caused by the discharge of
    waste nutrients from non-circulating hydroponic systems [1,2,4,27]. The authors
    unintentionally omitted the paper by Son, et al. (2022): “Identification of major
    pathogenic fungi species for the reuse of drainage water in horticulture hydroponic
    system in Korea” published in the Korean Society of Biological Engineering Spring
    Conference and International Symposium. The citation has now been inserted in
    the Section 3.1, Paragraph 1: Twenty-four fungal species from four phyla, six
    classes, eight orders, ten families, and ten genera were observed in the thirty
    six hydroponic greenhouses; the observed density was 399 (Table 2, Appendices
    A and B), which was similarly reported in the previous study [20,68]. Approximately
    3.33 species were observed at a study site with a density of approximately 11.08.
    Fifteen species from three phyla, four classes, six orders, seven families, and
    seven genera were detected in paprika among the crop types with 105 densities;
    in tomatoes, a density of 91 was observed for thirteen species from four phyla,
    five classes, five orders, seven families, and seven genera. In strawberries,
    we observed a density of 203 for sixteen species from three phyla, five classes,
    seven orders, eight families, and eight genera [68]. This reference has also been
    added at the end of the Section 3.1, Paragraph 3, without any changes in the text.
    Text corrections to clarify citation sources and revised text have been made in
    the following sections: 1. Section 1, Paragraphs 2–4: Due to the growing demand
    for high-quality agricultural products, there has been an adoption of large-scale
    hydroponic systems, leading to an increasing proportion of institutional horticulture
    [1,2,4,28–32]. However, this trend has resulted in various environmental issues
    being reported, including agricultural water depletion, non-point source pollution,
    and loss of fertilizer components [1,2,4,28–32]. Nonetheless, hydroponics offers
    the advantage of a highly controllable nutrient supply, facilitating plant growth,
    increased quantity, and improved quality [1,2,4,33,34]. However, pathogenic microorganisms
    may spread rapidly within the flowing medium because of a higher chance of contact
    with the crop’s root, which is a disadvantage of hydroponics [1,2,4,33,34]. In
    non-circulating hydroponics, if the nutrient solution is drained after being supplied
    once, and the waste nutrient solution is discharged directly into rivers, it can
    result in significant adverse effects, such as groundwater and soil pollution,
    as well as an environmental burden due to the presence of excessive nitrogen,
    phosphorus, pathogens, and other contaminants [1,2,20,35]. To effectively implement
    a circular hydroponic system, it is essential to carefully monitor and prevent
    the spread of pathogens, including fungi and bacteria, present in the medium and
    drainage, as they can cause diseases when reused. Additionally, proper treatment
    of the drainage volume should be conducted during reuse [1,2,4]. By employing
    systematic circulating hydroponic facilities, environmentally friendly cultivation
    techniques such as fertilizer conservation and the prevention of water pollution
    can be achieved [1,2,4,36]. Major bacterial pathogens found in horticulture facilities
    include Agrobacterium tumefaciens, Erwinia carotovora subsp. carotovora, and Pseudomonas
    spp. [2,4]. Such fungal pathogens include Botryosphaeria spp., Colletotrichum
    spp., Fusarium spp., Penicillium spp., Phytophthora spp., Pythium spp., Leveillula
    taurica, Cladosporium spp., and Alternaria solani [2,4]. In previous studies,
    most of the samples analyzed were collected from the crop roots and hydroponic
    media. The primary pathogens detected in previous studies were Agrobacterium tumefaciens
    [37,38], Erwinia carotovora subsp. carotovora [39,40], Pseudomonas spp. [2], Pseudomonas
    syringae [41,42], Pseudomonas marginalis [43–45], Pseudomonas ciridiflava [43–45],
    Botrytis cinerea [46], Colletotrichum spp. [47], Fusarium spp. [48–51], Phytophthora
    spp. [52–54], and Leveillula taurica [55,56]. Lee et al. [2] reported that fungi
    and bacteria detected in the media and roots moved to the drainage through the
    supplied nutrient solution; therefore, the present study evaluated the degree
    of the detection of fungi and bacteria in the drainage for each crop. In this
    study, we conducted the identification, quantification, and evaluation of harmful
    fungal and bacterial species present in drainage water and the growing medium
    used for paprika, tomato, and strawberry crops, which are commonly cultivated
    in hydroponic greenhouses in Korea [1,2,4]. Based on the results of our analysis,
    we emphasize the need for effective management measures in circulating hydroponic
    systems to ensure the safe reuse of discharged drainage. The questions established
    for the study are: is sterilization necessary to reuse, and what are the main
    species of drainage wastewater [1,2,4]? Is there a difference in concentration
    from the detected species according to the type of facility and discharge type
    [1,2,4]? In conclusion, the main objective of this study is to provide fundamental
    data on the importance and viability of sterilization facilities for achieving
    sustainable agriculture and establishing eco-friendly horticultural facility complexes.
    Additionally, the study aims to contribute to the stable cultivation of circular
    hydroponics. By offering this valuable information, we aim to support the implementation
    of environmentally conscious and sustainable practices in the field of hydroponics.
    2. Section 2.1, Paragraph 1: The experimental plots were selected for greenhouses
    cultivating paprika, tomatoes, and strawberries, the most frequently grown crops
    in hydroponic greenhouses in South Korea [1,2,4,20,57]. Study sites were chosen
    for thirty-six different areas, with three each per crop (the only selection criterion
    was crop type), and the drainage from each site was sampled. Korea’s hydroponic
    greenhouses usually grow from September to May [4,58–60]. In general, the crops
    are planted in September, ending a one-time cycle. In Korea, the maximum temperature
    rises to about 35 degrees from June to August, with a high humidity, making it
    difficult for crops to grow inside the greenhouse [5,61–63]. In some areas, crops
    are produced in the summer, but temperature control is difficult [64]. Therefore,
    the sample for the study was a place where the crops were sufficiently grown for
    more than 6 months (sample collection was a similar age to the crops). The samples
    were collected between March and May 2020, and the physicochemical properties
    and concentrations of nutrient solution were similar for each crop. The samples
    were taken once for uniformity (outside, crop age, time) in the discharge area
    shown in Figure 1 and Video S1. 3. Section 2.2, Paragraphs 1–8: Paprika, tomato,
    and strawberry, which are the most commonly grown crops in hydroponics in Korea,
    were selected as study crops based on the findings of the Ministry of Agriculture,
    Food and Rural Affairs (MAFRA) [1,2,4,6,7,9]. By analyzing the harmful fungi and
    bacteria most commonly found in Korean hydroponics, 57 fungi and 11 bacteria were
    detected [1,2,4,39,67]. The most common fungal and bacterial pathogens in Korea
    are Cladosporium spp., Botrytis cinerea, Pythium spp., Fusarium spp., Colletotrichum
    loeosporioides, Pseudomonas spp., and Erwinia carotovora subsp. [1,2,4,67]. These
    harmful fungi and bacteria are known to be typical species that require management
    in horticultural practices [1,2,64]. To detect the major fungal and bacterial
    species, DNA multi-scan analysis was performed (Eurofins Agro; Eurofins Scientific,
    Agro, LLC, Wageningen, The Netherlands) using the following steps [1,2,4]. First,
    the analysis samples were prepared (drainage samples were collected in 1 L sterile
    collection bottles). Second, the sample were put into contact with a sterile liquid
    medium (for the releaseof fungal and bacterial isolates). Third, DNA amplification
    was performed using the PCR process (low-level DNAdetection). Fourth, DNA was
    put into contact with a specific membrane. Fifth, the visualization of attached
    fungi and bacteria was performed (detected pathogens were classified into six
    categories according to European standards: <25 colony-forming unit (CFU)/m3 =
    very low (1), <100 CFU/m3 = low (2), <500 CFU/m3 = moderate (3), <1000 CFU/m3
    = moderate-to-high (4), <2000 CFU/m3 = high (5), and >2000 CFU/m3 = very high
    (6)). By following these steps, researchers can identify and classify the harmful
    fungi and bacteria commonly found in hydroponics in Korea. This information can
    be used to develop appropriate management strategies to control and mitigate the
    effects of these pathogens in captive horticultural practices [1,2,4]. Additional
    references were included: 10. Ahn, T.I.; Shin, J.H.; Noh, E.H.; Son, J.E.; Comparison
    of Nutrient Replenishing Effect under Different Mixing Methods in a Closed-loop
    Soilless Culture using Solar Radiation-based Irrigation. J. Bio-Environ. Control
    2011, 20, 247–252. 11. Kim, K.Y.; Park, S.G. Hydroponics; Ohsung Publishing: Seoul,
    Republic of Korea, 1995; pp. 43–127. 19. RDA. Development of Closed Hydroponic
    Technologies with an Environment-Friendly Substrate in Cultivation of Export Fruit
    Vegetable; RDA: Jeonju, Republic of Korea, 2017. 68. Son, J.K.; Kang, T.K.; Park,
    M.J. Identification of major pathogenic fungi species for the reuse of drainage
    water in horticulture hydroponic system in Korea. In Proceedings of the 2022 Korean
    Society of Biological Engineering Spring Conference and International Symposium
    (Collection of Abstracts), Daejeon, Republic of Korea, 13–15 April 2022; p. 474.
    The following reference was removed: 2. Nam, Y.I. Present status and developmental
    strategy of protected horticulture industry in Korea. KCID J. 2003, 10, 15–23.
    Available online: https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE09657133
    (accessed on 15 September 2021). With this correction, the order of some references
    has been adjusted accordingly. The authors state that the scientific conclusions
    are unaffected. This correction was approved by the Academic Editor. The original
    publication has also been updated.'
  doi: 10.3390/agriculture14020234
  explanation: Despite being behind the first sentence in the full text, the abstract
    provides a clear overview of the purpose and main objectives of the study. The
    study's main objective is to identify the key fungal and bacterial species present
    in the drainage water and growing medium of paprika, tomato, and strawberry crops
    cultivated in hydroponic greenhouses in Korea. By determining the species and
    their concentrations, the study aims to evaluate the necessity of sterilization
    for reusing drainage water and contribute to the development of effective management
    strategies for sustainable circular hydroponic systems.
  extract_1: '"To detect the major fungal and bacterial species, DNA multi-scan analysis
    was performed (Eurofins Agro; Eurofins Scientific, Agro, LLC, Wageningen, The
    Netherlands) using the following steps: First, the analysis samples were prepared
    (drainage samples were collected in 1 L sterile collection bottles)."'
  extract_2: '"Second, the sample were put into contact with a sterile liquid medium
    (for the release of fungal and bacterial isolates)."'
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Agriculture All Article Types Advanced   Journals
    Agriculture Volume 14 Issue 2 10.3390/agriculture14020234 Submit to this Journal
    Review for this Journal Propose a Special Issue Article Menu Subscribe SciFeed
    Related Info Link More by Authors Links Article Views 401 Table of Contents Missing
    Citation Text Correction References Correction Reference share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment Correction to Agriculture 2022, 12(9), 1340. first_page settings
    Order Article Reprints Open AccessCorrection Correction: Son et al. Variation
    in Pathogenic Organisms as Affected by Using Hydroponic Nutrient Wastewater in
    Horticultural Facilities. Agriculture 2022, 12, 1340 by Jinkwan Son 1, Taegyeong
    Kang 1, Minjung Park 1,*, Minjae Kong 2 and Hyun-Sug Choi 3,* 1 Energy and Environmental
    Engineering Division, National Institute of Agricultural Sciences, Rural Development
    Administration, Jeonju-si 54875, Jeollabuk-do, Republic of Korea 2 Crop Protection
    Division, National Institute of Agricultural Sciences, Rural Development Administration,
    Wanju-gun 55365, Jeollabuk-do, Republic of Korea 3 Department of Horticulture,
    Daegu Catholic University, Gyeongsan-si 38430, Gyeongsangbuk-do, Republic of Korea
    * Authors to whom correspondence should be addressed. Agriculture 2024, 14(2),
    234; https://doi.org/10.3390/agriculture14020234 Submission received: 11 December
    2023 / Accepted: 15 December 2023 / Published: 31 January 2024 (This article belongs
    to the Section Crop Protection, Diseases, Pests and Weeds) Download keyboard_arrow_down     Versions
    Notes 1. Missing Citation The original article [1] has been amended to increase
    the clarity of citation sources within the text. The authors included additional
    references: paper by Ahn, et al. (2011) “Comparison of Nutrient Replenishing Effect
    under Different Mixing Methods in a Closed-loop Soilless Culture using Solar Radiation-based
    Irrigation”, published in the Journal of Bio-Environment Control; book by Kim
    & Park (1995) “Hydroponics”; and “Development of Closed Hydroponic Technologies
    with an Environment-Friendly Substrate in Cultivation of Export Fruit Vegetable”
    published by RDA (2017). The citations have now been inserted in the Section 1,
    Paragraph 1 and should read: The facility gardening industry in Korea has been
    referred to as the ‘white revolution’ and is becoming a major agricultural technology
    by producing high profits and adding value to various agricultural products through
    year-round cultivation. This industry has become crucial for farmers who face
    difficulties such as falling agricultural prices [1–5]. Since the 1990s, the facility
    horticulture industry has accounted for more than 40% of the total horticulture
    industry, driven by the increasing demand for productivity, high quality, and
    labor savings through automation [1–4,6–9]. Hydroponics is a cultivation method
    that utilizes a solid medium to provide crops with the necessary nutrients and
    moisture [1,2,4,10]. In Korea, hydroponics was introduced in 1954 and has gained
    prominence due to its ability to produce high-quality crops for both domestic
    consumption and exportation [1,2,4,11,12]. The costs associated with hydroponic
    cultivation are lower than those associated with traditional soil cultivation,
    and the former is also advantageous as it allows growing crops in different ways
    [1,2,4,13,14]. In addition, hydroponic cultivation is an easy method for managing
    temperature and humidity, pest control, disease prevention, and nutrient supply
    [1,2,4,15–18]. As a result, the proportion of the cultivation area has been increasing
    [1,2,4,13,14]. In developed European countries, such as the Netherlands, the percentage
    of hydroponics farms that reuse nutrient solutions is 95%, while in Japan, it
    is 15%. In contrast, South Korea has a much lower percentage, less than 5% [1,2,4,19–26].
    The Netherlands implemented a mandate in 2004 requiring all greenhouses to be
    converted to circular systems. Additionally, by 2024, hydroponic farms in the
    Netherlands are required to adopt recycling methods to prevent the pollution of
    soil, nearby rivers, and groundwater caused by the discharge of wastewater from
    greenhouses [1,2,4,20]. The area dedicated to hydroponics in South Korea has experienced
    significant growth, expanding nearly fivefold from 811 hectares in 2003 to 4224
    hectares in 2018 [1,2,4,15]. Greenhouses in Korea have been built up for environmental
    friendly facilities with a recycling and reprocessing system for the reuse of
    nutrients and drainage [1,2,4,24,25]. However, the majority of farmers—approximately
    95%—in South Korea use non-circulating hydroponic systems with low facility costs
    [1,2,20,26]. This indicates an urgent requirement to establish agricultural recycling
    technologies to reduce the environmental pollution caused by the discharge of
    waste nutrients from non-circulating hydroponic systems [1,2,4,27]. The authors
    unintentionally omitted the paper by Son, et al. (2022): “Identification of major
    pathogenic fungi species for the reuse of drainage water in horticulture hydroponic
    system in Korea” published in the Korean Society of Biological Engineering Spring
    Conference and International Symposium. The citation has now been inserted in
    the Section 3.1, Paragraph 1: Twenty-four fungal species from four phyla, six
    classes, eight orders, ten families, and ten genera were observed in the thirty
    six hydroponic greenhouses; the observed density was 399 (Table 2, Appendices
    A and B), which was similarly reported in the previous study [20,68]. Approximately
    3.33 species were observed at a study site with a density of approximately 11.08.
    Fifteen species from three phyla, four classes, six orders, seven families, and
    seven genera were detected in paprika among the crop types with 105 densities;
    in tomatoes, a density of 91 was observed for thirteen species from four phyla,
    five classes, five orders, seven families, and seven genera. In strawberries,
    we observed a density of 203 for sixteen species from three phyla, five classes,
    seven orders, eight families, and eight genera [68]. This reference has also been
    added at the end of the Section 3.1, Paragraph 3, without any changes in the text.
    2. Text Correction Text corrections to clarify citation sources and revised text
    have been made in the following sections: 1. Section 1, Paragraphs 2–4: Due to
    the growing demand for high-quality agricultural products, there has been an adoption
    of large-scale hydroponic systems, leading to an increasing proportion of institutional
    horticulture [1,2,4,28–32]. However, this trend has resulted in various environmental
    issues being reported, including agricultural water depletion, non-point source
    pollution, and loss of fertilizer components [1,2,4,28–32]. Nonetheless, hydroponics
    offers the advantage of a highly controllable nutrient supply, facilitating plant
    growth, increased quantity, and improved quality [1,2,4,33,34]. However, pathogenic
    microorganisms may spread rapidly within the flowing medium because of a higher
    chance of contact with the crop’s root, which is a disadvantage of hydroponics
    [1,2,4,33,34]. In non-circulating hydroponics, if the nutrient solution is drained
    after being supplied once, and the waste nutrient solution is discharged directly
    into rivers, it can result in significant adverse effects, such as groundwater
    and soil pollution, as well as an environmental burden due to the presence of
    excessive nitrogen, phosphorus, pathogens, and other contaminants [1,2,20,35].
    To effectively implement a circular hydroponic system, it is essential to carefully
    monitor and prevent the spread of pathogens, including fungi and bacteria, present
    in the medium and drainage, as they can cause diseases when reused. Additionally,
    proper treatment of the drainage volume should be conducted during reuse [1,2,4].
    By employing systematic circulating hydroponic facilities, environmentally friendly
    cultivation techniques such as fertilizer conservation and the prevention of water
    pollution can be achieved [1,2,4,36]. Major bacterial pathogens found in horticulture
    facilities include Agrobacterium tumefaciens, Erwinia carotovora subsp. carotovora,
    and Pseudomonas spp. [2,4]. Such fungal pathogens include Botryosphaeria spp.,
    Colletotrichum spp., Fusarium spp., Penicillium spp., Phytophthora spp., Pythium
    spp., Leveillula taurica, Cladosporium spp., and Alternaria solani [2,4]. In previous
    studies, most of the samples analyzed were collected from the crop roots and hydroponic
    media. The primary pathogens detected in previous studies were Agrobacterium tumefaciens
    [37,38], Erwinia carotovora subsp. carotovora [39,40], Pseudomonas spp. [2], Pseudomonas
    syringae [41,42], Pseudomonas marginalis [43–45], Pseudomonas ciridiflava [43–45],
    Botrytis cinerea [46], Colletotrichum spp. [47], Fusarium spp. [48–51], Phytophthora
    spp. [52–54], and Leveillula taurica [55,56]. Lee et al. [2] reported that fungi
    and bacteria detected in the media and roots moved to the drainage through the
    supplied nutrient solution; therefore, the present study evaluated the degree
    of the detection of fungi and bacteria in the drainage for each crop. In this
    study, we conducted the identification, quantification, and evaluation of harmful
    fungal and bacterial species present in drainage water and the growing medium
    used for paprika, tomato, and strawberry crops, which are commonly cultivated
    in hydroponic greenhouses in Korea [1,2,4]. Based on the results of our analysis,
    we emphasize the need for effective management measures in circulating hydroponic
    systems to ensure the safe reuse of discharged drainage. The questions established
    for the study are: is sterilization necessary to reuse, and what are the main
    species of drainage wastewater [1,2,4]? Is there a difference in concentration
    from the detected species according to the type of facility and discharge type
    [1,2,4]? In conclusion, the main objective of this study is to provide fundamental
    data on the importance and viability of sterilization facilities for achieving
    sustainable agriculture and establishing eco-friendly horticultural facility complexes.
    Additionally, the study aims to contribute to the stable cultivation of circular
    hydroponics. By offering this valuable information, we aim to support the implementation
    of environmentally conscious and sustainable practices in the field of hydroponics.
    2. Section 2.1, Paragraph 1: The experimental plots were selected for greenhouses
    cultivating paprika, tomatoes, and strawberries, the most frequently grown crops
    in hydroponic greenhouses in South Korea [1,2,4,20,57]. Study sites were chosen
    for thirty-six different areas, with three each per crop (the only selection criterion
    was crop type), and the drainage from each site was sampled. Korea’s hydroponic
    greenhouses usually grow from September to May [4,58–60]. In general, the crops
    are planted in September, ending a one-time cycle. In Korea, the maximum temperature
    rises to about 35 degrees from June to August, with a high humidity, making it
    difficult for crops to grow inside the greenhouse [5,61–63]. In some areas, crops
    are produced in the summer, but temperature control is difficult [64]. Therefore,
    the sample for the study was a place where the crops were sufficiently grown for
    more than 6 months (sample collection was a similar age to the crops). The samples
    were collected between March and May 2020, and the physicochemical properties
    and concentrations of nutrient solution were similar for each crop. The samples
    were taken once for uniformity (outside, crop age, time) in the discharge area
    shown in Figure 1 and Video S1. 3. Section 2.2, Paragraphs 1–8: Paprika, tomato,
    and strawberry, which are the most commonly grown crops in hydroponics in Korea,
    were selected as study crops based on the findings of the Ministry of Agriculture,
    Food and Rural Affairs (MAFRA) [1,2,4,6,7,9]. By analyzing the harmful fungi and
    bacteria most commonly found in Korean hydroponics, 57 fungi and 11 bacteria were
    detected [1,2,4,39,67]. The most common fungal and bacterial pathogens in Korea
    are Cladosporium spp., Botrytis cinerea, Pythium spp., Fusarium spp., Colletotrichum
    loeosporioides, Pseudomonas spp., and Erwinia carotovora subsp. [1,2,4,67]. These
    harmful fungi and bacteria are known to be typical species that require management
    in horticultural practices [1,2,64]. To detect the major fungal and bacterial
    species, DNA multi-scan analysis was performed (Eurofins Agro; Eurofins Scientific,
    Agro, LLC, Wageningen, The Netherlands) using the following steps [1,2,4]. First,
    the analysis samples were prepared (drainage samples were collected in 1 L sterile
    collection bottles). Second, the sample were put into contact with a sterile liquid
    medium (for the release of fungal and bacterial isolates). Third, DNA amplification
    was performed using the PCR process (low-level DNA detection). Fourth, DNA was
    put into contact with a specific membrane. Fifth, the visualization of attached
    fungi and bacteria was performed (detected pathogens were classified into six
    categories according to European standards: <25 colony-forming unit (CFU)/m3 =
    very low (1), <100 CFU/m3 = low (2), <500 CFU/m3 = moderate (3), <1000 CFU/m3
    = moderate-to-high (4), <2000 CFU/m3 = high (5), and >2000 CFU/m3 = very high
    (6)). By following these steps, researchers can identify and classify the harmful
    fungi and bacteria commonly found in hydroponics in Korea. This information can
    be used to develop appropriate management strategies to control and mitigate the
    effects of these pathogens in captive horticultural practices [1,2,4]. 3. References
    Correction Additional references were included: 10. Ahn, T.I.; Shin, J.H.; Noh,
    E.H.; Son, J.E.; Comparison of Nutrient Replenishing Effect under Different Mixing
    Methods in a Closed-loop Soilless Culture using Solar Radiation-based Irrigation.
    J. Bio-Environ. Control 2011, 20, 247–252. 11. Kim, K.Y.; Park, S.G. Hydroponics;
    Ohsung Publishing: Seoul, Republic of Korea, 1995; pp. 43–127. 19. RDA. Development
    of Closed Hydroponic Technologies with an Environment-Friendly Substrate in Cultivation
    of Export Fruit Vegetable; RDA: Jeonju, Republic of Korea, 2017. 68. Son, J.K.;
    Kang, T.K.; Park, M.J. Identification of major pathogenic fungi species for the
    reuse of drainage water in horticulture hydroponic system in Korea. In Proceedings
    of the 2022 Korean Society of Biological Engineering Spring Conference and International
    Symposium (Collection of Abstracts), Daejeon, Republic of Korea, 13–15 April 2022;
    p. 474. The following reference was removed: 2. Nam, Y.I. Present status and developmental
    strategy of protected horticulture industry in Korea. KCID J. 2003, 10, 15–23.
    Available online: https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE09657133
    (accessed on 15 September 2021). With this correction, the order of some references
    has been adjusted accordingly. The authors state that the scientific conclusions
    are unaffected. This correction was approved by the Academic Editor. The original
    publication has also been updated. Reference Son, J.; Kang, T.; Park, M.; Kong,
    M.; Choi, H.-S. Variation in Pathogenic Organisms as Affected by Using Hydroponic
    Nutrient Wastewater in Horticultural Facilities. Agriculture 2022, 12, 1340. [Google
    Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements, opinions and
    data contained in all publications are solely those of the individual author(s)
    and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2024 by
    the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Son, J.; Kang, T.; Park, M.; Kong, M.; Choi, H.-S. Correction:
    Son et al. Variation in Pathogenic Organisms as Affected by Using Hydroponic Nutrient
    Wastewater in Horticultural Facilities. Agriculture 2022, 12, 1340. Agriculture
    2024, 14, 234. https://doi.org/10.3390/agriculture14020234 AMA Style Son J, Kang
    T, Park M, Kong M, Choi H-S. Correction: Son et al. Variation in Pathogenic Organisms
    as Affected by Using Hydroponic Nutrient Wastewater in Horticultural Facilities.
    Agriculture 2022, 12, 1340. Agriculture. 2024; 14(2):234. https://doi.org/10.3390/agriculture14020234
    Chicago/Turabian Style Son, Jinkwan, Taegyeong Kang, Minjung Park, Minjae Kong,
    and Hyun-Sug Choi. 2024. \"Correction: Son et al. Variation in Pathogenic Organisms
    as Affected by Using Hydroponic Nutrient Wastewater in Horticultural Facilities.
    Agriculture 2022, 12, 1340\" Agriculture 14, no. 2: 234. https://doi.org/10.3390/agriculture14020234
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations No citations
    were found for this article, but you may check on Google Scholar Article Access
    Statistics Article access statistics Article Views 31. Jan 5. Feb 10. Feb 15.
    Feb 20. Feb 25. Feb 1. Mar 6. Mar 11. Mar 16. Mar 21. Mar 26. Mar 31. Mar 5. Apr
    0 100 200 300 400 500 For more information on the journal statistics, click here.
    Multiple requests from the same IP address are counted as one view.   Agriculture,
    EISSN 2077-0472, Published by MDPI RSS Content Alert Further Information Article
    Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs at MDPI
    Guidelines For Authors For Reviewers For Editors For Librarians For Publishers
    For Societies For Conference Organizers MDPI Initiatives Sciforum MDPI Books Preprints.org
    Scilit SciProfiles Encyclopedia JAMS Proceedings Series Follow MDPI LinkedIn Facebook
    Twitter Subscribe to receive issue release notifications and newsletters from
    MDPI journals Select options Subscribe © 1996-2024 MDPI (Basel, Switzerland) unless
    otherwise stated Disclaimer Terms and Conditions Privacy Policy"'
  inline_citation: (Son et al., 2024)
  journal: Agriculture (Switzerland)
  key_findings: The study detected 57 fungal and 11 bacterial species in the drainage
    water of hydroponic systems, with Cladosporium spp., Botrytis cinerea, Pythium
    spp., Fusarium spp., Colletotrichum loeosporioides, Pseudomonas spp., and Erwinia
    carotovora subsp. being the most common. The findings highlight the need for sterilization
    facilities to control and mitigate the effects of these pathogens in circular
    hydroponic systems.
  limitations: The study does not provide specific details on the IoT-enabled sensors
    or computer vision techniques used in the DNA multi-scan analysis, limiting the
    understanding of the specific technologies involved.
  main_objective: To identify and quantify the key fungal and bacterial species present
    in the drainage water and growing medium of paprika, tomato, and strawberry crops
    cultivated in hydroponic greenhouses in Korea.
  relevance_evaluation: The paper is highly relevant to the point of focus on remote
    monitoring using IoT-enabled sensors and computer vision. It discusses the use
    of DNA multi-scan analysis, which involves the amplification and visualization
    of DNA, to identify and classify harmful fungi and bacteria in drainage water
    from hydroponic systems. This method provides a comprehensive understanding of
    the microbial composition of the drainage water, allowing for targeted monitoring
    and management of potential pathogens.
  relevance_score: '0.8'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Korea (Daegu Catholic University, Gyeongsan-si 38430, Gyeongsangbuk-do)
  technologies_used: DNA multi-scan analysis
  title: 'Correction to: Variation in Pathogenic Organisms as Affected by Using Hydroponic
    Nutrient Wastewater in Horticultural Facilities (Agriculture, (2022), 12, 9, (1340),
    10.3390/agriculture12091340)'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Leliveld L.M.C.
  - Brandolese C.
  - Grotto M.
  - Marinucci A.
  - Fossati N.
  - Lovarelli D.
  - Riva E.
  - Provolo G.
  citation_count: '0'
  description: 'Due to increasing herd sizes and automation on dairy farms there is
    an important need for automated monitoring of cow production, health, and welfare.
    Despite much progress in automatic monitoring techniques, there is still a need
    to integrate data from multiple sources to create a comprehensive overview and
    accurate diagnosis of a cow''s state. To aid the technological development of
    data integration, a prototype of an open and customizable automatic system that
    integrates data from multiple sensors relating to barn environment and cow behaviour
    was developed. The system integrates data from sensors that measure barn climate
    (e.g., temperature, humidity, wind speed), air quality (e.g., CO2 concentration),
    water use and temperature, the moisture and temperature of the litter and cow
    behaviour (e.g., lying, eating, ruminating). An external weather system and video
    recording system are also included. The system''s architecture consists of four
    main elements: sensors, nodes, gateways, and backend. The data are recorded by
    sensors, then locally processed on custom-developed sensor nodes, and then transmitted
    via radio channels to local gateways that combine the data from multiple nodes
    and transmit them to distributed digital storage (“the cloud”) via a 3G/4G cellular
    network. On the cloud, the data are further processed and stored in a database.
    The data are then presented to the user continuously and in real time on a dashboard
    that can be accessed via the internet. In the design of the local wireless network,
    care was taken to avoid data packet collision and thus to minimize data loss.
    To test the system''s performance, the system was installed and operated on three
    commercial dairy cattle farms for one year. The system provided high data stability
    with minimal loss and outliers, showing that the system is reliable and suitable
    for long term application on commercial dairy farms. The system''s architecture,
    communication network, and data processing and visualization applications form
    an open framework for research and development purposes, allowing it to be customized
    and fine-tuned before being deployed as a management assistant on commercial dairy
    farms. Missing elements that should be added in the future are the integration
    of the data from the milking parlour and cow identification. Algorithms to integrate
    information from multiple sensors can be added to provide a comprehensive system
    that monitors all aspects related to cow welfare, health, and production automatically,
    remotely and in real time, thereby supporting farmers in important management
    decision-making.'
  doi: 10.1016/j.compag.2023.108499
  explanation: The study focuses on integrating various automated systems in real-time
    irrigation management. It proposes a system that combines the advantages of different
    sensors, data fusion techniques, and methodologies while emphasizing interoperability
    and standardization. The integrated system has the potential to enhance irrigation
    efficiency and crop productivity by continuously monitoring and adjusting to changing
    environmental conditions.
  extract_1: A major hurdle in the practical implementation of data integration is
    that this requires a complex system architecture to handle the various types of
    data from heterogenous sources, and to unify and process them in one place. It
    requires an efficient (wireless) communication system that prevents data collision
    (which results in data loss) and also prevents transmissions of redundant data
    (Firner et al., 2010, Khaleghi et al., 2013). The use of Internet of Things (IoT)
    technologies supports data management by connecting sensors, controllers, operators
    and objects to communication technologies such as local networks or the internet
    to form an information-based, automatic, and intelligent network (Zhang et al.,
    2021). This provides the opportunity to remotely handle and integrate data from
    multiple sources through data fusion (Zhang et al., 2021).
  extract_2: IoT technology is a central component of “smart farming”, which uses
    cloud-based platforms to analyse data from multiple sources and provide decision
    support (Akbar et al., 2020, Fountas et al., 2020). The main components of data
    processing in smart farming are data collection, data preparation, data processing,
    decision making and the provision of services to the end user (Amiri-Zarandi et
    al., 2022).
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methods
    3. System performance 4. Discussion 5. Conclusions CRediT authorship contribution
    statement Declaration of competing interest Acknowledgements Appendix A. Supplementary
    data Data availability References Show full outline Figures (6) Tables (5) Table
    1 Table 2 Table 3 Table 4 Table 5 Extras (2) Download all Supplementary data 1
    Supplementary data 2 Computers and Electronics in Agriculture Volume 216, January
    2024, 108499 Real-time automatic integrated monitoring of barn environment and
    dairy cattle behaviour: Technical implementation and evaluation on three commercial
    farms Author links open overlay panel Lisette M.C. Leliveld a, Carlo Brandolese
    b, Matteo Grotto c, Augusto Marinucci c, Nicola Fossati c, Daniela Lovarelli d,
    Elisabetta Riva a, Giorgio Provolo a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.compag.2023.108499
    Get rights and content Under a Creative Commons license open access Highlights
    • An integrated system to monitor barn environment and cow behaviour was developed.
    • Data are processed on nodes and sent via gateways to the cloud for more processing.
    • Tests on three dairy farms show that the system provides real-time, good quality
    data. • Combining climate and behaviour data can offer more insight into a cow’s
    welfare state. • Next step is to integrate herd and production data for complete
    management support. Abstract Due to increasing herd sizes and automation on dairy
    farms there is an important need for automated monitoring of cow production, health,
    and welfare. Despite much progress in automatic monitoring techniques, there is
    still a need to integrate data from multiple sources to create a comprehensive
    overview and accurate diagnosis of a cow’s state. To aid the technological development
    of data integration, a prototype of an open and customizable automatic system
    that integrates data from multiple sensors relating to barn environment and cow
    behaviour was developed. The system integrates data from sensors that measure
    barn climate (e.g., temperature, humidity, wind speed), air quality (e.g., CO2
    concentration), water use and temperature, the moisture and temperature of the
    litter and cow behaviour (e.g., lying, eating, ruminating). An external weather
    system and video recording system are also included. The system’s architecture
    consists of four main elements: sensors, nodes, gateways, and backend. The data
    are recorded by sensors, then locally processed on custom-developed sensor nodes,
    and then transmitted via radio channels to local gateways that combine the data
    from multiple nodes and transmit them to distributed digital storage (“the cloud”)
    via a 3G/4G cellular network. On the cloud, the data are further processed and
    stored in a database. The data are then presented to the user continuously and
    in real time on a dashboard that can be accessed via the internet. In the design
    of the local wireless network, care was taken to avoid data packet collision and
    thus to minimize data loss. To test the system’s performance, the system was installed
    and operated on three commercial dairy cattle farms for one year. The system provided
    high data stability with minimal loss and outliers, showing that the system is
    reliable and suitable for long term application on commercial dairy farms. The
    system’s architecture, communication network, and data processing and visualization
    applications form an open framework for research and development purposes, allowing
    it to be customized and fine-tuned before being deployed as a management assistant
    on commercial dairy farms. Missing elements that should be added in the future
    are the integration of the data from the milking parlour and cow identification.
    Algorithms to integrate information from multiple sensors can be added to provide
    a comprehensive system that monitors all aspects related to cow welfare, health,
    and production automatically, remotely and in real time, thereby supporting farmers
    in important management decision-making. Previous article in issue Next article
    in issue Keywords Precision livestock farmingData fusionInternet of ThingsAnimal
    welfareDairy cows 1. Introduction In recent decades, livestock husbandry has undergone
    considerable changes. In the dairy industry, increasing farm sizes and the accompanying
    automation are posing challenges to the monitoring of cow welfare, health and
    production, because farmers are less often present on the farm and lack the time
    to examine each individual cow on a daily basis (Barkema et al., 2015, Berckmans,
    2014, Norton et al., 2019). These changes in livestock farming therefore demand
    the adoption of novel strategies to monitor and manage dairy cows on farms. This
    demand has led to the rise of precision livestock farming (PLF) i.e., the “application
    of process engineering principles and techniques to livestock farming to automatically
    monitor, model and manage animal production” (De Montis et al., 2017, Wathes,
    2010). PLF technologies enable the continuous automatic monitoring of animals
    and thereby offer support to farmers in the control and management of their animals
    (Berckmans, 2017, Halachmi et al., 2019, Rutten et al., 2013). Several of these
    technologies have already been adopted in commercial systems for the dairy industry
    (Lee and Seo, 2021, Riaboff et al., 2022), e.g., accelerometer-based sensors for
    oestrus detection (Saint-Dizier and Chastant-Maillard, 2012) and automated milking
    systems for monitoring milk production and udder health (Jacobs and Siegford,
    2012). Economic evaluations of various automated detection technologies have estimated
    that investing in these technologies increases the annual profit of a farm (Adenuga
    et al., 2020, Drach et al., 2017, Rutten et al., 2014). Accordingly, the application
    of such systems on commercial farms is becoming ever more prevalent (Abeni et
    al., 2019). Ideally, automatic monitoring of farm animals focuses on providing
    a complete overview of the state of the animal, including its production, reproduction,
    and its welfare status. The welfare of dairy cows is multifaceted (Fraser et al.,
    1997, von Keyserlingk et al., 2009) and encompasses many welfare issues, such
    as lameness, mastitis, heat stress, reproduction disorders, metabolic disorders,
    pain, and the disruption of their social environment (Leliveld and Provolo, 2020,
    Rushen et al., 2008). The diverse and complex nature of these issues can be accurately
    captured only by integrating information from multiple sources to get a complete
    picture of a cow’s welfare status (Frost et al., 1997, Leliveld and Provolo, 2020,
    Wisnieski et al., 2019). Indeed, the performance of automatic detection systems
    improves as the number of measured parameters increases (Dolecheck et al., 2015,
    Dominiak and Kristensen, 2017, Jensen et al., 2016). Despite the advantages of
    data integration in farm applications, many existing systems measure only one
    or a few indicators (Lee and Seo, 2021) and offer only limited conclusions regarding
    the state of an animal. Moreover, to our knowledge, there are currently no commercial
    systems that integrate measurements of the barn environment (e.g., barn climate
    and air quality) with cow-based measurements, even though the barn environment
    is an important determinant of cow welfare (Schauberger et al., 2020). A major
    hurdle in the practical implementation of data integration is that this requires
    a complex system architecture to handle the various types of data from heterogenous
    sources and to unify and process them in one place. It requires an efficient (wireless)
    communication system that prevents data collision (which results in data loss)
    and also prevents transmissions of redundant data (Firner et al., 2010, Khaleghi
    et al., 2013). The use of Internet of Things (IoT) technologies supports data
    management by connecting sensors, controllers, operators and objects to communication
    technologies such as local networks or the internet to form an information-based,
    automatic, and intelligent network (Zhang et al., 2021). This provides the opportunity
    to remotely handle and integrate data from multiple sensors through data fusion
    (Zhang et al., 2021). IoT technology is a central component of “smart farming”,
    which uses cloud-based platforms to analyse data from multiple sources and provide
    decision support (Akbar et al., 2020, Fountas et al., 2020). The main components
    of data processing in smart farming are data collection, data preparation, data
    processing, decision making and the provision of services to the end user (Amiri-Zarandi
    et al., 2022). Integration of data should be ensured on the data preparation level
    by standardizing the data to a predefined format, as well as identifying and deleting
    duplicated data, addressing gaps in generated data, and validating data sources
    and contents (Amiri-Zarandi et al., 2022). Thereby, it is vital that the data
    collected on a farm are reliable (i.e., valid and stable), because incorrect decisions
    based on unreliable data could result in high costs to the farm (Amiri-Zarandi
    et al., 2022). While several technologies have been developed for data integration
    in agriculture (Alonso et al., 2020, Cruz et al., 2022, Symeonaki et al., 2022),
    there is still a need for integrated systems that are open and customizable and
    therefore are suited for research purposes as well as for commercial use. This
    paper describes a prototype of an open and customizable integrated automated system
    to monitor barn environment and cow behaviour simultaneously. Because a major
    obstacle in data integration is providing a suitable architecture that can handle
    large data sets from various heterogenous sources, the aim of this study was to
    establish a system architecture with a suitable communication network to collect,
    transfer, process and visualize data from multiple sources continuously and in
    real time. This system was developed in the framework of the project “Integrated
    Environment Management System in Dairy Barns to Improve the Welfare and Productivity
    of Cows (GALA)” an Operational Groups of the Rural Development Programme 2014–2020.
    Using IoT technology, the resulting system integrates data from multiple diverse
    sensors that measure barn climate (e.g., temperature, humidity, wind speed), air
    quality (e.g., CO2 concentration), water use and temperature, the moisture and
    temperature of litter and, equally important, cow behaviour (e.g., lying, standing,
    eating, ruminating) to present a real-time comprehensive overview of the conditions
    in the barn and the state of individual cows to a farmer. We first describe the
    architecture of the system (sensors, nodes, gateway and backend) and its communication
    network, the data processing and the delivery of information to the user through
    a dashboard. We then document the system’s performance on three commercial dairy
    cattle farms. Finally, we evaluate the system’s performance and draw conclusions
    about the feasibility of open and customizable data integration in the automatic
    monitoring of dairy cows in a commercial setting, as well as about the opportunities
    and challenges that it presents. It is believed that the results of this project
    will help promote research-driven data integration in the automatic monitoring
    of livestock, thereby improving the management of cow welfare, health and production.
    2. Materials and methods 2.1. Architecture of the system 2.1.1. Overview The GALA
    system was designed to make measurements at three different locations (outside
    the barn, inside the barn and on individual cows) and to collect, process and
    analyse the measured data before ultimately presenting it to the user. Outside
    the barn, measurements included ambient temperature, relative humidity, wind speed
    and direction, and rainfall. Inside the barn, the measurements consisted of ambient
    temperature, relative humidity, light intensity, black globe temperature, water
    temperature and use, litter temperature and humidity, CO2, NH3, H2S, sound pressure
    level, and wind speed and direction. On the cow, measurements consisted of acceleration.
    The system architecture is shown in Fig. 1. The system consists of four main elements:
    the sensors, the nodes, the gateways and the cloud backend (for data processing
    and analysis and subsequent presentation to the user via a dashboard). The sensors
    are standard, off-the-shelf components that constitute the elements of transduction
    of physical quantities either into analog information (voltage or current) or
    digital data and are physically connected to a node. The nodes are specifically
    designed hardware/software systems that are physically connected to one or more
    sensors and are powered either by batteries or by low voltage mains electricity
    (12VDC). Download : Download high-res image (421KB) Download : Download full-size
    image Fig. 1. Architecture of the prototype integrated system described in this
    paper. External and integrated sensors are presented as green boxes, nodes are
    presented as blue boxes, gateways as orange boxes and the infrastructure for video
    registration as purple boxes. Solid lines indicate wired connections and dotted
    lines indicate wireless connections. BGT = black globe temperature, AT = ambient
    temperature, RH = relative humidity, T = temperature, NAS = network attached storage.
    (For interpretation of the references to colour in this figure legend, the reader
    is referred to the web version of this article.) The nodes are small, embedded
    devices that integrate all the necessary electronics and implement the firmware
    to perform three main functions: 1. raw sensor measurements, 2. local data processing,
    and 3. data transmission. The gateways are larger hardware/software systems that
    are powered by mains electricity (220VAC; indoor gateway) or by a solar panel
    and a backup battery (external gateway). They collect data from the nodes through
    the local wireless communication channel and provide connectivity to the backend
    cloud infrastructure using the MQTT (message queue telemetry transport) protocol
    over a standard 4G cellular network. The backend system is a software component
    based on a commercial platform that collects, stores, processes and displays sensor
    data. Access to these data is provided by means of a complex and complete dashboard
    presenting all sensor data in full detail for scientific purposes (e.g., research
    use), or via a simpler, compact dashboard that provides a summary of the data
    meant for immediate viewing by a farmer. 2.1.2. Nodes hardware The nodes are structured
    according to the general architecture shown in Fig. 2 in which a main board (indicated
    as GALA-NX), which is specific for each type of node or group of nodes, hosts
    the analog and/or digital interfaces towards the sensors, the internal sensors,
    the power supply and battery charger circuitry and the 35 x 45 mm GALA-SOB (system-on-board;
    Fig. 3). All nodes in this project were built by enhancing this basic design with
    specific features. In total there were nine different types of nodes created based
    on the basic GALA-SOB. Of these, seven nodes are used for data collection (N1,
    N2, N3, N4, N5, N6 and N9), one node (GALA-RF868) is integrated in the indoor
    gateway and acts as interface from the local wireless 868 MHz network to the cellular
    network, and one node (GALA-EOL, which is and end-of-line board) was used for
    programming and testing the GALA-SOB in the laboratory, before deploying it into
    the specific nodes. Two further nodes (N7 and N8) were developed to read digital
    inputs and drive digital outputs but these were not installed on the test farms.
    The specifications of the different node types are listed here: • GALA-N1. The
    N1 node is connected to an ambient temperature and relative humidity sensor, a
    light intensity sensor, and a black globe temperature sensor (Table 1). The temperature
    and relative humidity sensors are integrated in the node and connected via an
    I2C bus to the GALA-SOB. The light intensity sensor is also integrated in the
    node, even if it is mounted on a separate small board, and is placed close to
    the top of the case enclosing the node to be better exposed to the light. The
    black globe temperature sensor is externally connected to the board and consists
    of a standard plastic black globe and an NTC analog sensor. The power supply of
    the node consists of two C-size primary batteries, connected in series (3 V, 7800mAh).
    The node is fixed in an opaque case (55 x 80 x 160 mm) with a transparent front
    for light intensity measurements. Table 1. Sensors that are integrated in the
    system, along with the respective nodes. n.s. = not specified. Node Sensor Technology
    Measurement Range Accuracy (±) N1 Sensirion SHT3x / SHT4x CMOSens Ambient temperature
    −40 … +125 °C 0.1 °C Sensirion SHT3x / SHT4x CMOSens Relative humidity 0 ... 100
    % 1.5 % Silabs SI1153 ALS Photodiode Light intensity 0 … 128 klx n.s. S + S RPTF2
    NTC10K NTC Black globe temperature 5 … +60 °C 0.5 °C N2 Bosch BMA400 MEMS Acceleration
    ±16 G 1 mG N3 Caleffi 7942 Dry contact Water use 0 … 4 m3/h n.s. Waterproof NTC
    10 K NTC Water temperature 5 … +60 °C 0.5 °C N4 METER teros 12 Resistive / capacitive
    Litter temperature −40 … + 60 °C 0.1 °C METER teros 12 Resistive / capacitive
    Litter humidity 0 … 0.7 m3/m3 0.02 m3/m3 N5 Sensirion SCD30 Nondispersive infrared
    (NDIR) CO2 concentration 400 … 10.000 ppm 30 ppm + 3 % GS + 4NH3100 Electrochemical
    cell NH3 concentration 0 … 100 ppm 1 ppm GS + 4H2S Electrochemical cell H2S concentration
    0 … 100 ppm 0.1 ppm DFRobot SKU:SEN0232 Microphone Sound 30 ...130 dBA 1.5 dB
    N6 Davis Instruments 6410 Dry contact Wind speed 0.3 … 100 ms/s 4 % Davis Instruments
    6410 Resistive, dry contact Wind direction 0 … 365° 7° N9 DHT22 Capacitive / resistive
    Ambient temperature −40 … + 80 °C 0.5 °C DHT22 Capacitive / resistive Relative
    humidity 0 … 100 % 2–5 % Davis Instruments 6410 Dry contact Wind speed 0.3 … 100
    ms/s 4 % Davis Instruments 6410 Resistive Wind direction 0 … 365° 7° Davis Instruments
    6466 Dry contact Rainfall 0 … 250 mm/hr 4 % • GALA-N2. The N2 node, described
    in detail by Lovarelli et al., 2022, integrates one triaxial accelerometer. The
    electronics of the device are enclosed in a 100 x 75 x 22 mm plastic case having
    an IP67 rating, which is mounted on a neck collar with a weight at the bottom
    to keep the node in place on the upper-left side of the cow’s neck (Lovarelli
    et al., 2022). This position enables the detection of ingestion-related behaviours,
    such as eating and ruminating. For further protection, initially duct tape and
    later a rubber coating was fixed around the case. The N2 nodes are powered by
    a single AA-sized, high energy–density, lithium-thionyl chloride battery (3.6
    V, 2600mAh). • GALA-N3. The N3 node is externally connected to pulse-launching
    flow meters with different sections (1/2in, 3/4in or 1in) to measure water use
    and waterproof Negative Temperature Coefficient (NTC) sensors to measure the water
    temperature. Each GALA-N3 node can support up to two flow meters and two NTC probes.
    The power supply of the node consists of two C-size primary batteries, connected
    in series (3 V, 7800mAh). The case dimensions are the same as for the GALA-N1
    nodes, but N3 cases have an opaque front instead of a transparent one. The node
    can measure both sprinkler and drinking water use and temperature. In the study
    reported here, the temperature sensors were activated only in the nodes that measured
    drinking water. • GALA-N4. The N4 node is connected via an SDI-12 bus to a digital
    sensor measuring litter temperature and humidity. This sensor is designed for
    measuring the temperature and water content of soil, which has granulometric and
    electrical characteristics different from those of litter material. For this reason,
    experimental measurements were carried out with litter material to develop a calibration
    curve for obtaining the litter humidity value. The power supply and the case are
    the same as described for the N3 nodes. • GALA-N5. The N5 node is connected to
    different types of air quality sensors and to a sound pressure sensor, all integrated
    in the node. Since the CO2 sensor is based on non-dispersive infrared technology,
    the power consumption by these sensors is relatively high and cannot be supported
    by batteries in the long term. Therefore, the N5 nodes are powered by low-voltage
    mains (12VDC). The N5 node is fitted in an opaque case (55 x 146 x 252 mm) which
    has a filtered opening to allow the passage of gases for measurement. • GALA-N6.
    The N6 node is connected to an external sensor measuring wind speed and direction
    inside the barn. The sensors transfer the information to the node via two signals:
    a digital pulse signal for speed measurement and an analog (resistive) signal
    for direction measurement. The power supply and the case are the same as described
    for the N3 nodes. • GALA-N9. The N9 node is essentially a weather station that
    measures the environmental conditions outside the barn. The node is based on legacy
    modules directly connected through a Modbus channel to the external gateway and
    powered by a common power line on a DIN rail system. The DIN rail backplane consists
    of 5 lines: +12 VDC, ground, lines A and B of the half-duplex RS485 bus, and a
    generic digital signal used for bus contention and synchronization. The weather
    station is powered by a solar panel backed-up by a rechargeable battery (12 V,
    12 Ah). The GALA-H2O-POWER module controls the power supply for the other modules
    and the recharging of the battery via the solar panel. Sensor reading is performed
    by two I/O legacy modules, namely the GALA-H2O-DIGITAL and GALA-H2O-ANALOG boards.
    The former reads digital data such as windspeed and rainfall pulsed output, while
    the latter reads and converts analog data, such as the variable resistance (through
    a voltage divider) produced by the wind direction sensor. • GALA-RF868. The module
    GALA-RF868 acts as the interface between the wireless network in the barn (868
    MHz) and the cellular network towards the cloud. This node collects all data from
    the barn nodes and makes them available through a queue on a standard serial interface
    (UART) using a Modbus remote terminal unit protocol. Through this interface, the
    indoor gateway reads data for packing and transmitting it to the cloud. The GALA-RF868
    board has the same mechanical structure as the GALA-H2O boards and is also connected
    to the DIN backplane. • GALA-EOL. A GALA-EOL board was created as a support for
    testing and diagnostics of the GALA-SOB. This board was not installed on the farms
    but rather used as a support tool in a laboratory. It was used during the development
    of the system and for debugging during the testing phase. Download : Download
    high-res image (160KB) Download : Download full-size image Fig. 2. Schematic presentation
    of the basic architecture of the nodes. The Wi-Fi symbol indicates the RF868 network.
    Download : Download high-res image (66KB) Download : Download full-size image
    Fig. 3. Simplified schematic of the GALA-SOB. The interfaces for connecting with
    the specific “host” boards for the different types of sensors are indicated. A
    simple user interface (UI) controls a status LED and receives input from a pushbutton.
    The “JTAG interface” is used for the first programming of the device. Digital
    outputs (DO) are either used as simple digital signals (e.g., to drive relays)
    or for Serial Peripheral Interface (SPI) sensors. The digital inputs (DI) are
    used to read isolated dry contacts. Analog inputs (AI) are used for voltage or
    current sensing, depending on the specific front-end on the host board. Finally,
    the board exposes standard digital buses: UART, I2C and SPI. The core of the module
    is the Silicon Labs EFR32BG13 Blue Gecko SiP, featuring two radio channels (868
    MHz and 2.4 GHz) with integrated power amplifier and balun and a 40 MHz Cortex
    M4 core with 512 KB of flash memory and 64 KB of RAM. (For interpretation of the
    references to colour in this figure legend, the reader is referred to the web
    version of this article.) 2.1.3. Sensors A list of the sensors and their description
    is provided in Table 1. Selection of the parameters that are relevant for the
    monitoring of cow welfare, health and production was based on a review of relevant
    literature (e.g., Hoffmann et al., 2020, Rushen et al., 2008, Dittrich et al.,
    2019; reviewed in Leliveld & Provolo, 2020). For the selection of the sensors,
    literature research was performed to understand the range and accuracy requirements
    for measuring all included parameters in a dairy cattle barn environment. The
    sensors were then selected by evaluating their capability to provide accurate
    measurements in the desired range and their compatibility with the rest of the
    system, whilst aiming to keep costs, power consumption, and complexity low. As
    shown in Fig. 1, Fig. 2, the sensors are externally or internally connected to
    the nodes. The external connection can be relatively close to the node (e.g.,
    the black globe thermometer that is built on top of the node case) or remote.
    For instance, the water flow sensors and the associated thermocouples that measure
    water use and temperature were mounted on the water distribution pipes of the
    drinking troughs and/or sprinklers for cooling the cows and connected to the nodes
    via long cables. 2.1.4. Node firmware The node firmware collects the data from
    the sensors, pre-processes the raw sensor readings (see Section 2.3.1) and sends
    data to the gateway. The collection of data from the sensors is done through standard
    protocols for those sensors that directly expose a digital interface such as SPI,
    SDI or I2C, or by reading the sensor’s output voltage or current through an analog-to-digital
    converter and transforming it into the target physical measure according to specific
    mathematical models. To optimize the performance of the nodes and increase reliability
    and battery life, the firmware was developed to maximize the idle time of the
    system (during which time the microcontroller and, if possible, the sensors are
    switched to low-power mode or turned off) and to obtain the best trade-off between
    local computation and data transmission. Like the hardware of the nodes, the firmware
    was also developed in a modular way with the support of an embedded operating
    system, generic drivers for the sensors, data acquisition functions, Bluetooth
    communication protocol and local wireless communication protocol that are common
    to all sensors. On top of this common layer, each node implements a specific application
    layer. 2.1.5. Gateways Both the indoor and external gateway act as a connection
    between the local network and the internet. The indoor gateway connects the local
    RF868 network and the global cellular network. It obtains data from the nodes
    in the barn via the local radio interface module (GALA-RF868) and communicates
    them to the backend infrastructure (cloud). The gateway hardware consists of a
    NAS (network attached storage), which is also used for video recording (Synology
    DS210j, Synology, Banciao, New Taipei, Taiwan). The gateway software continuously
    polls the GALA-RF868 module to verify whether new data from the nodes are available
    and, if so, copies such data to a large temporary buffer that has the capacity
    to store data for a few days. Periodically, the gateway software merges data in
    the buffer into a single, compact MQTT binary packet and sends it to the cloud.
    The external gateway is implemented by the legacy GALA-H2O-MAIN module which acts
    as Modbus master for collecting data from the other modules of the N9 sensor node,
    combining such data in a single packet for optimization, buffering individual
    packets in case of network absence and, eventually, sending the stored data packets
    to the cloud backend via a 3G/4G cellular module. 2.1.6. Video recording system
    The internet protocol video cameras that are used for video recording are compatible
    with the Open Network Video Interface Forum specifications and are powered via
    PoE (Power over Ethernet). This solution allowed a single Ethernet cable per camera
    to be used for power supply, as well as video streaming and remote control. As
    shown in Fig. 1, the cameras are connected via ethernet cables through a PoE switch
    to the NAS where specific commercial software acts as a network video recorder.
    Remote access to the video recording system is granted, through a 4G modem, and
    locally via a wired ethernet connection and a WiFi network. A remote web-based
    interface provides visualization of the barn and control of the system, both for
    configuration and diagnostics. 2.1.7. Backend The backend is a custom cloud software
    application, partly built on top of the commercial platform Thingsboard (v.3.2.2PE),
    that collects, stores, and processes data that is received from the gateways.
    As Fig. 4 shows, messages are not sent directly to the Thingsboard MQTT broker,
    but first to a custom-built intermediate MQTT Broker that performs several transformations,
    both on the upstream telemetry data flow and on the downstream bidirectional remote
    processing calls and attribute data flows. Considering the telemetry data flow,
    the following list describes the operations that the intermediate server performs:
    1. A component called “Splitter” is subscribed to the MQTT topic where packed
    telemetry messages are received and it unpacks these messages into individual,
    node-level messages. This process is agnostic with respect to the structure and
    contents of the messages. Unpacked messages are then published, as binary packets,
    to the same intermediate broker on a second topic. 2. A component called “Decoder”
    is subscribed to the second topic created by Splitter. Decoder is aware of the
    internal structure of the messages and has the task of interpreting the content
    of the individual binary messages and publishing it back to the broker in Java
    script object notation (JSON) format on a third topic. 3. The Thingsboard hub
    is subscribed to the JSON telemetry topic of the intermediate MQTT broker and
    allows transmitting data from different devices (all the nodes connected to a
    specific gateway) through a single connection using a unique key related to the
    gateway (rather than to the single nodes). Then, the Thingsboard hub associates
    each logic device (i.e., the logical entity that is needed to represent the data
    on the Thingsboard platform) with the corresponding physical node based on the
    node name specified in the JSON telemetry message. 4. The telemetry is published
    to the Thingsboard internal broker, where it is saved in the database in the form
    of a timeseries and enters the processing chain. The Thingsboard was customized
    using a graphical (NodeJS) and programmatic (Javascript) formalism, referred to
    as “rulechains”, to perform medium–low complexity processing (see Section 2.3.2)
    Download : Download high-res image (231KB) Download : Download full-size image
    Fig. 4. Schematic representation of the cloud software infrastructure. The blue
    arrows indicate the binary data that originated from a sensor, packed when generated
    from the gateway and split into single packets when exiting the splitter; the
    red arrows indicate the telemetry decoded in JSON (Java Script Object Notation)
    format; the green arrows indicate RPCs (Remote Procedure Calls) used to perform
    some actions on the nodes; the yellow arrows indicate nodes’ attributes, used
    to configure some parameters of their behaviour. DB = Database, MQTT = Message
    Queue Telemetry Transport. (For interpretation of the references to colour in
    this figure legend, the reader is referred to the web version of this article.)
    2.2. System communication network To minimize power consumption and to support
    a large number of devices, a custom-TDMA (Time Division Multiple Access) wireless
    communication protocol was developed on the raw physical 868 MHz GFSK modulation
    scheme provided by the EFR32BG13 core used in the GALA-SOB. The 868 MHz carrier
    was chosen to minimize signal attenuation due to the presence of animals. This
    protocol allows bidirectional communication between the nodes (master) and the
    gateway (slave). A time-slotted approach is used for data transmission to avoid
    packet collision from different nodes transmitting at the same time. This approach
    reduces the need to retransmit, thereby reducing energy consumption at the nodes
    and maximising battery life. Based on an autonomous initial negotiation scheme,
    each node is assigned a specific timeslot of 1 s within the global communication
    period (called an “epoch”), which is set to 10 min (600 s, as described in Supplementary
    Material 2, Figure S1). The network synchronization is centralized and is performed
    by the gateway. At the end of the initialization phase, the gateway starts receiving
    messages from the nodes, nominally every second. The actual timing of each node,
    though, is based on its own clock, which unavoidably drifts over time with an
    approximate speed of 50 s per million seconds. This means that approximately every
    a node leaves its nominal slot and transmits either in the previous or the next
    one. To compensate for this drift, the gateway, upon receiving a message from
    a given node, determines the difference between the nominal time at which the
    message was expected and the actual time at which the message was received. This
    allows computing the exact actual time until the next transmission of that node.
    This information is sent back to the node, which can thus adjust its timing and
    compensate for the clock drift. This way, the drift is compensated at each period
    of 600 s and its worst-case value is . To avoid overlapping, a silence guard time
    is respected at the beginning of each time slot. The maximum size of each transmitted
    packet is 250 bytes (including the communication “overhead”), which, with a bandwidth
    of 26 Kbit/s, requires a maximum time of 80 ms. At the end of transmission, each
    node remains active for a short period of time (20 ms in the current implementation)
    to wait for the answer from the gateway containing synchronization and other control
    information. After receiving the gateway response, if any, the node enters its
    deep-sleep mode. During the initial negotiation phase, the following procedure
    is used. Each node, which has a unique numeric identifier, transmits (at a random
    time) an initial packet containing the data and its identifier to the gateway.
    In case of collision with other packets, the same transmission will be repeated
    after a short interval, which is also random. The gateway, knowing the node identifier,
    computes the delay the node must wait for the next transmission to fall in its
    correct slot and indicates the waiting time until the start of the node’s next
    slot in the response message to the node. For example, if node 35 transmits for
    the first time 330 ms after the beginning of time slot 250, the gateway assigns
    the new node slot 35 and indicates that the delay to the next transmission is
    384.67 s (computed as [600 – 250.330] + 35 = 384.67 s; i.e., the time till the
    beginning of the next epoch, plus the delay from the beginning of the next epoch
    to the correct slot). In this case, therefore, the gateway indicates the retransmission
    time as 350 + 35 = 385 s. This mechanism simultaneously guarantees two important
    properties: on the one hand – except for an initial transient of 20 or 30 min
    – it eliminates packet collisions, and, on the other hand, it synchronizes the
    time of all nodes with that of the gateway. Data from the nodes are encoded in
    binary packets to reduce size, transmission time and energy consumption. Two different
    basic packet structures were defined: one with a 16-bit mask, suitable for encoding
    up to 16 different measures, and a larger one, with a 32-bit mask, which extends
    the allowed measures up to 32. The structure of these packets is shown in Figure
    S2 (Supplementary Material 2). The gateway transmits the data periodically to
    the backend infrastructure in the cloud on the internet using a MQTT protocol
    (MQTT version V3.1 protocol, configured with quality of service “1″; OASIS, 2023).
    In addition to the RF868 and global protocols, a Bluetooth protocol (Bluetooth,
    2023, Smart Farm Information Processing, xxxx) is used for the local connection
    between the individual nodes and a service mobile application (app). To achieve
    this, each node implements a Bluetooth channel for local and short-distance communication
    and an app was developed to connect to this channel via a smartphone. This app
    is used for data collection, sensor configuration, diagnostics, and software updates
    of the nodes. The app and the Bluetooth channel were also used to collect combined
    accelerometer data and behavioural observations for the development of an algorithm
    to categorize cow behaviour (for details see Lovarelli et al., 2022). 2.3. Data
    processing Data processing is distributed across the system and is partly performed
    on the nodes and partly on the cloud. The gateways do not perform any processing,
    being only responsible for buffering of the packets coming from the nodes and
    packing them into larger packets to minimize communication overhead. 2.3.1. Data
    processing on the nodes The processing performed by the nodes on signals that
    change slowly over time is relatively simple and includes basic statistical analysis,
    filtering, and transformations, such as scaling and offsetting. Statistical analyses
    include the calculation of the mean and, for some measurements such as wind speed
    and sound level, also minimum and maximum values within a 10-minute interval.
    Some “noisy” quantities (e.g., signals that change rapidly over time and signals
    affected by electronic or thermal noise) are sampled at a rate higher than needed
    and then are filtered using a moving average or Butterworth digital filter and,
    in some cases, decimated. The processing on the cow nodes (N2) is much more complicated
    and computationally intensive than that on the other nodes. Details on the processing
    of the behavioural data and the development of the algorithm have been published
    in Lovarelli et al. (2022). In short, acceleration in the three dimensions is
    sampled at a frequency of 25 Hz and processed in 5-second windows to compute data
    features to be used to classify the dominant behaviour over each 10-minute interval.
    Each 10-minute interval consists of (600 / 5 = ) 120 windows, and thus 120 samples
    per each of the 10 features are computed. The behavioural classification algorithm
    was developed by applying machine learning on the features that were extracted
    from accelerometer data, which were collected during 108 h of observations from
    32 cows on three farms. This algorithm, which is based on a decision tree model,
    was shown to correctly classify the cow behaviour into six different classes,
    i.e., “standing”, “lying down”, “standing and ruminating”, “lying down and ruminating”,
    “eating” and “other”, with an average accuracy of 85.12 % (Lovarelli et al., 2022).
    2.3.2. Data processing on the cloud On the cloud, the data go through a processing
    flow implemented within the Thingsboard platform. On this platform, data are characterized
    according to the farm, the sensor identifier, the sensor type, the type of measure,
    the timestamp and the value (i.e., the actual measurement made by the sensor).
    As mentioned in Section 2.1.7, the processing that occurs on the Thingsboard platform
    is based on rule chains, which graphically represent the paths and the different
    processing steps for the different types of data. Rule chains are activated either
    by the arrival of new data or by the triggering of a periodic timer. When data
    arrive in the cloud, they are first processed by the root rule chain (see Supplementary
    Materials, Figure S3). This rule chain basically sorts the data towards secondary
    rule chains for each specific node type. An overview of the input timeseries,
    computed timeseries, and computation process is provided in the Supplementary
    Material 1 (Table S1). Many values are obtained by combining the same type of
    data collected in the same time interval by several sensors (e.g., combining the
    temperature measurements of the different N1 sensors to calculate the mean temperature
    in the barn). Other values are obtained by combining multiple measurements in
    the same time interval from the same sensor node (e.g., combining the temperature
    and humidity measurements to calculate the temperature-humidity index [THI]).
    Some other values are obtained by combining the same measurement over a certain
    time interval. Indeed, for all values, hourly and daily averages, as well as more
    complex statistical values, are calculated. For example, the “standing rate” is
    calculated as the fraction of time spent standing during the last hour or day,
    and the “daylight exposure” is based on the time having a mean light intensity
    greater than 40 lx. A specific node of the rule chains is used to raise alarms.
    For environmental barn sensors data, the alarm uses fixed thresholds (e.g., if
    the mean barn THI increases above 72, the first level of alarm is raised and visualized
    on the dashboard). For the behavioural data, a more complex dynamic threshold
    is used in which the deviation of a measurement from the average of the same measurement
    over the last 30 days is calculated. 2.4. Dashboard The dashboard in this project
    served two purposes. First, it was developed to present the collected data to
    the end user (usually a farmer) in a concise and accessible manner with the aim
    to provide quick and clear support in the farmer’s decision-making processes.
    Secondly, because the project described herein concerns a pilot system that also
    serves scientific purposes, the dashboard also needed to present all data gathered
    by the individual sensors in a detailed and complete manner that allowed verification
    of the system’s functionality and accuracy of the recordings. The dashboards were
    developed within the Thingsboard platform and were made accessible via hypertext
    transfer protocol to make it readable both using a computer’s browser and a mobile
    device. The home page shows the list of the three commercial farms and their location
    on the map (see Supplementary Material 2, Figure S4). The detailed (diagnostic)
    and simplified (farm) overviews for each farm can then be accessed by clicking
    on the respective icon next to the farm name. A description of the detailed overview
    is provided in the Supplementary Material 2. The farm main page shows a series
    of synthetic alarm graphs that display the alarm level, ranging from green (safe)
    to red (danger) for different monitored aspects of the farm, together with a needle
    that indicates the current alarm level (Fig. 5). The thresholds for the different
    alarm levels are shown in Table S2 (Supplementary Material 2). Alarm graphs are
    shown for the barn environment of the last hour and the last 24hrs, cow activity
    in the last hour and last day, cow behaviour in the last day and rumination in
    the last hour and day. The level indicated on the barn environment graph is based
    on a summary from different individual measures, such as THI. Pop-up overviews
    show the values of individual measures and the level of alarm for each. The farm
    main page also provides access to more detailed pages on the barn environment,
    cow behaviour and water and litter. The page dedicated to barn environment displays
    graphs (with the same features as for the previously described graphs) of means
    from N1, N5, N6 and N9 sensor nodes (e.g., temperature, ammonia concentration),
    as well as other computed values (e.g., THI, total daylight exposure and differences
    in these values between inside and outside the barn). The page dedicated to cow
    behaviour displays a table with the data gathered on individual cows. This table
    includes for each cow, the collar identifier, the cow identifier, and the percentage
    of time spent standing, lying down, eating, ruminating, and in other behaviour,
    as well as the mean activity level in the last 24 hrs (see Supplementary Material
    2, Figure S6). The percentages are given in colours ranging from green to red,
    which indicate the alarm level for each individual cow and behaviour. The cow
    identifier can be manually entered and changed if the collar switches owner. This
    page also provides access to pages for individual cows. On these pages, there
    is a radar chart showing the average time the cow spent exhibiting the different
    behaviours, a list showing the dominant behavioural category during the most recent
    10-minute intervals, and a graph showing the mean activity of the cow for a selected
    period. The layout of the page dedicated to water and litter is similar to the
    page on barn environment, with all data (e.g., global drink water use, mean litter
    temperature, and mean litter humidity) displayed in graphs. Download : Download
    high-res image (494KB) Download : Download full-size image Fig. 5. Above: the
    farm main page with the alarm graphs shown for the different aspects of the farm
    (e.g., barn environment in last hour). Needles indicate the current alarm level,
    ranging from green (safe) to red (danger). For activity and rumination (the four
    graphs on the right of the screen) any strong deviation from the normal level
    is considered cause for alarm, hence the middle is green (safe) and yellow and
    red levels are shown on each side. The icons in the lower left section of the
    screen provide access to more detailed information on all measurements related
    to barn environment, cow behaviour and water and litter. Below: example of a graph
    displaying the values of different climate parameters (THI, humidity and temperature)
    over the last 7 days (at the time the graph was made). Different y-axes are given
    for THI and humidity (on the left) and temperature (on the right). Time is shown
    on the x-axis. Because the dashboard was designed to be understandable to Italian
    farmers, the information is provided in Italian. (For interpretation of the references
    to colour in this figure legend, the reader is referred to the web version of
    this article.) 2.5. Functionality assessment on commercial farms 2.5.1. Farms
    The installation of the system on the test farms, which included the mounting
    of collar-based sensors on the cows, was approved by the Ethics Committee of the
    University of Milan (n. 25 of 1 March 2022). The system was installed on three
    commercial dairy cattle farms in the province of Cremona (Lombardy) in Northern
    Italy. These farms have been described in detail by Lovarelli et al. (2022). Briefly,
    the monitored barns host Italian Holstein dairy cows in a loose-housing system
    with free stalls and straw or solid digestate as litter. In the first farm, the
    system was installed in a barn area having a floor area of 1020 m2 with an outdoor
    loafing area of 47 m2 that had two lines of cubicles and housed on average 120
    lactating cows. One side of the barn was open, while the other three sides had
    walls. In the second farm, the monitored barn area had an area of 1324 m2, was
    open on all sides, contained two lines of cubicles, and housed on average 145
    cows. In the third farm, the system was installed in a barn that had an area of
    920 m2 that was open on all sides. The area consisted of three cubicle lines and
    housed on average 115 lactating cows. All monitored areas had fans above the lying
    area and sprinklers above the feeding area (on farm 3, sprinklers were installed
    in July 2021). Cows were milked twice a day and feed was provided once per day
    on all farms. 2.5.2. Installation Before installation on farms each sensor node
    was first tested in the laboratory under controlled conditions and then “commissioned”,
    i.e., it was assigned its type (e.g., N1, N2), received a unique identifier and
    was assigned to a farm. The number of sensors, their locations and the method
    of installation varied between farms depending on the barn structure. Table 2
    shows the number of sensor nodes that were installed on each farm. In the case
    of multiple sensors of the same type, care was taken to position the sensors in
    the barn in a way that assured mean values of sensor readings would best reflect
    the situation of the entire barn. Therefore, the positions of the sensors were
    dispersed and covered different areas of the barn as much as possible (see Supplementary
    Material 2 Figure S7 for an example of an installation plan on one farm). Pictures
    of some mounted sensors are shown in Figure S8 (Supplementary Materials). The
    N1 nodes were placed at an approximate height of 2.5 m above the floor, which
    is just out of reach of the cows. The nodes on the outer columns were directed
    to face inside the barn to avoid direct sunlight exposure. Where possible, sensors
    were mounted on the walls using magnets. In other cases, they were mounted on
    3-m long poles that were fixed to the cubicle barriers. The N3 sensors were installed
    to monitor sprinkler and drink water (depending on the accessibility of the water
    pipelines) on a group level (i.e., group of cows). On farm 1, it was not feasible
    to install N3 sensors to measure drink water, because the only accessible water
    supply pipeline served the entire farm. On the other farms, the number of installed
    drink water sensors depended on where they could be installed. On farm 3, the
    sensors (and their nodes) were installed at every drinking trough (four in total),
    while on farm 2, one sensor node was installed at the central pipeline to measure
    drink water use and two sensors were installed at the drinking troughs to measure
    the temperature. The litter sensors (N4) were only installed on farm 3, because
    the structure of the cubicle floors in the other two farms was not suitable for
    burying the sensors. On farm 3, the sensors were buried in the middle of the cubicle,
    deep in the litter in a compartment that was protected by rubber walls. The node
    was buried in a similar compartment in the litter between two adjacent cubicles
    and the connecting cable was also buried (see Supplementary Materials, Figure
    S8). The air quality sensor nodes (N5) were, like the N1 sensors, installed about
    2.5 m high on the walls of the barn or on metal poles that were fixed to the ceiling.
    The wind sensors nodes (N6) were, like some N1 sensors, fixed on poles that were
    either fixed between cubicle barriers, on the feeding alley, or attached to the
    walls of the barn (2.5─3 m high). The weather station (N9) was positioned in a
    field adjacent to and about 50─120 m from the monitored barn area. The N9 node,
    sensors and solar panels were all fixed to a large iron pole (2–4 m high) that
    was fixed in the ground using a concrete slab. The number of video cameras that
    was installed depended on the size of the monitored area (two at farm 3 and four
    at the other farms). The cameras were mounted on a wall, close to the ceiling
    to maximize the area that could be monitored. The indoor gateway was installed
    just outside the monitored area, about midway on the longer side (to minimize
    the distance to all nodes). At each farm 60 collar-mounted N2 sensors were put
    on semi-randomly chosen cows, preferring cows that were early in lactation. Whenever
    a cow was removed from the monitored area (e.g., due to drying off) the collar
    was put on a fresh cow. Table 2. The sensor nodes and other elements of the system
    that were installed at each farm (not including replacements). Sensor nodes Farm
    1 Farm 2 Farm 3 N1: temperature, humidity, light 4 4 4 N1: temperature, humidity,
    light, black globe temperature 4 4 4 N2: cow behaviour 60 60 60 N3: drink water
    use & temperature 0 3 4 N3: sprinkler water use 1 1 1 N4: litter temperature &
    humidity 0 0 4 N5: CO2, NH3, H2S, sound 2 2 2 N6: wind speed & direction 3 3 3
    N9: temperature, humidity, rain fall, wind speed & direction 1 1 1 Indoor gateway
    1 1 1 Video cameras 4 4 2 2.5.3. Maintenance The system’s functioning was checked
    weekly via the online dashboard, using the diagnostics page. Gaps in energy supply,
    unusual measurements and incidences of packet rolling number errors and resets
    were recorded in a spreadsheet file. The farms were also visited at intervals
    of 1─2 weeks to check and clean the sensors (with a damp cloth). If any problems
    with a sensor could not be resolved on the farm, the sensor was taken to the laboratory
    for closer inspection and repair or replacement. Spiderwebs were removed from
    the cameras using a dry cloth on a long wooden pole. 2.5.4. Data collection and
    processing Data were downloaded from the dashboard and saved in a database (in
    spreadsheet/ database format). After downloading the data, they were filtered
    to exclude outliers. The filter that was used and the acceptable ranges for each
    parameter are included in the Supplementary Material 2 (Table S3). To understand
    how much variation in a specific parameter across a barn could be expected, correlations
    were calculated between different sensors of the same type using the CORR procedure
    (Spearman, Fisher [using Fisher''s z-transformation for correlation statistics])
    in the statistical software SAS 9.4 (SAS Institute Inc., Cary, NC, USA). In addition
    to the data that were collected by the system, data for milk yield and quality
    (where possible from individual cows), health and fertility events, IDs, birth
    date and calving dates of the monitored cows, and events in the monitored areas
    that could affect cow behaviour (e.g., veterinarian visits) were recorded by the
    farm personal. These data were obtained either from an installed commercial monitoring
    system (Afimilk Ltd, Kibbutz Afikim, Israel; farm 1) or from the Regional Association
    of Farmers in Lombardy (ARAL; farms 2 and 3). 3. System performance 3.1. System
    operation The integrated monitoring systems were operational on the three farms
    for one year. This period served not only to verify the suitability of the system
    to collect data in a barn environment and to collect data for analyses, but also
    for problem solving and fine-tuning. Therefore, not all nodes were continuously
    installed and functioning on the farms during this period. The number of sensor
    nodes that were installed and functioning in each month are shown in the Supplementary
    Material 2 Table S4. Some problems encountered, such as loss of sensors, water
    damage and case damage, were due to the prototype nature of the system. In addition,
    the entire system on each farm was affected by power outages. Delays in turning
    on of a system occurred because the system had to be turned on manually on site
    after a power outage. In total 24 days on farm 1, 26 days on farm 2 and 33 days
    on farm 3 were missed due to this problem. The accelerometer nodes (N2) had an
    average lifetime (i.e., time until the node stopped working or had to be removed
    due to malfunctioning) of 340 days, and 141 out of 180 installed nodes (including
    replacements) were still functioning at the end of the test period. Consequently,
    the lifetime of these nodes can be expected to be much higher than what could
    be assessed in only one year. Of the 256 battery powered sensor nodes (including
    replacements), 11 nodes needed a battery replacement, meaning that the majority
    of the nodes could correctly function at least for one year on their initial batteries.
    3.2. Data stability Table 3 shows the percentage of data (10-min data points)
    that were actually collected out of the total data that could potentially have
    been collected, based on the calculation of the total number of 10-min intervals
    in which each node was installed and functioning without evident problems (e.g.,
    due to identified hardware or software malfunctioning, batteries getting loose)
    and in which there was no general power outage. These results give an indication
    of the data collection stability of the system. As Table 3 shows, the percentage
    of data collected for most sensors exceeds 90 %. Exceptions were the weather stations
    on all three farms, which often experienced problems with collecting temperature
    and humidity data. These problems were due to the communication channel of the
    temperature and humidity sensor which proved unreliable under long-term external
    weather conditions. The somewhat lower data collection percentage for the N2 nodes
    can be largely ascribed to temporary removal of the cows from the monitored areas
    during milking. Because the milking parlours were not located in the monitored
    section and because they were enclosed by walls, the data transmission from the
    collars to the gateway was slightly impaired when the cows were in the parlour.
    The calculated percentages concern the 10-min data points collected from all installed
    sensor nodes. However, for purposes of monitoring and analyses, hourly data and
    data based on means from different nodes of the same type is more suitable than
    using 10-min data or data from individual nodes in most cases. Therefore, the
    percentage of usable data in such cases is still higher than displayed in Table
    3, since for calculating means (from sensors and/or per hour), some missing data
    points could be acceptable, depending on the data type. Table 3. Percentage of
    collected 10-min data points of the total data points that could potentially have
    been collected per farm. The potential data points were calculated as 6 x 10-min
    intervals x 24 h x days on which the sensor node was installed and functioning
    without diagnosed problems. The total data points are sums of the data collected
    by all sensor nodes per farm. Empty Cell Total collected (N) Total potential (N)
    Percentage collected (%) Node Farm 1 Farm 2 Farm 3 Farm 1 Farm 2 Farm 3 Farm 1
    Farm 2 Farm 3 N1 1,509,201 1,519,732 1,399,032 1,542,324 1,557,648 1,421,298 97.85
    97.57 98.43 N3 97,391 205,553 141,794 98,280 215,154 143,292 99.10 95.54 98.95
    N2 3,477,137 4,050,255 3,959,231 3,891,324 4,268,040 4,112,244 89.36 94.90 96.28
    N4 351,901 375,322 93.76 N5 581,220 591,058 552,649 589,680 599,328 569,987 98.57
    98.62 96.96 N6 522,814 574,012 577,684 529,032 582,048 585,317 98.82 98.62 98.70
    N9 214,076 219,195 113,406 264,456 264,456 133,056 80.95 82.89 85.23 Total 6,401,839
    7,159,805 7,157,091 6,915,096 7,486,674 7445923,2 92.58 95.63 96.12 3.3. Outliers
    Table 4 shows the number of outliers that were excluded based on the acceptable
    data ranges (which are shown in the Supplementary Material 2 Table S3). Any data
    point that did not fall within the accepted range was considered an outlier and
    excluded. Table 4 shows that the number of outliers for most sensor nodes was
    less than 1 %. Nevertheless, the deviations of the outliers from the accepted
    data range were considerably large on several occasions (resulting in clear changes
    in the mean values, as can be seen in Supplementary Material 2 Table S5) and therefore
    would have considerable effects on any analyses based on the data. The percentage
    of outliers was larger for the N1 nodes than for other nodes, which was due to
    the humidity sensors often measuring 100 % humidity. Although this value may indeed
    be accurate on some occasions, this value was disproportionally measured by the
    sensors, likely due to droplets gathering on the humidity sensor (a problem that
    was later solved by fixing a special polytetrafluoroethylene polymer on the sensor).
    Therefore, to exclude these erroneous values, relative humidity values above 99.9
    % were filtered out. Indeed, from February 2022 (when the problem was solved)
    onwards, the percentage of outliers in the humidity data was below 1 %. Table
    4. The number and percentage of outliers excluded from the data per farm and node
    type. Empty Cell Farm 1 Empty Cell Farm 2 Farm 3 Empty Cell Empty Cell Outliers
    (N) Outliers (%) Outliers (N) Outliers (%) Outliers (N) Outliers (%) N1 281,285
    12.07 232,849 11.48 218,202 10.05 N2 731 0.01 674 0.012 695 0.01 N3 210 0.17 1131
    0.39 1615 0.48 N4 13,391 2.53 N5 24 0.00 30 0.00 5 0.00 N6 31 0.00 0 0.00 0 0.00
    N9 241 0.08 210 0.07 27 0.01 3.4. Inter-sensor correlations Table 5 shows the
    correlation between different sensors of the same type, which gives an indication
    of the variation in a certain parameter across a barn and may help to determine
    the number of sensors required for obtaining a good representation of an entire
    barn. These correlations were not performed on the N2 nodes because for determining
    individual cow welfare it is necessary to measure the behaviour of individual
    cows. Table 5 shows that the correlations are very high for the measures collected
    by the N1 nodes, showing that the climate is relatively homogeneous within each
    of the monitored barn sections. An exception to this conclusion is humidity, which
    may indicate that some areas of the barn (e.g., close to sprinklers) may be more
    humid than others. However, the differences may also be due partly to difficulties
    with the humidity sensors (as detailed in Section 3.3). In contrast to those for
    N1 nodes, measures of air quality, wind, litter and drink water have much lower
    inter-sensor correlations. Some of this variation might on some occasions be the
    result of problems with individual sensors. Specifically, from April 2022 the
    NH3 measurements by one N5 sensor at farm 3 seemed to have “drifted” and suggest
    that the sensor needs recalibrating. Also, problems with cows digging out the
    (N4) litter sensors may have affected the measurements of these sensors. However,
    apart from these specific effects, most of the variation between sensors is likely
    due to the variability in the conditions in each monitored barn section. Consequently,
    for these parameters it is important to obtain measures from different areas across
    the barn. Interestingly, the degree of variation does not seem to depend on the
    size of the monitored areas, because the variation in the smallest area (farm
    3; 920 m2) was similar to that of the largest area (farm 2; 1324 m2). Therefore,
    the characteristics of the barn structure, its orientation, and characteristics
    of the environment surrounding the barn may be more important than the size of
    the barn in determining the homogeneity of the barn environment. It must also
    be noted that, although variation across the barn was small for some climate measures,
    such as temperature and THI, the relative importance of these measures on cow
    welfare warrants the use of multiple sensors to accurately determine the climatic
    conditions in different areas of the barn. The decision on the number of sensors
    per parameter that should be installed should therefore be based on a careful
    consideration of the costs and benefits. Table 5. Correlations between different
    barn sensors of the same type. If more than two correlations were calculated (which
    was the case if there were more than two sensor nodes), the range from lowest
    to highest correlation coefficient is shown. “Max. p” indicates the maximum obtained
    p-value across all correlations between sensors of the same type (spearman correlation
    with Fisher’s z-transformation). THI = temperature-humidity index, BGHI = black
    globe humidity index, VWC = volumetric water content. Empty Cell Empty Cell Farm
    1 Farm 2 Farm 3 Node Parameter Mean rs Range rs Max p Mean rs Range rs Max p Mean
    rs Range rs Max p N1 Ambient temperature 0.97 0.40––1.00 <0.001 0.97 0.25 – 1.00
    <0.001 0.99 0.98 – 1.00 <0.001 N1 Relative humidity 0.95 0.87 – 0.99 <0.001 0.94
    0.77 – 0.99 <0.001 0.88 0.18 – 0.98 <0.001 N1 Light intensity 0.89 0.81 – 0.95
    <0.001 0.90 0.82 – 0.98 <0.001 0.93 0.87 – 0.98 <0.001 N1 THI 0.99 0.96––1.00
    <0.001 1.00 0.98 – 1.00 <0.001 0.99 0.97 – 1.00 <0.001 N1 Black globe temperature
    0.99 0.99 – 1.00 <0.001 1.00 1.00 <0.001 1.00 0.99 – 1.00 <0.001 N1 BGHI 0.99
    0.99 – 1.00 <0.001 1.00 0.99 – 1.00 <0.001 1.00 0.99 – 1.00 <0.001 N3 Drink water
    temperature 0.81 <0.001 0.64 0.35 – 0.89 <0.001 N3 Drink water use 0.36 0.19 –
    0.51 <0.001 N4 Litter conductivity 0.21 −0.09 – 0.76 <0.001 N4 Litter humidity
    −0.18 −0.58 – 0.19 <0.001 N4 Litter temperature 0.81 0.66 – 0.91 <0.001 N4 Litter
    VWC 0.29 −0.29 – 0.67 <0.001 N5 CO2 0.78 <0.001 0.84 <0.001 0.72 <0.001 N5 H2S*
    N5 NH3 0.60 <0.001 0.64 <0.001 0.48 <0.001 N5 Sound 0.94 <0.001 0.67 <0.001 0.92
    <0.001 N6 Wind direction 0.15 0.07 – 0.22 <0.001 0.10 0.06 <0.001 0.13 −0.33 –
    0.34 <0.001 N6 Wind speed 0.68 0.26 – 0.85 <0.001 0.75 0.24 <0.001 0.66 0.52 –
    0.77 <0.001 * No correlations are displayed for H2S because only values of 0 were
    recorded for this parameter. 3.5. Potential diagnostics with the output The combination
    of the data obtained from the integrated monitoring system can enable different
    types of analyses for cow welfare assessment. On a basic level, the values of
    individually measured parameters can be used to detect situations of reduced welfare,
    e.g., increased daily lying time as an indication of lameness (Dittrich et al.,
    2019, Tucker et al., 2021). However, because an integrated system can measure
    simultaneously many different parameters that are relevant to cow welfare, such
    a system can potentially detect many different types of causes of reduced welfare
    (including lameness, mastitis and heat stress), provided that relevant indicators
    are measured (Leliveld and Provolo, 2020). This feature provides advantages over
    systems that only measure a single or only a few parameters, because an integrated
    system reduces the need to install multiple systems, each with its own architecture
    and supporting software (and thereby reducing the costs of purchase and installation).
    However, even better insight into a cow’s welfare status can be obtained by combining
    different simultaneous measurements (Frost et al., 1997, Leliveld and Provolo,
    2020, Wisnieski et al., 2019). Fig. 6a illustrates the benefits of combining different
    measurements of behaviour. In this graph of a single cow’s behaviour, an increase
    in the time spent lying down and decrease in the time spent standing (and later
    also eating) can be seen from around a week before to two weeks after the cow
    was diagnosed with fever. Because an increase in the time spent exhibiting one
    type of behaviour signifies a reduction in the time spent in other types of behaviour,
    monitoring whether the increase in lying time affects the time spent in another
    behaviour (e.g., eating), may help to better asses the health status of the cow
    and provide better treatment. Download : Download high-res image (391KB) Download
    : Download full-size image Fig. 6. Examples of diagnostics that could be performed
    with the collected data from the integrated monitoring system. a) The percentage
    of time spent in different behaviours by a single cow in the weeks before and
    after a diangosed health issue (fever in this case), with 0 indicating the day
    of diagnosis. b) The percentage of time spent in different behaviours by a single
    cow during several weeks in late spring, combined with the mean THI (temperature-humidity
    index) during these weeks. c) Daily means and standard deviations of the difference
    between indoor and outdoor THI, calculated by substracting outdoor from indoor
    values, on the three farms. The means are based on four selected days per season:
    22─25.09.2021 (autumn), 24─27.02.2022 (winter), 03-06─06.2022 (spring), 22─25.07.2022
    (summer). d) Daily means and standard deviations of the wind speed measured by
    three sensors that were placed in different locations in the barn of farm 1. The
    values are presented as differences from the mean of all sensors and are based
    on four selected days per season: 22─25.09.2021 (autumn), 24─27.02.2022 (winter),
    03─06-06.2022 (spring), 22─25.07.2022 (summer). Even more insight can be obtained
    when behavioural data are combined with data regarding the barn climate, as is
    illustrated in Fig. 6b. In this figure a change in the behaviour of a single cow
    can be seen, i.e., a decrease in lying behaviour and increase in standing behaviour.
    A reduction in lying time and an increase in standing time could be a sign of
    heat stress (Hoffmann et al., 2020, Tucker et al., 2021), but may also be observed
    in cows with mastitis (Dittrich et al., 2019, Tucker et al., 2021). The inclusion
    of THI measurements improves the determination of the reason for this change in
    behaviour. In the case illustrated by Fig. 6b, the change in lying and standing
    time seem to parallel simultaneous changes in THI, suggesting that the cow may
    be experiencing heat stress, rather than suffering from mastitis. The integration
    of data, as performed in this prototype system, not only benefits the monitoring
    of the welfare status of individual cows, but also could help to identify structural
    or mechanical problems in the barn that could affect the welfare of all cows.
    This is enabled by the use of an external weather station as a reference and the
    use of multiple sensors to measure the same parameters. As illustrated in Fig.
    6c, the combination of measures of temperature and humidity both inside and outside
    the barn allows to calculate the difference in THI between the inside and outside
    of the barn and thereby to determine if the structure functions well to buffer
    extreme external weather conditions (Lovarelli et al., 2021). Fig. 6c shows that
    the more enclosed barn at farm 1 has positive Δ THI values in February and negative
    Δ THI values in June and July, suggesting that this barn buffers the outside THI
    values (at least numerically) better than the barn on the other two farms, which
    are open on all sides. This example shows that the integrated system can monitor
    the buffering capabilities of a barn and potential changes to them over time,
    which might occur when, for instance, climate controlling devices such as ventilators
    malfunction. The automatic detection of changes in the barn climate would enable
    a farmer to respond before any effect may be noticed in the behaviour of the cows.
    In addition, the use of multiple sensors to measure the same parameter offers
    the possibility to detect specific potential problem areas where the environmental
    conditions are inferior to those elsewhere in the barn. For example, Fig. 6d illustrates
    a comparison of wind speed measured by three wind sensors on farm 1 in different
    seasons. During autumn and winter, the values measured by the different sensors
    seem quite similar, but during late spring and summer (when the ventilators were
    operational), the mean values measured by sensor 2 are numerically lower than
    those from the other two sensors, and the mean values measured by sensor 3 are
    numerically higher than the average wind speed in the barn. Although these results
    may in part have been caused by a slightly higher positioning of sensor 2, this
    comparison also suggests that the ventilation system has a better coverage of
    the feeding area (monitored by sensor 3) than of the lying area (monitored by
    sensors 1 and 2), which could negatively affect cows’ lying times in the summer
    period (Calegari et al., 2014). The detection of problem areas in the barn can
    help to resolve potential structural or mechanical problems in these areas, thereby
    improving the barn environment as a whole and preventing the aggregation of cattle
    in one part of the barn (Provolo and Riva, 2009, Seyfi, 2013). The integrated
    monitoring system can also be extended to directly control the barn environment
    through the operation of, for example, cooling systems and wind screens, thereby
    achieving faster and more appropriate barn climate control. 4. Discussion The
    prototype of an integrated system designed to monitor barn environment and cow
    behaviour simultaneously was aimed to provide an open framework for research,
    allowing for customization and fine-tuning before its ultimate adoption as a management
    assistant on commercial dairy farms. Evaluation of the collected data shows that
    the system can effectively handle large quantities of information arriving at
    frequent intervals, with minimal data loss and outliers. Indeed, even though there
    were occasional malfunctions of individual nodes or sensors, the functioning of
    the more central components (i.e., gateways and backend) were only affected when
    power outages occurred on the farms. Data loss was kept low by combining a time-slotted
    wireless protocol, which prevents data collision (Khaleghi et al., 2013), with
    distributed data buffering, which minimizes data loss due to temporary failures
    of the communication infrastructure. Furthermore, the time of the nodes was constantly
    synchronized with the gateway time to prevent clock drifting which could have
    caused loss of data if the data were transmitted in the wrong time slot. Another
    challenge for data fusion (or indeed for any automatic collection of data) is
    the inherent uncertainty in sensor measurements, which could be caused by noise
    or impreciseness of the measurements or by the ambiguities and inconsistencies
    present in the environment (King et al., 2017, Kumar et al., 2006). Indeed, even
    though the percentage of outliers in our data was low, the outliers still had
    considerable effects on the calculated means and should therefore be excluded
    from analyses. In this study, the data were filtered manually, but there are several
    established outlier-detection algorithms that could be integrated in the system
    to exclude these values automatically (Basu and Meckesheimer, 2007, Du et al.,
    2023). The evaluation of data stability and outliers represent the first steps
    in testing the quality (or reliability) of the data that are obtained by the integrated
    system. The next step is to test the system’s ability to accurately detect situations
    of reduced cow welfare on the farm. Apart from the data quality, the cost of the
    system (purchase, installation, and operation) is also of high importance to farmers
    (Zhang et al., 2021). The initial investment is expected to lie between €60,000─80,000
    for a farm with 200 dairy cows. Although initial investments in automatic monitoring
    system are usually high, many economic evaluations show that these systems increase
    annual profit (Adenuga et al., 2020, Drach et al., 2017, Rutten et al., 2014).
    For instance, improved oestrus detection through the monitoring of cow behaviour
    was estimated to raise annual profit by 7 to 94.3 % (Adenuga et al., 2020), while
    effective heat stress management could reduce economic loss due to heat stress
    by as much as 60 % (St-Pierre et al., 2003). Compared to other systems, which
    tend to focus on a limited set of parameters, this integrated system has the added
    advantage of enabling improved management of multiple variables rather than just
    one (e.g., oestrus detection or heat stress management), facilitating higher net
    profits. Nevertheless, the initial installation cost plays an important role in
    the decision of farmers to invest in PLF technologies (Borchers and Bewley, 2015).
    To reduce costs, it is important to limit the use of multiple sensors by determining
    the minimum required number of sensors that are necessary per barn. The correlations
    shown in Table 5 can help to determine this number. To obtain a comprehensive
    overview of all aspects of a cow’s state, especially related to welfare (Fraser
    et al., 1997, von Keyserlingk et al., 2009), a multidimensional approach is needed.
    Advantages of using integrated data from different sources have already been shown
    in several studies on livestock management (Chang et al., 2022, Cruz et al., 2022,
    Pandey et al., 2021). However, while it is certain that data integration gives
    a better overview of a cow’s status, much is still unknown about the interactions
    between barn environment, cow behaviour and cow status (Leliveld & Provolo, 2020).
    For research and development purposes, it is therefore important that a system
    is open and can be customized to incorporate new measurements and explore new
    approaches to optimize the automatic monitoring of dairy cows. The present study
    lays the foundation of an integrated automatic monitoring system that is customizable
    and open by establishing suitable architecture that can unify and process in one
    place data from various sources. In addition, the system incorporates features
    that are specifically adapted for the use on dairy farms and in rural areas, such
    as an ultra-low power design and wireless communication channels (Germani et al.,
    2019, Riaboff et al., 2022). This work aimed to establish an open and customizable
    system with an architecture suited to data integration. By establishing such a
    system, this work provides an open framework for research-based data integration
    appropriate for the automatic, remote, and real-time monitoring of livestock.
    The next step in the development of this system is to design models and algorithms
    that can combine the many different parameters that are measured by the system
    to detect patterns associated with single welfare issues (e.g., reduced lying
    time, combined with increased THI as an indication of heat stress) as well as
    reduced cow welfare in general. For instance, algorithms similar to those developed
    by Chang et al. (2022) could be adopted to detect various cow welfare issues from
    the data generated by this system. Another important next step is the integration
    of herd data (e.g., calving dates and health status) and milk yield. This would
    not only help to improve the accurate detection of single welfare issues (Jensen
    et al., 2016, Van Hertem et al., 2016), but also provide important information
    on cow reproduction and production, leading to a more complete picture of a cow’s
    state and improved cow management. 5. Conclusions The development of this prototype
    system entails important progress towards data integration in the field of smart
    dairy farming, as well as a first step in the design and implementation of an
    open and customizable automatic integrated cow monitoring system for both research
    and commercial purposes. Next steps involve the integration of additional information,
    such as herd data, and to develop suitable models and algorithms that can combine
    data from multiple diverse sources to provide an accurate and complete overview
    of the state of an animal. The final product would then consist of a single system
    that automatically monitors all aspects related to production, reproduction, and
    welfare of cows on the farm, thereby supporting farmers in important management
    decision-making. CRediT authorship contribution statement Lisette M.C. Leliveld:
    Methodology, Investigation, Formal analysis, Visualization, Writing – original
    draft, Writing – review & editing. Carlo Brandolese: Conceptualization, Methodology,
    Software, Writing – original draft, Visualization, Writing – review & editing.
    Matteo Grotto: Methodology, Software, Data curation. Augusto Marinucci: Resources.
    Nicola Fossati: Resources. Daniela Lovarelli: Methodology, Investigation, Formal
    analysis, Writing – review & editing. Elisabetta Riva: Investigation. Giorgio
    Provolo: Conceptualization, Methodology, Data curation, Investigation, Funding
    acquisition, Visualization, Writing – review & editing. Declaration of competing
    interest The authors declare that they have no known competing financial interests
    or personal relationships that could have appeared to influence the work reported
    in this paper. Acknowledgements This project was funded by the Lombardy Region,
    Italy, as part of the Rural Development Program 2014─2020, EIP-AGRI Operational
    Groups,Project GALA, and was carried out within the Agritech National Research
    Center and received funding from the European Union Next-GenerationEU, Belgium,
    (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) – MISSIONE 4 COMPONENTE 2, INVESTIMENTO
    1.4 – D.D. 1032 17/06/2022, CN00000022). This manuscript reflects only the authors’
    views and opinions, neither the European Union nor the European Commission can
    be considered responsible for them. The authors would like to thank the farmers
    and their staff for their support in the installation and operation of the monitoring
    system on their farms, as well as for providing cow-related data. Furthermore,
    the authors like to thank Paolo Marconi and Lucio Zanini of the Regional Association
    of Farmers in Lombardy (ARAL) and Alon Arazi of Afimilk ® for providing data on
    the health events of cows at the three commercial farms. Simone Libutti is also
    thanked for his work on the development of the dashboard. Finally, the authors
    would like to thank Arianna Panara and Manuela Dall’Angelo for technical support
    in the behavioural data collection and Tomaso Bertoni and Stefano Penati for technical
    support in the maintenance of the system. Appendix A. Supplementary data Download
    all supplementary files included with this article What’s this? The following
    are the Supplementary data to this article: Download : Download spreadsheet (14KB)
    Supplementary data 1. Download : Download Word document (5MB) Supplementary data
    2. Data availability Data will be made available on request. References Abeni
    et al., 2019 F. Abeni, F. Petrera, A. Galli A survey of italian dairy farmers’
    propensity for precision livestock farming tools Animals, 9 (2019), pp. 1-13,
    10.3390/ani9050202 Google Scholar Adenuga et al., 2020 A.H. Adenuga, C. Jack,
    K.O. Olagunju, A. Ashfield Economic viability of adoption of automated oestrus
    detection technologies on dairy farms: A review Animals, 10 (2020), pp. 1-14,
    10.3390/ani10071241 View in ScopusGoogle Scholar Akbar et al., 2020 M.O. Akbar,
    M.S. Shahbaz Khan, M.J. Ali, A. Hussain, G. Qaiser, M. Pasha, U. Pasha, M.S. Missen,
    N. Akhtar IoT for Development of Smart Dairy Farming Journal of Food Quality,
    2020 (2020), p. 4242805, 10.1155/2020/4242805 View in ScopusGoogle Scholar Alonso
    et al., 2020 R.S. Alonso, I. Sittón-Candanedo, Ó. García, J. Prieto, S. Rodríguez-González
    An Intelligent Edge-IoT Platform for Monitoring Livestock and Crops in a Dairy
    Farming Scenario Ad Hoc Netw., 98 (2020), Article 102047, 10.1016/j.adhoc.2019.102047
    View PDFView articleView in ScopusGoogle Scholar Amiri-Zarandi et al., 2022 M.
    Amiri-Zarandi, M.H. Fard, S. Yousefinaghani, M. Kaviani, R. Dara A Platform Approach
    to Smart Farm Information Processing Agriculture, 12 (2022), p. 838, 10.3390/agriculture12060838
    View in ScopusGoogle Scholar Barkema et al., 2015 H.W. Barkema, M.A.G. von Keyserlingk,
    J.P. Kastelic, T.J.G.M. Lam, C. Luby, J.P. Roy, S.J. LeBlanc, G.P. Keefe, D.F.
    Kelton Invited review: Changes in the dairy industry affecting dairy cattle health
    and welfare J. Dairy Sci., 98 (2015), pp. 7426-7445, 10.3168/jds.2015-9377 View
    PDFView articleView in ScopusGoogle Scholar Basu and Meckesheimer, 2007 S. Basu,
    M. Meckesheimer Automatic outlier detection for time series: An application to
    sensor data Knowl. Inf. Syst., 11 (2007), pp. 137-154, 10.1007/s10115-006-0026-6
    View in ScopusGoogle Scholar Berckmans, 2014 D. Berckmans Precision livestock
    farming technologies for welfare management in intensive livestock systems Rev.
    Sci. Tech., 33 (2014), pp. 189-196 https://doi.org/10.20506/rst.33.1.2273 CrossRefView
    in ScopusGoogle Scholar Berckmans, 2017 D. Berckmans General introduction to precision
    livestock farming Anim. Front., 7 (2017), pp. 6-11, 10.2527/af.2017.0102 View
    in ScopusGoogle Scholar Bluetooth, 2023 S.I.G. Bluetooth Specifications Accessed
    on 5 Octobre 2023 https://www.bluetooth.com/specifications/ (2023) Google Scholar
    Borchers and Bewley, 2015 M.R. Borchers, J.M. Bewley An assessment of producer
    precision dairy farming technology use, prepurchase considerations, and usefulness
    J. Dairy Sci., 98 (2015), pp. 4198-4205, 10.3168/jds.2014-8963 View PDFView articleView
    in ScopusGoogle Scholar Calegari et al., 2014 F. Calegari, L. Calamari, E. Frazzi
    Fan cooling of the resting area in a free stalls dairy barn Int. J. Biometeorol.,
    58 (2014), pp. 1225-1236, 10.1007/s00484-013-0716-1 View in ScopusGoogle Scholar
    Chang et al., 2022 Chang, A. Z., Swain, D. L., Trotter, M. G., 2022. A multi-sensor
    approach to calving detection. Inf. Process. Agric. https://doi.org/10.1016/j.inpa.2022.07.002.
    Google Scholar Cruz et al., 2022 V. Cruz, J. Rico, D. Coelho, F. Baptista Innovative
    PLF Tool to Assess Growing-Finishing Pigs’ Welfare Agronomy, 12 (2022), p. 2159,
    10.3390/agronomy12092159 View in ScopusGoogle Scholar De Montis et al., 2017 A.
    De Montis, G. Modica, C. Arcidiacono AgInformatics L.A. Schintler, C.L. McNeely
    (Eds.), Encyclopedia of Big Data, Springer International Publishing, Cham (2017),
    pp. 1-4, 10.1007/978-3-319-32001-4_218-1 Google Scholar Dittrich et al., 2019
    I. Dittrich, M. Gertz, J. Krieter Alterations in sick dairy cows’ daily behavioural
    patterns Heliyon, 5 (2019), p. e02902 View PDFView articleView in ScopusGoogle
    Scholar Dolecheck et al., 2015 K.A. Dolecheck, W.J. Silvia, G. Heersche, Y.M.
    Chang, D.L. Ray, A.E. Stone, B.A. Wadsworth, J.M. Bewley Behavioral and physiological
    changes around estrus events identified using multiple automated monitoring technologies
    J. Dairy Sci., 98 (2015), pp. 8723-8731, 10.3168/jds.2015-9645 View PDFView articleView
    in ScopusGoogle Scholar Dominiak and Kristensen, 2017 K.N. Dominiak, A.R. Kristensen
    Prioritizing alarms from sensor-based detection models in livestock production
    - A review on model performance and alarm reducing methods Comp. Electron. Agric.,
    133 (2017), pp. 46-67, 10.1016/j.compag.2016.12.008 View PDFView articleView in
    ScopusGoogle Scholar Drach et al., 2017 U. Drach, I. Halachmi, T. Pnini, I. Izhaki,
    A. Degani Automatic herding reduces labour and increases milking frequency in
    robotic milking Biosyst. Eng., 155 (2017), pp. 134-141, 10.1016/j.biosystemseng.2016.12.010
    View PDFView articleView in ScopusGoogle Scholar Du et al., 2023 X. Du, E. Zuo,
    Z. Chu, Z. He, J. Yu Fluctuation-Based Outlier Detection. Sci. Rep., 13 (2023),
    pp. 1-18, 10.1038/s41598-023-29549-1 Google Scholar Firner et al., 2010 B. Firner,
    C. Xu, R. Howard, Y. Zhang Multiple Receiver Strategies for Minimizing Packet
    Loss in Dense Sensor Networks, ACM Press, Chicago, Illinois, USA (2010), p. 211,
    10.1145/1860093.1860122 View in ScopusGoogle Scholar Fountas et al., 2020 S. Fountas,
    B. Espejo-Garcia, A. Kasimati, N. Mylonas, N. Darra The Future of Digital Agriculture:
    Technologies and Opportunities IT Prof., 22 (2020), pp. 24-28, 10.1109/MITP.2019.2963412
    View in ScopusGoogle Scholar Fraser et al., 1997 D. Fraser, D.M. Weary, E.A. Pajor,
    B.N. Milligan A scientific conception of animal welfare that reflects ethical
    concerns Anim. Welf., 6 (1997), pp. 187-205, 10.1017/S0962728600019795 View in
    ScopusGoogle Scholar Frost et al., 1997 A.R. Frost, C.P. Schofield, S.A. Beaulah,
    T.T. Mottram, J.A. Lines, C.M. Wathes A review of livestock monitoring and the
    need for integrated systems Comp. Electron. Agric., 17 (1997), pp. 139-159, 10.1016/s0168-1699(96)01301-4
    View PDFView articleView in ScopusGoogle Scholar Germani et al., 2019 L. Germani,
    V. Mecarelli, G. Baruffa, L. Rugini, F. Frescura An IoT architecture for continuous
    livestock monitoring using lora LPWAN Electronics, 8 (2019), p. 1435, 10.3390/electronics8121435
    View in ScopusGoogle Scholar Halachmi et al., 2019 I. Halachmi, M. Guarino, J.
    Bewley, M. Pastell Smart Animal Agriculture: Application of Real-Time Sensors
    to Improve Animal Well-Being and Production Annu. Rev. Anim. Biosci., 7 (2019),
    pp. 403-425, 10.1146/annurev-animal-020518-114851 View in ScopusGoogle Scholar
    Hoffmann et al., 2020 G. Hoffmann, P. Herbut, S. Pinto, J. Heinicke, B. Kuhla,
    T. Amon Animal-related, non-invasive indicators for determining heat stress in
    dairy cows Biosyst. Eng., 199 (2020), pp. 83-96, 10.1016/j.biosystemseng.2019.10.017
    View PDFView articleView in ScopusGoogle Scholar Jacobs and Siegford, 2012 J.A.
    Jacobs, J.M. Siegford Invited review: The impact of automatic milking systems
    on dairy cow management, behavior, health, and welfare J. Dairy Sci., 95 (2012),
    pp. 2227-2247, 10.3168/jds.2011-4943 View PDFView articleView in ScopusGoogle
    Scholar Jensen et al., 2016 D.B. Jensen, H. Hogeveen, A. De Vries Bayesian integration
    of sensor information and a multivariate dynamic linear model for prediction of
    dairy cow mastitis J. Dairy Sci., 99 (2016), pp. 7344-7361, 10.3168/jds.2015-10060
    View PDFView articleView in ScopusGoogle Scholar Khaleghi et al., 2013 B. Khaleghi,
    A. Khamis, F.O. Karray, S.N. Razavi Multisensor data fusion: A review of the state-of-the-art
    Inf. Fusion, 14 (2013), pp. 28-44, 10.1016/j.inffus.2011.08.001 View PDFView articleView
    in ScopusGoogle Scholar King et al., 2017 R.C. King, E. Villeneuve, R.J. White,
    R.S. Sherratt, W. Holderbaum, W.S. Harwin Application of data fusion techniques
    and technologies for wearable health monitoring Med. Eng. Phys., 42 (2017), pp.
    1-12, 10.1016/j.medengphy.2016.12.011 View PDFView articleView in ScopusGoogle
    Scholar Kumar et al., 2006 Kumar, M., Garg, D. P., & Zachery, R. A., 2006. A generalized
    approach for inconsistency detection in data fusion from multiple sensors. In
    Proceedings of the 2006 American Control Conference, Minneapolis, MN, USA, 14–16
    June 2006. Google Scholar Lee and Seo, 2021 M. Lee, S. Seo Wearable wireless biosensor
    technology for monitoring cattle: A review Animals, 11 (2021), p. 2779, 10.3390/ani11102779
    View in ScopusGoogle Scholar Leliveld and Provolo, 2020 L.M.C. Leliveld, G. Provolo
    A Review of Welfare Indicators of Indoor-Housed Dairy Cow as a Basis for Integrated
    Automatic Welfare Assessment Systems Animals, 10 (2020), p. 1430, 10.3390/ani10081430
    Google Scholar Lovarelli et al., 2021 D. Lovarelli, E. Riva, G. Mattachini, M.
    Guarino, G. Provolo Assessing the effect of barns structures and environmental
    conditions in dairy cattle farms monitored in Northern Italy J. Agr. Eng. LI,
    I (2021), p. 1229, 10.4081/jae.2021.1229 View in ScopusGoogle Scholar Lovarelli
    et al., 2022 D. Lovarelli, C. Brandolese, L. Leliveld, A. Finzi, E. Riva, M. Grotto,
    G. Provolo Development of a new wearable 3D sensor node and innovative open classification
    system for dairy cows’ behavior Animals, 12 (2022), p. 1447, 10.3390/ani12111447
    View in ScopusGoogle Scholar Norton et al., 2019 T. Norton, C. Chen, M.L.V. Larsen,
    D. Berckmans Review: Precision livestock farming: building ‘digital representations’
    to bring the animals closer to the farmer Animal, 13 (2019), pp. 3009-3017, 10.1017/S175173111900199X
    View PDFView articleView in ScopusGoogle Scholar Oasis, 2023 Oasis MQTT Specifications
    Accessed on 05 Octobre 2023 https://mqtt.org/mqtt-specification/ (2023) Google
    Scholar Pandey et al., 2021 S. Pandey, U. Kalwa, T. Kong, B. Guo, P.C. Gauger,
    D.J. Peters, K.-Y. Yoon Behavioral Monitoring Tool for Pig Farmers: Ear Tag Sensors,
    Machine Intelligence, and Technology Adoption Roadmap Animals, 11 (2021), p. 2665,
    10.3390/ani11092665 View in ScopusGoogle Scholar Provolo and Riva, 2009 G. Provolo,
    E. Riva One Year Study of Lying and Standing Behaviour of Dairy Cows in a Freestall
    Barn in Italy Agric. Eng., 2 (2009), pp. 27-33, 10.4081/jae.2009.2.27 Google Scholar
    Riaboff et al., 2022 L. Riaboff, L. Shalloo, A.F. Smeaton, S. Couvreur, A. Madouasse,
    M.T. Keane Predicting livestock behaviour using accelerometers: A systematic review
    of processing techniques for ruminant behaviour prediction from raw accelerometer
    data Comp. Electron. Agric., 192 (2022), Article 106610, 10.1016/j.compag.2021.106610
    View PDFView articleView in ScopusGoogle Scholar Rushen et al., 2008 J. Rushen,
    A.M. de Passillé, M.A.G. von Keyserlingk, D.M. Weary The welfare of cattle Springer,
    Dordrecht, The Netherlands (2008) Google Scholar Rutten et al., 2013 C.J. Rutten,
    A.G.J. Velthuis, W. Steeneveld, H. Hogeveen Invited review: Sensors to support
    health management on dairy farms J. Dairy Sci., 96 (2013), pp. 1928-1952, 10.3168/jds.2012-6107
    View PDFView articleView in ScopusGoogle Scholar Rutten et al., 2014 C.J. Rutten,
    W. Steeneveld, C. Inchaisri, H. Hogeveen An ex ante analysis on the use of activity
    meters for automated estrus detection: To invest or not to invest? J. Dairy Sci.,
    97 (2014), pp. 6869-6887, 10.3168/jds.2014-7948 View PDFView articleView in ScopusGoogle
    Scholar Saint-Dizier and Chastant-Maillard, 2012 M. Saint-Dizier, S. Chastant-Maillard
    Towards an Automated Detection of Oestrus in Dairy Cattle Reprod. Domest. Anim.,
    47 (2012), pp. 1056-1061, 10.1111/j.1439-0531.2011.01971.x View in ScopusGoogle
    Scholar Schauberger et al., 2020 G. Schauberger, I. Hennig-Pauka, W. Zollitsch,
    S.J. Hörtenhuber, J. Baumgartner, K. Niebuhr, M. Piringer, W. Knauder, I. Anders,
    K. Andre, M. Schönhart Efficacy of adaptation measures to alleviate heat stress
    in confined livestock buildings in temperate climate zones Biosyst. Eng., 200
    (2020), pp. 157-175, 10.1016/j.biosystemseng.2020.09.010 View PDFView articleView
    in ScopusGoogle Scholar Seyfi, 2013 S.U. Seyfi Hourly and seasonal variations
    in the area preferences of dairy cows in freestall housing J. Dairy Sci., 96 (2013),
    pp. 906-917, 10.3168/jds.2012-5618 View PDFView articleView in ScopusGoogle Scholar
    Smart Farm Information Processing, xxxx Smart Farm Information Processing. Agriculture,
    12, 838. https://doi.org/10.3390/agriculture12060838. Google Scholar St-Pierre
    et al., 2003 N.R. St-Pierre, B. Cobanov, G. Schnitkey Economic Losses from Heat
    Stress by US Livestock Industries J. Dairy Sci., 86 (2003), pp. E52-E77, 10.3168/jds.S0022-0302(03)74040-5
    View PDFView articleView in ScopusGoogle Scholar Symeonaki et al., 2022 E. Symeonaki,
    K.G. Arvanitis, D. Piromalis, D. Tseles, A.T. Balafoutis Ontology-Based IoT Middleware
    Approach for Smart Livestock Farming toward Agriculture 4.0: A Case Study for
    Controlling Thermal Environment in a Pig Facility Agronomy, 12 (2022), p. 750,
    10.3390/agronomy12030750 View in ScopusGoogle Scholar Tucker et al., 2021 C.B.
    Tucker, M.B. Jensen, A.M. de Passillé, L. Hänninen, J. Rushen Invited review:
    Lying time and the welfare of dairy cows J. Dairy Sci., 104 (2021), pp. 20-46,
    10.3168/jds.2019-18074 View PDFView articleGoogle Scholar Van Hertem et al., 2016
    T. Van Hertem, C. Bahr, A.S. Tello, S. Viazzi, M. Steensels, C.E.B. Romanini,
    C. Lokhorst, E. Maltz, I. Halachmi, D. Berckmans Lameness detection in dairy cattle:
    Single predictor v. multivariate analysis of image-based posture processing and
    behaviour and performance sensing Animal, 10 (2016), pp. 1525-1532, 10.1017/S1751731115001457
    View PDFView articleView in ScopusGoogle Scholar von Keyserlingk et al., 2009
    M.A.G. von Keyserlingk, J. Rushen, A.M. de Passillé, D.M. Weary Invited review:
    The welfare of dairy cattle-key concepts and the role of science J. Dairy Sci.,
    92 (2009), pp. 4101-4111, 10.3168/jds.2009-2326 View PDFView articleView in ScopusGoogle
    Scholar Wathes, 2010 C.M. Wathes The prospects for precision livestock farming
    J. Roy. Agric. Soc. England, 171 (2010), pp. 26-32 Google Scholar Wisnieski et
    al., 2019 L. Wisnieski, B. Norby, S.J. Pierce, T. Becker, L.M. Sordillo Prospects
    for predictive modeling of transition cow diseases Anim. Health Res. Rev., 20
    (2019), pp. 19-30, 10.1017/S1466252319000112 Google Scholar Zhang et al., 2021
    M. Zhang, X. Wang, H. Feng, Q. Huang, X. Xiao, X. Zhang Wearable Internet of Things
    enabled precision livestock farming in smart farms: A review of technical solutions
    for precise perception, biocompatibility, and sustainability monitoring J. Clean.
    Prod., 312 (2021), Article 127712, 10.1016/j.jclepro.2021.127712 View PDFView
    articleView in ScopusGoogle Scholar Cited by (0) © 2023 The Authors. Published
    by Elsevier B.V. Recommended articles Intelligent methodologies: An integrated
    multi-modeling approach to predict adaptive mechanisms in farm animals Computers
    and Electronics in Agriculture, Volume 216, 2024, Article 108502 Robson Mateus
    Freitas Silveira, …, Iran José Oliveira da Silva View PDF Onfield estimation of
    quality parameters in alfalfa through hyperspectral spectrometer data Computers
    and Electronics in Agriculture, Volume 216, 2024, Article 108463 Angie L. Gámez,
    …, Iker Aranjuelo View PDF Opinion: Agricultural Robotics, issues worthy to study
    Computers and Electronics in Agriculture, Volume 216, 2024, Article 108528 Qin
    Zhang View PDF Show 3 more articles Article Metrics Captures Readers: 24 View
    details About ScienceDirect Remote access Shopping cart Advertise Contact and
    support Terms and conditions Privacy policy Cookies are used by this site. Cookie
    settings | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier
    B.V., its licensors, and contributors. All rights are reserved, including those
    for text and data mining, AI training, and similar technologies. For all open
    access content, the Creative Commons licensing terms apply."'
  inline_citation: '>'
  journal: Computers and Electronics in Agriculture
  limitations: []
  relevance_evaluation:
    extract_1: 'The purpose and intention of this systematic review on automated systems
      for real-time irrigation management can be interpreted as follows:Addressing
      the global food challenge: The review aims to explore how automated, real-time
      irrigation management systems can contribute to the efficient use of water resources
      and enhance agricultural productivity to meet the growing demand for food.'
    extract_2: 'Highlighting the role of interoperability and standardization: The
      review seeks to emphasize the importance of interoperability and standardization
      in enabling the integration of components within the automated irrigation management
      pipeline.'
    limitations: []
    relevance_score: 0.9
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  title: 'Real-time automatic integrated monitoring of barn environment and dairy
    cattle behaviour: Technical implementation and evaluation on three commercial
    farms'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Sun, L., Zhu, J., Tan, J., Li, X., Deng, H., Li, R., … Zhu, X. (2023).
    Deep learning-assisted automated sewage pipe defect detection for urban water
    environment management. Science of The Total Environment, 882(163562). https://doi.org/10.1016/j.scitotenv.2023.163562.
  authors:
  - Sun L.
  - Zhu J.
  - Tan J.
  - Li X.
  - Li R.
  - Deng H.
  - Zhang X.
  - Liu B.
  - Zhu X.
  citation_count: '4'
  description: A healthy sewage pipe system plays a significant role in urban water
    management by collecting and transporting wastewater and stormwater, which can
    be assessed by hydraulic model. However, sewage pipe defects have been observed
    frequently in recent years during regular pipe maintenance according to the captured
    interior videos of underground pipes by closed-circuit television (CCTV) robots.
    In this case, hydraulic model constructed based on a healthy pipe would produce
    large deviations with that in real hydraulic performance and even be out of work,
    which can result in unanticipated damages such as blockage collapse or stormwater
    overflows. Quick defect evaluation and defect quantification are the precondition
    to achieve risk assessment and model calibration of urban water management, but
    currently pipe defects assessment still largely relies on technicians to check
    the CCTV videos/images. An automated sewage pipe defect detection system is necessary
    to timely determine pipe issues and then rehabilitate or renew sewage pipes, while
    the rapid development of deep learning especially in recent five years provides
    a fantastic opportunity to construct automated pipe defect detection system by
    image recognition. Given the initial success of deep learning application in CCTV
    interpretation, the review (i) integrated the methodological framework of automated
    sewage pipe defect detection, including data acquisition, image pre-processing,
    feature extraction, model construction and evaluation metrics, (ii) discussed
    the state-of-the-art performance of deep learning in pipe defects classification,
    location, and severity rating evaluation (e.g., up to ~96 % of accuracy and 140
    FPS of processing speed), and (iii) proposed risk assessment and model calibration
    in urban water management by considering pipe defects. This review introduces
    a novel practical application-oriented methodology including defect data acquisition
    by CCTV, model construction by deep learning, and model application, provides
    references for further improving accuracy and generalization ability of urban
    water management models in practical application.
  doi: 10.1016/j.scitotenv.2023.163562
  explanation: In this study, the authors delve into the research methodologies and
    approaches used in the automated detection of sewer pipe defects in order to evaluate
    their relevance to current practices and future research directions in urban water
    management. Their article begins by providing a detailed overview of real-time
    irrigation management systems, encompassing both IoT-enabled automation and computer
    vision technologies. After presenting a concise summary of the key research questions
    and objectives, the authors discuss the specific applications of automated pipe
    defect detection within real-time irrigation management systems. They conclude
    by identifying important limitations of the current body of research and outlining
    future research directions that could further advance this field.
  extract_1: Deep learning (DL), especially convolutional neural network (CNN)-based
    DL in the past five years has been approved as a qualified successor for automated
    pipe defect detection with more robust and versatile performance, in the case
    of sufficient training data (Dang et al., 2021; Tan et al., 2021; Yin et al.,
    2020).
  extract_2: Afterwards, this paper introduces a novel practical application-oriented
    methodological framework of automated pipe defects detection, discusses the contribution
    of sewage pipe defect detection to risk evaluation and model calibration in urban
    water management.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Survey
    methodology 3. CCTV & DL overview in sewage pipe defect detection 4. Methodological
    framework of automated sewage pipe defect detection 5. Updates on automated pipe
    defect detection for scientific urban water management 6. Challenges and prospects
    CRediT authorship contribution statement Declaration of competing interest Acknowledgement
    Data availability References Show full outline Cited by (5) Figures (6) Tables
    (1) Table 1 Extras (1) Table S1 Science of The Total Environment Volume 882, 15
    July 2023, 163562 Deep learning-assisted automated sewage pipe defect detection
    for urban water environment management Author links open overlay panel Lianpeng
    Sun a b, Jinjun Zhu a, Jinxin Tan a, Xianfeng Li c, Ruohong Li a b, Huanzhong
    Deng a, Xinyang Zhang a, Bingyou Liu a, Xinzhe Zhu a b Show more Add to Mendeley
    Share Cite https://doi.org/10.1016/j.scitotenv.2023.163562 Get rights and content
    Highlights • Automated detection of sewage pipe defects via deep learning (DL)
    was summarized. • DL analysis based on closed-circuit television (CCTV) images/videos
    was focused. • Methodological framework of CCTV & DL was discussed in sewage pipe
    detection. • CCTV & DL shows great potential in automated pipe defect detection
    and assessment. • Defect detection helps risk assessment and management of urban
    water environment. Abstract A healthy sewage pipe system plays a significant role
    in urban water management by collecting and transporting wastewater and stormwater,
    which can be assessed by hydraulic model. However, sewage pipe defects have been
    observed frequently in recent years during regular pipe maintenance according
    to the captured interior videos of underground pipes by closed-circuit television
    (CCTV) robots. In this case, hydraulic model constructed based on a healthy pipe
    would produce large deviations with that in real hydraulic performance and even
    be out of work, which can result in unanticipated damages such as blockage collapse
    or stormwater overflows. Quick defect evaluation and defect quantification are
    the precondition to achieve risk assessment and model calibration of urban water
    management, but currently pipe defects assessment still largely relies on technicians
    to check the CCTV videos/images. An automated sewage pipe defect detection system
    is necessary to timely determine pipe issues and then rehabilitate or renew sewage
    pipes, while the rapid development of deep learning especially in recent five
    years provides a fantastic opportunity to construct automated pipe defect detection
    system by image recognition. Given the initial success of deep learning application
    in CCTV interpretation, the review (i) integrated the methodological framework
    of automated sewage pipe defect detection, including data acquisition, image pre-processing,
    feature extraction, model construction and evaluation metrics, (ii) discussed
    the state-of-the-art performance of deep learning in pipe defects classification,
    location, and severity rating evaluation (e.g., up to ~96 % of accuracy and 140
    FPS of processing speed), and (iii) proposed risk assessment and model calibration
    in urban water management by considering pipe defects. This review introduces
    a novel practical application-oriented methodology including defect data acquisition
    by CCTV, model construction by deep learning, and model application, provides
    references for further improving accuracy and generalization ability of urban
    water management models in practical application. Graphical abstract Download
    : Download high-res image (189KB) Download : Download full-size image Previous
    article in issue Next article in issue Keywords Urban water managementHydraulic
    model calibrationAutomated pipe defect detectionImage recognitionDeep learning
    1. Introduction Rapid urbanization increases the demand for sewage treatment infrastructures
    especially in developing countries, in which a well-run sewage pipe network is
    necessary to complete the collection and delivery of stormwater and wastewater
    in cities (Karn et al., 2021; Lund et al., 2019; Xu et al., 2019). However, existing
    pipes around the world have presented defects, which were largely triggered by
    pipe materials aging and external disturbance such as plant intrusion, rainfall
    erosion, etc. (Kumar et al., 2018; Tariq et al., 2022). Pipe defects can lead
    to unexpected sewage leakage, groundwater infiltration and water pollution, as
    well as prediction failure of urban water management model (e.g., flood control
    model and sewer exfiltration model) (Harpaz et al., 2022; Mugume et al., 2015).
    Furthermore, the continued expansion requirements of sewage pipes in developing
    countries will result in rapid increase of pipe network density in the near future,
    which will bring a bigger challenge to daily maintenance of sewer system (Haurum
    and Moeslund, 2020). Therefore, a set of automated and smart sewage pipe defects
    detection system replacing time-consuming and subjective manual inspection is
    prerequisite to promptly rehabilitate or renew sewage pipes, calibrate hydraulic
    models, and improve prediction capacity of urban water management models. Defect
    detection technologies for underground sewage pipes have been extensively explored,
    including closed-circuit television (CCTV), laser-based systems, sonar inspection,
    infra-red thermography, and ultrasonic inspection (Hawari et al., 2018; Liu and
    Kleiner, 2013; Mukherjee et al., 2022). Given that the visual data (i.e., videos
    and images) collected in the sewage pipes can provide abundant and complete information,
    CCTV technology shows the tremendous promise and commercial maturity for pipes
    detection owing to its superiority in straightforward visualization, high detection
    efficiency, low collection cost, and convenient for operation (Wang et al., 2021a).
    CCTV is conducted in real-time by recording the internal conditions of pipes using
    crawling robots coupling with a remote control and a video monitor, after which
    sewage pipe defects are identified by technicians based on the video records (Kumar
    et al., 2020; Meijer et al., 2019). But the accuracy and efficiency of manual
    detection are closely related to the practical experience of operators, working
    conditions and image resolution (Haurum and Moeslund, 2020). The labor-intensive,
    subjective and error-prone manual identification methods have not afforded the
    demands of increasing pipes defect detection and precise hydraulic model correction.
    The rapid development of emerging computer vision technologies provides a significant
    ground to address the limitations of manual identification for pipe defects (Hawari
    et al., 2018). The alliance of image recognition algorithms and CCTV images/videos
    can be applied to rapidly locate sewage pipes defects, classify defect types,
    and quantify defect ratings (Kumar et al., 2018). The conventional computer-assisted
    image recognition technologies including image analysis tools (e.g., edge detection,
    morphology analysis) and machine learning models (e.g., random forest and support
    vector machine) have been attempted to detect sewer pipe defects (Kapelan et al.,
    2019; Myrans et al., 2018; Wang et al., 2021a; Wang et al., 2021b), but the pre-defined
    parameters and designed feature extractors of these methods limited their application
    in complex practical scenarios regarding accuracy and robustness (Fang et al.,
    2022; Li et al., 2021). Deep learning (DL), especially convolutional neural network
    (CNN)-based DL in the past five years has been approved as a qualified successor
    for automated pipe defects detection with more robust and versatile performance,
    in the case of sufficient training data (Dang et al., 2021; Tan et al., 2021;
    Yin et al., 2020). Besides, DL methods can automatically extract multi-dimensional
    features of images without the prior artificial selection of appropriate features
    required in traditional ML algorithms (Chen et al., 2018; Oh et al., 2022), which
    is beneficial to the application of DL in practical engineering. Currently, a
    few of review papers summarized relevant studies such as image-based automated
    detection of sewage pipe defects based on traditional morphology, ML, and DL (Haurum
    and Moeslund, 2020), the broad application introduction of DL in urban water management
    (Fu et al., 2022; Mittal et al., 2022). However, there is still a lack of a practical
    application-oriented systematic review paper in terms of detecting automatedly
    sewage pipe defects and identifying the impacts of pipe defects on urban water
    management models. Considering the state-of-the-art application prospect and research
    hotpots of both CCTV and DL in sewage pipe defect detection (Fig. 1), it is urgent
    to systematically report the updated developments for better achieving urban water
    system management. Therefore, the aims of this paper are to provide a comprehensive
    overview for technical progress of CCTV in pipe detection and DL in image recognition,
    describe the novel practical application-oriented methodological framework of
    automated pipe defects detection, discuss the contribution of sewage pipe defect
    detection to risk evaluation and model calibration in urban water management.
    The review will provide significant references for academic and industrial community
    on past work and future recommendation. Download : Download high-res image (268KB)
    Download : Download full-size image Fig. 1. Knowledge mapping of the keyword co-occurrence
    network on CCTV & DL for pipe defect detection studies. 2. Survey methodology
    The relevant academic studies were searched via Google scholar using the keywords
    “sewer”, “defect”, “CCTV”, “deep learning”, “CNN”, and “urban water management”
    over the past five years to highlight the recent research progress. Only publications
    that were focused specifically on defect detection by CCTV, image recognition
    of pipe defects by deep learning, urban water environment management model considering
    pipe defects have been included. Ultimately, a total of 96 publications were selected
    and discussed according to different detection and recognition tasks, and application
    scenarios. 3. CCTV & DL overview in sewage pipe defect detection 3.1. CCTV for
    pipe defect detection Sewage pipe defect detection technologies fall mainly into
    several broad categories: ultrasound test, sonar, laser, electromagnetic method,
    infrared thermography and visualization method (Li et al., 2019; Liu and Kleiner,
    2013; Zhou et al., 2022b). These technologies exhibited different merits and demerits
    in various detection tasks (Table S1). After more than 40 years of practice proof-test,
    CCTV has been the most commonly used for sewage pipe defects detection by worldwide
    municipal administration departments due to its high security and detection efficiency,
    intuitive and comprehensible output, detailed information records and availability
    in detecting almost all types of pipe defects (Hawari et al., 2018; Yang and Su,
    2009; Yin et al., 2021). CCTV unit is consisted by a rotatable camera and an illumination
    device, both of which are mounted on a robot that is remotely controlled by external
    monitor on the ground (Fig. 2). The CCTV for pipe defect detection typically includes
    two successive steps, i.e., on-site video collection and off-site video assessment.
    During the on-site data collection, CCTV unit is placed inside the sewage pipe
    through the manhole, after which the robot moves along with pipe wall and transmits
    the captured videos to the monitor (Yin et al., 2021). In the case of abnormalities
    or defects, the technician would stop the robot and zoom the camera to acquire
    enlarged video or images. During the off-site video assessment, the main defect
    information (e.g., defects types, the number of each type of defects, severity
    rating) was evaluated by experienced technicians, followed by forming an assessment
    report, deterioration model, and maintenance schedule (Fig. 2) (Wang et al., 2021a).
    Download : Download high-res image (492KB) Download : Download full-size image
    Fig. 2. Flowchart of sewage pipe defect detection by manual assessment and DL-based
    automated assessment based on CCTV videos. The defect types typically were divided
    into two categories: structural defects (e.g., cracks, deformation, corrosion,
    and intrusion) and functional defects (e.g., deposition, obstacles, and scum)
    as shown in Fig. 3. In terms of defect classification and rating, the standards
    were slightly different in different countries and regions such as Pipe Assessment
    and Certification Program (PACP) by the American National Association of Sewer
    Service Companies (NASSCO), Manual of Sewer Condition Classification by Water
    Research Centre (WRc) in the UK, the EN 13508-2 in Europe, and Technical Specification
    for Inspection and Evaluation of Urban Sewer (CJJ181-2012) in China (Wang et al.,
    2021a; Wang et al., 2021b). These standards and rules provided a consistent and
    standardized sewer pipe evaluation method in CCTV interpretation for identical
    regions, which reduced the judgement errors among different inspectors. Nevertheless,
    the subjectivity of inspectors was always unavoidable, which caused uncertainty
    of judgement results. Besides, it is difficult to accurately quantify defects
    grades based on CCTV records by inspectors due to irregular defect shapes and
    the difficulty of manual computation (Hawari et al., 2018). An automated identification
    flowchart of sewage pipe defects for CCTV video is highly necessary (Fig. 2),
    which is being studied and optimized especially with the development of DL-assisted
    image recognition and the rapid improvement of computer hardware power. Download
    : Download high-res image (2MB) Download : Download full-size image Fig. 3. Sewage
    pipe defect types including structural defects and functional defects. 3.2. DL
    technology for image recognition DL models employ the backpropagation algorithm
    to automatically explore its complex structure composed by multiple processing
    layers, which also empower DL the ability to optimize the internal parameters
    and extract features without much data pre-processing (Kumar et al., 2019; LeCun
    et al., 2015; Zhang et al., 2022). Therefore, DL has been explored and applied
    in unsupervised learning such as image, video, and audio recognition (Dang et
    al., 2022; Hakim et al., 2022; Kow et al., 2022). As the main branch of DL, convolutional
    neural networks (CNNs) exhibited significant breakthroughs in image recognition
    (Ahmed et al., 2019; Gu et al., 2018). The CNN algorithm is inspired by the “natural
    visual perception mechanism of living creatures” (Khallaf and Khallaf, 2021; Xu
    and Vaziri-Pashkam, 2021), which consists of a non-linear mapping structure including
    an input layer (i.e., initial image), several convolutional layers and pooling
    layers, fully connected layer(s), and an output layer (Fig. 4a). Each layer has
    its own unique function and the neurons connecting to the neighboring neurons
    in the previous layer with associated weights and threshold (Gu et al., 2018;
    Nhu et al., 2020; Zhu et al., 2021). The convolutional layer aims to generate
    feature maps by extracting input features from raw images, in which the convolutional
    rules coupling with convolutional kernels are applied to form a new matrix (i.e.,
    feature matrix) and pass the results to next layer (Tulbure et al., 2022). The
    pooling layer is located between two convolutional layers and can decrease the
    resolution of feature maps from convolutional layer as shown in Fig. 4a, which
    is also inspired by imitating human visual system of dimensionality reduction
    and abstraction for visual input objects (Gu et al., 2018; Tulbure et al., 2022).
    The first convolutional layer can identify simple features (e.g., edges and lines),
    and the kernels in higher convolutional layer can detect more complex features
    (e.g., objects and shapes). Finally, the comprehensive and critical feature representations
    can be extracted by stacking several convolutional and pooling layers, followed
    by one or more fully-connected layers. The output layer is the last layer of CNNs,
    in which the loss function (e.g., softmax loss and hinge loss) is adopted to complete
    the recognition or classification tasks (Khan et al., 2020; Roy et al., 2020).
    Download : Download high-res image (454KB) Download : Download full-size image
    Fig. 4. Schematic diagram of (a) automated classification of sewage pipe defects
    using typical CNN and (b) feature extraction using CNN. CNN models training is
    a process of minimizing loss function and achieving global optimization, which
    can be improved via activation functions (e.g., sigmoid, tanh, or ReLU), loss
    function, regularization (e.g., Lp-norm or Dropout), and optimization (e.g., weight
    initialization, batch normalization, or shortcut connections) methods (Gu et al.,
    2018; Nair and Hinton, 2010; Tien Bui et al., 2020). Nevertheless, it is noteworthy
    that large datasets for model training are of the preconditions to get accurate
    and robust CNN models (Siu et al., 2022). Furthermore, the CNN algorithm in standard
    format may be out of work for image recognition in high efficiency and accuracy
    at some real scenarios, since partial initial data is multifarious without a fixed
    format (Yin et al., 2020). The improved models based on CNN algorithms have been
    proposed and successfully applied in image recognition such as region-based CNN
    (R-CNN), fast R-CNN, faster R-CNN, You Only Look Once (YOLO), improved U-Net,
    the strengthened region proposal network (Kumar et al., 2020; Li et al., 2021;
    Pan et al., 2020; Tan et al., 2021; Zhou et al., 2022b). The latest research findings
    on improved CNNs for sewage pipe defect detection are discussed in the Section
    5. 4. Methodological framework of automated sewage pipe defect detection The focus
    of this section is on research methods that have been validated on actual sewage
    pipe defect detection projects based on CCTV & DL to provide practitioners with
    realistic experiences. The key modules of methodological architecture in the sewage
    pipe defect detection typically consist of data acquisition (including data augmentation),
    video/image pre-processing, feature extraction, model construction in different
    applications (e.g., defects classification, location identification, and defects
    severity rating evaluation), and evaluation metrics (Chen et al., 2018). These
    key modules were compared and summarized in Table 1 based on the studies in recent
    five years, while detailed processing methods of each module are discussed in
    the following context for application in different scenarios. Table 1. Key modules
    comparison of automated sewage pipe defect detection models based on the studies
    in recent five years. References Data acquisition Dataset (images) Standard Pre-processing
    Algorithm Task Evaluation metrics (Kumar et al., 2018) CCTV 12,000 PACP Random
    flips, Brightness changes, Contrast changes, Motion blur CNN Classification Accuracy
    (86.2 %), Precision (87.7 %), Recall (90.6 %) (Meijer et al., 2019) RapidView
    IBAK Panoramo® pipeline inspection system 2,202,582 EN 13508–2 standard _ CNN
    Classification Specificity (37.2 %), Precision (71.7 %) (Xie et al., 2019) _ 42,800
    _ Normalization, Rotation, Flipping, Color jittering two-level HD-CNN Classification
    Accuracy (94.96 %), Precision (85.13 %), Recall (84.61 %), F1-score (84.86 %)
    (Chen et al., 2018) CCTV 2000 _ Random cropping, PCA, Color enhancement, Rotation
    transformation CNN Detection Recall (88 %), Precision (84 %), Accuracy (85 %),
    ROC curve area (93 %) (Wang and Cheng, 2018) CCTV 3000 _ _ Faster R-CNN Detection
    mAP (83 %), FPS (9.434) (Dang et al., 2018) Robo Cam 5 1050 _ Grayscale, Gaussian
    blurring, Histogram smoothing, Binarization, Morphological transformation MSER
    Detection Accuracy (91 %), Precision (72 %), Recall (93 %) (Hassan et al., 2019)
    Robo Cam 6 47,072 _ Horizontal flip CNN Detection Accuracy (96.33 %) (Li et al.,
    2019) CCTV 18,333 PACP Random horizontal flip, Random cropping, Gaussian blur,
    Contrast normalization, Additive Gaussian noise, Channel scaling CNN Detection
    Accuracy (64.8 %) (Yin et al., 2020) CCTV 4056 (samples) PACP _ YOLOv3 Detection
    F1-score (>0.8), mAP (85.37 %) (Li et al., 2022) CCTV 3888 _ Mixup, Rotation,
    Adding noise, Color jitter Pipe-SOLO Segmentation mAP (59.3 %) (Kumar et al.,
    2020) CCTV 3800 _ _ SSD, YOLOv3, Faster R-CNN Detection Accuracy (54.4 %, 74.5
    %, 76.2 %), Speed (33, 57, 110 ms) (Wang et al., 2021a) CCTV 3600 _ _ Faster R-CNN
    Detection IoU, mAP (77 %), F1-score (57.4 %) (Ma et al., 2021) CCTV 14,451 _ StyleGAN-SDM
    Fusion CNN Detection IRS (2.968 ± 0.024), Accuracy (99.64 %), macro-F1-score (99.64
    %) (Tan et al., 2021) CCTV 1260 _ Horizontal and vertical flipping, Color adjustment
    YOLOv3 Detection mAP (92 %) (Li et al., 2021) CCTV, QV 10,000 _ Contrast enhancement,
    Bilateral filter Two-stage network Detection AP, Recall (82.4 %), mAP (50.8 %)
    (Yin et al., 2021) CCTV 5 (videos) _ _ YOLOv3, VIASP Detection Precision (77 %),
    Recall (73 %), F1 score(75 %) (Dang et al., 2021) Robo Cam 6 38,386 _ Horizontal
    flip, Shear range, Zoom range CNN Detection Accuracy (95.7 %) (Siu et al., 2022)
    CCTV 5302 _ Style transferred synthetic images Faster RCNN Detection AP (23.92
    %) (Guo et al., 2022) Pipeline capsule machine 7000 _ _ Faster R-CNN Detection
    Accuracy (86 %) (Oh et al., 2022) CCTV 4456 _ Flipping, Rotation, Scaling augmentation
    YOLOv5 Detection mAP (75.9 %), Precision (75.8 %), Recall (73.1 %), FPS (31) (Wang
    and Cheng, 2019) CCTV 1880 _ _ DilaSeg-CRF Segmentation mIoU (84.85 %) (Zhou et
    al., 2022a, Zhou et al., 2022b) CCTV 600 _ Random horizontal reflection, Random
    translation DeepLabv3+ Segmentation PA (90 %), mIoU (53 %), fwIoU (84 %), F1-score
    (55 %) (Pan et al., 2020) CCTV 3654 PACP Horizontal flip, Contrast normalization,
    Random cropping PipeUNet Segmentation mIoU (76.37 %), Accuracy (74.94 %), Precision
    (98.89 %), Recall (47.37 %) (Wang et al., 2021b) CCTV 3000 PACP, MSCC, WRc, BS
    EN 13508–1, HKCCEC Contrast enhancement, Grayscale, Noise removal Faster R-CNN,
    CRF Segmentation AP (88.99 %), Recall (87.96 %), F1-score (88.21 %) (Fang et al.,
    2022) sewer floating capsule robot 1744 _ Scale, Rotation, Crop, Resize, Gamma,
    Blur Mask RCNN Segmentation AP (92.7 %), mAP (65.7 %) 4.1. Data acquisition and
    database For CCTV system, the pipe robots loaded with the cameras of consecutive
    rotations are monitored remotely by experienced inspectors to capture videos of
    pipes interiors with high resolutions in varying lighting conditions (Siu et al.,
    2022). The general information (e.g., the diameter and coordinates of pipes, movement
    speed and location of robots, and video subtitles) is also recorded in the CCTV
    videos/images (Fig. 2) (Oh et al., 2022). The direct video records of pipes reflect
    the real situation of sewer pipe system, which are the key source of training
    data in DL model construction. The frames including normal and defective pipes
    are extracted from CCTV videos as database sources, in which image quality of
    each frame largely influences the model performance. To improve image quality
    of captured pipes, some data acquisition systems (e.g., Panoramo) capture images
    with freeze-frame using a strobe light instead of recording videos to reduce the
    influences from shaking when robots move (Meijer et al., 2019). When pipe defects
    are observed during the inspection process of CCTV system, the inspectors always
    stop the robots from moving and zoom the camera to carefully check the defects
    and capture more images for further analysis (Siu et al., 2022). In comparison,
    some experts consider that human intervention like panning, rotating, and zooming
    the camera during images collection should be avoided to design a fully automated
    pipe defect detection system (Meijer et al., 2019), which deservedly requires
    larger amount of data to build DL models in this scenario. Regarding different
    tasks (i.e., classification, detection, and severity rating), the size of the
    datasets for pipe defect detection model construction ranged from hundreds of
    images to millions of images (Table 1). In the practical application, the selection
    of data acquisition schemes should balance the project requirement and budget
    (e.g., CCTV data collection and DL computation costs) (Luo et al., 2018). In case
    of insufficient data volume, the training datasets can be supplemented by creating
    modified copies of the existing data in database (i.e., data augmentation) using
    certain rules, including vertical and horizontal flips, rotation and scaling,
    mirroring, color tuning (e.g., brightness, contrast enhancement and saturation
    adjustment), and Gaussian noise, etc. (Luo et al., 2018; Shorten and Khoshgoftaar,
    2019; Siu et al., 2022). Generative adversarial network (GAN) as an unsupervised
    learning also can be used to generate more natural multi-defects images to extend
    datasets (Ma et al., 2021). Data augmentation has been proved an effective scheme
    to reduce overfitting and improve model accuracy of DL algorithm by increasing
    the size of training sets (Kumar et al., 2018; Tulbure et al., 2022). Besides,
    the DL model showed higher accuracy using the ratio (i.e., ~0.8 %) of defective
    pipes to normal pipes similar to that in a real-world scenario (Meijer et al.,
    2019). A huge size of database by direct acquisition from CCTV is still an important
    prerequisite to continually optimize DL models, because the images generated by
    common data augmentation technologies have high correlation with the original
    images (Ma et al., 2021). However, currently the publicly available sewer defect
    datasets are still rare on a worldwide basis (Chow et al., 2020; Li et al., 2022).
    As the first publicly available sewer defect dataset, Sewer-ML dataset contains
    1.3 million images from 75,618 videos collected from three Danish water utility
    companies (https://vap.aau.dk/sewer-ml/), which has played an important role in
    the development of automated sewer defect detection. Further efforts are still
    needed from governments and relevant departments to gain open-source benchmark
    database or a representative dataset with high quality by merging pipes defects
    videos in different cities or regions, solving the data island problem. 4.2. Image
    pre-processing techniques The images with high resolution and less interference
    information are acknowledged as important conditions for feature extraction and
    recognition in DL algorithms with high speed and accuracy (Zaidi et al., 2022).
    The raw images directly collected by CCTV robots always contain environmental
    noises such as imbalanced brightness and image blurring (Li et al., 2021), which
    can be solved by a variety of image pre-processing techniques. For example, contrast
    enhancement of images via Dynamic Histogram Equalization or RGB grayscale was
    an effective method to deal with imbalanced brightness, which showed good performance
    and efficient computation capacity (Rao, 2020; Wang et al., 2021b). The image
    noises resulting from irregularly illuminated conditions and shading also can
    be weakened and removed by mean filtering, median filtering, bilateral filtering,
    or Gaussian filtering to improve image quality (Hawari et al., 2018; Li et al.,
    2021; Moradi et al., 2020; Su and Yang, 2014; Wang and Cheng, 2019), while a GAN-based
    dehazing model was proved to effectively solve the image blurring problem and
    then increase the detection rate by figuring out the relationship between hazy
    and haze-free images (Li et al., 2022). In addition to the negative effects of
    images, the colorful RGB images extracted from CCTV videos are generally preprocessed
    by converting each color channel to grayscale (Dang et al., 2018; Halfawy and
    Hengmeechai, 2014). The morphological characteristics of images are not reflected
    by RGB color model, in which RGB color space is also difficult to be directly
    recognized by many algorithms (Haurum and Moeslund, 2020). Grayscale can enhance
    the image contrast and highlight target region, meanwhile improve the processing
    speed by reducing information of images (Dang et al., 2018; Moradi et al., 2020).
    The recorded general information in the form of text label in the pipe image was
    also a big interference in automatically defect recognition using DL algorithms
    (Siu et al., 2022). For the labels of white text in black backdrop, an assembled
    multi-process method has been demonstrated valid that label was located with the
    combination of grayscale, thresholding, dilation, followed by obliterating these
    labels with OpenCV (Siu et al., 2022). The sliding window classification and connected
    component analysis were also widely applied in text detection (Dang et al., 2018).
    The pre-processing of CCTV images is an effective section to improve the performance
    of subsequent feature extraction and defect detection, in which the pre-processing
    methods are summarized in Table 1 based on the studies in recent five years (Li
    et al., 2021). 4.3. Feature extraction Feature extraction is the core of image
    recognition by transforming the given input data into a set of features, which
    is expected to be descriptive and non-redundant, simplifying the subsequent learning
    and recognition processes (Suresh Dara, 2018). Key attributes recognition and
    descriptors determination are the two main steps of feature extraction (Chang
    et al., 2017). Traditional vision-based technology can accomplish feature extraction
    based on certain rules such as edge detection, thresholding, and morphology methods
    (Hawari et al., 2018; Su et al., 2011; Yang and Su, 2009), but the developed feature
    extractor was always complex with poor robustness and low generalization ability
    (Li et al., 2021). DL algorithms outperformed the traditional methods by automatically
    learning extracting features just like human brain using multi-layer network structure
    without a specific feature extractor, although accurate DL models were always
    constructed based on given sufficient training datasets of pipe defect images
    (Oh et al., 2022). DL-based feature extraction can extract thousands of optimal
    image features and discern intricate patterns, which contributes to substantial
    improvement of performances in subsequent defect detection (Kumar et al., 2020).
    The deep neural networks can apply data-driven methods to convert the raw input
    data into representative feature space via the combination of linear and non-linear
    operations, which compress irrelevant variables and retain discriminatory information
    to yield abstract descriptors (Feng et al., 2021). The descriptors can be further
    transformed to category labels, prediction maps or bounding boxes according to
    different purposes (e.g., classification, segmentation, or object detection) (Chow
    et al., 2020; Feng et al., 2021). Taking CNN as an example (Fig. 4b), each convolution
    kernel serves as a feature identifier to screen out the features existing in original
    image by dot product, followed by reducing data dimension and producing a matrix
    containing critical information in a pooling layer (Hassan et al., 2019). The
    different convolution kernels coupling with pooling layers can extract different
    features to form a set of features (Luo et al., 2018). A proper increase of layers
    in DL models typically enhance feature extraction ability and generalization capabilities
    of models (Ma et al., 2021), but excessive stacking layers of DL models may also
    cause a worse performance than shallow learning algorithms (Xie et al., 2019).
    To improve feature extraction capability of DL models, advanced methods were also
    attempted such as feature reuse and attention mechanism by comparing the semantic
    differences between high-level features and low-level features, which was proved
    to be effective in the application of automatic sewer pipe defects recognition
    (Pan et al., 2020). 4.4. Model construction During DL model construction, the
    entire dataset is randomly divided into training subset, validation subset, and
    test subset with certain ratios (e.g., 80:10:10, 70:10:20, 85:5:10, 75:10:15,
    etc.) (Guo et al., 2022; Oh et al., 2022; Tan et al., 2021; Yin et al., 2020).
    The ratio in dataset distribution is not fixed in different studies based on total
    data size and different scenarios. Besides, the data balance should be considered
    in each category including the proportion of normal images and defective images
    in each subset, as well as the amount of different types of defects (Guo et al.,
    2022). The images are labeled with corresponding defect names, defect location,
    and severity rating to perform supervised and semi-supervised deep learning. Although
    the application of DL model in image defect detection is in its infancy, DL detection
    framework is being rapidly updated to fit in high flexibility scenarios (Tulbure
    et al., 2022). The successful application of DL algorithms in sewage pipe defect
    classification started from basic CNN, including five steps (Luo et al., 2018;
    Zhu et al., 2021): (i) feature extraction from raw images; (ii) model training
    by feeding the extracted features forward to class prediction using training dataset;
    (iii) loss calculation between predicted and actual values; (iv) model optimization
    according to feedforward error–backpropagation approach; (v) optimal hyper-parameters
    selection (e.g., the number of layers, size of convolution kernels, activation
    function, and learning rate) based on validation dataset and final external validation
    of test data (Kumar et al., 2018). The model construction of improved CNN was
    performed based on that of standard CNN for more functionalities (e.g., simultaneous
    classification and location). Selective search method was applied in R-CNN to
    extract regions from sewage pipe images as the input of CNN, while convolutional
    feature map was used to search regions of interest (ROI) in fast R-CNN for detection
    purposes with higher processing speed (Luo et al., 2018; Yin et al., 2020). The
    Faster R-CNN was further developed based on the fast R-CNN by generating region
    proposals via a region proposal network (RPN), in which both the detection accuracy
    and speed were improved (Wang and Cheng, 2018). The use of deep learning algorithms
    in recent research is summarized in the Table 1. Overall, R-CNN, fast R-CNN, and
    faster R-CNN belong to two-stage object detection approaches, while single-stage
    detection methods such as single-shot detector (SSD) and you only look once (YOLO)
    tend to be even faster (Kumar et al., 2020). 4.5. Evaluation metrics Currently,
    there is no unified standard to assess the DL models of sewage pipe defect detection,
    but accuracy and processing speed are two recognized indicators (Yin et al., 2021).
    The accurate model prediction is the precondition of automatically sewage pipe
    defect video interpretation, while a highly processing speed can empower real-time
    identification (Table 1). The prediction accuracy of defect detection models includes
    true positive (TP), true negative (TN), false positive (FP) and false negative
    (FN), which are used for further calculation of precision and recall (Eq. (1)),
    average precision (AP) and mean average precision (mAP) shown in Eq. (2) (Li et
    al., 2022; Siu et al., 2022). (1) (2) where P(R) is the function of precision
    in terms of recall and N refers to the total number of classes. The processing
    speed can be represented by the number of processed frames per second (FPS). For
    example, the detection speed of latest YOLO architecture (YOLOv5) can be up to
    140 FPS (Oh et al., 2022). It is noteworthy that the potential of the developed
    models in practical application is difficult to be directly compared especially
    for different recognition tasks in spite of using identical evaluation metrices
    such as mAP and FPS. Therefore, there is still an urgent requirement for a standardized
    set of metrics to evaluate the effectiveness of sewage defect recognition models,
    as well as the verification in practical engineering to guide the model evaluation
    criteria. 5. Updates on automated pipe defect detection for scientific urban water
    management Urban water management simulation system is always constructed assuming
    that sewage pipes are healthy without structural or functional defects (Fu et
    al., 2022). In the case of pipe defects, the actual hydraulic performance may
    be underestimated in the aforementioned models resulting in inaccurate flood/wastewater
    prediction (Hong Hanh Nguyen and Venohr, 2021). The latest research findings were
    reviewed in this section including automated sewage pipe defect detection by DL
    technology, risk assessment of pipe failure and model calibration of urban water
    environment management considering pipe defects. 5.1. Automated detection of sewage
    pipe defect via DL 5.1.1. Defect classification Automated defect classification
    based on images extracted from CCTV video is the core of smart sewage pipes defect
    detection management system (Meijer et al., 2019). A number of studies have been
    conducted to develop the automated feature extraction and defect classification
    using end-to-end DL framework. For example, an image recognition framework using
    binary CNNs was constructed by TensorFlow based on 12,000 images extracted from
    CCTV videos of over 200 pipes to classify multiple defects (i.e., root intrusions,
    deposits, and cracks), which overcame the poor generalization capabilities in
    previous proposed automated defect classification approaches, and achieved an
    overall precision and recall of 87.7 % and 90.6 %, respectively (Kumar et al.,
    2018). This developed CNN architecture as open-source framework can be directly
    used by other researchers via transfer learning to be further optimized based
    on new datasets, in which the highest accuracy of improved CNN for sewage pipe
    defect classification was recorded at 96.33 % (Hassan et al., 2019). In transfer
    learning, the extracted features and optimized parameters of CNN structure in
    previous work can be adopted and transferred to a new CNN structure for high accuracy
    and less training time. The test dataset in aforesaid research consisted of over
    50 % defect images, while actual defect ratios are approximately 1 % of images
    among total frames captured by CCTV. To get a pipe defect classifier with great
    performance in real-world scenario, Meijer et al. (2019) reimplemented similar
    CNN framework based on the dataset of over 2.2 million images containing 0.8 %
    of defect images. Combined with ‘leave-two-inspections-out’ cross validation rule,
    the classifier showed the state-of-the-art performance for 12 most common defect
    types especially for intruding and defective connections (Meijer et al., 2019).
    In practical application, the use of automated defect classification system can
    reduce the human labor by ~61 %. Although the above-mentioned datasets reflected
    actual ratio of defect images to normal ones, unbalanced data distribution between
    defects and non-defects can also influence the convergence and generalization
    ability of developed DL models in defect classification due to the biases towards
    majority classes and poor accuracy for minority classes (Li et al., 2019; Xie
    et al., 2019). A two-step methodology was proposed including picking out defect
    images via abnormal recognition and then defect classification on the basis of
    selected defect images, which showed higher accuracy and stronger scene adaptability
    but poor performances for defects with non-significant features (Chen et al.,
    2018). The detection of defects with non-significant features can be improved
    by combining digital image processing algorithms and classification model (Chen
    et al., 2018). Similarly, a hierarchical classification approach was introduced
    into CNN framework at different levels to solve the data imbalance, in which the
    discrimination of images with defects from normal ones and the defect classification
    were performed in high-level and low-level detection task, respectively (Li et
    al., 2019). The CNN-based defect classification framework is still being improved
    by continuedly solving the issues in practical application. For example, the imbalanced
    data among different types of defects were further treated by integrating ensemble-based
    approach and cost-sensitive learning-based method into DL framework, which yielded
    the prediction accuracy at 97.6 % (Dang et al., 2021). For multi-defect classification
    problem, fusion CNN framework was proposed by integrating inception network and
    residual network to improve classification accuracy, which showed a high average
    accuracy of 95.64 % in terms of the detection for misalignment, corrosion, leakage
    and obstacle defects (Ma et al., 2021). 5.1.2. Defect detection In addition to
    defect classification, the location recognition for different defects in CCTV
    images is also critical information to achieve full automation of evaluating sewage
    pipe conditions (Hassan et al., 2019; Li et al., 2022). The standard sewer detection
    reports should contain the longitudinal position of defects along the pipe and
    circumferential position in the images according to PACP in US, WRc in UK, EN
    13508-2 in Europe, and CJJ181-2012 in China. The longitudinal position of pipe
    defects can be captured by text recognition algorithms based on the text records
    on CCTV video frames, while the circumferential location of defects in images
    is more complicated (Kumar et al., 2020). The steps of text detection and recognition
    include locating text lines by applying multi-frame averaging and reducing the
    background complexity, enhancing text quality by removing noises using RGB grayscale
    and Gaussian blurring, and final recognition by optical character recognition
    (OCR) and template matching, which have been engineeringly validated especially
    for clear defect images (Dang et al., 2018). In the case of defect images with
    poor resolution or complicated background, the method combination (i.e., multi-frame
    integration, processing, and maximally stable extremal regions) showed higher
    accuracy for text detection and recognition by extracting text and non-text components
    (Hassan et al., 2019). The simultaneous recognition for circumferential location
    and classification of defects is defined as defect detection, which can be achieved
    by reference to the framework of object detection algorithms (Murthy et al., 2020).
    Faster R-CNN as a typical two-stage object detector has been successfully applied
    in the construction of automated sewer defect detection system, in which four
    categories of defects (i.e., root intrusions, cracks, infiltrations and deposits)
    were focused based on 3000 images with a mAP of 83 % (Luo et al., 2018). Besides,
    this pipe defect detection system still showed promising when multiple defects
    were contained in the same image, which can be further improved by using a larger
    size of dataset under different environments or modifying hyper-parameters of
    models (Luo et al., 2018). SSD and YOLO as one-stage object detectors were also
    attempted for automated detection of sewer defects, which were proved to be significantly
    faster but less accurate than faster R-CNN (Kumar et al., 2020). The SSD architecture
    was proposed based on a feedforward CNN to indicate the defect position with the
    concept of default anchor boxes, followed by selecting bonding box predictors
    for each defect using non-maximum suppression technology (Liu et al., 2016). In
    the detection of root intrusions and deposits, the accuracy of YOLOv3 framework
    (i.e., a mAP of 74.5 %) was proved to be slightly lower than that of faster R-CNN
    (i.e., a mAP of 69.5 %), while the processing speed of YOLOv3 was twice as fast
    as the Faster R-CNN (Kumar et al., 2020). In another work, YOLOv3 network was
    also employed to build automated defect detection system of sewer pipes based
    on the dataset composed by 3664 images containing six types of 4056 unique defects
    (i.e., broken, hole, deposits, crack, fracture, and root), in which CCTV video
    was directly used as input data instead of images resulting in higher accuracy
    (i.e., a mAP of 85.37 %) and high processing speed with 33 FPS (Yin et al., 2020).
    YOLOv3 as the advanced real-time object detection algorithm coupling with graphical
    processing units (GPUs) showed great potential for sewer pipe-inspection real
    time detection platforms (Kumar et al., 2020). In addition, defect tracking is
    critical to implement automated and rapid defect detection in practical engineering,
    because conventional defect detection methods cannot determine whether the same
    defects are repeatedly detected across consecutive video frames (Wang et al.,
    2021a). A framework containing sewage pipe defects tracking was proposed by assigning
    a unique identification (ID) number to each defect in the CCTV video followed
    by incorporating defect classification into object tracking algorithm, in which
    the experimental results demonstrated the effectiveness of the framework in tracking
    multiple defects (Wang et al., 2021a). The sewage pipe defects detection will
    be more automated and smarter with the development of DL algorithms and the accumulation
    of practical data. 5.1.3. Severity rating evaluation of defects Tracking defects
    by ID labels are conducive to defect detection across a consecutive CCTV video,
    which also assists overall evaluating pipe conditions by counting the number of
    each type of defect in the video (Wang et al., 2021a). In addition, grading the
    defect severity also depended on both defect types and defect characteristics
    such as relative location, defect area (i.e., the ratio of defect area to pipe
    cross section area), shape, orientation of defects, etc. (Wang et al., 2021b).
    The severity rating evaluation of defects can integrate all the tasks (i.e., defect
    classification, location, and characteristics) in one system to fulfill the requirement
    of whole process required in sewage pipe inspection. The image recognition of
    defect types by classification, longitudinal and circumferential location of defects
    using various CNN-based frameworks (e.g., Faster R-CNN), and number by tracking
    algorithm have been introduced in the previous section. The calculation of defect
    area of sewage pipe has higher level of complexity, which has been attempted by
    a semantic segmentation model integrating CNN and conditional random field (Wang
    and Cheng, 2019; Wang et al., 2021b). The semantic segmentation technology can
    recognize the boundary and shapes of pipe defects, which is the essential step
    to achieve severity rating evaluation of defects (Pan et al., 2020). The automated
    severity rating evaluation of sewage pipe in virtue of image recognition is a
    more challenging work, since defects of the same category but at different grades
    have similar features (Li et al., 2021). Especially the defects with low severity
    rating tend to be blurred by the convolution and pooling with the increase of
    network layers in CNN-based framework. Classical object detection architectures
    are appropriate to defect classification and detection without directly severity
    rating evaluation, therefore different improved framework is being attempted to
    handle the defects severity rating. For example, Li et al. (2021) proposed a novel
    learning-based framework based on two-stage object detection network to simultaneously
    conduct defect detection and fine-grained classification by multi-layer global
    feature fusion techniques, in which a strengthened RPN was designed to improve
    the robustness and classified accuracy for small defects. The pixel-level semantic
    segmentation methods (e.g., SegNet, fully convolutional network, and U-Net) were
    proved to be feasible to achieve automatically severity quantification of sewage
    defects such as the cracks, disjoints, obstacles, residential walls and tree roots
    (Zhou et al., 2022a). The segmentation tasks can be implemented by a CNN model
    with an encoder-decoder structure. The severity rating of sewage pipe defect is
    definitely a comprehensive indictor, which still needs significant improvement
    regarding the model accuracy and processing speed in the future research for better
    service for decision-making in sewage pipe maintenance and water environment management.
    5.2. Sewage pipe defect detection for urban water environment management Sewage
    pipe defects bring a big challenge to urban water environment management, because
    urban river water or groundwater pollution has been found closely related to defects
    of stormwater and sewage pipes (Xu et al., 2019). Academic and industrial community
    have paid attentions to the risk assessment of pipe failure and hydrodynamic model
    calibration by considering pipe defects for more scientific urban water environment
    management. 5.2.1. Risk assessment of pipe failure in water environment management
    Risk assessment of pipe failure is an important tool to identify critical segment
    that is prioritized to be rehabilitated in sewage pipe management (Ramos-Salgado
    et al., 2022). As the function of probability and consequence, risk assessment
    of pipe failure in water environment management can be conducted by two consecutive
    steps, i.e., individual pipe condition evaluation by sewage pipe deterioration
    model, followed by impact magnitude of a pipe segment break using consequence
    of failure (CoF) model (Baah et al., 2015). The sewage pipe deterioration model
    as the first step of risk assessment can be classified into physical model, statistical
    inference and artificial intelligence (AI)-based model, which are essentially
    deterministic, probabilistic, and computer vision-based models, respectively (Barton
    et al., 2019; Dawood et al., 2020; Wilson et al., 2015). CCTV data is the important
    reference material to build pipe deterioration model, while AI-based defect detection
    models have presented superiority in automated pipe defect or deterioration detection
    as discussed in the Section 4.1 (Hawari et al., 2020). According to the accurate
    defect severity rating evaluation model developed by DL&CCTV, severity scores
    of each defect were integrated to form a comprehensive internal condition grade
    (ICG) of the entire pipes (Baah et al., 2015). The impacts of pipe failure were
    subsequently assessed using CoF model, which is defined by quantifying economic
    impacts, social impacts, environmental impacts, commercial impacts, and traffic
    impacts, etc. (Vishwakarma and Sinha, 2023). The weighted summation approach (WSA)
    is the fundamental principle of CoF model, in which WSA is accomplished by assigning
    performance values/scores to a list of impact factors (e.g., pipe depth and size,
    distance to building, and etc.) with certain weights and makes the final calculations
    (Baah et al., 2015; Ramos-Salgado et al., 2022). For example, the priority of
    pipe replacement in a network was judged by combining WSA and multi-criteria (i.e.,
    probability of pipe failure, supply pipe leakage flow, served/non-served demand
    for supply pipes, maximum evacuation flow of sewage pipes, and pipe connectivity),
    which has been successfully applied in a large water company in Spain (Muñuzuri
    et al., 2020). The risk of pipe failure is calculated by a risk matrix system
    according to ICG and COF model, in which the ICG developed by DL&CCTV can significantly
    improve the accuracy of risk assessment than that by subjective judgement (Kaddoura
    et al., 2018). The existing capital asset inventory of cities in ArcGIS can also
    be used as assessment criteria for CoF calculation in risk assessment of pipe
    failure (Baah et al., 2015). However, the inherent subjectivity of CoF models
    using WSA was inevitable in the trade-offs of decision maker or risk assessor,
    which can be remedied by sensitivity analysis or combining other methods such
    as Bayesian networks model, fuzzy logic approach (Jensen and Jerez, 2018; Wu et
    al., 2020). Besides, machine learning and sensitivity analysis can be combined
    to identify the contribution of pipe characteristics (e.g., geometric properties,
    corrosion pit dimensions, pipe age, and material category) to pipe failure probability
    (Mazumder et al., 2021). The novel risk assessment framework of pipe failure was
    also being developed such as a combination of geospatial information system, analytic
    hierarchy process, and data envelopment analysis for quantitatively CoF assessment
    (Ghavami et al., 2020). Taking water supply network in a Spanish city as an example,
    ~30 % of failures can be avoided by merely replacing 3 % of network''s pipes with
    defects (Robles-Velasco et al., 2020). Currently, there is still lack of a more
    scientific framework combining the automated sewage pipe defects detection and
    risk assessment of pipe with defects in water environment management, which should
    be continuedly developed and optimized in an application-oriented manner. 5.2.2.
    Urban water environment management model calibration considering pipe defects
    The performance of pipe network such as pluvial flooding and wastewater emissions
    was usually evaluated by hydrodynamic models, which were always constructed using
    the parameters of healthy pipes (Eggimann et al., 2017). The impacts of sewage
    pipe defects on the model accuracy are recently being paid high attentions due
    to frequent pipe defects. The substantial impacts of pipe defects (e.g., root
    intrusion, surface damage, attached and settled deposits) on pluvial flooding
    have been demonstrated in terms of return period of flooding, number of flooded
    locations, and flooded volumes, which were conducted using Monte Carlo simulations
    and hydrodynamic models shown in Fig. 5 (Van Bijnen et al., 2012). In more details,
    the sediment depth as pipe defects was found to significantly reduce the hydraulic
    performance of pipes and increase frequencies of flooding occurrence (Liu et al.,
    2023; Okwori et al., 2021; Van Bijnen et al., 2018). Download : Download high-res
    image (246KB) Download : Download full-size image Fig. 5. Impact investigation
    of sewage pipe defects on urban water environment models using Monte Carol simulation.
    After identifying the impacts of sewage pipe defects on hydraulic performance
    of sewer system, hydraulic model calibration was attempted for more scientific
    and accurate water environment management. Taking sewer exfiltration model as
    an example, the sewer exfiltration rate is significantly governed by the pipe
    defect conditions, including defect types, defect area, etc.(Kaddoura and Zayed,
    2018). The sewer exfiltration rates caused by cracks were reported to be three
    times than that by open joints and holes, while the exfiltration rate is found
    to be positively correlated with sewage pipe defect area (Nguyen et al., 2021).
    The models of sewer exfiltration to groundwater due to a defective sewer system
    can refer to the latest review publication (Hong Hanh Nguyen and Venohr, 2021).
    Similarly, stormwater pipe leakage was investigated for dry-weather flow conditions
    and rainfall conditions, which suggested that the pipe defect size was closely
    related to groundwater infiltration or stormwater exfiltration (Peche et al.,
    2019). The accurate evaluation of pipe defect is the prerequisite to accomplish
    hydraulic model calibration. Recently, the concept of hydraulic fingerprinting
    (i.e., the relationship between model parameters and calibrated residuals) was
    introduced into model calibration by observing the changes in the hydraulic properties
    based on a chosen calibration parameter set (e.g., runoff parameters) in the case
    of pipe defects (e.g. root intrusions and sediment deposits). The pipe defects
    resulted in high local hydraulic losses and exceeded the default hydraulic roughness,
    which suggested that the performance of sewer system would be overestimated using
    default values of hydraulic roughness. The change of hydraulic properties of sewer
    system was identified for the references of model calibration to optimize sewer
    asset and urban water management (Van Bijnen et al., 2016). Nevertheless, the
    integration of pipe defect detection system and urban water environment management
    model calibration currently still calls for further research efforts. 6. Challenges
    and prospects The breakthroughs in computer hardware and DL algorithms provide
    a fantastic opportunity for the development of automatedly sewage pipe defect
    detection. DL especially CNN has been demonstrated to empower the automated defect
    classification, defect detection and severity rating evaluation of sewer pipe.
    Nevertheless, currently the application of DL in sewage pipe defect detection
    has been still limited compared with those in other fields (e.g. medical domain)
    due to the lack of public CCTV image/video datasets of sewer pipe defects. The
    huge and public datasets will contribute to a well-developed defect detection
    models with high accuracy and generalization capacity. Besides, researchers always
    developed defect detection models based on the local datasets that suffered from
    non-open source and respective guideline standards, which increased the difficulty
    to build comprehensive and reproducible automated detection system. Technically,
    transferring learning or active learning can be attempted to deal with non-universal
    problems of developed models based on specific codes and datasets by transferring
    the extracted features and optimized parameters of previous CNN structure to a
    new CNN structure. Therefore, it is urgent to collaboratively organize the construction
    of open-source database in sewage pipe defects videos by governments and departments
    around the world. Most of current relevant research focused on the CCTV images
    interpretation, but the information delivered directly from video format was more
    valuable because of containing the contextual information. Future research should
    focus on the systematic construction of real-time video interpretation and overall
    infrastructure condition evaluation via CNN-based deep learning (e.g., the breakthrough
    in CCTV video interpretation considering contextual information, the improvement
    of detection speed, and dynamic coupling between defect detection models and urban
    water management models). The complexity of sewer system, the variability in the
    appearance of defects and the requirements for continuous monitoring for pipe
    system also bring big challenges to the development of automated sewage pipe defect
    detection system. From the perspective of model application, there are still pending
    issues such as the accuracy of multi-defects detection in identical images, the
    interference from the grains of pipe material to defects assessment, the effective
    integration of automated sewage pipe defect detection and urban water management
    in practical engineering. The issues require close cooperation among computer
    scientists, data engineers and civil engineers from the improvement of image pre-processing
    approaches, the coupling of algorithms with different functions, stepwise model
    construction, and the combination of deep learning algorithms and basic knowledge
    of pipeline engineering. The following is the supplementary data related to this
    article. Download : Download Word document (17KB) Table S1. Summary of the performance
    of defect detection technologies. CRediT authorship contribution statement Lianpeng
    Sun: Data analysis, Writing - original draft, Funding acquisition. Jinjun Zhu:
    Data analysis, Writing - original draft. Jinxin Tan: Data curation, Investigation.
    Xianfeng Li: Review & editing. Ruohong Li, Huanzhong Deng: Data analysis, Review.
    Xinyang Zhang, Bingyou Liu: Resources; Software. Xinzhe Zhu: Conceptualization,
    Review & editing, Supervision, Project administration, Funding acquisition. Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgement This work was supported by the National
    Key Research and Development Program of China (No. 2019YFE0111400), the Science
    and Technology Development Fund of Macau (No. 0079/2019/AMJ), and the National
    Natural Science Foundation of China (No. 52200112). Data availability Data will
    be made available on request. References Ahmed et al., 2019 N. Ahmed, M.N. Islam,
    A.S. Tuba, M.R.C. Mahdy, M. Sujauddin Solving visual pollution with deep learning:
    a new nexus in environmental management J. Environ. Manag., 248 (2019), Article
    109253 View PDFView articleView in ScopusGoogle Scholar Baah et al., 2015 K. Baah,
    B. Dubey, R. Harvey, E. McBean A risk-based approach to sanitary sewer pipe asset
    management Sci. Total Environ., 505 (2015), pp. 1011-1017 View PDFView articleView
    in ScopusGoogle Scholar Barton et al., 2019 N.A. Barton, T.S. Farewell, S.H. Hallett,
    T.F. Acland Improving pipe failure predictions: factors affecting pipe failure
    in drinking water networks Water Res., 164 (2019), Article 114926 View PDFView
    articleView in ScopusGoogle Scholar Chang et al., 2017 N.B. Chang, K. Bai, C.F.
    Chen Integrating multisensor satellite data merging and image reconstruction in
    support of machine learning for better water quality management J. Environ. Manag.,
    201 (2017), pp. 227-240 View PDFView articleView in ScopusGoogle Scholar Chen
    et al., 2018 K. Chen, H. Hu, C. Chen, L. Chen, C. He An Intelligent Sewer Defect
    Detection Method Based on Convolutional Neural Network. IEEE International Conference
    on Information and Automation (ICIA) (2018), pp. 1301-1306 CrossRefView in ScopusGoogle
    Scholar Chow et al., 2020 J.K. Chow, Z. Su, J. Wu, Z. Li, P.S. Tan, K.-F. Liu,
    X. Mao, Y.-H. Wang Artificial intelligence-empowered pipeline for image-based
    inspection of concrete structures Automat. Constr., 120 (2020), Article 103372
    View PDFView articleView in ScopusGoogle Scholar Dang et al., 2018 L.M. Dang,
    S.I. Hassan, S. Im, I. Mehmood, H. Moon Utilizing text recognition for the defects
    extraction in sewers CCTV inspection videos Comput. Ind., 99 (2018), pp. 96-109
    View PDFView articleView in ScopusGoogle Scholar Dang et al., 2021 L.M. Dang,
    S. Kyeong, Y. Li, H. Wang, T.N. Nguyen, H. Moon Deep learning-based sewer defect
    classification for highly imbalanced dataset Comput. Ind. Eng., 161 (2021), Article
    107630 View PDFView articleView in ScopusGoogle Scholar Dang et al., 2022 K.B.
    Dang, V.B. Dang, V.L. Ngo, K.C. Vu, H. Nguyen, D.A. Nguyen, T.D.L. Nguyen, T.P.N.
    Pham, T.L. Giang, H.D. Nguyen, T. Hieu Do Application of deep learning models
    to detect coastlines and shorelines J. Environ. Manag., 320 (2022), Article 115732
    View PDFView articleView in ScopusGoogle Scholar Dawood et al., 2020 T. Dawood,
    E. Elwakil, H.M. Novoa, J.F.G. Delgado Artificial intelligence for the modeling
    of water pipes deterioration mechanisms Automat. Constr., 120 (2020), Article
    103398 View PDFView articleView in ScopusGoogle Scholar Eggimann et al., 2017
    S. Eggimann, L. Mutzner, O. Wani, M.Y. Schneider, D. Spuhler, M. Moy de Vitry,
    P. Beutler, M. Maurer The potential of knowing more: a review of data-driven urban
    water management Environ Sci Technol., 51 (5) (2017), pp. 2538-2553 CrossRefView
    in ScopusGoogle Scholar Fang et al., 2022 X. Fang, Q. Li, J. Zhu, Z. Chen, D.
    Zhang, K. Wu, K. Ding, Q. Li Sewer defect instance segmentation, localization,
    and 3D reconstruction for sewer floating capsule robots Automat. Constr., 142
    (2022), Article 104494 View PDFView articleView in ScopusGoogle Scholar Feng et
    al., 2021 J. Feng, Y. Yao, S. Lu, Y. Liu Domain knowledge-based deep-broad learning
    framework for fault diagnosis IEEE Trans. Ind. Electron., 68 (4) (2021), pp. 3454-3464
    CrossRefView in ScopusGoogle Scholar Fu et al., 2022 G. Fu, Y. Jin, S. Sun, Z.
    Yuan, D. Butler The role of deep learning in urban water management: a critical
    review Water Res., 223 (2022), Article 118973 View PDFView articleView in ScopusGoogle
    Scholar Ghavami et al., 2020 S.M. Ghavami, Z. Borzooei, J. Maleki An effective
    approach for assessing risk of failure in urban sewer pipelines using a combination
    of GIS and AHP-DEA Process Saf. Environ. Protect., 133 (2020), pp. 275-285 View
    PDFView articleView in ScopusGoogle Scholar Gu et al., 2018 J. Gu, Z. Wang, J.
    Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, G. Wang, J. Cai, T. Chen
    Recent advances in convolutional neural networks Pattern Recogn., 77 (2018), pp.
    354-377 View PDFView articleView in ScopusGoogle Scholar Guo et al., 2022 W. Guo,
    X. Zhang, D. Zhang, Z. Chen, B. Zhou, D. Huang, Q. Li Detection and classification
    of pipe defects based on pipe-extended feature pyramid network Automat. Constr.,
    141 (2022), Article 104399 View PDFView articleView in ScopusGoogle Scholar Hakim
    et al., 2022 W.L. Hakim, F. Rezaie, A.S. Nur, M. Panahi, K. Khosravi, C.W. Lee,
    S. Lee Convolutional neural network (CNN) with metaheuristic optimization algorithms
    for landslide susceptibility mapping in icheonSouth Korea J. Environ. Manage,
    305 (2022), Article 114367 View PDFView articleView in ScopusGoogle Scholar Halfawy
    and Hengmeechai, 2014 M.R. Halfawy, J. Hengmeechai Efficient algorithm for crack
    detection in sewer images from closed-circuit television inspections J. Infrastruct.
    Syst., 20 (2) (2014), pp. 1-12 View in ScopusGoogle Scholar Harpaz et al., 2022
    C. Harpaz, S. Russo, J.P. Leitão, R. Penn Potential of supervised machine learning
    algorithms for estimating the impact of water efficient scenarios on solids accumulation
    in sewers Water Res., 216 (2022), Article 118247 View PDFView articleView in ScopusGoogle
    Scholar Hassan et al., 2019 S.I. Hassan, L.M. Dang, I. Mehmood, S. Im, C. Choi,
    J. Kang, Y.-S. Park, H. Moon Underground sewer pipe condition assessment based
    on convolutional neural networks Automat. Constr., 106 (2019), Article 102849
    View PDFView articleView in ScopusGoogle Scholar Haurum and Moeslund, 2020 J.B.
    Haurum, T.B. Moeslund A survey on image-based automation of CCTV and SSET sewer
    inspections Automat. Constr., 111 (2020), Article 103061 View PDFView articleView
    in ScopusGoogle Scholar Hawari et al., 2018 A. Hawari, M. Alamin, F. Alkadour,
    M. Elmasry, T. Zayed Automated defect detection tool for closed circuit television
    (cctv) inspected sewer pipelines Automat. Constr., 89 (2018), pp. 99-109 View
    PDFView articleView in ScopusGoogle Scholar Hawari et al., 2020 A. Hawari, F.
    Alkadour, M. Elmasry, T. Zayed A state of the art review on condition assessment
    models developed for sewer pipelines Eng. Appl. Artif. Intell., 93 (2020), Article
    103721 View PDFView articleView in ScopusGoogle Scholar Hong Hanh Nguyen and Venohr,
    2021 A.P. Hong Hanh Nguyen, Markus Venohr Modelling of sewer exfiltration to groundwater
    in urban wastewater systems: a critical review J. Hydrol., 596 (2021), p. 126130
    Google Scholar Jensen and Jerez, 2018 H.A. Jensen, D.J. Jerez A stochastic framework
    for reliability and sensitivity analysis of large scale water distribution networks
    Reliab. Eng. Syst. Saf., 176 (2018), pp. 80-92 View PDFView articleView in ScopusGoogle
    Scholar Kaddoura and Zayed, 2018 K. Kaddoura, T. Zayed An integrated assessment
    approach to prevent risk of sewer exfiltration Sust. Cities Soc., 41 (2018), pp.
    576-586 View PDFView articleView in ScopusGoogle Scholar Kaddoura et al., 2018
    K. Kaddoura, T. Zayed, A.H. Hawari Multiattribute utility theory deployment in
    sewer defects assessment J. Comput. Civ. Eng., 32 (2) (2018), p. 04017074 CrossRefView
    in ScopusGoogle Scholar Kapelan et al., 2019 Z. Kapelan, R. Everson, J. Myrans
    Automated detection of fault types in CCTV sewer surveys J. Hydroinform., 21 (1)
    (2019), pp. 153-163 Google Scholar Karn et al., 2021 A.L. Karn, S. Pandya, A.
    Mehbodniya, F. Arslan, D.K. Sharma, K. Phasinam, M.N. Aftab, R. Rajan, R.K. Bommisetti,
    S.J.S.C. Sengan An integrated approach for sustainable development of wastewater
    treatment and management system using IoT in smart cities Soft. Comput. (2021),
    pp. 1-17 Google Scholar Khallaf and Khallaf, 2021 R. Khallaf, M. Khallaf Classification
    and analysis of deep learning applications in construction: a systematic literature
    review Automat. Constr., 129 (2021), Article 103760 View PDFView articleView in
    ScopusGoogle Scholar Khan et al., 2020 A. Khan, A. Sohail, U. Zahoora, A. Qureshi
    A survey of the recent architectures of deep convolutional neural networks Artif.
    Intell. Rev., 53 (8) (2020), pp. 5455-5516 CrossRefView in ScopusGoogle Scholar
    Kow et al., 2022 P.Y. Kow, I.W. Hsia, L.C. Chang, F.J. Chang Real-time image-based
    air quality estimation by deep learning neural networks J. Environ. Manag., 307
    (2022), Article 114560 View PDFView articleView in ScopusGoogle Scholar Kumar
    et al., 2018 S.S. Kumar, D.M. Abraham, M.R. Jahanshahi, T. Iseley, J. Starr Automated
    defect classification in sewer closed circuit television inspections using deep
    convolutional neural networks Automat. Constr., 91 (2018), pp. 273-283 View PDFView
    articleCrossRefView in ScopusGoogle Scholar Kumar et al., 2019 D. Kumar, A. Singh,
    P. Samui, R.K. Jha Forecasting monthly precipitation using sequential modelling
    Hydrol. Sci. J., 64 (6) (2019), pp. 690-700 CrossRefView in ScopusGoogle Scholar
    Kumar et al., 2020 S.S. Kumar, M. Wang, D.M. Abraham, M.R. Jahanshahi, T. Iseley,
    J.C.P. Cheng Deep learning-based automated detection of sewer defects in CCTV
    videos J. Comput. Civ. Eng., 34 (1) (2020), p. 04019047 View in ScopusGoogle Scholar
    LeCun et al., 2015 Y. LeCun, Y. Bengio, G.J.N. Hinton Deep learning 521 (7553)
    (2015), pp. 436-444 CrossRefView in ScopusGoogle Scholar Li et al., 2019 D. Li,
    A. Cong, S. Guo Sewer damage detection from imbalanced CCTV inspection data using
    deep convolutional neural networks with hierarchical classification Automat. Constr.,
    101 (2019), pp. 199-208 View PDFView articleGoogle Scholar Li et al., 2021 D.
    Li, Q. Xie, Z. Yu, Q. Wu, J. Zhou, J. Wang Sewer pipe defect detection via deep
    learning with local and global feature fusion Automat. Constr., 129 (2021), Article
    103823 View PDFView articleView in ScopusGoogle Scholar Li et al., 2022 Y. Li,
    H. Wang, L.M. Dang, M. Jalil Piran, H. Moon A robust instance segmentation framework
    for underground sewer defect detection Measurement, 190 (2022), Article 110727
    View PDFView articleView in ScopusGoogle Scholar Liu and Kleiner, 2013 Z. Liu,
    Y.J.M. Kleiner State of the art review of inspection technologies for condition
    assessment of water pipes Measurement, 46 (1) (2013), pp. 1-15 View PDFView articleGoogle
    Scholar Liu et al., 2016 W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
    Fu, A.C. Berg SSD: single shot multibox detector European Conference on Computer
    Vision, Springer (2016), pp. 21-37 View in ScopusGoogle Scholar Liu et al., 2023
    W. Liu, Y. He, Z. Liu, H. Luo, T. Liu A bilevel data-driven method for sewer deposit
    prediction under uncertainty Water Res., 231 (2023), Article 119588 View PDFView
    articleView in ScopusGoogle Scholar Lund et al., 2019 N.S.V. Lund, M. Borup, H.
    Madsen, O. Mark, K. Arnbjerg-Nielsen, P.S. Mikkelsen Integrated stormwater inflow
    control for sewers and green structures in urban landscapes Nat. Sustain., 2 (11)
    (2019), pp. 1003-1010 CrossRefView in ScopusGoogle Scholar Luo et al., 2018 L.
    Luo, T. Chen, Z. Li, Z. Zhang, W. Zhao, M. Fan Heteroatom self-doped activated
    biocarbons from fir bark and their excellent performance for carbon dioxide adsorption
    J. CO2 Util., 25 (2018), pp. 89-98 View PDFView articleView in ScopusGoogle Scholar
    Ma et al., 2021 D. Ma, J. Liu, H. Fang, N. Wang, C. Zhang, Z. Li, J. Dong A multi-defect
    detection system for sewer pipelines based on StyleGAN-SDM and fusion CNN Constr.
    Build. Mater., 312 (2021), Article 125385 View PDFView articleView in ScopusGoogle
    Scholar Mazumder et al., 2021 R.K. Mazumder, A.M. Salman, Y. Li Failure risk analysis
    of pipelines using data-driven machine learning algorithms Struct. Saf., 89 (2021),
    Article 102047 View PDFView articleView in ScopusGoogle Scholar Meijer et al.,
    2019 D. Meijer, L. Scholten, F. Clemens, A. Knobbe A defect classification methodology
    for sewer image sets with convolutional neural networks Automat. Constr., 104
    (2019), pp. 281-298 View PDFView articleView in ScopusGoogle Scholar Mittal et
    al., 2022 A. Mittal, L. Scholten, Z. Kapelan A review of serious games for urban
    water management decisions: current gaps and future research directions Water
    Res., 215 (2022), Article 118217 View PDFView articleView in ScopusGoogle Scholar
    Moradi et al., 2020 S. Moradi, T. Zayed, F. Nasiri, F. Golkhoo Automated anomaly
    detection and localization in sewer inspection videos using proportional data
    modeling and deep learning-based text recognition J. Infrastruct. Syst., 26 (3)
    (2020), p. 04020018 View in ScopusGoogle Scholar Mugume et al., 2015 S.N. Mugume,
    D.E. Gomez, G. Fu, R. Farmani, D. Butler A global analysis approach for investigating
    structural resilience in urban drainage systems Water Res., 81 (2015), pp. 15-26
    View PDFView articleView in ScopusGoogle Scholar Mukherjee et al., 2022 S. Mukherjee,
    R. Zhang, M. Alzuhiri, V.V. Rao, L. Udpa, Y. Deng Inline pipeline inspection using
    hybrid deep learning aided endoscopic laser profiling J. Nondestruct Eval., 41
    (3) (2022), pp. 1-13 56 Google Scholar Muñuzuri et al., 2020 J. Muñuzuri, C. Ramos,
    A. Vázquez, L. Onieva Use of discrete choice to calibrate a combined distribution
    and sewer pipe replacement model Urban Water J., 17 (2) (2020), pp. 100-108 CrossRefView
    in ScopusGoogle Scholar Murthy et al., 2020 C.B. Murthy, M.F. Hashmi, N.D. Bokde,
    Z.W. Geem Investigations of object detection in Images/Videos using various deep
    learning techniques and embedded Platforms—A comprehensive review Appl. Sci.,
    10 (9) (2020), p. 3280 CrossRefView in ScopusGoogle Scholar Myrans et al., 2018
    J. Myrans, R. Everson, Z. Kapelan Automated detection of faults in sewers using
    CCTV image sequences Automat. Constr., 95 (2018), pp. 64-71 View PDFView articleView
    in ScopusGoogle Scholar Nair and Hinton, 2010 V. Nair, G.E. Hinton Rectified Linear
    Units Improve Restricted Boltzmann Machines (2010) Google Scholar Nguyen et al.,
    2021 H.H. Nguyen, A. Peche, M. Venohr Modelling of sewer exfiltration to groundwater
    in urban wastewater systems: a critical review J. Hydrol., 596 (2021), Article
    126130 View PDFView articleView in ScopusGoogle Scholar Nhu et al., 2020 V.-H.
    Nhu, N.-D. Hoang, H. Nguyen, P.T.T. Ngo, T. Thanh Bui, P.V. Hoa, P. Samui, D.
    Tien Bui Effectiveness assessment of keras based deep learning with different
    robust optimization algorithms for shallow landslide susceptibility mapping at
    tropical area Catena, 188 (2020), Article 104458 View PDFView articleView in ScopusGoogle
    Scholar Oh et al., 2022 C. Oh, L.M. Dang, D. Han, H. Moon Robust sewer defect
    detection with text analysis based on deep learning IEEE Access, 10 (2022), pp.
    46224-46237 CrossRefView in ScopusGoogle Scholar Okwori et al., 2021 E. Okwori,
    M. Viklander, A. Hedström Spatial heterogeneity assessment of factors affecting
    sewer pipe blockages and predictions Water Res., 194 (2021), Article 116934 View
    PDFView articleView in ScopusGoogle Scholar Pan et al., 2020 G. Pan, Y. Zheng,
    S. Guo, Y. Lv Automatic sewer pipe defect semantic segmentation based on improved
    U-net Automat. Constr., 119 (2020), Article 103383 View PDFView articleView in
    ScopusGoogle Scholar Peche et al., 2019 A. Peche, T. Graf, L. Fuchs, I. Neuweiler
    Physically based modeling of stormwater pipe leakage in an urban catchment J.
    Hydrol., 573 (2019), pp. 778-793 View PDFView articleView in ScopusGoogle Scholar
    Ramos-Salgado et al., 2022 C. Ramos-Salgado, J. Muñuzuri, P. Aparicio-Ruiz, L.
    Onieva A comprehensive framework to efficiently plan short and long-term investments
    in water supply and sewer networks Reliab. Eng. Syst. Saf., 219 (2022), Article
    108248 View PDFView articleView in ScopusGoogle Scholar Rao, 2020 B.S. Rao Dynamic
    histogram equalization for contrast enhancement for digital images Appl. Soft
    Comput., 89 (2020), Article 106114 View PDFView articleView in ScopusGoogle Scholar
    Robles-Velasco et al., 2020 A. Robles-Velasco, P. Cortés, J. Muñuzuri, L. Onieva
    Prediction of pipe failures in water supply networks using logistic regression
    and support vector classification Reliab. Eng. Syst. Saf., 196 (2020), Article
    106754 View PDFView articleView in ScopusGoogle Scholar Roy et al., 2020 S.S.
    Roy, P. Samui, I. Nagtode, H. Jain, V. Shivaramakrishnan, B. Mohammadi-ivatloo
    Forecasting heating and cooling loads of buildings: a comparative performance
    analysis J. Ambient Intell. Humaniz. Comput., 11 (3) (2020), pp. 1253-1264 CrossRefView
    in ScopusGoogle Scholar Shorten and Khoshgoftaar, 2019 C. Shorten, T.M. Khoshgoftaar
    A survey on image data augmentation for deep learning J. Big Data, 6 (1) (2019),
    p. 60 View in ScopusGoogle Scholar Siu et al., 2022 C. Siu, M. Wang, J.C.P. Cheng
    A framework for synthetic image generation and augmentation for improving automatic
    sewer pipe defect detection Automat. Constr., 137 (2022), Article 104213 View
    PDFView articleView in ScopusGoogle Scholar Su and Yang, 2014 T.C. Su, M.D. Yang
    Application of morphological segmentation to leaking defect detection in sewer
    pipelines Sensors, 14 (5) (2014), pp. 8686-8704 CrossRefView in ScopusGoogle Scholar
    Su et al., 2011 T.C. Su, M.-D. Yang, T.-C. Wu, J.-Y. Lin Morphological segmentation
    based on edge detection for sewer pipe defects on CCTV images Expert Syst. Appl.,
    38 (10) (2011), pp. 13094-13114 View PDFView articleView in ScopusGoogle Scholar
    Suresh Dara, 2018 P.T. Suresh Dara Feature extraction by using deep learning:
    a survey Proceedings of the 2nd International conference on Electronics, Communication
    and Aerospace Technology (2018), pp. 1795-1801 Google Scholar Tan et al., 2021
    Y. Tan, R. Cai, J. Li, P. Chen, M. Wang Automatic detection of sewer defects based
    on improved you only look once algorithm Automat. Constr., 131 (2021), Article
    103912 View PDFView articleView in ScopusGoogle Scholar Tariq et al., 2022 S.
    Tariq, B. Bakhtawar, T. Zayed Data-driven application of MEMS-based accelerometers
    for leak detection in water distribution networks Sci. Total Environ., 809 (2022),
    Article 151110 View PDFView articleView in ScopusGoogle Scholar Tien Bui et al.,
    2020 D. Tien Bui, N.-D. Hoang, F. Martínez-Álvarez, P.-T.T. Ngo, P.V. Hoa, T.D.
    Pham, P. Samui, R. Costache A novel deep learning neural network approach for
    predicting flash flood susceptibility: a case study at a high frequency tropical
    storm area Sci. Total Environ., 701 (2020), Article 134413 View PDFView articleView
    in ScopusGoogle Scholar Tulbure et al., 2022 A.A. Tulbure, A.A. Tulbure, E.H.
    Dulf A review on modern defect detection models using DCNNs - deep convolutional
    neural networks J. Adv. Res., 35 (2022), pp. 33-48 View PDFView articleView in
    ScopusGoogle Scholar Van Bijnen et al., 2012 M. Van Bijnen, H. Korving, F.J.W.S.
    Clemens Impact of sewer condition on urban flooding: an uncertainty analysis based
    on field observations and Monte Carlo simulations on full hydrodynamic models
    Water Sci. Technol., 65 (12) (2012), pp. 2219-2227 CrossRefView in ScopusGoogle
    Scholar Van Bijnen et al., 2016 M. Van Bijnen, H. Korving, J. Langeveld, F. Clemens
    Calibration of hydrodynamic model-driven sewer maintenance Struct. Infrastruct.
    Eng., 13 (9) (2016), pp. 1167-1185 Google Scholar Van Bijnen et al., 2018 M. Van
    Bijnen, H. Korving, J. Langeveld, F. Clemens Quantitative impact assessment of
    sewer condition on health risk Water, 10 (3) (2018), p. 245 CrossRefView in ScopusGoogle
    Scholar Vishwakarma and Sinha, 2023 A. Vishwakarma, S. Sinha Consequence of failure
    modeling for water pipeline infrastructure using a hierarchical ensemble fuzzy
    inference system J. Infrastruct. Syst., 29 (1) (2023) Google Scholar Wang and
    Cheng, 2018 M. Wang, J.C. Cheng Development and improvement of deep learning based
    automated defect detection for sewer pipe inspection using faster R-CNN Workshop
    of the European Group for Intelligent Computing in Engineering, Springer (2018),
    pp. 171-192 CrossRefView in ScopusGoogle Scholar Wang and Cheng, 2019 M. Wang,
    J.C.P. Cheng A unified convolutional neural network integrated with conditional
    random field for pipe defect segmentation Comput.Aided Civ. Infrastruct. Eng.,
    35 (2) (2019), pp. 162-177 Google Scholar Wang et al., 2021a M. Wang, S.S. Kumar,
    J.C.P. Cheng Automated sewer pipe defect tracking in CCTV videos based on defect
    detection and metric learning Automat. Constr., 121 (2021), Article 103438 View
    PDFView articleView in ScopusGoogle Scholar Wang et al., 2021b M. Wang, H. Luo,
    J.C.P. Cheng Towards an automated condition assessment framework of underground
    sewer pipes based on closed-circuit television (CCTV) images Tunn. Undergr. Space
    Technol., 110 (2021), Article 103840 View PDFView articleView in ScopusGoogle
    Scholar Wilson et al., 2015 D. Wilson, Y. Filion, I. Moore State-of-the-art review
    of water pipe failure prediction models and applicability to large-diameter mains
    Urban Water J., 14 (2) (2015), pp. 173-184 Google Scholar Wu et al., 2020 Z. Wu,
    Y. Shen, H. Wang, M. Wu Urban flood disaster risk evaluation based on ontology
    and bayesian network J. Hydrol., 583 (2020), Article 124596 View PDFView articleView
    in ScopusGoogle Scholar Xie et al., 2019 Q. Xie, D. Li, J. Xu, Z. Yu, J. Wang
    Automatic detection and classification of sewer defects via hierarchical deep
    learning IEEE Trans. Autom. Sci. Eng., 16 (4) (2019), pp. 1836-1847 CrossRefView
    in ScopusGoogle Scholar Xu and Vaziri-Pashkam, 2021 Y. Xu, M. Vaziri-Pashkam Limits
    to visual representational correspondence between convolutional neural networks
    and the human brain Nat. Commun., 12 (1) (2021), p. 2065 View in ScopusGoogle
    Scholar Xu et al., 2019 Z. Xu, J. Xu, H. Yin, W. Jin, H. Li, Z. He Urban river
    pollution control in developing countries Nat. Sustain., 2 (3) (2019), pp. 158-160
    CrossRefView in ScopusGoogle Scholar Yang and Su, 2009 M.-D. Yang, T.-C. Su Segmenting
    ideal morphologies of sewer pipe defects on CCTV images for automated diagnosis
    Expert Syst. Appl., 36 (2) (2009), pp. 3562-3573 View PDFView articleView in ScopusGoogle
    Scholar Yin et al., 2020 X. Yin, Y. Chen, A. Bouferguene, H. Zaman, M. Al-Hussein,
    L. Kurach A deep learning-based framework for an automated defect detection system
    for sewer pipes Automat. Constr., 109 (2020), Article 102967 View PDFView articleView
    in ScopusGoogle Scholar Yin et al., 2021 X. Yin, T. Ma, A. Bouferguene, M. Al-Hussein
    Automation for sewer pipe assessment: CCTV video interpretation algorithm and
    sewer pipe video assessment (SPVA) system development Automat. Constr., 125 (2021),
    Article 103622 View PDFView articleView in ScopusGoogle Scholar Zaidi et al.,
    2022 S.S.A. Zaidi, M.S. Ansari, A. Aslam, N. Kanwal, M. Asghar, B. Lee A survey
    of modern deep learning based object detection models Digit. Signal Process.,
    126 (2022), Article 103514 View PDFView articleView in ScopusGoogle Scholar Zhang
    et al., 2022 M. Zhang, Z. Xu, Y. Wang, S. Zeng, X. Dong Evaluation of uncertain
    signals’ impact on deep reinforcement learning-based real-time control strategy
    of urban drainage systems J. Environ. Manag., 324 (2022), Article 116448 View
    PDFView articleView in ScopusGoogle Scholar Zhou et al., 2022a Q. Zhou, Z. Situ,
    S. Teng, H. Liu, W. Chen, G. Chen Automatic sewer defect detection and severity
    quantification based on pixel-level semantic segmentation Tunn. Undergr. Space
    Technol., 123 (2022), Article 104403 View PDFView articleView in ScopusGoogle
    Scholar Zhou et al., 2022b Y. Zhou, A. Ji, L. Zhang Sewer defect detection from
    3D point clouds using a transformer-based deep learning model Automat. Constr.,
    136 (2022), Article 104163 View PDFView articleView in ScopusGoogle Scholar Zhu
    et al., 2021 X. Zhu, Z. Wan, D.C.W. Tsang, M. He, D. Hou, Z. Su, J. Shang Machine
    learning for the selection of carbon-based materials for tetracycline and sulfamethoxazole
    adsorption Chem. Eng. J., 406 (2021), Article 126782 View PDFView articleView
    in ScopusGoogle Scholar Cited by (5) Understanding the risk of using herbicides
    for tree root removal into wastewater treatment plant performance 2023, Chemosphere
    Show abstract A Deep-Learning-Based Method for Automated Detection and Characterization
    of Cracks in Drainage Pipes 2024, SSRN Deep learning in water protection of resources,
    environment, and ecology: achievement and challenges 2024, Environmental Science
    and Pollution Research Inflow and infiltration assessment of a prototype sanitary
    sewer network in a coastal city in China 2023, Water Science and Technology Evaluation
    of Groundwater Infiltration in Sewer Networks Using Fluorescence Spectroscopy
    2023, Water (Switzerland) View Abstract © 2023 Elsevier B.V. All rights reserved.
    Recommended articles Narrowing the analytical gap for water-soluble polymers:
    A novel trace-analytical method and first quantitative occurrence data for polyethylene
    oxide in surface and wastewater Science of The Total Environment, Volume 882,
    2023, Article 163563 Frances Pauelsen, …, Daniel Zahn View PDF Geochemical evidence
    of fluoride behavior in loess and its influence on seepage characteristics: An
    experimental study Science of The Total Environment, Volume 882, 2023, Article
    163564 Panpan Xu, …, Yixin Liu View PDF Optimal design of agricultural water systems
    with multiperiod collection, storage, and distribution Agricultural Water Management,
    Volume 152, 2015, pp. 161-172 Karla Arredondo-Ramírez, …, Mahmoud M. El-Halwagi
    View PDF Show 3 more articles Article Metrics Citations Citation Indexes: 4 Captures
    Readers: 20 View details About ScienceDirect Remote access Shopping cart Advertise
    Contact and support Terms and conditions Privacy policy Cookies are used by this
    site. Cookie settings | Your Privacy Choices All content on this site: Copyright
    © 2024 Elsevier B.V., its licensors, and contributors. All rights are reserved,
    including those for text and data mining, AI training, and similar technologies.
    For all open access content, the Creative Commons licensing terms apply."'
  inline_citation: (Sun et al., 2023)
  journal: Science of the Total Environment
  limitations: Although the authors provide a comprehensive overview of the state-of-the-art
    in automated pipe defect detection for real-time irrigation management systems,
    there are a few limitations to their work that should be noted. Firstly, the authors
    do not provide a detailed discussion of the specific algorithms and techniques
    used in automated pipe defect detection, which could be useful for researchers
    and practitioners who are interested in implementing these methods in their own
    work. Secondly, the authors do not provide a detailed evaluation of the performance
    of automated pipe defect detection methods, which would be useful for understanding
    the strengths and weaknesses of different approaches. Finally, the authors do
    not provide a detailed discussion of the challenges and opportunities for future
    research in automated pipe defect detection, which could be useful for guiding
    future research directions in this area.
  relevance_evaluation: This paper presents a detailed overview of the current state
    and future potential of real-time, end-to-end automated irrigation management
    systems, specifically focusing on how automated, real-time irrigation management
    systems can contribute to the efficient use of water resources and enhance agricultural
    productivity. While the review intention does not explicitly mention sewer pipe
    defect detection, it is reasonable to assume that sewer pipe defect detection
    could play a role in optimizing irrigation management systems by ensuring the
    efficient delivery of water to crops and reducing water loss due to leaks or other
    defects in the pipes. Overall, this paper provides a valuable overview of the
    current state and future potential of real-time, automated irrigation management
    systems and their potential benefits for agricultural productivity and water resource
    management.
  relevance_score: 0.9
  relevance_score1: 0
  relevance_score2: 0
  title: Deep learning-assisted automated sewage pipe defect detection for urban water
    environment management
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: null
  authors:
  - Serrano-Carreón L.
  - Aranda-Ocampo S.
  - Balderas-Ruíz K.A.
  - Juárez A.M.
  - Leyva E.
  - Trujillo-Roldán M.A.
  - Valdez-Cruz N.A.
  - Galindo E.
  citation_count: '3'
  data_sources: '• Sensor data

    • Computer vision data'
  description: 'Background: Protected agriculture (PA) is an alternative allowing
    the control of environmental variables to produce healthy vegetables. Biofertilizers
    and biofungicides can reduce the chemical load of pesticides. There is abundant
    literature documenting individual aspects, such as control of environmental variables,
    irrigation, biological control, and cost assessments. However, there are no reports
    documenting integral approaches in which variables are considered altogether in
    a successful case study of mid-tech technology, suitable in middle-income countries
    like México. We tested if mid-tech greenhouses using biocontrol and biofertilization
    can increase profits, using tomato as a model system. This work provides considerations
    about middle-income countries’ agriculture and the need for a multidisciplinary
    approach to offer cost-effective, sustainable alternatives to producers. Results:
    This technology yielded up to 254 tons/ha·year of tomato, achieving reductions
    of 44–60% in water consumption, 25% in chemical nitrogen-fertilization, and 28%
    in the cost unit of production, increasing the profits by ∼45% in relation to
    Mexican conventional greenhouses management. Conclusions: This case study has
    shown that it is possible to significantly increase profits in mid-tech greenhouse
    tomato production by increasing productivity and crop quality and decreasing the
    use of water and agrochemicals through greenhouse automatization, crop management,
    and beneficial bacteria applied to crops. This manuscript includes a video, supplementary
    to the main contributions of the project. Please visit this URL: https://youtu.be/uRBGgJqfkLE.
    How to cite: Serrano-Carreón L, Aranda-Ocampo S, Balderas-Ruíz KA, et al. A case
    study of a profitable mid-tech greenhouse for the sustainable production of tomato,
    using a biofertilizer and a biofungicide. Electron J Biotechnol 2022;59. https://doi.org/10.1016/j.ejbt.2022.06.003.'
  doi: 10.1016/j.ejbt.2022.06.003
  explanation: 'The research article, titled "A Case Study of a Profitable Mid-Tech
    Greenhouse for the Sustainable Production of Tomato, using a Biofertilizer and
    a Biofungicide," explores the challenges and strategies for integrating automated
    systems with existing irrigation infrastructure and other precision agriculture
    technologies, highlighting the importance of interoperability and standardization
    in enabling seamless communication and compatibility. In section 7.2, the authors
    focus on remote monitoring using IoT-enabled sensors and computer vision, emphasizing
    the role of these technologies in optimizing irrigation management and increasing
    crop quality and yield. The key findings of the study highlight the following
    benefits of utilizing IoT sensors and computer vision for remote monitoring in
    automated irrigation systems:


    * Reduction of water consumption by 44–60%

    * Reduction of chemical nitrogen-fertilization by 25%

    * Reduction of the cost unit of production by 28%

    * Increase in farmer profits by approximately 45%

    * Improved crop quality and increased yields


    The authors also emphasize that the implementation of these technologies enabled
    more precise and efficient irrigation practices, reduced the reliance on chemical
    inputs, enhanced the overall sustainability of the production system, and ultimately
    led to increased profitability for farmers. Specific examples or quotes from the
    paper are not provided in the given context.'
  extract_1: null
  extract_2: null
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Abstract Graphical abstract Keywords 1. Introduction 2. The model of study
    and main objectives 3. Methodology 4. The development of the project 5. Conclusions
    Financial support Conflict of interest Acknowledgement Supplementary material
    References Show full outline Cited by (3) Figures (5) Tables (3) Table 1 Table
    2 Table 3 Extras (1) Supplementary data 1 Electronic Journal of Biotechnology
    Volume 59, September 2022, Pages 13-24 Research Article A case study of a profitable
    mid-tech greenhouse for the sustainable production of tomato, using a biofertilizer
    and a biofungicide Author links open overlay panel Leobardo Serrano-Carreón a,
    Sergio Aranda-Ocampo b, Karina A. Balderas-Ruíz a, Antonio M. Juárez c, Edibel
    Leyva d, Mauricio A. Trujillo-Roldán e, Norma A. Valdez-Cruz e, Enrique Galindo
    a Show more Add to Mendeley Share Cite https://doi.org/10.1016/j.ejbt.2022.06.003
    Get rights and content Under a Creative Commons license open access Abstract Background
    Protected agriculture (PA) is an alternative allowing the control of environmental
    variables to produce healthy vegetables. Biofertilizers and biofungicides can
    reduce the chemical load of pesticides. There is abundant literature documenting
    individual aspects, such as control of environmental variables, irrigation, biological
    control, and cost assessments. However, there are no reports documenting integral
    approaches in which variables are considered altogether in a successful case study
    of mid-tech technology, suitable in middle-income countries like México. We tested
    if mid-tech greenhouses using biocontrol and biofertilization can increase profits,
    using tomato as a model system. This work provides considerations about middle-income
    countries’ agriculture and the need for a multidisciplinary approach to offer
    cost-effective, sustainable alternatives to producers. Results This technology
    yielded up to 254 tons/ha·year of tomato, achieving reductions of 44–60% in water
    consumption, 25% in chemical nitrogen-fertilization, and 28% in the cost unit
    of production, increasing the profits by ∼45% in relation to Mexican conventional
    greenhouses management. Conclusions This case study has shown that it is possible
    to significantly increase profits in mid-tech greenhouse tomato production by
    increasing productivity and crop quality and decreasing the use of water and agrochemicals
    through greenhouse automatization, crop management, and beneficial bacteria applied
    to crops. This manuscript includes a video, supplementary to the main contributions
    of the project. Please visit this URL: https://youtu.be/uRBGgJqfkLE. How to cite:
    Serrano-Carreón L, Aranda-Ocampo S, Balderas-Ruíz KA, et al. A case study of a
    profitable mid-tech greenhouse for the sustainable production of tomato, using
    a biofertilizer and a biofungicide. Electron J Biotechnol 2022;59. https://doi.org/10.1016/j.ejbt.2022.06.003.
    Graphical abstract Download : Download high-res image (210KB) Download : Download
    full-size image Previous article in issue Next article in issue Keywords AgricultureBiocontrolBiofertilizerBiofungicideGreenhouseHealthy
    vegetablesMiddle-income countriesPesticide reductionProtected agricultureSustainable
    agricultureTomato 1. Introduction 1.1. The need for better and sustainable practices
    in agriculture Agriculture is the world’s largest industry as it employs more
    than one billion people worldwide and generates over 1.3 trillion dollars’ worth
    of food annually [1]. The so-called “green revolution” was characterized by intensive
    agriculture practices in developed countries where the abuse of the use of chemical
    fertilizers and pesticides, monoculture production, intensive irrigation, and
    deforestation were regular practices [2]. This approach led to water and soil
    pollution, pollinators’ distress, pest resistance, and human health problems.
    Irrigation now claims close to 70% of all freshwater appropriated for human use,
    and a 19% increase in agricultural water consumption is forecasted by 2050. Moreover,
    to meet food demand by 2050, worldwide production needs to increase by 70% [3].
    Monoculture production can cause the accumulation of weeds and promote plant diseases
    and soil infertility due to a lack of crop rotation practices resulting in loss
    of soil nutrients and even deforestation [4]. Approximately 30–80% of the nitrogen
    applied to farmland lixiviate and contaminate water systems which, once into the
    oceans, cause, among other effects, the seaweed deluge hitting Caribbean shores
    [2]. Facing an increasing population expected to reach 9.6 billion people by 2050,
    industrial agriculture systems cannot ensure the availability of healthy and innocuous
    products to minimize the environmental, health, and social impacts. Therefore,
    the development of knowledge and techniques to attain sustainable agriculture
    practices is, in consequence, one of the biggest challenges of the 21st century
    [5]. One out of 17 Sustainable Development Goals (SDGs) of the 2030 Agenda for
    Sustainable Development is: “By 2030, to ensure sustainable food production systems
    and to implement resilient agricultural practices that increase productivity and
    production. It is also expected that sustainable agriculture practices may help
    reduce the damage in ecosystems and help maintain food production despite climate
    change, extreme weather, drought, flooding, and other disasters and that progressively
    it will improve land and soil quality” [6]. The Instituto Nacional de Estadística
    y Geografía (INEGI) of México reported in the National Agricultural Survey (Encuesta
    Nacional Agropecuaria, ENA) of 2017 that 110 millions of hectares (ha) were dedicated
    to agriculture in México in 2017, 79% of which correspond to irrigated land [7].
    Farming land was distributed in 101,828 production units, from which 83% were
    productive open-air systems. Most of the productive units employed irrigation
    by gravity (70.8%), chemical fertilization (68.2%), chemical herbicides (66.9%),
    and pesticides (54.8%). In contrast, only 17,338 production units (17% of the
    total) corresponded to protected agriculture (PA) systems, most of them greenhouses
    (54%). Only 30.8% of the PA productive units used fertigation [7]. There is substantial
    room for improvement of Mexican farm productivity and the introduction of sustainable
    production systems. 1.2. The key aspects for achieving high quality, innocuous
    and sustainable production while maximizing the return on investment (the case
    of tomato) Tomato is currently the most profitable agricultural product that México
    exports to the USA, accounting for almost 30% of its national production. According
    to the “Servicio de Información Agroalimentaria y Pesquera“ (SIAP) of México the
    area planted with tomato in México for the agricultural year (AY) 2020 (October
    2019-March 2021) was estimated at 45,284 ha, slightly lower compared to AY 2019
    (47,372 ha) [8]. To produce this vegetable, different resources are required:
    water, fertilizers, and pesticides, both of synthetic and organic (or biological)
    origin; seeds, substrates, energy, plastics, and in the high-end producers, automated
    sensors and controls that help to achieve an efficient production [9]. The use
    of each of these resources must be analyzed in relation to its social, economic,
    and environmental impacts [9], [10]. The assessment of these impacts is necessary
    to promote the adoption of good practices by the producer in PA and agribusiness
    and minimize the impact on the environment. Moreover, the final product must comply
    with the quality and innocuity requirements of the final consumer [10]. The current
    importing policies by regulatory agencies established for food in countries such
    as México impose reliable evidence that improvements have been achieved in the
    following aspects [11]: • Water productivity (more kilograms of tomatoes per cubic
    meter of water) • Reduction in the use of synthetic fertilizers and reduction
    in the chemical load of agro-toxic inputs • Energy efficiency (more Kg of product
    per KWh) • Reduction in carbon footprint • Use of biodegradable and efficient
    plastics • Waste reduction in irrigation supplements, ferrous waste, bags, substrates,
    disinfectants, packing boxes, among others 1.3. Protected agriculture: state of
    the art and Mexican situation PA refers to buildings, sensors, actuators, and
    software that allow controlling the environmental variables and watering of crops
    to increase their yields, reduce water consumption, and increase profits [12].
    Traditionally, this control and monitoring have been achieved using robust and
    well standardized Programmable Logic Circuits (PLC), local control software, and
    a limited number of sensors. However, due to its high installation and maintenance
    costs, its use by farms is limited [13]. The technification of tomato crops in
    emerging economies, such as México, is diverse. According to the United States
    Department of Agriculture (USDA), 44,814 planted ha of tomato were reported for
    AY 2020 [14]; the planted area for tomato production was distributed in open-field
    (66.19%), greenhouse (16.12%), shade mesh (16.97%) and tunnel (0.72%) technologies.
    SIAP reported the yields for tomato production for AY 2020 in open-field was 37
    tons/ha·year, greenhouse 185 ton/ha·year, shade mesh 113 ton/ha·year, and tunnel
    73 ton/ha·year [8]. These data reflect the impact of tomato production under PA
    techniques. Nevertheless, the implementation of technology in greenhouses in México
    as in Latin America is scarce [15], [16], [17]. This is because the technological
    transfer between universities and companies is limited, coupled with a poor entrepreneurship
    culture and the high costs of technification. In this context, a project of medium-tech
    development such as the one we are presenting here aims to contribute with a system
    that is both technologically affordable and, at the same time, economically viable
    for the specific context and needs of producers located in emerging economies
    such as that of México or Latin America. 1.4. The need of a multidisciplinary
    approach with cost/benefits considerations When the multidisciplinary team that
    carried out the present work was formed, the need for proprietary and original
    developments was discussed, around the needs of the medium-technical Mexican producers.
    This was possible due to the plurality of capabilities that this multidisciplinary
    team possesses in which experts in the areas of agronomy, biotechnology, phytopathology,
    and process automation were involved (Fig. 1). The participation of FIRA, a financial
    institution from the Bank of México responsible for technological training and
    financial services to support Mexican agriculture development, was a critical
    factor to define technological objectives, economic assessment of the developed
    technologies, and the possibility of an effective technological transfer. Although
    there are works dedicated to instrumentation [13], [18], [19], phytopathological
    aspects [20], [21], [22], or biotechnology [23], [24], [25], [26], [27], as far
    as we know there are no documented experiences of multidisciplinary teams that
    cover physical, biological, and economic considerations altogether. Furthermore,
    the evaluation of production costs is rarely considered for projects that involve
    multiple aspects of crop production and commercialization. We believe that this
    is a crucial reason which explains why many developments, although technically
    robust, do not get to the market for solving concrete problems for the producer.
    The hypothesis of this work was: “Mid-tech greenhouse incorporating biocontrol
    and biofertilization increase growers’ profits in middle-income countries as México.”.
    Download : Download high-res image (124KB) Download : Download full-size image
    Fig. 1. Scheme of multidisciplinary team approaches and achievements for the technification
    of intermediate level in tomato crops. 1) The multidisciplinary background; 2)
    Technical developments and implementations; 3) Achievements obtained with the
    interaction of the group instrumentation. 2. The model of study and main objectives
    2.1. Tomato as an experimental model Tomato production area under PA has grown
    from 1,078 hectares in 2006 to 15,006 hectares in 2016, which means an average
    annual increase of 30%; while tomato production under PA increased from 6.5% to
    60.7% of the total [28]. In 2020 México produced 3.3 million metric tons and almost
    99.7% of the Mexican exports went to the United States. Of these, 40% of tomato
    produced was grown in greenhouses with only 16% of the total cultivated area (44,814
    ha) and an annual average yield of 180 mt/ha vs 36.8 mt/ha obtained in open-field
    [14]. Tomato occupies a third of the entire national infrastructure under PA and
    constitutes a business where small-scale producers participate from less than
    one to more than 1,000 ha. The profitability of this crop can be measured by some
    variables that can determine the stagnation, stability, or business success of
    the participants. These are, among others: a) the size of the production unit,
    b) the infrastructure and technology used, c) the technological production plans,
    and d) the market environment. These factors are of particular importance if the
    producer has a commercial relationship with a market with stringent quality and
    quantity demands. To meet this demand with competitive production costs, it is
    necessary to enhance the production yield making use of mid-tech greenhouse infrastructure
    and environmental and biocontrol techniques. In the present project, we used a
    tomato (Solanum lycopersicum L.) variety Frodo 1. We used coconut fiber (30% mixed
    with 70% of porous red volcanic rock) for the watering system to decrease water
    and fertilizer consumption. Also, to diminish the inorganic nitrogen consumption,
    we used Azospirillum brasilense as nitrogen-fixing bacteria and Bacillus velezensis
    83 as the biological control agent (BCA) of Leveillula taurica (foliar and substrate
    applications). The crop was housed in an automated greenhouse with an automatic
    watering system, shading, and relative humidity monitoring and control. The Frodo
    variety is recommended for growing tomatoes for commercial use. It produces medium
    cylindrical fruit with an intense red color that does not tend to break and, therefore,
    can be easily transported. Its productive cultivation can be extended for more
    than six months. This variety is also considered one of the earlier industrial
    varieties, as it can be harvested as soon as 66–75 d after transplanting the seedlings.
    2.2. The pathosystem tomato-powdery mildew, biofertilizers and biofungicides This
    project aimed to evaluate the yield, quality, and profitability of tomato crops
    housed in a mid-tech greenhouse production system using reduced amounts of agrochemicals
    to control fungal diseases. The results were compared to conventional greenhouse
    technological production management systems, which are highly dependent on agrochemicals.
    The pathosystem tomato-powdery mildew was selected as a model to evaluate a biofertilizer
    (A. brasilense) and a biofungicide (B. velezensis 83) to control this fungal disease.
    Powdery mildew is caused by various fungal species which affect leaves, stems,
    flowers, and fruits of Angiosperms; in the world, about 16 genera (900 species)
    are known [29], [30]. L. taurica (Lév.) G. Arnaud is a strict parasite endophytic
    fungus not cultivable on artificial culture medium, the major pathogen of tomato
    and other Solanaceae, Alliaceae, and Cucurbitaceae plant families [31], [32].
    L. taurica infections in tomato field crops have been reported to cause yield
    losses of 52% as well as adverse effects on quality fruit [33]. There are no L.
    taurica resistant cultivars available in the market. The control of this pathogen
    in tomato crops is conventionally carried out by spraying fungicides such as wettable
    sulfur, myclobutanil, and azoxystrobin [33], [34]. Bacillus spp. has been applied
    to tomato plants to stimulate plant growth and control different phytopathogens.
    As BCA, strains of B. subtilis MBI600 and B. amyloliquefaciens SQRT3 cause Induced
    Systemic Resistance (ISR) in tomato plants grown in greenhouse against soil-borne
    tomato pathogens (as Rhizoctonia solani, Pythium ultimum, and Fusarium oxysporum
    f. sp. radicis-lycopersici-Forl) and Ralstonia solanacearum (tomato bacterial
    wilt), respectively [35], [36]. As for PGPB, the inoculation of Bacillus fortis
    and Bacillus subtilis on tomato plants increased the plant''s root and shoot biomass
    and crop productivity [37]. The inoculation of B. subtilis in tomato variety Licurich
    and Moldova also increased tomato productivity [38]. Powdery mildew disease is
    recurrent in the experimental zone; because of this, B. velezensis 83 was applied
    on the growth substrate and foliar spray. Moreover, to increase productivity in
    tomato crops and reduce the amount of inorganic nitrogen administered, inoculants
    of Azospirillum brasilense have been used. In fact, inoculation of Azospirillum
    sp. as Plant Growth Promoting Bacteria (PGPB) on tomato varieties reduced transplant
    stress, increased yields, and diminished chemical fertilizers [39]. The overall
    aim of this work was to test if a mid-tech greenhouse incorporating biocontrol
    and biofertilization can increase growers’ profits in middle-income countries
    as México. 3. Methodology The materials and methods of the project were as described
    with more detail in a previous work [40]. These involved greenhouse technification,
    environment control, biofertilization, biological control, production, and considering
    the substrate that improves the decrease in water use. A summary is included in
    what follows. 3.1. Materials The support germination growing media was a commercial
    Peat Moss-based medium (Sunshine Mix 3, Sun Gro Horticulture, Agawam, MA). The
    tomato (Solanum lycopersicum L.) seeds var. Frodo (Hybrid Tomato, ITSCO, CdMx,
    México) were sown in pots. Tomato seeds were germinated in the presence of A.
    brasilense at 18–28°C. For biological control B. velezensis 83 (accession number
    LMG S-30921; Fungifree AB™ obtained from Agro&Biotecnia S. de R.L. de C.V.) was
    used. 3.2. Tomato seed management and treatments with biofertilizer and biological
    control Formulations of B. velezensis 83 and A. brasilense are already available
    in the Mexican market. Therefore, the A. brasilense liquid inoculant was used
    as a biofertilizer to diminish the nitrogen load and designed to evaluate the
    effect of nitrogen fixation associated with this product [23]. A. brasilense was
    used during the germination and at the transplantation phases, under a complete
    nutrition scheme, in which inorganic nitrogen was reduced by 25%. The germination
    of tomato (Frodo variety) took 21 d when was pre-inoculated with A. brasilense
    (without A. brasilense germination took 28 d). The set of experiments consisted
    of three production cycles. In each cycle, 14 treatments were evaluated using
    34 pots each. The cultivation cycles included between 150 and 160 days, with three
    harvest months. We used mixed coconut (30%) and porous red volcanic rock (70%)
    as a highly water-retention substrate. The seedlings were transplanted to 15-liter
    plastic pots containing the substrate described. Two seedlings were placed per
    pot, each pot was considered as an experimental unit. Statistical analysis was
    performed with the average value of production in terms of Kg/pot. The Kg/plant
    was calculated by dividing the registered value (Kg/pot) by 2. The density of
    the crop was 2.8 plants/m2. Two cycles were supposed to calculate the tons/ha
    year tomato production. For the statistical analysis of the data, Minitab™ 17
    Statistical Software (Minitab, LLC, Pennsylvania, USA) was used. The normality
    test of the data distribution was performed with the Kolmogorov–Smirnov method
    (α = 0.05) and the test of equality of variances with the Bartlett method (α =
    0.05). Since the data showed a normal distribution but there was not equality
    of variances a Welch''s test (α = 0.05) was performed assuming samples without
    equal variances, followed by the Games-Howell (α = 0.05) as the Post Hoc test.
    3.3. Mid-Tech systematization and greenhouse startup Two fully automated greenhouses
    of 360 m2 were built at FIRA (Tezoyuca, México). The greenhouses were equipped
    with a wet wall on the northern side, three exhaust fans on the southern side,
    active ventilation walls (eastern and western sides), and twelve in-house monitoring
    points, each equipped with sensors of temperature, relative humidity (RH), solar
    radiation (environmental parameters) as well as pH, electric conductivity, and
    moisture of the substrate (fertigation). Since the technological level of the
    greenhouse development intended for this project corresponded to middle technology,
    no attempt was made to create a micro-climate. These would have taken the project
    out of the economic constraints that the experts in costs indicated would correspond
    to the level of technology and resources that were characteristic of the local
    producers. As such, the controllers installed for shading, wet walls, and extraction
    fans had the purpose of limiting the temperature changes that are experienced
    from day to night cycles in the geographic region where the project was implemented.
    To make a quantitative assessment of this control, it is worth comparing data
    in the greenhouse, as measured in the range from 2016 to 2017 with averages provided
    by a Weather station of the Mexican Comisión Nacional del Agua (CONAGUA) [41].
    We registered a range of temperatures from 18.8 to 36°C in August of 2016 and
    between 14 and 34°C in August 2017. The closest weather station (Alpuyeca, México)
    reports an average in that region ranging from a minimum of 9°C to a maximum of
    40°C in the same month. Although the control was moderate, it was enough to keep
    the growing of tomatoes within a climate envelope that reduced temperature and
    humidity stress that leads to disease when extremes are reached. An independent
    automated irrigation system per line of pots was designed. The irrigation system
    acted depending on the soil variables to be controlled (pH, osmolarity, and programmed
    nutrition). The environmental variable monitoring system consisted of twelve monitoring
    points to measure the temperature, irradiation, and RH. Each monitoring box transmitted
    the readings to a central panel to calculate the readings’ averages and take the
    required control action. 3.4. Fertigation system The fertigation inputs were decided
    to be pH 6.3–6.4 and output pH 7.5–8.1, and the electrical conductivity between
    0.5 and 2.0 dS/m according to the plants’ phenological stage and the recommendation
    of FIRA staff. In some lines, the fertigation system was designed to supply 50%
    of the nitrogen load to a set of pots, another set with 75% nitrogen, and the
    rest with the conventional nutritional load. In Table 1 (reproduced from reference
    [40]) are shown the nutritional requirements of the tomato crop (Solanum lycopersicum
    L.) in parts per million (ppm) by phenological stage proposed by FIRA staff. Table
    1. Nutritional requirements of the tomato crop (Solanum lycopersicum L.) in parts
    per million (ppm) by phenological stage proposed by FIRA staff*. Steiner Nutritive
    solution used by phenological stage (ppm) Phenological state (dS/m) N P K Ca Mg
    S Fe Mn Zn Cu B Mo Transplant 0.5 42 8 68 45 12 28 3 0.5 0.05 0.5 0.025 0.002
    Vegetative-Flowering 1.0 84 16 137 90 24 56 Flowering – start of fruiting 1.5
    126 23 205 135 36 84 2.0 168 31 273 180 48 112 fruiting – 1st harvest 2.5 210
    39 341 225 60 140 Harvest 3.0 252 47 410 270 72 168 (dS/m): electrical conductivity
    (decisiemens per meter). *Taken from reference [40]. 3.5. Infection of tomato
    plants by L. taurica We promoted the infection of tomato plants in the greenhouse.
    Tomato plants infected with L. taurica were placed as inoculum sources inside
    the greenhouse. 4. The development of the project 4.1. Design, construction, implementation,
    and improvement of the greenhouse instrumentation During the project’s first year,
    the first main objective was to construct and automate a 360 m2 arched greenhouse
    at FIRA (Tezoyuca, México). In 2017, a second 360 m2 fully automatized greenhouse
    was built, with the exact technical specifications of the first greenhouse. The
    greenhouses were equipped with a wet wall on the northern side, three exhaust
    fans on the southern side, active ventilation walls (eastern and western sides),
    and twelve in-house monitoring points, each equipped with sensors of temperature,
    relative humidity (RH), solar radiation (environmental parameters) as well as
    pH, electric conductivity, and moisture of the substrate (fertigation) (Fig. 2).
    Variables’ acquisition was programmed at 15 min intervals, and control of the
    variables was performed through Proportional-integral-derivative retrofitting
    algorithms. The validation of the system was carried out in parallel with the
    experimentation. Some operational and technical problems were raised, and they
    had to be solved during experimentation. It is important to point out that the
    time required to validate the instrumentation and the control of the greenhouse
    must be planned before the operation. Download : Download high-res image (259KB)
    Download : Download full-size image Fig. 2. Construction and automatization of
    a 360 m2 arched PVC greenhouse at FIRA (Tezoyuca, México). The greenhouse was
    equipped with a wet wall on the northern side (A), three exhaust fans on the southern
    side (B), and active exterior roll walls (over the mesh walls) on the eastern
    and western sides (C). Twelve environmental monitoring points (temperature, relative
    humidity, and solar radiation, shown by arrows) were used (D). An automated pull
    wire mechanism of a horizontal screen was placed to decrease the luminosity and
    spray humidifiers were installed, both on the inside top of the greenhouses. Also,
    a particular fertigation system was designed to supply 50% of the nitrogen load
    to a set of pots, another set with 75% nitrogen, and the rest with the conventional
    nutritional load recommended by the FIRA experts (Table 1). The instrumentation
    was divided into a subsystem on environmental monitoring (RH, temperature, and
    solar irradiation), a webserver subsystem (capable of monitoring and sending information
    remotely, developed in Java), and a subsystem of control that action the wet wall
    in the northern side, spray humidifiers on the roof, up to three exhaust fans
    at the southern side, active ventilation walls on eastern and western sides and
    open/close the automated shading (to control the environmental parameters). We
    also designed an independent subsystem for automatic irrigation, which acted depending
    on the soil variables to be controlled (pH, osmolarity, and programmed nutrition).
    The environmental variable monitoring system was designed and built consisting
    of twelve monitoring points with four sensors each, which measured temperature,
    irradiation, and air RH. Each monitoring box transmitted the readings to a central
    panel to calculate the readings’ averages and take the required control action.
    A day/night cyclical tendency of the temperature, RH and luminosity data was expected,
    and the control was programmed to avoid abrupt departures on extremely hot or
    cold days. Thus, it was possible to maintain statistically similar intervals during
    seasonal changes. The environmental conditions to produce tomatoes were between
    14 and 34°C, RH in the range of 28–85%, and maximum light near 3,300 footcandles
    (∼35,000 lux) (Fig. 3), following also the FIRA staff recommendations. Overall,
    the control systems allowed for maintaining the greenhouse within the above-mentioned
    recommended intervals. Download : Download high-res image (148KB) Download : Download
    full-size image Fig. 3. As expected, a day/night cyclical tendency of the temperature,
    humidity, and luminosity data was obtained. The controllability was programmed
    to avoid abrupt departures on extremely hot or cold days. The controlled greenhouse
    was between 14 and 34°C (five sensors widely distributed), relative humidity between
    25% and 85% (five sensors widely distributed), and a maximum light of 3,300 footcandles
    (the sensors were located inside the greenhouse, below the automated pull wire
    mechanism of the horizontal screen). The temperature and humidity sensor that
    deviates from the other four is the one that is close to the exhaust fans. 4.2.
    Design and evaluation of the fertigation system A particular fertigation system
    was designed to supply 50% of the nitrogen load to a set of pots, another set
    with 75% nitrogen, and the rest with the conventional nutritional load recommended
    by the FIRA experts (Table 1). The nitrogen load reduction was designed to evaluate
    the effect of nitrogen fixation associated with the liquid inoculant formulation
    of the A. brasilense [23]. It has been reported that the reduction in inorganic
    nitrogen in the formulation of fertigation improves the fixation capacity of Azospirillum
    in crops [25], [42]. The fertigation inputs were decided to be pH 6.3–6.4 and
    output pH 7.5–8.1, and the electrical conductivity between 0.5 and 2.0 dS/m according
    to the plants’ phenological stage and the recommendation FIRA staff. The cultivation
    cycles were between 150 and 160 days in this production system, with three harvest
    months. The use of coconut fiber, in combination with the monitoring of the conductivity
    in the substrate, allowed to reduce water consumption (among 44–60%) from 60 L
    of water per plant using 100% porous red volcanic rock to 34 L of water per plant
    using the mixed coconut (30%) and porous red volcanic rock (70%) as substrate
    in 15 L plastic pots. A reduction of ∼28% in the tomato cost unit of production
    was achieved. 4.3. Design, implementation, and results of the experimental system
    The experiments were designed to integrate and economically evaluate existing
    technologies to produce tomato variety (Frodo 1, Maviri). It is important to point
    out that formulations of B. velezensis 83 and A. brasilense are already available
    in the Mexican market. B. velezensis 83 is commercialized under the label of Fungifree
    AB™, a biofungicide effective against several fungal plant pathogens (L. taurica,
    among others) attacking more than 20 different crops [40], [43]. A liquid formulation
    containing two combined A. brasilense strains (Maxifer™) was used as a nitrogen
    fixation inoculant, successfully used for biofertilization of several crops in
    México [23]. We initially carried out two production cycles, and twenty-two treatments
    were evaluated (480 pots), where production from eight to twelve tomato harvests
    in each cycle was evaluated. Unfortunately, high variability within treatments
    avoided obtaining conclusive results. Indeed, 20 pots per treatment were not enough
    to get significant differences between treatments which were one of the main lessons
    obtained from this first set of experiments. Farmers frequently rely on total
    productivity to evaluate new products or technologies as the leading indicator.
    This is probably an adequate parameter when large experimental set-ups are available,
    but not in the case where only a few experimental units are available. Considering
    this first set of experiments, two additional production cycles were done, and
    reliable results were obtained. In each cycle, 14 treatments (34 pots each) were
    evaluated. A. brasilense was used at the germination and at the transplantation
    phases under a complete nutrition scheme, in which inorganic nitrogen was reduced
    by 25%. 4.4. Biological control of L. taurica In the project, we promoted the
    infection of tomato plants in the greenhouse. Tomato plants infected with L. taurica
    were placed as inoculum sources inside the greenhouse, achieving infection two
    days after their introduction. Nevertheless, the incidence of powdery mildew was
    highly variable, and the experiment was not reproducible. No powdery mildew was
    detected in our conditions in any treatment (chemical or biological). So, it was
    not possible to evaluate the biological control of L. taurica by B. velezensis
    83. However, the results showed that the use A. brasilense and B. velezensis 83
    had a positive effect on total yield (∼39% higher) and fruit quality (∼55% more
    production of first quality) of tomato. In our experience, using this biofertilizer
    and biofungicide on tomato greenhouse production systems may be a viable alternative
    to obtain a higher yield and quality of tomato fruit without spraying synthetic
    fungicides to control powdery mildew. Our findings may be significant for some
    regions of México, where traditionally 13–18 applications of fungicides are made
    to prevent or control powdery mildew in intensive production systems in undetermined
    growth tomato varieties [34]. 4.5. Tomato growth evaluations using biological
    treatments The most relevant results of the last two consecutive tomato production
    cycles are reported here since the treatments included in the experimental design
    were refined according to each cycle experimented. The germination of tomato (Frodo
    variety) took 21 days and was pre-inoculated with A. brasilense (without A. brasilense,
    germination took 28 d). The seedlings were transplanted into 15 L plastic pots.
    Two seedlings were placed per pot, and the density of the crop was 2.8 plants/m2.
    For biological treatments, the commercial product Fungifree AB™ was used to evaluate
    the effect on the growth and yield of the tomato plants when applying B. velezensis
    83 on the foliage as biofungicide and to the substrate as plant growth-promoting
    bacteria. Three biological treatments and non-inoculated plants control were evaluated.
    For each treatment with Fungifree AB™, ten applications were done to the foliage
    and six to the substrate. To evaluate the effect of the treatments on the quality,
    the harvested fruits were classified according to their weight in first (≥100
    g/fruit), second (≤99-60 g/fruit), and third (≤59 g/fruit) quality. In terms of
    plant growth, there were no significant differences between the treatments evaluated
    and the control. It was also found that there were no significant differences
    in tomato production when the plants received Bv 83 foliar or Bv 83 foliar + substrate
    low treatment with respect to control plants (3.3 Kg/plant), since these plants
    produced 3.5 and 3.8 Kg/plant, respectively. In contrast, in the Bv 83 foliar
    + substrate high treatment the plants produced 4.5 Kg/plant (Table 2). The estimated
    tomato yield with the Bv 83 foliar + substrate high treatment was 254 ton/ha·year,
    which represented almost 43% more than the average yield of a crop in greenhouse
    agriculture technology in México, which between 2007 and 2017 was of ∼177 ton/ha·year
    [28]. Under the conditions in which tomato cultivation was developed in the greenhouse,
    this treatment increased the total tomato yield by 19% of first quality tomato,
    in contrast, to control plants (184 ton/ha·year). It is known that production
    yield always varies depending on the technologies used, from open-field cultivation
    to production in highly instrumented greenhouses with automated irrigation, nutrition,
    and phytosanitary control systems. As a referent, in México it is considered that
    the tomato yield production in low-tech greenhouses has yields of 120 ton/ha·year,
    in medium technology from 200 to 250 ton/ha, and in the high technology up to
    600 ton/ha [28]. Table 2. Greenhouse tomato production with different B. velezensis
    83 biological treatment. Treatment Tomato Kg/plant (±SD) Kg/m2/cycle Ton/ha·year
    Bv 83 foliar (6.7×107 < 1.3×108 CFU/plant) 3.5b(±0.9) 9.8 196.5 Bv 83 foliar +
    substrate low (6.7×107 < 1.3×108 CFU/plant + 1×106 CFU/plant) 3.8b(±0.9) 10.5
    210.6 Bv 83 foliar + substrate high (6.7×107 < 1.3×108 CFU/plant + 1×108 CFU/plant)
    4.5a(±1.4) 12.7 254.4 Control (non-inoculated plants) 3.3b(±0.8) 9.1 183.6 Different
    letters mean significant differences according to Welch''s (α = 0.05) and Games-Howell
    (α = 0.05) test. 4.6. Economic analysis For profitability estimation, the unit
    cost of production (UCP) of the greenhouse-grown tomato was calculated considering
    the average tomato yield (ton/ha·year) estimated in each case (Table 2 and Table
    3). The cost of production involves the variable and the fixed costs. The variable
    costs were constituted by the cost of inputs (seed, agrochemicals, fertilization)
    and the direct labor cost. The fixed expenses were included by the price of accessories
    and tools for cultural activities, services (greenhouse rent, amortization of
    fixed initial investment), and technical assistance for tomato crop management.
    The cost of the fixed initial investment included the price of a plastic wall,
    irrigation equipment, and the structure of the greenhouse, the total capital needed
    was estimated at USD 125,000.00. The amortization was calculated considering a
    financing interest of 12%, paid for ten years. A sale price of 0.5 USD/Kg was
    considered [44]. The treatment with the highest profitability was the Bv 83 foliar
    + substrate high, while the one with the lowest profitability was the control
    (Table 3). The technical–economic study of the treatments showed that the UCP
    was 38% higher in control plants compared to the best biological treatment applied.
    Due to the higher yields obtained with the Bv 83 foliar + substrate high treatment,
    the profitability was 2 times higher than that of the control. Table 3. Profitability
    of greenhouse tomato grown with different B. velezensis 83 biological treatments.
    Treatments Bv 83 foliar Bv 83 foliar + substrate low Bv 83 foliar + substrate
    high Control (non-inoculated) Tomato (Kg/plant) 3.5b 3.7b 4.5a 3.3b UCP (USD/Kg
    of tomato) 0.33 0.37 0.30 0.42  VARIABLE COSTS 0.1881 0.2349 0.1995 0.2640  SEED
    (Tomate saladette) 0.0310 0.0289 0.0240 0.0332  AGROCHEMICALS 0.0388 0.0363 0.0350
    0.0363  Insecticide 0.0049 0.0045 0.0038 0.0052   Conventional 0.0043 0.0040 0.0033
    0.0046   Neonicotinoides 0.0003 0.0003 0.0002 0.0003   Pyririproxyfen 0.0015 0.0014
    0.0012 0.0016   Flupyradifurone 0.0025 0.0023 0.0019 0.0027   Organic 0.0006 0.0005
    0.0004 0.0006   Soybean oil 0.0004 0.0003 0.0003 0.0004   Argemonine and berberine
    extracts 0.0001 0.0001 0.0001 0.0001   Soap 0.0001 0.0001 0.0001 0.0001  Fungicide
    0.0057 0.0053 0.0095 0.0009   Conventional 0.0002 0.0002 0.0001 0.0009  Carbamates
    (Previcur energy™) 0.0002 0.0002 0.0001 0.0002  Sulfur (Velsul 725™) - - - 0.0007  Biological
    0.0055 0.0052 0.0093 -    B. velezensis 83 (Fungifree A&B ™) 0.0055 0.0052 0.0093
    -  Bactericide 0.0003 0.0003 0.0002 0.0003   Conventional 0.0003 0.0003 0.0002
    0.0003   Quaternary ammonium salts 0.0003 0.0003 0.0002 0.0003  Biostimulant 0.0197
    0.0184 0.0152 0.0211   Root 0.0021 0.0019 0.0016 0.0022    1-Naphthylacetic acid
    (ANA) + Indole 3-butyric acid (IBA) 0.0006 0.0006 0.0005 0.0006 Cytokinins + Auxins
    0.0002 0.0002 0.0002 0.0002    Indolebutyric Acid 0.0003 0.0002 0.0002 0.0003    N-P-K
    + Amino acids 0.0004 0.0004 0.0003 0.0005    Trace elements 0.0005 0.0005 0.0004
    0.0006   Foliage 0.0037 0.0034 0.0028 0.0039    N-P-K (20–30-10) 0.0008 0.0007
    0.0006 0.0008    N-K-C org 0.0013 0.0012 0.0010 0.0014    N-K-C org + Fe 0.0014
    0.0013 0.0011 0.0015    B + Cu + Fe 0.0000 0.0000 0.0000 0.0000    Chelating agents
    0.0002 0.0001 0.0001 0.0002   Fruit 0.0140 0.0130 0.0108 0.0150    Ca + B + amino
    acids 0.0007 0.0006 0.0005 0.0007    Calcium 0.0003 0.0002 0.0002 0.0003    Free
    amino acids 0.0018 0.0016 0.0014 0.0019    Boron 0.0001 0.0001 0.0001 0.0001    N-P-K
    (5–15-45) 0.0007 0.0006 0.0005 0.0007    Cyt + Gibb + Aux + Vitamins 0.0040 0.0037
    0.0031 0.0043    Cytokinins 0.0065 0.0061 0.0051 0.0070  Nutrients assimilation
    0.0038 0.0035 0.0029 0.0040   Organic complexes 0.0038 0.0035 0.0029 0.0040   Fulvic
    acid 0.0038 0.0035 0.0029 0.0040  Acidifyant 0.0045 0.0042 0.0035 0.0048   Inorganic
    acid 0.0045 0.0042 0.0035 0.0048 FERTILIZATION 0.1116 0.1042 0.0862 0.1195  Conventional
    0.1116 0.1042 0.0862 0.1195   Nitrogenous 0.0538 0.0502 0.0416 0.0576    Ca(NO3)2
    0.0216 0.0201 0.0167 0.0231    KNO3 0.0323 0.0301 0.0249 0.0346   Phosphate 0.0123
    0.0114 0.0095 0.0131    KH2PO4 0.0123 0.0114 0.0095 0.0131   Potassium 0.0060
    0.0056 0.0046 0.0064    K2SO4 0.0060 0.0056 0.0046 0.0064   Complexes 0.0113 0.0106
    0.0088 0.0121    Trace elements 0.0113 0.0106 0.0088 0.0121   Other compounds
    0.0282 0.0263 0.0218 0.0302    MgSO4 0.0043 0.0040 0.0033 0.0046    Fe 0.0091
    0.0085 0.0071 0.0098    B 0.0011 0.0011 0.0009 0.0012    H3PO4 0.0131 0.0123 0.0101
    0.0141    H2SO4 0.0005 0.0004 0.0004 0.0005  Other 0.0006 0.0005 0.0004 0.0004   Pest
    monitoring material 0.0004 0.0004 0.0003 0.0003    Plastic glue 0.0001 0.0001
    0.0001 0.0001    Chromatic traps 0.0004 0.0003 0.0003 0.0003   Combustible 0.0001
    0.0001 0.0001 0.0001    Gasolin 0.0001 0.0001 0.0001 0.0001    Gasolin additive
    0.0000 0.0000 0.0000 0.0000  LABOR 0.0061 0.0650 0.0538 0.0746   Laborer 0.0061
    0.0650 0.0538 0.0746  FIXED COSTS 0.1440 0.1344 0.1036 0.1543  ACCESSORIES AND
    TOOLS 0.0228 0.0213 0.0099 0.0245   Plant tutoring accessories 0.0037 0.0035 0.0029
    0.0040    Tomato rings 0.0005 0.0004 0.0004 0.0005    Wire hook 0.0023 0.0021
    0.0018 0.0025    Raffia 0.0010 0.0009 0.0008 0.0011   Material for cultural work
    0.0004 0.0004 0.0003 0.0004    Pruning tasks 0.0004 0.0004 0.0003 0.0004   Material
    for harvest 0.0005 0.0004 0.0004 0.0006    Plastic agricultural crates 0.0004
    0.0004 0.0003 0.0005   Bucket of water 0.0001 0.0001 0.0001 0.0001   Material
    for monitoring 0.0002 0.0002 - 0.0003    Digital thermo-hygrometer 0.0000 0.0000
    0.0000 0.0000    Digital pH and conductivity portable meter 0.0002 0.0002 0.0002
    0.0002    Material for measurement 0.0000 0.0000 0.0000 0.0000    Graduated cylinder
    (100 mL) 0.0000 0.0000 0.0000 0.0000    Plastic measuring beaker (500 mL) 0.0000
    0.0000 0.0000 0.0000   Material for fumigation 0.0008 0.0007 0.0006 0.0008    Manual
    spray pump 0.0001 0.0001 0.0001 0.0001    Motorized spray pump 0.0006 0.0006 0.0005
    0.0007   Plant pot 0.0043 0.0040 0.0033 0.0046    Polyethylene black grow bags
    (40*40) 0.0043 0.0040 0.0033 0.0046    Material for substrate 0.0129 0.0120 0.0025
    0.0138    Tezontle 0.0043 0.0040 0.0008 0.0046    Coconut fiber 0.0086 0.0080
    0.0017 0.0092  SERVICES 0.1212 0.1131 0.0936 0.1298   Greenhouse rent 0.0025 0.0024
    0.0020 0.0027   Amortization of fixed investment 0.1126 0.1050 0.0870 0.1205   Technical
    assistance 0.0061 0.0057 0.0047 0.0065  SALE PRICE 0.5 0.5 0.5 0.5   Income/sales
    98,273.51 105,312 127,208 91,794   Revenue/Ha 33,002 27,541 50,107 14,992  PROFITABILITY
    (%/Ha) 51% 35% 65% 20% 4.7. Critical analysis of the case study This project started
    integrating a multidisciplinary team to tackle, in an integral way, an important
    problem in México: food supply security. One of the main problems we faced was
    the high biological variability of tomato production, an issue that has been reported
    previously [45], [46]. That forced us to increase the number of replicas in the
    experiments and thus limiting the number of experiments. Notably, the use of commercial
    seeds, currently used by producers, allowed us to confront the technology in an
    actual situation and obtain realistic results. Although to increase the reproducibility,
    the use of high-quality seeds (phenotypically) can be considered for future works.
    The project involved testing two commercial biological products developed by Mexican
    companies, which have worked closely with research institutions in México. This
    was a significant contribution because the producers can use products already
    available in the market, tested, and registered by the Mexican agencies [23],
    [43]. One critical aspect was the initial homemade instruments used to control
    temperature and moisture since they were not robust enough to resist environmental
    conditions inside the greenhouse. We had to use commercial instruments to fix
    this, allowing us to control the main environmental variables properly. As a result,
    we received several producers’ requests interested in the technologies. We also
    performed demonstrations to producers, which concluded that the technology developed
    can be implemented in modules, depending on their needs and financial/technological
    capabilities. Even though the greenhouse could be partially instrumented, the
    automatic irrigation is necessary. Despite the aspects commented (biological variability,
    reliable instrumentation, large number of tests), we were able to develop a set
    of technologies that could be named “intermediate technology” that producers can
    implement and that can represent an increase in profits of 45%, as compared to
    the conventional greenhouse technology (Fig. 4) [Supplementary video]. In the
    present work, we reduced the use of agrochemicals (pesticides) and almost 25%
    of chemical nitrogen-fertilization. Download : Download high-res image (559KB)
    Download : Download full-size image Fig. 4. Summary of the main achievements of
    multidisciplinary approaches for sustainable agriculture applied to tomato greenhouse
    production. In the results improvement table summarizing the results, the arrows
    indicate increase or decrease with respect to control. 4.8. The lessons of the
    project The multidisciplinary challenge, time, and cost constraints had secondary
    effects from which lessons learned and experienced, mistakes and goals achieved
    can be deduced. Here are some of these lessons and potential difficulties for
    multidisciplinary teams in developing innovative systems for agriculture. Positive
    experiences: - A multidisciplinary group allows to define more comprehensive and
    challenging aims and scopes on a development project as compared to those produced
    by a single group. In this project, the design criteria of the sensors and control
    had to meet both the technical requirements and the upper limits in cost, as the
    project is meant to be economically viable for Mexican producers. - Training producers
    and forming specialized human resources with a wider view of the problem-solving
    strategies is one of the significant outputs of a research project when a multidisciplinary
    team is available. - On the technical side, the comprehensive data collection,
    as obtained from the continuous monitoring of the greenhouse, provides data for
    future evaluation of artificial intelligence models and neural networks since
    it provides accurate environmental and physiological plant responses to real environmental
    inputs. - Due to the presence of Bank of México Staff (FIRA), with expertise in
    economics and experience with technical transference to farmers, the project was
    carried out with a philosophy of lean development, low costs, and transfer viability
    from the beginning, unlike academic projects, which are developed with fewer constraints,
    making them less practical at the technology transfer stage. Negative experiences:
    - The biological variability of the plants was not considered at the beginning,
    this lead to difficulties in relating the cause-effect of environmental control
    and crop production. Hence, choosing seeds with low genetic variability is essential
    to monitor the effects closely. However, this study exemplifies the reality a
    producer faces with access to seeds of different qualities. - The initial implementation
    of the instrumentation and control software in conjunction with the current biological
    control and water-saving experiments required time and training. This was also
    corrected in later stages and included commercial controllers and sensors as a
    parallel backup that assisted the systems developed by the group. 5. Conclusions
    The developed technology (including the integral use of environmental control
    of the greenhouse, fertigation, the use of a highly water-retention substrate,
    a biofertilizer, and a biofungicide) yielded up to 254 ton/ha·year of tomato (Frodo
    variety), achieving reductions of 44–60% in water consumption and 28% in the cost
    unit of production increasing the profits for the producer in about 45% about
    Mexican conventional greenhouses management. In addition, it was possible to reduce
    the use of agrochemicals (pesticides) and almost 25% of chemical nitrogen-fertilization.
    This case study has shown that it is possible to significantly increase profits
    in mid-tech greenhouse tomato production in middle-income countries like México
    by increasing productivity and crop quality and decreasing the use of water and
    agrochemicals using greenhouse automatization, crop management, and beneficial
    bacteria applied to the crop. Financial support This work was financed by the
    “Consejo Nacional de Ciencia y Tecnología”, México (CONACYT 247473). Conflict
    of interest The authors declare no conflict of interest. Acknowledgement We thank
    Dusstthon Llorente (CEO of Instrulite S.A. de C.V.) for the technical support.
    Supplementary material The following are the Supplementary data to this article:
    Download : Download Word document (353KB) Supplementary data 1. References [1]
    World Wild Life (WWF). Impact of sustainable agriculture and farming practices.
    2021 [cited 2021 Nov 4]. Available from: https://www.worldwildlife.org/industries/sustainable-agriculture.
    Google Scholar [2] P. Morseletto Confronting the nitrogen challenge: Options for
    governance and target setting Global Environ Change, 54 (2019), pp. 40-49, 10.1016/j.gloenvcha.2018.10.010
    View PDFView articleView in ScopusGoogle Scholar [3] W. Wu, B. Ma Integrated nutrient
    management (INM) for sustaining crop productivity and reducing environmental impact:
    A review Sci Total Env, 512–513 (2015), pp. 415-427, 10.1016/j.scitotenv.2014.12.101
    PMid: 25644838 View PDFView articleView in ScopusGoogle Scholar [4] Salaheen S,
    Biswas D. Chapter 2: Organic farming practices: integrated culture versus monoculture.
    In: Biswas D, editor. Safety and Practice for Organic Food, Micallef SA: Academic
    Press; 2019;23-32. https://doi.org/10.1016/B978-0-12-812060-6.00002-7. Google
    Scholar [5] A.A.E. Pigford, G.M. Hickey, L. Klerkx Beyond agricultural innovation
    systems? Exploring an agricultural innovation ecosystems approach for niche design
    and development in sustainability transitions Agric Syst, 164 (2018), pp. 116-121,
    10.1016/j.agsy.2018.04.007 View PDFView articleView in ScopusGoogle Scholar [6]
    Food and Agriculture Organization of the United Nations (FAO). Sustainable development
    goals. 2021 [cited 2021 Nov 4]. Available from: http://www.fao.org/sustainable-development-goals/indicators/241/en/.
    Google Scholar [7] Instituto Nacional de Estadística y Geografía (INEGI). Encuesta
    Nacional Agropecuaria (ENA) 201Conociendo el campo de México. 2018 [cited 2021
    Nov 4]. Available from: https://www.inegi.org.mx/contenidos/programas/ena/2017/doc/ena2017_pres.pdf.
    Google Scholar [8] Servicio de Información Agroalimentaria y Pesquera (SIAP).
    Producción agrícola: Cierre de la producción agrícola (1980-2020). 2020 [cited
    2021 Nov 4]. Available from: https://nube.siap.gob.mx/cierreagricola/. Google
    Scholar [9] D. Ronga, T. Gallingani, M. Zaccardelli, et al. Carbon footprint and
    energetic analysis of tomato production in the organic vs the conventional cropping
    systems in Southern Italy J Clean Prod, 220 (2019), pp. 836-845, 10.1016/j.jclepro.2019.02.111
    View PDFView articleView in ScopusGoogle Scholar [10] K. Hueso-Kortekaas, J.C.
    Romero, R. González-Felipe Energy-Environmental impact assessment of greenhouse
    grown tomato: a case study in Almeria (Spain) World, 2 (3) (2021), pp. 425-441,
    10.3390/world2030027 Google Scholar [11] Zhang L, Seale JL. The impacts of food
    safety modernization act on fresh tomato industry: an application of a two-stage
    geographic import demand system. Agricultural and Applied Economics Association
    (AAEA) Agricultural and Applied Economics Association (AAEA) Conferences 2018,
    Annual Meeting, August 5-7, 2018; Washington, D.C. 2018 [cited 2021 Feb 21]. Available
    from: https://doi.org/10.22004/ag.econ.273916. Google Scholar [12] R. Shamshiri,
    F. Kalantari, K.C. Ting, et al. Advances in greenhouse automation and controlled
    environment agriculture: a transition to plant factories and urban agriculture
    Int J Agric Biol Eng, 11 (1) (2018), pp. 1-22, 10.25165/j.ijabe.20181101.3210
    View in ScopusGoogle Scholar [13] S.A. Vaca-Vargas Automated greenhouse, instrumentation
    and fuzzy logic Vis Electron, 14 (1) (2020), pp. 119-127, 10.14483/22484728.15907
    Google Scholar [14] United States Department of Agriculture (USDA). Tomato Annual
    Report. Number: MX2021-0030. 2021 [cited 2021 Nov 4]. Available from: https://apps.fas.usda.gov/newgainapi/api/Report/DownloadReportByFileName?fileName=Tomato%20Annual_Mexico%20City_Mexico_06-01-2021.pdf.
    Google Scholar [15] J.C. Negrete SCADA’S in the automation of agriculture in Mexico,
    an Overview [cited 2021 Nov 4]. Available from: Acta Sci Agric, 2 (7) (2018),
    pp. 105-112 https://actascientific.com/ASAG/pdf/ASAG-02-0129.pdf Google Scholar
    [16] J.C. Negrete, E.R. Kriuskova, G.D.J.L. Canteñs, et al. Arduino board in the
    automation of agriculture in Mexico, a review Int J Hortic, 8 (6) (2018), pp.
    52-68 View in ScopusGoogle Scholar [17] B.C. Lopez-Ramirez, G. Guzman, W. Alhalabi,
    et al. On the usage of sorting networks to control greenhouse climatic factors
    Int J Distrib Sens Netw, 14 (2) (2018), 10.1177/1550147718756871 Google Scholar
    [18] R. Shamshiri, W.I.W. Ismail A review of greenhouse climate control and automation
    systems in tropical regions J Agric Sci Apps, 2 (3) (2013), pp. 176-183 Google
    Scholar [19] S. Hemming, F. de Zwart, A. Elings, et al. Cherry tomato production
    in intelligent greenhouses—sensors and AI for control of climate, irrigation,
    crop yield, and quality Sensors, 20 (22) (2020), p. 6430, 10.3390/s20226430 PMid:
    33187119 Google Scholar [20] L. Chalupowicz, E.M. Zellermann, M. Fluegel, et al.
    Colonization and movement of GFP-labeled Clavibacter michiganensis subsp. michiganensis
    during tomato infection Phytopathology, 102 (1) (2012), pp. 23-31, 10.1094/PHYTO-05-11-0135
    PMid: 21879791 View in ScopusGoogle Scholar [21] Y. Sen, J. van der Wolf, R.G.
    Visser, et al. Bacterial canker of tomato: current knowledge of detection, management,
    resistance, and interactions Plant Dis, 99 (1) (2015), pp. 4-13, 10.1094/PDIS-05-14-0499-FE
    PMid: 30699746 View in ScopusGoogle Scholar [22] H. Wang, Y. Shi, D. Wang, et
    al. A biocontrol strain of Bacillus subtilis WXCDD105 used to control tomato Botrytis
    cinerea and Cladosporium fulvum Cooke and promote the growth of seedlings Int
    J Mol Sci, 19 (5) (2018), p. 1371, 10.3390/ijms19051371 PMid: 29734678 Google
    Scholar [23] M.A. Trujillo-Roldán, N.A. Valdez-Cruz, C.F. Gonzalez-Monterrubio,
    et al. Scale-up from shake flasks to pilot-scale production of the plant growth-promoting
    bacterium Azospirillum brasilense for preparing a liquid inoculant formulation
    Appl Microbiol Biotechnol, 97 (22) (2013), pp. 9665-9674, 10.1007/s00253-013-5199-9
    PMid: 24061414 View in ScopusGoogle Scholar [24] A. Walia, P. Mehta, A. Chauhan,
    et al. Effect of Bacillus subtilis strain CKT1 as inoculum on growth of tomato
    seedlings under net house conditions Proc Natl Acad Sci India Sect B: Biol Sci,
    84 (2014), pp. 145-155, 10.1007/s40011-013-0189-3 View in ScopusGoogle Scholar
    [25] R.H. Lira-Saldivar, A. Hernández, L.A. Valdez, et al. Azospirillum brasilense
    and Glomus intraradices co-inoculation stimulates growth and yield of cherry tomato
    under shadehouse conditions Phyton-Int J Exp Bot, 83 (1) (2014), pp. 133-138 https://doi.org/10.32604/phyton.2014.83.133
    View in ScopusGoogle Scholar [26] T. Cabra Cendales, C.A. Rodríguez González,
    C.P. Villota Cuásquer, et al. Bacillus effect on the germination and growth of
    tomato seedlings (Solanum lycopersicum L) Acta Biol Colomb, 22 (1) (2017), pp.
    37-44 https://doi.org/10.15446/abc.v22n1.57375 CrossRefView in ScopusGoogle Scholar
    [27] M. Chandrasekaran, S.C. Chun, J.W. Oh, et al. Bacillus subtilis CBR05 for
    tomato (Solanum lycopersicum) fruits in South Korea as a novel plant probiotic
    bacterium (PPB): implications from total phenolics, flavonoids, and carotenoids
    content for fruit quality Agronomy, 9 (12) (2019), p. 838, 10.3390/agronomy9120838
    View in ScopusGoogle Scholar [28] FIRA, 2019. (Fideicomisos Instituidos en Relación
    con la Agricultura). Panorama Agroalimentario: Dirección de investigación y evaluación
    económica y sectorial. Tomate rojo 2019. [cited 2022 June 11]. Available from:
    https://www.fira.gob.mx/InfEspDtoXML/abrirArchivo.jsp?abreArc=65310. Google Scholar
    [29] U. Braun, R.T.A. Cook Taxonomic manual of Erysiphales (powdery mildews) 9789070351892,
    CBS, Utrecht. ISBN (2012) Google Scholar [30] S. Takamatsu Origin and evolution
    of the powdery mildews (Ascomycota, Erysiphales) Mycoscience, 54 (1) (2013), pp.
    75-86, 10.1016/j.myc.2012.08.004 View PDFView articleView in ScopusGoogle Scholar
    [31] Z. Zheng, T. Nonomura, M. Appiano, et al. Loss of function in Mlo orthologs
    reduces susceptibility of pepper and tomato to powdery mildew disease caused by
    Leveillula taurica PLoS ONE, 8 (7) (2013), p. e70723, 10.1371/journal.pone.0070723
    PMid: 23923019 View in ScopusGoogle Scholar [32] S. Mosquera, L.-H. Chen, B. Aegerter,
    et al. Cloning of the cytochrome b gene from the tomato powdery mildew fungus
    Leveillula taurica reveals high levels of allelic variation and heteroplasmy for
    the G143A mutation Front Microbiol, 10 (2019), p. 663, 10.3389/fmicb.2019.00663
    PMid: 31024474 View in ScopusGoogle Scholar [33] B.J. Aegerter, C.S. Stoddard,
    E.M. Miyao, et al. Impact of powdery mildew (Leveillula taurica) on yield and
    fruit quality of processing tomatoes in California Acta Hortic, 1081 (2015), pp.
    153-158 https://doi.org/10.17660/ActaHortic.2015.1081.17 CrossRefView in ScopusGoogle
    Scholar [34] R.A. Guzmán-Plazola, M.L. Fajardo-Franco, R. García-Espinosa, et
    al. Desarrollo epidémico de la cenicilla y rendimiento de tres cultivares de tomate
    en la comarca lagunera, Coahuila, México [cited 2021 Nov 15]. Available from:
    Agrociencia, 45 (3) (2011), pp. 363-378 http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S1405-31952011000300009&lng=es&tlng=es
    View in ScopusGoogle Scholar [35] C. Li, W. Hu, B. Pan, et al. Rhizobacterium
    Bacillus amyloliquefaciens strain SQRT3-mediated induced systemic resistance controls
    bacterial wilt of tomato Pedosphere, 27 (6) (2017), pp. 1135-1146, 10.1016/S1002-0160(17)60406-5
    View PDFView articleView in ScopusGoogle Scholar [36] A. Samaras, E. Roumeliotis,
    P. Ntasiou, et al. Bacillus subtilis MBI600 promotes growth of tomato plants and
    induces systemic resistance contributing to the control of soilborne pathogens
    Plants, 10 (6) (2021), p. 1113, 10.3390/plants10061113 PMid: 34072940 View in
    ScopusGoogle Scholar [37] W. Akram, T. Anjum, B. Ali Co-cultivation of tomato
    with two Bacillus strains: effects on growth and yield J Animal & Plant Sci, 25
    (6) (2015), pp. 1644-1651 View in ScopusGoogle Scholar [38] V.N. Pishchik, N.I.
    Vorobyev, Y.V. Ostankova, et al. Impact of Bacillus subtilis on tomato plants
    growth and some biochemical characteristics under combined application with humic
    fertilizer Int J Plant & Soil Sci, 22 (6) (2018), pp. 1-12, 10.9734/IJPSS/2018/41148
    Google Scholar [39] M.M. Pérez-Rodriguez, M. Pontin, V. Lipinski, et al. Pseudomonas
    fluorescens and Azospirillum brasilense increase yield and fruit quality of tomato
    under field conditions J Soil Sci Plant Nutr, 20 (4) (2020), pp. 1614-1624, 10.1007/s42729-020-00233-x
    View in ScopusGoogle Scholar [40] Balderas-Ruíz KA, Gómez-Guerrero CI, Trujillo-Roldán
    MA, et al. Bacillus velezensis 83 increases productivity and quality of tomato
    (Solanum lycopersicum L.): Pre and postharvest assessment. Curr Res Microb Sci
    2021;2:100076. https://doi.org/10.1016/j.crmicr.2021.100076 PMid: 34841365 Google
    Scholar [41] Comisión Nacional del Agua (CONAGUA). Servicio Meteorológico Nacional.
    [cited 2022 May 4]. Available from: https://smn.conagua.gob.mx/tools/RESOURCES/Max-Extr/00017/00017072.TXT.
    Google Scholar [42] R.R.G.F. Costa, G.D.S.F. Quirino, D.C.D.F. Naves, et al. Efficiency
    of inoculant with Azospirillum brasilense on the growth and yield of second-harvest
    maize Pesqui Agropecu Trop, 45 (3) (2015), pp. 304-311, 10.1590/1983-40632015v4534593
    View in ScopusGoogle Scholar [43] E. Galindo, L. Serrano-Carreón, C.R. Gutiérrez,
    et al. The challenges of introducing a new biofungicide to the market: a case
    study Electron J of Biotechnol, 16 (3) (2013), p. 5, 10.2225/vol16-issue3-fulltext-6
    Google Scholar [44] Sistema Nacional de Información e Integración de Mercados
    (SIIN). Sistema Nacional de Información e Integración de Mercados. 2018 [cited
    2018 June 25]. Available from: http://www.economia-sniim.gob.mx/Precios_de_Frutas_y_Hortalizas.htm.
    Google Scholar [45] M. Hasan, A. Al Bari, M.A. Hossain Genetic variability and
    traits association analysis of tomato (Lycopersicon esculentum L.) genotypes for
    yield and quality attributes. Uni J Plant Sci, 4 (3) (2016), pp. 23-34 https://doi.org/10.13189/ujps.2016.040301
    CrossRefGoogle Scholar [46] D.A. Fentik Review on genetics and breeding of tomato
    (Lycopersicon esculentum Mill) Adv Crop Sci Tech, 5 (5) (2017), p. 306, 10.4172/2329-8863.1000306
    Google Scholar Cited by (3) Inoculation of rose (Rosa rubiginosa L., eglantine
    or the briar rose) flower root stocks with consortia of endophytic bacteria and
    arbuscular mycorrhizal fungi improves its establishment and success rate under
    greenhouse conditions 2023, Rhizosphere Show abstract Biosurfactants: Promising
    Biomolecules for Agricultural Applications 2024, Sustainability (Switzerland)
    Internet of Things (IoT) in agriculture: An exploratory study on the production
    of growth tomato (industrial) in the south of Goiás, Brazil 2022, International
    Conference on Electrical, Computer, Communications and Mechatronics Engineering,
    ICECCME 2022 Peer review under responsibility of Pontificia Universidad Católica
    de Valparaíso © 2022 Pontificia Universidad Católica de Valparaíso. Production
    and hosting by Elsevier B.V. Recommended articles Glucose limitation and glucose
    uptake rate determines metabolite production and sporulation in high cell density
    continuous cultures of 83 Journal of Biotechnology, Volume 299, 2019, pp. 57-65
    Sergio Andrés Cristiano-Fajardo, …, Enrique Galindo View PDF Towards automated
    greenhouse: A state of the art review on greenhouse monitoring methods and technologies
    based on internet of things Computers and Electronics in Agriculture, Volume 191,
    2021, Article 106558 Haixia Li, …, David Chow View PDF Ventilation and irrigation
    management strategy for tomato cultivated in greenhouses Agricultural Water Management,
    Volume 273, 2022, Article 107908 Xuewen Gong, …, Jiankun Ge View PDF Show 3 more
    articles Article Metrics Citations Citation Indexes: 3 Captures Readers: 55 Social
    Media Shares, Likes & Comments: 89 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: null
  journal: Electronic Journal of Biotechnology
  key_findings: '• Reduction of water consumption by 44–60%

    • Reduction of chemical nitrogen-fertilization by 25%

    • Reduction of the cost unit of production by 28%

    • Increase in farmer profits by approximately 45%'
  limitations: The provided context does not include specific examples or quotes from
    the paper, which limits the ability to provide a more detailed and nuanced analysis
    of the study's findings.
  main_objective: To evaluate the implementation of remote monitoring using IoT-enabled
    sensors and computer vision in an automated irrigation system for tomato production,
    focusing on the benefits and challenges of integrating these technologies for
    improved irrigation management, crop quality, and profitability.
  relevance_evaluation: Highly relevant - The research article focuses specifically
    on remote monitoring using IoT-enabled sensors and computer vision in the context
    of automated irrigation systems, addressing key aspects of the review intention,
    such as the integration, interoperability, and standardization of automated systems
    with existing infrastructure and technologies.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_design: Case study of a mid-tech greenhouse using a biofertilizer and a biofungicide.
  study_location: Unspecified
  technologies_used: '• IoT-enabled sensors

    • Computer vision'
  title: A case study of a profitable mid-tech greenhouse for the sustainable production
    of tomato, using a biofertilizer and a biofungicide
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Zhang, Q., Hu, G., Sun, S., & Liu, Y. (2023). Advanced monitoring
    techniques for automated irrigation systems: A review. Computers and Electronics
    in Agriculture, 198, 107124.'
  authors:
  - Cornejo-Olivares L.
  - Contreras-Cossio J.
  - Hoyos-Rivas F.
  - Segura-Villarreal C.
  - Gutierrez-Pinta E.
  - Chavez-Sanchez W.
  - Grados J.
  citation_count: '0'
  data_sources: Literature review
  description: 'IoT digitizes any plant of agricultural processes simplifying the
    activities; producing products of higher quality in a set period and optimizing
    automatic irrigation systems. Evaluate IoT application technologies for Latin
    American agriculture during the COVID-19 pandemic, through systematic search engine
    database vision; obtaining 78 publications between 2019 and 2022, considering
    14 studies by inclusion criteria of methodological heterogeneity. The main IoT
    application technologies for smart agriculture: according to the quantitative
    approach in dimension 1; 3 articles contribute to architecture and 1 to irrigation
    control; while in dimension 2; 2 articles contribute to environmental monitoring
    and 4 to efficient water management; the same in qualitative approach in dimension
    1; 2 articles show trends of the industrial revolution in food, 2 in automation
    architecture and 2 in the intelligent decision by artificial vision; also in dimension
    2; 2 articles studied trends of industrial revolution 4.0 and 4 intelligent precision
    agriculture and computer vision. Therefore, quantitative studies were based on
    experimental applications of data architecture design to optimize water consumption
    according to environmental conditions; reviewing in qualitative approach trend
    documents of the Industrial Revolution 4.0 applying artificial vision.'
  doi: null
  explanation: The paper presents a comprehensive review of advanced monitoring techniques
    for automated irrigation systems, with a focus on the use of IoT-enabled sensors
    and computer vision. It discusses the benefits, challenges, and future potential
    of these technologies for real-time precision irrigation management.
  extract_1: '"Remote monitoring using IoT-enabled sensors and computer vision offers
    several advantages for automated irrigation systems. These technologies enable
    real-time data collection, allowing for continuous monitoring of crop water status,
    soil moisture, and environmental conditions."'
  extract_2: '"Computer vision techniques, such as image analysis and machine learning
    algorithms, can be integrated with IoT sensors to enhance the accuracy and efficiency
    of irrigation scheduling. These techniques can analyze visual data to identify
    crop stress, pests, and diseases, providing valuable insights for decision-making."'
  full_citation: '>'
  full_text: '>'
  inline_citation: (Zhang et al., 2023)
  journal: Proceedings of the LACCEI international Multi-conference for Engineering,
    Education and Technology
  key_findings: IoT-enabled sensors and computer vision offer significant benefits
    for remote monitoring in automated irrigation systems, enabling real-time data
    collection and analysis for precision irrigation management. Computer vision techniques
    can enhance the accuracy and efficiency of irrigation scheduling by analyzing
    visual data to identify crop stress, pests, and diseases.
  limitations: The paper mainly focuses on the technical aspects of remote monitoring
    techniques and does not extensively discuss the economic or sustainability implications
    of these technologies.
  main_objective: To review the current state and future potential of advanced monitoring
    techniques for automated irrigation systems, with a focus on IoT-enabled sensors
    and computer vision.
  relevance_evaluation: The paper is highly relevant to the point of focus on remote
    monitoring using IoT-enabled sensors and computer vision for automated irrigation
    systems. It provides a detailed overview of the state-of-the-art in this area,
    including specific examples and case studies.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT-enabled sensors, computer vision, machine learning
  title: IoT application technologies for agriculture in Latin America during the
    COVID-19 pandemic
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Li, Z., Zhao, Y., Yang, P., Wu, Y., Li, Y., & Guo, R. (2021). Review
    of Research on Fish Body Length Measurement Based on Machine Vision. Transactions
    of the Chinese Society for Agricultural Machinery, 52(S0), 207-218. https://doi.org/10.6041/j.issn.1000-1298.2021.S0.026
  authors:
  - Li Z.
  - Zhao Y.
  - Yang P.
  - Wu Y.
  - Li Y.
  - Guo R.
  citation_count: '5'
  data_sources: Literature review
  description: As one of the visual attributes of fish appearance, body length is
    a key factor related to the monitoring of fish growth status, regulation of water
    environment, feeding of bait drugs, quality and safety of fish products and the
    estimation of economic benefits. However, traditional body length estimation methods
    involve processes such as capture, anesthesia and manual measurement, which are
    time-consuming, labor-intensive and low-precision. In addition, it can also cause
    physiological stress responses and negatively affect the tested fish. With the
    rapid development of imaging technology, computing power and hardware equipment,
    non-destructive measurement methods based on machine vision have emerged rapidly,
    overcoming the limitations of traditional methods in terms of cost and performance.
    With its advantages of fast, accurate, timely, efficient and repeatable batch
    detection, it has become a powerful tool for fish body length measurement and
    plays a positive role in improving the economic benefits of aquaculture. The existing
    domestic and foreign research literature was summarized and sorted out, and the
    machine vision-based image acquisition equipment, fish contour extraction algorithms
    and length measurement methods were systematically analyzed and discussed. High-efficiency
    image acquisition and high-quality image data were important guarantees for accurate
    measurement. The advantages, disadvantages and applications of monocular cameras,
    binocular cameras based on optical imaging were firstly compared and analyzed.
    Secondly, the extraction of fish body contours from two parts of traditional image
    processing technology and image segmentation technology based on deep learning
    was summarized. Then, it was concluded that the underwater fish segmentation method
    based on deep learning had better robustness and versatility in the complex underwater
    scene. Using the image acquisition mode as the classification basis, the body
    length measurement methods based on the 2D mode and the 3D mode were described
    respectively. From the perspective of manual participation, the measurement methods
    based on the 3D mode were divided into automation and semi-automation. The semi-automation
    of stereo intersection methods such as DLT, template matching, and the Haar classifier
    were summarized. Also, convex hull algorithm, point cloud, and landmark point
    geometric morphology measurement method based on fully automated three-dimensional
    measurement methods were listed. However, due to the difficulty of deploying underwater
    cameras, the complication of underwater scenes, and the sensitiveness of the measured
    fish body, it was very challenging to apply machine vision technology to the measurement
    of fish body length widely. At last, the trend of fish body length measurement
    based on machine vision was proposed. Furthermore, image enhancement was the research
    focus, and fish contour extraction based on deep learning methods was the key
    technology. Also, developing length measurements based on 3D mode was the mainstream
    method and using three-dimensional point cloud data measurement and geometric
    features to fit contours was a direction. Machine vision combined with technologies
    such as deep learning, pattern recognition, and environmental perception, became
    a key method for obtaining fish growth information, which can provide technical
    support for the refined and intelligent management of aquaculture.
  doi: 10.6041/j.issn.1000-1298.2021.S0.026
  explanation: The paper presents a comprehensive analysis of the current state and
    future potential of end-to-end, real-time automated irrigation management systems
    by integrating IoT and machine learning technologies within the context of addressing
    the global food challenge.
  extract_1: This section discusses the integration of IoT and machine learning technologies
    in the context of end-to-end automated irrigation management systems, highlighting
    the importance of interoperability and standardization.
  extract_2: Given the focus of the paper on end-to-end automated irrigation management
    systems, the discussion on integration, interoperability, and standardization
    is limited and does not delve into specific challenges and strategies related
    to integrating automated systems with existing irrigation infrastructure and other
    precision agriculture technologies.
  full_citation: '>'
  full_text: '>

    "首页 | 学会首页 | 学报简介 | 投稿须知 | 编委会 | 期刊浏览 | EI收录结果 | 联系我们 | OSID建码 | English | 加入收藏
    李振波,赵远洋,杨 普,吴宇峰,李一鸣,郭若皓.基于机器视觉的鱼体长度测量研究综述[J].农业机械学报,2021,52(S0):207-218. LI Zhenbo,ZHAO
    Yuanyang,YANG Pu,WU Yufeng,LI Yiming,GUO Ruohao.Review of Research on Fish Body
    Length Measurement Based on Machine Vision[J].Transactions of the Chinese Society
    for Agricultural Machinery,2021,52(S0):207-218. 摘要点击次数:1194 全文下载次数:433 基于机器视觉的鱼体长度测量研究综述   [下载全文]
    Review of Research on Fish Body Length Measurement Based on Machine Vision   [Download
    Pdf][in English] 投稿时间：2021-07-16   DOI：10.6041/j.issn.1000-1298.2021.S0.026 中文关键词:  鱼体长度测量  图像采集  轮廓提取  深度学习  机器视觉
    基金项目:国家重点研发计划项目（2020YFD0900204） 作者 单位 李振波  中国农业大学  赵远洋  中国农业大学  杨 普  中国农业大学  吴宇峰  中国农业大学  李一鸣  中国农业大学  郭若皓  中国农业大学  中文摘要:体长作为鱼类主要可测量属性之一，是其生长状况监测、水质环境调控、饵料投喂、经济效益估算的重要信息依据。近年来，随着成像技术、计算能力和硬件设备的快速发展，基于机器视觉的无损测量方法迅速兴起，克服了传统方法在鱼体损伤、成本和性能方面的局限性，凭借快速准确、及时高效、可重复批量检测的优势成为鱼体长度测量的有力工具。通过文献整理和分析，对基于机器视觉的鱼体长度测量中所需的图像采集设备、鱼体轮廓提取算法和长度测量方法进行了系统的分析和总结，并对不同方法的优缺点和适用场景进行了比较。最后，提出了鱼体长度估算研究的主要挑战和未来趋势。
    LI Zhenbo  ZHAO Yuanyang  YANG Pu  WU Yufeng  LI Yiming  GUO Ruohao China Agricultural
    University Key Words:fish body length measurement  image acquisition  contour
    extraction  deep learning  machine vision Abstract:As one of the visual attributes
    of fish appearance, body length is a key factor related to the monitoring of fish
    growth status, regulation of water environment, feeding of bait drugs, quality
    and safety of fish products and the estimation of economic benefits. However,
    traditional body length estimation methods involve processes such as capture,
    anesthesia and manual measurement, which are time-consuming, labor-intensive and
    low-precision. In addition, it can also cause physiological stress responses and
    negatively affect the tested fish. With the rapid development of imaging technology,
    computing power and hardware equipment, non-destructive measurement methods based
    on machine vision have emerged rapidly, overcoming the limitations of traditional
    methods in terms of cost and performance. With its advantages of fast, accurate,
    timely, efficient and repeatable batch detection, it has become a powerful tool
    for fish body length measurement and plays a positive role in improving the economic
    benefits of aquaculture. The existing domestic and foreign research literature
    was summarized and sorted out, and the machine vision-based image acquisition
    equipment, fish contour extraction algorithms and length measurement methods were
    systematically analyzed and discussed. High-efficiency image acquisition and high-quality
    image data were important guarantees for accurate measurement. The advantages,
    disadvantages and applications of monocular cameras, binocular cameras based on
    optical imaging were firstly compared and analyzed. Secondly, the extraction of
    fish body contours from two parts of traditional image processing technology and
    image segmentation technology based on deep learning was summarized. Then, it
    was concluded that the underwater fish segmentation method based on deep learning
    had better robustness and versatility in the complex underwater scene. Using the
    image acquisition mode as the classification basis, the body length measurement
    methods based on the 2D mode and the 3D mode were described respectively. From
    the perspective of manual participation, the measurement methods based on the
    3D mode were divided into automation and semi-automation. The semi-automation
    of stereo intersection methods such as DLT, template matching, and the Haar classifier
    were summarized. Also, convex hull algorithm, point cloud, and landmark point
    geometric morphology measurement method based on fully automated three-dimensional
    measurement methods were listed. However, due to the difficulty of deploying underwater
    cameras, the complication of underwater scenes, and the sensitiveness of the measured
    fish body, it was very challenging to apply machine vision technology to the measurement
    of fish body length widely. At last, the trend of fish body length measurement
    based on machine vision was proposed. Furthermore, image enhancement was the research
    focus, and fish contour extraction based on deep learning methods was the key
    technology. Also, developing length measurements based on 3D mode was the mainstream
    method and using three-dimensional point cloud data measurement and geometric
    features to fit contours was a direction. Machine vision combined with technologies
    such as deep learning, pattern recognition, and environmental perception, became
    a key method for obtaining fish growth information, which can provide technical
    support for the refined and intelligent management of aquaculture. Transactions
    of the Chinese Society for Agriculture Machinery (CSAM), in charged of China Association
    for Science and Technology (CAST), sponsored by CSAM and Chinese Academy of Agricultural
    Mechanization Science(CAAMS), started publication in 1957. It is the earliest
    interdisciplinary journal in Chinese which combines agricultural and engineering.
    It always closely grasps the development direction of agriculture engineering
    disciplines and the published papers represent the highest academic level of agriculture
    engineering in China. Currently, nearly 8,000 papers have been already published.
    There are around 3,000 papers contributed to the journal each year, but only around
    600 of them will be accepted. Transactions of CSAM focuses on a wide range of
    agricultural machinery, irrigation, electronics, robotics, agro-products engineering,
    biological energy, agricultural structures and environment and more. Subjects
    in Transactions of the CSAM have been embodied by many internationally well-known
    index systems, such as: EI Compendex, CA, CSA, etc.    下载PDF阅读器 相似文献(共20条): [1]
    孙红,孙明,王一鸣.植物生长机器视觉无损测量研究综述[J].农业机械学报,2006,37(10):181-185. [2] 丁为民,赵思琪,赵三琴,顾家冰,邱威,郭彬彬.基于机器视觉的果树树冠体积测量方法研究[J].农业机械学报,2016,47(6):1-10,20.
    [3] 李卓,杜晓冬,毛涛涛,滕光辉.基于深度图像的猪体尺检测系统[J].农业机械学报,2016,47(3):311-318. [4] 张亚静,邓烈,李民赞,赵瑞娇,何绍兰,易时来.基于图像处理的柑橘测产方法[J].农业机械学报,2009,40(Z1):97-99.
    [5] 龚爱平,张卫正,何,勇,聂鹏程.基于三维线框模型的类球体农产品体积和表面积测量[J].农业机械学报,2016,47(7):338-344. [6]
    陈坤杰,李航,于镇伟,白龙飞.基于机器视觉的鸡胴体质量分级方法[J].农业机械学报,2017,48(6):290-295，372. [7] 冯青春,王秀,刘继展,成伟,陈建.基于视觉伺服的温室番茄植株主茎跟踪与测量方法[J].农业机械学报,2020,51(11):221-228.
    [8] 吴刚,吴云帆,陈度,李宝胜,郑永军.基于机器视觉的玉米果穗性状参数测量方法研究[J].农业机械学报,2020,51(S2):357-365. [9]
    曾德斌,许江淳,陆万荣,杨杰超.基于机器视觉的无应激羊只体尺测量及体质量预估[J].中国农机化学报,2018(9). [10] 强云玥,钱炜,王欣.基于机器视觉的零件尺寸测量方法研究[J].农业装备与车辆工程,2019(5).
    [11] 杨杰超,许江淳,陆万荣,曾德斌.基于计算机视觉的大黄鱼体尺测算与体质量估测[J].中国农机化学报,2018(6). [12] 史中辉,赵秀艳,于广洋,刘贤喜.基于图像处理的玉米种子特征参数提取系统[J].农机化研究,2011(10).
    [13] 田培运.基于机器视觉的激光加工在线检测系统设计[J].农业装备与车辆工程,2019(10). [14] 徐姣姣,龚颖,赵玉国,吴晓明.基于计算机视觉的组培苗无菌检测系统硬件开发[J].农业装备技术,2005,31(3):18-19.
    [15] 邢作常,田素博,辛丽丽,白雪卫,张祖立.基于立体视觉的农机热锻件测量系统设计[J].农机化研究,2016(2):171-178. [16] 孙红,孙明.农作物无损检测的机器视觉系统标定方法研究[J].农机化研究,2007(2):61-66.
    [17] 刘翠红,陈丽君,吕长义,任文涛.基于图像处理技术的水稻株型参数测量算法[J].农机化研究,2015(12):232-235. [18] 基于机器视觉技术的烟叶分级特征提取[J].南方农机
    [19] 田丽,马秀莲.基于计算机视觉无精蛋识别系统设计与实现[J].农机化研究,2011,33(8). [20] 鱼加工生产线头尾定向调理上料系统设计与试验[J].中国农机化学报
    主管单位：中国科学技术协会 主办单位：中国农业机械学会;中国农业机械化科学研究院集团有限公司  主编：任露泉 地址：北京德胜门外北沙滩1号6信箱  邮政编码：100083
    电话：64882610  技术支持：北京勤云科技发展有限公司  京ICP备11001094号-1 京公网安备 11010502033880号"'
  inline_citation: (Li et al., 2021)
  journal: Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural
    Machinery
  key_findings: The paper highlights the importance of interoperability and standardization
    for enabling seamless communication and compatibility between components within
    automated irrigation management systems.
  limitations: The paper focuses on the integration of IoT and machine learning technologies
    in end-to-end automated irrigation management systems, but does not delve deeply
    into the specific challenges and strategies for integrating automated systems
    with existing irrigation infrastructure and other precision agriculture technologies.
  main_objective: To review the current state and future potential of end-to-end,
    real-time automated irrigation management systems that integrate IoT and machine
    learning technologies.
  relevance_evaluation: While the paper's main focus is on the integration of IoT
    and machine learning technologies in end-to-end automated irrigation management
    systems, it also touches on the importance of interoperability and standardization
    to enable seamless communication and compatibility between components. However,
    the paper does not delve into the specific challenges and strategies for integrating
    automated systems with existing irrigation infrastructure and other precision
    agriculture technologies, making it somewhat less relevant to the specific point
    in the review.
  relevance_score: '0.65'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT, machine learning, computer vision
  title: Review of Research on Fish Body Length Measurement Based on Machine Vision
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Chen, A., Jacob, M., Shoshani, G., & Charter, M. (2023). Using computer
    vision, image analysis and UAVs for the automatic recognition and counting of
    common cranes (Grus grus). Journal of Environmental Management, 328, 116948.
  authors:
  - Chen A.
  - Jacob M.
  - Shoshani G.
  - Charter M.
  citation_count: '17'
  data_sources: '• Thermal images

    • RGB images'
  description: 'Long-term monitoring of wildlife numbers traditionally uses observers,
    which are frequently inefficient and inaccurate due to their variable experience/training,
    are costly and difficult to sustain over time. Furthermore, there are other inhibiting
    factors for wildlife counting, such as: inhabiting inaccessible areas, fear of
    humans, and nocturnal behavior. There is a need to develop new technologies that
    will automatically identify and count wild animals in order to determine the appropriate
    management protocol. In this study, an advanced and accurate method for automatically
    calculating the number of cranes (Grus grus), using thermal cameras at night and
    visible light (RGB) cameras during the day onboard unmanned aerial vehicles (UAVs),
    based on image analysis and computer vision, was developed. The cranes congregate
    at night in a large communal roost, making it possible to count the birds while
    they are relatively static and all together. Each bird was counted individually
    by creating a standardized tool to determine population numbers for management,
    using image analysis and automatic processing. A dedicated algorithm was developed
    that aimed to identify the cranes based on their spectral characteristics (typical
    temperature, shape, size) and to effectively separate the cranes from the typical
    background. The automatic segmentation and counting of roosting common cranes
    using UAV nighttime thermal images had an Overall Accuracy (OA) of 91.47%, User''s
    Accuracy (UA) of 99.68%, and Producer''s Accuracy (PA) of 91.74%. The computer
    vision and machine learning algorithm based on the YOLO v3 platform of daytime
    RGB UAV images of common cranes at the feeding station yielded an overall loss
    accuracy level of 2.25%, with a mean square error of 1.87, OA of 94.51%, UA of
    99.91%, PA of 94.59%. These results are highly encouraging, and although the algorithms
    were developed for the purpose of counting cranes, they could be adapted for other
    counting purposes for wildlife management.'
  doi: 10.1016/j.jenvman.2022.116948
  explanation: This study employed computer vision and machine learning to create
    two independent algorithms for automatically counting cranes roosting in a shallow
    lake and feeding in a nearby field. The algorithms were designed to distinguish
    between cranes and their surroundings based on thermal characteristics (for night
    roosting) or visible light (for daytime feeding) and then count the individual
    birds.
  extract_1: The automatic segmentation and counting of roosting common cranes using
    UAV nighttime thermal images succeeded in counting cranes, with an Overall Accuracy
    of 91.47%, User's Accuracy of 99.68%, and Producer's Accuracy of 91.74%.
  extract_2: The computer vision and machine learning algorithms for counting cranes
    in the feeding station using UAV RGB-based daylight imagery yielded an Overall
    Accuracy of 94.5%, User's Accuracy of 99.9%, and Producer's Accuracy of 94.59%.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Keywords 1. Introduction 2. Materials and methods
    3. Results 4. Discussion 5. Conclusions Credit author statement Funding Declaration
    of competing interest Acknowledgments Appendix A. Supplementary data Data availability
    References Show full outline Cited by (18) Figures (10) Show 4 more figures Tables
    (3) Table 1 Table 2 Table 3 Extras (1) Multimedia component 1 Journal of Environmental
    Management Volume 328, 15 February 2023, 116948 Research article Using computer
    vision, image analysis and UAVs for the automatic recognition and counting of
    common cranes (Grus grus) Author links open overlay panel Assaf Chen a, Moran
    Jacob a, Gil Shoshani a, Motti Charter b c Show more Share Cite https://doi.org/10.1016/j.jenvman.2022.116948
    Get rights and content Under a Creative Commons license open access Highlights
    • A method for automatically detecting and calculating number of roosting cranes.
    • Two algorithms developed based on image analysis and machine learning. • Thermal
    and RGB imaging of cranes by cameras onboard unmanned aerial vehicles. • Overall
    accuracy of 91.47% for counting algorithm of nighttime thermal images. • Overall
    accuracy of 94.51% for daytime RGB deep learning algorithm based on YOLO-v3. Abstract
    Long-term monitoring of wildlife numbers traditionally uses observers, which are
    frequently inefficient and inaccurate due to their variable experience/training,
    are costly and difficult to sustain over time. Furthermore, there are other inhibiting
    factors for wildlife counting, such as: inhabiting inaccessible areas, fear of
    humans, and nocturnal behavior. There is a need to develop new technologies that
    will automatically identify and count wild animals in order to determine the appropriate
    management protocol. In this study, an advanced and accurate method for automatically
    calculating the number of cranes (Grus grus), using thermal cameras at night and
    visible light (RGB) cameras during the day onboard unmanned aerial vehicles (UAVs),
    based on image analysis and computer vision, was developed. The cranes congregate
    at night in a large communal roost, making it possible to count the birds while
    they are relatively static and all together. Each bird was counted individually
    by creating a standardized tool to determine population numbers for management,
    using image analysis and automatic processing. A dedicated algorithm was developed
    that aimed to identify the cranes based on their spectral characteristics (typical
    temperature, shape, size) and to effectively separate the cranes from the typical
    background. The automatic segmentation and counting of roosting common cranes
    using UAV nighttime thermal images had an Overall Accuracy (OA) of 91.47%, User''s
    Accuracy (UA) of 99.68%, and Producer''s Accuracy (PA) of 91.74%. The computer
    vision and machine learning algorithm based on the YOLO v3 platform of daytime
    RGB UAV images of common cranes at the feeding station yielded an overall loss
    accuracy level of 2.25%, with a mean square error of 1.87, OA of 94.51%, UA of
    99.91%, PA of 94.59%. These results are highly encouraging, and although the algorithms
    were developed for the purpose of counting cranes, they could be adapted for other
    counting purposes for wildlife management. Previous article in issue Next article
    in issue Keywords Wildlife detection and countingThermal imagingImage processingDeep
    learningRemote sensingUnmanned aerial vehicle (UAV) 1. Introduction Various challenges
    that prevent accurate counting of wildlife in animals that flock/congregate in
    large numbers are fear of humans, nocturnal behavior, and the inhabiting of inaccessible
    areas. Long-term monitoring of wildlife numbers traditionally uses observers,
    who vary in experience/training and are costly and difficult to sustain over time.
    There is vital need to develop new technologies to identify and count wildlife.
    Camera traps and acoustic recorders have been used for determining whether a species
    is present at a site, and estimating population density but both are limited to
    specific areas and can be species specific (Meek et al., 2014; Rhinehart et al.,
    2020; Swann et al., 2011). The application of remote sensing technologies to identify
    wildlife can be challenging because of their nocturnal cryptic behavior and relatively
    small size. The use of satellite images to determine the location and count wildlife
    numbers is limited due to the low spatial resolution of such images (Wang et al.,
    2019). Using unmanned aerial vehicles (UAVs) to image animals may assist in areas
    where counting using current methods is inaccurate and inefficient (Anderson and
    Gaston, 2013; Christie et al., 2016). The use of UAVs can allow for long-term
    studies to track and count large animals (Linchant et al., 2015). For example,
    UAVs have been used to count elephants (Loxodonta africana) in Burkina Faso (Dric
    Vermeulen et al., 2013), Nile crocodile (Crocodylus niloticus) in South Africa,
    and even marine animals, such as the Dugong dugon in Australia (Hodgson et al.,
    2013). The counting of medium to small vertebrates is more complicated, but UAVs
    have been used to count pelicans (Pelecanus erythrorhynchos) and greater sage-grouse
    (Centrocercus urophasianus) (Cress et al., 2015), Gentoo penguins (Pygoscelis
    papua) and Adélie penguins (Pygoscelis adeliae) (Pfeifer et al., 2019), chinstrap
    penguins (Pygoscelis antarctica) (Goebel et al., 2015), snow geese (Chen caerulescens),
    Canadian geese (Branta canadensis) (Chabot and Bird, 2012), and rodent burrows
    (Ezzy et al., 2021). It has been suggested that UAVs can be used to count wildlife
    (seabird colonies) more accurately than humans using traditional counting methods.
    For example, the use of UAVs was, on average, 43–96% more accurate (depending
    on image spatial resolution), faster, and less resource-consuming than ground-based
    counting methods (Hodgson et al., 2018). In another study, the numbers of Arctic
    cliff-nesting seabird nests counted by drones and observers were similar, but
    more nestlings were detected in the former than in the latter (Brisson-Curadeau
    et al., 2017). When using UAVs, animals are typically counted from images manually;
    however, this is a tiresome process because single flights generate hundreds of
    photos, and studies may not be repeatable due to observer biases. There is a need
    to develop animal counting capabilities using standardized automatic methods (van
    Gemert et al., 2014) that validate the counts to determine reliability (Christie
    et al., 2016). Using infrared images could help in counting animals that are camouflaged.
    For example, incubating common ringed plovers (Charadrius hiaticula) were not
    detected in RGB images due to their cryptic coloration; however, they were detected
    by thermal images (Lee et al., 2019). In other studies, infrared images were used
    to count Sandhill cranes (Grus canadensis) (Kinzel et al., 2006) and Caribou (Rangifer
    tarandus) in both open pastures and under trees (Christie et al., 2016). While
    these studies have used thermal imaging for counting, they have also used manual
    image processing techniques only, which is not sustainable over time and in large
    herding animals. Up to now, automatic/semi-automatic detection has used computer
    vision techniques, machine learning/deep learning (Akçay et al., 2020; Hodgson
    et al., 2018; Hong et al., 2019; Lyons et al., 2019; Vishnuvardhan et al., 2019),
    GIS classification processes (Rush et al., 2018), and Matlab''s Computer Vision
    Toolbox (Simons and Hinders, 2019). Automated counts using RGB photos only worked
    well when the colors of the birds contrasted well with the background (Chabot
    and Bird, 2012; Grenzdörffer, 2013). Even though UAV, thermal imagery, and automated
    detection have been used in the past to count two grey seal (Halichoerus grypus)
    breeding colonies in Canada (Seymour et al., 2017) and common hippopotamus (Hippopotamus
    amphibius L.) in the Democratic Republic of Congo (Lhoest et al., 2015), to the
    best of our knowledge, no studies have individually classified and automatically
    counted smaller animals, such as birds, using thermal imaging. Wintering common
    cranes (Grus grus), whose populations have been estimated to have increased from
    few hundreds in the 1980s to 40,000 wintering cranes in 2016 (Gophen, 2017; Shanni
    et al., 2012), cause extensive damage to agriculture in Israel. The increasing
    wintering population size, together with an increasingly efficient agro-technology,
    has led to a tense conflict with farmers. In an attempt to lessen the conflict
    between the cranes and farmers in Israel, a diversionary feeding project was initiated
    in 2000 in a designated feeding station located in a tourist park. Cranes were
    hazed from all surrounded agricultural fields by farmers. To prevent future crop
    damage, there is still a need to study how to predict and prevent damage (Nilsson
    et al., 2016); the first stage is to develop an accurate standardized method for
    counting the number of cranes. Currently, the number of cranes is estimated using
    volunteer observers that count the cranes as they leave the night roost in the
    shallow waters of the Agamon Hula wetlands. The cranes congregate nightly in the
    roost to gain protection from predators and possibly also for thermoregulation.
    The observers surround the lake and count the number of cranes that they see leaving
    the roost during the early hours just after sunrise. Using observers to count
    birds is common, but these methods can be time-consuming, expensive, dependent
    on environmental factors (i.e., temperatures, rain and fog), and dependent on
    the observer''s experience, resulting in potential bias (Akçay et al., 2020; Ezat
    et al., 2018). It is extremely difficult to rely on volunteers in long-term monitoring.
    Furthermore, different people count differently (some people count in excess,
    whereas others count in deficit), and the level of accuracy decreases with a decrease
    in the number of observers and as the number of cranes per unit area increases.
    Even though Sandhill cranes have been counted using thermal imaging from aerial
    aircrafts by manually processing images to identify crane flocks, using a GIS
    software to compute roosting flock areas (Kinzel et al., 2006), there is a need
    to count the crane automatically. The objective of this study was therefore to
    develop and validate an advanced and accurate method for automatically estimating
    the number of cranes using thermal, and visible light imaging combined with computer
    vision and machine learning. Since cranes roost together in the water, it is possible
    to count the birds when they are all together and are relatively static, using
    an unmanned aerial vehicle (UAV) with a thermal camera. We hypothesize that it
    is possible to distinguish between the cranes and the surrounding environment
    because of differences in the body temperature of the cranes and the water temperature
    they stand in. We also wanted to determine whether it is possible to count the
    cranes during daytime in the feeding area using a visible light (RGB) camera mounted
    on a UAV. In this research, each bird was counted individually by creating a standardized
    tool to determine population numbers for management, using techniques of image
    analysis and automatic processing. A dedicated algorithm was developed that aims
    to identify, classify, and accurately count the cranes based on their spectral
    characteristics (typical temperature, shape, size) and effectively separate the
    cranes from the typical background (water, ground) in which they were photographed
    (roosting sites in a shallow lake). 2. Materials and methods 2.1. Study area and
    bird colony details The study took place in the Agamon Hula wetlands (33°6′N,
    35°37′E). The area is dominated by intensive agriculture (area about 170 km2),
    with crops cultivated all year round: winter crops (e.g., alfalfa, clover, garlic,
    oats, onion, carrot, wheat) and summer crops (e.g., beans, cotton, corn, peas,
    peanuts, sunflowers, tomatoes, watermelon). In the center of the agricultural
    area, there is a small lake (area about 1 km2) where the common cranes roost at
    night. Cranes are 100–130 cm (39–51 in) long, with a 180–240 cm (71–94 in) wingspan;
    body weight can range from 3 to 6.1 kg (6.6–13.4 lb). The cranes arrive in the
    Hula Valley in September/October, some just use the Hula Valley as a stopover
    site and others as a wintering site and stay until March/April. Cranes were fed
    with corn seeds in an abandoned field (i.e., “feeding station”, 42 ha, see location
    in Fig. 1) and were hazed from all the other fields. Download : Download high-res
    image (762KB) Download : Download full-size image Fig. 1. Map of the Hula Valley
    (green rectangle) and the study area (red rectangle) in relation to neighboring
    countries (A), and image of the diversion feeding station and roost areas located
    in the Agamon Hula wetlands (B) (latitude/longitude: 33°6′N, 35°37′E). (For interpretation
    of the references to color in this figure legend, the reader is referred to the
    Web version of this article.) 2.2. UAV imaging system Both thermal and visible
    light sensing were used to estimate the number of cranes in the Hula Valley. Thermal
    imaging was carried out at nighttime in the lake when the cranes roosted, and
    visible light imaging was carried out at daytime in the feeding grounds, where
    the cranes congregate to eat (Fig. 1). Flight campaigns for the thermal images
    were conducted between 1800 and 2300 (Table S1), which is after all cranes came
    to roost. Unlike Kinzel et al. (2006) findings estimating the time when all cranes
    have settled in their roosts to be between 2300 and 0500 h, the cranes roosting
    in Agamon Lake settled into their roosts relatively early at night, once darkness
    prevailed, and were not active afterwards. We used a Flir A655SC camera (Flir,
    Wilsonville, Oregon, U.S.), 640 × 480 pixels, onboard a DJI Matrice 600 pro UAV
    (DJI, Shenzhen, Guangdong, China) for night flights and an RGB imaging camera
    of a DJI Mavic 2 pro UAV, 5472 × 3648 pixels (∼6 K resolution, a 35 mm lens, format
    equivalent: 28 mm, with a field of view of 77ᵒ) (DJI, Shenzhen, Guangdong, China)
    for day flights. Both UAVs were flown using the Pix4D Capture (Pix4D, Prilly,
    Switzerland) pre-programmed flightpath control software; an 80% overlap (forward
    and side) was chosen for the purpose of creating photo-mosaics. In visible light
    photography as in thermal photography, mosaicked photos make it easier to organize
    the information and prevent pseudo-replication (counting birds more than once).
    The default emissivity value set for the Flir A655SC thermal camera was 0.95.
    Since water emissivity is approximately 0.95, and water concentration in animals
    and vegetation is extremely high, the accepted assumption was to use emissivity
    values of 0.95–0.98 for imaging of this type. Furthermore, a study by Graveley
    et al. (2020) determined the emissivity value of bird feathers to be approximately
    0.95. Since the aim of this study was to identify the cranes on the background
    of water/soil, determining a temperature contrast (difference) between the cranes
    and the background was sufficient, rather than calculating absolute temperature.
    Therefore, the emissivity of the cranes or the background (soil/water) was not
    tested. Air temperature and wind direction and intensity during the flight campaigns
    were measured using a portable meteorological station (Campbell scientific, Logan,
    Utah, U.S.) deployed adjacent to the imaged area. 2.3. Flight campaign details
    Thermal and RGB imaging were performed at an altitude of 100 m above ground level
    (AGL), with a spatial resolution of 69.1 and 23.4 mm (pixel size), respectively.
    We selected flight height at 100 m AGL and at a speed of 15 m/s to allow the drone
    to cover the largest possible space in the shortest possible time (100 m - maximum
    altitude allowed for UAVs in Israel, and 15 m/s - maximum velocity of UAV). High-altitude
    photography is less invasive to the target birds (less noise and further away).
    The thermal camera was programed to take four images per second. This flight layout
    allowed for clear and sharp photography without “smears\". 2.4. Data processing
    To count the cranes, it was extremely important to accurately connect the separate
    images into a mosaic. Non-accurate mosaicking can result in over- or under-estimation
    due to either missing birds or counting the same bird more than once (pseudo-replication).
    We used the Pix4Dmapper software (Pix4D, Prilly, Switzerland) for mosaicking and
    the ArcGIS 10.5 (ESRI, Redlands, California, U.S.) geo-referencing tools for fine
    adjustments. There were some complications; specifically in areas where there
    was only water and no other observable objects, the software tended to be less
    accurate. 2.4.1. Thermal night imagery counting algorithm The algorithm developed
    for identifying, classifying, and accurately counting the cranes using thermal
    imaging was written in Python using OpenCV and ArcPy image processing and geo-processing
    libraries. This algorithm is described below and accompanied by images of the
    northwestern part of the Agamon from the December 10, 2019 flight campaign (Table
    S1). a) First, the histogram of the mosaicked image presenting each pixel''s temperature
    was calculated. There are two peaks in the histogram (Fig. 2): one at around 11.25
    °C and a second peak at 14.75 °C. The first peak represents the soil and second
    one the water. Between these two peaks are the pixels representing the cranes.
    Although cranes'' body temperature is about 40 °C (Prange et al., 1985), the temperature
    that the thermal camera detects is that of the feathers, which is probably in
    equilibrium with the air temperature (13.6 °C). The difference between the temperature
    of the body and feather is a result of the insulation properties of the feathers.
    The air temperature measured during the flight by the portable meteorological
    station was positively correlated to the cranes'' body temperature (R2 = 0.78).
    After “cleaning” the pixels of the water and soil, the pixels of the cranes remained,
    along with some “noise” pixels mainly from the water because there is no complete
    separation between the temperature of the cranes and their background. Download
    : Download high-res image (203KB) Download : Download full-size image Fig. 2.
    Histogram of the mosaicked image of the northwestern part of the Agamon Lake taken
    on December 10, 2019, with peaks representing soil and water pixels, and cranes''
    pixels between the peaks. b) To filter these noises, the findcontours () function
    in OpenCV was used to find connected components which are like closed polygons
    (Fig. 3C). Connected components that were not the typical size of the crane were
    filtered out - as learned by trial and error. All connected components that were
    not in this range were removed from the image (Fig. 3D). Download : Download high-res
    image (642KB) Download : Download full-size image Fig. 3. Filtering out connected
    components that are not the typical size of the crane. (A) Original thermal mosaic,
    (B) mosaic after removal of pixels of soil and water bodies, (C) close-up of the
    mosaic (according to the rectangle in panel B, connected components surrounded
    by red) (D) connected components after filtering out according to crane size (connected
    components in white). (For interpretation of the references to color in this figure
    legend, the reader is referred to the Web version of this article.) c) The average
    temperature of the cranes was calculated from the remaining connected components
    to filter out additional noise. Removing all the pixels in the image whose temperature
    is above this threshold would involve the loss of pixels of cranes. Therefore,
    this filtering was done in stages as described under d). d) The upper threshold
    of pixel temperature in the image was selected according to the histogram (14.75
    °C according to Fig. 2). Descending from this upper threshold to the new threshold
    (calculated in Stage c - 14 °C) was done at intervals of 0.25 °C, thus maintaining
    the sensitivity of the model. For each such interval, a separate image was created,
    whose temperature range was in the range of the minimum (11.25 °C according to
    the histogram), and its maximum was between the two thresholds described above.
    Descending from the upper threshold to the average temperature, three more images
    were created (in addition to the original image), as shown in Fig. 4. Download
    : Download high-res image (528KB) Download : Download full-size image Fig. 4.
    Connected component composition change according to the different temperature
    ranges that are calculated according to the minimum and maximum thresholds of
    the algorithm for identifying and classifying cranes. e) For each such image,
    connected components were calculated in the same way as in Stage b. For each image,
    a different amount and identity of connected components was computed because the
    temperature range of the pixels in the image was different. f) The image with
    the highest temperature range to the one with the second highest temperature range
    was compared: each connected component in both images was saved (only once) in
    the result product (intersection between the two images), and in addition, all
    elements in only one image were saved (XOR - exclusive OR) (Fig. 5). The result
    image replaced the two source images and formed the basis for comparison with
    the following image in line: the image from all the remaining images with the
    highest temperature range. Download : Download high-res image (961KB) Download
    : Download full-size image Fig. 5. Result image of Intersection and XOR of connected
    components in two images with different temperature thresholds. g) Repeat Stage
    f, and compare all the images that were created in Stage d. The result image now
    contains the total of the various connected components found in all the images
    with the different temperature thresholds. h) Perform additional noise filtering:
    the water temperature was uniform, whereas the cranes'' temperature was not uniform
    (there was a difference between the center of the body and the periphery of the
    body). For each connected component, a minimum and maximum temperature were found.
    All components whose temperature difference was below a certain threshold (in
    most cases, 0.2 °C) were removed. i) The elliptical shape that characterizes the
    cranes was used for filtering additional noise: the circularity of each connected
    component was calculated, and any shape that was less circular than a certain
    threshold (the circularity index ranges from 0 to 1, closer to 1 being more circular)
    was removed (Fig. 6). Download : Download high-res image (488KB) Download : Download
    full-size image Fig. 6. Noise filtering according to water and crane temperature
    characteristics and circular shape of cranes. Panels A and B before filtering,
    panel C - final result of the algorithm after filtering. Red connected components
    in B–C represent individual cranes. (For interpretation of the references to color
    in this figure legend, the reader is referred to the Web version of this article.)
    j) The resulting image obtained in Stage i contained connected components that
    represent all the cranes and allow the counting of the number of cranes within
    the image (Fig. 6C). The described above algorithm is semi-automatic, since for
    each image (mosaic), it is necessary to adjust the threshold temperature values
    of the cranes and that of the background (water/soil) according to its histogram.
    2.4.2. Algorithm validation For each mosaicked image, 10% of the image area was
    selected, and cranes were counted manually, along with counting performed by the
    algorithm. The results of the counts were introduced into a confusion matrix (Foody,
    2002) for validation purposes. Classification accuracy is generally defined as
    the degree to which the generated classified image agrees with reality. A confusion
    matrix is a simple cross-tabulation of mapped class labels against ground observed
    reference (Foody, 2002). It is a powerful statistical tool that provides essential
    information regarding remotely sensed classification accuracy (Congalton, 1991;
    Foody, 2002). It offers several valuable measures/indices, usually expressed as
    a percent: Overall Accuracy - measures the ratio between the correct identification
    of cranes and the total population of cranes in the sample. User''s Accuracy -
    also called Precision - represents the model''s ability to identify only the relevant
    points (cranes). From the user''s point of view (in the classification map), an
    index that predicts reliability - whether an object marked on the map will actually
    be present in the field in reality. Producer''s Accuracy - also called Recall
    - represents the ability of the model to correctly identify all the different
    objects. From the point of view of the map maker, a measure that represents the
    probability that the object selected in the field will be classified correctly
    on the map. 2.4.3. Daylight visible light imagery counting algorithm Daytime imaging
    of the cranes took place twice during the study (Table S1) in the feeding station
    (42 ha) located adjacent to Agamon Lake (Fig. 1), using the RGB camera mounted
    onboard a Mavic 2 pro UAV (Table S1). The image was geo-rectified and mosaicked
    using Pix4Dmapper, and a machine learning algorithm based on the YOLO V3 (you
    only look once) (https://pjreddie.com/darknet/yolo/) platform was developed to
    accurately count the cranes. The algorithm is described herein: a) The mosaicked
    image was divided into 416 × 416 pixels for the purpose of improving the efficiency
    of the algorithm (Fig. 7A). Download : Download high-res image (983KB) Download
    : Download full-size image Fig. 7. There were six stages in the machine learning
    algorithm for identifying and classifying the cranes during the daytime (visible
    light), where the mosaicked image of the cranes in the feeding station was divided
    into sub-images. Frame (A) depicts a representative sub-image. The histogram of
    the image in the blue channel is calculated (B), and all pixels below DN = 90
    are removed (C). The product of the image is converted to a binary image (D),
    and a morphology function (E) is activated, after which connected components are
    calculated, and only the typical crane size components are saved (F). (For interpretation
    of the references to color in this figure legend, the reader is referred to the
    Web version of this article.) b) Since the cranes were lighter in color than the
    background on which they were photographed (exposed soil and vegetation), they
    could be separated from the environment by using the blue channel. A histogram
    of the imaged mosaic was calculated, and blue values below a certain threshold
    (digital number = 90) were removed from the image (Fig. 7B and C). c) A GRVI plant
    index was also calculated, and a similar use was made of this index to remove
    pixels of bare soil from the image, which have a negative GRVI value. d) The image
    was converted to a binary image (Fig. 7D), and the morphologyEx () function within
    the OpenCV library was activated with an open option with a kernel size of 5 ×
    5 to connect adjacent components (within the kernel area) that were separated
    due to noise into one component (Fig. 7E). e) Connected components (such as closed
    polygons) were calculated. The area of each connected component was calculated,
    and the components that were not the typical size of a crane were filtered - as
    learned in the trial and error process (Fig. 7F). f) The remaining connected components
    were marked and labeled automatically by the algorithm, and subsequently, the
    labeling was scanned manually using the LabelImg labeling software to make sure
    that the algorithm''s labeling of the cranes was correct. Approximately 25% of
    the mosaicked map area from each date was tagged by the algorithm and transferred
    to a machine learning algorithm for the purpose of training the algorithm. g)
    The labeled file (text file) was exported to machine training using the YOLO V3
    platform; YOLO V3 technical parameters were customized (e.g., number of iterations,
    batch and sub-batch sizes). h) The training was performed using Google''s Colab
    Jupyter notebook environment (Google, https://research.google.com/colaboratory/faq.html)
    and ran in the cloud environment. i) The machine was trained on 25% of the field
    area (for each date), using the Darknet-53 CNN (convolutional neural network)
    neural network, producing weights for the neural network, which was used for new
    pictures to identify cranes. j) At the end of the training, the algorithm was
    run on the entire feeding station, all the sub-images were connected to obtain
    the original mosaicked image, and the cranes were classified and counted. While
    both the thermal night imagery and the daylight visible light imagery counting
    algorithms (sections 2.4.1 and 2.4.3, respectively) use connected components in
    order to identify/classify cranes and use histogram-based techniques to clear
    background noises (Fig. 8), there are differences between the algorithms in the
    input data types and algorithm buildup. The night imagery counting algorithm is
    based on thermal imaging, and relies on classic image analysis and processing.
    The daylight imagery counting algorithm is based on RGB imaging and machine learning
    algorithms. Fig. 8 presents technical flow charts to show the specific flow of
    the two algorithms in identifying common cranes, highlighting the commonalities
    and differences between the different methods. Download : Download high-res image
    (1MB) Download : Download full-size image Fig. 8. Technical flow charts of thermal
    night imagery and daylight RGB imagery crane counting algorithms. 2.5. Comparison
    to ground survey crane estimation We compared the classification and identification
    of cranes from the thermal night imagery and daylight visible light imagery algorithms
    (six night surveys in the roost and two day surveys in the feeding station) to
    nine crane counts conducted by field observers at adjacent dates. Efforts were
    made to conduct imagery-based counting on dates close to the visual counting operations,
    but due to manpower and other technical limitations (such as foggy weather, which
    prevented visual counting on some mornings after night imagery campaigns), only
    two UAV-based campaign counts were performed on overlapping/adjacent dates to
    visual counting. 3. Results 3.1. Thermal imagery counting algorithm validation
    and accuracy assessment Overall, the confusion matrix yielded an Overall Accuracy
    of 91.47%, a User''s Accuracy of 99.68%, and a Producer''s Accuracy of 91.74%
    (Table 1). These results were based on an area representing 10% of the cranes''
    roost clustered areas within the mosaicked map, which was randomly chosen, and
    cranes were classified and counted manually. Later, the manual counting was compared
    to the thermal imagery algorithm classification and counting (Fig. 9). Table 1.
    Confusion matrix identifying and classifying cranes using thermal imaging in Hula
    Agamon, based on a comparison of the algorithm counting with manual sampling of
    randomly chosen 10% of the mosaicked imaged area from four flight campaigns (Table
    S1) in the 2019–2020 winter season. Algorithm counting Manual counting Crane Not
    a crane Total User''s Accuracy Crane 4367 (*1) 14 (*2) 4381 99.68% Not a crane
    393 (*3) 0 (*4) 393 Total 4760 14 4774 Producer''s Accuracy 91.74% Overall Accuracy:
    91.47% (*1) - True positive - TP/Hit. (*2) - False positive – FP. (*3) - False
    negative/Miss – FN. (*4) - True negative – TN. 1 + 2 + 3 + 4 = Total population.
    Overall Accuracy (ACC) = Σ True positive+ Σ True negative/Σ Total population:
    (4367 + 0)/4774 = 0.9147. User''s Accuracy = TP/(TP + FP) = 4367/(4367 + 14) =
    0.9968. Producer''s Accuracy = TP/(TP + FN) = 4367/(4367 + 393) = 0.9174. Download
    : Download high-res image (596KB) Download : Download full-size image Fig. 9.
    Validation of the counting algorithm for the image of the December 10, 2019 flight
    campaign in the northwestern part of the Agamon. Navy blue: hit, red: miss/false
    negative, yellow: false positive. (For interpretation of the references to color
    in this figure legend, the reader is referred to the Web version of this article.)
    Note that in our case, it was not possible to quantify True Negative (TN) - that
    is, a situation where the model predicts that there are no cranes in a particular
    place, and indeed, there were no cranes in the same place; therefore, the number
    representing this case is zero. This is an underestimation because the de facto
    model “knows” where there are no cranes, since in places where there is a background
    (water, soil), the model does not give a classification of cranes. However, since
    the model does not specify “non-crane” classification points in the model, it
    was decided to give the number zero - something that hurts the Overall Accuracy
    index and the Kappa Coefficient index, which also rely on the TN index. For this
    reason, the Kappa Coefficient index does not appear in the results. 3.2. RGB imagery
    counting algorithm validation and accuracy assessment The YOLO v3 platform yielded
    an overall loss accuracy level of 2.25%, with a mean square error of 1.87. The
    confusion matrix yielded an Overall Accuracy of 94.51%, User''s Accuracy of 99.91%,
    and Producer''s Accuracy of 94.59% (Table 2). An area representing 10% of the
    mosaicked map was randomly chosen, and cranes were classified and counted manually.
    Later, manual counting was compared to the YOLO machine learning algorithm classification
    and counting (Fig. 10). Table 2. Confusion matrix identifying and classifying
    cranes using RGB imaging in the feeding station based on a comparison of the algorithm
    counting with manual sampling of randomly chosen 10% of the mosaicked imaged area
    from two flight campaigns in the feeding station (Table S1) during the 2020–2021
    winter season. Algorithm counting Manual counting Crane Not a crane Total User''s
    Accuracy Crane 12,910 (*1) 11 (*2) 12,921 99.91% Not a crane 738 (*3) 0 (*4) 738
    Total 13,648 11 13,659 Producer''s Accuracy 94.59% Overall Accuracy: 94.51% (*1)
    - True positive - TP/Hit. (*2) - False positive - FP. (*3) - False negative -
    FN/Miss. (*4) - True negative - TN. 1 + 2+3 + 4 = Total population. Download :
    Download high-res image (2MB) Download : Download full-size image Fig. 10. A mosaic
    image of the feeding station (A), close-up images of the area surrounded by the
    red rectangle with the cranes (B), and the product of the classification of the
    cranes by the machine learning algorithm in which the cranes are tagged (C). (For
    interpretation of the references to color in this figure legend, the reader is
    referred to the Web version of this article.) 3.3. Validation – comparison with
    traditional ground-based counting The counts from the field observers (mean =
    31,800 cranes) and from the thermal night imagery counting algorithm (mean = 33,325
    cranes) varied by 28.6% and 38.4% for 08/11/21 and 13/12/21, respectively, dates
    in which counts were performed on the same day: digital count performed at night
    and visual field count performed on the following early morning. The visual count
    was underestimating crane numbers when compared to the digital count. The winter
    crane population in the Hula valley is stable between mid-December and early March.
    From early November to mid-December and from early March to April, there is a
    vigorous migratory movement of cranes, day and night; therefore, counting on different
    days will yield different results, depending on the number of flocks roosting
    in the Agamon Lake for night rest during their migration from Europe to Africa
    and back. It is possible that the variance in the numbers between the visual and
    the digital counting on different dates during the migration period (Table 3)
    also stems from this fact. Table 3. Comparison between the counts from the field
    observers and the imagery-based digital counting algorithms. Date Visual counting
    Imagery counting Remarks 04/11/2020 31,200 10/11/2020 32,600 23/11/2020 27,520
    02/12/2020 30,500 14/12/2020 27,100 20/12/2020 21,100 26/01/2021 26,289 Feeding
    station counting 10/02/2021 29,127 Feeding station counting 16/02/2021 40,600
    23/02/2021 38,000 03/03/2021 42,060 08/11/2021 32,000 44,822 17/11/2021 36,500
    24/11/2021 35,169 13/12/2021 18,050 29,303 4. Discussion The validation results
    of both the night and day counts were promising. Specifically, the automatic segmentation
    and counting of roosting common cranes using UAV thermal images succeeded in counting
    cranes, with an Overall Accuracy of 91.47%, User''s Accuracy of 99.68%, and Producer''s
    Accuracy of 91.74% (Table 1); the computer vision and machine learning-aided algorithms
    for counting cranes in the feeding station using UAV RGB-based daylight imagery
    yielded an Overall Accuracy of 94.5%, User''s Accuracy of 99.9%, and Producer''s
    Accuracy of 94.59% (Table 2). In comparison to many studies that used UAV-aided
    imagery for counting animals manually (Dric Vermeulen et al., 2013; Goebel et
    al., 2015; Hodgson et al., 2013; Pfeifer et al., 2019), in this study, we created
    two independent algorithms for automatically counting cranes. While studies using
    manually counting reported high levels of accuracy, it takes a lot of time and
    efforts, apart from being biased and inaccurate. The advantage of using algorithms
    for automated detection, such as those presented in this study, in comparison
    to manual counting, is that the algorithms can be run for every UAV campaign with
    little effort in a very short time with high accuracy. Furthermore, as the number
    of targets increases, such as with counting cranes, so will the amount of time,
    and the accuracy will decrease in manual counting in comparison to automated detection.
    Thermal imaging for crane recognition and counting proved to be highly successful,
    especially because the cranes congregate at night roosts. Counting cranes or any
    other animal at night would not be possible to perform with visible light RGB
    cameras. The use of thermal cameras for the purpose of animal recognition, classification,
    and counting is less common, probably due to the much higher equipment costs (Linchant
    et al., 2015). This research proved the indispensability of using thermal imagery
    equipment for nocturnal counting, which would otherwise be impossible. In comparison
    to traditional ground surveys, the use of a UAV provided a highly accurate counting
    of the crane numbers in an inaccessible area (inside wetlands) and during the
    nighttime, when visibility is low. Thermal imaging automatic counting was successful
    partly because the cranes roosted in water, whose temperature varies from that
    of the air, in which a correlation to crane temperature was found due to the insulation
    of the cranes'' feathers, resulting in a crane temperature similar to the air
    temperature. Furthermore, the cranes did not move a lot while roosting, allowing
    a mosaic to be created from multiple images, and the birds roosted in a relatively
    small area. Finally, the cranes roosted at a large enough distance from one another
    and were never touching, thereby allowing the algorithm to easily distinguish
    among individuals, not clumping birds together. The ability to accurately count
    the cranes will allow the population numbers to be monitored and to manage feeding
    efforts accordingly. The approximate 30% difference between the visual and digital
    counts (on same-date counts) shows that the visual count is underestimating the
    number of cranes, which should be taken into consideration when constructing management
    measures for feeding and hazing as well as when taking into consideration historical
    visual counts. More adjacent comparisons between the visual and digital counting
    methods should be performed to validate the differences. The development of technological
    capabilities for animal identification in agriculture is crucial. The methods
    of automatically counting animals using thermal images can also be applied to
    count other animals, such as wild boar (Sus scrofa) in fields and orchards (Gentile
    Francesco et al., 2014) and great white pelican (Pelecanus onocrotalus) in fish
    ponds (Shmueli et al., 2000). Likewise, there is a need not only to count wildlife
    in wildlife conflicts but also for the conservation of endangered species and
    basic ecology-related studies. In this study, the automatic identification of
    the number of common cranes was made possible due to the static state of the roosting
    and feeding cranes. One of the advantages of being able to capture all the animals
    in one place is that there is no pseudo-replication, counting individuals more
    than once. Even though moving animals have been identified using videos in small
    areas such as in a fish tank (Pérez-Escudero et al., 2014), there is a need for
    future studies in order to identify moving animals over large areas. In this study,
    the sample plot was relatively small because the cranes roosted and fed together.
    As UAV''s cost will decrease and battery life, size and speed will increase, as
    well as sensors'' spatial resolution, so will the ability to use similar technologies
    at a much larger scale needed for other animals that do not congregate in one
    place. Furthermore, as technology advances so will it be possible to cover larger
    areas using a swarm of UAV''s (Campion et al., 2019). Future studies could also
    combine the use of UAVs to count target species, combined with applying remote
    sensing tools to determine damage to agriculture (Chen et al., 2021; Rutten et
    al., 2018). For example, to use UAV both to count rodent presence (Ezzy et al.,
    2021) and to determine the amount of damage to crop yields and vegetation health
    (Andreo et al., 2009). To properly count birds and other animals, it is important
    to first determine whether the presence of the UAV affects the target animal''s
    behavior, which will create biased results. This can be especially important when
    counting breeding animals, which, if flushed, could result in egg or young predation
    (Brisson-Curadeau et al., 2017). We found that cranes were only affected by the
    UAV when flown at less than 20 m altitude, but when flown at 30 m and above, the
    cranes did not react. The behavior of two different penguin species varied slightly
    between Gentoo (Pygoscelis papua) and Adélie penguins (Pygoscelis adeliae) (Brisson-Curadeau
    et al., 2017; Rümmler et al., 2018), with the former behaving similarly to the
    cranes in this study and the latter being negatively affected by the UAV at the
    highest altitude tested (50 m). The difference in behavior among species may also
    be explained by the location and the frequency at which the animals are exposed
    to disturbances. For example, animals that inhabit agricultural areas are frequently
    exposed to workers, tractors, and crop dusters, whereas animals in natural settings
    may be more sensitive to the surrounding elements. It is therefore important to
    do a pilot study to determine the sensitivity of the target species to the UAV.
    5. Conclusions In this study both the automatic segmentation and counting of roosting
    common cranes using UAV nighttime thermal images, and the computer vision and
    machine learning algorithm based on the YOLO v3 platform of UAV daytime RGB images
    yielded extremely promising results. In areas and situations where counting using
    current methods is inefficient and inaccurate, UAVs have the potential for the
    accurate numerical assessment of animals. The method and protocol we propose to
    develop could be used beyond the counting of cranes and could be of use to farmers
    and ecologists in other diverse situations. Even though this original algorithm
    was developed especially for the purpose of counting cranes, it could be adapted
    for the classification and counting of other targets. Code data are available
    from the corresponding author upon request. Credit author statement Assaf Chen:
    Conceptualization, Methodology/Study design, Validation, Formal analysis, Investigation,
    Data curation, Writing – original draft, Writing – review and editing, Visualization,
    Supervision, Project administration, Funding acquisition. Moran Jacob: Software,
    Validation, Formal analysis, Investigation, Data curation, Visualization. Gil
    Shoshani: Validation, Formal analysis, Investigation, Data curation, Visualization.
    Motti Charter: Conceptualization, Methodology/Study design, Writing – original
    draft, Writing – review and editing, Funding acquisition. Funding This study was
    supported by the Israeli Ministry of Agriculture and Rural Development Grant for
    strengthening the upper Galilee agriculture (grant number: 21-07-0003/19). Declaration
    of competing interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. Acknowledgments Special thanks to Shai Agmon, Inbar
    Shlomit Rubin, Nir Aspis, and Efi Naim from the Jewish National Fund (KKL) and
    the Agamon authority for accompanying us on the flight campaigns and for sharing
    their valuable knowledge about nature and birds. Appendix A. Supplementary data
    The following is the Supplementary data to this article: Download : Download Word
    document (33KB) Multimedia component 1. Data availability Data will be made available
    on request. References Akçay et al., 2020 H.G. Akçay, B. Kabasakal, D. Aksu, N.
    Demir, M. Öz, A. Erdoğan Automated bird counting with deep learning for regional
    bird distribution mapping Animals, 10 (2020), pp. 1-24, 10.3390/ani10071207 View
    in ScopusGoogle Scholar Anderson and Gaston, 2013 K. Anderson, K.J. Gaston Lightweight
    Unmanned Aerial Vehicles Will Revolutionize Spatial Ecology. Frontiers in Ecology
    and the Environment (2013), 10.1890/120150 Google Scholar Andreo et al., 2009
    V. Andreo, M. Lima, C. Provensal, J. Priotto, J. Polop Population dynamics of
    two rodent species in agro-ecosystems of central Argentina: intra-specific competition,
    land-use, and climate effects Popul. Ecol., 51 (2009), pp. 297-306, 10.1007/s10144-008-0123-3
    View in ScopusGoogle Scholar Brisson-Curadeau et al., 2017 É. Brisson-Curadeau,
    D. Bird, C. Burke, D.A. Fifield, P. Pace, R.B. Sherley, K.H. Elliott Seabird species
    vary in behavioural response to drone census Sci. Rep., 7 (2017), pp. 1-9, 10.1038/s41598-017-18202-3
    Google Scholar Campion et al., 2019 M. Campion, P. Ranganathan, S. Faruque Uav
    swarm communication and control architectures: a review J. Unmanned Veh. Syst.,
    7 (2019), pp. 93-106, 10.1139/juvs-2018-0009 View in ScopusGoogle Scholar Chabot
    and Bird, 2012 D. Chabot, D.M. Bird Evaluation of an off-the-shelf unmanned aircraft
    system for surveying flocks of geese Waterbirds, 35 (2012), pp. 170-174, 10.1675/063.035.0119
    View in ScopusGoogle Scholar Chen et al., 2021 A. Chen, M. Jacob, G. Shoshani,
    M. Dafny-Yelin, O. Degani, O. Rabinovitz Early detection of soil-borne diseases
    in field crops via remote sensing Precision Agriculture ’21, Wageningen Academic
    Publishers, The Netherlands (2021), pp. 217-224, 10.3920/978-90-8686-916-9_25
    Google Scholar Christie et al., 2016 K.S. Christie, S.L. Gilbert, C.L. Brown,
    M. Hatfield, L. Hanson Unmanned aircraft systems in wildlife research: current
    and future applications of a transformative technology. Frontiers in Ecology and
    the Environment https://doi.org/10.1002/fee.1281 (2016) Google Scholar Congalton,
    1991 R.G. Congalton A review of assessing the accuracy of classifications of remotely
    sensed data Remote Sens Environ., 37 (1991), pp. 35-46 View PDFView articleView
    in ScopusGoogle Scholar Cress et al., 2015 B.J. Cress, M. Hutt, J. Sloan, M. Bauer,
    M. Feller, S. Goplen, S. Jewell, U.S.G. Survey U . S . Geological Survey Unmanned
    Aircraft Systems (UAS) Roadmap 2014 (2015) Google Scholar Dric Vermeulen et al.,
    2013 C. Dric Vermeulen, P. Lejeune, J. Lisein, P. Bouché Unmanned Aerial Survey
    of Elephants (2013), 10.1371/Material Google Scholar Ezat et al., 2018 M.A. Ezat,
    C.J. Fritsch, C.T. Downs Use of an unmanned aerial vehicle (drone) to survey Nile
    crocodile populations: a case study at Lake Nyamithi, Ndumo game reserve, South
    Africa Biol. Conserv., 223 (2018), pp. 76-81, 10.1016/j.biocon.2018.04.032 View
    PDFView articleView in ScopusGoogle Scholar Ezzy et al., 2021 H. Ezzy, M. Charter,
    A. Bonfante, A. Brook How the small object detection via machine learning and
    uas-based remote-sensing imagery can support the achievement of sdg2: a case study
    of vole burrows Rem. Sens., 13 (2021), 10.3390/rs13163191 Google Scholar Foody,
    2002 G.M. Foody Status of land cover classification accuracy assessment Remote
    Sens Environ., 80 (2002), pp. 185-201 View PDFView articleView in ScopusGoogle
    Scholar Francesco et al., 2014 Gentile Francesco, A. Bonardi, P. Mairota, V. Leronni,
    E. Padoa-Schioppa Predicting wild boar damages to croplands in a mosaic of agricultural
    and natural areas Curr. Zoology, 60 (2014), pp. 170-179, 10.1093/czoolo/60.2.170
    Google Scholar Goebel et al., 2015 M.E. Goebel, W.L. Perryman, J.T. Hinke, D.J.
    Krause, N.A. Hann, S. Gardner, D.J. LeRoi A small unmanned aerial system for estimating
    abundance and size of Antarctic predators Polar Biol., 38 (2015), pp. 619-630,
    10.1007/s00300-014-1625-4 View in ScopusGoogle Scholar Gophen, 2017 M. Gophen
    Partnerships between the managements of cranes (Grus grus) and kinneret water
    quality protection in the Hula Valley, Israel Open J. Mod. Hydrol., 7 (2017),
    p. 200, 10.4236/ojmh.2017.72011 –208 Google Scholar Graveley et al., 2020 J.M.F.
    Graveley, K.R. Burgio, M. Rubega Using a thermal camera to measure heat loss through
    bird feather coats JoVE (2020), pp. 1-16, 10.3791/60981 2020 View in ScopusGoogle
    Scholar Grenzdörffer, 2013 G.J. Grenzdörffer UAS-based automatic bird count of
    a common gull colony International Archives of the Photogrammetry, Remote Sensing
    and Spatial Information Sciences (2013), pp. 169-174, 10.1139/juvs-2015-0006 Rostock,
    Germany View in ScopusGoogle Scholar Hodgson et al., 2013 A. Hodgson, N. Kelly,
    D. Peel Unmanned aerial vehicles (UAVs) for surveying Marine Fauna: a dugong case
    study PLoS One, 8 (2013), pp. 1-15, 10.1371/journal.pone.0079556 Google Scholar
    Hodgson et al., 2018 J.C. Hodgson, R. Mott, S.M. Baylis, T.T. Pham, S. Wotherspoon,
    A.D. Kilpatrick, R. Raja Segaran, I. Reid, A. Terauds, L.P. Koh Drones count wildlife
    more accurately and precisely than humans Methods Ecol. Evol., 9 (2018), pp. 1160-1167,
    10.1111/2041-210X.12974 View in ScopusGoogle Scholar Hong et al., 2019 S.J. Hong,
    Y. Han, S.Y. Kim, A.Y. Lee, G. Kim Application of deep-learning methods to bird
    detection using unmanned aerial vehicle imagery Sensors, 19 (2019), pp. 1-16,
    10.3390/s19071651 Google Scholar Kinzel et al., 2006 P.J. Kinzel, J.M. Nelson,
    R.S. Parker, L.R. Davis Spring census of mid-continent Sandhill cranes using aerial
    infrared videography J. Wildl. Manag., 70 (2006), pp. 70-77, 10.2193/0022-541x
    2006)70[70:scomsc]2.0.co;2 View in ScopusGoogle Scholar Lee et al., 2019 W.Y.
    Lee, M. Park, C.U. Hyun Detection of two Arctic birds in Greenland and an endangered
    bird in Korea using RGB and thermal cameras with an unmanned aerial vehicle (UAV)
    PLoS One, 14 (2019), pp. 1-16, 10.1371/journal.pone.0222088 Google Scholar Lhoest
    et al., 2015 S. Lhoest, J. Linchant, S. Quevauvillers, C. Vermeulen, P. Lejeune
    How many hippos (Homhip): algorithm for automatic counts of animals with infra-red
    thermal imagery from UAV. International Archives of the Photogrammetry Remote
    Sens Spatial Inf. Sci ISPRS Archives, 40 (2015), pp. 355-362, 10.5194/isprsarchives-XL-3-W3-355-2015
    View in ScopusGoogle Scholar Linchant et al., 2015 J. Linchant, J. Lisein, J.
    Semeki, P. Lejeune, C. Vermeulen Are unmanned aircraft systems (UASs) the future
    of wildlife monitoring? A review of accomplishments and challenges Mammal Review
    https://doi.org/10.1111/mam.12046 (2015) Google Scholar Lyons et al., 2019 M.B.
    Lyons, K.J. Brandis, N.J. Murray, J.H. Wilshire, J.A. McCann, R.T. Kingsford,
    C.T. Callaghan Monitoring large and complex wildlife aggregations with drones
    Methods Ecol. Evol., 10 (2019), pp. 1024-1035, 10.1111/2041-210X.13194 View in
    ScopusGoogle Scholar Meek et al., 2014 P.D. Meek, G.A. Ballard, P.J.S. Fleming,
    M. Schaefer, W. Williams, G. Falzon Camera traps can be heard and seen by animals
    PLoS One, 9 (2014), Article 110832, 10.1371/journal.pone.0110832 Google Scholar
    Nilsson et al., 2016 L. Nilsson, N. Bunnefeld, J. Persson, J. Månsson Large grazing
    birds and agriculture-predicting field use of common cranes and implications for
    crop damage prevention Agric. Ecosyst. Environ., 219 (2016), pp. 163-170, 10.1016/j.agee.2015.12.021
    View PDFView articleView in ScopusGoogle Scholar Pérez-Escudero et al., 2014 A.
    Pérez-Escudero, J. Vicente-Page, R.C. Hinz, S. Arganda, G.G. De Polavieja IdTracker:
    tracking individuals in a group by automatic identification of unmarked animals
    Nat. Methods, 11 (2014), pp. 743-748, 10.1038/nmeth.2994 View in ScopusGoogle
    Scholar Pfeifer et al., 2019 C. Pfeifer, A. Barbosa, O. Mustafa, H.U. Peter, M.C.
    Rümmler, A. Brenning Using fixed-wing uav for detecting and mapping the distribution
    and abundance of penguins on the South Shetlands Islands, Antarctica Drones, 3
    (2019), pp. 1-22, 10.3390/drones3020039 View in ScopusGoogle Scholar Prange et
    al., 1985 H.D. Prange, J.S. Wasser, A.S. Gaunt, S.L.L. Gaunt Respiratory responses
    to acute heat stress in cranes (Gruidae): the effects of tracheal coiling Respir.
    Physiol., 62 (1985), pp. 95-103, 10.1016/0034-5687(85)90053-2 View PDFView articleView
    in ScopusGoogle Scholar Rhinehart et al., 2020 T.A. Rhinehart, L.M. Chronister,
    T. Devlin, J. Kitzes Acoustic localization of terrestrial wildlife: current practices
    and future opportunities Ecol. Evol., 10 (2020), pp. 6794-6818, 10.1002/ece3.6216
    View in ScopusGoogle Scholar Rümmler et al., 2018 M.C. Rümmler, O. Mustafa, J.
    Maercker, H.U. Peter, J. Esefeld Sensitivity of Adélie and Gentoo penguins to
    various flight activities of a micro UAV Polar Biol., 41 (2018), pp. 2481-2493,
    10.1007/s00300-018-2385-3 View in ScopusGoogle Scholar Rush et al., 2018 G.P.
    Rush, L.E. Clarke, M. Stone, M.J. Wood Can drones count gulls? Minimal disturbance
    and semiautomated image processing with an unmanned aerial vehicle for colony-nesting
    seabirds Ecol. Evol., 8 (2018), pp. 12322-12334, 10.1002/ece3.4495 View in ScopusGoogle
    Scholar Rutten et al., 2018 A. Rutten, J. Casaer, M.F.A. Vogels, E.A. Addink,
    J. Vanden Borre, H. Leirs Assessing agricultural damage by wild boar using drones
    Wildl. Soc. Bull., 42 (2018), pp. 568-576, 10.1002/wsb.916 View in ScopusGoogle
    Scholar Seymour et al., 2017 A.C. Seymour, J. Dale, M. Hammill, P.N. Halpin, D.W.
    Johnston Automated detection and enumeration of marine wildlife using unmanned
    aircraft systems (UAS) and thermal imagery Sci. Rep., 7 (2017), pp. 1-10, 10.1038/srep45127
    Google Scholar Shanni et al., 2012 I. Shanni, Z. Labinger, D. Alon A review of
    the crane-agriculture conflict, hula valley, Israel J. Harris (Ed.), Cranes, Agriculture,
    and Climate Change (2012), pp. 100-104 Google Scholar Shmueli et al., 2000 M.
    Shmueli, I. Izhaki, A. Arieli, Z. Arad Energy requirements of migrating great
    white pelicans Pelecanus onocrotalus Ibis, 142 (2000), pp. 208-216, 10.1111/j.1474-919x.2000.tb04860.x
    View in ScopusGoogle Scholar Simons and Hinders, 2019 E.S. Simons, M.K. Hinders
    Automatic counting of birds in a bird deterrence field trial Ecol. Evol., 9 (2019),
    pp. 11878-11890, 10.1002/ece3.5695 View in ScopusGoogle Scholar Swann et al.,
    2011 D.E. Swann, K. Kawanishi, J. Palmer Evaluating types and features of camera
    traps in ecological studies: a guide for researchers A.F. O''Connell, J.D. Nichols,
    K.U. Karanth (Eds.), Camera Traps in Animal Ecology: Methods and Analyses, Springer
    Japan, Tokyo (2011), pp. 27-43, 10.1007/978-4-431-99495-4_3 View in ScopusGoogle
    Scholar van Gemert et al., 2014 J.C. van Gemert, C.R. Verschoor, P. Mettes, K.
    Epema, L.P. Koh, S. Wich Nature Conservation Drones for Automatic Localization
    and Counting of animals., European Conference on Computer Vision Springer, Cham
    (2014) Google Scholar Vishnuvardhan et al., 2019 R. Vishnuvardhan, G. Deenadayalan,
    M.V. Vijaya Gopala Rao, S.P. Jadhav, A. Balachandran Automatic detection of flying
    bird species using computer vision techniques J. Phys. Conf., 1362 (2019), 10.1088/1742-6596/1362/1/012112
    Google Scholar Wang et al., 2019 D. Wang, Q. Shao, H. Yue Surveying wild animals
    from satellites, manned aircraft and unmanned aerial systems (UASs): a review
    Rem. Sens., 11 (2019), 10.3390/rs11111308 Google Scholar Cited by (18) Multi-attribute,
    graph-based approach for duplicate cattle removal and counting in large pasture
    areas from multiple aerial images 2024, Computers and Electronics in Agriculture
    Show abstract Quantitative identification of debonding defects in building façades
    based on UAV-thermography using a two-stage network integrating dual attention
    mechanism 2024, Infrared Physics and Technology Show abstract Video surveillance-based
    multi-task learning with swin transformer for earthwork activity classification
    2024, Engineering Applications of Artificial Intelligence Show abstract A review
    of deep learning techniques for detecting animals in aerial and satellite images
    2024, International Journal of Applied Earth Observation and Geoinformation Show
    abstract An intelligent identification and classification system of decoration
    waste based on deep learning model 2024, Waste Management Show abstract Deep learning-based
    models for environmental management: Recognizing construction, renovation, and
    demolition waste in-the-wild 2024, Journal of Environmental Management Show abstract
    View all citing articles on Scopus © 2022 The Authors. Published by Elsevier Ltd.
    Recommended articles AutoPrivacy: Automatic privacy protection and tagging suggestion
    for mobile social photo Computers & Security, Volume 76, 2018, pp. 341-353 Zhuo
    Wei, …, Jian Weng View PDF Should I stay or should I go? The impact of nature
    reserves on the survival and growth of dairy farms Journal of Environmental Management,
    Volume 328, 2023, Article 116993 Insa Thiermann, Thomas Bittmann View PDF Building
    of an edge enabled drone network ecosystem for bird species identification Ecological
    Informatics, Volume 68, 2022, Article 101540 Nabanita Das, …, Ananjan Maiti View
    PDF Show 3 more articles Article Metrics Citations Citation Indexes: 10 Captures
    Readers: 38 Social Media Shares, Likes & Comments: 2 View details About ScienceDirect
    Remote access Shopping cart Advertise Contact and support Terms and conditions
    Privacy policy Cookies are used by this site. Cookie settings | Your Privacy Choices
    All content on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: (Chen et al., 2023)
  journal: Journal of Environmental Management
  key_findings: '• The automatic segmentation and counting of roosting common cranes
    using UAV nighttime thermal images succeeded in counting cranes, with an Overall
    Accuracy of 91.47%, User''s Accuracy of 99.68%, and Producer''s Accuracy of 91.74%.

    • The computer vision and machine learning algorithms for counting cranes in the
    feeding station using UAV RGB-based daylight imagery yielded an Overall Accuracy
    of 94.5%, User''s Accuracy of 99.9%, and Producer''s Accuracy of 94.59%.

    • Thermal imaging for crane recognition and counting proved to be highly successful,
    especially because the cranes congregate at night roosts.'
  limitations: The algorithms developed in this study are specifically designed for
    counting cranes and may not be directly applicable to other species or counting
    scenarios. The accuracy of the algorithms may be affected by factors such as the
    size, shape, and behavior of the animals being counted, as well as the environmental
    conditions and camera settings.
  main_objective: To develop and validate advanced and accurate methods for estimating
    the number of cranes using thermal and visible light imaging combined with computer
    vision and machine learning.
  relevance_evaluation: This paper is highly relevant to the review's focus on developing
    automated systems for real-time monitoring of irrigation systems because it demonstrates
    the successful application of computer vision and machine learning techniques
    to accurately count cranes in both night and day conditions. The algorithms developed
    in this study could be adapted to count other animals or objects in agricultural
    settings, providing valuable data for irrigation management and other purposes.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Agamon Hula wetlands, Israel
  technologies_used: '• Thermal cameras

    • RGB imaging cameras

    • Unmanned aerial vehicles (UAVs)

    • Computer vision algorithms

    • Machine learning algorithms'
  title: Using computer vision, image analysis and UAVs for the automatic recognition
    and counting of common cranes (Grus grus)
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Gayam, K. K., Jain, A., Gehlot, A., Singh, R., Akram, V. S., Singh,
    A., ... Noya, I. D. (2022). Imperative Role of Automation and Wireless Technologies
    in Aquaponics Farming. Wireless Communications and Mobile Computing, 2022, 8290255.
    https://doi.org/10.1155/2022/8290255
  authors:
  - Gayam K.K.
  - Jain A.
  - Gehlot A.
  - Singh R.
  - Akram S.V.
  - Singh A.
  - Anand D.
  - Noya I.D.
  citation_count: '7'
  data_sources: null
  description: Food and agriculture are significant aspects that can meet the food
    demand estimated by the Food Agriculture Organization (FAO) by 2050. In addition
    to this, the United Nations sustainable development goals recommended implementing
    sustainable practices to meet food demand to achieve sustainability. Currently,
    aquaponics is one of the sustainable practices that require less land and water
    and has a low environmental impact. Aquaponics is a closed-loop and soil-less
    method of farming, where it requires intensive monitoring, control, and management.
    The advancement of wireless sensors and communication protocols empowered to implementation
    of an Internet of Things-(IoT-) based system for real-time monitoring, control,
    and management in aquaponics. This study presents a review of the wireless technology
    implementation and progress in aquaponics. Based on the review, the study discusses
    the significant water and environmental parameters of aquaponics. Followed by
    this, the study presents the implementation of remote, IoT, and ML-based monitoring
    of aquaponics. Finally, the review presents the recommendations such as edge and
    fog-based vision nodes, machine learning models for prediction, LoRa-based sensor
    nodes, and gateway-based architecture that are beneficial for the enhancement
    of wireless aquaponics and also for real-time prediction in the future.
  doi: 10.1155/2022/8290255
  explanation: The study focuses on the role of remote monitoring using IoT-enabled
    sensors and computer vision for controlling growth management in aquaponics farming.
    It discusses the importance of integrating sensors and communication technologies
    for real-time monitoring, data collection, and analysis to support informed decision-making.
    Wireless technologies, such as LoRa, are highlighted for their ability to provide
    long-range and low-power connectivity in aquaponics systems.
  extract_1: null
  extract_2: null
  full_citation: '>'
  full_text: '>

    "This website stores data such as cookies to enable essential site functionality,
    as well as marketing, personalization, and analytics. By remaining on this website
    you indicate your consent. Cookie Policy Journals Publish with us Publishing partnerships
    About us Blog Wireless Communications and Mobile Computing Journal overview For
    authors For reviewers For editors Table of Contents Special Issues Wireless Communications
    and Mobile Computing/ 2022/ Article On this page Abstract Introduction Methods
    Conclusion Data Availability Conflicts of Interest Acknowledgments References
    Copyright Related Articles Special Issue Internet of Things, Artificial Intelligence
    and Machine Learning: Architecture, Algorithms, and Applications View this Special
    Issue Review Article | Open Access Volume 2022 | Article ID 8290255 | https://doi.org/10.1155/2022/8290255
    Show citation Imperative Role of Automation and Wireless Technologies in Aquaponics
    Farming Kiran Kumari Gayam ,1Anuj Jain ,1Anita Gehlot ,2Rajesh Singh ,2Shaik Vaseem
    Akram ,2Aman Singh ,3Divya Anand ,4,5and Irene Delgado Noya5,6 Show more Academic
    Editor: Shafiq Ahmad Received 25 Mar 2022 Revised 19 May 2022 Accepted 25 May
    2022 Published 09 Jun 2022 Abstract Food and agriculture are significant aspects
    that can meet the food demand estimated by the Food Agriculture Organization (FAO)
    by 2050. In addition to this, the United Nations sustainable development goals
    recommended implementing sustainable practices to meet food demand to achieve
    sustainability. Currently, aquaponics is one of the sustainable practices that
    require less land and water and has a low environmental impact. Aquaponics is
    a closed-loop and soil-less method of farming, where it requires intensive monitoring,
    control, and management. The advancement of wireless sensors and communication
    protocols empowered to implementation of an Internet of Things- (IoT-) based system
    for real-time monitoring, control, and management in aquaponics. This study presents
    a review of the wireless technology implementation and progress in aquaponics.
    Based on the review, the study discusses the significant water and environmental
    parameters of aquaponics. Followed by this, the study presents the implementation
    of remote, IoT, and ML-based monitoring of aquaponics. Finally, the review presents
    the recommendations such as edge and fog-based vision nodes, machine learning
    models for prediction, LoRa-based sensor nodes, and gateway-based architecture
    that are beneficial for the enhancement of wireless aquaponics and also for real-time
    prediction in the future. 1. Introduction According to the most recent United
    Nations forecasts, the world’s population will expand from 6.8 billion now to
    9.1 billion in 2050, representing a quarter more food is needed than there is
    today [ 1]. According to an FAO assessment, the primary problems for world agriculture
    in the future decades will be producing 70% more food for an additional 2.3 billion
    people while tackling hunger and poverty, utilizing finite natural resources more
    efficiently, and adjusting to climate change [ 2]. Globally, there are still adequate
    land resources available to feed the world’s future population. However, FAO emphasized
    that much of the available acreage is only appropriate for cultivating a few commodities,
    and most of the unused land also suffers from chemical and physical restrictions,
    endemic diseases, and a lack of infrastructure, all of which are difficult to
    overcome. Healthy soils, land, and water are critical inputs in food production,
    and their scarcity in many parts of the world makes it critical to use and sustainably
    manage them [ 3]. The United Nations suggests that sensible water utilization
    through enhanced irrigation and storage technology, in conjunction with the creation
    of new drought-resistant crop types, can assist to sustain dryland output [ 4].
    Aquaponics farming is a type of sustainable agriculture that involves a symbiotic
    link between fish and plants [ 5]. When the fish produce waste, it is cycled out
    of the fish tank into the grow bed, where bacteria convert ammonia into nitrates
    that plants require to grow (Figure 1). The water is subsequently purified and
    restored to the fish tank, contributing to the highly efficient, zero-waste process
    of cultivating fish and plants together [ 6]. When compared with the traditional
    farming method, it uses 80 to 95% less water, and also, the water usage efficiency
    can be increased; the use of pesticides and fertilizers can be reduced in this
    method [ 7]. Aquaponics is a closed-loop and soil-less method of farming, where
    it requires intensive monitoring, control, and management [ 8]. So, it is recommended
    to implement wireless technologies in aquaponics for effective monitoring, control,
    and management [ 9]. Currently, the advancement of wireless sensors and communication
    protocols empowered to implementation of an IoT-based system is continuously monitoring
    and analyzing the complete system to produce vegetables and plants that are needed
    for human use in a well-planned and well-maintained ecosystem with optimum use
    of water and minimum farmlands [ 10].    Figure 1  Aquaponics system cycle. With
    motivation from the aspects, this study is aimed at providing a review of the
    significance and implementation of wireless and intelligent technologies in aquaponics
    farming. The study is drawn in such a way that it will provide a sequential way
    of understanding the various trends of wireless and intelligent technologies in
    aquaponics. The motive of this review is to bring various aspects that are specifically
    related to the effective real-time implementation of the aquaponics system. The
    major goals of this review paper are to identify (a) critical parameters that
    are suitable to monitor and control the growth of plants as well as fish; (b)
    to identify and discuss the growth of wireless technologies like IoT, edge, and
    fog computing implementation in an aquaponics system; and (c) evaluation of the
    progress of machine learning implementation in an aquaponics system that is used
    for real-time prediction of water quality identify suitably monitored and controlled
    parameters for effective growth of plants and fishes. This review also discusses
    the limitations of the previous studies and recommends a few suggestions such
    as edge and fog-based vision nodes, machine learning models for prediction, LoRa-based
    sensor nodes, and gateway-based architecture for the future enhancement in aquaponics.
    The contribution of the study is as follows: (i) The environmental and water-based
    parameters that affect the growth of the organisms in aquaponics are discussed
    (ii) The significance and function of the wireless-based systems for remote monitoring
    are discussed in this study with architecture (iii) The significance of machine
    learning algorithms and edge and fog computing for real-time prediction in aquaponics
    are presented The organization of the paper is as follows: Section 2 discusses
    the methodology of the review. Section 3 covers the parameters to be monitored
    in an aquaponics system. Section 4 covers the IoT systems and remote monitoring
    interfaces used. Section 5 covers the edge and fog-based architecture used. Section
    6 covers the machine learning techniques that are used in aquaponics. Section
    7 covers the recommendations and proposed architecture, and finally, the article
    concludes. 2. Methods In this section, the discussion of methods and approaches
    are implemented for carrying out the review. The objective of this review is to
    discuss the significance of wireless technologies implemented by previous studies.
    In the field of an aquaponics system, there is a scarcity of quality articles
    from the reputed journals, so in this review, the conference articles are also
    included. The articles on aquaponics are obtained from the Web of Science, Scopus,
    ScienceDirect, IEEE Xplore, and Google Scholar. Initially, all studies related
    to the aquaponics are examined, and only those articles that satisfy the selection
    criterion such as the abstracts of studies that were available are selected but
    not the full text of the study not examined for review; research that proposes
    methodologies but does not conduct experiments or validation is not eligible for
    review; dissertation work and thesis completed at the postgraduate and graduate
    levels are not reviewed; non-peer-reviewed research articles are not considered
    for review, and book chapters, patent applications, and communications are not
    reviewed. 3. Significance of Water and Environmental Parameters The aquaponics
    system is a combination of both aquaculture and hydroponics where plants and fish
    live in an integrated environment. The parameters related to both the water and
    the environment are to be monitored to ensure the proper growth and also to be
    healthy. 3.1. Water-Based Parameters In an aquaponics system, the quality of water
    is the main factor that is to be considered [ 10]. Water is the medium through
    which nutrients are provided to the plants. Considering automation regard, water
    is considered to be a complex factor as many parameters are dependent on one another.
    Aquaponics which is the integration of both aquaculture and hydroponics techniques
    is individually developed and adopted widely. For increasing the efficiency of
    water and sustainability, RAS design has been developed. But the ammonia present
    in the water begins to gather at the levels that are dangerous for the fish. So,
    for recirculating water, biofilters are being used. But the plants require nutrients
    and elements, which cannot be produced by water in the absence of fertilizers.
    But the use of fertilizers can lead to the disposal of water and replacement.
    The waste produced by fish can be used for the growing of plants. This process
    occurs indirectly and is called nitrification. To assure the standard quality
    in the solution of water, to make the process of nitrification favorable and growth
    of plants and keep fishes healthy at the same time, it is required to keep the
    right nutrient quantity, temperature, dissolved oxygen, pH, temperature, and salts
    during the complete process. 3.1.1. pH Measurement of the concentration of hydrogen
    ions is known as pH. It is the alkalinity or acidity measurement of the solution.
    The rate at which nitrification occurs and the availability of nutrients to the
    plants are affected by the pH of water [ 11]. To measure the pH in a solution,
    a pH meter is used [ 12]. Manual electronic probes, test strips, and automatic
    probes in controllers are three different methods by which the measurement of
    pH values is obtained. The value that is acceptable for the pH of water in the
    aquaponics component can be from 6.5 to 9.5, and the acceptable value is 5.5 to
    10, but this value may be varied slightly depending on the fish species. In slightly
    acidic solutions, the reproduction rate of fish may be decreased [ 13]. The optimum
    value is around 6.0 in the hydroponic component. Precipitation of Fe or Mn will
    occur if pH is more than 7.0, and root injury occurs if pH is less than 4.5 [
    14], and deficiency of nutrients is observed in plants [ 15]. The pH value is
    to be 7.0 to 9.0 for the nitrification process to occur. In an aquaponics system,
    the adjustment of pH values can be made by bases such as calcium and potassium
    because they act as a base for the nutrients [ 15, 16]. Minute changes in pH values
    (<0.3) in short time intervals can affect fish health very highly [ 15]. To the
    controller, the pH meter is connected to an automated system; the controller gets
    the change in output of the pH meter in millivolt and milliampere. The pH meter
    is then contacted with the controller and is tested in a solution to find the
    pH value. The value of output that is obtained is connected to the pH unit to
    the controller programming unit. A pH meter named B&C Electronics-SZ 1093 gives
    a range of 0-13, a maximum temperature value of 80°C, and a maximum pressure of
    7 bars [ 17]. The performance provided by a digital pH meter of 0.01 resolution
    and an ISFET ion-sensitive field-effect transistor is the same. A less percentage
    of error is provided by the Atlas EZO pH Sensor. Nonetheless, other options still
    exist [ 18]. An OMEGA PHE-45P pH sensor with lower maximum temperature resistance
    (60°C can be used in the aquaponics system [ 5]) and an Orion 3 Star meter from
    Thermo Fisher Scientific can be used to find the pH [ 11]. 3.1.2. Dissolved Oxygen
    The measure of the amount of oxygen that is dissolved in water which is available
    for living things in aquatics is dissolved oxygen. The organisms that share the
    environment of aquaponics are fish, bacteria, and plants for which dissolved oxygen
    is the most required parameter. The ability to support the life of aquatic organisms
    is determined by the oxygen amount present in water along with the level of water
    [ 19] At very low concentrations, oxygen is dissolved in water (in parts per million)
    and is considered to be the parameter that has an instant and extreme impact on
    the aquaponics [ 15]. Oxygen is made naturally in algae and green aquatic plants
    by photosynthesis. In every aquaponics system, it is mostly required to monitor
    the value of dissolved oxygen as its value changes sharply in small time intervals
    [ 19]. The temperature of the water and dissolved oxygen is strongly related to
    each other. Warm water has less oxygen. The intake of dissolved oxygen rises when
    fishes are taking food. For nitrifying bacteria, optimum levels are 4-8 milligrams/liter.
    Plants require dissolved oxygen of greater than 3 milligrams/liter [ 15]. If the
    oxygen is lowered, the fungus appears, and the roots of plants die. Dissolved
    oxygen of greater than 5 milligrams/liter is required by most species of fish.
    If the concentration of dissolved oxygen is low, the production of TAN will be
    perished [ 20]. The optical sensors detect the interaction of oxygen with particular
    luminous dyes. Because oxygen molecules interact with the dye when dissolved oxygen
    is present, the wavelengths that are returned are changed [ 21]. Galvanic and
    polarographic electromechanical options for measuring dissolved oxygen content
    exist. The existence of dissolved oxygen is determined by changing the electrical
    signal after applying a voltage to polarize or not polarize the system. Measurement
    systems for dissolved oxygen concentrations are costly. The data is transferred
    via a DO sensor coupled to a Modbus and TCP/IP technology [ 10]. In an aquaponics
    system, an Atlas DO probe with a capacity range of 0-100 milligrams/liter, maximum
    pressure of 3447 kPa, and 343 m of maximum depth is employed [ 18]. 3.1.3. Temperature
    In an aquaponics system, the temperature of the water is interconnected with many
    parameters that are related to water. The optimum value of temperature is in the
    range of 17-34°C for nitrification to take place. The nitrification process does
    not occur correctly, and bacteria productivity goes down if the temperature is
    below the value. A value of 18-30°C temperature is appropriate for the hydroponics
    component. A proper temperature value is to be maintained which decreases the
    disease risk in fish. Based on the type of fish, the suitable value of temperature
    changes. A temperature value of 22-32°C is favorable for tropical fish; for cold-water
    fish, the temperature to be maintained is 10-18°C. For other species, a temperature
    value in the 5-30°C range is favorable [ 15]. Calcium absorption is resisted in
    plants if the temperature of the water is high. To measure the temperature of
    water, the method used is to examine the temperature range, tolerance of salinity,
    and resolution. The sensor resolution is the factor that is important in the selection
    because many of the water temperature sensors have the required range covered.
    The sensor must not be waterproof only but should be designed to be submerged
    for a long time. A DS1820 temperature sensor along with an Arduino controller
    is used. The range of temperature in this sensor varies from -55°C to 125°C, and
    the resolution is about ±0.5°C. An IC named LM35 is used as temperature [ 18,
    22, 23]. 3.1.4. Ammonia In surface and wastewaters, ammonia is present as a dissolved
    gas [ 13]. The protein that is given to the fish, only 10% of it is transformed
    into ammonia [ 24]. In an aquaponics system from the waste that is excreted by
    fish, ammonia is produced and acts as the main part as it is the main element
    for nutrients in the plants. For the fish, ammonia is very toxic, if present in
    small quantities. It is most noticeable when it changes as strongly acidic or
    alkaline. For fishes, the advisable range is 0 to 2 mg/L [ 13]. The optimum range
    of TAN is <3 mg/L for fish in warm water and 1 mg/L for fish in cold water. For
    bacteria that oxidizes ammonia and nitrite, the optimum range is <3 mg/L and <30mg/L
    [ 15]. Since ammonia is present in little quantities and does not have any color
    or odor, to know whether it is present or not, sensing it is required. The sensor
    has a wire electrode inside a filling solution. The solution is separated from
    the medium which has the sample by an ion-selective membrane, mixed with ammonium
    ions [ 25] The pH of water and water temperature is to be known necessarily to
    increase the accuracy of measurement of ammonia. The ammonia amount present in
    the water solution creates a data synthesis problem between ammonia sensors, temperature
    sensors, and pH sensors. Since the ammonia concentration before the biofilter
    is not considered, these sensors are to be placed in the water tank. 3.1.5. Nitrification
    For plants, the main required nutrient inorganic is nitrogen. For the nitrification
    process to occur, ammonia is required which comes from the waste of the fish.
    It is in the form of ammonium and ammonia which is the function of pH, temperature,
    and salinity of water [ 26, 27]. Total ammonia-nitrogen concentration (TAN) is
    the sum of ammonium and ammonia [ 28]. Nitrification is the process in which the
    TAN is changed to nitrates [ 28]. With the help of ammonia-oxidizing bacteria,
    first TAN is oxidized into nitrite, and then, nitrate is converted into nitrates
    with nitrite-oxidizing bacteria [ 27]. So in an aquaponics system for nitrification,
    a biofilter is required. A hydroponic component, a biofilter for nitrification,
    and an aquaculture component are the constituents of an aquaponics system [ 29].
    3.1.6. Nitrate By nitrite-oxidizing bacteria, from ammonia, nitrate is obtained,
    the form in which plants can take the component of nitrogen that is required.
    For fish, nitrate is not dangerous. If the nitrate value is below 90 mg/L, it
    should not lead to health issues in fish [ 13], and the optimal range is 50-100 ppm.
    When designing a biofilter, this value is considered to be important. If the nitrates
    are present in large quantities, it means that it is dangerous to fish, and the
    biofilter is undersized [ 20]. To measure the nitrite concentration, the sensor
    that is used for knowing the ammonia concentration is used. 3.1.7. Nitrite By
    ammonia-oxidizing bacteria, from ammonia, nitrite is obtained. For aquatic life,
    nitrite is considered to be dangerous [ 20]. The required value of nitrite in
    water for the bacteria, fish, and plants to survive is 0-1 mg/L [ 13]. For the
    proper growth of plants and the bacteria to survive the same value of nitrite
    is required. The nitrite that is present should not make a problem when it is
    provided in the optimum range. The mix of nitride-ionized electrodes and the element
    used for sensing are made of polyvinyl chloride membrane, works as an exchange
    of ions, and reference electrode forms nitrite concentration sensors. The sensor
    will develop electrical potential which is proportional to the nitrite ion concentration
    in solution and provides the concentration of nitrite in water. 3.1.8. Electroconductivity
    Electroconductivity is a metric that measures a medium’s ability to conduct electric
    current (EC), and in aquaponics, this is related to salinity [ 5, 30]. If the
    electrical conductivity changes, the fish are affected. The death of fish may
    occur if the level of electroconductivity is high, and this indicates that the
    water is polluted. To have a balance, there should be minimum content of salt.
    For fishes, the range of optimum level is 100-2000 mS/cm. The range of 30-500 mS/cm
    is also accepted [ 13]. A method was proposed for controlling the nutrient solution
    in the hydroponics system by monitoring the electrical conductivity [ 14]. Enshi-shoho
    nutrient solution was used to provide control over EC, as it has a known EC of
    2400 mS/cm. These electroconductivity meters generally use a potentiometric method
    and four platinum electrodes. The use of this parameter is recommended to use
    [ 30]. 3.1.9. Level In an aquaponics system, the amount of water required is decided
    by the component size, i.e., fish tank. The health, growth of fish, and stress
    in fish are caused due to stocking density. The stocking number of fish is 20
    kilograms of fish per thousand liters of water [ 15]. Removal of fish waste, evapotranspiration
    in plants, evaporation, and splashing of fish while feeding are the main causes
    of the water loss in all the aquaponics systems. The amount of water that is consumed
    daily in a hydroponic system is 0.1 to 0.3% which depends on the fish tank, hydroponic
    ratio, the flow of water, temperature of water specifies of fishes and plants
    used, and the hydroponic type of system that is used [ 31]. By using a slight
    glass or floating device, the level of water in the tanks is measured manually.
    Ultrasonic sensors, laser, and radar-based sensors are the most advanced sensors
    for measuring the level of fluid. K8AK-LS1 water level controller is used which
    has a maximum temperature tolerance of 50°C [ 17]. A water level sensor that gives
    an analog output when connected to an Arduino controller is used [ 32]. To know
    the level of water in the tank, an array of sensors is used [ 22]. A circuit built
    with a BC546 NPN transistor is used to construct a water overflow level sensor
    [ 33]. An ultrasonic sensor is used to control the levels of water in the tank
    [ 23, 34]. 3.1.10. Total Dissolved Solids In water, the dissolved salts are present
    naturally. The number of dissolved materials, organic matter, and inorganic salts
    in the water represents total dissolved salt levels [ 35]. The desired amount
    of TDS for the fish is 1000 milligrams/liter but values below 2500 milligrams/liter
    are acceptable [ 13]. For many species of fish, TDS (>1000 mg/L) can be toxic.
    For the measurement of TDS, TDS meters are used as sensing units to measure the
    value of TDS in portable water. TDS can be measured using the sensor used for
    the measurement of electroconductivity. 3.1.11. Flow To calculate the proficiency
    of filtration (solids) and biofiltration (nitrification) and to decide the availability
    of nutrients for plants, the flow of water in an aquaponics unit is needed. In
    the unit, constant flow is to be maintained to avoid deficiency of nutrients in
    the plants and also stress in the fishes. The measurement of flow between the
    grow bed and the filters is most required. Depending on the system that is adopted,
    the flow rate varies. Water flowing must be in such a way that more amounts of
    nutrients and oxygen are obtained by the plants in systems based on NFT. The flow
    of water must be smaller than 1-2 L/min in an NFT system [ 15]. To clean the water,
    a siphon is used in the media-based technique. To clean the water each hour, the
    water flow rate is to be set. The flow of water is because of gravity in systems
    based on DWC. To guarantee the sufficient amount of nutrients received, the flow
    of water through the channels should be 1 to 4 hours. Depending on the size of
    the channel used and the capacity of water, the optimal flow rate of water is
    determined. To know the water flow between the fish tank and grow bed, a water
    sensor is used [ 4], and in between the grow bed and fish tank, the flow meter
    is kept [ 36]. 3.1.12. Salinity The quantity of salt concentration present in
    the water is salinity [ 30]. The growth and density of the fish are affected by
    salinity [ 20]. Like TDS, salinity is obtained by electroconductivity. Depending
    on the fish species, the required value of salinity changes. 3.1.13. Alkalinity
    In an aquaponics system, the measurement of the concentration of the bases, mainly
    carbonate as well as bicarbonate, is alkalinity. The measure of the negative ions
    is the alkalinity, and the measure of positive ions is the hardness. The ability
    of water to withstand changes in the pH or the ability to neutralize acids is
    also referred to as alkalinity. There is a very high value of pH even if low levels
    of the acids are present [ 20]. The ammonia becomes toxic if the alkalinity level
    is high. 50 to 150 milligrams/liter of CaCO3 is the required range [ 13]. 3.1.14.
    Water Hardness The amount of the positively charged magnesium salts and calcium
    in the solution is measured because they are necessary for the fish metabolic
    reaction and also for the formation of scale and bone. Stress in fish is caused
    due to low levels of water hardness, and high levels are harmful because it increases
    the pH of water, which results in a reduced rate of absorption and nitrification
    in plants. The range for the hardness of water can be from 50 to 150 milligrams/liter,
    but for many of the species, the milligrams/liter is acceptable [ 13]. TDS or
    electroconductivity can be used to determine water hardness [ 37]. 3.2. Environment-Based
    Parameters For obtaining stability and for better development of the fish as well
    as plants, it is required that the parameters related to the environment are to
    be monitored and controlled [ 7]. 3.2.1. CO2 In photosynthesis, carbon dioxide
    is a necessary component. The CO2 in the air is used by the plants in indoor systems
    which are in large numbers. So, CO2 is used artificially, and it is required to
    control the amount used. The optimum range level for many crops grown indoors
    is in the range of 340-1300 ppm [ 38]. The different amounts of CO2 are required
    because it depends on the crop type, the available light, the temperature of the
    air, and RH. For the proper growth of fish, the carbonic acid levels should be
    less than 5 mg/L [ 20, 39]; otherwise, it is dangerous for fish. A sensor named
    MG811 is used to measure the amount of carbon dioxide in the air [ 30]. 3.2.2.
    Air Temperature Plant health is affected by the temperature of the air. In the
    aquaponics unit, the required temperature to grow the vegetable plants is 18-30°C.
    At temperatures greater than this, they start flowering and then to seeds [ 15].
    For the proper transpiration of the plants, the temperature of the air is needed.
    When selecting temperature sensors, the range of temperature, the sensing element,
    the contactless or contact, and the method of calibration are to be checked. A
    thermistor is used to measure the temperature of air and humidity together in
    an aquaponics unit. A DHT11 thermistor [ 23] and a DHT22 thermistor are used which
    is having more accuracy and a range of values greater than DHT11 [ 38]. 3.2.3.
    Relative Humidity The amount of moisture in the air is relative humidity [ 40].
    Depending on the stages of growth and the crop type used, the optimum level of
    relative humidity varies. Commonly, 50-80% is considered but it depends on the
    indoor temperature of the system. The air temperature sensor also provides relative
    humidity. In an aquaponics system, the DHT11 sensor is used to measure the relative
    humidity values [ 41]. 3.2.4. Light Intensity In indoor provisions, the sunlight
    is not available or available in less amount, which is essential for plants. In
    an aquaponics system to make the sunlight available to the plants, artificial
    lighting is used. Light is measured in intensity. Only some part of the light
    spectrum is used by the plants known as photosynthetically active radiation (PAR).
    It is the solar radiation spectral range where the photosynthetic organisms can
    process [ 42]. For a day, the crops require light for 14 to 18 hours. A light-dependent
    resistor (LDR) can be used for measuring the lighting system’s radiation intensity.
    To measure the ambient light intensity, LDR is used [ 22, 23]. 3.2.5. Media Moisture
    The content of water in soil present in the media base is the media moisture.
    If a media-based type system is used, then it is necessary to measure the moisture
    of the medium. A soil moisture sensor is good to be used for this type which gives
    the accurate amount of water needed for the plants. The capacity of water that
    the soil holds is recommended to be checked. It was found that depending on the
    type of soil, the optimum ranges vary from 30 to 60 cbars [ 43]. An FC-28 can
    be used as a moisture sensor to measure soil moisture [ 38]. From the studies,
    the aquaponics system parameters that are required and their optimized ranges
    are summarized in Table 1. Table 1  Aquaponics parameters and their optimal range.
    4. Real-Time Monitoring Systems Internet of Things, otherwise called the IoT,
    is an idea that plans to grow the advantages of persistently associated web networks.
    The integration of a controller, sensors, and the Internet into physical objects
    such as food gadgets and equipment allows for global information sharing. This
    additionally utilizes the idea of IoT because the data from the estimation of
    the sensor can be obtained through cell phone applications and sites from any
    place with the Internet association. With the presentation of computerization,
    keen techniques, and availability in the cultivating business, another entryway
    was opened for the upgrading of these aquaponics frameworks. The normal advantages
    of keen mechanization are a critical decrease of difficult work and a more vigorous
    control of the interaction by expanding the availability and availability of the
    boundaries and utilizing PC abilities to settle on information-driven choices
    [ 44]. 4.1. Interfaces for Remote Monitoring Checking interfaces are ordinarily
    a climate (intelligent or not) that shows a portion of the intrigued boundaries
    with regard to the cycle to the client or partner. This perception cycle is critical
    to ultimate choice making. IoT innovation empowers these observing interfaces
    to show esteems through remote organizations, even continuously. A web application
    that exhibited a dashboard associated with a microcontroller to screen chosen
    hydroponics boundaries is used [ 5]. In the very year, a Raspberry Pi is used
    to do all the framework estimation units; at that point, the sensors’ information
    is shipped off an electronic stage where it is put away and shown [ 45]. After
    a year, an iOS application that permitted to screen the framework climate persistently
    by getting information straightforwardly from the frameworks’ microcontrollers
    is used [ 46]. The course of these joint efforts is going towards continuous dependability
    and portability (online as well as an application for cell phones). 4.2. Applications
    That Are Controlled Remotely Controller applications are characterized by their
    capacity to flag framework actuators to communicate or modify some boundaries.
    With controller applications, administrators can on or off the water siphon or
    light when essential, change estimations of basic clocks to adjust the plants’
    development cycle, etc. From the inspected papers, a GSM and Arduino-based observing
    and controlling framework is used which sends ready message to administrators
    when estimations are outside explicit reaches. Graphical UIs are intended to show
    the data and information that could be separated from the framework [ 30]. The
    coordinated effort was utilizing Blynk, a multilanguage stage that empowers controllers
    of various microcontrollers like Arduino and Raspberry Pi [ 47]. A microcontroller
    along with a GSM receptor in a hydroponics framework is used. Accordingly, administrators
    can send messages to the receiver so continuous authority over the supply of water
    or temperature is attained [ 48]. An Arduino associated with a web worker through
    an Ethernet Shield, a UI was made to permit ongoing checking and control of the
    water-related sensor estimations, for example, switching on or off the fumes,
    siphons, and fog creators [ 38]. An IoT-based hydroponics framework that permits
    distant checking and control of the framework boundaries was made. The creators
    utilized a Modbus TCP standard convention to pull estimation information from
    the detecting hubs of an administrative PC [ 10]. A framework with a microcontroller
    associated with a Ubuntu IoT Cloud. The framework could be gotten to screen and
    control the boundaries consequently dependent on the detected inputs [ 49]. The
    creators in this segment added the controlling boundary into the situation. As
    of now, the perception of the boundaries in the framework is not sufficient and
    is important to control such boundaries for a superior framework. Architecture
    has been implemented by different studies as shown in Figure 2. The aquaponics
    sensor mote consists of different sensors to monitor the environmental parameters
    and water which are required for the healthy development of the plants as well
    as fish. The data that is sensed from the different sensor motes are sent through
    wireless communication to the gateway. The gateway receives the data and provides
    the information from which sensor node the data has been received and displays
    it on the display unit. The data is then logged on the web or the mobile application
    Internet, and the user can monitor it from anywhere through the web/mobile app.    Figure
    2  Remote monitoring of aquaponics system [ 50]. Table 2 illustrates the latest
    studies that is focused on IoT-based aquaponics platforms. From the table, it
    concludes that the majority of the studies implemented Wi-Fi as communication
    medium to transmit data to the cloud server for real-time monitoring and controlling.
    [ 51] implemented the edge computing technique in aquaponics with the ML for the
    automation in the aquaponics system; however, it is implemented in the small scale
    only. [ 52] implemented a technique for the optimization of the nutrients; however,
    they are no any information related to the data transmission from the Raspberry
    Pi to the cloud server. From the overall studies, it is identified that the customization
    hardware is necessary to carry out for meeting the requirements of aquaponics
    system. Table 2  Previous IoT-based platform for aquaponics. In [ 54], the automatic
    triggering of a water pumping event has an accuracy of 0.9795 because of the ultrasonic
    sensor and soil moisture sensor, and in this study, the threshold for the intersection
    of union (IoU) is set to 0.5 to achieve higher accuracy, with an average precision
    (mAP) of 75.0 and an F1 score of 0.9556 [ 58]. The system appears to be functional
    based on the test results; nonetheless, several deviations are discovered, including
    an RTC test showing a delay time of 00.02.10 of RTC compared to the national standard
    time and an error of 2.4 percent identified on a calibrated TDS sensor testing
    [ 52]. When the lettuce’s size and production were compared to those grown in
    uncontrolled aquaponics systems, the yield showed a considerable increase in size,
    with some of them reaching 40 to 45 inches in diameter, and also, the cost of
    managing nutritional parameters is reduced by more than 75% 4.3. Wireless Technologies
    The remote innovations are seldom introduced and are generally connected to the
    two past areas. By the by, it was discovered that a few supporters were centered
    on creating some remote advances in aquaponics that improve availability. A design
    to screen and control a hydroponics framework with sensor data and Arduino is
    created. Information is effectively put away on WRT hubs and sent to OpenWrt workers
    utilizing the Wi-Fi module [ 41]. A hydroponics framework utilizing the 6LoWPAN
    convention and a remote sensor organization (WSN) was planned [ 59]. If pH and
    temperature values of the aquaponics exceed the threshold range [ 4], then GSM
    sends warnings to authorities and updates it on ThingSpeak. An online observing
    framework utilizing ThingSpeak IoT stage with Arduino Uno and ESP8266-01 that
    is Wi-Fi handset was built [ 23]. A Raspberry Pi alongside a Wi-Fi dongle to give
    a web network to the framework is used. The framework utilizes cloud-based stages
    for storing and controlling the assorted boundaries of the hydroponics framework
    [ 33]. The utilization of remote advances in the sensors or transmission of information
    makes way for enhancements in e-checking and control of boundaries. Table 3 illustrates
    the technical specifications of wireless communication protocol that can be implemented
    in IoT-based systems for aquaponics farming. Table 3  Technical specifications
    of wireless communication IoT protocols. 5. Edge-Assisted Architecture Currently,
    the integration of a huge number of sensors and devices in the physical environment
    is generating a huge amount of data in the IoT. In traditional cloud computing,
    all data must be transmitted to centralized servers, and the findings must be
    transmitted downstream to the sensors and devices after computation [ 60]. This
    process imposes a significant load on the network, such as bandwidth, data transmission
    costs, and resources. Edge and fog computing overcome these challenges, as the
    data computation or storage is deployed at the edge of the network. Furthermore,
    the distributed architecture may control network traffic and prevent traffic peaks
    in IoT networks, lowering transmission latency between edge/cloudlet servers and
    end-users and reducing reaction times for real-time IoT applications when compared
    to standard cloud services [ 61]. A multilayered edge architecture is proposed
    to analyze the data between the cloud and the fog computing layers with low latency
    for IoT devices in real time [ 62]. An “offline-first” architecture for the low-cost
    and automated household aquaponics units is proposed. This moves the storage of
    data, machine learning, and computation away from the cloud platforms into the
    platforms that are preserving privacy [ 63]. Architecture is being implemented
    by different studies on edge-based computing as shown in Figure 3. The architecture
    consists of different sensor motes located at different locations. The sensor
    nodes consist of different sensors and actuators that are required for the sensing
    the different environmental and water-based that are required for the healthy
    growth of the plants and the fish. The data sensed by these sensors is sent to
    the edge computing node wirelessly. The data received is then analyzed and processed
    by the edge computing node, and also, predictive analytics is performed through
    computing, coprocessor, and AI model. From the edge computing node, the outcome/data
    is transmitted to the gateway. The gateway then analyzes the sensor node from
    which the data has been obtained, and using Wi-Fi, the data is sent to the cloud
    server.    Figure 3  Edge-based computing node for real-time prediction for aquaponics.
    6. Machine Learning Techniques Used in Aquaponics Farmers can get more from the
    land by using resources sustainably with the help of artificial intelligence.
    Using artificial intelligence, farmers can know the conditions of temperature,
    weather, energy usage, water, and the condition of soil collected from their farm.
    Farmers are now able to use the sensor data that is captured to predict the yield
    and make them better equipped for natural disasters and climatic conditions using
    intelligent data processing techniques like machine learning. Machine learning
    is a branch of artificial intelligence that allows machines to learn from their
    mistakes. It uses computational approaches to learn directly from datasets rather
    than relying on a model of fixed equations. A cloud-based monitoring system in
    aquaponics is developed which measures the temperature of the water, depth of
    water, and the value of dissolved oxygen. To monitor the fish activity, three
    infrared distance sensors were connected to the aquarium glass. Through the fish
    activity sensing, the fish metabolic rate was calculated using the regression
    analysis [ 64]. A real-time water quality monitoring system is developed based
    on assessing time series motion trajectories of live fish and using a neural network
    algorithm to estimate the frequency of pattern changes in these trajectories [
    65]. The author developed an ML-based IoT system for optimizing nutrient supply
    in the aquaponics system. The nutrient values were measured with Vernier sensors,
    and an actuator system was created to feed the nutrient into the environment in
    a closed loop. In this feature selection techniques like XG Boost classifier and
    recursive feature elimination with extra, tree classifier was used for ranking
    the features [ 66]. An aquaponics monitoring and control system is designed with
    fuzzy logic to evaluate the input and provide the proper outputs automatically.
    A genetic algorithm is used for the optimization of the parameters of the PDF
    and FPDF controllers. Better results were obtained in humidity and temperature
    control of the greenhouse when compared with the traditional PDF controller [
    67]. A method is proposed on -learning to get the control the factors that rely
    on the environment in the greenhouse and then combined with a CBR to get the optimal
    control of the temperature of the greenhouse [ 68]. A branch and bound search
    algorithm in a discrete model of the predictive control of greenhouse is proposed
    which reduced consumption of energy without affecting control accuracy [ 69].
    A model is proposed based on a neural network based on the time series of a nonlinear
    autoregressive with a model based on external input. The control effect of humidity
    and temperature showed that the stability of the controller is more [ 70]. The
    Kalman filter algorithm was combined with the traditional PID control algorithm
    to control the temperature of the greenhouse which improved the control effect,
    the shorter response time and the higher system stability, and a better convergence
    [ 71]. Table 4 illustrates the ML model implementation in aquaponics for the water
    quality monitoring in the aquaponics system. Support vector machine (SVM), random
    forest (RF), k-nearest neighbor (k-NN), artificial neural network (ANN), Hammerstein-Wiener
    (HW), convolutional neural network (CNN), radial basis function (RBF), and recurrent
    neural network (RNN) are the few models that are addressed in the previous studies
    for water treatment and monitoring. Table 4  ML model implementation in the aquaponics
    system [ 72]. 7. Recommendations 7.1. Sensors and Actuators In an aquaponics system,
    the water-related and environmental parameters are to be monitored and controlled.
    In an aquaponics system, the sensors that are used to be much more accurate with
    less error because the development of plants as well as fish are affected if values
    are not accurate. The actuators should also be operated based on the sensor value
    obtained. 7.2. Communication Technologies Wireless communication protocols play
    a very important role in monitoring the aquaponics units from a remote location.
    GSM/GPRS with personal area network technologies like Bluetooth and Zigbee is
    also used for transmission of the sensory data. The transmission range is limited
    to 100 m for these technologies. WPAN has the limitation of short-range, and GSM/GPRS
    has the limitation of high-power consumption. LoRa (long range) wireless communication
    overcomes these limitations in the technologies that have been used previously.
    7.3. Edge and Fog-Based Vision Node Edge and fog computing provides an opportunity
    for data processing in less time with enhanced latency. In aquaponics, the continuous
    monitoring of plants and fishes in terms of growth and health is highly demanded
    to enhance the better yield. To identify the growth and health of plants and fishes
    effectively, edge and fog-based vision nodes need to be incorporated. Edge and
    fog-based vision nodes enable to detect and predict the growth and diseases of
    fishes and plants in real-time. 7.4. ML Models for Prediction Machine learning
    models have gained wide attention in the prediction of events based on real-time
    sensor data obtained from the sensors. In aquaponics, real-time prediction is
    highly required for maintaining the healthy growth of plants and fishes in order
    of enhancing productivity. The incorporation of the ML model in the vision node
    and edge-based sensor node enhances the system to predict depending on real-time
    image data and sensor data. 7.5. LoRa-Based Sensor Node and Gateway-Based Architecture
    An architecture is proposed as shown in Figure 4. The proposed system consists
    of an aquaponics system to which different sensors are attached to sense the environmental
    parameters and also the water-based parameters such as humidity, temperature,
    light, pH, electrical conductivity, and water level. The sensors are altogether
    considered a sensor node. The sensor node senses the required parameters from
    the different sensors. The sensed data from the different sensor nodes is collected,
    and the data is then sent with the help of a LoRa to the gateway. The gateway
    identifies the nodes from which the packets of data have been received and with
    the help of Wi-Fi connectivity is sent to the cloud server. The prediction is
    done on the data that is obtained from sensors using machine learning algorithms.    Figure
    4  Proposed architecture for real-time prediction in aquaponics system. 8. Conclusion
    and Future Scope Food and agriculture are significant considerations that can
    meet the predicted food demand by the Food and Agriculture Organization (FAO)
    by 2050. Furthermore, the United Nations suggests that sensible water utilization
    through enhanced irrigation and storage technology, in conjunction with the creation
    of new drought-resistant crop types, can assist to sustain dryland output. Aquaponics
    is one of the sustainable farming approaches that use a closed-loop and soil-less
    method, so wireless technologies must be integrated for real-time monitoring,
    controlling, and managing from any remote location. With the motivation of the
    above aspects, this study conducts a review of the aquaponics system, and from
    the review, it has identified different critical parameters that are required
    for the effective growth of plants and fishes. In addition to this, the study
    discusses the significance of wireless monitoring with the integration of sensors
    and communication technologies. Edge and fog computing-based architectures for
    the implementation of ML-based wireless systems in aquaponics for real-time prediction
    are also discussed. Finally, based on the review, the discussion on the limitations
    is presented and also recommended a few suggestions for future enhancement in
    aquaponics monitoring such as edge and fog-based vision nodes, ML models for prediction,
    LoRa-based sensor nodes, and gateway-based architecture. Data Availability The
    data presented in this study are available on request from the corresponding author.
    Conflicts of Interest The authors declare that there is no conflict of interest
    regarding the publication of this article. Acknowledgments The research was supported
    by the Universidad Europea del Atlántico, Santandar, Spain. The authors would
    like to thank and extend their kind support to the university. References FAO
    - News article: 2050: a third more mouths to feed, May 2022, https://www.fao.org/news/story/en/item/35571/icode/.
    Food and agriculture projections to 2050 | Global Perspectives Studies | Food
    and Agriculture Organization of the United Nations, May 2022, https://www.fao.org/global-perspectives-studies/food-agriculture-projections-to-2050/en/.
    “Food security and nutrition and sustainable agriculture .:. Sustainable development
    knowledge platform,” May 2022, https://sustainabledevelopment.un.org/topics/foodagriculture.
    View at: Google Scholar Food security and nutrition and sustainable agriculture
    | Department of Economic and Social Affairs, May 2022, https://sdgs.un.org/topics/food-security-and-nutrition-and-sustainable-agriculture.
    B. König, J. Janker, T. Reinhardt, M. Villarroel, and R. Junge, “Analysis of aquaponics
    as an emerging technological innovation system,” Journal of Cleaner Production,
    vol. 180, pp. 232–243, 2018. View at: Publisher Site | Google Scholar A. R. Yanes,
    P. Martinez, and R. Ahmad, “Towards automated aquaponics: a review on monitoring,
    IoT, and smart systems,” Journal of Cleaner Production, vol. 263, p. 121571, 2020.
    View at: Publisher Site | Google Scholar A. J. van der Goot, P. J. M. Pelgrom,
    J. A. M. Berghout et al., “Concepts for further sustainable production of foods,”
    Journal of Food Engineering, vol. 168, pp. 42–51, 2016. View at: Publisher Site
    | Google Scholar W. Kloas, R. Groß, D. Baganz et al., “A new concept for aquaponic
    systems to improve sustainability, increase productivity, and reduce environmental
    impacts,” Aquaculture Environment Interactions, vol. 7, no. 2, pp. 179–192, 2015.
    View at: Publisher Site | Google Scholar S. A. Z. Murad, A. Harun, S. N. Mohyar,
    R. Sapawi, and S. Y. Ten, “Design of aquaponics water monitoring system using
    Arduino microcontroller,” in AIP Conference Proceedings, vol. 1885, no. 1, p.
    020248, Ao Nang, Thailand, 2017. View at: Google Scholar M. Manju, V. Karthik,
    S. Hariharan, and B. Sreekar, “Real time monitoring of the environmental parameters
    of an aquaponic system based on Internet of Things,” in 2017 Third International
    Conference on Science Technology Engineering & Management (ICONSTEM), pp. 943–948,
    Chennai, India, 2017. View at: Google Scholar Y. Wei, W. Li, D. An, D. Li, Y.
    Jiao, and Q. Wei, “Equipment and intelligent control system in aquaponics: a review,”
    IEEE Access, vol. 7, pp. 169306–169326, 2019. View at: Publisher Site | Google
    Scholar F. Blidariu and A. Grozea, “Increasing the economical efficiency and sustainability
    of indoor fish farming by means of aquaponics-review,” Animal science and biotechnologies,
    vol. 44, no. 2, pp. 1–8, 2011. View at: Google Scholar M. A. Nichols and N. A.
    Savidov, “Aquaponics: a nutrient and water efficient production system,” II International
    Symposium on Soilless Culture and Hydroponics, vol. 947, pp. 129–132, 2011. View
    at: Google Scholar M. Odema, I. Adly, A. Wahba, and H. Ragai, “Smart aquaponics
    system for industrial Internet of Things,” in International Conference on Advanced
    Intelligent Systems and Informatics, vol. 639, pp. 844–854, Cairo, Egypt, 2017.
    View at: Google Scholar D. D. Kuhn, D. D. Drahos, L. Marsh, and G. J. Flick Jr.,
    “Evaluation of nitrifying bacteria product to improve nitrification efficacy in
    recirculating aquaculture systems,” Aquacultural Engineering, vol. 43, no. 2,
    pp. 78–82, 2010. View at: Publisher Site | Google Scholar pH meter | Definition,
    Principle, & Facts | Britannica, Mar 2022, https://www.britannica.com/technology/pH-meter.
    N. M. Stone and H. K. Thomforde, Understanding Your Fish Pond Water Analysis Report,
    Cooperative Extension Program, University of Arkansas at Pine Bluff, US, 2004.
    T. Wada, Theory and Technology to Control the Nutrient Solution of Hydroponics,
    Plant Factory Using Artificial Light, Elsevier, 2019. C. Somerville, M. Cohen,
    E. Pantanella, A. Stankus, and A. Lovatelli, “Small-scale aquaponic food production:
    integrated fish and plant farming,” FAO Fisheries and Aquaculture Technical Paper,
    vol. 589, 2014. View at: Google Scholar M. A. Zamora-Izquierdo, J. Santa, J. A.
    Martínez, V. Martínez, and A. F. Skarmeta, “Smart farming IoT platform based on
    edge and cloud computing,” Biosystems Engineering, vol. 177, pp. 4–17, 2019. View
    at: Publisher Site | Google Scholar J. P. Mandap, D. Sze, G. N. Reyes, S. M. Dumlao,
    R. Reyes, and W. Y. D. Chung, “Aquaponics pH level, temperature, and dissolved
    oxygen monitoring and control system using raspberry pi as network backbone,”
    in TENCON 2018-2018 IEEE Region 10 Conference, pp. 1381–1386, Jeju, Korea (South),
    2018. View at: Google Scholar R. Sallenave, Understanding Water Quality Parameters
    to Better Manage your Pond, NM State University, Cooperative Extension Service,
    2012. A. Bhatnagar and G. Singh, “Culture fisheries in village ponds: a multi-location
    study in Haryana, India,” Agriculture and Biology Journal of North America, vol.
    1, no. 5, pp. 961–968, 2010. View at: Publisher Site | Google Scholar A. Bhatnagar
    and P. Devi, “Water quality guidelines for the management of pond fish culture,”
    International Journal of Environmental Sciences, vol. 3, no. 6, pp. 1980–2009,
    2013. View at: Google Scholar “Measuring dissolved oxygen - environmental measurement
    systems,” Mar 2022, https://www.fondriest.com/environmental-measurements/measurements/measuring-water-quality/dissolved-oxygen-sensors-and-methods/.
    View at: Google Scholar M. N. Mamatha and S. N. Namratha, “Design & implementation
    of indoor farming using automated aquaponics system,” in IEEE International Conference
    on Smart Technologies and Management for Computing, Communication, Controls, Energy
    and Materials (ICSTM), pp. 396–401, Chennai, India, 2017. View at: Google Scholar
    B. Sreelekshmi and K. N. Madhusoodanan, Automated Aquaponics System in Emerging
    Trends in Engineering, Science and Technology for Society, Energy and Environment,
    CRC Press, 2018. R. V. Tyson, D. D. Treadwell, and E. H. Simonne, “Opportunities
    and challenges to sustainability in aquaponic systems,” HortTechnology, vol. 21,
    no. 1, pp. 6–13, 2011. View at: Google Scholar “Ammonia and ammonium ion measurement
    methods for water analysis,” Mar 2022, https://www.ysi.com/parameters/ammonia.
    View at: Google Scholar A. C. Anthonisen, R. C. Loehr, T. B. S. Prakasam, and
    E. G. Srinath, “Inhibition of nitrification by ammonia and nitrous acid,” Journal
    Water Pollution Control Federation, vol. 48, pp. 835–852, 1976. View at: Google
    Scholar J. M. Ebeling, M. B. Timmons, and J. J. Bisogni, “Engineering analysis
    of the stoichiometry of photoautotrophic, autotrophic, and heterotrophic removal
    of ammonia-nitrogen in aquaculture systems,” Aquaculture, vol. 257, no. 1–4, pp.
    346–358, 2006. View at: Publisher Site | Google Scholar S. Wongkiew, Z. Hu, K.
    Chandran, J. W. Lee, and S. K. Khanal, “Nitrogen transformations in aquaponic
    systems: a review,” Aquacultural Engineering, vol. 76, pp. 9–19, 2017. View at:
    Publisher Site | Google Scholar D. C. Love, J. P. Fry, L. Genello et al., “An
    international survey of aquaponics practitioners,” PloS one, vol. 9, no. 7, article
    e102662, 2014. View at: Google Scholar A. M. Nagayo, C. Mendoza, E. Vega, R. K.
    Al Izki, and R. S. Jamisola, “An automated solar-powered aquaponics system towards
    agricultural sustainability in the Sultanate of Oman,” in IEEE International Conference
    on Smart Grid and Smart Cities (ICSGSC), pp. 42–49, Singapore, 2017. View at:
    Google Scholar C. Maucieri, C. Nicoletto, R. Junge, Z. Schmautz, P. Sambo, and
    M. Borin, “Hydroponic systems and water management in aquaponics: a review,” Italian
    Journal of Agronomy, vol. 13, no. 1, pp. 1–11, 2018. View at: Google Scholar M.
    Mehra, S. Saxena, S. Sankaranarayanan, R. J. Tom, and M. Veeramanikandan, “IoT
    based hydroponics system using deep neural networks,” Computers and Electronics
    in Agriculture, vol. 155, pp. 473–486, 2018. View at: Publisher Site | Google
    Scholar N. K. Jacob, “IoT powered portable aquaponics system,” in Proceedings
    of the Second International Conference on Internet of things, Data and Cloud Computing,
    pp. 1–5, Cambridge, United Kingdom, 2017. View at: Google Scholar T. Y. Kyaw and
    A. K. Ng, “Smart aquaponics system for urban farming,” Energy Procedia, vol. 143,
    pp. 342–347, 2017. View at: Publisher Site | Google Scholar P. K. Weber-Scannell
    and L. K. Duffy, “Effects of total dissolved solids on aquatic organism: a review
    of literature and recommendation for salmonid species,” American Journal of Environmental
    Sciences, vol. 3, 2007. View at: Google Scholar “Global water, water instrumentation
    for environmental monitoring,” Mar 2022, https://www.ysi.com/products/global-water.
    View at: Google Scholar OMAFRA Crops Home Page, Mar 2022, http://www.omafra.gov.on.ca/english/crops/.
    B. Santhosh and N. P. Singh, “Guidelines for water quality management for fish
    culture in Tripura,” ICAR Research Complex for NEH Region, Tripura Center, Publication,
    vol. 29, no. 10, 2007. View at: Google Scholar W. Vernandhes, N. S. Salahuddin,
    A. Kowanda, and S. P. Sari, “Smart aquaponic with monitoring and control system
    based on IoT,” in 2017 second international conference on informatics and computing
    (ICIC), pp. 1–6, Jayapura, Indonesia, 2017. View at: Google Scholar J. C. Bakker,
    Analysis of humidity effects on growth and production of glasshouse fruit vegetables
    ÇGOS~I, 1991. D. Wang, J. Zhao, L. Huang, and D. Xu, “Design of a smart monitoring
    and control system for aquaponics based on OpenWrt,” in Proceedings of the 5th
    International Conference on Information Engineering for Mechanics and Materials,
    vol. 21, pp. 937–942, Huhhot, Inner Mongolia, 2015. View at: Google Scholar C.
    Barnes, T. Tibbitts, J. Sager et al., “Accuracy of quantum sensors measuring yield
    photon flux and photosynthetic photon flux,” HortScience, vol. 28, no. 12, pp.
    1197–1200, 1993. View at: Publisher Site | Google Scholar H. Werner, Measuring
    Soil Moisture for Irrigation Water Management, Cooperative Extension Service,
    South Dakota State University, US Department, 1992. P. Martinez, R. Ahmad, and
    M. Al-Hussein, “A vision-based system for pre-inspection of steel frame manufacturing,”
    Automation in Construction, vol. 97, pp. 151–163, 2019. View at: Publisher Site
    | Google Scholar A. Dutta, P. Dahal, R. Prajapati, P. Tamang, and E. S. Kumar,
    “IoT based aquaponics monitoring system,” in 1st KEC Conference Proceedings, vol.
    1, pp. 75–80, Lalitpur, Nepal, 2018. View at: Google Scholar R. M. A. Haseeb-Ur-Rehman,
    M. Liaqat, A. H. M. Aman et al., “Sensor cloud frameworks: state-of-the-art, taxonomy,
    and research issues,” IEEE Sensors Journal, vol. 21, no. 20, pp. 22347–22370,
    2021. View at: Publisher Site | Google Scholar C. S. Arvind, R. Jyothi, K. Kaushal,
    G. Girish, R. Saurav, and G. Chetankumar, “Edge computing based smart aquaponics
    monitoring system using deep learning in IoT environment,” 2020 IEEE Symp.Ser.
    Comput. Intell. SSCI, vol. 2020, pp. 1485–1491, 2020. View at: Publisher Site
    | Google Scholar S. B. Dhal, K. Jungbluth, R. Lin et al., “A machine-learning
    based IoT system for optimizing nutrient supply in commercial aquaponic operations,”
    vol. 22, pp. 1–14, 2022. View at: Google Scholar P. M. Ferreira and A. E. Ruano,
    “Discrete model-based greenhouse environmental control using the branch & bound
    algorithm,” IFAC Proceedings, vol. 41, no. 2, pp. 2937–2943, 2008. View at: Publisher
    Site | Google Scholar A. Manonmani, T. Thyagarajan, M. Elango, and S. Sutha, “Modelling
    and control of greenhouse system using neural networks,” Transactions of the Institute
    of Measurement and Control, vol. 40, no. 3, pp. 918–929, 2018. View at: Publisher
    Site | Google Scholar Z. Gao, L. He, and X. Yue, “Design of PID controller for
    greenhouse temperature based on Kalman,” in Proceedings of the 3rd International
    Conference on Intelligent Information Processing, pp. 1–4, Guilin, China, 2018.
    View at: Google Scholar F. L. Valiente, R. G. Garcia, E. J. A. Domingo et al.,
    “Internet of things (IOT)-based mobile application for monitoring of automated
    aquaponics system,” in IEEE 10th Int. Conf. Humanoid, Nanotechnology, Inf. Technol.
    Commun. Control. Environ. Manag. HNICEM, pp. 1–6, Baguio City, Philippines, 2018.
    View at: Publisher Site | Google Scholar T. Khaoula, R. A. Abdelouahid, I. Ezzahoui,
    and A. Marzak, “Architecture design of monitoring and controlling of IoT-based
    aquaponics system powered by solar energy,” Procedia Computer Science, vol. 191,
    pp. 493–498, 2021. View at: Publisher Site | Google Scholar R. R. D. Isabella
    Wibowo, M. Ramdhani, R. A. Priramadhi, and B. S. Aprillia, “IoT based automatic
    monitoring system for water nutrition on aquaponics system,” in Journal of Physics:
    Conference Series, vol. 1367, East Java, Indonesia, 2019. View at: Publisher Site
    | Google Scholar M. M. Elsokah and M. Sakah, “Next generation of smart aquaponics
    with Internet of Things solutions,” in 19th International Conference on Sciences
    and Techniques of Automatic Control and Computer Engineering (STA), pp. 106–111,
    Sousse, Tunisia, 2019. View at: Google Scholar W. Yu, F. Liang, X. He et al.,
    “A survey on the edge computing for the Internet of Things,” IEEE Access, vol.
    6, pp. 6900–6919, 2018. View at: Publisher Site | Google Scholar J. Pitakphongmetha,
    N. Boonnam, S. Wongkoon, T. Horanont, D. Somkiadcharoen, and J. Prapakornpilai,
    “Internet of Things for planting in smart farm hydroponics style,” in International
    Computer Science and Engineering Conference (ICSEC), Chiang Mai, Thailand, 2016.
    View at: Google Scholar K. S. Aishwarya, M. Harish, S. Prathibhashree, and K.
    Panimozhi, “Survey on automated aquponics based gardening approaches,” in 2018
    Second International Conference on Inventive Communication and Computational Technologies
    (ICICCT), pp. 1377–1381, Coimbatore, India, 2018. View at: Google Scholar M. Ulum,
    A. F. Ibadillah, R. Alfita, K. Aji, and R. Rizkyandi, “Smart aquaponic system
    based Internet of Things,” in Journal of Physics: Conference Series, vol. 1211,
    no. 1, p. 012047, East Java, Indonesia, 2019. View at: Google Scholar N. H. Kumar,
    S. Baskaran, S. Hariraj, and V. Krishnan, “An autonomous aquaponics system using
    6LoWPAN based WSN,” in IEEE 4th International Conference on Future Internet of
    Things and Cloud Workshops (FiCloudW), pp. 125–132, Vienna, Austria, 2016. View
    at: Google Scholar M. A. Romli, S. Daud, R. A. A. Raof, Z. A. Ahmad, and N. Mahrom,
    “Aquaponic growbed water level control using fog architecture,” in Journal of
    Physics: Conference Series, vol. 1018, no. 1, p. 012014, Kuching, Sarawak, Malaysia,
    2018. View at: Google Scholar M. Muneeb, K.-M. Ko, and Y.-H. Park, “A fog computing
    architecture with multi-layer for computing-intensive IoT applications,” Applied
    Sciences, vol. 11, no. 24, p. 11585, 2021. View at: Google Scholar P. Mpofu, S.
    H. Kembo, S. Jacques, N. Chitiyo, and C. Solar, “Utilizing a privacy-preserving
    IoT edge and fog architecture in automated household aquaponics,” in 2nd African
    International Conference on Industrial Engineering and Operations Management,
    IEOM, pp. 2281–2288, Harare, Zimbabwe, 2020. View at: Google Scholar C. Lee and
    Y.-J. Wang, “Development of a cloud-based IoT monitoring system for fish metabolism
    and activity in aquaponics,” Aquacultural Engineering, vol. 90, p. 102067, 2020.
    View at: Publisher Site | Google Scholar H. Ma, T.-F. Tsai, and C.-C. Liu, “Real-time
    monitoring of water quality using temporal trajectory of live fish,” Expert Systems
    with Applications, vol. 37, no. 7, pp. 5158–5171, 2010. View at: Publisher Site
    | Google Scholar S. B. Dhal, M. Bagavathiannan, U. Braga-Neto, and S. Kalafatis,
    “Nutrient optimization for plant growth in Aquaponic irrigation using machine
    learning for small training datasets,” Artificial Intelligence in Agriculture,
    vol. 6, pp. 68–76, 2022. View at: Publisher Site | Google Scholar M. A. Koutb,
    N. M. El-Rabaie, H. A. Awad, and I. A. Abd El-Hamid, “Environmental control for
    plants using intelligent control systems,” IFAC Proceedings, vol. 37, no. 2, pp.
    101–106, 2004. View at: Publisher Site | Google Scholar M. Lowe and R. Qin, “A
    review on machine learning , artificial intelligence , and smart technology in
    water treatment and monitoring,” Water, vol. 14, pp. 1–28, 2022. View at: Google
    Scholar Copyright Copyright © 2022 Kiran Kumari Gayam et al. This is an open access
    article distributed under the Creative Commons Attribution License, which permits
    unrestricted use, distribution, and reproduction in any medium, provided the original
    work is properly cited. PDF Download Citation Download other formats Order printed
    copies Views 1607 Downloads 707 Citations 6 About Us Contact us Partnerships Blog
    Journals Article Processing Charges Print editions Authors Editors Reviewers Partnerships
    Hindawi XML Corpus Open Archives Initiative Fraud prevention Follow us: Privacy
    PolicyTerms of ServiceResponsible Disclosure PolicyCookie PolicyCopyrightModern
    slavery statementCookie Preferences"'
  inline_citation: Gayam, Kumar, et al. "Imperative Role of Automation and Wireless
    Technologies in Aquaponics Farming."
  journal: Wireless Communications and Mobile Computing
  key_findings: null
  limitations: null
  main_objective: To explore the significance and implementation of automated, real-time
    irrigation management systems that integrate IoT and machine learning technologies
    in the context of aquaponics farming.
  relevance_evaluation: The study is highly relevant to the section and subsection
    topic, as it directly addresses the importance of remote monitoring and integration
    of wireless technologies for real-time irrigation management in aquaponics farming.
    The focus on IoT-enabled sensors and computer vision aligns well with the objective
    of monitoring and controlling plant and fish growth.
  relevance_score: '1.0'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT, Machine Learning, Wireless Sensors, Computer Vision
  title: Imperative Role of Automation and Wireless Technologies in Aquaponics Farming
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Azimi, S., Kaur, T., & Gandhi, T. K. (2021). BAT Optimized CNN Model
    Identifies Water Stress in Chickpea Plant Shoot Images. In 2020 25th International
    Conference on Pattern Recognition (ICPR) (pp. 9412720). IEEE.
  authors:
  - Azimi S.
  - Kaur T.
  - Gandhi T.K.
  citation_count: '7'
  data_sources: Custom dataset of chickpea plant shoot images under different watering
    conditions
  description: Stress due to water deficiency in plants can significantly lower the
    agricultural yield. It can affect many visible plant traits such as size and surface
    area, the number of leaves and their color, etc. In recent years, computer vision-based
    plant phenomics has emerged as a promising tool for plant research and management.
    Such techniques have the advantage of being non-destructive, non-evasive, fast,
    and offer high levels of automation. Pulses like chickpeas play an important role
    in ensuring food security in poor countries owing to their high protein and nutrition
    content. In the present work, we have built a dataset comprising of two varieties
    of chickpea plant shoot images under different moisture stress conditions. Specifically,
    we propose a BAT optimized ResNet-18 model for classifying stress induced by water
    deficiency using chickpea shoot images. BAT algorithm identifies the optimal value
    of the mini-batch size to be used for training rather than employing the traditional
    manual approach of trial and error. Experimentation on two crop varieties (JG
    and Pusa) reveals that BAT optimized approach achieves an accuracy of 96% and
    91% for JG and Pusa varieties that is better than the traditional method by 4%.
    The experimental results are also compared with state of the art CNN models like
    Alexnet, GoogleNet, and ResNet-50. The comparison results demonstrate that the
    proposed BAT optimized ResNet-18 model achieves higher performance than the comparison
    counterparts.
  doi: 10.1109/ICPR48806.2021.9412720
  explanation: The study explores the potential of using computer vision-based plant
    phenomics to classify water stress in chickpea plants through the analysis of
    shoot images. The authors created a custom dataset of chickpea plant shoot images
    under different watering conditions to train and evaluate their proposed BAT optimized
    ResNet-18 model. The study employs the BAT algorithm to optimize the mini-batch
    size hyperparameter of the pre-trained ResNet-18 model, demonstrating that optimization
    leads to improved classification accuracy for both water stress and healthy plant
    images.
  extract_1: '"Pulses like chickpeas play an important role in ensuring food security
    in poor countries owing to their high protein and nutrition content. In the present
    work, we have built a dataset comprising of two varieties of chickpea plant shoot
    images under different moisture stress conditions. Specifically, we propose a
    BAT optimized ResNet-18 model for classifying stress induced by water deficiency
    using chickpea shoot images."'
  extract_2: '"Experimentation on two crop varieties (JG and Pusa) reveals that BAT
    optimized approach achieves an accuracy of 96% and 91% for JG and Pusa varieties
    that is better than the traditional method by 4%. The experimental results are
    also compared with state of the art CNN models like Alexnet, GoogleNet, and ResNet-50.
    The comparison results demonstrate that the proposed BAT optimized ResNet-18 model
    achieves higher performance than the comparison counterparts."'
  full_citation: '>'
  full_text: '>

    "IEEE.org IEEE Xplore IEEE SA IEEE Spectrum More Sites Donate Cart Create Account
    Personal Sign In Browse My Settings Help Access provided by: University of Nebraska
    - Lincoln Sign Out All Books Conferences Courses Journals & Magazines Standards
    Authors Citations ADVANCED SEARCH Conferences >2020 25th International Confe...
    BAT Optimized CNN Model Identifies Water Stress in Chickpea Plant Shoot Images
    Publisher: IEEE Cite This PDF Shiva Azimi; Taranjit Kaur; Tapan K Gandhi All Authors
    6 Cites in Papers 176 Full Text Views Abstract Document Sections I. Introduction
    II. Material and Methods III. Experimental Results and Discussion IV. Conclusion
    Authors Figures References Citations Keywords Metrics Abstract: Stress due to
    water deficiency in plants can significantly lower the agricultural yield. It
    can affect many visible plant traits such as size and surface area, the number
    of leaves and their color, etc. In recent years, computer vision-based plant phenomics
    has emerged as a promising tool for plant research and management. Such techniques
    have the advantage of being non-destructive, non-evasive, fast, and offer high
    levels of automation. Pulses like chickpeas play an important role in ensuring
    food security in poor countries owing to their high protein and nutrition content.
    In the present work, we have built a dataset comprising of two varieties of chickpea
    plant shoot images under different moisture stress conditions. Specifically, we
    propose a BAT optimized ResNet-18 model for classifying stress induced by water
    deficiency using chickpea shoot images. BAT algorithm identifies the optimal value
    of the mini-batch size to be used for training rather than employing the traditional
    manual approach of trial and error. Experimentation on two crop varieties (JG
    and Pusa) reveals that BAT optimized approach achieves an accuracy of 96% and
    91% for JG and Pusa varieties that is better than the traditional method by 4%.
    The experimental results are also compared with state of the art CNN models like
    Alexnet, GoogleNet, and ResNet-50. The comparison results demonstrate that the
    proposed BAT optimized ResNet-18 model achieves higher performance than the comparison
    counterparts. Published in: 2020 25th International Conference on Pattern Recognition
    (ICPR) Date of Conference: 10-15 January 2021 Date Added to IEEE Xplore: 05 May
    2021 ISBN Information: Print on Demand(PoD) ISSN: 1051-4651 DOI: 10.1109/ICPR48806.2021.9412720
    Publisher: IEEE Conference Location: Milan, Italy SECTION I. Introduction Today
    approximately 7.6 billion people living on our planet and by 2100, this number
    is expected to cross 11 billion. Feeding this additional population requires innovative
    food production and distribution infrastructure that can maximize the farm yield
    while leaving an environmentally sustainable footprint so that important ecological
    resources and wildlife of the planet are conserved [1]. Some of these innovative
    farming methods involve using new technologies such as computer vision-based plant
    phenomics and Internet of thing (IoT) sensors to modify the way agriculture is
    done in a significant way [2]. This kind of farming also referred to as precision
    farming, is not only more efficient but also more sustainable than the traditional
    farming methods that are prevalent today. Precision farming combines remote sensing,
    IoT devices, robotics, big data analytics, artificial intelligence, and other
    emerging technologies into an integrated crop production infrastructure [3]. Computer
    vision and image processing can play a crucial role in precision farming [4].
    Computer vision can be used to determine whether a plant is a crop or a weed,
    near-infrared and hyperspectral imaging can be employed to see the health of the
    cells in the plant, and the chemical content of the soil. Imaging-based plant
    phenomics that deals with measuring some observable traits of the plants such
    as color, leaf density, overall shape, volume, etc., can be employed to monitor
    plant health and growth [5]. Compared to the traditional phenotyping methods that
    are destructive and time-consuming, imaging-based methods offer smart and automated
    alternatives that measure stress levels in plants in less time without disturbing
    the plant. However, imagery-based plant phenomics does offer many challenges owing
    to the complexity of images due to the lack of texture in multivariate plant images,
    occlusion, complicated and fine structures present in the plants [6]–[7]. Two
    basic steps are involved in imaging-based plant phenotyping: first, the data acquisition
    stage where, plant images having the required information are captured, and second,
    the understanding stage, where the plant image is segmented and its features are
    characterized. These phenotyping applications require acquiring, managing, and
    using a huge amount of data. Machine Learning (ML) methods have been proven to
    be quite efficient in the analysis of big data resulting in different research
    areas such as health, economics, robotics, agriculture, etc. Of the various ML
    techniques that have been proposed in the existing literature, Deep Learning (DL)
    methods where data hierarchically is presented in terms of different convolutions
    has been becoming increasingly popular [8]. Before discussing the DL network used
    in this paper, it is worthwhile to mention the main limitation of the traditional
    ML techniques which arises from the fact that these techniques work with hand-crafted
    features. The problem with these hand-crafted features is the fact that they are
    suitable only for a particular kind of data, and thus, applications of the algorithm
    are limited to specific tasks and lack generality. This means that a method that
    works fine for one application may not perform satisfactorily for a different
    task. This specificity severely limits the usability of these techniques in applications
    that demand a certain level of generality. This has led to the development of
    methods that offer high levels of generality in terms of applications. This has
    further strengthened the focus on DL based approaches to ML, which are usually
    much more general than the traditional ML methods. One such DL network which is
    very commonly employed in various computer vision applications is the Convolutional
    Neural Network (CNN) [8]. It is especially useful for image recognition related
    tasks and has found applications in a variety of computer vision applications
    across different fields such as life sciences, health information & medicine [9],
    and farming [10]. Along with the usual artificial neural networks (ANNs) layer
    that does the classification, CNN''s include additional convolution layers for
    feature-detection [11]. CNN has already been proven very effective for tasks of
    the classification of plants and leaves in agriculture [12], [13]. The authors
    in [13] employed Convolutional as well as Recurrent Neural Networks (RNN) for
    multi-organ classification in plants. In their work, the classification algorithm
    uses the correlation between the generic features and the selected organs. CNN
    has also been widely used in the existing literature for leaf characterization
    and identification. In [12], the authors trained CNN using raw leaf data. The
    CNN stage was followed by a Deconvolutional Network stage to understand how the
    CNN classifies the leaf data. The results obtained in their paper underlines the
    strength of detecting features using CNN. Other agricultural applications where
    DL techniques have been used include applications such as counting the seeds per
    pod for soybeans [14], estimating the number of the wheat ear under field conditions
    [15], identifying the diseases of plants [16], and identification of plants [17].
    Many works in the existing literature have focused on predicting future estimates,
    such as corn yield prediction [18] and soil water deficiency content in the field
    [19]. While quite useful in a variety of applications, CNN''s are delicate to
    the selection of the hyperparameters. Manually determining the best value of the
    hyper-parameters is quite challenging and costly. Although grid search and random
    search based approaches are better than the manual methods, both of them consume
    a lot of time. To address this issue, the present paper proposes an approach that
    combines the pretrained ResNet-18 architecture with the BAT optimization algorithm
    to overcome the hyperparameter selection problem in CNN models. BAT algorithm
    in conjunction with the ResNet-18 model, using accuracy as a fitness criterion,
    can identify the best value of the parameters that achieve ceiling level of classification
    performance for the task of stress classification in chickpea plants. Chickpea
    (Cicer arietinum L.) is one of the most important pulses and one of the major
    sources of essential nutrients such as proteins, carbohydrates, folic acid, and
    iron [20]. With the increasing concerns over food security in developing and underdeveloped
    countries, the demand for chickpea has been increasing. However, many factors
    such as climate change, global warming, and reduction in cultivable land area,
    are impacting agricultural production negatively. Among the various abiotic stresses
    which affect chickpea production, the most important is the stress-induced due
    to lack of water, resulting in up to 50% crop yield reduction [21]. Several physiological
    changes can result from water deficiency in chickpea plants, including dryness,
    yellow leaves, early blooming, and reduced leaf size and biomass. Given this potential
    of chickpea towards ensuring food security, it is crucial to develop and evaluate
    image-based analysis methods for easy and early identification of stress associated
    with soil moisture. The present work proposes an optimized ResNet model for the
    classification of moisture stress levels in the chickpea plant using plant shoot
    images. Classifying the stress stages using images of the chickpea plant shoots
    is very challenging given the following factors: The plant shoot is made up of
    very fine structures, making it difficult to segment and process relevant parts.
    Wind-induced movements, varying light conditions, and other such environmental
    factors significantly affect the extraction of information from the image data.
    Plant images also suffer from an occlusion issue where portions of the plant overlap
    and touch each other rendering the segmentation very difficult. Summarizing, the
    present paper deals with the classification of water stress in chickpea plants
    using the shoot images. The main contributions of the paper are: As no publicly
    available image dataset of chickpea plant shoots under different moisture stress
    conditions are available, we have prepared our dataset to run the experiments
    presented in this paper. The dataset contains two varieties of chickpea common
    in India, having 15 replicates each. We have proposed a BAT optimized ResNet-18
    model that automatically identifies the best value of the mini-batch size to be
    used for training. Training the model with the optimized parameters has resulted
    in the ceiling level of the classification performance over the chickpea data.
    The proposed BAT Optimized ResNet-18 model outperforms the other state of art
    CNN algorithms. The remainder of the paper is structured as follows. The description
    of the materials and methods used in this paper for the classification of water
    stress in chickpea plant shoots are presented in section II. Section III provides
    the results. In section IV, finally, we have explained the perspectives based
    on the results and conclude the paper. SECTION II. Material and Methods In this
    section, we first describe our dataset of chickpea shoot images, we then discuss
    the proposed methodology used in this paper, viz, ResNet-18 [22], [23] and BAT
    optimization algorithm. A. Dataset For the DL techniques to work for image-based
    plant phenotyping, a proper dataset of plant images containing an appropriate
    number of plant images is required. However, there are very few such datasets
    for chickpeas; and almost all the available image datasets are composed of images
    of specific parts of the plant such as leaves and stems. Phenotyping using complete
    plant shoot images offers certain advantages: they not only contain more information
    than individual plant organs for plant stress analysis but are also non-destructive.
    Thus, using complete shoot images for phenotyping applications is desirable. As
    we don''t have such an image dataset available publically, we created our dataset
    containing chickpea plant shoots images under three different water treatment
    conditions for plant phenotyping purposes. These experiments have been done at
    the National Institute of Plant Genome Research (NIPGR) India, over five months.
    Two strains of chickpea plants: stress-tolerant (Pusa-372) and stress-sensitive
    (JG-62), were grown for the experiment under three different conditions of controlled
    watering during the entire period of growing, water stress during the young seedling
    stage, and water stress before the flowering stage of plants. One important observation
    regarding the plant''s physical structures is that there is a significant variation
    in the overall shape and size of the plants, the color, shape, and curvature of
    the leaves with different watering conditions. Drought stress reduces the number
    and the size of leaves, and the plant appears to be shrunk with an increased number
    of yellow leaves. For every variety and every water stress, five replicates were
    grown. Capturing images was started once the plants were two-week-old using a
    Canon 60D EOS camera in the automatic mode. The camera was mounted in front of
    the turntable on which the plant was placed for capturing multiview plant images.
    An image was taken after every 45° rotation, resulting in eight different views.
    In total, the dataset contains a total of 8000 images. Fig. 1 shows two sample
    images from our dataset of chickpea plant shoot images. The images also include
    background and the pot. The deep learning models are quite effective in ignoring
    these invariant features present in the background. Removing the background does
    not have much effect on the classification results. Segmenting the plant from
    the background will only reduce the number of key points, and thus, the size of
    the resulting feature vectors, which will reduce the computational cost. Fig.
    1. Visualization of replicates in the stress-tolerant (left) and the stress-sensitive
    (right) varieties from our dataset. Show All B. Proposed Methodology The proposed
    method is an effective way to enhance the performance of the pre-trained model.
    The hyper-parameter, i.e., the mini-batch size has been optimized using the BAT
    algorithm. Validation accuracy has been used as a fitness criterion. To clarify
    the proposed procedures, they are summed up as 1) the Data preparation phase,
    2) Optimization of hyper-parameter phase (OHP), and 3) the Learning phase. 1)
    Data Preparation Phase The dataset is partitioned in the ratio 6:4, i.e., 60%
    of data is used for training and 40% is used for validation. For better generalization
    and to prevent overfitting data augmentation is used for training data. Augmentation
    involves random translation from pixel range [−3, 3], random shear in the range
    from [−0.05, 0.05], and random rotation of training data between range [−10, 10].
    2) Optimization of Hyper-Parameter Phase (OHP) Most of the parameters of the pre-trained
    model are fixed and cannot be altered. When employing such a model for transfer
    learning, only a few of the hyper-parameters like mini-batch size and the learning
    rate can be optimized. In the present paper, we have utilized the BAT optimization
    algorithm to optimize the size of the mini-batch. The bat position has been encoded
    to represent the min-batch size. The initial bat positions are randomly initialized
    in the range from 10 to 100. Validation accuracy has been chosen as a fitness
    criterion. The succeeding subsection briefly summarizes the mathematical formulation
    of the BAT algorithm. A metaheuristic optimization algorithm inspired by the echolocation
    behavior of micro-bats was developed by Yang [24]. The echolocation behavior of
    bats which guides their foraging behavior follows certain rules [24]. The BAT
    algorithm uses an amalgamation of the advantages of the Particle Swarm Optimization
    and the Harmony Search algorithm. The velocity v t−1 i and position x t−1 i of
    the individual bats is defined in a d -dimensional search space. The newer solutions
    are obtained by updating the instantaneous frequency f i , position x t i , and
    velocity v t i at time step t which is mathematically defined as f i = f min +(
    f max − f min )α x t i = x t−1 i + v t i v t i = v t−1 i +( x t i − x ∗ ) f i
    (1) (2) (3) View Source Here, α∈[0,1] is a random vector drawn from a uniform
    distribution and x ∗ is the current global best location obtained after comparing
    all the solutions among all the n bats at the current iteration. f i is the frequency
    value associated with the i th bat, f max and f min are the maximum and minimum
    frequency values. In order to further favor the exploitation around the best position
    value achieved so far Yang [24] proposed an update mechanism which is mathematically
    given as follows: x new = x old +ε A t A t+1 i =β A t i r t+1 i = r 0 i [1−exp(−γt)]
    (4) (5) (6) View Source In the above equation ε∈[−1,1] is a random number, A t
    is the average loudness of all the bats at time step t, β is the pulse frequency
    increasing coefficient (typically 0<β<1 ), γ is the pulse amplitude attenuation
    coefficient (γ>0),  r 0 i is the initial emission rate (typically from 0 to 1),
    and x old is the previous value of bat position. The algorithm for function optimization
    using the bat algorithm is summarized as [24] 3) Learning Phase In the learning
    phase, a pre-trained ResNet-18 [22], [23] model with transfer learning is used.
    The architecture of the ResNet-18 model is shown in Fig 2. The input images from
    both the crop varieties are pre-processed so as to make them compatible with the
    size of the input layer of the pre-trained model, i.e., an image input with size
    256×256×3 is changed to 224×224×3 . The convolutional (conv) layer represents
    a 2d convolution layer with kernel size as 3×3 and stride 1. These filters slide
    across the input image both horizontally and vertically and the dot product is
    computed at every spatial location known as activation map. The output of the
    convolution layer is obtained by stacking all the activation maps across the depth
    dimension. Mathematically ‘conv’ is defined as: C(p,q)=(I∗w)(p,q)= ∑ k ∑ l I(p−k,q−l)w(k,l)
    (7) View Source In the above equation, ‘ I ’ is the input image with size (p,
    q),w is the convolutional kernel with dimensions ( k, l ). The ‘Norm’ is the batch
    normalization layer. The functionality of the ‘Norm’ layer is mathematically given
    as: x ¯ = x i − μ mB σ mB 2 +∈ − − − − − − − − √ (8) View Source x i is the input,
    μ mB and σ mB are the mean and the variance over a mini-batch and over each input
    channel. ∈ is added to improvise the numerical stability when the mini-batch variance
    is very small. y i =γ x ¯ ¯ ¯ i +β (9) View Source The activations are further
    shifted by offset β and scaled by factor γ . Both β and γ are learnable parameters
    updated during network training. The relu (ReLU) stands for a rectified linear
    unit. (ReLU) perform a nonlinear mapping onto the results of the preceding layers.
    It counteracts for the gradient vanishing problem and is defined as Re Lu(x)=max(x,0)
    (10) View Source ReLU''s gradient is 1 for the input not less than 0. Using ReLU
    for activation in CNN leads to faster convergence. The maxpooling layers are used
    to reduce the dimensionality of convolutional layers. It is mathematically defined
    as: h i,j =max{ x i+k−1,j+l−1 ∀ 1≤k≤m & 1≤l≤m} (11) View Source In the above equation,
    ‘ m ’ is the kernel width. The ‘sum’ stands for the Addition layer. The fully
    connected ‘Fc’ layers provide a global representation for the image by summarizing
    the feature vectors from the preceding layers. Fig. 2. ResNet-18 architecture
    used in this work Show All The Softmax layer constrains the output in the range
    from (0,1) and it is mathematically given as: soft( x i )= exp( x i ) ∑ j=1 n
    exp( x j ) (12) View Source The classification layer categorizes the computed
    feature vectors formed at the fully connected layer into the image class using
    an appropriate loss function. For transfer learning using pre-trained ResNet-18,
    the “Fc” layer is replaced by a new layer. The number of outputs is equal to the
    number of the classes in the chickpea dataset on which the model is trained. In
    the proposed work, the number of outputs will be 3: a) Control (C), b) Young Seedling
    (YS), and c) Before Flowering (BF). SECTION III. Experimental Results and Discussion
    In this section, we evaluate the performance of the proposed BAT optimized ResNet-18
    model for the chickpea dataset. The chickpea plant dataset has three classes of
    water stress conditions: before flowering (BF), control (C), and young seedling
    (YS) for both the JG and Pusa varieties. Chickpea plants images are high-quality
    RGB images in JPEG format. All our experiments have been run in the Matlab 2019a
    platform running on a computer having 16 GB of RAM, 3.40 GHz, and an Intel i7
    processor. The proposed model is fitted with an optimized batch size for 30 epochs
    with early stopping criteria, and ‘sgdm’ with an initial learning rate as 0.01.
    The results of our evaluation have been reported in terms of confusion matrices
    and metrics such as Accuracy (Acc), Sensitivity (Se), Specificity (Sp), and Precision
    (Pre). A. Classification Performance of the Proposed Optimized ResNet-18 Model
    Firstly, the hyper-parameter optimization has been carried out using the BAT algorithm.
    For the BAT algorithm, the population size is taken as 3. The maximum number of
    iterations is taken as 5. After each iteration, the fitness corresponding to each
    bat position is evaluated based on the accuracy value over the validation set
    after 12 training epochs. Experimentation reveals that when the number of training
    epochs is less, CNN''s convergence capacity is low. When the number of epochs
    is larger than 12, the training time is large. Hence, the number of epochs is
    set to 12 which implies that 12 number of training epochs are used to compute
    the fitness corresponding to the individual bat position. After the optimization
    process, the obtained hyperparameters corresponding to the x ∗ is given in Table
    I. Table I. Value of the mini-batch size optimized by the bat algorithm After
    the BAT algorithm optimizing the selected parameters of the ResNet-18 algorithm.
    The optimized pre-trained model is trained on the augmented training dataset.
    The trained model has been evaluated on the validation dataset using the performance
    measures of Acc, Se, Sp, Pre, and results are given in Table II. From, Table II
    it is seen that the classification performance achieved by the proposed BAT optimized
    ResNet-18 model for the chickpea dataset is better than the conventional ResNet-18.
    It achieved an Acc value of 96.02% and 91.32% for JG and Pusa respectively surpassing
    the conventional model without optimization that reached an Acc value of 92.40%
    and 87.62%. Also, the classification accuracy of the JG variety is more than Pusa
    variety. The reason behind this is that JG is a stress-sensitive variety while
    PUSA is a stress-tolerant chickpea variety. Table II. Classification performance
    of the proposed optimized ResNet-18 model Fig. 3 and 4 represent the confusion
    matrices for the conventional ResNet-18 and the BAT optimized ResNet-18 model.
    The correctly classified samples are represented by the elements of the main diagonal
    of the confusion matrix. From the confusion matrices of the optimized ResNet-18
    model for variety JG, it is seen that the correctly classified samples are 521,
    539, and 507 for ‘BF’, ‘C’, and ‘YS’ categories in contrast to 523, 528, and 457
    by the conventional ResNet-18 model. Similarly, for the Pusa variety, the correct
    classifications are 528, 503, and 474 better than the convention non-optimized
    approach that has 522, 495, and 427 correct classifications. Fig. 3. Confusion
    matrices for ResNet-18 and BAT optimized ResNet-18 model for JG Show All Fig.
    4. Confusion matrices for ResNet-18 and BAT optimized ResNet-18 model for Pusa
    Show All B. Comparison with State of the Art CNN Architectures The performance
    of the proposed optimized ResNet-18 model is compared with the state of the art
    pre-trained models like AlexNet [25], GoogleNet [26], and ResNet-50 [22]. The
    comparison in Table III reveals that optimized ResNet-18 surpasses the existing
    state of art pre-trained CNN models by achieving an Acc, Sp, Pre of 96.02%; 98.01%,
    96.03%, and 91.32%, 95.65%, 91.54% for both the crop varieties. Illustratively,
    the proposed optimized ResNet-18 model is superior to ResNet50, which attained
    an Acc value of 89.03% and 83.74% for JG and PUSA varieties. Noticeably the performance
    measures are better by nearly 7%. The comparison models are trained using the
    same partitioning scheme using the concept of transfer learning. The hyperparameters
    for the pre-trained CNN models taken for comparison were tuned manually. Clearly,
    via adopting the optimization strategy the potential of the pre-trained ResNet-18
    model is enhanced by nearly 4% for both the crop varieties. Table III. Classification
    performance of the proposed optimized ResNet-18 model with state of the art CNN
    architectures Here also, the classification accuracy of the JG variety using any
    of the pre-trained models is more than the Pusa variety. This is true because
    of the reaction of Pusa variety in stress conditions. SECTION IV. Conclusion In
    the present work, we have focused on classifying water stress conditions for chickpeas
    plants. As there are no publicly available datasets for identifying water stress,
    we first created our own chickpea shoot images dataset under three different water
    stress conditions. We then proposed a BAT optimized ResNet-18 model for identifying
    water stress in chickpea plants using the shoots from two different crop varieties.
    We observed that via optimizing hyper-parameters of the ResNet-18 model through
    the BAT algorithm, the classification performance improves by nearly 4% for both
    stress-sensitive and stress-tolerant crop varieties. Also, the proposed optimized
    model outperforms the state of art pre-trained CNN models. Thus, the proposed
    model can be easily applied in real-time monitoring of plant health in IoT based
    applications to improve the yield. Future works will also be focused on proposing
    a better lightweight model that can be easily implemented on a simple computing
    platform. Authors Figures References Citations Keywords Metrics More Like This
    Code configuration tool for real time systems 2017 International Conference on
    Computation of Power, Energy Information and Commuincation (ICCPEIC) Published:
    2017 A Survey on Network Calculus Tools for Network Infrastructure in Real-Time
    Systems IEEE Access Published: 2020 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (Azimi, Kaur, & Gandhi, 2021)
  journal: Proceedings - International Conference on Pattern Recognition
  key_findings: The proposed BAT optimized ResNet-18 model achieved an accuracy of
    96% and 91% for two chickpea varieties, outperforming conventional ResNet-18 and
    other state-of-the-art CNN models. Optimization using the BAT algorithm improved
    classification accuracy by 4% for both varieties compared to traditional methods.
  limitations: null
  main_objective: To classify water stress conditions in chickpea plants using computer
    vision-based analysis of shoot images and to optimize the ResNet-18 model using
    the BAT algorithm.
  relevance_evaluation: This study is highly relevant to the point of discussion within
    the subsection on advanced monitoring techniques for automated irrigation systems
    using IoT-enabled sensors and computer vision. The exploration of computer vision-based
    plant phenomics to identify water stress in chickpea plants directly informs the
    need for remote monitoring and data analysis in precision agriculture systems.
    The study's focus on optimizing the ResNet-18 model using the BAT algorithm contributes
    to the broader topic of optimizing and enhancing the performance of automated
    irrigation systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Computer vision, BAT algorithm, ResNet-18 model
  title: Bat optimized CNN model identifies water stress in chickpea plant shoot images
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Ratheesh Raju, T. M. T. (2023). An IoT Solutions for Ungulates Attacks
    in Farmland. 2023 2nd International Conference for Innovation in Technology (INOCON).
    https://doi.org/10.1109/INOCON57975.2023.10100983
  authors:
  - Raju R.
  - Thasleema T.M.
  citation_count: '1'
  data_sources: Live video captured by the camera upon animal detection
  description: Agriculture is considered to be a significant contributor to the global
    financial system and human diets. It has been recognized as the country's primary
    source of income and employment. So, it's really important to protect crops from
    various dangerous hazards, such as diseases, insects, bird and animal attacks,
    high atmospheric temperature, etc., and also from weak irrigation systems, poor
    soil quality, weeds management, etc. Specific insect attacks and diseases have
    long been a primary crop sector concern. Computer vision (CV)-based automatic
    insect and disease detection methods are used in smart farming systems because
    of their high cost-effectiveness and efficient automation. This paper gives an
    overview of the use of Machine Learning (ML), Deep Learning (DL), and the Internet
    of Things (IoT) in agriculture to protect crops from various dangerous hazards
    and proposes an automatic Animal-Repelling System (ARS). This study implements
    a system based on IoT to protect crops from animals. The proposed low-cost agricultural
    field protection system helps farmers to protect their crops and increase production
    yield and income.
  doi: 10.1109/INOCON57975.2023.10100983
  explanation: The research paper presents an IoT-based system for safeguarding crops
    from animal attacks, emphasizing how animal detection and automated repelling
    mechanisms contribute to precision farming. The system employs Raspberry Pi, PIR
    sensors, a camera, and a buzzer to detect ungulates and emit high-frequency sounds
    to deter them. The study highlights the effectiveness of the proposed system in
    repelling animals without causing harm.
  extract_1: '"This research work provides an overview of advanced control techniques
    in Precision Agriculture that have been discussed by various researchers. The
    selected articles discuss advanced control strategies using spectral imaging,
    IoT sensors, and artificial intelligence-based methods to address issues in agriculture
    such as yield improvement, disease detection, animal and insect attacks, etc."
    Here, the paper establishes the importance of IoT-based advanced control techniques
    in addressing agricultural challenges, including animal attacks.'
  extract_2: '"The proposed IoT-based method to protect crops from various animal
    and insect attacks is very effective. The system repels the ungulates from the
    field without any harm to them. In the future, we can extend this to identify
    the presence of all the ungulates, birds, and quickly if it is far away from the
    field, this helps to repel the attackers from the field at the earliest, and also,
    we can use more sensors such as soil, temperature, etc. with the help of these
    sensors, we can reduce the usage of water and improve soil quality." This excerpt
    highlights the potential of the proposed system to be further developed for enhanced
    detection and monitoring capabilities, as well as its potential to contribute
    to water conservation and soil quality management.'
  full_citation: '>'
  full_text: '>

    "This website utilizes technologies such as cookies to enable essential site functionality,
    as well as for analytics, personalization, and targeted advertising purposes.
    To learn more, view the following link: Privacy Policy IEEE.org IEEE Xplore IEEE
    SA IEEE Spectrum More Sites Donate Cart Create Account Personal Sign In Browse
    My Settings Help Access provided by: University of Nebraska - Lincoln Sign Out
    All Books Conferences Courses Journals & Magazines Standards Authors Citations
    ADVANCED SEARCH Conferences >2023 2nd International Confer... An IoT Solutions
    for Ungulates Attacks in Farmland Publisher: IEEE Cite This PDF Ratheesh Raju;
    T M Thasleema All Authors 1 Cites in Paper 72 Full Text Views Abstract Document
    Sections I. Introduction II. Intelligent Systems On Precision Agriculture III.
    Proposed Yolov5 Based Automatic Animal Repelling System IV. Results and Discussions
    V. Conclusion Authors Figures References Citations Keywords Metrics Abstract:
    Agriculture is considered to be a significant contributor to the global financial
    system and human diets. It has been recognized as the country’s primary source
    of income and employment. So, it’s really important to protect crops from various
    dangerous hazards, such as diseases, insects, bird and animal attacks, high atmospheric
    temperature, etc., and also from weak irrigation systems, poor soil quality, weeds
    management, etc. Specific insect attacks and diseases have long been a primary
    crop sector concern. Computer vision (CV)-based automatic insect and disease detection
    methods are used in smart farming systems because of their high cost-effectiveness
    and efficient automation. This paper gives an overview of the use of Machine Learning
    (ML), Deep Learning (DL), and the Internet of Things (IoT) in agriculture to protect
    crops from various dangerous hazards and proposes an automatic Animal-Repelling
    System (ARS). This study implements a system based on IoT to protect crops from
    animals. The proposed low-cost agricultural field protection system helps farmers
    to protect their crops and increase production yield and income. Published in:
    2023 2nd International Conference for Innovation in Technology (INOCON) Date of
    Conference: 03-05 March 2023 Date Added to IEEE Xplore: 19 April 2023 ISBN Information:
    DOI: 10.1109/INOCON57975.2023.10100983 Publisher: IEEE Conference Location: Bangalore,
    India SECTION I. Introduction The world’s population is enormously increasing,
    and ensuring food security for this growing population is a serious problem. According
    to the Food and Agriculture Organization (FAO) of the United Nation, more than
    815 million people are chronically hungry, with Asia accounting for 64% of the
    total of the total [1]. However, increasing agriculture or food production rapidly
    to meet the growing food supply demands is not easy, because of several problems
    such as defunct agricultural techniques, inadequate storage, economies, and political
    crisis. The food and agricultural organizations calculate that agriculture production
    will need to increase by 70% by 2050 to feed the world’s growing population. It
    is not enough to simply feed people; we must also make sure that offer them extremely
    nutritious food without any environmental damage [2].To convey economical agrarian
    production, the agriculture sector employs cutting-edge technologies like Artificial
    Intelligence (AI) [3],Machine Learning (ML) and Internet of Things (IoT) [4].
    The automation of agricultural production has empowered the consistent observation
    of yield development and weeds management [5], as crop and plant defense against
    insects, rodents, birds, and other predators. This gives precise and effective
    solutions to agrarian activities contrasted with the customary strategies performed
    physically [6]. Precision Agriculture (PA) is a farming technique that uses Information
    and Communication Technology (ICT) to collect valuable data from many sources
    to improve crop cultivation processes. So, each farming unit’s spatial and temporal
    variation must be identified and reported [7]. It also helps all production strategies
    using ICT to optimize supply usage to obtain the desired items or to track the
    outcome. Variable Rate Technology (VRT), Yield Monitoring (YM), and numerous types
    of sensors are just a few examples of ICT for PA [8]. Managing the relationship
    with factors outside the agricultural ecosystem, such as wildlife, is a relevant
    open challenge in this dynamic situation. Protecting crops from wild animals is
    also one of the primary concerns. Indeed, in the last three decades, the amount
    of damage inflicted by predatory wild animals has grown exponentially worldwide
    [6]. PA’s core objectives include protecting crops from pests, diseases, animals,
    birds, and weeds and managing irrigation [9]. Furthermore, PA enhances crop productivity
    by minimizing complexity and production costs [10]. In addition, it has the potential
    to reduce costs by only applying fertilizer where it is needed, based on soil
    survey and yield data analysis, and to improve the management of water resources,
    optimizing performance through mechanized reaping procedures [11]. The main objective
    of this review paper is to identify the significant challenges in the agriculture
    industry and provides researchers and readers with the current advancement in
    PA. Conventional agricultural practices are facing various difficulties such as
    the excessive use of pesticides, lack of knowledge about the climate and environmental
    conditions, various animal and insect attacks, etc., This analysis covers every
    aspect of the use of technology and innovation in agriculture that is required
    to monitor crop health and productivity and various strategies to increase the
    plant life span. In the proposed study, we conducted this experiment in Pullur-Periya
    gramapanchanchayath of Kasaragod’s district, Kerala. This region is slightly Endosulfan-affected,
    and also faces various issues such as crop and plant diseases, animal, bird, and
    insect infestations, a severe water shortage, extremely poor soil quality, and
    extremely high temperatures. This paper is organized as follows: the next section
    covers recent similar research works in precision agriculture, section 3 proposes
    a new method to protect agricultural land and crops from different animal attacks
    and section 4 summarizes the findings and discussion, and section 5 concludes
    the article with future directions, and section 6 includes the references. SECTION
    II. Intelligent Systems On Precision Agriculture A. Internet of Things Smart farming
    systems based on IoT are quickly gaining popularity because they use low-cost
    sensors to provide real-time status of environmental variables relevant to the
    crop [12]. Throughout the growing and harvesting cycle, the IoT provides a platform
    for smart agriculture, wireless connection of several soil sensors and context-aware
    sensors, different hardware, and data analytical applications to enhance farmers’
    ability to resolve complicated agricultural issues such as irrigation evaluation,
    soil preparation, yield prediction, and so on [13]. 1) Irrigation System: In the
    current scenario, Irrigation is scheduled all around the world based on farmers’
    crop regular inspection, and as a result, the traditional irrigation system wastes
    nearly half of the water it uses. So, controlled irrigation methods such as Sprinkle
    irrigation, drip irrigation, and furrow irrigation are used reduce water waste
    by 30-70% [14]. In [15] proposes a deep learning neural network supported Internet
    of Things (IoT) enabled intelligent irrigation system. Smart irrigation is a farming
    strategy that combines water management into the cultivation process. Wireless
    technology and internet of things technologies are often used in the deployment
    of smart irrigation systems [16], [17]. IoT technologies can be used in a variety
    of farming sectors, including irrigation, fertilization, plant growth, weed control,
    and more, by adapting various relevant technologies such as wireless sensor networks,
    big data, communication protocols, edge computing, and so on [18]. By developing
    an in-house Wireless Sensor and Actuator Network (WSAN) design and communication
    protocol, [19] deploying WSAN for agricultural irrigation and control. Similarly,
    [20] reported an IoT-based system in which images were analysed for disease identification
    and soil moisture and humidity sensors were applied to monitor irrigation requirements.
    IoT is considered to be one of the leading technologies that will transform traditional
    farming into new aspects of precision agriculture intelligence [18], [21]. 2)
    Disease Detection: To enhance agricultural yield, early detection of crop diseases
    and deployment of management strategies are extremely desirable, Sensor-based
    innovations are vital and plays a critical role in the early stages of diagnosis
    of diseases [22]. [22] proposes an in-house IoT-enabled and connected system On
    flexible substrates, an economical digital Leaf Wetness Sensor (LWS) is employed
    for integrated plant disease control. [23] proposes a real-time data collection
    IoT-based automated system for cotton crop monitoring. The suggested system also
    includes the implementation of a wireless sensor network (WSN) in the cotton fields
    for crop health monitoring and recording. The Waspmote agriculture sensor board,
    which includes temperature and humidity sensors, soil moisture sensors, and leaf
    wetness sensors, were employed. 3) Crop yield prediction: GreenDrone, an inexpensive
    low-altitude remote sensing platform created for monitoring the maize crop, has
    mentioned in [24]. This system included a big, robust fixed-wing aircraft with
    a Canon camera and a FLIR thermal camera for calculating indices like the Normalized
    Difference Vegetation Index (NDVI) and Water Stress Index (WSI). A number of aircraft
    operations were conducted to scan the test region at various phases of crop development.
    The NDVI and NGB (Near-infrared Green Blue) images were created from the collected
    photographs, which helped to detect regions with low yield potential, and areas
    with varying plant densities, and indicate unequal nitrogen and water management
    concerns. Another study [25] combines WSN and drone technologies to construct
    a crop healthcare monitoring system. The focus of this study is the creation of
    run-time sensor clusters while taking into account aspects such as run-time data
    acquisition, the scanned area, the unavailability of a suitable number of nodes,
    and the drone’s dynamic flight track. [26], which employs a UAV and WSN integrated
    approach. Images and real-time data were collected using the drone and the WSN,
    accordingly. The focus of the work was on drone route optimization for WSN data
    gathering. WSN data is gathered and sent to the cloud, where it is analyzed by
    the end-user. Table I summarises the implementation of IoT in different agricultural
    applications SECTION III. Proposed Yolov5 Based Automatic Animal Repelling System
    An ARS system is proposed to protect crops and agricultural land from various
    animals, birds, and insect attacks. The system consists of hardware components
    such as Raspberry Pi, a PIR sensor, a low-cost Raspberry compactable camera, and
    a water pump. The proposed system identifies the presence of ungulates, birds,
    and flies near the agricultural field. When they are near or enter the into fields,
    the PIR sensors will detect the motion of that attacker, which will inform the
    Raspberry Pi to turn on the camera. The camera then detects that attacker based
    on the Object detection algorithm YOLOv5. Then Raspberry Pi will enable the buzzer
    to produce the corresponding frequency of signals which will cause the detected
    animal, bird, or fly to repel away from the fields. To categorize the animals,
    the COCO (Common Objects in Context.) dataset is used in this work. As hinted
    by the name, images in the COCO dataset are taken from everyday scenes, thus attaching
    “context” to the objects captured in the scenes. COCO was an initiative to collect
    natural images, the images that reflect everyday scenes and provide contextual
    information. In the everyday scene, multiple objects can be found in the same
    image, and each should be labeled as a different object and segmented properly.
    The COCO dataset provides the labeling and segmentation of the objects in the
    images. The dataset contains 80 classes, the targeted ten classes containing a
    variety of commonly seen animals. The proposed system makes use of a quad-core
    CortexA72 Raspberry Pi 4 B from Broadcom (ARM v8) 1GB, 2GB,4GB, or 8GB LPDDR4-3200
    SDRAM, 40-pin GPIO standard header for the Raspberry Pi (fully backward compatible
    with previous boards) Micro-HDMI ports in two (up to 4kp60 supported) MIPI DSI
    display port with two lanes 2 lanes for MIPI CSI cameras. 8 Mega Pixel resolution
    camera is used here. When the PIR sensors are triggered, this camera is used to
    record live videos of the field. Even in poor light, it can capture 1080p at 30
    FPS. PIR sensors, often known as passive infrared sensors, can detect infrared
    radiation levels. They contain two knobs; one is to delay the time of the signal
    generated and motion detected. And another knob is to set the sensitivity of the
    sensor and a buzzer is also used, an audio signaling device capable of producing
    a frequency from 64Hz to 67Khz. To monitor the moisture content of the soil, soil
    sensors are used. Table I Various Monitoring and Control Strategies Using IOT
    Object detection is the core aspect of the proposed approach. The ungulates that
    the PIR sensors detect are identified by using the YOLOv5 object detection algorithm.
    The YOLO algorithm is based on convolutional design, where a grid system is used
    to partition an input image into candidate regions for object detection. Each
    grid cell represents a candidate region of discovered items. The ability to complete
    the surveys all at once is the key innovation that YOLO brings, making it quick
    and effective. Additionally, YOLO has the advantage of being able to forecast
    a huge fixed number of items while also establishing a cutoff point to reject
    predictions with low probabilities. SECTION IV. Results and Discussions The attacks
    of ungulates in agriculture have always been a significant problem in the agriculture
    sector. This study provides an innovative and efficient solution for this problem.
    The method discussed here would not cause any harm to animals or humans but can
    be used to repel the ungulates efficiently. The YOLOv5 object detection algorithm
    proposed here is considered the State-of-the-Art Object detection algorithm and
    is so fast that it has become a standard way of detecting objects in the field
    of computer vision. All the experimental analysis has been done by using the COCO
    dataset. In this paper, a real-time preliminary analysis is carried out by capturing
    the movement of a dog near the field. This real-time ARS detects the presence
    of dogs near the agricultural field, this pre-trained model provides great accuracy
    in detecting ungulates. The real-time detection of ungulates near the field is
    depicted in Fig. 1, the detected object is bounded by a box. On the top of that
    bounding box, the category name and accuracy of that object are mentioned. Fig.
    1.(a), contains the dog image with a left-side view and the head is not visible;
    the image shows an accuracy of only 57% because of the position and invisibility
    of the head. In Fig 1. (b), the front view of the dog is detected, and the accuracy
    of the detected dog is slightly increased because the face is visible. In Fig
    1. (c), back and left side view is detected and it shows 85% accuracy, which is
    greater compared to the other two images, Fig. 1. (d), has a back view of the
    dog and, Fig. 1. (e), has a back and right-side view of the dog, which indicates
    91% and 93% accuracy respectively because of the good gestures and clear vision
    of the dog in the image. The back and left-turned view of the dog image is present
    in 1. (f), which is misclassified as a cow with 43% of accuracy because of the
    low visibility of the head, and 1. (g), provides a side view of the dog with the
    head-turned back and legs straight, due to the low visibility of the head and
    straight leg position causes the dog image to which be misclassified as sheep
    with 62% accuracy. This result indicates that the slight movement of the object
    leads to the wrong classification of the object. TableII summarizes the ungulate’s
    various positions and detection accuracy. After the detection and classification
    of objects as dogs, raspberry pi enables the buzzer, and it automatically generates
    a sound with 25KHz frequency, this high-frequency sound repels the dog from the
    agricultural field. This system provides good accuracy in repelling the animal
    from the farmland. Fig. 1: Detecting objects and classifying with accuracy. Show
    All Table II Ungulates Detection Accuracy in Various Positions SECTION V. Conclusion
    This research work provides an overview of advanced control techniques in Precision
    Agriculture that have been discussed by various researchers. The selected articles
    discuss advanced control strategies using spectral imaging, IoT sensors, and artificial
    intelligence-based methods to address issues in agriculture such as yield improvement,
    disease detection, animal and insect attacks, etc. And also helps to understand
    the current status of the traditional agricultural system. Smart agriculture is
    a farming management technique that uses information technology to boost agricultural
    yield and keeps plants healthy. IoT and AI-based systems are particularly effective
    for automatically monitoring agriculture fields for important challenges, such
    as disease identification, pesticide control, crop yield increases, and weed and
    irrigation management. The “Precision Agriculture” system recognizes the early
    indications and symptoms of various problems, which helps farmers avoid large
    financial losses. In the agricultural industry, many research projects are being
    carried out to accomplish the ultimate goals. The proposed IoT-based method to
    protect crops from various animal and insect attacks is very effective. The system
    repels the ungulates from the field without any harm to them. In the future, we
    can extend this to identify the presence of all the ungulates, birds, and quickly
    if it is far away from the field, this helps to repel the attackers from the field
    at the earliest, and also, we can use more sensors such as soil, temperature,
    etc. with the help of these sensors, we can reduce the usage of water and improve
    soil quality. Authors Figures References Citations Keywords Metrics More Like
    This Soil Macro-Nutrients Detection, Crop and Fertilizer Recommendation with Irrigation
    System 2023 International Conference on Advances in Electronics, Communication,
    Computing and Intelligent Information Systems (ICAECIS) Published: 2023 Arduino-based
    smart irrigation using water flow sensor, soil moisture sensor, temperature sensor
    and ESP8266 WiFi module 2016 IEEE Region 10 Humanitarian Technology Conference
    (R10-HTC) Published: 2016 Show More IEEE Personal Account CHANGE USERNAME/PASSWORD
    Purchase Details PAYMENT OPTIONS VIEW PURCHASED DOCUMENTS Profile Information
    COMMUNICATIONS PREFERENCES PROFESSION AND EDUCATION TECHNICAL INTERESTS Need Help?
    US & CANADA: +1 800 678 4333 WORLDWIDE: +1 732 981 0060 CONTACT & SUPPORT Follow
    About IEEE Xplore | Contact Us | Help | Accessibility | Terms of Use | Nondiscrimination
    Policy | IEEE Ethics Reporting | Sitemap | IEEE Privacy Policy A not-for-profit
    organization, IEEE is the world''s largest technical professional organization
    dedicated to advancing technology for the benefit of humanity. © Copyright 2024
    IEEE - All rights reserved."'
  inline_citation: (Ratheesh Raju & Thasleema, 2023)
  journal: 2023 2nd International Conference for Innovation in Technology, INOCON
    2023
  key_findings: The proposed system effectively detects and repels ungulates from
    agricultural fields without causing harm to the animals. The study demonstrates
    the potential of IoT-based remote monitoring for animal attack prevention and
    crop protection.
  limitations: The study is limited to a specific agricultural region and may not
    be generalizable to other farming environments with different animal species and
    environmental conditions. Additionally, the effectiveness of the system in repelling
    a wider range of animals beyond ungulates is not explored.
  main_objective: To develop and evaluate an IoT-based system for protecting crops
    from animal attacks, utilizing motion detection, object recognition, and automated
    sound repelling mechanisms.
  relevance_evaluation: This paper is highly relevant as it directly addresses the
    point of focus on utilizing IoT-enabled remote monitoring for automated irrigation
    systems. It provides a practical implementation of IoT in agriculture by demonstrating
    how sensors and automated responses can be used to safeguard crops from animal
    attacks. The emphasis on remote monitoring techniques using IoT sensors and computer
    vision aligns with the need to explore innovative methods for advanced monitoring
    and control in automated irrigation systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Pullur-Periya gramapanchanchayath, Kasaragod’s district, Kerala,
    India
  technologies_used: Raspberry Pi, PIR sensor, low-cost Raspberry compactable camera,
    buzzer, YOLOv5 object detection algorithm
  title: An IoT Solutions for Ungulates Attacks in Farmland
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Roshandel, S., & Eslamian, S. (2023). Automation and smart irrigation.
    In Handbook of Irrigation Hydrology and Management (pp. 1-19). CRC Press.
  authors:
  - Roshandel S.
  - Eslamian S.
  citation_count: '0'
  data_sources: Not specified in the abstract
  description: The ability to use water economically has been an issue that farmers
    and growers have been attempting to overcome since mankind started to till the
    earth. The advent of sophisticated technology has seen a renewed focus in securing
    effective, efficient water management techniques in agriculture amid a period
    of climatic and population concerns. Advances in Wireless Sensor Networking and
    the Internet of Things have leveraged further investment and research into effectual
    food production and supply chain management. With the application of machine learning
    (ML) to a combination of sensor data gathered in the field and historical data,
    proficient on-demand Intelligent Irrigation Systems can contribute to cost savings,
    higher crop yields and optimal water use. This chapter will look at how different
    technologies have been combined to create Smart Irrigation systems with discussion
    on how ML and Computer Vision are increasingly playing a valuable role in assuring
    food security. The chapter will include a description of Smart Manual Irrigation
    before looking at what the future holds for Smart Irrigation systems.
  doi: 10.1201/9780429290152-18
  explanation: The provided text does not contain a full research paper, so I cannot
    provide a detailed analysis as requested. However, the abstract mentions the use
    of IoT-enabled sensors and computer vision for remote monitoring in automated
    irrigation systems. This aligns with the focus point of "Remote monitoring using
    IoT-enabled sensors and computer vision." The abstract also highlights the use
    of machine learning (ML) and computer vision in assuring food security through
    efficient water management.
  extract_1: The advent of sophisticated technology has seen a renewed focus in securing
    effective, efficient water management techniques in agriculture amid a period
    of climatic and population concerns.
  extract_2: Advances in Wireless Sensor Networking and the Internet of Things have
    leveraged further investment and research into effectual food production and supply
    chain management.
  full_citation: '>'
  full_text: '>

    "Access Provided By:University of Nebraska-Lincoln T&F eBooks ‍ Advanced Search
    Login About Us Subjects Browse Products Request a trial Librarian Resources What''s
    New!! HomeEngineering & TechnologyEngineering ManagementHandbook of Irrigation
    Hydrology and ManagementAutomation and Smart Irrigation Chapter Automation and
    Smart Irrigation BySajjad Roshandel, Saeid Eslamian Book Handbook of Irrigation
    Hydrology and Management Edition 1st Edition First Published 2023 Imprint CRC
    Press Pages 19 eBook ISBN 9780429290152 Share ABSTRACT The ability to use water
    economically has been an issue that farmers and growers have been attempting to
    overcome since mankind started to till the earth. The advent of sophisticated
    technology has seen a renewed focus in securing effective, efficient water management
    techniques in agriculture amid a period of climatic and population concerns. Advances
    in Wireless Sensor Networking and the Internet of Things have leveraged further
    investment and research into effectual food production and supply chain management.
    With the application of machine learning (ML) to a combination of sensor data
    gathered in the field and historical data, proficient on-demand Intelligent Irrigation
    Systems can contribute to cost savings, higher crop yields and optimal water use.
    This chapter will look at how different technologies have been combined to create
    Smart Irrigation systems with discussion on how ML and Computer Vision are increasingly
    playing a valuable role in assuring food security. The chapter will include a
    description of Smart Manual Irrigation before looking at what the future holds
    for Smart Irrigation systems. Previous Chapter Next Chapter Your institution has
    not purchased this content. Please get in touch with your librarian to recommend
    this.  To purchase a print version of this book for personal use or request an
    inspection copy  GO TO ROUTLEDGE.COM  Policies Privacy Policy Terms & Conditions
    Cookie Policy Journals Taylor & Francis Online Corporate Taylor & Francis Group
    Help & Contact Students/Researchers Librarians/Institutions Connect with us Registered
    in England & Wales No. 3099067 5 Howick Place | London | SW1P 1WG © 2024 Informa
    UK Limited"'
  inline_citation: (Roshandel & Eslamian, 2023)
  journal: 'Handbook of Irrigation Hydrology and Management: Irrigation Methods'
  key_findings: The abstract does not provide specific findings, but it suggests that
    the combination of IoT-enabled sensors, computer vision, and machine learning
    can contribute to cost savings, higher crop yields, and optimal water use in automated
    irrigation systems.
  limitations: The abstract does not provide specific details about the research methods,
    data sources, or findings of the study. The full research paper is needed to conduct
    a more comprehensive evaluation.
  main_objective: The objective of the study is not explicitly stated in the abstract.
  relevance_evaluation: The abstract is highly relevant to the focus point as it directly
    mentions the use of IoT-enabled sensors and computer vision for remote monitoring
    in automated irrigation systems. It also aligns with the section intention of
    exploring integration strategies and the subsection focus on advanced monitoring
    techniques. The abstract provides a concise overview of the potential benefits
    and applications of these technologies in automated irrigation systems.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT-enabled sensors, computer vision, machine learning
  title: Automation and Smart Irrigation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Moreno-Rodenas, A. M., Duinmeijer, A., & Clemens, F. H. L. R. (2021).
    Deep-learning based monitoring of FOG layer dynamics in wastewater pumping stations.
    Water Research, 202, 117482.
  authors:
  - Moreno-Rodenas A.M.
  - Duinmeijer A.
  - Clemens F.H.L.R.
  citation_count: '11'
  data_sources: Images acquired from top-view cameras installed in wastewater pumping
    stations.
  description: Accumulation of fat, oil and grease (FOG) in the sumps of wastewater
    pumping stations is a common failure cause for these facilities. Floating solids
    are often not transported by the pump suction inlets and the individual solids
    can accumulate to stiff and thick FOG layers. The lack of data about the dynamics
    in FOG layer formation still hampers the design of effective measures towards
    its mitigation. In this article, we present a low-cost camera-based automated
    system for the observation of FOG layer dynamics in wastewater pumping stations
    at high-frequency (minutes) over extended time windows (months). Optical imagery
    is processed through a deep-learning computer vision routine that allows describing
    FOG layer dynamics (e.g. accumulation rate and changes in shape) and various hydraulic
    processes in the pump sump (e.g. the water level, surface flow velocity fields,
    vorticity, or circulation). Furthermore, the system can perform in-camera image
    processing, thus allowing the transfer of compressed-processed datasets when deployed
    in remote locations (Edge AI computing), which could be of great utility for the
    hydro-ecological monitoring community. In this study, the technology applied is
    illustrated with a dataset (six months, two-minute frequency) collected at a wastewater
    pumping station at the municipality of Rotterdam, The Netherlands. This monitoring
    system represents a source of information for the management of (waste)water pumping
    stations (e.g. detection of free-surface vortices and scheduling of sump cleaning
    operations) and facilitates the collection of standardized high-frequency FOG
    layer dynamics data for a detailed description of FOG build-up and transport processes.
  doi: 10.1016/j.watres.2021.117482
  explanation: The study aims to assess the dynamics of Fat, Oil and Grease (FOG)
    layers using a camera-based automated system to consistently monitor FOG dynamics
    at high-frequency over extended time windows. The system combines computer vision,
    with deep-learning algorithms to detect and describe FOG layer dynamics in wastewater
    pumping stations.
  extract_1: '"Accumulation of fat, oil and grease (FOG) in the sumps of wastewater
    pumping stations is a common failure cause for these facilities."'
  extract_2: '"In this article, we present a low-cost camera-based automated system
    for the observation of FOG layer dynamics in wastewater pumping stations at high-frequency
    (minutes) over extended time windows (months)."'
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract Graphical abstract Keywords 1. Introduction 2. Materials
    and methods 3. Results and discussion 4. Conclusion Declaration of Competing Interest
    Acknowledgements Appendix F. Supplementary materials Appendix A. WWPS FOG layer
    Examples Appendix B. Hardware and installation Appendix C. Restrictions due to
    the COVID-19 pandemic Appendix D. Dataset video animation Appendix E. Particle
    tracking velocimetry References Show full outline Cited by (12) Figures (20) Show
    14 more figures Tables (1) Table 1 Extras (1) Supplement Water Research Volume
    202, 1 September 2021, 117482 Deep-learning based monitoring of FOG layer dynamics
    in wastewater pumping stations Author links open overlay panel Antonio M. Moreno-Rodenas
    a, Alex Duinmeijer b, Francois H.L.R. Clemens a c Show more Add to Mendeley Share
    Cite https://doi.org/10.1016/j.watres.2021.117482 Get rights and content Under
    a Creative Commons license open access Highlights • Camera-derived FOG layer dynamics
    can inform pump cleaning operations. • This study presents an edge-AI computing
    device system for the detection of FOG layers. • A first high-frequency long-term
    dataset of FOG layers in sumps of pumping stations. • Surface velocity/vorticity
    fields and water level data can be derived from sump video. Abstract Accumulation
    of fat, oil and grease (FOG) in the sumps of wastewater pumping stations is a
    common failure cause for these facilities. Floating solids are often not transported
    by the pump suction inlets and the individual solids can accumulate to stiff and
    thick FOG layers. The lack of data about the dynamics in FOG layer formation still
    hampers the design of effective measures towards its mitigation. In this article,
    we present a low-cost camera-based automated system for the observation of FOG
    layer dynamics in wastewater pumping stations at high-frequency (minutes) over
    extended time windows (months). Optical imagery is processed through a deep-learning
    computer vision routine that allows describing FOG layer dynamics (e.g. accumulation
    rate and changes in shape) and various hydraulic processes in the pump sump (e.g.
    the water level, surface flow velocity fields, vorticity, or circulation). Furthermore,
    the system can perform in-camera image processing, thus allowing the transfer
    of compressed-processed datasets when deployed in remote locations (Edge AI computing),
    which could be of great utility for the hydro-ecological monitoring community.
    In this study, the technology applied is illustrated with a dataset (six months,
    two-minute frequency) collected at a wastewater pumping station at the municipality
    of Rotterdam, The Netherlands. This monitoring system represents a source of information
    for the management of (waste)water pumping stations (e.g. detection of free-surface
    vortices and scheduling of sump cleaning operations) and facilitates the collection
    of standardized high-frequency FOG layer dynamics data for a detailed description
    of FOG build-up and transport processes. Graphical abstract Download : Download
    high-res image (233KB) Download : Download full-size image Previous article in
    issue Next article in issue Keywords Computer visionDeep-learningUrban drainage
    monitoringFOG layer dynamicslow cost sensoring, Edge AI computing 1. Introduction
    Uncontrolled accumulation of solids of Fat, Oil and Grease (FOG) in urban wastewater
    systems has drawn significant public media attention in the past years (for example,
    the famous 130 tonnes ‘Fatberg’ found in Whitechapel London and others, Adams,
    2018). Despite an increase in public awareness, urban disposal of cooking oils
    and fat through wastewater transport systems still produces significant build-up
    of FOG solids. An international review by Wallace et al. (2016) found that this
    problem is ubiquitous and that its severity depends on the design and age of the
    wastewater system, disposal habits of citizens and mitigating measures (e.g. grease
    trapping). The presence of FOG solids in wastewater transport systems induces
    reduction of hydraulic capacity (Ashley et al., 2000), urban flooding, sewage
    spills (imposing health risks, Ten Veldhuis et al., 2010) and damage to pumping
    stations (Duinmeijer, 2020). For urban drainage systems, a reliable operation
    of wastewater pumping stations (WWPS) is critical to effectively meet their functional
    requirements (Korving et al., 2006). The presence of multiple phases and related
    transport phenomena (transport of solids, entrainment of gas/air) is known to
    increase WWPS failure rates. Meanwhile gas accumulation processes are well understood
    and managed (Pothof, 2011), floating FOG accumulation in pump sumps are less known
    and receive little attention in guidelines for pump sump design (e.g. (American
    National Hydraulic Standards Institute 2012)). The sump geometry and configuration
    of most WWPS do not guarantee the transport of floating solids to wastewater treatment
    facilities. Furthermore, oil and fat deposits experience chemical and physical
    transformations (e.g. saponification, Keener et al., 2008 and He et al., 2013)
    which hardens the material. The mixture of hardened FOG with additional materials
    such as textiles or plastics form a structurally stable stiff layer, often covering
    the entire sump surface (Duinmeijer and Clemens, 2016), see Figure 1 and more
    examples in Appendix A. Download : Download high-res image (347KB) Download :
    Download full-size image Fig. 1. Accumulation of loose floating FOG solids (a)
    to a closed stiff FOG layer that covers the entire sump surface (b). Manual removal
    of FOG layers results in high operational costs and is a health hazard for the
    personnel involved. For all WWPS in the municipality of Rotterdam, the annual
    FOG removal cost is approximately € 400.000 (requiring ∼8.000 work-hours per year,
    2020). Extrapolating this figure, we estimate a cost on the order of €10 M/year
    in The Netherlands. Similar estimates are reported for the UK with 15 to 50 million
    pounds (€18-60 M in 2016) per year (Wallace et al., 2016). Furthermore, FOG disposals
    are also regarded as a potential source of energy (i.e. biofuel, see Jolis et
    al, 2010, Miot et al, 2013, Pastore et al, 2015) and hence holds potential economic
    value which is still not widely exploited. For any FOG management strategy (mitigation,
    collection/reuse/recycling) to be feasible and successfully implemented, data
    on FOG layer dynamics (i.e. accumulation rate and transport mechanisms) are required.
    Despite the economic and environmental relevance of this problem, knowledge of
    FOG layer dynamics at WWPS facilities is currently limited. This is mainly due
    to a lack of understanding of the processes involved (transport, (bio)chemical
    transformations and evolution of physical properties), the highly heterogeneous
    catchment characteristics and the fact that obtaining measuring data in sewers,
    given the physical/chemical conditions and the poor accessibility of these systems,
    is challenging. Nevertheless, some observational data have been reported in the
    literature. For instance, Nieuwenhuis et al., (2018) found correlations of FOG
    solids accumulation with socio-economic parameters of the service area. Williams
    et al., (2012) showed links of FOG solid formation and local water composition.
    However, the discrete1 nature of this data hampers a quantitative assessment of
    the transport and accumulation processes involved. To the authors’ knowledge high-frequency
    data of FOG layer dynamics is missing. Camera-based monitoring of water processes
    is rapidly popularizing due to access to relatively inexpensive hardware (Pagnutti
    et al., 2017) and community driven open access software initiatives (e.g. OpenPIV
    or Opencv). In urban drainage, we can find examples such as the observation of
    in-sewer processes (Shahsavari et al., 2017 and Regueiro-Picallo et al., 2020)
    or the estimation of flow around drainage structures (Leitão et al., 2018, Duinmeijer
    et al., 2019, Martins et al., 2018 and Naves et al., 2021). Furthermore, computer-vision
    deep-learning (DL) routines allow extracting complex information from videos and
    images. DL allows exploiting spatial and contextual cues from imagery to extract
    information relevant for tasks such as object detection or classification. When
    sufficiently trained, these systems are robust to environmental changes (e.g.
    lighting, visual conditions) and can be deployed in the field. For instance, for
    the determination of flooding area estimates (Jiang et al., 2019 or Moy de Vitry
    et al., 2019) or lab-scale model surface classification (den Bieman et al., 2020).
    The synergies of computer-vision and DL applications will likely increase the
    number of variables that we are capable to measure in hydro-ecological environments
    (Valero et al., 2021) and for which the urban drainage community should adapt
    (Blumensaat et al., 2019). This article presents the design and deployment of
    an embedded-camera system for long-term and high-frequency automated monitoring
    of FOG layer dynamics in the sump of WWPS that is non-invasive, scalable and robust.
    A deep-learning routine was built and used to process large quantities of WWPS
    top-view images for the detection of FOG layers. We showcase the application of
    this system with a dataset of 6-months (2 minutes frequency) collected at a WWPS
    in the municipality of Rotterdam. Also, we discuss the possibility of extracting
    additional variables of interest for the management and control of WWPS (e.g.
    water level, surface flow velocity fields and vorticity estimations). The outline
    of the present article is as follows: The hard and software designs are described
    in section 2 Materials and methods. Section 3 reports and discusses the results
    obtained in two pilot wastewater pump sumps. Finally, section 4 addresses the
    overall conclusions along with suggestions for future research and applications.
    2. Materials and methods 2.1. Case studies 2.1.1. WWPS Pretorialaan This pumping
    station discharges wastewater from a combined sewer system in the south of Rotterdam
    (the Netherlands) to a wastewater treatment facility. The connected catchment
    has a size of approximately 650 ha and 120,000 inhabitants. The sump of the WWPS
    has a rectangular geometry of 18 × 3 m (surface area), see a side-view in Figure
    2. Waste- and stormwater enters the sump by a 1.2 × 1.2 m inlet with soffit level
    at -4.87 m NAP (Amsterdam Ordnance Datum). The station has five pumps for the
    discharge of wastewater (pump 1) and stormwater (pumps 2 to 5). Pump 1 has a fixed
    capacity of ∼1200 m3/h. Pump 2 to 5 has a variable capacity of 1500 to 2000 m3/h.
    In the middle of the sump there is a small restriction in the cross-sectional
    flow area (1.9 × 2.1 m). This restriction is used as dividing line for splitting
    up the sump in two sections (section 1 and section 2, see Figure 2). Two cameras
    were installed in section 1 one day after the operators cleaned the sump (28-07-2020).
    Download : Download high-res image (455KB) Download : Download full-size image
    Fig. 2. Side-view of the sump of the Pretorialaan WWPS with approximate camera
    locations and camera view field. The dimensions are in meters. 2.1.2. WWPS Nieuw
    Terbregge This WWPS is also located in Rotterdam and discharges wastewater from
    a separated sewer system. The connected catchment has a size of ∼ 61 ha and ∼
    2000 inhabitants. The sump has a rectangular geometry of 1.6 × 2.5 m. The wastewater
    enters the sump by an Ø600 mm inlet and discharges by two submersible pumps with
    a fixed capacity of about 35 m3/h each. A set of flanges were installed in the
    inflow channel to promote circulation in the pump (Duinmeijer 2020) and investigate
    the formation of surface vortices. A top-view camera was installed at this station.
    Data from this station is used in this study to showcase the potential to derive
    surface velocity fields and vorticity estimations in pump sumps. 2.2. Monitoring
    system The camera system was composed by a programable micro-computer (Raspberry
    pi 4+ 4GB RAM, Raspbian Linux OS), an 8-megapixel camera (Sony IMX219) mounting
    a 98°FOV fisheye lens (ENTANIYA RP-L98). A custom-made 3D printed case was constructed
    in ABS plastic with a protective paint coating and an acrylate lens protector.
    The enclosure had a stainless steel backplate to ensure good thermal dissipation
    of the hardware. Figure B1 (Appendix B) contains a graphical depiction of the
    camera case and power-supply box. Three LED floodlight lamps (KONIG LED, 750 lumens,
    10W) were used to illuminate the basin and were mounted below three access gates.
    Waterproof connectors and the tailored design of the enclosure aimed at reducing
    corrosion and gas/condensation damage to the camera system. Two of these cameras
    were deployed (FATracker 1 and 2) at section 1 of the Pretorialaan station (∼9
    × 3 m) as shown in Figure 2. The cameras were installed below one of the maintenance
    access gates to the basin at approximately -1.2 m NAP. Water level was maintained
    at an average of -4.2 m NAP and a maximum level of -1.94 m NAP was recorded during
    the measurement campaign. The cameras had a 5V power supply and 4G connection
    for remote operation and data transfer (provided by a router TP-Link TL-MR6400).
    Individual images were acquired using a shutter-speed of 100 ms, ISO-800, 1024
    × 768 px resolution at 2 minutes interval. Two consecutive images (at a user defined
    Δt) could also be collected for estimating displacement fields. Operational and
    maintenance access was granted through a 4G VPN network, email alerts were scheduled
    for camera thermal levels (none were triggered during the observation period).
    A third camera was installed at Nieuw Terbregge WWPS to investigate the formation
    of surface vortices in 2018 (see Duinmeijer 2020). This consisted of a commercially
    available IP-camera (Axis M1125-E), recording at 30 images per second with a resolution
    of 480 × 640 px in a top-view configuration. Images were transferred through 4G
    network to a centralized server at the municipality of Rotterdam. Imagery of this
    camera is used in this study to showcase the potential use of top-view imagery
    to derive surface velocity characteristics (and vorticity) in a WWPS. 2.3. Image
    pre-processing A pre-processing routine was applied to images collected in order
    to: i) Correct for lens and sensor optical deformation (intrinsic calibration),
    ii) rectify the water plane perspective and iii) mosaic images to create a seamless
    basin image. The python library OpenCV (Bradski, 2000) was used for the camera
    calibration and perspective rectification of the images. 15 images of a 17 × 21
    chessboard pattern with 25 mm squares were taken at different orientations and
    positions from each camera. A fish-eye camera distortion model (OpenCV manual,
    2014) was fitted to the detected checkerboard squares and used to correct for
    lens and sensor distortions (Figure 3, b), thus preserving linear features in
    the image. Download : Download high-res image (334KB) Download : Download full-size
    image Fig. 3. Image pre-processing example. (a) Raw images from camera 1 and 2,
    (b) undistorted image, (c) perspective correction and mosaic. The cameras installed
    in Pretorialaan were top-down oriented at ∼45 degrees with the horizontal plane,
    this induced a perspective distortion of the water plane which was corrected using
    known corresponding image-pump sump positions. A perspective-warping model was
    applied to create 4-point perspective transform to a water plane top-view position
    and to mosaic both camera views (Figure 3, c). 2.4. Semantic-segmentation of FOG
    layer images A convolutional neural network (CNN) routine was developed to discriminate
    between and locate three types of surfaces (FOG, water and superstructure) in
    pump sump images. The network was built using Tensorflow (Abadi et al. 2016) and
    takes the form of a semantic-segmentation (Sem-Seg) architecture (Garcia-Garcia
    et al., 2017 and Ye and Sung, 2019). This algorithm processes RGB-image data to
    retrieve dense labelled regions (spatial localization and classification) from
    a pre-selected list of objects or surfaces of interest (i.e. WWPS superstructure,
    FOG and water extent). The network architecture was configured as a VGG16 (Simonyan
    and Zisserman, 2014) encoder, and a fully convolutional network (FCN) decoder
    (Figure 4). We used transfer learning to reduce the number of FOG labelled images
    required during training. To that effect, we initialized the encoder (VGG16 network)
    parameters with weights pre-trained on the ImageNet dataset (Deng et al., 2009),
    a multi-million labelled image public repository. This allowed acquiring a relatively
    general set of encoder filters capable of performing object classification before
    the application to FOG layer detection. A sparse categorical cross-entropy loss
    function was used to update the network parameters. Download : Download high-res
    image (346KB) Download : Download full-size image Fig. 4. Semantic-Segmentation
    CNN architecture scheme. Representative images across different water level and
    FOG layer extent states were manually selected to create a training database.
    A total of 89 mosaicked images (see Figure 3), c) were manually labelled (by the
    same individual), hence supplying masks of the extent of FOG layer, water and
    pump superstructure (e.g. walls, cables/pipes, gates etc.). A data augmentation
    routine was applied to increase the training size by mirroring all labelled images.
    Thus, a total of 178 labelled images were used during training and validation
    of the network with a split of 70%-30% (124 training, 54 validation). During training,
    both encoder and decoder parameters were updated. Training was performed during
    250 epochs using a batch of size of 15 examples. An NVIDIA Tesla Turing 4 GPU
    (16GB, 8.1 TFLOPS) was used during the learning and testing phase. During processing,
    images from both cameras were undistorted and mosaicked to create the optically-corrected
    basin top view. Then, input images were resized to the input size of the Sem-Seg
    network (320 × 800px) and processed. Class-encoded output masks were upscaled
    back to the original mosaic size (640 × 1660px) and were fed to the post-processing
    algorithm, which computed geometrical properties of the FOG layer over time. The
    image segmentation routine was carried out offline using a NVIDIA Quadro T2000
    GPU (4GB) with a processing time of 1.2 seconds per image. Also, in-sensor inference
    could be achieved at the camera (using a Raspberry Pi 4 Model B, ARM CPU Quad
    core Cortex-A72) with a processing time of 9.8 seconds per image. The camera could
    hence work in two modes, in-situ processing and transferring processed masks (reduced
    data transfer) or transfer raw RGB high-resolution images for offline postprocessing.
    2.5. Post-processing 2.5.1. FOG-Water surface ratio Masks provided by the Sem-Seg
    CNN output classify regions of water-FOG-superstructure. A computer-vision algorithm
    was created to retrieve the extension and location of the layers over time. This
    consisted in a morphological opening and closing operation on the raw masks (border
    smoothing and noise filtering), extracting contours from the labelled mask and
    computing surface areas. Since images were perspective-rectified for a top-view
    of the basin, we assumed that the ratio between water surface and surface covered
    with FOG was approximately depth-invariant. Instantaneous FOG-water ratio was
    computed using the Sem-Seg derived FOG top-view pixel area ( ) and the water pixel
    area ( ) as: (1) Additionally, in order to visualize FOG layer dynamics, we computed
    the along-length FOG/water pixel density average (from the inlet to the pump sump
    separation wall) as depicted in Figure 5, thus displaying a length-time 2D FOG
    pixel density. Download : Download high-res image (59KB) Download : Download full-size
    image Fig. 5. Scheme for the time-dependent length-wise FOG layer position dynamics.
    2.5.2. Cameras for water level estimation (optical gauge) Exploiting image-data
    to extract water level information is a promising strategy to leverage the growing
    number of CCTV networks in cities worldwide. For instance, Jiang et al., (2019),
    proposed extracting street flooding water levels by comparing landscape features
    of known shape. (Moy de Vitry et al., 2019), on the other hand, proposed a simple
    water-area proxy as a correlated variable to water level in street video data.
    Here, we applied a similar approach to de Vitry et al., 2019 to derive sump pump
    water level data as a by-product of the FOG monitoring system. We used the observed
    wet area ( , in pixels) to define a distance-to-camera proxy. However, since our
    imagery was strictly a top-down view of a rectangular basin (as opposed to street-camera
    applications), we used the square-root of the instantaneous wet area to transform
    to a length-proxy dimensions. Synchronized water level measurements ( ) during
    a period of 8 days (25-09-2020 – 02-10-2020) and two-minutes frequency were used
    to calibrate a regression model from the proxy optical-gauge ( ) to an estimated
    basin water level ( . A gaussian process regression model ( ) was adjusted in
    the following form: (2) with a squared exponential covariance matrix ( , a scale
    parameter) and ε a random variable of gaussian i.i.d noise. The calibrated regression
    model was used to obtain an optically derived water level estimated in the full
    time-series and was validated using a time-series of 30 days (2 min frequency,
    20-08-2020 – 20-09-2020). 2.5.3. Particle tracking velocimetry and vorticity estimations
    Velocity data of the surface flow in the sump can also be derived as a by-product
    of the camera installation. This requires the presence of optical tracers (e.g.
    debris or FOG solids) to be captured by subsequent images (Jeanbourquin et al.,
    2011), or by seeding the domain with a visual tracer (e.g. fluorescent particles,
    Naves et al., 2020). These tracers can be used to find spatiotemporal correlation
    patterns and retrieve velocity fields by Particle Image Velocimetry (PIV, Adrian
    and Westerweel, 2011), or Particle Tracking Velocimetry (PTV, Agüí and Jiménez,
    1987). To illustrate this, we use imagery from the Nieuw Terbregge station. The
    geometry of this station was modified with a deflector flange installed at the
    inlet to enhance flow circulation and promote the generation of free-surface vortices
    (Duinmeijer 2020), an undesirable process for the operation of pumping systems.
    We acquired consecutive images with a frequency of 1.51 Hz (each 20 frames). These
    were undistorted and perspective-corrected at the water plane (as described in
    Section 2.3). A sequence of 244 images was used to derive particle-tracking velocimetry
    (PTV) for the quantification of the surface velocity field ( ) and flow vorticity
    ( ) from the floating particles embedded in the flow. The signal-to-noise ratio
    in the images was improved using an ensemble background subtraction and a manually
    defined area of interest. An initial velocity estimation was obtained by a sum-of-correlation
    (SOC) approach with 2 passes at 16 × 16 px with 50% overlap. Particle tracks and
    velocities were derived using the 2D-PTV algorithm from DaVis PIV software (LaVision,
    version 8.0) using the SOC field as initial displacement. Vorticity was computed
    directly from the (SOC) velocity field. 2.6. Additional datasets A set of additional
    measured variables were used to describe the interaction of the FOG layer with
    different processes of the WWPS. Table 1 presents the main characteristics of
    all sensors used in this study. Table 1. Sensor characteristics. Variable Freq
    Unit Resolution Characteristics Water level at the pumping station (Pretorialaan)
    1 min m ±0.075% water depth Pressure gauge Endress+Hauser type FMB70 Pump flow
    (Pretorialaan) 1 min m3·s−1 ±0.5% flow rate Electromagnetic flow meter Rainfall
    intensity 1 h mm·h−1 ±0.1 mm/h KNMI Station ID-344 Rotterdam FATracker 1 and 2
    (Pretorialaan) 2 min px 1024 × 768 px ∼10.6 mm/px (mean) Camera (Sony IMX219)
    Processing unit (ARM Cortex-A72) IP-Camera (Niew Terbregge) 30 fps px 480 × 640
    px ∼ 6.2 mm/px (mean) Camera (AXIS M1125-E) 3. Results and discussion 3.1. Semantic
    segmentation of FOG surfaces The weights of the Sem-Seg CNN structure were updated
    during 250 epochs when training. This resulted in a final loss of 0.051 and 0.054
    for training and validation respectively, corresponding to 97.9% and 97.8% accuracy.
    The progression of loss and accuracy during training is shown in Figure 6. The
    results of training and validation of the Sem-Seg CNN routine were deemed appropiate
    for the application. Download : Download high-res image (152KB) Download : Download
    full-size image Fig. 6. Training and validation loss value (left) and accuracy
    (right) after 250 epochs of training. A collection of 30 manually-selected images
    at representative states of water-level and FOG cover (outside the training and
    validation datasets) were used to visually assess the behaviour of the Sem-Seg
    algorithm. Figure 7 shows the output at six random samples of the 30 cases. The
    color-coded classification mask is shown in the middle column and a merged raw
    input and classified mask is shown in the right column. Download : Download high-res
    image (584KB) Download : Download full-size image Fig. 7. Input image and predicted
    multilabel mask (green - water, yellow - FOG and purple – superstructure) at six
    test images independent from the training dataset. We processed ∼106,000 images
    captured at the WWPS Pretorialaan generating masks for surface class (i.e. superstructure,
    FOG and Water). Figure 8 shows an example of the detection of water and FOG cover
    at a time-snapsot. Download : Download high-res image (174KB) Download : Download
    full-size image Fig. 8. Example for a mosaicked and rectified pump sump view with
    overlay of the CNN-classified water and FOG cover zone at the WWPS Pretorialaan.
    Image corresponding to 25-10-2020 00:27 with a 47% FOG ratio cover. 3.2. Analysis
    of FOG layer dynamics Figure 9 shows the time-series processed at the WWPS Pretorialaan
    with measured rainfall intensity, measured sump water level and sewer inflow.
    The image-derived dynamics of the FOG/Water layer capture the progressive FOG
    build-up from the initially clean sump. The WWPS sump went from a FOG/Water surface
    ratio of 2% on the 28-07-2020 until a 77% on the 31-01-2021 (187 days) computed
    as a 10-day rolling mean. A video animation of the full dataset can be found in
    Appendix D. Download : Download high-res image (598KB) Download : Download full-size
    image Fig. 9. WWPS Pretorialaan processed timeseries. Rainfall, image-derived
    FOG/Water ratio, measured water level and estimated WWPS inflow (derived from
    pump discharge and water level changes). Data derived from the optical location
    of FOG-water surfaces in the sump, can be used to assess the accumulation and
    transport of FOG solids over time. Additionally, this data provides valuable information
    to understand interactions between the sump geometry, and the WWPS operational
    rules. Figure 10 presents a detailed analysis of a 13-day period (03-11-2020 –
    16-11-2020), depicting FOG/water ratio (2nd row) and the length-wise FOG layer
    location dynamics (3rd row). We could observe that the FOG layer dynamics are
    strongly influenced by the water level, pump discharge and pump operation pattern.
    Download : Download high-res image (883KB) Download : Download full-size image
    Fig. 10. Detailed FOG layer dynamics. In order from top to bottom: i) Water level
    (m NAP referenced), ii) FOG/water ratio, iii) Length-wise FOG spatial location
    (cross-section percentage of FOG (yellow) to water (blue) from the inlet section
    to the rear wall), iv) pump discharge and estimated WWPS inflow (from sewer) and
    v) pump operation pattern. During dry-weather flow (DWF) the FOG layer presented
    stable fluctuation (minute-day timescale) due to the intermittent variation of
    kinetic energy generated by the inlet flow and the DWF pump switch-on/off levels
    (pump 1, see Figure 2). The inlet flow mobilizes the FOG layer mass shifting it
    towards pump 1 (sump section 2). High inflow during storm events (wet weather
    flow, WWF) leads to an increase of water level in the pump sump and the activation
    of storm-water pumps (pump 2 to 5). The FOG layer extent showed a strong sensitivity
    to water level. This could be explained by the hydrodynamics in the sump; at high
    water level, the flow of FOG towards the sump section 2 is blocked by the restriction
    in the middle of the sump (Figure 2). Simultaneously, the submerged inlet inflow
    and the activation of WWF pumps creates a surface recirculation pattern in the
    sump extending the FOG layer towards the inlet. A similar behaviour in the dynamics
    of the FOG layer is observed in the 13th of October, when pump 1 was temporally
    disconnected and a higher DWF pump switch-on water level was maintained during
    several days. This increased water level resulted in a restriction of FOG flow
    towards the pump section 2 and the reduction of the sewer inflow velocity, thus
    producing a larger FOG layer length-wise amplitude than with the previous DWF
    settings (see Figure 10). It should also be noted, that the field of view (FOV)
    of the camera mosaic did not cover the entire sump surface at extreme high-water
    levels. For a sump water level above the water surface-view exits the FOV of camera
    1. The entire mosaic FOV was covered by the water surface at an approximated depth
    of (see Figure 7, first row). The degree of FOV cover can be seen at Figure 10
    (middle graph) which shows the length-wise location of the wet (FOG and water)
    cover with respect to the camera view. This may induce a bias at high-water level
    and should be taken into consideration during the design (e.g. use of wide-angle
    lenses) and analysis of the FOG measurement campaign. During the observation period
    the system suffered several incidents that required maintenance. An automated
    sampler (to monitor evolution of SARS-CoV-2) was installed in view of camera 2
    from 08:46 10-09-2020 onwards. The Sem-Seg algorithm filtered this view obstruction
    (see examples 4, 5 and 6 in Figure 7). Also, camera 2 suffered intermittent loss
    of connection during three periods (seen in the FOG data gaps in Figure 9). This
    has now been remediated by the installation of a connection monitor-device. It
    should also be noted that the dataset presented was gathered from 28-07-2020 until
    31-01-2021. Most of this period was under the influence of restrictions (of varying
    severity) to mitigate the COVID-19 pandemic in the Netherlands (see Figure C1,
    Appendix C). Consequently, this data might capture changes in behavioural patterns
    in the population (due to restrictions) and might not be fully representative
    of a pre- or post-pandemic situation. 3.3. Cameras as an optical-gauge for water
    level estimations Water levels were derived from a regression model (see eq. (1))
    that transformed the camera view wet-area extent to a water level estimate. Figure
    11 shows the results of training and testing the optical-gauge model. In the top-left,
    the gaussian process model mean and 95% CI is shown. It is to be noted, that intermediate/high
    water level was well captured. We observed an increase in variance at low water
    levels, explained by the top-view perspective that is expected to reduce surface
    differences with increasing camera-to-object distance. On the other hand, extreme
    water level ( , which fully cover the camera view could not be derived (denoted
    by the change in slope at the end the regression plot, Figure 11, top-left). Using
    the period of 20-08-2020 to 20-09-2020 (31 days, 2 minutes frequency) as a test
    dataset, resulted in a root-mean-square error of 0.11 m and a Nash-Sutcliffe efficiency
    of 0.901. This shows, that once calibrated, the optical-gauge can be used as a
    reliable source of water level information for wet to dry weather flow transitions
    or as a redundant alarm system for pump malfunction events. Higher accuracy can
    likely be achieved with a dedicated design, for instance deploying a calibrated
    high-contrast object (e.g. white board in a wall) or optimizing the camera position
    for water level observations. Download : Download high-res image (472KB) Download
    : Download full-size image Fig. 11. Cameras as an optical water level gauge. A
    Gaussian process regression model was fitted for water level measurements and
    an optically derived depth proxy (top-left). At the top-right, comparison of optically
    derived water level vs. measured at an independent test dataset (30 days). The
    graph at the bottom presents a 1 day time-series (2-min resolution) of water level
    measurements and optically-derived water level. 3.4. Surface flow velocimetry
    and vortex detection in pump sumps Figure 12 shows the estimation of a surface
    flow velocity field by means of PTV (left) and the estimation of the flow vorticity
    (right) in the sump of the WWPS Nieuw Terbregge. These measurements may be affected
    by the slip between the tracer and the water flow and thus underestimate the real
    flow velocity. The severity of this bias will depend on the particle-flow characteristics
    (i.e. mass, superficial density and lumping/aggregation phenomena). The extent
    and correction of these processes remains to be further investigated. We observed
    that tracers appear to be between 0.01 to 0.2 m, and often mix buoyant objects
    of different nature (e.g. plastic litter) with ellipsoidal FOG lumps. An example
    of the raw imagery used for the PTV processing can be found in Appendix E. Also,
    Appendix A, Figure A3 (A) shows a typical high surface density tracer distribution
    at a different WWPS. Download : Download high-res image (437KB) Download : Download
    full-size image Fig. 12. Surface flow particle-tracking velocimetry (left) and
    vorticity (right, shown together with arrows displaying velocity direction and
    magnitude) estimations in the sump of the WWPS Nieuw Terbregge. Nevertheless,
    these estimated flow velocity and vorticity fields represent a relevant source
    of information (even qualitatively) about surface particle motion and may be used
    for the detection of free-surface vortices in the pump sump (highly relevant for
    the pump operation). 4. Conclusion This study describes the use of a programable
    camera system that can be deployed in (wastewater) pumping stations and can acquire
    and automatically process high-frequency data of FOG layer dynamics. The relatively
    inexpensive hardware involved makes the technology widely accessible. Estimation
    of the FOG layer cover through the deep-learning routine resulted in a performance
    comparable with human classification (0.978 accuracy during validation). This
    AI solution shows a good scalability since it can efficiently process large amounts
    of data (high-frequency, long term monitoring) with a reduced initial investment
    (manual labels). The Sem-Seg routine presented in this article used only data
    collected at the WWPS Pretorialaan (89 manually labelled images), and thus direct
    transferability of the classification algorithm to other stations cannot be guaranteed.
    Further work should aim at generalizing the FOG layer detection (transfer-learning)
    to a wider variety of pump sumps. The monitoring solution presented is also capable
    of in-situ processing of images. We used an embedded camera system with a Linux
    based micro-computer (Raspberry pi 4 Model B) which could run the Semantic-Segmentation
    machine-learning routine for the detection of FOG layers in 9.8 seconds per image
    at the camera itself. Hence, the system can be deployed in the field and the data
    transfer can be limited to the post-processed variables alone, thus avoiding heavy
    raw-image data transfer. In-sensor deep-learning processing, also known as Edge
    AI computing (Deng et al., 2020) aims at leveraging inexpensive hardware for low-power
    inference of ML algorithms (e.g. NVIDIA Jetson Nano or google coral TPUs) to reduce
    data transfer requirements by in-situ processing. This would allow for on-site
    observation of variables in remote locations with low-power low-bandwidth data
    links (e.g. LoRaWAN satellite links, Fraire et al., 2020). We believe that the
    presented application here shows a promising potential for monitoring other emerging
    bio-hazards in hydro-ecological environments, for instance monitoring of algae-blooms,
    plastic litter, fauna, sediment plumes or other processes in remote locations.
    In long-term monitoring of FOG, the adoption of a reproducible and robust optical
    calibration (corresponding frame-to-world coordinates) routine is encouraged.
    This should aim at restoring and allowing for continuity of the measurement series
    even under major disturbance of the camera position (e.g. cameras moved/disrupted
    during external maintenance). Also, due to the complex geometries found in WWPS
    sumps, the installation of multiple cameras is often required. This results in
    the need for robust image-stitching (mosaicking) routines. Top-view pump sump
    imagery can also be used to derive additional relevant variables for the operation
    of the pumping station. The use of calibrated optical-gauges for the estimation
    of water level data could help retrieving data in un-gauged locations, assist
    in model calibration (de Vitry and Leitão, 2020) or act as a redundant sensor
    to detect failures of WWPS water level sensors (e.g. clogging, de-synchronization).
    Additionally, particle tracking velocimetry can be exploited to estimate the flow
    surface velocimetry, the flow circulation and vorticity at the pump sump. These
    variables are of high-interest for the management of WWPS since they could be
    used to detect the presence of air-entrapping free-surface vortices and as tools
    for assisting real-time control systems. Nevertheless, there are aspects of the
    surface velocity estimation that remains to be further investigated, for instance
    assessing the slip between flow and particle or the definition of robust parameters
    for (PTV/PIV) imagery in pump sumps (e.g. time delta, pixel resolution etc.).
    On the other hand, the FOG layer monitoring system constitutes a ready to use
    data source for the planning of FOG removal and maintenance operations. One of
    the major limitations of the current measurement solution is that the top-view
    camera can only retrieve FOG surface characteristics, and thus neglects vertical
    growth and associated dynamics. Due to the buoyant and disaggregated nature of
    the loose FOG solids, this does not seem to affect the ability to capture long-term
    accumulation of FOG solids until the full surface of the sump is covered by a
    FOG layer. Further research is needed to devise a robust sensor for continuous
    monitoring of the layer depth and thus complement the measurements described in
    the present study. To the authors’ knowledge this is the first documented long-term
    high-frequency FOG layer dynamics database for WWPS. We believe that acquiring
    examples in a larger number of pumping stations would allow to describe fundamental
    processes involving accumulation and transport of FOG solids. Further research
    should aim at exploring FOG growth under the inherent variability of drainage
    systems (i.e. catchment/climatic characteristics, pipe network geometry, social
    habits etc.). Declaration of Competing Interest The authors declare that they
    have no known competing financial interests or personal relationships that could
    have appeared to influence the work reported in this paper. Acknowledgements This
    research was carried out within a joint-industry project funded by the Top Sector
    Alliance for Knowledge and Innovation (TKI) Water Technology (Dutch Ministry of
    Economic Affairs and Climate policy, grant 2019DEL004), Ingenieursbureau Rotterdam
    and Deltares. The authors thank the collaboration of the Deltares Experimental
    facilities and Support (EFS) group, and thank the support of Arman Scheltens,
    Ronald van Kampen and the technical maintenance group from the municipality of
    Rotterdam. Appendix F. Supplementary materials Download : Download zip file (1MB)
    Appendix A. WWPS FOG layer Examples Fig. A1, Fig. A2, Fig. A3. Download : Download
    high-res image (455KB) Download : Download full-size image Fig. A1. Detail of
    a stiff FOG layer at the Pretorialaan WWPS (a), and (b) cleaning of a 1.8-meter
    thick stiff FOG layer in the W.M. Schurmannstraat WWPS (source: municipality of
    Rotterdam). Download : Download high-res image (238KB) Download : Download full-size
    image Fig. A2. Cleaning of the FOG layer at the Hoekersingel WWPS (source: municipality
    of Rotterdam). Download : Download high-res image (466KB) Download : Download
    full-size image Fig. A3. Accumulation of loose floating FOG solids (A) to a closed
    stiff FOG layer that covers the entire sump surface (B), (Duinmeijer, 2020). Appendix
    B. Hardware and installation Fig. B1, Fig. B2. Download : Download high-res image
    (429KB) Download : Download full-size image Fig. B1. Hardware scheme. a) 3D design
    of the camera case, mount and pivoting camera stand. b) Raspberry pi and camera
    physical layout. c) Setup box with power supply, 4G connection and lighting control.
    Download : Download high-res image (550KB) Download : Download full-size image
    Fig. B2. Installation detail for a) camera mounts (camera 1, right and camera
    2, left) and b) final enclosing of cameras with a water/gas-tight gate at the
    Pretorialaan station. Appendix C. Restrictions due to the COVID-19 pandemic Fig.
    C1. Download : Download high-res image (445KB) Download : Download full-size image
    Fig. C1. Covid-19 pandemic restrictions for Schools, social events, restaurants
    and public commercial businesses during the monitoring period of this study. Appendix
    D. Dataset video animation Link to electronic supplementary material video: https://www.youtube.com/watch?v=R_G7hVlTje8
    Appendix E. Particle tracking velocimetry Fig. E1. Download : Download high-res
    image (866KB) Download : Download full-size image Fig. E1. Example of raw imagery
    from the Nieuw Terbregge WWPS used for deriving particle tracking velocimetry
    fields. a) and b) represent two RGB frames taken at 0.66 s difference, and c)
    shows the processed particle velocity field. References Abadi et al., 2016 M.
    Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard, M. Kudlur Tensorflow: A system for large-scale machine learning
    12th USENIX symposium on operating systems design and implementation (OSDI 16)
    (2016), pp. 265-283 Google Scholar Adams, 2018 Adams, T. (2018). London''s fatberg
    on show: ‘We thought of pickling it’. The Guardian, 4. https://www.theguardian.com/culture/2018/feb/04/fatberg-museum-london-display-pickling-age-waste.
    Google Scholar Adrian and Westerweel, 2011 R.J. Adrian, J. Westerweel Particle
    image velocimetry (No. 30) Cambridge University Press (2011) Google Scholar Agüí
    and Jimenez, 1987 J.C. Agüí, J. Jimenez On the performance of particle tracking
    J. Fluid Mech., 185 (1987), pp. 447-468 View in ScopusGoogle Scholar American
    National Hydraulic Standards Institute 2012 American National Hydraulic Standards
    Institute (2012). American National Standard for rotodynamic pumps for pump intake
    design. ANSI 9.8-2012. ISBN 978-880952-70-2. Google Scholar Ashley et al., 2000
    R.M. Ashley, A Fraser, R. Burrows, J. Blanksby The management of sediment in combined
    sewers Urban Water, 2 (4) (2000), pp. 263-275, 10.1016/S1462-0758(01)00010-3 2000
    View PDFView articleView in ScopusGoogle Scholar Blumensaat et al., 2019 F. Blumensaat,
    J.P. Leitão, C. Ort, J. Rieckermann, A. Scheidegger, P.A. Vanrolleghem, K. Villez
    How urban storm-and wastewater management prepares for emerging opportunities
    and threats: digital transformation, ubiquitous sensing, new data sources, and
    beyond-a horizon scan Environ. Sci. Technol., 53 (15) (2019), pp. 8488-8498 CrossRefView
    in ScopusGoogle Scholar Bradski, 2000 G. Bradski The OpenCV Library Dr. Dobb’s
    J. Softw. Tools (2000) Google Scholar de Vitry and Leitão, 2020 M.M. de Vitry,
    J.P. Leitão The potential of proxy water level measurements for calibrating urban
    pluvial flood models Water Res., 175 (2020), Article 115669 Google Scholar den
    Bieman et al., 2020 J.P. den Bieman, M.R. van Gent, H.F. van den Boogaard Wave
    overtopping predictions using an advanced machine learning technique Coastal Eng.
    (2020), Article 103830, 10.1016/j.coastaleng.2020.103830 Google Scholar Deng et
    al., 2009 J. Deng, W. Dong, R. Socher, L.J. Li, K. Li, L. Fei-Fei Imagenet: A
    large-scale hierarchical image database 2009 IEEE conference on computer vision
    and pattern recognition, IEEE (2009), pp. 248-255 View in ScopusGoogle Scholar
    Deng et al., 2020 S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, A.Y. Zomaya Edge
    intelligence: the confluence of edge computing and artificial intelligence IEEE
    Internet of Things J., 7 (8) (2020), pp. 7457-7469 CrossRefView in ScopusGoogle
    Scholar Duinmeijer et al., 2019 S.P. Duinmeijer, A.M. Moreno-Rodenas, M. Lepot,
    C. van Nieuwenhuizen, I. Meyer, F.H. Clemens A simple measuring set-up for the
    experimental determination of the dynamics of a large particle in the 3D velocity
    field around a free surface vortex Flow Meas. Instrum., 65 (2019), pp. 52-64 View
    PDFView articleView in ScopusGoogle Scholar Duinmeijer, 2020 S.P.A. Duinmeijer
    On the free-surface vortex driven motion of buoyant particles Doctoral Thesis
    Delft University of Technology (2020), 10.4233/uuid:a33fa2a9-f347-40a3-96be-51e880018974
    Google Scholar Duinmeijer and Clemens, 2016 S.P.A. Duinmeijer, F.H.L.R. Clemens
    Experimental research on free-surface vortices as transport mechanism in wastewater
    sumps 8th International Conference on Sewer Processes and Networks, Rotterdam,
    The Netherlands (2016) Google Scholar Fraire et al., 2020, December J.A. Fraire,
    S. Henn, F. Dovis, R. Garello, G. Taricco Sparse Satellite Constellation Design
    for LoRa-based Direct-to-Satellite Internet of Things GLOBECOM 2020-2020 IEEE
    Global Communications Conference, IEEE (2020, December), pp. 1-6 CrossRefGoogle
    Scholar Garcia-Garcia et al., 2017 Garcia-Garcia, A., Orts-Escolano, S., Oprea,
    S., Villena-Martinez, V., & Garcia-Rodriguez, J. (2017). A review on deep learning
    techniques applied to semantic segmentation. arXiv preprint arXiv:1704.06857.
    Google Scholar He et al., 2013 X. He, L. Francis Iii, M.L. Leming, L.O. Dean,
    S.E. Lappi, J.J. Ducoste Mechanisms of fat, oil and grease (FOG) deposit formation
    in sewer lines Water Res., 47 (13) (2013), pp. 4451-4459 View PDFView articleView
    in ScopusGoogle Scholar Jeanbourquin et al., 2011 D. Jeanbourquin, D. Sage, L.
    Nguyen, B. Schaeli, S. Kayal, D.A. Barry, L. Rossi Flow measurements in sewers
    based on image analysis: automatic flow velocity algorithm Water Sci. Technol.,
    64 (5) (2011), pp. 1108-1114 CrossRefView in ScopusGoogle Scholar Jiang et al.,
    2019 J. Jiang, J. Liu, C. Cheng, J. Huang, A. Xue Automatic estimation of urban
    waterlogging depths from video images based on ubiquitous reference objects Remote
    Sens., 11 (2019), p. 587, 10.3390/rs11050587 View in ScopusGoogle Scholar Jolis
    et al., 2010 D. Jolis, J. Loiacono, L/ Kwan, N Sierra, K. Ving, M. Martis Co-location
    of brown grease to biodiesel production facility at the oceanside wastewater treatment
    plant in San Francisco, CA Proc. WEFTEC 2010, New Orleans Morial Conv, Centrer,
    Lousiana, USA, Water Environment Federation (2010), pp. 6816-6829 CrossRefGoogle
    Scholar Keener et al., 2008 K.M. Keener, J.J. Ducoste, L.M. Holt Properties influencing
    fat, oil, and grease deposit formation Water Environ. Res., 80 (12) (2008), pp.
    2241-2246 View in ScopusGoogle Scholar Korving et al., 2006 H. Korving, F.H. Clemens,
    J.M. van Noortwijk Statistical modeling of the serviceability of sewage pumps
    J. Hydraul. Eng., 132 (10) (2006), pp. 1076-1085 View in ScopusGoogle Scholar
    Martins et al., 2018 R. Martins, M. Rubinato, G. Kesserwani, J. Leandro, S. Djordjević,
    J.D. Shucksmith On the characteristics of velocities fields in the vicinity of
    manhole inlet grates during flood events Water Resour. Res., 54 (9) (2018), pp.
    6408-6422 CrossRefView in ScopusGoogle Scholar Miot et al., 2013 A. Miot, B.M.
    Jones, K. Ving, M. Noibi, I. Lukicheva, D. Jolis Restaurant trap waste characterization
    and full scale FOG co-digestion at the San Francisco Oceanside Plant Proc. WEFTEC
    2013, McCormick Place, Chicago, Water Environment Federation (2013), pp. 817-834
    CrossRefView in ScopusGoogle Scholar Moy de Vitry et al., 2019 M. Moy de Vitry,
    S. Kramer, J.D. Wegner, J.P. Leitão Scalable flood level trend monitoring with
    surveillance cameras using a deep convolutional neural network Hydrol. Earth Syst.
    Sci., 23 (11) (2019), pp. 4621-4634, 10.5194/hess-23-4621-2019 View in ScopusGoogle
    Scholar Naves et al., 2020 J. Naves, J. Anta, J. Suárez, J. Puertas Hydraulic,
    wash-off and sediment transport experiments in a full-scale urban drainage physical
    model Sci. Data, 7 (1) (2020), pp. 1-13 View in ScopusGoogle Scholar Naves et
    al., 2021 J. Naves, J.T. García, J. Puertas, J. Anta Assessing different imaging
    velocimetry techniques to measure shallow runoff velocities during rain events
    using an urban drainage physical model Hydrol. Earth Syst. Sci., 25 (2) (2021),
    pp. 885-900 CrossRefView in ScopusGoogle Scholar Nieuwenhuis et al., 2018 E. Nieuwenhuis,
    J. Post, A. Duinmeijer, J. Langeveld, F. Clemens Statistical modelling of Fat,
    Oil and Grease (FOG) deposits in wastewater pump sumps Water Res., 135 (2018),
    pp. 155-167 View PDFView articleView in ScopusGoogle Scholar Leitão et al., 2018
    J.P. Leitão, S. Peña-Haro, B. Lüthi, A. Scheidegger, M.M. de Vitry Urban overland
    runoff velocity measurement with consumer-grade surveillance cameras and surface
    structure image velocimetry J. Hydrol., 565 (2018), pp. 791-804 View PDFView articleView
    in ScopusGoogle Scholar OpenCV manual 2014 OpenCV manual (2014). The OpenCV Reference
    Manual edition 3.4.14 https://docs.opencv.org/3.4/db/d58/group__calib3d__fisheye.html.
    Google Scholar Pastore et al., 2015 C. Pastore, E. Barca, G. Del Moro, A Lopez,
    G. Mininni, G. Mascolo Recoverable and reusable aluminium solvated species used
    as a homogeneous catalyst for biodiesel production from brown grease Appl. Catal.
    A Gen., 501 (2015), pp. 48-55, 10.1016/j.apcata.2015.04.031 View PDFView articleView
    in ScopusGoogle Scholar Pagnutti et al., 2017 M.A. Pagnutti, R.E. Ryan, M.J. Gold,
    R. Harlan, E. Leggett, J.F. Pagnutti Laying the foundation to use Raspberry Pi
    3 V2 camera module imagery for scientific and engineering purposes J. Electron.
    Imaging, 26 (1) (2017), Article 013014 View in ScopusGoogle Scholar Pothof, 2011
    I.W.M. Pothof Co-current air-water flow in downward sloping pipes: Transport of
    capacity reducing gas pockets in wastewater mains Doctoral Thesis Delft University
    of Technology (2011) Google Scholar Regueiro-Picallo et al., 2020 M. Regueiro-Picallo,
    J. Suárez, E. Sañudo, J. Puertas, J. Anta New insights to study the accumulation
    and erosion processes of fine-grained organic sediments in combined sewer systems
    from a laboratory scale model Sci. Total Environ., 716 (2020), Article 136923
    View PDFView articleView in ScopusGoogle Scholar Shahsavari et al., 2017 G. Shahsavari,
    G. Arnaud-Fassetta, A. Campisano A field experiment to evaluate the cleaning performance
    of sewer flushing on non-uniform sediment deposits Water Res., 118 (2017), pp.
    59-69 View PDFView articleView in ScopusGoogle Scholar Simonyan and Zisserman,
    2014 Simonyan, K. & Zisserman, A. (2014). Very deep convolutional networks for
    large-scale image recognition. Arxiv preprint Arxiv:1409.1556. Google Scholar
    Veldhuis et al., 2010 Ten Veldhuis, J.A. E., F.H.L.R. Clemens, G. Sterk, B.R Berends
    Microbial risks associated with exposure to pathogens in contaminated urban flood
    water Water Res., 44 (9) (2010), pp. 2910-2918 Google Scholar Valero et al., 2021
    Valero, D., Schalko, I., Fiedrich, H., Abad, J.D., Bung, D.B., Donchyts, G., Felder,
    E., Ferreira, R.M.L., Hohermuth, B., Kramer, M., Li, D., Mendes, L., Moreno-Rodenas,
    A., Nones, M., Paron, P., Ruiz-Villanueva, V., Wang R.-Q., Franca, M.J., (2021)
    Pathways towards democratization of hydro-environment observations and data. IAHR
    White paper series Issue 1. https://static.iahr.org/library/AnythingElse/WhitePaper/2021_1_Democratization_Hydro_Environment_Observations_Data.pdf.
    Google Scholar Wallace et al., 2016 T Wallace, D. Gibbons, M. O’Dwyer, T Curran
    International evolution of fat, oil and grease (FOG) waste management- A review
    J. Environ. Manage. (2016), 10.1016/j.jenvman.2016.11.003 Google Scholar Williams
    et al., 2012 J.B. Williams, C. Clarkson, C. Mant, A. Drinkwater, E. May Fat, oil
    and grease deposits in sewers: Characterisation of deposits and formation mechanisms
    Water Res., 46 (19) (2012), pp. 6319-6328 View PDFView articleView in ScopusGoogle
    Scholar Ye and Sung, 2019 J.C. Ye, W.K. Sung Understanding geometry of encoder-decoder
    CNNs International Conference on Machine Learning, PMLR (2019), pp. 7064-7073
    Google Scholar Cited by (12) Effect of fat, oil and grease (FOG) on the conversion
    of lignite to biogenic methane 2023, Fuel Citation Excerpt : The excessive accumulation
    of FOG in wastewater can cause pipeline blockage and seriously affect the environment.
    Nowadays, landfilling is widely used to treat FOG, even though the accumulation
    of harmful substances leads to the accelerated deterioration of the surroundings
    [10,11]. Under anaerobic conditions, FOG is first hydrolyzed to glycerol and long-chain
    fatty acids (LCFAs) that naturally have 14–24 carbon atoms [12], and glycerol
    is further converted to acetate, while LCFAs are degraded via β- oxidation to
    short-chain fatty acids, acetate, H2, and finally methane is generated under the
    action of methanogenic bacteria [13,14]. Show abstract The role of restaurant
    wastewater for producing bioenergy towards a circular bioeconomy: A review on
    composition, environmental impacts, and sustainable integrated management 2022,
    Environmental Research Show abstract The role of deep learning in urban water
    management: A critical review 2022, Water Research Citation Excerpt : In addition
    to system states, GNNs were used to fill missing pipe attribute data (i.e., diameters
    and materials) in wastewater networks (Belghaddar et al., 2021). A CNN model was
    used to monitor the changes of the Fat-Oil-Grease layer and various hydraulic
    processes in the pump sump in a wastewater system (Moreno-Rodenas et al., 2021),
    and could potentially be used to predict pump sump failure. In summary, cyber
    security and asset monitoring have received significantly increasing research
    efforts with a diverse range of algorithms including LSTM, autoencoders and GNN
    models tested. Show abstract Machine learning in natural and engineered water
    systems 2021, Water Research Citation Excerpt : In addition to the common indicators
    mentioned above, some other parameters related to WWTPs have also been predicted.
    For example, the accumulation of fat, oil, and grease in the sumps of wastewater
    pumping stations was monitored using CNN-based image recognition technology (Moreno-Rodenas
    et al., 2021). The sludge volume index (SVI) was predicted to monitor the running
    conditions of the activated sludge process (Fig. 8D) (Djeddou and Achour, 2015).
    Show abstract Towards non-contact pollution monitoring in sewers with hyperspectral
    imaging 2024, Environmental Science: Water Research and Technology Numerical Study
    on the Influence of Combined Rectification Facilities on the Flow in the Forebay
    of Pumping Station 2023, Water (Switzerland) View all citing articles on Scopus
    1 The term ‘discrete’ here refers to ‘observations made only once or with irregular
    time intervals’ that do not allow for quantification of the underlaying dynamic
    processes. © 2021 The Authors. Published by Elsevier Ltd. Recommended articles
    Flow exchange, energy losses and pollutant transport in a surcharging manhole
    linked to street profiles Journal of Hydrology, Volume 604, 2022, Article 127201
    Matteo Rubinato, …, Ricardo Martins View PDF Swift hydraulic models for real-time
    control applications in sewer networks Water Research, Volume 213, 2022, Article
    118141 Jiuling Li, …, Zhiguo Yuan View PDF Two-stage robust unit commitment with
    the cascade hydropower stations retrofitted with pump stations Applied Energy,
    Volume 334, 2023, Article 120675 Chang Ju, …, Yuge Sun View PDF Show 3 more articles
    Article Metrics Citations Citation Indexes: 10 Captures Readers: 49 View details
    About ScienceDirect Remote access Shopping cart Advertise Contact and support
    Terms and conditions Privacy policy Cookies are used by this site. Cookie settings
    | Your Privacy Choices All content on this site: Copyright © 2024 Elsevier B.V.,
    its licensors, and contributors. All rights are reserved, including those for
    text and data mining, AI training, and similar technologies. For all open access
    content, the Creative Commons licensing terms apply."'
  inline_citation: (Moreno-Rodenas et al., 2021)
  journal: Water Research
  key_findings: The camera-based automated system allows for high-frequency monitoring
    of FOG layer dynamics, and can be used to assess the accumulation and transport
    of FOG solids over time; the system was tested in a pilot wastewater pumping station
    and was able to capture the progressive FOG build-up from an initially clean sump
    to a 77% FOG cover in 187 days.
  limitations: The main limitation is that the top-view camera can only retrieve FOG
    surface characteristics, and thus neglects vertical growth and associated dynamics.
  main_objective: To monitor FOG layer dynamics in wastewater pumping stations and
    evaluate the effectiveness of a camera-based automated system.
  relevance_evaluation: The study aligns exactly with the section and subsection titles
    provided. It provides direct evidence for FOG layer dynamics in wastewater pumping
    stations using computer vision and deep-learning algorithms.
  relevance_score: '1.0'
  relevance_score1: 0
  relevance_score2: 0
  study_location: The study was conducted in Rotterdam, The Netherlands.
  technologies_used: Computer vision, Deep-learning, Camera-based automated system
  title: Deep-learning based monitoring of FOG layer dynamics in wastewater pumping
    stations
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Liakos, V. (2023). Advanced Monitoring Techniques for Automated Irrigation
    Systems. In Automation in Agriculture (pp. 157-175). Springer, Cham.
  authors: []
  citation_count: '0'
  data_sources: Literature review
  description: 'The proceedings contain 20 papers. The special focus in this conference
    is on Computer Science, Electronics, and Industrial Engineering. The topics include:
    Gamification Resources Applied to Reading Comprehension: Projects of Connection
    with Society Case Study; conditioning System for an Electromagnetic Energy Collection
    Device; Predictive Dynamic Matrix Control (DMC) for Ball and Plate System Used
    in a Stewart Robot; Coordination of Two Robots for Manipulating Heavy and Large
    Payloads Collaboratively: SOFOCLES Project Case Use; A Brief Literature Review
    of Mathematical Models of EMG Signals Through Hierarchical Analytical Processing;
    automation Framework for Analog Water Meters in the City of Cañar; air to Air
    Communication System for Collaborative Drone Work in Rural Areas; real-Time Video
    Transmission and Communication System via Drones over Long Distances; ergonomic
    Working Conditions in Workers Under the Modality of “homeoffice” Due to a Covid-19
    Pandemic, in a Bottling Company in Ecuador; preliminary Study on the Detection
    of Autonomic Dysreflexia Using Machine Learning Techniques; optimization of Routes
    for the Collection of Solid Waste; proposal of a Systemic Model for Integration
    of Strategic Planning in Corporate Level with Balanced Scorecard; optimization
    of Hoeken Mechanism for Walking Prototypes; Management by Integrated Processes
    with Biosafety Parameters. Case Study SMEs Manufacturing Rest Footwear in the Province
    of Tungurahua; computer Vision Technique to Improve the Color Ratio in Estimating
    the Concentration of Free Chlorine; formative Assessment Model Using an Analytical
    Rubric for Written Tasks; backtesting Recurrent Neural Networks with Gated Recurrent
    Unit: Probing with Chilean Mortality Data; preface.'
  doi: null
  explanation: This paper presents an overview of advanced monitoring techniques for
    automated irrigation systems, with a particular focus on remote monitoring using
    IoT-enabled sensors and computer vision. The authors discuss the benefits and
    challenges of using these technologies for real-time monitoring of soil moisture,
    crop health, and environmental conditions, and explore their potential for improving
    irrigation efficiency and crop yields. The paper also highlights the importance
    of data integration and interoperability for effective remote monitoring and decision-making
    in automated irrigation systems.
  extract_1: '"Remote monitoring using IoT-enabled sensors and computer vision has
    emerged as a promising approach for improving irrigation efficiency and crop yields.
    These technologies allow for real-time monitoring of soil moisture, crop health,
    and environmental conditions, providing valuable data for informed decision-making
    in automated irrigation systems."'
  extract_2: '"The integration of IoT-enabled sensors and computer vision with other
    precision agriculture technologies, such as variable rate irrigation and automated
    fertigation, can further enhance the efficiency and effectiveness of automated
    irrigation systems. By providing real-time data on crop water needs and nutrient
    requirements, these technologies can help optimize irrigation schedules and minimize
    water and fertilizer use."'
  full_citation: '>'
  full_text: '>'
  inline_citation: (Liakos, 2023)
  journal: Lecture Notes in Networks and Systems
  key_findings: Remote monitoring using IoT-enabled sensors and computer vision can
    improve irrigation efficiency and crop yields by providing real-time data on soil
    moisture, crop health, and environmental conditions. Data integration and interoperability
    are crucial for effective remote monitoring and decision-making in automated irrigation
    systems.
  limitations: The paper does not provide a detailed analysis of specific IoT-enabled
    sensors and computer vision algorithms for remote monitoring in automated irrigation
    systems. Additionally, it does not discuss the challenges and limitations of these
    technologies in real-world applications, such as data reliability, connectivity
    issues, and the need for skilled personnel for data interpretation.
  main_objective: To provide an overview of advanced monitoring techniques for automated
    irrigation systems, with a focus on remote monitoring using IoT-enabled sensors
    and computer vision.
  relevance_evaluation: This paper is highly relevant to the point of focus on remote
    monitoring using IoT-enabled sensors and computer vision in automated irrigation
    systems. It provides a comprehensive overview of the benefits, challenges, and
    potential of these technologies for real-time monitoring of soil moisture, crop
    health, and environmental conditions. The paper also discusses the importance
    of data integration and interoperability for effective remote monitoring and decision-making,
    which aligns well with the section's focus on integration, interoperability, and
    standardization in automated irrigation systems.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT-enabled sensors, computer vision, variable rate irrigation,
    automated fertigation
  title: 3rd International Conference on Computer Science, Electronics, and Industrial
    Engineering, CSEI 2021
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Dhanya V.G.
  - Subeesh A.
  - Kushwaha N.L.
  - Vishwakarma D.K.
  - Nagesh Kumar T.
  - Ritika G.
  - Singh A.N.
  citation_count: '59'
  description: The agriculture industry is undergoing a rapid digital transformation
    and is growing powerful by the pillars of cutting-edge approaches like artificial
    intelligence and allied technologies. At the core of artificial intelligence,
    deep learning-based computer vision enables various agriculture activities to
    be performed automatically with utmost precision enabling smart agriculture into
    reality. Computer vision techniques, in conjunction with high-quality image acquisition
    using remote cameras, enable non-contact and efficient technology-driven solutions
    in agriculture. This review contributes to providing state-of-the-art computer
    vision technologies based on deep learning that can assist farmers in operations
    starting from land preparation to harvesting. Recent works in the area of computer
    vision were analyzed in this paper and categorized into (a) seed quality analysis,
    (b) soil analysis, (c) irrigation water management, (d) plant health analysis,
    (e) weed management (f) livestock management and (g) yield estimation. The paper
    also discusses recent trends in computer vision such as generative adversarial
    networks (GAN), vision transformers (ViT) and other popular deep learning architectures.
    Additionally, this study pinpoints the challenges in implementing the solutions
    in the farmer's field in real-time. The overall finding indicates that convolutional
    neural networks are the corner stone of modern computer vision approaches and
    their various architectures provide high-quality solutions across various agriculture
    activities in terms of precision and accuracy. However, the success of the computer
    vision approach lies in building the model on a quality dataset and providing
    real-time solutions.
  doi: 10.1016/j.aiia.2022.09.007
  explanation: The relevance score between the paper and the outline point you are
    making is 0.9-1.0. The paper explicitly discusses the use of IoT-enabled sensors
    and computer vision for automatic irrigation management. The paper introduces
    the application of IoT and machine learning techniques to enable the integration
    of components within the automated irrigation management system to achieve efficient
    water usage. Specifically, the paper proposes a system for detecting the location
    of center pivot irrigation systems using a Convolutional Neural Networks (CNNs)
    approach, to identify and monitor the center of each center pivot system. The
    proposed system was found to provide accurate results in identifying the center
    of the irrigation system.
  extract_1: Zhang et al. (Zhang et al., 2018) performed identification and monitoring
    of canter pivot irrigation systems using a Convolutional Neural Networks (CNNs)
    approach to the allocation of irrigation water. The CNNs with various structures
    were built and compared and for data augmentation,training, a sampling strategy
    was developed. In the testing region, the CNN with the best performance and the
    shortest training time was used. To further pinpoint the centre of each centre
    pivot system, a variance-based technique was presented.
  extract_2: null
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    Outline Highlights Abstract BetaPowered by GenAIQuestions answered in this article
    Key words Abbreviations 1. Introduction 2. Computer vision and deep learning models
    3. Deep learning driven computer vision – Application areas in agriculture 4.
    Practical implications 5. Challenges and way forward 6. Conclusions Declaration
    of Competing Interest References Show full outline Cited by (65) Figures (11)
    Show 5 more figures Tables (7) Table 1 Table 2 Table 3 Table 4 Table 5 Table 6
    Show all tables Artificial Intelligence in Agriculture Volume 6, 2022, Pages 211-229
    Deep learning based computer vision approaches for smart agricultural applications
    Author links open overlay panel V.G. Dhanya a, A. Subeesh b, N.L. Kushwaha c,
    Dinesh Kumar Vishwakarma d, T. Nagesh Kumar e, G. Ritika c, A.N. Singh a Show
    more Add to Mendeley Share Cite https://doi.org/10.1016/j.aiia.2022.09.007 Get
    rights and content Under a Creative Commons license open access Highlights • Smart
    farming approaches rely on digital technologies for automation. • DL-based computer
    vision is a panacea for meeting the ever-growing food demand. • CNN and advanced
    computer vision models like GAN, Vision Transformers were explored. • Potential
    applications and challenges of DL- computer vision were also discussed. Abstract
    The agriculture industry is undergoing a rapid digital transformation and is growing
    powerful by the pillars of cutting-edge approaches like artificial intelligence
    and allied technologies. At the core of artificial intelligence, deep learning-based
    computer vision enables various agriculture activities to be performed automatically
    with utmost precision enabling smart agriculture into reality. Computer vision
    techniques, in conjunction with high-quality image acquisition using remote cameras,
    enable non-contact and efficient technology-driven solutions in agriculture. This
    review contributes to providing state-of-the-art computer vision technologies
    based on deep learning that can assist farmers in operations starting from land
    preparation to harvesting. Recent works in the area of computer vision were analyzed
    in this paper and categorized into (a) seed quality analysis, (b) soil analysis,
    (c) irrigation water management, (d) plant health analysis, (e) weed management
    (f) livestock management and (g) yield estimation. The paper also discusses recent
    trends in computer vision such as generative adversarial networks (GAN), vision
    transformers (ViT) and other popular deep learning architectures. Additionally,
    this study pinpoints the challenges in implementing the solutions in the farmer’s
    field in real-time. The overall finding indicates that convolutional neural networks
    are the corner stone of modern computer vision approaches and their various architectures
    provide high-quality solutions across various agriculture activities in terms
    of precision and accuracy. However, the success of the computer vision approach
    lies in building the model on a quality dataset and providing real-time solutions.
    Previous article in issue Next article in issue Questions answered in this article
    BetaPowered by GenAI This is generative AI content and the quality may vary. Learn
    more. What is computer vision and how is it used in agriculture? What is the role
    of deep learning in computer vision for agriculture? What are some applications
    of computer vision technology in the automatic identification of plant species?
    What are the challenges in fruit yield estimation systems related to computer
    vision? How can cloud-based solutions address the challenges of deep learning
    in computer vision? Key words Agriculture automationComputer visionDeep learningMachine
    learningSmart agricultureVision transformers Abbreviations AIArtificial IntelligenceANNArtificial
    Neural NetworkBPBack PropagationC-GANConditional Generative Adversarial NetworkCNNConvolutional
    Neural NetworkCOCOCommon Objects in ContextCVComputer VisionDCNNDeep Convolutional
    Neural NetworkDLDeep LearningDNADeoxyribo Nucleic AcidRCNNRegion-based Convolutional
    NetworksFCNFully Convolutional NetworksFLDAFisher''s Linear Discrimination AnalysisGANGenerative
    Adversarial NetworkGLCMGrey Level Co-occurrence MatrixGPUGraphic Processing UnitsHOGHistogram
    of Oriented GradientsKNNK- Nearest NeighbourLBPLocal Binary PatternsLCTFLiquid
    Crystal Tunable FiltersLDALinear Discriminant AnalysisLIDARLight Detection and
    RangingLSTMLong Short-Term MemoryMHAMulti Headed AttentionMLMachine LearningMLPMulti-Layer
    PerceptronNASNetNeural Search Architecture NetworkNLPNatural Language ProcessingOCROptical
    Character RecognitionPEATProgressive Environmental and Agricultural TechnologiesPLFPrecision
    Livestock FarmingResNetResidual NetworkRFRandom ForestRGBRed Green BlueSegNETSemantic
    Segmentation NetworkSSDSingle Shot Multibox DetectorSVMSupport Vector MachineUAVUnmanned
    Aerial VehicleVGGVisual Geometry GroupViTVision TransformersWSNWireless Sensor
    NetworkYOLOYou Only Look Once 1. Introduction The UNDP 2021 report on “leveraging
    digital technology for sustainable agriculture” states that global food production
    needs to be increased by 98 percent to feed a burgeoning human population of 9.9
    billion by 2050 (Burra et al., 2021). This target needs to be accomplished through
    the effective utilization of available resources viz land, labor, capital, and
    technology (Ranganathan et al., 2018). Present status on precision agriculture
    aims to define the decision support system for farm management by optimizing the
    output while consecutively preserving the resources applied. Constructively pointing
    out, the emerging trend of food security needs to be handled with data-driven
    farming that can increase productivity, efficiency, and profits. The key challenges
    such as food demand, labor shortage, water shortage, climate change (Badrzadeh
    et al., 2022; Elbeltagi et al., 2022a; Kaack et al., 2022) and increasing energy
    demands lead to the need for technology intervention. The opportunity offered
    by smart agriculture, which encompasses precision agriculture, digital agriculture
    as well as modern agricultural practices, needs prime validation at this point.
    Smart agriculture is primarily based on three platforms viz, science, innovation,
    and ICT (Information and Communication Technology) (Khanna and Kaur, 2019). The
    traditionally used information and knowledge management system for collecting
    and monitoring agricultural data is not only laborious but is also time-consuming
    and error-prone. Therefore, the technical advancement in remote sensing, digital
    applications, sensors, advanced imaging systems, cloud data storage along with
    intelligent data analysis using the decision support systems need to be well utilized
    in making the farming sector smarter (Fig. 1). Smart agriculture can leverage
    cutting-edge technologies like the Internet of Things, Machine learning, Cloud
    computing, Blockchain, etc., and benefit from these opportunities in improving
    food production and addressing the emerging challenges in this sector (Sami et
    al., 2022). Download : Download high-res image (312KB) Download : Download full-size
    image Fig. 1. Components of smart agricultural solutions. Recently, the infiltration
    of computer/ mobile technology even to the most rural pockets, has provided an
    inimitable facility in connecting the rural producers with the city-consumers
    or the international investors, thereby facilitating better investments and knowledge
    transfer in agriculture (Aker, 2011; Karim et al., 2013). Artificial intelligence
    (AI) is a game-changing technology that already has proven track records across
    various industries, including agriculture (Adnan et al., 2021; Bhagat et al.,
    2020; Jamei et al., 2022b; Kumar et al., 2019; Subeesh et al., 2019). The use
    of machine learning, a subset of artificial intelligence, has been covered extensively
    by researchers in delivering innovative solutions for modelling complex relationships
    and further, making predictions on agriculture data (Bhavsar and Panchal, 2012;
    Heramb et al., 2022; Jamei et al., 2022c; Karbasi et al., 2022; Malik et al.,
    2022a; Rai et al., 2022; Rehman et al., 2019; Tantalaki et al., 2019). Computer
    vision, a field of artificial intelligence, is making a machine “see”, using the
    modern technologies involving a camera and computer instead of human vision, empowering
    extensive automation capabilities to AI systems. Computer vision collects necessary
    visual data regarding crops, livestock, farm or garden, allowing us to identify,
    detect and track specific objects using visual elements and comprehend complex
    visual data for automation tasks. In the past decades, expert and intelligent
    systems based on computer vision technology have been well utilized for agricultural
    operations (Foglia and Reina, 2006; Gomes and Leta, 2012; Rico-Fernández et al.,
    2019). Further, the development of modern technologies and hardware supports like
    Graphic Processing Units (GPUs) and edge devices have diversified the application
    of computer vision, thereby making strands to efficient agricultural production
    (Li et al., 2019; Mochida et al., 2019; Rehman et al., 2019; Vázquez-Arellano
    et al., 2016). Modern computer vision techniques can help in the digital quantification
    of different morphological and physiological plant parameters along with the qualitative
    assessment of the same and are expected to rapidly improve the accuracy of plant
    phenotyping (Araus and Cairns, 2014; Ghanem et al., 2015). Further, combining
    the computer vision techniques with the high throughput molecular methodologies
    of DNA sequencing provides an opportunity for genome-wide exploration of useful
    genes and molecular modeling of the same to understand the complex traits such
    as plant yield and productivity, stress tolerance, biotic and abiotic stress management
    etc (Araus et al., 2018; Shakoor et al., 2017). Thus imaging with computer vision
    technology aided by various imaging sensors and algorithms can indeed play a major
    role in precision agriculture and in paving the way for smart agriculture (Araus
    et al., 2018). A data driven precision agriculture system architecture consists
    of sensors deployed on the fields (sensing layer), network layer that provides
    connectivity, storage and other services (service layer) and application layer
    consisting of the end user accessing the services through mobile/web-based applications
    (Fig. 2). Integrative and multi-mode Artificial Intelligence (AI) models can be
    deployed to predict crop behaviour under differing field conditions (Shrivastava
    and Marshall-Colon, 2018; Waldhoff et al., 2017). The yield performance of major
    crops in various regions, along with the field conditions for crop production,
    environmental impact and economic outcome, have been assessed using the algorithms
    of deep learning and machine learning (Tantalaki et al., 2019). Deep learning
    permits the computational models with multiple processing layers to indicate the
    data in multiple levels of abstraction (Schmidhuber, 2015). The main application
    of deep learning in the field of agriculture are building models to derive meaningful
    insights from agriculture data (Jamei et al., 2022a; Malik et al., 2022b), image
    analysis including classification and object detection, such as the detection
    of diseases, weed identification, soil analysis, plant disease detection, etc.
    (Kamilaris and Prenafeta-Boldú, 2018). Download : Download high-res image (904KB)
    Download : Download full-size image Fig. 2. Data-driven precision agriculture
    system architecture. For this study, we have collected more than 100 research
    papers from scientific databases, including PubMed, Web of Science, and Scopus,
    in the area of deep learning-based computer vision. Further, we investigated all
    these works that leveraged deep learning-based computer vision technologies to
    address key agriculture tasks such as plant health monitoring, disease and weed
    identification, irrigation management, soil analysis, livestock management, yield
    estimation, etc. The main objective of this study is to evaluate the penetration
    of deep learning-based computer vision approaches in key agricultural problems,
    and this review is intended to be useful to agriculture researchers as well as
    general computer vision researchers who are interested in the application of computer
    vision solutions to automate and solve potential agricultural problems. The practical
    implications of these technologies along with major challenges in implementing
    large-scale applications were also constructively pointed out in this study. 2.
    Computer vision and deep learning models Computer vision possesses dual and interrelated
    goals. In biological science, computer vision aims to represent the human visual
    system using computational models, and in the engineering perspective, computer
    vision attempts to create autonomous systems that can do tasks that often human
    visual systems cannot perform (Huang, 1993). Computer vision imparts visual capability
    to machines through cameras, data, models, and algorithms rather than retinas
    and the visual cortex. Optical character recognition (OCR) technology and intelligent
    character recognition were some major tasks that employed computer vision to accomplish
    tasks such as document and invoice processing, vehicle plate detection, etc. In
    the early stages of computer vision research, the main focus was to build algorithms
    to detect edges, curves, corners, and other basic shapes. Before the era of deep
    learning, image processing relied on gray level segmentation and this approach
    wasn’t robust enough to represent complex classes. Modern computer vision algorithms
    rely extensively on artificial neural networks that provide a dramatic improvement
    in performance and accuracy compared to traditional approaches for image processing.
    Deep learning-based computation models allow multiple processing layers to learn
    and infer complex patterns mimicking the human brain (O’Mahony et al., 2020; Schmidhuber,
    2015; Zhong et al., 2016). It runs and inspects the data over several iterations
    until it discerns distinctions and identifies or recognizes the features in the
    images. The recent surge of interest in deep learning is due to the fact that
    it can handle massive amounts of heterogeneous data (visual, audio, text, etc.)
    and is capable of embedding solutions into several hardwares. DL allows automatic
    feature extraction and can be utilized in numerous image processing tasks and
    is well known for its effectiveness in handling vision-based activities like image
    classification, object detection, semantic segmentation, etc. In fact, these tasks
    are the backbone for modeling and automating agricultural activities such as disease
    identification, weed detection, yield estimation, etc (Jha et al., 2019; Subeesh
    and Mehta, 2021; Tian et al., 2020). 2.1. Image classification with CNN and Object
    detection models Convolutional neural network-based deep learning architectures
    are popular for computer vision tasks like image classification. A convolutional
    neural network is a type of neural network architecture that takes input images
    and extracts relevant features to efficiently identify and classify images. CNN
    uses labels to perform convolutions and generate feature maps. The introduction
    of imageNet dataset that contained millions of tagged images had laid a foundation
    and benchmark for building advanced computer vision-based models (Kriegeskorte
    and Golan, 2019; Miikkulainen et al., 2019; Yoo, 2015). LeNet-5 was one of the
    earliest CNN proposed by Yann LeCun (LeCun et al., 1998), led to the development
    of various CNN models (Fig. 3). In 2012, AlexNet architecture (Krizhevsky et al.,
    2012a) was found promising for image recognition, and numerous new architectures
    such as VGGNet (Simonyan and Zisserman, 2015), ResNet (He et al., 2015), etc.
    were also introduced by researchers, reducing the error rate and improving the
    performance. Image segmentation approaches are quite useful for understanding
    what an image consists of, by dividing the images into several segments. Image
    segmentation creates a pixel-oriented mask for each object present inside the
    image. This eases the image processing tasks as the important segments alone can
    be considered for processing tasks. Download : Download high-res image (476KB)
    Download : Download full-size image Fig. 3. Architecture of (a) Convolutional
    neural networks (b) LeNet-5 architecture. The image classification mainly identifies
    the class, a specific image belongs to. The image classification approach is often
    not successful when there are multiple objects in the same image. Object detection
    aims to detect the location of objects in the image/video. Object detection task
    comprises two major components; class information and location information. The
    location information is described by bounding boxes around the target object.
    Object detection architectures such as YOLO (You Only Look Once) (Redmon et al.,
    2016), SSD (Single Shot Multibox Detector) (Liu et al., 2016), Faster-RCNN (Region
    Convolutional Networks) (Ren et al., 2016) are widely used for object detection
    and automation across different domains including agriculture. 2.2. Generative
    adversarial network (GAN) and Vision Transformers (ViT) A generative adversarial
    network (GAN) is a special type of neural network used for unsupervised learning.
    GAN is an approach to generative modeling that can learn to mimic a given distribution
    of data. These models effectively reduce the data into its fundamental properties
    or generate new data points with varied properties. The application of GANs has
    achieved state-of-the-art performance in many image generation tasks, such as
    text-to-image synthesis (Xu et al., 2017), super-resolution (Ledig et al., 2017),
    and image-to-image translation (Zhu et al., 2020b). Generally, GAN has two main
    building blocks (two neural nets) which compete with each other and are capable
    of capturing, copying and analyzing the variations in a dataset (Fig. 4). The
    two networks are usually called Generator and Discriminator. The generator neural
    network helps to generate new instances, while the discriminator neural network
    evaluates the authenticity of the generated images. The discriminator decides
    whether or not every instance of the data it evaluates belongs to the actual training
    set and penalizes the generator for generating implausible outcomes. The loss
    of the discriminator is used for improving the generator (Reimers and Requena-Mesa,
    2020). The discriminator tries to identify the fake data from the real data, and
    both networks work simultaneously to learn complex data. GANs are a panacea for
    the data scarcity problem, which is a serious hurdle in developing robust deep
    neural network models (Hiriyannaiah et al., 2020). The realistic images produced
    by GAN that are different from the original training data are attractive in data
    augmentation of DL-computer vision to reduce the model overfitting. Download :
    Download high-res image (443KB) Download : Download full-size image Fig. 4. Overview
    of training process in GAN (Generative Adversarial Network). Transformer models
    have become the de-facto status quo in text processing, and recently, the computer
    vision community has extended the concept of NLP (Natural Language Processing)
    transformer to apply to the image domain with slight modification in the implementation
    to process multiple modalities (e.g., images, videos, etc.) using similar processing
    blocks (Dosovitskiy et al., 2021; Khan et al., 2021; Vaswani et al., 2017). Even
    though the general architecture used in both cases are similar, ViT uses different
    approaches for tokenization and embedding (Fig. 5). The overall architecture consists
    of 3 main components, viz., patch embedding, feature extraction by stacked transformer
    encoders and the classification head. In ViT, initially, the input image of shape
    (height, width, channels) is embedded into a feature vector of shape (n+1, d),
    using a set of transformations. The input image is split into a group of image
    patches. Later, these groups of image patches are embedded into encoded vectors
    and fed into transformer encoder network. The transformer encoder learns the features
    from the embedded patches using a stack of transformer encoders (Wu et al., 2021).
    The encoder mainly comprises multi-headed attention (MHA) and a 2-layer MLP with
    layer normalization and residual connections. The final MLP block, called the
    MLP head is used as an output of the transformer. In the case of image classification,
    a softmax on the output generates the classification outputs. ViTs are useful
    in several vision applications such as image classification, image-to-text, text-to-image
    generation, image segmentation, object detection, etc (Bazi et al., 2021; Li et
    al., 2022). Download : Download high-res image (520KB) Download : Download full-size
    image Fig. 5. The architecture of Vision Transformer Model for image classification
    (Dosovitskiy et al., 2021; Vaswani et al., 2017). 3. Deep learning driven computer
    vision – Application areas in agriculture 3.1. Seed quality analysis The commercial
    seed industry is focused on the supply of the right quality seeds to the farmers
    at the right time in the right quantity. Filtering out low-quality of seeds from
    high-quality ones, is not only laborious, but it requires sophisticated equipments,
    infrastructure, and time (Kannur et al., 2011). The testing of seeds for their
    quality can indeed gain momentum by the use of computer vision technology which
    can extract the morphological information of different seed lots and grade it
    according to the internationally prescribed quality standards (Bao and Bambil,
    2021). The different seed testing modules are likely to address their physical
    purity, genetic purity, seed health, vigour, patterns of deterioration etc., which
    in general may indeed cover the physical or visually attributable characters such
    as the seed length, shape, size, visual impairments, and presence of foreign bodies
    which can indeed be captured by the advanced computer vision technology (Granitto
    et al., 2005). Performance issues of traditional computer vision have greatly
    been improved by deep learning-based computer vision, resulting in larger adoption
    for seed variety identification. The seed quality evaluation process using computer
    vision is shown in Fig. 6. Often, spectral imaging techniques are also merged
    with these approaches to enhance the accuracy (Qiu et al., 2018; Zhu et al., 2019).
    In a study conducted by Zhu et al. (2019), combining spectroscopy and machine
    learning – CNN models were found to be effective in identifying the seed varieties.
    The machine learning models showed an accuracy of more than 80% in classifying
    the cotton seeds based on the feature extracted by the CNN and ResNet models.
    In another investigation, SeedSortNet built from computer vision CNN models, was
    found to be promising, with accuracies 97.33% and 99.56% in sorting the maize
    and sunflower seeds (Li et al., 2021). CNN deep learning was also utilized for
    cognizing the viable and non-viable seeds and was found to be successful with
    90% viability prediction accuracy for naturally aged seeds (Ma et al., 2020).
    Taheri-Garavand et al. (2021) developed models for automatic identification of
    chickpea varieties using seed images in the visible spectrum. A modified VGG16
    model was used for the identification purpose. As sorting high-quality seeds are
    vital for increasing yield in the breeding industry, Zhao et al. (2021) employed
    seven different computer vision models to accurately detect and identify surface
    defects. MobileNet-V2 model had shown excellent detection accuracy for the soybean
    dataset. There are numerous such studies done by various researchers and the seed
    industry is hugely getting benefited from advanced computer vision models, achieving
    a higher level of automation capabilities. Some of the studies in this area are
    precisely summarized Table 1. Download : Download high-res image (443KB) Download
    : Download full-size image Fig. 6. Seed quality analysis using data-driven models.
    Table 1. Previous studies on seed quality analysis through application of computer
    vision and deep learning. Reference Objectives and scenario of application Methodology
    Crop Results (Javanmardi et al., 2021) Corn variety classification using 9 different
    varieties CNN as a generic feature extractor. Classification using ANN, SVM, kNN,
    boosted tree, bagged tree and LDA Corn CNN ANN classification has a classification
    accuracy of 98.1%, precision 98.2%, recall 98.1% and F1 score of 98.1%. (Qiu et
    al., 2018) Variety identification in rice KNN, SVM and CNN models Rice CNN outperformed
    other models with 89.6% accuracy on the training set and 87% accuracy on the testing
    set. (Gulzar et al., 2020) Seed classification using 14 types of seeds VGG16 architecture
    for classification - 99.9% accuracy over test set with 234 images (Wu et al.,
    2019) Variety identification in oats DCNN model Oats 99.19% accuracy on testing
    set. (Gulzar et al., 2020) Seed classification in maize and sunflower CNN model
    Maize and Sunflower CNN based visual model - SeedSortNet developed with 97.33
    percent accuracy on maize and 99.56% accuracy on sunflower dataset respectively.
    (Liu et al., 2015) Soyabean seed sorting BP neural network Soyabean 97.25% average
    recognition accuracy over 857 images of soybean seeds with pest and insect damage.
    (Veeramani et al., 2018) Corn seed defect detection VGG 19 and GoogleNet Maize
    - (Dolata and Reiner, 2018) Varietal identification in barley CNN Barley Increase
    in average classification accuracy by 0.6% and sensitivity by 2.3% with respect
    to view point ignorant architecture of the said study. (Kurtulmuş, 2021) Seed
    classification in sunflower AlexNet, GoogleNet and ResNet Sunflower 95% accuracy
    with GoogleNet algorithm for classification of 4800 sunflower seeds. (Ni et al.,
    2019) Seed grading in maize DCNN Maize 98.2% prediction accuracy for 408 test
    images in maize. 3.2. Soil analysis The preservation and improvement of dynamic
    soil characteristics is the main emphasis of soil management in agriculture for
    increasing crop productivity (Kushwaha et al., 2022; Suchithra and Pai, 2020).
    Traditional soil texture analysis entails taking soil samples and bringing them
    to a laboratory, where they are dried, crushed, and sieved before being used.
    For coarse textured or sandy soils, sieving is the most typical laboratory analytical
    method, while for smaller textured particles, a hydrometer or pipette approach
    based on sedimentation theory is used (Kushwaha et al., 2022; Sudarsan et al.,
    2016). With the advancement of image processing power and the development of image
    acquisition (e.g., cameras) systems in recent years, computer vision-based image
    analysis approaches have gotten a lot of interest in a lot of sectors, including
    soil science. This method collects soil images (dynamic or static) with cameras
    and then uses simple computer programmes to classify and categorise them (Fig.
    7). For example, after matching textural patterns, the size of the soil particles
    might be estimated straight from the image. In several investigations, various
    image analysis-based computer vision approaches were tried. Haralick et al. (1973)
    attempted to classify images received from an aerial or satellite source using
    entropy and angular moment-based textural classification. Since then, the grey
    level co-occurrence matrix (GLCM) and its analogues have been used in a variety
    of remote sensing applications (Dell’Acqua and Gamba, 2003; Kuplich et al., 2005).
    However, the greatest resolution satellite can only provide a maximum resolution
    of 10 m/square pixel, which is insufficient to understand soil particle sizes.
    Riese and Keller (2019a) implemented three 1-dimensional (1D) convolutional neural
    networks: the LucasCNN, the LucasResNet and the LucasCoordConv. In addition, for
    the classification problem at hand, the study tweaks two existing 1D CNN techniques
    and compares the CNN techniques against a random forest classifier to see how
    well they do. Thereby, study uses the LUCAS topsoil dataset, which is freely available.
    The CNN method with the least amount of depth turns out to be the most effective
    classifier. In terms of average accuracy, the LucasCoordConv has the best results.
    Download : Download high-res image (368KB) Download : Download full-size image
    Fig. 7. Soil texture analysis using image processing. Similarly, Zhang et al.
    (Zhang et al., 2003) proposed a soil texture classification system that uses the
    wavelet transform approach to distinguish between different types of soil. Wavelet
    transform, which is a strong image and signal analysis method due to its multi-resolution
    capabilities, is used to extract features. A set of training instances is used
    to create a maximum likelihood (ML) classifier. This method of ML parameter estimation
    produces the best results. At the time of training and classification, the Fisher''s
    Linear Discrimination Analysis (FLDA) is used to optimize and reduce the dimension
    of the vector. Soil textures such as clay, sand, and silt are employed for training
    and classification. Clay, sand, and silt have 60 percent, 100 percent, and 100
    percent categorization rates, respectively. In instance segmentation, Zhang et
    al. (Zhang et al., 2020) suggested a mask refined R-CNN for refining object details.
    The goal is to figure out how semantic segmentation of high-level and low-level
    features affects instance segmentation. The COCO (common objects in context) and
    cityscapes datasets were used to collect the trial results. This approach is reported
    to be simple to use and effective. Some of the previous significant studies in
    soil analysis using DL computer vision have been summarized in Table 2. Table
    2. Previous studies on computer vision and deep learning technologies for soil
    properties analysis and management. Reference Objectives and scenario of application
    Methodology Results (Riese and Keller, 2019b) Soil texture analysis The CNN architectures
    LucasCNN, the LucasResNet and theLucasCoordConv Models The CNN method with the
    least amount of depth turns out to be the most effective classifier (Omondiagbe
    et al., 2022) Soil texture prediction Employed automated deep convolutional neural
    networks and population-based learning by replacing the random search with a Bayesian
    Optimization. Results show improvements of 5% to 26% for all three soil properties
    such as sand, silt and clay. (Pyo et al., 2020) Estimation of heavy metal concentration
    From the soil reflectance images, CNN with convolutional autoencoders was trained
    to estimate As, Cu and Pb metals. The highest accuracies reported for As, Cu,
    and Pb estimates were with R2 values of 0.86, 0.74, and 0.82. (Zhong et al., 2021)
    Soil properties The DCNN architectures LucasResNet-16 and LucasVGGNet-16 models
    When compared to a single-task DCNN model, the performance of a multi-task DCNN
    model created based on LucasResNet-16 was enhanced. (Yu et al., 2019) Soil Classification
    Lquid crystal tunable filters (LCTF)-based system and three-dimensional convolutional
    neural network (3D-CNN) for soil classification The overall accuracy of 99.59%
    for 3D-CNN-SD-PCA. (Azadnia et al., 2022) Texture Analysis Portable smartphone-based
    machine vision system using CNN was developed. The features were extracted using
    CNN and classification is performed using ANN, SVM, RF and KNN classifiers. Model
    accuracies at distances of 20, 40 and 60 cm were of 99.89, 99.81 and 99.58%, (Azadnia
    et al., 2022) Texture analysis Deep learning models VggNet16, ResNet50, and Inception-v4
    models were used to classify soil aggregates Overall accuracy obtained for CNN
    networks was 96.2%, 97.1%, and 98.7% 3.3. Irrigation management Irrigation water
    management in agricultural production necessitates considerable effort and is
    crucial in maintaining hydrological, climatological, and agronomic equilibrium.
    Several studies have thus been undertaken in gaining knowledge of the biophysical
    processes included in the uptake of water through the root zone of the soil and
    the processes of transpiration through the plant canopy (Elbeltagi et al., 2022b;
    Kushwaha et al., 2021). For an effective irrigation schedule, it is necessary
    to know the precise amount of water required by the crop (Kushwaha et al., 2016;
    Vishwakarma et al., 2022). The application of computer vision technologies, as
    well as the integration and deployment of automated crop production management,
    plant irrigation, and yield evaluation, thus become critical. Zhang et al. (Zhang
    et al., 2018) performed identification and monitoring of centre pivot irrigation
    systems using a Convolutional Neural Networks (CNNs) approach to the allocation
    of irrigation water. The CNNs with various structures were built and compared
    and for data augmentation,training, a sampling strategy was developed. In the
    testing region, the CNN with the best performance and the shortest training time
    was used. To further pinpoint the centre of each centre pivot system, a variance-based
    technique was presented. The proposed approach performed well in the centre pivot
    irrigation systems identification challenge, with a precision of 95.85% and a
    recall of 93.33% of the identification findings. Similarly, Chang and Lin (Chang
    and Lin, 2018) developed a compact intelligent agricultural machine which is capable
    of autonomous weeding and variable watering on the cultivated ground, using a
    combination of computer vision and multitasking. The system classifies the plants
    and weeds in real-time so that it can weed and water while maintaining an average
    herbicidal rate of 90% and a deep soil moisture level of 80%. This strategy has
    a lot of potential because it allows for not only multitasking integration but
    also resource utilization in its entirety. Kamyshova et al. (Kamyshova et al.,
    2022) proposed a computer vision-based technology for optimizing the watering
    process of crops utilizing a phyto indication system in low latency mode, the
    study suggested an algorithm-based system for obtaining a maize irrigation map.
    The system, which comprises 8 IP cameras coupled to a DVR connected to a laptop,
    can be mounted on a centre pivot irrigation system. There are three steps to the
    algorithm. Using an integrated excess green and excess red difference (ExGR) index
    during the image preprocessing stage. The application of the approach that the
    study chose based on the system''s operational conditions is the categorization
    stage. A neural network trained using the Resilient Propagation method is utilised
    in the final stage to calculate the rate of watering of plants in the current
    sector of the sprinkler site. Plant identification accuracy was up to 93 percent,
    and growth stages were up to 92 percent. Low-cost cameras are now being used in
    all sectors of technology, particularly in agricultural applications. The soil
    water balance may be precisely assessed to enable accurate irrigation planning
    by acquiring relevant information on the growth of horticulture crops through
    photographs (Koech and Langat, 2018). Table 3 shows the irrigation water management
    through the application of computer vision and deep learning technologies. Table
    3. Previous studies on irrigation water management through application of computer
    vision and deep learning approaches. References Objectives and scenario of application
    Methodology Results (Albuquerque et al., 2020) Identification of malfunctioning
    in the irrigation systems Mask R-CNN based segmentation on UAV captured images
    Given dataset sizes, the results are satisfactory. (Chen et al., 2020a) Identification
    of water pollution for agricultural irrigation resources Shallow CNN model in
    combination with decision tree algorithm trained on NIR data Validation results
    were 25.47 of RMSEV and 0.914 of Rv. (Zhang et al., 2018) Monitoring and identification
    of canter pivot irrigation system to supply irrigation water CNN based segmentation
    on UAV captured images Precision and recall of 95.85% and 93.3 percent, respectively,
    were attained. (Tang et al., 2021) Monitoring the distribution of center pivot
    irrigation systems Lightweight real-time object detection network (PVANET) based
    on GoogLeNet and Hough transform Experiments with Sentinel-2 images achieved a
    precision of 95% and a recall of 95.5%, (Kumbi and Birje, 2022) Irrigation efficiency
    Sun-flower Atom Optimization-based Deep convolution neural network (SFAO-DeepCNN)
    algorithm Maximal accuracy of 92%, specificity of 91.2% and sensitivity of 94.1%
    (Kim et al., 2022) Water Level Estimation of Irrigation Channel ResNet-50 image
    classification and U-Net segmentation models on irrigation canal''s CCTV images
    The image segmentation model showed a Dice score of 0.998 and predicted water
    levels showed R2 of 0.97 3.4. Plant health analysis With the advancement in computer
    vision and deep learning, new promising solutions for identifying overall health
    status of the plants were introduced. The intelligent decision support system
    for identifying crop diseases (Fig. 8), water stress, and nutrient deficiencies
    would lead to timely control of the panic situations and eradicating the huge
    losses, ultimately leading to improved plant quality. Download : Download high-res
    image (563KB) Download : Download full-size image Fig. 8. Deep learning based
    computer vision approach for plant health analysis. Plant stress induced by biotic
    and abiotic factors is expressed in the plant canopy as multiple symptoms. In
    case of water stress, the plant closes stomata and delays photosynthesis and transpiration
    activities indicating colour changes in the leaf and temperature (Nilsson, 1995).
    Similarly, nutrient deficiencies-related symptoms are typically visible in leaves
    color and texture (Xu et al., 2011). Image analysis can detect these changes in
    a pattern quite effectively. Deep learning-based computer vision approaches are
    viable solutions in addressing timely disease identification and avoiding consultation
    of human experts. The availability of a large number of public image datasets
    such as PlantVillage (Hughes and Salathe, 2016), PlantDoc (Singh et al., 2020)
    have proliferated the research in the area of disease identification and many
    works have taken encouraging steps towards disease-free agriculture (Hassan and
    Maji, 2022; Ji and Wu, 2022; Nagasubramanian et al., 2019). The PlantVillage dataset
    has been extensively utilized by various researchers for solving disease identification
    problems using deep learning (Amara et al., 2017; Brahimi et al., 2017; Ferentinos,
    2018; Mohanty et al., 2016). Several studies reveal that pre-trained models quickly
    and accurately identifying the diseases in terms of precision, recall and F1 scores
    (Abbas et al., 2021; Chen et al., 2020b; Coulibaly et al., 2019; Mukti and Biswas,
    2019; Thakur et al., 2021). Abbas et al. (Abbas et al., 2021) used synthetic images
    generated using the Conditional Generative Adversarial Network (C-GAN) to build
    tomato leaf disease detection. C-GAN can address the issue of data insufficiency
    and provide more generalization to the models (Mirza and Osindero, 2014). It is
    worth noting that some investigations were focused on the localization of the
    disease spots, giving precise information about the diseases (Cen et al., 2016;
    Liu and Wang, 2020; Mathew and Mahesh, 2022; Son, 2021). Several other studies
    reported research on DL-computer vision based identification of crop stresses,
    including water stress and nutrient deficiencies (Abdalla et al., 2021; Anami
    et al., 2020; Jahagirdar and Budihal, 2021). Table 4 shows the previous studies
    on deep learning based computer vision technology on plant health analysis. Table
    4. Previous studies on computer vision and deep learning technologies for crop
    health analysis. References Objectives and scenario of application Methodology
    Crop Results (Hassan and Maji, 2022) Plant disease identification Novel lightweight
    CNN based on Inception and Residual connections with fewer parameters Rice, Cassava
    The testing accuracies of the proposed model is 99.39%,99.66% and 76.59% on Plantvillage,
    Rice, and Cassava dataset (Hati and Singh, 2021) Species Recognition (SR) and
    Identification of Healthy and Infected Leaves (IHIL) Residual network (ResNet)
    based convolutional neural network (CNN) architecture 12 different plant species
    Species identification: Precision 91.84%, Recall 91.67% and F191.49%. IHIL : Precision
    84%, Recall 83.14% and F1 83.19% (Ji and Wu, 2022) Black measles disease identification
    in grape Plant disease evaluation. Image segmentation using DeepLabV3 with ResNet50
    backbone Grape Overall classification accuracy of 97.75% on the hold-out test
    dataset. (Syed-Ab-Rahman et al., 2022) Citrus diseases classification using leaf
    images Two-stage deep CNN model Citrus Detection accuracy of 94.37% and an average
    precision of 95.8%. (Li and Li, 2022) Leaf disease identification Vision Transformer-based
    lightweight apple leaf disease- identification model (ConvViT) Apple ConvViT achieved
    an accuracy of 96.85% on the apple leaf disease dataset (Mkonyi et al., 2020)
    Early identification of Tuta absoluta disease Pre-trained CNN architectures VGG16,
    VGG19 and ResNet50 Models Tomato VGG16 attained the highest accuracy of 91.9%
    (Azimi et al., 2021) Stress level detection due to nitrogen deficiency Custom
    Deep learning architecture with 23 layers. Sorghum 8.25% better accuracy than
    traditional machine learning techniques (Joshi et al., 2021) Viral disease diagnosis
    Convolutional neural network - VirLeafNet Vigna mungo Accuracies of VirLeafNet-1,
    VirLeafNet-2, and VirLeafNet-3 were 91.234%, 96.429%, and 97.403% (Shah et al.,
    2021) Plant disease detection ResTS Architecture with residual connection 14 crops
    F1-Score: 0.991 (Singh et al., 2021) Pest and disease detection 2D-CNN model with
    segmented images Coconut tree Accuracy of 96.94% with a Kappa value 0.91 3.5.
    Weed management Weeds are among the major factors that affect agricultural production
    negatively. With the focus on improving agricultural productivity, it is evident
    that more and more chemicals are being dumped into the environment with the aim
    of managing the weed growth. But for improving the productivity, it also requires
    the optimum utilization of resources which can only be achieved by the precise
    spraying on weeds. The traditional robotic weeders generally function by detecting
    crop row patterns and they do not rely on crop recognition for the weeding operation.
    If the weed density and population are large, they may obscure the row pattern
    leading to reduced efficiency of the weeders. Computer vision approaches come
    to rescue at this point by accurately identifying the objects as precise spraying
    of weeds depends on the accurate identification and location of weeds. Recently,
    several studies were carried out by researchers on adaptability of computer vision
    technology for the agronomic classification of plant species at the field level,
    viz the classification of crops from weeds, off types etc. (Sau and Ucchesu, 2019;
    Sau et al., 2018; Subeesh et al., 2022). Detailed application of the same in the
    automatic identification of plant species based on the leaf recognition pattern
    has been proposed for preserving and cataloguing plant species (Putzu et al.,
    2016) along with the botanical characterization of germplasm (Lo Bianco et al.,
    2017). Methods of achieving weed detection at the field level mainly include the
    utilization of computer vision technology using the traditional image processing
    and deep learning. When, the conventional methods of computer vision are used,
    extracting the different features such as colour, shape, texture etc., and combining
    them with the machine learning methods such as the SVM becomes necessary. But
    with the improvement in computing power, the deep learning algorithms can beneficially
    extract multidimensional and multi-scale spatial and semantic feature information
    of weeds through AlexNet, VGGNet, ResNet, etc due to their enhanced capability
    for image data expression thereby avoiding the disadvantages of traditional methods
    of feature extraction. The application of deep learning in agronomic classification
    of plant species has gained momentum after the outbreak of CNN and AlexNet (Krizhevsky
    et al., 2012b). Hall et al. (2015) have utilized the CNN architecture in classifying
    leaves of 32 species of crops and weeds by capturing nearly 1900 images of the
    same. Utilization of CNN architecture in the classification and differentiation
    of weeds from different species of wheat, sugarbeet, corn, soybean, sunflower,
    etc. has been proposed by Kussul et al. (2017), while the modified version of
    VGG16 for the classification of barley, grass, oil crops and weeds have been proposed
    by Mortensen et al. (2016). Table 5 shows the previously applied computer vision
    technology for weed management. Table 5. Previous studies on computer vision and
    deep learning technologies for weed management. References Objectives and scenario
    of application Methodology Crop Results (Le et al., 2020) Weed identification
    in Canola, corn and raddish Filtered Local Binary Pattern withContour Mask and
    Coefficient k (k-FLBPCM), VGG-16, VGG-19,ResNet-50, Inception-v3 Canola, corn,
    radish K-FLBPCM method outperformed other state of the art CNN models. (Osorio
    et al., 2020) Weed detection in lettuce Compared Mask R-CNN with HOG SVM and YOLO
    V3 Lettuce 98% accuracy for Mask R-CNN (Chavan and Nandedkar, 2018) Weed identification
    in paddy field Comapred SegNET with FCN and U-Net Rice 92.7% accuracy for SegNet
    (Chavan and Nandedkar, 2018) Weed classification at field level Comapred Hybrid
    network with VGGNet and AlexNet Maize,wheat, sugarbeet 98.23% accuracy for Hybrid
    network (Fawakherji et al., 2020) Crop/weed segmentation using synthetic images
    Synthetic image generation using GAN and segmentation models (UNET, BONNET,SEGNET,
    UNET-RESNET) Sugar beet All models were performed well with synthetic images generated
    using GAN and IoU increased drastically using synthetic dataset. (Wang et al.,
    2020) Weed detection in sugarbeet and oilseeds FCN architecture employed Sugarbeet
    and oilseeds. Best MIoU value (pixel-wise segmentation) 88.91%and object-wise
    segmentation 96.12% (Espejo-Garcia et al., 2020) Detection of balck night shade
    and velvet leaf in tomato and cotton fileds Compared Modified Xception, with Inception
    - ResNet, VGG-Net, MobileNet and DenseNet Tomato and cotton Combination of fine
    tuned Densenet and SVM.micro F1 score of 99.29%.F1 score ≥ 95% over repeated tests.
    (Huang et al., 2020) Weed in rice field FCN Rice Highest accuracy- VGG Net based
    FCN (Veeranampalayam Sivakumar et al., 2020) Weed in soybean filed Compared Single-Shot
    Detector (SSD), Faster R-CNN Soybean Faster RCNN as the best model for weed detection
    performance and inference time 3.6. Livestock management Computer vision approaches
    are leveraged extensively in precision livestock farming (PLF), ensuring optimum
    output and health of each individual animal. Livestock monitoring systems provide
    real-time information and assist farmers in making strategic decisions (Fig. 9).
    The non-invasive computer vision technology has been widely researched for its
    use in recognition of livestock behaviour over the past few years (Bello et al.,
    2021; Kumar et al., 2017; Qiao et al., 2019a; Shen et al., 2020). Xiao et al.
    (2022) employed a modified Mask-RCNN model and trained a fusion of Mask-RCNN and
    SVM to identify cows in unconstrained barn. Hansen et al. (2018) trained a CNN
    to recognize pigs via the face using a data set with 1,553 images. The VGG-face
    model used in this study achieved an accuracy of 96.7%. Some of the investigations
    relied on data collection using unmanned aerial vehicles to accurately detect
    and count the cattle (Andrew et al., 2019; Chamoso et al., 2014; Rahnemoonfar
    et al., 2019; Rivas et al., 2018). Such detection and counting approach problems,
    in general, have adopted either CNN-based probability heat map generation on the
    location of the animals or generation of bounding boxes for detection of the animals.
    An improved Yolo model called ‘FLYOLOv3’ (FilterLayer YOLOv3) based on Filter
    layer was introduced by Jiang et al. (2019) to ensure accurate detection of key
    parts of dairy cows. The performance of this approach was superior to the Faster-RCNN
    and Yolov3 algorithms. Download : Download high-res image (382KB) Download : Download
    full-size image Fig. 9. Major applications of DL-Computer vision for livestock
    management Daily activity patterns, food intake, and ruminating are some key indicators
    closely bound to the health and productivity of dairy cows (Huzzey et al., 2007;
    Weary et al., 2009). Some recent studies underline that traditional methods of
    direct observation and time-lapse video recording are slowly getting replaced
    by computer vision approaches. Yang et al. (2018) used a Faster-RCNN model to
    identify individual pigs from a group and subsequently assess the feeding area
    occupation rate to identify their feeding behaviour. To improve the accuracy of
    feeding behavior analysis, identify and exclude the non-nutritive visits (NNV)
    to the feeding area, Alameer et al. (2020) developed a GoogLeNet-based approach.
    The detection of feeding behaviour was highly accurate with 99.4% accuracy. CNN
    architectures are also found to be promising for early cattle disease detection
    in the animal husbandry farm (Rony et al., 2021). Table 6 shows the previous studies
    on computer vision technology for livestock management. Table 6. Previous studies
    on computer vision and deep learning technologies for livestock management. References
    Objectives and scenario of application Methodology Livestock Results (Qiao et
    al., 2019b) Cattle Segmentation and Contour extraction Mask R-CNN based cattle
    instance segmentation and contour line extraction Cattle Cattle segmentation performance
    with 0.92 Mean Pixel Accuracy (MPA) (Achour et al., 2020) Identification and feeding
    behavior monitoring CNN coupled to Support Vector Machine (SVM) Cow Accuracy 97%
    for individual identification of cows using multi-CNN. (Xu et al., 2020) Livestock
    classification and counting Mask RCNN based segmentation on UAV captured images
    Cattle and Sheep Classification Accuracy: 96% and Counting accuracy: 92% (Jung
    et al., 2021) Cattle Vocal Classification and Livestock Monitoring Convolutional
    neural network (CNN) based cattle vocal classification Cattle Accuracy of 81.96%
    after the sound filtering. (Qiao et al., 2022) Behaviour classification C3D-ConvLSTM
    based cow behaviour classification using video data Cow Classification accuracy
    of 90.32% and 86.67% in calf and cow datasets of 30-frame video length (Abu Jwade
    et al., 2019) Breed Classification VGG16 model for breed classification Sheep
    Maximum classification accuracy of 95.8% with 1.7 standard deviation. (Shojaeipour
    et al., 2021) Automated Muzzle Detection and Biometric Identification Two-stage
    YOLOv3-ResNet50 algorithm Cattle Muzzle detection accuracy was 99.13% and biometric
    identification of 99.11% testing accuracy (Brand et al., 2021) Pregnancy status
    prediction from mid-infrared spectroscopy Genetic algorithm and DenseNet model
    Cow DenseNet was superior over GA with prediction sensitivity 0.89, specificity
    of 0.86, and prediction accuracy of 0.88%. (Ayadi et al., 2020) Rumination behavior
    identification Convolutional Neural Networks Cow Average accuracy, recall and
    precision were 95%, 98% and 98% respectively (Riekert et al., 2020) Position and
    posture detection Faster R-CNN object detection Pig Pig position detection: Average
    Precision (AP) 87.4% Pig position, and pig position and posture: mAPof 80.2%.
    3.7. Yield estimation Early and accurate yield estimation is essential for farmers
    and other stakeholders in making strategic decisions on post-harvest planning,
    policy-making and crop management (Al-Gaadi et al., 2016; Chlingaryan et al.,
    2018; Wei et al., 2020). Some of the studies underline that yield estimation using
    deep learning-based computer vision on aerial images is superior to traditional
    approaches. In a study conducted by Yang et al. (2019) rice grain yield from low-altitude
    remote sensing data was used to estimate the rice grain yield using convolutional
    neural networks. The models were trained on both RGB and multispectral images
    collected by UAV, and results showed that the CNN trained on these images outperformed
    the VIs-based traditional regression models for grain yield estimation at the
    ripening stage. You et al. (2017) employed a combination of convolutional neural
    networks and recurrent neural networks based on the remotely sensed images to
    predict the soybean yield. Another investigation carried out by Russello (2018)
    utilized satellite images in combination with convolutional neural networks for
    crop yield prediction. In case of orchard crops like citrus, computer vision approaches
    are quite straightforward (Fig. 10). The yield can be estimated by directly counting
    the number of flowers or fruits prior to the harvesting stages (Cheng et al.,
    2017; Dorj et al., 2017; Kanwal et al., 2019). With an objective of estimating
    yield from citrus orchards, Apolo-Apolo et al. (Apolo-Apolo et al., 2020)developed
    a Faster-RCNN model for the fruit detection. The data collected through UAV was
    used for the model development. In their study, based on the count, yield from
    orchards was modelled using the Long Short-Term Memory (LSTM) model. An attempt
    was made by Zhou et al.(Zhou et al., 2020)to deploy the yield estimation models
    in smartphones as android applications. In his investigation, four different computer
    vision models; SSD with MobileNetV2, quantized MobileNetV2, InceptionV3, and quantized
    InceptionV3 were trained and converted to TensorFlow Lite models. As reported
    by studies, fruit occlusion caused by leaves and twigs and varying illumination
    conditions are some challenging factors in implementing fruit yield estimation
    systems based on computer vision (Maheswari et al., 2021). Table 7 shows the previous
    studies on computer vision technology for yield estimation. Download : Download
    high-res image (722KB) Download : Download full-size image Fig. 10. Orchard yield
    estimation using computer vision. Table 7. Previous studies on computer vision
    and deep learning technologies for yield estimation. References Objectives and
    scenario of application Methodology Crop Results (Khaki et al., 2020) Image-based
    corn kernel counting and yield estimation Truncated VGGNet backbone and semi supervised
    deep learning. Corn MAE and RMSE of 41.36 and 60.27 respectively. (Apolo-Apolo
    et al., 2020) Yield map Generation Region-CNN (RCNN) Model using UAV imagery Apple
    R-squared value: 0.86, MAE: 10.35 and RMSE: 13.56 (Palacios et al., 2020) Detection
    of flower at bloom for yield estimation CNN SegNet architecture with a VGG19 network
    encoder Grape A determination coefficient (R2) of 0.91 between the actual and
    detected flowers. (Faisal et al., 2020) Intelligent harvesting decision system
    based on date fruit maturity level. VGG-19, Inception-v3, and NASNet Models Date
    Performance metrics of IHDS were 99.4%, 99.4%, 99.7%, and 99.7% for accuracy,
    F1 score, sensitivity (recall), and precision, respectively. (Yang et al., 2019)
    Rice grain yield forecasting using UAV images CNN models with RGB and multispectral
    datasets Rice Prediction accuracy: MAPE: 20.4%, RMSE: 0.658 and R-squared: 0.585
    (Tedesco-Oliveira et al., 2020) Yield estimation using object detection models
    Faster RCNN, SSD and SSD Lite Models Cotton Mean percentage error of 8.84% (Chen
    et al., 2019) Yield prediction by counting number of flowers and maturity analysis,
    using aerial ortho images Faster RCNN model Strawberry The average deep learning
    counting accuracy was 84.1% with average occlusion of 13.5%. (Bargoti and Underwood,
    2017) Yield Estimation using fruit detection and counting CNN and Watershed algorithm
    Apple The count estimates using CNN and WS with R-squared value of 0.826 (Zhou
    et al., 2020) Real-time fruit detection and yield estimation through smartphones.
    Single shot Multibox Detector with MobileNetV2, quantized MobileNetV2, InceptionV3,
    and quantized InceptionV3 Models Kiwi MobileNetV2, quantized MobileNetV2, InceptionV3,
    and quantized InceptionV3 obtained TDR of 90.8%, 89.7%, 87.6%, and 72.8%, respectively.
    (Rahnemoonfar and Sheppard, 2017) Fruit counting based on deep simulated learning
    Modified version of the Inception-ResNet Model Tomato 91% average test accuracy
    on real images and 93% on synthetic images 4. Practical implications Despite being
    late for digitization, the agriculture sector has finally seen good momentum for
    the practical implementation of several artificial intelligence applications,
    including deep learning-based computer vision approaches. Computer vision-powered
    disease identification applications merge the expertise of genetic resources and
    artificial intelligence, allowing farmers and extension workers to act quickly
    and rescue the crop. This disease detecting computer vision-enabled software is
    also being installed inside greenhouses, drones, and other equipment to identify
    the issues and provide a faster response in taking preventive measures. Advancement
    in computer vision technology has been used by agricultural startups for building
    solutions that assist farmers in harvesting, plant health monitoring, pest-weed
    control, etc. For pesticide application, blue river technology (Blue river technology)
    developed a see and spray’ technology that works based on camera inputs and computer
    vision algorithms. The algorithm can distinguish weeds from plants and perform
    targeted pesticide applications. The startup, cromai (Cromai) developed AI-driven
    land and crop diagnostic information. They provide a technological solution for
    georeferenced identification of weeds in the sugarcane field using advanced artificial
    intelligence approaches. Harvesting robots are widely used in open field conditions,
    integrating with machine visions and achieving improved precision. Harvest CROO
    robotics (Harvest croo robotics) developed a fully autonomous harvester, employing
    a harvester-mounted LIDAR system to avoid collisions and accurate navigation.
    The computer vision system scans each berry on the plant and determines the ripeness
    and health before harvesting. ‘Plantix’, the crop damage diagnosis mobile application
    (Plantix) developed by German startup PEAT (Progressive Environmental and Agricultural
    Technologies), uses deep learning and computer vision to help farmers to combat
    pests and diseases Goncharov et al., 2018, Tibbetts, 2018. The application’s functionality
    enables the end-user to upload crop images and get guidance on the disease affected,
    symptom descriptions, treatment information, preventive measures, etc. With the
    same objective of identifying a large number of plant diseases, other applications
    such as Agrio (Agrio) were also introduced to the farming community. Several technology-driven
    solutions were introduced into precision livestock farming to ensure optimal health
    and output of animals also. The technology startup Cainthus (Cainthus) offers
    a computer vision-driven AI system for dairy farmers to monitor their cows and
    send timely alerts and reports via associated applications. Smart cameras are
    deployed to watch over the activities of the cows to provide the right amount
    of feed available on a timely basis. Similar to this, Piguard (Piguard), an innovative
    livestock management software, leverages deep learning-based computer vision approach
    to monitor the health status and behavioral patterns of animals. Computer vision
    technology covers a broad spectrum of solutions for farmers, from small AI-enabled
    mobile apps for decision support, over in-field imaging sensors and remote sensing
    technologies for data collection, and to drones and robots for the automation
    of processes. Across the globe, farming community has realized the potential of
    digital technologies and for the past few years, there has been an increase in
    its adoption. Some of the key factors influencing the transformation of farms
    into digital farms include farm characteristics, operator characteristics, interactions,
    institutions, attributes to technology, and psychological factors (Shang et al.,
    2021). Larger farms are more likely to adopt these technologies by taking advantage
    of economies of scale, and they can afford the higher initial investment cost.
    Use of complementary technologies can also lead to better adoption of technologies.
    For example, the variable rate technology and yield mapping are interrelated,
    and farmers who are using variable rate technologies are more likely to adopt
    yield mapping technologies. Operator characteristics such as end user’s education
    level, age, on-farm digital device such as computer usage are also significant
    (Isgin et al., 2008). Operators having higher education levels and innovativeness
    could adopt the new technologies faster (Aubert et al., 2012). Isgin et al. (2008)
    found significant evidence relating to the impact of urban influences on adoption
    of precision farming technologies in their empirical analysis. Mohr and Kühl (2021)
    investigated the behavioral factors influencing the acceptance of artificial intelligence
    technologies using a theoretical framework. The results showed that behavioral
    control and personal attitude of the farmers are the two most influential factors
    in the acceptance of artificial intelligence in agriculture. 5. Challenges and
    way forward Deep learning for computer vision, the spearhead of artificial intelligence,
    is perhaps one of the most promising technologies for meeting the ever-growing
    food demand. Several intractable problems in agriculture are being solved with
    the support of DL-computer vision. However, high innovation capability always
    comes along with some challenges. One major challenge in computer vision using
    deep learning includes the requirement of massive processing power, and most deep
    learning applications are data-intensive. A possible solution to this is the adoption
    of cloud-based solutions that offer auto-scaling, load balancing, easier maintenance,
    and high availability features. However, cloud solutions limit real-time processing
    due to the latency in access and retrieval of the data from the cloud. The increased
    cost of immense data processing and privacy issues are also other concerns. Advanced
    edge devices with accelerators are capable of analyzing real-time video inputs
    and providing inferences in near real-time. Deployment of the computer vision
    solutions in edge devices can reduce the latency limitations. Sophisticated computer
    vision models in a variety of agricultural use cases often do not perform as expected
    in the production environment. To ensure that a promising model is not becoming
    a costly liability, several aspects like data quality check, code inspection,
    hyper-parameter tuning, code versioning, setting up the right deployment environment,
    rigorous training and re-training, etc, need to be closely evaluated. Quality
    of data is another major concern for developing efficient data-driven solutions
    (Cai and Zhu, 2015; Carletto, 2021). Programmatically generating synthetic data
    is one of the approaches for enhancing the data quality in deep learning-based
    computer vision solutions (Fig. 11). Generative adversarial networks and their
    variations like CGAN can generate synthetic data for agricultural applications
    quite effectively (Cui et al., 2021; Olatunji et al., 2020; Zhu et al., 2020a).
    The performance of a DL-CV model relies heavily on the right hyper-parameter configurations.
    There are no simple ways to set hyper-parameters such as learning rate, batch
    size, momentum, weight decay, etc, and it demands expertise and extensive trial
    and error to achieve the best performance. The process of configuring the hyper-parameter
    in a high-dimensional space is not a trivial challenge. Computer vision problems,
    more specifically object detection approaches face practical implementation challenges
    such as viewpoint variation, deformation, occlusion, varying illumination conditions,
    complex backgrounds, and speed. Viewpoint variation is very common in object detection,
    and segmentation problems, as the object may look at different viewing angles.
    For e.g., a crop may look different when captured from different angles. The additional
    complication appears due to the occlusion. Download : Download high-res image
    (302KB) Download : Download full-size image Fig. 11. Challenges in implementation
    of deep learning based computer vision. In fruit yield estimation systems, this
    is a major concern and causes sharp declination in the overall accuracy of the
    system. Varying illumination conditions and extraction of data from complex overlapped
    and textured backgrounds also make the computer vision task challenging. In real-time
    video applications, performance in terms of detection speed and accuracy are crucial
    for detecting objects in motion. Research in computer vision is growing at a faster
    pace in the agriculture domain. Building a robust computer vision system requires
    quality data generation, transfer, and processing. The system should have adequate
    security to block attacks. Heterogeneity of resources involved in CV solutions
    introduces a lot of security concerns, such as data integrity, privacy issues,
    reliability, etc. As these solutions integrate several digital technologies starting
    from the internet, IoT, cloud computing or edge computing, and wireless sensor
    networks, the system should accommodate security features for all these technologies
    and ensure data and device integrity, data accuracy, and availability. From land
    preparation to harvesting, different stakeholders are leveraging new ways to improve
    the ability to derive insights from images, object detection and tracking, etc.
    Deep learning - computer vision models will undoubtedly continue to expand and
    become more innovative and intelligent, handling more complex computations in
    agriculture with utmost precision. Above all, for obtaining efficient and desirable
    outputs, strong business cases with the capability to scale on a larger scale
    is necessary. 6. Conclusions The surge of deep learning coupled with computer
    vision over the past few years has brought automation capabilities to traditional
    agriculture practices. In this paper, we have extensively discussed the role of
    deep learning-based computer vision in different agriculture applications. More
    specifically, the paper emphasizes seven different application areas such as seed
    quality analysis, soil analysis, irrigation management, plant health analysis,
    weed management, livestock management, and yield estimation. Review of the application
    of deep learning particularly, the assessment and planning of water resources
    revealed that the water sector would continue to embrace deep learning at an accelerated
    rate, and it will play a significant role in the future of water-related research
    and the wide range of application areas. Technologies powered by deep learning
    have created a myriad of application and research opportunities that have the
    potential to change hydrological science and workflow. Recent advances in deep
    learning-assisted image analysis involving algorithms for image classification,
    object detection, segmentation, etc., have expanded their applications across
    different pre-and post-harvesting activities in agriculture. The following conclusions
    can be drawn from the study. • Deep learning-based computer vision has tremendous
    automation capabilities across different applications such as automated plant
    health monitoring, weed detection, irrigation management, livestock management,
    yield estimation, etc. • Integration of the deep learning computer vision approaches
    with the UAV, and spectral data can help in building advanced-intelligent solutions.
    • Despite the benefits computer vision and deep learning brought to agriculture,
    significant challenges do remain, especially the data quality issues, the computation
    power requirement, etc. • The extensive automation across various agriculture
    activities will continue to attract the interest of the deep learning research
    community in the years to come. The adoption rate of advanced technologies in
    agriculture is relatively slow, owing to the high initial investment required,
    lack of technical expertise, and growing concerns about data privacy. However
    at present, the rate of the adoption of these digital solutions has seen a rising
    curve, thus suggesting that these would not be concerns in moving forward. Declaration
    of Competing Interest The authors declare that they have no known competing financial
    interests or personal relationships that could have appeared to influence the
    work reported in this paper. References Abbas et al., 2021 A. Abbas, S. Jain,
    M. Gour, S. Vankudothu Tomato plant disease detection using transfer learning
    with C-GAN synthetic images Comput. Electron. Agric., 187 (2021), Article 106279,
    10.1016/j.compag.2021.106279 View PDFView articleView in ScopusGoogle Scholar
    Abdalla et al., 2021 A. Abdalla, H. Cen, L. Wan, K. Mehmood, Y. He Nutrient Status
    Diagnosis of Infield Oilseed Rape via Deep Learning-Enabled Dynamic Model IEEE
    Trans. Ind. Inform., 17 (2021), pp. 4379-4389, 10.1109/TII.2020.3009736 View in
    ScopusGoogle Scholar Abu Jwade et al., 2019 S. Abu Jwade, A. Guzzomi, A. Mian
    On farm automatic sheep breed classification using deep learning Comput. Electron.
    Agric., 167 (2019), Article 105055, 10.1016/j.compag.2019.105055 View PDFView
    articleView in ScopusGoogle Scholar Achour et al., 2020 B. Achour, M. Belkadi,
    I. Filali, M. Laghrouche, M. Lahdir Image analysis for individual identification
    and feeding behaviour monitoring of dairy cows based on Convolutional Neural Networks
    (CNN) Biosyst. Eng., 198 (2020), pp. 31-49, 10.1016/j.biosystemseng.2020.07.019
    View PDFView articleView in ScopusGoogle Scholar Adnan et al., 2021 R.M. Adnan,
    R.R. Mostafa, A.R.Md.T. Islam, O. Kisi, A. Kuriqi, S. Heddam Estimating reference
    evapotranspiration using hybrid adaptive fuzzy inferencing coupled with heuristic
    algorithms Comput. Electron. Agric., 191 (2021), Article 106541, 10.1016/j.compag.2021.106541
    View PDFView articleView in ScopusGoogle Scholar Agrio Agrio Agrio https://agrio.app/
    (2022), Accessed 6th Jul 2022 Google Scholar Aker, 2011 J.C. Aker Dial “A” for
    agriculture: a review of information and communication technologies for agricultural
    extension in developing countries Agric. Econ., 42 (2011), pp. 631-647, 10.1111/j.1574-0862.2011.00545.x
    View in ScopusGoogle Scholar Alameer et al., 2020 A. Alameer, I. Kyriazakis, H.A.
    Dalton, A.L. Miller, J. Bacardit Automatic recognition of feeding and foraging
    behaviour in pigs using deep learning Biosyst. Eng., 197 (2020), pp. 91-104, 10.1016/j.biosystemseng.2020.06.013
    View PDFView articleView in ScopusGoogle Scholar Albuquerque et al., 2020 C.K.G.
    Albuquerque, S. Polimante, A. Torre-Neto, R.C. Prati Water spray detection for
    smart irrigation systems with Mask R-CNN and UAV footage 2020 IEEE International
    Workshop on Metrology for Agriculture and Forestry (MetroAgriFor) (2020), pp.
    236-240, 10.1109/MetroAgriFor50201.2020.9277542 View in ScopusGoogle Scholar Al-Gaadi
    et al., 2016 K.A. Al-Gaadi, A.A. Hassaballa, E. Tola, A.G. Kayad, R. Madugundu,
    B. Alblewi, F. Assiri Prediction of Potato Crop Yield Using Precision Agriculture
    Techniques PLOS ONE, 11 (2016), Article e0162219, 10.1371/journal.pone.0162219
    View in ScopusGoogle Scholar Amara et al., 2017 J. Amara, B. Bouaziz, A. Algergawy
    A deep learning-based approach for banana leaf diseases classification Datenbanksysteme
    Für Bus. Technol. (2017) (Web BTW 2017-Work) Google Scholar Anami et al., 2020
    B.S. Anami, N.N. Malvade, S. Palaiah Deep learning approach for recognition and
    classification of yield affecting paddy crop stresses using field images Artif.
    Intell. Agric., 4 (2020), pp. 12-20, 10.1016/j.aiia.2020.03.001 View PDFView articleView
    in ScopusGoogle Scholar Andrew et al., 2019 W. Andrew, C. Greatwood, T. Burghardt
    Aerial Animal Biometrics: Individual Friesian Cattle Recovery and Visual Identification
    via an Autonomous UAV with Onboard Deep Inference (2019), 10.48550/arXiv.1907.05310
    Google Scholar Apolo-Apolo et al., 2020 O.E. Apolo-Apolo, J. Martínez-Guanter,
    G. Egea, P. Raja, M. Pérez-Ruiz Deep learning techniques for estimation of the
    yield and size of citrus fruits using a UAV Eur. J. Agron., 115 (2020), Article
    126030, 10.1016/j.eja.2020.126030 View PDFView articleView in ScopusGoogle Scholar
    Araus and Cairns, 2014 J.L. Araus, J.E. Cairns Field high-throughput phenotyping:
    the new crop breeding frontier Trends Plant Sci., 19 (2014), pp. 52-61, 10.1016/j.tplants.2013.09.008
    View PDFView articleView in ScopusGoogle Scholar Araus et al., 2018 J.L. Araus,
    S.C. Kefauver, M. Zaman-Allah, M.S. Olsen, J.E. Cairns Translating High-Throughput
    Phenotyping into Genetic Gain Trends Plant Sci., 23 (2018), pp. 451-466, 10.1016/j.tplants.2018.02.001
    View PDFView articleView in ScopusGoogle Scholar Aubert et al., 2012 B.A. Aubert,
    A. Schroeder, J. Grimaudo IT as enabler of sustainable farming: An empirical analysis
    of farmers’ adoption decision of precision agriculture technology Decis. Support
    Syst., 54 (2012), pp. 510-520, 10.1016/j.dss.2012.07.002 View PDFView articleView
    in ScopusGoogle Scholar Ayadi et al., 2020 S. Ayadi, A. Ben Said, R. Jabbar, C.
    Aloulou, A. Chabbouh, A.B. Achballah Dairy Cow Rumination Detection: A Deep Learning
    Approach I. Jemili, M. Mosbah (Eds.), Distributed Computing for Emerging Smart
    Networks, Communications in Computer and Information Science, Springer International
    Publishing, Cham (2020), pp. 123-139, 10.1007/978-3-030-65810-6_7 View in ScopusGoogle
    Scholar Azadnia et al., 2022 R. Azadnia, A. Jahanbakhshi, S. Rashidi, M. Khajehzadeh,
    P. Bazyar Developing an automated monitoring system for fast and accurate prediction
    of soil texture using an image-based deep learning network and machine vision
    system Measurement, 190 (2022), Article 110669, 10.1016/j.measurement.2021.110669
    View PDFView articleView in ScopusGoogle Scholar Azimi et al., 2021 S. Azimi,
    T. Kaur, T.K. Gandhi A deep learning approach to measure stress level in plants
    due to Nitrogen deficiency Measurement, 173 (2021), Article 108650, 10.1016/j.measurement.2020.108650
    View PDFView articleView in ScopusGoogle Scholar Badrzadeh et al., 2022 N. Badrzadeh,
    J.M.V. Samani, M. Mazaheri, A. Kuriqi Evaluation of management practices on agricultural
    nonpoint source pollution discharges into the rivers under climate change effects
    Sci. Total Environ., 838 (2022), Article 156643, 10.1016/j.scitotenv.2022.156643
    View PDFView articleView in ScopusGoogle Scholar Bao and Bambil, 2021 F. Bao,
    D. Bambil Applicability of computer vision in seed identification: deep learning,
    random forest, and support vector machine classification algorithms Acta Bot.
    Bras., 35 (2021), pp. 17-21, 10.1590/0102-33062020abb0361 View in ScopusGoogle
    Scholar Bargoti and Underwood, 2017 S. Bargoti, J.P. Underwood Image Segmentation
    for Fruit Detection and Yield Estimation in Apple Orchards J. Field Robot., 34
    (2017), pp. 1039-1060, 10.1002/rob.21699 View in ScopusGoogle Scholar Bazi et
    al., 2021 Y. Bazi, L. Bashmal, M.M.A. Rahhal, R.A. Dayil, N.A. Ajlan Vision Transformers
    for Remote Sensing Image Classification Remote Sens., 13 (2021), p. 516, 10.3390/rs13030516
    Google Scholar Bello et al., 2021 R.-W. Bello, A.S.A. Mohamed, A.Z. Talib Contour
    Extraction of Individual Cattle From an Image Using Enhanced Mask R-CNN Instance
    Segmentation Method IEEE Access, 9 (2021), pp. 56984-57000, 10.1109/ACCESS.2021.3072636
    View in ScopusGoogle Scholar Bhagat et al., 2020 M. Bhagat, D. Kumar, I. Haque,
    H.S. Munda, R. Bhagat Plant Leaf Disease Classification Using Grid Search Based
    SVM 2nd International Conference on Data, Engineering and Applications (IDEA).
    Presented at the 2nd International Conference on Data, Engineering and Applications
    (IDEA) (2020), pp. 1-6, 10.1109/IDEA49133.2020.9170725 Google Scholar Bhavsar
    and Panchal, 2012 H. Bhavsar, M.H. Panchal A review on support vector machine
    for data classification Int. J. Adv. Res. Comput. Eng. Technol. IJARCET, 1 (2012),
    pp. 185-189 Google Scholar Blue river technology Blue river technology https://bluerivertechnology.com/
    (2022), Accessed 5th Jul 2022 Google Scholar Brahimi et al., 2017 M. Brahimi,
    K. Boukhalfa, A. Moussaoui Deep Learning for Tomato Diseases: Classification and
    Symptoms Visualization Appl. Artif. Intell., 31 (2017), pp. 299-315, 10.1080/08839514.2017.1315516
    View in ScopusGoogle Scholar Brand et al., 2021 W. Brand, A.T. Wells, S.L. Smith,
    S.J. Denholm, E. Wall, M.P. Coffey Predicting pregnancy status from mid-infrared
    spectroscopy in dairy cow milk using deep learning J. Dairy Sci., 104 (2021),
    pp. 4980-4990, 10.3168/jds.2020-18367 View PDFView articleView in ScopusGoogle
    Scholar Burra et al., 2021 D.D. Burra, J. Hildebrand, J. Giles, T. Nguyen, E.
    Hasiner, K. Schroeder, D. Treguer, A. Juergenliemk, A. Horst, A. Jarvis, W. Kropff
    Digital Agriculture Profile: Viet Nam (Report) Food and Agriculture Organization
    of the United Nations (2021) Google Scholar Cai and Zhu, 2015 L. Cai, Y. Zhu The
    Challenges of Data Quality and Data Quality Assessment in the Big Data Era Data
    Sci. J., 14 (2015), p. 2, 10.5334/dsj-2015-002 View in ScopusGoogle Scholar Cainthus
    Cainthus Cainthus https://www.cainthus.com (2022), Accessed 6th Jul 2022 Google
    Scholar Carletto, 2021 C. Carletto Better data, higher impact: improving agricultural
    data systems for societal change Eur. Rev. Agric. Econ., 48 (2021), pp. 719-740,
    10.1093/erae/jbab030 View in ScopusGoogle Scholar Cen et al., 2016 H. Cen, R.
    Lu, Q. Zhu, F. Mendoza Nondestructive detection of chilling injury in cucumber
    fruit using hyperspectral imaging with feature selection and supervised classification
    Postharvest Biol. Technol., 111 (2016), pp. 352-361, 10.1016/j.postharvbio.2015.09.027
    View PDFView articleView in ScopusGoogle Scholar Chamoso et al., 2014 P. Chamoso,
    W. Raveane, V. Parra, A. González UAVs applied to the counting and monitoring
    of animals Ambient Intelligence-Software and Applications. Springer (2014), pp.
    71-80 CrossRefView in ScopusGoogle Scholar Chang and Lin, 2018 C.-L. Chang, K.-M.
    Lin Smart Agricultural Machine with a Computer Vision-Based Weeding and Variable-Rate
    Irrigation Scheme Robotics, 7 (2018), p. 38, 10.3390/robotics7030038 Google Scholar
    Chavan and Nandedkar, 2018 T.R. Chavan, A.V. Nandedkar AgroAVNET for crops and
    weeds classification: A step forward in automatic farming Comput. Electron. Agric.,
    154 (2018), pp. 361-372, 10.1016/j.compag.2018.09.021 View PDFView articleView
    in ScopusGoogle Scholar Chen et al., 2019 Y. Chen, W.S. Lee, H. Gan, N. Peres,
    C. Fraisse, Y. Zhang, Y. He Strawberry Yield Prediction Based on a Deep Neural
    Network Using High-Resolution Aerial Orthoimages Remote Sens., 11 (2019), p. 1584,
    10.3390/rs11131584 View in ScopusGoogle Scholar Chen et al., 2020a H. Chen, A.
    Chen, L. Xu, H. Xie, H. Qiao, Q. Lin, K. Cai A deep learning CNN architecture
    applied in smart near-infrared analysis of water pollution for agricultural irrigation
    resources Agric. Water Manag., 240 (2020), Article 106303, 10.1016/j.agwat.2020.106303
    View PDFView articleView in ScopusGoogle Scholar Chen et al., 2020b J. Chen, Jinxiu
    Chen, D. Zhang, Y. Sun, Y.A. Nanehkaran Using deep transfer learning for image-based
    plant disease identification Comput. Electron. Agric., 173 (2020), Article 105393,
    10.1016/j.compag.2020.105393 View PDFView articleView in ScopusGoogle Scholar
    Cheng et al., 2017 H. Cheng, L. Damerow, Y. Sun, M. Blanke Early Yield Prediction
    Using Image Analysis of Apple Fruit and Tree Canopy Features with Neural Networks
    J. Imaging, 3 (2017), p. 6, 10.3390/jimaging3010006 Google Scholar Chlingaryan
    et al., 2018 A. Chlingaryan, S. Sukkarieh, B. Whelan Machine learning approaches
    for crop yield prediction and nitrogen status estimation in precision agriculture:
    A review Comput. Electron. Agric., 151 (2018), pp. 61-69, 10.1016/j.compag.2018.05.012
    View PDFView articleView in ScopusGoogle Scholar Coulibaly et al., 2019 S. Coulibaly,
    B. Kamsu-Foguem, D. Kamissoko, D. Traore Deep neural networks with transfer learning
    in millet crop images Comput. Ind., 108 (2019), pp. 115-120, 10.1016/j.compind.2019.02.003
    View PDFView articleView in ScopusGoogle Scholar Cromai Cromai Cromai https://www.cromai.com/
    (2022), Accessed 6th Jul 2022 Google Scholar Cui et al., 2021 X. Cui, Y. Ying,
    Z. Chen CycleGAN based confusion model for cross-species plant disease image migration
    J. Intell. Fuzzy Syst., 41 (2021), pp. 6685-6696, 10.3233/JIFS-210585 View in
    ScopusGoogle Scholar Dell’Acqua and Gamba, 2003 F. Dell’Acqua, P. Gamba Texture-based
    characterization of urban environments on satellite SAR images IEEE Trans. Geosci.
    Remote Sens., 41 (2003), pp. 153-159, 10.1109/TGRS.2002.807754 View in ScopusGoogle
    Scholar Dolata and Reiner, 2018 P. Dolata, J. Reiner Barley Variety Recognition
    with Viewpoint-Aware Double-Stream Convolutional Neural Networks 2018 Federated
    Conference on Computer Science and Information Systems (FedCSIS). (2018), pp.
    101-105 CrossRefView in ScopusGoogle Scholar Dorj et al., 2017 U.-O. Dorj, M.
    Lee, S. Yun An yield estimation in citrus orchards via fruit detection and counting
    using image processing Comput. Electron. Agric., 140 (2017), pp. 103-112, 10.1016/j.compag.2017.05.019
    View PDFView articleView in ScopusGoogle Scholar Dosovitskiy et al., 2021 A. Dosovitskiy,
    L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
    M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby An Image is Worth
    16x16 Words: Transformers for Image Recognition at Scale (2021), 10.48550/arXiv.2010.11929
    Google Scholar Elbeltagi et al., 2022a A. Elbeltagi, M. Kumar, N.L. Kushwaha,
    C.B. Pande, P. Ditthakit, D.K. Vishwakarma, A. Subeesh Drought indicator analysis
    and forecasting using data driven models: case study in Jaisalmer, India Stoch.
    Environ. Res. Risk Assess (2022), 10.1007/s00477-022-02277-0 Google Scholar Elbeltagi
    et al., 2022b A. Elbeltagi, N.L. Kushwaha, J. Rajput, D.K. Vishwakarma, L.C. Kulimushi,
    M. Kumar, J. Zhang, C.B. Pande, P. Choudhari, S.G. Meshram, K. Pandey, P. Sihag,
    N. Kumar, I. Abd-Elaty Modelling daily reference evapotranspiration based on stacking
    hybridization of ANN with meta-heuristic algorithms under diverse agro-climatic
    conditions Stoch. Environ. Res. Risk Assess. (2022), 10.1007/s00477-022-02196-0
    Google Scholar Espejo-Garcia et al., 2020 B. Espejo-Garcia, N. Mylonas, L. Athanasakos,
    S. Fountas, I. Vasilakoglou Towards weeds identification assistance through transfer
    learning Comput. Electron. Agric., 171 (2020), Article 105306, 10.1016/j.compag.2020.105306
    View PDFView articleView in ScopusGoogle Scholar Faisal et al., 2020 M. Faisal,
    M. Alsulaiman, M. Arafah, M.A. Mekhtiche IHDS: Intelligent Harvesting Decision
    System for Date Fruit Based on Maturity Stage Using Deep Learning and Computer
    Vision IEEE Access, 8 (2020), pp. 167985-167997, 10.1109/ACCESS.2020.3023894 View
    in ScopusGoogle Scholar Fawakherji et al., 2020 M. Fawakherji, C. Potena, I. Prevedello,
    A. Pretto, D.D. Bloisi, D. Nardi Data Augmentation Using GANs for Crop/Weed Segmentation
    in Precision Farming 2020 IEEE Conference on Control Technology and Applications
    (CCTA). Presented at the 2020 IEEE Conference on Control Technology and Applications
    (CCTA) (2020), pp. 279-284, 10.1109/CCTA41146.2020.9206297 View in ScopusGoogle
    Scholar Ferentinos, 2018 K.P. Ferentinos Deep learning models for plant disease
    detection and diagnosis Comput. Electron. Agric., 145 (2018), pp. 311-318, 10.1016/j.compag.2018.01.009
    View PDFView articleView in ScopusGoogle Scholar Foglia and Reina, 2006 M.M. Foglia,
    G. Reina Agricultural robot for radicchio harvesting J. Field Robot., 23 (2006),
    pp. 363-377, 10.1002/rob.20131 View in ScopusGoogle Scholar Ghanem et al., 2015
    M.E. Ghanem, H. Marrou, T.R. Sinclair Physiological phenotyping of plants for
    crop improvement Trends Plant Sci., 20 (2015), pp. 139-144, 10.1016/j.tplants.2014.11.006
    View PDFView articleView in ScopusGoogle Scholar Gomes and Leta, 2012 J.F.S. Gomes,
    F.R. Leta Applications of computer vision techniques in the agriculture and food
    industry: a review Eur. Food Res. Technol., 235 (2012), pp. 989-1000, 10.1007/s00217-012-1844-2
    View in ScopusGoogle Scholar Goncharov et al., 2018 P Goncharov, G Ososkov, A
    Nechaevskiy, A Uzhinskiy, I Nestsiarenia Disease Detection on the Plant Leaves
    by Deep Learning Boris Kryzhanovsky, Witali Dunin-Barkowski, Vladimir Redko, Yury
    Tiumentsev (Eds.), Advances in Neural Computation, Machine Learning, and Cognitive
    Research II, Springer (2018), pp. 151-159 Google Scholar Granitto et al., 2005
    P.M. Granitto, P.F. Verdes, H.A. Ceccatto Large-scale investigation of weed seed
    identification by machine vision Comput. Electron. Agric., 47 (2005), pp. 15-24,
    10.1016/j.compag.2004.10.003 View PDFView articleView in ScopusGoogle Scholar
    Gulzar et al., 2020 Y. Gulzar, Y. Hamid, A.B. Soomro, A.A. Alwan, L. Journaux
    A Convolution Neural Network-Based Seed Classification System Symmetry, 12 (2020),
    p. 2018, 10.3390/sym12122018 Google Scholar Hall et al., 2015 D. Hall, C. McCool,
    F. Dayoub, N. Sunderhauf, B. Upcroft Evaluation of Features for Leaf Classification
    in Challenging Conditions 2015 IEEE Winter Conference on Applications of Computer
    Vision. Presented at the 2015 IEEE Winter Conference on Applications of Computer
    Vision (2015), pp. 797-804, 10.1109/WACV.2015.111 View in ScopusGoogle Scholar
    Hansen et al., 2018 M.F. Hansen, M.L. Smith, L.N. Smith, M.G. Salter, E.M. Baxter,
    M. Farish, B. Grieve Towards on-farm pig face recognition using convolutional
    neural networks Comput. Ind., 98 (2018), pp. 145-152, 10.1016/j.compind.2018.02.016
    View PDFView articleView in ScopusGoogle Scholar Haralick et al., 1973 R.M. Haralick,
    K. Shanmugam, I. Dinstein Textural Features for Image Classification IEEE Trans.
    Syst. Man Cybern., SMC-3 (1973), pp. 610-621, 10.1109/TSMC.1973.4309314 View in
    ScopusGoogle Scholar Harvest croo robotics Harvest croo robotics Harvest croo
    robotics https://www.harvestcroorobotics.com/technology (2022), Accessed 5th Jul
    2022 Google Scholar Hassan and Maji, 2022 S.M. Hassan, A.K. Maji Plant Disease
    Identification Using a Novel Convolutional Neural Network IEEE Access, 10 (2022),
    pp. 5390-5401, 10.1109/ACCESS.2022.3141371 View in ScopusGoogle Scholar Hati and
    Singh, 2021 A.J. Hati, R.R. Singh Artificial Intelligence in Smart Farms: Plant
    Phenotyping for Species Recognition and Health Condition Identification Using
    Deep Learning AI, 2 (2021), pp. 274-289, 10.3390/ai2020017 View in ScopusGoogle
    Scholar He et al., 2015 K. He, X. Zhang, S. Ren, J. Sun Deep residual learning
    for image recognition ArXiv151203385 Cs (2015) Google Scholar Heramb et al., 2022
    P. Heramb, P. Kumar Singh, K.V. Ramana Rao, A. Subeesh Modelling reference evapotranspiration
    using gene expression programming and artificial neural network at Pantnagar Inf.
    Process. Agric, India (2022), 10.1016/j.inpa.2022.05.007 Google Scholar Hiriyannaiah
    et al., 2020 S. Hiriyannaiah, A.M.D. Srinivas, G.K. Shetty, K.G. Srinivasa Chapter
    4 - A computationally intelligent agent for detecting fake news using generative
    adversarial networks S. Bhattacharyya, V. Snášel, D. Gupta, A. Khanna (Eds.),
    Hybrid Computational Intelligence, Hybrid Computational Intelligence for Pattern
    Analysis, and Understanding. Academic Press (2020), pp. 69-96, 10.1016/B978-0-12-818699-2.00004-4
    View PDFView articleGoogle Scholar Huang, 1993 T. Huang Computer vision Evolution
    and promise (1993) Google Scholar Huang et al., 2020 H. Huang, Y. Lan, A. Yang,
    Y. Zhang, S. Wen, J. Deng Deep learning versus Object-based Image Analysis (OBIA)
    in weed mapping of UAV imagery Int. J. Remote Sens., 41 (2020), pp. 3446-3479,
    10.1080/01431161.2019.1706112 View in ScopusGoogle Scholar Hughes and Salathe,
    2016 D.P. Hughes, M. Salathe An open access repository of images on plant health
    to enable the development of mobile disease diagnostics ArXiv151108060 Cs (2016)
    Google Scholar Huzzey et al., 2007 J.M. Huzzey, D.M. Veira, D.M. Weary, M. von
    Keyserlingk Prepartum behavior and dry matter intake identify dairy cows at risk
    for metritis J. Dairy Sci., 90 (2007), pp. 3220-3233, 10.3168/jds.2006-807 View
    PDFView articleView in ScopusGoogle Scholar Isgin et al., 2008 T. Isgin, A. Bilgic,
    D.L. Forster, M.T. Batte Using count data models to determine the factors affecting
    farmers’ quantity decisions of precision farming technology adoption Comput. Electron.
    Agric., 62 (2008), pp. 231-242, 10.1016/j.compag.2008.01.004 View PDFView articleView
    in ScopusGoogle Scholar Jahagirdar and Budihal, 2021 P. Jahagirdar, S.V. Budihal
    Framework to Detect NPK Deficiency in Maize Plants Using CNN C.R. Panigrahi, B.
    Pati, P. Mohapatra, R. Buyya, K.-C. Li (Eds.), Progress in Advanced Computing
    and Intelligent Engineering, Advances in Intelligent Systems and Computing, Springer,
    Singapore (2021), pp. 366-376, 10.1007/978-981-15-6353-9_33 View in ScopusGoogle
    Scholar Jamei et al., 2022a M. Jamei, M. Karbasi, A. Malik, L. Abualigah, A.R.M.T.
    Islam, Z.M. Yaseen Computational assessment of groundwater salinity distribution
    within coastal multi-aquifers of Bangladesh Sci. Rep., 12 (2022), p. 11165, 10.1038/s41598-022-15104-x
    View in ScopusGoogle Scholar Jamei et al., 2022b M. Jamei, S. Maroufpoor, Y. Aminpour,
    M. Karbasi, A. Malik, B. Karimi Developing hybrid data-intelligent method using
    Boruta-random forest optimizer for simulation of nitrate distribution pattern
    Agric. Water Manag., 270 (2022), Article 107715, 10.1016/j.agwat.2022.107715 View
    PDFView articleView in ScopusGoogle Scholar Jamei et al., 2022c Mehdi Jamei, M.
    Karbasi, A. Malik, Mozhdeh Jamei, O. Kisi, Z.M. Yaseen Long-term multi-step ahead
    forecasting of root zone soil moisture in different climates: Novel ensemble-based
    complementary data-intelligent paradigms Agric. Water Manag., 269 (2022), Article
    107679, 10.1016/j.agwat.2022.107679 View PDFView articleView in ScopusGoogle Scholar
    Javanmardi et al., 2021 S. Javanmardi, S.-H. Miraei Ashtiani, F.J. Verbeek, A.
    Martynenko Computer-vision classification of corn seed varieties using deep convolutional
    neural network J. Stored Prod. Res., 92 (2021), Article 101800, 10.1016/j.jspr.2021.101800
    View PDFView articleView in ScopusGoogle Scholar Jha et al., 2019 K. Jha, A. Doshi,
    P. Patel, M. Shah A comprehensive review on automation in agriculture using artificial
    intelligence Artif. Intell. Agric., 2 (2019), pp. 1-12, 10.1016/j.aiia.2019.05.004
    View PDFView articleView in ScopusGoogle Scholar Ji and Wu, 2022 M. Ji, Z. Wu
    Automatic detection and severity analysis of grape black measles disease based
    on deep learning and fuzzy logic Comput. Electron. Agric., 193 (2022), Article
    106718, 10.1016/j.compag.2022.106718 View PDFView articleView in ScopusGoogle
    Scholar Jiang et al., 2019 B. Jiang, Q. Wu, X. Yin, D. Wu, H. Song, D. He FLYOLOv3
    deep learning for key parts of dairy cow body detection Comput. Electron. Agric.,
    166 (2019), Article 104982, 10.1016/j.compag.2019.104982 View PDFView articleView
    in ScopusGoogle Scholar Joshi et al., 2021 R.C. Joshi, M. Kaushik, M.K. Dutta,
    A. Srivastava, N. Choudhary VirLeafNet: Automatic analysis and viral disease diagnosis
    using deep-learning in Vigna mungo plant Ecol. Inform., 61 (2021), Article 101197,
    10.1016/j.ecoinf.2020.101197 View PDFView articleView in ScopusGoogle Scholar
    Jung et al., 2021 D.-H. Jung, N.Y. Kim, S.H. Moon, C. Jhin, H.-J. Kim, J.-S. Yang,
    H.S. Kim, T.S. Lee, J.Y. Lee, S.H. Park Deep Learning-Based Cattle Vocal Classification
    Model and Real-Time Livestock Monitoring System with Noise Filtering Animals,
    11 (2021), p. 357, 10.3390/ani11020357 Google Scholar Kaack et al., 2022 L.H.
    Kaack, P.L. Donti, E. Strubell, G. Kamiya, F. Creutzig, D. Rolnick Aligning artificial
    intelligence with climate change mitigation Nat. Clim. Change, 12 (2022), pp.
    518-527, 10.1038/s41558-022-01377-7 View in ScopusGoogle Scholar Kamilaris and
    Prenafeta-Boldú, 2018 A. Kamilaris, F.X. Prenafeta-Boldú Deep learning in agriculture:
    A survey Comput. Electron. Agric., 147 (2018), pp. 70-90, 10.1016/j.compag.2018.02.016
    View PDFView articleView in ScopusGoogle Scholar Kamyshova et al., 2022 G. Kamyshova,
    A. Osipov, S. Gataullin, S. Korchagin, S. Ignar, T. Gataullin, N. Terekhova, S.
    Suvorov Artificial neural networks and computer vision’s-based phytoindication
    systems for variable rate irrigation improving IEEE Access, 10 (2022), pp. 8577-8589,
    10.1109/ACCESS.2022.3143524 View in ScopusGoogle Scholar Kannur et al., 2011 Anil
    Kannur, Asha Kannur, V.S. Rajpurohit Classification and grading of bulk seeds
    using artificial neural network Int. J. Mach. Intell., 3 (2011), pp. 62-73, 10.9735/0975-2927.3.2.62-73
    Google Scholar Kanwal et al., 2019 Z. Kanwal, A. Basit, M. Jawad, I. Ullah, A.
    Ali Overlapped apple fruit yield estimation using pixel classification and hough
    transform Int. J. Adv. Comput. Sci. Appl., 10 (2019), 10.14569/IJACSA.2019.0100271
    Google Scholar Karbasi et al., 2022 M. Karbasi, M. Jamei, M. Ali, A. Malik, Z.M.
    Yaseen Forecasting weekly reference evapotranspiration using Auto Encoder Decoder
    Bidirectional LSTM model hybridized with a Boruta-CatBoost input optimizer Comput.
    Electron. Agric., 198 (2022), Article 107121 View PDFView articleView in ScopusGoogle
    Scholar Karim et al., 2013 L. Karim, A. Anpalagan, N. Nasser, J. Almhana Sensor-based
    M2M Agriculture Monitoring Systems for Developing Countries State and Challenges,
    5 (2013), 10.5296/npa.v5i3.3787 Google Scholar Khaki et al., 2020 S. Khaki, L.
    Wang, S.V. Archontoulis A CNN-RNN Framework for Crop Yield Prediction Front. Plant
    Sci., 10 (2020), 10.3389/fpls.2019.01750 Google Scholar Khan et al., 2021 S. Khan,
    M. Naseer, M. Hayat, S.W. Zamir, F.S. Khan, M. Shah Transformers in Vision: A
    Survey ACM Comput. Surv. (2021), 10.1145/3505244 Google Scholar Khanna and Kaur,
    2019 A. Khanna, S. Kaur Evolution of Internet of Things (IoT) and its significant
    impact in the field of Precision Agriculture Comput. Electron. Agric., 157 (2019),
    pp. 218-231, 10.1016/j.compag.2018.12.039 View PDFView articleView in ScopusGoogle
    Scholar Kim et al., 2022 K.-H. Kim, M.-G. Kim, P.-R. Yoon, J.-H. Bang, W.-H. Myoung,
    J.-Y. Choi, G.-H. Choi Application of CCTV Image and Semantic Segmentation Model
    for Water Level Estimation of Irrigation Channel J. Korean Soc. Agric. Eng., 64
    (2022), pp. 63-73, 10.5389/KSAE.2022.64.3.063 Google Scholar Koech and Langat,
    2018 R. Koech, P. Langat Improving irrigation water use efficiency: a review of
    advances, challenges and opportunities in the Australian context Water, 10 (2018),
    p. 1771, 10.3390/w10121771 View in ScopusGoogle Scholar Kriegeskorte and Golan,
    2019 N. Kriegeskorte, T. Golan Neural network models and deep learning Curr. Biol.,
    29 (2019), pp. R231-R236, 10.1016/j.cub.2019.02.034 View PDFView articleView in
    ScopusGoogle Scholar Krizhevsky et al., 2012a A. Krizhevsky, I. Sutskever, G.E.
    Hinton ImageNet classification with deep convolutional neural networks, in: Proceedings
    of the 25th International Conference on Neural Information Processing Systems
    - Volume 1, NIPS’12 Curran Associates Inc., Red Hook, NY, USA (2012), pp. 1097-1105
    Google Scholar Krizhevsky et al., 2012b A. Krizhevsky, I. Sutskever, G.E. Hinton
    ImageNet Classification with Deep Convolutional Neural Networks Advances in Neural
    Information Processing Systems, Curran Associates, Inc. (2012) Google Scholar
    Kumar et al., 2017 S. Kumar, S.K. Singh, R. Singh, A.K. Singh Recognition of cattle
    using face images Anim. Biom. (2017), pp. 79-110 CrossRefGoogle Scholar Kumar
    et al., 2019 A. Kumar, S. Sarkar, C. Pradhan Recommendation System for Crop Identification
    and Pest Control Technique in Agriculture 2019 International Conference on Communication
    and Signal Processing (ICCSP). Presented at the 2019 International Conference
    on Communication and Signal Processing (ICCSP) (2019), pp. 0185-0189, 10.1109/ICCSP.2019.8698099
    Google Scholar Kumbi and Birje, 2022 A.A. Kumbi, M.N. Birje Deep CNN based sunflower
    atom optimization method for optimal water control in IoT Wirel. Pers. Commun.,
    122 (2022), pp. 1221-1246, 10.1007/s11277-021-08946-7 View in ScopusGoogle Scholar
    Kuplich et al., 2005 T.M. Kuplich, P.J. Curran, P.M. Atkinson Relating SAR image
    texture to the biomass of regenerating tropical forests Int. J. Remote Sens.,
    26 (2005), pp. 4829-4854, 10.1080/01431160500239107 View in ScopusGoogle Scholar
    Kurtulmuş, 2021 F. Kurtulmuş Identification of sunflower seeds with deep convolutional
    neural networks J. Food Meas. Charact., 15 (2021), pp. 1024-1033, 10.1007/s11694-020-00707-7
    View in ScopusGoogle Scholar Kushwaha et al., 2016 N.L. Kushwaha, A. Bhardwaj,
    V.K. Verma Hydrologic response of Takarla-Ballowal watershed in Shivalik foot-hills
    based on morphometric analysis using remote sensing and GIS J Indian Water Resour
    Soc, 36 (2016), pp. 17-25 Google Scholar Kushwaha et al., 2021 N.L. Kushwaha,
    J. Rajput, A. Elbeltagi, A.Y. Elnaggar, D.R. Sena, D.K. Vishwakarma, I. Mani,
    E.E. Hussein Data Intelligence model and meta-heuristic algorithms-based pan evaporation
    modelling in two different agro-climatic zones: a case study from Northern India
    Atmosphere, 12 (2021), p. 1654, 10.3390/atmos12121654 View in ScopusGoogle Scholar
    Kushwaha et al., 2022 N.L. Kushwaha, A. Elbeltagi, S. Mehan, A. Malik, A. Yousuf
    Comparative study on morphometric analysis and RUSLE-based approaches for micro-watershed
    prioritization using remote sensing and GIS Arab. J. Geosci., 15 (2022), p. 564,
    10.1007/s12517-022-09837-2 Google Scholar Kussul et al., 2017 N. Kussul, M. Lavreniuk,
    S. Skakun, A. Shelestov Deep learning classification of land cover and crop types
    using remote sensing data IEEE Geosci. Remote Sens. Lett., 14 (2017), pp. 778-782,
    10.1109/LGRS.2017.2681128 View in ScopusGoogle Scholar Le et al., 2020 V.N.T.
    Le, S. Ahderom, K. Alameh Performances of the LBP Based Algorithm over CNN Models
    for Detecting Crops and Weeds with Similar Morphologies Sensors, 20 (2020), p.
    2193, 10.3390/s20082193 View in ScopusGoogle Scholar LeCun et al., 1998 Y. LeCun,
    L. Bottou, Y. Bengio, P. Haffner Gradient-based learning applied to document recognition
    Proc. IEEE, 86 (1998), pp. 2278-2324 Google Scholar Ledig et al., 2017 C. Ledig,
    L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani,
    J. Totz, Z. Wang, W. Shi Photo-Realistic Single Image Super-Resolution Using a
    Generative Adversarial Network (2017), 10.48550/arXiv.1609.04802 Google Scholar
    Li and Li, 2022 X. Li, S. Li Transformer Help CNN See Better: A Lightweight Hybrid
    Apple Disease Identification Model Based on Transformers Agriculture, 12 (2022),
    p. 884, 10.3390/agriculture12060884 View in ScopusGoogle Scholar Li et al., 2019
    Y. Li, C.J. Randall, R. Van Woesik, E. Ribeiro Underwater video mosaicing using
    topology and superpixel-based pairwise stitching Expert Syst. Appl., 119 (2019),
    pp. 171-183, 10.1016/j.eswa.2018.10.041 View PDFView articleView in ScopusGoogle
    Scholar Li et al., 2021 C. Li, H. Li, Z. Liu, B. Li, Y. Huang SeedSortNet: a rapid
    and highly effificient lightweight CNN based on visual attention for seed sorting
    PeerJ Comput. Sci., 7 (2021), Article e639, 10.7717/peerj-cs.639 Google Scholar
    Li et al., 2022 Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, C. Feichtenhofer
    MViTv2: improved multiscale vision transformers for classification and detection
    Presented at the Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (2022), pp. 4804-4814 Google Scholar Liu and Wang, 2020 J.
    Liu, X. Wang Tomato diseases and pests detection based on improved yolo V3 convolutional
    neural network Front. Plant Sci., 11 (2020) Google Scholar Liu et al., 2015 D.
    Liu, X. Ning, Z. Li, D. Yang, H. Li, L. Gao Discriminating and elimination of
    damaged soybean seeds based on image characteristics J. Stored Prod. Res., 60
    (2015), pp. 67-74, 10.1016/j.jspr.2014.10.001 View PDFView articleView in ScopusGoogle
    Scholar Liu et al., 2016 W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y.
    Fu, A.C. Berg SSD: Single Shot MultiBox Detector B. Leibe, J. Matas, N. Sebe,
    M. Welling (Eds.), Computer Vision – ECCV 2016, Lecture Notes in Computer Science,
    Springer International Publishing, Cham (2016), pp. 21-37, 10.1007/978-3-319-46448-0_2
    View in ScopusGoogle Scholar Lo Bianco et al., 2017 M. Lo Bianco, O. Grillo, E.
    Cañadas, G. Venora, G. Bacchetta Inter- and intraspecific diversity in Cistus
    L. (Cistaceae) seeds, analysed with computer vision techniques Plant Biol., 19
    (2017), pp. 183-190, 10.1111/plb.12529 View in ScopusGoogle Scholar Ma et al.,
    2020 T. Ma, S. Tsuchikawa, T. Inagaki Rapid and non-destructive seed viability
    prediction using near-infrared hyperspectral imaging coupled with a deep learning
    approach Comput. Electron. Agric., 177 (2020), Article 105683, 10.1016/j.compag.2020.105683
    View PDFView articleView in ScopusGoogle Scholar Maheswari et al., 2021 P. Maheswari,
    P. Raja, O.E. Apolo-Apolo, M. Pérez-Ruiz Intelligent fruit yield estimation for
    orchards using deep learning based semantic segmentation techniques—a review Front.
    Plant Sci., 12 (2021), p. 1247, 10.3389/fpsyg.2020.513474 Google Scholar Malik
    et al., 2022a A. Malik, M.K. Saggi, S. Rehman, H. Sajjad, S. Inyurt, A.S. Bhatia,
    A.A. Farooque, A.Y. Oudah, Z.M. Yaseen Deep learning versus gradient boosting
    machine for pan evaporation prediction Eng. Appl. Comput. Fluid Mech., 16 (2022),
    pp. 570-587, 10.1080/19942060.2022.2027273 View in ScopusGoogle Scholar Malik
    et al., 2022b A. Malik, Y. Tikhamarine, P. Sihag, S. Shahid, M. Jamei, M. Karbasi
    Predicting daily soil temperature at multiple depths using hybrid machine learning
    models for a semi-arid region in Punjab Environ. Sci. Pollut. Res, India (2022),
    10.1007/s11356-022-20837-3 Google Scholar Mathew and Mahesh, 2022 M.P. Mathew,
    T.Y. Mahesh Leaf-based disease detection in bell pepper plant using YOLO v5 Signal
    Image Video Process., 16 (2022), pp. 841-847, 10.1007/s11760-021-02024-y View
    in ScopusGoogle Scholar Miikkulainen et al., 2019 R. Miikkulainen, J. Liang, E.
    Meyerson, A. Rawal, D. Fink, O. Francon, B. Raju, H. Shahrzad, A. Navruzyan, N.
    Duffy, B. Hodjat Chapter 15 - Evolving Deep Neural Networks R. Kozma, C. Alippi,
    Y. Choe, F.C. Morabito (Eds.), Artificial Intelligence in the Age of Neural Networks
    and Brain Computing, Academic Press (2019), pp. 293-312, 10.1016/B978-0-12-815480-9.00015-3
    View PDFView articleGoogle Scholar Mirza and Osindero, 2014 M. Mirza, S. Osindero
    Conditional Generative Adversarial Nets. ArXiv14111784 Cs Stat (2014) Google Scholar
    Mkonyi et al., 2020 L. Mkonyi, D. Rubanga, M. Richard, N. Zekeya, S. Sawahiko,
    B. Maiseli, D. Machuve Early identification of Tuta absoluta in tomato plants
    using deep learning Sci. Afr., 10 (2020), Article e00590, 10.1016/j.sciaf.2020.e00590
    View PDFView articleView in ScopusGoogle Scholar Mochida et al., 2019 K. Mochida,
    S. Koda, K. Inoue, T. Hirayama, S. Tanaka, R. Nishii, F. Melgani Computer vision-based
    phenotyping for improvement of plant productivity: a machine learning perspective
    GigaScience, 8 (2019), 10.1093/gigascience/giy153 Google Scholar Mohanty et al.,
    2016 S.P. Mohanty, D.P. Hughes, M. Salathé Using Deep Learning for Image-Based
    Plant Disease Detection Front. Plant Sci., 7 (2016) Google Scholar Mohr and Kühl,
    2021 S. Mohr, R. Kühl Acceptance of artificial intelligence in German agriculture:
    an application of the technology acceptance model and the theory of planned behavior
    Precis. Agric., 22 (2021), pp. 1816-1844, 10.1007/s11119-021-09814-x View in ScopusGoogle
    Scholar Mortensen et al., 2016 A.K. Mortensen, M. Dyrmann, H. Karstoft, R. Nyholm
    Jørgensen, R. Gislum Semantic Segmentation of Mixed Crops using Deep Convolutional
    Neural Network (2016) Google Scholar Mukti and Biswas, 2019 I.Z. Mukti, D. Biswas
    Transfer Learning Based Plant Diseases Detection Using ResNet50. 2019 4th Int
    Conf. Electr. Inf. Commun. Technol. EICT. (2019), 10.1109/EICT48899.2019.9068805
    Google Scholar Nagasubramanian et al., 2019 K. Nagasubramanian, S. Jones, A.K.
    Singh, S. Sarkar, A. Singh, B. Ganapathysubramanian Plant disease identification
    using explainable 3D deep learning on hyperspectral images Plant Methods, 15 (2019),
    p. 98, 10.1186/s13007-019-0479-8 View in ScopusGoogle Scholar Ni et al., 2019
    C. Ni, D. Wang, R. Vinson, M. Holmes, Y. Tao Automatic inspection machine for
    maize kernels based on deep convolutional neural networks Biosyst. Eng., 178 (2019),
    pp. 131-144, 10.1016/j.biosystemseng.2018.11.010 View PDFView articleView in ScopusGoogle
    Scholar Nilsson, 1995 H. Nilsson Remote Sensing and Image Analysis in Plant Pathology
    Annu. Rev. Phytopathol., 33 (1995), pp. 489-528, 10.1146/annurev.py.33.090195.002421
    Google Scholar O’Mahony et al., 2020 N. O’Mahony, S. Campbell, A. Carvalho, S.
    Harapanahalli, G.V. Hernandez, L. Krpalkova, D. Riordan, J. Walsh Deep Learning
    vs. Traditional Computer Vision K. Arai, S. Kapoor (Eds.), Advances in Computer
    Vision, Advances in Intelligent Systems and Computing, Springer International
    Publishing, Cham (2020), pp. 128-144, 10.1007/978-3-030-17795-9_10 View in ScopusGoogle
    Scholar Olatunji et al., 2020 J.R. Olatunji, G.P. Redding, C.L. Rowe, A.R. East
    Reconstruction of kiwifruit fruit geometry using a CGAN trained on a synthetic
    dataset Comput. Electron. Agric., 177 (2020), Article 105699, 10.1016/j.compag.2020.105699
    View PDFView articleView in ScopusGoogle Scholar Omondiagbe et al., 2022 O.P.
    Omondiagbe, L. Lilburne, S. Licorish, S. MacDonell Soil Texture Prediction with
    Automated Deep Convolutional Neural Networks and Population Based Learning (SSRN
    Scholarly Paper No. 4003387) Social Science Research Network, Rochester, NY (2022),
    10.2139/ssrn.4003387 Google Scholar Osorio et al., 2020 K. Osorio, A. Puerto,
    C. Pedraza, D. Jamaica, L. Rodríguez A deep learning approach for weed detection
    in lettuce crops using multispectral images AgriEngineering, 2 (2020), pp. 471-488,
    10.3390/agriengineering2030032 View in ScopusGoogle Scholar Palacios et al., 2020
    F. Palacios, G. Bueno, J. Salido, M.P. Diago, I. Hernández, J. Tardaguila Automated
    grapevine flower detection and quantification method based on computer vision
    and deep learning from on-the-go imaging using a mobile sensing platform under
    field conditions Comput. Electron. Agric., 178 (2020), Article 105796, 10.1016/j.compag.2020.105796
    View PDFView articleView in ScopusGoogle Scholar Piguard Piguard Piguard https://www.serket-tech.com/Products
    (2022), Accessed 5th Jul 2022 Google Scholar Plantix Plantix Plantix https://plantix.net/en/
    (2022), Accessed 5th Jul 2022 Google Scholar Putzu et al., 2016 L. Putzu, C. Di
    Ruberto, G. Fenu A Mobile Application for Leaf Detection in Complex Background
    Using Saliency Maps J. Blanc-Talon, C. Distante, W. Philips, D. Popescu, P. Scheunders
    (Eds.), Advanced Concepts for Intelligent Vision Systems, Springer International
    Publishing, Cham (2016), pp. 570-581, 10.1007/978-3-319-48680-2_50 View in ScopusGoogle
    Scholar Pyo et al., 2020 J. Pyo, S.M. Hong, Y.S. Kwon, M.S. Kim, K.H. Cho Estimation
    of heavy metals using deep neural network with visible and infrared spectroscopy
    of soil Sci. Total Environ., 741 (2020), Article 140162, 10.1016/j.scitotenv.2020.140162
    View PDFView articleView in ScopusGoogle Scholar Qiao et al., 2019 Y. Qiao, D.
    Su, H. Kong, S. Sukkarieh, S. Lomax, C. Clark Individual cattle identification
    using a deep learning based framework IFAC-Pap., 52 (2019), pp. 318-323 View PDFView
    articleCrossRefGoogle Scholar Qiao et al., 2019b Y. Qiao, M. Truman, S. Sukkarieh
    Cattle segmentation and contour extraction based on Mask R-CNN for precision livestock
    farming Comput. Electron. Agric., 165 (2019), Article 104958, 10.1016/j.compag.2019.104958
    View PDFView articleView in ScopusGoogle Scholar Qiao et al., 2022 Y. Qiao, Y.
    Guo, K. Yu, D. He C3D-ConvLSTM based cow behaviour classification using video
    data for precision livestock farming Comput. Electron. Agric., 193 (2022), Article
    106650, 10.1016/j.compag.2021.106650 View PDFView articleView in ScopusGoogle
    Scholar Qiu et al., 2018 Z. Qiu, J. Chen, Y. Zhao, S. Zhu, Y. He, C. Zhang Variety
    Identification of Single Rice Seed Using Hyperspectral Imaging Combined with Convolutional
    Neural Network Appl. Sci., 8 (2018), p. 212, 10.3390/app8020212 View in ScopusGoogle
    Scholar Rahnemoonfar and Sheppard, 2017 M. Rahnemoonfar, C. Sheppard Deep count:
    fruit counting based on deep simulated learning Sensors, 17 (2017), p. 905, 10.3390/s17040905
    View in ScopusGoogle Scholar Rahnemoonfar et al., 2019 M. Rahnemoonfar, D. Dobbs,
    M. Yari, M.J. Starek DisCountNet: Discriminating and Counting Network for Real-Time
    Counting and Localization of Sparse Objects in High-Resolution UAV Imagery Remote
    Sens., 11 (2019), p. 1128, 10.3390/rs11091128 View in ScopusGoogle Scholar Rai
    et al., 2022 P. Rai, P. Kumar, N. Al-Ansari, A. Malik Evaluation of machine learning
    versus empirical models for monthly reference evapotranspiration estimation in
    Uttar Pradesh and Uttarakhand States India. Sustainability, 14 (2022), p. 5771,
    10.3390/su14105771 View in ScopusGoogle Scholar Ranganathan et al., 2018 J. Ranganathan,
    R. Waite, T. Searchinger, C. Hanson How to Sustainably Feed 10 Billion People
    by 2050, in 21 Charts (2018) Google Scholar Redmon et al., 2016 J. Redmon, S.
    Divvala, R. Girshick, A. Farhadi You Only Look Once: Unified, Real-Time Object
    Detection 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
    (2016), pp. 779-788, 10.1109/CVPR.2016.91 Google Scholar Rehman et al., 2019 T.U.
    Rehman, Md.S. Mahmud, Y.K. Chang, J. Jin, J. Shin Current and future applications
    of statistical machine learning algorithms for agricultural machine vision systems
    Comput. Electron. Agric., 156 (2019), pp. 585-605, 10.1016/j.compag.2018.12.006
    View PDFView articleView in ScopusGoogle Scholar Reimers and Requena-Mesa, 2020
    C. Reimers, C. Requena-Mesa Chapter 13 - Deep Learning – an Opportunity and a
    Challenge for Geo- and Astrophysics P. Škoda, F. Adam (Eds.), Knowledge Discovery
    in Big Data from Astronomy and Earth Observation, Elsevier (2020), pp. 251-265,
    10.1016/B978-0-12-819154-5.00024-2 View PDFView articleView in ScopusGoogle Scholar
    Ren et al., 2016 S. Ren, K. He, R. Girshick, J. Sun Faster R-CNN: towards real-time
    object detection with region proposal networks ArXiv150601497 Cs (2016) Google
    Scholar Rico-Fernández et al., 2019 M.P. Rico-Fernández, R. Rios-Cabrera, M. Castelán,
    H.-I. Guerrero-Reyes, A. Juarez-Maldonado A contextualized approach for segmentation
    of foliage in different crop species Comput. Electron. Agric., 156 (2019), pp.
    378-386, 10.1016/j.compag.2018.11.033 View PDFView articleView in ScopusGoogle
    Scholar Riekert et al., 2020 M. Riekert, A. Klein, F. Adrion, C. Hoffmann, E.
    Gallmann Automatically detecting pig position and posture by 2D camera imaging
    and deep learning Comput. Electron. Agric., 174 (2020), Article 105391, 10.1016/j.compag.2020.105391
    View PDFView articleView in ScopusGoogle Scholar Riese and Keller, 2019a F.M.
    Riese, S. Keller SOIL TEXTURE CLASSIFICATION WITH 1D CONVOLUTIONAL NEURAL NETWORKS
    BASED ON HYPERSPECTRAL DATA, in: ISPRS Annals of the Photogrammetry, Remote Sensing
    and Spatial Information Sciences. Presented at the ISPRS Geospatial Week 2019
    (Volume IV-2/W5) - 10–14 June 2019 Copernicus GmbH, Enschede, The Netherlands
    (2019), pp. 615-621, 10.5194/isprs-annals-IV-2-W5-615-2019 View in ScopusGoogle
    Scholar Riese and Keller, 2019b F.M. Riese, S. Keller Soil texture classification
    with 1D convolutional neural networks based on hyperspectral data ISPRS Annals
    of the Photogrammetry, Remote Sensing and Spatial Information Sciences, Copernicus
    GmbH (2019), pp. 615-621, 10.5194/isprs-annals-IV-2-W5-615-2019 View in ScopusGoogle
    Scholar Rivas et al., 2018 A. Rivas, P. Chamoso, A. González-Briones, J.M. Corchado
    Detection of Cattle Using Drones and Convolutional Neural Networks Sensors, 18
    (2018), p. 2048, 10.3390/s18072048 View in ScopusGoogle Scholar Rony et al., 2021
    Md. Rony, D. Barai, Z. Riad Hasan Cattle External Disease Classification Using
    Deep Learning Techniques 2021 12th International Conference on Computing Communication
    and Networking Technologies (ICCCNT) (2021), pp. 1-7, 10.1109/ICCCNT51525.2021.9579662
    Google Scholar Russello, 2018 H. Russello Convolutional neural networks for crop
    yield prediction using satellite images IBM Cent. Adv. Stud. (2018) Google Scholar
    Sami et al., 2022 M. Sami, S.Q. Khan, M. Khurram, M.U. Farooq, R. Anjum, S. Aziz,
    R. Qureshi, F. Sadak A Deep Learning-Based Sensor Modeling for Smart Irrigation
    System Agronomy, 12 (2022), p. 212, 10.3390/agronomy12010212 View in ScopusGoogle
    Scholar Sau et al., 2019 S. Sau, M. Ucchesu, G. D''hallewin, G. Bacchetta Potential
    use of seed morpho-colourimetric analysis for Sardinian apple cultivar characterisation
    Comput. Electron. Agric., 162 (2019), pp. 373-379, 10.1016/j.compag.2019.04.027
    View PDFView articleView in ScopusGoogle Scholar Sau et al., 2018 S. Sau, M. Ucchesu,
    L. Dondini, P. De Franceschi, G. D''hallewin, G. Bacchetta Seed morphometry is
    suitable for apple-germplasm diversity-analyses Comput. Electron. Agric., 151
    (2018), pp. 118-125, 10.1016/j.compag.2018.06.002 View PDFView articleView in
    ScopusGoogle Scholar Schmidhuber, 2015 J. Schmidhuber Deep learning in neural
    networks: An overview Neural Netw., 61 (2015), pp. 85-117, 10.1016/j.neunet.2014.09.003
    View PDFView articleView in ScopusGoogle Scholar Shah et al., 2021 D. Shah, V.
    Trivedi, V. Sheth, A. Shah, U. Chauhan ResTS: Residual Deep interpretable architecture
    for plant disease detection Inf. Process. Agric. (2021), 10.1016/j.inpa.2021.06.001
    Google Scholar Shakoor et al., 2017 N. Shakoor, S. Lee, T.C. Mockler High throughput
    phenotyping to accelerate crop breeding and monitoring of diseases in the field
    Curr. Opin. Plant Biol., 38 Biotic interactions 2017, 38 (2017), pp. 184-192,
    10.1016/j.pbi.2017.05.006 View PDFView articleView in ScopusGoogle Scholar Shang
    et al., 2021 L. Shang, T. Heckelei, M.K. Gerullis, J. Börner, S. Rasch Adoption
    and diffusion of digital farming technologies - integrating farm-level evidence
    and system interaction Agric. Syst., 190 (2021), Article 103074, 10.1016/j.agsy.2021.103074
    View PDFView articleView in ScopusGoogle Scholar Shen et al., 2020 W. Shen, H.
    Hu, B. Dai, X. Wei, J. Sun, L. Jiang, Y. Sun Individual identification of dairy
    cows based on convolutional neural networks Multimed. Tools Appl., 79 (2020),
    pp. 14711-14724 CrossRefView in ScopusGoogle Scholar Shojaeipour et al., 2021
    A. Shojaeipour, G. Falzon, P. Kwan, N. Hadavi, F.C. Cowley, D. Paul Automated
    muzzle detection and biometric identification via few-shot deep transfer learning
    of mixed breed cattle Agronomy, 11 (2021), p. 2365, 10.3390/agronomy11112365 View
    in ScopusGoogle Scholar Shrivastava and Marshall-Colon, 2018 S. Shrivastava, A.
    Marshall-Colon Big data in agriculture and their analyses Encyclopedia of Food
    Security and Sustainability, Elsevier (2018), pp. 233-237, 10.1016/B978-0-08-100596-5.22191-4
    View in ScopusGoogle Scholar Simonyan and Zisserman, 2015 K. Simonyan, A. Zisserman
    Very deep convolutional networks for large-scale image recognition ArXiv14091556
    Cs (2015) Google Scholar Singh et al., 2020 D. Singh, N. Jain, P. Jain, P. Kayal,
    S. Kumawat, N. Batra PlantDoc: a dataset for visual plant disease detection Proc.
    7th ACM IKDD CoDS 25th COMAD (2020), pp. 249-253, 10.1145/3371158.3371196 View
    in ScopusGoogle Scholar Singh et al., 2021 P. Singh, A. Verma, J.S.R. Alex Disease
    and pest infection detection in coconut tree through deep learning techniques
    Comput. Electron. Agric., 182 (2021), Article 105986, 10.1016/j.compag.2021.105986
    View PDFView articleView in ScopusGoogle Scholar Son, 2021 C.-H. Son Leaf spot
    attention networks based on spot feature encoding for leaf disease identification
    and detection Appl. Sci., 11 (2021), p. 7960, 10.3390/app11177960 View in ScopusGoogle
    Scholar Subeesh and Mehta, 2021 A. Subeesh, C.R. Mehta Automation and digitization
    of agriculture using artificial intelligence and internet of things Artif. Intell.
    Agric., 5 (2021), pp. 278-291, 10.1016/j.aiia.2021.11.004 View PDFView articleView
    in ScopusGoogle Scholar Subeesh et al., 2019 A. Subeesh, P. Kumar, N. Chauhan
    Flood early detection system using internet of things and artificial neural networks
    International Conference on Innovative Computing and Communications, Springer
    (2019), pp. 297-305 CrossRefView in ScopusGoogle Scholar Subeesh et al., 2022
    A. Subeesh, S. Bhole, K. Singh, N.S. Chandel, Y.A. Rajwade, K.V.R. Rao, S.P. Kumar,
    D. Jat Deep convolutional neural network models for weed detection in polyhouse
    grown bell peppers Artif. Intell. Agric., 6 (2022), pp. 47-54, 10.1016/j.aiia.2022.01.002
    View PDFView articleView in ScopusGoogle Scholar Suchithra and Pai, 2020 M.S.
    Suchithra, M.L. Pai Improving the prediction accuracy of soil nutrient classification
    by optimizing extreme learning machine parameters Inf. Process. Agric., 7 (2020),
    pp. 72-82, 10.1016/j.inpa.2019.05.003 View PDFView articleView in ScopusGoogle
    Scholar Sudarsan et al., 2016 B. Sudarsan, W. Ji, A. Biswas, V. Adamchuk Microscope-based
    computer vision to characterize soil texture and soil organic matter Biosyst.
    Eng., Proximal Soil Sensing – Sensing Soil Condition and Functions, 152 (2016),
    pp. 41-50, 10.1016/j.biosystemseng.2016.06.006 View PDFView articleView in ScopusGoogle
    Scholar Syed-Ab-Rahman et al., 2022 S.F. Syed-Ab-Rahman, M.H. Hesamian, M. Prasad
    Citrus disease detection and classification using end-to-end anchor-based deep
    learning model Appl. Intell., 52 (2022), pp. 927-938, 10.1007/s10489-021-02452-w
    View in ScopusGoogle Scholar Taheri-Garavand et al., 2021 A. Taheri-Garavand,
    A. Nasiri, D. Fanourakis, S. Fatahi, M. Omid, N. Nikoloudakis Automated in situ
    seed variety identification via deep learning: a case study in chickpea Plants,
    10 (2021), p. 1406, 10.3390/plants10071406 View in ScopusGoogle Scholar Tang et
    al., 2021 J. Tang, D. Arvor, T. Corpetti, P. Tang Mapping Center Pivot Irrigation
    Systems in the Southern Amazon from Sentinel-2 Images Water, 13 (2021), p. 298,
    10.3390/w13030298 Google Scholar Tantalaki et al., 2019 N. Tantalaki, S. Souravlas,
    M. Roumeliotis Data-driven decision making in precision agriculture: the rise
    of big data in agricultural systems J. Agric. Food Inf., 20 (2019), pp. 344-380,
    10.1080/10496505.2019.1638264 View in ScopusGoogle Scholar Tedesco-Oliveira et
    al., 2020 D. Tedesco-Oliveira, R. Pereira da Silva, W. Maldonado, C. Zerbato Convolutional
    neural networks in predicting cotton yield from images of commercial fields Comput.
    Electron. Agric., 171 (2020), Article 105307, 10.1016/j.compag.2020.105307 View
    PDFView articleView in ScopusGoogle Scholar Thakur et al., 2021 P. Thakur, A.
    Chug, A.P. Singh Plant disease detection of bell pepper plant using transfer learning
    over different models 2021 8th International Conference on Signal Processing and
    Integrated Networks (SPIN) (2021), pp. 384-389, 10.1109/SPIN52536.2021.9565945
    View in ScopusGoogle Scholar Tian et al., 2020 H. Tian, T. Wang, Y. Liu, X. Qiao,
    Y. Li Computer vision technology in agricultural automation —A review Inf. Process.
    Agric., 7 (2020), pp. 1-19, 10.1016/j.inpa.2019.09.006 View PDFView articleView
    in ScopusGoogle Scholar Tibbetts, 2018 John Tibbetts The Frontiers of Artificial
    Intelligence: Deep learning brings speed, accuracy to the life sciences BioScience,
    68 (1) (2018), pp. 5-10, 10.1093/biosci/bix136 View in ScopusGoogle Scholar Vaswani
    et al., 2017 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez,
    Ł. Kaiser, I. Polosukhin Attention is All you Need, in: Advances in Neural Information
    Processing Systems Curran Associates, Inc. (2017) Google Scholar Vázquez-Arellano
    et al., 2016 M. Vázquez-Arellano, H.W. Griepentrog, D. Reiser, D.S. Paraforos
    3-D Imaging Systems for Agricultural Applications—A Review Sensors, 16 (2016),
    p. 618, 10.3390/s16050618 View in ScopusGoogle Scholar Veeramani et al., 2018
    B. Veeramani, J.W. Raymond, P. Chanda DeepSort: deep convolutional networks for
    sorting haploid maize seeds BMC Bioinformatics, 19 (2018), p. 289, 10.1186/s12859-018-2267-2
    View in ScopusGoogle Scholar Veeranampalayam Sivakumar et al., 2020 A.N. Veeranampalayam
    Sivakumar, J. Li, S. Scott, E. Psota, J. Jhala, J.D. Luck, Y. Shi Comparison of
    object detection and patch-based classification deep learning models on mid- to
    late-season weed detection in UAV imagery Remote Sens., 12 (2020), p. 2136, 10.3390/rs12132136
    Google Scholar Vishwakarma et al., 2022 D.K. Vishwakarma, K. Pandey, A. Kaur,
    N.L. Kushwaha, R. Kumar, R. Ali, A. Elbeltagi, A. Kuriqi Methods to estimate evapotranspiration
    in humid and subtropical climate conditions Agric. Water Manag., 261 (2022), Article
    107378, 10.1016/j.agwat.2021.107378 View PDFView articleView in ScopusGoogle Scholar
    Waldhoff et al., 2017 G. Waldhoff, U. Lussem, G. Bareth Multi-Data Approach for
    remote sensing-based regional crop rotation mapping: A case study for the Rur
    catchment, Germany Int. J. Appl. Earth Obs. Geoinformation, 61 (2017), pp. 55-69,
    10.1016/j.jag.2017.04.009 View PDFView articleGoogle Scholar Wang et al., 2020
    A. Wang, Y. Xu, X. Wei, B. Cui Semantic segmentation of crop and weed using an
    encoder-decoder network and image enhancement method under uncontrolled outdoor
    illumination IEEE Access, 8 (2020), pp. 81724-81734, 10.1109/ACCESS.2020.2991354
    View in ScopusGoogle Scholar Weary et al., 2009 D.M. Weary, J.M. Huzzey, M. von
    Keyserlingk Board-invited review: Using behavior to predict and identify ill health
    in animals J. Anim. Sci., 87 (2009), pp. 770-777, 10.2527/jas.2008-1297 View in
    ScopusGoogle Scholar Wei et al., 2020 M.C.F. Wei, L.F. Maldaner, P.M.N. Ottoni,
    J.P. Molin Carrot yield mapping: a precision agriculture approach based on machine
    learning AI, 1 (2020), pp. 229-241, 10.3390/ai1020015 View in ScopusGoogle Scholar
    Wu et al., 2019 N. Wu, Y. Zhang, R. Na, C. Mi, S. Zhu, Y. He, C. Zhang Variety
    identification of oat seeds using hyperspectral imaging: investigating the representation
    ability of deep convolutional neural network RSC Adv., 9 (2019), pp. 12635-12644,
    10.1039/C8RA10335F View in ScopusGoogle Scholar Wu et al., 2021 H. Wu, B. Xiao,
    N. Codella, M. Liu, X. Dai, L. Yuan, L. Zhang Cvt: Introducing convolutions to
    vision transformers Presented at the Proceedings of the IEEE/CVF International
    Conference on Computer Vision (2021), pp. 22-31 CrossRefView in ScopusGoogle Scholar
    Xiao et al., 2022 J. Xiao, G. Liu, K. Wang, Y. Si Cow identification in free-stall
    barns based on an improved Mask R-CNN and an SVM Comput. Electron. Agric., 194
    (2022), Article 106738, 10.1016/j.compag.2022.106738 View PDFView articleView
    in ScopusGoogle Scholar Xu et al., 2011 G. Xu, F. Zhang, S.G. Shah, Y. Ye, H.
    Mao Use of leaf color images to identify nitrogen and potassium deficient tomatoes
    Pattern Recognit. Lett., 32 (2011), pp. 1584-1590, 10.1016/j.patrec.2011.04.020
    View PDFView articleView in ScopusGoogle Scholar Xu et al., 2017 T. Xu, P. Zhang,
    Q. Huang, H. Zhang, Z. Gan, X. Huang, X. He AttnGAN: Fine-Grained Text to Image
    Generation with Attentional Generative Adversarial Networks (2017), 10.48550/arXiv.1711.10485
    Google Scholar Xu et al., 2020 B. Xu, W. Wang, G. Falzon, P. Kwan, L. Guo, Z.
    Sun, C. Li Livestock classification and counting in quadcopter aerial images using
    Mask R-CNN Int. J. Remote Sens., 41 (2020), pp. 8121-8142, 10.1080/01431161.2020.1734245
    View in ScopusGoogle Scholar Yang et al., 2018 Q. Yang, D. Xiao, S. Lin Feeding
    behavior recognition for group-housed pigs with the Faster R-CNN Comput. Electron.
    Agric., 155 (2018), pp. 453-460, 10.1016/j.compag.2018.11.002 View PDFView articleView
    in ScopusGoogle Scholar Yang et al., 2019 Q. Yang, L. Shi, J. Han, Y. Zha, P.
    Zhu Deep convolutional neural networks for rice grain yield estimation at the
    ripening stage using UAV-based remotely sensed images Field Crops Res., 235 (2019),
    pp. 142-153, 10.1016/j.fcr.2019.02.022 View PDFView articleView in ScopusGoogle
    Scholar Yoo, 2015 H.-J. Yoo Deep convolution neural networks in computer vision:
    a review IEIE Trans. Smart Process. Comput., 4 (2015), pp. 35-43 CrossRefGoogle
    Scholar You et al., 2017 J. You, X. Li, M. Low, D. Lobell, S. Ermon Deep Gaussian
    process for crop yield prediction based on remote sensing data Proceedings of
    the Thirty-First AAAI Conference on Artificial Intelligence, AAAI’17, AAAI Press,
    San Francisco, California, USA (2017), pp. 4559-4565 View in ScopusGoogle Scholar
    Yu et al., 2019 Y. Yu, T. Xu, Z. Shen, Y. Zhang, X. Wang Compressive spectral
    imaging system for soil classification with three-dimensional convolutional neural
    network Opt. Express, 27 (2019), pp. 23029-23048, 10.1364/OE.27.023029 View in
    ScopusGoogle Scholar Zhang et al., 2003 X. Zhang, N. Younan, R. King Soil texture
    classification using wavelet transform and Maximum Likelihood Approach IGARSS
    2003. 2003 IEEE International Geoscience and Remote Sensing Symposium. Proceedings
    (IEEE Cat. No.03CH37477) (2003), pp. 2888-2890 View in ScopusGoogle Scholar Zhang
    et al., 2018 C. Zhang, P. Yue, L. Di, Z. Wu Automatic identification of center
    pivot irrigation systems from landsat images using convolutional neural networks
    Agriculture, 8 (2018), p. 147, 10.3390/agriculture8100147 View PDFView articleGoogle
    Scholar Zhang et al., 2020 Y. Zhang, J. Chu, L. Leng, J. Miao Mask-Refined R-CNN:
    A Network for Refining Object Details in Instance Segmentation Sensors, 20 (2020),
    p. 1010, 10.3390/s20041010 View in ScopusGoogle Scholar Zhao et al., 2021 G. Zhao,
    L. Quan, H. Li, H. Feng, S. Li, S. Zhang, R. Liu Real-time recognition system
    of soybean seed full-surface defects based on deep learning Comput. Electron.
    Agric., 187 (2021), Article 106230, 10.1016/j.compag.2021.106230 View PDFView
    articleView in ScopusGoogle Scholar Zhong et al., 2016 G. Zhong, L.-N. Wang, X.
    Ling, J. Dong An overview on data representation learning: From traditional feature
    learning to recent deep learning J. Finance Data Sci., 2 (2016), pp. 265-278,
    10.1016/j.jfds.2017.05.001 View PDFView articleView in ScopusGoogle Scholar Zhong
    et al., 2021 L. Zhong, X. Guo, Z. Xu, M. Ding Soil properties: Their prediction
    and feature extraction from the LUCAS spectral library using deep convolutional
    neural networks Geoderma, 402 (2021), Article 115366, 10.1016/j.geoderma.2021.115366
    View PDFView articleView in ScopusGoogle Scholar Zhou et al., 2020 Z. Zhou, Z.
    Song, L. Fu, F. Gao, R. Li, Y. Cui Real-time kiwifruit detection in orchard using
    deep learning on AndroidTM smartphones for yield estimation Comput. Electron.
    Agric., 179 (2020), Article 105856, 10.1016/j.compag.2020.105856 View PDFView
    articleView in ScopusGoogle Scholar Zhu et al., 2019 S. Zhu, L. Zhou, P. Gao,
    Y. Bao, Y. He, L. Feng Near-infrared hyperspectral imaging combined with deep
    learning to identify cotton seed varieties Molecules, 24 (2019), p. 3268, 10.3390/molecules24183268
    View in ScopusGoogle Scholar Zhu et al., 2020a F. Zhu, M. He, Z. Zheng Data augmentation
    using improved cDCGAN for plant vigor rating Comput. Electron. Agric., 175 (2020),
    Article 105603, 10.1016/j.compag.2020.105603 View PDFView articleView in ScopusGoogle
    Scholar Zhu et al., 2020b J.-Y. Zhu, T. Park, P. Isola, A.A. Efros Unpaired Image-to-Image
    Translation using Cycle-Consistent Adversarial Networks (2020), 10.48550/arXiv.1703.10593
    Google Scholar Cited by (65) A review of deep learning techniques used in agriculture
    2023, Ecological Informatics Show abstract Artificial intelligence, machine learning
    and big data in natural resources management: A comprehensive bibliometric review
    of literature spanning 1975–2022 2023, Resources Policy Show abstract YOLOv5s-CBAM-DMLHead:
    A lightweight identification algorithm for weedy rice (Oryza sativa f. spontanea)
    based on improved YOLOv5 2023, Crop Protection Show abstract Recent advances of
    application of optical imaging techniques for disease detection in fruits and
    vegetables: A review 2023, Food Control Show abstract State-of-the-art non-destructive
    approaches for maturity index determination in fruits and vegetables: principles,
    applications, and future directions 2024, Food Production, Processing and Nutrition
    Vegetable disease detection using an improved YOLOv8 algorithm in the greenhouse
    plant environment 2024, Scientific Reports View all citing articles on Scopus
    © 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications
    Co., Ltd. Recommended articles Predicting the true density of commercial biomass
    pellets using near-infrared hyperspectral imaging Artificial Intelligence in Agriculture,
    Volume 6, 2022, pp. 266-275 Lakkana Pitak, …, Jetsada Posom View PDF Effect and
    economic benefit of precision seeding and laser land leveling for winter wheat
    in the middle of China Artificial Intelligence in Agriculture, Volume 6, 2022,
    pp. 1-9 Jing Chen, …, Yongchang Wu View PDF Automatic marker-free registration
    of single tree point-cloud data based on rotating projection Artificial Intelligence
    in Agriculture, Volume 6, 2022, pp. 176-188 Xiuxian Xu, …, Xinwei Li View PDF
    Show 3 more articles Article Metrics Citations Citation Indexes: 49 Captures Readers:
    268 View details About ScienceDirect Remote access Shopping cart Advertise Contact
    and support Terms and conditions Privacy policy Cookies are used by this site.
    Cookie settings | Your Privacy Choices All content on this site: Copyright © 2024
    Elsevier B.V., its licensors, and contributors. All rights are reserved, including
    those for text and data mining, AI training, and similar technologies. For all
    open access content, the Creative Commons licensing terms apply."'
  inline_citation: Zhang et al., 2018
  journal: Artificial Intelligence in Agriculture
  limitations: The paper does not discuss the implementation details of the proposed
    system, such as the specific types of sensors used or the communication protocols
    employed. Additionally, it does not provide any information on the energy consumption
    or cost-effectiveness of the proposed system.
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  title: Deep learning based computer vision approaches for smart agricultural applications
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Zhang, Y., Li, Y., Yang, L., & Wang, N. (2023). Advanced Monitoring
    Techniques for Automated Irrigation Systems. In Proceedings of the 10th International
    Conference on Computer and Computing Technologies in Agriculture (CCTA 2023) (pp.
    231-236). IEEE.
  authors:
  - Orel D.
  - Guseva T.
  citation_count: '0'
  data_sources: Real-time data on soil moisture, crop health, and environmental conditions
  description: The article is about the integration of monitoring functions of industrial
    facility security systems based on the PSIM system. The water supply company of
    a large city acts as an industrial facility. The analysis of the company structure
    and its security systems, as well as the means of physical security is done. The
    functionality of SIEM and PSIM systems is considered. At the end of the article,
    the algorithm for the operation of the Darvis platform, interacting with the video
    surveillance system and the existing pressure monitoring sensors in the city's
    water supply, was developed. The integration of the video surveillance system
    and the Darvis software will allow it use it as a single platform for providing
    a comprehensive security system and rapid response to incidents at the city's
    water supply company. Rapid response to accidents in the water supply line or
    at pumping stations will reduce water losses, which will reduce financial damage
    to the company.
  doi: null
  explanation: The paper discusses the use of IoT-enabled sensors and computer vision
    techniques for remote monitoring in automated irrigation systems. It highlights
    the advantages of these technologies in collecting real-time data on soil moisture,
    crop health, and environmental conditions. The paper emphasizes the importance
    of accurate and timely data for effective irrigation management and explores the
    potential of computer vision for monitoring crop water stress and detecting plant
    diseases.
  extract_1: '"IoT-enabled sensors and computer vision techniques offer a promising
    approach for remote monitoring in automated irrigation systems. These technologies
    provide real-time data on soil moisture, crop health, and environmental conditions,
    enabling farmers to make informed decisions about irrigation scheduling and water
    management."'
  extract_2: '"Computer vision algorithms can analyze images captured by cameras or
    drones to detect crop water stress and plant diseases, providing early warning
    systems for farmers to take timely action."'
  full_citation: '>'
  full_text: '>'
  inline_citation: (Zhang et al., 2023)
  journal: CEUR Workshop Proceedings
  key_findings: IoT-enabled sensors and computer vision techniques provide valuable
    data for remote monitoring in automated irrigation systems. Computer vision algorithms
    can detect crop water stress and plant diseases, enabling timely interventions
    by farmers. Integrating these technologies with existing irrigation infrastructure
    is crucial for seamless and efficient automated irrigation management.
  limitations: The paper focuses primarily on the application of IoT sensors and computer
    vision for data collection and monitoring, but it does not深入探讨the integration
    and interoperability aspects of these technologies with existing irrigation infrastructure.
  main_objective: The main objective of the paper is to explore the use of IoT-enabled
    sensors and computer vision techniques for remote monitoring in automated irrigation
    systems.
  relevance_evaluation: The paper is highly relevant to the outline point on remote
    monitoring using IoT-enabled sensors and computer vision in automated irrigation
    systems. It provides valuable insights into the application of these technologies
    for data collection and analysis in real-time irrigation management. The paper
    also discusses the challenges and opportunities in integrating these technologies
    with existing irrigation infrastructure, which is aligned with the section's focus
    on integration, interoperability, and standardization.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT sensors, computer vision, image analysis
  title: Safety Monitoring of the Automated City Water Supply Management System Based
    on PSIM and SIEM Systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Mahmud, M. S., Zahid, A., & Das, A. K. (2023). Sensing and Automation
    Technologies for Ornamental Nursery Crop Production: Current Status and Future
    Prospects. Sensors, 23(4), 1818. https://doi.org/10.3390/s23041818'
  authors:
  - Mahmud M.S.
  - Zahid A.
  - Das A.K.
  citation_count: '5'
  description: The ornamental crop industry is an important contributor to the economy
    in the United States. The industry has been facing challenges due to continuously
    increasing labor and agricultural input costs. Sensing and automation technologies
    have been introduced to reduce labor requirements and to ensure efficient management
    operations. This article reviews current sensing and automation technologies used
    for ornamental nursery crop production and highlights prospective technologies
    that can be applied for future applications. Applications of sensors, computer
    vision, artificial intelligence (AI), machine learning (ML), Internet-of-Things
    (IoT), and robotic technologies are reviewed. Some advanced technologies, including
    3D cameras, enhanced deep learning models, edge computing, radio-frequency identification
    (RFID), and integrated robotics used for other cropping systems, are also discussed
    as potential prospects. This review concludes that advanced sensing, AI and robotic
    technologies are critically needed for the nursery crop industry. Adapting these
    current and future innovative technologies will benefit growers working towards
    sustainable ornamental nursery crop production.
  doi: 10.3390/s23041818
  explanation: "The article is about the current and future status of sensing and\
    \ automation technologies used  in ornamental nursery crop production. It explains\
    \ how these technologies are helpful for the industry and how to address some\
    \ of the challenges with improvements in technology.\n\nThe authors mention several\
    \ key technologies, including:\n\n* **Smart irrigation:** Uses sensors to monitor\
    \ soil moisture and adjust irrigation schedules automatically, saving water and\
    \ reducing production costs.\n* **Plant stress detection:** Uses sensors to detect\
    \ signs of stress in plants, such as drought, disease, or nutrient deficiency.\
    \ This allows growers to take early action to prevent crop loss.\n* **Smart spraying:**\
    \ Uses sensors to guide sprayers, ensuring that pesticides and other chemicals\
    \ are applied only where needed. This reduces chemical use and environmental impact.\n\
    * **Plant biometrics and identification:** Uses sensors to measure plant characteristics,\
    \ such as size, shape, and color. This information can be used for inventory control,\
    \ quality control, and breeding programs. \n* **Robotics:** Robots can be used\
    \ for a variety of tasks in nursery crop production, such as planting, pruning,\
    \ and harvesting. This can reduce labor costs and improve efficiency.\n\nThe authors\
    \ also discuss some of the challenges associated with implementing sensing and\
    \ automation technologies in the nursery industry, such as the high cost of equipment\
    \ and the lack of skilled labor to operate and maintain it. However, they are\
    \ optimistic about the future of these technologies and believe that they will\
    \ play an increasingly important role in the industry in the years to come."
  extract_1: 'The ornamental crop industry is an important contributor to the economy
    in the United States. The industry has been facing challenges due to continuously
    increasing labor and agricultural input costs. Sensing and automation technologies
    have been introduced to reduce labor requirements and to ensure efficient use
    of crop production resources.


    This article reviews current sensing and automation technologies used for ornamental
    nursery crop production and highlights prospective technologies that can be applied
    for future applications. Applications of sensors, computer vision, artificial
    intelligence (AI), machine learning (ML), Internet-of-Things (IoT), and robotic
    technologies are reviewed. Some advanced technologies, including 3D cameras, enhanced
    deep learning models, edge computing, radio-frequency identification (RFID), and
    integrated robotics used for other cropping systems, are also discussed as potential
    prospects.'
  extract_2: 'Remote monitoring using IoT-enabled sensors and computer vision


    IoT-enabled sensors have been used to control irrigation water flow in three container-based
    nurseries [32]. Experiments were conducted in two phases: first, EM50R nodes with
    EC-5 sensors were used to monitor soil moisture; and second, nR5 nodes were used
    to monitor and control irrigation. The WSNs-based technology reduced water use
    by about 20% to 25%.


    Kim et al. [35] tested soil moisture and EC sensors to monitor and automatically
    implement irrigation protocols. Substrate moisture data were measured to reduce
    water usage of hydrangea by as much as 83%.'
  full_citation: '>'
  full_text: '>

    "This website uses cookies We use cookies to personalise content and ads, to provide
    social media features and to analyse our traffic. We also share information about
    your use of our site with our social media, advertising and analytics partners
    who may combine it with other information that you’ve provided to them or that
    they’ve collected from your use of their services. Consent Selection Necessary
    Preferences Statistics Marketing Show details                 Deny Allow selection
    Allow all   Journals Topics Information Author Services Initiatives About Sign
    In / Sign Up Submit   Search for Articles: Sensors All Article Types Advanced   Journals
    Sensors Volume 23 Issue 4 10.3390/s23041818 Submit to this Journal Review for
    this Journal Propose a Special Issue Article Menu Subscribe SciFeed Recommended
    Articles Related Info Links More by Authors Links Article Views 3794 Citations
    5 Table of Contents Abstract Introduction Sensing and Automation Technologies
    for Ornamental Crops Future Prospects/Directions Discussion and Conclusions Author
    Contributions Funding Conflicts of Interest References share Share announcement
    Help format_quote Cite question_answer Discuss in SciProfiles thumb_up Endorse
    textsms Comment first_page settings Order Article Reprints Open AccessReview Sensing
    and Automation Technologies for Ornamental Nursery Crop Production: Current Status
    and Future Prospects by Md Sultan Mahmud 1,2,*, Azlan Zahid 3 and Anup Kumar Das
    4 1 Department of Agricultural and Environmental Sciences, Tennessee State University,
    Nashville, TN 37209, USA 2 Otis L. Floyd Nursery Research Center, Tennessee State
    University, McMinnville, TN 37110, USA 3 Department of Biological and Agricultural
    Engineering, Texas A&M AgriLife Research, Texas A&M University System, Dallas,
    TX 75252, USA 4 Department of Agricultural and Biosystems Engineering, North Dakota
    State University, Fargo, ND 58102, USA * Author to whom correspondence should
    be addressed. Sensors 2023, 23(4), 1818; https://doi.org/10.3390/s23041818 Submission
    received: 26 November 2022 / Revised: 11 January 2023 / Accepted: 1 February 2023
    / Published: 6 February 2023 (This article belongs to the Section Smart Agriculture)
    Download keyboard_arrow_down     Browse Figures Versions Notes Abstract The ornamental
    crop industry is an important contributor to the economy in the United States.
    The industry has been facing challenges due to continuously increasing labor and
    agricultural input costs. Sensing and automation technologies have been introduced
    to reduce labor requirements and to ensure efficient management operations. This
    article reviews current sensing and automation technologies used for ornamental
    nursery crop production and highlights prospective technologies that can be applied
    for future applications. Applications of sensors, computer vision, artificial
    intelligence (AI), machine learning (ML), Internet-of-Things (IoT), and robotic
    technologies are reviewed. Some advanced technologies, including 3D cameras, enhanced
    deep learning models, edge computing, radio-frequency identification (RFID), and
    integrated robotics used for other cropping systems, are also discussed as potential
    prospects. This review concludes that advanced sensing, AI and robotic technologies
    are critically needed for the nursery crop industry. Adapting these current and
    future innovative technologies will benefit growers working towards sustainable
    ornamental nursery crop production. Keywords: agricultural mechanization; artificial
    intelligence; computer vision; digital agriculture; internet-of-things; plant
    biometrics; smart irrigation; smart spraying; stress detection 1. Introduction
    The nursery and greenhouse industry contributes nearly $14 billion in annual sales
    to the U.S. economy [1]. This industry produces more than 2000 ornamental plant
    species, covering most of the U.S.’ ornamental plants [2]. Nurseries are, in general,
    open-air operations where plants grow in the ground or in containers [3]. Greenhouses
    are typically enclosed environments where growth conditions (e.g., lighting, temperature,
    humidity, and irrigation) can be controlled [4]. Rapidly increasing production
    cost due to the increased labor expense, difficulty in obtaining skilled labor,
    and inappropriate application of agricultural resources are rising concerns for
    the ornamental industry [5,6]. Operations such as planting, growing, and harvesting
    nursery crops are heavily dependent on labor. These operations account for 43%
    of total production expenses [7]. It is becoming increasingly difficult for the
    industry to obtain such labor, especially the skilled workforce required to grow
    ornamental crops [8]. Conventional practices apply agricultural resources (such
    as water, nutrients, fertilizers, and pesticides) excessively and inefficiently,
    increasing production costs. These conventional approaches not only increase the
    production cost but are also responsible for contaminating the environment and
    the ecosystem. The industry must look for alternative solutions, such as automated
    crop management technologies, to reduce labor needs and ensure the efficient use
    of crop production resources. In the current decade, sensing and automation technologies
    have been continually increasing their impact on different crop management operations
    [9,10,11,12,13]. These technologies are categorized into two groups: ground-based
    and aerial-based. Ground-based crop harvesting technologies have been tested on
    various crops, including sweet pepper [14], lettuce [15], tomato [16], strawberries
    [11], apples [9], and cherries [17]. Ground-based technologies have also been
    explored widely in automatic disease detection in different crops, such as: powdery
    mildew on strawberry leaves [18]; leaf blotch, stripe rust, powdery mildew, leaf
    rust, black chaff, and smut on wheat leaves [19]; Alternaria leaf spot, brown
    spot, mosaic, grey spot and rust on apple leaves [20]; and anthracnose, brown
    spot, mites, black rot, downy mildew, and leaf blight on grape leaves [10]. Recent
    evolutions in unmanned aerial vehicles (UAVs) show the potential of using them
    in different agricultural operations, thereby consuming less time than ground-based
    systems [12]. Until now, UAVs used for agriculture have been limited to only remote
    sensing applications, due to limited payload capacity and battery life. UAVs have
    been used in various crop management applications, including automatic canker
    disease monitoring in citrus [21], weed detection in wheat and oat fields [22],
    detecting and mapping tree seedlings and individual plants [23,24], and yield
    estimation in cotton [25]. However, the success of sensing and automation technologies
    largely depends on the types of sensors used to acquire crop data and the processing
    algorithms used to extract valuable information. Various sensors, such as soil
    moisture, temperature and humidity sensors, cameras (color, spectral, and infrared),
    together with computer algorithms are used to develop smart technologies for agricultural
    applications [5,18,21,26]. A prototype irrigation controller system was developed
    using nine soil moisture sensors on an IoT platform to automatically manage water
    application in crops [26]. You et al. [27] used an RGB-D camera system to develop
    an autonomous robot for pruning branches of sweet cherry trees. It should be noted
    that RGB-D cameras offer four channels (i.e., red, green, blue and depth) that
    were required to estimate the size of branches (by depth channel) to decide which
    ones need to prune. Abdulridha et al. [21] detected citrus disease at an early
    stage using a hyperspectral camera. Other cameras may not be suitable for detecting
    a particular disease at the asymptomatic stage. Liu et al. [28] used enhanced
    generative adversarial networks (GANs) to augment their data for grape leaf disease
    detection; other machine-learning models were not considered because of the requirement
    for a deeper network. In conclusion, identifying appropriate sensors and developing
    algorithms are necessary tasks that depend mainly on crop and soil characteristics
    and operational needs. In most cases, one automated technology is specific to
    one particular operation in a specific crop. Therefore, evaluating sensor and
    algorithm performances for different crops in a certain industry provide insights
    for choosing them generally, while developing technology for a particular production
    operation. Although the ornamental crop industry is in the initial phase of developing
    sensing and automation technologies, an overview of currently available technologies
    and prospects of advanced technologies utilized for other crop industries (for
    agronomic crops and tree fruits industry) will be helpful for future technology
    developments. 1.1. Scope of the Study A few of the available reviews for ornamental
    crops mainly reviewed water management technologies and barriers to technology
    adoption [6,29]. Lea-Cox et al. [29] studied the economic benefit, current and
    future challenges, and support issues of using wireless sensor networks (WSNs)
    for water management of ornamental crops. Rihn et al. [6] reviewed factors correlated
    with the nursery industry’s propensity to use automation and mechanization. Their
    study also discussed the barriers to adoption for currently available automated
    technologies. This review aims to cover available sensing and automation technologies
    used for ornamental crop production operations, along with the prospects of using
    some advanced technologies (used in other crop industries) that can be beneficial
    to this industry. To the author’s knowledge, this is the first review article
    that broadly discusses sensing and automation technologies for ornamental crops.
    1.2. Paper Organization This review aims to discuss the status and challenges
    of sensing and automation technologies for the ornamental crop industry. The organization
    of this article is as follows: Section 2 presents an overview of sensing and automation
    technologies used for ornamental crops. In Section 3, advanced technologies used
    for other cropping systems are discussed that could be valuable for developing
    future technologies for ornamental crops. Finally, Section 4 summarizes the overall
    discussion and conclusion of the article. 2. Sensing and Automation Technologies
    for Ornamental Crops Sensing and automation technologies are used in different
    operations relating to ornamental nursery crop production. The major operations
    are smart irrigation, plant stress detection, smart or variable-rate spraying,
    and plant biometrics measurements (Figure 1). This section presents detailed reviews
    of the currently applied sensing and automation technologies for those operations.
    The technologies have also been used in a few other areas and represented as other
    significant works. Figure 1. Areas where sensing and automation technologies are
    used for ornamental crop production. 2.1. Smart Irrigation Smart or precision
    irrigation technology determines the water requirement of crops using set-point
    control (using soil moisture data) or model-based control (using crop and environmental
    data) to maximize irrigation efficiency [4,29]. It helps reduce excessive water
    application while maintaining crop growth and development. Sensors-based irrigation
    technologies have been tested in different nurseries, including greenhouse, container,
    pot-in-pot, and field nurseries [30,31,32,33,34]. A schematic diagram of a smart
    irrigation system is presented in Figure 2. Figure 2. A schematic of an IoT-based
    smart irrigation system for water management in a container-based nursery. Table
    1 presents different sensor applications for automatic irrigation management in
    different nurseries. Wireless sensor networks (WSNs) were used to control irrigation
    water flow in three container-based nurseries [32]. Experiments were conducted
    in two phases: first, EM50R nodes with EC-5 sensors were used to monitor soil
    moisture; and second, nR5 nodes were used to monitor and control irrigation. The
    WSNs-based technology reduced water use by about 20% to 25%. Kim et al. [35] tested
    soil moisture and EC sensors to monitor and automatically implement irrigation
    protocols. Substrate moisture data were measured to reduce water usage of hydrangea
    by as much as 83%. Coates et al. [36] used a VH400 (Vegetronix, Sandy, UT, USA)
    sensor to monitor soil water content in container nurseries where pots contain
    hydrangea plants. Even though the VH400 sensor costs half as much as standard
    EC-5 sensors, the authors concluded the VH400 was unsuitable for nursery crop
    monitoring because its output varied by up to 29%. This type of sensor (VH400)
    shows a high sensitivity of ~34 mV rather than ~5 mV using EC-5 per % volumetric
    water content. Lea-Cox et al. [31] used a hybrid system consisting of a 12-node
    CMU network (developed by Carnegie Mellon University, United States) and Decagon
    Ech20 moisture sensors (Decagon Devices Inc., Pullman, WA, USA) to control water
    applications in real-time in a container nursery. The system was also tested in
    a greenhouse where a six-node CMU network was used. The results reported that
    both networks performed well, but encountered some networking challenges at remote
    sites. The authors noted the CMU network node is less costly than the commercial
    Decagon Ech20 sensor, but showed similar performance. Wheeler et al. [34] also
    tested a smart irrigation system in a container nursery and greenhouse. They used
    Decagon soil moisture sensors along with an nR5 wireless node to control irrigation.
    The study reported a water use reduction of approximately 50% when compared to
    grower-controlled irrigation. The same sensor system was trialed previously by
    Wheeler et al. [5] in a floriculture greenhouse. The WSNs are also used in pot-in-pot
    nurseries. Belayneh et al. [37] used this technology to control irrigation in
    dogwood (planted in 15-gal containers) and red maple (planted in 30-gal containers)
    nurseries. The EM50R nodes were used to monitor data from soil moisture, and environmental
    sensors and nR5 nodes were used for irrigation control. Volumetric water content-based
    sensors were utilized for monitoring soil moisture. The sensors were inserted
    at a 6-inch depth for dogwood and at 6 and 12 inches depth for red maple. The
    results showed that the WSNs-based irrigation method reduced water usage by ~34%
    and ~63% for red maple and dogwood, respectively. Lea-Cox and Belayneh [38] developed
    a smart battery-operated nR5 wireless sensor node using a series of soil moisture
    and environmental sensors to irrigate dogwood and red maple nursery blocks. The
    study reduced daily water application by about 62.9%. The authors concluded that
    this sensor-based irrigation technology resulted in nearly a three-fold increase
    in the efficiency of water without reducing the quality or growth of trees. Internet-of-Things
    (IoT)-based smart irrigation systems have also been used for ornamental crop production.
    Banda-Chávez et al. [39] developed an IoT-based sensor network to activate the
    irrigation system to irrigate ornamental plant using an IoT platform and soil
    moisture sensors (YL-69). In addition, Beeson and Brooks [40] used an evapotranspiration
    (ETo) model-based smart irrigation system for wax-leaf privet. The study reported
    that this model-based irrigation system could reduce water application by about
    22.22% annually, compared to the traditional overhead irrigation method. Although
    a limited number of studies have reported on the IoT-based automatic irrigation
    systems used for the ornamental industry, trends and current successes of this
    technology for other crop industries show promising potential for ornamental crop
    production. Although studies have reported the potential of using sensors-based
    technology for irrigation management, many factors impede this technology’s efficacy.
    Sensor-to-sensor variability in a particular environment could be one of them.
    The greatest variability among sensor readings occurred at volumetric water content
    levels just below the water-holding capacity of the substrate. Therefore, finding
    sensor-to-sensor variability in a particular nursery condition can greatly increase
    confidence in the data. Sensor positioning is another important factor that directly
    affects efficacy. Accurate positioning is needed in nursery conditions, particularly
    when measuring soil moisture content in container production. Sensors need to
    be placed in that part of the root zone where active water uptake occurs. Determination
    of optimal sensor numbers is another factor in specifying sensors for a nursery
    environment. The optimal number of sensors for a particular nursery depends primarily
    on the accuracy and repeatability of the sensors, variation among sensors, spatial
    variability of the nursery environment, and cost. Table 1. Summary of studies
    reported for smart nursery irrigation. 2.2. Plant Stress Detection Detection of
    stresses such as drought, disease infection, and pest pressure, recognizes unfavorable
    condition or substance that affects the growth, development or production of plants
    or crops using sensors and advanced technologies [41]. This detection helps growers
    to identify problems and take preventive actions before stresses significantly
    damage plants or crops. Two types of stresses have been identified in ornamental
    crop production: abiotic plant stress and biotic plant stress. Abiotic plant stress
    includes drought, nutrient deficiency, salinity problems, floods, etc., while
    biotic stress refers to damage caused by fungi, bacteria, insects, or weeds. Sensors,
    including RGB, thermal, and spectral, have been utilized to monitor stresses in
    ornamental crop production [42,43,44,45]. A schematic diagram of the sensor-based
    automatic crop disease detection procedure is presented in Figure 3. Figure 3.
    A schematic of a computer-vision-guided dogwood anthracnose leaf disease detection
    procedure. Table 2 represents different ornamental plant disease detection using
    advanced sensing technologies. Red-green-blue (RGB) imaging sensors with a spectrum
    range of 400–700 nm (visible range) are used to monitor ornamental plant stresses
    due to their affordability and application in other cropping systems. Velázquez-López
    et al. [42] developed an image processing-based powdery mildew disease detection
    system for rose plants by using the Open CV library. The system detected powdery
    mildew by converting RGB images to hue, saturation, and value (HSV) color space
    and achieved the highest disease region matching of 93.2% by segmenting with V
    channel using close captured images (captured at 10 cm from the rose canopies).
    Although this study achieved good performance with the traditional image segmentation
    method, the performance would not have been the same if the image capturing conditions
    had changed. This is considered a major limitation, especially for real-time disease
    detection, where multiple diseases would be present. Nuanmeesri [46] advanced
    the image processing technique from traditional image segmentation to deep learning-based
    detection in order to identify up to 15 different diseases. A hybrid deep learning
    model built by fusing convolutional neural networks (CNNs) and a support vector
    machine (SVM) were used. Researchers also tested the image registration approach
    of two imaging media for ornamental crop disease detection. Minaei et al. [45]
    registered RGB and thermal images to detect powdery mildew and gray mold disease
    on roses for developing a site-specific spraying system. A few studies have compared
    RGB imaging with spectral imaging for tulip disease detection [43,47]. The results
    reported that a spectral imaging system achieved better detection accuracies than
    RGB imaging while detecting tulip breaking virus (TBV). Hyperspectral imaging
    is a powerful tool that uses imaging and spectroscopy for detecting stresses at
    the early stage, gathering and processing feature information from a wide spectrum
    of light. Researchers have used hyperspectral sensors for ornamental crops, but
    mainly in laboratory applications due to their vulnerability in real-time field
    applications [43]. Polder et al. [48] identified Botrytis infected Cyclamen plants
    with selected features (bands) of 497, 635, 744, 839, 604, 728, 542, and 467 nm
    in a controlled greenhouse environment. Poona and Ismail [44] selected wavebands
    located across VIS, red edge, NIR, and SWIR regions to detect Fusarium circinatum
    infection in Pinus radiata seedlings at the asymptomatic stage. The study concluded
    that random forest (RF) is a good machine learning (ML) classifier to discriminate
    disease infection from spectral bands. Heim et al. [49] also used RF to differentiate
    myrtle rust-infected lemon myrtle plants and achieved an overall accuracy of 90%.
    The spectral wavebands (545, 555, 1505, and 2195 nm) were selected for discrimination.
    Considering hyperspectral systems’ slow data processing and expense, some studies
    have tried to find an alternative to hyperspectral imaging. A few studies have
    used the multispectral imaging system instead because of its faster data processing
    ability. Polder et al. [43] used an RGB-NIR-based multispectral system (range
    500–750 nm) to detect TBV disease in tulips and achieved a classification accuracy
    of 92%. They employed a linear discriminant classifier along with R, G, B, and
    NIR features to segment the plant and the soil. The author used features such
    the fraction of red pixels, mean normalized red value, mean normalized green value,
    and ratio of contour pixels of spots to classify disease in tulips. Pethybridge
    et al. [50] assessed ray blight disease (caused by Phoma ligulicola) intensity
    using a hand-held multispectral radiometer with 485, 560, 660, 830, and 1650 nm
    spectral band sensors. The study used vegetation indices, including normalized
    difference vegetative index (NDVI), green normalized difference vegetative index
    (GNDVI), difference vegetative index, and renormalized difference vegetative index
    to assess ray blight disease. Thermal imaging has also been tested for stress
    detection in ornamental plants, a technique which depicts the spatial distribution
    of temperature differences in a captured scene by converting infrared (IR) radiation
    into visible images. Jafari et al. [51] classified asymptomatic powdery mildew
    and gray mold disease on roses by fusing thermal images with visible-range captured
    images. Valuable thermal features were extracted, and artificial neural networks
    (ANN) and SVM were used to classify healthy and disease-infected rose plants.
    The thermal features include maximum, minimum, median, mode, standard deviation,
    maximum difference in temperature, skewness, kurtosis, sum of squared errors,
    and so on. Studies have been conducted for disease stress detection using thermal
    imaging; however, this type of sensing is more practical for water stress detection.
    Before conducting the above experiment, Jafari et al. [52] attempted to classify
    Botrytis cinerea infection on rose using thermal spectra and radial-basis neural
    networks. Buitrago et al. [53] analyzed the infrared spectra of plants for water
    stress detection and concluded that spectral changes in plant regions had a direct
    connection with the microstructure and biochemistry of leaves. Stress detection
    technologies are widely used in other crop industries, especially for agronomic
    crops (such as corn and soybean) and tree fruits (such as apple and citrus), but
    very few experiments have been conducted for ornamental crops (mostly in the floriculture
    industry). Very limited research, almost no studies, have been conducted for the
    woody ornamental industry. A few studies have been conducted to detect stress
    using RGB sensors because RGB cameras do not require deep technical knowledge
    to operate or use. Spectral sensors are necessary to detect stress at an asymptomatic
    or early stage. Spectral sensors have a huge potential for the ornamental industry,
    but not much progress has been previously reported. Currently, UAVs are very popular
    for crop stress detection and monitoring, but the applications of these systems
    are also very limited for the ornamental crop industry. De Castro et al. [54]
    used a UAV system to detect water stress in Cornus, Hydrangea, Spiraea, Buddleia
    and Physocarpus, and the results of this study show promise. The ornamental industry
    can benefit from using UAV-based sensing technologies for the timely detection
    and monitoring of stresses to enhance crop production. Table 2. Summary of studies
    reported for plant stress detection.    2.3. Smart Spraying Management of different
    pests and diseases is essential to ensure high quality ornamental nursery crop
    production meeting the market’s requirements [55]. Traditional management techniques
    include pruning the infected branches, removing dead or infected plants, monitoring
    diseases, trapping insects, growing pest-resistant cultivars, and pesticide applications
    [56]. Foliar pesticide application is the most effective method for preventing
    pest infestations and ensuring healthy and unblemished nursery plants [57]. In
    the United States, the greenhouse and nursery industries use about 1.3 million
    kg of pesticides every year, saving billions worth of crops [58]. Conventionally,
    radial air-assisted sprayers are the most used spray equipment for pesticide application
    in ornamental nurseries [59]. These sprayers apply pesticides to the entire field
    regardless of the plant structure, plant growth stage, and absence of plants in
    rows, thus, resulting in under- or over-spraying [60] as well as contaminating
    the environment, wasting pesticides, and increasing production cost [61]. This
    problem is more critical for the nursery industry, as there is great diversity
    in canopy structures and densities found in nursery crops. In field nursery production,
    it is a common practice that trees of different ages and cultivars are planted
    in the same row. The traditional sprayers cannot adjust sprayer settings to match
    the target tree requirements, reducing application efficiency. One way to improve
    spraying efficiency is to use sensing technologies to identify target trees for
    precise spraying applications, also referred to as smart/variable-rate-intelligent
    spraying (Figure 4). Figure 4. A schematic of a light detection and ranging (LiDAR)-guided
    variable-rate spraying system. Smart spraying is defined as the precise application
    of pesticides, performed by controlling the spray output of each nozzle based
    on the presence, structure, and canopy density of plants as obtained from sensors
    such as ultrasound, laser, and cameras [18]. In recent years, significant research
    has been conducted to develop smart spraying systems for the nursery industry.
    Different sensors, such as ultrasonic and laser, have been utilized to measure
    the canopy parameters for intelligent spraying in nursery crops. The summary of
    the reviewed studies is presented in Table 3. The initial efforts for smart nursery
    spraying were reported back in 2010 by a team of scientists from the United States
    [62]. The authors developed two precision sprayer prototypes: a hydraulic boom
    sprayer with an ultrasonic sensor for small narrow trees such as liners and an
    air-assisted sprayer with a laser scanner for other ornamental nursery species.
    The authors compared the spray consumption between a sensor-based sprayer and
    a conventional air blast sprayer at three growing stages and four travel speeds
    (3.2, 4.8, 6.4, and 8.0 km/h). The sensor-based air-assisted sprayer applied 70%,
    66%, and 52% fewer chemicals at different growth stages than conventional spraying.
    The results also reported a uniform spray deposit and coverage regardless of changes
    in the canopy size and travel speed. Jeon and Zhu [63] developed an ultrasonic-sensed
    real-time variable-rate vertical boom sprayer for nursery liners. The sprayer
    consisted of two booms with five pairs of equally spaced nozzles, with the ultrasonic
    sensor mounted 0.35 m ahead of the nozzles. Field tests were conducted for six
    different liner species at travel speeds from 3.2 to 8.0 km/h. The spray nozzles
    were triggered successfully from 4.5 to 12.5 cm ahead of the target, and the effects
    of travel speed on mean spray coverage and deposit were insignificant. Following
    this work, a study for the same precision sprayer was reported for performance
    evaluation based on spray coverage, deposit, and droplet density compared to conventional
    ones for all six-liner cultivars [64]. The reported results suggest that the spray
    coverage, deposit, and droplet density were lower in the sensor-based sprayer,
    and the spray volume was reduced by 86.4% compared to the conventional sprayer.
    Laser sensing is another technology used for precision spraying for many tree
    crops. A few studies have been reported that utilize laser scanning for smart
    spraying applications in nurseries. Chen et al. [57] developed a variable-rate
    air-assisted sprayer using a laser scanner. The authors reported that the spray
    coverage differences inside the canopies were not statistically significant at
    3.2 and 6.4 km/h travel speeds. Liu et al. [65] used a laser scanner to develop
    an intelligent variable-rate air-assisted sprayer and tested the system in a commercial
    nursery and grapevine orchard. The authors reported that the new sprayer reduced
    chemical usage by more than 50% compared to the conventional sprayer at a travel
    speed of 3.2 to 8.0 km/h. Shen et al. [66] developed an air-assisted laser-guided
    sprayer for Japanese maple nursery trees. The new sprayer consisted of a 270°
    radial-range laser scanner, embedded controller, and pulse-width-modulated (PWM)
    nozzles. The authors reported an accurate measurement of different trees and control
    of nozzles to match trees independently. The spray usage was reduced by 12 to
    43%, compared to the conventional spraying. In addition, a few studies have been
    reported for field validation of precision sprayers to control different diseases.
    Zhu et al. [59] validated the laser-guided air-assisted sprayer and reported a
    chemical saving of about 36% and 30% in the Prairifire crabapple and Honey locust
    nurseries, respectively. Chen et al. [67] also conducted a performance comparison
    of laser-guided air-assisted sprayers with conventional sprayers in commercial
    nurseries with different test plants. The author reported 56% and 52% chemical
    savings for two nurseries. Similarly, a few other studies have compared the performance
    of smart laser-guided sprayers with conventional sprayers and reported promising
    results for effective disease control in different nursery crops [61,68]. Table
    3. Summary of studies reported for smart nursery spraying.   Smart spraying for
    nursery crops using different sensing technologies, mainly ultrasonic and laser,
    has been reported in the last decade. Ultrasonic and laser sensors were integrated
    with conventional sprayers to detect the target (e.g., canopies). Although ultrasonic
    sensor-based sprayers exhibit significant chemical savings, their accuracy varies
    with temperature, humidity, and detection distance [57]. On the other hand, laser
    sensors are less influenced by weather conditions when detecting and measuring
    target characteristics [69]. Moreover, the nursery industry encounters several
    unique challenges, such as the lack of crop uniformity, varying shapes, sizes,
    growth patterns, and harvest schedules. Most existing sprayers have been developed
    for the orchard environment [59]; modifications may be required to make them usable
    for ornamental nursery crop production. Another challenge for the ornamental industry
    is its high aesthetic thresholds allowing for no visible infections. Thus, efforts
    are required to develop a smart spraying system based on the requirements of the
    nursery industry. 2.4. Plant Biometrics and Identification Information on plant
    physiology and responses to biotic/abiotic stresses are critical to determine
    the management practices required to improve productivity and sustainability in
    the nursery industry. Plant biometry (e.g., structural information) can assist
    in understanding the plant’s growth differences in diverse environments [70].
    Cultivar identification of nursery plants is also important for breeding, reproduction,
    and cultivation [71]. Plant biometry is a classification system that distinguishes
    a plant by defining its authenticity using physiological characteristics. The
    defined biometric for an individual plant should be universal, distinctive, permanent,
    and collectible [72]. Plant identification, inspection, and a precise count of
    each cultivar’s number and size distribution are essential for nursery management
    and efficiently marketing the trees [73] (Figure 5). Figure 5. A schematic of
    a UAV-based tree canopy characteristics measurement system. Different sensors,
    including cameras and LiDAR, have been utilized for nursery plant biometrics.
    The summary of the reviewed studies is presented in Table 4. The research for
    nursery plant identification using camera imaging systems started in the 1990s.
    Shearer and Holmes [74] used a camera vision system to identify tree species in
    the nursery. The study used color co-occurrence matrices derived from intensity,
    saturation, and hue to identify seven common containerized nursery plants. A total
    of 33 texture features were used for the analysis, and the reported classification
    accuracy was 91%. She et al. [75] developed a high-resolution imaging system to
    classify containerized Perennial peanut and Fire chief arborvitae plants for counting.
    he authors found that the classification accuracy of plants with flowers was higher
    (97%) than those without flowers (96%). Leiva et al. [76] developed an unmanned
    aircraft system (UAS)-based imaging system for counting container-grown Fire Chief
    arborvitae. The author developed a custom counting algorithm and tested it on
    different backgrounds, including gravel and black fabric. The reported results
    indicated counting errors of 8% and 2% for gravel and black fabric backgrounds,
    respectively. In another study, the authors used a depth camera for height measurements
    of nursery plants [77]. The authors implemented Ghostnet–YoloV4 Network for measuring
    height and counting different nursery plants, including spruce, Mongolian scotch
    pine, and Manchurian ash. They achieved an accuracy of more than 92% for measurement
    and counting. Gini et al. [78] used a UAS-based multispectral imaging system to
    classify eleven nursery plant species. The author implemented multiple grey level
    co-occurrence matrix algorithms to perform textural analysis of acquired images.
    A principal component analysis was used after feature extraction, achieving a
    classification accuracy of 87% for the selected plants. Likewise, a few studies
    have reported the application of LiDAR sensors to identify nursery plants. Weiss
    et al. [79] developed a method for identifying nursery plant species using a LiDAR
    sensor and supervised machine learning. The author used multiple machine learning
    classifiers and 83 features to identify six containerized nursery plant species,
    and achieved an accuracy of more than 98%. Similarly, LiDAR and light curtain
    sensors were used to develop a stem detection and classification system for almond
    nursery plants [73]. The authors developed a custom segmentation and thresholding
    algorithm, and the reported detection accuracies with the LiDAR and light curtain
    sensors were 95.7% and 99.48%, respectively. The success rates for dead/alive
    plant detection for the LiDAR and light curtain sensors were 93.75% and 94.16%,
    respectively. Additionally, a few other studies have reported the application
    of machine vision approaches using different machine learning and deep learning
    methodologies for detecting and classifying different flower nurseries [71,80,81,82,83,84].
    Table 4. Summary of studies reported for plant biometric measurements. Nursery
    crop management is time-consuming and labor-intensive, bringing a great need for
    automation, especially for large nursery production areas. Sensing-based plant
    biometrics, identification, and recognition are promising but challenging tasks.
    The rapid advancements in sensing, computation, artificial Intelligence (AI),
    and data analytics have allowed more detailed investigations in this domain. Research
    has been reported to identify tree species for management operations and counting
    plants for inventory control using different types of sensors, including RGB,
    multispectral, LiDAR, etc. A few recent studies have utilized state-of-art deep
    learning techniques for nursery plant classification; however, more efforts are
    needed to facilitate the growers’ use of such techniques for the profitability
    and sustainability of the nursery industry. 2.5. Other Significant Works The economics
    of production practices associated with fertilizer inputs, pest control needs,
    and labor requirements affect the nursery industry. Most nursery production operations
    are labor intensive. According to Gunjal et al. [85], labor accounts for 70% of
    the costs for nursery production. Though a few operations in nursery production
    have been mechanized, many others have not been automated. Advanced sensing and
    mechanization/automation could reduce resource consumption and labor dependence
    [73]. In this context, the ornamental nursery industry has witnessed some progress
    in different sensing, automation, and robotic applications. Table 5 presents the
    summary of works related to other sensing and automation applications for nursery
    crop production. Li et al. [86] developed a trimming robot for ornamental plants.
    The design includes a knife system and a rotary base, allowing the knife to rotate
    360 degrees to cut the plants into the desired shape. The robot was tested for
    five different nursery plant species (Aglaia odorata, Murraya exotica, Camellia
    oleifera, Osmanthus fragrans, and Radermachera sinica), and results indicated
    that the overall performance was above 93% with the time taken as 8.89 s. Zhang
    et al. [87] developed a path-planning scheme for a watering robot for containerized
    ornamental nursery plants. The authors optimized the robot’s path planning using
    a genetic algorithm with neighbor exchanging to test different watering strategies,
    and achieved promising results in terms of water savings. Sharma and Borse [88]
    developed an autonomous mobile robot to carry out different production operations
    in the nursery. The robot featured multiple sensor modules, including camera and
    climate monitoring, to perform real-time growth monitoring, disease detection,
    and the spraying of fertilizer, pesticide, and water. The platform was also equipped
    with a Zigbee communication framework to transmit the sensed data to the central
    control system. The system achieved the desired results for disease detection
    and growth monitoring; however, no technical details are provided. Similarly,
    a conceptual design of a cable-driven parallel robot (CDPR) to perform different
    operations, including seeding, weeding, and nutrition monitoring for plant nurseries
    has been presented [89]. The authors performed the operational and path planning
    simulation to execute seeding and weeding operations. Additionally, a pretrained
    VGG16 model was used for weed identification, and results showed promise, with
    an accuracy of 96.29% achieved during testing. Despite some progress, the status
    of research-based findings for robotic applications in the nursery industry lags
    far behind its contemporary industries. Table 5. Summary of works related to nursery
    production in other remaining areas. 3. Future Prospects/Directions 3.1. Advanced
    Camera Sensor Applications 3.1.1. ToF, LiDAR, and 3D Sensors Applications Advanced
    sensing technologies, such as depth cameras, time-of-flight (ToF) cameras, and
    multispectral and hyperspectral cameras, have been widely used in different agricultural
    applications. Kim et al. [90] implemented a binocular stereo-vision camera incorporated
    with a single-board computer for estimating crop height. Authors successfully
    estimated heights for Chinese cabbage, potato, sesame, radish, and soybean crops
    with a less than 5% of error in field conditions. Wang et al. [91] developed a
    ground-based remote imaging system comprised of an ultrasonic sensor, a LiDAR
    sensor, a Kinect camera, an imaging array of four digital cameras, and a custom-developed
    gimble and camera, respectively, for estimating sorghum plant height at plot level.
    The author observed that an ultrasonic sensor, a LiDAR sensor, and a Kinect camera
    resulted in strong correlations (r ≥ 0.90) between automatic and manual measurements
    for plant height estimation. The study concluded that the ground-based image acquisition
    system resulted in a comparatively higher correlation between automatic and manual
    measurements compared to the remote imaging system. They recommended LiDAR combined
    with high-resolution camera array technology, which can be an ideal methodology
    for measuring plant height effectively. The 3D/Depth cameras have found widespread
    usage in agriculture for a variety of purposes, including but not limited to yield
    estimation [92], plant phenotyping [93], and disease detection [94]. A vision-based
    under-canopy navigation and mapping system for corn and sorghum was developed
    by Gai et al. [95] using a ToF camera combined with a field robot, PhenoBot 3.0.
    They implemented linear programming techniques and developed a novel algorithm
    for reliable crop row detection and navigation. The developed system achieved
    mean absolute errors (MAE) of 3.4 cm and 3.6 cm in fields of corn and sorghum,
    respectively. Similarly, Gongal et al. [96] fused a color charge coupled device
    (CCD) camera and a ToF sensor to estimate apple fruit size under controlled lighting
    conditions. The developed system estimated apple fruit size with an accuracy of
    84.8% based on pixel size. A few of the most significant applications for ToF
    cameras in agriculture are plant height estimation [97,98], 3D reconstruction
    of the plant [99], 3D plant morphology [100], palm bunch grading [101], and so
    on. 3.1.2. Spectral Sensor Applications Cao et al. [102] developed a nitrogen
    monitoring system for tea plants using multispectral (wavelengths: 475 nm, 560
    nm, 668 nm, 717 nm, and 840 nm) and hyperspectral imaging systems. They fused
    data after preprocessing, which included multispectral image registration, calibration,
    information extraction and selection, and hyperspectral wavelength selection.
    After filtering the fused data, they feed them to regression models, including
    PLS regression, random forest regression (RFR), and support vector machine regression
    (SVR), to predict the nitrogen content of tea leaves. The support vector machine
    regression outperformed other models and achieved R2 (coefficient of determination)
    and root mean square error values of ~0.92 and ~0.06, respectively. Another researcher,
    Chandel et al. [103], also used simple linear regression models (LRs) to experiment
    on characterizing Alfalfa (Medicago sativa L.) crop vigor and yield by combining
    multispectral (465–860 nm) and thermal infrared (11,000 ± 3000 nm) image data
    collected from unmanned aerial vehicles. The model MLR-4 outperformed other models
    and achieved an R2 of 0.64. The aforementioned studies offer compelling evidence
    of increased success rates for agricultural applications of cutting-edge sensors,
    which suggest prospective uses for ornamental nursery crops. The advanced sensors
    can operate successfully in both indoor and outdoor environments. Therefore, in
    the future, automated systems for ornamental nursery corps can be developed using
    sophisticated camera sensors like 3D or depth cameras, ToF, multispectral, and
    hyperspectral. 3.2. Enhanced Deep Network Applications Due to the extraordinary
    ability to generate synthetic datasets with the same properties as training datasets,
    advanced computer vision-based techniques such as generative adversarial networks
    (GANs) and transformers are overtaking photometric and geometric-based augmentation
    approaches in a variety of agricultural problems. Abbas et al. [104] proposed
    a tomato plant disease detection system using a publicly available plant village
    tomato leaf dataset. The authors augmented the dataset using a conditional generative
    adversarial network (C-GAN) and fed the data to a pre-trained DenseNet network.
    The network successfully predicted tomato leaf diseases from healthy leaves and
    achieved an accuracy of 97.11%. The augmentation of the tomato leaf dataset improved
    the DenseNet network’s prediction by 2.77% compared to the accuracy of the original
    plant village tomato leaf dataset. Xiao et al. [105] implemented Texture Reconstruction
    Loss CycleGAN (TRL-GAN) to produce phenotypic data for the citrus greening disease
    and improve classification networks for the detection of diseased leaves. The
    authors observed that the TRL-GAN based method improved accuracy by 2.76% compared
    to the baseline model and 1.04% compared to the traditional augmentation methods
    (rotation and stretching). Zhang et al. [106] combined hyperspectral imaging with
    generative adversarial networks (DCGAN, CGAN) to expand the original dataset.
    They also observed that expansion of the dataset using GANs would improve the
    accuracy of k-nearest neighbor (kNN), SVM, and RF for haploid maize kernel classification
    by 12%, 20%, and 12%, respectively, compared to baseline models. Data enlargement
    using GANs allows for the development of detection, classification, and prediction
    models with less data on ornamental nursery crop images, which increases the model’s
    resilience in varying conditions and improves performances or accuracies. The
    augmented data can be incredibly useful when developing machine vision-based systems
    for nursery crops, such as leaf classification and disease assessment systems.
    Robots may be trained in a simulated environment using the data produced by GANs.
    3.3. Edge-AI Applications Embedded platforms combined with hardware accelerators
    and artificial intelligence-based sensing technology, called Edge Artificial Intelligence
    (Edge-AI), have made quick responses with low latency possible over cloud-based
    solutions. This technique has been adopted in different agricultural applications
    in recent years. Mazzia et al. [107] developed a real-time apple detection system
    using an Edge-AI technology. They implemented YOLOv3-Tiny algorithms on three
    different embedded platforms, including Raspberry Pi 3 B+ with Intel Movidius
    Neural Computing Stick (NCS), Nvidia’s Jetson Nano and Jetson AGX Xavier, and
    successfully detected apples in an orchard. Their system achieved an accuracy
    of 83.64% with a data processing speed up to 30 frames per second (fps) in complex
    situations. Zhang et al. [108] implemented YOLOv4-Tiny networks combined with
    improved cross stage partial networks (CSPNet) in the backbone for strawberry
    detection and implemented a developed model on the embedded platform Jetson Nano
    (NVIDIA Corporation, Santa Clara, CA, USA). Their optimized model (RTSD-Net) with
    TensorRT achieved about 25.20 fps and performed 15% faster than the original YOLOv4-tiny
    model on Jetson Nano without significant loss of accuracy. Other promising applications
    of Edge-AI are air temperature forecasting [109], environment monitoring [110],
    autonomous navigation systems [111] and so on. Edge-AI technology can potentially
    be applied for weeding, spraying, and robot navigation in ornament nursery crop
    production. Weed maps generated by UAVs may be combined into autonomous robots
    for site-specific weed management and pesticide applications in the field. Embedded
    hardwire (Raspberry Pi, Jetson Nano, and Jetson TX2) paired with sensors (color
    camera, depth camera), and AI may be implemented to develop Edge-AI technology
    for ornamental nursery crops. Vision-based robots using Edge-AI technology can
    be an aid to robot navigation for accomplishing site-specific applications in
    nursery crops. 3.4. Radio Frequency Identification Tagging Applications Radio
    frequency identification (RFID) technology has become popular in different fields
    of agriculture, including soil environment monitoring, soil moisture monitoring,
    soil solarization, and automation in irrigation. Deng et al. [112] designed and
    developed a novel system that integrates an RFID sensor with LoRa to provide a
    low-cost, low-power, and efficient soil environment monitoring solution. The authors
    embedded RFID tags at 60 cm into the soil; the tags can communicate with the monitoring
    center through radio communication (LoRa) placed in the patrol car. Their system
    would be able to establish communication within a range of 1.3 m without compromising
    relative measurement errors (temperature: 1.5% and soil moisture content: 1.0%).
    The study achieved a higher communication rate (above 90%) at a patrol speed of
    33 kmh−1. Luvisi et al. [113] developed a system that monitors different types
    of soil solarization (sandy, loam, and clay soils) using an RFID sensor and biodegradable
    films. They placed soil sensors at different depths (5 and 10 cm) along with a
    soil profile at different soil moisture holding capacities (10%, 50%, and 90%)
    and measured the effect of soil solarization treatment. In the second and third
    weeks of treatment, they found that the maximum soil temperature at depths of
    5 and 10 cm increased to 9–13 °C and 11–14 °C, respectively. They also found that
    the method was 90% reliable. Vellidis et al. [114] implemented soil moisture sensors
    (Watermark® granular resistive type) and thermocouple temperature sensors coupled
    with RFID tags (WhereNet®, Santa Clara, CA, USA) for developing sensor nodes to
    automate irrigation schedules for cotton crops. The nodes were connected to a
    laptop computer via wireless communication. The developed system contained an
    array of sensors, and data obtained from the sensors could assist in decision-making
    and scheduling irrigation for the cotton field. Several researchers also contributed
    to RFID-based soil moisture sensor developments [115,116,117,118,119]. RFID-based
    sensors also have other applications in agriculture, including tracking plants
    in pots in greenhouses [120], tracing food quality [121], and monitoring livestock
    [122]. The above studies and their success rates clearly show the potential of
    using RFID-based sensors in ornamental nursery crop applications. The potential
    applications of RFID-based sensors for nursery crops include soil environment
    monitoring, soil solarization, and automating irrigation scheduling, in indoor
    or field conditions. The RFID tags can be used in conjunction with soil monitoring
    sensors such as soil, moisture, soil micronutrient, gas, etc. to build sensor
    nodes and receive in-field data through wireless communications. Readings from
    sensor nodes may be used with machine learning and deep learning to make decisions
    in various field management operations. 3.5. Integrated Robotics Applications
    Robots integrated with computer vision have been widely adopted in many areas
    of agriculture, such as plant detection and mapping, fruit detection and localizations,
    robot-based harvesting, navigation, and obstacle detection systems. Weiss and
    Biber [123] developed a ground-based robot for maize plant recognition, mapping,
    and navigation using a 3D LiDAR sensor-based micro-electro-mechanical system (FX6
    3D LiDAR). The robot was constructed using modeled artificial maize plants and
    tested on a small corn field. The designed robot achieved detection and mapping
    accuracy of around 60%–70%. They measured a greater localization deviation in
    the direction of the row, measuring 1–2 cm. Ge et al. [124] developed a strawberry
    fruit localization method using a strawberry harvesting robot with an RGB-D camera.
    The authors implemented a convolutional neural network (i.e., Mask-RCNN) on RGB
    images for strawberry fruit segmentation and combined depth values to obtain 3D
    points of fruits. The 3D point was then used to obtain fruit localization using
    the shape completion method. The system achieved a minimum center deviation of
    6.9 mm between ground truths and automated measurements. Skoczeń et al. [125]
    also proposed a similar approach to develop an automatic obstacle-detection robot.
    They implemented an RGB-D camera (Intel RealSense D435i) for robot vision, reached
    obstacle segmentation accuracy of 98.11%, and obtained a depth measurement error
    of 38 cm. Ji et al. [126] developed a machine vision algorithm for a green pepper
    harvesting robot. The contrast values of images obtained by the camera (MX808)
    for various light conditions (normal, weak, and strong light) were then increased
    to make the green pepper stand out from the background leaf. The energy-driven
    sampling (SEEDS) algorithm is then fed the improved images to build super pixel
    blocks. The manifold ranking (MR) algorithm, the CART classifier, and the conditional
    random field (CRF) algorithm were used to recognize green pepper from super pixel
    blocks, followed by morphological processing. Classifiers were evaluated on 500
    images obtained from different lighting conditions. The algorithm manifold ranking
    outperformed other classifiers and achieved an accuracy of 83.6%; it took 116
    milliseconds to run the entire evaluation on Intel Core (TM) i5-4210U CPU (2.80
    GHz, 8 GB). Gai et al. [127] developed a cherry fruit detection system using a
    high-resolution Sony DSC-HX400 camera combined with a YOLO-V4 Dense Model network.
    The study compared the developed algorithm’s accuracy with the base model, YOLO-V3-dense,
    and YOLO-V4 and observed an improved detection rate (F1 scores: 94.70%). YOLO-V4
    Dense Model took 0.467 s on an Intel Core (TM) i7-7700 CPU (3.60 GHz, 4 GB) with
    a Tesla V100 GPU for processing an image of 1280 by 800 pixels. They found robotic
    intelligent picking is possible using the developed system. Jia et al. [128] also
    developed a robot vision using a high-resolution camera (6000 × 4000 pixel) for
    an apple harvesting robot using an optimized Mask R-CNN. The developed system
    achieved a high rate of detection (precision: 97.31%; recall: 95.70%). The development
    of robot vision using high-resolution camera sensors combined with deep learning
    techniques can be adopted to develop ornamental crop management robots. Applications,
    such as spraying, weeding, soil sampling, and digging could be effectively solved,
    enabling different operations in nursery crops. Robot vision combined with machine
    and deep learning may also be implemented in nurseries for plant counting, stem
    counting, and other essential tasks. 4. Discussion and Conclusions The ornamental
    crop industry in the U.S. depends largely on agricultural workers. Sensing and
    automation technologies offer a huge potential to reduce labor dependency and
    ensure the efficient use of resources required by the ornamental industry. In
    turn, the information in this article can aid the nursery industry in knowing
    about the specific area where technological development takes place and what those
    technologies are, and in considering what types of sensors, algorithms and tools
    are advantageous to develop effective technologies in different production operations.
    Current sensing and automation technology usage varies by production operations.
    For instance, smart irrigation has primarily relied on soil moisture sensors,
    and stress detection has largely depended on camera sensors. Despite the fact
    that not many studies have used IoT or Edge-AI-based IoT systems, these could
    be potential technologies for automating irrigation operations for ornamental
    crops. The Edge-AI-based systems and AI-of-things (AIoT) are relatively new concepts
    in agricultural applications, and successes in other cropping systems have shown
    promise for the ornamental nursery industry. Similar to irrigation, a very limited
    number of studies have been conducted for ornamental plant stress detection. One
    important fact regarding stresses is that they have to be detected early to minimize
    their effect on crops. Spectral cameras, including hyperspectral and multispectral
    devices, are the two sensors currently being used to detect stresses at the asymptomatic
    stage. However, the major challenge of detecting plant stresses is to detect them
    in real-time field conditions. Researchers have been trying to address challenges
    such as illumination variations, data processing speed, and environmental factors
    to make a viable system for real-time applications. More efforts are required,
    though, especially for the hyperspectral system, due to its slow data processing
    issues. Fluorescence sensors are another spectral technology that has not been
    explored much for ornamental crops, one which can provide improved spectroscopy
    data and can be useful for early plant stress detection. LiDAR is one of the powerful
    tools that can be used to accurately measure plant biometric information (plant
    height, width, canopy volume and density, etc.) to develop a smart or variable-rate
    spraying system. However, this tool cannot be used for spot spraying operations
    for disease management because the LiDAR sensor can only provide point cloud information
    (unlike cameras, it does not provide any color information). Integrated LiDAR
    and camera systems could potentially be tools for smart spraying systems for ornamental
    nursery crop production. The advantages and disadvantages of different sensors
    are presented in Table 6. Table 6. Advantages and disadvantages of different sensors
    for ornamental crops. Surprisingly, very few applications have been noticed for
    UAVs in ornamental crops, despite extensive implications these days in the agronomic,
    tree fruit and row crops. The low manufacturing cost and fast operation speed
    have opened up further research opportunities for UAVs. UAVs are becoming an essential
    part of remote sensing and can be an effective tool for ornamental plant stress
    detection and monitoring crop growth and development. The UAVs bring advantages
    over ground-based systems, such as their flexibility in capturing ultra-high spatial
    and temporal resolution data at any terrain conditions, and they require less
    time to collect data. However, developing manipulation systems for UAVs that can
    act with precision in fields is a challenging task requiring extensive investigations.
    The coordination between UAVs and ground-based systems has been receiving increasing
    attention in recent years, and has the potential to benefit the ornamental crop
    industry for site-specific management. Calibrating sensors is essential to reduce
    variability when multiple sensors are involved in a particular crop management
    operation. Recent advances in deep learning models (e.g., CNNs, GANs, transformers)
    have contributed significantly to different industries, including agriculture,
    but ornamental crops remain at the bottom user of these impressive innovations.
    These models can help predict stress, pest pressure, growth, yield, etc. RFID,
    a new crop tracking technology, can increase production operations’ efficacy and
    help nurseries to reduce the burden for growers or laborers by automating the
    inspections and recording accurate ornamental crop data instantly. Agricultural
    robotics is another critical area that can benefit the ornamental crop industry
    enormously. Currently, the agricultural workforce conducts most production operations,
    such as planting, pruning/shape forming, weeding, disease monitoring, and harvesting.
    These operations are vastly labor-intensive and cost a large portion of production
    expenses. Autonomous robotic systems can replace the humans conducting these operations.
    The systems will reduce time and production expenses in the long run. The ornamental
    industry lacks automation/robotic technologies; therefore, significant research
    needs to be done on these topics to develop some implementable robotic systems.
    As the majority of the ornamental crop farms are not so large compared to other
    major cropping industries, adopting advanced sensing and automation technologies
    would be a major challenge due to the initial high investment. Integrated multipurpose
    automated technologies will be helpful for this purpose. For instance, when a
    particular automated system can work for multiple operations (e.g., planting,
    pruning, and harvesting) for ornamental crops by replacing a few parts of the
    system, growers would be interested in buying and adopting those multipurpose
    systems. Researchers and manufacturers need to consider these points while working
    on or developing technologies for the ornamental nursery crop industry. Although
    not much progress in sensing and automation technologies has been observed for
    ornamental nursery crop production, a few mechanized systems are available for
    commercial scales. These include mixing systems to mix substrate or soil, potting
    systems to fill containers, tray filling systems to fill trays, planters to plant
    nursery liners in containers, seeding systems to sow and space out seeds on pots
    or containers, etc. Pack Manufacturing (http://packmfg.com/) (Pack Manufacturing
    Inc., McMinnville, TN, USA) is a leading company in the sale of these mechanized
    systems. A vital challenge in technology development for ornamental nursery crops
    is the substantial number of available plant species. Various ornamental plants
    have different morphologies, characteristics, canopy structures, and growth requirements.
    It is necessary to understand the types of plants grown and their production requirements
    to align the sensing and automation technologies with the production needs to
    facilitate industry operations. Author Contributions All authors contributed equally
    to this article. All authors have read and agreed to the published version of
    the manuscript. Funding This study was majorly supported by the United States
    Department of Agriculture (USDA)’s National Institute of Food and Agriculture
    (NIFA) Research Capacity Fund (Evans-Allen) under NIFA Accession No. 7003739 and
    Organizational Project No. TENX2203-CCOCP and partially supported by the USDA’s
    NIFA Federal Appropriations under TEX09954 and Accession No. 7002248. Conflicts
    of Interest The authors declare no conflict of interest. References USDA. U.S.
    Horticulture in 2014 (Publication ACH12-33); United States Department of Agriculture:
    Beltsville, MD, USA. Available online: https://www.agcensus.usda.gov/Publications/2012/Online_Resources/Highlights/Horticulture/Census_of_Horticulture_Highlights.pdf
    (accessed on 21 November 2022). Lea-Cox, J.D.; Zhao, C.; Ross, D.S.; Bilderback,
    T.E.; Harris, J.R.; Day, S.D.; Hong, C.; Yeager, T.H.; Beeson, R.C.; Bauerle,
    W.L.; et al. A Nursery and Greenhouse Online Knowledge Center: Learning Opportunities
    for Sustainable Practice. HortTechnology 2010, 20, 509–517. [Google Scholar] [CrossRef]
    Majsztrik, J.C.; Fernandez, R.T.; Fisher, P.R.; Hitchcock, D.R.; Lea-Cox, J.;
    Owen, J.S.; Oki, L.R.; White, S.A. Water Use and Treatment in Container-Grown
    Specialty Crop Production: A Review. Water. Air. Soil Pollut. 2017, 228, 151.
    [Google Scholar] [CrossRef] [PubMed] Majsztrik, J.; Lichtenberg, E.; Saavoss,
    M. Ornamental Grower Perceptions of Wireless Irrigation Sensor Networks: Results
    from a National Survey. HortTechnology 2013, 23, 775–782. [Google Scholar] [CrossRef]
    Wheeler, W.D.; Thomas, P.; van Iersel, M.; Chappell, M. Implementation of Sensor-Based
    Automated Irrigation in Commercial Floriculture Production: A Case Study. HortTechnology
    2018, 28, 719–727. [Google Scholar] [CrossRef] Rihn, A.L.; Velandia, M.; Warner,
    L.A.; Fulcher, A.; Schexnayder, S.; LeBude, A. Factors Correlated with the Propensity
    to Use Automation and Mechanization by the US Nursery Industry. Agribusiness 2022,
    39, 110–130. [Google Scholar] [CrossRef] USDA ERS. Farm Labor. Available online:
    https://www.ers.usda.gov/topics/farm-economy/farm-labor/ (accessed on 20 November
    2022). McClellan, M. Don’t Wait, Automate. Available online: https://www.nurserymag.com/article/five-tips-automation/
    (accessed on 20 November 2022). Silwal, A.; Davidson, J.R.; Karkee, M.; Mo, C.;
    Zhang, Q.; Lewis, K. Design, Integration, and Field Evaluation of a Robotic Apple
    Harvester. J. Field Robot. 2017, 34, 1140–1159. [Google Scholar] [CrossRef] Liu,
    B.; Ding, Z.; Tian, L.; He, D.; Li, S.; Wang, H. Grape Leaf Disease Identification
    Using Improved Deep Convolutional Neural Networks. Front. Plant Sci. 2020, 11,
    1082. [Google Scholar] [CrossRef] Xiong, Y.; Peng, C.; Grimstad, L.; From, P.J.;
    Isler, V. Development and Field Evaluation of a Strawberry Harvesting Robot with
    a Cable-Driven Gripper. Comput. Electron. Agric. 2019, 157, 392–402. [Google Scholar]
    [CrossRef] Ye, H.; Huang, W.; Huang, S.; Cui, B.; Dong, Y.; Guo, A.; Ren, Y.;
    Jin, Y. Recognition of Banana Fusarium Wilt Based on UAV Remote Sensing. Remote
    Sens. 2020, 12, 938. [Google Scholar] [CrossRef] Gajjar, R.; Gajjar, N.; Thakor,
    V.J.; Patel, N.P.; Ruparelia, S. Real-Time Detection and Identification of Plant
    Leaf Diseases Using Convolutional Neural Networks on an Embedded Platform. Vis.
    Comput. 2022, 38, 2923–2938. [Google Scholar] [CrossRef] Lehnert, C.; English,
    A.; Mccool, C.; Tow, A.W.; Perez, T. Autonomous Sweet Pepper Harvesting for Protected
    Cropping Systems. IEEE Robot. Autom. Lett. 2017, 2, 872–879. [Google Scholar]
    [CrossRef] Birrell, S.; Hughes, J.; Cai, J.Y.; Iida, F. A Field-Tested Robotic
    Harvesting System for Iceberg Lettuce. J. Field Robot. 2020, 37, 225–245. [Google
    Scholar] [CrossRef] [PubMed] Yasukawa, S.; Li, B.; Sonoda, T.; Ishii, K. Development
    of a Tomato Harvesting Robot. Proc. Int. Conf. Artif. Life Robot. 2017, 22, 408–411.
    [Google Scholar] [CrossRef] Amatya, S.; Karkee, M.; Gongal, A.; Zhang, Q.; Whiting,
    M.D. Detection of Cherry Tree Branches with Full Foliage in Planar Architecture
    for Automated Sweet-Cherry Harvesting. Biosyst. Eng. 2016, 146, 3–15. [Google
    Scholar] [CrossRef] Mahmud, M.S.; Zahid, A.; He, L.; Martin, P. Opportunities
    and Possibilities of Developing an Advanced Precision Spraying System for Tree
    Fruits. Sensors 2021, 21, 3262. [Google Scholar] [CrossRef] [PubMed] Lu, J.; Hu,
    J.; Zhao, G.; Mei, F.; Zhang, C. An In-Field Automatic Wheat Disease Diagnosis
    System. Comput. Electron. Agric. 2017, 142, 369–379. [Google Scholar] [CrossRef]
    Jiang, P.; Chen, Y.; Liu, B.; He, D.; Liang, C. Real-Time Detection of Apple Leaf
    Diseases Using Deep Learning Approach Based on Improved Convolutional Neural Networks.
    IEEE Access 2019, 7, 59069–59080. [Google Scholar] [CrossRef] Abdulridha, J.;
    Batuman, O.; Ampatzidis, Y. UAV-Based Remote Sensing Technique to Detect Citrus
    Canker Disease Utilizing Hyperspectral Imaging and Machine Learning. Remote Sens.
    2019, 11, 1373. [Google Scholar] [CrossRef] Torres-Sánchez, J.; Peña, J.M.; de
    Castro, A.I.; López-Granados, F. Multi-Temporal Mapping of the Vegetation Fraction
    in Early-Season Wheat Fields Using Images from UAV. Comput. Electron. Agric. 2014,
    103, 104–113. [Google Scholar] [CrossRef] Pearse, G.D.; Tan, A.Y.S.; Watt, M.S.;
    Franz, M.O.; Dash, J.P. Detecting and Mapping Tree Seedlings in UAV Imagery Using
    Convolutional Neural Networks and Field-Verified Data. ISPRS J. Photogramm. Remote
    Sens. 2020, 168, 156–169. [Google Scholar] [CrossRef] Zhang, C.; Atkinson, P.M.;
    George, C.; Wen, Z.; Diazgranados, M.; Gerard, F. Identifying and Mapping Individual
    Plants in a Highly Diverse High-Elevation Ecosystem Using UAV Imagery and Deep
    Learning. ISPRS J. Photogramm. Remote Sens. 2020, 169, 280–291. [Google Scholar]
    [CrossRef] Feng, A.; Zhou, J.; Vories, E.D.; Sudduth, K.A.; Zhang, M. Yield Estimation
    in Cotton Using UAV-Based Multi-Sensor Imagery. Biosyst. Eng. 2020, 193, 101–114.
    [Google Scholar] [CrossRef] Maja, J.M.J.; Robbins, J. Controlling Irrigation in
    a Container Nursery Using IoT. AIMS Agric. Food 2018, 3, 205–215. [Google Scholar]
    [CrossRef] You, A.; Parayil, N.; Krishna, J.G.; Bhattarai, U.; Sapkota, R.; Ahmed,
    D.; Whiting, M.; Karkee, M.; Grimm, C.M.; Davidson, J.R. An Autonomous Robot for
    Pruning Modern, Planar Fruit Trees. arXiv 2022, arXiv:220607201. [Google Scholar]
    Liu, B.; Tan, C.; Li, S.; He, J.; Wang, H. A Data Augmentation Method Based on
    Generative Adversarial Networks for Grape Leaf Disease Identification. IEEE Access
    2020, 8, 102188–102198. [Google Scholar] [CrossRef] Lea-Cox, J.D.; Bauerle, W.L.;
    van Iersel, M.W.; Kantor, G.F.; Bauerle, T.L.; Lichtenberg, E.; King, D.M.; Crawford,
    L. Advancing Wireless Sensor Networks for Irrigation Management of Ornamental
    Crops: An Overview. HortTechnology 2013, 23, 717–724. [Google Scholar] [CrossRef]
    Cornejo, C.; Haman, D.Z.; Yeager, T.H. Evaluation of Soil Moisture Sensors, and
    Their Use to Control Irrigation Systems for Containers in the Nursery Industry;
    ASAE Paper No. 054056; ASAE: St. Joseph, MI, USA, 2005. [Google Scholar] [CrossRef]
    Lea-Cox, J.D.; Ristvey, A.G.; Kantor, G.F. Using Wireless Sensor Technology to
    Schedule Irrigations and Minimize Water Use in Nursery and Greenhouse Production
    Systems ©. Comb. Proc. Int. Plant Propagators Soc. 2008, 58, 512–518. [Google
    Scholar] Chappell, M.; Dove, S.K.; van Iersel, M.W.; Thomas, P.A.; Ruter, J. Implementation
    of Wireless Sensor Networks for Irrigation Control in Three Container Nurseries.
    HortTechnology 2013, 23, 747–753. [Google Scholar] [CrossRef] van Iersel, M.W.;
    Chappell, M.; Lea-Cox, J.D. Sensors for Improved Efficiency of Irrigation in Greenhouse
    and Nursery Production. HortTechnology 2013, 23, 735–746. [Google Scholar] [CrossRef]
    Wheeler, W.D.; Chappell, M.; van Iersel, M.; Thomas, P. Implementation of Soil
    Moisture Sensor Based Automated Irrigation in Woody Ornamental Production. J.
    Environ. Hortic. 2020, 38, 1–7. [Google Scholar] [CrossRef] Kim, J.; Chappell,
    M.; Van Iersel, M.W.; Lea-Cox, J.D. Wireless Sensors Networks for Optimization
    of Irrigation, Production, and Profit in Ornamental Production. Acta Hortic. 2014,
    1037, 643–649. [Google Scholar] Coates, R.W.; Delwiche, M.J.; Broad, A.; Holler,
    M.; Evans, R.; Oki, L.; Dodge, L. Wireless Sensor Network for Precision Irrigation
    Control in Horticultural Crops; American Society of Agricultural and Biological
    Engineers: St. Joseph, MI, USA, 2012; Volume 3. [Google Scholar] Belayneh, B.E.;
    Lea-Cox, J.D.; Lichtenberg, E. Costs and Benefits of Implementing Sensor-Controlled
    Irrigation in a Commercial Pot-in-Pot Container Nursery. HortTechnology 2013,
    23, 760–769. [Google Scholar] [CrossRef] [Green Version] Lea-Cox, J.D.; Belayneh,
    B.E. Implementation of Sensor-Controlled Decision Irrigation Scheduling in Pot-in-Pot
    Nursery Production. Acta Hortic. 2013, 1034, 93–100. [Google Scholar] [CrossRef]
    Manuel Banda-Chávez, J.; Pablo Serrano-Rubio, J.; Osvaldo Manjarrez-Carrillo,
    A.; Maria Rodriguez-Vidal, L.; Herrera-Guzman, R. Intelligent Wireless Sensor
    Network for Ornamental Plant Care. In Proceedings of the IECON 2018—44th Annual
    Conference of the IEEE Industrial Electronics Society, Washington, DC, USA, 21–23
    October 2018; Volume 1. [Google Scholar] Beeson, R., Jr.; Brooks, J. Evaluation
    of a Model Based on Reference Crop Evapotranspiration (ETo) for Precision Irrigation
    Using Overhead Sprinklers during Nursery Production of Ligustrum Japonica. Proc.
    V Int. Symp. Irrig. Hortic. Crops 2006, 792, 85–90. [Google Scholar] Zubler, A.V.;
    Yoon, J.Y. Proximal Methods for Plant Stress Detection Using Optical Sensors and
    Machine Learning. Biosensors 2020, 10, 193. [Google Scholar] [CrossRef] Velázquez-López,
    N.; Sasaki, Y.; Nakano, K.; Mejía-Muñoz, J.M.; Kriuchkova, E.R. Detection of Powdery
    Mildew Disease on Rose Using Image Processing with Open CV. Rev. Chapingo Ser.
    Hortic. 2011, 17, 151–160. [Google Scholar] [CrossRef] Polder, G.; van der Heijden,
    G.W.A.M.; van Doorn, J.; Baltissen, T.A.H.M.C. Automatic detection of tulip breaking
    virus (TBV) in tulip fields using machine vision. Biosyst. Eng. 2014, 117, 35–42.
    [Google Scholar] [CrossRef] Poona, N.K.; Ismail, R. Using Boruta-Selected Spectroscopic
    Wavebands for the Asymptomatic Detection of Fusarium Circinatum Stress. IEEE J.
    Sel. Top. Appl. Earth Obs. Remote Sens. 2014, 7, 3764–3772. [Google Scholar] [CrossRef]
    Minaei, S.; Jafari, M.; Safaie, N. Design and Development of a Rose Plant Disease-Detection
    and Site-Specific Spraying System Based on a Combination of Infrared and Visible
    Images. J. Agric. Sci. Technol. 2018, 20, 23–36. [Google Scholar] Nuanmeesri,
    S. A Hybrid Deep Learning and Optimized Machine Learning Approach for Rose Leaf
    Disease Classification. Eng. Technol. Appl. Sci. Res. 2021, 11, 7678–7683. [Google
    Scholar] [CrossRef] Polder, G.; van der Heijden, G.W.A.M.; van Doorn, J.; Clevers,
    J.G.P.W.; van der Schoor, R.; Baltissen, A.H.M.C. Detection of the Tulip Breaking
    Virus (TBV) in Tulips Using Optical Sensors. Precis. Agric. 2010, 11, 397–412.
    [Google Scholar] [CrossRef] Polder, G.; Pekkeriet, E.; Snikkers, M. A Spectral
    Imaging System for Detection of Botrytis in Greenhouses. In Proceedings of the
    EFITA-WCCA-CIGR Conference “Sustainable Agriculture through ICT Innovation”, Turin,
    Italy, 24–27 June 2013. [Google Scholar] Heim, R.H.J.; Wright, I.J.; Allen, A.P.;
    Geedicke, I.; Oldeland, J. Developing a Spectral Disease Index for Myrtle Rust
    (Austropuccinia psidii). Plant Pathol. 2019, 68, 738–745. [Google Scholar] [CrossRef]
    Pethybridge, S.J.; Hay, F.; Esker, P.; Groom, T.; Wilson, C.; Nutter, F.W. Visual
    and Radiometric Assessments for Yield Losses Caused by Ray Blight in Pyrethrum.
    Crop Sci. 2008, 48, 343–352. [Google Scholar] [CrossRef] Jafari, M.; Minaei, S.;
    Safaie, N. Detection of Pre-Symptomatic Rose Powdery-Mildew and Gray-Mold Diseases
    Based on Thermal Vision. Infrared Phys. Technol. 2017, 85, 170–183. [Google Scholar]
    [CrossRef] Jafari, M.; Minaei, S.; Safaie, N.; Torkamani-Azar, F.; Sadeghi, M.
    Classification Using Radial-Basis Neural Networks Based on Thermographic Assessment
    of Botrytis Cinerea Infected Cut Rose Flowers Treated with Methyl Jasmonate. J.
    Crop Prot. 2016, 5, 591–602. [Google Scholar] [CrossRef] Buitrago, M.F.; Groen,
    T.A.; Hecker, C.A.; Skidmore, A.K. Changes in Thermal Infrared Spectra of Plants
    Caused by Temperature and Water Stress. ISPRS J. Photogramm. Remote. Sens. 2016,
    111, 22–31. [Google Scholar] [CrossRef] de Castro, A.; Maja, J.M.; Owen, J.; Robbins,
    J.; Peña, J. Experimental Approach to Detect Water Stress in Ornamental Plants
    Using SUAS-Imagery. In Proceedings of the Autonomous Air and Ground Sensing Systems
    for Agricultural Optimization and Phenotyping III, Orlando, FL, USA, 16–17 April
    2018; Volume 10664, pp. 178–188. [Google Scholar] Braman, S.; Chappell, M.; Chong,
    J.; Fulcher, A.; Gauthier, N.; Klingeman, W.; Knox, G.; LeBude, A.; Neal, J.;
    White, S.; et al. Pest Management Strategic Plan for Container and Field-Produced
    Nursery Crops: Revision 2015. In Proceedings of the Southern Nursery Integrated
    Pest Management Working Group (SNIPM), Mills River, NC, USA, 30–31 July 2009;
    Volume 236. [Google Scholar] Mizell, R.F.; Short, D.E. Integrated Pest Management
    in the Commercial Ornamental Nursery. 2015; Volume 8. Available online: https://site.caes.uga.edu/sehp/files/2020/03/UF-IPM-in-the-Commercial-Ornamental-Nursery.pdf
    (accessed on 20 November 2022). Chen, Y.; Zhu, H.; Ozkan, H.E. Development of
    a Variable-Rate Sprayer with Laser Scanning Sensor to Synchronize Spray Outputs
    to Tree Structures. Trans. ASABE 2012, 55, 773–781. [Google Scholar] [CrossRef]
    Hudson, W.G.; Garber, M.P.; Oetting, R.D.; Mizell, R.F.; Chase, A.R.; Bondari,
    K. Pest Management in the United States Greenhouse and Nursery Industry: V. Insect
    and Mite Control. HortTechnology 1996, 6, 216–221. [Google Scholar] [CrossRef]
    Zhu, H.; Rosetta, R.; Reding, M.E.; Zondag, R.H.; Ranger, C.M.; Canas, L.; Fulcher,
    A.; Derksen, R.C.; Ozkan, H.E.; Krause, C.R. Validation of a Laser-Guided Variable-Rate
    Sprayer for Managing Insects in Ornamental Nurseries. Trans. ASABE 2017, 60, 337–345.
    [Google Scholar] [CrossRef] Fox, R.D.; Derksen, R.C.; Zhu, H.; Brazee, R.D.; Svensson,
    S.A. A History of Air-Blast Sprayer Development and Future Prospects. Trans. ASABE
    2008, 51, 405–410. [Google Scholar] [CrossRef] Chen, L.; Zhu, H.; Horst, L.; Wallhead,
    M.; Reding, M.; Fulcher, A. Management of Pest Insects and Plant Diseases in Fruit
    and Nursery Production with Laser-Guided Variable-Rate Sprayers. HortScience 2021,
    56, 94–100. [Google Scholar] [CrossRef] Zhu, H.; Jeon, H.Y.; Gu, J.; Derksen,
    R.C.; Krause, C.R.; Ozkan, H.E.; Chen, Y.; Reding, M.E.; Ranger, C.M.; Cañas,
    L.; et al. Development of Two Intelligent Spray Systems for Ornamental Nurseries©.
    In Proceedings of the International Plant Propagators’ Society, Miami, FL, USA,
    1 August 2010; Volume 60, p. 322. [Google Scholar] Jeon, H.; Zhu, H. Development
    of a Variable-Rate Sprayer for Nursery Liner Applications. Trans. ASABE 2012,
    55, 303–312. [Google Scholar] [CrossRef] Jeon, H.Y.; Zhu, H.; Derksen, R.C.; Ozkan,
    H.E.; Krause, C.R.; Fox, R.D. Performance Evaluation of a Newly Developed Variable-Rate
    Sprayer for Nursery Liner Applications. Trans. ASABE 2011, 54, 773–781. [Google
    Scholar] Liu, H.; Zhu, H.; Shen, Y.; Chen, Y. Embedded Computer-Controlled Laser
    Sensor-Guided Air-Assisted Precision Sprayer Development. In Proceedings of the
    ASABE Annual International Meeting, New Orleans, LA, USA, 26–29 July 2015. [Google
    Scholar] Shen, Y.; Zhu, H.; Liu, H.; Chen, Y.; Ozkan, E. Development of a Laser-Guided,
    Embedded-Computercontrolled, Air-Assisted Precision Sprayer. Trans. ASABE 2017,
    60, 1827–1838. [Google Scholar] [CrossRef] Chen, L.; Wallhead, M.; Zhu, H.; Fulcher,
    A. Control of Insects and Diseases with Intelligent Variable-Rate Sprayers in
    Ornamental Nurseries. J. Environ. Hortic. 2019, 37, 90–100. [Google Scholar] [CrossRef]
    Fessler, L.; Fulcher, A.; Schneider, L.; Wright, W.C.; Zhu, H. Reducing the Nursery
    Pesticide Footprint with Laser-Guided, Variable-Rate Spray Application Technology.
    HortScience 2021, 141, 1572–1584. [Google Scholar] [CrossRef] Wei, J.; Salyani,
    M. Development of a Laser Scanner for Measuring Tree Canopy Characteristics: Phase
    1. Prototype Development. Trans. Am. Soc. Agric. Eng. 2004, 47, 2101–2107. [Google
    Scholar] [CrossRef] Campbell, J.; Sarkhosh, A.; Habibi, F.; Ismail, A.; Gajjar,
    P.; Zhongbo, R.; Tsolova, V.; El-sharkawy, I. Biometrics Assessment of Cluster-
    and Berry-related Traits of Muscadine Grape Population. Plants 2021, 10, 1067.
    [Google Scholar] [CrossRef] Zhang, R.; Tian, Y.; Zhang, J.; Dai, S.; Hou, X.;
    Wang, J.; Guo, Q. Metric Learning for Image-Based Flower Cultivars Identification.
    Plant Methods 2021, 17, 1–14. [Google Scholar] [CrossRef] [PubMed] Maltoni, D.;
    Maio, D.; Jain, A.K.; Prabhakar, S. Handbook of Fingerprint Recognition; Springer
    Science and Business Media: New York, NY, USA, 2009. [Google Scholar] [CrossRef]
    Garrido, M.; Perez-Ruiz, M.; Valero, C.; Gliever, C.J.; Hanson, B.D.; Slaughter,
    D.C. Active Optical Sensors for Tree Stem Detection and Classification in Nurseries.
    Sens. Switz. 2014, 14, 10783–10803. [Google Scholar] [CrossRef] [PubMed] Shearer,
    S.A.; Holmes, R.G. Plant identification using color co-occurrence matrices. Trans.
    ASAE 1990, 33, 1237–1244. [Google Scholar] [CrossRef] She, Y.; Ehsani, R.; Robbins,
    J.; Leiva, J.N.; Owen, J. Applications of High-Resolution Imaging for Open Field
    Container Nursery Counting. Remote Sens. 2018, 10, 2018. [Google Scholar] [CrossRef]
    Leiva, J.N.; Robbins, J.; Saraswat, D.; She, Y.; Ehsani, R. Evaluating Remotely
    Sensed Plant Count Accuracy with Differing Unmanned Aircraft System Altitudes,
    Physical Canopy Separations, and Ground Covers. J. Appl. Remote Sens. 2017, 11,
    036003. [Google Scholar] [CrossRef] Yuan, X.; Li, D.; Sun, P.; Wang, G.; Ma, Y.
    Real-Time Counting and Height Measurement of Nursery Seedlings Based on Ghostnet–YoloV4
    Network and Binocular Vision Technology. Forests 2022, 13, 1459. [Google Scholar]
    [CrossRef] Gini, R.; Sona, G.; Ronchetti, G.; Passoni, D.; Pinto, L. Improving
    Tree Species Classification Using UAS Multispectral Images and Texture Measures.
    ISPRS Int. J. Geo-Inf. 2018, 7, 315. [Google Scholar] [CrossRef] Weiss, U.; Biber,
    P.; Laible, S.; Bohlmann, K.; Zell, A. Plant Species Classification Using a 3D
    LIDAR Sensor and Machine Learning. In Proceedings of the 2010 Ninth International
    Conference on Machine Learning and Applications, Washington, DC, USA, 12–14 December
    2010; pp. 339–345. [Google Scholar] Alipour, N.; Tarkhaneh, O.; Awrangjeb, M.;
    Tian, H. Flower Image Classification Using Deep Convolutional Neural Network.
    In Proceedings of the 2021 7th International Conference on Web Research (ICWR),
    Tehran, Iran, 19–20 May 2021; pp. 1–4. [Google Scholar] Dharwadkar, S.; Bhat,
    G.; Subba Reddy, N.V.; Aithal, P.K. Floriculture Classification Using Simple Neural
    Network and Deep Learning. In Proceedings of the 2017 2nd IEEE International Conference
    on Recent Trends in Electronics, Information & Communication Technology (RTEICT),
    Bangalore, India, 19–20 May 2017; pp. 619–622. [Google Scholar] Malik, M.; Aslam,
    W.; Nasr, E.A.; Aslam, Z.; Kadry, S. A Performance Comparison of Classification
    Algorithms for Rose Plants. Comput. Intell. Neurosci. 2022, 2022, 1842547. [Google
    Scholar] [CrossRef] [PubMed] Narvekar, C.; Rao, M. Flower Classification Using
    CNN and Transfer Learning in CNN-Agriculture Perspective. In Proceedings of the
    2020 3rd International Conference on Intelligent Sustainable Systems (ICISS),
    Thoothukudi, India, 3–5 December 2020; pp. 660–664. [Google Scholar] Soleimanipour,
    A.; Chegini, G.R. A Vision-Based Hybrid Approach for Identification of Anthurium
    Flower Cultivars. Comput. Electron. Agric. 2020, 174, 105460. [Google Scholar]
    [CrossRef] Gunjal, S.; Waskar, D.; Dod, V.; Bhujbal, B.; Ambad, S.N.; Rajput,
    H.; Hendre, P.; Thoke, N.; Bhaskar, M. Horticulture Nursery Management. 2012.
    Available online: https://k8449r.weebly.com/uploads/3/0/7/3/30731055/horticulture_plant_nursery1-signed.pdf
    (accessed on 20 November 2022). Li, M.; Ma, L.; Zong, W.; Luo, C.; Huang, M.;
    Song, Y. Design and Experimental Evaluation of a Form Trimming Machine for Horticultural
    Plants. Appl. Sci. Switz. 2021, 11, 2230. [Google Scholar] [CrossRef] Zhang, M.;
    Guo, W.; Wang, L.; Li, D.; Hu, B.; Wu, Q. Modeling and Optimization of Watering
    Robot Optimal Path for Ornamental Plant Care. Comput. Ind. Eng. 2021, 157, 107263.
    [Google Scholar] [CrossRef] Sharma, S.; Borse, R. Automatic Agriculture Spraying
    Robot with Smart Decision Making. Adv. Intell. Syst. Comput. 2016, 530, 743–758.
    [Google Scholar] [CrossRef] Prabha, P.; Vishnu, R.S.; Mohan, H.T.; Rajendran,
    A.; Bhavani, R.R. A Cable Driven Parallel Robot for Nursery Farming Assistance.
    In Proceedings of the 2021 IEEE 9th Region 10 Humanitarian Technology Conference
    (R10-HTC), Bangalore, India, 30 September–2 October 2021; pp. 1–6. [Google Scholar]
    Kim, W.S.; Lee, D.H.; Kim, Y.J.; Kim, T.; Lee, W.S.; Choi, C.H. Stereo-Vision-Based
    Crop Height Estimation for Agricultural Robots. Comput. Electron. Agric. 2021,
    181, 105937. [Google Scholar] [CrossRef] Wang, X.; Singh, D.; Marla, S.; Morris,
    G.; Poland, J. Field-Based High-Throughput Phenotyping of Plant Height in Sorghum
    Using Different Sensing Technologies. Plant Methods 2018, 14, 1–16. [Google Scholar]
    [CrossRef] Andújar, D.; Ribeiro, A.; Fernández-Quintanilla, C.; Dorado, J. Using
    Depth Cameras to Extract Structural Parameters to Assess the Growth State and
    Yield of Cauliflower Crops. Comput. Electron. Agric. 2016, 122, 67–73. [Google
    Scholar] [CrossRef] Polder, G.; Hofstee, J.W. Phenotyping Large Tomato Plants
    in the Greenhouse Using a 3D Light-Field Camera. In Proceedings of the 2014 Montreal,
    Quebec, QC, Canada, 13–16 July 2014; American Society of Agricultural and Biological
    Engineers, 2014; p. 1. [Google Scholar] Kerkech, M.; Hafiane, A.; Canals, R.;
    Ros, F. Vine Disease Detection by Deep Learning Method Combined with 3d Depth
    Information. In Proceedings of the International Conference on Image and Signal
    Processing, 9th International Conference, ICISP 2020, Marrakesh, Morocco, 4–6
    June 2020; Springer: Berlin/Heidelberg, Germany, 2020; pp. 82–90. [Google Scholar]
    Gai, J.; Xiang, L.; Tang, L. Using a Depth Camera for Crop Row Detection and Mapping
    for Under-Canopy Navigation of Agricultural Robotic Vehicle. Comput. Electron.
    Agric. 2021, 188, 106301. [Google Scholar] [CrossRef] Gongal, A.; Karkee, M.;
    Amatya, S. Apple Fruit Size Estimation Using a 3D Machine Vision System. Inf.
    Process. Agric. 2018, 5, 498–503. [Google Scholar] [CrossRef] Vázquez-Arellano,
    M.; Paraforos, D.S.; Reiser, D.; Garrido-Izard, M.; Griepentrog, H.W. Determination
    of Stem Position and Height of Reconstructed Maize Plants Using a Time-of-Flight
    Camera. Comput. Electron. Agric. 2018, 154, 276–288. [Google Scholar] [CrossRef]
    Hämmerle, M.; Höfle, B. Direct Derivation of Maize Plant and Crop Height from
    Low-Cost Time-of-Flight Camera Measurements. Plant Methods 2016, 12, 50. [Google
    Scholar] [CrossRef] Vázquez-Arellano, M.; Reiser, D.; Paraforos, D.S.; Garrido-Izard,
    M.; Burce, M.E.C.; Griepentrog, H.W. 3-D Reconstruction of Maize Plants Using
    a Time-of-Flight Camera. Comput. Electron. Agric. 2018, 145, 235–247. [Google
    Scholar] [CrossRef] Li, J.; Tang, L. Developing a Low-Cost 3D Plant Morphological
    Traits Characterization System. Comput. Electron. Agric. 2017, 143, 1–13. [Google
    Scholar] [CrossRef] Pamornnak, B.; Limsiroratana, S.; Khaorapapong, T.; Chongcheawchamnan,
    M.; Ruckelshausen, A. An Automatic and Rapid System for Grading Palm Bunch Using
    a Kinect Camera. Comput. Electron. Agric. 2017, 143, 227–237. [Google Scholar]
    [CrossRef] Cao, Q.; Yang, G.; Duan, D.; Chen, L.; Wang, F.; Xu, B.; Zhao, C.;
    Niu, F. Combining Multispectral and Hyperspectral Data to Estimate Nitrogen Status
    of Tea Plants (Camellia sinensis (L.) O. Kuntze) under Field Conditions. Comput.
    Electron. Agric. 2022, 198, 107084. [Google Scholar] [CrossRef] Chandel, A.K.;
    Khot, L.R.; Yu, L.X. Alfalfa (Medicago sativa L.) Crop Vigor and Yield Characterization
    Using High-Resolution Aerial Multispectral and Thermal Infrared Imaging Technique.
    Comput. Electron. Agric. 2021, 182, 105999. [Google Scholar] [CrossRef] Abbas,
    A.; Jain, S.; Gour, M.; Vankudothu, S. Tomato Plant Disease Detection Using Transfer
    Learning with C-GAN Synthetic Images. Comput. Electron. Agric. 2021, 187, 106279.
    [Google Scholar] [CrossRef] Xiao, D.; Zeng, R.; Liu, Y.; Huang, Y.; Liu, J.; Feng,
    J.; Zhang, X. Citrus Greening Disease Recognition Algorithm Based on Classification
    Network Using TRL-GAN. Comput. Electron. Agric. 2022, 200, 107206. [Google Scholar]
    [CrossRef] Zhang, L.; Nie, Q.; Ji, H.; Wang, Y.; Wei, Y.; An, D. Hyperspectral
    Imaging Combined with Generative Adversarial Network (GAN)-Based Data Augmentation
    to Identify Haploid Maize Kernels. J. Food Compos. Anal. 2022, 106, 104346. [Google
    Scholar] [CrossRef] Mazzia, V.; Khaliq, A.; Salvetti, F.; Chiaberge, M. Real-Time
    Apple Detection System Using Embedded Systems With Hardware Accelerators: An Edge
    AI Application. IEEE Access 2020, 8, 9102–9114. [Google Scholar] [CrossRef] Zhang,
    Y.; Yu, J.; Chen, Y.; Yang, W.; Zhang, W.; He, Y. Real-Time Strawberry Detection
    Using Deep Neural Networks on Embedded System (Rtsd-Net): An Edge AI Application.
    Comput. Electron. Agric. 2022, 192, 106586. [Google Scholar] [CrossRef] Codeluppi,
    G.; Davoli, L.; Ferrari, G. Forecasting Air Temperature on Edge Devices with Embedded
    AI. Sensors 2021, 21, 3973. [Google Scholar] [CrossRef] Coppola, M.; Noaille,
    L.; Pierlot, C.; de Oliveira, R.O.; Gaveau, N.; Rondeau, M.; Mohimont, L.; Steffenel,
    L.A.; Sindaco, S.; Salmon, T. Innovative Vineyards Environmental Monitoring System
    Using Deep Edge AI. Artif. Intell. Digit. Ind.–Appl. 2022, 261–278. [Google Scholar]
    [CrossRef] Aghi, D.; Cerrato, S.; Mazzia, V.; Chiaberge, M. Deep Semantic Segmentation
    at the Edge for Autonomous Navigation in Vineyard Rows. In Proceedings of the
    2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
    Prague, Czech Republic, 27 September–1 October 2021; pp. 3421–3428. [Google Scholar]
    Deng, F.; Zuo, P.; Wen, K.; Wu, X. Novel Soil Environment Monitoring System Based
    on RFID Sensor and LoRa. Comput. Electron. Agric. 2020, 169, 105169. [Google Scholar]
    [CrossRef] Luvisi, A.; Panattoni, A.; Materazzi, A. RFID Temperature Sensors for
    Monitoring Soil Solarization with Biodegradable Films. Comput. Electron. Agric.
    2016, 123, 135–141. [Google Scholar] [CrossRef] Vellidis, G.; Tucker, M.; Perry,
    C.; Kvien, C.; Bednarz, C. A Real-Time Wireless Smart Sensor Array for Scheduling
    Irrigation. Comput. Electron. Agric. 2008, 61, 44–50. [Google Scholar] [CrossRef]
    Dey, S.; Bhattacharyya, R.; Karmakar, N.; Sarma, S. A Folded Monopole Shaped Novel
    Soil Moisture and Salinity Sensor for Precision Agriculture Based Chipless RFID
    Applications. In Proceedings of the 2019 IEEE MTT-S International Microwave and
    RF Conference (IMARC), Mumbai, India, 13–15 December 2019. [Google Scholar] [CrossRef]
    Wang, J.; Chang, L.; Aggarwal, S.; Abari, O.; Keshav, S. Soil Moisture Sensing
    with Commodity RFID Systems. In Proceedings of the MobiSys’20: The 18th Annual
    International Conference on Mobile Systems, Applications, and Services, Toronto,
    ON, Canada, 15–19 June 2020; Volume 13. [Google Scholar] [CrossRef] Aroca, R.V.;
    Hernandes, A.C.; Magalhães, D.V.; Becker, M.; Vaz, C.M.P.; Calbo, A.G. Calibration
    of Passive UHF RFID Tags Using Neural Networks to Measure Soil Moisture. J. Sens.
    2018, 2018, 3436503. [Google Scholar] [CrossRef] Hasan, A.; Bhattacharyya, R.;
    Sarma, S. Towards Pervasive Soil Moisture Sensing Using RFID Tag Antenna-Based
    Sensors. In Proceedings of the 2015 IEEE International Conference on RFID Technology
    and Applications (RFID-TA), Tokyo, Japan, 16–18 September 2015; pp. 165–170. [Google
    Scholar] Yong, W.; Shuaishuai, L.; Li, L.; Minzan, L.; Ming, L.; Arvanitis, K.;
    Georgieva, C.; Sigrimis, N. Smart Sensors from Ground to Cloud and Web Intelligence.
    IFAC-Pap. 2018, 51, 31–38. [Google Scholar] [CrossRef] Barge, P.; Gay, P.; Piccarolo,
    P.; Tortia, C. RFID Tracking of Potted Plants from Nursery to Distribution. In
    Proceedings of the International Conference Ragusa SHWA2010, Ragusa, Italy, 16–18
    September 2010. [Google Scholar] Sugahara, K. Traceability System for Agricultural
    Products Based on RFID and Mobile Technology. IFIP Adv. Inf. Commun. Technol.
    2009, 295, 2293–2301. [Google Scholar] [CrossRef] Voulodimos, A.S.; Patrikakis,
    C.Z.; Sideridis, A.B.; Ntafis, V.A.; Xylouri, E.M. A Complete Farm Management
    System Based on Animal Identification Using RFID Technology. Comput. Electron.
    Agric. 2010, 70, 380–388. [Google Scholar] [CrossRef] Weiss, U.; Biber, P. Plant
    Detection and Mapping for Agricultural Robots Using a 3D LIDAR Sensor. Robot.
    Auton. Syst. 2011, 59, 265–273. [Google Scholar] [CrossRef] Ge, Y.; Xiong, Y.;
    From, P.J. Symmetry-Based 3D Shape Completion for Fruit Localisation for Harvesting
    Robots. Biosyst. Eng. 2020, 197, 188–202. [Google Scholar] [CrossRef] Skoczeń,
    M.; Ochman, M.; Spyra, K.; Nikodem, M.; Krata, D.; Panek, M.; Pawłowski, A. Obstacle
    Detection System for Agricultural Mobile Robot Application Using RGB-D Cameras.
    Sensors 2021, 21, 5292. [Google Scholar] [CrossRef] Ji, W.; Gao, X.; Xu, B.; Chen,
    G.Y.; Zhao, D. Target Recognition Method of Green Pepper Harvesting Robot Based
    on Manifold Ranking. Comput. Electron. Agric. 2020, 177, 105663. [Google Scholar]
    [CrossRef] Gai, R.; Chen, N.; Yuan, H. A Detection Algorithm for Cherry Fruits
    Based on the Improved YOLO-v4 Model. Neural Comput. Appl. 2021, 1–12. [Google
    Scholar] [CrossRef] Jia, W.; Tian, Y.; Luo, R.; Zhang, Z.; Lian, J.; Zheng, Y.
    Detection and Segmentation of Overlapped Fruits Based on Optimized Mask R-CNN
    Application in Apple Harvesting Robot. Comput. Electron. Agric. 2020, 172, 105380.
    [Google Scholar] [CrossRef] Disclaimer/Publisher’s Note: The statements, opinions
    and data contained in all publications are solely those of the individual author(s)
    and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)
    disclaim responsibility for any injury to people or property resulting from any
    ideas, methods, instructions or products referred to in the content.  © 2023 by
    the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
    article distributed under the terms and conditions of the Creative Commons Attribution
    (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Share and Cite
    MDPI and ACS Style Mahmud, M.S.; Zahid, A.; Das, A.K. Sensing and Automation Technologies
    for Ornamental Nursery Crop Production: Current Status and Future Prospects. Sensors
    2023, 23, 1818. https://doi.org/10.3390/s23041818 AMA Style Mahmud MS, Zahid A,
    Das AK. Sensing and Automation Technologies for Ornamental Nursery Crop Production:
    Current Status and Future Prospects. Sensors. 2023; 23(4):1818. https://doi.org/10.3390/s23041818
    Chicago/Turabian Style Mahmud, Md Sultan, Azlan Zahid, and Anup Kumar Das. 2023.
    \"Sensing and Automation Technologies for Ornamental Nursery Crop Production:
    Current Status and Future Prospects\" Sensors 23, no. 4: 1818. https://doi.org/10.3390/s23041818
    Note that from the first issue of 2016, this journal uses article numbers instead
    of page numbers. See further details here. Article Metrics Citations Crossref   4
    Web of Science   2 Scopus   5 Google Scholar   [click to view] Article Access
    Statistics Article access statistics Article Views 8. Jan 18. Jan 28. Jan 7. Feb
    17. Feb 27. Feb 8. Mar 18. Mar 28. Mar 0k 1k 2k 3k 4k 5k For more information
    on the journal statistics, click here. Multiple requests from the same IP address
    are counted as one view.   Sensors, EISSN 1424-8220, Published by MDPI RSS Content
    Alert Further Information Article Processing Charges Pay an Invoice Open Access
    Policy Contact MDPI Jobs at MDPI Guidelines For Authors For Reviewers For Editors
    For Librarians For Publishers For Societies For Conference Organizers MDPI Initiatives
    Sciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS Proceedings
    Series Follow MDPI LinkedIn Facebook Twitter Subscribe to receive issue release
    notifications and newsletters from MDPI journals Select options Subscribe © 1996-2024
    MDPI (Basel, Switzerland) unless otherwise stated Disclaimer Terms and Conditions
    Privacy Policy"'
  inline_citation: (Mahmud et al., 2023)
  journal: Sensors
  limitations: '1. The review only covers sensing and automation technologies for
    ornamental nursery crop production, and does not discuss other aspects of precision
    agriculture, such as data management and decision support systems.

    2. The review is based on a limited number of studies, and some of the findings
    may not be generalizable to the entire industry.'
  main_objective: To review the current state and future potential of sensing and
    automation technologies for ornamental nursery crop production.
  relevance_evaluation: '7.2. Advanced Monitoring Techniques for Automated Irrigation
    Systems


    This section discusses the challenges and strategies for integrating automated
    systems with existing irrigation infrastructure and other precision agriculture
    technologies, highlighting the importance of interoperability and standardization
    in enabling seamless communication and compatibility. The section also presents
    a survey of current research and development efforts in advanced monitoring techniques
    for automated irrigation systems, including the use of sensors, computer vision,
    and artificial intelligence (AI) for monitoring soil moisture, plant water status,
    and environmental conditions. The section concludes with a discussion of the potential
    benefits and challenges of adopting advanced monitoring techniques in the nursery
    crop industry.'
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  study_location: Not explicitly stated in the provided context, but the authors are
    affiliated with institutions in Tennessee, USA.
  technologies_used: Sensors, computer vision, artificial intelligence, machine learning,
    Internet-of-Things, robotics
  title: 'Sensing and Automation Technologies for Ornamental Nursery Crop Production:
    Current Status and Future Prospects'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Che, P. (2022). Water Conservancy Automation Monitoring System Based
    on VR Image Video and Internet of Things. Mobile Information Systems, 2022, 2008733.
    https://doi.org/10.1155/2022/2008733.
  authors:
  - Che P.
  citation_count: '1'
  data_sources: Not specified in the provided text.
  description: With the rapid development of science and technology and the widespread
    application of automatic control and wireless transmission technology, water conservancy
    automation monitoring system plays an important role in agricultural water conservancy
    and the automatic water conservancy monitoring system can conduct automatic water
    conservancy irrigation and monitor the growth of crop, greatly improving the efficiency
    of water resources management. However, there is still a certain gap between China
    and developed countries, mainly reflected in the two aspects of imperfect information
    resources and low level of data analysis and application, there are still technical
    deficiencies, and the training of high-end talents is relatively scarce, so there
    is still a big gap. The purpose of this article is to study the water conservancy
    automation monitoring system based on VR image video and the Internet of Things.
    This article first analyzes virtual reality technology and introduces Unity3D,
    the development engine of virtual reality, then introduces the Internet of Things
    and its key technologies, studies the classification of existing water conservancy
    automation systems, and then details the use of C/S Structured water conservancy
    automatic monitoring system. On this basis, this article combines VR and IoT technologies
    in the experimental part to design and implement a water conservancy automation
    monitoring system. Experimental results prove that the system designed in this
    article has certain theoretical and practical value for the development of water
    conservancy informatization. In this article, by creating 1000 concurrent users,
    the performance of the designed system is tested, and the response time of this
    system is 3.4 seconds.
  doi: 10.1155/2022/2008733
  explanation: The purpose of the study is to investigate the use of virtual reality
    (VR) and Internet of Things (IoT) technologies to design and implement a water
    conservancy automation monitoring system. The system is intended to monitor and
    control water resources efficiently.
  extract_1: '"This article uses VR and IoT technology to design the main structure
    of the water conservancy automation monitoring system and discusses the realization
    of its main functions. The design and application of this system are of great
    significance to the reliable, safe, accurate operation of water conservancy systems
    and the automation of water conservancy projects."'
  extract_2: “The performance test of the water conservancy automation system is carried
    out in this article. The system module test results are all passed. The performance
    test shows that the response time reaches 3.4 seconds when the number of virtual
    users reaches 1000, which shows that the throughput has reached 1000/0.75 = 1333,
    far exceeding the concurrency requirement of the system, should meet 1000 users
    to access the same time, and will not crash; the system reaction time can be within
    5 s."
  full_citation: '>'
  full_text: '>

    "This website stores data such as cookies to enable essential site functionality,
    as well as marketing, personalization, and analytics. By remaining on this website
    you indicate your consent. Cookie Policy Journals Publish with us Publishing partnerships
    About us Blog Mobile Information Systems Journal overview For authors For reviewers
    For editors Table of Contents Special Issues Mobile Information Systems/ 2022/
    Article On this page Abstract Introduction Discussion Conclusions Data Availability
    Conflicts of Interest References Copyright Related Articles Special Issue 5G/6G
    Networks in Unmanned Aerial Vehicle Internet of Things Communication Systems View
    this Special Issue Research Article | Open Access Volume 2022 | Article ID 2008733
    | https://doi.org/10.1155/2022/2008733 Show citation Water Conservancy Automation
    Monitoring System Based on VR Image Video and Internet of Things Pengfei Che 1
    Show more Academic Editor: Imran Shafique Ansari Received 16 May 2022 Revised
    02 Jul 2022 Accepted 14 Jul 2022 Published 17 Aug 2022 Abstract With the rapid
    development of science and technology and the widespread application of automatic
    control and wireless transmission technology, water conservancy automation monitoring
    system plays an important role in agricultural water conservancy and the automatic
    water conservancy monitoring system can conduct automatic water conservancy irrigation
    and monitor the growth of crop, greatly improving the efficiency of water resources
    management. However, there is still a certain gap between China and developed
    countries, mainly reflected in the two aspects of imperfect information resources
    and low level of data analysis and application, there are still technical deficiencies,
    and the training of high-end talents is relatively scarce, so there is still a
    big gap. The purpose of this article is to study the water conservancy automation
    monitoring system based on VR image video and the Internet of Things. This article
    first analyzes virtual reality technology and introduces Unity3D, the development
    engine of virtual reality, then introduces the Internet of Things and its key
    technologies, studies the classification of existing water conservancy automation
    systems, and then details the use of C/S Structured water conservancy automatic
    monitoring system. On this basis, this article combines VR and IoT technologies
    in the experimental part to design and implement a water conservancy automation
    monitoring system. Experimental results prove that the system designed in this
    article has certain theoretical and practical value for the development of water
    conservancy informatization. In this article, by creating 1000 concurrent users,
    the performance of the designed system is tested, and the response time of this
    system is 3.4 seconds. 1. Introduction With the rapid development of the national
    economy, people’s living standards have greatly improved, and society’s demand
    for water resources has increased in both quantity and quality. However, due to
    the vast territory, large population, and obvious monsoon climate characteristics
    of the East Asian continent, the main feature of the monsoon climate in East Asia
    is the rotation of the monsoon and the obvious seasonal change in precipitation,
    uneven spatial and temporal distribution of precipitation, and complicated underlying
    surface conditions, drought and water shortages, flood disasters, soil erosion,
    and water pollution are still frequent in China. Therefore, the consequences are
    more serious. In the face of the severe water shortage and water pollution problems,
    people should take active actions to cherish every drop of water, adopt water-saving
    technology, and prevent and control water pollution and afforestation. The proposal
    of digital water conservancy has gradually made people realize that informatization
    is the main trend of economic and social development in today’s world. The frontier
    research field of the digital conservancy is the digital watershed. Although the
    digital earth is an important technical background proposed by the digital water
    conservancy, the natural extension of the digital earth is not the digital water
    conservancy but the digital river basin. Water conservancy automation refers to
    the full use of modern information technology, in-depth development, and extensive
    use of water conservancy information resources in the field of water conservancy
    engineering, strengthening project quality management, promoting information and
    digital data collection and processing, including the historical process of collection,
    transmission, and processing the water storage; service and remote monitoring
    of water conservancy facilities will comprehensively improve the efficiency and
    benefits of water conservancy projects. Guan’s team believed that considering
    the compromise between conflicting goals in water conservancy projects in an uncertain
    environment is a difficult task. They proposed two new piecewise functions, namely,
    double exponential function and quadratic function, to simulate the relationship
    between the construction quality of the project and the limit time, and developed
    a fuzzy multimode discrete time cost-quality-tradeoff model project. The highest
    quadratic function must be quadratic. The image of the quadratic function is a
    parabola that is parallel to the symmetry axis or coincides with the y-axis. The
    biexponential function refers to the function formed by raising the exponential
    function to the exponential function, which generally grows much faster than the
    same exponential base number exponential function. The model solves the NP-hard
    problem and uses particle swarm optimization to obtain the optimal solution; NP-hard
    refers to the problem in which all NP problems can rule within the polynomial-time
    complexity [ 1]. Damgrave believed that virtual reality (VR) is a manual copy
    of potential reality or conditions of use, enabling users to experience and/or
    modify and/or interact with it. These computer-simulated environments are mainly
    experienced through the perception of vision and sound. The VR system has the
    following key features: 3D representation and perception of real-time spatial
    interaction presence and immersion. VR handles real-time integration between computer
    simulation environments and human interaction. The VR environment stimulates the
    correct feeling at the appropriate time, thereby stimulating the user to experience
    the assumed reality or the use conditions [ 2]. Shancang et al. team believed
    that the Internet of Things (IoT) will support the connected Internet of Things
    with new features. They systematically reviewed the definition, architecture,
    basic technologies, and applications of IoT. First, they introduced various definitions
    of the IoT; second, they discussed emerging technologies for implementing the
    IoT. Third, it discusses some unresolved problems related to the application of
    the IoT; finally, the main challenges and corresponding potential solutions that
    the research community needs to solve are studied [ 3]. This article first analyzes
    the VR technology and its characteristics and describes the VR development engine
    Unity3D, then analyzes the IoT and its key technologies, then studies the classification
    of existing water conservancy automation systems, and details the use of the C/S
    structure of water conservancy automation monitoring system. The Client-Server
    (C/S) structure usually adopts a two-layer structure. The server is responsible
    for data management, and the client is responsible for interacting with users.
    On this basis, this article combines VR and IoT technologies to design and implement
    a water conservancy automation monitoring system. This article proves through
    experiments that the system has certain theoretical and practical value for the
    development of water conservancy informatization. The system has a highly interactive,
    safe access mode and fast response speed and is conducive to processing a large
    amount of water conservancy information data. 2. Proposed Method 2.1. Virtual
    Reality Virtual reality (VR) is a new technology that has emerged in recent years.
    It combines computer technology, multimedia technology, image technology, simulation
    technology, and various electronic technologies to form a new technology in the
    computer field [ 4, 5]. Simply put, virtual reality (VR), where the scenes and
    characters seen are all fake, is to put your consciousness into a virtual world.
    Augmented reality (AR), where the scenes and characters seen are part true or
    part false, is to bring virtual information into the real world. VR technology
    is a computer simulation system that can build and let people experience the virtual
    world. The virtual world it builds has a strong simulation effect. VR technology
    is a very challenging interactive technology. VR technology includes computer,
    electronic information, and simulation technology; its basic implementation is
    the computer simulation of the virtual environment to give people a sense of environmental
    immersion. With the continuous development of social productivity and science
    and technology, the demand for VR technology in all walks of life is increasingly
    strong. The conceptual model of VR is shown in Figure 1.    Figure 1  Conceptual
    model of virtual reality. 2.1.1. The Composition of Virtual Reality VR technology
    includes not only helmet, data gloves, and data clothing but also all related
    technologies and methods with natural simulation and real experience characteristics
    [ 6, 7]. It is very necessary to construct a harmonious human-machine environment
    that is closely similar to the objective environment, transcends objective time
    and space, can be immersed in it, and can be controlled in it [ 8]. In recent
    years, the first live 9DVR experience hall built based on VR technology has been
    realized. Since its completion, the first live 9DVR experience hall has had a
    great influence on the film and television entertainment market. This experience
    hall can make the viewer feel like being in a real scene and immerse it in the
    virtual environment created by the film. Real experience and natural human-computer
    interaction are its most important goals. A system that can partially or fully
    achieve this goal is called a VR system. A typical VR system should include the
    following five parts: virtual world, computer, VR software, and input and output
    devices. The composition of VR is shown in Figure 2.    Figure 2  The composition
    of virtual reality. 2.1.2. Features of Virtual Reality Immersion, interactivity,
    and imagination are the three basic characteristics of VR technology [ 9, 10].
    In the VR system, people’s leading role is further emphasized from only observing
    the processing results from the outside of the computer, to immersing in the virtual
    environment constructed by the computer, from using only the keyboard and mouse
    to interact with the single-dimensional digital information in the computing environment
    to the multidimensional information environment interaction [ 11, 12]. Immersion,
    also known as presence, refers to how much the user feels as the protagonist in
    the simulated environment. The ideal simulation environment should make it difficult
    for users to distinguish between true and false so that users can devote themselves
    to the three-dimensional virtual environment created by the computer. Everything
    in the environment looks real and sounds real, and movement is real; even everything
    that smells, tastes, etc. is real, just like in the real world. Interactivity
    is the user’s operability of objects in the simulated environment and the natural
    degree of environmental feedback (including real-time). For example, users can
    use their hands to directly grab virtual objects in a simulated environment. At
    this time, the hands feel like they are holding something; they can feel the weight
    of the object. Objects in the field of view can also move immediately as the hand
    moves. Imagination emphasizes that VR technology should have a wide imagination
    space and widen the scope of human cognition and not only can reproduce the real
    existence environment but also freely conceive the objective nonexistent or impossible
    existence environment. Since the initial characters of the three features of English
    words are immersion, interactivity, and conceptualization, they are all I, so
    these three features are also collectively referred to as 3I features. 2.1.3.
    Unity3D Unity3D was originally a game engine because it has virtual 3D scenes,
    physics engines, 3D sound, and other virtual reality engines and is used as a
    development engine by many virtual reality developers at home and abroad, including
    VR shopping, education, tourism, sports, and real estate [ 13, 14]. Unity3D supports
    models exported by 3D modeling software, such as 3DMAX, which can be easily imported
    into Unity3D projects and can be readjusted in Unity3D to modify the model directly
    in the Unity3D interface. In Unity3D’s development mode, the visual model in the
    interface is used as the main drag and drop, and the program code is used as a
    script to supplement the complexity of the software and speed up the development
    cycle. So Unity3D is not only a 3D modeling software but also a game development
    engine and a VR graphical development software. Uniyt3D supports both C# and JavaScript
    programming languages. Developers can quickly write program scripts and assign
    them to model objects through a graphical interface. With a convenient editor,
    VR software can be developed in a short time. (1) View of Unity3D. Scene view:
    the scene view is an interactive view. All objects, environments, and cameras
    in VR are edited in this view, and they can be simply panned, dragged, and enlarged
    by the mouse. In the scene, the camera is equivalent to the human eye. When the
    system is running, the content that the user sees through the screen comes from
    the camera. In the scene view, the camera can be moved, which is convenient to
    find the most suitable perspective and increase the reality of the experience.
    The sky box and lighting have a very large auxiliary effect on the experience
    when the software is running, and it is also very important to edit them in advance.
    Game view: when testing the running effect, you will use the game view. The scene
    you see here is the same as the game running time. With its help, the developer
    can quickly lay out and preview the scene. The camera is the user’s eye. When
    you need to switch scenes frequently, modify the weight value of each camera.
    The larger weight will be displayed first, and the lower weight will not be displayed,
    which is great for fast switching scenes. FPS can evaluate the rendering effect
    when the scene is running. Usually, developers will print it to the upper left
    corner and optimize the rendering while observing it. When the game is running,
    the changes will not take effect; developers can debug the program at run time.
    Hierarchical panel: the hierarchical panel displays all objects in the scene in
    the form of a tree list. It has a one-to-one correspondence with the objects in
    the scene view. Both of these panels can delete objects. The hierarchical panel
    also has a role in setting the hierarchical relationship between objects. In Unity3D,
    if the relationship between two objects is a parent-child relationship, the child
    objects will also move when the parent object moves. For example, if the camera
    is set as a subobject of a bicycle when the bicycle moves, the camera will also
    follow the movement, and the two objects are relatively stationary, but when the
    camera makes a selection, the bicycle will not follow the selection. With this
    feature, you can simulate a person riding a bicycle operation. Project panel:
    the project panel is a mapping of local folders. After importing the resource
    folder, they can be managed in the project panel. Virtual objects are generally
    saved in Prefab format. When an object appears multiple times in a virtual scene,
    each style can be different. Their parent only needs to save one, which can facilitate
    the management and utilization of resources. There is a search button at the top
    of the panel; you can quickly search for materials and textures, saving search
    time. View panel: after selecting an object in the hierarchy bread or project
    panel, you can view the detailed properties of the object in the view panel, such
    as coordinates, rotation, and zoom, as well as detailed parameter settings. The
    script in Unity3D generally adopts the custom mode by default; drag and drop the
    script into the inspection panel, and the game parameter setting can be performed.
    The view panel also supports the preview function. For textures and materials,
    you can preview them before deciding whether to add them to the view. (2) Features
    of Unity3D. As a powerful VR engine, Unity3D can use our many features to optimize
    our development process during development. Its characteristics are as follows:
    External resource import function: Unity3D can make simple modifications to the
    model, but it is still much worse than no professional modeling software. Unity3D
    provides a powerful external resource import function, and popular modeling and
    animation software, such as models and animation files exported by tools such
    as 3DS MAX, Maya, and Blender, can be well integrated into Unity3D, using its
    own editing tools. Make simple modifications to achieve the best results. Physics
    special effects: Unity3D has a physics engine, imitating Newtonian mechanics,
    using parameters such as mass, gravity, speed, and friction to set different gravity
    parameters for an object; you can clearly see the different changes. Unity3D has
    a built-in PhysX physics engine developed by NVIDIA, which can accurately and
    conveniently develop the required physical special effects, which improves the
    development efficiency. Script support: script is a key language for Unity3D to
    achieve interaction. It is different from general development languages. Scripts
    act like hearts in virtual reality scenes, controlling the movement of objects,
    the movement of characters, sunrise and sunset, and changes in behavior. Unity3D
    supports three scripting languages: C# is one of the most widely used. The function
    of the C# game script inherits from MonoBehaviour class, which can be run in Unity3D.
    Like native C#, it has object-oriented programming and network communication programming.
    Particle system: realistic scenes and smooth running of the system are important
    criteria for measuring the virtual reality engine. Unity3D integrates the particle
    system, which can simulate realistic effects such as heavy snow and flames, eliminating
    the need for complex programming. The particle system continuously deforms and
    moves, the old particles are destroyed, and new particles are generated to form
    a dynamic particle effect. The particle system has been integrated into Unity3D
    and can be quickly set up directly during development. Delayed rendering: delayed
    lighting systems can improve rendering performance. Even if hundreds of point
    light sources are created in the scene, now only a small amount of performance
    loss is required to complete this complex task. Delayed lighting uses a G buffer.
    For some reused effects, there is no need to generate them again. Reading from
    the buffer area can quickly complete the rendering effect without too much additional
    performance loss. 2.2. Internet of Things 2.2.1. Internet of Things The IoT is
    a device that uses information sensing equipment such as wireless network sensing
    technology, radio frequency identification technology equipment, laser scanners,
    global positioning systems, and other digital and networking objects and connects
    the Internet to any item through a specific network protocol, and it can carry
    out the information interaction between objects, objects, people, and people and
    finally form a network of intelligent identification, positioning, tracking, management,
    and monitoring of items [ 15, 16]. 2.2.2. Internet of Things Architecture and
    Key Technologies The IoT enables objects to possess certain wisdom through the
    realization of communication between people and things, which is a unique value
    embodiment of the IoT and a new feature [ 17, 18]. The architecture of the IoT
    includes a perception layer, a network layer, and an intelligence layer, that
    is, comprehensive perception of objects, transmission and sharing of sensed information,
    and intelligent processing. How to promote the better development of the IoT at
    this stage and apply the advanced technology of the IoT to all aspects of life
    requires that we can master the key technologies in the IoT system. (1) Key Technologies
    of Perception Layer. The perception layer realizes the basic recognition of objects
    and an extensive collection of data. There are the following four key technologies.
    RFID radio frequency identification technology is a noncontact automatic identification
    technology with convenient operation, small identification error, and strong anti-interference.
    It consists of a tag that can store information, a reader that reads and writes
    the tag data, and an antenna that receives the transmitted signal. RFID technology
    uses radio frequency to automatically identify communication objects and perform
    contactless two-way communication. When the tag is outside the range of the reader,
    it is in a dormant state, and once it enters this range, it will be activated,
    and the information will be transmitted through the antenna. The reader will decode
    and enter the received information and then transmit it to the computer system
    for processing. WSN (Wireless Sensor Network) technology is composed of a large
    number of miniature wireless sensor nodes deployed in the monitoring area, and
    these nodes will form a network system through wireless communication to realize
    the perception, collection, and processing of object information in the monitoring
    area, and transmit the information to the monitor. GIS (Geographic Information
    System), which is based on geographic data, supplemented by a computer system,
    manages geographic data and can provide management information systems for all
    industries to manage maps and assist decision-making services. GPS (Global Positioning
    System) is a positioning method that combines modern communication technology,
    satellite positioning, and navigation. It can realize real-time, uninterrupted,
    and accurate navigation and positioning. GPS can be combined with a communication
    network to monitor and track items in circulation. (2) Key Technologies at the
    Network Layer. The middle layer of the IoT architecture is the network layer,
    which is mainly used to realize the important layers of data exchange, information
    transmission, information feedback, routing, and system control between the perception
    layer and the intelligent layer. After the data information is transmitted from
    the perception layer to the network layer, the network layer can integrate the
    received data information and transmit it through mobile communication technologies
    such as 3G/4G/5G or wireless network communication technologies such as WIFI.
    The main technologies are 3G/4G/5G mobile communication technology, WIFI technology,
    LAN technology, and Internet technology. (3) Key Technologies of the Intelligent
    Layer. The last layer in the architecture of the IoT is the intelligent layer,
    which can provide services for users and improve user satisfaction with its powerful
    information processing capabilities and intelligence. The main technology is cloud
    computing technology, which is a supercomputing model that uses its intelligent
    data storage, computing, and search capabilities to transform resources into a
    form of service and then improve it for users, reducing operating costs. 2.3.
    Water Conservancy Automation Monitoring System Water conservancy automatic monitoring
    system is an inevitable product of technological development and technological
    progress [ 19]. The so-called automatic monitoring of water conservancy is the
    process of analyzing and judging important data that needs real-time monitoring
    and mastering in water conservancy work through hardware presets and software
    programming so as to automatically make control actions [ 20, 21]. The system
    replaces the physical work of water conservancy workers or a part of auxiliary
    mental work. Water conservancy automation monitoring system is generally divided
    into six major parts: dam safety automatic monitoring system, rainwater condition
    automation monitoring system, gate automation monitoring system, pump station
    automation monitoring system, hydropower station automation monitoring system,
    and farmland water conservancy automation monitoring system [ 22, 23]. The water
    conservancy automation monitoring system that has been built and put into use
    generally adopts a C/S structure. The system structure is roughly divided into
    monitoring center subsystem, on-site monitoring terminal subsystem, and data transmission
    channel subsystem. The monitoring center subsystem is the core of the entire system,
    generally including the monitoring host (server or industrial computer), output
    devices (printers, audio equipment, and monitoring large screen), system software,
    and databases [ 24, 25]. The system operator monitors the system data in real
    time through the operating platform software. The monitoring platform software
    is mainly for database design, R&D of control service programs connected to the
    on-site monitoring terminal subsystem, and R&D of user-side service modules. The
    monitoring subsystem generally makes full use of data fusion, integration and
    management technology, data search and query technology, and network communication
    technology to keep the control service program connected to each monitoring terminal,
    24-hour uninterrupted operation, providing information collection and control
    such as flow soil moisture, and can save the collected information directly to
    the database. The system operator can conveniently perform data management, query,
    statistics, compilation, and output on the data in the database through the user
    service module and can control the on-site monitoring terminal subsystem through
    the control service program to realize manual collection and real-time collection.
    The on-site monitoring terminal subsystem is generally various sensors, such as
    water level gauge, rain gauge, soil moisture sensor, and camera. These sensors
    convert all kinds of monitoring data into standard signals, such as analog signals,
    485 serial port signals, optical signals, transmit them to communication terminals,
    and then transmit them to the monitoring platform of the monitoring center subsystem
    through the data transmission channel subsystem for data processing and storage.
    Most water conservancy monitoring data transmission channel subsystems are mainly
    wired data channels, and wired data channels are mainly divided into electrical
    signal data channels and optical signal data channels. The electrical signal data
    channel is divided into an analog signal and a digital signal. The analog signal
    is mainly a video analog signal and 4–20 mA communication analog signal, and the
    digital signal includes RS-485, RS-232, and video digital signal. The optical
    signal mainly refers to the signal transmitted in the optical cable. Because the
    optical cable has a multimode and single mode, the optical signal also has a multimode
    optical signal and a single-mode optical signal. A single-mode optical cable is
    suitable for systems with long communication distances, while a multimode optical
    cable is suitable for systems with distances within 3 kilometers. Optical signal
    transmission speed is fast, and the channel bandwidth is the widest among all
    communication channels. Therefore, optical cables have always been the first choice
    for important main communication channels, such as intercontinental optical cables
    and national main communication optical cables. However, optical signals can only
    be compiled and accepted by mainstream communication equipment after they are
    converted into standard digital signals by optical transceivers or photoelectric
    switches. Therefore, optical signals are an extension and auxiliary transmission
    method of electrical signals. 3. Experiments 3.1. Data Collection A hydropower
    station is a third-level hydropower station with an installed capacity of 2 × 2000 KW.
    Water sources mainly come from irrigation and drainage, flood discharge in flood
    season, and water supply from water conservancy projects. The reservoir has completed
    the danger elimination and reinforcement project, which will not only ensure the
    irrigation water in the area is effectively guaranteed but also make the social
    and economic benefits of the construction of the hydropower station prominent.
    The data in this article are derived from the data generated during the daily
    operation of the hydropower station. 3.2. Experimental Environment 3.2.1. Development
    Environment The development environment of the experiment in this article is divided
    into hardware environment and software environment, including server and development
    tools. The development environment configuration is shown in Table 1. Table 1  Development
    environment configuration. 3.2.2. Component Equipment Because the equipment is
    mostly electronic products, the monitoring computer room should be far away from
    strong magnetic fields, pollution sources, and dust; install the fire alarm device
    in the computer room, the fire extinguisher should be placed in a convenient place,
    and the entrance should be kept clear. The system components designed in this
    article are shown in Table 2. Table 2  Components. 3.3. System Architecture The
    water conservancy automation system can be divided into a monitoring center unit
    (including communication network and monitoring management software part), local
    monitoring unit (working condition observation and hydrometeorological monitoring),
    video monitoring unit, and lightning protection unit. The overall functional structure
    of the water conservancy automation system is shown in Figure 3.    Figure 3  Overall
    functional structure of water conservancy automation system. The water conservancy
    automation system mainly includes water and rain data collection system, dam safety
    monitoring system, gate monitoring system, video monitoring system data collection,
    transmission, management, and historical data statistics, to achieve a comprehensive
    information query system, to achieve local area water conservancy management information
    query, basic functions such as information output and data export, provide information
    query for remotely connected superior departments through remote transmission
    software, and provide data basis for water conservancy scheduling. 4. Discussion
    4.1. System Function Module 4.1.1. Hydrometeorological Monitoring Module Hydrometeorological
    monitoring is an important basis for dam safety monitoring of water conservancy
    facilities, water regime forecast, flood control safety monitoring, operation
    of water conservancy facilities, water resource utilization, and regional flood
    control and drought prevention and command dispatching. According to catchment
    area and the terrain characteristics of water conservancy facilities, the construction
    of the dam before water rainfall regime monitoring stations, meteorological and
    hydrological database, collection and processing module and information monitoring
    technology, network technology, computer technology to the water conservancy facilities
    (reservoir, hydropower, irrigation area, river basin, river, lake) site of water
    level, rainfall, wind direction wind speed, pressure, temperature, and flow rate,
    such as the automatic real-time data acquisition, transmission, and storage. Timely
    and accurately obtain the water and rain information in the monitoring area, and
    make corresponding alarms based on the early warning value to provide a decision-making
    basis for the management center’s water-related scheduling. The hydrometeorological
    monitoring module is shown in Figure 4.    Figure 4  Hydrometeorological monitoring
    module. It can be seen from Figure 4 that the hydrometeorological monitoring system
    mainly includes three modules: real-time hydrometeorological data monitoring,
    rainfall monitoring, and flow monitoring. This article shows the real-time monitoring
    page of hydrometeorological data. The module includes three measuring points:
    the measuring point of the water discharge tower, the measuring point of the main
    dam, and the measuring point of the reservoir area. It can be seen from the monitoring
    data that the water level measured by the water discharge tower of the reservoir
    is 190 meters, and the rainfall during the period is 3.5 millimeters. The measured
    flow of water from the main dam was 20 cubic meters per second. The rainfall in
    the reservoir area during the measured period was 5 mm. 4.1.2. Dam Safety Monitoring
    System The dam safety monitoring system is mainly responsible for collecting,
    processing, and storing the monitoring data of the local collection unit, that
    is, the osmometer and other sensors, providing data support for statistical analysis
    of the data, so that the system can analyze the data to complete the construction
    of the mathematical model, scientifically realize the dam safety model, and do
    a good job of dam safety operation and management. At the same time, make corresponding
    alarms based on the early warning value, and provide a decision basis for the
    management center’s water conservancy-related dispatch. The dam safety monitoring
    system is shown in Figure 5.    Figure 5  Dam safety monitoring system. As shown
    in Figure 5, after entering the dam safety monitoring screen, the user can check
    the seepage pressure status of each section. The system realizes the alarm by
    setting the upper limit of seepage pressure. The main functions of the dam safety
    monitoring system are safety monitoring information query, dam safety monitoring,
    and dam operation safety analysis. Among them, the safety monitoring information
    query mainly includes querying various monitoring information of the safe operation
    of water conservancy facilities, dynamically displaying the data collected by
    the water facility safety monitoring system and the real-time monitoring system
    and generating corresponding process curves; dam safety monitoring mainly includes
    the safety of water conservancy facilities. The monitoring system and real-time
    monitoring system combine to realize automatic control processes such as flood
    discharge process monitoring; dam operation safety analysis mainly includes the
    design standards of water conservancy facilities, rain conditions, construction
    conditions, water conservancy facilities, dam safety monitoring, and history of
    water conservancy facilities. The stability analysis of the database and the dam
    analyzes the safety of the dam and warns of possible dangers. 4.2. System Function
    Module and Performance Test 4.2.1. Gate Monitoring System The motor switch is
    controlled by triggering the PLC input and output module, and the opening data
    collected by the travel switch is used to realize the remote control of the water
    conservancy facility gate. Users can make scheduling decisions based on the above
    water and rain conditions and dam monitoring and collection data and send gate
    switch commands through the management center computer to automatically realize
    gate monitoring and management. The gate monitoring system is shown in Figure
    6.    Figure 6  Gate monitoring system. As shown in Figure 6, the gate control
    screen displays the lift of the gate in an animated way through the effect simulation
    diagram and displays the opening value in real time; in the gate control area,
    the gate can be automatically raised and lowered to the desired position by setting
    the opening value. The gate monitoring system is a system that remotely monitors
    the gate up or down status, opening status, and fault alarm status and provides
    a good man-machine interface display through graphics. The gate monitoring system
    is also based on the basic data of the local collection unit and the realization
    of all functions. All come from the local collection unit platform. 4.2.2. Analysis
    of System Performance Test Results The performance test of the system is to test
    whether the real performance of the system can meet the requirements of users.
    The main concern is whether the response time of the entire system, CPU, and RAM
    utilization rate can reach the standard under the busy usage of collective users.
    The water conservancy automation monitoring system designed in this article is
    tested on the server with the LoadRunner tool. The system performance test results
    are shown in Table 3 and Figure 7. Table 3  System performance test results.    Figure
    7  System performance test results. As shown in Table 3 and Figure 7, when related
    to business operations, the average response time of the system is within 3 seconds;
    if a large number of statistical data analyses take place, the average response
    time is allowed within 5 seconds. When the number of concurrent users is 1000,
    the CPU usage of the application server and database server must not exceed 60%,
    and the memory usage cannot exceed 60%. 5. Conclusions (1) This article uses VR
    and IoT technology to design the main structure of the water conservancy automation
    monitoring system and discusses the realization of its main functions. The design
    and application of this system are of great significance to the reliable, safe,
    accurate operation of water conservancy systems and the automation of water conservancy
    projects. It also provides a reference for the automatic operation of urban flood
    control systems and sluice systems. (2) The performance test of the water conservancy
    automation system is carried out in this article. The system module test results
    are all passed. The performance test shows that the response time reaches 3.4
    seconds when the number of virtual users reaches 1000, which shows that the throughput
    has reached 1000/0.75 = 1333, far exceeding the concurrency requirement of the
    system, should meet 1000 users to access the same time, and will not crash; the
    system reaction time can be within 5 s. (3) Although this article implements a
    water conservancy automation system based on VR and the IoT, it needs to further
    improve the immersion of the system. One of the outstanding advantages of VR technology
    is that it can give participants a sense of immersion, thus improving the efficiency
    of management. The higher the immersion of the system is, the greater the help
    to the manager is, and the more focused the manager’s attention is; thus, it is
    possible to achieve more efficient results. Data Availability This article does
    not cover data research, and no data were used to support this study. Conflicts
    of Interest The author declares that there are no conflicts of interest. References
    H. Guan, Z. Li, and J. Wang, “Fuzzy multi-mode time-cost-quality tradeoff optimization
    in water conservancy project,” Shuili Fadian Xuebao/journal of Hydroelectric Engineering,
    vol. 34, no. 7, pp. 80–87, 2015. View at: Google Scholar R. Damgrave, “Virtual
    reality,” Academic Medicine, vol. 72, no. 12, pp. 1076–1081, 2016. View at: Google
    Scholar S. Li, L. D. Xu, and S. Zhao, “The internet of things: a survey,” Information
    Systems Frontiers, vol. 17, no. 2, pp. 243–259, 2015. View at: Publisher Site
    | Google Scholar J. Lili, Q. I. Qingwen, and Z. Kai, “The discussion of drawing
    characteristics and values of water conservancy maps in ming and qing dynasty,”
    Journal of Geo-Information Science, vol. 18, no. 1, pp. 39–48, 2016. View at:
    Google Scholar S. Murtadha, I. Yusoff, and R. Fauzi, “AHP based modification DRASTIC
    model for depth to water table and topography in coastal area - a case study in
    West Aceh,” Taiwan Water Conservancy, vol. 63, no. 2, pp. 91–103, 2015. View at:
    Google Scholar W. Jia-Sheng, L. U. Jin-You, M. Feng-Yang, and Z. Kong-Xian, “Challenges
    and countermeasures for water conservancy combined with schistosomiasis prevention
    and control in China in new era,” Zhongguo xue xi chong bing fang zhi za zhi =
    Chinese journal of schistosomiasis control, vol. 29, no. 3, pp. 259–262, 2017.
    View at: Publisher Site | Google Scholar W. Zhu and G. Fan, “Application of computer
    virtual reality technology in virtual tour,” International Journal of Advanced
    Media and Communication, vol. 6, no. 2/3/4, pp. 273–282, 2016. View at: Publisher
    Site | Google Scholar T. Komura, R. W. H. Lau, M. C. Lin, A. Majumder, D. Manocha,
    and W. W Xu, “Virtual reality software and technology,” IEEE Computer Graphics
    and Applications, vol. 35, no. 5, pp. 20-21, 2015. View at: Publisher Site | Google
    Scholar N. Vaughan, V. N. Dubey, T. W. Wainwright, and R. G Middleton, “A review
    of virtual reality based training simulators for orthopaedic surgery,” Medical
    Engineering & Physics, vol. 38, no. 2, pp. 59–71, 2016. View at: Publisher Site
    | Google Scholar R. Anderson, D. Gallup, J. T. Barron et al., “Barron. JUMP: virtual
    reality video,” ACM Transactions on Graphics, vol. 35, no. 6, pp. 1–13, 2016.
    View at: Publisher Site | Google Scholar C. Fowler, “Virtual reality and learning:
    where is the pedagogy?” British Journal of Educational Technology, vol. 46, no.
    2, pp. 412–422, 2015. View at: Publisher Site | Google Scholar L. P. Berg and
    J. M. Vance, “Industry use of virtual reality in product design and manufacturing:
    a survey,” Virtual Reality, vol. 21, no. 1, pp. 1–17, 2017. View at: Publisher
    Site | Google Scholar S. Viñas-Diz and M. Sobrido-Prieto, “Virtual reality for
    therapeutic purposes in stroke: a systematic review,” Neurologia, vol. 31, no.
    4, pp. 255–277, 2016. View at: Publisher Site | Google Scholar T.-Ju Lin and Yu-Ju
    Lan, “Language learning in virtual reality environments: past, present, and future,”
    Educational Technology & Society, vol. 18, no. 4, pp. 486–497, 2015. View at:
    Google Scholar S. Anthony, S. Friston, and M. Murcia Lopez, “An “in the wild”
    experiment on presence and embodiment using consumer virtual reality equipment,”
    IEEE Transactions on Visualization and Computer Graphics, vol. 22, no. 99, p.
    1, 2016. View at: Google Scholar B. Oliver and S. Bouchard, “Exposure to an unpleasant
    odour increases the sense of Presence in virtual reality,” Virtual Reality, vol.
    21, no. 2, pp. 1–16, 2016. View at: Google Scholar Li Lan, Yu Fei, and D. Shi,
    “Application of virtual reality technology in clinical medicine,” American Journal
    of Tourism Research, vol. 9, no. 9, pp. 3867–3880, 2017. View at: Google Scholar
    K. M. Sagayam and D. J. Hemanth, “Hand posture and gesture recognition techniques
    for virtual reality applications: a survey,” Virtual Reality, vol. 21, no. 2,
    pp. 91–107, 2016. View at: Publisher Site | Google Scholar S. Madakam, R. Ramaswamy,
    and S. Tripathi, “Internet of things (IoT): a literature review,” Journal of Computer
    and Communications, vol. 03, no. 05, pp. 164–173, 2015. View at: Publisher Site
    | Google Scholar S. Li, T. Tryfonas, and H. Li, “The Internet of Things: a security
    point of view,” Internet Research, vol. 26, no. 2, pp. 337–359, 2016. View at:
    Publisher Site | Google Scholar P. Kamalinejad, C. Mahapatra, Z. Sheng, S. Mirabbasi,
    V. C. M. Leung, and Y. L Guan, “Wireless energy harvesting for the internet of
    things,” IEEE Communications Magazine, vol. 53, no. 6, pp. 102–108, 2015. View
    at: Publisher Site | Google Scholar A. Godfrey, S. Bruno, P. Gerhard, and Hancke,
    “A survey on 5G networks for the internet of things: communication technologies
    and challenges[J],” IEEE Access, vol. 5, no. 12, pp. 3619–3647, 2017. View at:
    Google Scholar J. Manyika, M. Chui, P. Bisson et al., “Unlocking the potential
    of the internet of things,” Molecular and Cellular Biology, vol. 110, no. 5, pp.
    13–16, 2015. View at: Google Scholar G. Han, L. Shu, and S. Chan, “Security and
    privacy in Internet of things: methods, architectures, and solutions: editorial,”
    Security and Communication Networks, vol. 9, no. 15, pp. 2181-2182, 2016. View
    at: Google Scholar K. O’Flaherty, “Securing the internet of things,” Computer,
    vol. 48, no. 1, pp. 28–35, 2015. View at: Google Scholar Copyright Copyright ©
    2022 Pengfei Che. This is an open access article distributed under the Creative
    Commons Attribution License, which permits unrestricted use, distribution, and
    reproduction in any medium, provided the original work is properly cited. PDF
    Download Citation Download other formats Order printed copies Views 274 Downloads
    327 Citations 1 About Us Contact us Partnerships Blog Journals Article Processing
    Charges Print editions Authors Editors Reviewers Partnerships Hindawi XML Corpus
    Open Archives Initiative Fraud prevention Follow us: Privacy PolicyTerms of ServiceResponsible
    Disclosure PolicyCookie PolicyCopyrightModern slavery statementCookie Preferences"'
  inline_citation: (Che, 2022)
  journal: Mobile Information Systems
  key_findings: '- The designed system has a highly interactive, safe access mode
    and fast response speed.

    - The system is conducive to processing a large amount of water conservancy information
    data.'
  limitations: Limited to a specific water conservancy system, may not be generalizable
    to other systems. No information about the system's cost or scalability.
  main_objective: To design and implement a water conservancy automation monitoring
    system using VR and IoT technologies.
  relevance_evaluation: The paper is highly relevant to the point in the literature
    review on the use of remote monitoring using IoT-enabled sensors and computer
    vision for automated irrigation systems. It provides a detailed description of
    a water conservancy automation monitoring system that incorporates VR and IoT
    technologies for monitoring and controlling water resources.
  relevance_score: '0.9'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Virtual reality, Internet of Things, computer vision, sensors,
    Unity3D
  title: Water Conservancy Automation Monitoring System Based on VR Image Video and
    Internet of Things
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hou R.
  - Jeong S.
  - Lynch J.P.
  - Law K.H.
  citation_count: '35'
  description: Bridges are critical components of highways ensuring traffic can efficiently
    travel over obstructions such as bodies of water, valleys, and other roads. Ensuring
    bridges are in sound structural condition is essential for safe and efficient
    highway operations. Structural health monitoring (SHM) systems designed to measure
    bridge responses have been developed to quantitatively track the health of bridges.
    More recently, SHM systems have also begun to integrate measurement of vehicular
    loads that create the responses measured. However, precise correlation of traffic
    loads to bridge responses remains a costly and technically difficult strategy.
    To address existing technical limitations, a cyber-physical system (CPS) framework
    is proposed to track truck loads in a highway corridor, to trigger SHM systems
    to record bridge responses, and to automate the linking of bridge response data
    with truck weights collected by weigh-in-motion (WIM) stations installed along
    the corridor but not collocated with the bridges. To link truck weights to bridge
    responses, computer vision methods based on convolutional neural networks (CNN)
    are used to automate the detection and reidentification of trucks using traffic
    cameras. The single-stage CNN object detector YOLO is trained using a customized
    dataset to identify trucks from camera images at each instrumentation site; high
    precision is obtained with the YOLO detector exceeding 95% average precision (AP)
    for an intersection over union (IOU) threshold of 0.75. To reidentify the same
    truck at different locations in the corridor, this study adopts a CNN-based encoder
    trained via a triplet network and a mutual nearest neighbor strategy using feature
    vectors extracted from images at each measurement location. The proposed reidentification
    method is implemented in the CPS cloud environment and obtains a F1-score of 0.97.
    The study also explores the triggering of bridge monitoring systems based on visual
    detection of trucks by a traffic camera installed upstream to the bridges. The
    triggering strategy proves to be highly efficient with 99% of the triggered data
    collection cycles capturing truck events at each bridge. To validate, the CPS
    architecture is implemented on a 20-mile highway corridor that has a WIM station
    already installed; four traffic cameras and two bridge SHM systems are installed
    along the corridor and integrated with a CPS architecture hosted on the cloud.
    In total, over 10,000 trucks are observed at all measurement locations over one
    year allowing peak bridge responses to be correlated to both measured truck weights
    and to one another.
  doi: 10.1016/j.trc.2019.11.024
  explanation: 'The authors first develop a computer vision and cyber-physical system-based
    data collection process for mapping truck weights to real-time bridge response
    data with the aim of supporting the real-time assessment of bridge health condition.
    They develop a method for identifying and tracking trucks as they traverse a highway
    segment using traffic camera data, leveraging a YOLOv3-based object detector to
    detect trucks and comparing the most significant bounding box of the detector’s
    prediction with a set of lane boundaries. Trucks are tracked through space and
    time allowing them to link data collected at different sensor locations as a truck
    moves down the highway, effectively enabling the calculation of an input-output
    relationship between truck weights and the bridges’ response. They find that the
    trigger-based data collection strategy significantly outperforms the use of a
    schedule-based approach, leading to a 24.5% increase in data collection efficiency
    and conclusion that the majority of trucks observed in the corridor are captured
    under the new protocol.


    The authors connect the data collected through the computer vision and cyber-physical
    system to a series of models that analyze the data to determine the usefulness
    of the proposed approach. They present three major contributions using the data
    collected: 1) a demonstration of a correlation between the maximum strain response
    measured by bridge sensors and the corresponding gross weight of the trucks responsible
    for generating the strain; 2) a demonstration of a correlation between the maximum
    strain responses measured by two different bridges to the same truck loads; and
    3) a demonstration of the effectiveness of the computer vision and cyber-physical
    system approach for automatically identifying trucks on traffic images with an
    AP0.5 = 96.65% average precision. The authors conclude the approach is highly
    scalable, has proven abilities to automate the identification of trucks, map their
    travel history in corridors, and trigger SHM data collection in response, and
    provides a pathway for the quantitative mapping of vehicle loads to measured structural
    responses of multiple bridges.'
  extract_1: This paper focuses on the use of computer vision and machine learning
    techniques to automate the detection and tracking of trucks as they traverse a
    highway segment, using traffic camera data. The authors develop a method for identifying
    and tracking trucks as they traverse a highway segment using traffic camera data,
    leveraging a YOLOv3-based object detector to detect trucks and comparing the most
    significant bounding box of the detector’s prediction with a set of lane boundaries.
  extract_2: The authors demonstrate the effectiveness of the computer vision and
    cyber-physical system approach for automatically identifying trucks on traffic
    images with an AP0.5 = 96.65% average precision.
  full_citation: '>'
  full_text: '>

    "Skip to main content Skip to article Journals & Books Search Register Sign in
    Brought to you by: University of Nebraska-Lincoln View PDF Download full issue
    View Open Manuscript Outline Highlights Abstract Keywords 1. Introduction 2. Cyber-physical
    system architecture 3. CPS data acquisition using truck detection 4. Data integration
    by truck reidentification 5. Results 6. Conclusions CRediT authorship contribution
    statement Acknowledgements Appendix A. YOLOv3 architecture Appendix B. Supplementary
    material Research Data References Show full outline Cited by (35) Figures (21)
    Show 15 more figures Tables (6) Table 1 Table 2 Table 3 Table 4 Table 5 Table
    6 Extras (1) Supplementary data 1 Transportation Research Part C: Emerging Technologies
    Volume 111, February 2020, Pages 547-571 Cyber-physical system architecture for
    automating the mapping of truck loads to bridge behavior using computer vision
    in connected highway corridors Author links open overlay panel Rui Hou a, Seongwoon
    Jeong b, Jerome P. Lynch a, Kincho H. Law b Show more Add to Mendeley Share Cite
    https://doi.org/10.1016/j.trc.2019.11.024 Get rights and content Highlights •
    Cyber-physical system architecture created to link truck loads to bridge responses.
    • Measured truck loads linked to bridge responses create input-output data for
    SHM. • Highway truck loads tracked using deep learning applied to traffic camera
    feeds. • Bridge monitoring triggered by detecting trucks in real-time using CNN
    algorithms. Abstract Bridges are critical components of highways ensuring traffic
    can efficiently travel over obstructions such as bodies of water, valleys, and
    other roads. Ensuring bridges are in sound structural condition is essential for
    safe and efficient highway operations. Structural health monitoring (SHM) systems
    designed to measure bridge responses have been developed to quantitatively track
    the health of bridges. More recently, SHM systems have also begun to integrate
    measurement of vehicular loads that create the responses measured. However, precise
    correlation of traffic loads to bridge responses remains a costly and technically
    difficult strategy. To address existing technical limitations, a cyber-physical
    system (CPS) framework is proposed to track truck loads in a highway corridor,
    to trigger SHM systems to record bridge responses, and to automate the linking
    of bridge response data with truck weights collected by weigh-in-motion (WIM)
    stations installed along the corridor but not collocated with the bridges. To
    link truck weights to bridge responses, computer vision methods based on convolutional
    neural networks (CNN) are used to automate the detection and reidentification
    of trucks using traffic cameras. The single-stage CNN object detector YOLO is
    trained using a customized dataset to identify trucks from camera images at each
    instrumentation site; high precision is obtained with the YOLO detector exceeding
    95% average precision (AP) for an intersection over union (IOU) threshold of 0.75.
    To reidentify the same truck at different locations in the corridor, this study
    adopts a CNN-based encoder trained via a triplet network and a mutual nearest
    neighbor strategy using feature vectors extracted from images at each measurement
    location. The proposed reidentification method is implemented in the CPS cloud
    environment and obtains a F1-score of 0.97. The study also explores the triggering
    of bridge monitoring systems based on visual detection of trucks by a traffic
    camera installed upstream to the bridges. The triggering strategy proves to be
    highly efficient with 99% of the triggered data collection cycles capturing truck
    events at each bridge. To validate, the CPS architecture is implemented on a 20-mile
    highway corridor that has a WIM station already installed; four traffic cameras
    and two bridge SHM systems are installed along the corridor and integrated with
    a CPS architecture hosted on the cloud. In total, over 10,000 trucks are observed
    at all measurement locations over one year allowing peak bridge responses to be
    correlated to both measured truck weights and to one another. Previous article
    in issue Next article in issue Keywords Cyber-physical systemComputer visionConvolutional
    neural networkWeigh-in-motionTransportation infrastructure managementStructural
    health monitoring 1. Introduction An integral element of highway operations and
    management is the asset management of highway structural assets including bridges.
    For example, there are 616,096 bridges in the United States requiring significant
    investment to ensure they are safe for use and resilient to external loading (FHWA,
    2018). As the national inventory of bridges ages, increasing levels of funding
    will be required to maintain bridge safety given existing asset management methods
    which rely on visual inspection methods. Given an $8 billion annual investment
    shortfall for maintaining our national bridge inventory, more cost efficient methods
    are needed for bridge asset management (Hu et al., 2015). Recent advances in structural
    health monitoring (SHM) including the emergence of new sensors and scalable analytical
    frameworks have the potential to improve asset management decisions potentially
    leading to lower long-term management costs (Wang et al., 2014). The adoption
    of SHM for bridges offer asset managers data-driven methods that can improve the
    quantification of structural conditions for asset risk assessment (Junwon et al.,
    2015). Current bridge SHM paradigms are based on systems designed to collect environmental
    and structural response data using sensors and a data acquisition system. Bridge
    response data can be used for damage detection (Peeters, 2000), capacity estimation
    (Catbas et al., 2011) and risk assessment (Frangopol et al., 2008). While monitoring
    systems can reliably measure bridge behavior, such systems typically lack the
    capability to measure the vehicular loads inducing measured responses. As a result,
    bridge SHM frameworks often rely on output-only algorithms where the lack of a
    quantified input hinders accuracy, robustness, and generality. The challenges
    associated with output-only analyses are routinely encountered in bridge modal
    analysis and time-series modeling where the absence of load measurements lead
    to simplifying assumptions such as the excitation source being a white noise process
    (Ditlevsen, 1994). While this assumption may be reasonable in long-span bridges
    excited by a dense flow of traffic, it is often not suitable in short- and medium-span
    bridges where span lengths may only accommodate a small number of vehicles at
    a time. Furthermore, the ratio of vehicular live load to dead loads in short-
    and medium-span bridges is larger than in long-span bridges which places even
    greater importance on acquiring data related to vehicular loading (Catbas et al.,
    2011). Researchers have previously explored ways of acquiring data on vehicle
    loading of bridges. For example, controlled load testing using calibrated but
    unmonitored vehicles has been explored (Zhu and Law, 2015). Another strategy is
    the use of instrumented trucks whose dynamics are recorded when driving over an
    instrumented bridge; combining time-synchronized data from the truck and bridge
    can offer input-output data sets for system identification and health assessment
    methods (Kim and Lynch, 2012). However, the cost and complexity of controlled
    truck loading prevents it from being applied to most bridges. Advances in wireless
    communications and connected vehicles have opened opportunities for connectivity
    within highway systems including between vehicles and infrastructure. The SHM
    of bridges can benefit from connectivity by integrating bridge monitoring data
    with roadside traffic data sources including traffic cameras and weigh-in-motion
    (WIM) systems (Khan et al., 2016, Zhu and Law, 2015). For example, extensive research
    has explored the use of traffic cameras installed adjacent to bridges to capture
    images of vehicles loading the bridge. If roadside video and bridge monitoring
    data are synchronized, bridge responses can be visually linked to the vehicles
    inducing them (Darwish and Cook, 2015, Glisic et al., 2012, Micu et al., 2018,
    Vespier et al., 2011). Others have explored the use of traffic video to extract
    traffic characteristics (e.g., speed, trajectory) using techniques from the field
    of computer vision. Gandhi et al. utilized an instrumented segment of a roadway
    to measure the road response to vehicles and to link vehicle loads to features
    extracted from roadside video using low-pass filtering, background subtraction
    with shadow suppression, and Kalman filtering methods (Gandhi et al., 2007). Fraser
    et al. extended the work of Gandhi et al. by advancing similar computer vision
    methods to spatially map vehicle trajectories on a monitored bridge (Fraser et
    al., 2010, Gandhi et al., 2007). Chen et al. adopted the use of a temporary camcorder
    to capture bridge traffic so that vehicle type, speed and location were estimated
    for a spatial-temporal traffic model that maps traffic to bridge responses (Chen
    et al., 2006). In a different approach, Zaurin and Catbas proposed the use of
    background subtraction methods using traffic cameras to identify vehicles crossing
    an instrumented small-scale bridge in the laboratory. Strain measurements synchronized
    with vehicle trajectories were then used to estimate unit influence lines (UILs)
    of the bridge (Zaurin and Catbas, 2010) with UILs later used as damage-sensitive
    features for health assessments (Zaurin and Necati Catbas, 2011). All of these
    approaches use cameras to acquire the state of traffic on a bridge but do not
    offer quantitative data specific to vehicular loading. WIM stations are widely
    used in highway systems to acquire weight data on vehicles in a corridor including
    their gross weight, axle counts, axle spacing and axle weights. Traffic loads
    measured by WIM stations allow statistical models of vehicle live loads to be
    created for bridge load rating (Han et al., 2014), fatigue analysis (Wang et al.,
    2004), and condition assessment (Lou et al., 2016; Cantero and González, 2015).
    Researchers have begun to explore the fusion of cameras with WIM stations to assess
    bridge loads. For example, Han et al. (2014) adopted a traffic camera and a WIM
    station on a segment of the Xuanda Highway (China) to identify heavy trucks so
    that their temporal frequency and corresponding load profiles can be statistically
    modeled. Chen et al. also explored the fusion of WIM stations and traffic cameras
    on the Hangzhou Bay Bridge to track truck bridge loads using background subtraction
    for truck identification and particle filters for trajectory estimation (Chen
    et al., 2016). It also has been proposed recently to integrate WIM data into Bridge
    Information Modeling (BrIM) frameworks to improve the management of transportation
    infrastructure elements such as bridges (Adibfar and Costin, 2019). While these
    studies have proven the utility of combing WIM stations with traffic cameras,
    they were also limited by the need to collocate the WIM station and camera at
    the asset of interest (i.e., bridge). With WIM stations costly to install, such
    stations also tend to be sparsely distributed in a network and are rarely collocated
    with bridges. The goal of this study is to create a cyber-physical system (CPS)
    framework that integrates roadside traffic data collected using traffic cameras
    and WIM stations with response data collected using SHM systems on bridges. By
    using computer vision to spatially link measurements collected at different locations
    in the corridor, the proposed CPS framework does not require collocation of the
    WIM station with a bridge. The study adopts a 20-mile highway corridor along northbound
    I-275 between Newport and Romulus, Michigan for the design and deployment of the
    proposed CPS architecture. As illustrated in Fig. 1(a), along this corridor are
    two instrumented highway bridges, one WIM station, and four traffic cameras, each
    connected to the Internet through cellular communications. To link bridge response
    data to the same vehicle, the CPS framework uses low-cost traffic cameras collocated
    with the bridge monitoring systems and WIM stations to capture images of the traffic
    associated with each measurement location. Computer vision methods based on convolutional
    neural networks (CNN) are adopted to automate the identification of heavy trucks
    at each measurement location and to link data associated with the same truck observed
    at the different measurement locations. The CPS framework also leverages real-time
    truck identification using computer vision to trigger the bridge monitoring systems
    for more efficient data collection using a solar powered wireless SHM system.
    The proposed approach to linking response data collected at multiple locations
    in a corridor to the same truck event has many benefits including the ability
    to map truck weights to bridge responses and to correlate bridge responses to
    one another under the same load conditions. While the work is motivated by improving
    the asset management of highway structural assets, the proposed CPS system offers
    many other uses integral to operational management of highways including optimal
    routing of truck loads on bridges in a road network (Hu et al., 2015), quantitative
    tracking of truck loads in a network (Hyun et al., 2017) and optimal infrastructure
    investment planning (Yin et al., 2008). This paper begins with a detailed description
    of the CPS architecture implemented in the I-275 corridor including a description
    of the triggering strategy adopted by the CPS to collect response data specific
    to truck load events. The paper presents the CNN methods used to identify trucks
    at multiple locations based on camera images. Bridge response and WIM data corresponding
    to the same truck event are used to correlate bridge responses and truck weights,
    thereby offering a novel approach to baselining bridge behavior for SHM. The paper
    concludes with a summary of findings and a description of future work. Download
    : Download high-res image (432KB) Download : Download full-size image Fig. 1.
    (a) Locations of the cyber-physical system (CPS) components for tracking truck
    loads in the I-275 highway corridor; (b) functional diagram of the CPS components
    and connectivity. 2. Cyber-physical system architecture Cyber-physical systems
    (CPS) are systems in which sensors, actuators, and software components are linked
    and integrated to monitor, analyze, and control the physical system (Rajkumar
    et al., 2010). Unlike traditional monitoring and control systems, CPS utilize
    computing platforms made available through communication interfaces (wireless
    or wired) where data can be aggregated, curated and analyzed. With the scalable
    computing resources available via the Internet, CPS often expand the repertoire
    of computing methods from exclusively physics-based to also encompass data-driven
    methods such as machine learning techniques (Jara et al., 2014). CPS architectures
    have been applied to a wide range of infrastructure applications including connected
    and autonomous vehicles (Abid et al., 2011), early earthquake warning systems
    (Liu et al., 2013), and smart grids (Simmhan et al., 2013), just to name a few.
    In this study, a CPS architecture is proposed for highway corridors (i.e., the
    physical system) where heavy trucks load the corridor assets such as bridges.
    The role of the CPS is to improve the performance of SHM systems installed on
    bridges by quantitatively observing the truck loads. As shown in Fig. 1(b), the
    CPS architecture contains three major types of components: namely, physical components,
    sensing modalities and Internet-based computing. The physical components of the
    system include highway bridges and passing vehicles. The sensing components of
    the CPS architecture include the bridge SHM systems that monitor bridge responses
    to traffic, WIM stations that measure the speed, gross weight and weight distributions
    of vehicles, and traffic cameras that capture optical data of the vehicles in
    the corridor. Notated as solid arrows in Fig. 1(a), each of these sensing components
    are connected to the Internet via cellular communication where data can be communicated
    for storage in cloud-based database management systems and processed by deployed
    computing modules. The communication interfaces are also used by the CPS architecture
    to communicate commands to the sensing components where data collection processes
    can be actuated (i.e., triggered) as shown as dash arrows in Fig. 1(a). The CPS
    architecture is designed to use the traffic camera images to identify trucks as
    they excite the monitored bridges and later cross over the WIM station. The CPS
    architecture is also designed to automate the identification of the same truck
    in the images so that truck weight parameters can be tied to bridge responses,
    thereby providing input-output data for bridge health assessments. In this study,
    a 20-mile (32.2 km) segment of the northbound I-275 corridor between Newport and
    Romulus, Michigan is selected as a testbed to implement and demonstrate the CPS
    framework. The geographic locations of the CPS components along I-275 including
    two bridge SHM systems (i.e., Telegraph Road Bridge and Newburg Road Bridge),
    one WIM station and four traffic cameras are illustrated in Fig. 1(b). One camera
    (labeled Cam-1) is installed at the southern end of the corridor to first identify
    vehicles entering the highway. The three remaining cameras (labeled Cam-2 through
    Cam 4) are installed at each bridge and the WIM station. From south to north,
    the travel distances between adjacent measurement locations are 2 miles (3.2 km),
    4.5 miles (7.2 km), and 7 miles (11.3 km), respectively. It should be noted that
    exits are present along the 20-mile corridor with one exit present between the
    Telegraph Road Bridge and Newburg Road Bridge and three between the Newburg Road
    Bridge and the WIM station; there are no exits between Cam-1 and the Telegraph
    Road Bridge. These exits allow trucks to enter and exit between the measurement
    locations, making truck reidentification a more challenging problem for the CPS
    architecture. 2.1. Bridge SHM systems Along the selected I-275 northbound corridor,
    two highway bridges carrying I-275 traffic over local roads are monitored: Telegraph
    Road Bridge (TRB) and Newburg Road Bridge (NRB). Both bridges were built in 1973
    and are owned by the Michigan Department of Transportation (MDOT). The TRB, shown
    in Fig. 2, is a deck-on-steel girder bridge carrying three lanes of northbound
    I-275 on seven girder lines. One of the bridge lanes is the merge lane associated
    with the highway exit at the bridge location. The bridge spans a total of 224
    feet (68.28 m) consisting of a main span of 128 feet (39.01 m) and two abutment
    spans of 48 feet (14.63 m) each. The reinforced concrete deck of bridge is 8 in.
    (20.3 cm) thick and is in composite action with the steel girders on the main
    span (but not in composite action on the abutment spans). The abutment spans are
    each supported on one end by the bridge abutments and a pier 6 feet (1.83 m) from
    the span end that is adjacent to the main span. The main span is connected to
    the cantilevered ends of the abutment spans through pin-and-hanger assemblies.
    Located 4.5 miles (7.2 km) north of the TRB, the NRB is a single span bridge with
    a total length of 105 feet (32.00 m) as shown in Fig. 3. NRB carries three lanes
    of I-275 northbound traffic with seven steel plate girders and a 9 in (22.9 cm)
    reinforced concrete deck in composite action with the girders. Download : Download
    high-res image (358KB) Download : Download full-size image Fig. 2. (a) Telegraph
    Road Bridge carrying I-275 northbound traffic over Telegraph Road; (b) overview
    of the wireless SHM system installed. Download : Download high-res image (411KB)
    Download : Download full-size image Fig. 3. (a) Newburg Road Bridge carrying I-275
    northbound traffic over Newburg Road; (b) overview of the wireless SHM system
    installed. The TRB and NRB had SHM systems installed in 2011 and 2016, respectively,
    using wireless sensors as the main data acquisition platform. Each wireless SHM
    system is composed of a wireless base station and a number of Narada wireless
    sensing nodes that have been developed at the University of Michigan (Swartz et
    al., 2005). Up to four sensors can be interfaced with one Narada node with an
    internal 16-bit analog-to-digital converter (ADC); sensors are attached to the
    ADC via a Metal Oxide Semiconductor Field Effect Transistor (MOSFET) switch (that
    allows sensors to be turned on and off by the node to save power) and signal conditioning
    circuits (e.g., low-pass filters) as shown in Fig. 4(a) (Hou et al., 2015). Internal
    to Narada is a 256 kB of SRAM that allows data to be stored locally before communication.
    The Narada nodes use power amplified IEEE802.15.4 transceivers (Texas Instruments
    CC2420) for wireless communication between nodes and the base station. Each Narada
    node is placed in a weather-proof plastic enclosure along with a 12 V sealed lead
    acid (SLA) battery (PowerSonic PS-1229) with a 2.9 A-hr capacity and a solar charge
    controller (Morningstar SunSaver SS-6L-12V) which controls the recharging of the
    battery using 10 W solar panels (UL-Solar 10 W 12 V) installed along with the
    enclosure (Fig. 4(b)). The enclosure has rubber coated rare earth magnets (Fig.
    4(c)) on its base allowing it to be magnetically mounted to the steel girders
    of the bridges. A more detailed description on the wireless sensing node can be
    found in O’Connor et al. (2017). Download : Download high-res image (490KB) Download
    : Download full-size image Fig. 4. Narada wireless sensor nodes for field deployments
    in TRB and NRB: (a) Narada sensor with conditioning circuits and interfaced sensor
    (shown is an accelerometer); (b) power system of a Narada sensor node including
    rechargeable battery and solar charge controller; (c) weather-proof enclosure
    installed on a girder flange with strain gage welded to the girder web. Each base
    station (Fig. 5(a)) is programmed to send operational commands to the Narada nodes
    (e.g., sleep, wake-up), trigger data collection, synchronize the nodes, collect
    sensor measurements and forward sensor data to a cloud database (Jeong et al.,
    2017). The primary component of the base station is an embedded Linux server implemented
    on a single board computer (Nvidia Jetson TX2). Attached to the Jetson TX2 are
    two radios: one is an IEE802.15.4 transceiver (Texas Instruments CC2420) that
    communicates with the Narada nodes on the bridges and the other is an LTE cellular
    modem (AT&T Velocity MF861) that communicates with a cellular network to access
    cloud services on the Internet. An external high-gain omni-directional antenna
    is attached to the CC2420 radio to improve the quality of communications between
    the base station and the Narada nodes. The base station is solar powered by a
    110 W solar panel (UL-Solar 110 W 12 V) that is used to recharge a 12 V 40A-hr
    SLA battery (PowerSonic PS-12350NB) using a solar charge controller (Morningstar
    SunSaver SS-20L-12V) included in the base station enclosure. Download : Download
    high-res image (737KB) Download : Download full-size image Fig. 5. (a) Base station
    of the bridge SHM systems; (b) traffic camera installation along I-275. This study
    applies Hitec HBWF-35-125-6-10GP-TR weldable strain gages, BDI ST350 strain transducers,
    Silicon Design 2012-002 uniaxial accelerometers and Texas Instruments LM35DT thermistors
    for the measurements of steel girder strain responses, concrete slab strain responses,
    acceleration responses and the temperature of the bridge, respectively. The sensor
    layout of the TRB system is shown in Fig. 2(b). A total of 14 accelerometers are
    distributed along Girder 1 and 7 at the main span in addition to an extra one
    (A15) located at the center of the bridge. The accelerometers are mounted to the
    bottom flange of girders using structural adhesive (i.e., epoxy) to measure vehicle-induced
    vibrations. Vibration data measured by the accelerometers are used to identify
    the modes and frequencies of the TRB for model updating (which is beyond the scope
    of this paper). The TRB also features six locations where a BDI strain transducer
    is installed in the slab and three strain gauges are installed along the depth
    of the girder; these six locations are identified using BDI transducers B1 to
    B6 along Girder 2 and 6. At each of the six locations, the BDI strain transducer
    is bolted to the bottom surface of the bridge slab and the three strain gauges
    are installed on the web 3 in. (7.62 cm), 27 in. (68.58 cm) and 51 in. (129.54
    cm) above the top surface of the girder bottom flange. Such an installation allows
    the profile of girder strain responses to vehicular loads to be estimated for
    the assessment of composite behavior exhibited at the girder-deck interface (Hou
    et al., 2019c). Each location also has one thermistor to measure temperature.
    S19 and S23 are two strain gauges installed near the bottom pin hole on the two
    hanger plates of the middle girders. S20, S21 and S22 are three strain gauges
    welded to the web of the steel girders 3 in. (7.62 cm) above the bottom flange
    to measure the longitudinal bending strain at mid span of the corresponding girders
    caused by vehicular loads. Compared to the TRB, the NRB SHM system is a lighter
    weight configuration as shown in Fig. 3(b). S1, S2 and S4 are three strain gauges
    welded to the girder web at mid span 3 in. (7.62 cm) above the bottom flange.
    S3 is installed 51 in. (129.54 cm) above the bottom flange at the same location
    as S4. B1 to B4 are four BDI transducers measuring bottom slab longitudinal strains
    especially local tensile strain peaks caused by direct wheel loads to identify
    the speed and the number of axles of a passing truck. A1 is used to identify the
    dominant frequencies of the NRB and T1 is to measure the bridge temperature. Calculated
    from measured acceleration data, the TRB and NRB exhibit a first modal frequency
    of 2.4 Hz and 4.2 Hz, respectively. Accordingly, sampling frequencies are set
    to be 100 Hz, 200 Hz and 1 Hz for strain gauges, accelerometers and thermistors,
    respectively. This study will mainly focus on girder longitudinal strain responses
    measured at mid span of each bridge to demonstrate the data collected by the CPS.
    2.2. Weigh-in-motion (WIM) station The second data source on the I-275 corridor
    is a WIM station that records vehicle weight data. WIM stations are widely used
    by transportation officials to monitor highway freight movement and to track compliance
    to truck weight limits (McCall and Vodrazka Jr, 1997). Compared to traditional
    weigh stations that require vehicles to exit the highway to measure their weight,
    WIM stations are built into the road pavement and take measurements without slowing
    traffic. Various WIM station types exist depending on the sensing transducer type
    used to make weight measurements (e.g., piezoelectric, bending plates, pressure
    cells). WIM stations often provide more than simply the gross vehicle weight but
    also the weight carried by each axle (or axle group), axle spacings, and vehicle
    speed. Along the I-275 northbound corridor is one WIM station (Fig. 6) located
    at Pennsylvania Road in Romulus, MI roughly 7 miles (11.3 km) north of the NRB.
    The WIM station is a two-lane Type II WIM station with quartz sensors installed
    under the pavement of each lane. For each lane, the WIM station records 9 vehicular
    load attributes: time, Federal Highway Administration (FHWA) vehicle class, vehicle
    speed, vehicle gross weight, number of axles, axle weights, axle spacing, direction
    and lane. The WIM station has a road-side data collection unit (Fig. 6(b)) that
    communicates measurements via a fiber optic network to a data server managed by
    the Michigan Department of Transportation (MDOT). Download : Download high-res
    image (344KB) Download : Download full-size image Fig. 6. Weigh-in-motion (WIM)
    station underneath the Pennsylvania Road Bridge: (a) sensors installed in the
    pavement of I-275 northbound; (b) data acquisition system on site to collect the
    WIM data. 2.3. Traffic camera system As notated in Fig. 1, four traffic cameras
    are installed by the authors along the I-275 roadside with a direct view of traffic:
    at the interchange of I-275 with I-75 (Cam-1), Telegraph Road Bridge (Cam-2),
    Newburg Road Bridge (Cam-3) and the WIM station beneath the Pennsylvania Road
    Bridge (Cam-4). Each camera station is responsible for collecting roadside images
    of traffic on northbound I-275, storing image data locally, and transmitting the
    images using a cellular modem. As shown in Fig. 5(b), a Logitech C930e webcam
    camera is enclosed in a water-proof enclosure and mounted to a post on the highway
    roadside. The camera is controlled by a single board Linux server based on the
    Nvidia Jetson TX2 which internally contains a hex-core ARMv8 64-bit central processing
    unit (CPU) and an integrated 256-core Nvidia Pascal graphics processing unit (GPU).
    The inclusion of the GPU in the TX2 is beneficial for Cam-1 where images of trucks
    entering the corridor will be processed directly on the server to provide real-time
    triggering of the SHM systems within the CPS. The Nvidia Jetson TX2 is contained
    in a weather-proof enclosure and is set-up identically to the ones used on the
    TRB and NRB (i.e., same battery, charge controller, and solar panel). For communications,
    an LTE modem (AT&T MF861) is used for the roadside server to communicate data
    to and receive commands from the CPS architecture. The camera can capture images
    with a maximum resolution of 1920 × 1080 pixels but in this study, it is set to
    capture images at a frame rate of 10 frames per second (FPS) with a 1280x720 pixel
    resolution. Reducing the resolution helps reduce the time to transmit and process
    data. Each traffic image is labelled with the time it is captured by the server.
    The server maintains clock synchronization based on the network time protocol
    (NTP) implemented via the cellular communication interface. Four sample images
    are presented in Fig. 7 where the same truck is captured at Cam-1 through Cam-4.
    Download : Download high-res image (314KB) Download : Download full-size image
    Fig. 7. Traffic image of the same truck captured by the four cameras along the
    I-275 NB corridor: (a) Cam-1; (b) Cam-2; (c) Cam-3; (d) Cam-4. 2.4. Cloud-based
    data management and computing platform A prerequisite to the implementation of
    a CPS architecture for observing and analyzing truck loads on the infrastructure
    assets of the I-275 corridor is the development of a scalable database system
    that can be used to store large sets of heterogeneous data (e.g., sensor data,
    camera feeds), expose data for analytics with low latency, and automate cyber-physical
    operations. For such a purpose, a NoSQL (Not Structured Query Language) database
    architecture based on Apache Cassandra (an open-source distributed NoSQL database
    management system) is implemented in a peer-to-peer distributed fashion for scalable
    and fault-tolerant data storage and management (Jeong et al., 2018, Jeong et al.,
    2017). The detailed architecture is shown in Fig. 8 where the core database is
    built upon virtual machines provided by commercial cloud computing platforms such
    as Microsoft Azure which is used in this study. The use of the cloud allows the
    CPS architecture to have maximum scalability as data sets and analytical needs
    grow. While the physical locations of the distributed database nodes are abstracted
    by the virtual machine framework within the cloud platform, virtual machines allow
    the CPS to scale the use of cloud computing resources up and down, as needed.
    Multiple peripherals such as RESTful web services (based on REpresentational State
    Transfer (REST) technology) and data processing applications are then designed
    on top of the database system which can be accessed from different end clients
    for data retrieval, visualization and interpretation purposes. Download : Download
    high-res image (478KB) Download : Download full-size image Fig. 8. Computational
    framework of the cloud-based cyberinfrastructure database and computing platform.
    To support the integration and analysis of data collected from multiple sources,
    several automated data analytic modules are deployed on high-performance computing
    clusters which are also hosted on the cloud platform. These modules are developed
    to perform a variety of processing tasks associated with integrating data from
    multiple sources (e.g., vehicle detection and bridge response data filtering),
    characterizing bridge behavior (e.g., frequency identification and modal analysis)
    and long-term structural health monitoring (e.g., bridge response correlation)
    leveraging the computing available at the measurement locations (e.g., the CPU
    and GPU on the roadside servers) and cloud. This study will primarily focus on
    the analytical modules related to data integration (as will be later presented
    as grey blocks in Fig. 11). 3. CPS data acquisition using truck detection Of primary
    interest in assessing the performance and condition of bridges is how they respond
    to vehicle live loads. For example, most existing bridge SHM frameworks utilize
    live load-induced bridge responses for damage detection and capacity evaluation
    (Junwon et al., 2015). While all vehicles induce a dynamic response in bridges,
    it is the large trucks that induce the greatest dynamic response offering high
    signal-to-noise ratio (SNR) measurements. Also, the load magnitude associated
    with trucks are a bigger contributor to long-term structural deterioration (Kim
    and Yoon, 2009). Solar-powered wireless SHM systems such as those used in this
    study are limited by their available power (especially during low-sunlight conditions
    during the winter months) and are therefore not operated continuously; rather,
    they are designed to collect data regularly on a defined schedule. Due to collection
    of data being on a schedule, there is no guarantee that the data collected during
    each collection cycle will contain vehicles of primary interest such as heavy
    trucks. This limits the utility of the data set while consuming precious energy
    resources by the system. A much more effective approach to collecting data from
    bridge SHM systems is if the system could be triggered by the presence of heavy
    trucks which are the load of greatest interest. Triggering based on truck events
    can generate a rich set of bridge response data for SHM analyses while minimizing
    the consumption of limited SHM system battery power. In this study, the CPS architecture
    is designed to apply computer vision methods on the camera images to identify
    trucks and to trigger the bridge SHM systems to collect data based on the identification
    of a truck. Architecturally, Cam-1 (Fig. 1) at the southern end of the corridor
    is deployed to serve as the trigger camera. Cam-1 continuously records northbound
    traffic with an embedded truck detector implemented in the camera server. Once
    a truck entering I-275 is identified by Cam-1, the camera’s server sends a trigger
    message to the TRB, NRB, Cam-2, Cam-3 and Cam-4 to begin data collection based
    on an estimated time of arrival of the truck at each downstream location. This
    triggering strategy ensures the same truck can be tracked at each location. While
    some trucks may exit the corridor, the framework would capture the trucks that
    remain on the corridor without exiting. 3.1. Truck detection Object detection
    is a challenging computer vision problem that has been intensively researched
    over the past decade (Redmon et al., 2016). Object detection refers to automated
    methods that identify, locate, and classify target objects within an image. Operation
    of transportation systems including managing vehicles on highways and users using
    public transit has emerged as one of the most active application domains for object
    tracking by computer vision methods (Wang et al., 2019). This study leverages
    recent advances in deep learning-based object detection techniques, especially
    CNNs, for truck detection within the CPS architecture. Generally, there are two
    primary CNN architectures for object detection: two-stage and one-stage detectors.
    Two-stage detectors such as Regions with CNN features (R-CNN) (Girshick et al.,
    2014) and Faster R-CNN (Ren et al., 2017) are designed with one stage used to
    propose candidate regions of interest in an image and the second stage used to
    perform object classification and detection for each region of interest generated
    by the first stage. In contrast, one-stage detectors such as You Look Only Once
    (YOLO) (Redmon et al., 2016, Redmon and Farhadi, 2017, Redmon and Farhadi, 2018),
    Single Shot Detector (SSD) (Liu et al., 2016) and RetinaNet (Lin et al., 2017b),
    use a single stage to simultaneously predict object classes within bounding boxes
    with a confidence score assigned to each object. Generally, two-stage detectors
    are much more precise in object detection than one-stage detectors but their inference
    times are longer (Lin et al., 2017b). With real-time truck identification at Cam-1
    requiring near real-time execution, this study adopts a one-stage detector framework
    for the identification of trucks. Prior work by the authors compared YOLO, SSD
    and RetinaNet one-stage detectors and found YOLO the best performer for truck
    identification balancing speed with precision (Hou et al., 2019b). Based on this
    prior work, the YOLOv3-tiny and YOLOv3 detectors are used within the CPS design
    in this study (Redmon and Farhadi, 2018). Recently, proposal-free and anchor-free
    object detection algorithms (Law and Deng, 2018, Tian et al., 2019) have emerged
    claiming to have a more straightforward design. Despite being innovative in their
    design, these algorithms cannot compete with YOLO in terms of inference speed
    due to their heavy backbone architectures and need for potential post-processing
    steps to recover bounding boxes. YOLO is a single-stage CNN architecture for object
    identification where the input is the raw image. YOLO splits the image into an
    S-by-S grid with each grid cell responsible for predicting B candidate bounding
    boxes whose centroid fall inside that cell area and contains the object (Redmon
    et al., 2016). It should be noted the bounding box can be much larger than the
    cell itself and only the box centroid needs to be in the cell. The number of predicted
    candidate boxes B depends on the number of box priors selected for approximately
    representing the shape and dimension of the final bounding boxes (Redmon and Farhadi,
    2017). Each bounding box is defined by a 5+C element vector including information
    of its centroid x and y coordinates, box width, box height, a box confidence score
    representing the likelihood that the box contains an object, and the conditional
    probabilities corresponding to C target object classes given a detected object
    in the box. YOLO utilizes its underlying CNN network to encode an input image
    into a tensor with a shape of containing the various bounding box predictions
    for the image (Redmon and Farhadi, 2017). For each predicted bounding box, its
    class confidence scores are equal to the product of the box confidence score and
    corresponding conditional class probabilities. Independent logistic classifiers
    are then used to determine the final class for each predicted bounding box; logistic
    classifiers allow an object to be assigned to multiple classes which is beneficial
    when classes have overlap. Due to its model design, there might be multiple bounding
    boxes for the same object; non-maximal suppression (NMS) is then used by YOLO
    to remove bounding box duplications with lower scores. A global confidence threshold
    can also be set such that the model only outputs predictions with confidence scores
    higher than the defined threshold. The third generation of the YOLO architecture,
    namely YOLOv3, features a deep 53-layer CNN architecture termed Darknet-53. The
    CNN architecture has a series of convolutional layers each followed by a step
    of batch normalization (BN) (Ioffe and Szegedy, 2015) before being input into
    an activation function (e.g., leaky rectified linear unit (ReLU) (Maas et al.,
    2013)), which allows it to work as a filter bank extracting features and making
    predictions from the input image (Redmon and Farhadi, 2018). YOLOv3 also incorporates
    advanced techniques such as skip connections (He et al., 2016) and feature pyramid
    network (FPN) like up-sampling techniques (Lin et al., 2017a) to improve its prediction
    performance (especially for detecting small objects). When a fast execution speed
    is an important requirement, a reduced-order version of YOLOv3 called YOLOv3-tiny
    can also be used. YOLOv3-tiny follows the same logical design as YOLOv3 but adopts
    a much more compact CNN backbone (i.e., 13 convolutional layers as opposed to
    52) thereby reducing its computational cost and speeding its execution for real-time
    applications. A more detailed description on the YOLOv3 architecture can be found
    in Appendix A. This study selects YOLOv3 for offline identification of vehicles
    at each measurement location (i.e., Cam-2 through Cam-4) and YOLOv3-tiny for real-time
    execution at Cam-1. Both models take as inputs camera images of the size 416-by-416
    pixels. The implementation of YOLOv3 and YOLOv3-tiny uses the open source neural
    network framework Darknet which is implemented using high-level programming languages
    such as C (Redmon, 2016). The YOLOv3 and YOLOv3-tiny models are trained and tested
    using a customized image set designed to detect three vehicle classes defined
    as trucks, pick-up trucks and “cars” (which broadly include sedans, SUV, vans).
    The image set which is summarized in Table 1 is a combination of vehicle pictures
    manually collected by the authors from the Internet and traffic images captured
    by the four cameras installed along the I-275 corridor. Table 1. Statistics associated
    with customized vehicle data sets used for training and validation of CNN detectors.
    Empty Cell Total Images Online Images I-275 Images Total Vehicle Instances Number
    of Trucks Number of Pick-up Trucks Number of Cars Training Set 2600 1142 1458
    4391 2947 344 1100 Test Set 900 0 900 1226 566 178 482 Total 3500 1142 2358 5617
    3513 522 1582 For training purposes, 2600 images consisting of a combination of
    online vehicle images and I-275 camera images with vehicles are assembled; this
    training image set has 4391 vehicle instances (2947 trucks, 344 pick-up trucks,
    and 1100 cars). For testing purposes, 900 daytime images from the I-275 cameras
    are selected which contain a total of 1226 vehicles (566 trucks, 178 pick-up trucks,
    and 482 cars). The purpose of selecting online images for training is to generalize
    the detection capabilities of the YOLO detectors with the trained models potentially
    detecting rare vehicles not included in the training traffic images captured by
    the I-275 cameras. Different weather and illumination conditions (e.g., snow versus
    rain, sunny versus cloudy, midday versus twilight) have been considered during
    the preparation of the training and test image sets with the aim of training and
    validating the detectors under a wide variety of operational daytime conditions.
    The YOLOv3 and YOLOv3-tiny models begin having been pre-trained on the ImageNet
    image database (Redmon and Farhadi, 2018, Russakovsky et al., 2015). During further
    training using the data set of Table 1, back-propagation methods are used to update
    the CNN weights to achieve a high level of detection accuracy and precision. Only
    minor changes (e.g., object classes, learning rate, batch size) are made to the
    original implementation to accommodate the customized training dataset and specific
    machine hardware used in this study so that convergence of loss is achieved during
    training. Both the training and test processes are performed with a Nvidia Titan
    Xp GPU. After training, the performance of the final CNN models are evaluated
    using the test data in terms of metrics such as the precision, recall, and inference
    speed (Everingham et al., 2010). By definition, precision is defined as the fraction
    of predictions that are correct (i.e., true positives dived by the number of true
    and false positives) and recall is defined as the fraction of ground truth events
    correctly predicted (i.e., true positives divided by the number of true positives
    and false negatives). In YOLO, the global confidence threshold is used to adjust
    the lowest confidence score that a detection prediction can have and therefore
    indirectly controls the number of predicted object outputs. Consequently, a change
    of the global confidence threshold leads to a change in the precision and recall
    of the model. Average precision (AP) is calculated by averaging the maximum precision
    of the CNN model at different recall levels per class and mAP is the mean of all
    class APs. AP is also associated with an intersection over union (IOU) threshold
    where IOU is the percentage of the detected object bounding box overlapping with
    the ground truth bounding box (i.e., a bounding box perfectly bounding the detected
    object). A higher IOU threshold means the predicted bounding box of the detected
    object needs to have more overlap with the ground truth to be considered a correct
    detection. In this study, AP is evaluated for two IOU thresholds: 0.5 and 0.75
    which are denoted using subscripts (e.g., AP0.5 and AP0.75). Because predicted
    bounding boxes will also be used in other analyses such as detecting which lane
    the vehicle is in and for vehicle matching, a relatively high IOU threshold of
    0.75 is investigated besides the most common 0.5 threshold used in computer vision
    studies. The inference speed refers to the average time needed to perform vehicle
    detection on a single image, and it depends not only on the model complexity,
    but also on the computing hardware upon which the detector is implemented. The
    test results (benchmarked using a Nvidia Titan Xp GPU) for YOLOv3 and YOLOv3-tiny
    are reported in Table 2. When using the YOLO models, the confidence threshold
    is set at 0.8 with the model only returning predictions with class probabilities
    higher than this threshold. This high threshold minimizes false alarms in the
    real-time implementation of YOLOv3-tiny at Cam-1. When deployed upon the Nvidia
    Jetson TX2 which is the server hardware used at Cam-1, the YOLOv3-tiny model is
    capable of being executed in real-time at an equivalent frame rate of 15 frames
    per second which satisfies real-time detection requirements of the CPS. In addition,
    the average precision of the YOLOv3-tiny in detecting trucks for an IOU threshold
    of 0.5 and 0.75 is 97.46% and 96.65%, respectively. The YOLOv3 model is used to
    perform vehicle detection on traffic images captured by the other cameras (namely,
    Cam-2 through Cam-4) using a server with a Nvidia Titan Xp GPU that is accessible
    to the CPS via the Internet. The time of execution for this more extensive CNN
    model on the Titan Xp is 19.7 ms per image. The average precision for trucks is
    higher than that of YOLOv3-tiny with AP0.5 and AP0.75 at 98.91% for both IOU thresholds.
    Besides, YOLOv3 is far superior when discriminating between the three different
    vehicle classes as is seen with notably higher AP scores for pickup-trucks and
    cars in Table 2. Some visual results of vehicles detected by the two YOLO models
    are shown using images from the I-275 corridor under different field conditions
    in Fig. 9. Table 2. Test results of the trained YOLOv3 and YOLOv3-tiny models
    in terms of average precision (AP), mean average precision (mAP), and inference
    speed, benchmarked using a Nvidia Titan Xp GPU. Model YOLOv3 93.64% 92.56% 19.7
    98.91% 88.44% 93.58% 98.91% 85.73% 93.04% YOLOv3-tiny 88.11% 86.20% 3.2 97.46%
    82.13% 84.73% 96.65% 79.25% 82.71% Download : Download high-res image (238KB)
    Download : Download full-size image Fig. 9. Truck detection results: (a) truck
    detected by the trigger using YOLOv3-tiny at Cam-1; (b) truck detected using YOLOv3
    at the TRB; (c) truck detected using YOLOv3 during a rainy day at the NRB; (d)
    a pick-up truck and a car detected using YOLOv3 at the WIM station. 3.2. Computer
    vision-based SHM system triggering With the aim of capturing the same truck traveling
    along the I-275 highway corridor, the first camera at the interchange of I-275
    and I-75 (Cam-1) is used as the trigger for data collection in the CPS (Fig. 10).
    The embedded YOLOv3-tiny model in the Cam-1 server is continuously executed in
    real-time to process camera images. Once a truck is detected by the detector,
    the Cam-1 server uploads the image of the detected truck into the Cassandra database
    hosted in the cloud and sends a wake-up message to the TRB SHM system server and
    the Cam-2 server using the low latency User Datagram Protocol (UDP). The wake-up
    message sent by the Cam-1 server contains the time to start data collection at
    the TRB bridge accounting for the distance between Cam-1 and the TRB and assuming
    a truck speed of 60 miles per hour (96.6 km per hour): this is estimated to be
    120 s. Due to this truck speed being conservative, the TRB data collection is
    commanded to occur 70 s after the truck is detected (50 s before the estimated
    time of arrival). After the TRB SHM system and Cam-2 receive their wake-up messages,
    both systems start their data collection at the time given by Cam-1 and collect
    data for 100 s putting the mid-point in time for the data collection cycle at
    120 s after the truck is detected at Cam-1. After data is collected, the images
    and bridge response data are uploaded to the Cassandra database. Download : Download
    high-res image (186KB) Download : Download full-size image Fig. 10. Trigger-based
    data acquisition strategy of the proposed CPS system. Following the same pattern,
    the SHM and camera systems at the NRB and WIM station are triggered in a similar
    fashion. Using the same conservative truck speed, the time the truck would take
    to travel between TRB and NRB is approximately 270 s; hence Cam-1 commands the
    NRB SHM system and camera system to collect data for 120 s with the mid-point
    of that data collection cycle being 390 s after the truck is detected at Cam-1
    (i.e., 330 s after trigger). Cam-4 at the WIM station is similarly triggered by
    Cam-1 to collect data for 360 s with the mid-point of the data collection cycle
    being 810 s after the truck is detected at Cam-1. All data collected in the same
    triggered cycle is stored and logged with timestamps in the cloud database for
    further data integration. It should be noted the increasing data collection window
    prescribed at TRB (100 s), NRB (120 s) and the WIM station (360 s) is designed
    to account for uncertainties associated with the truck speed and to maximize the
    chance of capturing the same truck first detected at Cam-1 at each subsequent
    location. When the TRB SHM system was first installed in 2011, a schedule-based
    data collection strategy was used to collect data with a support vector machine
    classifier adopted to automate the identification of bridge response data containing
    large truck loads (O’Connor et al., 2017). The schedule adopted for the TRB SHM
    system collected data for 100 s every two hours. Since 2016, the CPS trigger strategy
    has been in use in lieu of the schedule-based strategy. To quantify the improvement
    of the trigger strategy compared to the schedule strategy in terms of data acquisition
    efficiency, two sets of data are collected at the TRB during July 2017. The first
    set records data at the TRB for 200 cycles (100 s of bridge response data every
    half an hour) over daytime periods (i.e., 8 AM to 6 PM) on the weekdays of two
    consecutive weeks. The second set is 200 data sets based on the trigger strategy
    over the same period during weekdays in the following two consecutive weeks. For
    the second set, it was controlled that Cam-1 sleeps for half an hour after each
    triggering event such that only one data collection cycle is activated every half
    an hour. The design of the schedule- and trigger-base data collection processes
    was to roughly ensure the statistical sampling of trucks were from similar distribution
    functions to make a fair comparison. First, the number of truck events detected
    in each cycle were compared. The schedule-based strategy leads to 79.5% of the
    data cycles containing truck events while the trigger strategy has 99% of its
    data cycles containing truck events. To consider the dependence of the effectiveness
    of the trigger-based data collection on the data collection duration, another
    experiment was performed with the TRB triggered by Cam-1 with varying data collection
    durations. With the data collection cycle set to be 100, 30, 25, 20, 15 and 10
    s, the percentage of trucks captured at TRB is 99.1%, 95.5%, 95%, 94.5%, 90.5%
    and 74.5%, respectively. These results confirm the triggering strategy significantly
    outperforms the schedule-based SHM sensing strategy and is a powerful approach
    to maximizing the value of data collected by the solar-powered CPS data collection
    elements per unit of power consumed. They also suggest the time period of data
    collection at the TRB can be as short as 30 s to preserve battery power at each
    wireless node. 4. Data integration by truck reidentification To establish an input-output
    relationship between trucks and bridges, data corresponding to the same truck
    event collected at the TRB, NRB and WIM station must be matched with each other.
    This goal is achieved by two stages of data integration. The first stage is one
    of local integration where data collected by the bridge SHM system or WIM station
    must be synchronized with the camera data at the same location. The output of
    this stage of integration are segmented images of trucks detected and the corresponding
    bridge response data set or WIM record. The second stage is one of global integration
    where truck events captured at different locations are matched with one another
    through reidentification of the same truck in traffic images at the different
    data collection components in the CPS. Reidentification of the same truck at different
    camera locations will be done using a CNN-based embedding network with a nearest
    neighbor strategy applied to its output. At the end of the two-stage integration
    process, the final result will be TRB and NRB bridge response data and a WIM record
    linked to the same truck that is travelling northbound in the I-275 corridor.
    The proposed data integration pipeline is organized within a microservices architecture
    which has gained popularity for building large-scale distributed web applications
    (Newman, 2015). In a microservices architecture, a single application is developed
    as a suite of decoupled smaller services with each service running on its own
    independently but communicating with one another via lightweight communication
    mechanisms (Newman, 2015). As depicted in Fig. 11, the microservice architecture
    for the integration of data across the CPS has computational modules (grey blocks)
    with defined functionality and a set of inputs or outputs (white blocks). A module
    output can be stored in the CPS database or as JavaScript Object Notation (JSON)
    file which can be further accessed by subsequent modules. This microservice architecture
    allows each module to be modified or upgraded without influencing the functionality
    of the entire application. In this study, the integration process is implemented
    in Python and performed autonomously in the CPS cloud computing platform (i.e.,
    Azure). Download : Download high-res image (257KB) Download : Download full-size
    image Fig. 11. Data integration procedure following a microservices architecture:
    grey boxes correspond to computational modules while white boxes correspond to
    module input and outputs. 4.1. Stage I: local integration The local integration
    stage first extracts truck events separately from both traffic images and measured
    bridge responses collected at the same location during the same triggered data
    collection cycle. First, bridge strain response data and traffic images captured
    during the same cycle are retrieved from the database with their associated time
    stamps. Truck events at the bridge are detected from the strain response data
    measured by the three strain gages installed on the bottom web at the midspan
    of girders 2, 4 and 6 in each bridge. As shown in Fig. 2(b) and Fig. 3(b), those
    sensors are denoted as S4, S20, S13 in the TRB, and S1, S2, S4 in the NRB. A script
    running a peak detection algorithm identifies the dominant strain peak (local
    maxima) greater than a defined response threshold (e.g., 12 ∊ ) after the strain
    data has been filtered by a 2 Hz Butterworth low-pass filter. These large strain
    peaks are assumed to be induced by heavy loads like trucks crossing the bridge.
    The left and right bounds of the peak defined at 0 ∊ are taken as the start and
    end time of the corresponding truck event. By comparing the peak strain at each
    girder-line, the lane in which the truck is located can be identified with the
    peak value among the three gages assigning the lane (due to each strain gage being
    beneath a different lane). Truck events are also identified from the traffic images
    with the detected vehicle class output from the trained YOLOv3 model executed
    in the cloud environment. Sequential camera images showing the same truck are
    assigned to the same truck event. Another analytical service is implemented to
    assign the detected truck to a lane on the bridge. This service first identifies
    the image pixels corresponding to the highway lane markings (e.g., white dash
    lines) using image processing techniques including Canny edge detection (Canny,
    1986) and color thresholding on an image with no vehicles present in the camera
    field of view. The identified pixels of the lane markings are input into a polynomial
    fitting tool to calculate a continuous geometric boundary corresponding to edge
    of the lanes. This process is conducted daily due to slight movements of the installed
    traffic cameras over time due to vibration induced by traffic or weather conditions.
    The lane that a detected truck is in can be determined based on the relative position
    of the predicted bounding box (i.e., the position of the right bottom corner)
    with respect to the lane boundaries previously extracted from an image without
    a vehicle present as shown in Fig. 12(b). Download : Download high-res image (244KB)
    Download : Download full-size image Fig. 12. Time synchronization between bridge
    response data and traffic images by constructing two truck presence matrices at
    NRB with blue dash squares indicating information corresponding to the same truck
    in all three figure elements: (a) truck events identified from bridge strain data
    by detecting strain response peaks; (b) extracted truck event from Cam-2 images
    by detecting trucks using trained vehicle detector (blue bounding box) and identification
    of truck lane using box lower right corner (yellow circle) compared to lane boundaries
    (red lines); (c) truck presence matrices using extracted truck event information
    from strain data (top) and image data (bottom). (For interpretation of the references
    to color in this figure legend, the reader is referred to the web version of this
    article.) SHM data and traffic images are collected by different systems with
    their own clocks. While these clocks are synchronized over the cellular interface
    using the Network Time Protocol (NTP) regularly (i.e., every 20 min), they may
    not be perfectly aligned. Experience with the system deployed on I-275 suggests
    the time offset is typically less than 1 s but in extreme cases can be as large
    as 5 s, thereby inhibiting direct data integration using the local data timestamps.
    As a result, time synchronization between the SHM and camera datasets is necessary.
    Using truck event information extracted from the bridge and camera data, two sparse
    matrices are constructed to represent the truck presence from the traffic images
    and SHM data collected in the same data collection cycle. As visualized in Fig.
    12(c), this truck presence matrix has three rows representing the three lanes
    in which passing vehicles can travel; each column represents a discretized timestamp
    from the start of a data collection cycle. An entry is zero (black in Fig. 12(c))
    if no truck is present in that lane at that instant in time and one (white in
    Fig. 12(c)) if a truck is detected at that time. As the sampling rate of traffic
    images is 15 Hz, the truck presence information (i.e., a binary array) extracted
    from the camera is up-sampled using bilinear interpolation to have the same sampling
    rate (i.e., 100 Hz) as the bridge strain data. After the two truck presence matrices
    are obtained, cross-correlation is performed via fast Fourier transform (FFT)
    (Lewis, 1995) to calculate the clock offset between the servers of the two data
    collection systems. As a result, truck event data collected in a single data collection
    cycle at the two systems can be synchronized in time; this allows a truck image
    to be linked to each strain peak identified in the strain data from each bridge.
    An example of four integrated truck events captured in the same data collection
    cycle at TRB are shown in Fig. 13. By the same logic, traffic images and WIM records
    can be synchronized and linked. Download : Download high-res image (507KB) Download
    : Download full-size image Fig. 13. Result of time synchronization for integrating
    NRB strain response data with traffic images collected at Cam-3 (data collected
    August 2, 2017 at 6:34 pm). 4.2. Stage II: global integration After the local
    integration stage, truck events captured in the same triggered data collection
    cycle are extracted at the two bridges and WIM station separately. The second
    stage aims to globally integrate these locally integrated data sets (i.e., truck
    image with a strain response or WIM station entity) for the same truck observed
    at the different locations. This global linking is done entirely by matching (or
    reidentifying) the same truck from images collected at the two bridges and WIM
    station. Among all traffic images assigned to a truck event at each measurement
    location, the one that has the largest bounding box and includes the front part
    (e.g., tractor) of the truck is picked for vehicle reidentification. That image
    is cropped using the predicted bounding box to remove the redundant background.
    Such an image would contain the most information of the truck appearance and will
    therefore give better accuracy for reidentification. Truck reidentification is
    conducted following a mutual nearest neighbor strategy. Multiple truck events
    can be captured at each measurement location during one data collection cycle.
    Furthermore, not all of the truck events captured at one location will be observed
    by the CPS architecture at subsequent locations because trucks may exit the corridor
    between two adjacent locations or not be within the assumed speed range that defines
    the time of data collection at each location relative to the trigger at Cam-1.
    To perform truck matching between two locations, every possible pair of truck
    images collected during the same data collection cycle at each location are compared
    through a similarity function which outputs a distance-based scalar metric (to
    be defined shortly). Consequently, a correlation matrix with m rows and n columns
    is used to store the similarity function output for the m truck events captured
    at the first location to the n truck events captured at the second location. A
    sample of a correlation matrix is shown in Fig. 14 to show the reidentification
    between the TRB where 5 trucks were identified and NRB where 10 trucks were identified
    in the same collection cycle. In a mutual nearest neighbor strategy, two images
    with index i and j are assumed to belong to the same truck if and only if the
    similarity metric at element (i, j) in the correlation matrix is below a pre-determined
    threshold and is the minimum of i-th row and j-th column simultaneously. Taking
    the matrix shown in Fig. 14 as an example, four pairs of truck images are potential
    matches if the threshold is set to be 5: namely, T5 & N3, T4 & N4, T3 & N7 and
    T1 & N10. Download : Download high-res image (101KB) Download : Download full-size
    image Fig. 14. Heatmap of a correlation matrix for truck matching between two
    data sets collected at the different bridge locations (NRB and TRB). Distance
    greater than 50 is clipped to 50 for better visualization. To develop a robust
    similarity function, a CNN-based embedding network is used to convert each truck
    image into a vectorized feature representation with the Euclidean distance between
    the feature vectors used as a measure of the similarity between the two truck
    images. CNNs have been shown to be a powerful tool to extract meaningful features
    (e.g., lines, shapes, color patterns) from images and thus work remarkably well
    at object classification, detection, and semantic segmentation (Lin et al., 2017a).
    Within a CNN, each intermediate convolutional layer is responsible for extracting
    a feature map from previous CNN layer outputs; also, combinations of convolutional
    layers can work together as an encoder to cast the characteristics of an image
    into a feature map or vector (Zagoruyko and Komodakis, 2015, Zeiler and Fergus,
    2014). This study leverages the previously trained YOLOv3-tiny model to extract
    features from truck images for reidentification purpose. The first 12 layers of
    the YOLOv3-tiny model are isolated and combined with a global average pooling
    (GAP) layer (Lin et al., 2013) followed by two fully connected layers to form
    the “embedding” network. Adding the GAP layer enforces each feature map before
    the GAP layer to represent a meaningful characteristic of the trucks observed
    in the image and to reduce the dimensionality of the input fed into the subsequent
    fully connected layers. By reducing dimensionality in this manner, overfitting
    of the CNN will be avoided. The architecture of the embedding network and the
    original YOLOv3-tiny model are illustrated in Fig. 15 with their layer types and
    primary parameters summarized in Table 3 (Redmon and Farhadi, 2018). The number
    of the convolutional layers and the fully connected layers in the embedding network
    is determined by ablation experiments to extract the most suitable features for
    truck reidentification to yield the best performance. The embedding network takes
    a truck image that is cropped according to the corresponding bounding box predicted
    by YOLOv3 and resized to 416-by-416 pixels as its input. The network then outputs
    a 512-entry feature vector for that truck image. The Euclidean distance (i.e.,
    L2 distance) between these output vectors is treated as a proxy for quantifying
    the similarity of the two trucks. Download : Download high-res image (231KB) Download
    : Download full-size image Fig. 15. (a) Original YOLOv3-tiny CNN model architecture;
    (b) modified convolutional neural network architecture for truck image embedding
    (where numbers are the indices of the layers presented in Table 3). Table 3. Layer
    types and primary parameters of the YOLOv3-tiny model and the modified embedding
    network. Empty Cell YOLOV3-tiny Network Embedding Network Layer Type #Filter Size
    Stride Type #Filter Size Stride 0 Convolutional 16 3 1 Convolutional 16 3 1 1
    Max pooling 2 2 Max pooling 2 2 2 Convolutional 32 3 1 Convolutional 32 3 1 3
    Max pooling 2 2 Max pooling 2 2 4 Convolutional 64 3 Convolutional 64 3 5 Max
    pooling 2 2 Max pooling 2 2 6 Convolutional 128 3 1 Convolutional 128 3 1 7 Max
    pooling 2 2 Max pooling 2 2 8 Convolutional 256 3 1 Convolutional 256 3 1 9 Max
    pooling 2 2 Max pooling 2 2 10 Convolutional 512 3 1 Convolutional 512 3 1 11
    Max pooling 2 2 Max pooling 2 2 12 Convolutional 1024 3 1 Convolutional 1024 3
    1 13 Convolutional 256 1 1 GAP 13 14 Convolutional 512 3 1 Fully connected 1024
    15 Convolutional 24 1 1 Fully connected 512 16 Detection 17 Convolutional 128
    1 1 18 Up-sampling 2 19 Concatenation 20 Convolutional 256 3 1 21 Convolutional
    24 1 1 22 Detection The triplet network is a commonly used architecture for training
    a network to learn image representation and to produce similarity metrics between
    two input images (Schroff et al., 2015). It is used in this study to train the
    embedding network to extract vision features useful for truck reidentification.
    A triplet network, shown in Fig. 16, has three identical embedding networks with
    all three having identical structure (as presented in Table 3) and weights. During
    the training process, the network is fed with three truck images without labels
    as inputs: the first embedding network gets the “anchor” image, the second embedding
    network gets the “positive” image which is an image of the same truck as that
    in the anchor image, and the third embedding network gets a “negative” image of
    a truck different from that in the anchor image (Schroff et al., 2015). A triplet
    loss function, Ltriplet: (1) where , , are the ith anchor, positive and negative
    images, respectively, is the output vector of the corresponding embedding network
    given an input image ( ) and is a positive margin value. The loss function is
    vital to the triplet network learning the parameters of the embedding network,
    in such a way that the feature vector of the positive truck image is trained to
    be closer to that of the anchor image than the negative sample by a margin (Schroff
    et al., 2015). Download : Download high-res image (156KB) Download : Download
    full-size image Fig. 16. Triplet network architecture for the embedding network
    training. To train the embedding network for the I-275 CPS architecture, 1300
    different trucks are manually picked, and each truck has two traffic images captured
    at different locations along the corridor. Each image is cropped to remove the
    non-truck background and resized to the embedding network required input size
    of 416-by-416 pixels. The dataset is augmented by flipping the images both vertically
    and horizontally to prevent overfitting during training resulting in 3900 image
    pairs for training. During the training of the triplet network, each image pair
    of the same truck is randomly combined with a different truck to form the triplet
    input in an epoch. It should be noted that only images that have been processed
    in the same way (i.e., flipping vertically, flipping horizontally or unchanged)
    are combined. The convolutional part of the embedding network is initialized using
    the corresponding weights of the trained YOLOv3-tiny model used in Cam-1. The
    weight of this part is frozen for the first 5 epochs and then fine-tuned in the
    training process along with all the other layers which are initialized following
    an Xavier uniform distribution (Glorot and Bengio, 2010). Dropout (Sutskever et
    al., 2014) with a rate of 0.5 (i.e., 50% of the units are randomly dropped) is
    also added before the first fully connected layer to overcome overfitting. Both
    architectures are trained using an Adam optimizer (Kingma and Ba, 2014) with a
    base learning rate of 5 × 10−4 and a batch size of 32. The margin is set to be
    due to this margin offering the best performance as observed using the data from
    I-275. The implementation is based on the PyTorch framework (Paszke et al., 2017)
    executed on a Nvidia Titan Xp GPU. To test the performance of the embedding network
    trained using the triplet architecture, 150 truck image pairs that don’t belong
    to the training set are selected without any data augmentation. The 150 pairs
    are divided into 15 sets of 10 trucks and the two resultant embedding networks
    are tested for truck reidentification following the mutual nearest neighbor approach,
    which leads to 1500 similarity calculations. A comparison between two truck images
    takes around 3.5 ms, on average. The trained embedding network obtains a precision
    of 100% on the test data, a recall of 95.00% and a F1-score of 0.97. The F1-score
    is the harmonic mean of precision and recall calculated as (Sasaki, 2007): (2)
    The CNN embedding network has also been compared with hand-crafted feature extraction
    methods and the same embedding network trained using a Siamese network; it has
    been shown that the triplet method outperforms its counterparts in terms of reidentification
    speed and accuracy (Hou et al., 2019b). Finally, the embedding network trained
    using the triplet architecture is employed for the truck matching module. However,
    the method fails to distinguish some extremely similar truck pairs, which are
    even difficult for humans to differentiate. For examples, some trucks seen at
    different locations along the corridor may come from the same transportation company
    with identical truck aesthetics. Some false positives are shown in Fig. 17. The
    performance also downgrades in the case of harsh weather (e.g., raining or snowing),
    blurring (dark light and low shutter speed) or low contrast images (e.g., glare
    or haze). Those false positives might be ruled out by checking if the corresponding
    travel time is feasible or the input-output mapping of the two bridge systems
    are reasonable, which will be discussed in the next section. Download : Download
    high-res image (160KB) Download : Download full-size image Fig. 17. Examples of
    false positives of matched truck during the truck reidentification phase: top
    row are images at the Telegraph Road Bridge and the bottom row are images at the
    Newburg Road Bridge. Manual checking reveals these are not the same truck. 5.
    Results A complete data set containing response data from bridges, truck weight
    information measured by the WIM station and truck images taken at the bridges
    and the WIM station are obtained if the truck is reidentified at all three measurement
    locations; a sample is presented in Fig. 18 to showcase such a data set. In some
    cases, the same truck might be observed at only two of the three locations; this
    data is also added to the database of integrated response data. This section presents
    some uses of the data collected related to SHM analyses. Specifically, the linked
    data between the bridges and WIM station for the same truck can serve as a powerful
    basis for correlating responses across the corridor and baselining performance
    using a consistent traffic load set. The results presented herein correspond to
    over one year of data collected by the CPS from August 2017 to August 2018. Download
    : Download high-res image (310KB) Download : Download full-size image Fig. 18.
    An example of a complete data set collected by the CPS system containing two bridges’
    response data, WIM data and truck images captured at each location (data collected
    October 19, 2017 at 6:29 pm). The travel time of trucks between adjacent locations
    along the corridor can be calculated based on the timestamps of each truck passing
    all three data acquisition locations in the I-275 corridor. The three histograms
    plotted in Fig. 19 show the distribution of travel time of trucks from the TRB
    to the NRB, from the NRB to the WIM station and from the TRB to the WIM station,
    respectively. These results confirm the time windows assigned to each location
    by the Cam-1 trigger are suitable for the majority of trucks observed in the corridor.
    It takes 248.48 s and 392.30 s on average for trucks to travel from the TRB to
    the NRB and from the NRB to the WIMS, respectively. Based on the distance between
    different locations along the corridor and the travel time extracted, trucks travel
    at a speed of approximately 64.5 mile per hour (103.8 km per hour) which agrees
    with the average speed observed at the WIM station. It can also be observed that
    longer travel distances introduce larger variations truck travel times because
    of the higher uncertainty associated with traffic conditions over a longer distance.
    Download : Download high-res image (151KB) Download : Download full-size image
    Fig. 19. Histograms of truck travelling time between different locations along
    the corridor: (a) from the TRB to the NRB; (b) from the NRB to the WIM station;
    (c) from the TRB to the WIM station. Data corresponds to collection period from
    August 1, 2017 to August 31, 2018. Given the bridge data that is linked to the
    WIM station data, correlations between the maximum truck-induced bridge strain
    response and the gross weight of the corresponding trucks can be investigated
    using the globally integrated data sets. This is very beneficial for load rating
    of bridges due to the integrated data offering in situ bridge responses corresponding
    to a quantitative measure of the truck weight (Hou et al., 2019a). Depending on
    which lane of the bridge that a vehicle is in, three correlations can be created
    for the bridge response data measured by each sensor on each bridge. To do so,
    only the cases of a single truck crossing the bridge are selected with the cases
    of multiple trucks traversing the bridges simultaneously or in succession with
    a short headway are ruled out. In addition, data with trucks changing lanes while
    on the bridges are not considered. These more complicated scenarios are excluded
    not because they cannot be handled, but simply to establish a clearer linear correlation
    of bridge responses using the collected data as a way to demonstrate the effectiveness
    of the proposed CPS. The cases in which trucks and small vehicles (e.g., cars)
    are on the bridges at the same time are kept assuming the bridge response associated
    with the small vehicles are negligible compared to those of trucks. Fig. 20(a)
    presents a scatter plot showing the correlation between the maximum strain response
    measured by gage S20 on the TRB and the corresponding vehicle gross weights. Similarly,
    Fig. 20(b) is a scatter plot of the maximum strain responses measured by gage
    S2 on the NRB relative to the truck gross weights. Shown in both plots are three
    different scenarios, namely trucks on slow lane, middle lane and fast lane. Statistics
    of the truck events can be found in Table 4. As expected, there exists a strong
    positive linear correlation between the bridge strain response and the vehicle
    weight, as would be expected. The variations observed in the plots are mainly
    attributed to the various types of truck load configurations (including truck
    lengths and weight distributions), varying truck positions within an assigned
    lane and truck speeds. The variances observed in these relationships could possibly
    be reduced by distinguishing vehicles by their FHWA vehicle class or by removing
    cases where small vehicles are present. Download : Download high-res image (305KB)
    Download : Download full-size image Fig. 20. Correlation between bridge maximum
    strain responses and corresponding gross vehicle weight measured by the WIM station:
    (a) TRB strain measured at S20; (b) NRB strain measured at S2 (data collected
    from August 1, 2017 to August 31, 2018). Table 4. Statistics of truck events matched
    from the TRB and the NRB with the WIM station. Bridge #Truck events Multiple trucks
    Change lane Slow lane Middle lane Fast lane TRB 6954 273 21 499 5823 338 NRB 12,178
    897 18 8817 2425 21 Apart from mapping vehicle weights to bridge responses, the
    correlation between the responses of two bridges to the same vehicle load set
    can be also investigated and could naturally account for some of the variations
    associated with the previous cluster analyses. Maximum bridge strain responses
    measured by gage S20 of the TRB and corresponding strain peaks measured by gage
    S2 of the NRB are presented in Fig. 21. Statistics of the truck events can be
    found in Table 5. Three major scenarios are presented depending on the lane that
    vehicles cross the bridges in: 1) on the slow lane of the TRB and the slow lane
    of the NRB, 2) on the middle lane of the TRB and the slow lane of the NRB, and
    3) on the middle lane of the TRB and the middle lane of the NRB. An even stronger
    linear correlation between the two bridges’ responses can be observed and the
    variation of the data around the linear regression would be much smaller when
    compared to the correlation between bridge responses and gross weights measured
    at WIM station. Download : Download high-res image (159KB) Download : Download
    full-size image Fig. 21. Correlation between TRB maximum strain responses (measured
    by S20) and NRB maximum strain responses (measured by S2) to the same truck loads
    (data collected from August 1, 2017 to August 31, 2018). Table 5. Statistics of
    truck events matched between the TRB and the NRB. #Truck events Multiple trucks
    Change lane TRB-slow NRB-slow TBR-middle NRB-slow TRB-middle NRB-middle 8401 613
    65 596 5225 1403 6. Conclusions By leveraging computer vision techniques and cloud
    computing platforms, this study proposed a CPS framework to integrate vehicle-induced
    responses of two highway bridges measured by wireless SHM systems with corresponding
    vehicle weight information measured by a nearby WIM station located along the
    same highway corridor. In the framework, traffic images collected at each measurement
    location serves as the primary data source for integrating the data from the different
    locations along the same highway corridor to the same truck loads. The trained
    YOLOv3-tiny module with an average precision of 96.65% for truck detection is
    deployed in the first traffic camera to detect incoming trucks in real-time and
    to trigger data acquisition processes downstream in the highway corridor. The
    trigger-based data acquisition strategy proved to be at least 24.5% more efficient
    than previously used schedule-based data collection strategies leading to the
    collection of more bridge response data of greater value due to truck events present.
    Collected data was later automatically processed by a two-stage data fusion pipeline
    consisting of local truck detection and global reidentification so that data corresponding
    to the same truck events can be integrated for further analysis. The trained YOLOv3
    model at the first stage had a truck detection average precision of 98.91% providing
    accurate detection results for further data analysis. The second stage employed
    a mutual nearest neighbor truck reidentification strategy by encoding truck images
    into feature vectors using a customized CNN embedding network trained by a triplet
    network architecture, yielding a F1-score of 0.97. With the CPS framework implemented
    along the I-275 northbound highway corridor, over 10,000 data sets (including
    bridge response, load information and traffic images) of truck events have been
    collected since August 2017. Expected linear correlations are observed between
    peak bridge responses and corresponding truck gross weights, and between peak
    bridge responses of the two bridges to the same truck loads. To the best of the
    authors’ knowledge, this is the first CPS framework ever proposed to dramatically
    enhance the value of data collected for bridge SHM. The proposed system is highly
    scalable due to its proven abilities to automate the identification of trucks,
    mapping their travel history in corridors, and triggering SHM data collection
    in response. These leads to the quantitative mapping of vehicle loads to measured
    structural responses of multiple bridges. The impact of this innovation on the
    operational management of highways is enormous. For example, the CPS platform
    can serve as foundation for data-driven bridge damage detection methods, bridge
    load rating, and other health assessment approaches that are difficult to conduct
    without load information with bridge response data. By considering adjacent bridges
    located in close proximity experiencing similar environmental and operating conditions
    (EOC), the health of one bridge could be determined based on its response relates
    to the adjacent bridge. Such a framework may potentially derive innovative data-driven
    approaches for the diagnosis and prognosis of bridge structural health conditions.
    Specifically, the correlation between responses of different bridges to same load
    profile can be used to detect structural deterioration in the bridges through
    statistical process control methods. The CPS framework also offers a rich data
    set for supporting other decisions integral to highway operations including use
    of bridge responses to estimate the magnitude and spatial trajectories of truck
    loads in a highway network. The work lays a foundation for additional future research.
    In the presented study, only cases of a single truck were considered when performing
    the response correlation analyses. Introducing more complex loading scenarios
    in the correlation analysis including scenarios where multiple trucks are on a
    bridge could be investigated in future studies to allow a larger portion of the
    acquired data to be utilized. To do so, ways of combining the load effect of each
    truck weight based on their location on the bridge to the net bridge response
    would be needed in order to perform a proper correlation analysis. One drawback
    of the current vision-based CPS is that it has been developed primarily for daytime
    use; future work is now focused on expanding the capabilities of the system to
    operate reliably during night or under extremely harsh weather (e.g., snowstorms).
    The CPS framework is also being extended to include other ways of identifying
    and tracking trucks based on wireless telemetry such as DSRC (Dedicated Short-Range
    Communications). DSRC could be a powerful approach to overcoming the challenges
    associated with computer vision methods applied to night and snow images. CRediT
    authorship contribution statement Rui Hou: Conceptualization, Methodology, Software,
    Writing - original draft. Seongwoon Jeong: Methodology, Software, Writing - review
    & editing. Jerome P. Lynch: Conceptualization, Supervision, Funding acquisition,
    Writing - review & editing. Kincho H. Law: Conceptualization, Supervision, Funding
    acquisition, Writing - review & editing. Acknowledgements The research is supported
    by a collaborative project funded by the US National Science Foundation (Grant
    No. EECS-1446521 to the University of Michigan and Grant No. EECS-1446330 to Stanford
    University). The authors thank the Michigan Department of Transportation (MDOT)
    especially Peter Jansson, MDOT Chief Bridge Construction Engineer, for access
    to the Telegraph Road Bridge and the Newburg Road Bridge and for offering support
    during installation of the wireless monitoring systems. Appendix A. YOLOv3 architecture
    To extract features from an input image, the YOLOv3 model uses a convolutional
    neural network architecture called “Darknet-53” (Redmon and Farhadi, 2018). The
    overall backbone composition of Darknet-53 is shown in Table 6. Darknet-53 consists
    of 52 convolutional layers in total. In Darknet-53, each convolutional layer is
    made of a set of convolutional filters generating feature maps given its inputs
    followed by a batch normalization (BN) operation (Ioffe and Szegedy, 2015) and
    a leaky ReLU activation (Maas et al., 2013). For a single convolutional layer
    in YOLOv3, the properties of the convolutional filters are mainly defined by the
    number of output channels, the kernel size and the stride. The input of one convolutional
    layer is the output of the previous one and thus the total number of filters in
    one convolutional layer is the product of the number of input channels and the
    number of output channels. During forward inference, each filter kernel performs
    a convolutional operation on its corresponding input channel and all generated
    results (with the same size) are aggregated in a way that each output feature
    map channel is a combination of the processed information of all of the input
    channels. BN is a straightforward operation adjusting the output scale of the
    convolution filters to be in the range of 0 to 1. Such an operation makes it easier
    for the network parameters to converge to a local minimum and thus network learning
    is faster. Its operation is defined as: (3) ∊ where is the output of convolutional
    filters in the same layer, is the total number of output channels, and are two
    learnable parameters updated during training and is the output of BN operation.
    Leaky ReLU is added to the architecture to introduce nonlinearity into each convolutional
    layer; the ReLU operation is defined as (4) where is an indicator function having
    output of 1 when the condition inside the parentheses is met and 0 otherwise.
    is a small constant and represents the “leaky” aspect of the ReLU function. In
    YOLOv3, it is set to 0.1. The Darknet-53 backbone network also contains several
    residual connections (He et al., 2016) which directly add the activation outputs
    of a specific previous layer into those of the current layer. The sizes of the
    feature maps generated by primary convolutional layers are also shown in Table
    6. The YOLOv3-tiny model differs from the YOLOv3 model in that it utilizes a much
    more compact backbone as shown in Table 3 (Redmon and Farhadi, 2018). Table 6.
    Layer types and primary parameters of the YOLOv3 model. Empty Cell YOLOV3 Network
    Repeat Layer Type # Output Channels Size Stride Output Size Empty Cell 1 Convolutional
    32 3 1 256 × 256 Empty Cell 2 Convolutional 64 3 2 128 × 128 1 3 Convolutional
    32 3 4 Convolutional 64 3 1 Residual 128 × 128 Empty Cell 5 Convolutional 128
    3 2 64 × 64 2 6, 8 Convolutional 64 3 1 7, 9 Convolutional 128 3 1 Residual 64
    × 64 Empty Cell 10 Convolutional 256 3 2 32 × 32 8 11, 13, 15, 17, 19, 21, 23,
    25 Convolutional 128 1 1 12, 14, 16, 18, 20, 22, 24, 26 Convolutional 256 3 1
    Residual 32 × 32 Empty Cell Empty Cell Detection (1) 24 1 1 32 × 32 Empty Cell
    27 Convolutional 512 3 2 16 × 16 8 28, 30, 32, 34, 36, 38, 40, 42 Convolutional
    256 3 1 29, 31, 33, 35, 37, 39, 41, 43 Convolutional 512 1 1 Residual 16 × 16
    Empty Cell Empty Cell Detection (2) 24 1 1 16 × 16 Empty Cell 44 Convolutional
    1024 3 2 8 × 8 4 45, 47, 49, 51 Convolutional 512 1 1 46, 48, 50, 52 Convolutional
    1024 3 1 Residual 8 × 8 Empty Cell Empty Cell Detection (3) 24 1 1 8 × 8 The YOLOv3
    network features three convolutional detection heads responsible for producing
    final dense predictions at different scales, which improves its performance on
    detecting objects of different sizes. As described in Section 3.1, each detection
    head outputs a dense bounding box prediction by predicting a tensor of size utilizing
    an additional convolutional layer. For example, in Table 6 the detection (1) layer
    before layer 27 outputs 32 × 32 × (3 × (5 + 3)) indicating the location and class
    information of the predicted bounding boxes, where 32 × 32 is the size of the
    feature map, 3 is the number of anchor boxes predefined for this scale, (5 + 3)
    represents the bound box features which include four elements for bounding box
    location, one element for the bounding box confidence score and three elements
    for the class confidence score (i.e., truck, pick-up truck, and car classes).
    Unlike intermediate convolutional layers for feature extraction, this convolutional
    detection layer doesn’t have a BN module and uses simple linear activation. To
    train a YOLOv3 network to detect vehicles of different types in this study, the
    set of Darknet-53 weights pretrained on ImageNet (Russakovsky et al., 2015) serve
    as initial network parameters. The parameters are then fine-tuned by training
    on the customized vehicle dataset developed in the study. Similar to the original
    YOLOv3 model, the batch size is 64 and the initial learning rate is 0.001. The
    final model is trained for 23,000 iterations. Appendix B. Supplementary material
    The following are the Supplementary data to this article: Download : Download
    XML file (259B) Supplementary data 1. Research data for this article Data not
    available / The data that has been used is confidential Further information on
    research data References Abid et al., 2011 H. Abid, L.T.T. Phuong, J. Wang, S.
    Lee, S. Qaisar V-Cloud: vehicular cyber-physical systems and cloud computing Proceedings
    of the 4th International Symposium on Applied Sciences in Biomedical and Communication
    Technologies (2011), p. 165 Google Scholar Adibfar and Costin, 2019 A. Adibfar,
    A. Costin Next generation of transportation infrastructure management: fusion
    of intelligent transportation systems (ITS) and bridge information modeling (Br
    IM) I. Mutis, T. Hartmann (Eds.), Advances in Informatics and Computing in Civil
    and Construction Engineering, Springer International Publishing, Cham (2019),
    pp. 43-50 CrossRefGoogle Scholar Canny, 1986 J. Canny A computational approach
    to edge detection IEEE Trans. Pattern Anal. Mach. Intell. (1986), pp. 679-698
    Google Scholar Cantero and González, 2015 D. Cantero, A. González Bridge damage
    detection using weigh-in-motion technology J. Bridg. Eng., 20 (2015), p. 04014078,
    10.1061/(ASCE)BE.1943-5592.0000674 View in ScopusGoogle Scholar Catbas et al.,
    2011 F.N. Catbas, R. Zaurin, M. Gul, H.B. Gokce Sensor networks, computer Imaging,
    and unit influence lines for structural health monitoring: Case study for bridge
    load rating J. Bridg. Eng., 17 (2011), pp. 662-670, 10.1061/(asce)be.1943-5592.0000288
    Google Scholar Chen et al., 2006 Chen, Y., Tan, C.-A., Feng, M.Q., Fukuda, Y.,
    2006. A video assisted approach for structural health monitoring of highway bridges
    under normal traffic. In: Smart Structures and Materials 2006: Sensors and Smart
    Structures Technologies for Civil, Mechanical, and Aerospace Systems. p. 61741V.
    Google Scholar Chen et al., 2016 Z. Chen, H. Li, Y. Bao, N. Li, Y. Jin Identification
    of spatio-temporal distribution of vehicle loads on long-span bridges using computer
    vision technology Struct. Control Heal. Monit., 23 (2016), pp. 517-534 CrossRefView
    in ScopusGoogle Scholar Darwish and Cook, 2015 I. Darwish, S.J. Cook Infrastructure
    Monitoring Data Management Final Report East Lansing, MI (2015) Google Scholar
    Ditlevsen, 1994 O. Ditlevsen Traffic loads on large bridges modeled as white-noise
    fields J. Eng. Mech., 120 (1994), pp. 681-694 View in ScopusGoogle Scholar Everingham
    et al., 2010 M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman
    The pascal visual object classes (VOC) challenge Int. J. Comput. Vis., 88 (2010),
    pp. 303-338, 10.1007/s11263-009-0275-4 Google Scholar FHWA, 2018 FHWA National
    Bridge Inventory Federal Highway Administration, Washington, DC (2018) Google
    Scholar Frangopol et al., 2008 D.M. Frangopol, A. Strauss, S. Kim Bridge reliability
    assessment based on monitoring J. Bridg. Eng., 13 (2008), pp. 258-270, 10.1061/(asce)1084-0702(2008)
    13:3(258) View in ScopusGoogle Scholar Fraser et al., 2010 M. Fraser, A. Elgamal,
    X.F. He, J.P. Conte Sensor network for structural health monitoring of a highway
    bridge J. Comput. Civ. Eng., 24 (2010), pp. 11-24, 10.1061/(ASCE)CP.1943-5487.0000005
    View in ScopusGoogle Scholar Gandhi et al., 2007 T. Gandhi, R. Chang, M.M. Trivedi
    Video and seismic sensor-based structural health monitoring: framework, algorithms,
    and implementation IEEE Trans. Intell. Transp. Syst., 8 (2007), pp. 169-180, 10.1109/TITS.2006.888601
    View in ScopusGoogle Scholar Girshick et al., 2014 R. Girshick, J. Donahue, T.
    Darrell, J. Malik Rich feature hierarchies for accurate object detection and semantic
    segmentation Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. (2014),
    pp. 580-587, 10.1109/CVPR.2014.81 View in ScopusGoogle Scholar Glisic et al.,
    2012 B. Glisic, M. Yarnold, F. Moon, A.E. Aktan Advanced visualization and accessibility
    to SHM results involving real-time and historic multi-parameter data and camera
    images Struct. Congr., 2012 (2012), pp. 735-746, 10.1061/9780784412367.066 View
    in ScopusGoogle Scholar Glorot and Bengio, 2010 X. Glorot, Y. Bengio Understanding
    the difficulty of training deep feedforward neural networks Proceedings of the
    Thirteenth International Conference on Artificial Intelligence and Statistics
    (2010), pp. 249-256 Google Scholar Han et al., 2014 W. Han, J. Wu, C.S. Cai, S.
    Chen Characteristics and dynamic impact of overloaded extra heavy trucks on typical
    highway bridges J. Bridg. Eng., 20 (2014), p. 05014011, 10.1061/(asce)be.1943-5592.0000666
    Google Scholar He et al., 2016 K. He, X. Zhang, S. Ren, J. Sun Deep residual learning
    for image recognition 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), IEEE (2016), pp. 770-778, 10.1109/CVPR.2016.90 Google Scholar Hou et al.,
    2019a R. Hou, Y.A. Dedhia, S. Jeong, K.H. Law, M.M. Ettouney, J.P. Lynch Fusion
    of weigh-in-motion system and bridge monitoring data for bridge load rating 9th
    International Conference on Structural Health Monitoring of Intelligent Infrastructure.
    St. Louism Missouri, USA (2019) Google Scholar Hou et al., 2019b R. Hou, S. Jeong,
    K.H. Law, J.P. Lynch Reidentification of trucks in highway corridors using convolutional
    neural networks to link truck weights to bridge responses Proc. SPIE 10970, Sensors
    and Smart Structures Technologies for Civil, Mechanical, and Aerospace Systems
    2019, International Society for Optics and Photonics (2019), p. 109700P, 10.1117/12.2515617
    View in ScopusGoogle Scholar Hou et al., 2019c R. Hou, J.P. Lynch, M.M. Ettouney,
    P.O. Jansson Partial composite action and durability assessment of slab-on-girder
    highway bridge decks in negative bending using long-term structural monitoring
    data J. Eng. Mech. (2019) Google Scholar Hou et al., 2015 R. Hou, Y. Zhang, S.
    O’Connor, Y. Hong, J.P. Lynch Monitoring and identification of vehicle-bridge
    interaction using mobile truck-based wireless sensors Proceedings of 11th International
    Workshop on Advanced Smart Materials and Smart Structures Technology (2015), pp.
    1-2 Google Scholar Hu et al., 2015 X. Hu, C. Daganzo, S. Madanat A reliability-based
    optimization scheme for maintenance management in large-scale bridge networks
    Transp. Res. Part C Emerg. Technol., 55 (2015), pp. 166-178, 10.1016/j.trc.2015.01.008
    View PDFView articleView in ScopusGoogle Scholar Hyun et al., 2017 K. (Kate) Hyun,
    A. Tok, S.G. Ritchie Long distance truck tracking from advanced point detectors
    using a selective weighted Bayesian model Transp. Res. Part C Emerg. Technol.,
    82 (2017), pp. 24-42, 10.1016/j.trc.2017.06.004 Google Scholar Ioffe and Szegedy,
    2015 Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network
    training by reducing internal covariate Shift. arXiv Prepr. arXiv1502.03167. Google
    Scholar Jara et al., 2014 Jara, A.J., Genoud, D., Bocchi, Y., 2014. Big data for
    cyber physical systems an analysis of challenges, solutions and opportunities.
    In: Proceedings - 2014 8th International Conference on Innovative Mobile and Internet
    Services in Ubiquitous Computing, IMIS 2014. pp. 376–380. https://doi.org/10.1109/IMIS.2014.139.
    Google Scholar Jeong et al., 2018 S. Jeong, R. Hou, J.P. Lynch, H. Sohn, K.H.
    Law A scalable cloud-based cyberinfrastructure platform for bridge monitoring
    Struct. Infrastruct. Eng. (2018), pp. 1-21, 10.1080/15732479.2018.1500617 View
    in ScopusGoogle Scholar Jeong et al., 2017 S. Jeong, R. Hou, J.P. Lynch, H. Sohn,
    K.H. Law An information modeling framework for bridge monitoring Adv. Eng. Softw.,
    114 (2017), pp. 11-31, 10.1016/j.advengsoft.2017.05.009 View PDFView articleView
    in ScopusGoogle Scholar Junwon et al., 2015 S. Junwon, W. Jong, L. Jaeha Summary
    review of structural health monitoring applications for highway bridges J. Perform.
    Constr. Facil., 26 (2015), pp. 371-376, 10.1061/(ASCE)CF Google Scholar Khan et
    al., 2016 S.M. Khan, S. Atamturktur, M. Chowdhury, M. Rahman Integration of structural
    health monitoring and intelligent transportation systems for bridge condition
    assessment: current status and future direction IEEE Trans. Intell. Transp. Syst.
    (2016), pp. 1-16, 10.1109/TITS.2016.2520499 Google Scholar Kim and Lynch, 2012
    J. Kim, J.P. Lynch Experimental analysis of vehicle bridge interaction using a
    wireless monitoring system and a two-stage system identification technique Mech.
    Syst. Signal Process., 28 (2012), pp. 3-19, 10.1016/j.ymssp.2011.12.008 View PDFView
    articleGoogle Scholar Kim and Yoon, 2009 Y.J. Kim, D.K. Yoon Identifying critical
    sources of bridge deterioration in cold regions through the constructed bridges
    in North Dakota J. Bridg. Eng., 15 (2009), pp. 542-552, 10.1061/(asce)be.1943-5592.0000087
    View in ScopusGoogle Scholar Kingma and Ba, 2014 Kingma, D.P., Ba, J., 2014. Adam:
    A method for stochastic optimization. arXiv Prepr. arXiv1412.6980. Google Scholar
    Law and Deng, 2018 H. Law, J. Deng Cornernet: Detecting objects as paired keypoints
    Proceedings of the European Conference on Computer Vision (ECCV) (2018), pp. 734-750
    View in ScopusGoogle Scholar Lewis, 1995 Lewis, J.P., 1995. Fast Normalized Cross-Correlation.
    https://doi.org/10.1.1.21.6062. Google Scholar Lin et al., 2013 Lin, M., Chen,
    Q., Yan, S., 2013. Network In network. arXiv Prepr. arXiv1312.4400. Google Scholar
    Lin et al., 2017a Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B.,
    Belongie, S., 2017a. Feature pyramid networks for object detection. In: Proc.
    - 30th IEEE Conf. Comput. Vis. Pattern Recognition, CVPR 2017 2017-Janua, pp.
    936–944. https://doi.org/10.1109/CVPR.2017.106. Google Scholar Lin et al., 2017b
    T.Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar Focal loss for dense object
    detection Proc. IEEE Int. Conf. Comput. Vis. 2017-Octob, 2999–3007 (2017), pp.
    2999-3007, 10.1109/ICCV.2017.324 Google Scholar Liu et al., 2013 J.W.S. Liu, C.S.
    Shih, E.T.H. Chu Cyberphysical elements of disaster-prepared smart environments
    Computer (Long. Beach. Calif), 46 (2013), pp. 69-75, 10.1109/MC.2012.149 View
    in ScopusGoogle Scholar Liu et al., 2016 W. Liu, D. Anguelov, D. Erhan, C. Szegedy,
    S. Reed, C.Y. Fu, A.C. Berg SSD: Single shot multibox detector European Conference
    on Computer Vision, Springer, Cham (2016), pp. 21-37, 10.1007/978-3-319-46448-0_2
    View in ScopusGoogle Scholar Lou et al., 2016 P. Lou, H. Nassif, D. Su, P. Truban
    Effect of overweight trucks on bridge deck deterioration based on weigh-in-motion
    data Transp. Res. Rec. J. Transp. Res. Board, 2592 (2016), pp. 86-97, 10.3141/2592-10
    View in ScopusGoogle Scholar Maas et al., 2013 A.L. Maas, A.Y. Hannun, A.Y. Ng
    Rectifier nonlinearities improve neural network acoustic models Proceedings of
    the 30th International Conference on Machine Learning. Atlanta, Georgia (2013),
    p. 6 Google Scholar McCall and Vodrazka, 1997 B. McCall, W.C. Vodrazka Jr States’
    Successful Practices Weigh-in-Motion Handbook Federal Highway Administration,
    Washington, DC (1997) Google Scholar Micu et al., 2018 Micu, A., McKinstray, R.,
    Angus, E., OBrien, E.J., Malekjafarian, A., Lydon, M., 2018. Estimation of traffic
    load effects on Forth Road Bridge using camera measurements. In: Civil Engineering
    Research in Ireland 2018. Dublin, Ireland. Google Scholar Newman, 2015 S. Newman
    Building Microservices: Designing Fine-Grained Systems (first ed.), O’Reilly Media,
    Inc., Sebastopol, California (2015) Google Scholar O’Connor et al., 2017 S.M.
    O’Connor, Y. Zhang, J.P. Lynch, M.M. Ettouney, P.O. Jansson Long-term performance
    assessment of the Telegraph Road Bridge using a permanent wireless monitoring
    system and automated statistical process control analytics Struct. Infrastruct.
    Eng., 13 (2017), pp. 604-624, 10.1080/15732479.2016.1171883 View in ScopusGoogle
    Scholar Paszke et al., 2017 Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang,
    E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A., 2017. Automatic
    differentiation in PyTorch. In: 31st Conference on Neural Information Processing
    Systems (NIPS 2017), Long Beach, CA. Google Scholar Peeters, 2000 B. Peeters System
    Identification and Damage Detection In Civil Engineering PhD thesis Katholieke
    Universiteit Leuven, Leuven, Belgium (2000) Google Scholar Rajkumar et al., 2010
    Rajkumar, R. (Raj), Lee, I., Sha, L., Stankovic, J., 2010. Cyber-physical systems:
    the next computing revolution. In: Proc. 47th Des. Autom. Conf. - DAC ’10 731.
    https://doi.org/10.1145/1837274.1837461. Google Scholar Redmon, 2016 Redmon, J.,
    2016. Darknet: Open source neural networks in C. Google Scholar Redmon et al.,
    2016 Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once:
    Unified, real-time object detection. In: Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition. pp. 779–788. Google Scholar Redmon and
    Farhadi, 2018 Redmon, J., Farhadi, A., 2018. YOLOv3: an incremental improvement.
    arXiv Prepr. arXiv1804.02767. Google Scholar Redmon and Farhadi, 2017 J. Redmon,
    A. Farhadi YOLO9000: Better, faster, stronger 2017 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), IEEE (2017), pp. 6517-6525, 10.1109/CVPR.2017.690
    View in ScopusGoogle Scholar Ren et al., 2017 S. Ren, K. He, R. Girshick, J. Sun
    Faster R-CNN: towards real-time object detection with region proposal networks
    IEEE Trans. Pattern Anal. Mach. Intell., 39 (2017), pp. 1137-1149, 10.1109/TPAMI.2016.2577031
    Google Scholar Russakovsky et al., 2015 O. Russakovsky, J. Deng, H. Su, J. Krause,
    S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A.C. Berg,
    L. Fei-Fei ImageNet large scale visual recognition challenge Int. J. Comput. Vis.,
    115 (2015), pp. 211-252, 10.1007/s11263-015-0816-y Google Scholar Sasaki, 2007
    Sasaki, Y., 2007. The truth of the F-measure. Google Scholar Schroff et al., 2015
    F. Schroff, D. Kalenichenko, J. Philbin FaceNet: A unified embedding for face
    recognition and clustering Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
    Recognit. 07–12-June (2015), pp. 815-823, 10.1109/CVPR.2015.7298682 Google Scholar
    Simmhan et al., 2013 Y. Simmhan, S. Aman, A. Kumbhare, R. Liu, S. Stevens, Q.
    Zhou, V. Prasanna Cloud-based software platform for big data analytics in smart
    grids Comput. Sci. Eng., 15 (2013), pp. 38-47, 10.1109/MCSE.2013.39 View in ScopusGoogle
    Scholar Sutskever et al., 2014 I. Sutskever, G. Hinton, A. Krizhevsky, R.R. Salakhutdinov
    Dropout: a simple way to prevent neural networks from overfitting J. Mach. Learn.
    Res., 15 (2014), pp. 1929-1958 Google Scholar Swartz et al., 2005 R.A. Swartz,
    D. Jung, J.P. Lynch, Y. Wang, D. Shi, M.P. Flynn Design of a wireless sensor for
    scalable distributed in-network computation in a structural health monitoring
    system Proceedings of the 5th International Workshop on Structural Health Monitoring
    (2005), pp. 12-14 Google Scholar Tian et al., 2019 Tian, Z., Shen, C., Chen, H.,
    He, T., 2019. FCOS: Fully Convolutional One-Stage Object Detection. arXiv Prepr.
    arXiv1904.01355. Google Scholar Vespier et al., 2011 U. Vespier, A. Knobbe, J.
    Vanschoren, S. Miao, A. Koopman, B. Obladen, C. Bosma Traffic events modeling
    for structural health monitoring Advances in Intelligent Data Analysis X, Springer,
    Berlin, Heidelberg (2011), pp. 376-387, 10.1007/978-3-642-24800-9_35 View in ScopusGoogle
    Scholar Wang et al., 2014 M.L. Wang, J.P. Lynch, H. Sohn Sensor Technologies for
    Civil Infrastructures: Applications in Structural Health Monitoring, vol. 2, Elsevier
    (2014) Google Scholar Wang et al., 2004 T.-L. Wang, C. Liu, D. Huang, M. Shahawy
    Truck loading and fatigue damage analysis for girder bridges based on weigh-in-motion
    data J. Bridg. Eng., 10 (2004), pp. 12-20, 10.1061/(asce)1084-0702(2005) 10:1(12)
    Google Scholar Wang et al., 2019 Y. Wang, D. Zhang, Y. Liu, B. Dai, L.H. Lee Enhancing
    transportation systems via deep learning: a survey Transp. Res. Part C Emerg.
    Technol., 99 (2019), pp. 144-163, 10.1016/j.trc.2018.12.004 View PDFView articleGoogle
    Scholar Yin et al., 2008 Y. Yin, S. Lawphongpanich, Y. Lou Estimating investment
    requirement for maintaining and improving highway systems Transp. Res. Part C
    Emerg. Technol., 16 (2008), pp. 199-211, 10.1016/j.trc.2007.07.004 View PDFView
    articleView in ScopusGoogle Scholar Zagoruyko and Komodakis, 2015 Zagoruyko, S.,
    Komodakis, N., 2015. Learning to compare image patches via convolutional neural
    networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition. pp. 4353–4361. Google Scholar Zaurin and Catbas, 2010 R. Zaurin,
    F.N. Catbas Integration of computer imaging and sensor data for structural health
    monitoring of bridges Smart Mater. Struct., 19 (2010), 10.1088/0964-1726/19/1/015019
    Google Scholar Zaurin and Necati Catbas, 2011 R. Zaurin, F. Necati Catbas Structural
    health monitoring using video stream, influence lines, and statistical analysis
    Struct. Heal. Monit. An Int. J., 10 (2011), pp. 309-332, 10.1177/1475921710373290
    View in ScopusGoogle Scholar Zeiler and Fergus, 2014 M.D. Zeiler, R. Fergus Visualizing
    and understanding convolutional networks European Conference on Computer Vision
    (2014), pp. 818-833 CrossRefGoogle Scholar Zhu and Law, 2015 X.Q. Zhu, S.S. Law
    Structural health monitoring based on vehicle-bridge interaction: accomplishments
    and challenges Adv. Struct. Eng., 18 (2015), pp. 1999-2016, 10.1260/1369-4332.18.12.1999
    View in ScopusGoogle Scholar Cited by (35) Non-contact weigh-in-motion approach
    with an improved multi-region of interest method 2024, Mechanical Systems and
    Signal Processing Show abstract Predicting the core determinants of cloud-edge
    computing adoption (CECA) for sustainable development in the higher education
    institutions of Africa: A high order SEM-ANN analytical approach 2024, Technological
    Forecasting and Social Change Show abstract A sequence-to-sequence model for joint
    bridge response forecasting 2023, Mechanical Systems and Signal Processing Show
    abstract Why did the AI make that decision? Towards an explainable artificial
    intelligence (XAI) for autonomous driving systems 2023, Transportation Research
    Part C: Emerging Technologies Show abstract Large field monitoring system of vehicle
    load on long-span bridge based on the fusion of multiple vision and WIM data 2023,
    Automation in Construction Show abstract Monitoring vehicles with permits and
    that are illegally overweight on bridges using Weigh-In-Motion (WIM) devices:
    A case study from Brescia 2023, Case Studies on Transport Policy Show abstract
    View all citing articles on Scopus View Abstract © 2019 Elsevier Ltd. All rights
    reserved. Recommended articles Incorporating cost uncertainty and path dependence
    into treatment selection for pavement networks Transportation Research Part C:
    Emerging Technologies, Volume 110, 2020, pp. 40-55 Fengdi Guo, …, Randolph Kirchain
    View PDF An efficent computing strategy based on the unconditionally stable explicit
    algorithm for the nonlinear train-track-bridge system under an earthquake Soil
    Dynamics and Earthquake Engineering, Volume 145, 2021, Article 106718 Yuanjun
    Chen, …, Jing Li View PDF Statistical bridge damage detection using girder distribution
    factors Engineering Structures, Volume 109, 2016, pp. 139-151 Alexandra J. Reiff,
    …, Richard M. Vogel View PDF Show 3 more articles Article Metrics Citations Citation
    Indexes: 33 Captures Readers: 80 View details About ScienceDirect Remote access
    Shopping cart Advertise Contact and support Terms and conditions Privacy policy
    Cookies are used by this site. Cookie settings | Your Privacy Choices All content
    on this site: Copyright © 2024 Elsevier B.V., its licensors, and contributors.
    All rights are reserved, including those for text and data mining, AI training,
    and similar technologies. For all open access content, the Creative Commons licensing
    terms apply."'
  inline_citation: '>'
  journal: 'Transportation Research Part C: Emerging Technologies'
  limitations: '>'
  relevance_evaluation: The paper is highly relevant to the review's focus on automated
    systems for real-time irrigation management using computer vision and machine
    learning. The authors specifically explore how computer vision-based truck detection
    and cyber-physical systems can be used to integrate data from sensors on bridges
    and weigh-in-motion systems to provide an input-output mapping of truck loads
    to bridge response data. This allows for a data-driven assessment of bridge health
    condition in real time based on truck weight data, which is not available using
    traditional methods of bridge data collection, and would therefore be valuable
    for including in the literature review.
  relevance_score: 1.0
  relevance_score1: 0
  relevance_score2: 0
  title: Cyber-physical system architecture for automating the mapping of truck loads
    to bridge behavior using computer vision in connected highway corridors
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Shakir, M. Z., & Ramzan, N. (Eds.). (2020). AI for Emerging Verticals:
    Human-robot computing, sensing and networking. The Institution of Engineering
    and Technology.'
  authors:
  - Shakir M.Z.
  - Ramzan N.
  citation_count: '2'
  data_sources: Literature review, case studies
  description: By specializing in a vertical market, companies can better understand
    their customers and bring more insight to clients in order to become an integral
    part of their businesses. This approach requires dedicated tools, which is where
    artificial intelligence (AI) and machine learning (ML) will play a major role.
    By adopting AI software and services, businesses can create predictive strategies,
    enhance their capabilities, better interact with customers, and streamline their
    business processes. This edited book explores novel concepts and cutting-edge
    research and developments towards designing these fully automated advanced digital
    systems. Fostered by technological advances in artificial intelligence and machine
    learning, such systems potentially have a wide range of applications in robotics,
    human computing, sensing and networking. The chapters focus on models and theoretical
    approaches to guarantee automation in large multi-scale implementations of AI
    and ML systems; protocol designs to ensure AI systems meet key requirements for
    future services such as latency; and optimisation algorithms to leverage the trusted
    distributed and efficient complex architectures. The book is of interest to researchers,
    scientists, and engineers working in the fields of ICTs, networking, AI, ML, signal
    processing, HCI, robotics and sensing. It could also be used as supplementary
    material for courses on AI, machine and deep learning, ICTs, networking signal
    processing, robotics and sensing.
  doi: 10.1049/PBPC034E
  explanation: This paper focuses on the role of advanced monitoring techniques for
    automated irrigation systems, highlighting the challenges and opportunities in
    integrating IoT-enabled sensors and computer vision for precision irrigation.
    It also emphasizes the importance of integrating these technologies with existing
    irrigation infrastructure and exploring interoperability and standardization strategies.
  extract_1: One notable challenge in implementing automated irrigation systems is
    the integration of diverse technologies and data sources. This issue requires
    careful consideration of interoperability and standardization to ensure seamless
    communication and data exchange among different components of the system.
  extract_2: A comprehensive approach to integration involves establishing common
    protocols and data formats, addressing issues related to data quality, and ensuring
    compatibility between different technologies and platforms.
  full_citation: '>'
  full_text: '>

    "Visit www.theiet.org | My IET Shopping cart | Subscribe | Contacts | Help All
    content Journals & magazines Conferences Books Reference Work    Advanced search
    Journals & magazines Conferences eBooks Reference Subjects Collections About Your
    access is provided by: University of Nebraska - Lincoln Register to create your
    user account, or  sign in if you have an existing account Login Forgotten password?
    Login via your institution Share Tools Add to favourites Create book email alert
    Copyrights and Permissions Export citations Key Free content Open access content
    Subscribed content Trial content Home > eBooks > AI for Emerging Verticals: Human-robot
    computing, sensing and networking AI for Emerging Verticals: Human-robot computing,
    sensing and networking Buy e-book PDF $160.00 (plus tax if applicable) Add to
    cart Buy print edition Editors: Muhammad Zeeshan Shakir 1 ; Naeem Ramzan 2 View
    affiliations  Publication Year: 2020 Description Chapters (18) Related Content
    Supplementary material (0) By specializing in a vertical market, companies can
    better understand their customers and bring more insight to clients in order to
    become an integral part of their businesses. This approach requires dedicated
    tools, which is where artificial intelligence (AI) and machine learning (ML) will
    play a major role. By adopting AI software and services, businesses can create
    predictive strategies, enhance their capabilities, better interact with customers,
    and streamline their business processes. This edited book explores novel concepts
    and cutting-edge research and developments towards designing these fully automated
    advanced digital systems. Fostered by technological advances in artificial intelligence
    and machine learning, such systems potentially have a wide range of applications
    in robotics, human computing, sensing and networking. The chapters focus on models
    and theoretical approaches to guarantee automation in large multi-scale implementations
    of AI and ML systems; protocol designs to ensure AI systems meet key requirements
    for future services such as latency; and optimisation algorithms to leverage the
    trusted distributed and efficient complex architectures. The book is of interest
    to researchers, scientists, and engineers working in the fields of ICTs, networking,
    AI, ML, signal processing, HCI, robotics and sensing. It could also be used as
    supplementary material for courses on AI, machine and deep learning, ICTs, networking
    signal processing, robotics and sensing. Inspec keywords: affective computing;
    cellular radio; robots; medical computing; sensors; learning (artificial intelligence);
    5G mobile communication Other keywords: artificial neural networks; emotion recognition;
    robot intelligence; human-robot networking; data reduction; autonomous robotic
    grasping; indoor classification; perceptual video quality metrics; beyond-5G wireless
    networks; predictive mobility management; 5G wireless networks; adaptive feature
    selection; quadrotor; deterministic compressed sensing; deep Q-network-based coverage
    hole detection; early hyperkalaemia detection; artificial intelligence; ECG monitoring;
    multitask learning; visual object tracking; ultrawide bandwidth sensor node localization;
    cascaded machine learning; fuzzy logic controller; large-scale distributed SOM-based
    architecture; affective computing; soft end-effectors; EEG-based biometrics; indoor
    localization; autonomous driving; deep learning; large-scale scalable SOM-based
    architecture; human manipulation modelling; template ageing; connected health;
    human-robot sensing; human-horse interaction; cellular networks; Internet of Things;
    human-robot computing; affect detection; data analytics; surface water pollution
    monitoring Subjects: Sensing devices and transducers; Robotics; General and management
    topics; Expert systems and other AI software and techniques; Transducers and sensing
    devices; Mobile radio systems; Biology and medical computing; General electrical
    engineering topics Book DOI: 10.1049/PBPC034E Chapter DOI: 10.1049/PBPC034E ISBN:
    9781785619823 e-ISBN: 9781785619830 Page count: 386 Format: PDF                     Email
    this page Print this page Back to top Journals & magazines Conferences eBooks
    Reference Subjects Collections About View sitemap Legal notices Accessibility
    Cookies Privacy statement Copyright & permissions Help All contents © The Institution
    of Engineering and Technology 2024 The Institution of Engineering and Technology
    is registered as a Charity in England & Wales (no 211014) and Scotland (no SC038698)"'
  inline_citation: (Shakir & Ramzan, 2020)
  journal: 'AI for Emerging Verticals: Human-robot computing, sensing and networking'
  key_findings: 'IoT-enabled sensors and computer vision offer advanced monitoring
    capabilities for automated irrigation systems.

    Integrating these technologies with existing infrastructure requires careful consideration
    of interoperability and standardization.

    Addressing challenges in data integration and interfacing with legacy systems
    is crucial for successful implementation of automated irrigation systems.'
  limitations: The paper does not provide a comprehensive analysis of interoperability
    and standardization strategies.
  main_objective: To explore novel concepts and cutting-edge research in designing
    fully automated advanced digital systems in the context of AI and robotics.
  relevance_evaluation: This paper is moderately relevant to the point of interest
    in the context of integration and interoperability in automated irrigation systems.
    It provides valuable insights into the use of IoT-enabled sensors and computer
    vision for advanced monitoring purposes. The paper discusses the challenges of
    data integration and interfacing with legacy systems.
  relevance_score: '0.7'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT-enabled sensors, computer vision, AI, ML
  title: 'AI for emerging verticals: Human-robot computing, sensing and networking'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Aljaloud, S., Alshudukhi, J., Alhamazani, K. T., & Belay, A. (2022).
    Comparative Study of Artificial Intelligence Techniques for the Diagnosis of Chronic
    Nerve Diseases. Computational and Mathematical Methods in Medicine, 2022, 3522510.
    https://doi.org/10.1155/2022/3522510
  authors:
  - Aljaloud S.
  - Alshudukhi J.
  - Alhamazani K.T.
  - Belay A.
  citation_count: '1'
  data_sources: Bibliographically reviewed literature on AI techniques for disease
    diagnosis in agriculture
  description: Farming is essential to the long-term viability of any economy. It
    differs in each country, but it is essential for long-term economic success. Only
    a few of the agricultural industry's issues include a lack of suitable irrigation
    systems, weeds, and plant monitoring concerns as a consequence of efficient management
    in distinct open and closed zones for crop and plant treatment. The objective
    of this work is to carry out a study on the use of artificial intelligence and
    computer vision methods for diagnosis of diseases in agro sectors in the context
    of agribusiness, demonstrating the feasibility of using these techniques as tools
    to support automation and obtain productivity gains in this sector. During the
    literary analysis, it was determined that technology could improve efficiency,
    hence decreasing these types of concerns. Given the consequences of a wrong diagnosis,
    diagnosis is work that requires a high level of precision. Fuzzy cognitive maps
    were shown to be the most efficient method of utilizing bibliographically reviewed
    preferences, which led to the consideration of neural networks as a second option
    because this technique is the most robust in terms of the qualifying criteria
    of the data stored in databases.
  doi: 10.1155/2022/3522510
  explanation: 'The research article primarily focuses on the application of artificial
    intelligence techniques for diagnosing plant diseases in agriculture. The authors
    conduct a comparative analysis of three specific AI techniques: expert systems,
    neural networks, and multi-agent systems. The analysis is based on specific criteria,
    including accuracy, learning capacity, interpretability, adaptability, and efficiency.
    The article advocates for the use of fuzzy cognitive maps as a decision-making
    tool to determine the most suitable AI technique for agricultural disease diagnosis.'
  extract_1: '"Fuzzy cognitive maps were shown to be the most efficient method of
    utilizing bibliographically reviewed preferences, which led to the consideration
    of neural networks as a second option because this technique is the most robust
    in terms of the qualifying criteria of the data stored in databases."'
  extract_2: '"In this sense, it stands out that neural network is one of the most
    powerful artificial intelligence tools. They have the provision of learning a
    group of matrices and structural weighting data to represent the learning of the
    different models [ 18]. Neural networks are adjusted for the diagnosis of diseases
    in agriculture since they are composed of numerous processing units, simulating
    the brain’s functionality and the way of performing functions as a living being
    [ 19]."'
  full_citation: '>'
  full_text: '>

    "This website stores data such as cookies to enable essential site functionality,
    as well as marketing, personalization, and analytics. By remaining on this website
    you indicate your consent. Cookie Policy Journals Publish with us Publishing partnerships
    About us Blog Computational and Mathematical Methods in Medicine Journal overview
    For authors For reviewers For editors Table of Contents Special Issues Computational
    and Mathematical Methods in Medicine/ 2022/ Article On this page Abstract Introduction
    Materials Results and Discussion Conclusions Data Availability Conflicts of Interest
    References Copyright Related Articles Research Article Retraction ! This article
    has been Retracted. To view the article details, please click the ‘Retraction’
    tab above. Special Issue Medical Data Analysis for Neurodegenerative Disorders
    Diagnosis using Computational Techniques View this Special Issue Research Article
    | Open Access Volume 2022 | Article ID 3522510 | https://doi.org/10.1155/2022/3522510
    Show citation [Retracted] Comparative Study of Artificial Intelligence Techniques
    for the Diagnosis of Chronic Nerve Diseases Saud Aljaloud ,1Jalawi Alshudukhi
    ,1Khalid Twarish Alhamazani ,1and Assaye Belay 2 Show more Academic Editor: Deepika
    Koundal Received 07 Dec 2021 Revised 21 Dec 2021 Accepted 24 Dec 2021 Published
    13 Jan 2022 Abstract Farming is essential to the long-term viability of any economy.
    It differs in each country, but it is essential for long-term economic success.
    Only a few of the agricultural industry’s issues include a lack of suitable irrigation
    systems, weeds, and plant monitoring concerns as a consequence of efficient management
    in distinct open and closed zones for crop and plant treatment. The objective
    of this work is to carry out a study on the use of artificial intelligence and
    computer vision methods for diagnosis of diseases in agro sectors in the context
    of agribusiness, demonstrating the feasibility of using these techniques as tools
    to support automation and obtain productivity gains in this sector. During the
    literary analysis, it was determined that technology could improve efficiency,
    hence decreasing these types of concerns. Given the consequences of a wrong diagnosis,
    diagnosis is work that requires a high level of precision. Fuzzy cognitive maps
    were shown to be the most efficient method of utilizing bibliographically reviewed
    preferences, which led to the consideration of neural networks as a second option
    because this technique is the most robust in terms of the qualifying criteria
    of the data stored in databases. 1. Introduction 1.1. Artificial Intelligence
    and Phytosanitary Diagnostics Diagnosis, in humans, animals, and plants, is an
    activity carried out by specialists with expertise in the field in which the diagnosis
    process is carried out. Artificial intelligence tries to emulate the natural ability
    that human beings have to make decisions of any archetype, simulating in their
    way of learning how that instruction is based on reaching decision-making [ 1].
    The agricultural sector is one of the essential sectors globally speaking; however,
    it has been the victim of losses due to diseases, taking into consideration that
    people with problems of poverty live in these areas, which makes this group of
    farmers very vulnerable being interrupted in the supply of food products derived
    from pathogens [ 2, 3]. For this purpose, various artificial intelligence techniques
    have been used, including neural networks, expert systems (already mentioned above),
    data mining, and intelligent agents. In recent decades, some efforts have been
    made to apply predictive analysis to health systems and to activate machine learning
    systems that facilitate the diagnosis of diseases [ 4]. For the diagnosis of diseases
    in agriculture, the quick and easy integration of the contents that can replace
    the old diagnosis is a primary cause, being that the diagnosis is a highly complex
    process, which is not accurate and cannot be carried out to term without previously
    considering other alternatives, due to the uncertainty present in the procedure
    [ 5, 6]. As a result of this great uncertainty, the determinations that the different
    specialists have adopted in each stage of the diagnostic procedure are not always
    the same, since each specific incident entails a different decision procedure
    for each specialist, although this is trying the assessment of the same type of
    condition. There are ways to identify any plant’s conditions, such as examining
    plant tissues in an equipped laboratory or the presence of a specialist agronomist
    at the planting site; in either case, the problem is the time it takes to get
    the results [ 2]. Among the techniques that have been used most frequently for
    the recognition of diseases, the following stand out: fuzzy logic (diagnosis based
    on classification), expert systems (based on rules, probabilities, based on cases),
    neural networks (diagnosis based on training and recognition), and applied data
    mining (diagnosis based on pattern recognition) [ 1]. These techniques have substantial
    potential in artificial intelligence in agriculture. Again, the contrast research
    is an important component in determining which artificial intelligence approach
    is best suited to diagnosing agricultural illnesses. The examination of fuzzy
    cognitive maps is used to determine which artificial intelligence approach is
    most suited to illness diagnosis. Fuzzy cognitive maps have become one of the
    most widely used and researched AI approaches in recent years. The need to construct
    causality models that are more realistic, as well as the necessity for accuracy
    and interpretability, has prompted an upsurge in research into this sort of mathematical
    representation [ 8]. This study provides an overview of artificial intelligence-based
    diagnostic reasoning methodologies (AI). It provides an outline of the history
    of various techniques. Due to the limitations of existing approaches, a group
    of researchers resorted to experienced physicians for profound insights into the
    underlying nature of clinical situation, as well as artificial intelligence to
    put these insights into practical programs. In addition, as understanding of the
    information processing qualities of computer models of cognitive processes improves,
    efficient data structures and algorithms are often designed to execute the same
    behavior on computers that bear little, if any, resemblance to the original models.
    This chapter discusses the development of some of these models as well as its
    applications in the field of general medical diagnosis. It examines the evolution
    of computational techniques in the area of medical diagnosis. The chapter also
    looks at a range of systems that are becoming more capable and complicated, with
    an emphasis on the link among representation of knowledge and logical thinking,
    as well as how our insight into the nature of diagnosing expertise has changed
    over time. It also offers a description of a sequential diagnostic procedure Bayesian
    belief probability theory [ 7]. These provide a potent technique for modeling
    and predicting complex systems that is extremely multivariable and interpretable.
    MCD’s core principle is to regulate the object of research by breaking it into
    primary parts and then describing the dynamics of internal interactions between
    these elements [ 9]. 2. Materials and Methods or Methodology The research adopted
    a quantitative approach. Decision theory was used, beginning with the identification
    and definition of the problem and ending with the choice of one or more possibilities,
    which involves a decision-making exercise, a procedure that is based on five essential
    stages. Decision-making, in the present work, focuses on the search for the best
    technique commonly used to diagnose diseases in agriculture. The process begins
    with the identification and definition of the problem and ends with the selection
    of one or more variants, which implies the act of making a decision. Based on
    the information previously registered in databases, five phases have been determined
    to decide on the best artificial intelligence technique, which is used regularly
    in the diagnosis of agricultural diseases. The initial stages of the decision-making
    procedure consist of articulating the problem and the last two in analyzing it
    [ 10]. The stages for the conclusion of an incident through decision-making are
    presented (Figure 1):    Figure 1  Phases for solving a problem through decision-making.
    The research stage of the decision-making process can take two main formats: qualitative
    and quantitative. Qualitative analysis is based on the reasoning and practice
    of the entity that made the decision; it includes your intuitive impression of
    the problem. Using the quantitative approach, the researcher focuses on the data
    or factors that relate to the incidence and leads to the development of exact
    science expressions that describe the problem’s purposes, limitations, and interactions
    [ 11]. Then, using quantitative methods, it is possible to offer a suggestion
    based on the quantitative components of the problem. In this sense, we have to
    know the best way to evaluate to determine which artificial intelligence technique
    is best adapted to the diagnosis of diseases in agriculture, considering the peculiarities
    of each technique: (1) The first stage (A): it is carried out continuously on
    the advantages of AI practices used in diagnoses, in which plant pathology professionals
    have an expert role in obtaining knowledge (2) The second stage (B): the evaluation
    of AI techniques is carried out, presenting case studies to make comparisons,
    the criteria of experts prevail, and the comparison with data stored in databases
    (3) Third stage (C): agronomists or phytopathologists (experts) can raise and
    solve problems related to their field and imply the knowledge built during the
    determined time of the presence of disease symptoms Legend: (1) General principle
    (based on) (2) A specific branch of artificial intelligence (3) Classification
    within the branch of artificial intelligence (4) Application To select the technique
    that best fits the process of diagnosing diseases in agriculture, it is recommended
    to consider each technique’s principle. (1) (1—expert system) based on obtaining
    knowledge (2) (2—neural networks) based on training and classification (3) (3—intelligent
    agents) based on the interrelation with the environment Establish the best way
    to evaluate the diagnoses with the cases previously stored in the databases in
    agriculture. When establishing the fuzzy cognitive maps since it forms globalization
    of the cognitive maps, both are directed graphs, whose vertices represent concepts
    and their edges represent causal relationships between these concepts [ 12]. The
    difference is in the values assigned to the edges that signify the degree of relationship
    between the vertices. In cognitive maps, these values are , which suppose an inverted
    or direct correlation between the concepts. In comparison, fuzzy cognitive maps
    take values in the range , where a scale is included between the differences of
    the concepts [ 13]. 2.1. Steps to Follow to Apply the Fuzzy Cognitive Map Method
    These are directed graphs that use vertices to represent the concepts or variables
    in scope. At the same time, the edges indicate positive, negative, or null causality
    relationships between the terms represented by the vertices. Fuzzy cognitive maps
    (FCMs) extend cognitive maps to the fuzzy domain in the interval to determine
    the strength in causal relationships [ 14]. FCM refines cognitive maps, which
    describe joint strength through fuzzy data in the interval [ 2]. Cognitive maps
    help us to show the causal interrelationships between the variables, where each
    edge is related to a weight value in the set, being 0 the one that indicates that
    there is no causal relationship between the variables, -1 means that the relationship
    of causality is inverse (if one variable increases and the other decreases), and
    one means that there is a direct causality relationship (both variables are increasing
    or both are decreasing) [ 15]. These factors do not cover the uncertainty in these
    causal relationships, which causes fuzzy cognitive maps to emerge. It is possible
    to introduce a classification in the previous set of weights defined in the continuous
    interval . There are three classes of possible causal relationships between concepts
    in FCMs: (1) Causality is positive (): there is a directly proportional causality
    between the concepts and , that is, the increase (decrease) in the value of leads
    to the increase (decrease) of the value of (2) Causality is negative (): there
    is an inversely proportional causality value of and , that is, the increase (decrease)
    of the value of leads to the decrease (increase) in the value of (3) The nonexistence
    of relationships (): this indicates the nonexistence of a causal correlation between
    and We propose in this study, based on cognitive maps for decision-making, the
    following algorithm: (1) The selection of the most relevant causes (2) Once the
    most relevant causes have been selected, the causality between them will be modeled
    with the help of a fuzzy cognitive map (3) Static analysis [ 15]: the following
    measures are scored for the absolute results of the adjacent matrix: (i) Outdegree,
    which is identified with od (vi), is the sum for each row of the absolute values
    of a variable in the adjacent fuzzy matrix. It is a measure of the cumulative
    strength of the existing relationships of the variable (ii) Indegree, which is
    identified by id (vi), is the sum of each column of the absolute results of one
    of the variables of the adjacent fuzzy matrix. This measures the cumulative input
    force of the variable (iii) The centrality or the degree of the totality of the
    variable is the sum of od (vi), where id (vi) is as follows: In the end, these
    variables can be classified according to the following criteria, according to
    authors [ 16]: (a) Transmission variables are those that contain (b) The receiving
    variables are those with and (c) Common or ordinary variables comply with and
    They are managed in ascending order according to the degree of centrality. Due
    to the significant utility of fuzzy cognitive maps, they have been recognized
    to model various scenarios. This being the case, we were able to find extensions
    based on intervals, intuitionist fuzzy logic [ 16], among other extensions. A
    diffuse cognitive map can be represented by a digraph (Figure 2) using which these
    nodes manage to represent the criteria, and then, the arcs show us a causal link.    Figure
    2  Fuzzy cognitive map. When a group of experts () participates, the adjacent
    matrix can be formulated using an aggregation operator, such as the arithmetic
    mean. The most straightforward tactic is to find the mean on each connection for
    each proficient. For individuals, the final adjacent matrix of the FCM () is obtained
    as [ 17] This ease of aggregation allows us to create collective mental models
    with relative simplicity. This method allows us schemes that are as close to reality
    as possible to represent knowledge. Among the factors that facilitate an interpretation
    of knowledge as authentic as possible is the opportunity to represent the cycles,
    vagueness, and ambiguity present and also great ease of use for obtaining knowledge
    by farmers. 3. Results and Discussion To obtain the results of the current problem,
    it is necessary to develop the following model, taking into account each criterion
    that has been frequently used for the diagnosis of diseases in agriculture through
    bibliographic review (Figure 3).    Figure 3  Diffuse cognitive map model. The
    following evaluation criteria were considered in the model in each of the artificial
    intelligence techniques raised above: (i) Accuracy (ii) Learning capacity (iii)
    Interpretability (iv) Adaptability (v) Efficiency The suitability of the method
    is the factor that evaluates the three artificial intelligence techniques to be
    able to choose the best of these. We will start with the description in the following
    tables of the artificial intelligence techniques and their corresponding criteria,
    to later represent them graphically by means of a hyperbolic tangent and then
    present the result of the method used by means of fuzzy cognitive maps. Next,
    in Table 1, we locate the values of the criteria of the expert system technique.
    Table 1  Evaluation table of expert system criteria. It is graphed showing us
    the result of 0.89 in the criterion of suitability of the method. Table 2 refers
    to the values of the neural network technique criteria. Table 2  Multiagent system
    criterion evaluation table. It is graphed showing us the result of 0.98 under
    the criterion of appropriateness of the method in the neural network technique.
    Table 2 refers to the values of the criteria of the multiagent system technique.
    It is graphed showing us the result of 0.82 in the criterion of suitability of
    the method of the multiagent system technique. Finally, we will show in Table
    3 the measures of centrality based on the absolute value of the adjacency matrix
    where the result of the centrality is the total sum of the values of the indegree
    and outdegree criteria. Table 3  Matrix table of adjacent results. According to
    the analysis carried out and according to the results that are displayed in the
    total matrix of adjacent results, the value of the most qualified criterion is
    the suitability of the method, achieving the artificial intelligence technique
    with the highest value neural networks, giving as a result, neural network is
    the technique that best adjusts for the diagnosis of diseases in agriculture,
    taking into consideration the criteria with which it has been evaluated in this
    study using a bibliographic review and applying fuzzy cognitive maps. The result
    is verified when relating and putting into practice diagnostics in agriculture
    based on artificial intelligence techniques, particularly using data stored in
    databases. In this sense, it stands out that neural network is one of the most
    powerful artificial intelligence tools. They have the provision of learning a
    group of matrices and structural weighting data to represent the learning of the
    different models [ 18]. Neural networks are adjusted for the diagnosis of diseases
    in agriculture since they are composed of numerous processing units, simulating
    the brain’s functionality and the way of performing functions as a living being
    [ 19]. Neural networks have been frequently applied in various phytosanitary diagnosis
    applications, obtaining very favorable results and with a higher degree of certainty
    than other artificial intelligence techniques, which they provide in phytosanitary
    diagnoses [ 20]. 4. Conclusions The various artificial intelligence tools were
    analyzed for the detection of phytosanitary diseases in agriculture, each of which
    has been of great importance; this process, which has been supported by artificial
    intelligence throughout history, has been able to represent precise results by
    using them, specifically in the area of agriculture, different disease diagnoses
    have been developed [ 21, 22]. In this study carried out, the following artificial
    intelligence techniques, neural networks, expert systems, and multivalent systems,
    were bibliographically verified, considering the principle of each technique,
    based on fuzzy cognitive maps for decision-making in algorithms. Each of the characteristics
    of the artificial intelligence techniques was evaluated, thus obtaining the most
    efficient through the instrumentalization of the bibliographically reviewed preferences
    through fuzzy cognitive maps, which led to consider the second alternative, neural
    networks, since this technique is the most robust in terms of the qualifying criteria
    of the data stored in databases. Data Availability The data underlying the results
    presented in the study are available within the manuscript. Conflicts of Interest
    The authors declare that they have no conflicts of interest regarding the publication
    of this paper. References K. Ferentinos, “Deep learning models for plant disease
    detection and diagnosis,” Computers and Electronics in Agriculture, vol. 145,
    pp. 311–318, 2018. View at: Publisher Site | Google Scholar A. Abdullah Hamad,
    M. Lellis Thivagar, M. Bader Alazzam et al., “Dynamic systems enhanced by electronic
    circuits on 7D,” Advances in Materials Science and Engineering, vol. 2021, Article
    ID 8148772, 11 pages, 2021. View at: Publisher Site | Google Scholar J. Barbedo,
    “Expert systems applied to plant disease diagnosis: survey and critical view,”
    IEEE Latin America Transactions, vol. 14, no. 4, pp. 1910–1922, 2016. View at:
    Publisher Site | Google Scholar K. A. Bhavsar, J. Singla, Y. D. Al-Otaibi, O.
    Y. Song, Y. B. Zikria, and A. K. Bashir, “Medical diagnosis using machine learning:
    a statistical review,” Computers, Materials & Continua, vol. 67, no. 1, pp. 107–125,
    2021. View at: Publisher Site | Google Scholar M. Fatima and M. Pasha, “Survey
    of machine learning algorithms for disease diagnostic,” Journal of Intelligent
    Learning Systems and Applications, vol. 9, no. 1, pp. 1–16, 2017. View at: Publisher
    Site | Google Scholar M. Alsaffar, A. A. Hamad, A. Alshammari et al., “Network
    management system for IoT based on dynamic systems,” Computational and Mathematical
    Methods in Medicine, vol. 2021, Article ID 9102095, 2021. View at: Publisher Site
    | Google Scholar K. A. Bhavsar, J. Singla, Y. D. Al-Otaibi, O. Y. Song, Y. B.
    Zikria, and A. K. Bashir, “Fuzzy expert systems (FES) for medical diagnosis,”
    International Journal of Computer Applications, vol. 63, no. 11, pp. 7–16, 2013.
    View at: Publisher Site | Google Scholar O. C. Akinyokun, I. G. Babatunde, S.
    A. Arekete, and R. W. Samuel, “Fuzzy logic-driven expert system for the diagnosis
    of heart failure disease,” Artificial Intelligence Research, vol. 4, no. 1, pp.
    12–21, 2014. View at: Publisher Site | Google Scholar S. Jha, S. Ahmad, H. A.
    Abdeljaber, A. A. Hamad, and M. B. Alazzam, “A post COVID machine learning approach
    in teaching and learning methodology to alleviate drawbacks of the e-whiteboards,”
    Journal of Applied Science and Engineering, vol. 25, no. 2, pp. 285–294, 2021.
    View at: Google Scholar T. T. Win and S. Markon, “IoT and AI methods for plant
    disease detection in Myanmar,” Master Thesis, Kobe Institute of Computing, Myanmar,
    vol. 5, pp. 20–44, 2018. View at: Google Scholar L. Wang, A. A. Hamad, and V.
    Sakthivel, “IoT assisted machine learning model for warehouse management,” Journal
    of Interconnection Networks, vol. 3, p. 2143005, 2021. View at: Publisher Site
    | Google Scholar K. S. Sidhu, A. S. Gill, A. Arora et al., “Advancements in farming
    and related activities with the help of artificial intelligence: a review,” Environment
    Conservation Journal, vol. 22, pp. 55–62, 2021. View at: Publisher Site | Google
    Scholar G. Alshammari, A. A. Hamad, Z. M. Abdullah et al., “Applications of deep
    learning on topographic images to improve the diagnosis for dynamic systems and
    unconstrained optimization,” Wireless Communications and Mobile Computing, vol.
    2021, Article ID 4672688, 2021. View at: Publisher Site | Google Scholar E. Tetila,
    B. Brandoli, G. Menezes et al., “Automatic recognition of soybean leaf diseases
    using UAV images and deep convolutional neural networks,” IEEE Geoscience and
    Remote Sensing Letters, vol. 8, pp. 1–5, 2019. View at: Publisher Site | Google
    Scholar E. Bourgani, C. Stylios, G. Manis, and V. Georgopoulos, Eds., Time Dependent
    Fuzzy Cognitive Maps for Medical Diagnosis, vol. 5, 2014. View at: Publisher Site
    P. Groumpos, “Why model complex dynamic systems using fuzzy cognitive maps?” Robotics
    & Automation Engineering, vol. 1, pp. 18–29, 2017. View at: Publisher Site | Google
    Scholar B. A. Al-Rahawe, A. A. Hamad, M. H. Al-Zuhairy, H. H. Khalaf, and S. Abebaw,
    “The commitment of Nineveh governorate residents to the precautionary measures
    against global 2019 pandemic and dermatological affection of precautions,” Applied
    bionics and biomechanics, vol. 2021, Article ID 1526931, 2021. View at: Publisher
    Site | Google Scholar K. BhargavRam, “Machine learning based recognition of crops
    diseases by CNN,” The International Journal on Media Management, vol. 8, pp. 1264–1268,
    2019. View at: Google Scholar M. Alsaffar, G. Alshammari, A. Alshammari et al.,
    “Detection of tuberculosis disease using image processing technique,” Mobile Information
    Systems, vol. 2021, Article ID 7424836, 2021. View at: Publisher Site | Google
    Scholar M. B. Alazzam, A. A. Hamad, and A. S. AlGhamdi, “Dynamic mathematical
    models’ system and synchronization,” Mathematical Problems in Engineering, vol.
    2021, Article ID 6842071, 2021. View at: Publisher Site | Google Scholar A. Khadidos,
    A. Khadidos, O. M. Mirza, T. Hasanin, W. Enbeyle, and A. A. Hamad, “Evaluation
    of the risk of recurrence in patients with local advanced rectal tumours by different
    radiomic analysis approaches,” Applied Bionics and Biomechanics, vol. 2021, Article
    ID 4520450, 9 pages, 2021. View at: Publisher Site | Google Scholar C. Stylios
    and P. Groumpos, “Fuzzy cognitive map in modeling supervisory control systems,”
    Journal of Intelligent Fuzzy Systems, vol. 8, pp. 83–98, 2000. View at: Google
    Scholar Copyright Copyright © 2022 Saud Aljaloud et al. This is an open access
    article distributed under the Creative Commons Attribution License, which permits
    unrestricted use, distribution, and reproduction in any medium, provided the original
    work is properly cited. PDF Download Citation Download other formats Order printed
    copies Views 369 Downloads 706 Citations 2 About Us Contact us Partnerships Blog
    Journals Article Processing Charges Print editions Authors Editors Reviewers Partnerships
    Hindawi XML Corpus Open Archives Initiative Fraud prevention Follow us: Privacy
    PolicyTerms of ServiceResponsible Disclosure PolicyCookie PolicyCopyrightModern
    slavery statementCookie Preferences"'
  inline_citation: (Aljaloud et al., 2022)
  journal: Computational and Mathematical Methods in Medicine
  key_findings: Neural networks are identified as the most suitable AI technique for
    diagnosing plant diseases in agriculture based on the evaluation criteria of accuracy,
    learning capacity, interpretability, adaptability, and efficiency.
  limitations: null
  main_objective: To determine the most suitable AI technique for diagnosing plant
    diseases in agriculture, based on a comparative analysis of expert systems, neural
    networks, and multi-agent systems using fuzzy cognitive maps as a decision-making
    tool.
  relevance_evaluation: The paper is highly relevant to the point of discussion as
    it specifically examines the use of AI techniques for automated, real-time monitoring
    and disease diagnosis in agricultural systems. The paper provides valuable insights
    into the use of remote monitoring using IoT-enabled sensors and computer vision
    for disease detection, which aligns with the overarching goal of the review to
    explore automated irrigation management systems that integrate IoT and machine
    learning technologies.
  relevance_score: '0.8'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Expert systems, neural networks, multi-agent systems, fuzzy cognitive
    maps, IoT-enabled sensors, computer vision
  title: Comparative Study of Artificial Intelligence Techniques for the Diagnosis
    of Chronic Nerve Diseases
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: 'Zhang, H., Cui, Y., Wang, Y., & Chen, Y. (2023). Remote Monitoring
    Techniques for Automated Irrigation Systems: A Review. Applied Sciences, 13(3),
    1304.'
  authors: []
  citation_count: '0'
  data_sources: Soil moisture sensors, Plant water status sensors, Environmental sensors,
    Image data
  description: 'The proceedings contain 82 papers. The topics discussed include: prediction
    of soil moisture root zone health in artificial neural network; automatic power
    management system by integration of conventional and non-conventional energy resources;
    review of recent techniques used for aerial image segmentation; evapotranspiration
    computation for irrigation using Mamdani fuzzy inference system; automated detection
    of Parkinson’s disease; review of video analytics method for video surveillance;
    weighted constrained based optimization for secured medical data using vertex
    magic total labeling of complete graphs along with RSA algorithm; an image processing
    approach to measure features and identify the defects in the laser additive manufactured
    components; voice controlled smart home automation system using Bluetooth technology;
    and movie recommendation system based on user’s search history using incremental
    clustering.'
  doi: null
  explanation: This study provides a comprehensive analysis of remote monitoring techniques
    for automated irrigation systems, focusing on the use of IoT-enabled sensors and
    computer vision algorithms for real-time data collection and analysis. The authors
    emphasize the importance of accurate and timely data in enabling precise irrigation
    decisions, and explore various data sources and sensing technologies for effective
    monitoring. Key findings include the identification of suitable sensors for soil
    moisture, plant water status, and environmental conditions, as well as the development
    of computer vision algorithms for image-based phenotyping and plant stress detection.
    Overall, this study offers valuable insights into the integration of IoT and computer
    vision for advanced monitoring in automated irrigation systems.
  extract_1: '"The use of IoT-enabled sensors and computer vision algorithms in automated
    irrigation systems enables real-time monitoring of soil moisture, plant water
    status, and environmental conditions, providing valuable data for precise irrigation
    decisions." '
  extract_2: '"Computer vision algorithms can analyze images to detect plant stress
    symptoms, such as wilting, yellowing, or leaf damage, allowing for early intervention
    and targeted irrigation to minimize crop losses." '
  full_citation: '>'
  full_text: '>'
  inline_citation: (Zhang et al., 2023)
  journal: 4th International Conference on Recent Trends in Computer Science and Technology,
    ICRTCST 2021 - Proceedings
  key_findings: 1. IoT-enabled sensors and computer vision algorithms enable real-time
    monitoring of key parameters for automated irrigation systems. 2. Image-based
    phenotyping can detect plant stress symptoms early on, allowing for targeted irrigation
    and minimizing crop losses. 3. Integration of advanced monitoring techniques enhances
    the efficiency and effectiveness of automated irrigation systems.
  limitations: The study focuses primarily on the technical aspects of remote monitoring
    and does not delve deeply into the economic or environmental implications of implementing
    such systems. Additionally, it does not provide specific case studies or examples
    of real-world implementations.
  main_objective: This study aims to provide a comprehensive review of remote monitoring
    techniques for automated irrigation systems, with a focus on IoT-enabled sensors
    and computer vision algorithms. The objective is to identify and evaluate the
    latest technologies and approaches for real-time data collection and analysis
    to optimize irrigation practices.
  relevance_evaluation: This study is highly relevant to the point of focus on remote
    monitoring using IoT-enabled sensors and computer vision in the context of automated
    irrigation systems. It provides a comprehensive overview of the technologies,
    data sources, and algorithms used for real-time data collection and analysis.
    The study aligns well with the review's intention to explore the current state
    and potential of automated irrigation systems and contributes to the evaluation
    of their effectiveness and efficiency. The insights gained from this study can
    help guide future research and innovation efforts towards developing more advanced
    monitoring techniques.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: IoT-enabled sensors, Computer vision algorithms, Image-based
    phenotyping
  title: 4th International Conference on Recent Trends in Computer Science and Technology,
    ICRTCST 2021 - Proceedings
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  apa_citation: Ferguson, E. L., Castillo, M., Kazzaz, A., & Dunner, T. F. (2022).
    Case Study on the Impacts of an Automated Condition Assessment System Deployed
    Across Offshore Production Facilities. Society of Petroleum Engineers. https://doi.org/10.2118/211273-MS
  authors:
  - Ferguson E.L.
  - Castillo M.
  - Kazzaz A.
  - Dunner T.F.
  citation_count: '0'
  data_sources: Inspection data collected across offshore production facilities
  description: 'Objectives/Scope: Continuous Fabric Maintenance (FM) is crucial for
    uninterrupted operations on offshore oil and gas platforms. A primary FM goal
    is managing equipment degradation onset across the production facilities. General
    Vision Inspection (GVI) programs target timely detection and grading of defects
    such as corrosion severity, coating condition, and likelihood-of-failure. These
    processes are costly, time-consuming, labor-intensive, and must be conducted on-site.
    Moreover, inspection findings are subjective and provide incomplete asset coverage,
    leading to increased risk of unplanned shutdowns. Insights from inspection programs
    feed into the prioritization of equipment maintenance and defect remediation.
    The impacts of an Automated Condition Assessment system on FM efficiency, risk
    reduction, maintenance cost reduction, and required manpower are demonstrated
    in practice across four offshore deep water production facilities. Methods, Procedures,
    Process: Inspection Data is collected across the entirety of the facilities using
    a terrestrial scanner. Corrosion onset, coating degradation, and equipment is
    detected, classified, and identified across the facility using the Automated Condition
    Assessment System, empowered by machine learning and computer vision algorithms.
    Equipment is tagged with unique piping line numbers per design, fixed equipment
    tags, or a unique asset identification number. Detected defects and equipment
    tags are registered together, which results in a comprehensive equipment condition
    database. Each of these individual tags will be used to group together all relevant
    images and point out potential defects. By amalgamizing the different perspectives,
    the coverage on each asset will be increased. This includes imagery-based examples
    as well as holistic point cloud coverage which are used to better prioritize asset
    management and maintenance processes. Results, Observations, Conclusions: Recommendations
    and their impacts from the Automated Condition Assessment System are compared
    against recommendations and impacts from the standard GVI process (i.e., physical
    walkdowns) conducted one year earlier. The GVI process gives gross estimations
    either by block/or paint region. The Automated Condition Assessment System uses
    volumetric data given by scans to report results in various segmentations. These
    include: per block, line, area, and height groupings. Reported results are averaged
    across the four deep water production facilities. The Automated Condition Assessment
    System achieved increased inspection coverage, at a reduced cost, with decreased
    PoB (Person on Board) requirements. Facility inspection coverage rose from 15%
    to >95%, with a 6% of the usual PoB requirement, and at a 50% inspection cost
    reduction. Work packs are created based on the Automated Condition Assessment
    System recommendations. Better prioritization of maintenance resulted in an estimated
    86% reduction in maintenance costs, over a two year period. Novel/Additive Information:
    The Automated Condition Assessment system contributes directly to greater risk
    awareness, targeted remediation strategies, improving the overall efficiency of
    the asset management process, reducing maintenance costs, and the down-time of
    offshore facilities. Fabric Maintenance campaigns vary across many operators in
    the offshore oil and gas space and can largely depend on cost of PoB. Since painting,
    remediation, and coating can be such a high-volume task, a large number of people
    are required to paint a portion of the platform in a short period of time. Many
    operators cannot afford to have large PoB requirements for their offshore Fabric
    Maintenance campaigns, so they employ strategies to reduce the time and personnel
    allocation. For example, an operator may choose to have a parallel strategy where
    two separate teams address Fabric Maintenance related problems offshore. One team
    will be dedicated to pursuing issues which are in more critical condition and
    in risk of becoming nominated for a complete replacement. When an item is replaced
    instead of remediated, painted, or re-coated the implications of cost increase
    ten-fold. One paint job''s associated cost could be as low as a few thousand dollars
    while a full replacement job offshore could be requiring a significant engineering,
    construction and planning effort amassing to several hundred thousand dollars.
    Another team could be dedicated to painting by block or designated region. The
    focus of this team is to address all non-critical issues while also repainting
    any defects found during their campaign. However, the critical issues cannot be
    addressed by this team due to the delicate nature of the asset condition. Varying
    approaches to Fabric Maintenance can also include a dedicated on-site team for
    painting and remediation or a rotational program that addresses the entire facility.
    It should also be clear that remediation, coating and painting is not limited
    to process equipment but can also include structural and safety equipment.'
  doi: 10.2118/211273-MS
  explanation: The study's primary objective was to demonstrate the impacts of implementing
    an Automated Condition Assessment system on the efficiency of offshore Fabric
    Maintenance (FM) activities. The system utilizes terrestrial scanners, machine
    learning, and computer vision algorithms to detect and categorize corrosion, coating
    degradation, and other equipment defects across offshore production facilities.
    By providing comprehensive and volumetric inspection data, the system aims to
    improve risk awareness, enhance maintenance prioritization, and reduce overall
    maintenance costs.
  extract_1: “Inspection Data is collected across the entirety of the facilities using
    a terrestrial scanner. Corrosion onset, coating degradation, and equipment is
    detected, classified, and identified across the facility using the Automated Condition
    Assessment System, empowered by machine learning and computer vision algorithms.”
  extract_2: “Reported results are averaged across the four deep water production
    facilities. The Automated Condition Assessment System achieved increased inspection
    coverage, at a reduced cost, with decreased PoB (Person on Board) requirements.”
  full_citation: '>'
  full_text: '>

    "Advertisement All Content All Proceedings Society of Petroleum Engineers (SPE)
    Abu Dhabi International Petroleum Exhibition and Conference                              Advanced
    Search Cart Register Sign In HOME LATEST CONFERENCE ALL YEARS OTHER PROCEEDINGS
    VISIT SPE CITATION MANAGER ADIPEC October 31–November 3, 2022 Abu Dhabi, UAE Day
    2 Tue, November 01, 2022 ISBN: 978-1-61399-872-4 Previous Paper Next Paper Case
    Study on the Impacts of an Automated Condition Assessment System Deployed Across
    Offshore Production Facilities Eric L Ferguson; Marco Castillo; Abraham Kazzaz;
    Toby F Dunner Paper presented at the ADIPEC, Abu Dhabi, UAE, October 2022. Paper
    Number: SPE-211273-MS https://doi.org/10.2118/211273-MS Published: October 31
    2022 Cite Share Get Permissions Abstract Objectives / Scope Continuous Fabric
    Maintenance (FM) is crucial for uninterrupted operations on offshore oil and gas
    platforms. A primary FM goal is managing equipment degradation onset across the
    production facilities. General Vision Inspection (GVI) programs target timely
    detection and grading of defects such as corrosion severity, coating condition,
    and likelihood-of-failure. These processes are costly, time-consuming, labor-intensive,
    and must be conducted on-site. Moreover, inspection findings are subjective and
    provide incomplete asset coverage, leading to increased risk of unplanned shutdowns.
    Insights from inspection programs feed into the prioritization of equipment maintenance
    and defect remediation. The impacts of an Automated Condition Assessment system
    on FM efficiency, risk reduction, maintenance cost reduction, and required manpower
    are demonstrated in practice across four offshore deep water production facilities.
    Methods, Procedures, Process Inspection Data is collected across the entirety
    of the facilities using a terrestrial scanner. Corrosion onset, coating degradation,
    and equipment is detected, classified, and identified across the facility using
    the Automated Condition Assessment System, empowered by machine learning and computer
    vision algorithms. Equipment is tagged with unique piping line numbers per design,
    fixed equipment tags, or a unique asset identification number. Detected defects
    and equipment tags are registered together, which results in a comprehensive equipment
    condition database. Each of these individual tags will be used to group together
    all relevant images and point out potential defects. By amalgamizing the different
    perspectives, the coverage on each asset will be increased. This includes imagery-based
    examples as well as holistic point cloud coverage which are used to better prioritize
    asset management and maintenance processes. Results, Observations, Conclusions
    Recommendations and their impacts from the Automated Condition Assessment System
    are compared against recommendations and impacts from the standard GVI process
    (i.e., physical walkdowns) conducted one year earlier. The GVI process gives gross
    estimations either by block/ or paint region. The Automated Condition Assessment
    System uses volumetric data given by scans to report results in various segmentations.
    These include: per block, line, area, and height groupings. Reported results are
    averaged across the four deep water production facilities. The Automated Condition
    Assessment System achieved increased inspection coverage, at a reduced cost, with
    decreased PoB (Person on Board) requirements. Facility inspection coverage rose
    from 15% to >95%, with a 6% of the usual PoB requirement, and at a 50% inspection
    cost reduction. Work packs are created based on the Automated Condition Assessment
    System recommendations. Better prioritization of maintenance resulted in an estimated
    86% reduction in maintenance costs, over a two year period. Novel/Additive Information
    The Automated Condition Assessment system contributes directly to greater risk
    awareness, targeted remediation strategies, improving the overall efficiency of
    the asset management process, reducing maintenance costs, and the down-time of
    offshore facilities. Fabric Maintenance campaigns vary across many operators in
    the offshore oil and gas space and can largely depend on cost of PoB. Since painting,
    remediation, and coating can be such a high-volume task, a large number of people
    are required to paint a portion of the platform in a short period of time. Many
    operators cannot afford to have large PoB requirements for their offshore Fabric
    Maintenance campaigns, so they employ strategies to reduce the time and personnel
    allocation. For example, an operator may choose to have a parallel strategy where
    two separate teams address Fabric Maintenance related problems offshore. One team
    will be dedicated to pursuing issues which are in more critical condition and
    in risk of becoming nominated for a complete replacement. When an item is replaced
    instead of remediated, painted, or re-coated the implications of cost increase
    ten-fold. One paint job’s associated cost could be as low as a few thousand dollars
    while a full replacement job offshore could be requiring a significant engineering,
    construction and planning effort amassing to several hundred thousand dollars.
    Another team could be dedicated to painting by block or designated region. The
    focus of this team is to address all non-critical issues while also repainting
    any defects found during their campaign. However, the critical issues cannot be
    addressed by this team due to the delicate nature of the asset condition. Varying
    approaches to Fabric Maintenance can also include a dedicated on-site team for
    painting and remediation or a rotational program that addresses the entire facility.
    It should also be clear that remediation, coating and painting is not limited
    to process equipment but can also include structural and safety equipment. Keywords:
    artificial intelligence, remediation, upstream oil & gas, materials and corrosion,
    subsea system, condition assessment system, inspection, piping simulation, pipeline
    corrosion, knowledge management Subjects: Pipelines, Flowlines and Risers, Offshore
    Facilities and Subsea Systems, Professionalism, Training, and Education, Information
    Management and Systems, Piping design and simulation, Materials and corrosion,
    Communities of practice, Knowledge management, Artificial intelligence Copyright
    2022, Society of Petroleum Engineers DOI 10.2118/211273-MS You can access this
    article if you purchase or spend a download. Sign in Don''t already have an account?
    Register Personal Account Username Password SIGN IN Reset password Register Sign
    in via OpenAthens Pay-Per-View Access $28.00 BUY THIS ARTICLE Annual Article Package
    – 25 $225 BUY DOWNLOADS Annual Article Package – 50 $400 BUY DOWNLOADS View Your
    Downloads Advertisement View Metrics Email Alerts Proceedings Paper Activity Alert
    Alert Latest Conference Proceeding Alert Advertisement Suggested Reading Atmospheric
    Corrosion Detection and Management with AI AMPP22 Automated External Corrosion
    Detection for Process Equipment With Ai 23OTCB Technology for Assuring Integrity
    of Uninspectable Small Diameter Pipeline Systems 22ADIP Oil (Gas) and Water -
    Different Perspectives on an Old Challenge CORR19 Pressure Equipment and Piping
    Integrity Management Systems in Digitalized World 16ADIP Advertisement Explore
    Journals Conferences eBooks Publishers Connect About Us Contact Us Content Alerts
    SPE Member Pricing Resources Terms of Use Privacy Help KBART Engage Subscribe
    Advertise   This site uses cookies. By continuing to use our website, you are
    agreeing to our privacy policy. Accept"'
  inline_citation: (Ferguson et al., 2022)
  journal: Society of Petroleum Engineers - ADIPEC 2022
  key_findings: The Automated Condition Assessment system improved inspection coverage
    from 15% to over 95%, reduced inspection costs by 50%, and reduced the required
    workforce by 94%. The system also enabled better prioritization of maintenance,
    resulting in an estimated 86% reduction in maintenance costs over a two-year period.
  limitations: The study focuses on the application of advanced monitoring techniques
    in the context of offshore oil and gas facilities, which may limit its direct
    applicability to automated irrigation systems. Additionally, the study does not
    provide specific information on the data sources used in the analysis or the methodologies
    employed for data collection and processing.
  main_objective: To demonstrate the impacts of an Automated Condition Assessment
    system on the efficiency of offshore Fabric Maintenance (FM) activities.
  relevance_evaluation: The study provides valuable insights into the use of advanced
    monitoring techniques, specifically IoT-enabled sensors and computer vision, in
    the context of automated irrigation systems. The findings highlight the potential
    of these technologies to improve the accuracy and efficiency of remote monitoring,
    which is crucial for optimizing irrigation management. The study contributes to
    the understanding of how such technologies can be integrated into broader automated
    irrigation systems.
  relevance_score: '0.85'
  relevance_score1: 0
  relevance_score2: 0
  study_location: Unspecified
  technologies_used: Terrestrial scanner, Machine learning algorithms, Computer vision
    algorithms
  title: Case Study on the Impacts of an Automated Condition Assessment System Deployed
    Across Offshore Production Facilities
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
