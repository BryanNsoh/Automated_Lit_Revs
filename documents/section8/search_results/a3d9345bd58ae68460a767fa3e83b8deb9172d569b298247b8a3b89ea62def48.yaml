- analysis: '>'
  authors:
  - Szabo S.M.
  - Hawkins N.S.
  - Germeni E.
  citation_count: '0'
  description: Objectives Qualitative methods allow in-depth exploration of patient
    experiences and can provide context for healthcare decision making. Frameworks
    for patient-based evidence in health technology assessment (HTA) are expanding;
    yet, how extensively qualitative methods are currently used is unclear. This review
    characterized the extent and quality of qualitative data submitted to National
    Institute for Health and Care Excellence (NICE) and Canadian Agency for Drugs
    and Technologies in Health (CADTH) for HTA. Methods NICE and CADTH submissions
    from September 2019 to August 2021 were reviewed. Submission characteristics and
    features of patient-based evidence included within submissions were extracted.
    The quality of qualitative reporting was assessed using the CASP checklist. Results
    Patient-based evidence was included in 83/107 NICE and 119/124 CADTH submissions.
    A small proportion described qualitative data collection (NICE=14; CADTH=24) and
    analysis (NICE=6; CADTH=9) methods. One-to-one interviews were the most common
    data collection method, and thematic analysis was exclusively used. Thirty-three
    percent of NICE submissions scored >7 yes responses on CASP, versus 78 percent
    of CADTH submissions. Conclusions Although patient-based evidence was common in
    the submissions reviewed, only 14/107 NICE and 24/124 CADTH submissions involved
    formal qualitative data collection. Use of formal analysis methods was even rarer
    and reporting tended to be brief. At present, there is little guidance about qualitative
    evidence most likely to be informative and therefore to potentially impact decision
    making. Ensuring, however, that qualitative data are collected and analyzed in
    a systematic, rigorous way will maximize their usefulness and ensure that patient
    voices are clearly heard.
  doi: 10.1017/S0266462323002829
  full_citation: '>'
  full_text: '>

    "We use cookies to distinguish you from other users and to provide you with a
    better experience on our websites. Close this message to accept cookies or find
    out how to manage your cookie settings. Discover Content Products and Services
    Home Home Browse subjects Publications Open research Services About Cambridge
    Core Access provided by Register Log in Cart ( 0 ) Home >Journals >International
    Journal of Technology Assessment in Health Care >Volume 40 Issue 1 >The extent
    and quality of qualitative evidence included... English Français International
    Journal of Technology Assessment in Health Care Article contents Abstract Objectives
    Methods Results Conclusions Background Methods Results Discussion Conclusions
    Competing interest References The extent and quality of qualitative evidence included
    in health technology assessments: a review of submissions to NICE and CADTH Published
    online by Cambridge University Press:  21 December 2023 Shelagh M. Szabo [Opens
    in a new window] , Neil S. Hawkins  and Evi Germeni [Opens in a new window] Show
    author details Article Figures Metrics Save PDF Share Cite Rights & Permissions
    [Opens in a new window] Abstract Objectives Qualitative methods allow in-depth
    exploration of patient experiences and can provide context for healthcare decision
    making. Frameworks for patient-based evidence in health technology assessment
    (HTA) are expanding; yet, how extensively qualitative methods are currently used
    is unclear. This review characterized the extent and quality of qualitative data
    submitted to National Institute for Health and Care Excellence (NICE) and Canadian
    Agency for Drugs and Technologies in Health (CADTH) for HTA. Methods NICE and
    CADTH submissions from September 2019 to August 2021 were reviewed. Submission
    characteristics and features of patient-based evidence included within submissions
    were extracted. The quality of qualitative reporting was assessed using the CASP
    checklist. Results Patient-based evidence was included in 83/107 NICE and 119/124
    CADTH submissions. A small proportion described qualitative data collection (NICE=14;
    CADTH=24) and analysis (NICE=6; CADTH=9) methods. One-to-one interviews were the
    most common data collection method, and thematic analysis was exclusively used.
    Thirty-three percent of NICE submissions scored >7 yes responses on CASP, versus
    78 percent of CADTH submissions. Conclusions Although patient-based evidence was
    common in the submissions reviewed, only 14/107 NICE and 24/124 CADTH submissions
    involved formal qualitative data collection. Use of formal analysis methods was
    even rarer and reporting tended to be brief. At present, there is little guidance
    about qualitative evidence most likely to be informative and therefore to potentially
    impact decision making. Ensuring, however, that qualitative data are collected
    and analyzed in a systematic, rigorous way will maximize their usefulness and
    ensure that patient voices are clearly heard. Keywords qualitative research health
    technology assessment patient-based evidence decision making health policy Type
    Assessment Information International Journal of Technology Assessment in Health
    Care , Volume 40 , Issue 1 , 2024 , e6 DOI: https://doi.org/10.1017/S0266462323002829
    [Opens in a new window] Creative Commons This is an Open Access article, distributed
    under the terms of the Creative Commons Attribution-NonCommercial-NoDerivatives
    licence (http://creativecommons.org/licenses/by-nc-nd/4.0), which permits non-commercial
    re-use, distribution, and reproduction in any medium, provided that no alterations
    are made and the original article is properly cited. The written permission of
    Cambridge University Press must be obtained prior to any commercial use and/or
    adaptation of the article. Copyright © The Author(s), 2023. Published by Cambridge
    University Press Background In 2020, an international task force put forth a new
    consensus definition of health technology assessment (HTA) ( Reference O’Rourke,
    Oortwijn and Schuller 1). HTA was defined as a multidisciplinary process using
    explicit methods to determine the value of a health technology to inform healthcare
    decision making, with a goal of promoting an equitable, efficient, and high-quality
    health system ( Reference O’Rourke, Oortwijn and Schuller 1). Traditionally HTA
    has focused on the clinical and cost-effectiveness of the health technologies
    being evaluated ( Reference O’Rourke, Oortwijn and Schuller 1). However, more
    recently the use of patient-based evidence in HTA, defined as “knowledge that
    originates directly from patients about their experiences of health, quality of
    life, health care, health services, and health research” ( Reference Staniszewska
    and Soderholm Werko 2), has been gaining attention. There are many features of
    patient-based evidence that make it well-suited to contributing to HTA. These
    types of data can speak to issues like which aspects of treatment value are important
    to consider from the patient’s perspective and help ensure the relevance of the
    decision outcomes to those who will use the novel interventions under consideration.
    In addition, the inclusion of patient-based evidence reflects a commitment to
    partnership with an active role for patients within HTA ( Reference Staniszewska
    and Soderholm Werko 2). Patient-based evidence may be derived through quantitative
    assessments such as surveys, and also through qualitative research ( Reference
    Facey, Hansen and Single 3). Qualitative research describes a set of methodologies
    that aim to grasp phenomena in a holistic way, to understand the meaning behind
    these within their own context, and to generate theories to explain observed trends
    ( Reference Charmaz 4; Reference Strauss and Corbin 5). As such, these methods
    are well-suited to obtain rich, in-depth information about patients’ experiences,
    needs, preferences, and attitudes about their care and health ( Reference Coast
    6). Within HTA, they can provide important complementary evidence to the findings
    from quantitative studies that estimate clinical and cost-effectiveness or health-related
    quality-of-life (HRQoL) impact ( Reference Facey, Hansen and Single 3). They can
    also expand on ad-hoc and anecdotal evidence from individual stakeholders often
    collected as part of the process of patient input into HTA ( Reference Facey,
    Hansen and Single 3). In recognition of their potential value for informing healthcare
    decision making, researchers are noting opportunities for expanding the contribution
    of qualitative methods within health economics and HTA ( Reference Coast 6– Reference
    Booth 8). At the same time, frameworks to expand patient engagement within HTA
    – including from agencies such as the National Institute for Health and Care Excellence
    (NICE) and Canadian Agency for Drugs and Technologies in Health (CADTH) – are
    being developed ( Reference Facey, Hansen and Single 3;9;10). However, despite
    acknowledgment of the potential role for qualitative methods, how widely these
    methods are presently used to inform patient-based evidence within the HTA submission
    process is unclear. The objective of this review was to characterize the contemporary
    use of qualitative data provided as patient-based evidence within HTA submissions,
    including methods employed, quality of data generated, and the objectives and
    topics described. Methods This review included all submissions to the Technology
    Appraisals or Highly Specialized Technologies programs at NICE, and to the Common
    Drug Review (CDR) or Pan-Canadian Oncology Drug Review (PCODR) at CADTH, with
    recommendations issued between 1 October 2019 and 30 September 2021. At the time
    of the review, CADTH operated two pan-Canadian single-drug technology assessment
    processes, with pCODR specifically assessing oncology drugs and CDR assessing
    all other drugs (11;12). The NICE programs selected for inclusion represent the
    subset of NICE programs also focused on appraising medicine-based health technologies.
    The search was implemented within the NICE (https://www.nice.org.uk/guidance/published?ndt=Guidance&ndt=Quality%20standard)
    and CADTH (https://www.cadth.ca/reimbursement-review-reports) websites on 30 September
    2021. The two-year study period was selected to focus on the most contemporary
    evidence available at the time of initiating the review. From each identified
    submission, we extracted the following information: the name of the product, indication,
    target diseases and therapeutic area, and the outcome of the submission (i.e.,
    whether the product was recommended for reimbursement or rejected). We also extracted
    details of any patient-based research initiatives described within individual
    submissions, including who provided input (e.g., patients, caregivers), the topics
    covered and objectives of the research, methods used for data collection and analysis,
    and whether the submitted data had been published in a peer-reviewed journal.
    Given the wide variation in methods described for collecting patient-based evidence,
    a taxonomy was developed to help categorize approaches in a meaningful way. Submissions
    were therefore classified into mutually exclusive categories according to whether
    they 1) provided an explicit description of both qualitative data collection and
    analysis methods (either stand-alone or in the context of mixed-methods studies);
    2) included information only on qualitative data collection methods, without a
    description of how data were analyzed (either stand-alone or in the context of
    mixed-methods studies); 3) described input from a small number of patients (≤5
    participants) directly in an ad hoc fashion (either stand-alone or in the context
    of mixed-methods studies); 4) reported quantitative surveys that incorporated
    (qualitative) free-text comments; 5) reported other quantitative data collection
    methods (without accompanying qualitative methods or use of free-text comments);
    6) did not directly include patient data (either from quantitative studies, qualitative
    studies, or based on direct patient feedback); or 7) provided insufficient information
    to determine the methods used. Drawing on Germeni and Szabo’s proposed framework
    for qualitative research in HTA ( Reference Germeni and Szabo 13), we subsequently
    classified submissions reporting qualitative data collection methods (categories
    1 and 2 above) in terms of the purpose that the qualitative research was aiming
    to serve. Specifically, these purposes included: (i) assessing acceptability and
    subjective value; (ii) understanding perspectives and providing context; (iii)
    laying the groundwork for subsequent quantitative exercises; (iv) contributing
    to economic model development; and (v) reaching groups other methods cannot reach
    (Figure 1). For subjective value, we considered whether perceptions of safety
    and effectiveness were comparative or provided for one treatment in isolation.
    Figure 1. Germeni and Szabo’s proposed framework for qualitative research in HTA
    ( Reference Germeni and Szabo 13). The CASP checklist for qualitative research
    (14) was used to appraise the quality of methods and validity of findings, from
    submissions providing an explicit description of both qualitative data collection
    and qualitative data analysis methods when they generated patient-based evidence
    (category 1). The CASP checklist systematically rates whether specific elements
    in qualitative studies are present (“Yes”), absent (“No”), or unclear (“Can’t
    tell”), using responses to ten questions that address issues around the validity
    of results, actual results reported, and helpfulness of results locally. Based
    on ratings assigned to each submission, the mean number of “yes” responses per
    submission was tabulated, as was the proportion of submissions with more than
    7 “yes” responses. Targeted literature searches of EMBASE, MEDLINE, and Google
    Scholar were conducted to ascertain whether the data resulting from initiatives
    using well-established qualitative methods were published in the peer-reviewed
    literature. Searches were conducted using keywords from the research topic (e.g.,
    “asthma” and “burden”) and methods (e.g., “qualitative” or “interview”), as well
    as any details provided regarding the study team who conducted the research. If
    applicable, grey literature-based report titles were also used to guide these
    searches. Results We identified a total of 231 submissions for which the submission
    processes were complete and recommendations had been issued between 1 October
    2019 and 30 September 2021: 107 (46.3 percent) from NICE and 124 (53.7 percent)
    from CADTH. Characteristics of submissions The most frequent therapeutic areas
    covered in NICE submissions were oncology (n = 56; 52.3 percent), rare diseases
    (n = 42, 39.2 percent), rheumatology (n = 7; 6.5 percent), and metabolic diseases
    (n=7; 6.5 percent). The most frequent therapeutic areas covered in CADTH submissions
    were oncology (n = 58; 46.8 percent), rare diseases (n = 50, 40.3 percent), and
    neurology (n = 9; 7.3 percent). For 8 (7.5 percent) NICE submissions and 21 (16.9
    percent) CADTH submissions, the outcome of the review process was a decision to
    not recommend reimbursement. The most frequent reason for a lack of recommendation
    for reimbursement was that the economic analyses did not demonstrate cost-effectiveness.
    Use of qualitative methods in submissions The methods used to collect patient-based
    evidence in the reviewed HTA submissions are presented in Figure 2. Of the 107
    NICE and 124 CADTH submissions, 83 (NICE) and 119 (CADTH) included patient-based
    evidence in some form (derived from quantitative studies, qualitative studies,
    or through direct patient feedback). However, only 14 NICE and 24 CADTH submissions
    provided an explicit description of systematic qualitative data collection initiatives.
    One-to-one interviews were the most common method used (reported in 8 NICE and
    22 CADTH submissions), followed by focus groups (reported in 6 NICE and 11 CADTH
    submissions). The use of qualitative data analysis methods was reported even less
    frequently than the use of data collection methods, observed in 6 NICE and 9 CADTH
    submissions. All submissions that included details of qualitative data analysis,
    used thematic analysis. A large number of submissions (31 NICE and 52 CADTH) presented
    data collected exclusively by survey-based methods involving a combination of
    closed- and open-ended questions with free text fields. Only one NICE and two
    CADTH submissions reported exclusive use of qualitative data collection methods
    (i.e., surveys with free-text fields were not employed). Figure 2. Categorization
    of HTA submissions based on methods used to collect patient-based evidence*. Objectives
    of qualitative research initiatives As shown in Table 1, in terms of the objectives
    of each exercise, the most frequent focus was to understand perspectives and provide
    context (observed in 100 percent of both CADTH and NICE submissions involving
    qualitative data collection or analysis). The Asthma Canada advocacy group, for
    instance, used a mixed-methods approach involving qualitative interviews and a
    quantitative survey to document the experiences of Canadians living with severe
    asthma, and the attendant social, financial, and emotional implications of the
    diagnosis (15). The next most common objective was to understand acceptability
    and subjective value (in 83.3 percent of CADTH and 64.7 percent of NICE submissions
    involving qualitative methods). An example addressing both of these objectives
    is provided by a qualitative study involving one-on-one interviews with Canadians
    with hemophilia A and their caregivers that aimed to understand the lived experience
    but also to solicit feedback on patient perceptions of the acceptability and merit
    of current and future treatments (16). In submissions with qualitative initiatives
    that described treatment acceptability and subjective value, while patients frequently
    described perceptions of the clinical effectiveness or HRQoL implications of treatment,
    comments on comparative effectiveness, safety, or HRQoL impact were rare. Table
    1. Objectives of the qualitative research initiatives a included as part of CADTH
    and NICE submissions a Corresponding to categories 1 and 2, in Figure 1. CADTH,
    Canadian Agency for Drugs and Technologies in Health; NICE, National Institute
    for Health and Care Excellence. Other broad objectives (i.e., laying the groundwork
    for quantitative exercises, contributing to economic models or reaching groups
    other methods cannot reach) were infrequent focuses of the qualitative research
    initiatives reviewed. An example of a qualitative initiative contributing to an
    economic model comes from the submission for Luxturna (voretigene neparvovec;
    Novartis) where focus groups were used to understand the costs of blindness, including
    indirect costs, from the patient perspective; and key insights on the types of
    costs to consider within the economic analyses were taken from the findings of
    those focus groups (17). The percentage of submissions focusing on rare diseases
    was similar or slightly higher among the subset of appraisals involving qualitative
    data collection or analysis methods (50.0 percent for NICE and 41.7 percent for
    CADTH) compared with the overall set of appraisals reviewed (39.2 percent for
    NICE and 39.5 percent for CADTH) only one appraisal specifically called out disease
    rarity to support their use of qualitative methods. That appraisal that included
    a Canadian interview-based study in viral keratoconjunctivitis reported that qualitative
    methods were used because of the small sub-population indicated and to understand
    heterogeneous patient experiences (18). Quality appraisal and publication of qualitative
    research The quality of methods and validity of findings of the initiatives from
    the 6 NICE and 9 CADTH submissions describing both qualitative data collection
    and analysis methods were evaluated using the CASP checklist. On average, methodologic
    descriptions were more comprehensive in CADTH submissions compared with those
    submitted to NICE; nonetheless, most descriptions of the methodology and results
    provided within the submissions lacked adequate reporting of one or more key study
    design elements (Figure 3). The mean number of yesses on the CASP checklist for
    the 6 NICE submissions using qualitative analysis was 7.0. and 33 percent of submissions
    had >7 yes responses. The mean number of yesses on the CASP checklist for the
    9 CADTH submissions using qualitative analysis was 7.8, and 78 percent of submissions
    had >7 yes responses. Aspects of the CASP checklist where submissions were most
    often deficient (i.e., rated “no” or “unclear”) included adequately considering
    the relationship between researcher and participants, considering ethical issues,
    and providing sufficiently rigorous data analysis. Figure 3. Percentage of NICE
    and CADTH submissions meeting CASP checklist quality assessment criteria. Publication
    of qualitative studies was infrequent. Two manuscripts and one poster were identified
    of studies related to CADTH submissions, and one manuscript and two posters were
    identified related to NICE submissions. Discussion This review sought to summarize
    the contemporary use of qualitative patient-based evidence contributing as supportive
    evidence within the HTA process. Within the submissions reviewed, the use of systematic
    qualitative data collection methods was infrequent, and the use of systematic
    methods for both collection and analysis of qualitative data was even rarer. Furthermore,
    the description of the studies to generate qualitative patient-based evidence
    presented within the submissions was often inadequate. Many of the qualitative
    data collection exercises lacked explicit descriptions of the methods informing
    data analysis, or when included, the terms used to describe analysis methods were
    often used imprecisely ( Reference Kiger and Varpio 19). Thematic analysis was
    exclusively used, and no other approaches (such as framework analysis or interpretative
    phenomenological analysis) were identified. Expanding the types of qualitative
    approaches used may serve to broaden the potential applicability of these methods
    within HTA. This review also documents the purposes for which qualitative methods
    are presently being used in providing patient-based evidence in this aspect of
    the HTA process. Given the wide acknowledgment of the usefulness of qualitative
    methods for understanding the meaning individuals attach to experiences ( Reference
    Charmaz 4; Reference Strauss and Corbin 5), it is not surprising that these methods
    were widely applied to understand perspectives and provide context from patients
    or caregivers. Gathering in-depth data on acceptability and potential uptake challenges
    from treatment-experienced individuals was also a common objective. However, while
    the patient-perceived clinical or HRQoL treatment impact was frequently discussed,
    understanding how a patient considers the new versus an existing treatment (i.e.,
    patient-based evidence of comparative effectiveness) was not regularly described.
    Only a small number of initiatives were identified that laid the groundwork for
    quantitative exercises or informed the development of economic models (17;20),
    despite these being commonly cited health economic applications of qualitative
    research ( Reference Coast 6). Only one study particularly called out the value
    of qualitative methods for reaching populations that other methods cannot (18).
    This is in spite of qualitative methods being well-suited to rare disease research,
    where small patient populations can make the use of quantitative methods more
    challenging ( Reference Germeni, Vallini, Bianchetti and Schulz 21). It is particularly
    surprising given that approximately 40 percent of submissions included in this
    review were for rare disease products. At present, how important decision makers
    find patient-based research initiatives focusing on these objectives is unclear.
    Formalized guidance that points researchers and patient groups to the topics most
    useful to investigate using qualitative methods would be helpful for ensuring
    patient-generated evidence is useful to complement the ongoing quantitative research
    for HTA. While HTA stakeholder feedback is being sought to help define, for example,
    how quantitative preference studies can inform HTA ( Reference van Overbeeke,
    Forrester, Simoens and Huys 22), similar guidance for qualitative data is lacking.
    The qualitative initiatives submitted as supportive evidence for HTA often scored
    low on the CASP checklist. Aspects that were particularly problematic included
    consideration of the relationship between researcher and participants, discussion
    of ethical issues, and methods for qualitative analysis. To understand how these
    limitations to reporting identified here compared to qualitative work done in
    other contexts, we juxtaposed them with some recently published systematic reviews
    of qualitative evidence ( Reference Butterworth, Wood and Rowe 23– Reference Schober
    and Abrahamsen 25). In general, summary estimates of CASP scores presented within
    published qualitative evidence syntheses tended to be higher than those calculated
    for the initiatives within submissions to HTA. Most qualitative evidence in the
    published reviews scored highly for discussion of ethical issues. However, consistent
    with our research, two areas where published qualitative studies also tended to
    be rated as deficient were in providing adequate descriptions of methods of analysis,
    and consideration of the relationship between the researcher and participant.
    It is worth noting that the methods of the studies used to generate patient-based
    evidence for HTA tended to be described in a more expanded fashion in the few
    identified publications ( Reference Wiley, Khoury and Snihur 26) versus what was
    presented in the submissions themselves. This potentially suggests that these
    methodological issues may in fact have been considered by the research team but
    were not fully documented within the HTA submission. It also highlights an opportunity
    for patient groups and manufacturers to clarify the reporting of their qualitative
    work through reference to the CASP or other available checklists (14), prior to
    submitting these data for review. An additional method by which the perceived
    credibility of patient-based research – both quantitative and qualitative – can
    be assessed is through peer-reviewed publication ( Reference Facey, Hansen and
    Single 3). However, publication of the reviewed research submitted to NICE and
    CADTH was uncommon. We recognize that publication of research findings is a time-consuming
    process, and the timelines for this may not align with those for data submission
    to healthcare decision makers. It is possible that publication of some of these
    research projects is pending, or that the findings of the research were published
    but not identified by the study team in our targeted search. However, it is also
    important to acknowledge the documented potential barriers to publishing qualitative
    findings, including the systematic favoring of more “striking” research findings
    (e.g., time-lag bias), or reviewers’ reported lack of confidence in the quality
    or description of qualitative methods ( Reference Toews, Booth and Berg 27; Reference
    Toews, Glenton and Lewin 28). Despite these challenges, the rigors of having undergone
    the peer review process, in combination with the use of qualitative research reporting
    checklists or design-specific reporting frameworks, could help refine methodologic
    descriptions that would also enhance the quality of qualitative research submitted
    to HTA. Strengths of the review include the use of a well-accepted and commonly-used
    checklist to assess the quality of the reporting of included studies, and the
    comprehensive approach to reviewing and characterizing a large sample of submissions
    to two leading HTA agencies with a strong focus on patient engagement. In addition
    to the logistic convenience of their reviews being published in English, we selected
    NICE and CADTH because both HTAs focus on patient input into aspects of their
    processes. However, given that these two agencies are very explicit in how they
    assess the economic value of interventions, it is not surprising that there is
    still a large focus on the quantitative aspects required for informing cost-effectiveness
    analyses. How these findings reflect the situation with HTA agencies with less
    of a focus on economic aspects (like the Institute for Quality and Efficiency
    in Healthcare (IQWIG) in Germany, for example) is unknown. Similarly, because
    processes for patient input and engagement are HTA agency-specific, how the findings
    of this review would extend to HTA agencies outside of Canada, England, and Wales
    is unclear. Other limitations include that the syntheses presented in this review
    are based on data presented within the HTA submission. This may not adequately
    reflect the level of rigor within the studies themselves if described in a different
    context (e.g., within manuscripts that had undergone the peer review process).
    The ratings of the research initiatives informing the HTA submissions were assigned
    by a single reviewer and these aspects may be interpreted differently by other
    reviewers. There was variability in the level of description and data provided
    across submissions; and details provided must be considered in the context of
    differing requirements for CADTH and NICE submissions with respect to patient
    evidence. Finally, it is important to acknowledge the potential impact of COVID-19
    on the findings of this review, as part of the review period (but not necessarily
    individual study data collection periods) coincided with the timing of the pandemic.
    While the pandemic certainly created challenges to research in general, it also
    offered some advantages to qualitative research methods (use of teleconferencing
    tools, easier and more widespread participant engagement ( Reference Cornejo,
    Bustamante, Del Rio, De Toro and Latorre 29)). These changes in methodology are
    persisting even as the pandemic abates. An interesting future direction would
    be to conduct a follow-up review to assess how the extent and quality of qualitative
    research contributing to HTA changes over the coming years. Conclusions Both quantitative
    and qualitative methods may be used to provide patient-based evidence for HTA
    ( Reference Facey, Hansen and Single 3), and the choice of approach and method
    should be dictated by the research question and needs of the stakeholders, rather
    than philosophical or ideological grounds ( Reference Murphy, Dingwall, Greatbatch,
    Parker and Watson 7). In addition to the need for quantitative evidence clearly
    outlined (30), NICE identifies and acknowledges the importance of qualitative
    evidence for informing HTA ( Reference Booth 8). Similarly, CADTH is a leader
    with respect to the level of patient engagement and input into various parts of
    their HTA processes, and they provide evidence on their website of situations
    where patient evidence helped shape HTA decisions (9). However, at present, there
    is little direction from decision makers about the types of qualitative evidence
    most useful to submit, or how these data should be synthesized and communicated
    ( Reference Booth 8;9). In the 107 NICE and 124 CADTH HTA submissions reviewed
    within the present study, although inclusion of patient-based evidence was common
    (occurring in 83 NICE and 119 CADTH submissions), use of formal qualitative methods
    of collection was infrequent, described in 14 NICE and 24 CADTH submissions. Use
    of formal methods of analysis was even rarer, occurring in only 6 NICE and 9 CADTH
    submissions. When these were included, reporting tended to be brief and/or inconsistent.
    While interest in providing patient-based evidence for HTA is increasing, we feel
    that the focus should be on collecting and analyzing these data in a systematic
    and rigorous way to help ensure their usefulness and credibility. As well, promoting
    methodologically-sound qualitative research should be done in the same way as
    quantitative research. Certainly, publication of findings – which is viewed as
    a key component in generating patient-derived evidence ( Reference Facey, Hansen
    and Single 3) – will be important to furthering this goal; and also the use of
    well-accepted and commonly-used checklists to aid in study design, transparency,
    and reporting. Additionally, we would suggest focusing the research on topics
    that quantitative methods are less well-suited to address. The in-depth insight
    that qualitative methods can provide can be used to help illuminate how patients’
    experiences, attitudes, and preferences ( Reference Coast 6) will affect their
    adoption of the health technologies under consideration. In our view, such topics
    could include using qualitative methods to inform the development of quantitative
    patient preference studies, and understanding comparative safety and efficacy
    from the perspective of the patients who have experience with the health technology.
    In addition, qualitative methods can aid in demonstrating the patient relevance
    of specific parameter inputs considered within an economic model for that new
    technology. We would also encourage researchers to make use of the breadth of
    available qualitative methods – considering also ethnography, or grounded theory
    for example – to extend on research conducted using thematic analysis. At the
    same time, a better understanding of the influence of qualitative research on
    decision making should be investigated, potentially through observations of deliberative
    processes. To complement investigations of the deliberative process, research
    to understand HTA stakeholder preferences around qualitative data, and where they
    see key opportunities would be helpful; as has been previously performed for quantitative
    preference studies for example ( Reference van Overbeeke, Forrester, Simoens and
    Huys 22; Reference Huls, Whichello, van Exel, Uyl-de Groot and de Bekker-Grob
    31), would be helpful. Going forward, guidance from decision makers around key
    topics or areas for investigation may be helpful for ensuring that the patient
    voice is clearly heard, while the results of well-conducted qualitative studies
    fill specific gaps in knowledge to inform HTA. Competing interest The authors
    declare none. References 1 O’Rourke, B, Oortwijn, W, Schuller, T, International
    Joint Task Group. The new definition of health technology assessment: A milestone
    in international collaboration. Int J Technol Assess Health Care. 2020;36:187–190.CrossRefGoogle
    ScholarPubMed 2 Staniszewska, S, Soderholm Werko, S. Mind the evidence gap: The
    use of patient-based evidence to create “complete HTA” in the twenty-first century.
    Int J Technol Assess Health Care. 2021;37:e46.CrossRefGoogle ScholarPubMed 3 Facey,
    KM, Hansen, HP, Single, ANV. Patient involvement in health technology assessment.
    Singapore: Springer; 2017.CrossRefGoogle Scholar 4 Charmaz, K. Constructing grounded
    theory. London: Sage; 2014.Google Scholar 5 Strauss, AL, Corbin, J. Basics of
    qualitative research. Grounded theory procedures and techniques. London: Sage;
    1990.Google Scholar 6 Coast, J. Qualitative methods for health economics. London,
    UK: Rowman & Littlefield International; 2017.Google Scholar 7 Murphy, E, Dingwall,
    R, Greatbatch, D, Parker, S, Watson, P. Qualitative research methods in health
    technology assessment: A review of the literature. Health Technol Assess. 1998;2:1–274.CrossRefGoogle
    ScholarPubMed 8 Booth, A. A methodological update on the use of qualitative evidence
    in health technology assessment: Report by the decision support unit. Sheffield,
    UK: School of Health and Related Research, University of Sheffield; 2020.Google
    Scholar 9 CADTH. CADTH framework for patient engagement in health technology assessment.
    2022. Available from: https://www.cadth.ca/cadth-framework-patient-engagement-health-technology-assessment.Google
    Scholar 10 NICE. Public involvement at NICE. 2022. Available from: https://www.nice.org.uk/about/nice-communities/nice-and-the-public/public-involvement.Google
    Scholar 11 Canadian Agency for Drugs and Therapeutics in Health. Procedures for
    the CADTH pan-Canadian Oncology Drug Review. 2020.Google Scholar 12 CAnadian Agency
    for Drugs and Therapeutics in Health. Procedures for CADTH reimbursement reviews.
    2023.Google Scholar 13 Germeni, E, Szabo, S. Beyond clinical and cost-effectiveness:
    The contribution of qualitative research to health technology assessment. Int
    J Technol Assess Health Care. 2023;39:e23.CrossRefGoogle ScholarPubMed 14 Critical
    Appraisal Skills Program (CASP). CASP Checklist: 10 questions to help you make
    sense of a qualitative research. 2018. Available from: https://casp-uk.net/images/checklist/documents/CASP-Qualitative-Studies-Checklist/CASP-Qualitative-Checklist-2018_fillable_form.pdf.Google
    Scholar 15 Canadian Agency for Drugs and Therapeutics in Health. CADTH Common
    Drug Review Patient Input: Indacaterol acetate/mometasone furoate (Atectura Breezhaler).
    2020. Available from: https://www.cadth.ca/sites/default/files/cdr/relatedinfo/SR0646%20Atectura%20Breezhaler%20-%20Patient%20Group%20Input_for%20posting.pdf.Google
    Scholar 16 Canadian Agency for Drugs and Therapeutics in Health. CADTH Common
    Drug Review Patient Input: Emicizumab (Hemlibra). 2020. Available from: https://www.cadth.ca/sites/default/files/cdr/relatedinfo/Hemlibra%20-%20Patient%20Group%20Input_For%20Posting.pdf
    (accessed 15-Dec-22).Google Scholar 17 National Institute for Health and Care
    Excellence. Highly Specialised Technology: Voretigene neparvovec for treating
    inherited retinal dystrophies caused by RPE65 gene mutations [ID1054]. 2019. Available
    from: https://www.nice.org.uk/guidance/hst11/evidence/committee-papers-pdf-6908685661.Google
    Scholar 18 Canadian Agency for Drugs and Therapeutics in Health. CADTH Common
    Drug Review Patient Input: Cyclosporine (Verkazia). 2019. Available from: https://www.cadth.ca/sites/default/files/cdr/relatedinfo/SR0615_Verkazia_PI%20Submission1.pdf.Google
    Scholar 19 Kiger, ME, Varpio, L. Thematic analysis of qualitative data: AMEE Guide
    No. 131. Med Teach. 2020;42:846–854.CrossRefGoogle ScholarPubMed 20 Canadian Agency
    for Drugs and Therapeutics in Health. CADTH Common Drug Review Patient Input:
    Luspatercept (Reblozyl). 2020. Available from: https://www.cadth.ca/sites/default/files/cdr/relatedinfo/Reblozyl%20-%20Patient%20Group%20Input%20for%20Posting.pdf.Google
    Scholar 21 Germeni, E, Vallini, I, Bianchetti, MG, Schulz, PJ. Reconstructing
    normality following the diagnosis of a childhood chronic disease: Does “rare”
    make a difference? Eur J Pediatr. 2018;177:489–495.CrossRefGoogle Scholar 22 van
    Overbeeke, E, Forrester, V, Simoens, S, Huys, I. Use of patient preferences in
    health technology assessment: perspectives of Canadian, Belgian and German HTA
    representatives. Patient. 2021;14:119–128.CrossRefGoogle ScholarPubMed 23 Butterworth,
    H, Wood, L, Rowe, S. Patients’ and staff members’ experiences of restrictive practices
    in acute mental health in-patient settings: Systematic review and thematic synthesis.
    BJPsych Open. 2022;8:e178.CrossRefGoogle ScholarPubMed 24 Goddard, BMM, Hutton,
    A, Guilhermino, M, McDonald, VM. Parents’ decision making during their child’s
    asthma attack: Qualitative systematic review. J Asthma Allergy. 2022;15:1021–1033.CrossRefGoogle
    ScholarPubMed 25 Schober, TL, Abrahamsen, C. Patient perspectives on major lower
    limb amputation – A qualitative systematic review. Int J Orthop Trauma Nurs. 2022;46:100958.CrossRefGoogle
    ScholarPubMed 26 Wiley, RE, Khoury, CP, Snihur, AWK, et al. From the voices of
    people with haemophilia A and their caregivers: Challenges with current treatment,
    their impact on quality of life and desired improvements in future therapies.
    Haemophilia. 2019;25:433–440.CrossRefGoogle ScholarPubMed 27 Toews, I, Booth,
    A, Berg, RC, et al. Further exploration of dissemination bias in qualitative research
    required to facilitate assessment within qualitative evidence syntheses. J Clin
    Epidemiol. 2017;88:133–139.CrossRefGoogle ScholarPubMed 28 Toews, I, Glenton,
    C, Lewin, S, et al. Extent, awareness and perception of dissemination bias in
    qualitative research: An explorative survey. PLoS One. 2016;11:e0159290.CrossRefGoogle
    ScholarPubMed 29 Cornejo, M, Bustamante, J, Del Rio, M, De Toro, X, Latorre, MS.
    Researching with qualitative methodologies in the time of coronavirus: Clues and
    challenges. Int J Qual Methods. 2023;22:16094069221150110.CrossRefGoogle ScholarPubMed
    30 National Institute for Health and Care Excellence. Published Guidance, NICE
    advice and quality standards. 2023. Available from: https://www.nice.org.uk/guidance/published?ngt=Technology%20appraisal%20guidance&ndt=Guidance.Google
    Scholar 31 Huls, SPI, Whichello, CL, van Exel, J, Uyl-de Groot, CA, de Bekker-Grob,
    EW. What is next for patient preferences in health technology assessment? A systematic
    review of the challenges. Value Health. 2019;22:1318–1328.CrossRefGoogle ScholarPubMed
    You have Access Open access Related content AI-generated results: by UNSILO [Opens
    in a new window] HEALTH TECHNOLOGY ASSESSMENT OF MEDICAL DEVICES: A SURVEY OF
    NON-EUROPEAN UNION AGENCIES Type Article Title HEALTH TECHNOLOGY ASSESSMENT OF
    MEDICAL DEVICES: A SURVEY OF NON-EUROPEAN UNION AGENCIES Authors Oriana Ciani
    , Britni Wilcher , Carl Rudolf Blankart , Maximilian Hatz , Valentina Prevolnik
    Rupel , Renata Slabe Erker , Yauheniya Varabyova  and Rod S. Taylor   Journal
    International Journal of Technology Assessment in Health Care Published online:
    5 June 2015 EVALUATION OF PATIENT AND PUBLIC INVOLVEMENT INITIATIVES IN HEALTH
    TECHNOLOGY ASSESSMENT: A SURVEY OF INTERNATIONAL AGENCIES Type Article Title EVALUATION
    OF PATIENT AND PUBLIC INVOLVEMENT INITIATIVES IN HEALTH TECHNOLOGY ASSESSMENT:
    A SURVEY OF INTERNATIONAL AGENCIES Authors Laura Weeks , Julie Polisena , Anna
    Mae Scott , Anke-Peggy Holtorf , Sophie Staniszewska  and Karen Facey   Journal
    International Journal of Technology Assessment in Health Care Published online:
    10 November 2017 Introducing patients'' and the public''s perspectives to health
    technology assessment: A systematic review of international experiences Type Article
    Title Introducing patients'' and the public''s perspectives to health technology
    assessment: A systematic review of international experiences Authors Marie-Pierre
    Gagnon , Marie Desmartis , Dolorès Lepage-Savary , Johanne Gagnon , Michèle St-Pierre
    , Marc Rhainds , Renald Lemieux , Francois-Pierre Gauvin , Hugo Pollender  and
    France Légaré   Journal International Journal of Technology Assessment in Health
    Care Published online: 25 January 2011 USING EXPERT OPINION IN HEALTH TECHNOLOGY
    ASSESSMENT: A GUIDELINE REVIEW Type Article Title USING EXPERT OPINION IN HEALTH
    TECHNOLOGY ASSESSMENT: A GUIDELINE REVIEW Authors Theresa Hunger , Petra Schnell-Inderst
    , Narine Sahakyan  and Uwe Siebert   Journal International Journal of Technology
    Assessment in Health Care Published online: 9 August 2016 Key principles for the
    improved conduct of health technology assessments for resource allocation decisions
    Type Article Title Key principles for the improved conduct of health technology
    assessments for resource allocation decisions Authors Michael F. Drummond , J.
    Sanford Schwartz , Bengt Jönsson , Bryan R. Luce , Peter J. Neumann , Uwe Siebert  and
    Sean D. Sullivan   Journal International Journal of Technology Assessment in Health
    Care Published online: 4 July 2008 Stories of Patient Involvement Impact in Health
    Technology Assessments: A Discussion Paper Type Article Title Stories of Patient
    Involvement Impact in Health Technology Assessments: A Discussion Paper Authors
    Ann N.V. Single , Karen M. Facey , Heidi Livingstone  and Aline Silveira Silva   Journal
    International Journal of Technology Assessment in Health Care Published online:
    24 July 2019 SYSTEMS FOR ROUTINE INFORMATION SHARING IN HTA Type Article Title
    SYSTEMS FOR ROUTINE INFORMATION SHARING IN HTA Authors Kerstin Hagenfeldt , José
    Asua , Sergio Bellucci , Malene Fabricius Jensen , Berit Mørland , Wija Oortwijn
    , Rachid Salmi , Andrew Stevens  and Gabriël H. M. ten Velden   Journal International
    Journal of Technology Assessment in Health Care Published online: 21 May 2002
    CAPACITY BUILDING IN AGENCIES FOR EFFICIENT AND EFFECTIVE HEALTH TECHNOLOGY ASSESSMENT
    Type Article Title CAPACITY BUILDING IN AGENCIES FOR EFFICIENT AND EFFECTIVE HEALTH
    TECHNOLOGY ASSESSMENT Authors Debjani Mueller , Iñaki Gutiérrez-Ibarluzea , Tara
    Schuller , Marco Chiumente , Jeonghoon Ahn , Andres Pichon-Riviere , Sebastian
    García-Martí , David Grainger , Elizabeth Cobbs  and Marco Marchetti   Journal
    International Journal of Technology Assessment in Health Care Published online:
    17 October 2016 MAPPING OF HEALTH TECHNOLOGY ASSESSMENT IN SELECTED COUNTRIES
    Type Article Title MAPPING OF HEALTH TECHNOLOGY ASSESSMENT IN SELECTED COUNTRIES
    Authors Wija Oortwijn , Pieter Broos , Hindrik Vondeling , David Banta  and Lora
    Todorova   Journal International Journal of Technology Assessment in Health Care
    Published online: 2 December 2013 Librarians Authors Publishing partners Agents
    Corporates Accessibility Our blog News Contact and help Cambridge Core legal notices
    Feedback Sitemap Select your country preference Afghanistan Aland Islands Albania
    Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua and Barbuda
    Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain Bangladesh
    Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia and Herzegovina
    Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei Darussalam
    Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman Islands
    Central African Republic Chad Channel Islands, Isle of Man Chile China Christmas
    Island Cocos (Keeling) Islands Colombia Comoros Congo Congo, The Democratic Republic
    of the Cook Islands Costa Rica Cote D''Ivoire Croatia Cuba Cyprus Czech Republic
    Denmark Djibouti Dominica Dominican Republic East Timor Ecuador Egypt El Salvador
    Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas) Faroe Islands
    Fiji Finland France French Guiana French Polynesia French Southern Territories
    Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada Guadeloupe
    Guam Guatemala Guernsey Guinea Guinea-bissau Guyana Haiti Heard and Mc Donald
    Islands Honduras Hong Kong Hungary Iceland India Indonesia Iran, Islamic Republic
    of Iraq Ireland Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati
    Korea, Democratic People''s Republic of Korea, Republic of Kuwait Kyrgyzstan Lao
    People''s Democratic Republic Latvia Lebanon Lesotho Liberia Libyan Arab Jamahiriya
    Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia
    Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico
    Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro
    Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands Netherlands
    Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island
    Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory, Occupied
    Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto
    Rico Qatar Reunion Romania Russian Federation Rwanda Saint Kitts and Nevis Saint
    Lucia Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe
    Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia
    Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands
    Spain Sri Lanka St. Helena St. Pierre and Miquelon Sudan Suriname Svalbard and
    Jan Mayen Islands Swaziland Sweden Switzerland Syrian Arab Republic Taiwan Tajikistan
    Tanzania, United Republic of Thailand Togo Tokelau Tonga Trinidad and Tobago Tunisia
    Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab
    Emirates United Kingdom United States United States Minor Outlying Islands United
    States Virgin Islands Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam
    Virgin Islands (British) Wallis and Futuna Islands Western Sahara Yemen Zambia
    Zimbabwe Join us online Rights & Permissions Copyright Privacy Notice Terms of
    use Cookies Policy © Cambridge University Press 2024"'
  inline_citation: '>'
  journal: International Journal of Technology Assessment in Health Care
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'The extent and quality of qualitative evidence included in health technology
    assessments: A review of submissions to NICE and CADTH'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Espinoza J.C.
  - Sehgal S.
  - Phuong J.
  - Bahroos N.
  - Starren J.
  - Wilcox A.
  - Meeker D.
  citation_count: '0'
  description: 'Introduction: Integrating social and environmental determinants of
    health (SEDoH) into enterprise-wide clinical workflows and decision-making is
    one of the most important and challenging aspects of improving health equity.
    We engaged domain experts to develop a SEDoH informatics maturity model (SIMM)
    to help guide organizations to address technical, operational, and policy gaps.
    Methods: We established a core expert group consisting of developers, informaticists,
    and subject matter experts to identify different SIMM domains and define maturity
    levels. The candidate model (v0.9) was evaluated by 15 informaticists at a Center
    for Data to Health community meeting. After incorporating feedback, a second evaluation
    round for v1.0 collected feedback and self-assessments from 35 respondents from
    the National COVID Cohort Collaborative, the Center for Leading Innovation and
    Collaboration''s Informatics Enterprise Committee, and a publicly available online
    self-assessment tool. Results: We developed a SIMM comprising seven maturity levels
    across five domains: data collection policies, data collection methods and technologies,
    technology platforms for analysis and visualization, analytics capacity, and operational
    and strategic impact. The evaluation demonstrated relatively high maturity in
    analytics and technological capacity, but more moderate maturity in operational
    and strategic impact among academic medical centers. Changes made to the tool
    in between rounds improved its ability to discriminate between intermediate maturity
    levels. Conclusion: The SIMM can help organizations identify current gaps and
    next steps in improving SEDoH informatics. Improving the collection and use of
    SEDoH data is one important component of addressing health inequities.'
  doi: 10.1017/cts.2023.691
  full_citation: '>'
  full_text: '>

    "We use cookies to distinguish you from other users and to provide you with a
    better experience on our websites. Close this message to accept cookies or find
    out how to manage your cookie settings. Discover Content Products and Services
    Home Home Browse subjects Publications Open research Services About Cambridge
    Core Access provided by Register Log in Cart ( 0 ) Home >Journals >Journal of
    Clinical and Translational Science >Volume 7 Issue 1 >Development of a social
    and environmental determinants... English Français Journal of Clinical and Translational
    Science Article contents Abstract Introduction: Methods: Results: Conclusion:
    Introduction Materials and methods Results Discussion Conclusion Funding statement
    Competing interests References Development of a social and environmental determinants
    of health informatics maturity model Published online by Cambridge University
    Press:  07 December 2023 Juan C. Espinoza [Opens in a new window] , Shruti Sehgal
    [Opens in a new window] , Jimmy Phuong , Neil Bahroos , Justin Starren , Adam
    Wilcox  and Daniella Meeker Show author details Article Figures Metrics Save PDF
    Share Cite Rights & Permissions [Opens in a new window] Abstract Introduction:
    Integrating social and environmental determinants of health (SEDoH) into enterprise-wide
    clinical workflows and decision-making is one of the most important and challenging
    aspects of improving health equity. We engaged domain experts to develop a SEDoH
    informatics maturity model (SIMM) to help guide organizations to address technical,
    operational, and policy gaps. Methods: We established a core expert group consisting
    of developers, informaticists, and subject matter experts to identify different
    SIMM domains and define maturity levels. The candidate model (v0.9) was evaluated
    by 15 informaticists at a Center for Data to Health community meeting. After incorporating
    feedback, a second evaluation round for v1.0 collected feedback and self-assessments
    from 35 respondents from the National COVID Cohort Collaborative, the Center for
    Leading Innovation and Collaboration’s Informatics Enterprise Committee, and a
    publicly available online self-assessment tool. Results: We developed a SIMM comprising
    seven maturity levels across five domains: data collection policies, data collection
    methods and technologies, technology platforms for analysis and visualization,
    analytics capacity, and operational and strategic impact. The evaluation demonstrated
    relatively high maturity in analytics and technological capacity, but more moderate
    maturity in operational and strategic impact among academic medical centers. Changes
    made to the tool in between rounds improved its ability to discriminate between
    intermediate maturity levels. Conclusion: The SIMM can help organizations identify
    current gaps and next steps in improving SEDoH informatics. Improving the collection
    and use of SEDoH data is one important component of addressing health inequities.
    Keywords Social determinants of health informatics maturity models health equity
    clinical and translational research Type Research Article Information Journal
    of Clinical and Translational Science , Volume 7 , Issue 1 , 2023 , e266 DOI:
    https://doi.org/10.1017/cts.2023.691 [Opens in a new window] Creative Commons
    This is an Open Access article, distributed under the terms of the Creative Commons
    Attribution-NonCommercial licence (http://creativecommons.org/licenses/by-nc/4.0/),
    which permits non-commercial re-use, distribution, and reproduction in any medium,
    provided the original article is properly cited. The written permission of Cambridge
    University Press must be obtained prior to any commercial use. Copyright © The
    Author(s), 2023. Published by Cambridge University Press on behalf of The Association
    for Clinical and Translational Science Introduction Social determinants of health
    (SDoH) are defined by the World Health Organization as “the conditions in which
    people are born, grow, work, live, and age, and the wider set of forces and systems
    shaping the conditions of daily life” and include factors like socioeconomic status,
    education, neighborhood and physical environment, employment, social support networks,
    and access to health care [1]. Despite significant advances in high-quality healthcare
    access, health inequities in the United States persist by race, ethnicity, sexual
    orientation, gender identity, and disability, as well as by economic and community-level
    factors such as geographic location, poverty status, and employment. Socioeconomic
    status is perhaps the central concept that brings together the set of social determinants
    that shape health and plays a critical role in driving disparate health outcomes
    [ Reference Stringhini, Carmeli and Jokela 2]. One study estimated that socioeconomic
    factors alone may account for 47% of health outcomes, while health behaviors,
    clinical care, and the physical environment account for 34, 16, and 3% of health
    outcomes, respectively [ Reference Hood, Gennuso, Swain and Catlin 3]. In recent
    years, both public health and not-for-profit organizations have increasingly been
    focused on addressing SDoH and their impact on each individual’s overall wellness
    and the ability of people to access healthcare services. For example, the Department
    of Health and Human Services (HHS) launched Healthy People 2030 [4], which has
    made a broader audience aware of SDoH issues. The COVID-19 pandemic has also shed
    light on the systemic drivers of health inequities in our society as well as innovative
    technology solutions to help mitigate the effects of the pandemics [ Reference
    Peretz, Islam and Matiz 5– Reference Phuong, Zampino and Dobbins 9], specifically,
    the need for clinical data sharing and the collection and integration of high-quality
    SDoH data. The pandemic has also revealed the coupled role of the clinical health
    system and the non-clinical aspects that influence patient well-being and ability
    to maintain physical and mental wellness. The role of the social environment and
    structural barriers inherent to access to care and supportive resources has led
    to a focus on community-level effects and information captured by different spatial
    data products, sometimes termed as social and environmental determinants of health
    (SEDoH) to emphasize the role of the environment [ Reference Kingsbury, Abajian
    and Abajian 10]. Fundamentally, to address health disparities and improve patient
    health outcomes, health systems need the ability to gather, track, quantify, and
    use SEDoH data. Unfortunately, many of our health information systems and medical
    terminologies were not designed to capture this information [ Reference Chen,
    Tan and Padman 11]. This, combined with the fact that our understanding of SEDoH
    is still evolving, has resulted in many organizations making little or no progress
    in meaningfully gathering SEDoH data. One way to address the current state is
    through maturity models. Maturity models are self-evaluation tools that reflect
    best practices and can help organizations identify gaps, prioritize investments,
    and develop institutional roadmaps and strategies for growth. They are “based
    on the premise that people, organizations, functional areas, processes, etc.,
    evolve through a process of development and growth towards a more advanced maturity
    accomplishing several stages [ Reference Carvalho, Rocha and Abreu 12].” Examples
    of clinical informatics maturity models include The HIMSS Electronic Medical Record
    Adoption Model [13], the Continuity of Care Maturity Model [14], and the Quintegra
    maturity model for electronic healthcare (eHMM) [ Reference Sharma 15]. There
    has also been active development of research informatics maturity models, focusing
    on domains like research data warehouses and research data sharing [ Reference
    Knosp, Barnett, Anderson and Embi 16– Reference Champieux, Solomonides and Conte
    18]. Models exist covering a variety of domains, including the PACS (picture archiving
    and communication systems) [ Reference van de Wetering and Batenburg 19] and Healthcare
    Analytics Adoption Model for data analysis [ Reference Sanders 20], but no models
    focus on SEDoH information that will facilitate organizational advancements. The
    2019 Parkland Center for Clinical Innovation SDOH Maturity Model [ Reference Miff
    21] is intended to guide healthcare system executives when designing population
    health programs and developing patient-specific treatment plans, but it is focused
    on the behaviors and actions of leadership teams, rather than informatics. Informed
    by our prior work focused on SEDoH [ Reference Phuong, Zampino and Dobbins 9,
    Reference Phuong, Hong and Palchuk 22– Reference Phuong, Riches and Madlock-Brown
    24] and supported by the Center for Data to Health (CD2H), we set out to develop
    and evaluate an SEDoH informatics maturity model. The goal of this project is
    to create a tool that can help organizations identify gaps and prioritize investments
    in their policies, processes, and technologies related to the capture and use
    of SEDoH data. Ultimately, better data and data-driven decision-making will help
    improve health outcomes for underserved and marginalized populations. Materials
    and methods Development phase To develop the SEDoH informatics maturity model
    (SIMM), we leveraged the guidance provided by CD2H in developing self-assessment
    maturity tools, aligned with CD2H Informatics Maturity and Best Practices Core’s
    mission of supporting development and dissemination of best practices in data
    use and informatics to the Clinical and Translational Science Award (CTSA) community
    [25]. Establishing the maturity model was a multi-step process (Fig. 1). First,
    we established a core expert group consisting of developers, informatics champions,
    and subject matter experts at the University of Southern California, Children’s
    Hospital Los Angeles, and University of Washington. The next step involved extensive
    training on maturity model processes and procedures, including a review of over
    30 existing informatics maturity models curated by author AW and colleagues. Conceptually,
    the model was informed by HHS’ Healthy People 2030 and the socioecological model
    of health (Fig. 2). The core group identified the different dimensions/domain
    axes of the maturity model and iteratively reviewed them with external reviewers
    from CTSA hubs/CD2H leadership. This was followed by defining the maturity levels
    for the model. Seven levels were chosen instead of the typical five to provide
    additional granularity and specificity on the various intermediate steps toward
    SEDoH data maturity across the five domains. In all cases, we used a consensus
    approach in which all topics and levels were discussed until we arrived at a unanimous
    decision. Figure 1. Process diagram illustrating the steps in the development
    of the social and environmental determinants of health informatics maturity model.
    SIMM = social & environmental determinants of health informatics maturity model,
    CD2H = Center for Data to Health, N3C = National COVID Cohort Collaborative, IEC
    = the Center for Leading Innovation and Collaboration’s Informatics Enterprise
    Committee. Figure 2. Conceptual model for social and environmental determinants
    of health informatics maturity model. Colors and icons adapted from Healthy People
    2030, US Department of Health and Human Services, Office of Disease and Health
    Promotion. SDoH = social determinants of health, SEM = socioecological model of
    health. Alignment of Maturity Model with Principles of Implementation Science.
    Due to the complex nature of adopting SEDoH into healthcare delivery systems,
    we aimed to better align maturity modeling with principles of Implementation Science.
    For example, in addition to descriptions of technical assets and processes, we
    included domains for data collection policies and organizational and strategic
    maturity. These allow for the self-evaluation of organizational needs that have
    been highlighted in Implementation Science frameworks such as the Practical, Robust
    Implementation, and Sustainability Model and the updated Consolidated Framework
    for Implementation Research that explicitly address organizational and structural
    determinants of achieving maturity goals, such as policy, management, and resourcing
    postures [ Reference Feldstein and Glasgow 26, Reference Damschroder, Reardon,
    Widerquist and Lowery 27]. Model description We established a seven-level maturity
    model to map how SEDoH processes can reach maturity in each of five domains. This
    model may reflect the evolution, improvement, and transformation of an organization’s
    capacity over time and captures its capabilities at each intermediate level. The
    scope of every domain was iteratively defined during the development of the SIMM.
    During the development phase, the characteristics of maturity levels were established.
    A generic description of maturity levels outlined in Table 1 served as a foundation
    for developing maturity statements for every domain. To support the use of the
    SIMM tool, detailed narrative descriptions were developed for each of the seven
    maturity levels across all five domains (Table 2). The narratives for every level
    provide a detailed explanation of the stages from the simplest, ad hoc stage to
    the advanced, systematized, and optimized level. The goal of this maturity model
    was to offer an opportunity for institutions to have a structured way to identify
    their current level of capability or maturity with SEDoH information and find
    the gap between where they are and where they want to be to remain competitive
    and innovative. In this model, an institution is defined to be at a specific maturity
    level for a given domain when the process attributes for the lower maturity level(s)
    were fully achieved and the attributes for the specific maturity level were fully
    or largely achieved. Table 1. Generic description of each maturity level in the
    social and environmental determinants of health maturity model (SIMM). SEDoH =
    the social and environmental determinants of health. Table 2. Maturity level descriptions
    for the five domains of the social and environmental determinants of health informatics
    maturity model (SIMM) ACS = American community survey; = clinical decision support;
    CDS PDF = portable document format; EPA = Environmental Protection Agency; FHIR
    = fast healthcare interoperability resources; HER = electronic health record;
    HL7 = health level 7; SEDoH = social and environmental determinants of health;
    USDA = United States department of agriculture. Data sources for social and environmental
    determinants of health (SEDoH) informatics maturity model include Person-level
    data (SEDoH data about an individual, ideally self-reported, and often collected
    using standardized and validated instruments) and Contextual data (Data about
    the environments in which a patient lives). All of the levels and descriptions
    outlined below apply to both personal and contextual data, except where noted.
    **Primarily for contextual data, but also personal data where relevant;, patients
    are providing personal-level SDoH data to some other entity, and the health system
    in question is integrating it into their data. The model also includes a description
    and quantification of data sources that captures whether institutions are using
    patient-level SEDoH data (e.g., self-reported assessments) or contextual SEDoH
    data (e.g., geocoded variables extracted from the American Community Survey [28]).
    This section of the tool helps organizations catalog their SEDoH data sources
    and qualify patient-level data with a categorical hierarchy of ad hoc tools, validated
    and standardized tools, and validated tools augmented with items specific to the
    population of interest. Evaluation phase We performed two rounds of evaluation
    of the SIMM with respondents recruited from the CTSA network and CTSA-related
    activities, such as the Informatics Enterprise Committee (IEC), National COVID
    Cohort Collaborative (N3C), and CD2H. First round To evaluate the relevance of
    the identified domain axes and the established levels of maturity, the first version
    of the SIMM (v0.9) was reviewed at the CD2H Informatics Maturity and Best Practices
    community meeting in May 2020 by 15 informatics experts. Data were collected using
    Zoom’s polling feature. The goal of the review meeting was to discuss and solicit
    comments on the candidate maturity model. To select a level for each domain axis,
    the attendees were asked to think about the highest level in the SIMM at which
    their organization consistently and comprehensively operates on a daily basis.
    The recommendations of the working group were compiled, and modifications were
    made to better describe the levels of maturity (SIMM v1.0) as well as to provide
    more context for improvement opportunities. In the case of conflicting recommendations,
    the same consensus approach described above was used. Second round We gathered
    data to evaluate SIMM v1.0 across three different opportunities. SIMM (v1.0) was
    presented at the N3C SDoH workgroup meeting in July 2020 and the University of
    Rochester’s Center for Leading Innovation and Collaboration IEC meeting in February
    2021. During these virtual meetings, an overview of the need, rationale, and applications
    of the model was presented, and the attendees were asked for feedback. Then, they
    were sent a Google Form survey via e-mail that included the self-assessment and
    an opportunity to provide additional feedback on the tool itself. Comments were
    received from 21 respondents (12 IEC and nine N3C). The survey was anonymous and
    did not gather information about the individual or their home institution, but
    it was only distributed to individuals who are part of the CTSA Network. The third
    evaluation opportunity in Round 2 was a publicly available online self-assessment.
    SIMM v1.0 was built using REDcap MariaDB SQL instance and added to the University
    of Washington’s Maturity Model Self-Assessment Toolkit, along with other models
    such as the Research Informatics & Open Science Maturity model [25] (the toolkit
    is now being managed by Northwestern University). Users accessed the SIMM v1.0
    assessment within the Maturity Model Resource Portal, an HTML front-end interface
    designed on top of a react.io framework with the REDCap server after institutional
    identity and access management steps. Users could login with their portal credentials,
    or complete any assessment anonymously as a guest. In total, 14 respondents representing
    11 institutions (one from the South, seven from the Midwest, and three from the
    West Coast) completed the self-assessment between July 2020 and February 2022.
    Ethical considerations This project does not meet the definition of human subjects
    research and was deemed exempt by the Children’s Hospital Los Angeles IRB, IRB#
    CHLA-20-00249. Results We developed a SIMM comprising seven advancing levels of
    maturity across five domain areas of data collection policies, data collection
    methods and technologies, technology platforms for analysis and visualization,
    analytics capacity, and operational and strategic impact (Fig. 3). The model also
    includes an assessment of data sources for both person-level and contextual SEDoH
    data. Figure 3. The social and environmental determinants of health informatics
    maturity model (SIMM) with Data Sources Assessment. EHR = electronic health record.
    SEDoH = social and environmental determinants of health, ACS = American Community
    Survey, SSDI = social security death index, USDA = United States Department of
    Agriculture, EPA = Environmental Protection Agency, CDC = Centers for Disease
    Control and Prevention. SIMM Assessment Results – First Round (v0.9) As seen in
    Figure 4, fifteen informaticists participated in the first round to evaluate SIMM
    v0.9. Over half of the respondents reported that their organizations had no standardized
    policies in place for collecting SEDoH data across their organization. Our data
    suggest that it was mostly ad hoc or some individuals followed consistent data
    collection practices. For the data collection methods and technologies domain,
    a majority of the respondents endorsed level 4 as the lowest maturity level, indicating
    that although SEDoH data was collected electronically using third-party platforms
    (e.g., Purple Binder), it was not integrated with the EHR. The maturity of technology
    platforms for analysis and visualization ranged from level 1 to level 6, with
    the majority centered around the middle levels. Similarly, with regard to SEDoH
    data, analytics capacity spanned from the lowest to the advanced maturity stages.
    Figure 4. Comparison of responses to the social and environmental determinants
    of health informatics maturity model (SIMM) self-assessment survey between version
    0.9 (First round, n = 15) and version 1.0 (Second round, n = 35). SIMM Assessment
    Results – Second Round (v1.0) After revising the SIMM, 35 participants evaluated
    SIMM v1.0 in the second round. The maturity of the data collection policies and
    the data collection methods domain was centered around the middle levels. The
    majority of organizations indicated a higher maturity for the technology platforms
    for analysis and visualization domain. Analytics capacity dimension ranged from
    the least advanced to the most advanced maturity level and a similar trend was
    witnessed for the operational and strategic impact domain. Discussion SEDoH informatics
    maturity model While the concept of maturity models is not new to the healthcare
    field, its application to the SEDoH domain has not been extensively researched.
    In the present study, we defined a seven-level maturity model with a three-fold
    objective: first, to assist organizations in self-assessing their current level
    of informatics maturity in relation to SEDoH capability, second, to help organizations
    with derivation of a gap analysis, and third, to outline steps that they can take
    to improve their current level of maturity. The SEDoH maturity model consists
    of 35 maturity statements categorized under five domains of data collection policies,
    data collection methods and technologies, technology platforms for analysis and
    visualization, analytics capacity, and operational and strategic impact. Domain
    experts reviewed our SEDoH maturity model to evaluate the conceptual completeness
    of the model (i.e., appropriateness of the domains and levels, as well as usefulness
    and ease of use). Two rounds of assessment and feedback by experts at N3C, IEC,
    and across the CTSA network suggest reasonable real-world applicability. The distributions
    of responses were broader and more even in all but one domain in the second evaluation
    round compared to the first round (Fig. 4), suggesting that the edits made to
    the SIMM tool for v1.0 increased its capacity to discriminate between stages of
    maturity. Our assessments reveal that a consistent finding across both stages
    of assessment was that none of the respondents suggested level 1 for three out
    of the five domains, (i.e., “Data collection policies,” “Data collection methods
    and technologies,” “Operational and strategic impact”). This could be seen as
    a manifestation of the initial commitment of many organizations to improve efforts
    related to SEDoH data. Another interpretation could be that existing maturity
    in other informatics domains might translate to a basic level of maturity for
    SEDoH data (no other informatics maturity assessments were performed, but this
    could be a reasonable assumption given that all respondents were from the CTSA
    network). Further, an important finding of the round one assessment was that organizations
    endorsed maturity levels 4, 5, and 6 for the “Data collection methods and technologies”
    dimension of the maturity model, with over half suggesting the middle-level maturity
    at level 4. This growing momentum likely reflects a confluence of several phenomena:
    first, an increasing recognition of the importance of SEDoH at a community, regional,
    and national level [ Reference Fuchs 29], especially since the COVID-19 pandemic,
    as well as recent initiatives leading to the adoption of solutions that connect
    healthcare and community partner stakeholders together to move the needle on social
    determinants. In addition, the recent federal reforms of health care and health
    information technology (IT) facilitate data collection by extending investment
    in electronic health records to healthcare providers that receive public funds
    [ Reference Siegel and Nolan 30], as well as incorporating social determinants
    into EHR to promote patient and population health [ Reference Bazemore, Cottrell
    and Gold 31]. Novel initiatives have emerged to address socioeconomic and non-medical
    health determinants within the framework of the healthcare delivery system. These
    encompass Medicaid efforts led by states or health plans, multipayer federal and
    state initiatives, and provider-level activities aimed at recognizing and resolving
    the social and non-medical needs of their patients [ Reference Alley, Asomugha,
    Conway and Sanghavi 32]. Importance of SEDoH and SIMM: Now More Than Ever Despite
    a shift in our understanding of health and its determinants over the past few
    decades and improvements in medical care and in disease prevention, population-level
    health inequalities in healthcare result in $309 billion in losses to the economy
    annually, disproportionately affecting minoritized and underserved communities
    [ Reference Ubri and Artiga 33]. In recent years, public health leaders and researchers
    in the United States have increasingly recognized that medical care alone cannot
    adequately improve health overall or reduce health disparities without also addressing
    where and how people live [ Reference Braveman, Egerter and Williams 34]. Social,
    economic, and environmental factors influence health behaviors and are primary
    drivers of health outcomes. For instance, children born to parents who have not
    completed high school education are more likely to live in an environment that
    presents obstacles to healthy living, such as exposure to trash, substandard housing,
    unsafe living conditions, limited access to playgrounds, etc [ Reference Singh,
    Siahpush and Kogan 35]. The COVID-19 pandemic highlighted on a national scale
    many of the existing inequities. According to the CDC, people of color had higher
    rates of infection, hospitalization, and death due to COVID-19, resulting from
    an increased risk of exposure to the virus due to living, working, and transportation
    conditions, as well as witnessed increased barriers to treatment due to existing
    disparities in access to health care [36]. Capturing and collecting SEDoH data
    in clinical settings is essential to reduce health inequities, improve health,
    and control healthcare costs [ Reference Singh, Daus and Allender 37, Reference
    Blizinsky and Bonham 38], according to a 2014 report from the Institute of Medicine
    [39]. For example, Hennepin County Medical Center standardized food insecurity
    screening at two outpatient clinics that used an EHR-based food resource referral
    system. It was observed that systematic screening increased Senior Care referrals
    by 1,450% (P < .001) and Pediatrics recorded a 275% referral increase (P < .001)
    [ Reference Hager, De Kesel Lofthus, Balan and Cutts 40]. Given the broad recognition
    of the increasing need to address SEDoH in healthcare, the SIMM is a timely and
    important tool to help identify areas where organizations are not operating optimally
    and allow them to determine strategies that can improve their operations and processes.
    Further, the impact of maturity model implementation can be learned from a recent
    mixed methods case study in Australia that indicated that higher digital health
    maturity was associated with better outcomes, including maintaining a patient
    health record, tracking patient experience data, tracking the patient journey,
    and mitigating the clinical risk [ Reference Woods, Dendere and Eden 41]. Challenges
    in accessing and analyzing SEDoH data While it is being increasingly accepted
    that SEDoH data should be routinely incorporated into clinical decision-making
    to improve patient outcomes and operational efficiency, collecting and analyzing
    this data remain inherently challenging. It is unclear whether or not SDoH are
    frequently recorded as part of the standard clinical care [ Reference Garg, Boynton-Jarrett
    and Dworkin 42]. Our study findings also demonstrate that standardized data collection
    policies were endorsed only by individuals or small organizational subunits. Data
    harmonization across clinical, public health, and administrative datasets continues
    to be a significant challenge [ Reference Camacho-Rivera, Jessica, Hsueh, Wetter
    and Zhu 43]. Moreover, while it is easy to free-text this information within the
    electronic health record (EHR), extracting unstructured data from narrative notes
    requires time-consuming manual chart reviews or the use of advanced natural language
    processing [ Reference Dorr, Bejan, Pizzimenti, Singh, Storer and Quinones 44].
    In addition, the healthcare industry primarily relies on administrative claims
    data to evaluate SDoH [ Reference Hatef, Rouhizadeh and Tia 45,]. However, it
    is important to realize that the documentation of this data within the EHR is
    inconsistent and spotty due to lack of standards for capturing this data in a
    structured format. To address these unmet needs, SEDoH extraction research from
    unstructured clinic notes has been increasing [ Reference Patra, Sharma and Vekaria
    46]. A variety of SEDoH, such as substance use, work, housing situation, environmental
    factors, physical activity, sexual factors, transportation, education, and language,
    have all been annotated in clinical corpora [ Reference Lybarger, Ostendorf and
    Yetisgen 47– Reference Han, Zhang and Shi 50]. The i2b2 NLP Smoking Challenge
    introduced a corpus of 502 notes with tobacco use status labels [ Reference Uzuner,
    Goldstein, Luo and Kohane 51]. As a tool, the SIMM can help organizations understand
    their current state as it relates to SEDoH data and develop a systematic approach
    to improvement, but they will still need to look to the literature, best practices,
    and guidelines for specific solutions, such as protocols to improve race and ethnicity
    data collection [ Reference Ulmer, McFadden and Nerenz 52, Reference Vega Perez,
    Hayden and Mesa 53]. Ultimately, higher levels of SEDoH informatics maturity will
    increase the quality and timeliness of SEDoH data, thus allowing clinicians to
    make more informed, targeted medical decisions that deliver more equitable, high-quality
    care. Taking into account the growing significance of SEDoH, informatics maturity
    models, and the pressing need to integrate SEDoH data into health informatics,
    our work describes a multidimensional approach to measure organizational SEDoH
    capability, including items for data collection policies, data collection methods,
    technology and analytics capability, and for operational and strategic impact.
    There are some limitations to our work. First, the participants in our evaluation
    rounds are not representative of all healthcare organizations in the US; they
    were primarily academics affiliated with CTSAs and/or had a pre-existing interest
    in SEDoH (such as the members of the N3C SDoH workgroup). Second, this work was
    conducted at a period of time when there was increased knowledge, awareness, and
    acceptance of the concept of SEDoH data and organizational maturity. Finally,
    our work did not include non-academic healthcare organizations and community organizations.
    The validity and utility of the SIMM outside of academic medical centers will
    need to be evaluated in future studies. Conclusion Clinical and economic motivations
    have encouraged healthcare providers, health insurance companies, local governments,
    and community-based organizations to take action to address SEDoH. The initial
    findings of this exploratory study suggest that the proposed SIMM is applicable
    for identifying an organization’s SEDoH informatics capacity and providing a roadmap
    for moving to the next level. As a result of identifying the strong and weak points
    of the SEDoH data workflow, depending on the assessment findings, improvement
    opportunities can be identified by an organization. Additional work is still needed
    to evaluate the generalizability of the SEDoH maturity model and identify specific
    strategies and protocols to increase maturity. Acknowledgments We are grateful
    for the time and contributions of the faculty and staff who regularly participate
    in N3C, CD2H, IEC, and other CTSA Network activities who provided invaluable insights
    and responded to the self-assessment surveys. Study data were collected and managed
    using REDCap electronic data capture tools (Harris et al. 2009) hosted at the
    Institute of Translational Health Sciences (ITHS). Funding statement This work
    was supported by grants UL1TR001855 (SC CTSI), UL1TR001422 (NUCATS), and UL1TR002319
    (ITHS) from the National Center for Advancing Translational Science (NCATS) of
    the U.S. National Institutes of Health. This work also received support from the
    National Center for Data to Health (CD2H) grant (NIH/NCATS U24TR002306) and supplemental
    funding from the National COVID Cohort Collaborative (NIH/NCATS U24TR002306-04S3).
    The content is solely the responsibility of the authors and does not necessarily
    represent the official views of the National Institutes of Health. Competing interests
    JE is a paid consultant for Sanofi. The other authors declare no competing interests.
    References 1 World Health Organization. Social determinants of health. (https://www.who.int/health-topics/social-determinants-of-health#tab=tab_1).
    Accessed March 22, 2023.Google Scholar 2 Stringhini, S, Carmeli, C, Jokela, M,
    et al. Socioeconomic status and the 25 × 25 risk factors as determinants of premature
    mortality: a multicohort study and meta-analysis of 1·7 million men and women.
    Lancet. 2017;389(10075):1229–1237. doi: 10.1016/S0140-6736(16)32380-7.CrossRefGoogle
    ScholarPubMed 3 Hood, CM, Gennuso, KP, Swain, GR, Catlin, BB. County health rankings:
    relationships between determinant factors and health outcomes. Am J Prev Med.
    2016;50(2):129–135. doi: 10.1016/j.amepre.2015.08.024.CrossRefGoogle ScholarPubMed
    4 Office of Disease Prevention and Health Promotion. Healthy People 2030. Social
    Determinants of Health. (https://health.gov/healthypeople/priority-areas/social-determinants-health).
    Accessed March 20, 2023.Google Scholar 5 Peretz, PJ, Islam, N, Matiz, LA. Community
    health workers and covid-19 - addressing social determinants of health in times
    of crisis and beyond. N Engl J Med. 2020;383(19):e108. doi: 10.1056/NEJMp2022641.CrossRefGoogle
    ScholarPubMed 6 Rollston, R, Galea, S. COVID-19 and the social determinants of
    health. Am J Health Promot. 2020;34(6):687–689. doi: 10.1177/0890117120930536b.CrossRefGoogle
    ScholarPubMed 7 Shah, GH, Shankar, P, Schwind, JS, Sittaramane, V. The detrimental
    impact of the COVID-19 crisis on health equity and social determinants of health.
    J Public Health Manag Pract. 2020;26(4):317–319. doi: 10.1097/PHH.0000000000001200.CrossRefGoogle
    ScholarPubMed 8 Ramírez, IJ, Lee, J. COVID-19 emergence and social and health
    determinants in Colorado: a rapid spatial analysis. Int J Environ Res Public Health.
    2020;17(11):3856. doi: 10.3390/ijerph17113856 Published 2020 May 29.CrossRefGoogle
    Scholar 9 Phuong, J, Zampino, E, Dobbins, N, et al. Extracting patient-level social
    determinants of health into the OMOP common data model. AMIA Annu Symp Proc. 2022;2021:1989–1998.Google
    ScholarPubMed 10 Kingsbury, P, Abajian, H, Abajian, M, et al. SEnDAE: a resource
    for expanding research into social and environmental determinants of health. Comput
    Methods Programs Biomed. 2023;238:107542. doi: 10.1016/j.cmpb.2023.107542.CrossRefGoogle
    ScholarPubMed 11 Chen, M, Tan, X, Padman, R. Social determinants of health in
    electronic health records and their impact on analysis and risk prediction: a
    systematic review. J Am Med Inform Assoc. 2020;27(11):1764–1773. doi: 10.1093/jamia/ocaa143.CrossRefGoogle
    ScholarPubMed 12 Carvalho, JV, Rocha, Á., Abreu, A. Maturity models of healthcare
    information systems and technologies: a literature review. J Med Syst. 2016;40(6):131.
    doi: 10.1007/s10916-016-0486-5.CrossRefGoogle ScholarPubMed 13 Electronic Medical
    Record Adoption Model (EMRAM). HIMSS. (https://www.himss.org/what-we-do-solutions/digital-health-transformation/maturity-models/electronic-medical-record-adoption-model-emram).
    Accessed March 24, 2023.Google Scholar 14 Continuity of Care Maturity Model (CCMM).
    HIMSS. (https://www.himss.org/what-we-do-solutions/digital-health-transformation/maturity-models/continuity-care-maturity-model-ccmm).
    Accessed March 28, 2023.Google Scholar 15 Sharma, B., Electronic Healthcare Maturity
    Model (eHMM). White Paper. Quintegra Solutions Limited, 2008. (http://www.quintegrasolutions.com/eHMM%20White%20Paper.pdf)
    Accessed March 27, 2023.Google Scholar 16 Knosp, BM, Barnett, WK, Anderson, NR,
    Embi, PJ. Research IT maturity models for academic health centers: early development
    and initial evaluation. J Clin Transl Sci. 2018;3(5):289–294. doi: 10.1017/cts.2018.339.CrossRefGoogle
    Scholar 17 Knosp, BM, Dorr, DA, Campion, TR. Maturity in enterprise data warehouses
    for research operations: analysis of a pilot study. J Clin Transl Sci. 2023;7(1):e70.
    doi: 10.1017/cts.2023.23.CrossRefGoogle ScholarPubMed 18 Champieux, R, Solomonides,
    A, Conte, M, et al. Ten simple rules for organizations to support research data
    sharing. PLoS Comput Biol. 2023;19(6):e1011136. doi: 10.1371/journal.pcbi.1011136.CrossRefGoogle
    ScholarPubMed 19 van de Wetering, R, Batenburg, R. A PACS maturity model: a systematic
    meta-analytic review on maturation and evolvability of PACS in the hospital enterprise.
    Int J Med Inform. 2009;78(2):127–140. doi: 10.1016/j.ijmedinf.2008.06.010.CrossRefGoogle
    Scholar 20 Sanders, D. The Healthcare Analytics Adoption Model: A Roadmap to Analytic
    Maturity (white paper). 2020. (https://www.healthcatalyst.com/learn/white-papers/the-healthcare-analytics-adoption-model-a-roadmap-to-analytic-maturity)
    Accessed March 18, 2023.Google Scholar 21 Miff, S. THE TIME IS NOW FOR HEALTH
    SYSTEMS TO GET SERIOUS ABOUT SOCIAL DETERMINANTS OF HEALTH. (https://pccinnovation.org/the-time-is-now-for-health-systems-to-get-serious-about-social-determinants-of-health/)
    Accessed Febraury 24, 2023.Google Scholar 22 Phuong, J, Hong, S, Palchuk, MB,
    et al. Advancing interoperability of patient-level social determinants of health
    data to support COVID-19 research. AMIA Jt Summits Transl Sci Proc. 2022;2022:396–405.Google
    ScholarPubMed 23 Madlock-Brown, C, Wilkens, K, Weiskopf, N, et al. Clinical, social,
    and policy factors in COVID-19 cases and deaths: methodological considerations
    for feature selection and modeling in county-level analyses. BMC Public Health.
    2022;22(1):747. doi: 10.1186/s12889-022-13168-y.CrossRefGoogle ScholarPubMed 24
    Phuong, J, Riches, NO, Madlock-Brown, C, et al. Social determinants of health
    factors for gene-environment COVID-19 research: challenges and opportunities.
    Adv Genet (Hoboken). 2022;3(2):2100056. doi: 10.1002/ggn2.202100056.CrossRefGoogle
    ScholarPubMed 25 Advancing the Application of Maturity Models. (https://cd2h.org/node/156)
    Accessed June 27, 2023.Google Scholar 26 Feldstein, AC, Glasgow, RE. A practical,
    robust implementation and sustainability model (PRISM) for integrating research
    findings into practice. Jt Comm J Qual Patient Saf. 2008;34(4):228–243. doi: 10.1016/s1553-7250(08)34030-6.Google
    ScholarPubMed 27 Damschroder, LJ, Reardon, CM, Widerquist, MAO, Lowery, J. The
    updated consolidated framework for implementation research based on user feedback.
    Implement Sci. 2022;17(1):75. doi: 10.1186/s13012-022-01245-0 Published 2022 Oct
    29.CrossRefGoogle ScholarPubMed 28 United States Census Bureau. American Community
    Survey (ACS). (https://www.census.gov/programs-surveys/acs). Accessed October
    25, 2023.Google Scholar 29 Fuchs, VR. Social determinants of health: caveats and
    nuances. JAMA. 2017;317(1):25–26. doi: 10.1001/jama.2016.17335.CrossRefGoogle
    ScholarPubMed 30 Siegel, B, Nolan, L. Leveling the field--ensuring equity through
    national health care reform. N Engl J Med. 2009;361(25):2401–2403. doi: 10.1056/NEJMp0909323.CrossRefGoogle
    ScholarPubMed 31 Bazemore, AW, Cottrell, EK, Gold, R, et al. Community vital signs’:
    incorporating geocoded social determinants into electronic records to promote
    patient and population health. J Am Med Inform Assoc. 2016;23(2):407–412. doi:
    10.1093/jamia/ocv088.CrossRefGoogle ScholarPubMed 32 Alley, DE, Asomugha, CN,
    Conway, PH, Sanghavi, DM. Accountable health communities—addressing social needs
    through medicare and medicaid. N Engl J Med. 2016;374(1):8–11.CrossRefGoogle ScholarPubMed
    33 Ubri, P, Artiga, A. Disparities in health and health care: five key questions.
    The Henry J. Kaiser Family Foundation. (http://kff.org/disparities-policy/issue-brief/disparities-in-health-and-health-care-five-key-questions-and-answers)
    Accessed March 27, 2022.Google Scholar 34 Braveman, P, Egerter, S, Williams, DR.
    The social determinants of health: coming of age. Annu Rev Public Health. 2011;32(1):381–398.
    doi: 10.1146/annurev-publhealth-031210-101218.CrossRefGoogle ScholarPubMed 35
    Singh, GK, Siahpush, M, Kogan, MD. Neighborhood socioeconomic conditions, built
    environments, and childhood obesity. Health Aff (Millwood). 2010;29(3):503–512.
    doi: 10.1377/hlthaff.2009.0730.CrossRefGoogle ScholarPubMed 36 CDC, Risk for COVID-19
    Infection, Hospitalization, and Death By Race/Ethnicity. (https://www.cdc.gov/coronavirus/2019-ncov/covid-data/investigations-discovery/hospitalization-death-by-race-ethnicity.html)
    Accessed April 11, 2023.Google Scholar 37 Singh, GK, Daus, GP, Allender, M, et
    al. Social determinants of health in the united states: addressing major health
    inequality trends for the nation, 1935-2016. Int J MCH AIDS. 2017;6(2):139–164.
    doi: 10.21106/ijma.236.CrossRefGoogle ScholarPubMed 38 Blizinsky, KD, Bonham,
    VL. Leveraging the learning health care model to improve equity in the age of
    genomic medicine. Learn Health Syst. 2018;2(1):e10046. doi: 10.1002/lrh2.10046.CrossRefGoogle
    ScholarPubMed 39 Committee on the Recommended Social and Behavioral Domains and
    Measures for Electronic Health Records; Board on Population Health and Public
    Health Practice; Institute of Medicine. Capturing Social and Behavioral Domains
    and Measures in Electronic Health Records: Phase 2. Washington (DC): National
    Academies Press (US); 2015,https://www.ncbi.nlm.nih.gov/books/NBK268995/ doi:
    10.17226/18951.CrossRefGoogle Scholar 40 Hager, K, De Kesel Lofthus, A, Balan,
    B, Cutts, D. Electronic medical record-based referrals to community nutritional
    assistance for food-insecure patients. Ann Fam Med. 2020;18(3):278–278. doi: 10.1370/afm.2530.CrossRefGoogle
    ScholarPubMed 41 Woods, L, Dendere, R, Eden, R, et al. Perceived impact of digital
    health maturity on patient experience, population health, health care costs, and
    provider experience: mixed methods case study. J Med Internet Res. 2023;25:e45868.
    doi: 10.2196/45868.CrossRefGoogle ScholarPubMed 42 Garg, A, Boynton-Jarrett, R,
    Dworkin, PH. Avoiding the unintended consequences of screening for social determinants
    of health. JAMA. 2016;316(8):813–814. doi: 10.1001/jama.2016.9282.CrossRefGoogle
    ScholarPubMed 43 Camacho-Rivera, Marlene, Jessica, Y, et al. Social determinants
    of health during the COVID-19 pandemic in the US: precision through context. In:
    Hsueh, PYS, Wetter, T, Zhu, X, eds. Personal Health Informatics. Cognitive Informatics
    in Biomedicine and Healthcare. Cham: Springer; 2022:397–425.CrossRefGoogle Scholar
    44 Dorr, D, Bejan, CA, Pizzimenti, C, Singh, S, Storer, M, Quinones, A. Identifying
    patients with significant problems related to social determinants of health with
    natural language processing. Stud Health Technol Inform. 2019;264:1456–1457. doi:
    10.3233/SHTI190482.Google ScholarPubMed 45 Hatef, E, Rouhizadeh, M, Tia, I, et
    al. Assessing the availability of data on social and behavioral determinants in
    structured and unstructured electronic health records: a retrospective analysis
    of a multilevel health care system. JMIR Med Inform. 2019;7(3):e13802. doi: 10.2196/13802.CrossRefGoogle
    ScholarPubMed 46 Patra, BG, Sharma, MM, Vekaria, V, et al. Extracting social determinants
    of health from electronic health records using natural language processing: a
    systematic review. J Am Med Inform Assoc. 2021;28(12):2716–2727. doi: 10.1093/jamia/ocab170.CrossRefGoogle
    ScholarPubMed 47 Lybarger, K, Ostendorf, M, Yetisgen, M. Annotating social determinants
    of health using active learning, and characterizing determinants using neural
    event extraction. J Biomed Inform. 2021;113:103631. doi: 10.1016/j.jbi.2020.103631.CrossRefGoogle
    ScholarPubMed 48 Chapman, AB, Jones, A, Kelley, AT, et al. ReHouSED: a novel measurement
    of veteran housing stability using natural language processing. J Biomed Inform.
    2021;122:103903. doi: 10.1016/j.jbi.2021.103903.CrossRefGoogle ScholarPubMed 49
    Lybarger, K, Yetisgen, M, Uzuner, Ö. The 2022 n2c2/UW shared task on extracting
    social determinants of health. J Am Med Inform Assoc. 2023;30(8):1367–1378. doi:
    10.1093/jamia/ocad012.CrossRefGoogle ScholarPubMed 50 Han, S, Zhang, RF, Shi,
    L, et al. Classifying social determinants of health from unstructured electronic
    health records using deep learning-based natural language processing. J Biomed
    Inform. 2022;127:103984. doi: 10.1016/j.jbi.2021.103984.CrossRefGoogle ScholarPubMed
    51 Uzuner, O, Goldstein, I, Luo, Y, Kohane, I. Identifying patient smoking status
    from medical discharge records. J Am Med Inform Assoc. 2008;15(1):14–24. doi:
    10.1197/jamia.M2408.CrossRefGoogle ScholarPubMed 52 Ulmer, C, McFadden, B, Nerenz,
    DR. Institute of medicine (US) subcommittee on standardized collection of race/ethnicity
    data for healthcare quality improvement. Race, Ethnicity, and Language Data: Standardization
    for Health Care Quality Improvement. Washington (DC): National Academies Press
    (US); 2009.Google Scholar 53 Vega Perez, RD, Hayden, L, Mesa, J, et al. Improving
    patient race and ethnicity data capture to address health disparities: a case
    study from a large urban health system. Cureus. 2022;14(1):e20973. doi: 10.7759/cureus.20973.Google
    ScholarPubMed You have Access Open access Related content AI-generated results:
    by UNSILO [Opens in a new window] Toward standardization, harmonization, and integration
    of social determinants of health data: A Texas Clinical and Translational Science
    Award institutions collaboration Type Article Title Toward standardization, harmonization,
    and integration of social determinants of health data: A Texas Clinical and Translational
    Science Award institutions collaboration Authors Catherine K. Craven , Linda Highfield
    , Mujeeb Basit , Elmer V. Bernstam , Byeong Yeob Choi , Robert L. Ferrer , Jonathan
    A. Gelfond , Sandi L. Pruitt , Vaishnavi Kannan , Paula K. Shireman , Heidi Spratt
    , Kayla J. Torres Morales , Chen-Pin Wang , Zhan Wang , Meredith N. Zozus , Edward
    C. Sankary  and Susanne Schmidt   Journal Journal of Clinical and Translational
    Science Published online: 9 January 2024 Using informatics to advance translational
    science: Environmental scan of adaptive capacity and preparedness of Clinical
    and Translational Science Award Program hubs Type Article Title Using informatics
    to advance translational science: Environmental scan of adaptive capacity and
    preparedness of Clinical and Translational Science Award Program hubs Authors
    Bart Ragon , Boris B. Volkov , Chris Pulley  and Kristi Holmes   Journal Journal
    of Clinical and Translational Science Published online: 16 May 2022 Maturity in
    enterprise data warehouses for research operations: Analysis of a pilot study
    Type Article Title Maturity in enterprise data warehouses for research operations:
    Analysis of a pilot study Authors Boyd M. Knosp , David A. Dorr  and Thomas R.
    Campion   Journal Journal of Clinical and Translational Science Published online:
    20 February 2023 Scale-up of the Accrual to Clinical Trials (ACT) network across
    the Clinical and Translational Science Award Consortium: a mixed-methods evaluation
    of the first 18 months Type Article Title Scale-up of the Accrual to Clinical
    Trials (ACT) network across the Clinical and Translational Science Award Consortium:
    a mixed-methods evaluation of the first 18 months Authors Elaine H. Morrato ,
    Lindsay A. Lennox , Elaina R. Sendro , Anne L. Schuster , Harold A. Pincus , Jennifer
    Humensky , Gary S. Firestein , Lee M. Nadler , Robert Toto  and Steven E. Reis   Journal
    Journal of Clinical and Translational Science Published online: 30 June 2020 The
    Evolve to Next-Gen ACT Network: An evolving open-access, real-world data resource
    primed for real-world evidence research across the Clinical and Translational
    Science Award Consortium Type Article Title The Evolve to Next-Gen ACT Network:
    An evolving open-access, real-world data resource primed for real-world evidence
    research across the Clinical and Translational Science Award Consortium Authors
    Elaine H. Morrato , Lindsay A. Lennox , James W. Dearing , Anne T. Coughlan ,
    Elaina S. Gano , Doug McFadden , Nallely Mora , Harold Alan Pincus , Gary S. Firestein
    , Robert Toto  and Steven E. Reis   Journal Journal of Clinical and Translational
    Science Published online: 29 September 2023 Leadership and administration to advance
    translational science: Environmental scan of adaptive capacity and preparedness
    of Clinical and Translational Science Award Program hubs Type Article Title Leadership
    and administration to advance translational science: Environmental scan of adaptive
    capacity and preparedness of Clinical and Translational Science Award Program
    hubs Authors Boris B. Volkov , Bart Ragon , Kristi Holmes , Elias Samuels , Anita
    Walden  and Keith Herzog   Journal Journal of Clinical and Translational Science
    Published online: 23 May 2022 Adaptive capacity and preparedness of Clinical and
    Translational Science Award Program hubs: Overview of an environmental scan Type
    Article Title Adaptive capacity and preparedness of Clinical and Translational
    Science Award Program hubs: Overview of an environmental scan Authors Boris B.
    Volkov , Bart Ragon , Jamie Mihoko Doyle  and Miriam A. Bredella   Journal Journal
    of Clinical and Translational Science Published online: 12 May 2022 Engaging community
    in the translational process: Environmental scan of adaptive capacity and preparedness
    of Clinical and Translational Science Award Program hubs Type Article Title Engaging
    community in the translational process: Environmental scan of adaptive capacity
    and preparedness of Clinical and Translational Science Award Program hubs Authors
    Boris B. Volkov , Verónica Hoyo  and Joe Hunt   Journal Journal of Clinical and
    Translational Science Published online: 15 June 2022 Integrating special and underserved
    populations in translational research: Environmental scan of adaptive capacity
    and preparedness of Clinical and Translational Science Award (CTSA) program hubs
    Type Article Title Integrating special and underserved populations in translational
    research: Environmental scan of adaptive capacity and preparedness of Clinical
    and Translational Science Award (CTSA) program hubs Authors Verónica Hoyo , Raj
    C. Shah , Gaurav Dave  and Boris B. Volkov   Journal Journal of Clinical and Translational
    Science Published online: 7 June 2022 Informatics education for translational
    research teams: An unrealized opportunity to strengthen the national research
    infrastructure Type Article Title Informatics education for translational research
    teams: An unrealized opportunity to strengthen the national research infrastructure
    Authors Eneida A. Mendonca , Rachel L. Richesson , Harry Hochheiser , Dan M. Cooper
    , Meg N. Bruck  and Eta S. Berner   Journal Journal of Clinical and Translational
    Science Published online: 28 October 2022 Librarians Authors Publishing partners
    Agents Corporates Accessibility Our blog News Contact and help Cambridge Core
    legal notices Feedback Sitemap Select your country preference Afghanistan Aland
    Islands Albania Algeria American Samoa Andorra Angola Anguilla Antarctica Antigua
    and Barbuda Argentina Armenia Aruba Australia Austria Azerbaijan Bahamas Bahrain
    Bangladesh Barbados Belarus Belgium Belize Benin Bermuda Bhutan Bolivia Bosnia
    and Herzegovina Botswana Bouvet Island Brazil British Indian Ocean Territory Brunei
    Darussalam Bulgaria Burkina Faso Burundi Cambodia Cameroon Canada Cape Verde Cayman
    Islands Central African Republic Chad Channel Islands, Isle of Man Chile China
    Christmas Island Cocos (Keeling) Islands Colombia Comoros Congo Congo, The Democratic
    Republic of the Cook Islands Costa Rica Cote D''Ivoire Croatia Cuba Cyprus Czech
    Republic Denmark Djibouti Dominica Dominican Republic East Timor Ecuador Egypt
    El Salvador Equatorial Guinea Eritrea Estonia Ethiopia Falkland Islands (Malvinas)
    Faroe Islands Fiji Finland France French Guiana French Polynesia French Southern
    Territories Gabon Gambia Georgia Germany Ghana Gibraltar Greece Greenland Grenada
    Guadeloupe Guam Guatemala Guernsey Guinea Guinea-bissau Guyana Haiti Heard and
    Mc Donald Islands Honduras Hong Kong Hungary Iceland India Indonesia Iran, Islamic
    Republic of Iraq Ireland Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya
    Kiribati Korea, Democratic People''s Republic of Korea, Republic of Kuwait Kyrgyzstan
    Lao People''s Democratic Republic Latvia Lebanon Lesotho Liberia Libyan Arab Jamahiriya
    Liechtenstein Lithuania Luxembourg Macau Macedonia Madagascar Malawi Malaysia
    Maldives Mali Malta Marshall Islands Martinique Mauritania Mauritius Mayotte Mexico
    Micronesia, Federated States of Moldova, Republic of Monaco Mongolia Montenegro
    Montserrat Morocco Mozambique Myanmar Namibia Nauru Nepal Netherlands Netherlands
    Antilles New Caledonia New Zealand Nicaragua Niger Nigeria Niue Norfolk Island
    Northern Mariana Islands Norway Oman Pakistan Palau Palestinian Territory, Occupied
    Panama Papua New Guinea Paraguay Peru Philippines Pitcairn Poland Portugal Puerto
    Rico Qatar Reunion Romania Russian Federation Rwanda Saint Kitts and Nevis Saint
    Lucia Saint Vincent and the Grenadines Samoa San Marino Sao Tome and Principe
    Saudi Arabia Senegal Serbia Seychelles Sierra Leone Singapore Slovakia Slovenia
    Solomon Islands Somalia South Africa South Georgia and the South Sandwich Islands
    Spain Sri Lanka St. Helena St. Pierre and Miquelon Sudan Suriname Svalbard and
    Jan Mayen Islands Swaziland Sweden Switzerland Syrian Arab Republic Taiwan Tajikistan
    Tanzania, United Republic of Thailand Togo Tokelau Tonga Trinidad and Tobago Tunisia
    Turkey Turkmenistan Turks and Caicos Islands Tuvalu Uganda Ukraine United Arab
    Emirates United Kingdom United States United States Minor Outlying Islands United
    States Virgin Islands Uruguay Uzbekistan Vanuatu Vatican City Venezuela Vietnam
    Virgin Islands (British) Wallis and Futuna Islands Western Sahara Yemen Zambia
    Zimbabwe Join us online Rights & Permissions Copyright Privacy Notice Terms of
    use Cookies Policy © Cambridge University Press 2024"'
  inline_citation: '>'
  journal: Journal of Clinical and Translational Science
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Development of a social and environmental determinants of health informatics
    maturity model
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Zulqarnain R.M.
  - Ma W.X.
  - Siddique I.
  - Ahmad H.
  - Askar S.
  citation_count: '0'
  description: The relationship between two variables is an essential factor in statistics,
    and the accuracy of the results depends on the data collected. However, the data
    collected for statistical analysis can be unclear and difficult to interpret.
    One way to predict how one variable will change about another is by using the
    correlation coefficient (CC), but this method is not commonly used in interval-valued
    Pythagorean fuzzy hypersoft set (IVPFHSS). The IVPFHSS is a more advanced and
    generalized form of the Pythagorean fuzzy hypersoft set (PFHSS), which allows
    for more precise and accurate analysis. In this research, we introduce the correlation
    coefficient (CC) and weighted correlation coefficient (WCC) for IVPFHSS and their
    essential properties. To demonstrate the applicability of these measures, we use
    the COVID-19 pandemic as an example and establish a prioritization technique for
    order preference by similarity to the ideal solution (TOPSIS) model. The technique
    is used to study the problem of optimizing the allocation of hospital beds during
    the pandemic. This study provides insights into the importance of utilizing correlation
    measures for decision-making in uncertain and complex situations like the COVID-19
    pandemic. It is a robust multi-attribute decision-making (MADM) methodology with
    significant importance. Subsequently, it is planned to increase a dynamic bed
    allocation algorithm based on biogeography to accomplish the superlative decision-making
    system. Moreover, numerical investigations deliberate the best decision structures
    and deliver sensitivity analyses. The efficiency of our encouraged algorithm is
    more consistent than prevalent models, and it can effectively control and determine
    the optimal configurations for the study.
  doi: 10.1038/s41598-024-53923-2
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    01 April 2024 A fair bed allocation during COVID-19 pandemic using TOPSIS technique
    based on correlation coefficient for interval-valued pythagorean fuzzy hypersoft
    set Rana Muhammad Zulqarnain, Wen-Xiu Ma, Imran Siddique, Hijaz Ahmad & Sameh
    Askar  Scientific Reports  14, Article number: 7678 (2024) Cite this article 66
    Accesses Metrics Abstract The relationship between two variables is an essential
    factor in statistics, and the accuracy of the results depends on the data collected.
    However, the data collected for statistical analysis can be unclear and difficult
    to interpret. One way to predict how one variable will change about another is
    by using the correlation coefficient (CC), but this method is not commonly used
    in interval-valued Pythagorean fuzzy hypersoft set (IVPFHSS). The IVPFHSS is a
    more advanced and generalized form of the Pythagorean fuzzy hypersoft set (PFHSS),
    which allows for more precise and accurate analysis. In this research, we introduce
    the correlation coefficient (CC) and weighted correlation coefficient (WCC) for
    IVPFHSS and their essential properties. To demonstrate the applicability of these
    measures, we use the COVID-19 pandemic as an example and establish a prioritization
    technique for order preference by similarity to the ideal solution (TOPSIS) model.
    The technique is used to study the problem of optimizing the allocation of hospital
    beds during the pandemic. This study provides insights into the importance of
    utilizing correlation measures for decision-making in uncertain and complex situations
    like the COVID-19 pandemic. It is a robust multi-attribute decision-making (MADM)
    methodology with significant importance. Subsequently, it is planned to increase
    a dynamic bed allocation algorithm based on biogeography to accomplish the superlative
    decision-making system. Moreover, numerical investigations deliberate the best
    decision structures and deliver sensitivity analyses. The efficiency of our encouraged
    algorithm is more consistent than prevalent models, and it can effectively control
    and determine the optimal configurations for the study. Similar content being
    viewed by others Global prediction of extreme floods in ungauged watersheds Article
    Open access 20 March 2024 An overview of clinical decision support systems: benefits,
    risks, and strategies for success Article Open access 06 February 2020 AI in health
    and medicine Article 20 January 2022 Introduction A correlation coefficient is
    a statistical measure that is commonly used to evaluate the degree of relationship
    between two variables. In industrial engineering, correlation coefficients are
    commonly used to analyze the relationships between different factors that affect
    the performance of a system. For example, in manufacturing, a CC can be used to
    determine how the machine speed, the number of workers, or other factors affect
    the production rate. However, using probabilistic methods to analyze real-world
    industrial problems can be challenging due to the large amount of arbitrary data
    involved. Complex systems often have imprecise uncertainties that make it difficult
    to obtain accurate probability estimates. In addition, there may be insufficient
    data to correct the information obtained from these methods. So, outcomes based
    on probability models are not always helpful to professionals. To overcome these
    limitations, multiple attribute decision-making (MADM) has been identified as
    a more effective way to make appropriate decisions when uncertainties and imperfections
    are present. To report these issues, Zadeh1 anticipated the concept of fuzzy sets
    (FS), which can help condense redundant and apprehensive data. Turksen2 also offered
    the idea of interval value fuzzy sets (IVFS), which represent membership values
    as intervals rather than numbers, making them better suited for describing uncertainty
    in complex systems. Using an IVFS in fuzzy control is important to make accurate
    decisions in complex situations. The fuzzy TOPSIS approach was established by
    Chen3 to find the closeness coefficient, which was further prolonged by Ashtiani
    et al.4 to determine multi-criteria decision-making (MCDM) challenges in the IVFS
    setting considering the membership degrees (MD). However, traditional FS and IVFS
    have limitations in handling non-membership degrees (NMD) in decision-making (DM)
    assessments. Atanassov5 protracted the intuitionistic fuzzy sets (IFS) contracting
    theory with the abovementioned drawbacks. The TOPSIS method has been used in green
    supply chain management for IFS by Rouyendegh et al.6. Atanassov7 extended his
    IFS theory to IVIFS. Several techniques for computing the CC of IFS and IVIFS
    were developed by Hung and Wu8, Bustince and Burillo9, and Mitchell10. The IVIFS
    TOPSIS has been utilized for supplier selection by Tiwari et al.11. The Pythagorean
    fuzzy set (PFS) was created by Yager12 to address the shortcomings of existing
    FS theories in handling inconsistent and uncertain data. Yager improved upon the
    basic condition \\({\\mathcal{T}} + {\\mathcal{J}} \\le 1\\) by revising it to
    \\({\\mathcal{T}}^{2} + {\\mathcal{J}}^{2} \\le 1\\) to correct errors in the
    system. Since then, other researchers have built upon Yager''s work in PFS theory.
    Biswas & Sarkar13 proposed the TOPSIS model for PFS to resolve multi-criteria
    group decision-making (MCGDM) complexities. Einstein-weighted geometric aggregation
    operators (AOs) for multi-attribute group decision-making (MAGDM). In recent years,
    Pythagorean fuzzy sets (PFS) have emerged as a powerful tool in DM. In this context,
    Wei and Lu14 proposed using power AOs to solve MADM problems in PFS. Similarly,
    Wang and Li15 investigated using power Bonferroni mean operators with Pythagorean
    fuzzy numbers (PFNs) to explore their interaction and potential applications in
    DM. Zhang16 proposed a DM method that uses similarity measures to address multi-criteria
    group decision-making (MCGDM) obstacles in PFS scenarios. Peng and Yang17 extended
    the PFS theory to include interval-valued Pythagorean fuzzy sets (IVPFS), which
    allowed for the development of a DM system based on their method. Garg18 presented
    a novel score function for IVPFS and proposed the TOPSIS technique using his developed
    function. Despite these advancements, there are still limitations in dealing with
    uncertainties and vagueness in parametric chemistry. Nonetheless, PFS theory and
    its extensions have proved useful tools for DM in various fields. Existing methods
    often struggle with handling uncertainties and vagueness in parametric chemistry,
    which can be ambiguous, obscure, or equivocal. Researchers have been exploring
    new mathematical tools and theories to overcome these limitations. One of these
    is the soft set (SS) introduced by Molodtsov19. This general mathematical tool
    has proven useful in addressing vague or uncertain problems. Maji et al.20 combined
    FS and SS to create fuzzy soft sets (FSS) to build upon the SS theory. Maji et
    al.21 extended this intuitionistic fuzzy soft set (IFSS) with essential operations
    and properties. In addition to these developments, Garg and Arora22 proposed a
    leeway of the TOPSIS method based on correlation coefficients (CC). This extension
    addressed the complexities involved in MADM. Jiang et al.23 introduced an extension
    to the IFSS called interval-valued IFSS (IVIFSS). They presented fundamental operations
    for IVIFSS to address uncertainties and vagueness in DM problems. Ma et al.24
    utilized the choice and score values of IVIFSS and proposed an innovative DM method
    to handle complex problems that involve multiple criteria. The proposed method
    enables decision-makers to consider the importance of criteria and the alternatives''
    performances more efficiently. Khan et al.25 developed MADM approaches for generalized
    IVIFSS, which includes basic operations such as union, intersection, and complement,
    to address realistic complications. The proposed approach considers both the preference
    and non-preference information and provides a more comprehensive evaluation of
    alternatives. To deal with MADM issues in IVIFSS, Zulqarnain et al.26 established
    the TOPSIS and AOs for IVIFSS. The proposed approach enables decision-makers to
    handle complex DM problems involving multiple attributes, and the AOs provide
    flexibility in selecting the appropriate operator based on the problem''s nature.
    Garg and Arora27 offered a nonlinear programming method to determine MADM convolutions
    under IVIFSS, considering the weight of each criterion and the degree of importance
    of each alternative''s performance in a more effective way. The proposed method
    provides an optimal solution that considers the uncertainties and vagueness of
    the DM problem. Researchers have been exploring new mathematical tools and theories
    to address the limitations of existing methods in handling uncertainties and vagueness
    in parametric chemistry. One such tool is the Pythagorean fuzzy soft set (PFSS),
    which combines two existing theories, PFS and SS. Peng et al.28 proposed Pythagorean
    fuzzy soft sets (PFSSs), which combine PFS and SS and possess desirable characteristics.
    Athira et al.29 developed and used entropy and distance measures for PFSSs in
    DM. They also introduced new similarity and distance measures30 for PFSS that
    offer greater flexibility than IFSS or FSS. In addition to these advancements,
    several extensions have been made to PFSS. For instance, Riaz et al.31,32 developed
    m polar PFSS, while Hua et al.33 extended PFSS to possibility PFSS. Zulqarnain
    et al.34 extended the Einstein operational laws for PFSS and introduced the Einstein-ordered
    weighted ordered geometric AO for PFSS, which was utilized to establish a MAGDM
    technique. They also proposed the Einstein-ordered weighted average AO for PFSS35
    and applied CC to settle the TOPSIS method for PFSS36. All of these advancements
    offer promising avenues for addressing the challenges posed by uncertainties and
    vagueness in parametric chemistry, and they demonstrate the vast potential of
    these tools and theories for solving multifaceted, realistic complications. Zulqarnain
    et al.37 settled AOs for IVPFSS and presented a MAGDM approach for solving real-world
    problems. When dealing with DM problems, models based on SS settings have proven
    useful due to their simplicity, as they only require single-parameter assessments.
    However, in some situations, dividing parameters into more specific subcategories
    may be necessary. This is where hypersoft sets (HSS)38 come into play, allowing
    for incorporating multiple sub-parameters into DM. While SS models may be sufficient
    in certain scenarios, they cannot differentiate parameters into subcategories.
    In several DM situations, it is necessary to consider parameters as sub-parameters
    to obtain more accurate results. Incorporating HSS into the DM process makes creating
    more comprehensive models considering multiple sub-parameters possible, resulting
    in a more accurate decision assessment. There are various approaches to HSS, each
    with corresponding DM methods. For example, Rahman et al.39 introduced the possibility
    IFHSS and established DM methods using similarity measures. Rahman et al.40 demonstrated
    a DM methodology for neutrosophic HSS. Zulqarnain et al.41 extended the TOPSIS
    for IFHSS and the DM methodologies using the AOs. Debnath42 introduced the basic
    operations for the interval-valued intuitionistic fuzzy hypersoft set (IVIFHSS)
    to regulate MCDM obstacles. The correlation-based TOPSIS method is introduced
    in43 to choose the most appropriate thermal energy storage technique. Zulqarnain
    et al.44 extended IFHSS to PFHSS and presented the fundamental operations. Zulqarnain
    et al.45 raised the AOs in the IVPFHSS setting and demonstrated an MCDM technique
    to resolve DM complications. Recently, the spread of major transmissible viruses
    has posed a main risk to worldwide human health, life safety, and monetary evolution.
    The COVID-19 pandemic has exaggerated the world, with millions of inveterate cases
    and demises reported by the World Health Organization. The rapid spread of the
    virus has led to a shortage of hospital beds, making it difficult to provide proper
    care for both COVID-19 patients and non-COVID-19 patients. This has increased
    the threat of infection transmission and persistent decease, highlighting the
    urgent need for hospital administrators to allocate beds properly. In response
    to this need, our work aims to develop an efficient and effective bed allocation
    technique during the COVID-19 pandemic to ensure that hospitals can adequately
    treat patients while preventing the virus''s spread. Due to the time-varying and
    highly uncertain needs of hospital resource requirements, hospital managers face
    the challenge of balancing the limited assets of dissimilar kinds of zones and
    patients. Research on cross-infection prevention in infectious disease hospitalizations
    in Singapore and Italy has focused on three categories: placing patients at risk
    of COVID-19 in isolation beds to manage isolated patients46,47. In response to
    the COVID-19 pandemic, hospitals have implemented various measures to prevent
    the spread of the virus and ensure patient safety. One such measure is providing
    each patient with personal protective equipment (PPE) upon admission. This is
    important in preventing the transmission of the virus from infected patients to
    others in the hospital. In addition, various hospitals have set up bumper zones
    in reserve rooms, passable rooms, or common wards48. These buffer wards are used
    to separate new patients hospitalized for a specific period and screen COVID-19
    patients to prevent the spread of the virus. This methodology has been implemented
    in countries like Egypt and China49,50 and has proven effective in reducing the
    risk of cross-infection and improving patient outcomes. But, the first preference
    can clue to a severe deficiency of isolation beds, and the second route, despite
    the endowment of surplus particular protecting equipment to hospitalized patients,
    can quiet chief to nosocomial infection. On the contrary, asymptomatic and latent
    COVID-19 patients can be successfully recognized by patients'' opinions in a buffer
    ward. During the COVID-19 pandemic, there has been a surge in demand for hospital
    services, leading to a strain on resources and medical staff. To manage this situation,
    hospitals have implemented various measures to prioritize COVID-19 patients while
    ensuring that non-COVID-19 patients receive timely and appropriate treatment.
    One such measure is using buffer wards, designated areas within hospitals where
    patients are monitored and treated for a specific period. To improve DM processes
    and address flaws, an integrated approach is needed. However, spies may be hesitant
    to share information due to concerns about security and confidentiality. This
    can lead to incomplete or asymmetrical information, further complicating the DM
    process. Considering all available information is emphasized to tackle this issue,
    and the concept of IVPFHSS was introduced. IVPFHSS is a mathematical tool that
    enables decision-makers to account for uncertainty and vagueness in the data and
    make informed decisions by considering the full range of possibilities. By using
    IVPFHSS, decision-makers can consider a wide range of factors and assess the importance
    of each in the overall DM process. This helps create a comprehensive structure
    for DM that considers all relevant information and provides a more accurate picture
    of the situation. Research motivation IVPFHSS sets incorporate higher levels of
    uncertainty and frequently encourage a higher degree of vagueness. To effectively
    deal with convoluted inconsistency during human cognitive and DM projects, the
    effective TOPSIS approach, which has formerly concentrated on particular contexts
    and fuzzy settings, needs to be extended to the IVPFHSS environment. As the TOPSIS
    strategy is a highly competent method for handling DM procedures, it has gained
    limited application in medical DM and healthcare facilities. IVPFHSS is a tool
    that utilizes HSS and IVPFS to assist in managing inconsistencies, variations,
    and inaccurate data. TOPSIS plays an important role in DM challenges for integrating
    disparate aims into an organized assessment. On the other hand, the currently
    available TOPSIS strategy has limitations in that it cannot measure composite
    interval-valued Pythagorean fuzzy hypersoft numbers (IVPFHSNs). The CC and WCC
    have been modified to reflect the specific circumstances of IVPFHSS better. Because
    of this improvement, the model suggested has grown better than conventional TOPSIS
    methodologies. Still, the previous approach’s outcomes are unfavourable, and assessing
    the bias toward the more favourable model can be complicated. Prior studies showed
    that the TOPSIS strategy, particularly26, can’t be sufficiently flexible for delivering
    beneficial concepts and reliable outcomes since it cannot identify sub-attributes
    of a possible solution, reducing its efficiency. Furthermore, the TOPSIS approach
    described in43 can understand various alternative sub-attributes, making it more
    flexible and productive. Moreover, the model does not differentiate between different
    MD (NMD) functionality levels in the overall development. Consequently, there
    is a bias towards the alternatives, making it challenging to determine their suitability
    partiality. TOPSIS will also be developed for MADM obstacles, employing the correlation
    measures produced to resolve these obstacles. The suggested strategy improvement
    has culminated in a more pragmatic methodology for navigating interval form alternatives
    in the IVPFHSS area. It is because of alterations performed to the CC and WCC
    that enables the framework to become more adaptable and more capable of delivering
    reliable outcomes. Despite these developments, the present approach continues
    to face difficulties in establishing partiality for the different approaches,
    and the outcomes are not always favourable. The correlation coefficient, a well-known
    statistical measure, relates to IVPFHSS information in this research. It is an
    innovative utilization of the correlation coefficient that has never been explored
    in this setting. This study enriches our comprehension of the interactions among
    several factors in IVPFHSS information by employing this measure. In contrast,
    our suggested TOPSIS approach takes into account sub-attributes in interval form
    and can manage a large number of alternative sub-attributes. The suggested approach
    performs MADM complications more effectively and equitably. Main contributions
    The most recent decision-making (DM) research emphasized the significance of integrating
    unpredictability and insufficient information during DM procedures. It is due
    to real-world issues commonly including insufficient or undetermined information,
    keeping sound choices impossible. A significant approach for overcoming such obstacles
    is IVPFHSS, which integrates the favourable aspects of both HSS and IVPFS. The
    frequently applied CC measure can not specifically mimic the viable assessment
    of alternatives in DM protocols utilizing IVPFHSS since it fails to interpret
    for assured aspects. IVPFHSS is a systematic technique for dealing with hesitancy,
    discrepancies, and incomplete data. To overcome this issue, this study aims to
    develop new CC and weighted CC (WCC) metrics based on imprecise data in the context
    of IVPFHSS. The following are the study''s objectives: One of the major achievements
    of this investigation includes a structure for examining the informational energies
    that comprise IVPFHSS circumstances. These energies indicate how much knowledge
    is contained within an FS, and determining these individuals is important to developing
    appropriate correlation measures. The researchers used informational energies
    and correlation measures to establish the CC and WCC measures stated in this research
    have been optimized for IVPFHSS. Within the IVPFHSS structure, these measures
    are intended to measure the associations among various characteristics or parameters
    in a more precise way. This innovation expands the available statistical approaches
    for interpreting and analyzing IVPFHSS information. The requirements mentioned
    above consider the insufficient information in IVPFHSS to deliver an improved
    exact assessment of the real significance of substitutes in DM techniques. Based
    on the presented correlation measures (CC and WCC), this study constructs a TOPSIS
    technique using IVPFHSS data. This distinctive strategy helps decision-makers
    examine and prioritize alternatives in MADM challenges properly. The combination
    of TOPSIS and the suggested correlation measures boosts the decision-making procedure''s
    precision as well as dependability. This study demonstrates that integrating the
    correlation measures and TOPSIS in the proposed method contributed to a reliable
    MADM strategy. This technique offers significant benefits by offering decision-makers
    an entire structure for evaluating and assigning priority to prospects in the
    environment of IVPFHSS. This strategy''s adaptability promotes the reliability
    as well as the efficacy of decision-making procedures in several kinds of fields.
    Moreover, we utilized it in evaluating DM distraction and bed allocation during
    covid 19 pandemic, and picking the most practical for benchmarking. Comparative
    analyses have been presented to ensure the feasibility of the developed TOPSIS
    methodology, demonstrating that the suggested method is more productive and effective
    than existing TOPSIS methodologies. This study seeks to add to the field of DM
    under uncertainty by providing these new techniques and supporting decision-makers
    to make better-informed and more precise choices. In Section “Introduction”, the
    introduction provides an overview of the need for a new TOPSIS method for handling
    uncertainty and incomplete information. Section “Preliminaries” outlines the basic
    concepts that support the research, including an overview of IVPFHSS and the existing
    TOPSIS technique. Section “Correlation coefficient for interval valued pythagorean
    fuzzy hypersoft set” focuses on introducing informational energies for IVPFHSS
    and developing CC correlation measures with significant properties. In section
    “Weighted correlation coefficient for interval valued pythagorean fuzzy hypersoft
    set”, the WCC correlation measure is introduced, along with its crucial properties.
    In section “Proposed TOPSIS approach based on correlation coefficient to resolve
    MADM problem under IVPFHSS”, the paper explains how the TOPSIS technique has been
    developed using newly developed correlation measures to solve multiple attribute
    decision-making (MADM) problems. This section details the methodology employed
    to develop the TOPSIS technique and how it addresses the limitations of the existing
    MADM techniques. Section “I. Challenges in implementing fair bed allocation policies
    during the COVID-19 Pandemic” presents a numerical exploration that aims to determine
    the bed allocation for the most critical patients of COVID-19. Finally, section
    “Discussion and comparative analysis” provides a comparative analysis of the proposed
    model to ensure its pragmatism. The section compares the performance of the proposed
    model with other existing models and highlights the advantages of the proposed
    model. The comparative analysis also includes a discussion of the limitations
    of the proposed model and potential avenues for future research. Preliminaries
    This section will discuss some basic concepts necessary to understand our research’s
    structure and organization. These concepts will provide a foundation for the topics
    and ideas presented later in the paper. By recalling these fundamental notions,
    we hope to provide a clear and concise overview of the background and context
    of our research. Definition 1 17 An interval-valued Pythagorean fuzzy set \\({\\mathfrak{T}}\\)
    over a universe of discourse \\({U}\\), such as \\({\\mathfrak{T}} \\subseteq
    {U}\\) it is defined as $${\\mathfrak{T}} = \\left\\{ {\\left. { \\left( {{\\mathfrak{u}}_{i},\\left(
    {{\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }} \\left( {{\\mathfrak{u}}_{i} } \\right),
    {\\mathcal{J}}_{{{\\mathfrak{T}}_{j} }} \\left( {{\\mathfrak{u}}_{i} } \\right)}
    \\right)} \\right)} \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}$$ where,
    \\({\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }} \\left( {{\\mathfrak{u}}_{i} } \\right)
    = \\left[ {{\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i}
    } \\right),{ }{\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right)} \\right]\\) and \\({\\mathcal{J}}_{{{\\mathfrak{T}}_{j} }} \\left(
    {{\\mathfrak{u}}_{i} } \\right) = \\left[ {{\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]\\) be the MD
    and NMD intervals. Also, \\(\\left[ {{\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathfrak{T}}_{j}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right] \\subseteq \\left[
    {0, 1} \\right]\\) and \\(\\left[ {{\\mathcal{J}}_{{{\\mathfrak{T}}_{j} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right] \\subseteq \\left[
    {0, 1} \\right]\\), \\(0 \\le {\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathfrak{T}}_{j}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right), {\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) \\le 1\\), such as \\(0
    \\le \\left( {{\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{{\\mathfrak{T}}_{j} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} \\le 1\\). Definition
    2 19 A soft set over a universe of discourse \\({U}\\) and a set of attributes
    \\(\\varsigma\\) is a pair \\(\\left( {\\Lambda,{\\mathfrak{T}}} \\right)\\) where
    \\(\\Lambda\\) is a mapping from \\({\\mathfrak{T}}\\) to the power set of \\({U}\\),
    denoted as \\(P\\left( {U} \\right)\\) and defined as follows: \\(\\left( {\\Lambda
    ,{\\mathfrak{T}}} \\right) = \\left\\{ {\\Lambda \\left( \\varsigma \\right) \\in
    {\\mathcal{P}}\\left( {U} \\right):\\varsigma \\in \\varsigma,\\Lambda \\left(
    \\varsigma \\right) = \\emptyset if \\varsigma \\notin {\\mathfrak{T}}} \\right\\}\\)
    Definition 3 28 Let \\({U}\\) and \\(\\varsigma\\) be a universe of discourse
    and a set of attributes. Then, the Pythagorean fuzzy soft set over \\({U}\\) is
    defined as follows: $${\\mathfrak{T}}_{{{\\mathfrak{u}}_{i} }} \\left( {\\varsigma_{j}
    } \\right){ } = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {{\\mathcal{T}}_{{{\\mathfrak{T}}_{j}
    }} \\left( {{\\mathfrak{u}}_{i} } \\right), {\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }} \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)} \\right)} \\right| {\\mathfrak{u}}_{i}
    \\in {U}} \\right\\}$$ where \\({\\mathfrak{T}}\\) be a mapping such as \\({\\mathfrak{T}}:\\varsigma
    \\to P^{{U}}\\), \\(P^{{U}}\\) shows the subsets of Pythagorean fuzzy soft sets
    \\(\\forall {\\mathfrak{u}}_{i} \\in {U}\\) and satisfied the following conditions
    such as: \\({\\mathcal{T}}_{{{\\mathfrak{T}}_{j} }} \\left( {{\\mathfrak{u}}_{i}
    } \\right), {\\mathcal{J}}_{{{\\mathfrak{T}}_{j} }} \\left( {{\\mathfrak{u}}_{i}
    } \\right) \\in \\left[ {0, 1} \\right]\\) and \\(0 \\le \\left( {{\\mathcal{T}}_{{{\\mathfrak{T}}_{j}
    }} \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{{\\mathfrak{T}}_{j}
    }} \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} \\le 1\\). Definition
    4 38 Suppose we have a universe of discourse, denoted by \\({U}\\), which contains
    \\(n\\) elements \\(\\left\\{ {{\\mathfrak{u}}_{1},{\\mathfrak{u}}_{2},{\\mathfrak{u}}_{3},\\ldots,{\\mathfrak{u}}_{n}
    } \\right\\}\\), \\(\\left( {n \\ge 1} \\right)\\). We also have a set of attributes,
    denoted by \\(\\varsigma\\), which contains \\(m\\) attributes \\(\\left\\{ {\\varsigma_{1}
    ,{ }\\varsigma_{2} ,{ }\\varsigma_{3} ,{ } \\ldots ,{ }\\varsigma_{m} } \\right\\}\\)
    indicates the set of parameters and \\({\\mathfrak{T}}_{i}\\) be the conforming
    sub-parameters, such as \\({\\mathfrak{T}}_{i} \\cap {\\mathfrak{T}}_{j} = {{\\varphi
    }}\\), where \\(i \\ne j\\) and \\(i, j \\in \\left\\{ {1,2,3 \\ldots n} \\right\\}\\).
    Let \\({\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3}
    \\times \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    = \\left\\{ {\\left. {\\left( {\\varsigma_{{1j_{1} }},\\varsigma_{{2j_{2} }},\\ldots,\\varsigma_{{nj_{n}
    }} } \\right)} \\right|\\varsigma_{{1j_{1} }} \\in {\\mathfrak{T}}_{1} ,\\varsigma_{{2j_{2}
    }} \\in {\\mathfrak{T}}_{2},\\ldots,\\varsigma_{{nj_{n} }} \\in {\\mathfrak{T}}_{n}
    } \\right\\}\\) be a collection of multi-sub-attributes, where \\(1 \\le j_{1}
    \\le \\alpha\\), \\(1 \\le j_{2} \\le \\beta\\), and \\(1 \\le j_{n} \\le \\gamma\\),
    and \\(\\alpha ,\\beta ,\\gamma \\in {\\text{N}}\\). A pair \\(\\left( {\\Lambda
    ,{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = \\left( {\\Lambda, {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    } \\right)} \\right)\\) is entitled as HSS, and its mapping is defined as: \\(\\Lambda
    :{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    \\to {\\mathcal{P}}\\left( {U} \\right)\\). Also, it can be defined as. \\(\\left(
    {\\Lambda, {\\mathop {\\mathfrak{T}}\\limits^{{...}}} } \\right)\\) = \\(\\left\\{
    {\\widehat{{\\varsigma_{ij} }}, \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    }} \\left( {\\widehat{{\\varsigma_{ij} }}} \\right): \\widehat{{\\varsigma_{ij}
    }} \\in {\\mathop {\\mathfrak{T}}\\limits^{{...}}} , \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    }} \\left( {\\widehat{{\\varsigma_{ij} }}} \\right) \\in {\\mathcal{P}}\\left(
    {U} \\right)} \\right\\}\\). Definition 5 42 Suppose we have a universe of discourse,
    denoted by \\({U}\\), which contains \\(n\\) elements \\(\\left\\{ {{\\mathfrak{u}}_{1},{\\mathfrak{u}}_{2},{\\mathfrak{u}}_{3},\\ldots,{\\mathfrak{u}}_{n}
    } \\right\\}\\), \\(\\left( {n \\ge 1} \\right)\\). We also have a set of attributes,
    denoted by \\(\\varsigma\\), which contains \\(m\\) attributes \\(\\left\\{ {\\varsigma_{1}
    ,{ }\\varsigma_{2} ,{ }\\varsigma_{3} ,{ } \\ldots ,{ }\\varsigma_{m} } \\right\\}\\)
    denotes the set of attributes and \\({\\mathfrak{T}}_{i}\\) be the conforming
    sub-attributes, such as \\({\\mathfrak{T}}_{i} \\cap {\\mathfrak{T}}_{j} = {{\\varphi
    }}\\), where \\(i \\ne j\\) and \\(i, j \\in \\left\\{ {1,2,3 \\ldots n} \\right\\}\\).
    Let \\({\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3}
    \\times \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    = \\left\\{ {\\left. {\\left( {\\varsigma_{{1j_{1} }},\\varsigma_{{2j_{2} }},\\ldots,\\varsigma_{{nj_{n}
    }} } \\right)} \\right|\\varsigma_{{1j_{1} }} \\in {\\mathfrak{T}}_{1} ,\\varsigma_{{2j_{2}
    }} \\in {\\mathfrak{T}}_{2},\\ldots,\\varsigma_{{nj_{n} }} \\in {\\mathfrak{T}}_{n}
    } \\right\\}\\) be a collection of multi-sub-attributes, where \\(1 \\le j_{1}
    \\le \\alpha\\), \\(1 \\le j_{2} \\le \\beta\\), and \\(1 \\le j_{n} \\le \\gamma\\),
    and \\(\\alpha ,\\beta ,\\gamma \\in {\\text{N}}\\). A pair \\(\\left( {\\Lambda
    ,{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = \\left( {\\Lambda, {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    } \\right)} \\right)\\) is entitled an IVIFHSS, and its mapping is defined as:
    \\(\\Lambda :{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3}
    \\times \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    \\to IVIFS^{U}\\). Also, it can be defined as \\(\\left( {\\Lambda, {\\mathop
    {\\mathfrak{T}}\\limits^{{...}}} } \\right) = \\left\\{ {\\left( {\\widehat{{\\varsigma_{ij}
    }}, \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right)} \\right): \\widehat{{\\varsigma_{ij} }} \\in {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    , \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right) \\in IVIFS^{U} \\in \\left[ {0, 1} \\right]} \\right\\}\\), where
    \\(\\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right) = \\left\\{ {{\\mathfrak{u}}, \\left( { {\\mathcal{T}}_{{\\Lambda
    \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}} \\left( {\\mathfrak{u}} \\right),{
    }{\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right){ }} \\right): {\\mathfrak{u}} \\in {U}} \\right\\}\\),
    and \\({\\mathcal{T}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right) = \\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]\\) and
    \\({\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right) = \\left[ {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]\\) represents
    the MD and NMD, respectively, and \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right), {\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) \\in \\left[ {0,
    1} \\right]\\), such as \\(0 \\le {\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) + {\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) \\le 1\\). The IVIFHSN
    can be written as \\(\\Lambda = \\left\\{ {\\left( { \\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right],{ }\\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right\\}\\). Definition 6 44 Suppose we have a universe of discourse, denoted
    by \\({U}\\), which contains \\(n\\) elements \\(\\left\\{ {{\\mathfrak{u}}_{1},{\\mathfrak{u}}_{2},{\\mathfrak{u}}_{3},\\ldots,{\\mathfrak{u}}_{n}
    } \\right\\}\\), \\(\\left( {n \\ge 1} \\right)\\). We also have a set of attributes,
    denoted by \\(\\varsigma\\), which contains \\(m\\) attributes \\(\\left\\{ {\\varsigma_{1}
    ,{ }\\varsigma_{2} ,{ }\\varsigma_{3} ,{ } \\ldots ,{ }\\varsigma_{m} } \\right\\}\\)
    denotes the set of attributes and \\({\\mathfrak{T}}_{i}\\) be the conforming
    sub-attributes, such as \\({\\mathfrak{T}}_{i} \\cap {\\mathfrak{T}}_{j} = {{\\varphi
    }}\\), where \\(i \\ne j\\) and \\(i, j \\in \\left\\{ {1,2,3 \\ldots n} \\right\\}\\).
    Let \\({\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3}
    \\times \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    = \\left\\{ {\\left. {\\left( {\\varsigma_{{1j_{1} }},\\varsigma_{{2j_{2} }},\\ldots,\\varsigma_{{nj_{n}
    }} } \\right)} \\right|\\varsigma_{{1j_{1} }} \\in {\\mathfrak{T}}_{1},\\varsigma_{{2j_{2}
    }} \\in {\\mathfrak{T}}_{2},\\ldots,\\varsigma_{{nj_{n} }} \\in {\\mathfrak{T}}_{n}
    } \\right\\}\\) be a collection of multi-sub-attributes, where \\(1 \\le j_{1}
    \\le \\alpha\\), \\(1 \\le j_{2} \\le \\beta\\), and \\(1 \\le j_{n} \\le \\gamma\\),
    and \\(\\alpha ,\\beta ,\\gamma \\in {\\text{N}}\\). A pair \\(\\left( {\\Lambda
    ,{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = \\left( {\\Lambda, {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    } \\right)} \\right)\\) is entitled as PFHSS, and its mapping is defined as: \\(\\Lambda
    :{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    \\to PFHS^{U}\\). Also, it can be defined as \\(\\left( {\\Lambda, {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    } \\right) = \\left\\{ {\\left( {\\widehat{{\\varsigma_{ij} }}, \\Lambda_{{ {\\mathop
    {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}
    \\right): \\widehat{{\\varsigma_{ij} }} \\in {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    , \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right) \\in PFS^{U} \\in \\left[ {0, 1} \\right]} \\right\\}\\), where \\(\\Lambda
    _{{\\mathop {\\mathfrak{T}}\\limits^{{...}}}} \\left( {\\widehat{{\\varsigma _{{ij}}
    }}} \\right) = \\left\\{ {\\left\\langle {{\\mathfrak{u}},~{\\mathcal{T}}_{{\\Lambda
    \\left( {\\widehat{{\\varsigma _{{ij}} }}} \\right)}} \\left( {\\mathfrak{u}}
    \\right),~{\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma _{{ij}} }}}
    \\right)}} \\left( {\\mathfrak{u}} \\right)} \\right\\rangle :~{\\mathfrak{u}}
    \\in {U}} \\right\\}\\), where \\({\\mathcal{T}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right)}} \\left( {\\mathfrak{u}} \\right)\\) and \\({\\mathcal{J}}_{{\\Lambda
    \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}} \\left( {\\mathfrak{u}} \\right)\\)
    represents the MD and NMD, respectively, such as \\({\\mathcal{T}}_{{\\Lambda
    \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}} \\left( {\\mathfrak{u}} \\right)\\),
    \\({\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right) \\in \\left[ {0, 1} \\right]\\), and \\(0 \\le
    \\left( {{\\mathcal{T}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{\\Lambda
    \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}} \\left( {\\mathfrak{u}} \\right)}
    \\right)^{2} \\le 1\\). The PFHSN can be specified as \\(\\Lambda = \\left\\{
    {\\left( { {\\mathcal{T}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right),{ }{\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right)}} \\left( {\\mathfrak{u}} \\right){ }} \\right)} \\right\\}\\). Definition
    7 45 Suppose we have a universe of discourse, denoted by \\({U}\\), which contains
    \\(n\\) elements \\(\\left\\{ {{\\mathfrak{u}}_{1},{\\mathfrak{u}}_{2},{\\mathfrak{u}}_{3},\\ldots,{\\mathfrak{u}}_{n}
    } \\right\\}\\), \\(\\left( {n \\ge 1} \\right)\\). We also have a set of attributes,
    denoted by \\(\\varsigma\\), which contains \\(m\\) attributes \\(\\left\\{ {\\varsigma_{1}
    ,{ }\\varsigma_{2} ,\\varsigma_{3} ,{ } \\ldots ,{ }\\varsigma_{m} } \\right\\}\\)
    indicates the set of parameters and \\({\\mathfrak{T}}_{i}\\) be the conforming
    sub-parameters, such as \\({\\mathfrak{T}}_{i} \\cap {\\mathfrak{T}}_{j} = {{\\varphi
    }}\\), where \\(i \\ne j\\) and \\(i, j \\in \\left\\{ {1,2,3 \\ldots n} \\right\\}\\).
    Let \\({\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3}
    \\times \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    = \\left\\{ {\\left. {\\left( {\\varsigma_{{1j_{1} }} ,\\varsigma_{{2j_{2} }},\\ldots,\\varsigma_{{nj_{n}
    }} } \\right)} \\right|\\varsigma_{{1j_{1} }} \\in {\\mathfrak{T}}_{1},\\varsigma_{{2j_{2}
    }} \\in {\\mathfrak{T}}_{2},\\ldots,\\varsigma_{{nj_{n} }} \\in {\\mathfrak{T}}_{n}
    } \\right\\}\\) be a collection of multi-sub-attributes, where \\(1 \\le j_{1}
    \\le \\alpha\\), \\(1 \\le j_{2} \\le \\beta\\), and \\(1 \\le j_{n} \\le \\gamma\\),
    and \\(\\alpha ,\\beta ,\\gamma \\in {\\text{N}}\\). A pair \\(\\left( {\\Lambda
    ,{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = \\left( {\\Lambda, {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    } \\right)} \\right)\\) is called an IVPFHSS, and its mapping is defined as: \\(\\Lambda
    :{\\mathfrak{T}}_{1} \\times {\\mathfrak{T}}_{2} \\times {\\mathfrak{T}}_{3} \\times
    \\cdots \\times {\\mathfrak{T}}_{n} = {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    \\to IVPFS^{U}\\). Also, it can be defined as \\(\\left( {\\Lambda, {\\mathop
    {\\mathfrak{T}}\\limits^{{...}}} } \\right) = \\left\\{ {\\left( {\\widehat{{\\varsigma_{ij}
    }}, \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right)} \\right): \\widehat{{\\varsigma_{ij} }} \\in {\\mathop {\\mathfrak{T}}\\limits^{{...}}}
    , \\Lambda_{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }} \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right) \\in IVPFS^{U} \\in \\left[ {0, 1} \\right]} \\right\\}\\), where
    \\(\\Lambda _{{ {\\mathop {\\mathfrak{T}}\\limits^{{...}}} }}\\left( {\\widehat{{\\varsigma
    _{{ij}} }}} \\right) = \\left\\{ {\\left\\langle {{\\mathfrak{u}},~{\\mathcal{T}}_{{\\Lambda
    \\left( {\\widehat{{\\varsigma _{{ij}} }}} \\right)}} \\left( {\\mathfrak{u}}
    \\right),~{\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma _{{ij}} }}}
    \\right)}} \\left( {\\mathfrak{u}} \\right)} \\right\\rangle :~{\\mathfrak{u}}
    \\in {U}} \\right\\}\\), and \\({\\mathcal{T}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij}
    }}} \\right)}} \\left( {\\mathfrak{u}} \\right) = \\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]\\) and
    \\({\\mathcal{J}}_{{\\Lambda \\left( {\\widehat{{\\varsigma_{ij} }}} \\right)}}
    \\left( {\\mathfrak{u}} \\right) = \\left[ {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]\\) represents
    the MD and NMD, respectively, and \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right), {\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) \\in \\left[ {0,
    1} \\right]\\), such as \\(0 \\le \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} \\le 1\\). The IVPFHSN can be written
    as \\(\\Lambda = \\left\\{ {\\left( { \\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right],{ }\\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right\\}\\). Zulqarnain et al.45 introduced a set of algebraic operational laws
    for IVPFHSS. They used these operational laws to introduce a set of AOs for IVPFHSS.
    $$\\begin{gathered} {\\text{IVPFHSWA}}\\left( {\\Lambda_{{\\widehat{{\\varsigma_{11}
    }}}} ,\\Lambda_{{\\widehat{{\\varsigma_{12} }}}},\\ldots \\ldots \\ldots ,\\Lambda_{{\\widehat{{\\varsigma_{nm}
    }}}} } \\right) \\hfill \\\\ = \\left( {\\sqrt {1 - \\mathop \\prod \\limits_{j
    = 1}^{m} \\left( {\\mathop \\prod \\limits_{i = 1}^{n} \\left( {1 - \\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]^{2} } \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} } ,\\mathop \\prod \\limits_{j = 1}^{m}
    \\left( {\\mathop \\prod \\limits_{i = 1}^{n} \\left( {\\left[ {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} } \\right) \\hfill \\\\ \\end{gathered}$$
    $$\\begin{gathered} {\\text{IVPFHSWG}}\\left( {\\Lambda_{{\\widehat{{\\varsigma_{11}
    }}}} ,\\Lambda_{{\\widehat{{\\varsigma_{12} }}}},\\ldots \\ldots \\ldots ,\\Lambda_{{\\widehat{{\\varsigma_{nm}
    }}}} } \\right) \\hfill \\\\ = \\left( {\\mathop \\prod \\limits_{j = 1}^{m} \\left(
    {\\mathop \\prod \\limits_{i = 1}^{n} \\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} ,\\sqrt {1 - \\mathop \\prod \\limits_{j
    = 1}^{m} \\left( {\\mathop \\prod \\limits_{i = 1}^{n} \\left( {1 - \\left[ {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]^{2} } \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} } } \\right) \\hfill \\\\ \\end{gathered}$$
    The weight vectors \\(\\Omega _{i}\\) and \\({\\upgamma }_{j}\\) are used in a
    DM process where experts evaluate attributes of alternatives. The vector \\(\\Omega
    _{i}\\) represents the weights given by the experts, where \\(i = 1,2,...,n\\)
    and \\(n\\) is the total number of experts, such as \\(\\Omega _{i} > 0, \\mathop
    \\sum \\nolimits_{i = 1}^{n}\\Omega _{i} = 1\\). Similarly, the vector \\({\\upgamma
    }_{j}\\) represents the weights given by the experts, where \\(j = 1,2,...,m\\)
    and \\(m\\) is the total number of attributes, such as \\({\\upgamma }_{j} > 0,
    \\mathop \\sum \\nolimits_{j = 1}^{m} {\\upgamma }_{j} = 1\\). The above-presented
    AOs for IVPFHSS cannot compute the most appropriate alternative using the closeness
    coefficient. To overcome these drawbacks, we are going to introduce correlation
    coefficients for IVPFHSS. Correlation coefficient for interval valued pythagorean
    fuzzy hypersoft set In the consequent section, we present the CC for IVPFHSS with
    their essential properties. Definition 8 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then, the informational energies between \\(\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right)\\) and \\(\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)\\) can be
    defined as: $${\\mathcal{E}}_{IVPFHSS} \\left( {\\Lambda ,{\\mathcal{A}}} \\right)
    = \\mathop \\sum \\limits_{k = 1}^{m} \\mathop \\sum \\limits_{i = 1}^{n} \\left(
    {\\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} } \\right)$$
    (1) $${\\mathcal{E}}_{IVPFHSS} \\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)
    = \\mathop \\sum \\limits_{k = 1}^{m} \\mathop \\sum \\limits_{i = 1}^{n} \\left(
    {\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left(
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} } \\right)
    .$$ (2) These informational energies measure the total amount of information contained
    in the IVPFHSS. Definition 9 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right)
    = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then, the correlation between \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right)\\)
    and \\(\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)\\) can be defined as:
    $${\\mathcal{C}}_{IVPFHSS} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{
    }\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) = \\sum\\limits_{k
    = 1}^{m} {\\sum\\limits_{i = 1}^{n} {\\left( {\\begin{array}{*{20}c} {\\left(
    {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + }
    \\\\ {\\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} } \\\\
    \\end{array} } \\right)} }$$ (3) Proposition 10 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then 1. \\({\\mathcal{C}}_{IVPFHSS} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right),{ }\\left( {\\Lambda ,{\\mathcal{A}}} \\right)} \\right) = {\\mathcal{E}}_{IVPFHSS}
    \\left( {\\Lambda ,{\\mathcal{A}}} \\right)\\) 2. \\({\\mathcal{C}}_{IVPFHSS}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right)} \\right) = {\\mathcal{C}}_{IVPFHSS} \\left( {\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right), \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right)} \\right)\\). Proof 1: As \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right)
    = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be an IVPFHSS.
    Using Eq. (3), we get $${\\mathcal{C}}_{IVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\Lambda ,\\mathcal{A}\\right)\\right)=\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+\\\\
    {\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)$$
    $${\\mathcal{C}}_{IVPFHSS} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{
    }\\left( {\\Lambda ,{\\mathcal{A}}} \\right)} \\right) = \\mathop \\sum \\limits_{k
    = 1}^{m} \\mathop \\sum \\limits_{i = 1}^{n} \\left( {\\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} } \\right)$$ \\({\\mathcal{C}}_{IVPFHSS}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right)} \\right) = {\\mathcal{E}}_{IVPFHSS} \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right)\\). Proof 2: Proof is very simple and straightforward. Definition 11
    Let \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\left\\{ {\\left. {\\left(
    {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then, CC between them is defined as: $$\\begin{aligned} & {\\mathbb{C}}_{{IVPFHSS}}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{}\\left( {{\\mathcal{G}},{~\\mathcal{B}}}
    \\right)} \\right) = \\frac{{{\\mathcal{C}}_{{IVPFHSS}} \\left( {\\left( {\\Lambda
    ,{\\mathcal{A}}} \\right),{}\\left( {{\\mathcal{G}},{~\\mathcal{B}}} \\right)}
    \\right)}}{{\\sqrt {{\\mathcal{E}}_{{IVPFHSS}} \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right)} \\sqrt {{\\mathcal{E}}_{{IVPFHSS}} \\left( {{\\mathcal{G}},{~\\mathcal{B}}}
    \\right)} }} \\hfill \\\\ & \\quad= \\frac{{\\mathop \\sum \\limits_{{k = 1}}^{m}
    \\mathop \\sum \\limits_{{i = 1}}^{n} \\left( {\\begin{aligned} & {\\left( {{\\mathcal{T}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} }
    \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} +
    \\left( {{\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2}
    }\\\\ & \\quad {+ \\left( {{\\mathcal{J}}_{{\\Lambda _{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left(
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right)} \\right)^{2} *\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2}
    } \\\\ \\end{aligned} } \\right)}}{\\begin{aligned}& {\\sqrt {\\mathop \\sum \\limits_{{k
    = 1}}^{m} \\mathop \\sum \\limits_{{i = 1}}^{n} \\left( {\\left( {{\\mathcal{T}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} }
    \\right)} \\right)^{4} + \\left( {{\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4}
    + \\left( {{\\mathcal{J}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{J}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right)} \\right)^{4} } \\right)}}\\\\ & \\quad{ \\sqrt {\\mathop \\sum \\limits_{{k
    = 1}}^{m} \\mathop \\sum \\limits_{{i = 1}}^{n} \\left( {\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} +
    \\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} +
    \\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} } \\right)} }\\end{aligned}
    } \\hfill \\\\ \\end{aligned}$$ (4) Theorem 12 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then, the following possessions are held: 1. \\(0 \\le {\\mathbb{C}}_{IVPFHSS}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right)} \\right) \\le 1\\) 2. \\({\\mathbb{C}}_{IVPFHSS} \\left(
    {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)} \\right) = {\\mathbb{C}}_{IVPFHSS} \\left( {\\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right),\\left( {\\Lambda ,{\\mathcal{A}}} \\right)} \\right)\\)
    3. If \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right)\\), i.e., I\\(\\forall\\) \\(i\\), \\(j\\), \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), \\({\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), and \\({\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), then \\({\\mathbb{C}}_{IVPFHSS}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right)} \\right) =\\) 1. Proof 1. \\({\\mathbb{C}}_{IVPFHSS}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right)} \\right) \\ge 0\\) is obvious. Now, weIwill demonstrate
    \\({\\mathbb{C}}_{IVPFHSS} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) \\le 1\\). Employing Eq. (3).
    $${\\mathcal{C}}_{IVPFHSS} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{
    }\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) =$$ $$\\begin{aligned}&
    \\mathop \\sum \\limits_{k = 1}^{m} \\mathop \\sum \\limits_{i = 1}^{n} \\left(
    {{\\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2}}}\\right.
    \\\\ & \\quad + \\left.{ \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} {*}\\left(
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} *\\left(
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{2} } \\right) \\end{aligned}$$
    $$\\begin{aligned} & = \\mathop \\sum \\limits_{k = 1}^{m} \\left( { \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} *\\left(
    {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} + \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} *\\left(
    {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2}} \\right. \\\\ & \\quad
    + \\left. { \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}}
    }}^{\\ell } \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} {*}\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} *\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{1} } \\right)} \\right)^{2} } \\right)\\end{aligned}$$
    $$\\begin{aligned} & + \\mathop \\sum \\limits_{{k = 1}}^{m} \\left( {\\left(
    {{\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}f_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2} +
    \\left( {{\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2} *\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2}
    } \\right. \\\\ & \\quad + \\left. { \\left( {{\\mathcal{J}}_{{\\Lambda _{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2} {\\text{*}}\\left(
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2} + \\left( {{\\mathcal{J}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{2}
    } \\right)} \\right)^{2} *\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{2} } \\right)} \\right)^{2}
    } \\right) \\end{aligned}$$ . $$+$$ $$\\vdots$$ $$+$$ $$\\begin{aligned} & \\mathop
    \\sum \\limits_{k = 1}^{m} \\left( { \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} *\\left(
    {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} + \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} *\\left(
    {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} }\\right.\\\\ & \\quad\\left.{+
    \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} {*}\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\upsilon } \\left(
    {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} *\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{n} } \\right)} \\right)^{2} } \\right)
    \\end{aligned}$$ $$\\begin{aligned} & {\\mathcal{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &\\quad=\\left\\{\\begin{array}{c}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\\\\\quad
    +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\ \\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\\\\\quad
    +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right\\}\\end{aligned}$$
    $$+$$ $$\\left\\{\\begin{array}{c}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{1}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{1}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{{{ \\varsigma }}}_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{1}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{1}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{{{ \\varsigma
    }}}_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{2}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{2}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{{{ \\varsigma }}}_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{2}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{2}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{{{
    \\varsigma }}}_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{m}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{m}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{{{ \\varsigma }}}_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{m}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{m}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right\\}$$
    $$+$$ $$\\vdots$$ $$+$$ $$\\left\\{\\begin{array}{c}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\\\\\quad
    +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right\\}$$
    $$+\\sum_{k=1}^{m}\\left(\\begin{array}{l}\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\right)\\\\
    \\quad+\\left({\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right)\\\\\\quad
    +\\cdots \\cdots +\\left({\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\right)\\end{array}\\right)$$
    $$+\\sum_{k=1}^{m}\\left(\\begin{array}{c}\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\right)+\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right)\\\\
    +\\cdots \\cdots +\\left({\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\right)\\end{array}\\right)$$
    Employing the Cauchy Schwarz inequality $$\\begin{aligned} & {{\\mathcal{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}^{2}\\\\
    &\\quad\\le \\sum_{k=1}^{m}\\left\\{\\begin{array}{l}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right)+\\\\
    \\quad \\dots +\\left({\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right)+\\\\
    \\left({\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right)+\\\\
    \\quad \\dots +\\left({\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right)\\end{array}\\right\\}\\\\
    &\\quad\\quad\\times \\sum_{k=1}^{m}\\left\\{\\begin{array}{l}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right)+\\\\
    \\quad \\dots +\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right)+\\\\
    \\left({\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right)+\\\\
    \\quad \\dots +\\left({\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right)\\end{array}\\right\\}\\end{aligned}$$
    $$\\begin{aligned} & {{\\mathcal{C}}_{IVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}^{2}\\\\
    &\\quad\\le \\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left\\{\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right\\}\\\\
    &\\quad\\quad\\times \\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left\\{\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right\\}\\end{aligned}$$
    $${{\\mathcal{C}}_{IVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}^{2}\\le
    {\\mathcal{E}}_{IVPFHSS}\\left(\\Lambda ,\\mathcal{A}\\right)\\times {\\mathcal{E}}_{IVPFHSS}\\left(\\mathcal{G},\\mathcal{B}\\right)$$
    Using Definition 11, we get . $${\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\le 1.$$
    So, it is verified that \\(0\\le {\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\le 1.\\)
    Proof 2. As \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    From Eq. (4) $$\\begin{aligned}& {\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &
    \\quad=\\frac{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)}{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}\\\\
    \\quad \\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}\\end{array}}\\end{aligned}$$
    $$\\begin{aligned}=\\frac{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)}{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}\\\\
    \\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}\\end{array}}\\end{aligned}$$
    $$={\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\mathcal{G},\\mathcal{B}\\right), \\left(\\Lambda
    ,\\mathcal{A}\\right)\\right)$$ Proof 3. As we know that $$\\begin{aligned}&{\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &\\quad=\\frac{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad+{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)
    }{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}
    \\\\ \\quad \\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}
    \\end{array}}\\end{aligned}$$ As $${\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{0k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)$$ $${\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)$$ $${\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)$$ $${\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)$$ $$\\begin{aligned}&{\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\&=\\frac{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)
    }{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}
    \\\\ \\quad \\sqrt{\\sum_{j=1}^{m}\\left({\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)}\\end{array}}\\end{aligned}$$
    $${\\mathbb{C}}_{IVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)=1.$$
    Definition 13. Let \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    Then, the CC between them is also defined as: $${\\mathbb{C}}_{IVPFHSS}^{1}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)=\\frac{{\\mathcal{C}}_{IVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}{max\\left\\{{\\mathcal{E}}_{IVPFHSS}\\left(\\Lambda
    ,\\mathcal{A}\\right), {\\mathcal{E}}_{IVPFHSS}\\left(\\mathcal{G},\\mathcal{B}\\right)\\right\\}}$$
    $$\\begin{aligned}&{\\mathbb{C}}_{IVPFHSS}^{1}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\\\quad&=\\frac{\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\\\quad+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)
    }{max\\left\\{\\begin{array}{l}\\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right),\\\\\\quad
    \\sum_{k=1}^{m}\\sum_{i=1}^{n}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\end{array}\\right\\}}\\end{aligned}$$
    (5) Theorem 14 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\left\\{
    {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then the following properties hold: 1. \\(0 \\le {\\mathbb{C}}_{IVPFHSS}^{1} \\left(
    {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)} \\right) \\le 1\\). 2. \\({\\mathbb{C}}_{IVPFHSS}^{1} \\left( {\\left(
    {\\Lambda ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)}
    \\right) = {\\mathbb{C}}_{IVPFHSS}^{1} \\left( {\\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right),\\left( {\\Lambda ,{\\mathcal{A}}} \\right)} \\right)\\) 3. If \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), \\({\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), and \\({\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)\\) \\(\\forall\\)
    \\(i, k\\). Then \\({\\mathbb{C}}_{IVPFHSS}^{1} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) = 1\\). Proof.
    TheIproofIof case 2 is straightforward, and case 3 is like Theorem 12, which involves
    case 3 as well. Also, it''s apparent that. \\({\\mathbb{C}}_{IVPFHSS}^{1} \\left(
    {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)} \\right) \\ge 0\\) in case 1. To complete the proof, we only need to
    show that \\({\\mathbb{C}}_{IVPFHSS}^{1} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right),{ }\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) \\le 1\\).
    This can be shown by using the inequality \\({\\mathcal{C}}_{IVPFHSS} \\left(
    {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)} \\right)^{2} \\le {\\mathcal{E}}_{IVPFHSS} \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right) \\times {\\mathcal{E}}_{IVPFHSS} \\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)\\). Therefore, \\({\\mathcal{C}}_{IVPFHSS} \\left( {\\left( {\\Lambda
    ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right)
    \\le max\\left\\{ {{\\mathcal{E}}_{IVPFHSS} \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right), {\\mathcal{E}}_{IVPFHSS} \\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)}
    \\right\\}\\). Hence, \\({\\mathbb{C}}_{IVPFHSS}^{1} \\left( {\\left( {\\Lambda
    ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)}
    \\right) \\le 1\\). Before making a final decision in real-life decision-making
    applications, examining the weight of factors and expert opinions is critical.
    This is particularly pertinent in IVPFHSS, while the weights designated to particular
    alternatives can considerably impact the outcomes. In this study, we propose a
    novel weighted correlation coefficient (WCC) for IVPFHSS that considers decision
    makers'' and alternatives'' weights. Weighted correlation coefficient for interval
    valued pythagorean fuzzy hypersoft set The proposed method takes into account
    the weights of experts and parameters such as; \\(\\Omega = \\left\\{ {\\Omega
    _{1} ,{ }\\Omega _{2} ,{ }\\Omega _{3},\\ldots ,{ }\\Omega _{n} } \\right\\}^{T}\\)
    and \\(\\gamma = \\left\\{ {{\\upgamma }_{1} ,{{ \\gamma }}_{2} ,{{ \\gamma }}_{3},\\ldots
    ,{{ \\gamma }}_{m} } \\right\\}^{T}\\) be the weights for experts and parameters.
    Where, \\(\\Omega _{i} > 0\\), \\(\\mathop \\sum \\nolimits_{i = 1}^{m}\\Omega
    _{i} = 1\\) and \\({\\upgamma }_{k} > 0\\), \\(\\mathop \\sum \\nolimits_{k =
    1}^{m} {\\upgamma }_{k} = 1\\), they are ensuring that their sum is equal to one.
    By doing so, the WCC can provide a more accurate representation of the DM process.
    Definition 15 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\left\\{ {\\left.
    {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) be two IVPFHSS.
    Then, their weighted informational energies can be defined as: $${\\mathcal{E}}_{WIVPFHSS}
    \\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\mathop \\sum \\limits_{k = 1}^{m}
    \\gamma_{k} \\left( {\\mathop \\sum \\limits_{i = 1}^{n}\\Omega _{i} \\left( {\\left(
    {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left(
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} } \\right)}
    \\right)$$ (6) $${\\mathcal{E}}_{WIVPFHSS}\\left(\\mathcal{G},\\mathcal{B}\\right)=\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)$$
    (7) Definition 16 Let \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    Then, the weighted correlation between them can be defined as: $${\\mathcal{C}}_{WIVPFSS}\\left(\\left(\\mathcal{F},\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)=\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+\\\\
    {\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)\\right)$$
    (8) Proposition 17 Let \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    Then 1. \\({\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\Lambda
    ,\\mathcal{A}\\right)\\right)={\\mathcal{E}}_{WIVPFHSS}\\left(\\Lambda ,\\mathcal{A}\\right)\\)
    2. \\({\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)={\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\mathcal{G},\\mathcal{B}\\right),\\left(\\Lambda
    ,\\mathcal{A}\\right)\\right)\\) Proof: The proof of the above-stated proposition
    is straightforward. Definition 18 Let \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    Then, the WCC between them can be defined as: $$\\begin{aligned} & {\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &
    \\quad =\\frac{{\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}{\\sqrt{{\\mathcal{E}}_{WIVPFHSS}\\left(\\Lambda
    ,\\mathcal{A}\\right)}\\sqrt{{\\mathcal{E}}_{WIVPFHSS}\\left(\\mathcal{G},\\mathcal{B}\\right)}}\\\\&\\quad=\\frac{\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad+{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)\\right)}{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}
    \\\\ \\quad \\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}\\end{array}}\\end{aligned}$$
    (9) Theorem 19 Let \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    If \\(\\Omega ={\\left\\{{\\Omega }_{1}, {\\Omega }_{2}, {\\Omega }_{3},\\dots,{\\Omega
    }_{n}\\right\\}}^{T}\\) be a weight vector for experts and \\(\\gamma ={\\left\\{{\\gamma
    }_{1}, {\\gamma }_{2}, {\\gamma }_{3},\\dots,{\\gamma }_{m}\\right\\}}^{T}\\)
    be the weight vector for attributes, such as \\({\\Omega }_{i}>0\\), \\(\\sum_{i=1}^{n}{\\Omega
    }_{i}=1\\) and \\({\\gamma }_{k}>0\\), \\(\\sum_{k=1}^{m}{\\gamma }_{k}=1\\).
    Then, the WCC satisfied the subsequent properties: 1. 0 \\(\\le\\) \\({\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\) \\(\\le\\)
    1 2. \\({\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)={\\mathbb{C}}_{WIVPFHSS}\\left(
    \\left(\\mathcal{G},\\mathcal{B}\\right),\\left(\\Lambda ,\\mathcal{A}\\right)\\right)\\)
    3. If \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left(\\mathcal{G},\\mathcal{B}\\right)\\),
    i.e., \\(\\forall\\) \\(i, k\\) \\({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{{{
    \\varsigma }}}_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\), \\({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{k}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{k}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{i}\\right)\\), \\({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\), and \\({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{{{ \\varsigma }}}_{k}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{i}\\right)={\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{{{
    \\varsigma }}}_{k}}}}^{{{\\upsilon}}}\\left({\\mathfrak{u}}_{i}\\right)\\), then
    \\({\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)=1\\).
    Proof 1. \\({\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\ge 0\\) is obvious. Now,IweIwillIdemonstrate
    \\({\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\le
    1\\). Using Eq. (8). $${\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),\\left(\\mathcal{G},\\mathcal{B}\\right)\\right)=$$
    $$\\begin{aligned} & \\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right.\\right.\\\\&\\quad
    \\left. \\left. + {\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2} +{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\right)\\right)\\end{aligned}$$
    $$\\begin{aligned} & =\\sum_{k=1}^{m}{\\gamma }_{k}\\left({\\Omega }_{1}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\right.\\\\&\\quad
    \\left.+{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\right)+\\end{aligned}$$
    $$\\begin{aligned} & \\sum_{k=1}^{m}{\\gamma }_{k}\\left({\\Omega }_{2}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right.\\right.\\\\
    & \\quad\\left.\\left.+{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right)\\right)\\end{aligned}$$
    $$+$$ $$\\vdots$$ $$+$$ $$\\begin{aligned} & \\sum_{k=1}^{m}{\\gamma }_{k}\\left({\\Omega
    }_{n}\\left({\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\right.\\right.\\\\
    & \\quad\\left.\\left.+{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\right)\\right)\\end{aligned}$$
    $$\\begin{aligned} & {\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right) \\\\ & \\quad = \\left\\{ \\begin{array}{c}{\\gamma
    }_{1}\\left({\\Omega }_{1}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\\\
    \\quad+{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)\\right)+\\\\
    {\\gamma }_{2}\\left({\\Omega }_{1}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)\\right)+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}\\left({\\Omega }_{1}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)\\right)\\end{array}\\right\\}\\end{aligned}$$
    $$+$$ $$\\left\\{\\begin{array}{c}{\\gamma }_{1}\\left({\\Omega }_{2}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\end{array}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right)\\right)+\\\\
    {\\gamma }_{2}\\left({\\Omega }_{2}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\end{array}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right)\\right)+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}\\left({\\Omega }_{2}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\end{array}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\right)\\right)\\end{array}\\right\\}$$
    $$+$$ $$\\vdots$$ $$+$$ $$\\left\\{\\begin{array}{c}{\\gamma }_{1}\\left(\\begin{array}{c}{\\Omega
    }_{n}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right)+\\\\
    {\\gamma }_{2}\\left({\\Omega }_{n}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)\\right)+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}\\left({\\Omega }_{n}\\begin{array}{c}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right)\\end{array}\\right\\}$$
    $$=\\left\\{\\begin{array}{c}{\\gamma }_{1}\\left(\\begin{array}{c}\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    {\\gamma }_{2}\\left(\\begin{array}{c}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}\\left(\\begin{array}{c}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right\\}$$
    $$+\\left\\{\\begin{array}{c}{\\gamma }_{1}\\left(\\begin{array}{c}\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    {\\gamma }_{2}\\left(\\begin{array}{c}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}\\left(\\begin{array}{c}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right\\}$$
    $$+$$ $$\\vdots$$ $$+$$ $$\\left\\{\\begin{array}{c}{\\gamma }_{1}\\left(\\begin{array}{c}\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    {\\gamma }_{2}\\left(\\begin{array}{c}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}\\left(\\begin{array}{c}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\right\\}$$
    $$=\\left\\{\\begin{array}{c}\\left(\\begin{array}{c}\\sqrt{{\\gamma }_{1}}\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{1}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)+\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{1}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{1}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)+\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{c}\\sqrt{{\\gamma }_{2}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{1}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)+\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{2}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{1}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)+\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\begin{array}{c}\\left(\\begin{array}{c}\\sqrt{{\\gamma }_{m}}\\sqrt{{\\Omega
    }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{1}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)+\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{m}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{1}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)+\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{1}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\end{array}\\right\\}$$
    $$+\\left\\{\\begin{array}{c}\\left(\\begin{array}{c}\\sqrt{{\\gamma }_{1}}\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{2}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)+\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{1}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{2}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)+\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{c}\\sqrt{{\\gamma }_{2}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{2}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)+\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{2}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{2}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)+\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\begin{array}{c}\\left(\\begin{array}{c}\\sqrt{{\\gamma }_{m}}\\sqrt{{\\Omega
    }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{2}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)+\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{m}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{2}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)+\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{2}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\end{array}\\right\\}$$
    $$+$$ $$\\vdots$$ $$+$$ $$\\left\\{\\begin{array}{c}\\left(\\begin{array}{c}\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)+\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{1}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)+\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{1}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{c}\\sqrt{{\\gamma }_{2}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{n}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)+\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{2}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{n}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)+\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{2}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\begin{array}{c}\\left(\\begin{array}{c}\\sqrt{{\\gamma }_{m}}\\sqrt{{\\Omega
    }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{n}}\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)+\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}+\\\\
    \\sqrt{{\\gamma }_{m}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{n}}\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)+\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}*\\sqrt{{\\gamma
    }_{m}}\\sqrt{{\\Omega }_{n}}{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{2}\\end{array}\\right)\\end{array}\\end{array}\\right\\}$$
    Employing the Cauchy Schwarz inequality $$\\begin{aligned} & {\\mathcal{C}}_{WIVPFHSS}{\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}^{2} \\\\
    & \\quad \\le \\left\\{\\begin{array}{c}\\left(\\begin{array}{c}{\\gamma }_{1}{\\Omega
    }_{1}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right\\}+\\\\
    {\\gamma }_{2}{\\Omega }_{1}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right\\}+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}{\\Omega }_{1}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right\\}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{c}{\\gamma }_{1}{\\Omega }_{2}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right\\}+\\\\
    {\\gamma }_{2}{\\Omega }_{2}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right\\}+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}{\\Omega }_{2}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right\\}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\begin{array}{c}\\left(\\begin{array}{c}{\\gamma }_{1}{\\Omega
    }_{n}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right\\}+\\\\
    {\\gamma }_{2}{\\Omega }_{n}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right\\}+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}{\\Omega }_{n}\\left\\{{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right\\}\\end{array}\\right)\\end{array}\\end{array}\\right\\}
    \\\\ & \\quad \\times \\left\\{\\begin{array}{c}\\left(\\begin{array}{c}{\\gamma
    }_{1}{\\Omega }_{1}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right\\}+\\\\
    {\\gamma }_{2}{\\Omega }_{1}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right\\}+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}{\\Omega }_{1}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{1}\\right)\\right)}^{4}\\right\\}\\end{array}\\right)+\\\\
    \\left(\\begin{array}{c}{\\gamma }_{1}{\\Omega }_{2}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right\\}+\\\\
    {\\gamma }_{2}{\\Omega }_{2}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right\\}+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}{\\Omega }_{2}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{2}\\right)\\right)}^{4}\\right\\}\\end{array}\\right)+\\\\
    \\vdots \\\\ +\\\\ \\begin{array}{c}\\left(\\begin{array}{c}{\\gamma }_{1}{\\Omega
    }_{n}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{1}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right\\}+\\\\
    {\\gamma }_{2}{\\Omega }_{n}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{2}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right\\}+\\\\
    \\vdots \\\\ +\\\\ {\\gamma }_{m}{\\Omega }_{n}\\left\\{{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{m}}}}^{\\ell}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma}_{m}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{n}\\right)\\right)}^{4}\\right\\}\\end{array}\\right)\\end{array}\\end{array}\\right\\}\\end{aligned}$$
    $$\\begin{aligned} {\\mathcal{C}}_{{IVPFHSS}} & \\left( {\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right),\\left( {{\\mathcal{G}},{~\\mathcal{B}}} \\right)} \\right)^{2} \\\\
    & \\le \\mathop \\sum \\limits_{{k = 1}}^{m} \\mathop \\sum \\limits_{{i = 1}}^{n}
    \\left\\{ {\\left( {\\left( {{\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} +
    \\left( {{\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} } \\right) + \\left( {\\left(
    {{\\mathcal{J}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{J}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right)} \\right)^{4} } \\right)} \\right\\} \\\\ & \\;\\;\\; \\times \\mathop
    \\sum \\limits_{{k = 1}}^{m} \\mathop \\sum \\limits_{{i = 1}}^{n} \\left\\{ {\\left(
    {\\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} + \\left( {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4}
    } \\right) + \\left( {\\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} +
    \\left( {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon
    } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right)^{4} } \\right)} \\right\\}
    \\\\ \\end{aligned}$$ $${\\mathcal{C}}_{WIVPFHSS}{\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}^{2}\\le {\\mathcal{E}}_{WIVPFHSS}\\left(\\Lambda
    ,\\mathcal{A}\\right)\\times {\\mathcal{E}}_{WIVPFHSS}\\left(\\mathcal{G},\\mathcal{B}\\right)$$
    Using Definition 11, we get $${\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\le 1$$ So, it is verified that
    \\(0\\le {\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\le 1\\). Proof 2. As \\(\\left(\\Lambda
    ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i}, \\left(\\left[{\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    From Eq. (9) $$\\begin{aligned} & {\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &
    \\quad =\\frac{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)\\right)}{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}
    \\\\ \\quad \\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}\\end{array}}
    \\end{aligned}$$ $$=\\frac{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left(\\begin{array}{l}{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)\\right)}{\\begin{array}{l}\\sqrt{\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}\\\\
    \\quad \\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}\\end{array}}$$
    $$={\\mathbb{C}}_{WIVPFHSS}\\left( \\left(\\mathcal{G},\\mathcal{B}\\right), \\left(\\Lambda
    ,\\mathcal{A}\\right)\\right)$$ Proof 3. As $$\\begin{aligned} & {\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &\\quad=\\frac{\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+\\\\
    {\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)\\right)
    }{\\left(\\begin{array}{c}\\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left({\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}\\\\
    \\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)}\\end{array}\\right)}\\end{aligned}$$
    As $${\\mathcal{T}}_{{\\Lambda _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma
    _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),\\;\\;{\\mathcal{T}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right),\\;\\;{\\mathcal{J}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} }
    \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right)\\;\\;{\\text{and}}\\;{\\mathcal{J}}_{{\\Lambda
    _{{\\widehat{{\\varsigma _{k} }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i}
    } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma _{k} }}}}
    }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right).\\;\\;{\\text{So,}}$$
    $$\\begin{aligned} & {\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &\\quad=\\frac{\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left\\{\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right\\}\\right)
    }{\\left(\\begin{array}{c}\\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega
    }_{i}\\left\\{\\left({\\left({\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma
    }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right\\}\\right)}
    \\\\ \\sqrt{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left\\{\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)+\\left({\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right\\}\\right)}\\end{array}\\right)}\\end{aligned}$$
    $${\\mathbb{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)=1$$
    Definition 20 Let \\(\\left(\\Lambda ,\\mathcal{A}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) and \\(\\left(\\mathcal{G},\\mathcal{B}\\right)=\\left\\{\\left({\\mathfrak{u}}_{i},
    \\left(\\left[{\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right],
    \\left[{\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right),
    {\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right]\\right)\\right)
    \\left. {} \\right| {\\mathfrak{u}}_{i}\\in \\mathsf{U}\\right\\}\\) be two IVPFHSS.
    Then, the WCC between them is also defined as: $$\\begin{aligned} & {\\mathbb{C}}_{WIVPFHSS}^{1}\\left(\\left(\\Lambda
    ,\\mathcal{A}\\right), \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)\\\\ &
    \\quad =\\frac{{\\mathcal{C}}_{WIVPFHSS}\\left(\\left(\\Lambda ,\\mathcal{A}\\right),
    \\left(\\mathcal{G},\\mathcal{B}\\right)\\right)}{max\\left\\{{\\mathcal{E}}_{WIVPFHSS}\\left(\\Lambda
    ,\\mathcal{A}\\right), {\\mathcal{E}}_{WIVPFHSS}\\left(\\mathcal{G},\\mathcal{B}\\right)\\right\\}}\\\\
    & \\quad =\\frac{\\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left(\\begin{array}{c}{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\\\
    \\quad +{\\left({\\mathcal{J}}_{{\\Lambda }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}*{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{2}\\end{array}\\right)\\right)}{max\\left\\{\\begin{array}{c}\\sum_{k=1}^{m}{\\gamma
    }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\Lambda
    }_{\\widehat{{ \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right),
    \\\\ \\sum_{k=1}^{m}{\\gamma }_{k}\\left(\\sum_{i=1}^{n}{\\Omega }_{i}\\left({\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{T}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\ell}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}+{\\left({\\mathcal{J}}_{{\\mathcal{G}}_{\\widehat{{
    \\varsigma }_{k}}}}^{\\upsilon}\\left({\\mathfrak{u}}_{i}\\right)\\right)}^{4}\\right)\\right)\\end{array}\\right\\}}\\end{aligned}$$
    (10) Theorem 21 Let \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\left\\{
    {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right) } \\right|{\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) and \\(\\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right) = \\left\\{ {\\left. {\\left( {{\\mathfrak{u}}_{i},\\left(
    {\\left[ {{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell
    } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right], \\left[
    {{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell }
    \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)}
    \\right)} \\right| {\\mathfrak{u}}_{i} \\in {U}} \\right\\}\\) beItwo IVIFHSS.
    Then, 1. \\(0 \\le {\\mathbb{C}}_{WIVPFHSS}^{1} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) \\le 1.\\)
    2. \\({\\mathbb{C}}_{WIVPFHSS}^{1} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) = {\\mathbb{C}}_{WIVPFHSS}^{1}
    \\left( {\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right),\\left( {\\Lambda ,{\\mathcal{A}}}
    \\right)} \\right)\\) 3. If \\(\\left( {\\Lambda ,{\\mathcal{A}}} \\right) = \\left(
    {{\\mathcal{G}},{ \\mathcal{B}}} \\right)\\), i.e., \\(\\forall\\) \\(i\\), \\(j\\),
    \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), \\({\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{T}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), \\({\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), and \\({\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right) = {\\mathcal{J}}_{{{\\mathcal{G}}_{{\\widehat{{\\varsigma_{k}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)\\), then \\({\\mathbb{C}}_{WIVPFHSS}^{1}
    \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{
    \\mathcal{B}}} \\right)} \\right) = 1\\). Proof. The proof of case 2 is very easy
    and straightforward, and case 3 is like Theorem 12, which involves case 3 as well.
    Also, it’s apparent that \\({\\mathbb{C}}_{WIVPFHSS}^{1} \\left( {\\left( {\\Lambda
    ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)}
    \\right) \\ge 0\\) in case 1. To complete the proof, we only need to show that
    \\({\\mathbb{C}}_{WIVPFHSS}^{1} \\left( {\\left( {\\Lambda ,{\\mathcal{A}}} \\right),{
    }\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right) \\le 1\\). This can
    be shown by using the inequality \\({\\mathcal{C}}_{WIVPFHSS} \\left( {\\left(
    {\\Lambda ,{\\mathcal{A}}} \\right),{ }\\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)} \\right)^{2} \\le {\\mathcal{E}}_{WIVPFHSS} \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right) \\times {\\mathcal{E}}_{WIVPFHSS} \\left( {{\\mathcal{G}},{ \\mathcal{B}}}
    \\right)\\). Therefore, \\({\\mathcal{C}}_{WIVPFHSS} \\left( {\\left( {\\Lambda
    ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right)
    \\le max\\left\\{ {{\\mathcal{E}}_{WIVPFHSS} \\left( {\\Lambda ,{\\mathcal{A}}}
    \\right), {\\mathcal{E}}_{WIVPFHSS} \\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)}
    \\right\\}\\). Hence, \\({\\mathbb{C}}_{WIVPFHSS}^{1} \\left( {\\left( {\\Lambda
    ,{\\mathcal{A}}} \\right),\\left( {{\\mathcal{G}},{ \\mathcal{B}}} \\right)} \\right)
    \\le 1\\). Proposed TOPSIS approach based on correlation coefficient to resolve
    MADM problem under IVPFHSS The TOPSIS methodology, frequently employed in MADM,
    presents several advantages for determining and tackling exacerbated decision
    concerns. TOPSIS methods offer several significant benefits, such as The TOPSIS
    approach is simple manipulation to pick up and execute. It provides decision-makers
    with a simple way to assess and evaluate alternatives according to multiple factors.
    This method enables the concurrent assessment of several criteria or features.
    It permits decision-makers to analyze alternatives using different parameters.
    TOPSIS conducts an extensive evaluation by considering both the positive ideal
    solution (PIS) and the negative ideal solution (NIS). This technique assists in
    identifying alternates that are almost identical to the positive ideal and least
    similar to the negative ideal, which leads to an improved equitable assessment.
    TOPSIS is a robust strategy for various data types involving numerical, grouping,
    and linguistic variables. This versatility helps decision-makers consider an extensive
    spectrum of information and opinions in their selections. The TOPSIS approach
    classifies or encourages the alternatives, providing the outcome simple to analyze
    and explore. Decision-makers are able to rapidly identify the most appropriate
    solutions and understand the conflicts among different criteria. TOPSIS is extensively
    studied and approved in a wide range of fields. Several uses in daily life scenarios
    for decision-making have shown their efficacy and applicability. This method can
    handle a number of scales for measuring, including interval-valued, fuzzy, and
    qualitative information. Furthermore, due to its versatility, it can be used for
    a broad spectrum of decision-making challenges, irrespective of the nature or
    type of details provided. In the context of MADM problems, the TOPSIS approach
    proposed by Hwang and Yoon51 is widely used to rank alternatives. In the subsequent
    segment, we encompass the TOPSIS methodology for IVPFHSS based on the CC. Proposed
    MADM approach In this section, we consider a set of alternatives \\({\\mathfrak{H}}
    = \\left\\{ {{\\mathfrak{H}}^{1} ,{ }{\\mathfrak{H}}^{2} ,{ }{\\mathfrak{H}}^{3},\\ldots
    ,{ }{\\mathfrak{H}}^{s} } \\right\\}\\), a group of experts \\({\\mathcal{H}}
    = \\left\\{ {{\\mathcal{H}}_{1} ,{ }{\\mathcal{H}}_{2} ,{ }{\\mathcal{H}}_{3},\\ldots
    ,{ }{\\mathcal{H}}_{n} } \\right\\}\\), and a collection of considered attributes
    \\(\\varsigma = \\left\\{ {\\varsigma_{1} ,{ }\\varsigma_{2} ,{ }\\varsigma_{3}
    ,{ } \\ldots ,{ }\\varsigma_{m} } \\right\\}\\) with multi-sub-attributes \\(\\varsigma^{\\prime}
    = \\left\\{ {\\left( {\\varsigma_{1\\rho },\\varsigma_{2\\rho },\\ldots,\\varsigma_{m\\rho
    } } \\right) \\forall \\rho \\in \\left\\{ {1, 2, 3, \\ldots ,t} \\right\\}} \\right\\}\\).
    The weights of professionals and the sub-attributes are given by \\(\\Omega\\)
    = \\(\\left( {\\Omega _{1} ,{ }\\Omega _{1},\\ldots ,{ }\\Omega _{n} } \\right)^{T}\\)
    and \\(\\gamma = \\left( {\\gamma_{1} ,{ }\\gamma_{2} ,{ }\\gamma_{3},\\ldots
    ,{ }\\gamma_{\\rho } } \\right)^{T}\\), respectively, where \\(\\Omega _{i} >
    0\\), \\(\\mathop \\sum \\limits_{i = 1}^{n}\\Omega _{i} = 1\\), and \\(\\gamma_{\\rho
    } > 0\\), \\(\\mathop \\sum \\limits_{\\rho = 1}^{t} \\gamma_{\\rho } = 1\\).
    The assortment of multi-sub-attributes can be indicated as \\(\\varsigma^{\\prime}
    = { }\\left\\{ {\\widehat{{\\varsigma_{\\rho } }}:\\rho \\in \\left\\{ {1, 2,
    \\ldots ,t} \\right\\}} \\right\\}\\). The group of professionals \\(\\left\\{
    {{\\mathcal{H}}_{i} :{ }i{ } = 1,{ }2, \\ldots ,{ }n} \\right\\}\\) assess the
    substitutes \\(\\left\\{ {{\\mathfrak{H}}^{\\left( z \\right)} :{ }z{ } = { }1,{
    }2,{ } \\ldots ,{ }s} \\right\\}\\) because of the particular sub-attributes \\(\\varsigma^{\\prime}
    = { }\\left\\{ {\\widehat{{\\varsigma_{\\rho } }}:\\rho \\in \\left\\{ {1, 2,
    \\ldots ,t} \\right\\}} \\right\\}\\) in IVPFHSNs form, such as \\(\\left( {{\\mathfrak{H}}^{\\left(
    z \\right)} ,{ }\\varsigma^{\\prime}} \\right)_{n \\times \\rho } = \\left( {\\left[
    {{\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\ell } ,{ }{\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\upsilon } } \\right], \\left[ {{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\ell } ,{ }{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\upsilon
    } } \\right]} \\right)_{n \\times \\rho }\\). The expert''s judgment for each
    substitute in the form of IVPFHSNs can be epitomized as \\(\\Delta_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\left( z \\right)} = \\left( {\\left[ {{\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\ell } ,{ }{\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\upsilon
    } } \\right], \\left[ {{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\ell
    } ,{ }{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\upsilon } } \\right]}
    \\right)_{n \\times \\rho }\\), where \\(0 \\le {\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\ell } ,{ }{\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\upsilon
    },{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij} }}}}^{\\ell } ,{ }{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\upsilon } \\le 1\\) and \\(\\left( {{\\mathcal{T}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\upsilon } } \\right)^{2} + \\left( {{\\mathcal{J}}_{{\\widehat{{\\varsigma_{ij}
    }}}}^{\\upsilon } } \\right)^{2} \\le 1\\), \\(\\forall i, j\\). The proposed
    TOPSIS technique algorithm is as follows: Step 1. The experts evaluate the preferences
    of each alternative in the form of IVPFHSNs, such as: $${\\left({\\mathfrak{H}}^{\\left(z\\right)},
    {{{\\varsigma}}}{\\prime}\\right)}_{n\\times \\rho }=\\begin{array}{c}{\\mathcal{H}}_{1}\\\\
    {\\mathcal{H}}_{2}\\\\ \\vdots \\\\ {\\mathcal{H}}_{n}\\end{array}\\left(\\begin{array}{cccc}\\left(\\left[{\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{11}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{ \\varsigma }_{11}}}^{\\upsilon}\\right],
    \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma }_{11}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{
    \\varsigma }_{11}}}^{\\upsilon}\\right]\\right)& \\left(\\left[{\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{12}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{ \\varsigma }_{12}}}^{\\upsilon}\\right],
    \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma }_{12}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{
    \\varsigma }_{12}}}^{\\upsilon}\\right]\\right)& \\cdots & \\left(\\left[{\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{1\\rho }}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{ \\varsigma }_{1\\rho
    }}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma }_{1\\rho
    }}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{1\\rho }}}^{\\upsilon}\\right]\\right)\\\\
    \\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{21}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{21}}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma
    }_{21}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{21}}}^{\\upsilon}\\right]\\right)&
    \\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{22}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{22}}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma
    }_{22}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{22}}}^{\\upsilon}\\right]\\right)&
    \\cdots & \\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{2\\rho }}}^{\\ell},
    {\\mathcal{T}}_{\\widehat{{ \\varsigma }_{2\\rho }}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{
    \\varsigma }_{2\\rho }}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{2\\rho
    }}}^{\\upsilon}\\right]\\right)\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\
    \\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{n1}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{n1}}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma
    }_{n1}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{n1}}}^{\\upsilon}\\right]\\right)&
    \\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{n2}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{n2}}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma
    }_{n2}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{n2}}}^{\\upsilon}\\right]\\right)&
    \\cdots & \\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{n\\rho }}}^{\\ell},
    {\\mathcal{T}}_{\\widehat{{ \\varsigma }_{n\\rho }}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{
    \\varsigma }_{n\\rho }}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{n\\rho
    }}}^{\\upsilon}\\right]\\right)\\end{array}\\right)$$ (11) Step 2. Normalize the
    decision matrices using the normalization rule and reduce the sub-parameters in
    the same type. $${{\\mathfrak{H}}^{\\left(z\\right)}}_{\\widehat{{ \\varsigma
    }_{ij}}}= \\left\\{\\begin{array}{c}{\\left({{\\mathfrak{H}}^{\\left(z\\right)}}_{\\widehat{{
    \\varsigma }_{ij}}}\\right)}^{c}={\\left(\\left[{\\mathcal{J}}_{\\widehat{{ \\varsigma
    }_{ij}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{ij}}}^{\\upsilon}\\right],
    \\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{ij}}}^{\\ell}, {\\mathcal{T}}_{\\widehat{{
    \\varsigma }_{ij}}}^{\\upsilon}\\right]\\right)}_{n\\times \\rho } cost\\,\\,
    type \\,\\, sub-parameters\\\\ {{\\mathfrak{H}}^{\\left(z\\right)}}_{\\widehat{{
    \\varsigma }_{ij}}}= {\\left(\\left[{\\mathcal{T}}_{\\widehat{{ \\varsigma }_{ij}}}^{\\ell},
    {\\mathcal{T}}_{\\widehat{{ \\varsigma }_{ij}}}^{\\upsilon}\\right], \\left[{\\mathcal{J}}_{\\widehat{{
    \\varsigma }_{ij}}}^{\\ell}, {\\mathcal{J}}_{\\widehat{{ \\varsigma }_{ij}}}^{\\upsilon}\\right]\\right)}_{n\\times
    \\rho } benefit \\,\\,type \\,\\,sub-parameters\\end{array}\\right.$$ (12) Step
    3. Evaluate the weighted decision matrices for each alternative \\({\\overline{\\mathfrak{H}}}^{\\left(
    z \\right)} = \\left( {\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{\\left( z \\right)}
    } \\right)_{n \\times \\rho }\\), where $${\\overline{\\mathfrak{H}}}^{\\left(
    z \\right)} = \\gamma_{j}\\Omega _{i} \\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{\\left(
    z \\right)} = \\left( {\\sqrt {1 - \\left( {\\left( {1 - \\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]^{2} } \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} } ,\\left( {\\left( {\\left[ {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} } \\right)$$ (13) $${\\overline{\\mathfrak{H}}}^{\\left(
    z \\right)} = \\gamma_{j}\\Omega _{i} \\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{\\left(
    z \\right)} = \\left( {\\left( {\\left( {\\left[ {{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\ell } \\left( {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{T}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]} \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} ,\\sqrt {1 - \\left( {\\left( {1 - \\left[
    {{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij} }}}} }}^{\\ell } \\left(
    {{\\mathfrak{u}}_{i} } \\right),{ }{\\mathcal{J}}_{{\\Lambda_{{\\widehat{{\\varsigma_{ij}
    }}}} }}^{\\upsilon } \\left( {{\\mathfrak{u}}_{i} } \\right)} \\right]^{2} } \\right)^{{\\Omega
    _{i} }} } \\right)^{{{\\upgamma }_{j} }} } } \\right)$$ (14) where \\(\\Omega\\)
    = \\(\\left( {\\Omega _{1} ,{ }\\Omega _{1},\\ldots ,{ }\\Omega _{n} } \\right)^{T}\\)
    and \\(\\gamma = \\left( {\\gamma_{1} ,{ }\\gamma_{2} ,{ }\\gamma_{3},\\ldots
    ,{ }\\gamma_{\\rho } } \\right)^{T}\\), be the weights of experts and sub-parameters,
    respectively, such as: \\(\\Omega _{i} > 0\\), \\(\\mathop \\sum \\limits_{i =
    1}^{n}\\Omega _{i} = 1\\), and \\(\\gamma_{\\rho } > 0\\), \\(\\mathop \\sum \\limits_{\\rho
    = 1}^{t} \\gamma_{\\rho } = 1\\). Step 4. Determine the PIA and NIA evaluate the
    indices \\(\\hbar_{{\\widehat{{\\varsigma_{ij} }}}} = {\\text{arg }}max_{z} \\left\\{
    {\\theta_{{\\widehat{{\\varsigma_{ij} }}}}^{\\left( z \\right)} } \\right\\}\\)
    and \\({\\mathfrak{g}}_{{\\widehat{{\\varsigma_{ij} }}}} = {\\text{arg }}min_{z}
    \\left\\{ {\\theta_{{\\widehat{{\\varsigma_{ij} }}}}^{\\left( z \\right)} } \\right\\}\\)
    such as: $$\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left( z \\right)
    + }} = \\left( {{\\mathcal{T}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{ + } ,{\\text{~}}{\\mathcal{J}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{ + } } \\right)_{{n \\times m}} = \\left( {{\\bar{\\mathcal{T}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( {\\hbar _{{\\widehat{{\\varsigma _{{ij}} }}}} } \\right)}}
    ,{\\text{~}}{\\bar{\\mathcal{J}}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    {\\hbar _{{\\widehat{{\\varsigma _{{ij}} }}}} } \\right)}} } \\right)$$ (15) and
    $$\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left( z \\right) - }} = \\left(
    {{\\mathcal{T}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{ - } ,{\\text{~}}{\\mathcal{J}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{ - } } \\right)_{{n \\times m}} = \\left( {{\\bar{\\mathcal{T}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( {{\\mathfrak{g}}_{{\\widehat{{\\varsigma _{{ij}} }}}} }
    \\right)}} ,{\\text{~}}{\\bar{\\mathcal{J}}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    {{\\mathfrak{g}}_{{\\widehat{{\\varsigma _{{ij}} }}}} } \\right)}} } \\right)$$
    (16) Step 5. By using the weighted decision matrices \\({\\overline{\\mathfrak{H}}}^{\\left(
    z \\right)}\\) and PIA \\(\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{{\\left(
    z \\right)+ }}\\), find the CC such as: $$\\begin{gathered} \\kappa ^{{\\left(
    z \\right)}} = {\\mathbb{C}}_{{IVPFHSS}} \\left( {{\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} ,{\\text{~}}\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    z \\right) + }} } \\right) = \\frac{{{\\mathcal{C}}_{{IVPFHSS}} \\left( {{\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} ,\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left( z \\right)
    + }} } \\right)}}{{\\sqrt {{\\mathcal{E}}_{{IVPFHSS}} {\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} } \\sqrt {{\\mathcal{E}}_{{IVPFHSS}} \\Delta _{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right) + }} } }} = \\hfill \\\\ \\frac{{\\mathop \\sum
    \\nolimits_{{j = 1}}^{m} \\mathop \\sum \\nolimits_{{i = 1}}^{n} \\left( {{\\bar{\\mathcal{T}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} {\\text{*}}{\\mathcal{T}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{ + } + {\\text{~}}{\\bar{\\mathcal{J}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} {\\text{*}}{\\mathcal{J}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{ + } } \\right)}}{{\\sqrt {\\mathop \\sum \\nolimits_{{j = 1}}^{m}
    \\mathop \\sum \\nolimits_{{i = 1}}^{n} \\left( {\\left( {{\\bar{\\mathcal{T}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} } \\right)^{2} + {\\text{~}}\\left( {{\\bar{\\mathcal{J}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} } \\right)^{2} } \\right)} \\sqrt {\\mathop
    \\sum \\nolimits_{{j = 1}}^{m} \\mathop \\sum \\nolimits_{{i = 1}}^{n} \\left(
    {\\left( {{\\mathcal{T}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{ + } } \\right)^{2}
    + {\\text{~}}\\left( {{\\mathcal{J}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{ +
    } } \\right)^{2} } \\right)} }} \\hfill \\\\ \\end{gathered}$$ (17) Step 6. By
    using the weighted decision matrices \\({\\overline{\\mathfrak{H}}}^{\\left( z
    \\right)}\\) and NIA \\(\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{{\\left( z
    \\right) - }}\\) find the CC: $$\\begin{gathered} \\tau ^{{\\left( z \\right)}}
    = {\\mathbb{C}}_{{IVPFHSS}} \\left( {{\\bar{\\mathfrak{H}}}^{{\\left( z \\right)}}
    ,{\\text{~}}\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left( z \\right)
    - }} } \\right) = \\frac{{{\\mathcal{C}}_{{IVPFHSS}} \\left( {{\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} ,{\\text{~~}}\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    z \\right) - }} } \\right)}}{{\\sqrt {{\\mathcal{E}}_{{IVPFHSS}} {\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} } \\sqrt {{\\mathcal{E}}_{{IVPFHSS}} {\\text{~~}}\\Delta _{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right) - }} } }} = \\hfill \\\\ \\frac{{\\mathop \\sum
    \\nolimits_{{j = 1}}^{m} \\mathop \\sum \\nolimits_{{i = 1}}^{n} \\left( {{\\bar{\\mathcal{T}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} {\\text{*}}{\\mathcal{T}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{ - } + {\\text{~}}{\\bar{\\mathcal{J}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} {\\text{*}}{\\mathcal{J}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{ - } } \\right)}}{{\\sqrt {\\mathop \\sum \\nolimits_{{j = 1}}^{m}
    \\mathop \\sum \\nolimits_{{i = 1}}^{n} \\left( {\\left( {{\\bar{\\mathcal{T}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} } \\right)^{2} + {\\text{~}}\\left( {{\\bar{\\mathcal{J}}}_{{\\widehat{{\\varsigma
    _{{ij}} }}}}^{{\\left( z \\right)}} } \\right)^{2} } \\right)} \\sqrt {\\mathop
    \\sum \\nolimits_{{j = 1}}^{m} \\mathop \\sum \\nolimits_{{i = 1}}^{n} \\left(
    {\\left( {{\\mathcal{T}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{ - } } \\right)^{2}
    + {\\text{~}}\\left( {{\\mathcal{J}}_{{\\widehat{{\\varsigma _{{ij}} }}}}^{ -
    } } \\right)^{2} } \\right)} }} \\hfill \\\\ \\end{gathered}$$ (18) Step 7. Compute
    the relative closeness to the ideal solution for each alternative. $${\\beth }^{{\\left(
    z \\right)}} = \\frac{{{\\daleth }\\left( {{\\bar{\\mathfrak{H}}}^{{\\left( z
    \\right)}} ,{\\text{~}}\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    z \\right) - }} } \\right)}}{{{\\daleth }\\left( {{\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} ,{\\text{~}}\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    z \\right) + }} } \\right) + {\\daleth }\\left( {{\\bar{\\mathfrak{H}}}^{{\\left(
    z \\right)}} ,{\\text{~}}\\Delta _{{\\widehat{{\\varsigma _{{ij}} }}}}^{{\\left(
    z \\right) - }} } \\right)}}$$ (19) where \\({\\daleth }\\left( {{\\overline{\\mathfrak{H}}}^{\\left(
    z \\right)} ,{ }\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{{\\left( z \\right)+
    }} } \\right) = 1 - \\kappa^{\\left( z \\right)}\\) and \\({\\daleth }\\left(
    {{\\overline{\\mathfrak{H}}}^{\\left( z \\right)} ,{ }\\Delta_{{\\widehat{{\\varsigma_{ij}
    }}}}^{{\\left( z \\right) - }} } \\right) = 1{ } - { }\\tau^{\\left( z \\right)}\\).
    Step 8. The suitability of a substitute is determined by the closeness coefficient,
    where a higher value of the closeness coefficient indicates that the alternative
    is more appropriate. Step 9. Examine the classification. The flow chart of the
    introduced correlation-based TOPSIS model is presented in the following Fig. 1.
    Figure 1 Flow chart of the proposed correlation-based TOPSIS method. Full size
    image Challenges in implementing fair bed allocation policies during the COVID-19
    pandemic The COVID-19 pandemic has caused a tremendous burden on healthcare systems
    worldwide, with an overwhelming number of patients requiring hospitalization and
    medical care. This has led to the implementation of fair bed allocation policies
    to ensure patients receive the care they need based on their medical conditions
    and severity. However, implementing appropriate bed allocation policies has not
    been without its challenges. One of the primary challenges is the shortage of
    hospital beds and medical resources. Hospitals have been struggling to meet the
    demands of COVID-19 patients, leading to the rationing of medical resources, such
    as ventilators and personal protective equipment. This shortage has made it difficult
    to allocate beds fairly, as insufficient resources exist. Another challenge is
    the lack of standardization in bed allocation policies across different healthcare
    systems. The criteria for bed allocation can vary between hospitals and even between
    countries. This lack of standardization can lead to disparities in patient care
    and outcomes, as some patients may have better access to resources than others.
    Additionally, there may be challenges in determining the severity of a patient’s
    condition and medical needs. This can be incredibly challenging for patients with
    underlying health conditions or atypical symptoms. In these cases, it may be challenging
    to determine which patients require hospitalization and which patients can be
    safely treated at home. Furthermore, there may be ethical dilemmas when implementing
    fair bed allocation policies. For example, deciding who should receive medical
    resources when there is a shortage can be challenging. Healthcare providers may
    face the difficult decision of choosing between patients, which can be emotionally
    taxing and have an enduring influence on the patient and their loved ones. Implementing
    fair bed allocation policies during the COVID-19 pandemic has been challenging
    due to the shortage of medical resources, lack of standardization in policies,
    difficulties in determining the severity of patients'' conditions, and ethical
    dilemmas. Addressing these challenges will require collaboration between healthcare
    systems, policymakers, and healthcare providers to ensure that patients receive
    the care they need pretty and equitably. Data-driven decision-making (D3U) has
    become essential in predicting the spread and stages of the pandemic and in predictive
    bed allocation. D3U has become integral to our daily lives because it analyzes
    large amounts of data and provides accurate predictions. It has helped healthcare
    providers to make informed decisions and allocate resources effectively. Predictive
    bed allocation has become a crucial aspect of healthcare management, especially
    during the pandemic, when hospitals have been overwhelmed by the number of patients
    seeking treatment. The ability to predict the spread and stages of the pandemic
    has helped policymakers make informed decisions regarding implementing public
    health measures, such as social distancing and lockdowns. In a D3U (Dedicated
    COVID-19 Unit), where patients seek treatment for COVID-19, there are several
    fundamental factors to consider for bed allocation. Firstly, the severity of the
    patient''s symptoms and the level of medical intervention required is critical.
    Patients with severe symptoms such as respiratory distress, low oxygen saturation
    levels, or requiring ventilation support may need to be allocated to high-dependency
    units or ICU beds. Secondly, the patient''s age and underlying medical conditions
    must be considered. Older patients or those with underlying health conditions,
    such as heart or lung disease, may require specialized care or monitoring. Patients
    undergoing surgical procedures or weakened immune systems may also require isolation
    to prevent infections. Thirdly, the availability of medical equipment and resources
    is essential in bed allocation. Hospitals must have the necessary equipment, such
    as ventilators, monitors, and oxygen supplies, to ensure proper care and treatment
    of patients. Lastly, the availability of medical staff and their expertise in
    treating COVID-19 patients is also a significant factor in bed allocation. Hospitals
    must have sufficient medical personnel trained in COVID-19 care and management
    to ensure patients receive the best treatment. The severity of the patient’s symptoms
    and the level of medical intervention \\(\\left( {\\varsigma_{1} } \\right)\\)
    The severity of a patient''s symptoms and the level of medical intervention required
    are two fundamental factors that must be considered for bed allocation in D3U.
    The severity of a patient''s symptoms is an essential indicator of their condition
    and the level of medical intervention needed to manage their illness. For instance,
    a patient with severe symptoms may require a higher medical intervention than
    a mild one. This could include interventions such as oxygen therapy, IV fluids,
    or mechanical ventilation. The level of medical intervention required also plays
    a critical role in determining the appropriate bed allocation for patients. Patients
    who require a high level of medical intervention, such as those needing critical
    care or intensive care, will require specialized beds equipped with the necessary
    medical equipment and facilities. Patients who require less medical intervention
    may be allocated to less specialized beds. The severity of a patient''s symptoms
    and the level of medical intervention required are essential factors to consider
    in bed allocation in D3U. These factors can help to ensure that patients receive
    the appropriate level of care and are allocated to the most suitable beds based
    on their medical needs. Patient''s age and underlying medical conditions \\(\\left(
    {\\varsigma_{2} } \\right)\\) When allocating beds in a D3U, it is important to
    consider the patient’s age and underlying medical conditions. Age and medical
    conditions are significant factors that can affect a patient’s recovery and the
    level of medical intervention they require. Older patients and those with underlying
    medical conditions such as diabetes, heart disease, or respiratory problems may
    require more medical intervention and monitoring. They may also take longer to
    recover, and their conditions deteriorate more rapidly. Therefore, it is essential
    to allocate beds to ensure patients receive the appropriate care and support.
    Younger patients or those with no underlying medical conditions may require less
    medical intervention and may recover more quickly. Allocating beds to these patients
    may help free up resources for those requiring more critical care. It is also
    important to note that some medical conditions may increase the risk of complications,
    such as respiratory failure, which requires immediate and intensive care. In such
    cases, allocating beds to patients with these conditions must be prioritized to
    ensure timely and effective treatment. Patient age and underlying medical conditions
    are critical factors that must be considered when allocating beds in a D3U. Careful
    consideration of these factors can help to ensure that patients receive the appropriate
    level of care and that resources are allocated efficiently. Medical equipment
    and resources an essential factors in bed allocation \\(\\left( {\\varsigma_{3}
    } \\right)\\) When allocating beds in a D3U, it is essential to consider the availability
    of medical equipment and resources. The type of medical equipment and resources
    needed for each patient can vary significantly depending on their medical condition
    and the severity of their symptoms. For example, patients with severe respiratory
    distress may require access to ventilators, while patients with cardiac issues
    may need access to specialized monitoring equipment. Additionally, patients with
    infectious diseases may require isolation rooms and specialized personal protective
    equipment to prevent the spread of the disease. The availability of medical equipment
    and resources can vary depending on the location and size of the D3U. Sometimes,
    larger hospitals may have more extensive resources and equipment than smaller
    clinics or medical centers. Therefore, it is crucial to consider the medical facility’s
    capabilities and the resources available when allocating beds to patients. Moreover,
    it is also essential to ensure that the medical equipment and resources are adequately
    maintained and serviced to ensure proper functioning. Regular maintenance and
    service of medical equipment can help prevent equipment failure and ensure patients
    receive the necessary medical care promptly. Medical equipment and resources are
    critical factors that must be considered when allocating beds in a D3U. The availability
    and proper functioning of medical equipment can significantly impact patient outcomes
    and can be the difference between life and death in some cases. Therefore, it
    is crucial to ensure that medical facilities have access to the necessary equipment
    and resources and that they are adequately maintained to ensure optimal patient
    care. Availability of medical staff and their expertise in treating COVID-19 patients
    \\(\\left( {\\varsigma_{4} } \\right)\\) The availability of medical staff and
    their expertise in treating COVID-19 patients is crucial in bed allocation for
    D3U. The demand for healthcare workers has increased significantly due to the
    pandemic, and hospitals must ensure they have enough staff to handle the patient
    load. Not only is the number of available medical staff important, but also their
    level of expertise in treating COVID-19 patients. COVID-19 is a new and complex
    disease, and treating it requires specialized knowledge and skills. Hospitals
    must ensure that their staff is properly trained in the latest treatment protocols
    and have the necessary equipment to provide high-quality care to patients. The
    shortage of medical staff and the burden on those working in the healthcare industry
    have been major challenges during the COVID-19 pandemic. Staffing shortages can
    lead to increased workloads and burnout, compromising patient care quality. Hospitals
    must prioritize the safety and well-being of their staff and provide them with
    the necessary resources to do their job effectively. In addition to ensuring adequate
    staffing levels and expertise, hospitals must also consider the potential for
    staff members to contract the virus themselves. This risk can be mitigated by
    providing adequate personal protective equipment (PPE) and ensuring proper infection
    control protocols are in place. The availability of medical staff and their expertise
    in treating COVID-19 patients is critical in bed allocation for D3U. Hospitals
    must ensure that they have enough staff with the appropriate training and equipment
    to provide high-quality care to patients while also prioritizing the safety and
    well-being of their staff. Numerical example Suppose \\({\\mathfrak{T}} = \\left\\{
    {{\\mathfrak{T}}^{1} ,{ }{\\mathfrak{T}}^{2} ,{ }{\\mathfrak{T}}^{3} ,{ }{\\mathfrak{T}}^{4}
    } \\right\\}\\) be a collection of different COVID-19 patients (alternatives)
    and \\({\\mathcal{H}} = \\left\\{ {{\\mathcal{H}}_{1} ,{ }{\\mathcal{H}}_{2} ,{
    }{\\mathcal{H}}_{3} ,{ }{\\mathcal{H}}_{4} } \\right\\}\\) be a team of medical
    experts with weights \\(\\left( {0.2,{ }0.4,{ }0.3,{ }0.1} \\right)^{T}\\). The
    hospital has only one bed available, and the medical experts evaluate which patient
    is most affected by the COVID-19 virus. First of all, medical experts considered
    the criteria for bed allocation for COVID-19 patients, such as \\(\\varsigma =
    \\left\\{ {\\varsigma_{1} ,{ }\\varsigma_{2} ,{ }\\varsigma_{3} ,{ }\\varsigma_{4}
    } \\right\\}\\), where \\(\\varsigma_{1} :\\) the severity of the patient’s symptoms
    and the level of medical intervention, \\(\\varsigma_{2} :\\) patient’s age and
    underlying medical conditions, \\(\\varsigma_{3} :\\) medical equipment and resources
    is an essential factor in bed allocation, and \\(\\varsigma_{4} :\\) availability
    of medical staff and their expertise in treating COVID-19 patients. The severity
    of the patient’s symptoms and the level of medical intervention = \\(\\varsigma_{1}\\)
     = \\(\\left\\{ {\\varsigma_{11} = {\\text{Patients }}\\;{\\text{who}}\\;{\\text{
    require}}\\;{\\text{ a }}\\;{\\text{high}}\\;{\\text{ level }}\\;{\\text{of }}\\;{\\text{medical}}\\;{\\text{
    intervention}},{ }\\varsigma_{12} = {\\text{Patients }}\\;{\\text{who }}\\;{\\text{require}}\\;{\\text{
    less}}\\;{\\text{ medical}}\\;{\\text{ intervention}}} \\right\\}\\), Patient’s
    age and underlying medical conditions = \\(\\varsigma_{2}\\) = \\(\\left\\{ {\\varsigma_{21}
    = {\\text{Older patients}},{ }\\varsigma_{22} = {\\text{Younger patients}}} \\right\\}\\)
    Medical equipment and resources an essential factors in bed allocation = \\(\\varsigma_{3}\\)
    = \\(\\left\\{ {\\varsigma_{31} = Medical \\;equipment\\; and \\;resources\\;
    is \\;an\\; essential \\;factor\\; in\\; bed \\;allocation} \\right\\}\\) and
    Availability of medical staff and their expertise in treating COVID-19 patients
    = \\(\\varsigma_{4}\\)  = \\(\\left\\{ {\\varsigma_{41} = availability \\;of \\;medical
    \\;staff, \\;\\varsigma_{42} = availability \\;of\\; experienced\\; medical\\;
    experts} \\right\\}\\). Let \\(\\varsigma^{\\prime} = \\varsigma_{1} \\times \\varsigma_{2}
    \\times \\varsigma_{3} \\times \\varsigma_{4}\\) represents the set of sub-parameters.
    \\(\\begin{gathered} \\varsigma ^{\\prime} = \\varsigma _{1} \\times \\varsigma
    _{2} \\times \\varsigma _{3} \\times \\varsigma _{4} = \\left\\{ {\\varsigma _{{11}}
    ,~\\varsigma _{{12}} } \\right\\} \\times \\left\\{ {\\varsigma _{{21}} ,~\\varsigma
    _{{22}} } \\right\\} \\times \\left\\{ {\\varsigma _{{31}} } \\right\\} \\times
    \\left\\{ {\\varsigma _{{41}} ,~\\varsigma _{{42}} } \\right\\} = \\hfill \\\\
    \\left\\{ \\begin{gathered} \\left( {\\varsigma _{{11}} ,~\\varsigma _{{21}} ,~\\varsigma
    _{{31}} ,~\\varsigma _{{41}} } \\right),~\\left( {\\varsigma _{{11}} ,~\\varsigma
    _{{21}} ,~\\varsigma _{{31}} ,~\\varsigma _{{42}} } \\right),~\\left( {\\varsigma
    _{{11}} ,~\\varsigma _{{22}} ,~\\varsigma _{{31}} ,~\\varsigma _{{41}} } \\right),~\\left(
    {\\varsigma _{{11}} ,~\\varsigma _{{22}} ,~\\varsigma _{{31}} ,~\\varsigma _{{42}}
    } \\right), \\hfill \\\\ \\left( {\\varsigma _{{12}} ,~\\varsigma _{{21}} ,~\\varsigma
    _{{31}} ,~\\varsigma _{{41}} } \\right),~\\left( {\\varsigma _{{12}} ,~\\varsigma
    _{{21}} ,~\\varsigma _{{31}} ,~\\varsigma _{{42}} } \\right),~\\left( {\\varsigma
    _{{12}} ,~\\varsigma _{{22}} ,~\\varsigma _{{31}} ,~\\varsigma _{{41}} } \\right),~\\left(
    {\\varsigma _{{12}} ,~\\varsigma _{{22}} ,~\\varsigma _{{31}} ,~\\varsigma _{{42}}
    } \\right) \\hfill \\\\ \\end{gathered} \\right\\} \\hfill \\\\ \\end{gathered}\\),
    \\(\\varsigma^{\\prime} = \\left\\{ {\\hat{\\varsigma }_{1} ,\\hat{\\varsigma
    }_{2},\\hat{\\varsigma }_{3},\\hat{\\varsigma }_{4},\\hat{\\varsigma }_{5},\\hat{\\varsigma
    }_{6},\\hat{\\varsigma }_{7},\\hat{\\varsigma }_{8} } \\right\\}\\) shows the
    collection of sub-attributes with weights \\(\\gamma_{j} = \\left( {.2, .05, .15,
    .1, .15, .18, .05, .12} \\right)^{T}\\). In the context of COVID-19 patients,
    various aspects are considered to evaluate the patient''s health status, such
    as oxygen saturation, respiratory rate, blood pressure, etc. A group of professionals
    evaluates the patients based on these aspects and provides their preferences for
    each patient in the IVPFHSN form. The medical experts judge the most affected
    patient who needs bed on an emergency basis considering the above-stated symptoms
    under deliberated parameters. Tables 1, 2, 3 and 4 show the evaluation of COVID-19
    patients based on various sub-attributes of these aspects. These evaluations are
    used to determine the most affected COVID-19 patients using the TOPSIS-based MADM
    problem. The TOPSIS approach evaluates the positive and negative ideal solutions
    for the DM process. It helps to accumulate the PIS and NIS and prolong the methodology
    for IVPFHSS built on the CC. The experts’ weights and attributes determine the
    most affected patient. The structure of the planned TOPSIS method is used to determine
    each patient''s closeness coefficient, and the exaggerated value of the closeness
    coefficient spectacles the most affected patient and deserves bed on an urgent
    basis. Table 1 Expert’s estation for \\({\\mathfrak{T}}^{1}\\) in IVPFHSNs. Full
    size table Table 2 Expert’s estimation for \\({\\mathfrak{T}}^{2}\\) in IVPFHSNs.
    Full size table Table 3 Expert’s estimation for \\({\\mathfrak{T}}^{3}\\) in IVPFHSNs.
    Full size table Table 4 Expert''s estimation for \\({\\mathfrak{T}}^{4}\\) in
    IVPFHSNs. Full size table Step 1. The experts deliver their preferences for alternatives
    \\({\\mathfrak{T}} = \\left\\{ {{\\mathfrak{T}}^{1} ,{ }{\\mathfrak{T}}^{2} ,{
    }{\\mathfrak{T}}^{3} ,{ }{\\mathfrak{T}}^{4} } \\right\\}\\) in the form of IVPFHSNs,
    such as in the following Tables 1, 2, 3 and 4: Step 2: Indicates that all considered
    parameters are the same type, so there is no need to normalize them. This means
    that the IVPFHSN values provided by the expert group for each patient in Tables
    1, 2, 3 and 4 can be directly used as the decision matrix for each alternative.
    Step 3: Tables 5, 6, 7 and 8 show the weighted decision matrix \\({\\overline{\\mathfrak{T}}}^{\\left(
    z \\right)} = \\left( {\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{\\left( z \\right)}
    } \\right)_{n \\times \\rho }\\) for each patient, the values in each cell represent
    the weighted IVPFHSN score for that particular sub-attribute. Table 5 Weighted
    decision matrix for \\({\\overline{\\mathfrak{T}}}^{\\left( 1 \\right)}\\). Full
    size table Table 6 Weighted decision matrix for \\({\\overline{\\mathfrak{T}}}^{\\left(
    2 \\right)}\\). Full size table Table 7 Weighted decision matrix for \\({\\overline{\\mathfrak{T}}}^{\\left(
    3 \\right)}\\). Full size table Table 8 Weighted decision matrix for \\({\\overline{\\mathfrak{T}}}^{\\left(
    4 \\right)}\\). Full size table Step 4. Employed Eqs.( 5.5 and 5.6) to compute
    the PIA and NIA, respectively. $${{\\Delta }_{\\widehat{{ \\varsigma }_{ij}}}^{(z)}}^{+}=\\left[\\begin{array}{cccccccc}\\left(\\begin{array}{c}\\left[0.0159,
    0.0227\\right],\\\\ \\left[0.9301, 0.9693\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0227,
    0.0272\\right], \\\\ \\left[0.9713, 0.9728\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0361,
    0.0452\\right], \\\\ \\left[0.4953, 0.5271\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0375,
    0.0379\\right], \\\\ \\left[0.6373, 0.6852\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0159,
    0.0227\\right],\\\\ \\left[0.9421, 0.9573\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0344,
    0.0419\\right],\\\\ \\left[0.7245, 0.7532\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0472,
    0.0476\\right], \\\\ \\left[0.5763, 0.5860\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0057,
    0.0097\\right], \\\\ \\left[0.9631, 0.9906\\right]\\end{array}\\right)\\\\ \\left(\\begin{array}{c}\\left[0.0432,
    0.0477\\right], \\\\ \\left[0.5534, 0.5748\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0235,
    0.0379\\right], \\\\ \\left[0.5936, 0.6517\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0363,
    0.0439\\right], \\\\ \\left[0.5814, 0.6258\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0472,
    0.0476\\right], \\\\ \\left[0.5463, 0.5762\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0357,
    0.0397\\right], \\\\ \\left[0.5437, 0.6725\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0363,
    0.0416\\right], \\\\ \\left[0.6465, 0.7053\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0134,
    0.0219\\right],\\\\ \\left[0.7142, 0.7235\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0334,
    0.0367\\right], \\\\ \\left[0.6549, 0.6878\\right]\\end{array}\\right)\\\\ \\left(\\begin{array}{c}\\left[0.0071,
    0.0102\\right], \\\\ \\left[0.9683, 0.9862\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0363,
    0.0438\\right], \\\\ \\left[0.6365, 0.7053\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0053,
    0.0177\\right], \\\\ \\left[0.9227, 0.9552\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0238,
    0.0317\\right], \\\\ \\left[0.9549, 0.9683\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0363,
    0.0416\\right], \\\\ \\left[0.6465, 0.7053\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0063,
    0.0133\\right], \\\\ \\left[0.9788, 0.9867\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0375,
    0.0379\\right], \\\\ \\left[0.6373, 0.6852\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0432,
    0.0477\\right], \\\\ \\left[0.5534, 0.5748\\right]\\end{array}\\right)\\\\ \\left(\\begin{array}{c}\\left[0.0427,
    0.0567\\right], \\\\ \\left[0.5836, 0.5906\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0572,
    0.0676\\right], \\\\ \\left[0.5863, 0.5960\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0053,
    0.0177\\right], \\\\ \\left[0.9416, 0.9823\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0236,
    0.0361\\right], \\\\ \\left[0.6152, 0.6232\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0063,
    0.0133\\right], \\\\ \\left[0.9698, 0.9867\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0432,
    0.0477\\right], \\\\ \\left[0.5534, 0.5748\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0572,
    0.0676\\right], \\\\ \\left[0.5863, 0.5960\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0142,
    0.0249\\right], \\\\ \\left[0.5267, 0.6172\\right]\\end{array}\\right)\\end{array}\\right]$$
    $${{\\Delta }_{\\widehat{{ \\varsigma }_{ij}}}^{\\left(z\\right)}}^{-}=\\left[\\begin{array}{cccccccc}\\left(\\begin{array}{c}\\left[0.0456,
    0.0697\\right], \\\\ \\left[0.4621, 0.5069\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0235,
    0.0397\\right], \\\\ \\left[0.5963, 0.6171\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0174,
    0.0539\\right],\\\\ \\left[0.6945, 0.7962\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0241,
    0.0371\\right], \\\\ \\left[0.5231, 0.5472\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0512,
    0.0576\\right], \\\\ \\left[0.6163, 0.6260\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0237,
    0.0478\\right], \\\\ \\left[0.9379, 0.9526\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0454,
    0.0818\\right], \\\\ \\left[0.8612, 0.8975\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0039,
    0.0189\\right], \\\\ \\left[0.9414, 0.9743\\right]\\end{array}\\right)\\\\ \\left(\\begin{array}{c}\\left[0.0432,
    0.0477\\right], \\\\ \\left[0.5534, 0.5748\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0175,
    0.0534\\right],\\\\ \\left[0.6941, 0.7981\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0314,
    0.0491\\right],\\\\ \\left[0.6754, 0.7323\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0427,
    0.0567\\right], \\\\ \\left[0.5836, 0.5906\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0175,
    0.0534\\right],\\\\ \\left[0.6941, 0.7981\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0314,
    0.0491\\right],\\\\ \\left[0.6754, 0.7323\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0327,
    0.0467\\right], \\\\ \\left[0.5836, 0.6106\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0349,
    0.0473\\right], \\\\ \\left[0.5749, 0.6387\\right]\\end{array}\\right)\\\\ \\left(\\begin{array}{c}\\left[0.0142,
    0.0549\\right], \\\\ \\left[0.5867, 0.6872\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0247,
    0.0474\\right], \\\\ \\left[0.9379, 0.9526\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0214,
    0.0394\\right], \\\\ \\left[0.5876, 0.6291\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0136,
    0.0561\\right], \\\\ \\left[0.6256, 0.7135\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0474,
    0.0808\\right], \\\\ \\left[0.8511, 0.8935\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0191,
    0.0540\\right], \\\\ \\left[0.6146, 0.6372\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0252,
    0.0448\\right],\\\\ \\left[0.9227, 0.9552\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0535,
    0.0921\\right], \\\\ \\left[0.8709, 0.9079\\right]\\end{array}\\right)\\\\ \\left(\\begin{array}{c}\\left[0.0427,
    0.0567\\right], \\\\ \\left[0.5836, 0.5906\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0136,
    0.0361\\right], \\\\ \\left[0.6956, 0.7135\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0242,
    0.0451\\right], \\\\ \\left[0.5867, 0.6327\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0235,
    0.0497\\right], \\\\ \\left[0.5863, 0.6771\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0264,
    0.0507\\right], \\\\ \\left[0.9336, 0.9493\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0138,
    0.0557\\right], \\\\ \\left[0.9043, 0.9275\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0174,
    0.0539\\right],\\\\ \\left[0.6945, 0.7962\\right]\\end{array}\\right)& \\left(\\begin{array}{c}\\left[0.0307,
    0.0588\\right],\\\\ \\left[0.6687, 0.9229\\right]\\end{array}\\right)\\end{array}\\right]$$
    Step 5. Determine the CC among \\({\\overline{\\mathfrak{T}}}^{\\left( z \\right)}\\)
    and PIA \\(\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{{\\left( z \\right)+ }}\\)
    using Eq. (17). \\(\\kappa^{\\left( 1 \\right)} = 0.99241\\), \\(\\kappa^{\\left(
    2 \\right)} = 0.99537\\), \\(\\kappa^{\\left( 3 \\right)} = 0.99327\\), \\(\\kappa^{\\left(
    4 \\right)} = 0.99245\\). Step 6. Determine the CC among \\({\\overline{\\mathfrak{T}}}^{\\left(
    z \\right)}\\) and PIA \\(\\Delta_{{\\widehat{{\\varsigma_{ij} }}}}^{{\\left(
    z \\right) - }}\\) using Eq. (18). \\(\\tau^{\\left( 1 \\right)} = 0.99362\\),
    \\(\\tau^{\\left( 2 \\right)} = 0.99402\\), \\(\\tau^{\\left( 3 \\right)} = 0.99319\\),
    \\(\\tau^{\\left( 4 \\right)} = 0.99724\\). Step 7. Compute the closeness coefficient
    using Eq. (19). \\({\\beth }^{\\left( 1 \\right)} = 0.45669\\), \\({\\beth }^{\\left(
    2 \\right)} = 0.56362\\), \\({\\beth }^{\\left( 3 \\right)} = 0.50295\\), \\({\\beth
    }^{\\left( 4 \\right)} = 0.26770\\). Step 8. The values of the closeness coefficient
    \\({\\mathfrak{T}}^{\\left( 2 \\right)}\\) is maximum, which shows that \\({\\mathfrak{T}}^{\\left(
    2 \\right)}\\) the most critical patient. The above calculation represents the
    most affected patient by COVID-19, is \\({\\mathfrak{T}}^{\\left( 2 \\right)}\\).
    So the patient \\({\\mathfrak{T}}^{\\left( 2 \\right)}\\) is the most deserved
    patient for bed. Step 9. Patients ranking \\({\\mathfrak{T}}^{\\left( 2 \\right)}
    > {\\mathfrak{T}}^{\\left( 3 \\right)} > {\\mathfrak{T}}^{\\left( 1 \\right)}
    > {\\mathfrak{T}}^{\\left( 4 \\right)}\\). Discussion and comparative analysis
    In this section, we will compare the proposed method with other commonly used
    approaches and discuss their strengths and weaknesses to determine the effectiveness
    of the presented method. Supremacy of the planned approach This study offers a
    methodology to tackle the challenges of implementing fair bed allocation policies
    during the COVID-19 pandemic. We suggest a correlations-based TOPSIS method to
    explain MADM obstacles under IVPFHSS. Compared to existing approaches, the proposed
    method is more accurate and compatible in dealing with MADM concerns. It is also
    versatile and easy to understand, allowing for unlike outcomes with inconsistency,
    liability, and amendment. The proposed method can be modified for different models
    with vibrant ordering features to contest their opinions. Structural studies and
    estimates were conducted to demonstrate that the proposed methodology''s consequences
    are comparable to those of hybrids. The proposed method is also flexible, allowing
    frequent FS, IFS, and PFS structures to be converted into special environments
    of IVPFHSS by collecting certain settings. Object-related details can be explicitly
    selected and tentatively, making it suitable for merging imprecise and vague figures
    in DM plans. Through their research and valuation, the authors found that the
    planned scheme''s outcomes are more communal than any offered organization. Still,
    they noted that the projected DM enlargement involves a massive breadth of realities
    to relieve distress in the statistics than presented DM systems. The proposed
    structure is effective, flexible, and convenient, superior to other FS, IFS, and
    PFS hybrid structures. Furthermore, adding suitable affinities can crack some
    consolidation developments of IVPFS into IVPFHSS. Adding infrequent and imprecise
    certainties to the present actual platform is unexpected. Now, details roughly
    well-being can be nominated methodically and accurately. Over the DM procedure,
    imaginary and alarming details are cooperative. So, the intended structure would
    be enhanced, significant, surprising, and compound mixture FS settings. Table
    9 demonstrates the features of the established and predominant techniques, clearly
    comparing the proposed method with existing ones. The proposed methodology offers
    a promising solution to the challenges in implementing fair bed allocation policies
    during the COVID-19 pandemic, and its flexibility and adaptability make it a valuable
    tool for various DM agendas. Table 9 Sensitivity analysis of the planned model
    with prevailing models. Full size table The main reason for developing an innovative
    TOPSIS method is to address the limitations of existing methods. The planned methodology
    is unique because it is based on the IVPFHSS, which allows for a more thorough
    evaluation of the sub-attributes since both the MD and NMD. Previous hybrid structures
    such as FS, IVFS, IFS, IVIFS, PFS, IVPFS, FSS, IVFSS, IFSS, IVIFSS, IFHSS, IVIFHSS,
    PFSS, IVPFSS, and PFHSS were found to be insufficient in delivering comprehensive
    facts about the state. The proposed correlation-based TOPSIS method for IVPFHSS
    offers a more accurate and precise approach to resolving MADM complications. Moreover,
    this method is more versatile and adaptable, enabling it to cater to different
    situations with varying levels of disparity, guilt, and modification. The research
    shows that the anticipated technique''s outcomes are more comprehensive and efficient
    than the existing methods. Table 9 illustrates the superiority of the proposed
    hybrid structure of FS over other hybrid systems, emphasizing its significant
    potential in the field. The proposed methodology has enormous potential in various
    applications, including organizational studies, medical research, and decision-making
    processes that require integrating inaccurate and imprecise data. The planned
    methodology significantly advances decision-making under IVPFHSS, offering a more
    efficient and effective way to tackle MADM obstacles. The research demonstrates
    that the proposed method is superior to existing methods and has enormous potential
    for various applications. But, in some severe circumstances, the developed TOPSIS
    model is unable to handle the situation, such as \\(\\left( {MD^{\\upsilon } }
    \\right)^{2} + \\left( {NMD^{\\upsilon } } \\right)^{2} > 1\\). Comparative studies
    After reviewing various research studies, it has been found that the correlation-based
    TOPSIS technique established in this study is comparable to existing designs.
    However, the main advantage of this method is that it considers additional information
    related to the sub-parameters of the alternatives to deal with uncertainties in
    the statistics. This method can provide more precise and more empirical object-related
    facts, making it helpful in defining incorrect and enigmatic realities in the
    DM practice. Comparative analysis shows that previous methods did not have the
    same intention progression in scheduling techniques because of the PIA and NIA
    obligations. In the developed TOPSIS model, PIA and NIA are measured founded on
    formative CC incentive at an assumed auxiliary level, which reduces the loss of
    facts in the process. The benefit of this method is that it communicates not only
    the degree of sensitivity but also the degree of correspondence between interpretations,
    thereby avoiding conclusions built on destructive bases. The proposed TOPSIS model
    is more efficient than recent techniques and related measures because it considers
    the degree of perception and similarity among clarifications. This method also
    deals with concerns related to the parameterization of alternatives and the handling
    of multi-sub-attributes, which other TOPSIS methodologies cannot handle. The comparison
    consequences spectacle that the finest choice of the proposed method is consistent
    with the established scheme, demonstrating the proposed method''s reliability
    and productivity. The proposed correlation-based TOPSIS method is a valuable addition
    to the DM toolbox, as it provides more accurate and reliable results than previous
    techniques when dealing with uncertainty and multi-sub-attributes. In the context
    of TOPSIS methods, it has been observed that existing methods, such as fuzzy TOPSIS3
    and IVFS TOPSIS4, only consider the MD of alternatives in the form of fuzzy and
    interval-valued fuzzy numbers correspondingly. Other approaches, such as IFS TOPSIS6
    and IVIFS TOPSIS11, consider both the MD and NMD in the ranking of the alternatives,
    but they cannot deal with the parameterization of alternatives. But, these TOPSIS
    methodologies cannot deal with when the \\(MD + NMD > 1\\). Biswas & Sarkar13
    and Garg18 developed the TOPSIS approach for PFS and IVPFS, respectively, which
    capably deals with the mentioned above shortcomings. Garg and Arora22 developed
    TOPSIS methods under IFSS, and Zulqarnain et al.26 developed TOPSIS methods under
    IVIFSS. However, these methods are not capable of handling sub-attributes of substitutes.
    Additionally, the correlation-based TOPSIS methods for PFSS36 also cannot take
    sub-attributes of reserves. Although some TOPSIS methods have made an effort to
    address concerns related to handling sub-attributes of alternatives and the information
    in intervals forms, such as the TOPSIS technique for IVIFHSS established in43
    and the AOs developed in45 under the IVPFHSS setting, these methods may not always
    be sufficient to address all specific circumstances. For example, the TOPSIS method
    in52 handles sub-attributes of alternatives but ignores the information in interval
    form, while the AOs in45 can take some of the drawbacks mentioned above but may
    not be effective in all cases. While existing AOs can capably address some of
    the concerns mentioned earlier, there are still particular circumstances where
    these operators fail to handle the problem adequately. This is where the proposed
    TOPSIS model comes in. The model is designed to deal with these concerns expertly,
    providing a more comprehensive and effective solution for ranking alternatives.
    To prove the efficiency of the proposed TOPSIS model, a comparison was conducted
    with other existing methods, and the outcome is presented in Table 10. The ranking
    of the top four patients (alternatives) indicates that the best option suggested
    by the new method aligns with the commonly used approach. This highlights the
    reliability and productivity of the projected technique, suggesting that it can
    provide accurate and reliable results in various scenarios. Table 10 Comparative
    analysis with existing models. Full size table The planned TOPSIS model successfully
    addresses the limitations of other TOPSIS methods, including the inability to
    handle the parametrization of alternatives and the lack of consideration for sub-parameters.
    Additionally, it deals with information in interval form and multi-sub-attributes
    of alternatives, which other TOPSIS methods either ignore or cannot handle. These
    concerns are essential in decision-making processes, and the proposed TOPSIS model
    is designed to handle them expertly. The effectiveness of the proposed TOPSIS
    model is demonstrated by its ability to determine the most critical patient, \\({\\mathfrak{T}}^{\\left(
    2 \\right)}\\), who needs a bed on an emergency basis using all available alternatives
    (patients). The sufficiently of alternatives considered by the proposed TOPSIS
    method pays to a similar absolute decision as the other TOPSIS methods, indicating
    the consistency and efficiency of the proposed technique. The comparison results
    in Table 10 confirm the superiority of the proposed TOPSIS model in determining
    the optimal choice among the available alternatives. The proposed TOPSIS model
    represents a significant advancement in the field of decision-making and has the
    potential to make a significant impact in various industries and applications.
    Its ability to handle various concerns and effectively determine the best alternative
    among many options makes it a valuable tool for decision-makers. The graphical
    demonstration of comparative analysis is given in the following Fig. 2. Figure
    2 Comparative analysis with the existing models. Full size image Several assessment
    standards will be applied to assess the excellence of the outcomes generated by
    the suggested methodology. These are a few popular ways to assess solutions’ reliability:
    Performing an analysis of sensitivity is helpful in evaluating the stability of
    the responses that have been collected. By altering or modifying the source information
    or variables, we can observe why the order of the alternatives fluctuates. A stable
    rating through numerous instances indicates a more satisfactory response. Assessing
    the advocated strategy’s consequences with existing standard outcomes or well-defined
    methods presents an overview of the efficacy of the outcomes. If the suggested
    technique frequently surpasses or exceeds the effectiveness of comparable methods,
    it signifies a high-quality strategy. Take confirmation and feedback from subject
    matter experts or decision-makers. Their expertise and experience may give significant
    perspectives on the overall quality of the solutions. The views of experts are
    able to determine whether or not the calculated ranks reflect their assumptions
    or desires. Evaluate decision-makers compliance with the responses delivered by
    the presented methodology. Decision-makers’ opinions and personal opinions may
    give a measure of the standard and applicability of alternatives in executing
    their specific requirements and needs. Case studies must be executed, or the suggested
    approach can be implemented in real-world situations that require decision-making.
    Analyze the findings and determine whether the approaches correlate with the actual
    efficiency or consequences in specific scenarios. Real-world executions deliver
    significant proof to demonstrate the technique''s effectiveness and reliability.
    Advantages of the proposed model The results obtained through the proposed approach,
    which integrates correlation measures with the TOPSIS technique, offer multiple
    benefits contributing to the technique’s effectiveness in MADM challenges. Let’s
    talk about these values as well as established approach performance: The suggested
    technique permits a more precise evaluation of the interactions among attributes
    or criteria within the IVPFHSS structure by including the correlation measures
    (CC and WCC). This promotes decision-making precision by identifying the fundamental
    associations and interactions between the different factors implicated. The improvement
    in sensitivity results in more reliable assessment and evaluation of decisions.
    In some cases, experts may face difficulty in making absolute choices about attributes
    and their associated sub-attributes. This model addresses this issue by operating
    the interval-valued Pythagorean fuzzy parameterization concept, which considers
    aspects'' irregular performance and associated sub-attributes. The technique promotes
    transparency in decision-making by demonstrating a clear ranking or preference
    order of the alternatives. Since this approach presents a structured and monitored
    review system, decision-makers are able to examine and interpret the results.
    Integrity facilitates effective communication, collaboration, and decision making.
    The schematic TOPSIS scheme enables professionals to deliver evaluations founded
    on two-dimensional MD and NMD values, permitting them to approve or differ with
    assessments independently in the form of intervals. This approach provides greater
    flexibility for the experts, enabling them to express their opinions. This study
    demonstrates that the proposed approach effectively deals with intricate decision-making
    circumstances. This method may effectively handle unpredictability, inconsistencies,
    and varying information scales inside the IVPFHSS structure by using correlation
    measures and TOPSIS. The method''s durability enables it to be applied in an extensive
    range of practical problems and decision scenarios, generating reliable and predictable
    consequences. The proposed TOPSIS model allows for estimates based on multiple
    sub-attributes rather than just a single parameter. This more efficient procedure
    enables experts to provide a more comprehensive and accurate assessment of the
    alternatives considered in the HSS environment. Overall, the proposed model is
    more reliable and effective than existing methods, offering greater flexibility
    and accuracy for experts. The proposed strategy is applicable to a number of sectors
    and industries where MADM obstacles be found, such as logistics, transportation,
    finance, and sustainability, among others. Its flexibility is derived from its
    ability to cope with various kinds of information and factors, which enables it
    to be effectively employed in a range of decision cases. Conclusion This work
    presents an innovative TOPSIS approach that deals with the challenges of inadequate
    information, inadequacy, and inattention in the IVPFHSS setting by assessing the
    MD and NMD of n-tuple sub-attributes for the examined aspects. The recommended
    strategy has several advantages, including addressing the ambiguous nature of
    attributes and sub-attributes, assisting specialists in offering evaluations using
    2D MD and NMD and presenting evaluations as HSS sub-attributes. The study aims
    to provide new correlation measures for IVPFHSS, such as CC and WCC. TOPSIS is
    a mechanism sub-attributes, and specialists provide to determine MADM challenges
    using the proposed CC and WCC. The extended correlation measures have been extensively
    researched and aid in determining correlation indices and closeness coefficients
    for evaluating the PIA (NIA) and rank of alternatives. A computational demonstration
    has demonstrated the suggested technique’s efficacy and uniqueness for bed allocation
    of the essential COVID-19 patient, and a comparative study was performed to validate
    its effectiveness. The intended TOPSIS model proved more credible and effective
    than conventional TOPSIS methodologies and AOs, illustrating its ability to assist
    decision-makers in the DM procedure. Future research can look into different MADM
    approaches, such as VIKOR and MABAC, to deal with DM challenges. Also, it may
    be extended to interval-valued q-rung orthopair fuzzy hypersoft sets along with
    its basic operations and several AOs with their distinct DM strategies. We can
    further integrate interval valued q-rung orthopair fuzzy hypersoft numbers with
    MADM, MCDM, MCGDM, and MAGDM strategies to carry out real-life applications such
    as material selection, diagnostics, information fusion, pattern identification,
    as well as supply chain management. The present study delivers a substantial improvement
    in the area of DM by providing an efficient TOPSIS approach that will handle various
    types of data unpredictability and contradiction issues. Data availability The
    datasets used during the current study available from the corresponding author
    on reasonable request. References Zadeh, L. A. Fuzzy sets. Inf. Control 8, 338–353
    (1965). Article   Google Scholar   Turksen, I. B. Interval valued fuzzy sets based
    on normal forms. Fuzzy Sets Syst. 20(2), 191–210 (1986). Article   MathSciNet   Google
    Scholar   Chen, C. T. Extensions of the TOPSIS for group decision-making under
    fuzzy environment. Fuzzy Sets Syst. 114(1), 1–9 (2000). Article   Google Scholar   Ashtiani,
    B., Haghighirad, F. & Makuiali Montazer, A. G. Extension of fuzzy TOPSIS method
    based on interval-valued fuzzy sets. Appl. Soft Comput. 9(2), 457–461 (2009).
    Article   Google Scholar   Atanassov, K. T. Intuitionistic fuzzy sets. In Intuitionistic
    Fuzzy Sets 1–137 (Physica, 1999). Rouyendegh, B. D., Yildizbasi, A. & Üstünyer,
    P. Intuitionistic fuzzy TOPSIS method for green supplier selection problem. Soft
    Comput. 24(3), 2215–2228 (2020). Article   Google Scholar   Atanassov, K. T. Interval
    valued intuitionistic fuzzy sets. In Intuitionistic Fuzzy Sets. Studies in Fuzziness
    and Soft Computing, vol 35 (Physica, 1999). https://doi.org/10.1007/978-3-7908-1870-3_2.
    Hung, W. L. & Wu, J. W. Correlation of intuitionistic fuzzy sets by centroid method.
    Inf. Sci. 144(1–4), 219–225 (2002). Article   MathSciNet   Google Scholar   Bustince,
    H. & Burillo, P. Correlation of interval-valued intuitionistic fuzzy sets. Fuzzy
    Sets Syst. 74(2), 237–244 (1995). Article   MathSciNet   Google Scholar   Mitchell,
    H. A correlation coefficient for intuitionistic fuzzy sets. Int. J. Intell. Syst.
    19(5), 483–490 (2004). Article   Google Scholar   Tiwari, A., Lohani, Q. D. &
    Muhuri, P. K. Interval-valued intuitionistic fuzzy TOPSIS method for supplier
    selection problem. In 2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
    1–8 (IEEE, 2020). Yager, R. R. Pythagorean membership grades in multi-criteria
    decision making. IEEE Trans. Fuzzy Syst. 22(4), 958–965 (2013). Article   Google
    Scholar   Biswas, A. & Sarkar, B. Pythagorean fuzzy TOPSIS for multicriteria group
    decision-making with unknown weight information through entropy measure. Int.
    J. Intell. Syst. 34(6), 1108–1128 (2019). Article   Google Scholar   Wei, G. &
    Lu, M. Pythagorean fuzzy power aggregation operators in multiple attribute decision
    making. Int. J. Intell. Syst. 33(1), 169–186 (2018). Article   MathSciNet   Google
    Scholar   Wang, L. & Li, N. Pythagorean fuzzy interaction power Bonferroni means
    aggregation operators in multiple attribute decision making. Int. J. Intell. Syst.
    35(1), 150–183 (2020). Article   MathSciNet   CAS   Google Scholar   Zhang, X.
    A novel approach based on similarity measure for Pythagorean fuzzy multiple criteria
    group decision making. Int. J. Intell. Syst. 31(6), 593–611 (2016). Article   Google
    Scholar   Peng, X. & Yang, Y. Fundamental properties of interval-valued Pythagorean
    fuzzy aggregation operators. Int. J. Intell. Syst. 31(5), 444–487 (2016). Article   Google
    Scholar   Garg, H. A new improved score function of an interval-valued Pythagorean
    fuzzy set based TOPSIS method. Int. J. Uncertaint. Quantif. 7(5), 463 (2017).
    Article   Google Scholar   Molodtsov, D. Soft set theory—first results. Comput.
    Math. Appl. 37(4–5), 19–31 (1999). Article   MathSciNet   Google Scholar   Maji,
    P. K., Biswas, R. & Roy, A. R. Fuzzy soft sets. J. Fuzzy Math. 9, 589–602 (2001).
    MathSciNet   Google Scholar   Maji, P. K., Biswas, R. & Roy, A. R. Intuitionistic
    fuzzy soft sets. J. Fuzzy Math. 9, 677–692 (2001). MathSciNet   Google Scholar   Garg,
    H. & Arora, R. TOPSIS method based on correlation coefficient for solving decision-making
    problems with intuitionistic fuzzy soft set information. AIMS Math. 5(4), 2944–2966
    (2020). Article   MathSciNet   Google Scholar   Jiang, Y., Tang, Y., Chen, Q.,
    Liu, H. & Tang, J. Interval-valued intuitionistic fuzzy soft sets and their properties.
    Comput. Math. Appl. 60(3), 906–918 (2010). Article   MathSciNet   Google Scholar   Ma,
    X., Qin, H. & Abawajy, J. Interval-valued intuitionistic fuzzy soft sets based
    decision making and parameter reduction. IEEE Trans. Fuzzy Syst. 30(2), 357–369
    (2020). Article   Google Scholar   Khan, M. J., Kumam, P., Liu, P. & Kumam, W.
    Another view on generalized interval valued intuitionistic fuzzy soft set and
    its applications in decision support system. J. Intell. Fuzzy Syst. 38(4), 4327–4341
    (2020). Article   Google Scholar   Zulqarnain, R. M., Xin, X. L., Saqlain, M.
    & Khan, W. A. TOPSIS method based on the correlation coefficient of interval-valued
    intuitionistic fuzzy soft sets and aggregation operators with their application
    in decision-making. J. Math. 2021, 1–16 (2021). MathSciNet   Google Scholar   Garg,
    H. & Arora, R. A nonlinear-programming methodology for multi-attribute decision-making
    problem with interval-valued intuitionistic fuzzy soft sets information. Appl.
    Intel. 48(8), 2031–2046 (2018). Article   Google Scholar   Peng, X., Yang, Y.
    & Song, J. Pythagoren fuzzy soft set and its application. Comput. Eng. 41(7),
    224–229 (2015). Google Scholar   Athira, T. M., John, S. J. & Garg, H. A novel
    entropy measure of pythagorean fuzzy soft sets. AIMS Math. 5(2), 1050–1061 (2020).
    Article   MathSciNet   Google Scholar   Athira, T. M., John, S. J. & Garg, H.
    Entropy and distance measures of pythagorean fuzzy soft sets and their applications.
    J. Intell. Fuzzy Syst. 37(3), 4071–4084 (2019). Article   Google Scholar   Riaz,
    M., Naeem, K. & Afzal, D. Pythagorean m-polar fuzzy soft sets with TOPSIS method
    for MCGDM. Punjab Univ. J. Math. 52(3), 21–46 (2020). MathSciNet   Google Scholar   Riaz,
    M., Naeem, K. & Afzal, D. A similarity measure under pythagorean fuzzy soft environment
    with applications. Comput. Appl. Math. 39(4), 1–17 (2020). Article   MathSciNet   Google
    Scholar   Jia-Hua, D., Zhang, H. & He, Y. Possibility pythagorean fuzzy soft set
    and its application. J. Intell. Fuzzy Syst. 36(1), 413–421 (2019). Article   Google
    Scholar   Zulqarnain, R. M., Siddique, I. & EI-Morsy, S. Einstein-ordered weighted
    geometric operator for pythagorean fuzzy soft set with its application to solve
    MAGDM problem. Math. Probl. Eng. 2022, 1–14 (2022). Google Scholar   Zulqarnain,
    R. M. et al. Pythagorean fuzzy soft Einstein ordered weighted average operator
    in sustainable supplier selection problem. Math. Probl. Eng. 2021, 1–16 (2021).
    Google Scholar   Zulqarnain, R. M., Xin, X. L., Siddique, I., Khan, W. A. & Yousif,
    M. A. TOPSIS method based on correlation coefficient under pythagorean fuzzy soft
    environment and its application towards green supply chain management. Sustainability
    13(4), 1642 (2021). Article   Google Scholar   Zulqarnain, R. M., Siddique, I.,
    Iampan, A. & Baleanu, D. Aggregation operators for Interval-valued Pythagorean
    fuzzy soft set with their application to solve Multi-attribute group decision
    making problem. CMES-Comput. Model. Eng. Sci. 131(3), 1717–1750 (2022). Google
    Scholar   Smarandache, F. Extension of soft set to hypersoft set, and then to
    plithogenic hypersoft set. Neutrosophic Sets Syst. 22, 168–170 (2018). Google
    Scholar   Rahman, A. U., Saeed, M., Khalifa, H. A. E. W. & Afifi, W. A. Decision
    making algorithmic techniques based on aggregation operations and similarity measures
    of possibility intuitionistic fuzzy hypersoft sets. AIMS Math. 7(3), 3866–3895
    (2022). Article   MathSciNet   Google Scholar   Rahman, A. U., Saeed, M., Alodhaibi,
    S. S. & Abd, H. Decision making algorithmic approaches based on parameterization
    of neutrosophic set under hypersoft set environment with fuzzy, intuitionistic
    fuzzy and neutrosophic settings. CMES-Comput. Model. Eng. Sci. 128(2), 743–777
    (2021). Google Scholar   Zulqarnain, R. M., Xin, X. L. & Saeed, M. Extension of
    TOPSIS method under intuitionistic fuzzy hypersoft environment based on correlation
    coefficient and aggregation operators to solve decision making problem. AIMS Math.
    6(3), 2732–2755 (2021). Article   MathSciNet   Google Scholar   Debnath, S. Interval-valued
    intuitionistic hypersoft sets and their algorithmic approach in multi-criteria
    decision making. Neutrosophic Sets Syst. 48, 226–250 (2022). Google Scholar   Zulqarnain,
    R. M. et al. Prioritization of thermal energy storage techniques using TOPSIS
    method based on correlation coefficient for interval-valued intuitionistic fuzzy
    hypersoft set. Symmetry 15(3), 615 (2023). Article   ADS   Google Scholar   Zulqarnain,
    R. M., Xin, X. L. & Saeed, M. A Development of Pythagorean fuzzy hypersoft set
    with basic operations and decision-making approach based on the correlation coefficient.
    Theory Appl. Hypersoft Set 2021, 85–106 (2021). Google Scholar   Zulqarnain, R.,
    Siddique, I., Ali, R., Jarad, F. & Iampan, A. Aggregation operators for interval-valued
    pythagorean fuzzy hypersoft set with their application to solve MCDM problem.
    CMES-Comput. Model. Eng. Sci. 135(1), 619–651 (2023). Google Scholar   Wee, L.
    E. et al. Minimizing intra-hospital transmission of COVID-19: The role of social
    distancing. J. Hosp. Infect. 105, 113–115 (2020). Article   CAS   PubMed   PubMed
    Central   Google Scholar   Asperges, E. et al. Rapid response to COVID-19 outbreak
    in Northern Italy: How to convert a classic infectious disease ward into a COVID-19
    response centre. J. Hosp. Infect. 105, 477–479 (2020). Article   PubMed   PubMed
    Central   Google Scholar   Wee, L. E. I. et al. Containing COVID-19 outside the
    isolation ward: The impact of an infection control bundle on environmental contamination
    and transmission in a cohorted general ward. Am. J. Infect. Control 48, 1056–1061
    (2020). Article   PubMed   PubMed Central   Google Scholar   Shaheen, S. et al.
    Rapid guide to the management of cardiac patients during the COVID-19 pandemic
    in Egypt: “A position statement of the Egyptian Society of Cardiology”. Egypt.
    Heart J. 72, 1–9 (2020). Google Scholar   He, H. et al. Establishment and practice
    of “dual-triage and double-buffering” model in the management of COVID-19 in large
    comprehensive hospitals. Chin. Hosp. Manag. 40, 53–55 (2020). Google Scholar   Zulqarnain,
    R. M., Siddique, I., Jarad, F., Ali, R. & Abdeljawad, T. Development of TOPSIS
    technique under pythagorean fuzzy hypersoft environment based on correlation coefficient
    and its application towards the selection of antivirus mask in COVID-19 pandemic.
    Complexity 2021, 1–27 (2021). ADS   Google Scholar   Hwang, C. L. & Yoon, K. Multiple
    Attribute Decision Making: Methods and Applications : a State-of-the-art Survey
    (Springer-Verlag, 1981). Download references Acknowledgements Research Supporting
    Project number (RSP2024R167), King Saud University, Riyadh, Saudi Arabia. Funding
    This Project is funded by King Saud University, Riyadh, Saudi Arabia. Author information
    Authors and Affiliations School of Mathematical Sciences, Zhejiang Normal University,
    Jinhua, 321004, Zhejiang, China Rana Muhammad Zulqarnain & Wen-Xiu Ma Department
    of Mathematics, University of Sargodha, Sargodha, 40100, Pakistan Imran Siddique
    Section of Mathematics, International Telematic University Uninettuno, Corso Vittorio
    Emanuele II, 39, 00186, Rome, Italy Hijaz Ahmad Department of Statistics and Operations
    Research, College of Science, King Saud University, P.O. Box 2455, 11451, Riyadh,
    Saudi Arabia Sameh Askar Contributions R.M.Z: Methodology, Conceptualization,
    Design, Analysis, Writing—original draft, Writing—review & editing. W.X.M: Methodology,
    Supervision, Writing—original draft, Writing—review & editing. I.S.: Validation,
    Design, Analysis, Writing—original draft. H.A.: Software, Formal analysis, Writing—original
    draft. S.A.: Analysis, Funding, Formal analysis, Validation, Writing—original
    draft, Writing—review & editing. Corresponding author Correspondence to Wen-Xiu
    Ma. Ethics declarations Competing interests The authors declare no competing interests.
    Additional information Publisher''s note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article''s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article''s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Zulqarnain, R.M., Ma, WX., Siddique,
    I. et al. A fair bed allocation during COVID-19 pandemic using TOPSIS technique
    based on correlation coefficient for interval-valued pythagorean fuzzy hypersoft
    set. Sci Rep 14, 7678 (2024). https://doi.org/10.1038/s41598-024-53923-2 Download
    citation Received 23 March 2023 Accepted 06 February 2024 Published 01 April 2024
    DOI https://doi.org/10.1038/s41598-024-53923-2 Share this article Anyone you share
    the following link with will be able to read this content: Get shareable link
    Provided by the Springer Nature SharedIt content-sharing initiative Keywords Pythagorean
    fuzzy hypersoft set Hypersoft set Correlation coefficient Weighted correlation
    coefficient TOPSIS MADM COVID-19 Bed allocation Subjects Applied mathematics Computational
    science Mathematics and computing Comments By submitting a comment you agree to
    abide by our Terms and Community Guidelines. If you find something abusive or
    that does not comply with our terms or guidelines please flag it as inappropriate.
    Download PDF Sections Figures References Abstract Introduction Preliminaries Correlation
    coefficient for interval valued pythagorean fuzzy hypersoft set Weighted correlation
    coefficient for interval valued pythagorean fuzzy hypersoft set Proposed TOPSIS
    approach based on correlation coefficient to resolve MADM problem under IVPFHSS
    Challenges in implementing fair bed allocation policies during the COVID-19 pandemic
    Discussion and comparative analysis Conclusion Data availability References Acknowledgements
    Funding Author information Ethics declarations Additional information Rights and
    permissions About this article Comments Advertisement Scientific Reports (Sci
    Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press releases Press
    office Contact us Discover content Journals A-Z Articles by subject Protocol Exchange
    Nature Index Publishing policies Nature portfolio policies Open access Author
    & Researcher services Reprints & permissions Research data Language editing Scientific
    editing Nature Masterclasses Research Solutions Libraries & institutions Librarian
    service & tools Librarian portal Open research Recommend to library Advertising
    & partnerships Advertising Partnerships & Services Media kits Branded content
    Professional development Nature Careers Nature Conferences Regional websites Nature
    Africa Nature China Nature India Nature Italy Nature Japan Nature Korea Nature
    Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies
    Legal notice Accessibility statement Terms & Conditions Your US state privacy
    rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A fair bed allocation during COVID-19 pandemic using TOPSIS technique based
    on correlation coefficient for interval-valued pythagorean fuzzy hypersoft set
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Lakhan A.
  - Grønli T.M.
  - Bellavista P.
  - Memon S.
  - Alharby M.
  - Thinnukool O.
  citation_count: '0'
  description: Intelligent transport systems (ITS) provide various cooperative edge
    cloud services for roadside vehicular applications. These applications offer additional
    diversity, including ticket validation across transport modes and vehicle and
    object detection to prevent road collisions. Offloading among cooperative edge
    and cloud networks plays a key role when these resources constrain devices (e.g.,
    vehicles and mobile) to offload their workloads for execution. ITS used different
    machine learning and deep learning methods for decision automation. However, the
    self-autonomous decision-making processes of these techniques require significantly
    more time and higher accuracy for the aforementioned applications on the road-unit
    side. Thus, this paper presents the new offloading ITS for IoT vehicles in cooperative
    edge cloud networks. We present the augmented convolutional neural network (ACNN)
    that trains the workloads on different edge nodes. The ACNN allows users and machine
    learning methods to work together, making decisions for offloading and scheduling
    workload execution. This paper presents an augmented federated learning scheduling
    scheme (AFLSS). An algorithmic method called AFLSS comprises different sub-schemes
    that work together in the ITS paradigm for IoT applications in transportation.
    These sub-schemes include ACNN, offloading, scheduling, and security. Simulation
    results demonstrate that, in terms of accuracy and total time for the considered
    problem, the AFLSS outperforms all existing methods.
  doi: 10.1186/s13677-024-00640-w
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Get published
    Explore Journals Books About Login Journal of Cloud Computing Advances, Systems
    and Applications About Articles Submission Guidelines Submit manuscript Research
    Open access Published: 02 April 2024 IoT workload offloading efficient intelligent
    transport system in federated ACNN integrated cooperated edge-cloud networks Abdullah
    Lakhan, Tor-Morten Grønli, Paolo Bellavista, Sajida Memon, Maher Alharby & Orawit
    Thinnukool  Journal of Cloud Computing  13, Article number: 79 (2024) Cite this
    article 82 Accesses Metrics Abstract Intelligent transport systems (ITS) provide
    various cooperative edge cloud services for roadside vehicular applications. These
    applications offer additional diversity, including ticket validation across transport
    modes and vehicle and object detection to prevent road collisions. Offloading
    among cooperative edge and cloud networks plays a key role when these resources
    constrain devices (e.g., vehicles and mobile) to offload their workloads for execution.
    ITS used different machine learning and deep learning methods for decision automation.
    However, the self-autonomous decision-making processes of these techniques require
    significantly more time and higher accuracy for the aforementioned applications
    on the road-unit side. Thus, this paper presents the new offloading ITS for IoT
    vehicles in cooperative edge cloud networks. We present the augmented convolutional
    neural network (ACNN) that trains the workloads on different edge nodes. The ACNN
    allows users and machine learning methods to work together, making decisions for
    offloading and scheduling workload execution. This paper presents an augmented
    federated learning scheduling scheme (AFLSS). An algorithmic method called AFLSS
    comprises different sub-schemes that work together in the ITS paradigm for IoT
    applications in transportation. These sub-schemes include ACNN, offloading, scheduling,
    and security. Simulation results demonstrate that, in terms of accuracy and total
    time for the considered problem, the AFLSS outperforms all existing methods. Introduction
    Internet of Things (IoT) enabled public transport has been increasing daily [1].
    Different modes of public transport, such as the metro, train, and bus, are operated
    in many metropolises around the world [2]. IoT-based applications like traffic
    prediction, ticketing, and trip planning are widely utilized in public transport.
    In European countries, it is common for trams and buses to be ridden on the road,
    while the metro and train are ridden on distinct routes [3, 4]. As a result, public
    transport providers collaborate to offer user-friendly services to passengers.
    Intelligent transport systems (ITS) are an emerging paradigm comprising artificial
    intelligence (AI) schemes, vehicles, traffic, fog, and cloud nodes. ITS offers
    automated services such as traffic prediction, vehicle collision detection, and
    pedestrian and vehicle detection based on mobility services in fog cloud networks
    [5]. Recently, significant advancements have been made in the Intelligent Transport
    Systems (ITS) paradigm, utilizing cooperative edge and cloud networks to achieve
    various objectives. Several studies such as [6,7,8] suggested secure and road
    traffic maintenance ITS systems based on ubiquitous fog cloud services in smart
    cities. However, these studies only considered one type of transport mode in their
    architectures. These studies investigated many modes of transport-enabled road
    traffic prediction [9,10,11,12,13]. The different modes, such as buses and vans,
    bikes, and cars, are considered in these studies. The pedestrian, road traffic,
    and collision types of objectives were investigated in these studies. The secure
    and big data-enabled processing of mobility-aware vehicle services for multi-mode
    transports was investigated in these studies [14,15,16,17,18,19,20]. These studies
    processed the heterogeneous data that was collected from different IoT sources.
    Using edge computing, these studies have implemented federated learning and convolutional
    neural network (CNN) schemes to process data from IoT sources. Edge computing
    is a subset of cloud computing that brings computing and storage services to radio
    networks. However, all the studies above deepened upon the black box of AI to
    make any decision without involving humans. There are many research questions
    in ITS for IoT applications in edge cloud networks. (i) Existing ITS paradigms
    only consider roadside transport modes and provide services with a homogeneous
    routing map in smart cities. (ii) The presented AI schemes implicitly decide on
    IoT data with the explicit interaction of humans. (iii) Existing ITS are straightforward
    and need to consider the many constraints of IoT applications in their research
    paradigms. This paper presents the novel Federated-IoT-enabled Intelligent Transport
    System (ITS) in Road Augmented Convolutional Neural Network (ACNN) Integrated
    Edge Networks. We consider the cooperative edge and cloud nodes together to perform
    workload offloading of vehicles and pedestrians on them. The objective is to optimize
    the time, cost, and security of IoT applications in edge cloud networks. The paper
    makes the following novel contributions to the research work: In this paper, we
    present an augmented federated learning scheduling scheme (AFLSS). Whereas AFLSS
    is an algorithmic methodology and consists of different sub-schemes: ACNN, offloading,
    scheduling, and security schemes in the ITS paradigm for transport IoT applications.
    In the paper, we present the federated learning-enabled ITS paradigm, which comprises
    different public transport modes in edge cloud networks. IoT applications such
    as traffic detection, ticketing, and trip planning are considered in this work.
    We introduce a security scheme based on a secure hashing algorithm (SHA-256) [21]
    to process the different routing data securely. We present the ACNN scheme, in
    which all parameters are configured explicitly to improve transparency, security,
    time, and cost for IoT applications in edge cloud networks. The paper has the
    following subsections: “Related work” section discusses the existence of ITS paradigms
    and their findings and limitations. “Proposed intelligent transport system” section
    shows the components of the proposed ITS paradigm. “Proposed methodology” section
    shows the algorithm framework for the considered problem. “Experimental design”
    section shows the performance evaluation and results discussion. “Conclusion and
    future” section is the research conclusion and future work of the study. Related
    work These days, the usage of IoT transportation applications has been growing
    progressively. For instance, traffic prediction, ticketing validation, accident
    monitoring, map routing searching, and trip planning require real-time data on
    transports from the mobility environment. ITS obtained the optimal results in
    these network-enabled IoT applications. In related work, we discussed road-unit-side
    services based on cooperative edge cloud networks. The network management-enabled
    networking position enabled services with the mobility suggested in [1]. IoT applications
    offload workloads for execution and can invoke network-enabled routing services
    from any location in smart cities. However, this work focused on fixed services
    and scheduling decisions made during application design. The cooperative vehicle
    and pedestrian-enabled ITS paradigms presented in these studies [2,3,4]. These
    studies suggested cooperative schemes in which vehicles can offload their workload
    and communicate with each other to avoid any collisions with pedestrians in smart
    cities. These studies presented the trajectory positioning enabled locations,
    collected the content-aware IoT applications, and processed services in ITS networks.
    However, these studies consider a single type of transport mode and fixed pedestrian
    data. The security constraints for the data have yet to be considered in these
    studies. The offloading ITS paradigms for the security of IoT data are presented
    in these studies [5,6,7]. The federated learning integrated with the graph neural
    network and optimized the ITS services for IoT applications. These applications
    collect and offload different types of data: traffic, IoT location, and vehicle
    and pedestrian data from different sources in smart cities. Federated learning
    enabled ITS to process secure data among edge cloud networks. Computing nodes,
    such as edge and cloud computing, are placed in different places in smart cities
    to support transport data and allow seamless invoking of services in other places
    in networks. SHA-256 schemes are implemented in federated learning to process
    and offload cooperative data among computing nodes. The secure big data offloading
    of traffic data among cooperatives processed by ITS paradigms in these studies
    [8,9,10]. The spatial big data networks of traffic signals and vehicle IoT devices
    are processed on different edge cloud networks in the networks. The Hadoop framework
    was deployed in these studies. The data was trained and tested based on the CNN
    scheme. Due to different modalities such as audio, video, and text, these studies
    processed the data on other CNN channels in the edge networks. However, due to
    heterogeneous nodes, data security is a critical challenge in ITS paradigms. The
    data security in heterogeneous nodes, such as edge and cloud nodes, is based on
    centralized and blockchain technologies presented in these works [11,12,13,14,15].
    The ITS paradigms utilize various security methods, such as proof of work, proof
    of credibility, and Byzantine fault-tolerant schemes, to ensure the safety of
    IoT applications running on diverse computing nodes. However, data modality training
    and validation on the centralized incurred higher power consumption, resources,
    and costs for IoT applications and service providers in ITS paradigms. The fine-grained
    and coarse-grained offloading and scheduling enabled semi-vertical federated learning
    based on CNN efficient ITS paradigms are presented in [16,17,18,19,20]. The main
    goal is to meet the training and validation of data modality at different nodes
    with the minimum resource consumption and time at heterogeneous computing nodes.
    Federated learning allows different edge computing nodes to train different modality
    data and aggregate them on the centralized node for making the final decision.
    Data security, time, and cost constraints are optimized in these paradigms. These
    studies [21,22,23,24,25] studies suggested joint offloading and scheduling schemes
    for vehicular applications in cooperative edge cloud networks. The goal was to
    minimize application processing services’ response time, energy, and cost. These
    studies [26, 27, 25, 28,29,30] suggested fine-grained and coarse-grained offloading
    and scheduling schemes for vehicular applications. These studies considered security
    constraints during the offloading and scheduling of IoT vehicular applications
    on edge cloud networks. The secure coarse-grained offloading scheme (SCOS) and
    fine-grained secure offloading scheme (SFOS) in these studies suggested the security
    and time mechanism on different nodes such as local vehicle nodes, wireless nodes,
    and cloud nodes. To our knowledge, Federated-IoT-enabled Intelligent Transport
    Systems (ITS) in Road Augmented Convolutional Neural Network (ACNN) Integrated
    Edge Networks have yet to be studied. The augmented CNN in federated learning
    added the setting of a method in which humans and machines perform together instead
    of decisions made by the machine learning algorithms. We integrated federated
    learning on different edge cloud networks to train and test data based on CNN
    methods. Proposed intelligent transport system This paper considers the different
    transport modes such as trams, buses, metros, and trains with different routing
    operations and data in smart cities. This paper presents the ITS system as shown
    in Fig. 1 consisting of different levels and components. The local routing-level
    servers collect and provide infrastructure-level information in ITS. These are
    distinct local servers such as cars, pedestrians, buses, and trams managed by
    their local routing operation in their regions. However, the local routing level
    only collects information from vehicles and pedestrians through the signal and
    cameras. Fig. 1 IoT workload offloading efficient intelligent transport system
    in federated ACNN integrated cooperated edge-cloud networks Full size image The
    operation level consisted of heterogeneous, scalable edge nodes in distributed
    ITS at the smart city level. Each edge node trains the data collected from the
    routing level using an augmented convolutional neural network (ACNN). Due to the
    volume of data, we divided the transport data into local datasets based on federated
    learning. Level 3 is the highly scalable, centralized cloud-based server that
    aggregates all trained data offloaded from edge computing and aggregates them
    as a single large dataset. We detail the proposed ITS paradigm’s process in the
    problem formulation. We differentiate all transport by their local routes in the
    proposed ITS paradigm. In our work, we assume that L number of route data points
    is generated by local routing. Each local server has speed and fixed resources,
    e.g., and . We consider the number of pedestrians walking on the road-unit-side
    on the designated routes in smart cities. In ITS, we are also considering ticketing
    IoT applications. Therefore, passengers can pay and validate tickets in different
    transports. Therefore, we differentiate the transport modes by their types. The
    number of cars is represented by . The number of public buses is represented by
    . We present the trams in this expression . As defined above, each component (e.g.,
    pedestrian, car, tram, and bus) has its own generated local routes. Therefore,
    for the operation, all data is offloaded to the number of edge nodes, e.g., .
    Each edge and cloud node is scalable and heterogeneous and contains the resources
    and speed in the ITS paradigm. Each edge node trained the offloaded routing data
    based on ACNN deep learning methods based on given parameters. We consider the
    tuple of parameters of ACNN in the following way: . We represent the trained data
    based on their features by . The particular dataset expressed as s and S shows
    the aggregated dataset of all merged trained datasets into one centralized server.
    We assume that the considered problem has different constraints, e.g., as shown
    in Fig. 1. It is assumed that IoT applications, such as ticketing, pedestrian
    and vehicle detection, the timetable of transport routing, and searching by ,
    are utilized. Therefore, each IoT application has many constraints, such as ,
    during their execution in the ITS paradigm on the road-unit side. We determined
    the local routing time in the following way:. (1) Equation (1) determines the
    local processing time of local routing of different transports in level 1. (2)
    Equation (2) determines the edge processing time of operation on different offloaded
    data from the local routing in ITS. (3) Equation (2) determines the edge processing
    time of operation on different offloaded data from the local routing in ITS. (4)
    Equation (4) shows the communication time between local routing to the edge and
    cloud nodes during offloading and downloading data. (5) Equation (5) determines
    the encryption process of all device requests with public and private keys. (6)
    Equation (6) determines the encryption process of all requests on all devices
    with public and private keys. The accuracy of the prediction is determined in
    the following way:. (7) Equation (7) determined the accuracy of the applications
    for the given tasks on the request data. The total time for the IoT applications
    is determined in the following way: (8) Equation (8) shows the total processing
    and communication time on different levels during processing in the ITS paradigm.
    Proposed methodology This paper proposes the AFLSS algorithm methodology, which
    consists of different schemes. AFLSS consisted of offloading, CNN, and scheduling
    schemes to solve the combinatorial problem with the given constraints. Algorithm
    1 AFLSS algorithm methodology In AFLSS, all IoT applications are connected to
    the centralized server and invoke services from the centralized server. The centralized
    server is federated learning-enabled, which offers transport services based on
    trained and merged data from different datasets. IoT applications perform parallel
    offloading of data, invoke services from the local routing server, and are centralized
    during execution in the ITS paradigm. AFLSS Algorithm 1 has the following steps:
    The IoT applications offload the data securely to the local routing servers. At
    the operation level, the operation servers at the edge nodes train and validate
    the offload. The ACNN trained the data locally at the edge servers and offloaded
    the centralized server for further decision with the explicit parameters. All
    the edge servers and centralized servers in the ITS paradigm are collaborated
    based on oriental federated learning schemes. Fig. 2 Scenario: AFLSS algorithm
    methodology working schemes Full size image Due to the many sub-schemes, we discussed
    the AFLSS methodology in detail based on the given scenario, as shown in Fig.
    2. The local routing servers collected the vehicle, pedestrian, and signal data
    from their distinct routes. The data collection process collects data from different
    sources, such as pedestrian devices, vehicle devices, traffic signals, and routes.
    It is assumed to be the data input for the edge nodes. The edge nodes are heterogeneous
    and process the locally generated raw data with the pre-processing technique and
    train them based on an augmented CNN scheme before offloading the centralized
    server for further decision-making. The augmented CNN is explainable and has customized
    layers, such as training and security, added to the CNN algorithm. The decision
    parameters, such as training, security, and offloading, are explicitly set in
    our ACNN scheme. All the locally trained datasets at edge nodes, e.g., S, L, K,
    are offloaded to the centralized server for the final decision and invoking services
    for IoT applications. The centralized process scheduled the merged trained datasets
    for the IoT applications and offered them many services in the ITS paradigm, as
    shown in Fig. 2. Local routing data processing Local routing is a complex process
    in smart cities. In particular, along with pedestrians, different vehicle modes
    are used on the road. For instance, electric scooters, cars, bicycles, trucks,
    buses, and other vehicles. Therefore, safety for pedestrians is paramount. Consequently,
    we suggest the life safety ITS paradigm, which collects information on all transport
    modes and traffic data from different sources as part of the local routing. In
    this paper, the local routing shows the basis of data collection from various
    IoT devices during execution in intelligent cities. We present the local routing
    scheme with other steps as shown in Algorithm 2. The local routing servers only
    collect information in the form of data from different sources. The data has various
    forms, including images, location coordinates, and text. Algorithm 2 shows data
    collection from other sources, and servers offload data in real-time to the edge
    servers for further pre-processing and training based on given requirements. Algorithm
    2 AFLSS algorithm methodology Edge nodes ACNN scheme We integrated the security
    in the ACNN scheme, where the security of offloaded workloads was validated based
    on deadline, resource, and time constraints in the system. In our framework, we
    consider the different heterogeneous edge nodes that are placed in the local smart
    cities. These edge nodes are heterogeneous, which means they have different computing
    capabilities and resource thresholds. We connected all edge nodes through federated
    learning as operation nodes, where distinct datasets are trained at local edges,
    and the security of their datasets is maintained at the edge nodes. Algorithm
    3 determines the training of datasets on different edge nodes based on augmented
    CNN. We have integrated the different datasets for transport applications on different
    edge nodes, pedestrians, vehicle detection, trams, and traffic. We trained datasets
    on local autonomous edge nodes using augmented CNN and federated learning. The
    trained weights of the datasets were then sent to the aggregated node so that
    they could be run. We define the steps of Algorithm 3 in the following way: The
    Algorithm 3 takes the input as different computing nodes, datasets, and hyper-parameters
    on different channels on different computing nodes. This expression S are federated
    weights, where trained datasets are offloaded to the aggregated for computing.
    We set the different hyper-parameters of augmented CNN, such as convolutional,
    hidden, dense, and fully connected layers, with the modification. Here, we added
    the security layer for encrypting the trained models before sharing them with
    the aggregated node. Each trained model has weight and security with a distinct
    index, as shown in steps 1 to 6. Each edge node implements an augmented CNN where
    channels process the different dataset values and extract their features, as shown
    in steps 7 to 34. We encrypted the trained model with additional two-way verification
    based on advanced standard encryption (AES-256) asymmetric schemes. Both public
    and private keys are used to encrypt and decrypt trained models for merge and
    execution. The softmax function gets the results based on the probability function
    for different channels on different edge nodes. The softmax function has different
    probability values, such as 0.3, 0.7, and 0.6. The probability value, e.g., 0.3
    in the softmax function, determined that we trained according to the given requirements,
    and it is the same for all nodes. Algorithm 3 Secure edge assisted ACNN scheme
    Federated learning enabled services and scheduling scheme We implemented federated
    learning with additional nodes to improve the efficiency of transport services
    for different kinds of IoT applications. For instance, ticket validation and pedestrian
    and vehicle detection help avoid collisions in smart cities. We trained all workloads
    at different layer edge nodes and compared them to cloud computing for service
    providing to the applications. The scalability and interoperability of cooperated
    edge and cloud nodes are flexible and can be increased and decreased at runtime.
    We have defined the federated learning process in Algorithm 4, which consists
    of various steps. In steps 1 to 5, the locally trained datasets from different
    edge nodes are offloaded and merged at the aggregated node for decision-making
    and analysis for other applications. From steps 11 to 14, we performed federated
    learning. In our case, federated learning is the central server, where different
    edge nodes and applications are integrated to perform their tasks. Algorithm 4
    performs the different functions in federated learning, such as the Total function
    and accuracy for different applications based on given task data. The decision
    will be based on a trained aggregated model for the assigned tasks. Federated
    learning is implemented at the central worker nodes, offering different services
    and transport modes to pedestrians in smart cities. Algorithm 4 Federated learning
    enabled scheduling and aggregation scheme Time complexity and space complexity
    This paper considers workload offloading and scheduling in cooperative edge cloud
    networks. We considered the workloads, such as pedestrians, transport modes, and
    training data. To solve the problem, we presented the AFLSS algorithm scheme,
    which consisted of different schemes. Therefore, we determined the time complexity
    by O(NlogN). Meanwhile, N shows the different number of schemes involved in AFLSS,
    and log(N) shows the number of operations performed in the various schemes during
    workload offloading and scheduling in cooperated edge cloud networks. Experimental
    design In the performance evaluation, we show the performance of proposed methods
    for different transport applications on road-unit side services. The simulator
    consisted of different parameters, as shown in Table 1. We consider the different
    simulation parameters for conducting the different experiments for the problem.
    We repeated the experiments many times due to leveraging new data into the simulator
    and obtaining optimal results on real case scenarios. Simulation parameters shown
    in Table 1 consisted of different parameter details such as developing languages,
    computing configuration, data, and different configuration setting values for
    experiments. We configured various JAVA, Python, Kotlin, and C programming parameters.
    Table 1 Simulation parameters Full size table We exploited the real-time transport
    data of Reuter Oslo, which is available publicly for experiments. The data has
    a variety of parameters. However, we use only a few parameters according to our
    research problem. The considered parameters are transport type, positioning (start
    and end), route type with distinct identification numbers (ID), and traffic signals
    in smart cities. Table 2 illustrates the dataset of roadside unit paths in Oslo
    city. We downloaded the dataset of OSLO city from the Reuter website, the biggest
    public transport company in Oslo, Noways. Table 2 shows different routers for
    different transports and pedestrians and single waiting times in Oslo city. The
    implemented data is publicly available, updated, and leveraged after 24 hours.
    Table 2 Dataset: Transports and road-unit Oslo Full size table Implementation
    of AFLSS framework We denoted the implementation components of the simulator in
    different classes, as shown in Fig. 3. We told the elements of object-oriented
    programming (OOP). In the simulator, we can reuse components from top-level abstraction
    to implementation, such as overloading and overriding. The method overloading
    shows that different transport methods, such as trams, trains, buses, and metros,
    can be implemented as inherited methods. In essence, Object-Oriented Programming
    (OOP) and Unified Modeling Language (UML) collaborate to encourage the generation
    of reusable elements by providing an organized and visual method for software
    design. OOP principles steer the crafting of modular and encapsulated classes,
    whereas UML diagrams function as tools for communication and visualization, facilitating
    the design and depiction of these reusable elements within a system. We denoted
    the simulation of the proposed work in different unified modeling languages (UML).
    The simulator AFLSS starts with editable encapsulated interfaces, which can be
    changed with depreciation. These interfaces are inherited from sub-classes such
    as routing, operation, and aggregated classes. The polymorphism allows multi-mode
    public transport methods to be implemented with different parameters for a single
    objective in IoT transport applications. Fig. 3 Unified modeling language-assisted
    simulator classes Full size image Object-Oriented Programming (OOP) and Unified
    Modeling Language (UML) allow the simulator to change the different versions,
    such as reusable elements, through various classes and object-based concepts.
    After implementing the simulator, the method depreciation and version will be
    changed at runtime. Therefore, the code with different versions will be provided
    in future updates, and a new programming interface application (API) will be upgraded
    for new features in the simulator. Case study augmented CNN We executed the different
    cases in the simulator, such as the vehicle-to-vehicle information and pedestrian-to-vehicle
    information on the other road unit sides, as shown in Fig. 4. Therefore, all transport
    applications have various services for public transport traffic in smart cities.
    It allows pedestrians and users to get information about public transport from
    other complex streets available for travel. The pedestrian can choose the optimal
    path for walking with less traffic, and the vehicle can choose the best route
    for driving and executing in intelligent cities. Therefore, the simulator is flexible,
    and more road unit-side services can be implemented, as shown in Fig. 4. Fig.
    4 Cooperative edge cloud road services for pedestrian and vehicle detection Full
    size image Result analysis We design the setting of the experiments with different
    constraints such as accuracy, precision, recall, time, security, and resources
    during offloading and scheduling for vehicular applications. In the result analysis,
    we conducted different experiments according to the proposed architecture 1. The
    architecture consisted of local routing, operational, and scheduling data for
    IoT transport applications. Table 3 shows the analysis of different metrics for
    all IoT applications based on different methods. We compare the performances of
    methods with metrics such as method, application, total (minutes), accuracy, F1
    score, recall, and precision. We determined the total time in minutes, and accuracy
    and other metrics are determined in percentages. Table 3 shows that AFLSS outperforms,
    has higher accuracy due to the augmented amendment in methods, and has a lower
    total time due to dividing the tasks offloaded into different edge and cloud networks.
    Table 4 shows the performances of different vehicles during pedestrian detection
    and vehicle detection on the streets. IoT transport applications show pedestrian
    information availability to vehicles. Therefore, traffic, pedestrian, and vehicle
    information is collected at runtime. Table 4 shows that AFLSS outperformed all
    other vehicle and pedestrian detection methods in smart cities. We processed the
    50,000 pedestrian image data and 50,0000 image data for training methods and detecting
    their object nature in smart cities. Figure 5 shows the performances of different
    methods in routing local data processing for ticket validation among different
    transport modes. These three methods, such as AFLSS, CNN, and FCNN, were integrated
    into the simulation environment to analyze the performance of local routing cooperation
    among different transport modes. The local routing computing nodes offload their
    data to the edge nodes for further analysis and training based on designated parameters.
    The parameters are data size, offloading time, ticket validation, and training
    accuracy to perform the different tasks. We consider the different tasks, such
    as vehicle cooperation among different transport modes due to customers traveling
    with the same ticket. Figure 5 shows that AFLSS outperformed CNN and FCNN regarding
    ticket validation among transport modes. The main reason is that we trained and
    divided the process into different parts, such as local routing, operation, and
    scheduling, and trained data on edge nodes with the minimum delay and higher accuracy.
    The aggregation server combines different transports from different edge servers
    and offers services to the IoT application. It gained optimal results in terms
    of ticket validation accuracy as compared to existing federated learning and CNN
    methods for IoT transport applications. The main reason for the AFLSS algorithm
    is to perform optimally than existing CNN and FCNN is that, AFLSS augments and
    changes the parameter and feature settings at the runtime of offloading and scheduling
    when tasks are missing their deadlines. However, existing CNN and FCNN are fixed
    and do not allow to change the parameters, therefore, many tasks missed their
    deadlines and consumed much more resources as compared to AFLSS in simulation.
    Table 3 IoT Transport application results analysis Full size table Table 4 Pedestrian
    and vehicle detection results analysis Full size table Fig. 5 Performance of methods
    local routing and IoT ticket validation application among different transport
    modes Full size image Fig. 6 Road cooperation performances for different IoT transport
    applications Full size image Figure 6 shows the performances of different methods
    for road cooperation between vehicles in cooperative edge cloud networks. We simulated
    this situation with different configuration parameters, such as 500 locations,
    2000 road paths, and 100 to 200 meters for vehicle detection and collision avoidance.
    Figure 6 shows that AFLSS has higher accuracy in complex locations to detect vehicles
    and avoid collisions than CNN FCNN studies. The existing methods suffered from
    augmented properties, where deep learning and machine learning models depend on
    black-box decision models. The augmented properties allow the methods to decide
    with machines and humans to avoid any wrong vehicle collision. Therefore, the
    proposed work has higher accuracy than existing studies. We conducted the experiments
    based on real-time levering data from the dataset, to determine the traffic of
    transports on the road-unit side in smart cities. Therefore, we are considering
    the different cases such as pedestrian detection on the road-unit side, vehicle
    ratio, and transport detection on the roadside. AFLSS, outperformed in all cases
    as compared to existing methods to run the transport in the simulation. Fig. 7
    IoT transport implementation at augmented CNN in smart cities Full size image
    Figure 7 shows different scenarios such as P2V (pedestrian to vehicle), V2V (vehicle
    to vehicle), and V2O (vehicle to object) performances in IoT applications in cooperative
    edge cloud networks. As shown in Fig. 7, the performance determined that road
    cooperation among moving and stationary objects can be detected and processed
    to avoid collisions on the road-unit side. Figure 7 shows that the proposed method
    obtained low total delays compared to existing methods to perform the aforementioned
    tasks in cooperative edge cloud networks. We implemented different cases where
    different applications and services could be used to avoid network collision and
    traffic detection. Traffic prediction and service transport mode availability
    can be determined for traveling in networking. Predicting traffic on different
    types of public transportation is a complicated and always-growing field that
    combines large data sets, advanced machine learning techniques, and real-time
    data flows. The main goal is to improve transportation systems, make operations
    more efficient, and make commuting better for everyone. Therefore, optimal results
    were obtained in different cases during the simulation, as shown in Fig. 7. The
    proposed AFLSS outperformed in all cases and obtained the optimal results during
    the simulation of given data for experiments. Fig. 8 Security offloading trade-off
    between time and validity Full size image We exploited the different methods as
    baseline methods. This baseline research [26, 25, 27,28,29,30] approach implemented
    detailed and broad offloading and scheduling plans for vehicular applications.
    These investigations took into account security limitations while offloading and
    scheduling IoT vehicular applications on edge cloud networks. The studies introduced
    the Secure Coarse-Grained Offloading Scheme (SCOS) and Fine-Grained Secure Offloading
    Scheme (SFOS), outlining security and time mechanisms across various nodes, including
    local vehicle nodes, wireless nodes, and cloud nodes. Figure 8 shows the performance
    of security validity with time constraints between vehicles and edge and cloud
    servers. The x-axis shows the number of different multi-modal transport vehicles
    offload workloads with the consumer IoT devices with the parallel sequences and
    y-axis shows the security validity ratio with the time constraints in edge cloud
    networks. We integrated these baseline approaches with the proposed scheme such
    as AFLSS, FSOS, and CSOS. Figure 8 shows that AFLSS has a higher ratio of validation
    according to time constraints as compared to existing coarse-grained and fine-grained
    offloading schemes for transport applications. With the random stochastic vehicle
    IoT workloads such as x=32 transport vehicles have lower time constraints ratio
    with the security validation such as Y 0.727078 with AFLSS as compared to existing
    studies. However, other approaches with the x=16 and x=32 have higher time delay
    with the security validity among nodes as compared to proposed AFLSS during offloading
    and scheduling edge cloud networks. We devised an ACNN security mechanism based
    on the AES-256 algorithm where time, resource, deadline, and security validity
    are the features of tasks during offloading and scheduling on different IoT consumers’
    edge cloud networks. Therefore, AFLSS has optimal results of security validation
    with the trade-off between time and security among different offloading and scheduling
    workloads on different computing nodes for vehicular applications. Finding and
    limitation The main finding of this work is to design a practical cooperative
    ITS system for IoT transport applications. We have enhanced resource management,
    time utilization, and security constraints for IoT applications. However, resource
    instability leads to higher costs and overhead in resource allocations. Therefore,
    we aim to address these factors and constraints in the next version of our work
    and extend its application to other countries in the intelligent transport system.
    Conclusion and future In contemporary times, the intelligent transport system
    (ITS) offers various cooperative edge cloud services for roadside vehicular applications.
    The applications exhibited diverse functionalities, including ticket validation
    across transport modes and detecting vehicles and objects to prevent road collisions.
    The offloading process between cooperative edge and cloud networks was pivotal
    when devices (e.g., vehicles and mobile devices) had resource constraints, prompting
    them to offload their workloads for execution. ITS employed various machine learning
    and deep learning methods for decision automation. However, the autonomous decisions
    made by these techniques in the past consumed a significant amount of time and
    required higher accuracy for the aforementioned applications on the road-unit
    side. This research paper presents a novel offloading Intelligent Transport System
    (ITS) for IoT vehicles within cooperative edge cloud networks. The augmented convolutional
    neural network (ACNN) was presented as a model that trained workloads on different
    edge nodes. The ACNN facilitated collaboration between users and machine methods,
    enabling them to make decisions regarding offloading and scheduling workload execution
    jointly. The paper also introduced an augmented federated learning scheduling
    scheme (AFLSS). AFLSS, an algorithmic method, comprised various sub-schemes that
    collaboratively operated within the ITS paradigm for IoT applications in transportation.
    These sub-schemes encompass ACNN, offloading, scheduling, and security. Simulation
    results indicated that AFLSS surpassed all existing methods regarding accuracy
    and total time for the problem. In the future, we plan to apply blockchain technology
    to address the interoperability and cost issues in the proposed architecture for
    IoT transport applications. Availability of data and materials The data and code
    are available on the following URL: https://github.com/ABDULLAH-RAZA/Pedestrian-Vehicle-Non-Dataset.
    We added the different data additions with the different publications. References
    Liu Q, Liu R, Zhang Y, Yuan Y, Wang Z, Yang H, Ye L, Guizani M, Thompson JS (2023)
    Management of positioning functions in cellular networks for time-sensitive transportation
    applications. IEEE Trans Intell Transp Syst Autili M, Chen L, Englund C, Pompilio
    C, Tivoli M (2021) Cooperative intelligent transport systems: Choreography-based
    urban traffic coordination. IEEE Trans Intell Transp Syst 22(4):2088–2099 Article   Google
    Scholar   Ahmed U, Srivastava G, Djenouri Y, Lin JCW (2021) Deviation point curriculum
    learning for trajectory outlier detection in cooperative intelligent transport
    systems. IEEE Trans Intell Transp Syst 23(9):16514–16523 Article   Google Scholar   Richter
    A, Löwner MO, Ebendt R, Scholz M (2020) Towards an integrated urban development
    considering novel intelligent transportation systems: Urban development considering
    novel transport. Technol Forecast Soc Chang 155:119970 Article   Google Scholar   Gupta
    BB, Gaurav A, Marín EC, Alhalabi W (2022) Novel graph-based machine learning technique
    to secure smart vehicles in intelligent transportation systems. IEEE Trans Intell
    Transp Syst 24(8, August 2023):8483–8491 Arthurs P, Gillam L, Krause P, Wang N,
    Halder K, Mouzakitis A (2021) A taxonomy and survey of edge cloud computing for
    intelligent transportation systems and connected vehicles. IEEE Trans Intell Transp
    Syst 23(7, July 2022):6206–6221 Telang S, Chel A, Nemade A, Kaushik G (2021) Intelligent
    transport system for a smart city. Security and privacy applications for smart
    city development. Springer, p 171–187 Fantin Irudaya Raj E, Appadurai M (2022)
    Internet of things-based smart transportation system for smart cities. In: Intelligent
    Systems for Social Good: Theory and Practice, Springer, p 39–50 Liu C, Ke L (2023)
    Cloud assisted Internet of things intelligent transportation system and the traffic
    control system in the smart city. J Control Decis 10(2):174–187. Taylor \\& Francis.
    Article   Google Scholar   Kaffash S, Nguyen AT, Zhu J (2021) Big data algorithms
    and applications in intelligent transportation system: a review and bibliometric
    analysis. Int J Prod Econ 231:107868 Article   Google Scholar   Kumar R, Kumar
    P, Tripathi R, Gupta GP, Kumar N, Hassan MM (2021) A privacy-preserving-based
    secure framework using blockchain-enabled deep-learning in cooperative intelligent
    transport system. IEEE Trans Intell Transp Syst 23(9):16492–16503 Article   Google
    Scholar   Lv Z, Li Y, Feng H, Lv H (2021) Deep learning for security in digital
    twins of cooperative intelligent transportation systems. IEEE Trans Intell Transp
    Syst 23(9):16666–16675 Article   Google Scholar   Ahmed I, Zhang Y, Jeon G, Lin
    W, Khosravi MR, Qi L (2022) A blockchain-and artificial intelligence-enabled smart
    iot framework for sustainable city. Int J Intell Syst 37(9):6493–6507 Article   Google
    Scholar   Liao S, Wu J, Bashir AK, Yang W, Li J, Tariq U (2021) Digital twin consensus
    for blockchain-enabled intelligent transportation systems in smart cities. IEEE
    Trans Intell Transp Syst 23(11):22619–22629 Article   Google Scholar   Zhao J,
    Chang X, Feng Y, Liu CH, Liu N (2022) Participant selection for federated learning
    with heterogeneous data in intelligent transport system. IEEE Trans Intell Transp
    Syst 24(1):1106–1115 Article   Google Scholar   Manias DM, Shami A (2021) Making
    a case for federated learning in the internet of vehicles and intelligent transportation
    systems. IEEE Netw 35(3):88–94 Article   Google Scholar   Zhang C, Zhang S, James
    J, Yu S (2021) Fastgnn: A topological information protected federated learning
    approach for traffic speed forecasting. IEEE Trans Ind Inform 17(12):8464–8474
    Article   Google Scholar   Lim WYB, Huang J, Xiong Z, Kang J, Niyato D, Hua XS,
    Leung C, Miao C (2021) Towards federated learning in uav-enabled internet of vehicles:
    A multi-dimensional contract-matching approach. IEEE Trans Intell Transp Syst
    22(8):5140–5154 Article   Google Scholar   Zhu Y, Liu Y, James J, Yuan X (2021)
    Semi-supervised federated learning for travel mode identification from gps trajectories.
    IEEE Trans Intell Transp Syst 23(3):2380–2391 Article   Google Scholar   Gadekallu
    TR, Pham QV, Huynh-The T, Bhattacharya S, Maddikunta PKR, Liyanage M (2021) Federated
    learning for big data: A survey on opportunities, applications, and future directions.
    arXiv preprint arXiv:2110.04160 Bensalem H, Blaquière Y, Savaria Y (2021) Acceleration
    of the secure hash algorithm-256 (sha-256) on an fpga-cpu cluster using opencl.
    In: 2021 IEEE international symposium on circuits and systems (ISCAS), IEEE, p
    1–5 Tang H, Wu H, Zhao Y, Li R (2021) Joint computation offloading and resource
    allocation under task-overflowed situations in mobile-edge computing. IEEE Trans
    Netw Serv Manag 19(2):1539–1553 Article   Google Scholar   Qu G, Wu H, Li R, Jiao
    P (2021) Dmro: A deep meta reinforcement learning-based task offloading framework
    for edge-cloud computing. IEEE Trans Netw Serv Manag 18(3):3448–3459 Article   Google
    Scholar   Tang C, Wu H (2022) Joint optimization of task caching and computation
    offloading in vehicular edge computing. Peer Peer Netw Appl 15:854–869 Tang C,
    Yan G, Wu H, Zhu C (2023) Computation offloading and resource allocation in failure-aware
    vehicular edge computing. IEEE Trans Consum Electron Huang X, Yu R, Xie S, Zhang
    Y (2020) Task-container matching game for computation offloading in vehicular
    edge computing and networks. IEEE Trans Intell Transp Syst 22(10, October 2021):6242–6255
    Article   Google Scholar   Lakhan A, Groenli TM, Muhammad G, Tiwari P (2024) Evolutionary
    meta-heuristic offloading and scheduling schemes enabled industrial cyber-physical
    system. IEEE Syst J Lakhan A, Mohammed MA, Abdulkareem KH, Deveci M, Marhoon HA,
    Nedoma J, Martinek R (2022) A multi-objectives framework for secure blockchain
    in fog-cloud network of vehicle-to-infrastructure applications. Knowledge-Based
    Syst 15:854–869 Tang C, Wu H (2022) Reputation-based service provisioning for
    vehicular fog computing. J Syst Archit 131:102735 Article   Google Scholar   Xu
    X, Shen B, Yin X, Khosravi MR, Wu H, Qi L, Wan S (2020) Edge server quantification
    and placement for offloading social media services in industrial cognitive IoV.
    IEEE Trans Ind Inform 17(4):2910–2918 Article   Google Scholar   Download references
    Acknowledgements This research was developed at the Ubiquitous Computing Technology
    Laboratory at Kristiania University College and funded by the Embedded System
    and Computational Science lab at the College of Arts, Media, and Technology, Chiang
    Mai University. Funding This research received partial support from both Kristiania
    University College and Chiang Mai University, which collaborated on this project.
    The author acknowledges the support from Chiang Mai University, and the research
    group of Embedded Systems and Computational Science, which received partial support
    through donations. Author information Authors and Affiliations Ubiquitous Computing
    Technology Laboratory, Kristiania University College, Kirkegata 24-26, 0153, Oslo,
    Norway Abdullah Lakhan & Tor-Morten Grønli Department of Computer Science and
    Engineering, Alma Mater Studiorum - Universitá di Bologna, Bologna, Italy Paolo
    Bellavista Department of Computer System Engineering, Dawood University of Engineering
    and Technology, Karachi, Pakistan Sajida Memon Computer Science Department, Taibah
    University, Medina, Saudi Arabia Maher Alharby Embedded System and Computational
    Science Lab, College of Arts, Media and Technology, Chiang Mai University, Chiang
    Mai, 50200, Thailand Orawit Thinnukool Contributions Abdullah Lakhan: Write the
    original manuscript, data analysis, and methodology. Tor-Morten Grønli: Supervision.
    Paolo Bellavista: Data and architecture design. Sajida Memon: Design experiment
    and results. Maher Alharby: Experiment and Result analysis Orawit Thinnukool:
    Funding, manuscript reading, improvement, and refinement. Corresponding author
    Correspondence to Orawit Thinnukool. Ethics declarations Ethics approval and consent
    to participate For all the algorithms, simulations, figures, and tables we wrote,
    there is no copyright issue with the existing studies. Competing interests The
    authors declare no competing interests. Additional information Publisher''s Note
    Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article''s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article''s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Lakhan, A., Grønli,
    TM., Bellavista, P. et al. IoT workload offloading efficient intelligent transport
    system in federated ACNN integrated cooperated edge-cloud networks. J Cloud Comp
    13, 79 (2024). https://doi.org/10.1186/s13677-024-00640-w Download citation Received
    28 January 2024 Accepted 17 March 2024 Published 02 April 2024 DOI https://doi.org/10.1186/s13677-024-00640-w
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Edge AI ACNN Federated learning Cloud Transport applications
    Download PDF Collection Edge-cloud computing cooperation for task offloading in
    internet-of-things Sections Figures References Abstract Introduction Related work
    Proposed intelligent transport system Proposed methodology Experimental design
    Conclusion and future Availability of data and materials References Acknowledgements
    Funding Author information Ethics declarations Additional information Rights and
    permissions About this article Advertisement Support and Contact Jobs Language
    editing for authors Scientific editing for authors Leave feedback Terms and conditions
    Privacy statement Accessibility Cookies Follow SpringerOpen By using this website,
    you agree to our Terms and Conditions, Your US state privacy rights, Privacy statement
    and Cookies policy. Your privacy choices/Manage cookies we use in the preference
    centre. © 2024 BioMed Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: Journal of Cloud Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: IoT workload offloading efficient intelligent transport system in federated
    ACNN integrated cooperated edge-cloud networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hoteit R.
  - Hassoun A.
  - Bou Sanayeh E.
  - Saade M.C.
  - Honein-AbouHaidar G.
  - Akl E.A.
  citation_count: '0'
  description: 'Background: The coronavirus disease 2019 (COVID-19) pandemic has strained
    healthcare systems globally, particularly in terms of access to medicines. Lebanon
    has been greatly affected by the pandemic, having faced concomitant financial
    and economic crises. The objective of the study was to understand the experiences
    of patients with COVID-19 in Lebanon, as well as those of their families, and
    healthcare providers, with regards to their treatment decisions and accessibility
    to COVID-19 medicines. Methods: For this qualitative study, we conducted 28 semi-structured
    interviews. We used purposive sampling to recruit participants with a diverse
    range of perspectives. The data collection phase spanned from August to November
    2021 and was conducted virtually. After transcribing and translating the interviews,
    we employed thematic analysis to identify recurring themes and patterns. Results:
    In total, 28 individuals participated in this study. Participants highlighted
    challenges owing to the COVID-19 pandemic and economic crisis. Accessing COVID-19
    medicines posed major hurdles for physicians and patients, given limited availability,
    global shortages, local circumstances, community hoarding and stockpiling by pharmacies.
    Providers based treatment decisions on research, local and international practice
    guidelines, experiences and expert feedback. Patients sought information from
    social media, community members and physicians, as well as through word of mouth.
    Accessing medicines involved navigating the healthcare system, the black market,
    charities, personal networks and political parties and sourcing from abroad. The
    medicines were either free, subsidized or at inflated costs. Conclusions: This
    study highlights the diversity and complexity of factors influencing decision-making
    and accessing medicines during the COVID-19 pandemic in Lebanon. Future research
    should explore strategies for ensuring medicine access during crises, drawing
    insights from comparative studies across different countries.'
  doi: 10.1186/s12961-024-01131-9
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Explore
    journals Get published About BMC Login Health Research Policy and Systems Home
    About Articles Submission Guidelines Submit manuscript Research Open access Published:
    27 March 2024 Choosing and accessing COVID-19 treatment options: a qualitative
    study with patients, caregivers, and health care providers in Lebanon Reem Hoteit
    , Aya Hassoun, Elie Bou Sanayeh, Marie Christelle Saade, Gladys Honein-AbouHaidar
    & Elie A. Akl   Health Research Policy and Systems  22, Article number: 38 (2024)
    Cite this article 193 Accesses 4 Altmetric Metrics Abstract Background The coronavirus
    disease 2019 (COVID-19) pandemic has strained healthcare systems globally, particularly
    in terms of access to medicines. Lebanon has been greatly affected by the pandemic,
    having faced concomitant financial and economic crises. The objective of the study
    was to understand the experiences of patients with COVID-19 in Lebanon, as well
    as those of their families, and healthcare providers, with regards to their treatment
    decisions and accessibility to COVID-19 medicines. Methods For this qualitative
    study, we conducted 28 semi-structured interviews. We used purposive sampling
    to recruit participants with a diverse range of perspectives. The data collection
    phase spanned from August to November 2021 and was conducted virtually. After
    transcribing and translating the interviews, we employed thematic analysis to
    identify recurring themes and patterns. Results In total, 28 individuals participated
    in this study. Participants highlighted challenges owing to the COVID-19 pandemic
    and economic crisis. Accessing COVID-19 medicines posed major hurdles for physicians
    and patients, given limited availability, global shortages, local circumstances,
    community hoarding and stockpiling by pharmacies. Providers based treatment decisions
    on research, local and international practice guidelines, experiences and expert
    feedback. Patients sought information from social media, community members and
    physicians, as well as through word of mouth. Accessing medicines involved navigating
    the healthcare system, the black market, charities, personal networks and political
    parties and sourcing from abroad. The medicines were either free, subsidized or
    at inflated costs. Conclusions This study highlights the diversity and complexity
    of factors influencing decision-making and accessing medicines during the COVID-19
    pandemic in Lebanon. Future research should explore strategies for ensuring medicine
    access during crises, drawing insights from comparative studies across different
    countries. Peer Review reports Introduction The emergence of the coronavirus disease
    2019 (COVID-19) instigated a global health crisis, presenting formidable challenges
    to healthcare systems and economies across the world [1, 2]. Since its first appearance
    in December 2019 in China, severe acute respiratory coronavirus 2 (SARS-CoV-2)
    has infected around 700 million individuals, resulting in a staggering death toll
    exceeding 6.9 million by November 2023 [3, 4]. Owing to COVID-19-related lockdowns
    and the heightened demand for essential medications, drug shortages have become
    a significant global problem [5, 6]. Prior to the pandemic, healthcare systems
    in low- and middle-income countries (LMICs) suffered from limited financial resources,
    healthcare workforce shortages and unavailability of medications [7,8,9]. The
    pandemic further strained these already fragile health systems [2]. For example,
    the strong demand for medications to treat COVID-19 patients, including analgesics,
    sedatives, antibiotics, hydroxychloroquine and remdesivir, considerably affected
    medication accessibility and inadvertently encouraged black market activity [10,11,12,13].
    Escalating medication prices rendered these crucial medications unaffordable for
    many patients, particularly in LMICs [13,14,15]. The economic and financial crises
    in Lebanon, coupled with the Beirut Port’s destruction, severely impacted the
    entire healthcare sector, affecting hospitals, healthcare providers and the pharmaceutical
    and medical supply industry [16]. The COVID-19 pandemic worsened this situation,
    posing two major challenges: the selection of appropriate therapies and ensuring
    access to these treatments [17, 18]. This shortage of prescription drugs in Lebanon
    peaked during the COVID-19 pandemic [19]. Factors influencing decisions regarding
    COVID-19 treatment can be complicated and multidimensional. They encompass an
    individual patient’s medical history, comorbidities and risk factors, as well
    as the availability and efficacy of various therapeutic options [20]. Therapeutic
    management in the early stages of the pandemic was challenging owing to uncertainty
    and continuously evolving evidence [21]. Clinicians attempted to manage COVID-19
    using a variety of treatments that targeted numerous possible mechanisms, such
    as antiviral, anti-inflammatory and immunomodulatory drugs [22]. There was also
    misinformation in various media outlets about the benefits of some medications
    for either preventing or treating COVID-19 [23]. This resulted in an increase
    in risky self-medication with several over-the-counter medications [2, 24]. The
    objective of the study is to understand the experiences of patients with COVID-19
    in Lebanon, as well as those of their families, and healthcare providers, with
    regards to their treatment decisions and accessibility to COVID-19 medicines.
    Methods Study design This study adopted a descriptive qualitative research design
    using semi-structured individual interviews (refer to Appendix 1 for the interview
    guide). The qualitative approach utilized is rooted in naturalistic inquiry and
    offers a wide array of theoretical or philosophical orientations, sampling techniques
    and data-gathering strategies [25]. Participants We recruited participants from
    different regions in Lebanon. Eligible participants belonged to one of the following
    groups: 1. physicians and nurses directly involved in caring for patients diagnosed
    with COVID-19 2. hospital and community pharmacists involved in dispensing medications
    for patients diagnosed with COVID-19 3. patients previously diagnosed with COVID-19
    4. family members or caregivers of patients previously diagnosed with COVID-19.
    We excluded patients who were psychologically unable to participate or provide
    coherent and clear descriptions of their experiences. Sampling and recruitment
    We used purposeful sampling by approaching individuals belonging to the groups
    of interest. We also used snowballing sampling by asking participants to refer
    us to other eligible individuals. Additionally, physicians and pharmacists assisted
    in the recruitment of potential former patients and caregivers. The Institutional
    Review Board (IRB) at the American University of Beirut (AUB) approved the study.
    All participants provided oral consent prior to participation. The interviewers
    took all precautions to guarantee participants’ anonymity and confidentiality.
    Participants were informed that their participation was entirely voluntary and
    that they could opt-out at any time. Data collection Following an explanation
    of the study’s objectives, we interviewed participants virtually in either English
    or Arabic, depending on their preferences. We audio-recorded interviews following
    participants’ consent. We conducted a total of 28 interviews, and we ceased to
    collect data when thematic saturation was reached, that is, no new themes emerged
    from the data analysis [26]. Two team members (AH and EBS) conducted the interviews
    between August and November of 2021. The individuals received thorough training
    on conducting interviews, focusing on techniques to remain neutral and nonjudgemental
    and to sustain the interviewees’ engagement in the subject matter. To enhance
    the quality of data collection, we held regular debriefing meetings following
    the initial interviews. These meetings provided an opportunity for reflection
    on the data collection process and identification of areas of improvement. Data
    analysis The interviewers transcribed the audio-recorded interviews, and translated
    them into English when applicable. Another team member (RH) verified transcript
    accuracy by checking them against the audio recordings. We employed Quirkos, a
    qualitative analysis software, for coding and organizing the data. We applied
    Braun and Clarke’s six-step thematic analysis approach [27]. In phase 1, GHA and
    RH read a few transcripts independently to familiarize themselves with the information
    and established a preliminary framework for data coding. In phase 2, they independently
    annotated the transcripts line by line. They assigned labels to each idea (coding),
    leaving room for new codes as they emerged. In phase 3, GHA, EAA and RH reviewed
    the coded transcripts and identified emerging themes, along with quotes that illustrated
    each theme. In phase 4, GHA, EAA and RH reviewed and refined the list of emerging
    themes, and created a thematic map. In phase 5, they outlined the final thematic
    framework. Finally, in phase 6, we developed a complete narrative of the findings
    and selected interviewee quotes for each theme and sub-theme. Increasing rigour
    All interviewers received training in interviewing skills, maintaining consistency
    and rigour [28]. We also made sure that interviewers had no prior relationship
    with participants, fostering objectivity and minimizing bias [28]. We interviewed
    participants in their preferred language as a way to ensure their understanding
    of the questions and their ability to easily express their thoughts [28, 29].
    To ensure transferability, we employed triangulation by compiling viewpoints of
    various population groups [29]. We halted data collection upon reaching saturation
    [30], ensuring comprehensive data coverage and depth. We verified transcript accuracy
    by checking them against the audio recordings [31]. Three members of our research
    team (GHA, EAA and RH) actively participated in the analysis and the generation
    of codes, themes and subthemes. In reporting this study, we adhered to the highest
    standards by following the Consolidated Criteria for Reporting Qualitative Research
    (COREQ) checklist [32]. Results Demographics We recruited 28 participants: 3 community
    pharmacists, 4 hospital pharmacists, 8 physicians, 1 nurse, 3 patients and 9 caregivers.
    The interviews lasted about 40 min on average. Emerging themes The following themes
    emerged in relation to the experiences of participants with regards to treatment
    decisions and accessibility to COVID-19 medicines: country crises, access challenges,
    cost challenges, drivers for providers’ decision-making, drivers for patients
    and caregivers’ decision-making and accessing medicines (Fig. 1). Fig. 1 Factors
    influencing decision-making and accessing medicines during the COVID-19 pandemic
    in Lebanon Full size image When the majority of participants expressed the same
    opinions, we used the term “most participants”; otherwise, we used the terms “many”
    or “few” as appropriate. When citing quotes from participants, we used the following
    acronyms: physicians (Phys), nurses (Nurse), community pharmacists (C-Pharm),
    hospital pharmacists (H-Pharm), patients (Pt) and caregivers (Cg). Country crises
    Most participants discussed current events in the country, including the COVID-19
    pandemic, the economic crisis and the financial crisis. “Because of the current
    situation in Lebanon, we’re seeing things we never thought we would” (H-Pharm
    02). “COVID-19 came around in March 2020, and Lebanon had already started its
    economic crisis” (H-Pharm 04). Participants noted their experiences with the financial
    crisis and the closure of banks. “I woke up in the morning at 6:45 am, wore my
    clothes and went down to the bank but found it closed […], I went down to [..],
    same thing, it closed as I was on my way so I said, ‘where should I get them the
    money now?; they asked for money for the tests” (Cg 08). One caregiver also mentioned
    that the economic crisis had forced them to work multiple jobs. “I work two jobs
    but now if you work in Lebanon the salaries are not enough” (Cg 08). In addition
    to this, participants complained about how the country’s financial situation impacted
    access to medicines. “The purchasing power of the patients had already decreased.
    And like all countries when it comes to medication and the healthcare system,
    which was already collapsing in Lebanon, it’s common that in the end, the patient
    has to self-fund their treatment. And patients in low economical standing will
    have no access to treatment” (H-Pharm 04). Access challenges The crisis impacted
    the supply of COVID-19 medications. First, there were drug shortages directly
    related to the COVID-19 medicines, similar to the global crisis “… we saw that
    everyone is facing this, worldwide… Then we went into shortages, we didn’t have
    anymore because the consumption had increased” (H-Pharm 02). “Even the Colchicine,
    we heard about it, and we went around and looked for it and it was very hard for
    us to find it easily” (Cg 02). “For example, remdesivir they used for my mother-in-law
    we got six injections, and the first day we tried to get it we called the pharmacies
    they told us they don’t have it. In the hospital, there was no remdesivir because
    it was getting brought based on an order from the company and it needed a prescription
    from the doctor to obtain” (Cg 01). Second, the challenging circumstances in Lebanon
    had an additional impact on the accessibility of medicines. “The effect of the
    medications is unrelated to COVID-19. I mean, the availability of the medications.
    Now, even after COVID-19 has decreased, we have shortages in medications that
    are unrelated to COVID-19, it has to do with the economic situation. So, it’s
    not COVID-19 that made the crisis in medications, not at all” (Phys 08). “It was
    hard to provide medications because of the terrible situation of Lebanon” (Nurse
    01). Many pharmacists discussed community hoarding. “People were running to the
    pharmacies to secure one of these medications even if they did not need it at
    the time, just for the sake of keeping it at home just in case, which led to a
    huge shortage of supply” (C-Pharm 02). However, patients were concerned about
    pharmacies stockpiling medications. “Exactly, because of the economic crisis selling
    the drug was not beneficial for the pharmacies so they started keeping it for
    emergency cases and selling it at the black market rate. After all the demand
    was very high” (Cg 07). Few physicians mentioned lack of availability of medicines
    specifically in the hospitals, particularly for the new medications. “They had
    to get the medications outside of the hospital…the shortage was due to the fact
    that basically, this is a new medication, and it hasn’t been brought to Lebanon
    yet.” (Phys 08). A few pharmacists stated that they always had a backup plan.
    “To be honest we never had really bad shortages, we never fully ran out, we always
    had a plan B. When there was no more dexamethasone for IV, we prepared other corticoids,
    even if it wasn’t mentioned in any studies or guidelines. We used to prepare them
    and keep them as backups in case they were ever needed” (H-Pharm 04). Cost challenges
    Both providers and patients noted unaffordable costs as another factor affecting
    access to medications, considering the devaluation of the currency. “The single
    pill got to about 50 USD, it was very expensive. So, it was really expensive for
    most people. Its actual price was 5000 Lira” [H-Pharm 04; note that at the time
    5000 Lira was worth less than 5 US dollars (USD)]. “Even tablets like vitamins
    are available but they are very expensive, not everyone can afford them” (Cg 04).
    Patients complained about price manipulation. “The prices were definitely manipulated
    because when I would buy a medication I would find more than one price tag… The
    lozenges for her throat used to cost 19 000 Lira and now it costs 45 000 Lira.
    They put more than one label on the box of medication, there are about three price
    tags on it” (Cg 06). Additionally, it was mentioned that the pricing of medications
    was changed to the US currency. “Yes, most of the time they were fresh dollar”
    (Phys 08). Drivers of decision-making for providers During the COVID-19 pandemic,
    several factors influenced decisions by providers, patients and caregivers about
    which medicines to use or not use. Owing to the rapid development of evidence,
    providers were compelled to rely on research to prescribe certain medications.
    “There was a committee that used to review the data available and to review all
    the evidence at the time and make the decisions.” (Phys 04). “… the COVID-19 protocol
    changed every couple of days. Every once in a while, a new study would appear,
    a new update, and it would change again” (H-Pharm 01). Providers also relied on
    clinical practice guidelines developed either locally or internationally, for
    example, by the WHO. “The medications we were prescribing were based on WHO” (Phys
    03). Although they relied on those guidelines, some providers expressed hesitations
    about them. “Yes, we were following the guidelines of treatment of COVID-19, we
    would tell this is the medication that needs to be taken because this is what
    the guidelines say. We are not sure of the guidelines, but this is what is needed
    now” (Phys 09). Additionally, the providers’ prior experience or trial and error
    played a role in the decision-making process. “At first, personally, I did not
    have much experience with this disease but later on and after I acquired some
    experience, I was finally able to give my own opinion on the matter” (C-Pharm
    01). Reliance on local peers with different specialities played a significant
    role in decision-making. “Because we had several specialities – cardiovascular,
    internal medicine, and others – everyone did their research and every week we
    would meet and explain to each other… Everyone gave their inputs and propositions,
    in their own specialities, about which drugs might be good, and which drugs were
    used in which cases” (H-Pharm 04). International expertise was also sought during
    the pandemic. “We also had video conferences with hospitals and ICUs in France
    and the United States, because we had physicians that went and studied in those
    countries and still had contacts, we did one video conference with France, and
    one with the US to ask about their protocols. And there were discussions about
    what’s best. And when Actemra was first being used by the ones we talked to somewhere
    in Houston, we weren’t using it yet in our hospital. After the video conference,
    they found that their patients are showing good results, so it was added to our
    protocol..” (H-Pharm 04). It is of note that the country’s situation and drug
    availability influenced the decision-making process. As expressed by many doctors:
    “We were following the new guidelines, taking into consideration the situation
    of the country and the availability of the drugs and imaging” (Phys 02). Drivers
    of decision-making for patients and caregivers Patients and caregivers relied
    on social media to decide on drug purchases. “Yes. Honestly, they saw me crying
    and I had posted on Instagram that if anyone please could help with their experience
    because there were no studies at that time” (Cg 03). “At first people used to
    wait for what the media says and then come running to the pharmacies to buy these
    medications, it happened first with vitamin C then 2 weeks later with vitamin
    D then it was the zinc 25 mg and then zinc 50 mg turns” (C-Pharm 01). “Even the
    colchicine, we heard about it on social media, and we went around and looked for
    it and it was very hard for us to find easily” (Cg 02). Patients and caregivers
    were also influenced by people in their communities, some of whom had experience
    with COVID-19. “Other people around us who also had corona, everyone that got
    corona would say take this and do this” (Pt 02). “My dad caught it in the beginning,
    so I started asking people to see what we could do. One of my friends told me
    that there was a person who took this medicine, and they told me to try it, so
    I decided to do that” (Cg 03). Patients reported different attitudes about consulting
    with their doctors. “We heard about remdesivir and asked the doctor, he told us
    he can’t advise us to take it or not, if we would like to try it based on other
    patients and not on medical research then go ahead” (Cg 07). Some made decisions
    on their own on the basis of word of mouth. “My friend called me and told to me
    not listen to the doctors and to take zithromax. I bought it and took one pill”
    (Pt 08). Accessing medicines We have identified two subthemes under the theme
    of “accessing medicines”: information about how to get the medicines and the sources
    of medicines. Information about how to get the medicines Typically, patients obtained
    information about how to get the medicines from healthcare providers, including
    nurses, physicians and pharmacists. “Yes sure!… [local charities] used to give
    those medications (remdesivir, Actemra, etc.) for free. And there were some other
    providers. We used to indicate the providers to the families of the patients”
    (Nurse 01). Patients also inquired about the source of medicines from recovered
    COVID-19 patients: “From other people around us who also had corona” (Pt 02).
    Sources of medicines Patients and their caregivers obtained the medicines either
    through the healthcare system or from outside the healthcare systems, including
    the black market, nongovernmental organizations (NGOs), personal networks, political
    parties and outside of the country. Healthcare system When patients were admitted,
    few reported that COVID-19 medicines were available in the hospital. “They were
    all found in the hospital” (Cg 05). However, for several patients, their family
    members had to seek medicines from community pharmacies. “We got them from the
    pharmacy” (Cg 01). Medicines were obtained at no cost thanks to a subsidy by the
    Lebanese government. “So, it was for free if it was from the Ministry” (Cg 02).
    However, some other medicines were purchased on an unsubsidized basis and at high
    cost. “A few pills were for 1 300 000 Lebanese Lira in the pharmacy” (Cg 02).
    Black market Owing to the limited supply and urgent need for COVID-19 medications,
    the black market flourished. “There were two more weeks, and the Ministry was
    supposed to secure it, but we needed it urgently, so they gave us the number of
    someone who sells it in the black market and he got it for us” (Pt 03). The black
    market was viewed as a double-edged sword because it allowed access but at an
    inflated cost. “They gave us five remdesivir and one Actemra for US$ 1200” (Cg
    07). “The remdesivir is like. So, he made us pay US$ 700 for one,.. So, US$ 4200
    for six pills” (Pt 03). Because of the country’s financial crisis, inflated black
    market prices presented a major challenge for patients. “… it was a challenge
    for us to financially secure the medicine. And of course, him asking for US$ 4200
    cash was not something easy for someone to get and pay, but if it is the only
    solution of course we would do it” (Pt 03). Charities Charities supported patients
    in accessing their medicines either for free or through financial support. “Suppose
    I were to get COVID-19 now, my name would go down at the municipality and they
    get you vitamin C and vitamin D – a charity organization, not from the government”
    (Cg 08). “I paid 1 million and the rest was on the charity organization” (Cg 08);
    “For ivermectin there were a lot of organizations trying to supply it, it’s a
    very cheap drug… that costs US$ 4. There were also a lot of organizations trying
    to supply remdesivir, ‘Hariri’ (a local charity) was trying to help with it since
    hospitals did not have it, people were going to her villa to get it, it costs
    I think about US$ 4000” (Phys 02). Personal network Caregivers of patients with
    COVID-19 used their personal networks, including family and friends: “Also, from
    a person who knows a pharmacist he’s friends with, they got them for us” (Cg 02).
    “We had to get the baricitinib from someone we know, who got it for us from the
    Ministry” (Cg 02). Political parties Political parties also supplied medicines
    to their supporters. “There were parties that were obtaining them, like [name
    of political parties]. Those were for free as a donation from [name of political
    party]” (Pt 02). From outside of the country Typically, family or friends helped
    by purchasing medicines while travelling. “At the time, an Iraqi who is friends
    with my relative got it and he paid US$ 400” (Pt 08). Out-of-country purchases
    were driven by either lack of local supply or inflated costs. “The ivermectin
    was still not in Lebanon, so we got it elsewhere, from a woman who lives in Africa,
    she got it for us and sent it. And we started with cortisone, this is from day
    1” (Cg 02). “My cousin sent it from Sweden, she sent zinc and vitamin C because
    vitamin C here now costs 60 000 Lira, before it cost 14 000 Lira and now it costs
    60 000” (Cg 06). Discussion This study aimed to to understand the experiences
    of patients with COVID-19 in Lebanon, as well as those of their families, physicians,
    nurses and pharmacists, with regards to their treatment decisions and accessibility
    to COVID-19 medicines. The participants highlighted the country’s difficulties,
    especially the severe impact of COVID-19 pandemic and the economic crisis. Access
    to COVID-19 medicines and their costs were major challenges according to the three
    groups interviewed. Limited access related to global shortage of medicines, the
    local challenging circumstances, community hoarding (according to pharmacists)
    and stockpiling by pharmacies (according to patients). For providers, the decision-making
    process for COVID-19 treatments was shaped by research evidence, local and international
    practice guidelines, previous experiences and feedback from both local and international
    experts. Patients and their caregivers relied on social media, community members,
    physicians and word of mouth. Information on how to get the medicines was obtained
    from either healthcare providers or patients who recovered from COVID-19. Accessing
    medicines involved navigating through the healthcare system (hospitals and pharmacies),
    as well as outside that system, including the black market, charities, personal
    networks, political parties and outside of the country. Across these different
    sources, the medicines were either free, subsidized or at inflated costs. Comparison
    to similar studies A major finding in our study was the accessibility of patients
    and healthcare providers to needed medicines. This is corroborated by other studies
    conducted in Lebanon [33, 34] and low-and middle-income countries [35]. The global
    impact of lockdowns on medicine manufacturing, supply and distribution contributed
    to shortages during the high-demand period of the COVID-19 pandemic [36, 37].
    Furthermore, Lebanon has faced severe economic and financial crises starting in
    2019, which severely hindered the capacity to import vital healthcare equipment
    and medicines [38, 39]. Indeed, the World Bank characterized the crisis as “among
    the world’s worst since the 1850s” [40]. The lack of government reimbursement
    further hindered hospitals in procuring necessary medications and medical supplies
    [41]. Consequently, individuals affected by COVID-19 in Lebanon resorted to unregulated
    sources, including the black market, often resulting in inflated prices and the
    risk of expired or counterfeit drugs [13,14,15, 42]. Moreover, in line with our
    findings, other studies found that healthcare providers followed both international
    and national guidelines when deciding on potential treatments for COVID-19 patients
    [43, 44]. However, in the absence of effective medications, discussion on various
    social media platforms encouraged self-medication and the use of herbal medicines
    [45, 46]. In addition, a recent study conducted in Jordan assessing the usage
    of medications and natural products amidst the second wave of COVID-19 revealed
    that individuals primarily sought guidance from family and friends, with social
    media platforms serving as significant sources of advice concerning the use of
    these medications [47]. The same study showed that pharmacists notably played
    a significant role in guiding individuals on choosing these treatments compared
    with other healthcare providers [47].This highlights the impact of social media
    on treatment choices and emphasizes the need for disseminating accurate and evidence-based
    information. Strengths and limitations To our knowledge, this is the first study
    in Lebanon to comprehensively explore the interplay between country crises and
    medication accessibility during the COVID-19 pandemic, offering valuable insights
    into the unique challenges faced by the country. We explored in-depth the lived
    experiences of our participants, ensuring the representation of the perspectives
    of healthcare providers, patients and caregivers. Also, we used a rigorous qualitative
    methodology (please refer to the “Increasing rigour” section). There are several
    limitations to consider. Firstly, the study focuses primarily on Lebanon, which
    may limit the findings’ generalizability to other countries with distinct settings
    and healthcare systems. Moreover, there is a possibility of recall bias among
    participants, as their recollections of events and experiences concerning medication
    accessibility during the crisis might be influenced by subjective interpretations
    or memory lapses. Additionally, the sampling technique employed might introduce
    selection bias, as participants were recruited through purposive sampling. Furthermore,
    it is important to note that this study is based on a specific snapshot in time
    during the COVID-19 pandemic. Consequently, its findings may not fully encapsulate
    the dynamic and evolving nature of the crisis or account for potential shifts
    in medication accessibility and decision-making processes over time. Conclusions
    This study sheds light on the wide range of factors influencing treatment decisions
    during the COVID-19 pandemic in Lebanon. It also unveils how patients and their
    families had to access medications either through the formal healthcare systems
    or through black markets and other channels. Plans are needed to address medicine
    availability, affordability and equitable distribution during similar future crises.
    There is an urgent need for collaborative efforts involving stakeholders, policy-makers
    and key systems such as Meditrack and AMAN within the Ministry of Public Health
    [48, 49]. These initiatives are intended to establish resilient and sustainable
    drug supply chains and to ensure timely and equitable access to medications for
    all individuals, particularly in times of crisis. Furthermore, improving collaboration
    among healthcare providers, expediting medication access and creating patient
    support programs can alleviate the difficulties that people seeking treatment
    confront. For example, streamlining communication between hospitals, pharmacies
    and primary care doctors could speed up the prescription and dispensing processes.
    Future research should focus on effective strategies to ensure medicine access
    during crises. Comparative research across different countries can provide valuable
    insights into successful tactics that can be tailored across different countries.
    What is already known on this topic global healthcare systems have been strained
    owing to the COVID-19 pandemic, leading to challenges in medicine access; and
    Lebanon’s healthcare system has been significantly impacted by the pandemic and
    financial crises, affecting the availability of medicines. What this study adds
    it uncovers key factors influencing both healthcare providers and patients in
    their treatment decisions, providing a comprehensive perspective; and it describes
    varied sources for medicines, including informal networks and the black market.
    How this study might affect research, practice or policy the findings emphasize
    the necessity for strategies that ensure continuous medicine access, particularly
    during times of crises and economic instability. Availability of data and materials
    The datasets analysed during the current study available from the corresponding
    author on reasonable request. Abbreviations AUB: American University of Beirut
    Cg: Caregivers COREQ: Consolidated Criteria for Reporting Qualitative Research
    COVID-19: Coronavirus disease 2019 C-Pharm: Community pharmacists H-Pharm: Hospital
    pharmacists IRB: Institutional review board LMICs: Low- and middle-income countries
    MOPH: Lebanese Ministry of Public Health NGOs: Nongovernmental organizations Phys:
    Physicians Pt: Patients SARS-CoV-2: Severe acute respiratory syndrome coronavirus
    2 USD: US dollar References Legido-Quigley H, Asgari N, Teo YY, Leung GM, Oshitani
    H, Fukuda K, et al. Are high-performing health systems resilient against the COVID-19
    epidemic? Lancet. 2020;395(10227):848–50. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Kretchy IA, Asiedu-Danso M, Kretchy J-P. Medication management and adherence
    during the COVID-19 pandemic: perspectives and experiences from low-and middle-income
    countries. Res Social Adm Pharm. 2021;17(1):2023–6. Article   PubMed   Google
    Scholar   WHO. WHO Coronavirus (COVID-19) Dashboard: World Health Organization;
    2022. https://covid19.who.int/. Hoteit R, Yassine HM. Biological properties of
    SARS-CoV-2 variants: epidemiological impact and clinical consequences. Vaccines.
    2022;10(6):919. Article   CAS   PubMed   PubMed Central   Google Scholar   Badreldin
    HA, Atallah B. Global drug shortages due to COVID-19: Impact on patient care and
    mitigation strategies. Res Social Adm Pharm. 2021;17(1):1946–9. Article   PubMed   Google
    Scholar   Sánchez DIR, Vogler S. Shortages of medicines to treat COVID-19 symptoms
    during the first wave and fourth wave: analysis of notifications reported to registers
    in Austria, Italy, and Spain. Pharmacy (Basel). 2023;11(4). Agampodi TC, Agampodi
    SB, Glozier N, Siribaddana S. Measurement of social capital in relation to health
    in low and middle income countries (LMIC): a systematic review. Soc Sci Med. 2015;128:95–104.
    Article   PubMed   Google Scholar   Adam T, de Savigny D. Systems thinking for
    strengthening health systems in LMICs: need for a paradigm shift. Health Policy
    Plan. 2012;27(suppl 4):iv1-iv3. Reeves A, Gourtsoyannis Y, Basu S, McCoy D, McKee
    M, Stuckler D. Financing universal health coverage – effects of alternative tax
    structures on public health systems: cross-national modelling in 89 low-income
    and middle-income countries. Lancet. 2015;386(9990):274–80. Article   PubMed   PubMed
    Central   Google Scholar   Farmer KC. Stress and strain on the US drug supply:
    the intersection of shortages, globalization, counterfeit products, and throw
    in a global COVID-19 pandemic. J Am Pharm Assoc. 2021;61(1):e85–6. Article   CAS   Google
    Scholar   Martin AJ, Shulder S, Dobrzynski D, Quartuccio K, Pillinger KE. Antibiotic
    use and associated risk factors for antibiotic prescribing in COVID-19 hospitalized
    patients. J Pharm Pract. 2021:08971900211030248. Dagrou A, Chimhutu V. I Buy medicines
    from the streets because i am poor: a qualitative account on why the informal
    market for medicines thrive in ivory coast. Inquiry. 2022;59:00469580221086585.
    Plata GG. The black market for COVID-19 antiviral drugs. Br Med J. 2022;377. Shuchman
    M. Low-and middle-income countries face up to COVID-19. Nat Med. 2020. White CM.
    Counterfeit drugs: a major issue for vulnerable citizens throughout the world
    and in the United States. J Am Pharm Assoc. 2021;61(1):e93–8. Article   Google
    Scholar   El-Harakeh A, Haley SJ. Improving the availability of prescription drugs
    in Lebanon: a critical analysis of alternative policy options. Health Res Policy
    Syst. 2022;20(1):106. Article   PubMed   PubMed Central   Google Scholar   Filip
    R, Gheorghita Puscaselu R, Anchidin-Norocel L, Dimian M, Savage WK. Global challenges
    to public health care systems during the COVID-19 pandemic: a review of pandemic
    measures and problems. J Personal Med. 2022;12(8):1295. Article   Google Scholar   Patrucco
    F, Gavelli F, Fagoonee S, Solidoro P, Undas A, Pellicano R. Current treatment
    challenges in the COVID-19 pandemic. 2021. Das M. Lebanon faces critical shortage
    of drugs. Lancet Oncol. 2021;22(8):1063. Article   PubMed   PubMed Central   Google
    Scholar   Robinson PC, Liew DF, Tanner HL, Grainger JR, Dwek RA, Reisler RB, et
    al. COVID-19 therapeutics: challenges and directions for the future. Proc Natl
    Acad Sci. 2022;119(15): e2119893119. Article   PubMed   PubMed Central   Google
    Scholar   Siemieniuk RA, Bartoszko J, Ge L, Zeraatkar D, Izcovich A, Kum E, et
    al. Drug treatments for COVID-19: living systematic review and network meta-analysis.
    Br Med J. 2021;373: n967. Google Scholar   Dixit SB, Zirpe KG, Kulkarni AP, Chaudhry
    D, Govil D, Mehta Y, et al. Current approaches to COVID-19: therapy and prevention.
    Indian J Crit Care Med. 2020;24(9):838. Article   CAS   PubMed   PubMed Central   Google
    Scholar   El Mikati IK, Hoteit R, Harb T, El Zein O, Piggott T, Melki J, et al.
    Defining misinformation and related terms in health-related literature: scoping
    review. J Med Internet Res. 2023;25: e45731. Article   PubMed   PubMed Central   Google
    Scholar   Jirjees F, Ahmed M, Sayyar S, Amini M, Al-Obaidi H, Aldeyab MA. Self-medication
    with antibiotics during COVID-19 in the eastern Mediterranean region countries:
    a review. Antibiotics. 2022;11(6):733. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Colorafi KJ, Evans B. Qualitative descriptive methods in health science
    research. HERD. 2016;9(4):16–25. Article   PubMed   PubMed Central   Google Scholar   Strauss
    A, Corbin J. Basics of qualitative research techniques. 1998. Braun V, Clarke
    V. Using thematic analysis in psychology. Qual Res Psychol. 2006;3(2):77–101.
    Article   Google Scholar   Mays N, Pope C. Qualitative research: rigour and qualitative
    research. Br Med J. 1995;311(6997):109–12. Article   CAS   Google Scholar   Tobin
    GA, Begley CM. Methodological rigour within a qualitative framework. J Adv Nurs.
    2004;48(4):388–96. Article   PubMed   Google Scholar   Varpio L, Ajjawi R, Monrouxe
    LV, O’Brien BC, Rees CE. Shedding the cobra effect: problematising thematic emergence,
    triangulation, saturation and member checking. Med Educ. 2017;51(1):40–50. Article   PubMed   Google
    Scholar   Long T, Johnson M. Rigour, reliability and validity in qualitative research.
    Clin Eff Nurs. 2000;4(1):30–7. Article   Google Scholar   Tong A, Sainsbury P,
    Craig J. Consolidated criteria for reporting qualitative research (COREQ): a 32-item
    checklist for interviews and focus groups. Int J Qual Health Care. 2007;19(6):349–57.
    Article   PubMed   Google Scholar   Khattar G, Hallit J, El Chamieh C, Bou SE.
    Cardiovascular drug shortages in Lebanon: a broken heart. Health Econ Rev. 2022;12(1):24.
    Article   PubMed   PubMed Central   Google Scholar   Osman M, Kasir D, Kassem
    II, Hamze M. Shortage of appropriate diagnostics for antimicrobial resistance
    in Lebanese clinical settings: a crisis amplified by COVID-19 and economic collapse.
    J Glob Antimicrob Resist. 2021;27:72. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Boro E, Stoll B. Barriers to COVID-19 health products in low-and middle-income
    countries during the COVID-19 pandemic: a rapid systematic review and evidence
    synthesis. Front Public Health. 2022;10: 928065. Article   PubMed   PubMed Central   Google
    Scholar   EMA. Availability of medicines during COVID-19 pandemic European Medicines
    Agency; 2023 https://www.ema.europa.eu/en/human-regulatory/overview/public-health-threats/coronavirus-disease-covid-19/availability-medicines-during-covid-19-pandemic.
    Newton PN, Bond KC, Adeyeye M, Antignac M, Ashenef A, Awab GR, et al. COVID-19
    and risks to the supply and quality of tests, drugs, and vaccines. Lancet Glob
    Health. 2020;8(6):e754–5. Article   PubMed   PubMed Central   Google Scholar   Bou
    Sanayeh E, El Chamieh C. The fragile healthcare system in Lebanon: sounding the
    alarm about its possible collapse. Health Econ Rev. 2023;13(1):21. Article   PubMed   PubMed
    Central   Google Scholar   HRW. Lebanon: COVID-19 Worsens Medical Supply Crisis:
    Human Rights Watch; 2021. https://www.hrw.org/news/2020/03/24/lebanon-covid-19-worsens-medical-supply-crisis.
    WorldBank. The World Bank In Lebanon 2022. https://www.worldbank.org/en/country/lebanon/overview.
    Accessed 17 Nov 2023. Shallal A, Lahoud C, Zervos M, Matar M. Lebanon is losing
    its front line. J Glob Health. 2021;11. AmnestyInternational. Lebanon: Government
    must address medication shortages and healthcare crisis 2023. https://www.amnesty.org/en/latest/news/2023/02/lebanon-government-must-address-medication-shortages-and-healthcare-crisis/.
    Pan H, Peto R, Henao-Restrepo A, Preziosi M, Sathiyamoorthy V, Abdool Karim Q,
    et al. Consortium WST (2021) repurposed antiviral drugs for COVID-19-Interim WHO
    solidarity trial results. N Engl J Med. 2021;384:497–511. Article   CAS   PubMed   Google
    Scholar   Lamontagne F, Agoritsas T, Siemieniuk R, Rochwerg B, Bartoszko J, Askie
    L, et al. A living WHO guideline on drugs to prevent COVID-19. Br Med J. 2021;372.
    Roman YM, Burela PA, Pasupuleti V, Piscoya A, Vidal JE, Hernandez AV. Ivermectin
    for the treatment of coronavirus disease 2019: a systematic review and meta-analysis
    of randomized controlled trials. Clin Infect Dis. 2022;74(6):1022–9. Article   CAS   PubMed   Google
    Scholar   Garegnani LI, Madrid E, Meza N. Misleading clinical evidence and systematic
    reviews on ivermectin for COVID-19. BMJ Evid Based Med. 2022;27(3):156–8. Article   PubMed   Google
    Scholar   Thiab SH, Nassar RI, Thiab S, Basheti IA. Medications and natural products
    used in Jordan for prevention or treatment of COVID-19 infection during the second
    wave of the pandemic: a cross-sectional online survey. Saudi Pharm J. 2022;30(6):856–62.
    Article   CAS   PubMed   PubMed Central   Google Scholar   MOPH. MediTrack Project
    - Track & Trace System for Pharmaceuticals: Ministry of Public Health; 2020. https://www.moph.gov.lb/en/Drugs/index/0/15088.
    MOPH. Steps to Obtain a Unique Health ID, Enter the Medical Record into AMAN Program
    and Follow Up Through the Hotline 1214: Ministry of Public Health; 2023. https://www.moph.gov.lb/en/Pages/0/22862/moph-mobile-application-#/en/Pages/0/68151/steps-to-obtain-a-unique-health-id-enter-the-medical-record-into-aman-program-and-follow-up-through-.
    Download references Acknowledgements Not applicable. Funding This research did
    not receive any specific grant from funding agencies in the public, commercial
    or not-for-profit sectors. Author information Authors and Affiliations Clinical
    Research Institute, American University of Beirut, Riad-El-Solh, P.O. Box: 11-0236,
    Beirut, 1107 2020, Lebanon Reem Hoteit School of Public Health, University of
    Saskatchewan, Saskatoon, SK, S7N 5A2, Canada Aya Hassoun Department of Internal
    Medicine, American University of Beirut Medical Center, Beirut, Lebanon Elie Bou
    Sanayeh, Marie Christelle Saade & Elie A. Akl Rafic Hariri School of Nursing,
    American University of Beirut, Beirut, Lebanon Gladys Honein-AbouHaidar Department
    of Health Research Methods, Evidence, and Impact (HEI), McMaster University, Hamilton,
    Canada Elie A. Akl Contributions Concept and design: AH and EAA. Data collection:
    EBS, AH and MCS. Data analysis: RH, GHA and EAA. Data interpretation: RH, GHA
    and EAA. Drafting of the manuscript: RH, AH and EAA . All authors reviewed and
    approved the submitted version of the manuscript. Corresponding author Correspondence
    to Elie A. Akl. Ethics declarations Ethics approval and consent to participate
    The Institutional Review Board (IRB) at the American University of Beirut (AUB)
    approved the study. Consent for publication All participants provided oral consent
    prior to participation. Competing interests The authors declare that they have
    no competing interests. Additional information Publisher’s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Open Access This article is licensed under
    a Creative Commons Attribution 4.0 International License, which permits use, sharing,
    adaptation, distribution and reproduction in any medium or format, as long as
    you give appropriate credit to the original author(s) and the source, provide
    a link to the Creative Commons licence, and indicate if changes were made. The
    images or other third party material in this article are included in the article''s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article''s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/)
    applies to the data made available in this article, unless otherwise stated in
    a credit line to the data. Reprints and permissions About this article Cite this
    article Hoteit, R., Hassoun, A., Bou Sanayeh, E. et al. Choosing and accessing
    COVID-19 treatment options: a qualitative study with patients, caregivers, and
    health care providers in Lebanon. Health Res Policy Sys 22, 38 (2024). https://doi.org/10.1186/s12961-024-01131-9
    Download citation Received 15 August 2023 Accepted 09 March 2024 Published 27
    March 2024 DOI https://doi.org/10.1186/s12961-024-01131-9 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    COVID-19 Medication accessibility Medication shortages Treatment decisions Economic
    crisis Black market Healthcare system Healthcare professionals Download PDF Download
    ePub Sections Figures References Abstract Introduction Methods Results Discussion
    Conclusions What is already known on this topic What this study adds How this
    study might affect research, practice or policy Availability of data and materials
    Abbreviations References Acknowledgements Funding Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Health Research Policy and Systems ISSN: 1478-4505 Contact us Submission enquiries:
    Access here and click Contact Us General enquiries: info@biomedcentral.com Read
    more on our blogs Receive BMC newsletters Manage article alerts Language editing
    for authors Scientific editing for authors Policies Accessibility Press center
    Support and Contact Leave feedback Careers Follow BMC By using this website, you
    agree to our Terms and Conditions, Your US state privacy rights, Privacy statement
    and Cookies policy. Your privacy choices/Manage cookies we use in the preference
    centre. © 2024 BioMed Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: Health Research Policy and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Choosing and accessing COVID-19 treatment options: a qualitative study with
    patients, caregivers, and health care providers in Lebanon'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ali I.
  - Wassif K.
  - Bayomi H.
  citation_count: '0'
  description: 'Sensors, wearables, mobile devices, and other Internet of Things (IoT)
    devices are becoming increasingly integrated into all aspects of our lives. They
    are capable of gathering enormous amounts of data, such as image data, which can
    then be sent to the cloud for processing. However, this results in an increase
    in network traffic and latency. To overcome these difficulties, edge computing
    has been proposed as a paradigm for computing that brings processing closer to
    the location where data is produced. This paper explores the merging of cloud
    and edge computing for IoT and investigates approaches using machine learning
    for dimensionality reduction of images on the edge, employing the autoencoder
    deep learning-based approach and principal component analysis (PCA). The encoded
    data is then sent to the cloud server, where it is used directly for any machine
    learning task without significantly impacting the accuracy of the data processed
    in the cloud. The proposed approach has been evaluated on an object detection
    task using a set of 4000 images randomly chosen from three datasets: COCO, human
    detection, and HDA datasets. Results show that a 77% reduction in data did not
    have a significant impact on the object detection task’s accuracy.'
  doi: 10.1038/s41598-024-57385-4
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    26 March 2024 Dimensionality reduction for images of IoT using machine learning
    Ibrahim Ali, Khaled Wassif & Hanaa Bayomi  Scientific Reports  14, Article number:
    7205 (2024) Cite this article 243 Accesses 3 Altmetric Metrics Abstract Sensors,
    wearables, mobile devices, and other Internet of Things (IoT) devices are becoming
    increasingly integrated into all aspects of our lives. They are capable of gathering
    enormous amounts of data, such as image data, which can then be sent to the cloud
    for processing. However, this results in an increase in network traffic and latency.
    To overcome these difficulties, edge computing has been proposed as a paradigm
    for computing that brings processing closer to the location where data is produced.
    This paper explores the merging of cloud and edge computing for IoT and investigates
    approaches using machine learning for dimensionality reduction of images on the
    edge, employing the autoencoder deep learning-based approach and principal component
    analysis (PCA). The encoded data is then sent to the cloud server, where it is
    used directly for any machine learning task without significantly impacting the
    accuracy of the data processed in the cloud. The proposed approach has been evaluated
    on an object detection task using a set of 4000 images randomly chosen from three
    datasets: COCO, human detection, and HDA datasets. Results show that a 77% reduction
    in data did not have a significant impact on the object detection task’s accuracy.
    Similar content being viewed by others Constructing energy-efficient mixed-precision
    neural networks through principal component analysis for edge intelligence Article
    17 January 2020 Elderly and visually impaired indoor activity monitoring based
    on Wi-Fi and Deep Hybrid convolutional neural network Article Open access 18 December
    2023 Wrapper-based deep feature optimization for activity recognition in the wearable
    sensor networks of healthcare systems Article Open access 18 January 2023 Introduction
    In recent years, the explosion of sensors, wearables, mobiles, and other Internet
    of Things (IoT) devices has been changing how we live and work. The applications
    of IoT services have started pervading all industrial sectors, from smart homes
    and cities to education, healthcare, transportation, supply chain management,
    and logistics. There are many forecasts for the huge growth of the IoT. Analysts
    predict that there will be 41.6 billion connected IoT devices1, and the global
    economic impact of the IoT will be between USD 2.7 trillion and 6.2 trillion by
    20252. Image data from IoT sensor devices are exchanged over the network for storage,
    processing, or control. To realize the benefits of smart IoT systems and extract
    value from collected images, data analytics is essential in the cloud by transferring
    this data to the cloud for storage and processing. For example, images from a
    smart transportation system are transferred to a far-off data center for storage
    and processing. Attempting to transfer all those images to the cloud for processing
    will increase latencies and put a strain on communication networks. Those connected
    devices are limited in the analytics they can perform because of limited computation
    power, storage capacity, and other factors. Edge computing is a distributed computing
    paradigm that brings processing and data storage closer to the sources of data.
    This is expected to improve response times and save bandwidth3. It is an architecture
    rather than a specific technology. The edge server acts as a connection between
    a private network in an organization and the cloud. It can be used for processing
    offloading and can act as an intermediary between the cloud and IoT devices by
    performing a reduction on data and sending the reduced data to the cloud for further
    processing. Deep learning4 is a subclass of machine learning (ML) that plays a
    vital role in creating a smarter IoT. It has shown remarkable results in various
    fields, including dimensionality reduction and image recognition. The combination
    of deep learning and dimensionality reduction enhances the capabilities of IoT
    systems by enabling efficient data processing, accurate pattern recognition, and
    adaptability to changing conditions. These techniques contribute to making IoT
    applications more intelligent, responsive, and resource-efficient. An autoencoder
    is a type of deep neural network that can be used to learn efficient data encoding
    in an unsupervised manner. In this paper, two trained autoencoder models are compared
    in terms of their data reduction capabilities and their impact on machine learning
    tasks within the cloud server. Additionally, a comparative analysis is conducted
    between autoencoder models and principal component analysis (PCA) to explore variations
    between the two approaches. Four primary scenarios are taken into consideration.
    The initial scenario represents the baseline, where dimensionality reduction is
    not applied. This scenario will be used to evaluate the other scenarios and compare
    our approach against the results of this baseline scenario. In the second and
    third scenarios, two different models of autoencoders are employed to reduce the
    image dimensionality on the edge server, and a machine learning (ML) task is run
    on the decoded images in the cloud. In the fourth scenario, principal component
    analysis (PCA) is used on the edge to encode images similar to the second and
    third scenarios. Then the cloud machine learning task is carried out on the encoded
    images. The scenarios are carried out for the task of object detection using a
    set of 4000 images randomly chosen from three different data sets. Results show
    that autoencoders can decrease network bandwidth without significantly affecting
    the accuracy of machine-learning tasks. The rest of this article is organized
    as follows: “Background” provides background concepts, and “Related works” reviews
    related work. The methodology is described in “Methodology”, and the experiment
    and results are presented in “Experiment and results”. Finally, “Conclusion and
    future work” presents a conclusion and future work. Background This section introduces
    some concepts about deep learning, principal component analysis, and edge-cloud
    architecture: Deep learning Deep learning (DL) is highly valuable for learning
    complex models due to its ability to automatically extract complex patterns and
    features from data. Using neural networks with multiple layers, deep learning
    can discern hierarchical relationships within information, enabling the modeling
    of complex structures. This makes it particularly effective in tasks like image
    recognition, natural language processing, and pattern recognition, where understanding
    complex details is crucial5. The capacity of deep learning to learn from vast
    datasets and capture precise patterns allows it to play a vital role in various
    sectors, including healthcare, transportation, and others6. The autoencoder (AE)7
    is a valuable model in dimensionality reduction, simplifying complex datasets
    by capturing essential features. By learning efficient representations, autoencoders
    compress high-dimensional data into a lower-dimensional space. This reduction
    not only aids in preserving critical information but also accelerates computational
    processes. Autoencoders find applications in diverse fields, from image and signal
    processing to feature extraction, contributing to improved efficiency and streamlined
    analysis in various tasks. The autoencoder takes the data, propagates it through
    a number of hidden layers to understand and condense its structure, and finally
    generates that data again. The autoencoder uses two types of networks: the first
    is called an encoder, and the other is a decoder, with the layers inside the encoder
    reflected in the decoder. Principal component analysis Utilizing principal component
    analysis (PCA)8 in dimensionality reduction is a fundamental approach to streamline
    complex datasets and enhance computational efficiency. PCA identifies the principal
    components, which are linear combinations of the original features capturing the
    maximum variance in the data. By focusing on these key components, PCA allows
    for the reduction of data dimensions while preserving essential information. This
    process not only accelerates computational tasks but also aids in mitigating issues
    associated with high-dimensional data, such as the curse of dimensionality. In
    various applications, ranging from image and signal processing to machine learning,
    PCA proves instrumental in simplifying data representations, facilitating more
    effective analysis, and improving the overall performance of algorithms. Edge-cloud
    architecture Edge devices9 play a pivotal role in the Internet of Things (IoT)
    ecosystem by bringing computational power closer to the data source. Unlike traditional
    cloud-centric models, edge computing allows for real-time processing and analysis
    of data at or near the point of origin. This minimizes latency, reduces the strain
    on communication networks, and enhances overall system efficiency. Edge devices
    enable quicker decision-making for applications like smart cities, healthcare,
    and industrial automation. By distributing computing tasks between the edge and
    the cloud, these devices contribute to a more responsive, scalable, and resilient
    IoT infrastructure. In line with this strategy, the edge device will be used to
    apply dimensionality reduction methods to the image data before sending it to
    the cloud. Related works In the past few years, edge computing has been gaining
    considerable attention from both the research and industry sectors because it
    promises to reduce network traffic and latencies and reduce reliance on the cloud10,11.
    Ghosh, Ananda, et al.12 proposed combining the edge-cloud architecture for IoT
    data analytics by leveraging edge nodes to reduce data transfer. To process data
    near the source, sensors are grouped according to locations, and feature learning
    is performed on the nearby edge node. They conducted experiments on a machine-learning
    task, specifically classification. The evaluation was performed on a task of human
    activity recognition from sensor data using the Mobile Health text-based dataset.
    The results demonstrated that the approach could reduce both data and the corresponding
    network traffic by up to approximately 80% with no significant loss of accuracy,
    especially when applying a large sliding window in the preprocessing phase. Couturier,
    Salman, et al.13 implemented a denoising super-resolution deep learning model
    to restore high-quality images, with the application server receiving degraded
    images at a high compression ratio from the sender side. The experimental analysis
    demonstrates the effectiveness of this solution in enhancing the visual quality
    of compressed and downscaled images. As a result, the proposed approach effectively
    reduces the overall communication overhead and power consumption of constrained
    Multimedia Internet of Things (MIoT) devices. Sood et al.14 propose a two-stage
    network traffic anomaly detection system compatible with the ETSI-NFV standard
    5G architecture. Their architecture involves reducing dimensionality to compress
    the sample size at the edge of 5G networks, along with a deep neural network (DNN)
    classifier for detecting traffic anomalies. They utilized the UNSW-NB15 dataset
    and demonstrated that, with a dimensionality reduction factor of 81%, the achieved
    detection accuracy is 98%. Sujitha, Ben, et al.15 proposed a method comprising
    two convolutional neural networks (CNNs) and a Lempel–Ziv Markov chain algorithm
    (LZMA)-based image codec. They presented a new image compression method for remote
    sensing using CNN. To balance image quality and compression efficiency, they used
    two CNNs, one on the encoding side and the other on the decoding side. The results
    proved the effectiveness of the presented method, which achieves an average peak
    signal-to-noise ratio (PSNR) of 49.90 dB and an average space-saving (SS) of 89.38%.
    Krishnaraj et al.16 utilized a discrete wavelet transform (DWT)-based deep learning
    model for image compression on the Internet of Underwater Things (IOUT), achieving
    effective compression with better reconstructed image quality. A convolutional
    neural network (CNN) is utilized on both the encoding and decoding sides. The
    DWT-CNN model attains an average peak signal-to-noise ratio (PSNR) of 53.961 with
    an average space-saving (SS) of 79.7038%. Zebang Song et al.17 demonstrated a
    lossy image compression architecture that leverages existing deep learning methods
    to achieve high coding efficiency. They designed a densely connected autoencoder
    structure for lossy image compression. Experiments show that their method significantly
    outperforms JPEG and JPEG2000 and can produce better visual results with sharp
    edges, rich textures, and fewer artifacts. Fournier and Aloise18 proposed an empirical
    comparison between autoencoders and traditional dimensionality reduction methods.
    They evaluated the performance of PCA compared to Isomap and a deep autoencoder.
    For the evaluations, a K-Nearest Neighbor (KNN) classifier was used, and the results
    show that PCA computation time is faster than that of its neural network counterparts.
    Some of the discussed studies did not use edge-cloud architecture integration
    with the IoT, and some of them focused on other data reduction methods without
    employing autoencoders or PCA. Additionally, some of them didn’t apply their evaluations
    to images. In contrast, our work explores the use of deep learning approaches
    for image dimensionality reduction on edge servers to decrease network traffic
    and latencies caused by data transfer to the cloud. We also apply an object detection
    machine learning task on the cloud to evaluate the approach. Methodology This
    section introduces the methodology of the edge-cloud architecture and also presents
    methods for data reduction with the autoencoder and PCA. The overall architecture
    of the edge cloud is described in Fig. 1, illustrating its three main components:
    IoT sensors, serving as the data source; edge servers; and the cloud server. The
    initiation of the edge-cloud architecture involves receiving data from IoT sensors
    at the edge. The diagram further illustrates a potential scenario where data from
    diverse sensors is directed to various edge nodes, and all nodes subsequently
    forward this data to a centralized location. This setup allows machine learning
    tasks running in the cloud to benefit from data originating from various sources,
    including edge nodes. While specific tasks can be performed on the edge nodes,
    they would only have access to data from a subset of sensors. Data reduction can
    also occur at the edge to minimize the amount of data transmitted to the cloud.
    Figure 1 Edge-cloud architecture for IoT. Full size image Previous research has
    demonstrated the effectiveness of autoencoders and PCA in the field of dimensionality
    reduction. In some cases, autoencoders not only reduce dimensionality but can
    also detect repetitive structures19,20. Figure 2 describes the autoencoder architecture,
    which takes input data and processes it through several hidden layers. The number
    of neurons in the hidden layers is smaller than the number of neurons in the input
    layer, forcing an autoencoder to learn the internal structure of the data. Figure
    2 Autoencoder architecture. Full size image To integrate the autoencoder into
    the edge-cloud architecture, the encoder component of the network is located on
    the edge, while the decoder component is on the cloud. This way, when high-dimensional
    data arrives at the edge node, it is reduced to a smaller number of dimensions
    according to the encoder architecture. After this data is sent to the cloud, it
    can be reconstructed using the cloud-based decoder component of the autoencoder
    and then utilized for ML tasks. A pre-trained model of the autoencoder was used
    in the experiments. Because autoencoder training requires a significant amount
    of time and computation, it must take place on high-spec devices such as the cloud
    or computers equipped with GPUs. Principal Component Analysis (PCA) is a widely
    used linear dimensionality reduction technique. It is quicker and less expensive
    to compute than autoencoders. Also, it is quite similar to a single-layered autoencoder
    with a linear activation function. This paper explores four fundamental scenarios,
    as illustrated in Fig. 3. Figure 3 Computation models. Full size image Scenario
    1 This represents the default scenario, where image data from sensors is transmitted
    directly to the cloud server, and machine learning tasks are executed directly
    using the original data. Scenario 2 Data from sensors is sent to edge nodes, where
    data reduction is performed using principal component analysis (PCA). Encoded
    data is then sent to the cloud, where machine learning tasks are carried out with
    the encoded data. Scenario 3 The edge nodes perform dimensionality reduction on
    the data using a two-layer autoencoder, which is a trained model with two hidden
    layers. Machine learning tasks are then carried out on the cloud directly with
    the decoded data. Scenario 4 Similar to scenario 3, but utilizing a three-layer
    autoencoder at the edge. The three-layer autoencoder is a trained model with three
    hidden layers. Experiment and results This section describes the experiments and
    the results. Experiment preparation In our approach, a dataset comprising 6000
    images has been used to train the autoencoder. The trained model will be used
    in the experiments to perform dimensionality reduction with the image data at
    the edge. The training performed on a machine has the following benefits: Nvidia
    GM107M [GeForce GTX 960 M] Intel CoreTM i7-6700HQ CPU @ 2.60 GHz 16 GB of RAM
    And the training model parameters include: Optimizer: Adam Epochs: 50 Activation:
    ReLu The dataset, which comprises 6000 images, was selected from both the COCO
    and DIV2K datasets: The Microsoft Common Objects in Context (MS COCO) dataset21
    is a large-scale dataset used for object detection, segmentation, key-point detection,
    and captioning. It comprises over 328K images with varying sizes and resolutions,
    each annotated with 80 object categories and five captions describing the scene.
    The DIV2K dataset22 comprises 1000 diverse 2K-resolution RGB images. All images
    were manually collected and have a resolution of 2K pixels on at least one axis
    (vertical or horizontal). DIV2K encompasses a wide diversity of content, ranging
    from people, handmade objects, and environments to natural scenery, including
    underwater scenes. Figures 4 and 5 display the training and validation losses
    for the two-layer and three-layer autoencoders, respectively. In the two-layer
    autoencoder, the training loss was 0.00362, and the validation loss was 0.00359.
    For the three-layer autoencoder, the training loss was 0.00205, and the validation
    loss was 0.00203. Additionally, the Structural Similarity Index Measure (SSIM)23
    is calculated for both models. SSIM is a method for predicting the perceived quality
    of digital television, cinematic pictures, and other types of digital images and
    videos. It is employed to measure the similarity between two images. The training
    shows that the Multi-Scale Structural Similarity Index Measure (MS-SSIM) on validation
    is 0.85716 for the two-layer autoencoder and 0.88425 for the three-layer autoencoder.
    This indicates a higher-quality reconstruction for the three-layer autoencoder
    compared to the two-layer autoencoder. Figure 4 Training and validation loss for
    the two-layer autoencoder. Full size image Figure 5 Training and validation loss
    for the three-layer autoencoder. Full size image When we increased the number
    of epochs to more than 50 and the number of hidden layers to more than three layers,
    overfitting occurred. Increasing the epoch size and the number of hidden layers
    provides the model with more time to converge to an optimal solution, potentially
    resulting in improved accuracy. However, there is a risk of overfitting during
    training, where the model may become too specialized for the training data, capturing
    noise. This could lead to a reduction in accuracy on the validation or test set.
    In machine learning, it is important to maintain the accuracy of the final machine
    learning task as high as possible. Since the primary objective of the proposed
    architecture is to reduce network traffic and latencies, considering the amount
    of data that can be reduced at the edge is also important. Machine learning task
    The proposed approach has been evaluated for the task of image object detection
    using YOLO, which stands for ‘You Only Look Once’. YOLO is a technique employed
    for real-time object recognition and detection in various images. It treats object
    detection as a regression problem, providing class probabilities for observed
    images. Convolutional neural networks (CNN) are utilized in the algorithm for
    rapid object identification. As the name implies, the approach requires only one
    forward propagation through a neural network to detect objects24. Data sets A
    set of 4000 images was used in the object detection task experiments, randomly
    chosen from three different datasets. The three datasets were selected to represent
    the diversity of the data used in the experiments. The datasets are: The MS COCO
    dataset21. The human detection dataset25 comprises 921 images from closed-circuit
    television (CCTV) footage, encompassing both indoor and outdoor scenes with varying
    sizes and resolutions. Among these, 559 images feature humans, while the remaining
    362 do not. The dataset is sourced from CCTV footage on YouTube and the Open Indoor
    Images dataset. The HDA Person Dataset26 is a multi-camera, high-resolution image
    sequence dataset designed for research in high-definition surveillance. 80 cameras,
    including VGA, HD, and Full HD resolutions, were recorded simultaneously for 30
    min in a typical indoor office scenario during a busy hour, involving more than
    80 people. Most of the image data is captured by traditional cameras with a resolution
    of 640 × 480. Experiments The following four experiments were conducted, aligning
    with the four scenarios outlined in Fig. 3. The experiment was executed according
    to the flow in Fig. 6, starting with the data from the camera sensors or the existing
    collection of images. An Android mobile application was developed to run on Lenovo
    tablets, responsible for transferring images to the edge servers (via the edge
    node’s IP address and socket programming). Furthermore, the edge performs dimensionality
    reduction methods on the received images. Figure 7 shows a developed simulation
    desktop application used in edge nodes to receive images from sensors, manage
    the dimensionality reduction method, and transmit encoded data to the cloud server.
    Figure 6 Flowchart diagram of the experiment. Full size image Figure 7 Simulations
    of edge device application. Full size image Experiment 1 Images are sent directly
    to the cloud from sensors, where an object detection task is performed on the
    data and the accuracy is measured. It will be used later to evaluate other experiments.
    Experiment 2 The principal component analysis (PCA) is utilised at the edge nodes
    to instantly reduce the dimensionality of data, and then the encoded data is sent
    to the cloud. The object detection task was carried out on the encoded data in
    the cloud, and the accuracy was computed. Experiment 3 Utilizing the encoder component
    of the autoencoder on the edge, the two-layer autoencoder encodes images in real
    time. The edge application directly transmits the encoded images to the cloud.
    Subsequently, the autoencoder’s decoder component operates in the cloud to decode
    the data. The decoded images are then used for the object detection task, and
    the accuracy is computed. Experiment 4 Similar to Experiment 3, but employing
    the three-layer autoencoder. The encoding and decoding times are taken into consideration
    for the autoencoder and PCA; the following three (Tables 1, 2, 3) provide examples
    of the encoding and decoding times for three different groups of images with different
    sizes and resolutions. It was noticed that the three-layer autoencoder’s encoding
    and decoding time was greater than the two-layer autoencoder’s in the chosen samples
    of images because it kept the quality of the decoded images close to the original
    ones. Table 1 Group 1 (high resolutions). Full size table Table 2 Group 2 (medium
    resolutions). Full size table Table 3 Group 3 (low resolutions). Full size table
    Results Two aspects of the system were evaluated: the impact of data reduction
    on the ML task accuracy and the degree of data reduction. For experiment 1, the
    accuracy, recall, precision, and F1-Score were calculated, and the results were
    all the same, or very close, at 93.06%. For experiment 2, the results were 84.66%.
    For experiment 3, the results were 87.63%. And for Experiment 4, the results were
    89.14%. The accuracy of the object detection task using an autoencoder and PCA
    is compared in Fig. 8. It is noticeable that when using the three-layer model
    of the autoencoder, the machine learning task achieved good accuracy close to
    the original scenario, which is better than using the two-layer model. However,
    when using PCA to encode the data, the machine learning task achieved less accuracy
    than an autoencoder. Figure 8 Accuracy outcomes. Full size image As the results
    show, increasing the number of hidden layers in the autoencoder model by one improves
    the quality of the autoencoder’s latent representation when decoding the data
    and results in high machine-learning accuracy. Additionally, the following (Figs.
    9, 10, 11) compare the accuracy of the object detection task using different groups
    of images with various sizes and resolutions. In group #1, the accuracy for the
    native experiment was 100, while using both models of the autoencoder resulted
    in an accuracy of 90, and it was 80 when using PCA. For group #2, the accuracy
    for the native experiment was 89, and when using the two-layer and three-layer
    autoencoders, the accuracy was 82 and 86, respectively, and it was 80 when using
    PCA. In group #3, the accuracy for the native experiment was 84, and when using
    the two-layer and three-layer autoencoders, the accuracy was 74 and 79, respectively,
    and it was 77 when using PCA. It was observed that an increase in image resolution
    enhances the quality of the decoded images produced by the autoencoder decoder
    part, resulting in improved accuracy for object detection tasks. Figure 9 Group
    #1: accuracy of the object detection task using 10 images totaling 52.7 MB in
    size (high resolution). Full size image Figure 10 Group #2: accuracy of the object
    detection task using 10 images totaling 2.6 MB in size (medium resolution). Full
    size image Figure 11 Group #3: accuracy of the object detection task using 10
    images totaling 107.1 KB in size (low resolution). Full size image Because the
    main objective of our approach is to reduce network traffic and latencies, it
    is important to examine how much the proposed approach reduces data size. Figure
    12 compares the uploaded original data size to the total size for other experiments.
    Figure 13 shows the percentage of the total size of uploaded images. It can be
    seen that the data is reduced from 710 MB for the original data to 142.1 MB when
    using the two-layer autoencoder (i.e. an 80% reduction), 163.9 MB when using the
    three-layer autoencoder (i.e. a 77% reduction), and 226.3 MB when using PCA (i.e.
    a 68% reduction). Consequently, the data sent to the cloud is significantly reduced,
    which is especially important in the case of large data quantities such as those
    in the IoT. Figure 12 Data size for different scenarios. Full size image Figure
    13 Percentage of uploaded data. Full size image The experiments presented here
    show that, by using autoencoders, we were able to reduce the dimensionality of
    the images without significantly impacting the accuracy of machine-learning tasks.
    Additionally, images with high resolution and quality exhibited better results
    than images with low quality in terms of object detection tasks and the autoencoder
    decoder components when decoding the encoded data. Based on these outcomes, it
    is clear that applying this approach can effectively lower both the bandwidth
    usage and storage needs of IoT devices. Moreover, increasing the compression rate
    in a deep learning autoencoder for images will improve storage efficiency and
    faster transmission, but at the cost of decreased image quality and potential
    loss of information. The trade-off between compression and image fidelity needs
    to be carefully managed based on the goals and constraints of the particular application
    or use case. Conclusion and future work Massive amounts of data have been generated
    through data collected across IoT applications, mostly through the sensors connected
    to the devices, and this trend is expected to continue. There will be an increase
    in network traffic and latency if all of this data is attempted to be transferred
    to the cloud for processing and storage. To address these challenges, this work
    proposes combining edge and cloud architectures for IoT and utilizing machine
    learning, specifically autoencoders and PCA, to reduce the quantity of data sent
    to the cloud. The autoencoder’s encoder component is placed at the edge. Afterward,
    the data is transferred to the cloud for additional processing. The original data
    can be restored using the decoder component of the autoencoder and then used directly
    for the machine learning task, such as object detection. The proposed approach
    was evaluated on a set of 4000 images randomly chosen from three datasets: COCO,
    human detection, and HDA datasets. Results show that the autoencoder model is
    capable of significantly reducing the size of uploaded images without a significant
    impact on machine learning task accuracy. The suggested approach is used and examined
    only for images. Future research will explore how the suggested approach might
    be applied to various types of data. Moreover, the research will examine how the
    suggested methodology might be used for various machine learning tasks and with
    various datasets. Data availability The datasets used or analysed during the current
    study are available from the corresponding author upon reasonable request. References
    Tripathi, A., Sindhwani, N., Anand, R. & Dahiya, A. Role of IoT in smart homes
    and smart cities: challenges, benefits, and applications. In IoT Based Smart Applications
    (eds Tripathi, A. et al.) 199–217 (Springer, 2022). Google Scholar   Alsharif,
    M. H., Jahid, A., Kelechi, A. H. & Kannadasan, R. Green IoT: A review and future
    research directions. Symmetry 15, 757. https://doi.org/10.3390/sym15030757 (2023).
    Article   ADS   Google Scholar   Saba, T. et al. Cloud-edge load balancing distributed
    protocol for IoE services using swarm intelligence. Cluster Comput. 26, 2921–2931.
    https://doi.org/10.1007/s10586-022-03916-5 (2023). Article   Google Scholar   Kabir,
    M. F., Chen, T. & Ludwig, S. A. A performance analysis of dimensionality reduction
    algorithms in machine learning models for cancer prediction. Healthc. Anal. 3,
    100125. https://doi.org/10.1016/j.health.2022.100125 (2023). Article   Google
    Scholar   Rajyalakshmi, V. & Lakshmanna, K. Detection of car parking space by
    using hybrid deep DenseNet optimization algorithm. Int. J. Netw. Manag. https://doi.org/10.1002/nem.2228
    (2023). Article   Google Scholar   Ramesh, B. & Lakshmanna, K. Multi head deep
    neural network prediction methodology for high-risk cardiovascular disease on
    diabetes mellitus. CMES Comput. Model. Eng. Sci. 137, 2513–2528 (2023). Google
    Scholar   Shinde, K., Itier, V., Mennesson, J., Vasiukov, D. & Shakoor, M. Dimensionality
    reduction through convolutional autoencoders for fracture patterns prediction.
    Appl. Math. Model. 114, 94–113. https://doi.org/10.1016/j.apm.2022.09.034 (2023).
    Article   MathSciNet   Google Scholar   Huang, D., Jiang, F., Li, K., Tong, G.
    & Zhou, G. Scaled PCA: A new approach to dimension reduction. Manag. Sci. 68(3),
    1678–1695. https://doi.org/10.1287/mnsc.2021.4020 (2022). Article   Google Scholar   Babar,
    M. et al. An optimized IoT-enabled big data analytics architecture for edge-cloud
    computing. IEEE Internet Things J. 10, 3995–4005. https://doi.org/10.1109/JIOT.2022.3157552
    (2022). Article   PubMed   PubMed Central   Google Scholar   Fazeldehkordi, E.
    & Grønli, T. M. A survey of security architectures for edge computing-based IoT.
    IoT 3, 332–365. https://doi.org/10.3390/iot3030019 (2022). Article   Google Scholar   Kong,
    L. et al. Edge-computing-driven internet of things: A survey. ACM Comput. Surv.
    55, 1–41. https://doi.org/10.1145/3555308 (2022). Article   CAS   Google Scholar   Ghosh,
    A. M. & Grolinger, K. Edge-cloud computing for Internet of Things data analytics:
    Embedding intelligence in the edge with deep learning. IEEE Trans. Ind. Inf. 17,
    2191–2200. https://doi.org/10.1109/TII.2020.3008711 (2021). Article   Google Scholar   Noura,
    H. N., Azar, J., Salman, O., Couturier, R. & Mazouzi, K. A deep learning scheme
    for efficient multimedia IoT data compression. Ad Hoc Netw. 138, 102998. https://doi.org/10.1016/j.adhoc.2022.102998
    (2023). Article   Google Scholar   Sood, K. et al. Intrusion detection scheme
    with dimensionality reduction in next generation networks. IEEE Trans. Inf. Forensics
    Secur. 18, 965–979. https://doi.org/10.1109/TIFS.2022.3233777 (2023). Article   Google
    Scholar   Sujitha, B. et al. Optimal deep learning based image compression technique
    for data transmission on industrial Internet of things applications. Trans. Emerg.
    Telecommun. Technol. 32, e3976. https://doi.org/10.1002/ett.3976 (2021). Article   Google
    Scholar   Krishnaraj, N., Elhoseny, M., Thenmozhi, M., Selim, M. M. & Shankar,
    K. Deep learning model for real-time image compression in Internet of Underwater
    Things (IoUT). J. RealTime Image Process. 17, 2097–2111. https://doi.org/10.1007/s11554-019-00879-6
    (2020). Article   Google Scholar   Zebang, S., Sei-ichiro, K. Densely connected
    AutoEncoders for image compression. In Proceedings of the 2nd International Conference
    on Image and Graphics Processing. 78–83 (2019) Fournier, Q., Aloise, D. Empirical
    comparison between autoencoders and traditional dimensionality reduction methods.
    In 2019 IEEE Second International Conference on Artificial Intelligence and Knowledge
    Engineering (AIKE). 211–214 (2019) Laskar, MTR., Chen, C., Johnston, J., Fu, XY.,
    Bhushan, TN., S, Corston-Oliver, S. An auto encoder-based dimensionality reduction
    technique for efficient entity linking in business phone conversations. In Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval. 3363–3367 (2022) Abdulhammed, R., Musafer, H., Alessa,
    A., Faezipour, M. & Abuzneid, A. Features dimensionality reduction approaches
    for machine learning based network intrusion detection. Electronics 8, 322. https://doi.org/10.3390/electronics8030322
    (2019). Article   Google Scholar   Lin TY et al. Microsoft coco: Common objects
    in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6–12, 2014, Proceedings. Vol. 8693. 740–755. (Springer, 2014). Agustsson
    E, Timofte R. Ntire 2017 challenge on single image super-resolution: Dataset and
    study. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops. 126–135 (2017). Sara, U., Akter, M. & Uddin, M. S. Image quality assessment
    through FSIM, SSIM, MSE and PSNR—A comparative study. J. Comput. Commun. 7, 8–18.
    https://doi.org/10.4236/jcc.2019.73002 (2019). Article   Google Scholar   Redmon
    J, Farhadi A. YOLO9000: Better, faster, stronger. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition. 7263–7271 (2017). Werner, C. Human
    Detection Dataset. https://www.kaggle.com/datasets/constantinwerner/human-detection-dataset
    (2021). Accessed 7 April 2022. Nambiar, A., Taiana, M., Figueira, D., Nascimento,
    J. C. & Bernardino, A. A multi-camera video dataset for research on high-definition
    surveillance. Int. J. Mach. Intell. Sens. Signal Process. 1, 267–286. https://doi.org/10.1504/IJMISSP.2014.066428
    (2014). Article   Google Scholar   Download references Funding Open access funding
    provided by The Science, Technology & Innovation Funding Authority (STDF) in cooperation
    with The Egyptian Knowledge Bank (EKB). Author information Authors and Affiliations
    Computer Science Department, Faculty of Computers and Artificial Intelligence,
    Cairo University, Giza, Egypt Ibrahim Ali, Khaled Wassif & Hanaa Bayomi Contributions
    All authors contributed to the system design and reviewed the manuscript. Corresponding
    author Correspondence to Ibrahim Ali. Ethics declarations Competing interests
    The authors declare no competing interests. Additional information Publisher''s
    note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article''s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article''s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Ali, I., Wassif,
    K. & Bayomi, H. Dimensionality reduction for images of IoT using machine learning.
    Sci Rep 14, 7205 (2024). https://doi.org/10.1038/s41598-024-57385-4 Download citation
    Received 07 March 2023 Accepted 18 March 2024 Published 26 March 2024 DOI https://doi.org/10.1038/s41598-024-57385-4
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Edge computing Deep learning IoT Autoencoder Subjects Computer
    science Information technology Software Comments By submitting a comment you agree
    to abide by our Terms and Community Guidelines. If you find something abusive
    or that does not comply with our terms or guidelines please flag it as inappropriate.
    Download PDF Sections Figures References Abstract Introduction Background Related
    works Methodology Experiment and results Results Conclusion and future work Data
    availability References Funding Author information Ethics declarations Additional
    information Rights and permissions About this article Comments Advertisement Scientific
    Reports (Sci Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press
    releases Press office Contact us Discover content Journals A-Z Articles by subject
    Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open
    access Author & Researcher services Reprints & permissions Research data Language
    editing Scientific editing Nature Masterclasses Research Solutions Libraries &
    institutions Librarian service & tools Librarian portal Open research Recommend
    to library Advertising & partnerships Advertising Partnerships & Services Media
    kits Branded content Professional development Nature Careers Nature Conferences
    Regional websites Nature Africa Nature China Nature India Nature Italy Nature
    Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy
    choices/Manage cookies Legal notice Accessibility statement Terms & Conditions
    Your US state privacy rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Dimensionality reduction for images of IoT using machine learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Halabi M.
  - Khoury K.
  - Alomar A.
  - Dahdah J.E.
  - Hassan O.
  - Hayyan K.
  - Bishara E.
  - Moussa H.
  citation_count: '0'
  description: Robotic-assisted surgery has gained momentum in the pursuit of improved
    minimally invasive procedures. The adoption of new robotic platforms, such as
    the Versius, raises concerns about safety, efficacy, and learning curves. This
    study compares the Versius to the well-established da Vinci in terms of operative
    time and patient population. Retrospective data collection was conducted on patient
    data from inguinal hernia surgery, ventral hernia surgery, and cholecystectomies
    performed between February 2022 and March 2023 at the American Hospital of Dubai.
    Only experienced cases were included, ensuring proficiency with robotic technology.
    Versius had longer procedure times in inguinal and ventral hernia surgeries but
    not in cholecystectomy. No intraoperative complications were observed in either
    system. This study demonstrates that Versius can provide comparable outcomes to
    the da Vinci in abdominal surgery, with no observed intraoperative complications.
  doi: 10.1007/s11701-023-01806-5
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Journal of Robotic Surgery Article
    Operative efficiency: a comparative analysis of Versius and da Vinci robotic systems
    in abdominal surgery Research Open access Published: 22 March 2024 Volume 18,
    article number 132, (2024) Cite this article Download PDF You have full access
    to this open access article Journal of Robotic Surgery Aims and scope Submit manuscript
    Mouhammad Halabi, Kayanne Khoury, Abdulrahman Alomar, Joseph El Dahdah, Obai Hassan,
    Khadija Hayyan, Engy Bishara & Hatem Moussa  1163 Accesses Explore all metrics
    Abstract Robotic-assisted surgery has gained momentum in the pursuit of improved
    minimally invasive procedures. The adoption of new robotic platforms, such as
    the Versius, raises concerns about safety, efficacy, and learning curves. This
    study compares the Versius to the well-established da Vinci in terms of operative
    time and patient population. Retrospective data collection was conducted on patient
    data from inguinal hernia surgery, ventral hernia surgery, and cholecystectomies
    performed between February 2022 and March 2023 at the American Hospital of Dubai.
    Only experienced cases were included, ensuring proficiency with robotic technology.
    Versius had longer procedure times in inguinal and ventral hernia surgeries but
    not in cholecystectomy. No intraoperative complications were observed in either
    system. This study demonstrates that Versius can provide comparable outcomes to
    the da Vinci in abdominal surgery, with no observed intraoperative complications.
    Similar content being viewed by others The death of laparoscopy Article 22 March
    2024 Review of emerging surgical robotic technology Article 13 February 2018 Ultrasound-guided
    robotic surgical procedures: a systematic review Article Open access 21 March
    2024 Introduction Over the years, robot-assisted surgery’s potential to deliver
    better outcomes in minimally invasive surgery (MIS) has gained considerable attention.
    Slowly straying from traditional and conventional surgical methods, healthcare
    centers worldwide have begun adopting this transformative approach with the aim
    of delivering better post-operative outcomes as well as minimizing recovery time
    [1]. When introducing a new robotic platform into a hospital system, prioritizing
    patient safety is paramount. Given the swift adoption of this technology by hospitals
    worldwide, it is crucial to exercise caution in order to minimize the risk of
    adverse events. Introducing a novel robotic system with comparable developmental
    milestones to its predecessors, the CMR Versius features individual robotic arms
    mounted on separate carts, eliminating the need for docking to the trocars. The
    company places a strong emphasis on addressing issues of cost-effectiveness, seamless
    connectivity, and simplified software updates, aligning with a biomedical engineering
    approach [2]. Although previous studies have discussed the implementation of the
    Versius robotic surgery platform for abdominal surgery [3,4,5,6], none have compared
    its performance to that of another robotic system, such as the da Vinci, a relatively
    well-established robot that has pioneered the domain of robot-assisted surgery.
    It is imperative to do so in order to assess the feasibility of using the Versius
    to deliver safe and reproducible outcomes. This study describes the multifaceted
    process of implementing the Versius surgical robotic system at an experienced
    robotic surgery center. Herein, we present the data regarding the performance
    of the Versius in inguinal hernia, ventral hernia, and cholecystectomy procedures
    across both demographic and clinical parameters. Methods This study retrospectively
    reviewed maintained data for patients who underwent hernia and gallbladder surgery
    between February 2022 and March 2023 at the American Hospital of Dubai (AHD).
    The robotic platforms used were the da Vinci Robot (Xi–OS4; v9, Intuitive Surgical,
    Inc., Sunnyvale, California) and the CMR Versius (Cambridge, United Kingdom).
    The study was approved by the AHD Institutional Review Board. The data included
    in this study encompasses experienced cases only (an experienced case was defined
    after a surgeon has surpassed the learning curve, which was after 15 cases) that
    were performed by a single experienced surgeon, which has performed 700 procedures
    utilizing the da Vinci system and 230 procedures using the Versius system. Thus,
    only data collected following the completion of comprehensive training and certification
    were included in this study. This selective approach ensured that outcomes Reflected
    the surgeon’s proficiency with the robotic technology. Cases at the beginning
    of the learning curve (<15 cases) were excluded from this study. The following
    variables were analyzed: age, sex, body mass index (BMI), and past medical history
    (which included comorbidities, such as hypertension, diabetes mellitus, dyslipidemia,
    and cardiovascular disease), previous surgeries, and total operative time. The
    total operative time for any robotic procedure was defined as console start to
    console finish; no docking times were included. Additionally, for analytical clarity,
    comorbidities were quantified as a unified category denoted by \"1\" without specific
    individual specifications. All patients in this study were required to be over
    the age of 18 and had to have undergone either cholecystectomy, inguinal hernia
    repair, or ventral hernia repair between February 2022 and March 2023. Any patients
    that did not fit these criteria were excluded from this study. Data are presented
    as mean ± standard deviation. Analysis of 249 patients was performed using the
    student’s t test for means and chi-square analysis for categorical variables.
    Significance was defined as P < 0.05. Results Table 1 summarizes demographic,
    clinical, and procedural data regarding inguinal hernia surgery performed with
    either the Versius or the da Vinci. The mean age for patients undergoing surgery
    with the Verisus was 44 years old (±11.3 SD), while that of da Vinci patients
    was 46.2 years old (±12.1 SD), with no significant difference observed (P = 0.29).
    In terms of gender distribution, 6% of patients that underwent surgery using the
    Versius robot were female, while 11.5% of the patients that underwent surgery
    with the da Vinci were female (P = 0.234). Table 1 Inguinal hernia Full size table
    BMI was significantly different between both groups (P = 0.023), with the mean
    BMI of the Versius patient group being 25.6 kg/m2 (±3.1 SD) and the da Vinci group
    being 27.1 kg/m2 (±4.2 SD). Alcohol use was significantly greater (P = 0.03) in
    the Versius group (24%) when compared to the da Vinci group (10.3%). Cases of
    hypertension were significantly greater (P = 0.003) in the da Vinci group (14.1%)
    when compared to the Versius group (0%). Overall, there was no significant difference
    (P = 0.07) in the number of comorbidities between both groups. The mean procedure
    length in the Versius group was significantly longer than that of the da Vinci
    group (103 ± 28.5 vs. 72.3 ± 28.3 SD mins respectively, P < 0.001). Table 2 presents
    data on ventral hernia surgery. There was no significant difference in the mean
    age of patients who underwent surgery using the Versius and those using da Vinci
    (P = 0.164) at 37.1 years old (±8.1 SD) and 42.6 years old (±14.5 SD), respectively.
    The gender distribution was significantly different between both groups (P = 0.009),
    with females making up 75% of patients that underwent surgery using the Versius
    as compared to 30% using da Vinci. Table 2 Ventral hernia Full size table No significant
    differences in social history were observed between both groups. Cases of hypertension
    were significantly greater (P = 0.009) in the da Vinci group (35%) when compared
    to the Versius group (0%). History of abdominal surgery was noted in 37.5% of
    patients in the Versius group compared to 55% in the da Vinci group, demonstrating
    no significant difference (P = 0.194). Altogether, there was no significant difference
    in the number of comorbidities between both groups (P = 0.118). The mean procedure
    length was significantly longer in the Versius group when compared to the da Vinci
    group (135 ± 46.1 vs. 94.1 ± 36.5 mins, P = 0.007). Table 3 outlines the data
    on cholecystectomy procedures. The mean age of patients in the Versius group was
    44.3 years old (±13.6 SD), while that of the da Vinci group was 44.3 years old
    (±10.4 SD), with no significant difference (P = 0.987). Females comprised 59.3%
    of patients in the Versius group, which is insignificantly different when compared
    to the 64.5% comprising the da Vinci group (P = 0.404). There was no significant
    difference observed between the BMI in both groups (P = 0.661), with the mean
    BMI in the Versius group being 29.0 kg/m2 (±4.3 SD) and the da Vinci group being
    29.52 kg/m2 (±6.3 SD). Cases of hypertension were significantly greater (P = 0.002)
    in the da Vinci group (19.4%) as compared to those in the Versius group (0%).
    Patients with diabetes mellitus in the Versius group were recorded at 11.1%, while
    those in the da Vinci group were at 22.6%, indicating no significant difference
    (P = 0.144). A history of abdominal surgery was noted in 38.9% of patients in
    the Versius group as opposed to 35.5% in the da Vinci group, with no significant
    difference observed (P = 0.392). Differences in patients’ overall number of comorbidities
    were insignificant between both groups (P = 0.793). Table 3 Cholecystectomy Full
    size table In contrast to both inguinal hernia and ventral hernia surgeries, no
    significant difference was observed in the procedure length between the Versius
    and the da Vinci (72.7 ± 23.2 vs. 82.6 ± 24.1 mins respectively, P = 0.079). Table
    4 condenses the data on all surgeries. There was no significant difference observed
    in the mean age of patients (P = 0.204) in both the Versius group and the da Vinci
    group, with values recorded at 43.23 years old (±12.21 SD) and 45.19 years old
    (±12.1 SD), respectively. The gender distribution consisted of 39.2% of patients
    being female in the Versius group compared to 27.1% in the da Vinci group. BMI
    was insignificantly different (P = 0.381) with the mean BMI in the Versius group
    being 27.53 kg/m2 (±4.11 SD) and that of the da Vinci group being 28.04 kg/m2.
    (±5.0 SD). Cases of hypertension were significantly greater (P < 0.001) in the
    da Vinci group (18.6%) when compared to those in the Versius group (0%). The difference
    in patients with a history of cardiovascular disease was not significant (P =
    0.435) when comparing the Versius group (6.7%) to the da Vinci group (5.4%). History
    of abdominal surgery was noted in 35.8% of patients in the Versius group compared
    to 40.3% in the da Vinci group, demonstrating no significant difference (P = 0.331).
    Overall, there was no significant difference (P = 0.141) in the number of comorbidities
    between both patient groups. Table 4 All procedures Full size table In this entire
    series, we observed no intraoperative complications in both the Versius and the
    da Vinci groups. Discussion In this study, we conducted a comprehensive analysis
    of demographic, clinical characteristics, and procedure lengths for patients undergoing
    surgery with the Versius surgical system to validate the safety and determine
    whether the risks associated with obtaining a new robotic surgery platform are
    outweighed by its outcomes. We were able to demonstrate that surgical procedures
    with the Versius can have comparable outcomes to that of the da Vinci in abdominal
    surgery. Furthermore, the learning curve on this new system was very minimal as
    all the procedures done were performed by experienced robotic surgeons, thus making
    the learning curve obsolete. Patients undergoing surgery with the Versius shared
    similar demographics to those who underwent surgery with the da Vinci from our
    initial experience. Although CMR’s Versius exhibits an array of positive characteristics
    in minimal access surgery—such as an ergonomic platform that allows for flexible
    placement of ports and versatility leading to enhanced proficiency—surgical systems,
    like any man-made machine, are subject to faults that may hinder certain outcomes.
    A study on robotic oncologic colorectal surgery by Huscher et al. demonstrates
    that the high degree of Versius’s versatility is, among other factors, due to
    the fact that the robotic arms are independent of one another. Despite this, the
    independence of robotic arms is also a contributing factor to increased operative
    time, as they tend to clash together without coordinated surveillance. Docking
    was found to be time-consuming as well. Versius is a novel robot that slightly
    differs from the standard of other robotic systems, leading to extra precautionary
    measures to be taken to decrease the fighting and clashing of instruments [7].
    Also, the positions of bedside units (BSUs) were found to limit the assistant
    surgeon from having a proper position, which led to uncomfortable adjustments
    when performing high-risk tasks. Another point found within this study is that
    the platform’s dexterity was not optimal due to missing certain anatomical structures
    while adjusting the directions of the instruments [7]. However, the flexibility
    of the bedside units provided increased flexibility in port placement, which has
    reduced the learning curve, making the initial experience with such a system preferable
    [8,9,10]. In spite of capturing high-quality images due to 3D technology, Huscher
    et al. reports that improved systems, such as near-infrared imaging for fluorescence,
    hyperspectral imaging, and picture-in-picture visualization, are not available
    on the platform. In addition, Versius does not have specific instruments, such
    as coagulation devices, staplers, aspirators, advanced sealers, and clip appliers
    [6, 7]. In robotic-assisted visceral surgery, Wehrmann et al. found that the focal
    point of the platform is always set to one space, making it nearly impossible
    to change from one part of the abdomen to another e.g., the right upper quadrant
    to the right lower quadrant, leading to the necessity of utilizing laparoscopic
    steps alongside robotic [6]. In another study on robotic oncologic colorectal
    surgery, Collins et al. also found difficulties in attempts to simultaneously
    work in more than one abdominal quadrant. Collins et al. reports that external
    clashing of robotic arms occurred due to increased mobility of the colon and longer
    instrument paths. As previously mentioned, lack of specific instruments led to
    the need to adopt a hybrid approach of laparoscopic and robotics in specific patient
    populations e.g., obese patients, which was evident in Collins et al.’s experience.
    In the case of a lower anterior resection, laparoscopic mobilization of the splenic
    flexure was done to save time, while in an extended right hemicolectomy, robotic
    splenic flexure mobilization was performed initially, followed by completion laparoscopically
    [11]. When compared to the da Vinci, the Versius robotic surgery platform demonstrated
    significantly longer operating times in inguinal hernia repair and ventral hernia
    repair, however, no significant difference was seen in procedure length for cholecystectomy
    cases. The patient population used in this study was also comparable, with very
    few significant differences in comorbidities between the two. Our study is retrospective
    in nature; as such it is only natural that we have important limitations that
    should be outlined. We had to keep our inclusion/exclusion criteria as broad as
    possible in order to obtain a reasonable number of patients for analysis. This
    may have introduced heterogeneity into our study which may have led to confounding.
    Considering our study was conducted at a single center, this limited our ability
    to generalize our results. Moreover, our sample size was restricted, which may
    have affected the statistical power and the reliability of our results. To our
    knowledge, we have the highest volume of patients undergoing robotic surgery in
    the Middle East and North Africa region. The success of our program has allowed
    us to adopt both the Versius and the da Vinci robotic platforms, which allows
    us to serve as a center for comparative studies. Although CMR has not proposed
    precautions to prevent clashing of robotic arms, altering the position of BSUs
    and calibrating them through a tailored approach to each patient, based on their
    anatomy and BMI, alongside monitoring of the equipment can prevent clashing. Since
    Versius is fundamentally based on end-user feedback, i.e., the surgical team,
    adjustments can be made by adding specific instruments that minimize the need
    for a hybrid approach. Updates can also be made to the platform’s interface by
    adding revolutionized visualization systems such as near-infrared imaging. This
    study highlights that while the Versius system has its unique challenges, it can
    be a valuable addition for new robotic surgery programs and offers potential benefits
    that can be harnessed with the right adaptations and movements. Conclusion This
    study compared the initial outcomes of the Versius and da Vinci robotic systems
    in abdominal surgery. Both systems were free of intraoperative complications,
    revealing relatively safe outcomes when comparing the Versius to the da Vinci.
    At the moment, the main limitations of the Versius seem to be instrument availability
    as well as its fixed focal point. While the Versius does offer remarkable advantages
    in terms of flexibility and versatility, further research is needed to optimize
    its performance and address its limitations, thus emphasizing the need for continued
    advancements in the world of robotic surgery. Data availability The data presented
    in this study are available on request from the corresponding author. The data
    are not publicly available due to privacy and ethical reasons. References Sheetz
    KH, Claflin J, Dimick JB (2020) Trends in the adoption of robotic surgery for
    common surgical procedures. JAMA Netw Open 3. https://doi.org/10.1001/jamanetworkopen.2019.18911
    Butterworth J, Sadry M, Julian D, Haig F (2021) Assessment of the training program
    for Versius, a new innovative robotic system for use in minimal access surgery.
    BMJ Surg Interv Health Technol 3. https://doi.org/10.1136/bmjsit-2020-000057 El
    Dahdah J, Halabi M, Kamal J et al (2022) Initial experience with a novel robotic
    surgical system in abdominal surgery. J Robot Surg 17:841–846 Article   PubMed   Google
    Scholar   Kelkar DS, Kurlekar U, Stevens L et al (2023) An early prospective clinical
    study to evaluate the safety and performance of the Versius surgical system in
    robot-assisted cholecystectomy. Ann Surg 277:9–17. https://doi.org/10.1097/SLA.0000000000005410
    Article   PubMed   Google Scholar   Soumpasis I, Nashef S, Dunning J et al (2023)
    Safe implementation of a next-generation surgical robot: first analysis of 2,083
    cases in the Versius surgical registry. Ann Surg 278:e903–e910. https://doi.org/10.1097/SLA.0000000000005871
    Article   PubMed   Google Scholar   Wehrmann S, Tischendorf K, Mehlhorn T et al
    (2023) Clinical implementation of the Versius robotic surgical system in visceral
    surgery-A single centre experience and review of the first 175 patients. Surg
    Endosc 37:528–534. https://doi.org/10.1007/s00464-022-09526-x Article   PubMed   Google
    Scholar   Huscher C, Marchegiani F, Cobellis F et al (2022) Robotic oncologic
    colorectal surgery with a new robotic platform (CMR Versius): hope or hype? A
    preliminary experience from a full-robotic case-series. Tech Coloproctol 26:745–753.
    https://doi.org/10.1007/s10151-022-02626-9 Article   PubMed   PubMed Central   Google
    Scholar   Morton J, Hardwick RH, Tilney HS et al (2021) Preclinical evaluation
    of the versius surgical system, a new robot-assisted surgical device for use in
    minimal access general and colorectal procedures. Surg Endosc 35:2169–2177. https://doi.org/10.1007/s00464-020-07622-4
    Article   PubMed   Google Scholar   Haig F, Medeiros ACB, Chitty K, Slack M (2020)
    Usability assessment of Versius, a new robot-assisted surgical device for use
    in minimal access surgery. BMJ Surg Interv Health Technol 2. https://doi.org/10.1136/bmjsit-2019-000028
    Kayser M, Krebs TF, Alkatout I, et al (2022) Evaluation of the Versius robotic
    surgical system for procedures in small cavities. Children 9. https://doi.org/10.3390/children9020199
    Collins D, Paterson HM, Skipworth RJE, Speake D (2021) Implementation of the Versius
    robotic surgical system for colorectal cancer surgery: First clinical experience.
    Colorectal Dis 23:1233–1238. https://doi.org/10.1111/codi.15568 Article   PubMed   Google
    Scholar   Download references Funding This research did not receive any specific
    grant from funding agencies in the public, commercial, or not-for-profit sectors.
    Author information Authors and Affiliations Department of Surgery, American Hospital
    Dubai, Dubai, United Arab Emirates Mouhammad Halabi, Engy Bishara & Hatem Moussa
    School of Medicine, Royal College of Ireland –Bahrain, Busaiteen, Bahrain Mouhammad
    Halabi, Kayanne Khoury, Abdulrahman Alomar, Obai Hassan & Khadija Hayyan Department
    of Cardiovascular Medicine, Heart and Vascular Institute, Cleveland Clinic Foundation,
    Cleveland, OH, USA Joseph El Dahdah Contributions MH: Literature Search, Data
    Collection, Data Analysis, Manuscript Writing. KK: Literature Search, Data Analysis
    and Manuscript Writing. AA: Literature Search and Manuscript Writing. JD: Data
    Analysis and Manuscript Review. OH: Data Collection and Manuscript Writing. KH:
    Data Collection. EB: Data Collection and Manuscript Writing. HM: Manuscript Review
    and is the Article Guarantor. Corresponding author Correspondence to Hatem Moussa.
    Ethics declarations Conflict of interest HM is a proctor for Intuitive Surgical
    and Cambridge Medical Robotics Surgical. MH, KK, AA, JD, OH, KH, and EB have no
    competing interests to declare. Ethical approval Ethics approval was obtained
    from the American Hospital Dubai institutional review board. Consent to participate
    Not applicable. Consent to publish Not applicable. Additional information Publisher''s
    Note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article''s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article''s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Halabi, M., Khoury,
    K., Alomar, A. et al. Operative efficiency: a comparative analysis of Versius
    and da Vinci robotic systems in abdominal surgery. J Robotic Surg 18, 132 (2024).
    https://doi.org/10.1007/s11701-023-01806-5 Download citation Received 20 October
    2023 Accepted 23 December 2023 Published 22 March 2024 DOI https://doi.org/10.1007/s11701-023-01806-5
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Robotic surgery Versius da Vinci Cholecystectomy Hernia Use
    our pre-submission checklist Avoid common mistakes on your manuscript. Sections
    References Abstract Introduction Methods Results Discussion Conclusion Data availability
    References Funding Author information Ethics declarations Additional information
    Rights and permissions About this article Advertisement Discover content Journals
    A-Z Books A-Z Publish with us Publish your research Open access publishing Products
    and services Our products Librarians Societies Partners and advertisers Our imprints
    Springer Nature Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage
    cookies Your US state privacy rights Accessibility statement Terms and conditions
    Privacy policy Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA)
    (3000133814) - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Journal of Robotic Surgery
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Operative efficiency: a comparative analysis of Versius and da Vinci robotic
    systems in abdominal surgery'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - He Q.
  - Zhao H.
  - Feng Y.
  - Wang Z.
  - Ning Z.
  - Luo T.
  citation_count: '0'
  description: Powered by data-driven technologies, precision agriculture offers immense
    productivity and sustainability benefits. However, fragmentation across farmlands
    necessitates distributed transparent automation. We developed an edge computing
    framework complemented by auction mechanisms and fuzzy optimizers that connect
    various supply chain stages. Specifically, edge computing offers powerful capabilities
    that enable real-time monitoring and data-driven decision-making in smart agriculture.
    We propose an edge computing framework tailored to agricultural needs to ensure
    sustainability through a renewable solar energy supply. Although the edge computing
    framework manages real-time crop monitoring and data collection, market-based
    mechanisms, such as auctions and fuzzy optimization models, support decision-making
    for smooth agricultural supply chain operations. We formulated invisible auction
    mechanisms that hide actual bid values and regulate information flows, combined
    with machine learning techniques for robust predictive analytics. While rule-based
    fuzzy systems encode domain expertise in agricultural decision-making, adaptable
    training algorithms help optimize model parameters from the data. A two-phase
    hybrid learning approach is formulated. Fuzzy optimization models were formulated
    using domain expertise for three key supply chain decision problems. Auction markets
    discover optimal crop demand–supply balancing and pricing signals. Fuzzy systems
    incorporate domain knowledge into interpretable crop-advisory models. An integrated
    evaluation of 50 farms over five crop cycles demonstrated the high performance
    of the proposed edge computing-oriented auction-based fuzzy neural network model
    compared with benchmarks.
  doi: 10.1186/s13677-024-00626-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Get published
    Explore Journals Books About Login Journal of Cloud Computing Advances, Systems
    and Applications About Articles Submission Guidelines Submit manuscript Research
    Open access Published: 21 March 2024 Edge computing-oriented smart agricultural
    supply chain mechanism with auction and fuzzy neural networks Qing He, Hua Zhao,
    Yu Feng, Zehao Wang, Zhaofeng Ning & Tingwei Luo  Journal of Cloud Computing  13,
    Article number: 66 (2024) Cite this article 244 Accesses Metrics Abstract Powered
    by data-driven technologies, precision agriculture offers immense productivity
    and sustainability benefits. However, fragmentation across farmlands necessitates
    distributed transparent automation. We developed an edge computing framework complemented
    by auction mechanisms and fuzzy optimizers that connect various supply chain stages.
    Specifically, edge computing offers powerful capabilities that enable real-time
    monitoring and data-driven decision-making in smart agriculture. We propose an
    edge computing framework tailored to agricultural needs to ensure sustainability
    through a renewable solar energy supply. Although the edge computing framework
    manages real-time crop monitoring and data collection, market-based mechanisms,
    such as auctions and fuzzy optimization models, support decision-making for smooth
    agricultural supply chain operations. We formulated invisible auction mechanisms
    that hide actual bid values and regulate information flows, combined with machine
    learning techniques for robust predictive analytics. While rule-based fuzzy systems
    encode domain expertise in agricultural decision-making, adaptable training algorithms
    help optimize model parameters from the data. A two-phase hybrid learning approach
    is formulated. Fuzzy optimization models were formulated using domain expertise
    for three key supply chain decision problems. Auction markets discover optimal
    crop demand–supply balancing and pricing signals. Fuzzy systems incorporate domain
    knowledge into interpretable crop-advisory models. An integrated evaluation of
    50 farms over five crop cycles demonstrated the high performance of the proposed
    edge computing-oriented auction-based fuzzy neural network model compared with
    benchmarks. Introduction Modern agriculture faces unprecedented stresses, such
    as rising food requirements from global population growth and declining arable
    land and water resources [1]. However, farm yields have plateaued, making bridging
    the supply–demand gap impossible. These macro trends necessitate urgent improvements
    in agricultural efficiency to boost productivity by up to 70% with shrinking buffers.
    Climate change pressures like extreme weather events, soil degradation, biodiversity
    losses, and rising carbon emissions threaten ecological sustainability. Agriculture
    accounts for over 25% of greenhouse gas emissions, highlighting the sizable decarbonization
    potential. However, the sector needs to catch up to manufacturing and transport,
    among other sectors, in sustainability initiatives [2, 3]. Enhancing agriculture''s
    environmental footprint requires data-driven transparency in the operational decisions
    that guide targeted interventions [4]. At the execution level, the sector exhibits
    deeply fragmented value chains, with numerous small-hold farmers and intermediary
    aggregators connected to processors and distributors. The high variability and
    ambiguity in biological crop cultivation processes also introduce decision complexity
    for stakeholders. Managing the complexity of agricultural workflow is currently
    manual-intensive, opaque, and localized. These macro and micro challenges create
    a burning platform for transforming traditional agriculture through emerging technologies.
    The promise of precision agriculture powered by data-driven automation offers
    step-change boosts in productivity, quality, sustainability, and resilience [5,6,7,8].
    Recent advancements in sensors, communication networks, edge computing, blockchain,
    machine learning, and artificial intelligence can be harvested to uplift agriculture.
    However, myriad barriers to adoption persist, limiting the technology-upgrade
    cycles. Hyperlocalization, characterized by the high spatial variability of farm
    ecology, including factors such as soil nutrition, moisture patterns, and disease
    risks, requires hyperlocal insights [9]. Centralized systems must capture these
    microclimatic nuances. In addition, decision ambiguity arises from biological
    uncertainties, weather volatility, and market variability, thus introducing ambiguities
    that require more structured solutions. Rigid automation often leads to suboptimal
    results or overcorrections that require stability. Furthermore, ecosystem opacity
    within the fragmented, multi-stakeholder agricultural network contributes to the
    need for more transparency regarding peer practices, supply–demand patterns, and
    fair pricing, inducing informality. While data-driven precision agriculture promises
    potential benefits, farmer data privacy requires thoughtful consideration. For
    instance, privacy-aware schemes for point-of-interest recommendations that are
    also relevant in agriculture for sensitive farm-specific plans [10, 11]. Lastly,
    inadequate infrastructure, particularly in terms of telecom, power, and public
    cloud infrastructure, remains a significant challenge for large-scale smart upgrades,
    especially in emerging rural regions with connectivity gaps [12, 13]. Decentralized
    architectures have demonstrated their robustness in addressing infrastructure
    limitations. Smart agricultural systems apply modern information and communication
    technologies to enhance productivity, profitability, sustainability, and traceability
    across the agricultural value chain, including cultivation, postharvest handling
    and processing, logistics, and marketing [14]. Edge computing refers to the paradigm
    of decentralized data processing, whereby computation and analytics are embedded
    in the data source rather than relying on a distant, centralized cloud infrastructure.
    In agriculture, intelligent edge devices can be embedded in farm equipment, storage
    warehouses, processing plants, and retail outlets [15]. Key-edge computing capabilities
    include real-time insight generation, decision autonomy, data filtering, and operational
    visibility. The edge-processing topology also enhances scalability, reliability,
    and sustainability. Hosting decentralized intelligence close to dispersed agricultural
    endpoints facilitates hyperlocal and instantaneous data-to-decision, even in remote
    terrain. Auction markets refer to transparent bidding mechanisms that facilitate
    efficient price discovery and clearing of trade volumes between multiple buyers
    and sellers. Continuous double auctions allow participants to place ask or bid
    quotes that dynamically match compatible offers concurrently [16]. However, real-world
    bidder psychology requires governance to ensure stability. Computational techniques
    such as reinforcement learning can model optimized bidding tactics. Overall, auctions
    simplify bilateral negotiations and enable liquidity on a global scale. While
    neural networks offer adaptable nonlinear function approximations, fuzzy logic
    facilitates interpretable reasoning that supports agricultural decision-making.
    Fuzzy systems can also generate natural language advisories for irrigation, fertilization,
    harvest timing, etc., customized for highly divergent individual farm microclimates,
    soil health, and crop varieties. Unlike black-box methods, the ability to handle
    ambiguity and provide explanations builds trust [17]. Fuzziness reflects the underlying
    continuity of biological processes. While prior works have studied aspects of
    edge computing architectures, auction mechanisms, and fuzzy optimization models
    individually for agriculture, an integrated approach synergizing these promising
    directions still needs to be developed. Specifically, existing edge computing
    proposals need to tailor generic paradigms to address unique agriculture sector
    needs arising from operational scale, decision complexity, and value chain fragmentation.
    Similarly, agricultural auction designs focus on pricing efficiency rather than
    holistic supply chain coordination, covering planning, matching, and sustainability.
    Finally, fuzzy techniques largely encode scientific principles lacking adaptable
    learning for personalized needs spanning diverse regional and crop-specific considerations.
    Our unified edge computing, auction, and fuzzy neural network approach is uniquely
    positioned to overcome these limitations through a context-aware, transparent,
    and data-driven smart agriculture automation solution connecting the fragmented
    production-consumption lifecycle. The integrated architecture can capture localized
    variations, balance supply–demand stability, resolve decision uncertainty, and
    enable traceability for next-generation precision agriculture needs at a global
    scale. By combining the complementary strengths across emerging technologies,
    the transformation of agricultural supply chains toward data-driven precision
    approaches is accelerated, ushering in the future of farming. Accordingly, the
    main contributions of this study are as follows: 1) We propose an edge computing
    framework tailored to agricultural requirements. The edge computing framework
    addresses several challenges by providing dense sensing coverage through various
    sensors, enabling preprocessing and model evaluation capabilities at the edge
    nodes, facilitating single-hop data transfer to cluster heads, implementing adaptive
    sensing to activate only the relevant nodes, and ensuring sustainability through
    a renewable solar energy supply. 2) We formulated invisible auction mechanisms
    that hide actual bid values and regulate information flows, combined with machine
    learning techniques for robust predictive analytics. 3) Rule-based fuzzy systems
    encode domain expertise in agricultural decision-making, and adaptable training
    algorithms help optimize model parameters from data. A two-phase hybrid learning
    approach is formulated. Fuzzy optimization models were formulated using domain
    expertise for three key supply chain decision problems. The remainder of this
    paper is organized as follows. Related work section  reviews related studies.
    Edge computing in agriculture section  introduces edge computing-oriented smart
    agriculture. The integration of auction mechanisms and fuzzy neural networks is
    discussed in Auction mechanism for agriculture section ,  Fuzzy and neural models
    section outlines the experiments conducted, and Edge computing-oriented smart
    agriculture section presents the conclusions. Related work Edge computing in agriculture
    Edge computing has emerged as a promising paradigm for addressing the challenges
    of data processing and decision-making in agriculture. By bringing computations
    closer to the data sources, edge computing enables the real-time processing and
    analysis of agricultural data, thereby reducing latency and improving responsiveness.
    Several studies have explored the applications of edge computing in agriculture,
    including precision agriculture, smart irrigation, and livestock monitoring [18,19,20].
    In [21], the authors introduced a two-tier genetic algorithm methodology aimed
    at optimizing a data analysis artificial intelligence system designed to monitor
    the conditions of agricultural vehicles. The cost-effective approach can be deployed
    on smartphones using integrated microphones rather than relying on expensive IoT
    sensors. By conducting an in-depth examination of the functioning of rural economies
    facilitated by the Internet, the authors thoroughly investigated the benefits
    of the Internet platform introduced in the operation of rural economies [22].
    Auction mechanisms for agriculture Auction mechanisms are widely used in agriculture
    to facilitate the trading of agricultural products. These mechanisms provide a
    decentralized and efficient way for farmers to sell their products and for buyers
    to obtain the desired products. Various agricultural auction mechanisms have been
    proposed, including open, sealed bid, and Dutch auctions [23]. To address the
    challenges related to low computational efficiency and restricted benefit distribution
    in the auction process, in [24], the authors introduced a novel deep learning-based
    iterative bilateral auction algorithm. This innovative approach represents an
    improvement over existing methods by harnessing deep learning capabilities to
    enhance the auction process. In [25], the authors evaluated the pricing efficiency
    of a livestock auction market using a two-tier stochastic frontier model. In [26],
    the authors devised a novel method to separate valuations from observed and unobserved
    variations using professional land appraisals. Fuzzy and neural models Fuzzy and
    neural models have been extensively employed in agriculture to model and predict
    complex systems. Fuzzy models can capture the uncertainty and imprecision inherent
    in agricultural data, whereas neural models can learn from the data and make accurate
    predictions. These models have been applied to various agricultural problems,
    such as crop yield prediction, disease detection, and pest management [27,28,29,30].
    Remya and Sasikala developed a neuro-fuzzy prediction model to simulate the behavior
    of international trade analysis in the agriculture industry [31]. Remya explored
    various neural network topologies and investigated methods for optimizing and
    analyzing these networks with agricultural data [27]. Ramana et al. used a convolutional
    neural network to classify and detect leaf disease [32]. Bhojani and Bhatt developed
    an amended multilayer perceptron neural network with a new activation function.
    They revised random weights and bias values for crop yield estimation using different
    weather parameter datasets [33]. Zhang et al. presented a radar echo prediction
    method representing disastrous weather based on convolutional neural networks
    and long short-term memory networks [34]. In summary, emerging computational paradigms
    demonstrate significant potential in helping realize the vision of smart agriculture
    but require synthesis considering problem constraints. Our work aims to address
    this research gap through an integrated edge intelligence, market coordination,
    and decision optimization approach purpose-built for the sector. Edge computing-oriented
    smart agriculture System model This study presents an edge computing framework
    complemented by auction mechanisms and fuzzy optimizers that connect various supply
    chain stages, as shown in Fig. 1. Fig. 1 Overall framework Full size image Edge
    computing offers powerful capabilities that enable real-time monitoring and data-driven
    decision-making in smart agriculture. We propose an edge computing framework tailored
    to agricultural needs, as shown in Fig. 2. Fig. 2 Proposed edge computing framework
    for smart agriculture Full size image The framework comprises three sections:
    the sensing layer, the edge computing layer, and the growth data model. Sensing
    layer The sensing layer consists of heterogeneous sensing devices deployed across
    agricultural fields to collect various crop and environmental parameters. Sensor
    nodes can be categorized as follows. Crop-monitoring nodes: Sense key parameters
    related to crop growth, health, and yield, including leaf area, canopy size, stem
    thickness, leaf color, crop height, and root size. Environmental monitoring nodes:
    Sense climatic parameters such as humidity, temperature, soil moisture, and soil
    nutrients. Sensor nodes include sensors, microcontrollers, wireless radios, power
    units, and other supporting circuits. Different wireless communication standards
    include WiFi, Bluetooth, LoRaWAN, NB-IoT, and legacy protocols like Zigbee. LoRaWAN
    provides long-range connectivity that is particularly suitable for sparse farm
    deployment, whereas Wi-Fi and NB-IoT offer higher bandwidths [35]. Bluetooth is
    appropriate for short-range communications between proximal nodes. Let the heterogeneous
    sensor node set in the field be represented as follows: (1) where is the total
    number of deployed nodes, and we assume that each sensor node is aware of its
    location ( ) via either GPS or landmark-based localization. Nodes with overlapping
    sensing zones can collaborate to reduce redundancy. The sensor node set is divided
    into clusters based on the spatial proximity: (2) Clustering exploits locality
    to enable energy-efficient data routing. Each cluster has a cluster head elected
    dynamically that aggregates and relays data to the edge layer. Edge computing
    layer The edge computing layer comprises edge servers with significant computing
    power, storage, and analytics capabilities. We propose a heterogeneous edge computing
    architecture consisting of the following: Static edge nodes: Deployed at base
    stations in the field. Mobile edge nodes: Mounted on autonomous ground robots
    or UAVs. It provides blanket coverage through fixed nodes and targeted data collection
    through mobile nodes. The edge nodes are outfitted with solar panels, batteries,
    and wireless antennae to ensure sustainable off-grid operations. Key capabilities
    offered by the edge computing layer include (i) Cluster data aggregation: Combine
    sensor data from nodes within clusters; (ii) Preprocessing and storage: Filter
    noise, detect outliers and temporally store data; (iii) Growth stage identification:
    Classify current growth phase based on crop parameters; (iv) Analytics: Environmental
    and yield predictions via ML models; (v) Control policies: Adaptive sensing frequencies,
    irrigation levels etc. These edge-centric functions distribute computations closer
    to the sensors, avoid cluttering the cloud, and support real-time agriculture.
    Next, we formulated mathematical models for crop and environmental sensing data.  Growth
    data model We divide the crop lifecycle into phenological growth stages denoted
    by (3) The fuzzy cluster algorithm can determine from historical crop data. Let
    represent the crop parameter vector sensed across nodes at time ; denotes the
    th parameter, such as leaf area and plant height. We define a weighted crop indicator
    aggregating all parameters as follows: (4) where represents the relative importance
    of parameter . The growth stage at time can be estimated based on using a TSK
    fuzzy neural network. For example, if representing leaf area and plant height,
    and , then . 0.7 and 0.3 are the weights, while 0.6 and 0.8 are the parameter
    values. The weights scale the parameter values before summing. Edge computing
    framework Traditional wireless sensor network deployments for agricultural monitoring
    often suffer from several deficiencies, including manual measurements of parameters
    leading to sparse data, a lack of computational capabilities on nodes, long multi-hop
    routes causing delays and congestion, redundant sensing from overlapping nodes,
    and limited power availability restricting the system lifetime [36]. Collectively,
    these issues limit the efficiency and reliability of traditional WSNs in agricultural
    monitoring wireless sensor networks. Our proposed edge computing framework addresses
    several of these challenges by providing dense sensing coverage through a variety
    of sensors, enabling preprocessing and model evaluation capabilities at edge nodes,
    facilitating single-hop data transfer to cluster heads, implementing adaptive
    sensing to activate only relevant nodes, and ensuring sustainability through a
    renewable solar energy supply. This comprehensive approach aims to significantly
    enhance the efficiency and effectiveness of agriculture monitoring wireless sensor
    networks. Consequently, the framework can collect high-resolution spatiotemporal
    data to better capture crop dynamics. Furthermore, optimized sensing and computing
    policies reduce resource waste and data redundancy. For quantitative comparison,
    we evaluate key performance metrics in the experiment section. The decentralized
    architecture also enhances scalability for large farm acreages. Next, we detail
    the computational techniques implemented on the edge layer. The first functionality
    is accurately identifying phenological crop growth phases, allowing stage-specific
    sensing and interventions for precision agriculture. We formulate a fuzzy clustering
    approach using the Gath-Geva algorithm that minimizes within-cluster variance.
    Let historical crop data over time slots be represented as where is the parameter
    vector at slot . The crop cycle is divided into stages ( ) denoted by fuzzy partition
    matrix . Element defines the membership of slot in stage . The cluster centers
    are . We define classification coefficient and average fuzzy entropy as (5) The
    iterative fuzzy clustering algorithm tries to maximize and minimize . The steps
    are summarized as follows: Step 1. Initialize: Partition matrix , clusters , iterations
    , weight . Step 2. Compute cluster centers using membership . Step 3. Determine
    cluster covariance and prior probability. Step 4. Calculate fuzzy maximum likelihood
    distance measure. Step 5. Update partition matrix . Step 6. Repeat steps 2–5 until
    . Step 7. Choose optimal based on best and β . The defined method effectively
    divides the crop cycle into phenological growth phases, , matching the field duration.
    Next, we predicted the current stage based on the sensed indicators. To determine
    the growth phase, we designed a Takagi–Sugeno (TS) fuzzy neural network model
    comprising five layers: input, fuzzification, rule, aggregation, and output. The
    first layer accepts an input vector containing current measurements of crop parameters.
    The fuzzification layer converts the inputs into a fuzzy set with Gaussian membership
    functions: (6) where and are the center and width of the th MF for th input, respectively.
    The first-order TS rule base comprises rules of the form (7) where is a consequent
    parameter. The net output is computed as where firing strength . For training,
    we used an extreme learning machine to randomly initialize the input layer weights
    and optimize the output layer weights analytically using the Moore–Penrose inverse.
    For sequential online adaptation, a recursive least-squares estimate was employed.
    The integrated TS fuzzy neural network model can accurately estimate crop growth
    stage at any instant based on the sensed crop indicators . Stage-specific control
    policies are then enacted. Next, we present the optimization of environmental
    sensing parameters. Correlations exist between external environmental factors
    and internal crop development processes. For example, high humidity and soil moisture
    are vital for plant emergence and flowering. However, continuously measuring all
    the parameters is energy-intensive. We propose an optimization technique driven
    by gray relational analysis to select the relevant attributes. Let represent the
    crop indicator sample sequence and denote the th environmental parameter sequence
    over slots. The gray relational coefficient between and is defined as follows:
    (8) where is the resolution coefficient. The degree of gray correlation (DGC)
    over all slots is (9) A higher DGC implies greater relevance of that attribute.
    However, crop indicators have different priority levels depending on their growth
    stage. Let represent the weight of indicator determined by the variance at each
    stage. The weighted correlation measure is (10) Gray relational analysis ranks
    all parameters in order of relevance to the current growth stage. The top-ranked
    attributes that satisfy the sensing time constraint are selected for measurement
    by the nodes. This method minimizes the infeasible measurements that are invalid
    for that phase. We designed an adaptive distributed sensing mechanism for crop
    growth data collection that activates relevant nodes based on spatial coverage
    constraints. Let represent a set of selected sensor nodes. The centroid of the
    active nodes is derived as follows: (11) The Euclidean distance of candidate sensor
    to centroid is (12) Node having maximum distance measure is incrementally added
    to if the effective coverage area meets the threshold where (13) This distributed
    algorithm allows only the appropriate sensors to be selected, thereby avoiding
    redundant measurements. The pseudocode is presented in Algorithm 1. Algorithm
    1. Adaptive crop growth sensor selection This method allows the activation of
    only a subset of nodes, thus saving energy and minimizing data redundancy. Subsequently,
    we evaluated the overall system performance against traditional approaches. Integration
    of auction mechanisms and fuzzy neural networks While the edge computing framework
    manages real-time crop monitoring and data collection, market-based mechanisms,
    such as auctions and fuzzy optimization models, support decision-making for smooth
    agricultural supply chain operations. Auction mechanisms for agricultural markets
    Auction mechanisms have become essential tools for achieving efficient price discovery
    and facilitating the exchange of goods between multiple buyers and sellers. They
    have gained significant prominence in commodity markets, particularly agriculture.
    These include automated matching, where continuous double auctions automatically
    pair compatible ask and bid orders, thereby saving manual effort and ensuring
    suitable trades; price discovery, as the ongoing interaction of agents leads to
    the emergence of market-clearing equilibrium prices that reflect fair valuation;
    allocation efficiency, where auction-clearing algorithms allocate goods to buyers
    willing to pay the highest price, promoting allocative efficiency; and transparency
    provided by centralized order books, offering insight into current prices and
    market depth, unlike in opaque bilateral negotiations. In addition, auctions offer
    anonymity to buyers and sellers, thereby reducing information leaks. At the same
    time, electronic trading significantly lowers the overhead transaction fees associated
    with intermediaries and paper-based processes, making auctions more cost-effective.
    Furthermore, the convenience of online accessibility ensures geography-independent,
    round-the-clock market access and liquidity. Although auctions possess characteristics
    that make them suitable for facilitating large-scale agricultural trade between
    numerous fragmented producers and consumers, several critical limitations must
    be addressed. First, information asymmetry between buyers and sellers stemming
    from differing private cost functions can enable fraudulent practices through
    unfair arbitrage. Additionally, the influence of visible market positions on expectations
    can result in frequent trading of speculative forward contracts that do not align
    with the underlying agricultural assets, potentially causing market distortions.
    Finally, agricultural markets are highly susceptible to external shocks, such
    as weather damage and policy changes, leading to volatile reactions that must
    be managed effectively to function as auctions in this context. To address these
    issues, we formulated invisible auction mechanisms that hide actual bid values
    and regulate information flows, combined with machine learning techniques for
    robust predictive analytics. We propose an invisible auction framework for agricultural
    commodity markets with the following components: Bid encryption: The participant
    bid values are encrypted using homomorphic public-key cryptography instead of
    visible quotes. (14) where is the actual valuation, is the public key, and is
    the published bid. Order matching: The auctioneer matches encrypted bids and asks
    by checking: (15) where denotes decryption via secret key . Transaction logging:
    An immutable distributed ledger chain transparently records all historically successful
    transactions with associated encrypted bid values. Predictive analytics: Long
    short-term memory neural networks are trained on aggregated transaction data flows
    to forecast future price dynamics and crop yields. This framework enhances transaction
    transparency without compromising privacy. Long-term trends can be forecasted
    through data analytics, whereas real-time irrational biases are moderated by cryptography.
    Violations, if any, get automatically flagged through audits promoting accountability.
    Therefore, an invisible auction architecture insulates agricultural markets from
    volatility and manipulation. Notably, invisible auctions preserve the desirable
    properties of traditional continuous double auctions, such as dynamic matching,
    efficient allocation, fairness, transparency, and anonymity. Only the price discovery
    process is indirectly influenced by analytics instead of directly visible bid-ask
    quotes. Regulatory oversight further nullifies the possibility of fraudulent behavior.
    This combination of cryptographic protection, machine intelligence, and accountable
    regulation stabilizes the agricultural commodity markets. Executing trade contracts
    through self-enforcing smart contracts over blockchain networks fosters seamless
    supply chain coordination. Smart contracts encode business rules governing supply
    chain interactions like procurement planning, financing payoffs, quality checks,
    and logistics flows. Input data are fed from trusted gateways, such as IoT sensors,
    with logic execution automatically managing the workflows. Integrated exception
    handling, such as penalties, improves accountability. Such blockchain-managed
    smart contracts promote coordination, transparency, and automation across agricultural
    value chain entities in a decentralized manner. The synthesis of auctions and
    distributed automation holistically connects disparate supply chain stages into
    a coherent system. Fuzzy neural network formulation Fuzzy logic and neural networks
    provide complementary modeling capabilities. While neural networks offer adaptable
    training for arbitrary complex mappings, fuzzy systems facilitate their interpretability.
    We formulate an integrated 5-layer architecture, as shown in Fig. 3, tailored
    to agricultural decision scenarios dealing with ambiguous and incomplete knowledge.
    Fig. 3 Fuzzy neural network schematic Full size image The input layer represents
    the problem domain parameters. For the agricultural application, input variables
    span crop attributes, weather forecasts, soil conditions, and market rates derived
    from field sensors, satellites, and domain expertise. Let vector denote the input
    variables. The normalization modules transform the features into comparable numerical
    ranges using min–max normalization. (16) This preprocessing enhances the training
    stability. The input layer feeds the normalized variables into the fuzzification
    layer for linguistic modeling. Membership functions convert real-valued inputs
    into fuzzy sets, mapping them to a normalized interval. Commonly adopted forms
    include triangular, trapezoidal, Gaussian, and bell curves with tunable parameters.
    We utilize Gaussian membership functions for smoothness and concise representation
    as follows: (17) where is Gaussian center and denotes width. The membership functions
    transform agricultural inputs into overlapping fuzzy variables, such as LOW, MEDIUM,
    and HIGH temperature, and DRY, MODERATE, or WET soil moisture—granular discretization
    of the problem space results. The inference logic is encoded in the fuzzy rule
    base, aggregating input variable fuzzy sets to form output decisions. Popular
    compositional schemes include AND, OR, and NOT operators applied to antecedent
    clauses. We used conjunctive fuzzy rules, with each clause joined by an AND. (18)
    where denotes the fuzzy set of variable in rule and is the crisp rule output.
    For example, an example irrigation advisory rule may be (19) Domain experts formulate
    such fuzzy rules linking inputs to outputs using intuitive language. Automated
    methods also assist rulebase generation from data. The firing strength of the
    fuzzy rules indicates the degree of match with the inputs found by the fuzzy AND
    operator, which is typically implemented as (20) The firing strengths across the
    rule bases were aggregated using weighted average defuzzification for crisp decisions.
    (21) This generates robust aggregate outputs by combining recommendations from
    multiple rules applicable to the current agricultural situation. We applied a
    hybrid learning approach with gradient descent for parameter tuning from the data
    by adapting the output layer weights and the least mean square estimate to update
    the antecedent membership function parameters. Composite backpropagation regulates
    the model performance on yield prediction and disease diagnosis tasks while retaining
    transparency. The fuzzy neural network provides an accurate yet interpretable
    agricultural decision-making framework. Fuzzy model training algorithms While
    rule-based fuzzy systems encode domain expertise in agricultural decision-making,
    adaptable training algorithms help optimize model parameters from the data. A
    two-phase hybrid learning approach is formulated. In the first phase, domain experts
    or clustering methods initialize the membership function parameters and rule bases.
    For example, the fuzzy variable MOISTURE can be defined as. (22) The membership
    functions translate the input moisture percentages into degrees of association
    with the fuzzified sets, LOW, MEDIUM, and HIGH. Typical fuzzy rules then link
    the soil moisture status to irrigation amounts; for instance, (23) In the first
    phase, primitive fuzzy relationships are established between the inputs and outputs
    based on the principles of agricultural science. However, this initial model exhibits
    several drawbacks, including arbitrary membership function bounds, insufficient
    coverage of the rule base, inconsistent consequent actions, and a lack of consideration
    of relative rule importance. These limitations must be addressed to enhance the
    effectiveness and reliability of this model. Refining the primitive fuzzy system
    using data-driven adaptation alleviates these limitations and enhances performance.
    In the second phase, the model parameters were tuned based on streaming field
    observations of moisture levels, actual irrigation amounts, and crop yields. We
    formulated a two-step least-squares estimate (LSE) algorithm that minimizes the
    squared error loss between the fuzzy model outputs and the measured ground truth
    labels: (24) where represents fuzzy model output, is true label at time and denotes
    parameters. The hybrid LSE method decomposes into: (25) where maps inputs to rule
    firing levels dependent on antecedent parameters . The function aggregates rule
    outputs based on consequent weights . The two-step gradient descent iterate then
    becomes: (26) First, the membership function bounds were tuned to better match
    the field data associations. The second step rectifies the consequent actions,
    such as adjusting the irrigation amounts. Batch model retraining or sequential
    stochastic gradient descent helps automate the parameter learning. Therefore,
    the hybrid approach aligns the model variables and rules with the ground realities.
    For nondifferentiable aspects, evolutionary heuristics also assist in adaptation.
    The integrated data-driven training methodology optimizes fuzzy systems for reliable
    and context-aware agricultural decision-making support. Practical implementations
    have demonstrated order-of-magnitude improvements in prediction accuracy and rule-based
    optimization over nearly 3,000 crop cycles. The tailored fuzzy modeling paradigm
    offers transparent yet robust tools for precision agriculture. Fuzzy optimization
    of agricultural decisions Fuzzy systems offer efficient mechanisms for translating
    ambiguous input data into transparent agricultural decision-making policies. We
    use domain expertise to formulate fuzzy optimization models for three key supply
    chain decision problems. Precision agriculture requires the optimal dynamic allocation
    of resources such as water, fertilizers, and pesticides based on crop stages,
    weather patterns, and soil conditions. We encode this as a multi-objective optimization
    problem. (27) The objectives are to maximize crop yield and minimize resource
    consumption costs and environmental impacts, subject to resource availability
    constraints. We designed a Mamdani-type fuzzy inference system with a rulebase:
    (28) where linguistic variables, such as LOW and MEDIUM, model soil moisture and
    resource application levels, respectively. Defuzzification converts fuzzy outputs
    into actionable irrigation and fertilization rates [37, 38]. Common strategies
    include the centroid, mean-max, and maximum criteria. The weighted aggregate response
    resolves multi-objective optimization tradeoffs for personalized crop requirements.
    Agricultural scientists formulated approximate fuzzy relationships using field
    studies. Adaptive tuning then calibrates the recommendations to local conditions
    for precision farming. Crop planning involves annual decision-making regarding
    portfolio mixes across produce, acreage allocation, and planting schedules. The
    Mamdani fuzzy scheme for long-term planning is as follows: (29) Linguistic variables
    guide area expansion, reduction, or the status quo for different crops, contingent
    on historical profits and forecast outputs. Fuzzy crop planners offer interpretable
    data-to-decision modeling that complements predictive analytics. Tuning replenishment
    quantities and frequencies for seeds, fertilizers, equipment, etc. minimizes warehousing
    costs. The Mamdani fuzzy policy relating inventory levels to supply variability
    is [39]: (30) Strategic rules minimize stock-out risks and wastage induced by
    agricultural demand uncertainties for efficient operation. Fuzzy inventory controllers
    allow the embedding of domain insights and adaptive calibration. Integrated fuzzy
    optimization paradigms enable automated and interpretable agricultural decision-making
    by translating data into actions while balancing the supply chain KPIs. Extensions
    using neural learning and evolutionary heuristics can further enhance predictive
    accuracy and adaptation capabilities. Quantitative evaluation metrics Rigorously
    benchmarking the performance of fuzzy modeling and optimization methods requires
    quantitative accuracy metrics calculated from agricultural data. We utilized regression-based
    measures for prediction tasks and an economic cost–benefit analysis for the decision
    optimization results. Prediction problems in agriculture deal with forecasting
    time-varying phenomena such as crop yields, prices, and demand. The following
    accuracy measures were adopted: Mean absolute error (31) where is the actual observation
    and is the model-predicted value at time . Root mean-squared error (32) Mean absolute-percentage
    error (33) Coefficient of determination (34) Lower MAE, RMSE, and MAPE values,
    along with higher values, indicate superior predictive accuracy. Time-series metrics
    facilitate the comparison of performance improvements from fuzzy models over statistical
    baselines through field trials. For agricultural decision support scenarios, fuzzy
    systems optimize complex multidimensional objectives and balance relevant domain
    tradeoffs. Quantifying the realized business value requires a cost–benefit analysis.
    Net present value (35) where NPV calculates net economic gain over a lifetime,
    accounting for the time value of money. Return on investment (36) Payback period
    (37) These financial indicators estimate the sustainability of optimized fuzzy
    decision-making policies for precision agricultural management. Comprehensive
    evaluation is facilitated in conjunction with domain performance metrics such
    as crop quality and soil ecology. Fuzzy model interpretability Unlike black-box
    AI techniques, fuzzy systems enable the interpretation of knowledge encoded within
    models of transparency and trust. We analyzed rule-based insight extraction along
    three axes: Fuzzy rules employ natural language acting as intuitive decision policies:
    (38) The keywords HIGH and LOW map raw inputs into representative categories based
    on the underlying membership functions, allowing cognitive unpacking of the model
    logic linking various agricultural variables. Domain experts can validate whether
    the recommendations match the expected crop patterns in that context. This contrasts
    with the inscrutable weights in deep neural networks. Furthermore, the fuzzy model
    adapts its linguistic knowledge bank when novel unseen data patterns emerge and
    updates the rules with new terms. Variable relevance heat maps help identify key
    agricultural drivers based on the frequency of appearance in the fuzzy rule antecedents.
    (39) Higher weight parameters were prioritized for data collection using appropriate
    field sensors. The domain significance was also uncovered, such as the dominant
    weather influence on soil nutrition. Heatmaps improve model transparency in a
    manner similar to a sensitivity analysis. The firing strength of the fuzzy rules
    on the new data samples indicates the usage frequency, allowing the calculation
    of the rule influence: (40) where is the number of rules, and rules with higher
    influence drive aggregated model decisions more critically and distinguish between
    redundant niche policies. Such analysis enhances user trust and model debugging.
    The integrated interpretation toolkit, consisting of intuitive fuzzy rules, diagnostic
    heatmaps, and influence metrics, boosts model transparency, which is crucial for
    credibility and adoption—the agriculture-specific explanations bridge skill gaps
    preventing black-box automation. Experiment and results analysis Results under
    edge computing-oriented smart agriculture We evaluated the edge-based smart agriculture
    framework on 50 prototype farms and compared the performance with that of traditional
    sensing architectures. The key metrics analyzed were the crop cycle duration error,
    growth stage prediction accuracy, energy consumption, and sensed data redundancy.
    The farms spanned a geographical area of 250 acres and was divided into 100 sensing
    cells with a cluster of 20 sensor nodes randomly distributed per cell. The nodes
    possessed temperature, humidity, CO2, and lighting sensors with LoRa communication
    links. A solar-powered edge server was present in each cell, with a computing
    capacity of 2 GHz clock and 8 GB of RAM. The edge nodes also had a cellular 4G
    hookup for cloud analytics. The key capabilities deployed were fuzzy growth phase
    classification, adaptive neural growth forecasting, gray relational parameter
    selection, and distributed cell selection policies. Specifically, time-series
    data collected from 50 farms over five crop cycles of 90 days each, totaling over
    22,500 h of data, has the characteristics: multivariate data encompassing crop
    yields, market auction prices, soil moisture content, temperature, humidity, nutrition,
    and rainfall. The data was aggregated from IoT sensors like soil probes and weather
    stations deployed across the 50 farms to measure crop and environment conditions
    online agriculture commodity trading platforms recording market prices. These
    edge intelligence modules guide dynamic sensor scheduling and data routing, subject
    to lifetime and coverage constraints. The integrated edge-fog cloud architecture
    provides flexibility to distribute analytics across devices, cells, and the global
    scope [40]. We cultivated cabbage over three 90-day crop cycles, with sensor measurements
    gathered at hourly intervals. Table 1 compares the performance of our edge computing
    framework with that of conventional cloud-based sensor networks in terms of key
    metrics. Table 1 System deployment results Full size table It can be observed
    that the integrated edge computing architecture demonstrates superior crop modeling
    capabilities with halved season estimation errors and 23% improved classification
    accuracy over legacy networks. Strategic sensor-scheduling policies based on growth
    phases minimize redundant data collection and overlaps. Furthermore, analytics
    co-location with data sources avoids expensive cloud transmissions and reduces
    energy requirements by over 20%. Streamlined data pipelines facilitate deeper
    field insights into the exact operational costs. Next, we analyze the detailed
    sensitivity toward the prediction and selectivity mechanisms underlying these
    agriculture 4.0 productivity gains. The results are shown in Table 2. Table 2
    Cabbage growth phase accuracy Full size table Fuzziness captures intermediately
    transitioning states better than rigid discrete models. Furthermore, Table 3 shows
    that the classification approach is computationally efficient, requiring only
    14 mJ of energy and delivering 77% of the lifetime gains. Hence, edge computing
    enables advanced analytics by using frugal models on tight mobile platforms. Table
    3 Energy consumption comparison Full size table The context-aware parameter selection
    scheme dynamically detects relevant attributes over the cabbage crop cycle using
    gray relational analysis, with the Pearson coefficient as a similarity metric.
    Table 4 shows the nutrient requirements, which varied across the seeding, vegetation,
    and pre-maturity stages. Our model automatically activated the corresponding sensors,
    MOISTURE during growth and NPK during flowering. Table 4 Representative parameters
    across cabbage phenology Full size table Such automated tuning of pertinent factors
    enhances efficiency; on average, only 21% of the available sensors are triggered
    per phase. Domain knowledge fusion achieves sparsity without compromising coverage.
    Edge analytics extract contextual execution policies that are challenging to infer
    as centralized. The decentralized sensor coordination protocol dynamically partitions
    cells into active sensing zones and candidate regions iteratively minimizing the
    (41) The distance metric ensures that dispersed sensors are selected, thereby
    capturing wider samples. Furthermore, the effective coverage area is (42) Thresholds
    prevent overlap. Unlike centralized controllers, distributed policies respond
    faster to local moisture fluctuations. Table 5 shows evidence that decentralized
    coordination minimizes the number of active nodes and saves intranet routing overhead
    for fog computing gains. Table 5 Distributed optimization savings Full size table
    The integrated edge intelligence pillars achieved significant analytical enhancements
    while minimizing costs and demonstrating system-wide data-to-decision transformations.
    Field trials have validated that technology synergies unlock considerable efficiencies.
    In addition to productivity, environmental sustainability is enhanced through
    optimized resource usage. Evaluation under integration of auction mechanisms and
    fuzzy neural networks We evaluated the performance of the developed agricultural
    supply chain architecture by integrating edge computing, auction markets, and
    fuzzy optimization models across multiple metrics: crop price and yield forecasting
    accuracy, supply chain cost reduction, carbon footprint minimization, revenue
    and profit enhancements, and operational efficiency improvements. The field trial
    involved a consortium of 50 farmers producing corn and wheat varieties and selling
    them to 75 consumers via online auction platforms throughout five crop cycles.
    Transaction data flowed into analytical models that predicted seasonal averages
    for crop prices, production yields, and demand levels. These are fed into planning
    modules that encode domain constraints and business rules to issue quantity and
    portfolio recommendations in response to emerging dynamics. For prediction accuracy,
    the proposed edge computing-oriented auction-based fuzzy neural network (EC-aFNN)
    should be compared with the four benchmarks: (i) BMAE-Net [41]: data-driven weather
    prediction network, (ii) Bayesian neural network (BNN) [42]: corn yield prediction
    based on remotely sensed variables, (iii) random forest regression (RFR) [43]:
    yield and quality prediction of winter rapeseed (iv) dingo-optimized sand piper
    (DOSP) [44]: automatic crop yield prediction framework designed with two-stage
    classifiers, as shown in Fig. 4. Fig. 4 Forecasting performance comparison Full
    size image Superior accuracy metrics directly and positively affect various aspects
    of smart agricultural supply chain operations. Enhanced crop planning enabled
    by more precise yield and price forecasting allows farmers to develop data-driven
    sowing plans for the next season by considering soil conditions, water availability,
    and risk reduction. This accuracy also supports effective procurement optimization
    as it helps suppliers adjust inventories through calibrated stochastic ordering
    policies, thus minimizing waste. Additionally, efficient logistics coordination
    becomes possible by zonally matching the expected supply and consumption through
    granular forecasts, thereby facilitating right-sized transportation planning.
    Moreover, the stability of market dynamics improved significantly. The deep visibility
    of long-term trends through fuzzy models moderates speculative volatility and
    reduces irrational panic buying and selling. Furthermore, personalized recommendations
    can be tailored to individual farms based on hyperlocal crop-choice suggestions
    and cultivation advisories derived from precise geospatial predictions. Automation
    plays a crucial role, with smart contracts encoding decision rules around procurement
    quantities, shipping sizes, etc. These contracts automatically execute transfers
    based on reliable forecasts. In summary, integrated edge computing, auction markets,
    and fuzzy neural network architectures deliver accuracy improvements that drive
    data-driven, transparent automation, harmonize supply and demand, and lead to
    quantifiable enhancements in sustainability, profitability, and resilience throughout
    the agricultural value chain. Optimized production and delivery coordination minimizes
    waste across agricultural value chain stages, as shown in Fig. 5. Total food loss
    was reduced by 29%, thus lowering operational costs. Fig. 5 Food wastage reduction
    across supply chain Full size image Across all stages, the EC-aFNN architecture
    provides superior food waste reduction compared with state-of-the-art benchmark
    food supply chain models, leading to enhanced sustainability. Supply chain transparency
    and coordination eliminate excess resource usage, as shown in Fig. 6. Fig. 6 Agricultural
    sustainability enhancements Full size image It can be observed that the integrated
    edge computing, auction markets, and fuzzy optimization framework provide 31–55%
    superior sustainability improvements along with energy, water, fertilizer, and
    pesticide reduction over the BNN, which highlights the strengths of our approach.
    Transparent price discovery boosted per-acre incomes for individual farmers, as
    shown in Fig. 7. Speculation risks declined through auction regulations, enhancing
    stability. Fig. 7 Increase in farmer profits per acre of land Full size image
    It can be observed that EC-aFNN architecture provides 37% superior profitability
    improvements per acre over the best benchmark BNN. Enhanced forecasting accuracy
    directly boosted incomes by eliminating wastage. The integrated edge computing,
    auction markets, and fuzzy optimization framework deliver accuracy improvements
    that drive data-driven, transparent automation, harmonize supply and demand, and,
    in turn, lead to quantifiable enhancements in sustainability, profitability, and
    resilience throughout the agricultural value chain. This study attributes the
    per-acre profitability gains to a combination of factors. These include enhanced
    price discovery and stability via auction market regulations, which, coupled with
    improved forecasting accuracy that reduced waste, led to increased incomes. Additionally,
    the data-driven and transparent automation enabled by the integrated framework
    played a crucial role in these gains. Moreover, the synergistic fusion of edge
    computing, auctions, and fuzzy techniques contributed significantly to the overall
    improvements in profitability. Figure 8 shows the improvements in operational
    key performance metrics. Fig. 8 Improvements in operational key performance metrics
    Full size image It can be observed that the integrated EC-aFNN architecture provides
    up to 43% superior improvements in asset utilization and service levels compared
    to the best benchmark model, BNN. Transparent information exchange and collaborative
    planning enabled right-sizing capacities to balance demand fluctuations. Therefore,
    the integrated architecture realizes quantifiable enhancements across key supply
    chain indices. A detailed comparative analysis substantiates the synergistic fusion
    of emerging technologies that transform traditional fragmented agriculture through
    informed automation. Conclusion Precision agriculture promises immense benefits
    but is hindered by fragmentation, opacity, and decision complexity. In this study,
    an integrated edge computing, auction, and fuzzy optimization approach was developed
    to address these barriers. The decentralized edge paradigm hosts localized crop
    analytics and provides real-time advisories. Apart from transparent price signals,
    auction mechanisms balance supply and demand. Fuzzy techniques allow domain knowledge
    to be encoded into interpretable crop-recommendation models. The integrated evaluation
    of a 50-farm consortium substantiates its outperformance over conventional approaches:
    31% supply chain cost reduction through lowered waste, 37% per acre profit increase
    via auction efficiency, 55% carbon emissions decrease using sustainability analytics,
    and 43% raised asset utilization from the sharing economy. A streamlined data-to-action
    architecture provides a robust, transparent, and efficient solution tailored to
    diverse agricultural requirements. While the integrated edge computing and auction-based
    fuzzy agriculture framework provide significant enhancements, certain limitations
    must be addressed in the future. Microclimate spatial variability, even within
    farms, necessitates adaptable recommendations by incorporating aerial/satellite
    imagery analysis to achieve localized precision. Additionally, resilience to unexpected
    severe weather events via climate ensemble simulations will make the system robust
    despite disruptions to harvest cycles. Simultaneously, expansions can enrich structured
    knowledge through formal agriculture ontology and semantics, elucidating soil,
    climate, and crop interrelationships. Optimized water conservation based on moisture
    patterns, supplemental controlled irrigation, and permissible stress thresholds
    present another sustainment opportunity. Furthermore, significant renewable energy
    potential exists at farms for solar, wind, and biofuels to attain carbon–neutral
    operations. Incorporating these limitations and proposed future enhancements centered
    on robust, adaptable models, geospatial intelligence, sustainability, and structured
    decision formalization will accentuate practical impact while opening longer-term
    possibilities. Availability of data and materials No datasets were generated or
    analysed during the current study. References Sawkar RH, Hiregoudar LG, Bharadwaj
    S (2020) Aquaponics: a modern agriculture technology to overcome water scarcity
    and drought. J Geol Soc India 95:108–109 Article   Google Scholar   Utamima A,
    Reiners T, Ansaripoor AH (2022) Evolutionary neighborhood discovery algorithm
    for agricultural routing planning in multiple fields. Ann Oper Res 316:955–977
    Article   Google Scholar   Nyam YS, Kotir JH, Jordaan AJ et al (2021) Developing
    a conceptual model for sustainable water resource management and agricultural
    development: the case of the Breede River Catchment Area, South Africa. Environ
    Manage 67:632–647 Article   CAS   PubMed   Google Scholar   Ding Y, Sun C (2022)
    Does agricultural insurance promote primary industry production? Evidence from
    a quasi-experiment in China, Geneva. Pap Risk Insur Issues Pract 47:434–459 Google
    Scholar   Misara R, Verma D, Mishra N et al (2022) Twenty-two years of precision
    agriculture: a bibliometric review. Precision Agric 23:2135–2158 Article   Google
    Scholar   Nowak B (2021) Precision agriculture: where do we stand? A review of
    the adoption of precision agriculture technologies on field crops farms in developed
    countries. Agric Res 10:515–522 Article   Google Scholar   Duncan E, Glaros A,
    Ross DZ et al (2021) New but for whom? Discourses of innovation in precision agriculture.
    Agric Hum Values 38:1181–1199 Article   Google Scholar   Shaikh TA, Mir WA, Rasool
    T et al (2022) Machine learning for smart agriculture and precision farming: towards
    making the fields talk. Arch Computat Methods Eng 29:4557–4597 Article   Google
    Scholar   Lu HL, Chang YH, Wu BY (2020) The compare organic farm and conventional
    farm to improve sustainable agriculture, ecosystems, and environment. Org Agr
    10:409–418 Article   Google Scholar   Qi LY, Liu YW, Zhang YL et al (2022) Privacy-aware
    point-of-interest category recommendation in internet of things. IEEE Internet
    Things J 9:21398–31408 Article   Google Scholar   Liu YW, Zhou XK, Kou HZ et al.
    Privacy-preserving point-of-interest recommendation based on simplified graph
    convolutional network for geological traveling. ACM Trans Intell Syst Technol.
    2023. Hsu CH, Lin HH, Jhang SW et al (2021) Does environmental engineering help
    rural industry development? Discussion on the impact of Taiwan’s “special act
    for forward-looking infrastructure” on rural industry development. Environ Sci
    Pollut Res 28:40137–40150 Article   Google Scholar   Pearsall H, Gutierrez-Velez
    VH, Gilbert MR et al (2021) Advancing equitable health and well-being across urban–rural
    sustainable infrastructure systems. npj Urban Sustain. 1:26 Article   Google Scholar   Kumar
    CS, Anand RV (2023) A review of energy-efficient secured routing algorithm for
    IoT-Enabled smart agricultural systems. J Biosyst Eng 48:339–354 Article   Google
    Scholar   Shi H, Li Q (2022) Edge computing and the internet of things on agricultural
    green productivity. J Supercomput 78:14448–14470 Article   Google Scholar   Mittelmann
    M, Bouveret S, Perrussel L (2022) Representing and reasoning about auctions. Auton
    Agent Multi-Agent Syst 36:20 Article   Google Scholar   Zhang K, Hao WN, Yu XH
    et al (2023) Research on a kind of multi-objective evolutionary fuzzy system with
    a flowing data pool and a rule pool for interpreting neural networks. Int J Fuzzy
    Syst 25:575–600 Article   Google Scholar   Oteyo IN, Marra M, Kimani S et al (2021)
    A survey on mobile applications for smart agriculture. SN Comput Sci 2:293 Article   Google
    Scholar   Wang X, Ni D (2023) Internet based rural economic entrepreneurship based
    on mobile edge computing and resource allocation. Soft Comput. https://doi.org/10.1007/s00500-023-08620-z
    Article   PubMed   PubMed Central   Google Scholar   Zhang YA, Sun Z, Zhang C
    et al (2021) Body weight estimation of yak based on cloud edge computing. J Wireless
    Com Netw 2021:6 Article   Google Scholar   Gupta N, Khosravy M, Patel N et al
    (2020) Economic data analytic AI technique on IoT edge devices for health monitoring
    of agriculture machines. Appl Intell 50:3990–4016 Article   Google Scholar   Li
    C, Sha Z, Sun T (2023) Rural households’ internet use on common prosperity: evidence
    from the Chinese social survey. Soc Indic Res 170:797–823 Article   Google Scholar   Liu
    P (2021) Balancing cost effectiveness and incentive properties in conservation
    auctions: experimental evidence from three multi-award reverse auction mechanisms.
    Environ Resource Econ 78:417–451 Article   Google Scholar   Feng Y, Mei D, Zhao
    H (2023) Auction-based deep learning-driven smart agricultural supply chain mechanism.
    Appl Soft Comput 149:111009 Article   Google Scholar   Chiu LJV, Taure LW, Groh
    YT (2022) Pricing efficiency in livestock auction markets: a two-tier frontier
    approach. Agric Econ 53:139–151 Article   Google Scholar   Seifert S, Huettel
    S (2023) Is there a risk of a winner’s curse in farmland auctions? Eur Rev Agric
    Econ 50:1140–1177 Article   Google Scholar   Remya S (2022) An adaptive neuro-fuzzy
    inference system to monitor and manage the soil quality to improve sustainable
    farming in agriculture. Soft Comput 26:13119–13132 Article   Google Scholar   Acharjya
    DP, Rathi R (2022) An integrated fuzzy rough set and real coded genetic algorithm
    approach for crop identification in smart agriculture. Multimed Tools Appl 81:35117–35142
    Article   Google Scholar   MohebbiTafreshi G, Nakhaei M, Lak R (2020) A GIS-based
    comparative study of hybrid fuzzy-gene expression programming and hybrid fuzzy-artificial
    neural network for land subsidence susceptibility modeling. Stoch Environ Res
    Risk Assess 34:1059–1087 Article   Google Scholar   Kaya NS, Pacci S, DemiragTuran
    I et al (2023) Comparing geographic information systems-based fuzzy-analytic hierarchical
    process approach and artificial neural network to characterize soil erosion risk
    indexes. Rend Fis Acc Lincei 34:1089–1104 Article   Google Scholar   Remya S,
    Sasikala R (2020) Performance evaluation of optimized and adaptive neuro fuzzy
    inference system for predictive modeling in agriculture. Comput Electr Eng 86:106718
    Article   Google Scholar   Ramana K, Aluvala R, Kumar MR et al (2022) Leaf disease
    classification in smart agriculture using deep neural network architecture and
    IoT. J Circuits Syst Comput 31:2240004 Article   Google Scholar   Bhojani SH,
    Bhatt N (2020) Wheat crop yield prediction using new activation functions in neural
    network. Neural Comput Appl 32:13941–13951 Article   Google Scholar   Zhang L,
    Huang ZY, Liu W et al (2021) Weather radar echo prediction method based on convolution
    neural network and Long Short-Term memory networks for sustainable e-agriculture.
    J Clean Prod 298:126776 Article   Google Scholar   Wang J (2022) Analysis of wireless
    communication networks under edge computing scenarios. Wireless Netw 28:3665–3676
    Article   Google Scholar   Dhillon SK, Madhu C, Kaur D et al (2020) A review on
    precision agriculture using wireless sensor networks incorporating energy forecast
    techniques. Wireless Pers Commun 113:2569–2585 Article   Google Scholar   Shepelev
    GI (2022) Effects of Defuzzification methods on the results of comparing fuzzy
    alternatives. Sci Tech Inf Proc 49:364–370 Article   Google Scholar   Vassiliev
    AE, Vegner AV, Golubeva DE et al (2023) Increasing the quality indicators of the
    functioning of fuzzy solvers at the Defuzzification stage. J Commun Technol Electron
    68:810–818 Article   Google Scholar   Wang G, Wang H, Long Z (2021) Norm approximation
    of mamdani fuzzy system to a class of integrable functions. Int J Fuzzy Syst 23:833–848
    Article   Google Scholar   Oprea SV, Bâra A (2023) An Edge-Fog-Cloud computing
    architecture for IoT and smart metering data. Peer-to-Peer Netw Appl 16:818–845
    Article   Google Scholar   Kong JL, Fan XM, Jin XB et al (2023) BMAE-Net: a data-driven
    weather prediction network for smart agriculture. Agronomy-Basel 13:625 Article   Google
    Scholar   Ma YC, Zhang Z, Kang YH et al (2021) Corn yield prediction and uncertainty
    analysis based on remotely sensed variables using a Bayesian neural network approach.
    Remote Sens Environ 259:112408 Article   Google Scholar   Rajkovic D, Jeromela
    AM, Pezo L et al (2022) Yield and quality prediction of winter rapeseed-artificial
    neural network and random forest models. Agronomy-Basel 12:58 Article   Google
    Scholar   Kolipaka VRR, Namburu A. An automatic crop yield prediction framework
    designed with two-stage classifiers: a meta-heuristic approach. Multimed Tools
    Appl. 2023. Download references Funding This paper was supported by General Projects
    of Zhengzhou Soft Science Research Program in 2023 (Granted No. 8), Annual Program
    of Philosophy and Social Science Planning of Henan Province (Grant No. 2023CJJ113),
    and Chongqing Transportation Science and Technology Project (Granted No.s CQJT-2023CZ28-1
    and CQJT-2023CZ16-1). Author information Authors and Affiliations School of Economics
    and Business Administration, Chongqing University, Chongqing, 400044, China Qing
    He, Hua Zhao & Yu Feng School of Economic and Management, Chongqing Normal University,
    Chongqing, 401331, China Qing He & Zhaofeng Ning School of Social Development,
    East China Normal University, Shanghai, 200241, China Zehao Wang School of Management,
    Lanzhou University, Lanzhou, 73000, China Tingwei Luo Contributions Q.H. contributed
    to conception and writing; H.Z. and Y.F. contributed to methodology; Z.W. contributed
    to data analysis; Z.N. contributed to software; T.L. contributed to polishing.
    Corresponding author Correspondence to Yu Feng. Ethics declarations Ethics approval
    and consent to participate This declaration is “not applicable”. Competing interests
    The authors declare no competing interests. Additional information Publisher’s
    Note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article''s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article''s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article He, Q., Zhao, H.,
    Feng, Y. et al. Edge computing-oriented smart agricultural supply chain mechanism
    with auction and fuzzy neural networks. J Cloud Comp 13, 66 (2024). https://doi.org/10.1186/s13677-024-00626-8
    Download citation Received 08 January 2024 Accepted 07 March 2024 Published 21
    March 2024 DOI https://doi.org/10.1186/s13677-024-00626-8 Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Edge computing Smart agricultural supply chain Auction Fuzzy neural networks Download
    PDF Collection Mobile Edge Computing Meets AI Sections Figures References Abstract
    Introduction Related work Edge computing-oriented smart agriculture Integration
    of auction mechanisms and fuzzy neural networks Conclusion Availability of data
    and materials References Funding Author information Ethics declarations Additional
    information Rights and permissions About this article Advertisement Support and
    Contact Jobs Language editing for authors Scientific editing for authors Leave
    feedback Terms and conditions Privacy statement Accessibility Cookies Follow SpringerOpen
    By using this website, you agree to our Terms and Conditions, Your US state privacy
    rights, Privacy statement and Cookies policy. Your privacy choices/Manage cookies
    we use in the preference centre. © 2024 BioMed Central Ltd unless otherwise stated.
    Part of Springer Nature."'
  inline_citation: '>'
  journal: Journal of Cloud Computing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Edge computing-oriented smart agricultural supply chain mechanism with auction
    and fuzzy neural networks
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Papaiz F.
  - Dourado M.E.T.
  - de Medeiros Valentim R.A.
  - Pinto R.
  - de Morais A.H.F.
  - Arrais J.P.
  citation_count: '0'
  description: Prognosticating Amyotrophic Lateral Sclerosis (ALS) presents a formidable
    challenge due to patients exhibiting different onset sites, progression rates,
    and survival times. In this study, we have developed and evaluated Machine Learning
    (ML) algorithms that integrate Ensemble and Imbalance Learning techniques to classify
    patients into Short and Non-Short survival groups based on data collected during
    diagnosis. We aimed to identify individuals at high risk of mortality within 24
    months of symptom onset through analysis of patient data commonly encountered
    in daily clinical practice. Our Ensemble-Imbalance approach underwent evaluation
    employing six ML algorithms as base classifiers. Remarkably, our results outperformed
    those of individual algorithms, achieving a Balanced Accuracy of 88% and a Sensitivity
    of 96%. Additionally, we used the Shapley Additive Explanations framework to elucidate
    the decision-making process of the top-performing model, pinpointing the most
    important features and their correlations with the target prediction. Furthermore,
    we presented helpful tools to visualize and compare patient similarities, offering
    valuable insights. Confirming the obtained results, our approach could aid physicians
    in devising personalized treatment plans at the time of diagnosis or serve as
    an inclusion/exclusion criterion in clinical trials.
  doi: 10.1186/s12911-024-02484-5
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Explore
    journals Get published About BMC Login BMC Medical Informatics and Decision Making
    Home About Articles Submission Guidelines Collections Join The Board Submit manuscript
    Research Open access Published: 19 March 2024 Ensemble-imbalance-based classification
    for amyotrophic lateral sclerosis prognostic prediction: identifying short-survival
    patients at diagnosis Fabiano Papaiz, Mario Emílio Teixeira Dourado Jr., Ricardo
    Alexsandro de Medeiros Valentim, Rafael Pinto, Antônio Higor Freire de Morais
    & Joel Perdiz Arrais  BMC Medical Informatics and Decision Making  24, Article
    number: 80 (2024) Cite this article 293 Accesses 1 Altmetric Metrics Abstract
    Prognosticating Amyotrophic Lateral Sclerosis (ALS) presents a formidable challenge
    due to patients exhibiting different onset sites, progression rates, and survival
    times. In this study, we have developed and evaluated Machine Learning (ML) algorithms
    that integrate Ensemble and Imbalance Learning techniques to classify patients
    into Short and Non-Short survival groups based on data collected during diagnosis.
    We aimed to identify individuals at high risk of mortality within 24 months of
    symptom onset through analysis of patient data commonly encountered in daily clinical
    practice. Our Ensemble-Imbalance approach underwent evaluation employing six ML
    algorithms as base classifiers. Remarkably, our results outperformed those of
    individual algorithms, achieving a Balanced Accuracy of 88% and a Sensitivity
    of 96%. Additionally, we used the Shapley Additive Explanations framework to elucidate
    the decision-making process of the top-performing model, pinpointing the most
    important features and their correlations with the target prediction. Furthermore,
    we presented helpful tools to visualize and compare patient similarities, offering
    valuable insights. Confirming the obtained results, our approach could aid physicians
    in devising personalized treatment plans at the time of diagnosis or serve as
    an inclusion/exclusion criterion in clinical trials. Peer Review reports Introduction
    Amyotrophic Lateral Sclerosis (ALS) is a rare, incurable, and progressive neurodegenerative
    disease that impacts the human motor system. The communication between the brain
    and muscles gradually deteriorates, ultimately resulting in paralysis and death.
    While its etiology remains unknown, it typically afflicts individuals aged between
    40 and 70, affecting both men and women. ALS exhibits significant clinical heterogeneity,
    manifesting diverse symptoms and disease progression patterns among patients [1,
    2]. The average life expectancy post-symptom onset ranges from 2 to 5 years, with
    a global annual incidence of approximately 1.9 cases per 100,000 individuals [3].
    Given the complexity of ALS and its variable clinical presentation, accurately
    predicting outcomes such as survival time and disease progression rate poses a
    substantial challenge for physicians. Therefore, it is imperative to conduct research
    aiming to develop and validate prognostic models to achieve more precise predictive
    results. Machine Learning (ML) has emerged as a powerful tool in improving disease
    diagnosis and prognosis. In the context of ALS, recent studies have explored various
    ML approaches for diverse predictive tasks [4,5,6,7]. ML algorithms can extract
    information from training data, convert it to knowledge, and apply it to solve
    various types of problems, such as classification, regression, and clustering
    [8]. Learning from complex domains (e.g., ALS disease) is challenging, and ML
    techniques like Ensemble Learning can help improve predictive performance. Ensemble
    Learning combines single predictive models to build a more complex one, aiming
    to surpass the performance of each constituent model separately [9]. Additionally,
    medical data analysis often involves dealing with imbalanced datasets. This issue
    arises when there is a significant imbalance in the number of samples between
    different classes, resulting in a substantially lower representation of one class
    compared to the others. Typically, the target prediction is linked to samples
    from the minority class, as in the case of detecting patients with lung cancer
    through the analysis of tomography images, where there are significantly more
    images of healthy patients (the majority class) than those depicting lung cancer
    cases (the minority class). In such scenarios, ML models frequently exhibit bias
    towards samples belonging to the majority class, resulting in an elevated misclassification
    rate within the minority class [10]. To mitigate the issue of imbalance, resampling
    techniques such as Undersampling and Oversampling can be employed [11]. Efforts
    in ALS prognosis using ML should be directed toward the development of Clinical
    Decision Support (CDS) systems. CDS systems are computer programs designed to
    assist healthcare professionals in making more informed and timely decisions by
    integrating current patient data with historical information from other patients,
    facilitating data-driven decision making [12]. We deemed it essential to develop
    CDS systems that are feasible for use on a large scale in primary care, taking
    into account financial limitations. One approach to achieving this goal is to
    select biological markers (biomarkers) commonly used in routine ALS clinical practice.
    Such biomarkers may include clinical evaluations, assessment of functional capabilities,
    and respiratory function measurements. These biomarkers are often derived from
    less expensive and complex procedures, making them more accessible. Notably, some
    ML algorithms present results that humans cannot easily understand, decreasing
    their interpretability (e.g., Artificial Neural Networks or Support Vector Machines).
    Interpretability, in this context, refers to the comprehensibility of the decisions
    made by the ML algorithm [13]. Addressing this issue is crucial to ensure the
    acceptance of CDS systems in clinical practice, as physicians require explanations
    for patient classifications. Hence, the development of CDS systems must prioritize
    interpretability concerns. Existing frameworks can be explored to elucidate the
    predictions generated by ML models, one of which is the Shapley Additive Explanations
    (SHAP) framework [14]. SHAP employs a game-theoretic approach to clarify the prediction
    for a specific instance by quantifying the contribution (SHAP value) of each feature
    to the classification process. Consequently, SHAP values provide insights into
    the influence of individual features on the final prediction and their relative
    significance when compared to other features. Related work Van der Burgh et al.
    [4] demonstrated the positive impact of using Magnetic Resonance Images (MRI)
    and clinical information to classify ALS patients into survival groups (Short,
    Medium, and Long). They developed Deep Neural Networks models and obtained an
    accuracy of 84%. This study presented a high risk of model overfitting due to
    the reduced number of samples analyzed (n = 135). Kueffner et al. [15] presented
    a crowdsourcing challenge involving more than 30 teams. One of the target predictions
    was the probability of survival at 12-, 18-, and 24-months using patient data
    from the first three months of records. A team using a Gaussian Process Regression
    model obtained the best performance compared to the others (Z-score ≈ 12). These
    studies used different performance metrics, and thus, it was not possible to directly
    compare their performance with this study. Grollemund et al. [16] presented a
    model based on Dimensionality Reduction to predict one-year survival probabilities.
    They used the Uniform Manifold Approximation and Projection (UMAP) algorithm to
    reduce the data. The resultant 2D projection was divided into three areas to classify
    the patients into Low, Intermediate, and High probabilities groups. The proposed
    classifier obtained superior performance (F1 score: 96%, Balanced Accuracy: 91%)
    than the Random Forest and Logistic Regression models. However, the total comprehension
    of the relationship between input and output variables cannot be obtained because
    the adopted model is considered a black box approach, which degrades its interpretability.
    In this study, we achieved a slightly lower Balanced Accuracy (88%) and evaluated
    models that were also considered black boxes. Differently, we delivered global
    and local explanations regarding the prediction mechanisms, including feature
    importance analysis and their correlations with the target variable. Tavazzi et
    al. [17] presented a strategy based on a mutual information-weighted k-NN algorithm
    to handle missing values in clinical register datasets. One target was to classify
    patients into Short and Long survival groups. The authors evaluated Naïve Bayes
    classifiers built on the dataset using the proposed imputation method and achieved
    a superior performance (AUC: 82%) compared to classifiers using other imputation
    approaches. We achieved a superior AUC (93%) in this study, which suggests a greater
    effectiveness of our approach. Our contribution In this study, we have delved
    into the utilization of Ensemble and Imbalance Learning techniques to enhance
    the prediction accuracy for ALS patients with short survival expectancy. Our primary
    aim was to classify patients into Short and Non-Short survival groups based on
    data collected at the time of diagnosis. The Short survival group comprises individuals
    who die within 24 months from the onset of symptoms, indicating a rapid disease
    progression rate. This 24-month threshold was chosen based on the typical life
    expectancy of ALS patients, which ranges from 2 to 5 years. Hence, our goal was
    to identify patients in critical condition during diagnosis. This classification
    is essential for providing timely information to patients and their families,
    improving the quality of end-of-life care, and facilitating treatment and resource
    planning. The analyzed dataset showed a significant data imbalance, with 13% representing
    the minority class (Short) and 87% representing the majority class (Non-Short).
    Our focus was centered on the examination of biomarkers commonly encountered in
    routine ALS clinical practice. The proposed solution combined Ensemble and Imbalance
    learning techniques to improve the prediction of critical ALS patients at diagnosis
    time. Our Ensemble-Imbalance approach obtained the best performance, achieving
    a Balanced Accuracy of 88% and a Sensitivity of 96% using a Neural Network model
    as the base classifier. Furthermore, we employed the SHAP framework to provide
    insights into how the best model conducted patient classifications. The principal
    contributions of our study encompass: (i) the development and evaluation of models
    through an Ensemble-Imbalance-based approach, resulting in improved performance
    in identifying critically affected ALS patients at the time of diagnosis, (ii)
    delivering both global and local explanations regarding the model’s prediction
    mechanisms, including the identification of pivotal features and their correlations
    with the target variable, and (iii) offering an effective preprocessing methodology
    for ALS patient data that enabled the extraction of relevant ALS characteristics
    using biomarkers commonly encountered in clinical practice. Methods To ensure
    the systematic execution of our experiments, we organized our models into two
    distinct scenarios: Single-Model and Ensemble-Imbalance. In the initial phase,
    we designed and evaluated models using state-of-the-art machine learning algorithms,
    including k-Nearest Neighbors (k-NN), Decision Tree (DT), Random Forest (RF),
    Support Vector Machines (SVM), Naïve Bayes (NB), and Neural Networks (NN). These
    models constituted the Single-Model scenario. Subsequently, we utilized the top
    ten models for each algorithm as base classifiers to develop and evaluate the
    Ensemble-Imbalance-based models. Following this, we selected the best-performing
    model for each algorithm and scenario and conducted a comparative analysis of
    their results (see Supplementary Information for more details). Finally, we employed
    the SHAP framework to elucidate how the overall best model executed patient classifications,
    offering insights into the significance of each feature, in addition to providing
    both global and local interpretability of the model. The data analyzed in this
    study can be accessed from the PRO-ACT website (https://ncri1.partners.org/ProACT).
    It is important to note that data derived from this database cannot be shared
    due to restrictions. However, comprehensive details about the source code employed
    in this study, including data preprocessing, model development, hyperparameter
    settings, and software versions, are available at the public repository https://github.com/fabianopapaiz/ensemble_imbalance_model_for_als_prognosis.
    Patient data All data used in this article were sourced from the Pooled Resource
    Open-Access ALS Clinical Trials Database (PRO-ACT) [18]. PRO-ACT is the largest
    open-access dataset available for ALS disease. It comprises over 11,600 records,
    which contain historical data from 29 clinical trials on ALS. The dataset provides
    information on clinical, functional, respiratory, laboratory exams, death reports,
    and other biomarkers. The data available in the PRO-ACT Database have been volunteered
    by PRO-ACT Consortium members. For the purposes of our study, we extracted a range
    of pertinent information, including demographic details (age, weight, height,
    and gender), the administration of the Riluzole drug, familial medical history,
    Forced Vital Capacity (FVC; expressed as a percentage of normal for a healthy
    individual, adjusted for gender, age, and height), Slow Vital Capacity (SVC),
    Body Mass Index (BMI), El Escorial diagnostic criteria, ALS Functional Rate Scale
    (ALSFRS), and Revised ALS Functional Rate Scale (ALSFRS-R). The ALSFRS scale comprises
    ten inquiries focused on assessing various physical functionalities, such as speech,
    swallowing, handwriting, turning in bed, walking, climbing stairs, and respiratory
    [19]. The ALSFRS-R scale replaced the single respiratory function question with
    three more detailed questions [20]. Design of the experiments We put forward an
    ML pipeline divided into the stages as shown in Fig. 1, which were detailed hereafter.
    All experiments were performed using the Python programming language and packages
    for data analysis, machine learning, and data visualization, including Pandas,
    Scikit-Learn, Imbalance-Learning, Matplotlib, Seaborn, SHAP, and NumPy. Fig. 1
    ML pipeline design to execute the experiments of this study Full size image Data
    preprocessing Patient data collected during diagnosis was analyzed, as previously
    mentioned. To facilitate their analysis, temporal features were transformed into
    static data through a technique known as Summary Measures. This approach offers
    several advantages, including simplicity of interpretation, compatibility with
    uneven time intervals between measurements, and statistical robustness and validity
    [21]. We utilized values recorded on the date of diagnosis for temporal features
    such as FVC, SVC, and BMI. In instances where these values were unavailable, we
    selected the measurement closest to the diagnosis date for the respective samples.
    As recommended in our previous study [22], we analyzed the slope of each ALSFRS
    question separately instead of the total slope. This approach enabled us to perform
    a more granular examination of functional loss characteristics among patients,
    aiding in the identification of the most pertinent ALSFRS questions for our target
    prediction. Gordon and Lerner [6] presented an approach to merge data from both
    ALSFRS and ALSFRS-R scales by combining the samples using only information about
    Dyspnea (question 10) for those assessed with the ALSFRS-R scale. This enabled
    them to convert the ALSFRS-R scale to ALSFRS, thereby expanding the sample size.
    In alignment with this approach, we adopted the same strategy in this study, as
    51% of the PRO-ACT samples were assessed using the ALSFRS-R scale. Consequently,
    questions 11 and 12 of the ALSFRS-R scale were not included in our analysis. To
    model the ALSFRS questions as non-temporal variables, we summarized their data
    into single slope values. These slopes were calculated as depicted in Eq. (1),
    where 4 represents the maximum question score, Question Score at Diagnosis denotes
    the score assessed at (or closest to) the time of diagnosis, and Disease Duration
    is the time in months between symptom onset and the time of diagnosis. (1) Additional
    features were created to store information about the age at symptom onset, the
    BMI, and whether the patient deceased within 24 months from symptom onset (Survival
    Group). The age at onset was calculated using information about the age at diagnosis
    and the disease duration. The BMI was calculated using the patient weight and
    height collected at the diagnosis. The Survival Group feature was used to classify
    patients with respect to the target prediction, i.e., into the Short and Non-Short
    survival groups. The Short survival group included patients who died within 24
    months from the onset of the symptoms. We excluded patients whose last visit was
    within 24 months of disease onset and who were not marked as deceased in the PRO-ACT
    database. We performed a complete case analysis, whereby our preprocessed dataset
    comprised solely of samples with no missing feature values. Features exhibiting
    a significant percentage of missing values were excluded: SVC (87%) and El Escorial
    (71%). This action was imperative to prevent the loss of a substantial number
    of samples. Before being used by the ML models, the features were scaled to a
    range between 0 and 1, and the dataset was partitioned into Training and Validation
    subsets. We allocated 80% of the samples for training the models and reserved
    20% for validation. Models development This phase encompassed two key steps: (i)
    splitting the training data using a 5-fold Cross-Validation (CV) repeated three
    times and (ii) executing the models using a grid search strategy. We developed
    models using the following ML algorithms: k-NN, NB, DT, RF, SVM, and NN. In the
    Single-Model scenario, the models were directly executed using the 5-fold CV strategy
    in conjunction with diverse hyperparameter configurations as part of the grid
    search. In the Ensemble-Imbalance scenario, the models were executed using the
    following classifiers: Balanced Bagging (DT, SVM, NN, NB, and k-NN) and Balanced
    Random Forest (RF). These classifiers integrate Ensemble and Resampling techniques
    to increase the classification performance for minority classes without a significant
    decrease for the majority class. Initially, this approach creates multiple independent
    subsets of the original training data. Then, the number of samples in the different
    classes is equalized for each subset by randomly removing samples from the majority
    class using the Random Undersampling [11] method. Finally, instances of the same
    base classifier are trained using each of these subsets, and the final prediction
    is computed using a voting or averaging mechanism. Figure 2 provides an overview
    of the Ensemble-Imbalance classifier proposed in this study. The ten best classifiers
    for each algorithm obtained from the Single-Model scenario were utilized as base
    classifiers to create the models of the Ensemble-Imbalance scenario. Fig. 2 Overview
    of the Ensemble-Imbalance classifier proposed in this study. First, independent
    undersampled subsets are generated from the Training set using the Random Undersampling
    method. Then, classifiers created using a specific ML algorithm learn from these
    subsets (each classifier accesses only one subset). Finally, a majority voting
    strategy is used to classify patients into survival groups Full size image Selecting
    and retraining the best models The top-performing models were chosen based on
    their Balanced Accuracy achieved using the Training set in both scenarios. Afterward,
    the best models were retrained (refitted) using the Training set and used to make
    predictions by accessing the Validation set. All obtained validation performance
    metrics were recorded and subjected to subsequent analysis and comparison. Models
    evaluation In our evaluation and comparative analysis of all ML models, we employed
    the following metrics in this order: Balanced Accuracy, Sensitivity, and Specificity.
    Sensitivity and Specificity were utilized as they signify the proportion of correctly
    classified Short and Non-Short survival patients, respectively. It is worth noting
    that Sensitivity held greater significance than Specificity in our evaluation,
    given our priority was to correctly classify Short survival patients, as they
    represent the critical cases. Balanced Accuracy was selected as the appropriate
    metric for evaluating the experiments as it represents the arithmetic average
    of Sensitivity and Specificity. Consequently, a higher Balanced Accuracy signifies
    superior predictive performance concerning both groups of patients. We applied
    the Bonferroni correction method to ascertain whether the performance attained
    in the Ensemble-Imbalance scenario significantly surpassed that of the Single-Model
    scenario for each algorithm. It is essential to counteract the multiple comparisons
    problem due to the number of executions using 5-Fold CV repeated three times.
    Feature importance and model explanation Following the evaluation and identification
    of the best overall model (specifically, the Ensemble-Imbalance-based model utilizing
    NN as the base classifier), we conducted an in-depth analysis of how this model
    classifies patients using the SHAP framework. We detailed the significance of
    each feature for the classification process in the results section, providing
    comprehensive insights into global and local interpretability. To generate SHAP
    values and explanations, we employed the Kernel-Explainer class. All SHAP graphs
    were produced using the functionalities provided by this framework. Results Data
    preprocessing This study accessed ALS patient data from the PRO-ACT database.
    Despite its large number of samples (over 11,600), we used only 17% of the available
    data. We opted to perform a complete case analysis, which reduced the number of
    samples that could be included due to a high percentage of missing values. The
    preprocessed dataset encompassed 1,967 patients, each characterized by 23 features.
    This dataset exhibited an Imbalance Ratio of 6.9 concerning the distribution of
    the minority and majority classes, with Short survival comprising 13% and Non-Short
    constituting 87% of the cases. Table 1 provides a comprehensive overview of all
    features analyzed in this study, along with their respective values and distributions.
    Table 1 Features analyzed in this study with details on overall distribution and
    by survival group Full size table Performance obtained by algorithm and scenario
    Figure 3 visually depicts the top validation performances achieved by each algorithm
    and scenario. The “p” alongside each algorithm’s name indicates the p-value calculated
    after employing the Bonferroni correction method. Algorithms that exhibited significantly
    improved performance in the Ensemble-Imbalance scenario were denoted by the ⋆
    symbol. We applied the same method to compare the performance of algorithms in
    the Ensemble-Imbalance scenario. The Neural Networks outperformed significantly
    (p-value ≤ 0.001) the others (DT, RF, SVM, and k-NN). Fig. 3 Comparison of the
    best performances obtained by each algorithm and scenario. The “p” represents
    the p value obtained after applying the Bonferroni correction method to compare
    the performance in both scenarios for each algorithm. Algorithms presenting a
    significantly better performance in the Ensemble Imbalance scenario were highlighted
    using the ⋆ symbol. Neural Networks performed significantly better (p-value ≤ 0.001)
    than the others in the Ensemble-Imbalance scenario (highlighted in bold and blue
    font) Full size image Feature importance and model explanation Following the evaluation
    and selection of the overall best model (the Ensemble-Imbalance based model using
    NN as a base classifier), we utilized the SHAP framework to obtain insights into
    how this model conducted patient classifications. Figure 4 provides valuable information
    for comprehending the global interpretability of the model. The left graph displays
    the ranking of feature importance based on their average impact on the model’s
    output. The right graph illustrates the correlations of feature values with the
    target prediction. SHAP values on the x-axis exceeding zero indicate that the
    feature value drove the prediction into the Short survival group, whereas those
    below zero into the Non-Short group. Figure 5 elucidates the global interpretability
    by detailing the impact on model prediction according to each feature value. Due
    to space constraints, we present the top ten most relevant features. Fig. 4 Ranking
    of feature importance and their correlations with the target variable (Short/Non-Short)
    for the best model in the Ensemble-Imbalance scenario. The x-axis on the left
    displays the average impact on the model prediction for each feature (mean absolute
    SHAP value). The x-axis on the right demonstrates the impact on the model prediction
    (SHAP value) concerning the feature values. Positive SHAP values indicate that
    the feature value led the model prediction toward the Short survival group, while
    negative SHAP values pushed it toward the Non-Short group Full size image Fig.
    5 Average impact on model prediction for the top ten most important features detailed
    according to their values. Positive (red) and negative (blue) SHAP values drive
    the prediction into Short and Non-Short survival groups, respectively Full size
    image Figure 6 offers an illustration of how SHAP local interpretability can be
    leveraged to elucidate the classification of any given patient based on their
    feature values. This Figure displays information for two patients (A and B) extracted
    from the Validation set. While Patient A was classified into the Non-Short survival
    group, Patient B was placed into the Short group. Subfigures “a” and “b” show
    individualized classifications for both patients. The classification process was
    driven differently according to their feature values (displayed in gray font within
    parenthesis). Subfigure “c” illustrates the classification process by comparing
    both patients on a feature-by-feature basis. Fig. 6 Examples of using the SHAP
    Decision plot to explain how the model classified patients into Short and Non-Short
    survival groups. Subfigures “a” and “b” show individualized classifications for
    each patient, where the process was conducted according to their feature values
    (displayed in gray font within parenthesis). Subfigure “c” shows the classification
    process comparing feature by feature for both patients. These graphs must be read
    from bottom to top. The slope of the line within each feature area indicates when
    the feature value drove the prediction toward the Non-Short (left sloping) or
    the Short (right sloping) groups. The longer the line length within the feature
    area, the more significant the impact of its value on the model prediction Full
    size image Discussion This study assessed the application of Ensemble and Imbalance
    Learning to enhance the prediction of short-survival ALS patients at the time
    of diagnosis. Our focus was on the analysis of patient data commonly encountered
    in routine ALS clinical practice, obtained through a less complex process. We
    discuss the results obtained in the following subsections. Predictive performances
    In the Ensemble-Imbalance scenario, most of the algorithms (5 out of 6) exhibited
    significantly improved performance when compared with the Single-Model scenario
    (Fig. 3). The proposed Ensemble-Imbalance approach notably increased Sensitivity
    without compromising Balanced Accuracy. This is crucial as it improves the classification
    of critical patients. The only exception was Naïve Bayes, where the difference
    between the scenarios was not statistically significant (p-value: 0.346). In the
    Single-Model scenario, k-NN was the most affected by the data imbalance problem,
    achieving a Balanced Accuracy of 0.68 and showing a tendency to favor the majority
    class (Non-Short). The Ensemble-Imbalance-based model using Neural Networks as
    a base classifier (EI-NN) outperformed the others significantly (Balanced Accuracy:
    0.88; Sensitivity: 0.96; Specificity: 0.80; p-value ≤ 0.001). The Decision Tree,
    SVM, and Random Forest models demonstrated similar performances to EI-NN. We assume
    these four models are proper for composing a CDS system inference mechanism for
    classifying critical ALS patients based on data collected at diagnosis. Our approach
    yielded promising results, but further validation with unseen data, preferably
    real-world patient data, is necessary to eliminate bias toward the minority class
    (Short). This step is essential for a more robust model comparison. Data preprocessing
    The data preprocessing proposed and executed in this study proved to be highly
    efficient, enabling ML algorithms to gain a comprehensive understanding of ALS
    characteristics. Even in the Single-Model scenario, Neural Networks, Random Forest,
    and SVM models achieved good performance, considering the data imbalance and the
    complexity of ALS. In the context of ALS prognosis, a data categorization approach
    may be more effective than direct utilization of the actual feature values. Future
    studies could explore alternative definitions of categorical values to assess
    their impact on performance. Our results also highlighted the feasibility of constructing
    ML solutions using less complex biomarkers. We consider it essential to develop
    feasible CDS systems for primary care, eliminating the need for more complex and
    costly biomarkers such as genetics. ALS disease presents an inherent complexity,
    and future work can extend our approach by employing clustering methods to find
    groups of patients with correlated clinical features and, thus, develop more effective
    models based on the clusters identified. We can cite FCAN-MOPSO [23] and Biclustering
    [24] as examples of such methods. Features importance and model explanation A
    comprehensive analysis of the results revealed valuable insights into understanding
    the global interpretability of the model, the importance of the features, and
    their correlations with target prediction (refer to Figs. 4 and 5). Many features
    displayed substantial correlations with the target, underscoring their importance
    in identifying critical patients at the time of diagnosis. Table 2 provides details
    on the type of correlation (positive/negative) for the top ten ranked features
    based on their categorical-ordinal values. Diagnostic Delay, BMI, and Riluzole
    exhibited the most relevant negative correlations with the target. Conversely,
    Q6-Dressing & Hygiene, Q9-Climbing Stairs, Q8-Walking, Q1-Speech, Sex, Age at
    Onset, and FVC showed the most relevant positive correlations. Table 2 Features
    correlations with the target variable ordered by the type and importance Full
    size table The Diagnostic Delay was the most relevant among the features. We can
    observe that 66% of patients in the Short survival group were diagnosed within
    the first eight months of the onset of the disease (Table 1). This is an important
    biomarker, although it is necessary to analyze the following features to understand
    which signs led to a faster diagnosis. Following the ranking, questions Q6, Q9,
    and Q8 of the ALSFRS scale appear as the most relevant, thus correlating the degree
    of lower motor neuron degeneration with a worse prognosis. This aligns with what
    was reported by Al-Chalabi et al. [25]. Other factors that represented a worse
    prognosis also conform with the literature on ALS, such as male gender, being
    older at diagnosis, and having an abnormal FVC. Previous studies using ML applied
    to ALS prognosis have also identified these features as survival predictors [6,
    15, 16, 26]. Hence, we conclude that the proposed Ensemble-Imbalance approach
    effectively learned from patient data to extract crucial ALS disease characteristics.
    The most significant characteristics for identifying critical ALS patients at
    the time of diagnosis were: (i) shorter diagnostic time (≤ 8 months); (ii) higher
    decline (slope ≥ 0.14) in ALSFRS Q6 (Dressing & Hygiene), Q9 (Climbing-Stairs),
    Q8 (Walking), and Q1 (Speech); (iii) male gender; (iv) age ≥ 60 years old; (v)
    abnormal FVC; (vi) not treated with Riluzole; and (vii) underweight (BMI ≤ 18.4).
    Figure 6 illustrates the local interpretability of the model based on the SHAP
    results. Subfigures “a” and “b” provide personalized predictions for two patients
    extracted from the Validation set. Patient A was classified into the Non-Short
    survival group, whereas Patient B was classified into the Short. Please note that
    their feature values influenced the classification process (displayed in gray
    font within parentheses). Our approach enables the identification of the most
    influential features contributing to disease progression. Consequently, physicians
    can direct symptomatic treatment to enhance the patient’s quality of life. For
    example, Patient B exhibited a significant functional decline in Q1-Speech and
    Q9-Climbing Stairs. This information could guide physicians in deciding that speech
    and physical therapies are necessary. Moreover, this information may be employed
    as inclusion or exclusion criteria in clinical trials, facilitating the selection
    of patients with predefined characteristics. Subfigure “c” details both patients
    feature-by-feature within the same graph, providing a valuable resource for visualizing
    and comparing two or more patients, thereby revealing their similarities and differences.
    This study has certain limitations. The data analyzed were extracted from clinical
    trials rather than a population-based registry. Consequently, there is a risk
    that it may not fully represent the entire ALS population due to the inclusion
    and exclusion criteria applied. Furthermore, the dataset exclusively comprised
    ALS patients from the United States of America. It is essential to validate the
    results using data from other regions with different genetic backgrounds (e.g.,
    South America, Africa, or Asia). Additionally, information about cognitive impairment
    at the time of diagnosis was absent in the PRO-ACT database, likely due to its
    use as an exclusion criterion in clinical trials. Nonetheless, this biomarker
    has been previously identified as a significant contributor to a worse prognosis,
    independent of specific motor impairments [27]. Therefore, it potentially holds
    relevance as a feature for classifying critical patients at the time of diagnosis.
    Conclusion This study evaluated the use of Machine Learning to predict short survival
    in ALS patients by analyzing biomarkers collected at the time of diagnosis. We
    focused on analyzing biomarkers commonly encountered in daily ALS clinical practice,
    thus avoiding the need for more complex and costly biomarkers such as genetics
    or imaging. Our findings demonstrate that the proposed Ensemble-Imbalance approach
    can significantly enhance predictive performance in classifying critical patients
    during diagnosis. Furthermore, we provided detailed insights into how the model
    generates predictions, emphasizing both global and local interpretability. In
    future work, we intend to leverage these findings to develop a Clinical Decision
    Support (CDS) system for classifying critical ALS patients using data collected
    from Brazilian patients. This represents a crucial step toward confirming the
    results obtained in this study. Availability of data and materials No datasets
    were generated or analysed during the current study. References Chiò A, Pagani
    M, Agosta F, Calvo A, Cistaro A, Filippi M. Neuroimaging in amyotrophic lateral
    sclerosis: insights into structural and functional changes. Lancet Neurol. 2014;13:1228–1240.
    URL: http://www.sciencedirect.com/science/article/pii/S147444221470167X. https://doi.org/10.1016/S1474-4422(14)70167-X.
    Swinnen B, Robberecht W. The phenotypic variability of amyotrophic lateral sclerosis.
    Nat Rev Neurol. 2014;10:661–70. https://doi.org/10.1038/nrneurol.2014.184. Article   PubMed   Google
    Scholar   Nowicka N, Juranek J, Juranek JK, Wojtkiewicz J. Risk factors and emerging
    therapies in amyotrophic lateral sclerosis. Int J Mol Sci. 2019;20. URL: https://www.mdpi.com/1422-0067/20/11/2616.
    https://doi.org/10.3390/ijms20112616. van der Burgh HK, Schmidt R, Westeneng HJ,
    de Reus MA, van den Berg LH, van den Heuvel MP. Deep learning predictions of survival
    based on MRI in amyotrophic lateral sclerosis. NEUROIMAGE-CLINICAL. 2017;13:361–9.
    https://doi.org/10.1016/j.nicl.2016.10.008. Article   PubMed   Google Scholar   Halbersberg
    D, Lerner B. 2019. Temporal Modeling of Deterioration Patterns and Clustering
    for Disease Prediction of ALS Patients, in: 2019 18th IEEE International Conference
    On Machine Learning And Applications (ICMLA), pp. 62–68. https://doi.org/10.1109/ICMLA.2019.00019.
    Journal Abbreviation: 2019 18th IEEE International Conference On Machine Learning
    And Applications (ICMLA). Gordon J, Lerner B. Insights into amyotrophic lateral
    sclerosis from a machine learning perspective. J Clin Med. 2019;8(10):1578. https://doi.org/10.3390/jcm8101578.
    Article   PubMed   PubMed Central   Google Scholar   Pires S, Gromicho M, Pinto
    S, Carvalho M, Madeira SC. 2018. Predicting Noninvasive Ventilation in ALS Patients
    Using Stratified Disease Progression Groups, in: 2018 IEEE International Conference
    on Data Mining Workshops (ICDMW), Singapore. pp. 748–757. Journal Abbreviation:
    2018 IEEE International Conference on Data Mining Workshops (ICDMW).https://doi.org/10.1109/ICDMW.2018.00113.
    Kubat M. An Introduction to Machine Learning. Cham: Springer International Publishing;
    2017. https://doi.org/10.1007/978-3-319-63913-0. Book   Google Scholar   Rokach
    L. Ensemble-based classifiers. Artificial Intell Rev. 2010;33:1–39. https://doi.org/10.1007/s10462-009-9124-7.
    Article   Google Scholar   Chawla NV. 2005. Data Mining for Imbalanced Datasets:
    An Overview, in: Maimon, O., Rokach, L. (Eds.), Data Mining and Knowledge Discovery
    Handbook. Springer-Verlag, New York, pp. 853–867. https://doi.org/10.1007/0-387-25465-X_40.
    Fernández A, Galar M, García S, Herrera F, Krawczyk B, Prati RC, 2018a. Learning
    from Imbalanced Data Sets. 1st ed. 2018 ed., Springer International Publishing:
    Imprint: Springer, Cham. https://doi.org/10.1007/978-3-319-98074-4. Beeler PE,
    Bates DW, Hug BL. Clinical decision support systems. Swiss Medical Weekly. 2014;144:w14073.
    https://doi.org/10.5167/uzh-122774. https://doi.org/10.4414/smw.2014.14073. Miller
    T. Explanation in artificial intelligence: insights from the social sciences.
    Artificial Intelligence. 2019;267:1–38. URL: https://linkinghub.elsevier.com/retrieve/pii/S0004370218305988.
    https://doi.org/10.1016/j.artint.2018.07.007. Lundberg SM, Lee SI. 2017. A unified
    approach to interpreting model predictions, in: Guyon I, Luxburg UV, Bengio S,
    Wallach H, Fergus R, Vishwanathan S, Garnett R. (Eds.). Advances in Neural Information
    Processing Systems 30. Curran Associates, Inc., pp. 4765–4774. Kueffner R, Zach
    N, Bronfeld M, Norel R, Atassi N, Balagurusamy V, Di Camillo B, Chio A, Cudkowicz
    M, Dillenberger D, Garcia-Garcia J, Hardiman O, Hoff B, Knight J, Leitner ML,
    Li G et al. Stratification of amyotrophic lateral sclerosis patients: a crowdsourcing
    approach. Sci Rep. 2019;9. https://doi.org/10.1038/s41598-018-36873-4. Grollemund
    V, Chat GL, Secchi-Buhour MS, Delbot F, Pradat-Peyre JF, Bede P, Pradat PF. Development
    and validation of a 1-year survival prognosis estimation model for Amyotrophic
    Lateral Sclerosis using manifold learning algorithm UMAP. Sci Rep. 2020;10:13378.
    URL: https://www.nature.com/articles/s41598-020-70125-8. https://doi.org/10.1038/s41598-020-70125-8.
    Tavazzi E, Daberdaku S, Vasta R, Calvo A, Chiò A, Di Camilo B. Exploiting mutual
    information for the imputation of static and dynamic mixed-type clinical data
    with an adaptive k-nearest neighbours approach. BMC Med Inform Decis Mak. 2020;20(Suppl
    5):174. https://doi.org/10.1186/s12911-020-01166-2. Article   PubMed   PubMed
    Central   Google Scholar   Atassi N, Berry J, Shui A, Zach N, Sherman A, Sinani
    E, Walker J, Katsovskiy I, Schoenfeld D, Cudkowicz M, Leitner M. The pro-act database.
    Neurology. 2014;83:1719–1725. URL: https://n.neurology.org/content/83/19/1719.
    https://doi.org/10.1212/WNL.0000000000000951. Neurol A. The amyotrophic lateral
    sclerosis functional rating scale: Assessment of activities of daily living in
    patients with amyotrophic lateral sclerosis. Arch Neurol. 1996;53:141–147. https://doi.org/10.1001/archneur.1996.00550020045014.
    Cedarbaum JM, Stambler N, Malta E, Fuller C, Hilt D, Thurmond B, Nakanishi A.
    The alsfrs-r: a revised als functional rating scale that incorporates assessments
    of respiratory function. J Neurological Sci. 1999;169:13–21. URL: https://linkinghub.elsevier.com/retrieve/pii/S0022510X99002105.
    https://doi.org/10.1016/S0022-510X(99)00210-5. Matthews JN, Altman DG, Campbell
    MJ, Royston P. Analysis of serial measurements in medical research. Br Med J.
    1990;300:230–5. https://doi.org/10.1136/bmj.300.6719.230. Article   CAS   Google
    Scholar   Papaiz F, Dourado MET, Valentim RAdM, de Morais AHF, Arrais JP. Machine
    learning solutions applied to amyotrophic lateral sclerosis prognosis: a review.
    Front Computer Sci. 2022;4. https://doi.org/10.3389/fcomp.2022.869140. Hu L, Yang
    Y, Tang Z, He Y, Luo X. FCAN-MOPSO: an improved fuzzy-based graph clustering algorithm
    for complex networks with multiobjective particle swarm optimization. IEEE Trans
    Fuzzy Syst. 2023;31(10):3470–84. https://doi.org/10.1109/TFUZZ.2023.3259726. Article   Google
    Scholar   Matos J, Pires S, Aidos H, Gromicho M, Carvalho M, Madeira SC, 2020.
    Unravelling Disease Presentation Patterns in ALS Using Biclustering for Discriminative
    Meta-Features Discovery. In: Rojas, I., Valenzuela, O., Rojas, F., Herrera, L.,
    Ortuño, F. (eds) Bioinformatics and Biomedical Engineering. IWBBIO 2020. Lecture
    Notes in Computer Science, vol 12108. Springer, Cham. https://doi.org/10.1007/978-3-030-45385-5_46.
    Al-Chalabi A, Hardiman O, Kiernan MC, Chio` A, Rix-Brooks B, Van Den Berg LH.
    Amyotrophic lateral sclerosis: moving towards a new classification system. Lancet
    Neurol. 2016;15:1182–1194. URL: https://linkinghub.elsevier.com/retrieve/pii/S1474442216301995.
    https://doi.org/10.1016/S1474-4422(16)30199-5. Leão T, Madeira SC, Gromicho M,
    de Carvalho M, Carvalho AM. Learning dynamic Bayesian networks from time-dependent
    and time-independent data: Unraveling disease progression in Amyotrophic Lateral
    Sclerosis. J Biomed Inform. 2021;117:103730. URL: https://www.sciencedirect.com/science/article/pii/S1532046421000599.
    https://doi.org/10.1016/j.jbi.2021.103730. Manera U, Calvo A, Daviddi M, Canosa
    A, Vasta R, Torrieri MC, Grassano M, Brunetti M, D’Alfonso S, Corrado L, Marchi
    FD, Moglia C, D’Ovidio F, Mora G, Mazzini L, Chiò A. Regional spreading of symptoms
    at diagnosis as a prognostic marker in amyotrophic lateral sclerosis: a population-based
    study. J Neurol. Neurosurg Psychiatry. 2020;91:291–297. URL: https://jnnp.bmj.com/content/91/3/291.
    https://doi.org/10.1136/jnnp-2019-321153. Download references Acknowledgements
    Data used in the preparation of this article were obtained from the Pooled Resource
    Open-Access ALS Clinical Trials (PRO-ACT) Database. As such, the following organizations
    and individuals within the PRO-ACT Consortium contributed to the design and implementation
    of the PRO-ACT Database and/or provided data, but did not participate in the analysis
    of the data or the writing of this report: Amylyx Pharmaceuticals, Inc., ALS Therapy
    Alliance, Cytokinetics, Inc., Knopp Biosciences, Neuraltus Pharmaceuticals, Inc.,
    Neurological Clinical Research Institute, MGH, Northeast ALS Consortium, Novartis,
    Prize4Life Israel, Regeneron Pharmaceuticals, Inc., Sanofi, Teva Pharmaceutical
    Industries, Ltd., The ALS Association. Funding This study was funded by the Brazilian
    Ministry of Health through the Scientific and Technological Development Applied
    to ALS project, conducted by the Laboratory of Technological Innovation in Health
    (LAIS) at the Federal University of Rio Grande do Norte (UFRN). The funder had
    no role in the study design, data collection, analysis, interpretation, or manuscript
    writing. Author information Authors and Affiliations Federal University of Rio
    Grande Do Norte, Natal, Brazil Fabiano Papaiz, Mario Emílio Teixeira Dourado Jr.,
    Ricardo Alexsandro de Medeiros Valentim & Rafael Pinto University of Coimbra,
    Coimbra, Portugal Fabiano Papaiz & Joel Perdiz Arrais Federal Institute of Rio
    Grande Do Norte, Natal, Brazil Fabiano Papaiz, Rafael Pinto & Antônio Higor Freire
    de Morais Contributions Conceptualization and Methodology: F.P., M.E.T.D.J., A.H.F.M.,
    and J.P.A.; Supervision: M.E.T.D.J., A.H.F.M., and J.P.A.; Data curation: F.P.,
    M.E.T.D.J., and J.P.A.; Investigation, Writing- Original draft preparation, Visualization,
    and Software: F.P.; Writing- Reviewing and Editing: R.P., M.E.T.D.J., R.A.M.V.,
    A.H.F.M., and J.P.A.. All authors have reviewed and approved the final version
    of the manuscript, taking responsibility for its content and the decision to submit
    it for publication. Corresponding author Correspondence to Fabiano Papaiz. Ethics
    declarations Ethics approval and consent to participate Data used in this study
    were obtained from the Pooled Resource Open-Access ALS Clinical Trials (PRO-ACT)
    Database. Data from these trials were donated to the PRO-ACT Database for research
    purposes only and under the explicit conditions that The ALS Association and all
    users of the data would maintain the anonymity of subjects and not attempt to
    discover the identity of any subject. The participating medical centers obtained
    patient consents and approvals. For further information on PRO-ACT and the participating
    medical centers, please refer to https://ncri1.partners.org/ProACT/Home/Index.
    Consent for publication Not applicable. Competing interests The authors declare
    no competing interests. Additional information Publisher’s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Supplementary Information Supplementary Material 1. Rights and permissions
    Open Access This article is licensed under a Creative Commons Attribution 4.0
    International License, which permits use, sharing, adaptation, distribution and
    reproduction in any medium or format, as long as you give appropriate credit to
    the original author(s) and the source, provide a link to the Creative Commons
    licence, and indicate if changes were made. The images or other third party material
    in this article are included in the article''s Creative Commons licence, unless
    indicated otherwise in a credit line to the material. If material is not included
    in the article''s Creative Commons licence and your intended use is not permitted
    by statutory regulation or exceeds the permitted use, you will need to obtain
    permission directly from the copyright holder. To view a copy of this licence,
    visit http://creativecommons.org/licenses/by/4.0/. The Creative Commons Public
    Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies
    to the data made available in this article, unless otherwise stated in a credit
    line to the data. Reprints and permissions About this article Cite this article
    Papaiz, F., Dourado, M.E.T., de Medeiros Valentim, R.A. et al. Ensemble-imbalance-based
    classification for amyotrophic lateral sclerosis prognostic prediction: identifying
    short-survival patients at diagnosis. BMC Med Inform Decis Mak 24, 80 (2024).
    https://doi.org/10.1186/s12911-024-02484-5 Download citation Received 16 December
    2023 Accepted 14 March 2024 Published 19 March 2024 DOI https://doi.org/10.1186/s12911-024-02484-5
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Amyotrophic lateral sclerosis Prognosis Machine learning Health
    informatics Download PDF Sections Figures References Abstract Introduction Methods
    Results Discussion Conclusion Availability of data and materials References Acknowledgements
    Funding Author information Ethics declarations Additional information Supplementary
    Information Rights and permissions About this article Advertisement BMC Medical
    Informatics and Decision Making ISSN: 1472-6947 Contact us General enquiries:
    journalsubmissions@springernature.com Read more on our blogs Receive BMC newsletters
    Manage article alerts Language editing for authors Scientific editing for authors
    Policies Accessibility Press center Support and Contact Leave feedback Careers
    Follow BMC By using this website, you agree to our Terms and Conditions, Your
    US state privacy rights, Privacy statement and Cookies policy. Your privacy choices/Manage
    cookies we use in the preference centre. © 2024 BioMed Central Ltd unless otherwise
    stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: BMC Medical Informatics and Decision Making
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Ensemble-imbalance-based classification for amyotrophic lateral sclerosis
    prognostic prediction: identifying short-survival patients at diagnosis'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Noda T.
  - Koizumi T.
  - Yukitake N.
  - Yamamoto D.
  - Nakaizumi T.
  - Tanaka K.
  - Okuyama J.
  - Ichikawa K.
  - Hara T.
  citation_count: '0'
  description: The underwater environment is filled with various sounds, with its
    soundscape composed of biological, geographical, and anthropological sounds. Our
    work focused on developing a novel method to observe and classify these sounds,
    enriching our understanding of the underwater ecosystem. We constructed a biologging
    system allowing near-real-time observation of underwater soundscapes. Utilizing
    deep-learning-based edge processing, this system classifies the sources of sounds,
    and upon the tagged animal surfacing, it transmits positional data, results of
    sound source classification, and sensor readings such as depth and temperature.
    To test the system, we attached the logger to sea turtles (Chelonia mydas) and
    collected data through a cellular network. The data provided information on the
    location-specific sounds detected by the sea turtles, suggesting the possibility
    to infer the distribution of specific species of organisms over time. The data
    showed that not only biological sounds but also geographical and anthropological
    sounds can be classified, highlighting the potential for conducting multi-point
    and long-term observations to monitor the distribution patterns of various sound
    sources. This system, which can be considered an autonomous mobile platform for
    oceanographic observations, including soundscapes, has significant potential to
    enhance our understanding of acoustic diversity.
  doi: 10.1038/s41598-024-56439-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    16 March 2024 Animal-borne soundscape logger as a system for edge classification
    of sound sources and data transmission for monitoring near-real-time underwater
    soundscape Takuji Noda, Takuya Koizumi, Naoto Yukitake, Daisuke Yamamoto, Tetsuro
    Nakaizumi, Kotaro Tanaka, Junichi Okuyama, Kotaro Ichikawa & Takeshi Hara  Scientific
    Reports  14, Article number: 6394 (2024) Cite this article 766 Accesses 3 Altmetric
    Metrics Abstract The underwater environment is filled with various sounds, with
    its soundscape composed of biological, geographical, and anthropological sounds.
    Our work focused on developing a novel method to observe and classify these sounds,
    enriching our understanding of the underwater ecosystem. We constructed a biologging
    system allowing near-real-time observation of underwater soundscapes. Utilizing
    deep-learning-based edge processing, this system classifies the sources of sounds,
    and upon the tagged animal surfacing, it transmits positional data, results of
    sound source classification, and sensor readings such as depth and temperature.
    To test the system, we attached the logger to sea turtles (Chelonia mydas) and
    collected data through a cellular network. The data provided information on the
    location-specific sounds detected by the sea turtles, suggesting the possibility
    to infer the distribution of specific species of organisms over time. The data
    showed that not only biological sounds but also geographical and anthropological
    sounds can be classified, highlighting the potential for conducting multi-point
    and long-term observations to monitor the distribution patterns of various sound
    sources. This system, which can be considered an autonomous mobile platform for
    oceanographic observations, including soundscapes, has significant potential to
    enhance our understanding of acoustic diversity. Similar content being viewed
    by others Population coding of strategic variables during foraging in freely moving
    macaques Article Open access 05 March 2024 Satellite mapping reveals extensive
    industrial activity at sea Article Open access 03 January 2024 Using DeepLabCut
    for 3D markerless pose estimation across species and behaviors Article 21 June
    2019 Introduction The underwater environment, representing the world’s most extensive
    habitat, encompasses a diverse array of sounds produced by various organisms (biophony).
    In addition to biophony, this environment comprises geological sounds, such as
    sounds caused by wind-driven waves and rain (geophony), and sounds generated by
    human activities like shipping and underwater construction (anthrophony). Soundscapes
    featuring this assortment of sounds, defined as “environmental sounds in terms
    of their spatial, temporal, and frequency attributes and the types of sources
    contributing to the sound field” (ISO 18405:2017, ISO, 2017)1,2. Within diverse
    marine soundscapes, the frequency ranges, sound pressure level (SPL), and particle
    motion (PM) components exhibit vast geographical variation, reflecting the complex
    dynamics of natural and anthropogenic influences2,3. Natural sounds such as waves
    and rain (geophony) contribute to the soundscape, with waves in the 200–2000 Hz
    range4 and rain creating sounds in the 15–20 kHz range5. Human activities such
    as shipping, underwater construction, and offshore wind farm operations create
    artificial sounds that span a wide frequency spectrum, which can be categorized
    into low-frequency sounds ranging from 10 to 500 Hz, medium-frequency sounds spanning
    500 Hz–25 kHz, and high-frequency sounds exceeding 25 kHz6. Marine life, ranging
    from large whales to small invertebrates, produces sounds spanning from infrasonic
    (< 20 Hz) to ultrasonic frequencies (> 20 kHz), serving crucial ecological functions
    such as navigation, communication, and foraging3,7,8. However, most of these biological
    sounds are emitted between 10 Hz and 20 kHz7,8,9, indicating the crucial importance
    of monitoring this frequency range for understanding the acoustic environment
    of these organisms. In recent years, noise issues in the ocean, and conflicts
    between marine organisms and human communities have gained increasing attention2.
    Anthropogenic noise, for instance, can interfere with marine animals’ natural
    auditory signal processing, causing “masking”10,11, which narrows their communication
    space12. Furthermore, the increases in cyclones and ocean heat waves, driven by
    climate change, can also alter biophony2. Specifically, coral reef degradation
    associated with these events can dramatically change the surrounding soundscape,
    with community composition shifts reflected in reduced overall acoustic energy
    and altered complexity and diversity of reef soundscapes13. In light of these
    challenges, passive acoustic monitoring (PAM), a remote sensing technique using
    a hydrophone to capture the underwater world''s soundscape, has become increasingly
    valuable. PAM serves as a noninvasive method for assessing biodiversity through
    analysis of the soundscape and plays a crucial role in observing underwater environmental
    changes and their impacts on marine life6,7,9,14. Real-time identification and
    dissemination of underwater soundscape information could help raise public awareness
    of aquatic noise issues and biodiversity stewardship. Soundscape information could
    also help monitor and manage biodiversity and coral reef healthiness in Marine
    Protected Areas (MPAs) and for monitoring illegal fishing or navigation. Instead
    of merely summarizing the underwater soundscape using traditional acoustic indicators
    like sound pressure, complexity, or entropy15, a more practical approach may be
    to focus on identifying specific sound sources. This is essential because these
    conventional acoustic indicators, developed primarily for terrestrial research,
    may not effectively represent the biodiversity of the underwater environment16,17.
    Providing more detailed sound classification results promotes understanding of
    diverse sound sources and enables more informed actions to address conflicts between
    organisms and human activities. A potential system for real-time soundscape monitoring
    could involve deploying buoys equipped with acoustic recording, sound classification
    functions, and satellite or cellular network transmission capabilities18. However,
    fixed-point observation limits its spatial coverage, and particularly in coastal
    areas, installation of recorders and buoys is often challenging due to the overlapping
    with fisheries and maritime activities. To complement this limitation, mobile
    platforms such as marine gliders and drifting recorders have been introduced19,20.
    Although those methods extend spatial coverage, they are not always suitable in
    shallow waters and coral reef areas because of their complex geographical characteristics.
    Given the high biodiversity in coastal areas, an alternative monitoring platform
    is required for effective environmental assessment. Biologging has recently emerged
    as an effective method for oceanographic observations, treating animals like sea
    turtles and seals as autonomous moving platforms. Since 2002, instrumented animals
    have contributed over 650,000 conductivity-temperature-depth (CTD) profiles to
    the Global Ocean Observing System (GOOS) network via the Global Telecommunication
    System (GTS)21. Animal-based observation is particularly effective in polar and
    coastal regions22,23,24, where Argo floats and ship-based observations are difficult.
    Thus, biologging may serve as an efficient real-time soundscape monitoring system,
    filling the gaps in areas where traditional methods such as Argo floats and ship-based
    observations are challenging. However, no reported system utilizes animals for
    real-time monitoring of underwater soundscapes. In the realm of real-time monitoring
    of underwater soundscapes through biologging, a crucial aspect is the implementation
    of sound classification. Confronting the challenge of transmitting raw audio data
    in real-time, primarily due to bandwidth and storage constraints of biologging
    devices25,26, it becomes imperative to process sounds directly on the device,
    a technique known as edge processing27. This method is significant as it allows
    only essential, classified data to be transmitted, reducing the volume of data
    and enhancing the functionality of biologging devices. Sound classification has
    evolved from feature-based methods28, machine learning29, and more recently, deep
    learning approaches30,31,32, which has been identified as the most accurate for
    underwater sound classification33. The strength of deep learning lies in its ability
    to autonomously extract complex features from raw audio data and accurately identify
    intricate patterns34. Deep learning uses layered neural networks for pattern recognition
    and decision-making with minimal human input35, efficiently processing large data
    volumes, ideal for real-time biologging in edge processing. Before the practical
    implementation of biologging for soundscape observation, which would include the
    development of applications for notifications and establishing field operation
    systems, it is essential to first develop the monitoring devices themselves. This
    study aimed to develop a biologging device (i.e., a soundscape logger) to classify
    sound sources and transmit the classification results in near-real time. Specifically,
    the objectives of this study were (1) to design the soundscape logger and (2)
    to collect data in the field using the logger developed as a demonstration. This
    study conducted experiments on the green turtle Chelonia mydas, as the model species,
    that come ashore to lay eggs in the coral reef area of Ishigaki Islands, Okinawa,
    Japan. The choice of sea turtles as the model species was motivated by several
    factors. Primarily, their behavior of surfacing for breathing allows for near-real-time
    observation without the need for physical device retrieval36. Furthermore, we
    specifically focused on sea turtles during the nesting season. These turtles typically
    engage in multiple nesting landings within a single season37, providing opportunities
    for direct logger recovery, crucial for evaluating the prototype. Additionally,
    concentrating on the coastal coral reef habitat, a region of rich biodiversity
    and human activity38, aligns with our aim to assess equipment in diverse acoustic
    environments. Moreover, sea turtles, which migrate post-breeding37 can be utilized
    in biologging studies for environmental monitoring, as their movements provide
    valuable ancillary environmental data21,22,23. While the present study emphasized
    the coastal regions during the nesting period for the purpose of direct device
    retrieval, the broad habitat range of sea turtles suggests their potential applicability
    in expansive oceanic monitoring in future research initiatives. Materials and
    methods Concept of the animal-borne soundscape logger The logger has a sound classification
    function, environmental measurement function, behavior measurement function, GPS
    capabilities, and a data transmission function (Fig. 1). Precisely when attached
    to a surfacing animals such as sea turtles seals, and sharks, the logger records
    and classifies sound data, measures environmental data such as depth, temperature,
    salinity, and dissolved oxygen, and behavior data, including acceleration and
    geomagnetism, while the animal is underwater. Upon surfacing, the turtle''s position
    is recorded using GPS, and the sound classification results and sensor measurement
    data are transmitted to the cloud (online server) via radio waves. Figure 1 Conceptual
    diagram of the animal-borne soundscape logger. The logger, attached to a sea turtle,
    (1) records and classifies sound while recording environmental data such as depth,
    temperature, salinity, and dissolved oxygen, as well as behavior data, including
    acceleration and geomagnetism, while the turtle is underwater. (2) Upon surfacing,
    the logger records the turtle’s position using GPS and transmits sound classification
    results and sensor measurement data to the cloud via cellular or satellite communication.
    (3) The data transmitted to the cloud is stored on a server and can be accessed
    through a web-based system. Full size image Two methods of data transmission from
    the sea are considered: cellular network communication and satellite communication,
    such as Argos25 and Iridium39. While satellite communication employs radio waves
    to transmit data via satellites, cellular networks use radio waves for ground-based
    tower-to-device communication. The data sent to the cloud can be accessed through
    a web-based system. In this study, we primarily utilized cellular networks available
    in the coastal regions of Japan for monitoring sea turtles nesting on the shore.
    Recognizing, however, that marine animals often venture into offshore areas beyond
    the reach of cellular networks, we also developed a logger variant capable of
    Argos satellite communication. This variant, which uses radio waves for satellite
    data transmission, is designed for use in open ocean regions and will be independently
    validated in future studies. Importantly, the Argos satellite communication system
    enables data collection even during the post-nesting migration of sea turtles,
    ensuring continuous monitoring as they move into open oceanic waters. Although
    Argos communication was not employed in this specific study, our approach integrates
    both cellular and satellite communication methods—both utilizing radio waves—to
    showcase a comprehensive strategy for marine animal monitoring. The classified
    sound types obtained, in this case, can be combined with location data to illustrate
    where and what kinds of sounds were observed, essentially creating a “soundscape
    map” and a histogram of sound types in that marine area. While location data is
    only recorded when the animals, such as sea turtles, surface for breathing40,
    a time-lag occurs since sound classification data are collected underwater. To
    address this challenge, sounds classified within a certain time before and after
    the location data acquisition are considered to be associated with that specific
    location, creating an integrated dataset of sound types and positions. While the
    current methodology of our study does not encompass the use of inertial navigation
    data from sensors like speed and direction sensors for underwater positioning41,
    integrating such data could potentially enable more precise correlations between
    classified sounds and their specific locations. Furthermore, by combining this
    position-associated sound type data with environmental conditions like depth,
    temperature, salinity, and dissolved oxygen, as well as behavior information derived
    from metrics like acceleration, it becomes possible to simultaneously understand
    under what conditions these sounds were observed. Development of the deep learning
    analysis of sound classification Target underwater sounds Given the prevalence
    of biological, environmental, and anthropogenic sounds below 20 kHz2,3,42, this
    study focused on sounds ranging from 60 Hz to 20 kHz, excluding clicks from small
    cetaceans above 20 kHz. This study selected 52 sound classes consisting of biological,
    geographical, and anthropogenic sounds as target sounds to classify them at three
    levels of detail. The levels range from primary classification (Level 1) of biophony,
    geophony, or anthrophony, to more detailed classification (Level 2), such as distinguishing
    fish sounds from marine mammal sounds within biophony or vessel sounds from construction
    sounds within anthrophony. The most detailed classification (Level 3) specifies
    animal species, call types within the same species, and vessel types (ferry, tanker,
    pleasure boat, etc.). A library of the 52 sounds (51 sound classes plus background
    noise) for constructing a deep learning model was compiled, with details of the
    52 sounds provided in Supplementary Table S1. The recording methodology for the
    target sounds is detailed in Tanaka et al. (unpublished data), and a database
    of sound sources is being developed in our project. To briefly explain, the sound
    source library was created using video and acoustic recorders deployed in the
    field at Sekisei Lagoon around Yaeyama Islands and other marine areas and in tanks
    housing individual organisms. Previously recorded data in other research were
    also incorporated. It should be noted that for distant sounds, direct observation
    of the emitting species is not always possible. However, the calls of whales and
    dolphins, for example, have been reported in various studies43,44. Experts in
    marine mammal acoustics have identified these sounds based on spectrograms, observations,
    habitat ranges, and characteristics of sounds from prior research. For fish sounds,
    recordings were made in the field with stationary cameras. As not all species
    known to produce sounds were captured on video, the observed organisms were captured
    and their sound production was confirmed in tanks, thereby identifying the sounds.
    When recording in tanks, we ensured that only the intended organism was present,
    allowing us to confidently attribute the sound to that organism. Recording techniques
    to avoid tank reverberation involve considerations of tank size, microphone placement,
    and environment setup, with details available in Akamatsu et al.45. This study
    did not account for overlaps in sound sources. The library excludes recordings
    with overlapping sounds, and a short time window of 2 s was adopted (details of
    sound processing are described later) to minimize potential overlap. It is important
    to note that the dataset includes a variety of sound types, ranging from continuous
    sounds such as those from ships to short, pulse-like sounds typical of fish. This
    diversity is considered in the analysis despite the exclusion of overlapping sounds.
    Tank experiments were conducted at Churaumi Aquarium, Hakkeijima Sea Paradise,
    and in tanks established at a fishery cooperative on Ishigaki Island. Acoustic
    recorders employed included AUSOMS Mini, and AUSOMS ver. 3 (Aqua Sound Inc., Kobe,
    Japan), and LoggLaw CAM (Biologging Solutions Inc., Kyoto, Japan). A GoPro Hero
    7 video camera (GoPro Inc., San Mateo, CA, USA) was concurrently used alongside
    the acoustic recorder, except when using the LoggLaw CAM, which recorded both
    underwater sound and video as a single unit. Sound classification algorithm In
    this study, we focused on sound sources ranging from 60 Hz to 20 kHz, with the
    objective of classifying these sound sources using loggers attached to sea turtles.
    The hydrophone used featured an omnidirectional design with a flat sensitivity
    response from 1 Hz to 100 kHz. Audio data was captured at a sampling rate of 44.1
    kHz and a 16-bit resolution. Detailed specifications and setup information are
    provided in the “Developed soundscape logger” section. This equipment facilitated
    the development of a specialized algorithm for sound source classification within
    the specified frequency range. Given adequate computational resources, complex
    deep learning algorithms, such as YOLO46 and Mask R-CNN47 for object detection
    or audio separation algorithms like TasNet48, can be utilized. However, for the
    practical implementation of a biologging device, the device size should be minimized
    to facilitate animal attachment, and the device should operate solely on battery
    power without an external supply. Thus, the classification algorithm needs to
    be computationally efficient. After conducting extensive preliminary trials, we
    employed a simple two-step process (Fig. 2) for sound source classification. Specifically,
    the process involved pre-processing the raw sound to generate a spectrogram image,
    followed by image classification using a deep learning model. This two-step method
    aligns with the approach described by Tanaka et al. (unpublished data), and detailed
    algorithmic background is described in that study. Here, we further optimized
    the signal processing for the biologging device, which is described in detail
    in the subsequent sections. Figure 2 Overview of the sound classification algorithm.
    The sound classification algorithm follows a simple two-step process: pre-processing
    the raw sound to generate a spectrogram image and subsequently using a deep learning
    model to classify the spectrogram image. Full size image Selection of processing
    unit While the logger relies exclusively on battery power, the battery size must
    be constrained to facilitate attachment to a sea turtle. Moreover, as deep learning
    models typically undergo retraining and updating upon data revision, loggers should
    be designed to accommodate continuous model updates. Potential computing units
    for executing deep learning models include CPUs (microcontrollers), GPUs (Graphics
    Processing Units), FPGAs (Field-Programmable Gate Arrays), and specialized AI
    chips (e.g., https://www.coral.ai/). Upon comparison, CPUs were deemed appropriate
    for this application due to their relatively low power consumption and ease of
    model updating, despite the drawback of extended processing times. We opted for
    a two-step approach for the sound classification algorithm consisting of sound
    pre-processing and deep learning. To maximize power efficiency in a biologging
    device, executing the classification process using the microcontroller’s internal
    memory and RAM (Random Access Memory) rather than relying on external memory or
    RAM is preferable. Employing external memory or RAM necessitates power and time
    for data input/output, expands the physical space required for component installation,
    and increases standby power consumption. Thus, we selected a microcontroller with
    ample internal memory and RAM to run deep learning models in-house and with relatively
    low power consumption (STM32 H7 series from STMicroelectronics Inc, Geneva, Switzerland,
    featuring up to 1 Mbyte of internal Flash memory and 512 Kbytes of RAM as a stand-alone
    computation storage area). This microcontroller was among those with extensive
    internal memory and RAM available when the logger was designed (December 2021).
    Implementation method in microcontroller Google’s TensorFlow Lite (https://www.tensorflow.org/lite)
    is a framework for executing deep learning on microcontrollers and other embedded
    devices. Among TensorFlow Lite’s models is MobileNet49, an efficient image classification
    model lightweight enough to implement on mobile devices with constrained computing
    resources and battery power. A preliminary study proposed two enhanced versions
    of MobileNet, ver. 2, and ver. 3, following the initial ver. 1 release. However,
    since ver. 2 and ver. 3 required excessive memory and RAM for the selected microcontroller,
    rendering them unsuitable for STM32 H7 series microcontroller implementation,
    we opted for ver. 1. MobileNet employs two key variables: input image size and
    width multiplier. The width multiplier is a hyperparameter that scales down the
    number of channels in each layer, effectively controlling the network’s ‘width’.
    This reduction in channels decreases the model''s computational cost and size,
    enhancing efficiency, especially for mobile or resource-limited environments.
    Similarly, a smaller input image size requires less memory and RAM for processing.
    It’s important to note that while reducing the width multiplier (with a maximum
    of 1.0) and the input image size lessens memory and computational demands, it
    may impact the model’s accuracy. However, to perform image classification using
    deep learning on a microcontroller, the memory and RAM consumed by the entire
    process, from spectrogram calculation to actual classification by the deep learning
    model, must fit within the microcontroller''s memory and RAM capacity. We utilized
    X-Cube AI from STMicroelectronics Inc. (https://www.st.com/en/embedded-software/x-cube-ai.html)
    to verify which settings can be implemented in the microcontroller when the input
    size and width multiplier of the image are altered. The models employed for verification
    were sample models downloaded from TensorFlow Hub (https://tfhub.dev/). All models
    used were fully quantized 8-bit models. Validation results indicated that width
    multipliers of 0.25 and 0.50 could be implemented for image sizes 128, 160, and
    192, while a width multiplier of 0.25 could be implemented for image size 224
    (Fig. 3a). Figure 3 Flash and RAM size and processing time simulation. (a) Flash
    size (Mbyte) and RAM size (Kbyte) consumed during microcontroller execution for
    varying input image sizes (128 × 128, 160 × 160, 192 × 192, and 224 × 224 pixels,
    indicated by colors) and different width multipliers (0.25, 0.50, and 1.00, indicated
    by the number next to the point on the polyline) of MobileNet ver. 1. This data,
    reflecting the resource consumptions, is a recalculation in our environment, referencing
    the content described in the following URL (https://github.com/EEESlab/mobilenet_v1_stm32_cmsis_nn).
    (b) Processing time (Y-axis) for running MobileNet ver. 1 on the STM32H7 microcontroller
    at its maximum clock speed of 480 MHz under the conditions of input image size
    (128 × 128, 160 × 160, 192 × 192, and 224 × 224 pixels, indicated by colors) and
    width multiplier (x-axis) compatible with the STM32H7 microcontroller. Full size
    image Additionally, for continuous sound classification, one image is generated
    for every 2 s of sound data, as detailed in subsequent sections, requiring classification
    to be completed within a maximum of 2 s for timely processing. Therefore, we evaluated
    the time consumption of MobileNet conditions with varying width multipliers and
    input image sizes, executable on the previously mentioned microcontroller, using
    IAR Embedded Workbench (IAR Systems, Uppsala, Sweden). Our findings indicate that
    with the STM32 H7 microcontroller operating at 480 MHz, its maximum clock speed,
    processing can be completed within one second (Fig. 3b). Although the verification
    mentioned above has identified the MobileNet image size and width multiplier suitable
    for microcontroller execution, pre-processing must also be considered. For instance,
    in a PC with ample computing resources, the following processing has proven effective
    in preliminary studies for pre-processing (Tanaka et al., unpublished data). For
    sound data pre-processing, a spectrogram is computed using 1024 points for the
    short-time Fourier transform (STFT) with a 0.9 overlap ratio for 10 s of input
    data. The calculations are performed using Fast Fourier Transform (FFT). Notably,
    the frequency axis of the spectrogram is represented on a logarithmic scale, enhancing
    the detail in lower frequencies and more effectively covering the broad frequency
    range, compared to a linear scale. To reduce stationary background noise, a median
    filter is applied directly to the 10-s spectrogram data. In this process, the
    median value is determined for each data point, effectively smoothing the signal
    and reducing noise. This technique is commonly used in signal processing to preserve
    sharp signal features while removing noise, making it particularly suitable for
    enhancing the clarity of spectrogram data. Subsequently, for sound classification,
    a 2-s time window is used to analyze the audio data. This window slides forward
    by 1 s for each step, overlapping with half of the previous window. As a result,
    for every 10 s of audio data, we generate nine overlapping segments, each 2 s
    in length. From each of these segments, an image is created, resulting in a total
    of nine images for each 10-s interval. However, performing the same pre-processing
    on a microcontroller was challenging due to memory and RAM size limitations. Thus,
    we investigated how altering the input sound’s time window length and the FFT
    overlap ratio would impact the system’s feasibility on a microcontroller. Moreover,
    to minimize the memory and RAM requirements for pre-processing within the microcontroller,
    we opted to compute in single precision (32-bit floating point format) instead
    of double precision (64-bit floating point format). Single precision offers sufficient
    accuracy for our needs while using less computational resources compared to double
    precision, which provides higher numerical accuracy but at a greater memory cost.
    Longer input time window lengths for the sound data, such as 10 s, make it more
    probable for the median filter to separate the target sound from stationary background
    noise for sounds occurring within a relatively short duration (e.g., within 1
    s). In our study, we chose a 0.5 FFT overlap ratio to address the limitations
    of the microcontroller’s RAM. The window length for each FFT bin is fixed at 1024,
    maintaining a consistent temporal resolution. While a higher overlap ratio like
    0.9 would spread the energy over more bins, leading to a smoother appearance in
    the spectrogram, it significantly increases the data volume. This increase in
    data volume, as demonstrated in Supplementary Table S2, is not feasible within
    our hardware constraints. Therefore, we established a 5-s input sound time window
    length and a 0.5 FFT overlap ratio for the logger, striking a balance between
    the requirements of our data analysis and the memory capacity of the hardware.
    Based on the abovementioned validation, we further simulated memory and RAM consumption
    from sound data input to pre-processing and MobileNet execution. We found that
    the deep learning model can be executed on a microcontroller with a 5-s window,
    a 0.5 FFT overlap ratio, a 192 × 192 image size, and a 0.25 width multiplier.
    In this setup, the 192 × 192 image size corresponds to a 2-s audio spectrogram,
    where each pixel on the horizontal axis represents 0.015 s (2 s ÷ 192 pixels).
    Vertically, the range of 60 Hz–20 kHz is displayed on a logarithmic scale across
    192 pixels, with each pixel representing a different frequency range that expands
    at higher frequencies and contracts at lower frequencies. Additionally, the use
    of square images aligns with the MobileNet model’s design for square inputs, allowing
    us to fully utilize its architecture and pre-trained weights. This pre-processing
    algorithm was employed to process a library of 52 sound sources (Supplementary
    Table S1) and generate spectrogram images for constructing a deep learning model.
    It is important to note that while the study categorized these sounds into three
    levels of detail, the algorithm was trained exclusively on the most detailed classification,
    Level 3. This level includes the specification of animal species, call types within
    the same species, and vessel types, etc. By focusing on this granular level, the
    model inherently learns to distinguish the broader categories of Level 1 (biophony,
    geophony, anthrophony) and Level 2 (subcategories within biophony and anthrophony).
    Therefore, classification of Level 3 allows for an implicit classification of
    the higher-level categories (Level 1 and Level 2), ensuring a comprehensive classification
    across all levels of detail. Evaluation of sound classification performance A
    sound dataset comprising a total of 5999 samples was used in this study, with
    the detailed number of samples for each class provided in Supplement 1. Of the
    total sound data, 70% was utilized as training data and 30% as test data. The
    training data was subsequently used to create a TensorFlow model for classifying
    soundscape image data using MobileNet ver. 1 transfer learning. The model was
    converted to a TensorFlowLite model and fully quantized, making all weights and
    activation outputs 8-bit integer data. The pre-processing was programmed in C
    (https://www.gnu.org/software/gnu-c-manual/gnu-c-manual.html) on a PC, and the
    same code was run on the microcontroller to ensure consistency in the image data
    calculated by the pre-processing. The program for creating the TensorFlowLite
    model was developed on Google Colab (https://colab.research.google.com/) using
    Python (https://docs.python.org/3/reference/index.html). The accuracy of the classification
    results was evaluated using the following metrics. $$Precision = \\frac{TP}{{TP
    + FP}}$$ $$Recall = \\frac{TP}{{TP + FN}}$$ $$F{ - }measure = \\frac{2Recall \\cdot
    Precision}{{Recall + Precision}}$$ $$Accuracy = \\frac{TP + TN}{{TP + FP + FN
    + TN}}$$ TP stands for True Positive, FP for False Positive, TN for True Negative,
    and FN for False Negative. The precision represents the percentage of predicted
    positive data that are positive. The recall represents the percentage of truly
    positive data correctly predicted as positive. The F-measure, calculated as the
    harmonic mean, is an evaluation metric to assess the balance between Precision
    and Recall. Additionally, machine learning algorithms commonly provide an accuracy
    metric, calculated as the ratio of correct predictions to the total number of
    data points. In image classification, accuracy is assessed in two scenarios: when
    the model’s most probable classification label (Top1) matches the correct answer
    and when the correct answer is among the top five most probable classification
    labels predicted by the model (Top5). Therefore, accuracy was evaluated in both
    cases to provide a comprehensive performance evaluation. To present the results
    of multiple classification classes, a Confusion Matrix was employed. Data transmission
    In the realm of mobile telecommunications, 4G, particularly LTE (Long Term Evolution),
    is globally prevalent50. LTE is a standard for wireless broadband communication,
    providing enhanced speeds and network capacity. Our research primarily concentrated
    on LTE-M (Long Term Evolution for Machines)51, a novel communication standard
    recently introduced for the Internet of Things (IoT), also known as CAT M1 (Category
    M1). LTE-M is known for its enhanced data communication efficiency and power-saving
    capabilities compared to LTE. The LTE-M standard specifies a maximum data rate
    of 1 Mbps, and for this study, we utilized a communication module capable of transmitting
    a maximum of 560 bytes per packet. Data transmission is accomplished through LTE-M,
    along with MQTT (Message Queuing Telemetry Transport)52, a lightweight messaging
    protocol. The transmitted information is then processed by Amazon Web Services
    (AWS), a cloud computing platform provided by Amazon.com, Inc. (Seattle, USA).
    The data first passes through the AWS IoT Hub for initial handling, is decoded
    using AWS Lambda, a serverless computing service, and ultimately stored in Amazon
    DynamoDB, a NoSQL database service, all of which are part of the comprehensive
    suite of services offered by AWS. We selected a cellular network provided by NTT
    Docomo Inc. (Tokyo, Japan) for this research, which enables communication from
    the island coast to approximately 20 km offshore in the experimental sea area
    of the Yaeyama Islands (https://www.docomo.ne.jp/area/). Testing the logger on
    sea turtles Developed soundscape logger The developed data logger measures 160
    mm in length, 100 mm in width, and 85 mm in height, excluding a connector and
    hydrophone. It weighs 2 kg in the air and approximately 0.6 kg in the water (Fig.
    4a). It is equipped with a 3-axis accelerometer, a 3-axis magnetometer, a depth
    sensor, a temperature sensor, and a GPS. The accelerometer, magnetometer, depth
    sensor, and temperature sensor record data every 60 s, while the GPS operates
    similarly to the FastLoc GPS40, which is widely utilized in marine animal tracking
    and enables fast positioning when sea turtles surface. In a preliminary accuracy
    evaluation test conducted with sea turtles in a 10 m × 10 m outdoor tank, we obtained
    a total of 1500 location data points from three loggers used in the test. The
    root mean square error (RMSE) for these data points, representing the deviation
    from the turtles'' nearly fixed latitude and longitude within the tank, was 32.04 ± 26.8
    m. An AS-1 hydrophone (Aquarian Hydrophones, WA, USA) was utilized, featuring
    an omnidirectional design with a flat sensitivity response of − 208 (± 2) dB re:
    V/μPa from 1 Hz to 100 kHz. Audio data was sampled at 44.1 kHz with a 16-bit resolution,
    and recordings were made at 30-min intervals for 5 min (300 s) in a duty cycle.
    The 5-min audio data were subsequently processed internally within the logger
    as follows. 1. Divide the 300-s data into sixty 5-s segments. 2. Compute a spectrogram
    (with 0.5 FFT overlap) for each 5-s segment and apply a median filter. 3. Overlap
    and slide the resulting 5-s spectrogram data by 1 s within a 2-s time window,
    generating four images (192 × 192 pixels) per 2-s time window. 4. Perform image
    classification on each image using the pre-built TensorFlow Lite MobileNet ver.
    1 model, with the outcome as the audio classification result. Figure 4 Soundscape
    logger and the deployment to a sea turtle. (a) A photograph illustrating the developed
    data logger, and (b) the data logger deployed on a green sea turtle (Chelonia
    mydas). Full size image The processing flow described above generates 240 images
    from the 5-min data, with each image representing a distinct sound, yielding 240
    classification results, one for each sound. Although it is possible to transmit
    all classification results if continuous stable communication is available, when
    attached to a sea turtle, communication can only occur when the turtle surfaces
    to breathe and the logger, affixed to its carapace, is exposed to the air. Therefore,
    to reduce the data size, summarized data were sent. Various data summarization
    methods could be considered, but in this case, we assumed that the probability
    of each class from the 240 classification results was averaged over the 240. The
    class with the highest probability in its top three was used as the data for a
    single sound classification result. This summary is considered the most dominant
    sound classification result within the 5-min interval. One set of data for a single
    classification result was further combined with depth, temperature, vectorized
    acceleration (the square root of the sum of the squares of the three-axis acceleration
    components), and vectorized magnetic data (the square root of the sum of the squares
    of the three-axis magnetic components) measured at the beginning of the 5 min
    during the sound measurement. The vectorized acceleration and magnetic data, along
    with the depth and temperature, are crucial for understanding the activity level53
    and geolocation54 of the animal. These metrics are essential for comprehensively
    monitoring the animal''s behavior and movements, and therefore, are measured and
    transmitted along with the sound classification data. The logger also has a saltwater
    switch to detect when a sea turtle surfaces, allowing for GPS measurements and
    data transmission via LTE-M. GPS measurements are taken each time a sea turtle
    surfaces. Each transmission packet can store up to 13 classification or position
    values. The logger initiates transmission each time it becomes feasible, prioritizing
    previously unsent data chronologically. If three consecutive transmission attempts
    fail, the data is discarded, and the subsequent new data is transmitted. Although
    the saltwater switch enables the logger to prepare for transmission, it requires
    at least 6–10 s for the LTE-M transmission function to activate and establish
    communication. Transmission may fail due to sea turtle behavior or wave conditions.
    As green turtles remain on the surface for approximately 10 s per breath (see,
    for example, the post-dive surfacing duration in Okuyama et al.55), it was considered
    achievable for communication to occur despite the relatively long activation time.
    With the above settings, the logger is equipped with a 30,000 mAh lithium polymer
    battery. It is designed to operate for 40 consecutive days under specific conditions:
    the accelerometer, magnetometer, depth sensor, and temperature sensor record data
    every 60 s, and recordings are made for 5 min at 30-min intervals in a duty cycle.
    This estimate assumes no transmission, as the transmission frequency and power
    consumption greatly depend on the sea turtle’s breathing rate and the local signal
    environment. Since the timing of sound classification and GPS positioning differ,
    the most recent GPS location within an hour before or after the sound classification
    was used as the location where the sound classification occurred. If the loggers
    can be retrieved directly, the unaggregated classification results and all GPS
    position data, 3-axis accelerometer, 3-axis magnetometer, depth sensor, and temperature
    sensor data can be downloaded. Field trials using sea turtles To validate the
    practical application of the developed loggers for field data collection, they
    were attached to nesting green sea turtles (Chelonia mydas). Surveys were conducted
    at Ibaruma Beach, Ishigaki Island, Okinawa Prefecture, Japan (24°20′ N, 123°50′
    E) during the summer of 2022. This site has been continuously monitored in previous
    studies56, and the logger attachment procedures were consistent with those used
    in past research (Okuyama et al., unpublished data). Briefly detailing the process,
    after they finished oviposition, we measured the straight carapace length and
    width (SCL and SCW) and the body weight, cleaned the carapace with sandpaper,
    and attached the tags securely to the carapace using epoxy resin (Konishi Co.,
    Ltd. Osaka, Japan) and fiberglass cloth (Tokyo Garasu Kikai Co., Ltd. Tokyo, Japan)
    (Fig. 4b). After the resin had completely dried, the turtles were released into
    the sea. Sea turtles that come ashore to lay eggs do so multiple times within
    a single nesting season, allowing for direct logger collection. The loggers were
    designed to transmit data via LTE-M communication when the turtles surfaced; however,
    since not all data could be transmitted, logger retrieval was conducted whenever
    possible during subsequent nesting events. Despite the relative heft of our loggers,
    which were 2 kg in air, their weight comprised less than 3% of the average body
    weight of the nesting green sea turtles we studied. These turtles, mature females
    coming ashore for nesting, had an average straight carapace length (SCL) of about
    100 cm, aligning with previous studies on the same nesting beach that reported
    an average weight of 131 kg (based on measurements of 120–153 kg, average 131
    kg, N = 6; Okuyama et al., unpublished data and Obe et al.57). This proportion
    falls comfortably within the traditionally recommended limit of 3–5% of body weight
    for attachments58, a guideline designed to minimize behavioral impacts. We took
    care to select turtles without limb injuries or deformities for logger attachment,
    and all loggers were successfully retrieved after observing the turtles’ normal
    nesting behavior on their return. The loggers and epoxy were completely removed
    at the time of recovery. Analyses were performed regarding data recovery rates,
    the composition of acquired sound species, and the relationships among sound species,
    location, and depth data (acceleration, magnetism, and temperature data were not
    analyzed in this study). The data recording period was calculated from the difference
    between the first and last recording date and time of the data recovered from
    the internal memory of the recaptured loggers. For sound classification results
    combined with location data that yielded a relatively large number of points (e.g.,
    more than 10 points) per sound source (e.g., Giant moray Gymnothorax javanicus;
    see “Results” for details), a kernel density distribution (a nonparametric method
    used to estimate data distribution) was calculated as an example of how sound
    species distribution (soundscape maps) could be represented. All analyses were
    performed using Python. Ethics statement The study was carried out in compliance
    with the ARRIVE guidelines. All turtle handling procedures were conducted under
    special supplemental permits obtained from the Ministry of the Environment and
    Okinawa Prefecture (Ministry of the Environment permit number 2207282, Okinawa
    Prefecture permit number 4-2), and ethics approval from the Animal Experimentation
    Committee of the Fisheries Technology Institute (YA2022-03). Results Sound classification
    performance A fully quantized TensorFlowLite model, based on the MobileNet ver.
    1, was developed using the training image data, resulting in a 370 kByte model
    that could fit within the RAM size of the microcontroller. Simulations with the
    created model demonstrated Top1 accuracy rates of 91.3% for Level 1, 86.7% for
    Level 2, and 62.3% for Level 3. The accuracy rates for the Top 5 model were 97.1%
    for Level 1, 96.3% for Level 2, and 86.3% for Level 3. Recall, Precision, and
    F-measures are presented in Table 1. To provide a comprehensive assessment of
    our model’s performance across all classes, the confusion matrices for each classification
    level (Level 1, Level 2, Level 3) are illustrated in Supplementary Figs. S1, S2,
    and S3, offering insights into the model’s accuracy for each category. Table 1
    Classification results for 52 sound types using the TensorFlowLite model. Full
    size table Field trials using sea turtles In total, data for more than 110 h were
    obtained from seven individuals (Table 2). Although two out of the seven individuals
    experienced communication problems with the loggers, preventing any data recovery
    via LTE-M communication, a considerable amount of sound classification and sensor
    measurement data (153 points) and position data (778 points) were obtained from
    the other five turtles through LTE-M communication (Table 2). Furthermore, data
    were successfully retrieved from all seven individuals upon recapture. With an
    average measurement period of less than one day (Table 2), the expected continuous
    operation in terms of power was not attained; however, the recovery of sound classification
    results, sensor measurement results, and location data was achieved. We could
    recover sound classification and sensor measurement results (240 points) and position
    data (1092 points) for the seven directly recovered individuals. Among the five
    individuals with at least one successful LTE-M communication, 91.6% of the measured
    data for sound classification results and sensor measurements, and 95.3% for location
    data, were retrieved through LTE-M communication. Table 2 Size of sea turtles
    fitted with loggers, duration of data recording obtained from the attached loggers,
    number of data points recovered via LTE-M communication, and number of data points
    retrieved directly from the logger’s internal memory. Full size table The relationship
    between depth, location, and classification results was summarized using data
    from the loggers of seven directly collected individuals. The percentage of sounds
    classified as background noise was 76.4 ± 15.8%, accounting for more than 70%
    of the total. Since all data from ID4 was background noise, this individual was
    excluded from the subsequent analysis. The following analysis is based on the
    results from the remaining six individuals. The composition of the sound classification
    results is displayed in Fig. 5. The composition for the six individuals was primarily
    composed of the Giant moray Gymnothorax javanicus (39.0 ± 40.2%) and the short-finned
    pilot whales Globicephala macrorhynchus (33.5 ± 22.7%), which had higher average
    percentages among the six individuals. Blue damselfish Chrysipitera cyanea was
    also observed in the composition. Although short-finned pilot whales are present
    in the field waters, the relatively high frequency of observations at shallow
    depths may suggest potential inaccuracies in the data. In the case of coral reef
    areas near the sea surface, the influence of currents and waves can also be assumed.
    Examining the sea turtle depth data, 73.7% of the sound observations were made
    when the experienced depth was shallower than 2 m (Supplementary Fig. S4). However,
    it is unclear whether the sea turtles were on the bottom or not due to the lack
    of data on the exact water depth at the site; we only extracted data when the
    experienced depth was 2 m or deeper, as currents and waves on the sea surface
    in coral reef areas close to the surface may affect errors in sound classification.
    In this case, the number of short-finned pilot whales substantially decreased
    from 17 to 3, while the number of Giant morays remained almost the same, decreasing
    from 11 to 10. By combining the location data and sound classification results
    with the data extracted only at depths greater than 2 m, a soundscape map could
    be generated (Fig. 6). For the Giant moray, the kernel density distribution (Supplementary
    Fig. S5) was drawn from the observation points as an example. Figure 5 Sound source
    classifications from the field observations. Composition of sound source classifications
    obtained from each individual (6 individuals). This figure represents data collected
    at various depths. Further explanation regarding the use of depth-related data
    is provided in the “Results” section. Full size image Figure 6 Soundscape map.
    Soundscape map: map of classified sounds and observed positions obtained from
    loggers (six individuals). This map primarily displays biophony, as no data were
    available for anthrophony and geophony when combined with the location data. Full
    size image Discussions The logger developed in this study is considered the world''s
    first biologging device system that employs deep learning for sound source classification
    and transmits the data when organisms surface. This innovative approach has the
    potential to greatly enhance our understanding of marine ecosystems by providing
    detailed insights into the acoustic activities of various organisms. With the
    integration of positional data, the capability to generate a kernel density distribution
    has been suggested by the example of the Giant moray in the results. However,
    we recognize that these representations are preliminary and require further validation
    to confirm their accuracy in depicting organism distributions. The implications
    of this technology extend to various aspects of biophony, anthrophony, and geophony.
    Moving forward, it is expected that multi-point and long-term observations through
    biologging are expected to facilitate the creation of detailed soundscape maps
    that accurately reflect the distribution of various sound sources. Our study demonstrates
    that animals can serve as autonomous mobile platforms for collecting soundscape
    information in addition to oceanographic data. Real-time monitoring of soundscapes,
    including source-specific information, can enhance our understanding of sound
    diversity and enable more targeted actions when addressing conflicts between animals
    and human activities. The system is expected to be helpful in areas where buoy
    installation is challenging. A similar approach involving gliders has been employed19,20,
    but gliders are often challenging to operate in shallow coastal areas. Previous
    studies aimed to detect particular marine mammal sounds, such as those from whales
    and dolphins, rather than identifying multiple sound sources in biophony, anthrophony,
    and geophony, as in this study. An example of sea turtles being used for sound
    observations exists59; however, the study focused on measuring sea turtles'' escape
    behavior in response to ship sounds rather than soundscape observations. In this
    study, a microcontroller was selected as the hardware for sound processing. Implementing
    the same pre-processing and deep learning model on a microcontroller as on a PC
    with ample computational resources is challenging. However, we demonstrated that
    a biologging device could be implemented by reducing the complexity of the model
    and pre-processing. The results for the level 3 classification showed that although
    the Top 1 accuracy is not high at approximately 60%, relatively high accuracy
    can be achieved by considering the Top 5 accuracy (more than 85%). The execution
    of pre-processing and models depends on the microcontroller’s performance. As
    the demand for AI at the edge continues to grow27, low-power-consumption microcontrollers
    with edge processing capabilities are being developed to meet this demand60. Consequently,
    more significant amounts of RAM and memory can be installed in microcontrollers,
    and more energy-efficient microcontrollers will become available in the future.
    At that point, it will be possible to further increase the pre-processing time
    window length and FFT overlap ratio using the present method. Additionally, deep
    learning models will be able to incorporate more complex, higher-accuracy models.
    One advantage of utilizing a microcontroller in this study is the updatability
    of the deep learning model. Although the current model was constructed using 52
    sound classes, in the future, as the number of sound sources and samples increases
    and a retrained model is developed, the model can be easily replaced. The fully
    quantized 8-bit model allows the current system to handle up to 256 sound classes.
    A dedicated AI chip could be employed for embedded applications; however, it would
    be costly to develop and require designing the chip to facilitate model replacement.
    Regarding power-saving, more efficient methods can be considered. This study conducted
    sound observations for 5 min every 30 min. However, sea turtle behavior also influences
    the sounds observed during those 5 min. For example, sounds were recorded when
    sea turtles were at or near the surface (e.g., shallower than 2 m as in this study).
    During such times, the sound of the sea turtles'' breathing and wave or current
    noise hinder accurate monitoring of the environmental sounds composing the soundscape.
    Therefore, while it is difficult to determine whether the sea turtles were on
    the bottom due to the lack of data on the exact water depth at the site, excluding
    recordings at shallow experienced depths would allow for avoiding unnecessary
    data collection. In the future, it is essential to consider how to summarize and
    transmit the sound classification results obtained from the deep learning model.
    For depth and temperature data, previous studies26,61 have effectively reduced
    the amount of transmitted data by summarizing the behavior of target organisms.
    In this study, we calculated the average probability for each class based on 5
    min of continuous classification and decided to transmit the top three classifications.
    Although the transmission design did not include the Top 5 data for each image
    classification, using the average probability of continuous classification over
    5 min allows for obtaining classification results with a relatively high probability
    during that timeframe. This approach enables the assessment of the sounds comprising
    the average soundscape over 5 min but sounds that infrequently occur during the
    period would receive lower rankings and may not be captured. It should be noted
    that our system may face challenges in accurately classifying sounds in two scenarios:
    when multiple sound sources overlap in a single spectrogram and when encountering
    sounds not present in the training database. This study made an effort to minimize
    the impact of overlapping sound sources by adopting a short time window of 2 s
    for sound recording; however, it should be noted that cases with overlapping sounds
    were not explicitly accounted for, as the sound library excludes recordings with
    such overlaps. When the system encounters either sounds not present in the training
    database or potential overlaps in the spectrogram, these sounds are likely to
    be classified into one of the nearest matching classes we have, which could lead
    to misclassification. The system calculates the probability of each spectrogram
    belonging to every class, resulting in a probability distribution across all the
    classes for every individual image. Although this method provides valuable insights,
    it has limitations in distinguishing overlapped sounds or recognizing novel ones
    not included in the training set. Based on the classification results obtained
    during the field experiment, certain species commonly encountered by local fishermen
    (personal communication with Mr. Taira; this is not an experiment on human participants)
    and reported in literatures62,63,64 were observed, such as Moray eels and blue
    damselfish. However, the classification outcomes also showed certain biological
    sounds commonly detected with a higher frequency at shallow depths, such as pilot
    whale sound, thus highlighting the potential for misclassification. Although the
    sound classifier installed on the sea turtle logger is expected to capture a variety
    of sounds that would be experienced by sea turtles, sounds associated with sea
    turtle behavior, including breathing, swimming, and the noise generated from interactions
    with natural structures, were not explicitly included in the sound classification
    model used in this study. Consequently, such sounds may have been mistakenly classified
    as one of the 52 sound species. To address the limitation of our current sound
    classification model, which does not include sounds directly associated with sea
    turtle behavior, developing a dedicated sound library representing the unique
    acoustic characteristics of sea turtles is crucial. Furthermore, it is important
    to note that within our project, we employed a hydrophone-integrated video logger
    (LoggLaw CAM) in a separate experiment within our project. This device was attached
    to sea turtles not only to capture sounds directly associated with their behavior,
    but also to monitor the ambient acoustic environment, including sounds produced
    by other marine organisms. The results of this experiment, which will provide
    valuable insights into the acoustic characteristics of sea turtle behavior, will
    be detailed in a forthcoming publication. While uncertainties remain regarding
    the accuracy of field observations, the significance of this study lies in the
    successful remote collection of near-real-time sound classification data from
    sea turtles equipped with loggers incorporating edge processing and a deep learning
    model. In order to enhance the reliability of the classification system, it is
    essential to repeat the cycles of conducting field validation of classification
    performance, expanding the training data set, and developing a more robust model.
    Future plans include evaluating the reliability of data with playback experiments.
    However, the main focus of this research is the development of the hardware, pre-processing
    techniques, and algorithms, making a comprehensive reliability evaluation beyond
    the scope of this study. In practical applications, the updated model can be readily
    installed in the loggers, facilitating advancements in sound classification performances.
    As an important aspect of our research, the loggers used in this study were initial
    prototypes, specifically designed to validate our device concept. In this phase,
    we primarily utilized cellular networks available in the coastal regions of Japan
    for monitoring sea turtles nesting on the shore. Recognizing that marine animals
    often venture into offshore areas beyond the reach of cellular networks, we also
    developed a logger variant capable of Argos satellite communication. This new
    model, while retaining all the key specifications of the original design, weighs
    approximately half as much as the original. It’s worth noting that the outcomes
    and insights gained from the use of the Argos communication system will be reported
    separately, highlighting its efficacy and potential in further research applications.
    Importantly, the Argos satellite communication system enables data collection
    even during the post-nesting migration of sea turtles, ensuring continuous monitoring
    as they move into open oceanic waters. The current trend towards smaller and more
    energy-efficient electronic components, as well as high-performance microcontrollers
    or chips tailored for edge AI60,65, has created an environment where loggers can
    be further miniaturized. Additionally, these advancements make it feasible to
    equip loggers with more complex deep learning models for enhanced edge processing
    capabilities. These advancements not only enhance our research but also pave the
    way for new practical applications. Data availability Data are available from
    the Dryad Digital Repository (https://datadryad.org/stash/share/MN3MhpzQVaoed-vXJtbYL1eByqstEDL7FSbTRICxDTQ).
    References International Organization for Standardization (ISO). Underwater acoustics—Terminology
    (ISO 18405:2017, ISO, 2017). https://www.iso.org/standard/62406.html (2017). Duarte,
    C. M. et al. The soundscape of the Anthropocene ocean. Science 371, 6529. https://doi.org/10.1126/science.aba4658
    (2021). Article   CAS   Google Scholar   Montgomery, J. C. & Radford, C. A. Marine
    bioacoustics. Curr. Biol. 27, R502–R507 (2017). Article   CAS   PubMed   Google
    Scholar   Curtis, K. R., Howe, B. M. & Mercer, J. A. Low-frequency ambient sound
    in the North Pacific: Long time series observations. J. Acoust. Soc. Am. 106,
    3189–3200 (1999). Article   ADS   Google Scholar   Haxel, J. H., Dziak, R. P.
    & Matsumoto, H. Observations of shallow water marine ambient sound: The low frequency
    underwater soundscape of the central Oregon coast. J. Acoust. Soc. Am. 133, 2586–2596
    (2013). Article   ADS   PubMed   Google Scholar   Hildebrand, J. Anthropogenic
    and natural sources of ambient noise in the ocean. Mar. Ecol. Prog. Ser. 395,
    5–20 (2009). Article   ADS   Google Scholar   Lindseth, A. & Lobel, P. Underwater
    soundscape monitoring and fish bioacoustics: A review. Fishes 3, 36 (2018). Article   Google
    Scholar   Kaatz, I. M., Rice, A. N. & Lobel, P. S. How fishes use sound: Quiet
    to loud and simple to complex signaling. In Reference Module in Life Sciences
    (Elsevier, 2017). https://doi.org/10.1016/B978-0-12-809633-8.03083-1. Putland,
    R. L., Constantine, R. & Radford, C. A. Exploring spatial and temporal trends
    in the soundscape of an ecologically significant embayment. Sci. Rep. 7, 5713
    (2017). Article   ADS   CAS   PubMed   PubMed Central   Google Scholar   Richardson,
    W. J., Greene, C. R. Jr., Malme, C. I. & Thomson, D. H. Marine Mammals and Noise
    (Elsevier, 1995). https://doi.org/10.1016/C2009-0-02253-3. Book   Google Scholar   Erbe,
    C. et al. The effects of ship noise on marine mammals—a review. Front. Mar. Sci.
    2019, 6. https://doi.org/10.3389/fmars.2019.00606 (2019). Article   Google Scholar   Malakoff,
    D. A push for quieter ships. Science 328, 1502–1503. https://doi.org/10.1126/science.328.5985.1502
    (2010). Article   ADS   CAS   PubMed   Google Scholar   Kennedy, E. V., Holderied,
    M. W., Mair, J. M., Guzman, H. M. & Simpson, S. D. Spatial patterns in reef-generated
    noise relate to habitats and communities: Evidence from a Panamanian case study.
    J. Exp. Mar. Biol. Ecol. 395, 85–92 (2010). Article   Google Scholar   Tanaka,
    K. et al. Spatiotemporal variations in the acoustic presence of dugongs and vessel
    traffic around Talibong Island, Thailand: Inputs for local coastal management
    from passive acoustical aspects. Ocean Coast Manag. 245, 106810 (2023). Article   Google
    Scholar   Pieretti, N. & Danovaro, R. Acoustic indexes for marine biodiversity
    trends and ecosystem health: Acoustic indexes in marine environments. Philos.
    Trans. R. Soc. B: Biol. Sci. 375, 1814. https://doi.org/10.1098/rstb.2019.0447
    (2020). Article   Google Scholar   Mooney, T. A. et al. Listening forward: Approaching
    marine biodiversity assessments using acoustic methods: Acoustic diversity and
    biodiversity. R. Soc. Open Sci. 2020, 7. https://doi.org/10.1098/rsos.201287 (2020).
    Article   Google Scholar   McKenna, M. F. et al. Advancing the interpretation
    of shallow water marine soundscapes. Front. Mar. Sci. 8, 1456 (2021). Article   Google
    Scholar   Baumgartner, M. F. et al. Persistent near real-time passive acoustic
    monitoring for baleen whales from a moored buoy: System description and evaluation.
    Methods Ecol. Evol. 10, 1476–1489 (2019). Article   Google Scholar   Baumgartner,
    M. F. et al. Real-time reporting of baleen whale passive acoustic detections from
    ocean gliders. J. Acoust. Soc. Am. 134, 1814–1823 (2013). Article   ADS   PubMed   Google
    Scholar   Kowarski, K. A. et al. Near real-time marine mammal monitoring from
    gliders: Practical challenges, system development, and management implications.
    J. Acoust. Soc. Am. 148, 1215–1230 (2020). Article   ADS   PubMed   Google Scholar   McMahon,
    C. R. et al. Animal Borne Ocean Sensors—AniBOS—an essential component of the global
    ocean observing system. Front. Mar. Sci. 2021, 8. https://doi.org/10.3389/fmars.2021.751840
    (2021). Article   Google Scholar   Doi, T., Storto, A., Fukuoka, T., Suganuma,
    H. & Sato, K. Impacts of temperature measurements from sea turtles on seasonal
    prediction around the Arafura aea. Front. Mar. Sci. 2019, 6 (2019). Google Scholar   Miyazawa,
    Y. et al. Temperature profiling measurements by sea turtles improve ocean state
    estimation in the Kuroshio-Oyashio Confluence region. Ocean Dyn. 69, 267–282 (2019).
    Article   ADS   Google Scholar   Harcourt, R. et al. Animal-borne telemetry: An
    integral component of the ocean observing toolkit. Front. Mar. Sci. 2019, 6 (2019).
    Google Scholar   Boehme, L. et al. Technical note: Animal-Borne CTD-satellite
    relay data loggers for real-time oceanographic data collection. Ocean Sci 2009,
    5. www.ocean-sci.net/5/685/2009/ (2009). Fedak, M., Lovell, P., Mcconnell, B.
    & Hunter, C. Overcoming the Constraints of Long Range Radio Telemetry from Animals:
    Getting More Useful Data from Smaller Packages 1. vol. 42 http://www.cls.fr/html/
    (2002). Shi, W., Cao, J., Zhang, Q., Li, Y. & Xu, L. Edge computing: Vision and
    challenges. IEEE Internet Things J. 3, 637–646 (2016). Article   Google Scholar   Ichikawa,
    K. et al. Dugong (Dugong dugon) vocalization patterns recorded by automatic underwater
    sound monitoring systems. J. Acoust. Soc. Am. 119, 3726–3733 (2006). Article   ADS   PubMed   Google
    Scholar   Tanaka, K., Ichikawa, K., Kittiwattanawong, K., Arai, N. & Mitamura,
    H. Automated classification of dugong calls and tonal noise by combining contour
    and MFCC features. Acoust. Aust. 49, 385–394 (2021). Article   Google Scholar   Zhong,
    M. et al. Beluga whale acoustic signal classification using deep learning neural
    network models. J. Acoust. Soc. Am. 147, 1834–1841 (2020). Article   ADS   PubMed   Google
    Scholar   Demertzis, K., Iliadis, L. S. & Anezakis, V. D. Extreme deep learning
    in biosecurity: The case of machine hearing for marine species identification.
    J. Inf. Telecommun. 2, 492–510 (2018). Google Scholar   Bergler, C. et al. ORCA-SPOT:
    An automatic killer whale sound detection toolkit using deep learning. Sci. Rep.
    9, 145 (2019). Article   ADS   Google Scholar   Hu, G. et al. Deep learning methods
    for underwater target feature extraction and recognition. Comput. Intell. Neurosci.
    2018, 1–10 (2018). Article   ADS   Google Scholar   Purwins, H. et al. Deep learning
    for audio signal processing. IEEE J. Sel. Top. Signal Process. 13, 206–219 (2019).
    Article   ADS   Google Scholar   LeCun, Y., Bengio, Y. & Hinton, G. Deep learning.
    Nature 521, 436–444 (2015). Article   ADS   CAS   PubMed   Google Scholar   Hays,
    G. C. & Hawkes, L. A. Satellite tracking sea turtles: Opportunities and challenges
    to address key questions. Front. Mar. Sci. 5, 145 (2018). Article   Google Scholar   Peter,
    L. L., John, A. M. & Jeanette, W. The Biology of Sea Turtles Vol. 2 (CRC Press,
    2002). Google Scholar   Hughes, T. P. et al. Coral reefs in the Anthropocene.
    Nature 546, 82–90 (2017). Article   ADS   CAS   PubMed   Google Scholar   Piacenza,
    J., Piacenza, S., Mayoral, S., Kenney, A. & Shields, N. Design opportunities for
    sea turtle satellite tracking devices. In Proceedings of the ASME Design Engineering
    Technical Conference vol. 4 (American Society of Mechanical Engineers (ASME),
    2018). Dujon, A. M., Lindstrom, R. T. & Hays, G. C. The accuracy of Fastloc-GPS
    locations and implications for animal tracking. Methods Ecol. Evol. 5, 1162–1169
    (2014). Article   Google Scholar   Mitani, Y. et al. A method for reconstructing
    three-dimensional dive profiles of marine mammals using geomagnetic intensity
    data: Results from two lactating Weddell seals. Polar Biol. 26, 311–317 (2003).
    Article   Google Scholar   Slabbekoorn, H. et al. A noisy spring: The impact of
    globally rising underwater sound levels on fish. Trends Ecol. Evol. 25, 419–427.
    https://doi.org/10.1016/j.tree.2010.04.005 (2010). Article   PubMed   Google Scholar   Jones,
    B., Zapetis, M., Samuelson, M. M. & Ridgway, S. Sounds produced by bottlenose
    dolphins ( Tursiops ): A review of the defining characteristics and acoustic criteria
    of the dolphin vocal repertoire. Bioacoustics 29, 399–440 (2020). Article   Google
    Scholar   Au, W. W. L. et al. Acoustic properties of humpback whale songs. J.
    Acoust. Soc. Am. 120, 1103–1110 (2006). Article   ADS   PubMed   Google Scholar   Akamatsu,
    T., Okumura, T., Novarini, N. & Yan, H. Y. Empirical refinements applicable to
    the recording of fish sounds in small tanks. J. Acoust. Soc. Am. 112, 3073–3082
    (2002). Article   ADS   PubMed   Google Scholar   Redmon, J., Divvala, S., Girshick,
    R. & Farhadi, A. You Only Look Once: Unified, Real-Time Object Detection (Springer,
    2015). https://doi.org/10.48550/arXiv.1506.02640. Book   Google Scholar   He,
    K., Gkioxari, G., Dollár, P. & Girshick, R. Mask R-CNN. In Proceedings of the
    IEEE International Conference on Computer Vision (ICCV) 2961–2969 (2017). https://doi.org/10.48550/arXiv.1703.06870.
    Luo, Y. & Mesgarani, N. Conv-TasNet: Surpassing ideal time-frequency magnitude
    masking for speech separation. IEEE/ACM Trans. Audio Speech Lang. Process. 27,
    1256–1266 (2019). Article   PubMed   PubMed Central   Google Scholar   Howard,
    A. G. et al. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
    Applications (Springer, 2017). https://doi.org/10.48550/arXiv.1704.04861. Book   Google
    Scholar   International Telecommunication Union (ITU). Mobile Network Coverage.
    Facts and Figures 2023. https://www.itu.int/itu-d/reports/statistics/2023/10/10/ff23-mobile-network-coverage/
    (2023). Borkar, S. R. Long-term evolution for machines (LTE-M). In LPWAN Technologies
    for IoT and M2M Applications 145–166 (Elsevier, 2020). https://doi.org/10.1016/B978-0-12-818880-4.00007-7.
    Mishra, B. & Kertesz, A. The use of MQTT in M2M and IoT systems: A survey. IEEE
    Access 8, 201071–201086 (2020). Article   Google Scholar   Qasem, L. et al. Tri-axial
    dynamic acceleration as a proxy for animal energy expenditure; should we be summing
    values or calculating the vector?. PLoS One 7, e31187 (2012). Article   ADS   CAS   PubMed   PubMed
    Central   Google Scholar   Klimley, A. P., Flagg, M., Hammerschlag, N. & Hearn,
    A. The value of using measurements of geomagnetic field in addition to irradiance
    and sea surface temperature to estimate geolocations of tagged aquatic animals.
    Anim. Biotelemetry 5, 19 (2017). Article   Google Scholar   Okuyama, J. et al.
    Surfacers change their dive tactics depending on the aim of the dive: Evidence
    from simultaneous measurements of breaths and energy expenditure. Proc. R. Soc.
    B Biol. Sci. 281, 1795 (2014). Google Scholar   Okuyama, J. et al. Quarter-Century
    (1993–2018) nesting trends in the peripheral populations of three sea turtle species
    at Ishigakijima Island, Japan. Chelon. Conserv. Biol. 19, 101–110 (2020). Article   Google
    Scholar   Obe, Y. et al. The relationship between the number of breaths and diving
    profile of a nesting green turtle, Chelonia mydas. In PROCEEDINGS of the Design
    Symposium on Conservation of Ecosystem (The 13th SEASTAR2000 workshop) (Kyoto
    University Design School, 2014). Watanabe, Y. Y. & Papastamatiou, Y. P. Biologging
    and Biotelemetry: Tools for Understanding the Lives and Environments of Marine
    Animals. Annu. Rev. Anim. Biosci. 11, 247–267 (2023). Article   PubMed   Google
    Scholar   Tyson, R. B. et al. Novel bio-logging tool for studying fine-scale behaviors
    of marine turtles in response to sound. Front. Mar. Sci. 4, 785 (2017). Article   Google
    Scholar   Sipola, T., Alatalo, J., Kokkonen, T. & Rantonen, M. Artificial intelligence
    in the IoT era: A review of edge AI hardware and software. In 2022 31st Conference
    of Open Innovations Association (FRUCT) 320–331 (IEEE, 2022). https://doi.org/10.23919/FRUCT54823.2022.9770931.
    Kishida, N. et al. A validation of abstracted dive profiles relayed via the Argos
    satellite system: A case study of a loggerhead turtle. Anim. Biotelemetry 10,
    1 (2022). Article   Google Scholar   Michael, M., Yohei, N., Takuro, S. & Katsumi,
    T. Leptocephali collected in light traps near coral reef habitats of Ishigaki
    Island in the southern Ryukyu Island chain. Coast Mar. Sci. 34, 47–54 (2010).
    Google Scholar   Takuro, S., Kazumasa, H., Osamu, A., Yoshitake, T. & Hiroyuki,
    K. Disturbed coral reefs and the effects upon the structure of fish communities
    at Ishigaki Island, Japan. Fisher. Sci. 68, 139–142 (2002). Article   Google Scholar   Tamaki,
    S. Commercial Fishes and Shellfishes of Okinawa (Okinawa Times, 2021). Google
    Scholar   Ambrogio, S. et al. An analog-AI chip for energy-efficient speech recognition
    and transcription. Nature 620, 768–775 (2023). Article   ADS   CAS   PubMed   PubMed
    Central   Google Scholar   Download references Acknowledgements We thank members
    of AquaSound Inc., IDEA Consultants Inc., the Japanese Academy of Underwater Science,
    divers, and fishermen of Ishigaki Island for their efforts in recording the sound
    library. We thank the advisors for this project, Dr. Nobuaki Arai, Dr. Tomonari
    Akamatsu, and Dr. Masazumi Shioda, for their constructive comments. This work
    was supported by the Innovative Science and Technology Initiative for Security,
    Grant Number JPJ004596, Acquisition, Technology, and Logistics Agency (ATLA),
    Japan. Author information Authors and Affiliations Biologging Solutions Inc.,
    Kyoto, Japan Takuji Noda, Takuya Koizumi, Naoto Yukitake, Daisuke Yamamoto & Tetsuro
    Nakaizumi Japan Fisheries Science and Technology Association, Tokyo, Japan Kotaro
    Tanaka & Takeshi Hara Ocean Policy Research Institute of the Sasakawa Peace Foundation,
    Tokyo, Japan Kotaro Tanaka Fisheries Technology Institute, Japan Fisheries Research
    and Education Agency, Okinawa, Japan Junichi Okuyama Field Science Education and
    Research Center, Kyoto University, Kyoto, Japan Kotaro Ichikawa Contributions
    T. N. and T. K. conceived the concept of this logger with advice from K. I. and
    T. H. The logger specification was designed by T.N. and T. K., with advice on
    sound processing from K. I. and K. T. N. Y., D. Y., and T. N. developed and manufactured
    the loggers. Field experiments using sea turtles were conducted under the guidance
    and cooperation of J. O. T. N., T. K., N. Y., D. Y., K. T., J. O., and K. I. conducted
    the fieldwork. T. N. and N. Y. programmed a cloud system to accumulate the transmitted
    data. T. N. constructed the deep-learning model and analyzed the data obtained
    in the field experiments. T. N. wrote the draft manuscript. All authors contributed
    to the writing (review & editing) and gave final approval for publication. Corresponding
    author Correspondence to Takuji Noda. Ethics declarations Competing interests
    The authors declare no competing interests. Additional information Publisher''s
    note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Supplementary Information Supplementary Information.
    Rights and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article''s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article''s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Noda, T., Koizumi, T., Yukitake,
    N. et al. Animal-borne soundscape logger as a system for edge classification of
    sound sources and data transmission for monitoring near-real-time underwater soundscape.
    Sci Rep 14, 6394 (2024). https://doi.org/10.1038/s41598-024-56439-x Download citation
    Received 16 June 2023 Accepted 06 March 2024 Published 16 March 2024 DOI https://doi.org/10.1038/s41598-024-56439-x
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Soundscape Biologging Edge classification Deep learning Ocean
    noise Subjects Animal migration Biodiversity Electrical and electronic engineering
    Marine biology Physical oceanography Comments By submitting a comment you agree
    to abide by our Terms and Community Guidelines. If you find something abusive
    or that does not comply with our terms or guidelines please flag it as inappropriate.
    Download PDF Sections Figures References Abstract Introduction Materials and methods
    Results Discussions Data availability References Acknowledgements Author information
    Ethics declarations Additional information Supplementary Information Rights and
    permissions About this article Comments Advertisement Scientific Reports (Sci
    Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press releases Press
    office Contact us Discover content Journals A-Z Articles by subject Protocol Exchange
    Nature Index Publishing policies Nature portfolio policies Open access Author
    & Researcher services Reprints & permissions Research data Language editing Scientific
    editing Nature Masterclasses Research Solutions Libraries & institutions Librarian
    service & tools Librarian portal Open research Recommend to library Advertising
    & partnerships Advertising Partnerships & Services Media kits Branded content
    Professional development Nature Careers Nature Conferences Regional websites Nature
    Africa Nature China Nature India Nature Italy Nature Japan Nature Korea Nature
    Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies
    Legal notice Accessibility statement Terms & Conditions Your US state privacy
    rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Animal-borne soundscape logger as a system for edge classification of sound
    sources and data transmission for monitoring near-real-time underwater soundscape
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Selvanarayanan R.
  - Rajendran S.
  - Algburi S.
  - Ibrahim Khalaf O.
  - Hamam H.
  citation_count: '0'
  description: Soil health is essential for whirling stale soil into rich coffee-growing
    land. By keeping healthy soil, coffee producers may improve plant growth, leaf
    health, buds, cherry and bean quality, and yield. Traditional soil monitoring
    is tedious, time-consuming, and error-prone. Enhancing the monitoring system using
    AI-based IoT technologies for quick and precise changes. Integrated soil fertility
    control system to optimize soil health, maximize efficiency, promote sustainability,
    and prevent crop threads using real-time data analysis to turn infertile land
    into fertile land. The RNN-IoT approach uses IoT sensors in the coffee plantation
    to collect real-time data on soil temperature, moisture, pH, nutrient levels,
    weather, CO2 levels, EC, TDS, and historical data. Data transmission using a wireless
    cloud platform. Testing and training using recurrent neural networks (RNNs) and
    gated recurrent units gathered data for predicting soil conditions and crop hazards.
    Researchers are carrying out detailed qualitative testing to evaluate the proposed
    RNN-IoT approach. Utilize counterfactual recommendations for developing alternative
    strategies for irrigation, fertilization, fertilizer regulation, and crop management,
    taking into account the existing soil conditions, forecasts, and historical data.
    The accuracy is evaluated by comparing it to other deep learning algorithms. The
    utilization of the RNN-IoT methodology for soil health monitoring enhances both
    efficiency and accuracy in comparison to conventional soil monitoring methods.
    Minimized the ecological impact by minimizing water and fertilizer utilization.
    Enhanced farmer decision-making and data accessibility with a mobile application
    that provides real-time data, AI-generated suggestions, and the ability to detect
    possible crop hazards for swift action.
  doi: 10.1038/s41598-024-56954-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    15 March 2024 Empowering coffee farming using counterfactual recommendation based
    RNN driven IoT integrated soil quality command system Raveena Selvanarayanan,
    Surendran Rajendran, Sameer Algburi, Osamah Ibrahim Khalaf & Habib Hamam  Scientific
    Reports  14, Article number: 6269 (2024) Cite this article 604 Accesses 9 Altmetric
    Metrics Abstract Soil health is essential for whirling stale soil into rich coffee-growing
    land. By keeping healthy soil, coffee producers may improve plant growth, leaf
    health, buds, cherry and bean quality, and yield. Traditional soil monitoring
    is tedious, time-consuming, and error-prone. Enhancing the monitoring system using
    AI-based IoT technologies for quick and precise changes. Integrated soil fertility
    control system to optimize soil health, maximize efficiency, promote sustainability,
    and prevent crop threads using real-time data analysis to turn infertile land
    into fertile land. The RNN-IoT approach uses IoT sensors in the coffee plantation
    to collect real-time data on soil temperature, moisture, pH, nutrient levels,
    weather, CO2 levels, EC, TDS, and historical data. Data transmission using a wireless
    cloud platform. Testing and training using recurrent neural networks (RNNs) and
    gated recurrent units gathered data for predicting soil conditions and crop hazards.
    Researchers are carrying out detailed qualitative testing to evaluate the proposed
    RNN-IoT approach. Utilize counterfactual recommendations for developing alternative
    strategies for irrigation, fertilization, fertilizer regulation, and crop management,
    taking into account the existing soil conditions, forecasts, and historical data.
    The accuracy is evaluated by comparing it to other deep learning algorithms. The
    utilization of the RNN-IoT methodology for soil health monitoring enhances both
    efficiency and accuracy in comparison to conventional soil monitoring methods.
    Minimized the ecological impact by minimizing water and fertilizer utilization.
    Enhanced farmer decision-making and data accessibility with a mobile application
    that provides real-time data, AI-generated suggestions, and the ability to detect
    possible crop hazards for swift action. Similar content being viewed by others
    Digital Regenerative Agriculture Article Open access 26 March 2024 Historical
    impacts of grazing on carbon stocks and climate mitigation opportunities Article
    15 March 2024 The interplay between microbial communities and soil properties
    Article 20 October 2023 Introduction In the nineteenth century, coffee cultivation
    in Brazil began to expand into the Cerrado region. The Cerrado is a vast region
    of savanna with poor soils. However, Brazilian coffee farmers developed new techniques
    for improving the soil, such as using lime and fertilizer. As a result, the Cerrado
    is now the world''s largest coffee-producing region. The components nitrogen,
    phosphorus, potassium, calcium, magnesium, Sulphur, and iron can all be found
    in soil that is considered fertile1. The optimal soil for growing coffee is loamy
    soil, found in the state of north Karnataka, India which has a good combination
    of texture, drainage, and water retention. Coffee plantation soil requires well-drained
    soil to prevent waterlogging and root rot. Coffee plants have extensive root systems
    that extend deep into the soil to absorb nutrients and water2. The presence of
    nutrient-rich soil serves as the fundamental basis for the optimal growth and
    development of coffee plants, hence facilitating the production of superior-grade
    coffee beans. Fertility refers to the soil''s ability to provide essential nutrients
    for plant growth, such as nitrogen, phosphorus, and potassium. Healthy soil leads
    to healthier coffee plants, which produce higher yields of high-quality beans.
    Coffee plants thrive in slightly acidic soil with a pH of 5.0–6.5. Crop covering,
    composting, organic fertilizer, reduced tillage, water conservation, and shade
    management are ancient soil fertility strategies. Restoration of dry land to rich
    soil using IoT sensors to monitor and enhance coffee plantation soil health is
    creative and successful3. Soil sensors measure nitrogen, phosphorus, and potassium.
    Soil temperature sensors show how temperature affects plant growth and nutrient
    uptake. Farmers can protect coffee plants from extreme temperatures by tracking
    soil temperature patterns. Soil temperature sensors show how temperature affects
    plant growth and nutrient uptake. Analyzing soil temperature patterns protects
    coffee plants from extreme temperatures4. IoT sensors help farmers optimize irrigation,
    fertilization, and other soil management activities for healthier soil and higher
    crop yields by giving real-time soil data. Recurrent neural networks (RNNs) are
    a type of artificial neural network (ANN) that are well-suited for processing
    sequential data like IoT sensor time-series data. RNNs are useful for time series
    prediction, anomaly detection, and natural language processing because they can
    learn and understand temporal patterns and interdependencies in datasets. Recurrent
    neural networks (RNNs) provide the capability to analyze records of soil moisture
    data, enabling the prediction of forthcoming moisture levels. Conduct a comprehensive
    examination of soil nutrient data to predict potential nutrient deficits, hence
    facilitating the efficient and effective application of fertilizers by farmers.
    This research gap identifies characteristic patterns in soil sensor data, with
    a special emphasis on sudden changes in moisture and nutrient levels. Regular
    soil monitoring will monitor the alterations in soil condition and prompt safeguards
    will be implemented5. Develop predictive models that provide suggestions for optimal
    irrigation, fertilization, and other soil management practices. The limited scope
    of traditional soil testing often focuses on soil organic matter, microbial activity,
    and soil structure. However, the use of organic improvement in soil may result
    in delayed progress. Generalized suggestions derived from past data may not consider
    the particular soil characteristics, regional climate, or contemporary methods
    of management. The information provided Erroneous interpretation of soil test
    findings and suggestions might result in misguided conclusions and unproductive
    actions. Insufficient historical analysis, which fails to account for previous
    management techniques and local environmental conditions, can have an influence
    on evaluations of soil health status and long-term patterns. This paper is organized
    as follows: “Literature review” section presents a review of related work in Recurrent
    Neural Networks with the Internet of Things using deep learning. “The proposed
    model” section describes the proposed RNN-IoT approach in detail. “Results and
    discussion” section presents the experimental setup and results. “Conclusion”
    section discusses the results and compares RNN-IoT with existing methods. “Challenges
    and future possibilities” section concludes the paper and discusses future research
    directions. Literature review Aarthi, R., Sivakumar, D., et al., proposed the
    optimal watering schedule and fertilizer application rate. IoT software platforms
    that can be used to develop smart soil property analysis systems such as Thing
    Speak, Blynk, Cayenne, Node-RED, and Azure IoT Hub. Smart soil property analysis
    systems can provide real-time soil conditions data, allowing farmers to respond
    quickly to any changes. Future work is to develop more sophisticated machine learning
    models to improve the accuracy of the predictions6. Na, A., Isaac, W., Varshney,
    S, et al., proposed an Internet of Things (IoT)-based system for remote monitoring
    of soil characteristics a system that employs sensors to collect data on soil
    properties, such as pH, electrical conductivity (EC), moisture, temperature, and
    then communicates this data to a cloud platform or another remote location. These
    soil qualities include electrical conductivity (EC), pH, and temperature7. Jain,
    N., Awasthi, Y et al., proposed an IoT-based soil analysis system using optical
    sensors and multivariate regression is a system that uses optical sensors to measure
    the color and reflectance of soil and then uses multivariate regression to predict
    soil properties, such as pH, organic matter, and nutrient content. IoT-based soil
    analysis systems can be made small and portable, making them ideal for field use.
    Future work on IoT-based soil analysis systems could focus on making IoT-based
    soil analysis systems more affordable and accessible to small-scale farmers8.
    Patil, P et al., proposed the implementation of IoT to determine the level of
    bicarbonate in the soil a system that uses sensors to measure the pH and electrical
    conductivity (EC) of soil and then uses this data to calculate the bicarbonate
    level. IoT-based bicarbonate detection systems can provide users with the data
    they need to make informed decisions about irrigation, fertilization, and other
    agricultural practices9. Adrian Z et al. proposed integrating soil pH measurement
    into an Internet of Things (IoT) application a concept that involves using sensors
    to measure soil pH and then transmitting this data to a cloud platform or other
    remote location using a wireless communication protocol. The dataset collected
    by IoT-based soil pH measurement systems typically includes soil pH, Temperature,
    Humidity, Electrical conductivity (EC), and Timestamp10 as illustrated in Table
    1. Table 1 Literature review for soil health based on different proposed models.
    Full size table IoT (Internet of Things) is a key enabling technology for smart
    agriculture, as it allows for the collection and analysis of data from sensors
    in real-time. IoT-based soil parameter measurement systems can provide real-time
    data on soil parameters, which allows farmers to respond quickly to changes. Future
    work on IoT-based soil parameter measurement systems could focus on making the
    systems easier to set up and maintain18. The materials and methods section describes
    how the soil is fertilized using IoT sensors and Cloud storage. The proposed model
    The primary objective of this research study was to monitor the overall condition
    of the soil and to design an advanced algorithm called RNN-IoT. The present techniques
    for measuring soil health based on existing and past guidelines have yielded inaccurate
    results. The suggested approach has successfully addressed all of the aforementioned
    limitations and obtained a predictable outcome in transforming barren land into
    sustainable and rich soil suitable for coffee cultivation. Build an IoT sensor
    network to monitor soil health Moisture and temperature sensors, Purpose: Measure
    soil moisture and temperature, Version used: N95S31B outdoor NB. Carbon dioxide
    level sensors, Purpose:Measure CO2 concentration, Version used: CO2 concentration
    transmitter with 0–10 V, Measurement: CO2 concentration (0–10 V). Soil water level
    indication, Purpose: Indicate soil water level, Version used: Nordic nRF9160 SiP.
    GPS sensors, Purpose: Record location data, Version used: U-blox NEO-M8N. LDR
    soil color sensors, Purpose: Measure NPK color value, Version used: MNS2-9-W2-CM-020.
    Time stamps, Purpose: Record timestamps, Version used: Maxim Integrated DS3231.
    GPS units, Purpose: Record location data. Sensors measure soil potassium, phosphorus,
    and nitrogen levels and concentrations together with NPK color value indicators19.
    As demonstrated in Fig. 1, the N95S31B NB-IoT outdoor temperature and humidity
    sensor accurately measures air temperature and relative humidity. Figure 1 Working
    prototype for testing soil fertility. Full size image It subsequently sends data
    to an IoT server over NB-IoT. CO2 sensors measure CO. The waterproof, sturdy CO2
    concentration transmitter outputs 0–10 V. Nordic nRF9160 SiP is a low-power, single-chip
    cellular IoT solution. Its Arm Cortex-M33 CPU, low-power RF transceiver, and multiple
    ports make it adaptable. The Nordic nRF9160 SiP''s ADC peripheral measures soil
    moisture. SiP receives soil moisture sensor data20. The Soil EC NPK PH Sensor
    monitors NPK, EC, and pH. Soil electrical conductivity (EC) may indicate salt
    concentration. Higher EC values imply plant-harming soil salt. Growing plants
    need nitrogen, phosphorus, and potassium. It means 20% nitrogen, phosphorus, and
    potassium in the soil. Soil pH indicates acidity or alkalinity. Acidic soils have
    pH < 7, while alkaline soils have pH > 7. Electrical conductivity sensors detect
    soil solution. NPK sensors evaluate soil solution light absorption at different
    wavelengths. This data can track soil health trends and identify areas that need
    specialist soil management. LDR soil color sensors like the MNS2-9-W2-CM-020 improve
    coffee plantation soil health monitoring21. The Maxim Integrated DS3231 time stamp
    may improve coffee plantation soil health monitoring in numerous ways. Plantation
    operators can use time stamps to find patterns and trends in soil health data
    as indicated in Fig. 2. Figure 2 Virtual abstract for proposed system. Full size
    image Protocol for interfacing serial technology (IST) Soil moisture sensors measure
    volumetric water content. Two electrodes detect soil electrical resistance, which
    fluctuates with water content. Sensor output is usually a soil moisture-proportional
    voltage or frequency as indicated in Eq. (1). Where VWC is the volumetric water
    content which finds the percentage of soil volume occupied by water, can be calculated
    based on the weight of the wet soil minus the weight of the dry soil which divides
    the weight of the dry soil. εa is the apparent dielectric permittivity of the
    soil, εb is the dielectric permittivity of dry soil, εc is the dielectric permittivity
    of water. Where ρn is the neutron count rate in the soil, ρd is the neutron count
    rate in dry soil, and ρw is the neutron count rate in water as indicated in Eq.
    (2). $${\\text{VWC}} = \\left( {\\upvarepsilon {\\text{a}} -\\upvarepsilon {\\text{b}}}
    \\right)/\\left( {\\upvarepsilon {\\text{w}} -\\upvarepsilon {\\text{c}}} \\right)$$
    (1) $${\\text{VWC}} = \\left( {\\uprho {\\text{n}} -\\uprho {\\text{d}}} \\right)/\\left(
    {\\uprho {\\text{w}} -\\uprho {\\text{d}}} \\right)$$ (2) $${\\text{VWC}} = {\\text{M}}
    + {\\text{N}}*{\\text{R}}$$ (3) where R is soil electrical resistance and M and
    N are calibration constants from Eq. (3). A soil temperature sensor is needed
    to understand soil activity and plant growth22. It employs thermistors or thermocouples,
    whose resistance changes with temperature. Equation (4) shows that the sensor''s
    voltage or current output is proportional to soil temperature. Where T is soil
    temperature, R is thermistor electrical resistance, and A and B are calibration
    coefficients. $${\\text{T}} = {\\text{A}} + {\\text{B}}*\\ln \\left( {\\text{R}}
    \\right)$$ (4) Water level indicators assess soil water table depth. It employs
    a pressure transducer or ultrasonic sensor to detect water-induced pressure or
    sound reflection. Sensor output is generally a water level-proportional voltage
    or frequency. An NPK color value indicator evaluates soil nitrogen, phosphorus,
    and potassium levels. It measures soil extract color with a colorimetric sensor.
    Sensor output is generally an RGB color value or a numerical NPK level value.
    A GPS sensor locates the soil monitoring spot. GPS satellites inform it of its
    latitude, longitude, and altitude. The sensor sends NMEA messages with location
    coordinates as indicated in Eqs. (5), (6), and (7). $${\\text{Latitude}}={\\text{ArcTan}}2\\left({\\text{sin}}\\left({\\upvarphi
    }\\right),{\\text{cos}}\\left({\\upvarphi }\\right)*{\\text{cos}}\\left(\\uplambda
    \\right)\\right)$$ (5) $${\\text{Longitude}} = {\\text{ArcTan}}2\\left( {\\sin
    \\left(\\uplambda \\right),\\cos \\left(\\uplambda \\right)*\\cos \\left( {\\upvarphi
    } \\right) - \\sin \\left( {\\upvarphi } \\right)*\\sin \\left(\\updelta \\right)}
    \\right)$$ (6) $${\\text{Altitude}} = {\\text{R}}*\\left( {\\sin \\left( {{\\upvarphi
    } - {\\upvarphi }_{0} } \\right) - \\left( {{\\text{R}}_{0} + {\\text{h}}} \\right)/{\\text{R}}*\\sin
    \\left( {{\\upvarphi }_{0} } \\right)} \\right)$$ (7) where \\({\\upvarphi }\\)
    is the geodetic latitude, \\(\\uplambda\\) is the longitude, \\(\\updelta\\) is
    the sun’s declination, R is the Earth''s mean radius (6371 km), R0 is its equatorial
    radius (6378 km), h is the GPS antenna’s height above the Earth’s surface, and
    \\({\\upvarphi }_{0}\\) is the reference ellipsoid’s latitude (43.66° LDRs (light-dependent
    resistors) measure soil color to determine nutrient content. It usually uses an
    LDR, which changes resistance when exposed to different wavelengths of light.
    Sensor output is generally a resistance value that fluctuates proportionately
    with soil color as indicated in Eq. (8). $${\\text{R}}=({\\text{Cr}}+{\\text{Cg}}+{\\text{Cb}})/({\\text{Cw}}+{\\text{Cg}}+{\\text{Cb}})$$
    (8) where R is the reflected light, Cr is the red light component of the incident
    light, Cg is the green light component of the incident light, Cb is the blue light
    component of the incident light, and Cw is the white light component of the incident
    light. Soils with higher organic matter content tend to reflect more red light
    and less blue light, while soils with lower organic matter content tend to reflect
    more blue light and less red light. A pH sensor monitors the acidity or alkalinity
    of the soil. Typically, it employs a glass electrode that generates a voltage
    that varies by the pH of the solution that it is surrounded by. The output of
    the sensor is often in the form of a voltage, and its magnitude is typically proportional
    to the pH of the soil as shown in Eq. (9). $${\\text{pH}}=-{\\text{log}}10\\left({\\text{aH}}+\\right)$$
    (9) pH is soil acidity or alkalinity and pH+ is hydrogen ion activity in the soil
    solution. Each soil health measurement is timestamped. Even when the gadget is
    off, its real-time clock (RTC) preserves precise time. Synchronizing soil health
    data with other sources and tracking changes requires the timestamp. Table 2 estimates
    dry land-rich soil in coffee plantations2. IoT sensors may be connected to Arduino
    using IST. The simple and efficient IST protocol is ideal for connecting many
    sensors to an Arduino microcontroller. It includes Master Out Slave, in Master
    in Slave Out, Slave Select, and Serial Clock. SCLK synchronizes master–slave data
    transfer. MOSI and MISO signals send data from master to slave and slave to master.
    The owner chooses the slave via SS. A start condition from the master device starts
    IST communication with the slave. Most starts are high-to-low SS signal changes.
    After supplying the start condition, the master device can send a MOSI bit per
    SCLK clock pulse to the slave device. The slave device receives MISO data and
    delivers a bit to the master device for each SCLK clock pulse. Stop signals from
    master devices to slave devices halt IST communication. SS signals typically rise
    during a standstill. Arduino microcontroller board with enough computing power
    for data handling and communication interfaces. Use appropriate cables to connect
    each sensor to the microcontroller board for proper pinout and voltage. In Algorithm
    1, each sensor''s communication protocols are configured for data acquisition.
    Table 2 Parameters to fertile the coffee plantation soil from dry land. Full size
    table Algorithm 1 Interfacing Serial Technologies (IST) Full size image Configuration
    of sensor data transmission for arduino microcontroller For analysts to collect
    precise measurements of crucial soil characteristics like temperature, moisture,
    PH, CO2, fertilizer level, and nutrient concentrations, sensors are submerged
    far below the surface of the soil23. The Arduino microcontroller serves as the
    system''s primary central processing unit (CPU). It is in charge of gathering
    information from the sensors, processing that information, and then sending it
    to a computer or Laptop, Mobile Phone for utilization as indicated in Fig. 3.
    Arduino Uno, Wi-Fi module ESP8266, Jumper wires, breadboard, and 5 V Power Supply.
    Figure 3 Sensors connected to an Arduino microcontroller. Full size image From
    an Arduino microcontroller to a Node MCU to the cloud Arduino microcontrollers
    capture soil health data from connected sensors as the main data acquisition unit
    communicating with linked sensors, reading output values, and processing sensor
    data as requested. Arduino microcontrollers gather soil health data from sensors
    as the main data-collecting unit communicating with linked sensors, reading output
    values, and processing sensor data24. The Node MCU stores data in the cloud and
    uses AWS IoT Core for visualization, and these services are accessed through the
    Node MCU as indicated in Fig. 4. Figure 4 The connection between Arduino board
    to cloud storage. Full size image Cloud Interface provides dashboards or visualization
    tools to monitor the uploaded sensor data in real time or over time. The data
    can be analyzed to identify patterns, trends, and anomalies, providing valuable
    insights into the monitored environment or process. Analyzed sensor data can be
    used to trigger actions, such as sending alert messages based on soil health conditions.
    Recurrent neural network The raw data was delivered to the input layer, where
    it was often preprocessed and normalized to ensure network interoperability. Load
    the CSV file into Panda’s data frame. Preprocess the data by performing necessary
    steps such as cleaning, handling missing values, and feature selection. Sequence
    formation has sequence length is a variable that determines the length of each
    sequence to be created. An empty list called sequences is initialized to store
    the generated sequences. Formula for GRU-based Soil Health Monitoring where, h_t
    is the hidden state of the GRU at time step t, h_{t − 1} is the hidden state of
    the GRU at time step t − 1, h_t̃ is the candidate hidden state at time step t,
    gate_update is the update gate, gate_reset is the reset gate. $${h}_{t}=gat{e}_{update}*{h}_{\\left\\{t-1\\right\\}}+gat{e}_{reset}*{h}_{\\widetilde{t}}$$
    (10) The update gate controls which information is allowed to flow into the hidden
    state from the previous time step where W_h is the weight matrix for the hidden-to-hidden
    state connection, U_x is the weight matrix for the input-to-hidden state connection,
    b_h is the bias vector for the hidden state, x_t is the input at time step t.
    $${h}_{\\widetilde{t}}=tanh\\left({W}_{h}*{h}_{\\left\\{t-1\\right\\}}+{U}_{x}*{x}_{t}+{b}_{h}\\right)$$
    (11) Loops from 0 to len(data)—sequence length + 1. This loop generates dataset
    sequences. Unfold the RNN into interconnected layers with input, forget, cell,
    and output gates. Input the current input into the input gate to regulate information
    flow into the hidden state. Apply the forget gate on the previous hidden state
    to determine how much to remember. Update the cell state depending on the input
    gate, forget gate and previous hidden state cell state that stores sequence long-term
    memory. $${C}_{t}={f}_{t}*{C}_{\\left\\{t-1\\right\\}}+{i}_{t}*tanh\\left({W}_{h}*{h}_{\\left\\{t-1\\right\\}}+{U}_{x}*{x}_{t}+{b}_{c}\\right)$$
    (12) Consider Ct as the cell state at time step t, \\({C}_{\\left\\{t-1\\right\\}}\\)
    at time step t − 1, and \\({h}_{\\left\\{t-1\\right\\}}\\) as the forget gate
    \\({i}_{t}\\) is the input gate, tanh is the hyperbolic tangent activation function,
    \\({W}_{h}\\) and \\({U}_{x}\\) are the hidden-to-hidden and input-to-hidden weight
    matrices, respectively. At time step t, \\({b}_{c}\\) is the bias vector for the
    cell state, \\({x}_{t}\\) is the input, and \\({h}_{\\left\\{t-1\\right\\}}\\)
    is the hidden state. Use the output gate to create the current time step output
    that passes cell state information. Gated recurrent units (GRUs) architecture
    trains the dataset properly depending on soil prediction features. At each time
    step, calculate the error between expected and actual output. Back propagates
    the error via the unrolled RNN to calculate weight error gradients. Update weights
    to reduce inaccuracy. Prediction and Analysis in the RNN model increase warnings
    and suggestions by predicting soil health metrics from real-time sensor inputs.
    Historical sensor data, growth trends, and FFNN soil health results train the
    model25. Change specifics to simulate scenarios. Counter-factual recommendation
    generation Parameters within historical data to represent various ‘what-if’ situations.
    It compares with the actual generated out with historical data to generate recommendations.
    They are comparing the recommendation using three ways Present data—Historical
    data, Historical data—Historical data, and Present data—Present data as illustrated
    in Fig. 5. Figure 5 Counter-factual recommendations. Full size image For example,
    let us contemplate a hypothetical situation in which a soil health monitoring
    system, based on feedforward neural network (FFNN) architecture, provides a recommendation
    for the optimal application rate of fertilizer for a particular agricultural area.
    Counterfactual suggestion has the potential to be employed to generate alternative
    fertilizer recommendations. Manhattan distance, often known as taxi cab distance,
    is calculated by adding the absolute coordinate differences of two places. Imagine
    traveling solely horizontally or vertically in Manhattan’s grid. Distance is calculated
    using present and historical data. Consider N points in K-dimensional space, where
    2 <  = N <  = 10^ {5} and 1 <  = K <  = 5. Find the location with the lowest Manhattan
    distance from the N places. Manhattan distance is two points measured along right-angle
    axes. It is |x1 − x2| +|y1 − y2| on a plane with p1 at (x1, y1) and p2 at (x2,
    y2). Sorting the points in all K dimensions and outputting the middle components
    of each dimension reduces the Manhattan distance. $$Manhattan\\;Distance = \\left|
    {s1 - s2} \\right| + \\left| {r1 - r2} \\right|$$ (13) The Manhattan Distance,
    which indicates the dissimilarity between the current suggestion and several alternative
    fertilizer options, may be calculated as shown in Eq. (13), where |s1 − s2| denotes
    the absolute difference between the two points’ s-coordinates. |r1 − r2| is the
    absolute difference in r-coordinates. The calculation considers nutritional content,
    application rates, and environmental effects. The system may create counterfactual
    suggestions with Manhattan Distances below a threshold. This keeps alternate options
    comparable to the original advice while allowing for varied perspectives. The
    implementation phase collects and analyzes data. Results and discussion The field
    implementation of the AI-powered system involved deploying IoT sensors in selected
    infertile agricultural fields to collect real-time soil data. These sensors continuously
    monitor soil properties such as moisture, temperature, pH, nutrient levels, and
    electrical conductivity. XG-booster is used for the pre-training model for predicting
    specific soil properties (e.g., nutrient content, and moisture levels) based on
    sensor data. The collected data was then fed into the AI system, which employed
    RNNs and GRUs to identify patterns and trends in soil health indicators as indicated
    in Fig. 6. Figure 6 Agriculture land setup for testing soil fertility. (a) Agricultural
    infertile land, (b) 5-week monitoring land, (c) 6–10 week monitoring land, (d)
    11–15 weeks monitoring land, (e) 16–20 weeks monitoring land, (f) Fertile soil.
    Full size image Evaluation setup Hardware setup has a Microcontroller—Arduino
    Uno heart of any IoT device, and it is responsible for collecting data from sensors,
    processing that data, and sending it to the cloud. Sensors- Soil moisture sensor,
    soil temperature sensor, pH sensor, and nutrient sensor. Wireless Communication
    Modules—Wi-Fi communication module, Bluetooth communication module, cellular communication
    module, Irrigation system actuator, fertilizer applicator actuator. AC power supply,
    DC power supply, battery, SD card, USB flash drive. Software setup has Windows
    operating system, python 3.6 Amazon Web Services (AWS)0.5, Data Visualization
    Tools—Matplotlib, simulates the suggested model on PC i5-8600 k, GeForce 1050Ti
    4 GB, 16 GB RAM, 250 GB SSD, and 1 TB HDD. The suggested model is evaluated using
    False Negative, True Positive, True Negative, and False Positive metrics using
    the Recurrent Neural Network and Counter-Factual Recommendation Generation. Field
    setup for improving soil fertility Enhancing the fertility of the soil is essential
    for achieving sustainable agriculture and achieving maximum crop yields. To evaluate
    and enhance the fertility of the soil throughout a range of monitoring periods,
    this experiment describes a field design that makes use of several land portions.
    Datasets are collected in real-time from Madikeri, Coorg, Karnataka total of 640,
    monthly monitoring based on historical data from the past three years recorded
    around 300582. Performance evaluation compared with RNN A proposed model is compared
    to existing models to determine its accuracy. Table 3 shows random forest, SVM,
    K-means clustering, GAN, and feed-forward neural network. This performance is
    measured using several metrics, including accuracy, precision, recall, and F1
    score. Accuracy is the most straightforward metric, measuring the proportion of
    correct predictions made by the model. Precision measures the proportion of correct
    positive predictions. Recall, also known as sensitivity, measures the proportion
    of actual positive instances that are correctly identified as positive. Table
    3 Performance analysis for the proposed model. Full size table $$Accuracy = (True\\;
    Positives + True \\;Negatives) / (Total\\; Samples)$$ (14) $$Precision = True\\;
    Positives / (True\\; Positives + False\\; Positives)$$ (15) $$Recall = True\\;
    Positives / (True \\;Positives + False \\;Negatives)$$ (16) $$F1 \\;Score = 2
    * (Precision * Recall) / (Precision + Recall)$$ (17) $$Specificity = True \\;Negatives
    / (True \\;Negatives + False\\; Positives)$$ (18) The proposed method achieves
    94.25% accuracy, while random forest, SVM, K-means clustering, GAN, and FFNN obtain
    81.22%, 81.66%, 82.12%, 88.12%, and 89.73%. Existing approaches take longer to
    calculate all datasets. Figure 7 shows that the suggested technique detects events
    better than current methods. High EC and TDS readings indicate excessive salinity,
    which can stress and reduce coffee plant output. Most of the 30-day monitoring
    period had optimum EC and TDS values. NPK levels reflect coffee plant nutrient
    availability. Low nitrogen, phosphorus, and potassium (NPK) levels can decrease
    coffee plant output. The reduction in NPK levels from day 11–30 suggests dietary
    replenishment. During days 1–10, the soil moisture was a little dry, but otherwise,
    it was excellent. The rain prevented soil desiccation and reduced coffee plant
    stress. Figure 8 shows that soil temperature also affects coffee plant development.
    Coffee plants like soil temperatures between 20 and 25 °C. During the monitoring
    period, soil temperature stayed within the optimal range, promoting root growth
    and nutrient absorption. Figure 7 Result analysis for the proposed approach. Full
    size image Figure 8 Mobile application. Full size image Optimization of the model
    To keep the coffee plants healthy and the farm skiing, it is important to monitor
    the soil health of the plantations. Beans with more robust scents and tastes are
    a product of soil that is both healthy and well-tended. Farmers may maximize water
    and fertilizer efficiency and cut down on production expenses by learning what
    their soil requires to be healthy. To maintain healthy and fertile soil over the
    long term, it is important to evaluate it regularly for signs of nutrient imbalances.
    Weekly data analysis utilizing the RNN model is used to monitor soil conditions,
    and farmers are updated on health-related information and comments about soil
    quality. For a more precise prediction, we compare the soil condition from the
    second week of monitoring with that from the first week. In a similar way comparing
    week 2 with week 1 generates correct results, comparing week 3 with week 2 and
    so on also yields accurate results. Measurements taken to enrich the soil of coffee
    plantations in arid regions When substantially monitoring and tracking various
    essential indicators such as electrical conductivity (EC), total dissolved solids
    (TDS), nitrogen (N), phosphorus (P), potassium (K), soil moisture, soil temperature,
    carbon dioxide (CO2), and soil pH, agricultural practitioners can promptly detect
    potential issues and implement appropriate measures to uphold an optimal ecosystem
    for their coffee crops. The soil solution''s EC and TDS levels indicate salt content
    as indicated in Table 4. Table 4 Measurement Range to fertile the coffee plantation
    soil from dry land. Full size table CO2 levels rose somewhat from day 11–20, indicating
    a healthier soil microbial community. Coffee plants thrive best at 5.5–6.5 soil
    pH as illustrated in Fig. 9. The soil pH remained within the correct range during
    the monitoring period, ensuring that coffee plants could easily obtain nutrients
    as shown in Table 5. Rainfall is vital to soil hydration. The monitoring period
    was without significant precipitation from day one to day fourteen. This shows
    that irrigation was needed to maintain soil moisture levels throughout this era.
    Day 15 brought 15 mm of rain, restoring soil moisture. Coffee plant soil health
    measurements were within optimal levels for most of the 140-day monitoring period
    from week 1 to week 20. Figure 9 Weekly monitoring using the IoT Platform. Full
    size image Table 5 Parameters to fertile the coffee plantation soil from dry land.
    Full size table Counter-factual recommendation RNN-IoT model uses historical data
    and real-time sensor readings to predict future soil states and potential crop
    threats. Based on these predictions, the system generates counterfactual recommendations—alternative
    actions your farm could take to achieve desired outcomes. Mobile application sample
    The mobile application was created with the open-source Google framework Flutter,
    employing JavaScript. The RCNN model has been integrated and implemented, resulting
    in the generation of recommendations. These recommendations are created using
    Counterfactual suggestions. Figure 10 is a discussion based on the question raised
    by the agriculturist from various parts of the coffee plantation growers. Soil-rich
    mobile-based applications will provide recommendations after analyzing sensor
    data. Agriculturist question was “How do soil conditions that are too dry, too
    wet, too cold, or too hot affect coffee plants”. The recommendation is generated
    as High or low soil pH can make it difficult for coffee plants to absorb nutrients.
    Solution: Apply soil amendments to adjust the pH to the optimal range. The specific
    type of soil amendment to apply will depend on the current soil pH and the desired
    pH range. Similarly, “How does rainfall affect coffee plants, and what can coffee
    growers do to mitigate the negative effects of extreme rainfall events”. The recommendation
    is generated as Insufficient or excessive rainfall can lead to water stress or
    soil erosion. Irrigate during periods of low rainfall and implement erosion control
    measures during periods of high rainfall. Monitoring rainfall patterns is important
    to ensure that coffee plants receive the right amount of water26. The proposed
    model is trained using the Training Set, which enables it to discover hyperlinks
    and patterns in the data. To minimize loss and enhance predictions. To evaluate
    the model''s performance and adjust its hyper-parameters, a validation set is
    utilized during its development. Evaluates the model''s ability to be extrapolated
    to new data, which helps avoid excess fitting. The testing set is utilized for
    the final and unbiased evaluation of the model''s performance once all training
    and hyper-parameter adjustments have been finished. Ensures the model''s ability
    to apply knowledge to unfamiliar data and offers an accurate evaluation of its
    capabilities as shown in Fig. 11. The total dataset is divided into 60% for training,
    20% for validation, and 20% for testing. The difference between predicted and
    actual values is computed using a loss function by mean square error. The loss
    is propagated backward through time, updating model parameters (weights and biases)
    to minimize the loss. Figure 10 Farmer’s query-based recommendation. Full size
    image Figure 11 Training and validation loss versus accuracy of RNN over counterfactual
    recommendation. Full size image A confusion matrix is an illustrative format that
    exhibits the success of a proposed theory27. The data represents the observed
    vs projected results for each week of monitoring. Within the realm of soil fertility,
    weekdays may be classified as several tiers of fertility. For instance, weeks
    1–7 exhibit low fertility, weeks 8–14 demonstrate medium fertility, and weeks
    15–20 showcase high fertility, making them suitable for cultivation as shown in
    Fig. 12. Figure 12 Multiple confusion matrix for monitoring soil accuracy. Full
    size image By using IoT sensors to track soil moisture, temperature, pH, and nutrient
    levels, coffee farmers can gain valuable insights into their crops'' health and
    needs. Figure 13 data can then be used to make informed decisions about irrigation,
    fertilization, and other agricultural practices—a 20-week soil monitoring study
    that tracks the soil condition in a coffee plantation. The study was conducted
    in a coffee plantation in Madekari, Coorg, and Karnataka, India. Ten IoT sensors
    were installed on the plantation, and data was collected every hour for 20 weeks.
    The sensors measured soil moisture, temperature, and pH at a depth of 10 cm throughout
    20 weeks. Due to heavy rainfall in the early weeks of the research, soil moisture
    was maximum. Dry weather diminished soil moisture. The study''s soil temperature
    peaked in the first weeks and then dropped as the weather cooled. pH was almost
    consistent throughout the investigation. Figure 13 IoT sensors monitoring land
    view. Full size image 20-week. The investigation produced soil moisture, temperature,
    and pH maps. These maps helped identify under and over-watered crop regions and
    regulate the irrigation system. The maps were also utilized to identify acidic
    and alkaline soil on the farm and adapt the fertilization program. Conclusion
    The foreseeable future of coffee production demands beyond merely cultivating
    a diverse range of coffee beans; it involves establishing a sustainable and lucrative
    coffee business that will endure for future generations. Utilizing sophisticated
    sensors allows for the monitoring of environmental variables, including precipitation,
    humidity, and temperature, to optimize agricultural productivity. Internet of
    Things (IoT) devices have the potential to accurately assess the levels of water
    and nitrogen present in soil. Furthermore, by assessing the CO2 concentration
    in agricultural areas, it is possible to accurately monitor evapotranspiration
    rates, therefore enhancing the monitoring of soil health. This requires equipping
    coffee growers with the necessary tools and expertise to develop intelligent soil—a
    flexible and robust ecosystem that supports the growth of healthy coffee plants
    while reducing environmental harm. Farmers are provided with immediate updates
    on the condition of their soil and receive alerts on any potential issues. This
    enables farmers to promptly address any early signs of soil degradation. The Recurrent
    Neural Network and Counterfactual Recommendations utilize advanced algorithms
    to forecast future trends and provide tailored soil cures for youthful agriculturists.
    Knowledgeable farmers make well-informed choices regarding their soil, leading
    to higher crop production, better quality, and more profitability. The proposed
    method has overcome all of the above drawbacks and achieved the predictable result
    for converting infertile land to fertile land sustainable for coffee plantations.
    The future direction is moving towards installed outside Internet of Things devices
    are subject to severe weather, dust, wind, temperature, and other environmental
    hazards. This is one of the main issues with these devices. Unexpected mechanical
    failure of the complex devices could occur as a result of unfavorable environmental
    circumstances. Therefore, the raw materials used to construct the Internet of
    Things (IoT) devices used in smart farming must be able to endure such harsh climatic
    conditions. This will ensure that the devices last longer and provide more consistent
    results. Coffee cherry early detection and recommendation based on advanced deep
    learning technology which empowers farmers to cultivate a sustainable future for
    coffee, one smart earth revolution at a time. Challenges and future possibilities
    Global research on the Internet of Things (IoT) and sensor-based smart farming
    have revealed positive results. It is employed in several small-scale agricultural
    domains a rural farm, where farmers have less technological expertise, may provide
    more challenges. Even so, the execution of the project on a broad scale is still
    awaiting completion. An obstacle of great importance is the financial burden associated
    with the deployment and installation of IoT-tagged sensors and accessories in
    vast agricultural areas. Furthermore, there is ambiguity regarding the expenses
    associated with implementation and the monetary benefits that may be obtained.
    Deploying IoT-enabled technology incurs substantial expenses for hardware, software,
    and system operation. Supplementary costs may encompass energy usage, system upkeep,
    service enrollment, and labor fees for operating combined hardware and software.
    Enhancing farmers'' digital literacy on a global scale is crucial to facilitating
    the extensive use of IoT technologies. Lack of comprehension and consciousness
    of IoT-based technologies in agriculture might result in the underutilization
    of intelligent systems in farming28. Government policymakers should develop economic
    strategies to facilitate the successful and efficient implementation of Internet
    of Things (IoT) technology by farmers in agricultural areas. Data privacy and
    security issues might hinder large-scale IoT and smart system adoption. Attackers
    can manipulate cloud server data to harm automated agricultural activities in
    farmlands. Attacks can negatively impact agricultural output and prevent good
    environmental management. IoT data security concerns contribute to the sluggish
    adoption of smart farming systems. Encryption is crucial for protecting critical
    data and digital systems in smart farming from global cyberattacks. Integrating
    cryptography with strong keys can reduce cyber threats on cloud systems. Additional
    methods, such as integrating multiparty computing with homomorphic encryption
    or block-chain, can yield accurate results. Experimental statement No plants and
    Animals are disturbed for research work. All image datasets information regarding
    plant-based processes or applications was obtained from publicly available and
    ethically sourced data and publications. Plant Collection materials are complying
    with institutional, national, and international guidelines and legislation. So
    we confirm that all methods were performed in accordance with the relevant guidelines/regulations/legislation.
    Data availability The dataset utilized and analyzed in our research is publicly
    accessible to the public fertility land in the Zenodo communities Raveena. (2023).
    Empowering Coffee Farming Using Counterfactual Recommendation based RNN-IoT Integrated
    Soil Fertility Control System [Data set]. Zenodo. https://doi.org/10.5281/zenodo.10416960.
    The coding system along with additional data are accessible upon adequate request
    from the initial and coauthor authors. References Alharbi, M., Rajagopal, S. K.,
    Rajendran, S. & Alshahrani, M. Plant disease classification based on ConvLSTM
    U-net with fully connected convolutional layers. Traitement Signal 40(1), 157
    (2023). Article   Google Scholar   Kumar, R. S., Thanarajan, T. & Alotaibi, Y.
    Brain tumor: Hybrid feature extraction based on UNet and 3DCNN. Comput. Syst.
    Sci. Eng. 45(2), 2093–2109 (2023). Article   Google Scholar   Xiao, L. et al.
    Mineral coating enhances the carbon sequestration capacity of biochar derived
    from Paulownia biowaste. Agronomy 13(9), 2361 (2023). Article   CAS   Google Scholar   Mustafa,
    A. et al. Long-term fertilization alters the storage and stability of soil organic
    carbon in Chinese paddy soil. Agronomy 13(6), 1463 (2023). Article   CAS   Google
    Scholar   Saikia, D. & Khatoon, R. Smart monitoring of soil parameters based on
    IoT. Int. J. Adv. Technol. Eng. Explor. 9(88), 401 (2022). Google Scholar   Aarthi,
    R., Sivakumar, D. & Mariappan, V. Smart soil property analysis using IoT: A case
    study implementation in backyard gardening. Procedia Comput. Sci. 218, 2842–2851
    (2023). Article   Google Scholar   Na, A., Isaac, W., Varshney, S. & Khan, E.
    An IoT-based system for remote monitoring of soil characteristics. in 2016 International
    Conference on Information Technology (InCITe)-the next generation IT summit on
    the Internet of things: Connect your Worlds, 316–320 (2016). Jain, N., Awasthi,
    Y. & Jain, R. K. An IoT-based soil analysis system using optical sensors and multivariate
    regression. Int. J. Exp. Res. Rev. (IJERR). 31, 23–32 (2023). Article   Google
    Scholar   Patil, P., Vimala, M. S., Valarmathi, K. & Rose, L. Implementation of
    IoT to determine the level of bicarbonate in soil. in Implementation of IoT to
    Determine the Level of Bicarbonate in Soil, vol. 12, 5862–5876 (2023). Zarnescu,
    A., Ungurelu, R., Macovei, M. I. & Varzaru, G. Integrating soil pH measurement
    into an Internet of Things application. Sci. Pap.-Ser. B Hortic. 62, 703–708 (2018).
    Google Scholar   Ajit, P. et al. IoT based pH reader. Int. J. Adv. Res. Sci. Commun.
    Technol. (IJARSCT) 5(1), 119–123 (2021). Google Scholar   Kamelia, L., Nugraha,
    Y. S., Effendi, M. R. & Priatna, T. The IoT-based monitoring systems for humidity
    and soil acidity using wireless communication. in 2019 IEEE 5th International
    Conference on Wireless and Telematics (ICWT), 1–4 (2019). Ogudo, K. A., Surendran,
    R. & Khalaf, O. I. Optimal artificial intelligence based automated skin lesion
    detection and classification model. Comput. Syst. Sci. Eng. 44(1), 693–707 (2023).
    Article   Google Scholar   Deshpande, G., Goswami, M., Kolhe, J., Khandagale,
    V., Khope, D., Patel, G., Doijad, R., Mujumdar, M., Singh, B. B. & Ganeshi, N.
    IoT-Based Low-Cost Soil Moisture and Soil Temperature Monitoring System, p. 23.
    arXiv:2206.07488 (2023). Pechlivani, E. M., Papadimitriou, A., Pemas, S., Ntinas,
    G. & Tzovaras, D. IoT-based agro-toolbox for soil analysis and environmental monitoring.
    Micromachines 14(9), 1698 (2023). Article   PubMed   PubMed Central   Google Scholar   Vidhya,
    P., Ninshiya Mary, J., Yamuna Mary, J., Suriya Ponselvi, R., & Mr, K.S. IoT-based
    soil content analysis. J. Pharm. Negat. Results. 100–112 (2023). Ayyasamy, S.
    & Jhosiah Felips, J. F. Role of Internet of Things (IoT) in the protection of
    soil and plant life from acid rain disasters: A survey. IJCRT. 22(20), 10–20 (2023).
    Google Scholar   Mutyalamma, A. V., Yoshitha, G., Dakshyani, A. & Padmavathi,
    B. V. Smart agriculture to measure humidity temperature moisture Ph. and nutrient
    values of the soil using IoT. Int. J. Eng. Adv. Technol. (IJEAT). 9(5), 11–33
    (2020). Google Scholar   Subahi, A. F., Khalaf, O. I. & Alotaibi, Y. I modified
    the self-adaptive Bayesian algorithm for smart heart disease prediction in IoT
    system. Sustainability 14(21), 14208–14218 (2022). Article   Google Scholar   Raveena,
    S. & Surendran, R. Sustainable fertilizers in coffee plantation: Hybrid Recommendation
    for agricultural producers. in 2023 5th International Conference on Inventive
    Research in Computing Applications (ICIRCA), 1664–1671 (2023). Sowmiya, E. & Sivaranjani,
    S. Smart system monitoring on soil using the Internet of Things (IoT). Int. Res
    J. Eng. Technol. (IRJET) 4(2), 1070–1099 (2017). Google Scholar   Manivasan, V.,
    Rathinavel, J. P., Khanna, A. K. & Visu, P. Soil and water compatibility testing
    based on IOT. Int. J. Adv. Netw. Appl. 332–334 (2019). Schirrmann, M., Gebbers,
    R., Kramer, E. & Seidel, J. Soil pH mapping with an on-the-go sensor. Sensors.
    11(1), 573–598 (2011). Article   ADS   CAS   PubMed   PubMed Central   Google
    Scholar   Raut, S. & Chitre, V. Soil monitoring and testing using IoT for fertility
    level and crop prediction. in Proceedings of the 3rd International Conference
    on Advances in Science & Technology (ICAST), vol. 1, 1–19 (2020). Selvanarayanan,
    R., Rajandran, S. & Alotaibi, Y. using hierarchical agglomerative clustering in
    E-nose for coffee aroma profiling: Identification, quantification, and disease
    detection. Instrum. Mes. Métrol. 22(4), 1–23 (2023). Google Scholar   Surendran,
    R., Khalaf, O. I. & Tavera Romero, C. A. Deep learning based intelligent industrial
    fault diagnosis model. Comput. Mater. Contin. 70(3) (2022). Rajak, P., Ganguly,
    A., Adhikary, S. & Bhattacharya, S. Internet of Things and smart sensors in agriculture:
    Scopes and challenges. J. Agric. Food Res. 1(14), 100776 (2023). Google Scholar   Tamilvizhi,
    T., Alotaibi, Y. & Rajendran, S. It improved wolf swarm optimization with deep-learning-based
    movement analysis and self-regulated human activity recognition. AIMS Math. 8(5),
    12520–12539 (2023). Article   Google Scholar   Download references Funding The
    authors thank the Natural Sciences and Engineering Research Council of Canada
    (NSERC) and New Brunswick Innovation Foundation (NBIF) for the financial support
    of the global project. These granting agencies did not contribute to the design
    of the study and collection, analysis, and interpretation of data. Author information
    Authors and Affiliations Department of Computer Science and Engineering, Saveetha
    School of Engineering, Saveetha Institute of Medical and Technical Sciences, Chennai,
    602 117, India Raveena Selvanarayanan & Surendran Rajendran College of Engineering
    Techniques, Al-Kitab University, Kirkuk, Iraq Sameer Algburi Department of Solar,
    Al-Nahrain Research Centre for Renewable Energy, Al-Nahrain University, Jadriya,
    Baghdad, Iraq Osamah Ibrahim Khalaf University de Moncton, Moncton, NB, E1A 3E9,
    Canada Habib Hamam Contributions Conceptualization, S.A., and R.S.; methodology,
    S.R.; software, R.S.; validation, S.A., S.R., and R.S.; formal analysis, O.I.K.;
    investigation, S.R.; resources, O.I.K.; data curation, R.S.; writing—original
    draft preparation, S.R.; writing—review and editing, S.R.; visualization, H.H.;
    supervision, R.S.; project administration, S.R.; funding acquisition, S.A, O.I.K,
    H.H. All authors have read and agreed to the published version of the manuscript.
    Corresponding author Correspondence to Surendran Rajendran. Ethics declarations
    Competing interests The authors declare no competing interests. Additional information
    Publisher''s note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Open Access This article is licensed under a Creative Commons Attribution 4.0
    International License, which permits use, sharing, adaptation, distribution and
    reproduction in any medium or format, as long as you give appropriate credit to
    the original author(s) and the source, provide a link to the Creative Commons
    licence, and indicate if changes were made. The images or other third party material
    in this article are included in the article''s Creative Commons licence, unless
    indicated otherwise in a credit line to the material. If material is not included
    in the article''s Creative Commons licence and your intended use is not permitted
    by statutory regulation or exceeds the permitted use, you will need to obtain
    permission directly from the copyright holder. To view a copy of this licence,
    visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions About
    this article Cite this article Selvanarayanan, R., Rajendran, S., Algburi, S.
    et al. Empowering coffee farming using counterfactual recommendation based RNN
    driven IoT integrated soil quality command system. Sci Rep 14, 6269 (2024). https://doi.org/10.1038/s41598-024-56954-x
    Download citation Received 21 December 2023 Accepted 13 March 2024 Published 15
    March 2024 DOI https://doi.org/10.1038/s41598-024-56954-x Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    Soil monitoring system Monitoring sensor Recurrent neural network (RNN) Gated
    recurrent units (GRU) IoT sensor Counterfactual recommendation Subjects Computational
    science Computer science Environmental biotechnology Environmental impact Information
    technology Comments By submitting a comment you agree to abide by our Terms and
    Community Guidelines. If you find something abusive or that does not comply with
    our terms or guidelines please flag it as inappropriate. Download PDF Sections
    Figures References Abstract Introduction Literature review The proposed model
    Results and discussion Conclusion Challenges and future possibilities Data availability
    References Funding Author information Ethics declarations Additional information
    Rights and permissions About this article Comments Advertisement Scientific Reports
    (Sci Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press releases
    Press office Contact us Discover content Journals A-Z Articles by subject Protocol
    Exchange Nature Index Publishing policies Nature portfolio policies Open access
    Author & Researcher services Reprints & permissions Research data Language editing
    Scientific editing Nature Masterclasses Research Solutions Libraries & institutions
    Librarian service & tools Librarian portal Open research Recommend to library
    Advertising & partnerships Advertising Partnerships & Services Media kits Branded
    content Professional development Nature Careers Nature Conferences Regional websites
    Nature Africa Nature China Nature India Nature Italy Nature Japan Nature Korea
    Nature Middle East Privacy Policy Use of cookies Your privacy choices/Manage cookies
    Legal notice Accessibility statement Terms & Conditions Your US state privacy
    rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Empowering coffee farming using counterfactual recommendation based RNN driven
    IoT integrated soil quality command system
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Knudsen B.M.
  - Søndergaard S.R.
  - Stacey D.
  - Steffensen K.D.
  citation_count: '0'
  description: 'Background: After curative surgery for early-stage breast cancer,
    patients face a decision on whether to undergo surgery alone or to receive one
    or more adjuvant treatments, which may lower the risk of recurrence. Variations
    in survival outcomes are often marginal but there are differences in the side
    effects and other features of the options that patients may value differently.
    Hence, the patient’s values and preferences are critical in determining what option
    to choose. It is well-researched that the use of shared decision making and patient
    decision aids can support this choice in a discussion between patient and clinician.
    However, it is still to be investigated what impact the timing and format of the
    patient decision aid have on shared decision making outcomes. In this trial, we
    aim to investigate the impact of a digital pre-consult compared to a paper-based
    in-consult patient decision aid on patients’ involvement in shared decision making,
    decisional conflict and preparedness to make a decision. Methods: The study is
    a randomised controlled trial with 204 patients at two Danish oncology outpatient
    clinics. Eligible patients are newly diagnosed with early-stage breast cancer
    and offered adjuvant treatments after curative surgery to lower the risk of recurrence.
    Participants will be randomised to receive either an in-consult paper-based patient
    decision aid or a pre-consult digital patient decision aid. Data collection includes
    patient and clinician-reported outcomes as well as observer-reported shared decision
    making based on audio recordings of the consultation. The primary outcome is the
    extent to which patients are engaged in a shared decision making process reported
    by the patient. Secondary aims include the length of consultation, preparation
    for decision making, preferred role in shared decision making and decisional conflict.
    Discussion: This study is the first known randomised, controlled trial comparing
    a digital, pre-consult patient decision aid to an identical paper-based, in-consult
    patient decision aid. It will contribute evidence on the impact of patient decision
    aids in terms of investigating if pre-consult digital patient decisions aids compared
    to in-consult paper-based decision aids support the cancer patients in being better
    prepared for decision making. Trial registration: ClinicalTrials.gov (NCT05573022).'
  doi: 10.1186/s12885-024-12086-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Explore
    journals Get published About BMC Login BMC Cancer Home About Articles Submission
    Guidelines Collections Join The Board Submit manuscript Study Protocol Open access
    Published: 12 March 2024 Impact of timing and format of patient decision aids
    for breast cancer patients on their involvement in and preparedness for decision
    making - the IMPACTT randomised controlled trial protocol Bettina Mølri Knudsen
    , Stine Rauff Søndergaard , Dawn Stacey & Karina Dahl Steffensen   BMC Cancer  24,
    Article number: 336 (2024) Cite this article 258 Accesses Metrics Abstract Background
    After curative surgery for early-stage breast cancer, patients face a decision
    on whether to undergo surgery alone or to receive one or more adjuvant treatments,
    which may lower the risk of recurrence. Variations in survival outcomes are often
    marginal but there are differences in the side effects and other features of the
    options that patients may value differently. Hence, the patient’s values and preferences
    are critical in determining what option to choose. It is well-researched that
    the use of shared decision making and patient decision aids can support this choice
    in a discussion between patient and clinician. However, it is still to be investigated
    what impact the timing and format of the patient decision aid have on shared decision
    making outcomes. In this trial, we aim to investigate the impact of a digital
    pre-consult compared to a paper-based in-consult patient decision aid on patients’
    involvement in shared decision making, decisional conflict and preparedness to
    make a decision. Methods The study is a randomised controlled trial with 204 patients
    at two Danish oncology outpatient clinics. Eligible patients are newly diagnosed
    with early-stage breast cancer and offered adjuvant treatments after curative
    surgery to lower the risk of recurrence. Participants will be randomised to receive
    either an in-consult paper-based patient decision aid or a pre-consult digital
    patient decision aid. Data collection includes patient and clinician-reported
    outcomes as well as observer-reported shared decision making based on audio recordings
    of the consultation. The primary outcome is the extent to which patients are engaged
    in a shared decision making process reported by the patient. Secondary aims include
    the length of consultation, preparation for decision making, preferred role in
    shared decision making and decisional conflict. Discussion This study is the first
    known randomised, controlled trial comparing a digital, pre-consult patient decision
    aid to an identical paper-based, in-consult patient decision aid. It will contribute
    evidence on the impact of patient decision aids in terms of investigating if pre-consult
    digital patient decisions aids compared to in-consult paper-based decision aids
    support the cancer patients in being better prepared for decision making. Trial
    registration ClinicalTrials.gov (NCT05573022). Introduction Patient involvement
    in treatment decisions during consultation between patients and clinicians is
    an essential element for increasing the patient''s quality of life and decision
    quality [1]. One approach to involve patients in decisions about their treatment
    is through shared decision making (SDM), which is a collaborative process between
    patient and clinician to find the best match between available treatment options
    and the patient’s informed preferences and values [2]. It is particularly in the
    field of cancer that SDM has gained momentum in recent years, as cancer treatment
    involves advanced treatment options, leaving the patient with difficult decisions,
    for example undergoing one or more adjuvant treatments if the risk of recurrence
    is minimal while considering the side effects and late effects caused by the adjuvant
    treatment. While, survival rates and side effects are often decisive for the recommended
    treatment, less attention is paid to other long-term consequences and quality
    of life [3]. A recent study reported that cancer patients primarily require information
    on quality of life and the impact of side effects, while clinicians focus on the
    survivorship outcomes in their consultations. This emphasises the necessity for
    patients and clinicians to communicate about the possible treatment options including
    all benefits and harms [4]. Patients have different values or preferences for
    potential benefits and harms across options, making it preference-sensitive, but
    they may need guidance on the right choice for their circumstances. SDM is an
    approach in which the clinician and patient work together to select treatments
    based on clinical evidence as well as the patient’s informed preferences [5].
    SDM is particularly important in preference-sensitive decisions, where there is
    more than one clinically appropriate treatment option, each with benefits and
    harms, and in which the patient’s values and preferences are critical in determining
    the chosen intervention [6]. Due to the complexity and challenges within cancer
    care, this is an area requiring SDM. A cancer diagnosis is life-changing, and
    often cancer-related decisions made by the patient are affected by uncertainty,
    and emotions may negatively impact cognition [7]. Thus, many patients need support
    to make a high-quality decision based on informed clinical knowledge of available
    treatment options congruent with their preferences. One way to support patients
    in their decision-making process and to inform them about the disease and its
    possible treatment options is by the use of patient decision aids (PtDAs), which
    are the most well-studied SDM interventions [8, 9]. PtDAs are tools designed to
    provide patients with evidence on harms and benefits of options, help them clarify
    what matters most to them, and empower them to make decisions. The use of PtDAs
    has been shown to create more realistic expectations of possible benefits and
    harms, improved knowledge, reduced decisional conflict, etc. [9]. However, there
    are barriers to patient participation in SDM [10, 11], and studies show that patient
    preparedness is an important factor; PtDAs are more helpful to patients actively
    considering their options versus those who have yet to start to think about their
    options or have already made a choice [12, 13]. PtDAs can be given to the patient
    before the consultation, thus facilitating preparation before discussing the decision
    with their clinician. In this case, tools can be interactive, digital solutions
    (apps, videos, websites etc.) or paper-based [9]. PtDAs primarily designed to
    be used independently by patients before their clinician visit – pre-consult tools
    – are widely evaluated in many different clinical settings. A 2024 Cochrane review
    of randomised trails reported widespread evaluation of PtDAs delivered pre-consult,
    and findings demonstrate that pre-consult tools increase preparedness to make
    decisions and reduce decisional conflict [9]. Authors argue that pre-consult tools
    provide patients with sufficient knowledge to participate constructively in decision-making
    [13, 14], and some patients prefer to assess the PtDA in the comfort of their
    own home, alone or with relatives [15]. PtDAs can also be designed primarily for
    clinicians and patients to use together within the consultation to structure the
    clinical counselling and facilitate SDM during the consultation [8, 14]. These
    in-consult tools help clinicians discuss treatment options and stimulate the integration
    of patient preferences into the decision-making process [14]. In-consult tools
    have shown to reduce decisional conflict, enhance the feeling of being involved
    in the decision-making process and establish a higher degree of shared or collaborative
    role when using in-consult PtDAs as opposed to a patient- or clinician-controlled
    role [9, 16, 17]. Although there is consistent evidence that pre-consult and in-consult
    PtDAs increase patient engagement, no randomised controlled studies compare them
    to determine whether one approach leads to better SDM outcomes [9, 18]. The 2024
    Cochrane review established that there are knowledge gaps between the two about
    the timing of PtDA (before or during the consultation), patient-clinician communication
    (role and level of SDM) and format of PtDAs (digital or paper-based) [9]. In summary,
    little is known about the timing and format of a PtDA, and further research is
    critical given the increasing and widespread interest and investment in PtDAs
    [19]. Adding to this knowledge gap is the increasing use of health technologies,
    including digital PtDAs, which has accelerated over the past two decades and has
    various implications for patients. Digital health has improved quality, safety
    and efficiency in healthcare—but digital health also represents a transformational
    shift in how care and treatment are delivered [20]. Some patients do not cope
    well with this, and it is, therefore, essential that new digital solutions in
    health care, such as digital PtDAs, are evidence-based. This study aims to explore
    the use of pre-consult digital PtDAs versus in-consult paper-based PtDAs to investigate
    if the format and timing of introduction of a PtDA for breast cancer patients
    in the decision making process have an impact on SDM, decisional conflict and
    preparedness to make decisions. The overall hypothesis is that the patients experience
    a higher degree of shared decision making when an in-consult PtDA is used. Methods
    and analysis Study design The IMPACTT study is designed as a randomised, controlled
    trial with two arms in a 1:1 allocation ratio and a primary endpoint of patient-reported
    involvement in shared decision making. The study protocol is reported following
    the CONSORT guidelines [21] and registered at ClinicalTrials.gov (NCT05573022).
    Setting Enrolment takes place at two Danish oncology outpatient clinics in the
    Region of Southern Denmark offering adjuvant treatment to breast cancer patients.
    The oncologists are the same at both sites, and they have received the same training
    in SDM and the use of the paper-based PtDA. During the consultations, an oncologist
    informs breast cancer patients on the adjuvant treatments options after undergoing
    curative surgery for early-stage breast cancer (e.g. chemotherapy or biologically
    targeted therapy). Eligibility criteria Patients over 18 years of age with histologically
    verified early-stage breast cancer who had curative surgery are eligible to participate
    in the study. Patients are only eligible if they can read and understand Danish
    and have a smartphone or tablet to which it is possible to download an app (the
    digital PtDA). Eligible patients are contacted by phone before the consultation
    in the outpatient oncology clinic to enable randomisation of patients willing
    to participate and to allow time for patients in the digital arm to access and
    use the digital PtDA before the consultation. Patients are therefore excluded
    if there are less than 24 h between the final consultation with the surgeons and
    the first consultation in the oncology outpatient clinic. Recruitment procedure
    At study initiation, the oncologists will be asked to sign a written consent for
    audio recording the consultations when the adjuvant treatment options are discussed.
    The consent is only given once and includes all consultations with eligible patients
    throughout the study period. Following standard procedures, patients with verified
    breast cancer are discussed at multidisciplinary team meetings (MDT) to identify
    the available management options. Eligible patients are then identified by a study
    nurse the day after the MDT based on the descriptive note from the MDT available
    in the electronic patient record. The note indicates: a) the relevant adjuvant
    treatments for the specific patient; b) when the patient is scheduled for the
    final consultation with the surgeon; and c) when the patient has their first consultation
    at the oncology outpatient clinic to discuss the available adjuvant treatment
    options (see Fig. 1). Fig. 1 Recruitment procedure flow diagram with eligibility,
    interventions and data collection. Q: Questionnaire. O: Observer measure. PI:
    Principle investigator Full size image The principal investigator telephones the
    eligible patients, identified by the study nurse, to obtain oral consent and randomise
    them. Randomisation will be performed as block randomisation with an equal allocation
    ratio (1:1) using a computer-generated table of block sizes of 4 or 6, created
    and supervised by a data manager at OPEN, Open Patient Data Explorative Network,
    Odense University Hospital, Region of Southern Denmark. Interventions Intervention
    arm I will be introduced to a paper-based PtDA by the clinician in the consultation.
    Participants in intervention arm II will be invited to access the digital PtDA
    via an app before the consultation. The paper-based PtDA will not be used during
    consultation in intervention arm II. Both arms will be exposed to the same PtDA
    in terms of the lexical, visual and structural content, only the format (digital
    versus paper-based) is different. The PtDA used in this study is based on the
    generic patient decision aid template developed and clinically tested by the Center
    for Shared Decision Making, Vejle Hospital, Denmark and based on the International
    Patient Decision Aid Standards (IPDAS) [22]. The PtDA is customised for use by
    patients with early-stage breast cancer who went through curative surgery. The
    PtDA specifically indicates the decision about having none, one or more adjuvant
    treatments depending on the patient''s value assigned to survival rates, side
    effects, long-term consequences, and quality of life. The PtDA involves a five-step
    procedure to make explicit clarification of the patient’s values for adjuvant
    treatment and presents options A) chemotherapy, B) radiation, C) hormone therapy,
    D) antibody therapy, and E) no treatment including benefits and harms of each
    option, outcome probabilities, and personal stories describing experiences of
    others that are relevant to the decision at hand. The options offered to the patient
    depend on the patient’s age and tumor characteristics. The paper-based version
    has been available for use during consultations in the clinic since 2017 [23].
    Outcomes The primary outcome is the extent to which patients are involved in a
    shared decision making process reported by the patient using the SDM Process_4
    questionnaire [24]. This instrument has four items and demonstrated good acceptability
    with high response rates and very low missing data. In a national study of 10
    different medical conditions, the SDM Process_4 did not show floor or ceiling
    effects. Calculations showed moderate to good reliability (ranging from 0.54 to
    0.87), and retest reliability was moderate (0.64). The construct validity assessment
    indicated that higher scores correlated with better decision quality and that
    patients were less likely to think they had made the wrong decision [24]. Secondary
    outcomes, using well tested instruments, are the patient-perceived level of SDM
    [25], patient engagement [26], preparation for decision making scale [12], decisional
    conflict scale [7], and Control preferences Scale [27]. Additional secondary outcomes
    are consultation length, clinician-perceived level of SDM [28], and observer-perceived
    patient involvement in SDM [29, 30]. All questionnaires are validated and translated
    into Danish (see Table 1 for further details). Patients’ demographic characteristics
    will include age, gender, education, occupation, and living arrangements. Table
    1 Content of data collection outcome measures Full size table Sample size According
    to the SDM Process_4 user guide, Sepucha and Fowler [24] suggest assumptions for
    the sample size calculations, including a common standard deviation of σ = 1 and
    a mean difference between intervention arm I and II at δ = 0.5*SD. Further, assuming
    a significance level of α = 0.05 and a statistical power of 1-β = 0.8 (80%), a
    total of 204 patients will be recruited in the study which included an extra 10%
    to compensate for missing data and dropouts. For secondary outcomes, the observer-perceived
    patient involvement in SDM (OPTION5) will be used to analyze the audio recordings
    of the consultations between patient and clinician. A separate sample size calculation
    was conducted for this outcome; based on a previous RCT study with a mean OPTION5
    score of 26.6 in the control arm [31], the calculation of sample size for the
    OPTION5 measure requires 35 patients in each arm to achieve 80% power (alpha at
    0.05) to demonstrate a difference of at least 13.4 points (50% increase in observed
    level of SDM) [31]. Patients will be consecutively asked for permission to audio
    record the consultation until the required sample size is achieved; hence, no
    dropouts or missing data are expected. Therefore, a total of 70 audio recordings
    are required. Data collection Study data will be collected and managed using the
    REDCap electronic data capture tool (copyright Vanderbilt University, version
    12.0.33 [32]) hosted at OPEN (Open Patient Data Explorative Network, Odense University
    Hospital, Region of Southern Denmark). After oral consent, patients will be randomised
    via REDCap and are immediately sent an electronic questionnaire via e-Boks to
    be answered before the consultation (a total of 13 questions, Patient Q1 in Table
    1). E-Boks is a secure national platform for digital, personal mail used by public
    authorities to citizens. All Danish citizens are required to use e-Boks but can
    be exempted from use if they fulfil specific requirements. If exempted from use,
    the patient will receive the questionnaire by mail. In the first questionnaire,
    enrolled patients give their written, electronic consent to participate and are
    asked to indicate if they consent to audio recording of the consultation. Patients
    in intervention arm II will be asked to checkmark if they have accessed and used
    the digital PtDA and, if not, to state the reason it was not used. The day after
    the consultation with the oncologist discussing adjuvant treatment, enrolled patients
    will receive the second questionnaire from REDCap (a total of 42 questions, Patient
    Q2 in Table 1). Patients in intervention arm II will also be asked to checkmark
    if the clinician used the paper-based PtDA during consultation. The patient will
    receive a reminder to complete the questionnaire after three and six days. Clinicians
    will be administered one 9-item questionnaire on paper which they are asked to
    fill in right after the consultation while they have the consultation fresh in
    mind (Clinician Q2, Table 1). Clinicians will also be asked if patients in intervention
    arm II unprovokedly took out their smartphone or tablet during the consultation
    to use the digital PtDA. Afterwards, answers will be registered manually in REDCap
    by the principal investigator. Audio recordings of clinician and patient consultations
    (N = 70) are collected (Observer Q1, Table 1) for later analysis. The nurse participating
    in the consultation is responsible for initiating the recording. Timeline Enrolment
    is estimated to be completed within a period of 20 months. Patient and public
    involvement Two patient representatives, who are currently or have previously
    undergone cancer treatment themselves, will be involved as co-researchers in scoring
    the observer-perceived measure OPTION5 to investigate if patients identify elements
    of perceived significance in the consultations not noticed by the researchers.
    The rating team consisting of two researchers and two patient representatives
    will initially meet for training in scoring to ensure that they are calibrated
    in scoring the five themes of OPTION5. The 70 audio recordings will be divided
    between the two patient representatives, while the two researchers listen to and
    score all 70 audio recordings. The process and the patients'' influence on the
    analysis process will be documented in an impact log [33]. The log is completed
    with the patients, and the day''s audio recordings are evaluated and debriefed.
    The entire process of involvement will be reported and documented using GRIPP2
    [34]. The patient representatives will also be asked to complete the PPEET self-reported
    co-researcher questionnaire [35]. Statistical methods Data will be stored in a
    secure server in OPEN Analyse. Descriptive analyses will be performed to explore
    exchangeability between the two study arms. Data will be presented in a table
    describing the study population on key characteristics. The main analyses will
    be performed as mixed effect models by incorporating the clinician as a random
    effect. If there is no significant variation, analyses will be performed using
    linear regression. The 95% confidence intervals will be estimated using bootstrapping.
    In case of a screwed distribution on key characteristics (non-exchangeability)
    between the study arms, multivariate analyses will be performed adjusting for
    potential confounding factors. Primary and secondary outcomes will be analysed
    as sum scores and sub scores, and sub-analyses including stratification by sex
    and age groups. Continuous measures will be presented as mean with standard deviations
    (SD) or median with interquartile range (IQR) depending on if the findings are
    normally distributed?. Means will be compared using a 2-sided t-test, medians
    using a non-parametric K-sample test on the equality of medians. A 2-sided p-value
    of ≤ 0.05 will be used to determine significance. The consultation length will
    be measured using the audio recordings and analysed descriptively. All analyses
    will be performed as intention-to-treat analyses, and in case a patient did not
    access the app before consultation, per-protocol analyses will be performed as
    a sub-analysis. Missing data will be handled according to the scoring instructions
    of the instruments. Audio recordings of clinician and patient consultations will
    be rated by a team consisting of the principal investigator, a second researcher,
    and two patient representatives. Before rating, an initial calibration between
    the raters will be performed to avoid inter-observer variation and to reach a
    scoring consensus. Any discrepancies will be resolved by agreement or a fifth
    rating. Audio recordings will be stored in a secure server at OPEN. The team rating
    the audio recordings will be blinded such that raters will not know which arm
    the patients were randomised. Overall OPTION scores will be compared using the
    Wilcoxon-Mann–Whitney two-sample rank-sum test. Inter-rater reliability will be
    calculated using Interclass Correlation Coefficients (ICC). A reasonable threshold
    is above 0.6 [36]. Perspectives Due to national clinical guidelines on cancer
    treatment, patients are treated very similarly, and key decision time points are
    often the same. While the guidelines aim at offering all cancer patients the same
    treatment options, there is still a decision to be made for the individual patient
    based on their preferences. The increasing number of more advanced and individualised
    treatment options complicates the choice, and taking the patient’s situation and
    preferences into account when planning treatment is often necessary to achieve
    the best possible outcomes. This calls for more evidence-based tools to support
    patient involvement and to guide patients in making decisions about their own
    treatment with their clinician. In this study, patient and clinician perceived
    level of involvement in SDM, as well as observed reported SDM, will produce a
    comprehensive assessment of the patient’s engagement in decision making. Findings
    should enable us to improve the format and timing of administering a PtDA to future
    patients. For the individual patient, the results may contribute to better decision
    quality with the potential of providing improved quality of life, as well as reducing
    overtreatment at the hospitals. Dissemination The results of this study will be
    published in international peer-reviewed journals and presented at national and
    international conferences. Authorship is defined according to the recommendations
    for conduct, reporting, editing, and publication of scholarly work by the International
    Committee of Medical Journal Editors (ICMJE) [37]. The patient representatives
    involved as co-researchers will be offered authorship if their contribution agrees
    with the ICMJE recommendations or they will be acknowledged in the publication.
    As well, they will be offered active participation with poster or abstract presentations
    at national or international conferences with the principal investigator. Availability
    of data and materials The datasets used and/or analysed during the current study
    are available from the corresponding author on reasonable request. References
    Mathers N, Ng CJ, Campbell MJ, et al. Clinical effectiveness of a patient decision
    aid to improve decision quality and glycaemic control in people with diabetes
    making treatment choices: a cluster randomised controlled trial (PANDAs) in general
    practice. BMJ Open. 2012;2(6):e001469. Article   PubMed   PubMed Central   Google
    Scholar   Steffensen KD, Hansen DG, Espersen K, et al. “SDM: HOSP”-a generic model
    for hospital-based implementation of shared decision making. PLoS ONE. 2023;18(1):e0280547.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Katz SJ, Belkora J,
    Elwyn G. Shared decision making for treatment of cancer: challenges and opportunities.
    J Oncol Pract. 2014;10(3):206–8. Article   PubMed   PubMed Central   Google Scholar   Ankolekar
    A, Vanneste BG, Bloemen-van Gurp E, et al. Development and validation of a patient
    decision aid for prostate Cancer therapy: from paternalistic towards participative
    shared decision making. BMC Med Inform Decis Mak. 2019;19(1):1–11. Article   Google
    Scholar   Coulter A, Collins A. Making shared decision-making a reality. London:
    King’s Fund; 2011. p. 621. Google Scholar   Wennberg JE, Fisher ES, Skinner JS.
    Geography and the debate over medicare reform: a reform proposal that addresses
    some underlying causes of Medicare funding woes: geographic variation and lack
    of incentive for efficient medical practices. Health Aff. 2002;21(Suppl1):W96–112.
    Article   Google Scholar   O’Connor AM. Validation of a decisional conflict scale.
    Med Decis Making. 1995;15(1):25–30. Article   CAS   PubMed   Google Scholar   Nelson
    WA, Donnellan JJ, Elwyn G, et al. Implementing shared decision making: an organizational
    imperative. In: Shared decision making in health care: achieving evidence-based
    patient choice. 2016. p. 3. Google Scholar   Stacey D, Lewis KB, Smith M, et al.
    Decision aids for people facing health treatment or screening decisions. Cochrane
    Database Syst Rev. 2024;(1):CD001431. Joseph-Williams N, Elwyn G, Edwards A. Knowledge
    is not power for patients: a systematic review and thematic synthesis of patient-reported
    barriers and facilitators to shared decision making. Patient Educ Couns. 2014;94(3):291–309.
    Article   PubMed   Google Scholar   Légaré F, Witteman HO. Shared decision making:
    examining key elements and barriers to adoption into routine clinical practice.
    Health Aff. 2013;32(2):276–84. Article   Google Scholar   Bennett C, Graham ID,
    Kristjansson E, et al. Validation of a preparation for decision making scale.
    Patient Educ Couns. 2010;78(1):130–3. Article   PubMed   Google Scholar   Joseph-Williams
    N, Abhyankar P, Boland L, et al. What works in implementing patient decision aids
    in routine clinical settings? A rapid realist review and update from the international
    patient decision aid standards collaboration. Med Decis Making. 2021;41(7):907–37.
    Article   PubMed   Google Scholar   Thompson R, Trevena L. Demystifying decision
    aids: a practical guide for clinicians. Shared decision making in health care:
    achieving evidence-based patient choice. 3rd ed. Oxford: Oxford University Press;
    2016. p. 1. Google Scholar   Banegas MP, McClure JB, Barlow WE, et al. Results
    from a randomized trial of a web-based, tailored decision aid for women at high
    risk for breast cancer. Patient Educ Couns. 2013;91(3):364–71. Article   PubMed   PubMed
    Central   Google Scholar   Hæe M, Wulff CN, Fokdal L, et al. Development, implementation
    and evaluation of patient decision aids supporting shared decision making in women
    with recurrent ovarian cancer. PEC Innovation. 2023;2:100120. Article   PubMed   Google
    Scholar   Wulff CN, Hæe M, Hansen DG, et al. Shared decision making in recurrent
    ovarian cancer: Implementation of patient decision aids across three departments
    of oncology in Denmark. PEC Innov. 2022;1:100095. Article   PubMed   PubMed Central   Google
    Scholar   Scalia P, Durand M-A, Berkowitz JL, et al. The impact and utility of
    encounter patient decision aids: systematic review, meta-analysis and narrative
    synthesis. Patient Educ Couns. 2019;102(5):817–41. Article   PubMed   Google Scholar   Elwyn
    G, Montori VM, Edwards A. Tools to engage patients in clinical encounters. Shared
    Dec Mak Health Care. 2016;3:57–63. Article   Google Scholar   Wiljer D, Charow
    R, Costin H, et al. Defining compassion in the digital health age: protocol for
    a scoping review. BMJ Open. 2019;9(2):e026338. Article   PubMed   PubMed Central   Google
    Scholar   Schulz KF, Altman DG, Moher D. CONSORT 2010 statement: updated guidelines
    for reporting parallel group randomised trials. J Pharmacol Pharmacother. 2010;1(2):100–7.
    Article   PubMed   PubMed Central   Google Scholar   Olling K, Bechmann T, Madsen
    PH, et al. Development of a patient decision aid template for use in different
    clinical settings. Eur J Pers Cent Healthc. 2019;7(1):50–60. Google Scholar   Olling
    K, Stie M, Winther B, et al. The impact of a patient decision aid on shared decision-making
    behaviour in oncology care and pulmonary medicine—A field study based on real-life
    observations. J Eval Clin Pract. 2019;25(6):1121–30. Article   PubMed   Google
    Scholar   Sepucha K, Fowler F. Shared decision making process_4 User Guide v.1.0,
    2018. Kriston L, Scholl I, Hölzel L, et al. The 9-item Shared Decision Making
    Questionnaire (SDM-Q-9). Development and psychometric properties in a primary
    care sample. Patient Educ Counsel. 2010;80(1):94–9. Article   Google Scholar   Barr
    PJ, Thompson R, Walsh T, et al. The psychometric properties of CollaboRATE: a
    fast and frugal patient-reported measure of the shared decision-making process.
    J Med Internet Res. 2014;16(1):e3085. Article   Google Scholar   Degner LF, Sloan
    JA, Venkatesh P. The control preferences scale. Can J Nurs Res Arch. 1997;29(3):21–43.
    Scholl I, Kriston L, Dirmaier J, et al. Development and psychometric properties
    of the Shared Decision Making Questionnaire–physician version (SDM-Q-Doc). Patient
    Educ Couns. 2012;88(2):284–90. Article   PubMed   Google Scholar   Barr PJ, O’Malley
    AJ, Tsulukidze M, et al. The psychometric properties of observer OPTION5, an observer
    measure of shared decision making. Patient Educ Couns. 2015;98(8):970–6. Article   PubMed   Google
    Scholar   Elwyn G, Tsulukidze M, Edwards A, et al. Using a ‘talk’model of shared
    decision making to propose an observation-based measure: Observer OPTION5 Item.
    Patient Educ Couns. 2013;93(2):265–71. Article   PubMed   Google Scholar   Probst
    MA, Lin MP, Sze JJ, et al. Shared decision making for syncope in the emergency
    department: a randomized controlled feasibility trial. Acad Emerg Med. 2020;27(9):853–65.
    Article   PubMed   PubMed Central   Google Scholar   Harris PA, Taylor R, Minor
    BL, et al. The REDCap consortium: building an international community of software
    platform partners. J Biomed Inform. 2019;95:103208. Article   PubMed   PubMed
    Central   Google Scholar   Kok M. Guidance document: evaluating public involvement
    in research. 2018 Staniszewska S, Brett J, Simera I, et al. GRIPP2 reporting checklists:
    tools to improve reporting of patient and public involvement in research. BMJ.
    2017;358:j3453. Abelson J, Li K, Wilson G, et al. Supporting quality public and
    patient engagement in health system organizations: development and usability testing
    of the Public and Patient Engagement Evaluation Tool. Health Expect. 2016;19(4):817–27.
    Article   PubMed   Google Scholar   Elwyn G, Grande SW, Barr P. Observer OPTION
    5 manual. Darthmouth: The Dartmouth Institute for health policy and clinical practice;
    2016. Google Scholar   ICMJE Recommendations 2022 update. Available from: https://www.icmje.org/recommendations/.
    Accessed 17 May 2023. Download references Acknowledgements We acknowledge OPEN,
    Open Patient data Explorative Network, Odense University Hospital, Region of Southern
    Denmark, for hosting study data collected and managed using REDCap (Research Electronic
    Data Capture) electronic data capture tools. The authors thank participating patients
    and the study nurses of the Department of Oncology at Vejle and Esbjerg Hospital
    for making the study feasible. Funding Open access funding provided by University
    of Southern Denmark This work is fully funded by the Research Fund at Lillebaelt
    Hospital, the Region of Southern Denmark and the Cancer Foundation. Author information
    Authors and Affiliations Center for Shared Decision Making, Lillebaelt Hospital
    - University Hospital of Southern Denmark, Beriderbakken 4, 7100, Vejle, Denmark
    Bettina Mølri Knudsen & Karina Dahl Steffensen Department of Regional Health Research,
    Faculty of Health Sciences, University of Southern Denmark, Campusvej 55, 5230,
    Odense M, Denmark Bettina Mølri Knudsen, Stine Rauff Søndergaard, Dawn Stacey
    & Karina Dahl Steffensen Department of Oncology, Lillebaelt Hospital – University
    Hospital of Southern Denmark, Vejle, Beriderbakken 4, 7100, Vejle, Denmark Stine
    Rauff Søndergaard School of Nursing, University of Ottawa, 451 Smyth Rd, Ottawa,
    ON, K1H 8M5, Canada Dawn Stacey Centre for Implementation Research, Ottawa Hospital
    Research Institute, 725 Parkdale Ave, Ottawa, ON, K1Y 4E9, Canada Dawn Stacey
    Contributions Conception and protocol design – BK, SR, DS and KS. Funding acquisition
    – BK and KS. Drafting of the article – BK. Critical revisions of the article—BK,
    SR, DS and KS. Final approval of the version to be published—BK, SR, DS and KS.
    Corresponding author Correspondence to Bettina Mølri Knudsen. Ethics declarations
    Ethics approval and consent to participate The potential risk of harm is considered
    minimal. Patients in intervention arm II can choose not to access the digital
    PtDA if it feels uncomfortable. Participants will receive information orally when
    contacted by telephone and using written information in e-Boks with a link to
    the electronic questionnaire. All participants have at least 24 h of deliberation
    time from oral consent to scheduled consultation. Permission has been obtained
    by the Region of Southern Denmark to contact the patients before the consultation
    to enable randomisation before consultation (approval ID 22/6110). Both patients
    and healthcare staff involved in the consultation will give written consent to
    audio recording. Data collection and processing are approved by the Region of
    Southern Denmark (approval ID 22/9278). Data will be stored in a secure server
    in OPEN and will be deleted five years after the end of the study. Access to data
    will be limited to the principal investigator by login and password. Approval
    from the Danish Health Research Ethics Committee is not required as studies using
    questionnaires or observations are not considered an intervention according to
    the Committee Act (decision on non-approval ID 20212000–187). Approval has been
    obtained from the Research Ethics Committee at the University of Southern Denmark
    (approval ID 23/555) regarding ethical considerations for involvement of patient
    representatives as co-researchers. Consent for publication Not applicable. Competing
    interests The authors declare that they have no competing interests. Additional
    information Publisher’s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Supplementary Information
    Supplementary Material 1. Rights and permissions Open Access This article is licensed
    under a Creative Commons Attribution 4.0 International License, which permits
    use, sharing, adaptation, distribution and reproduction in any medium or format,
    as long as you give appropriate credit to the original author(s) and the source,
    provide a link to the Creative Commons licence, and indicate if changes were made.
    The images or other third party material in this article are included in the article''s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article''s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/)
    applies to the data made available in this article, unless otherwise stated in
    a credit line to the data. Reprints and permissions About this article Cite this
    article Knudsen, B.M., Søndergaard, S.R., Stacey, D. et al. Impact of timing and
    format of patient decision aids for breast cancer patients on their involvement
    in and preparedness for decision making - the IMPACTT randomised controlled trial
    protocol. BMC Cancer 24, 336 (2024). https://doi.org/10.1186/s12885-024-12086-z
    Download citation Received 18 October 2023 Accepted 04 March 2024 Published 12
    March 2024 DOI https://doi.org/10.1186/s12885-024-12086-z Share this article Anyone
    you share the following link with will be able to read this content: Get shareable
    link Provided by the Springer Nature SharedIt content-sharing initiative Keywords
    RCT Breast cancer Shared decision making Patient decision aid Download PDF Sections
    Figures References Abstract Introduction Methods and analysis Perspectives Dissemination
    Availability of data and materials References Acknowledgements Funding Author
    information Ethics declarations Additional information Supplementary Information
    Rights and permissions About this article Advertisement BMC Cancer ISSN: 1471-2407
    Contact us Submission enquiries: bmccancer@biomedcentral.com General enquiries:
    ORSupport@springernature.com Read more on our blogs Receive BMC newsletters Manage
    article alerts Language editing for authors Scientific editing for authors Policies
    Accessibility Press center Support and Contact Leave feedback Careers Follow BMC
    By using this website, you agree to our Terms and Conditions, Your US state privacy
    rights, Privacy statement and Cookies policy. Your privacy choices/Manage cookies
    we use in the preference centre. © 2024 BioMed Central Ltd unless otherwise stated.
    Part of Springer Nature."'
  inline_citation: '>'
  journal: BMC Cancer
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Impact of timing and format of patient decision aids for breast cancer patients
    on their involvement in and preparedness for decision making - the IMPACTT randomised
    controlled trial protocol
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Ruiz‐Preciado L.A.
  - Pešek P.
  - Guerra-Yánez C.
  - Ghassemlooy Z.
  - Zvánovec S.
  - Hernandez-Sosa G.
  citation_count: '0'
  description: Emerging areas such as the Internet of Things (IoT), wearable and wireless
    sensor networks require the implementation of optoelectronic devices that are
    cost-efficient, high-performing and capable of conforming to different surfaces.
    Organic semiconductors and their deposition via digital printing techniques have
    opened up new possibilities for optical devices that are particularly suitable
    for these innovative fields of application. In this work, we present the fabrication
    and characterization of high-performance organic photodiodes (OPDs) and their
    use as an optical receiver in an indoor visible light communication (VLC) system.
    We investigate and compare different device architectures including spin-coated,
    partially-printed, and fully-printed OPDs. The presented devices exhibited state-of-the-art
    performance and reached faster detection speeds than any other OPD previously
    reported as organic receivers in VLC systems. Finally, our results demonstrate
    that the high-performance of the fabricated OPDs can be maintained in the VLC
    system even after the fabrication method is transferred to a fully-inkjet-printed
    process deposited on a mechanically flexible substrate. A comparison between rigid
    and flexible samples shows absolute differences of only 0.2 b s−1 Hz−1 and 2.9 Mb s−1
    for the spectral efficiency and the data rate, respectively.
  doi: 10.1038/s41598-024-53796-5
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    08 February 2024 Inkjet-printed high-performance and mechanically flexible organic
    photodiodes for optical wireless communication Luis Arturo Ruiz‐Preciado, Petr
    Pešek, Carlos Guerra-Yánez, Zabih Ghassemlooy, Stanislav Zvánovec & Gerardo Hernandez-Sosa  Scientific
    Reports  14, Article number: 3296 (2024) Cite this article 696 Accesses Metrics
    Abstract Emerging areas such as the Internet of Things (IoT), wearable and wireless
    sensor networks require the implementation of optoelectronic devices that are
    cost-efficient, high-performing and capable of conforming to different surfaces.
    Organic semiconductors and their deposition via digital printing techniques have
    opened up new possibilities for optical devices that are particularly suitable
    for these innovative fields of application. In this work, we present the fabrication
    and characterization of high-performance organic photodiodes (OPDs) and their
    use as an optical receiver in an indoor visible light communication (VLC) system.
    We investigate and compare different device architectures including spin-coated,
    partially-printed, and fully-printed OPDs. The presented devices exhibited state-of-the-art
    performance and reached faster detection speeds than any other OPD previously
    reported as organic receivers in VLC systems. Finally, our results demonstrate
    that the high-performance of the fabricated OPDs can be maintained in the VLC
    system even after the fabrication method is transferred to a fully-inkjet-printed
    process deposited on a mechanically flexible substrate. A comparison between rigid
    and flexible samples shows absolute differences of only 0.2 b s−1 Hz−1 and 2.9
    Mb s−1 for the spectral efficiency and the data rate, respectively. Similar content
    being viewed by others 245 MHz bandwidth organic light-emitting diodes used in
    a gigabit optical wireless data link Article Open access 03 March 2020 All-printed
    large-scale integrated circuits based on organic electrochemical transistors Article
    Open access 07 November 2019 Organic photovoltaics for simultaneous energy harvesting
    and high-speed MIMO optical wireless communications Article Open access 23 February
    2021 Introduction Optical technologies have extensive applications in our daily
    lives and continue to offer multiple options for development in fields such as
    communication, healthcare, and automation1,2. In recent years, the relevance of
    optical technologies has become increasingly apparent thanks to emerging areas
    such as the internet of things (IoT), wearable and mobile devices. At the same
    time, the use of organic semiconductors has opened up new possibilities for optical
    devices that are particularly suitable for these innovative fields of application.
    In contrast to inorganic semiconductors, organic devices can be solution-processed,
    allowing for a wider range of substrates, offering light-weight and increased
    mechanical flexibility3,4. Moreover, organic semiconductors can be deposited via
    digital printing techniques, thus enabling higher freedom of design and additive
    manufacturing of integrated systems5. In this regard, inkjet printing has emerged
    as a particularly attractive approach, allowing high-throughput, personalization
    and cost-efficient production of electronic devices. In optical sensing, inkjet
    printing has been utilized to fabricate state-of-the-art organic photodiodes (OPDs)
    with comparable properties to their inorganic counterparts6,7,8,9,10. Due to the
    chemical nature of organic semiconductors, OPDs offer wide photoresponse tunability,
    making them suitable for a large range of applications requiring both broadband
    and wavelength-specific light detection7,11,12,13,14,15. One of the main fields
    of application for optical technologies lies in the field of communications. In
    this regard, visible light communication (VLC), which uses the visible light band,
    has recently emerged as a promising complementary technology to the radio frequency
    (RF)-based wireless communication16. VLC, by its nature, offers different features
    including high data rates, not being affected by the RF-induced electromagnetic
    interference, and inherent security, thus making it a suitable technology for
    emerging applications such as virtual reality, indoor positioning, high-quality
    video streaming, smart lighting and Li-Fi17. Recent research work on VLC systems
    has achieved important progress by (i) adopting optimized techniques18; (ii) using
    innovative methods such as simultaneous power generation and data transfer19,20;
    and (iii) successfully demonstrating proof-of-concept applications21,22,23. The
    majority of previous studies, however, have been reported on conventional inorganic-based
    transmitters and receivers. Research works reported on organic-based devices are
    limited, with the focus being on organic transmitters such as organic light emitting
    diodes (OLEDs)18,24,25,26,27,28,29. Although OPDs have shown significant progress
    in terms of detection speed, with literature reports showing values that are well-suited
    for VLC applications5,30,31, there are still limited literature reports successfully
    demonstrating data transmission in VLC systems using OPD-receivers. Moreover,
    most of these reports utilize OPDs with low bandwidths and deposited mainly on
    rigid substrates19,32,33,34,35,36,37,38. These studies achieved spin-coated OPDs
    with bandwidths between 0.5 and 3 MHz for rigid samples and below 500 kHz for
    flexible samples. In this work, we present the fabrication and characterization
    of high-performance OPDs and their successful integration in a VLC system. We
    investigate and compare different device architectures including spin-coated,
    partially-printed, and fully-printed OPDs. Our results demonstrate that the high-performance
    of the fabricated OPDs can be maintained in the VLC system even after the fabrication
    method is transferred to a fully-inkjet-printed process deposited on a mechanically
    flexible substrate. At the device level, we achieve responsivities above 300 mA
    W−1 and reach bandwidths within the range of 2 to 5 MHz at a reverse voltage of
     − 2 V. At the system level, we demonstrate a spectral efficiency and a data rate
    of 6.6 b s−1 Hz−1 and 20 Mb s−1, respectively. A comparison between rigid and
    flexible samples shows absolute differences of only 0.2 b s−1 Hz−1 and 2.9 Mb
    s−1 for the spectral efficiency and the data rate, respectively. The presented
    results utilizing an industrially relevant printing technique such as inkjet printing
    will contribute to the future implementation of organic receivers into indoor
    VLC applications and provide the basis for incorporating OPDs into advanced multi-device
    systems such as IoT and optical wireless sensor networks. Results and discussion
    Design and fabrication Figure 1 shows the different architectures as well as the
    layouts employed in the fabricated OPDs. The device which fabrication was based
    on the spin-coating method is presented in Fig. 1a. This device was fabricated
    on an indium tin oxide (ITO) covered glass substrate. ITO was used as a transparent
    electrode onto which a SnO2 film used as a hole-blocking layer, and the bulk heterojunction
    (BHJ) active layer formed by the donor poly(3-hexylthiophene) (P3HT) and the small-molecule
    acceptor 5,5′-[[4,4,9,9-Tetraoctyl-4,9-dihydro-s-indaceno[1,2-b:5,6-b’]dithiophene-2,7-diyl]bis(2,1,3-benzothiadiazole-7,4-diylmethylidyne)]bis[3-ethyl-2-thioxo-4-thiazolidinone]
    (IDTBR) were fabricated by spin coating. A MoO3 film used as electron-blocking
    layer, and Ag used as the complementary electrode were vacuum evaporated. The
    pixel active area of this and all other employed devices is defined by the overlap
    between the electrodes and is equal to 1 mm2. Figure 1b corresponds to a partially-printed
    device. In this case, the P3HT:IDTBR active layer was inkjet-printed, while the
    rest of the stack remained the same as in the spin-coated sample. Reduced material
    consumption and spatial separation between devices can be readily seen when comparing
    the photograph of the fabricated device to that of the spin-coated sample. Figure
    1c corresponds to a fully-printed device, which was fabricated on a poly(ethylene
    2,6-naphthalate) (PEN) flexible substrate. In terms of architecture, the fully
    inkjet-printed device consists of Ag as the bottom electrode, SnO2 as the hole-blocking
    layer, P3HT:IDTBR as the BHJ active layer, and poly(3,4-ethylenedioxythiophene)
    polystyrene sulfonate (PEDOT:PSS) as the top transparent electrode. Due to their
    compatibility with flexible substrates and the use of industrially relevant printing
    techniques, the fabricated printed OPDs could potentially facilitate numerous
    flexible and wearable applications in a variety of fields. Herein, we investigate
    their suitability for indoor VLC systems and the change in performance that occurs
    when the fabrication process is changed from spin coating to inkjet printing.
    Figure 1 OPD architectures. (a) Material stack and layout of the spin-coated devices.
    (b) Material stack and layout of the partially-printed devices. In this case the
    P3HT:IDTBR active layer was inkjet-printed. (c) Material stack and layout of the
    fully-printed devices. In this case all materials were inkjet-printed. Full size
    image Characteristics of OPDs for different architectures Figure 2 shows the J-V
    characteristics, spectral responsivity (SR), and 3 dB-bandwidth of the three different
    architectures being investigated. It is important to point out that, the ink formulations
    optimized for inkjet printing differ from those optimized for spin coating. Consequently,
    the active layers of the spin-coated samples shown here use chlorobenzene (CB)
    as their solvent system, (Fig. 1a), different from the active layers of printed
    samples using 1,2-Dichlorobenzene (DCB), (see Fig. 1b,c). This change in solvent
    composition is necessary for better compatibility with the printing process, particularly
    in order to avoid clogging of the printing nozzles. By taking advantage of the
    higher boiling-point of DCB as compared to CB, we achieve a lower evaporation
    rate and an optimized ink jetting from the printing cartridge. Figure 2a–c presents
    the J-V characteristics, where each device was measured under dark and monochromatic
    light (λ = 520 nm) conditions with the incident optical power reaching up to 15 mW cm−2.
    As it can be seen, the three architectures show a characteristic rectifying behavior
    with dark currents of 189, 39, and 227 nA cm−2 measured at  − 2 V for the spin-coated,
    partially-printed, and fully-printed cases, respectively. These values demonstrate
    a successful device stack for the different architectures employed. As expected,
    the current output of the devices increases with the illumination intensity. Supplementary
    Fig. S1a–c shows that, when biased at  − 2 V, the devices provide a good linear
    dynamic range (LDR) with values of 81, 97, and 79 dB for the spin-coated, partially-printed
    and fully-printed cases, respectively. Figure 2 OPD characteristics for different
    architectures. (a–c) J-V characteristics of the three different architectures
    studied. A monochromatic light source (λ = 520 nm) was used for illumination.
    (d–f) Spectral responsivities of the three different architectures studied. (g–i)
    Cut-off frequency measurements of the three different architectures studied. Full
    size image Figure 2d–f depicts the SR values at the bias voltage values of 0 and
     − 2 V. The three architectures exhibit a broad photoresponse, characteristic
    of their P3HT:IDTBR active layer, covering the complete visible range and extending
    into the near-infrared (NIR). The current response of the spin-coated and the
    partially-printed samples is consistent across the entire spectral range, exhibiting
    a steady increase in the response from 400 nm to their peak value at 740 nm, followed
    by a gradual decrease until 850 nm. In terms of the responsivity of the devices,
    the change in solvent system did not have a strong effect as seen in Fig. 2d and
    e. The detailed investigation of the BHJ morphological differences of the two
    ink formulations and relation to the figures-of-merit are out of the scope of
    this study. However, they should be addressed in future work as they are fundamental
    to the optimization of OPD-based VLC systems. The fully-printed device shows a
    more abrupt increase in responsivity with a shoulder-like shape between 400 and
    600 nm. The fully-printed device also yields higher overall photoresponse performance.
    This change in SR can be attributed to the different active layer thicknesses
    in the samples. While the spin-coated and partially printed devices have a similar
    active layer thickness of ~ 220 and ~ 240 nm; the fully-printed devices have an
    increased thickness of ~ 350 nm. A thicker active layer allows for higher light
    absorption and can lead to an increase in photoresponse. In addition, they can
    also lead to changes in the shape of their SR profiles. This can be further appreciated
    in Supplementary Fig. S2, which shows the SR of the fully-printed sample as compared
    to another fully-printed sample of thinner (~ 180 nm) active layer thickness.
    The fully-printed sample with decreased thickness shows a similar spectral shape
    to the partially-printed and spin-coated samples. The change in SR suggests that
    even moderate adjustments in active layer thickness can significantly affect the
    SR. These findings are in agreement with previous literature investigating the
    effects of varying BHJ thickness39,40,41,42. The performance of fully-printed
    devices with thinner active layers was lower compared to the devices with thicker
    active layers, therefore, devices with thicker active layers were employed in
    further characterizations. In general, all devices reach responsivities above
    200 mA W−1 with the fully-printed architecture achieving a maximum SR value of
    376 mA W−1. The peak responsivity of the spin-coated sample is 222 mA W−1 (at
    740 nm,  − 2 V), and the peak responsivity of the partially-printed sample is
    268 mA W−1 (at 740 nm,  − 2 V). These SR values are similar to other values reported
    for state-of-the-art devices7,8,12,43,44,45,46,47,48,49,50. Figure 2g–i shows
    the frequency response of the fabricated OPD devices. The OPDs reached  − 3 dB
    cut-off frequency bandwidths (ƒ−3 dB) between 2 and 5 MHz at a bias voltage of
     − 2 V. The spin-coated architecture achieved the maximum value with a ƒ−3 dB
    of 4.4 MHz. In contrast, the partially-printed samples on glass show a ƒ−3 dB
    of 2.8 MHz. As a comparison, we investigated the bandwidth of a reference spin-coated
    sample containing an active layer with the same solvent system (DCB) as the printed
    devices (see Supplementary Fig. S3). It can be seen that both samples achieve
    a similar performance with a ƒ−3 dB ~ 2.8 MHz at  − 2 V. This result suggests
    that the fabrication process can be switched from spin-coating to printing while
    maintaining the bandwidth of the device. However, the variation in performance
    shown from Fig. 2g–h originates from the different processing of the active layers
    such as the ink formulation. Previous studies have shown that a change of solvent
    from CB to the higher boiling point DCB can directly influence the domain size
    and crystallinity in P3HT-based blends51,52, including P3HT:IDTBR BHJs53. Changes
    in our ink formulations can, therefore, influence the exciton diffusion and the
    charge separation efficiency via these mechanisms, directly affecting the performance
    of the device. Furthermore, the mechanically-flexible samples reached a maximum
    ƒ−3 dB of 2.1 MHz at  − 2 V, demonstrating a high device performance even for
    fully-printed, flexible samples. The lower ƒ−3 dB is attributed to the higher
    active layer thickness in the fully-printed device as compared to the partially-printed
    device. This is supported by previous results from our group showing that a P3HT:IDTBR
    active layer thickness of ~ 200 nm represents a well-balanced trade-off between
    the transit- and RC-limit for our devices, and showing that a further increase
    in the active layer thickness leads to an increase in the transit time and thus
    to a drop in ƒ−3 dB6. In general, the reported ƒ−3 dB values are among the highest
    values reported for OPDs5,30,31, and are higher than those of other OPDs previously
    tested in VLC systems32,33,34,35,36,37,54. OPDs-based VLC Figure 3a shows a schematic
    block diagram of the VLC system being employed. Here, we have adopted the multi
    carrierless amplitude and phase (m-CAP) modulation format, and its generation
    is outlined in the literature55. First, the m-CAP signal generated in Matlab was
    uploaded to an arbitrary waveform generator, the output of which is used for intensity
    modulation of the LED (an OSRAM LW W5SM with a bandwidth of 1.2 MHz) via the bias-tee
    module. The spectrum of the LED, which matches the OPD responsivity range, can
    be seen in Supplementary Fig. S4. Moreover, a focal lens was used at the transmitter
    to increase the optical power level at the receiver (i.e. higher signal-to-noise
    ratio (SNR)), resulting in an intensity of 21 mW cm−2. At the receiver, the different
    OPD architectures, namely, spin-coated, partially-printed, and fully-printed,
    were tested individually. A transimpedance amplifier (Thorlabs AMP140) was used
    to boost the regenerated m-CAP signal level. The captured signal using a real-time
    oscilloscope was processed in the Matlab domain in order to regenerate the estimated
    data stream for comparison with the transmitted data sequence to determine the
    bit error rate (BER). Figure 3 Integration of OPDs within VLC system. (a) Schematic
    block diagram of the VLC system. In the presented system, the three different
    OPD architectures were individually tested as receivers. (b) Measurements of spectral
    efficiency and data rate for the three different OPD architectures. The fully
    printed samples were tested both in rigid and mechanically flexed scenarios. The
    inset shows a schematic of the sample under flat and bent conditions. The bending
    radius on the test structure used for bending was set to 50 mm. (c) Photograph
    of the fully-printed and flexible OPD samples conforming to different curved surfaces.
    Full size image Figure 3b displays the results of the VLC measurements for the
    different OPD architectures. The figure shows the maximum system data rate and
    spectral efficiency of the described OPDs. The corresponding spectrums received
    during measurement can be seen in Supplementary Fig. S5. Note, all measurements
    were performed with the OPD biased at  − 2 V and meeting the target 7% forward
    error correction BER limit. Moreover, in our VLC-system we are limited by the
    3 dB bandwidth of the LED and the photodetectors. As mentioned above, the data
    in Fig. 3b was obtained by transmitting an m-CAP signal through the LED + OPD
    channel (see Fig. 3a). This modulation format allows us to divide the occupied
    bandwidth into subbands that are affected by the distortion independently and
    load a different amount of data into each subband allowing us to effectively use
    a higher bandwidth beyond the 3 dB bandwidth of our devices. The maximum spectral
    efficiency and the maximum data rate were measured at 1 and 5 MHz, respectively.
    In both figures of merit, we observed an equivalent operation for the different
    devices. While the spectral efficiency for the spin-coated OPD was the highest
    at 6.6 b s−1 Hz−1, this corresponds to only 0.3 and 0.2 b s−1 Hz−1 absolute difference
    with respect to the partially- and fully-printed OPDs, respectively. For the obtained
    data rates, we observed a similar outcome with an absolute difference of only
    2.9 Mb s−1 between the spin-coated and printed samples. In this case, the maximum
    data rate corresponds to 20 Mb s−1. The achieved data rates are sufficient for
    several applications including (i) indoor IoT for identification and promotion
    information, control signals, and indoor localization21, (ii) intelligent transportation
    systems22, and (iii) E-health such as communication between on-body sensors to
    IoT nodes23. Overall, the results show that all OPD devices exhibit properties
    that are suitable for VLC systems with performance comparable to other advanced
    OPDs used in VLC systems19,32,33,34,35,36,37,38. Most importantly, we have demonstrated
    that the developed OPDs maintain their performance in the VLC system even after
    adopting the fabrication method based on the fully-inkjet-printed process deposited
    on a flexible substrate. This is noteworthy because the printed samples possess
    bending capabilities and manufacturing advantages that may be exploited for a
    variety of new applications such as IoT technologies, wearable and on-body sensors.
    In this regard, we also tested the performance of the OPDs under mechanical stress
    by measuring both the spectral efficiency and data rate at different bending radiuses
    of the receiver substrate. As shown in Fig. 3b, the fully printed OPDs were able
    to maintain their performance under the bending conditions. The inset in Fig.
    3b shows a schematic of the bending conditions, where a semi-cylindrical platform
    with a bending radius of 50 mm was used for bending the sample. A bending radius
    of 100 mm yielded the same results maintaining the achievable spectral efficiency
    and data rate. Incompatible with the OPD-encapsulation method (i.e. lack of mechanical
    flexibility of the encapsulation glue), no tests were carried out in the VLC system
    for bending radiuses below 50 mm. Nevertheless, as depicted in Fig. 3c, a device
    with a bending radius of ≥ 50 mm can have several useful applications as it resembles
    the positioning of the sample over many curved surfaces such as bottles or tubes,
    as well as the hand or forearm of a person. For complementarity, we characterized
    the SR of the flexible samples at radiuses of 100, 50, 10 and 5 mm. These measurements
    were performed outside the VLC system and inside a nitrogen-filled glovebox, where
    we measured the generated photocurrent of OPDs without encapsulation and under
    monochromatic light (λ = 520 nm) at an intensity of 10 mW cm−2. The results displayed
    in Supplementary Fig. S6 show that our devices are capable of maintaining a high
    and consistent photoresponse despite exposure to the increased bending conditions.
    Table 1 summarizes the VLC and SR measurements for the OPD samples under different
    bending conditions. In terms of spectral response, we observed a maximum absolute
    difference of only 17 mA W−1 when comparing the flexible sample under bent and
    flat conditions. Thus, we expect that with enhanced encapsulation compatibility
    of the OPDs used in the VLC system, the devices could also maintain a consistent
    data rate and spectral efficiency performance even under the increased bending
    conditions that we tested in terms of photoresponse. Although modifications to
    the encapsulation method may be necessary for radiuses below 50 mm, radiuses of ≥ 50
    mm remain highly favorable as compared to rigid samples. Thus, we consider the
    presented OPD devices to be a promising option in IoT applications with uneven
    and curved surfaces. Table 1 Summary of VLC and SR measurements performed on the
    different OPD samples. Full size table Conclusion In summary, we have, for the
    first time, demonstrated the successful integration of high-performance OPDs with
    different architectures into an indoor VLC system. The presented devices offer
    improved performance compared with other OPDs previously reported in literature.
    We reported, ƒ−3 dB values between 2 and 5 MHz at  − 2 V and visible-range SR
    values above 200 mA W−1 for all architectures. By switching the fabrication process,
    we showed that the performance of the developed OPDs in a VLC link can be maintained
    despite shifting the fabrication method to a fully-inkjet-printed process deposited
    on a mechanically flexible substrate. We showed that the absolute difference in
    the spectral efficiency is only 0.2 b s−1 Hz−1 between rigid and flexible samples.
    With the bending capabilities, fast detection speeds, and manufacturing advantages
    of the printed samples, we expect these devices to be used in IoT applications
    with curved surfaces as part of VLC systems. Future work will aim to explore new
    encapsulation methods for the developed OPDs oriented towards high mechanical
    flexibility in order to further facilitate their implementation into new wearable
    applications and into other advanced and complex systems compatible with IoT.
    Methods OPD fabrication The fabrication of the devices via spin coating and inkjet
    printing was performed in air inside a cleanroom environment. Only the preparation
    of active layer solutions and the annealing of the deposited active layers was
    performed inside a glovebox with nitrogen environment. For encapsulation, devices
    were covered with a curable adhesive. The processing of the spin-coated and partially-printed
    OPDs took place over glass substrates containing pre-structured ITO electrodes.
    SnO2 layers (25 nm) were spin-coated on top using a solution of commercial SnO2
    nanoparticle ink (Avantama N-31). Subsequently, the samples were annealed at 120
    °C for 5 min. Independent solutions of P3HT (RIEKE Metals) and the small-molecular
    IDTBR (1-MATERIALS) were prepared in CB (40 mg/mL) for the spin-coated devices
    or DCB (20 mg/mL) for the partially-printed devices and then mixed in a 1:1 ratio
    to form the BHJ. Thereafter, the active layers were either spin-coated (220 nm),
    or inkjet-printed (240 nm) followed by an annealing step at 140 °C for 10 min.
    Afterwards, MoO3 (30 nm) and silver (100 nm) layers were thermally evaporated
    on top to complete the devices. The processing of the fully-printed OPDs took
    place over PEN foils by using a Dimatix printer (DMP 2831), with 10 pL 16 nozzle
    Fujifilm Dimatix cartridges. In this case, an Argon plasma treatment was performed
    on the PEN substrates for one minute in order to adjust the surface energy prior
    to the OPD fabrication. Afterwards, Ag bottom electrodes (100 nm) were printed
    using a 30–35 wt% Ag nanoparticle ink (Sigma-Aldrich TGME Silver Dispersion) and
    annealed at 120 °C for 10 min. The SnO2 layers were then printed on top of the
    Ag electrodes with a modified solution using the commercial nanoparticle ink combined
    with diethylene glycol (DEG) in a 2:1 mixture. After this, the samples were annealed
    at 120 °C for 5 min. The BHJ solution was formed through preparation of independent
    solutions of P3HT (RIEKE Metals) and IDTBR (1-MATERIALS) in DCB (20 mg/mL) that
    were stirred overnight and then mixed together (1:1). The resulting solution was
    inkjet-printed (350 nm) in air, followed by an annealing step inside a N2-filled
    glovebox at 140 °C for 10 min. Finally, a transparent electrode (300 nm) based
    on PEDOT:PSS (Clevios FHC-Solar Heraeus) with 0.3 vol% of Zonyl FS-300 (Fluka
    analytical) was inkjet printed as top electrode and annealed at 120 °C for 5 min
    to complete the OPD stack. Filtering of all solutions was done using 0.45 µm polyvinylidene
    fluoride (PVDF) filters. The inkjet-printing waveforms were designed independently
    for each layer. To determine the thickness of the spin-coated and printed layers
    we employed a surface profilometer (Veeco, Dektak 150). OPD characterization at
    device level To measure the current–voltage (I–V) characteristics we utilized
    an Agilent 4155 C semiconductor parameter analyzer. I-V measurements under illumination
    were performed using an LED light source with a wavelength of λ = 520 nm. The
    LED was powered with a Keithley 2636 A source measure unit (SMU). The light intensity
    was adjusted using neutral density (ND) filters (Thorlabs NDUVxxA/NE5xxB). To
    perform SR measurements a monochromator (Acton SP-2150i) was used to selectively
    filter light from a 450 W OSRAM XBO Xenon discharge lamp. The light was modulated
    at a frequency of 173 Hz with a chopper wheel, and the OPD signal was amplified
    using a Femto DHPCA-100 amplifier. An SR830 lock-in amplifier was used to measure
    the output signal. To characterize the bandwidth, we varied the frequency of a
    square-light signal and measured the transient current under illumination. The
    light was modulated using an Agilent 33,522 A function generator. As a light source,
    we used an Oxxius LBX520 laser. The signal was recorded using an Agilent DSO 6102 A
    oscilloscope. Characterization of OPDs in VLC link For the VLC with OPD-based
    receiver, we generated the data stream and converted into the m-CAP in the Matlab
    domain and then loaded it into an arbitrary function generator (Teledyne LeCroy
    T3AWG3252) to generate the electrical version of the m-CAP signal. The m-CAP signal
    was used for intensity modulation of the LED (OSRAM LW W5SM, 1.2 MHz) via a bias
    tee (Mini-Circuits BIAS-TEE ZFBT-4R2GW). A focusing lens was used at the transmitter
    in order to increase the optical power level at the receiver. To bias the OPDs
    during the measurements we utilized a SMU. The output of the OPD was amplified
    using a transimpidance amplifier (Thorlabs AMP140) with the gain and MHz bandwidth
    of 10 kV A−1 and 10 MHz, respectively, the output of which was captured using
    a real-time oscilloscope (Keysight DSO0104A) for further processing in the Matlab
    domain. After down-sampling and demodulation, we compared the recovered M-ary
    quadrature amplitude modulation (M-QAM) symbols with the transmitted data for
    BER estimation. We used a BER that corresponds to the limit of 7% forward error
    correction. To improve the system throughput, we used a pilot binary phase-shift
    keying (BPSK) signal to load an appropriate k-value to individual subcarriers
    based on the measured SNR. As an optimization technique, we used a β-optimization
    as described in56. Data availability The data that supports the findings of this
    study are available from the corresponding author upon reasonable request. References
    Nabet, B. Photodetectors: Material, Devices and Applications (Elsevier-Woodhead,
    2016). Google Scholar   Yun, I. Photodiodes-From Fundamentals to Applications
    (InTechOpen, 2012). Book   Google Scholar   Forrest, S. R. The path to ubiquitous
    and low-cost organic electronic appliances on plastic. Nature 428, 911–918 (2004).
    Article   ADS   CAS   PubMed   Google Scholar   Sekitani, T. & Someya, T. Stretchable,
    large-area organic electronics. Adv. Mater. 22, 2228–2246 (2010). Article   CAS   PubMed   Google
    Scholar   Strobel, N., Seiberlich, M., Eckstein, R., Lemmer, U. & Hernandez-Sosa,
    G. Organic photodiodes: Printing, coating, benchmarks, and applications. Flex.
    Print. Electron. 4, 043001 (2019). Article   CAS   Google Scholar   Strobel, N.,
    Seiberlich, M., Rödlmeier, T., Lemmer, U. & Hernandez-Sosa, G. Non-fullerene-based
    printed organic photodiodes with high responsivity and megahertz detection speed.
    ACS Appl. Mater. Interfaces 10, 42733–42739 (2018). Article   CAS   PubMed   Google
    Scholar   Gasparini, N. et al. Visible and near-infrared imaging with nonfullerene-based
    photodetectors. Adv. Mater. Technol. 3, 1800104 (2018). Article   Google Scholar   Cesarini,
    M., Brigante, B., Caironi, M. & Natali, D. Reproducible, high performance fully
    printed photodiodes on flexible substrates through the use of a polyethylenimine
    interlayer. ACS Appl. Mater. Interfaces 10, 32380–32386 (2018). Article   CAS   PubMed   Google
    Scholar   Tordera, D. et al. A high-resolution thin-film fingerprint sensor using
    a printed organic photodetector. Adv. Mater. Technol. 4, 1900651 (2019). Article   CAS   Google
    Scholar   Ruiz-Preciado, L. A. et al. Monolithically printed all-organic flexible
    photosensor active matrix. NPJ Flex. Electron. 7, 6 (2023). Article   CAS   Google
    Scholar   Strobel, N. et al. Color-selective printed organic photodiodes for filterless
    multichannel visible light communication. Adv. Mater. 32, 1908258 (2020). Article   CAS   Google
    Scholar   Simone, G. et al. On the origin of dark current in organic photodiodes.
    Adv. Opt. Mater. 8, 1901568 (2020). Article   CAS   Google Scholar   Yoon, S.,
    Ha, Y.-H., Kwon, S.-K., Kim, Y.-H. & Chung, D. S. Fabrication of high performance,
    narrowband blue-selective polymer photodiodes with dialkoxynaphthalene-based conjugated
    polymer. ACS Photonics 5, 636–641 (2018). Article   CAS   Google Scholar   Lyons,
    D. M. et al. Narrow band green organic photodiodes for imaging. Org. Electron.
    15, 2903–2911 (2014). Article   CAS   Google Scholar   Wang, Y. et al. Narrowband
    organic photodetectors – towards miniaturized, spectroscopic sensing. Mater. Horiz.
    9, 220–251 (2022). Article   MathSciNet   CAS   PubMed   Google Scholar   Ghassemlooy,
    Z., Nero Alves, L., Zvanovec, S. & Khalighi, M. A. Visible Light Communications:
    Theory and Applications (CRC Press, 2017). Book   Google Scholar   Haas, H., Yin,
    L., Wang, Y. & Chen, C. What is LiFi?. J. Light. Technol. 34, 1533–1544 (2016).
    Article   ADS   Google Scholar   Chvojka, P. et al. Expanded multiband super-nyquist
    CAP modulation for highly bandlimited organic visible light communications. IEEE
    Syst. J. 14, 2544–2550 (2020). Article   ADS   Google Scholar   Tavakkolnia, I.
    et al. Organic photovoltaics for simultaneous energy harvesting and high-speed
    MIMO optical wireless communications. Light Sci. Appl. 10, 41 (2021). Article   ADS   CAS   PubMed   PubMed
    Central   Google Scholar   Chen, X., Min, C. & Guo, J. Visible light communication
    system using silicon photocell for energy gathering and data receiving. Int. J.
    Opt. 2017, 1–5 (2017). Article   Google Scholar   Nguyen, D. T., Park, S., Chae,
    Y. & Park, Y. VLC/OCC hybrid optical wireless systems for versatile indoor applications.
    IEEE Access 7, 22371–22376 (2019). Article   Google Scholar   Aksu, H., Babun,
    L., Conti, M., Tolomei, G. & Uluagac, A. S. Advertising in the IoT Era: Vision
    and challenges. IEEE Commun. Mag. 56, 138–144 (2018). Article   Google Scholar   Gardašević,
    G., Katzis, K., Bajić, D. & Berbakov, L. Emerging wireless sensor networks and
    internet of things technologies—foundations of smart healthcare. Sensors 20, 3619
    (2020). Article   ADS   PubMed   PubMed Central   Google Scholar   Haigh, P. A.,
    Ghassemlooy, Z. & Papakonstantinou, I. 1.4-Mb/s white organic LED transmission
    system using discrete multitone modulation. IEEE Photonics Technol. Lett. 25,
    615–618 (2013). Article   ADS   Google Scholar   Chaleshtori, Z. N., Zvanovec,
    S., Ghassemlooy, Z., Eldeeb, H. B. & Uysal, M. Coverage of a shopping mall with
    flexible OLED-based visible light communications. Opt. Express 28, 10015 (2020).
    Article   ADS   PubMed   Google Scholar   Haigh, P. A., Ghassemlooy, Z., Rajbhandari,
    S. & Papakonstantinou, I. Visible light communications using organic light emitting
    diodes. IEEE Commun. Mag. 51, 148–154 (2013). Article   Google Scholar   Sajjad,
    M. T. et al. Fluorescent red-emitting BODIPY oligofluorene star-shaped molecules
    as a color converter material for visible light communications. Adv. Opt. Mater.
    3, 536–540 (2015). Article   CAS   Google Scholar   Haigh, P. A., Ghassemlooy,
    Z., Papakonstantinou, I. & Le Minh, H. 2.7 Mb/s with a 93-kHz white organic light
    emitting diode and real time ANN equalizer. IEEE Photonics Technol. Lett. 25,
    1687–1690 (2013). Article   ADS   Google Scholar   Nazari Chaleshtori, Z., Burton,
    A., Zvanovec, S., Ghassemlooy, Z. & Chvojka, P. Comprehensive optical and electrical
    characterization and evaluation of organic light-emitting diodes for visible light
    communication. Opt. Eng. 59, 1 (2020). Article   Google Scholar   García De Arquer,
    F. P., Armin, A., Meredith, P. & Sargent, E. H. Solution-processed semiconductors
    for next-generation photodetectors. Nat. Rev. Mater. 2, 16100 (2017). Article   ADS   Google
    Scholar   Shan, T., Hou, X., Yin, X. & Guo, X. Organic photodiodes: Device engineering
    and applications. Front. Optoelectron. 15, 49 (2022). Article   PubMed   PubMed
    Central   Google Scholar   Ghassemlooy, Z. et al. Visible light communications:
    3.75 Mbits/s data rate with a 160 kHz bandwidth organic photodetector and artificial
    neural network equalization [Invited]. Photonics Res. 1, 65 (2013). Article   Google
    Scholar   Haigh, P. A. et al. A 1-Mb/s visible light communications link with
    low bandwidth organic components. IEEE Photonics Technol. Lett. 26, 1295–1298
    (2014). Article   Google Scholar   Cho, S. et al. Small molecule based organic
    photo signal receiver for high-speed optical wireless communications. Adv. Sci.
    9, 2203715 (2022). Article   CAS   Google Scholar   Arredondo, B. et al. Visible
    light communication system using an organic bulk heterojunction photodetector.
    Sensors 13, 12266–12276 (2013). Article   ADS   PubMed   PubMed Central   Google
    Scholar   Salamandra, L. et al. A comparative study of organic photodetectors
    based on P3HT and PTB7 polymers for visible light communication. Org. Electron.
    81, 105666 (2020). Article   CAS   Google Scholar   Chow, C.-W. et al. Pre-distortion
    scheme to enhance the transmission performance of organic photo-detector (OPD)
    based visible light communication (VLC). IEEE Access 6, 7625–7630 (2018). Article   Google
    Scholar   Vega-Colado, C. et al. An all-organic flexible visible light communication
    system. Sensors 18, 3045 (2018). Article   ADS   PubMed   PubMed Central   Google
    Scholar   Armin, A. et al. Thick junction broadband organic photodiodes: Thick
    junction broadband organic photodiodes with extremely low dark current. Laser
    Photonics Rev. 8, 924–932 (2014). Article   ADS   CAS   Google Scholar   Armin,
    A. et al. Quantum efficiency of organic solar cells: Electro-optical cavity considerations.
    ACS Photonics 1, 173–181 (2014). Article   CAS   Google Scholar   Armin, A., Jansen-van
    Vuuren, R. D., Kopidakis, N., Burn, P. L. & Meredith, P. Narrowband light detection
    via internal quantum efficiency manipulation of organic photodiodes. Nat. Commun.
    6, 6343 (2015). Article   ADS   CAS   PubMed   Google Scholar   Nickel, F. et
    al. Spatial mapping of photocurrents in organic solar cells comprising wedge-shaped
    absorber layers for an efficient material screening. Sol. Energy Mater. Sol. Cells
    104, 18–22 (2012). Article   CAS   Google Scholar   Pu, K. et al. A flexible sensitive
    visible-NIR organic photodetector with high durability. Adv. Mater. Technol. 8,
    2300207 (2023). Article   CAS   Google Scholar   Huang, J. et al. Green-solvent-processed
    high-performance broadband organic photodetectors. ACS Appl. Mater. Interfaces
    15, 37748–37755 (2023). Article   CAS   PubMed   Google Scholar   Sung, M. J.,
    Yoon, S., Kwon, S.-K., Kim, Y.-H. & Chung, D. S. Synthesis of Phenanthro[1,10,9,8-
    cdefg]carbazole-based conjugated polymers for green-selective organic photodiodes.
    ACS Appl. Mater. Interfaces 8, 31172–31178 (2016). Article   CAS   PubMed   Google
    Scholar   Byeon, H. et al. Flexible organic photodetectors with mechanically robust
    zinc oxide nanoparticle thin films. ACS Appl. Mater. Interfaces 15, 10926–10935
    (2023). Article   CAS   PubMed   Google Scholar   Liess, A. et al. Ultranarrow
    bandwidth organic photodiodes by exchange narrowing in merocyanine H- and J-aggregate
    excitonic systems. Adv. Funct. Mater. 29, 1805058 (2019). Article   Google Scholar   Biele,
    M. et al. Spray-coated organic photodetectors and image sensors with silicon-like
    performance. Adv. Mater. Technol. 4, 1800158 (2019). Article   Google Scholar   Shafian,
    S. & Kim, K. Panchromatically responsive organic photodiodes utilizing a noninvasive
    narrowband color electrode. ACS Appl. Mater. Interfaces 12, 53012–53020 (2020).
    Article   CAS   PubMed   Google Scholar   Lan, Z. et al. Filter-free band-selective
    organic photodetectors. Adv. Opt. Mater. 8, 2001388 (2020). Article   CAS   Google
    Scholar   Kadem, B. Y., Hassan, A. K. & Cranton, W. The effects of organic solvents
    and their co-solvents on the optical, structural, morphological of P3HT:PCBM organic
    solar cells. in AIP Conference Proceedings 1758, 020006-1-020006–9 (2016). Yusli,
    M. N., Way Yun, T. & Sulaiman, K. Solvent effect on the thin film formation of
    polymeric solar cells. Mater. Lett. 63, 2691–2694 (2009). Article   CAS   Google
    Scholar   Liu, J., Zeng, S., Jing, P., Zhao, K. & Liang, Q. Investigating the
    effect of cosolvents on P3HT/O-IDTBR film-forming kinetics and film morphology.
    J. Energy Chem. 51, 333–341 (2020). Article   Google Scholar   Chen, H. et al.
    A 1.9Mbps OFDM-based all-organic visible light communication system. in 2016 IEEE
    International Conference on Communication Systems (ICCS). Shenzhen, China, 1–6
    (IEEE, 2016). Haigh, P. A. et al. Multi-band carrier-less amplitude and phase
    modulation for bandlimited visible light communications systems. IEEE Wirel. Commun.
    22, 46–53 (2015). Article   Google Scholar   Pešek, P. et al. Experimental multi-user
    VLC system using non-orthogonal multi-band CAP modulation. Opt. Express 28, 18241
    (2020). Article   ADS   PubMed   Google Scholar   Download references Acknowledgements
    The authors thank Peter Krebsbach and Mervin Seiberlich for fruitful discussion.
    This work was financially supported by Deutsche Forschungsgemeinschaft (DFG, German
    Research Foundation) through grant HE 7056/6-1. This work was supported by CTU
    SGS project SGS23/168/OHK3/3T/13 and COST Action CA19111 (Newfocus). Funding Open
    Access funding enabled and organized by Projekt DEAL. Author information Authors
    and Affiliations Light Technology Institute, Karlsruhe Institute of Technology,
    Engesserstr. 13, 76131, Karlsruhe, Germany Luis Arturo Ruiz‐Preciado & Gerardo
    Hernandez-Sosa InnovationLab, Speyererstr. 4, 69115, Heidelberg, Germany Luis
    Arturo Ruiz‐Preciado & Gerardo Hernandez-Sosa Faculty of Electrical Engineering,
    Czech Technical University in Prague, Dejvice-Praha 6, 16627, Prague, Czech Republic
    Petr Pešek, Carlos Guerra-Yánez & Stanislav Zvánovec Optical Communications Research
    Group, Faculty of Engineering and Environment, Northumbria University, Newcastle,
    UK Zabih Ghassemlooy Institute of Microstructure Technology, Karlsruhe Institute
    of Technology, Hermann-von-Helmholtz-Platz 1, 76344, Eggenstein-Leopoldshafen,
    Germany Gerardo Hernandez-Sosa Contributions L.A.R.-P, P.P, Z.G. S.Z. and G.H.-S.
    discussed the methodology of the experiments. L.A.R.-P. fabricated the samples
    and performed the OPD characterization. P.P. and C.G.-Y. assembled the VLC measurement
    setup. L.A.R.-P, P.P. and C.G.-Y. measured the samples in the VLC system, analyzed
    the data and discussed the measurements. All authors discussed the results after
    the measurements. L.A.R.-P wrote the original manuscript. The manuscript results
    were discussed by all authors and all authors provided comments on the draft of
    the manuscript. Z.G., S.Z and G.H.-S. supervised the research. S.Z. and G.H.-S.
    coordinated the research project. Corresponding authors Correspondence to Stanislav
    Zvánovec or Gerardo Hernandez-Sosa. Ethics declarations Competing interests The
    authors declare no competing interests. Additional information Publisher''s note
    Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Supplementary Information Supplementary Information.
    Rights and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article''s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article''s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Ruiz‐Preciado, L.A., Pešek, P.,
    Guerra-Yánez, C. et al. Inkjet-printed high-performance and mechanically flexible
    organic photodiodes for optical wireless communication. Sci Rep 14, 3296 (2024).
    https://doi.org/10.1038/s41598-024-53796-5 Download citation Received 15 September
    2023 Accepted 05 February 2024 Published 08 February 2024 DOI https://doi.org/10.1038/s41598-024-53796-5
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Subjects Electrical and electronic engineering Electronic devices Fibre
    optics and optical communications Imaging and sensing Materials for devices Optoelectronic
    devices and components Polymers Semiconductors Comments By submitting a comment
    you agree to abide by our Terms and Community Guidelines. If you find something
    abusive or that does not comply with our terms or guidelines please flag it as
    inappropriate. Download PDF Sections Figures References Abstract Introduction
    Results and discussion Conclusion Methods Data availability References Acknowledgements
    Funding Author information Ethics declarations Additional information Supplementary
    Information Rights and permissions About this article Comments Advertisement Scientific
    Reports (Sci Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press
    releases Press office Contact us Discover content Journals A-Z Articles by subject
    Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open
    access Author & Researcher services Reprints & permissions Research data Language
    editing Scientific editing Nature Masterclasses Research Solutions Libraries &
    institutions Librarian service & tools Librarian portal Open research Recommend
    to library Advertising & partnerships Advertising Partnerships & Services Media
    kits Branded content Professional development Nature Careers Nature Conferences
    Regional websites Nature Africa Nature China Nature India Nature Italy Nature
    Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy
    choices/Manage cookies Legal notice Accessibility statement Terms & Conditions
    Your US state privacy rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Inkjet-printed high-performance and mechanically flexible organic photodiodes
    for optical wireless communication
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Plunge S.
  - Szabó B.
  - Strauch M.
  - Čerkasova N.
  - Schürz C.
  - Piniewski M.
  citation_count: '0'
  description: Input data collection, quality assurance and preparation are central
    but time_consuming steps in environmental modeling. Errors due to manual processing
    of model input data can result in an incorrect representation of an environmental
    system and may consequently lead to implausible model simulations. Correct input
    data preparation and thorough quality check at an early stage of the model setup
    procedure are essential to build confidence in model simulation results. Typically,
    in environmental model applications, many steps in the input data preparation
    phase have to be repeated with the inflow of new, additional or corrected data.
    In this study, we selected the widely used SWAT + ecohydrological model as an
    illustrative example to investigate challenges related to input data preparation.
    To assist in these tasks, we developed an R package named SWATprepR, which provides
    functions for typical and repeating SWAT + model input data preparation tasks.
    The package supports the preparation of weather input files, atmospheric deposition,
    soil parameters, crop rotations, and observed (control or calibration) data, to
    name a few, presently with focus on European applications. The SWATprepR functions
    are integrated in R script workflows and can help SWAT + modelers to avoid repetitive
    tasks, secure reproducibility and transparently document the data processing steps.
    Application of the package is illustrated with a test case of a SWAT + model for
    a small catchment in central Poland.
  doi: 10.1186/s12302-024-00873-1
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Get published
    Explore Journals Books About Login Environmental Sciences Europe About Articles
    Submission Guidelines Submit manuscript Research Open access Published: 11 March
    2024 SWAT + input data preparation in a scripted workflow: SWATprepR Svajunas
    Plunge, Brigitta Szabó, Michael Strauch, Natalja Čerkasova, Christoph Schürz &
    Mikołaj Piniewski  Environmental Sciences Europe  36, Article number: 53 (2024)
    Cite this article 648 Accesses 1 Altmetric Metrics Abstract Input data collection,
    quality assurance and preparation are central but time_consuming steps in environmental
    modeling. Errors due to manual processing of model input data can result in an
    incorrect representation of an environmental system and may consequently lead
    to implausible model simulations. Correct input data preparation and thorough
    quality check at an early stage of the model setup procedure are essential to
    build confidence in model simulation results. Typically, in environmental model
    applications, many steps in the input data preparation phase have to be repeated
    with the inflow of new, additional or corrected data. In this study, we selected
    the widely used SWAT + ecohydrological model as an illustrative example to investigate
    challenges related to input data preparation. To assist in these tasks, we developed
    an R package named SWATprepR, which provides functions for typical and repeating
    SWAT + model input data preparation tasks. The package supports the preparation
    of weather input files, atmospheric deposition, soil parameters, crop rotations,
    and observed (control or calibration) data, to name a few, presently with focus
    on European applications. The SWATprepR functions are integrated in R script workflows
    and can help SWAT + modelers to avoid repetitive tasks, secure reproducibility
    and transparently document the data processing steps. Application of the package
    is illustrated with a test case of a SWAT + model for a small catchment in central
    Poland. Introduction Rapid changes in the global environment bring challenges
    for the protection of ecosystems, which demand evidence-based policy making [1].
    Environmental modeling is an essential part of it, as policy makers can better
    understand the potential impacts of their decisions on the environment and identify
    the most effective strategies for mitigating or adapting to environmental challenges
    [2]. Transparency of methodology and reproducibility are crucial prerequisites
    for modeling studies intended to inform policy decisions [3]. Surprisingly, even
    among published peer-reviewed studies, these essential elements are often absent
    [4, 5]. This deficiency is creating legal challenges for policymakers, where environmental
    decision-making is based on modeling as input data, parameters, model assumptions
    and validation processes are most often questioned [3]. Furthermore, missing transparency
    and reproducibility in hydrological modeling studies raises concerns about the
    scientific quality of the results. This has resulted in an increasing demand from
    funding agencies and journals for the disclosure of the original data and code
    used in computations, highlighting its crucial role in scientific quality control
    [6]. The idea of scripted workflows designed for environmental modeling has been
    provided to solve this weakness in modeling studies [5, 7]. The main principle
    is that a common set of scripts is provided, with components designed to download
    and process input data, restructure model inputs to conform with required formats,
    run scenarios, extract results and compare them with the “baseline” or each other.
    If those scripts are prepared with commonly used open source scripting languages
    such as Python [8] or R [9] and released as packages via software sharing, collaboration
    and version control platforms (as GitHub, GitLab, Bitbucket, etc.), then multiple
    possibilities for collaboration on further development and tailored adaptation
    of the tools are presented. However, these tools have to be properly generalized
    and documented, which is rarely the case as modelers primarily use scripting to
    aid in their own modeling applications. For example, the Soil and Water Assessment
    Tool (SWAT), a semi-distributed process-based ecohydrological modeling tool, has
    been employed for a duration exceeding a couple of decades [10]. It is free, open
    source and has been used worldwide for a great variety of surface water environment-related
    questions [11,12,13,14]. Records in the SWAT Literature Database provide around
    6000 scientific papers [15]. Its official website also provides multiple solutions
    for model tailoring to different questions or tools to prepare different inputs.
    However, the application of those tools in sequential order requires adaptation
    to multiple file formats, software installations, understanding of different graphical
    user interfaces and involves a lot of manual manipulation, which is difficult
    to connect programmatically and is error-prone. Some open source scripted tools
    have been made available for SWAT and the most recent SWAT + model versions [16].
    As an example, SWAT + AW [5] presented a user-friendly, Python-based scripted
    workflow for catchment modeling. This tool utilizes preprocessed input data to
    facilitate the assemblance of a SWAT + model setup. Other Python-based examples
    to be mentioned are PySWAT [17], swatpy [18] or SpotSWATplus, which are mainly
    designed for coupling SWAT + /SWAT with the SPOTpy library [19] to edit model
    parameters and run it. However, those packages are not in active development at
    the moment. Currently (as of July 2022), two packages are promoted on the official
    SWAT model website Footnote 1 which are developed using R: SWATrunR [20] and R-SWAT
    [21]. These packages are designed primarily for model sensitivity assessment,
    calibration, validation and uncertainty analysis. Their application examples are
    reported in several studies [22,23,24] and, based on development records in GitHub
    repositories, these packages are actively developed and updated. There are other
    available SWAT + related open source R packages in active development, such as:
    SWATdoctR [25], designed for model setup verification; SWATfarmR [26]—a tool for
    preparing advanced agricultural management schedules for SWAT models, SWATbuildR
    [27, 28]—a comprehensive tool for building SWAT + model setups that includes connectivity
    between spatial objects. Those packages have been developed, used, updated, and
    tested within the EU-funded Horizon 2020 research and innovation OPTAIN project
    (OPtimal strategies to reTAIN and re-use water and nutrients in small agricultural
    catchments across different soil-climatic regions in Europe) and described in
    the project’s modeling protocol [28]. Presented packages could be connected to
    a scripted workflow. Nevertheless, the tools mentioned necessitate preprocessed
    data, overlooking the labor-intensive process of preparing/preprocessing input
    data. Collection of such data, quality checks and preparation are probably the
    most time consuming stages in environmental modeling, which require proper diligence
    and verification to ensure smoothness of modeling effort in later steps [29].
    Open source scripted tools provide automatization to save time and proper documentation,
    how raw data were treated. Moreover, corrected or updated data frequently become
    available during the lifetime of projects. Thus, having a scripted workflow covering
    the input data preparation step is highly beneficial. Additionally, during an
    input data preparation stage, numerous questions could arise for the modeler.
    For instance, data have been obtained, but how accurate, complete, consistent
    is it? How to identify outliers and how to treat them correctly (remove or correct)?
    What data cover the temporal and spatial resolution needed? How to obtain important
    model input data or parameters, which are not measured in the field, and how to
    fill data gaps? Another set of questions are connected to data format handling.
    For instance, which units are needed for a model and how to convert to them? What
    format and in which structure data should be delivered for a model? What files
    have to be provided or updated so the model finds and uses delivered data in the
    correct way? How to track data preparation, handling and correct mistakes? How
    to document data handling so it could be reported and also used to add data in
    later stages, if more data become available or if someone else needs to update
    a modeling project, etc.? These are just a few examples of simple questions, which
    are likely to arise during the data preparation stage. Answering and providing
    solutions for them is time consuming. A valuable way to move forward and save
    modelers’ time would be through a systematic approach that offers open-source
    tools with pre-existing answers to some of these questions, along with a flexible
    framework to seamlessly incorporate new solutions. To further foster the use of
    automatized and transparent open source modeling workflows, this article introduces
    a new tool developed as an R package named SWATprepR with a collection of functions
    to preprocess input data and derive some missing parameters for the SWAT + model
    by providing demonstration examples of the tool’s functionality for one case study.
    The current version of the tool covers the important steps of input data preparation
    in SWAT + modeling, including weather and climate, atmospheric deposition, soil
    parameters, crop rotation, observation, and point source data. Since the package
    is open-source, it allows users to easily integrate their own solutions to enhance
    its capabilities and address additional requirements. SWATprepR package features
    The SWAT + model is a process-based, semi-distributed, small watershed to river
    basin-scale model, and it requires multiple types of input data [30]. These data
    should be identified, collected, quality assessed, cleaned and transformed to
    model usable formats. Proper input data preparation is often the most labor intense
    and prolonged phase in the modeling process. Figure 1 provides an overview of
    the main data required by the model. Fig. 1 Data requirements for SWAT/SWAT + model
    Full size image SWATprepR 1.0.2 version of the package includes functions which
    provide solutions to six different topics: weather data, atmospheric deposition,
    climate projection data, soil parameters, crop rotation and point source data.
    Functions can be categorized as designed for (i) loading data from the templates
    or online sources; (ii) plotting and cleaning data; (iii) calculating missing
    model parameters/data; and (iv) writing model input files (see Table 1). Detailed
    examples of application are presented in the https://biopsichas.github.io/SWATprepR/
    website. This section is used to present an overview of existing functionalities.
    Yet other functionalities are easily accessible with different R packages as well
    (examples provided on package website). Table 1 SWATprepR package features and
    functionality (X-functionality is supported in version 1.0.0) Full size table
    Weather and climate input Weather data hold significant importance for hydrological
    models as it determines the form of precipitation (whether it''s solid or liquid)
    and drives major water fluxes: e.g., evapotranspiration, and water flows within
    various media. Consequently, significant emphasis must be placed on quality assurance
    for meteorological variables. The SWATprepR package presents multiple options
    in addressing this concern, as depicted in Fig. 2. Fig. 2 Main functions and their
    functionalities for weather data in SWATprepR package Full size image The main
    function is load_template(), which loads data from the Excel Footnote 2 template
    (named ‘weather_data.xlsx’) included in the package. The template requires typical
    information for meteorological stations, such as name, coordinates, altitude,
    and available time series for variables required by the SWAT + model (precipitation,
    temperature, wind speed, humidity, solar radiation). The function loads data into
    the R environment in a specific object, represented as a nested list format, which
    is used by this package. Once the data have been imported, the user can apply
    all other functions to the object. Two other functions could be applied to load
    weather data from different formats to the same object. The load_swat_weather()
    function can be used to load weather data directly from SWAT + input text files
    and the load_weather_netcdf() function can be applied to load weather data directly
    from Network Common Data Form (NetCDF) files format [31], which is often used
    to store large datasets, such as climate time series data. Loaded data can be
    examined with multiple functions. For example, function plot_weather() can be
    used to perform a quality check on time series data. This function generates an
    interactive plot that shows data from all available stations. It offers various
    options for aggregation over multiple time intervals and provides different summarization
    functions such as mean, median, sum, standard deviation, minimum, maximum, and
    more. By using the plot_weather_compare() function, the user can extend this capability
    to compare two datasets. Function plot_wgn_comparison() generates a plot for comparing
    weather statistical values for two datasets, which might be needed in the assessment
    of weather data from projected climate datasets. Upon loading and inspecting the
    data, the modeler may encounter situations in which certain stations have data
    gaps of different length. In such cases, different methods can be applied to fill
    in these gaps. For this purpose, the package provides the interpolate() function.
    To use this function, the modeler is required to provide a basin shapefile and
    a DEM raster file for the catchment area. Based on a user-defined grid spacing
    interval, the function creates virtual stations with interpolated weather variable
    data. The interpolation process is performed using the inverse distance weighting
    (IDW) method with a user-defined exponent parameter [32]. While there exist more
    sophisticated techniques for spatial interpolation of weather data [33], the IDW
    method has been widely used in different contexts for all weather variables required
    by SWAT + [34]. Another important input to SWAT + are weather statistical parameters,
    referred to as the input to the weather generator (WGEN or WGN). SWAT + uses the
    weather statistical data to fill gaps in daily weather data for short periods
    of time and to calculate plant growth initiation parameters. Despite the capability
    of in-build weather generator, the statistical data and its functionality are
    not recommended for simulating extended periods of missing data. To assist with
    this input, the official SWAT model website offers various tools, including the
    WGN Parameters Estimation Tool [35], WGN Excel macro [36], SWAT Precipitation
    Input Preprocessors and Dewpoint Estimation [37]. These tools require data preparation
    in different formats and demand familiarity with their respective functionalities.
    By using the SWATprepR package, in contrast, the modeler can calculate the required
    parameters with just a single command: prepare_wgn(), provided that weather data
    have been imported into the R environment. The remaining two functions, add_weather()
    and prepare_climate(), provide two options to write weather data into the SWAT + model
    setup database. The first function requires three elements: an object containing
    loaded weather data, an object containing the calculated weather generator parameters,
    and the SWAT + model setup database in.sqlite format. The prerequisite is that
    the setup database should not have any pre-existing weather station or weather
    generator parameters entered into it. The add_weather() function adds weather
    time series, weather station data, and weather generator parameters to the model
    setup. The prepare_climate() function provides the option to transform the weather
    time series data directly into the formatted text files, which are used by the
    SWAT + model executable. Atmospheric deposition input SWAT + provides the option
    to include the observed atmospheric nitrogen deposition data into the model simulation.
    The input file requires a reduced (NH4) and oxidized form (NO3) of nitrogen in
    dry (kg/ha/year) and wet deposition (mg/l). These data may be available at specific
    locations or collected with field measurements. Another source of such data are
    atmospheric models [38]. SWATprepR supports the extraction of atmospheric deposition
    data from such models and adds it to the SWAT + model setup database. The function
    get_atmo_dep() uses the basin boundary shapefile as an input and downloads the
    required atmospheric deposition data directly from Meteorological Synthesizing
    Centre—West (MSC-W) model output data provided by the European Monitoring and
    Evaluation Programme (EMEP). The EMEP domain covers the geographic area between
    30° N-82° N latitude and 30° W-90° E longitude [38]. Another function available
    in the current version of the package is add_atmo_dep(). To utilize it, the output
    from the get_atmo_dep() function and the path to the model setup database in.sqlite
    format is required. This function enables the incorporation of atmospheric deposition
    data into the model, allowing for the inclusion of deposition data ranging from
    daily to annual averages for a single station (Additional file 1). Soil parameters
    Soil characteristics are an important factor in determining the pathways of water
    when it reaches the land surface. Accurate soil parameters describing water and
    nutrient retention capacity and flow conditions in the soil matrix are essential
    for a successful and reliable SWAT + modeling study. Despite the current advancements
    in observations and data availability, in many study areas complete sets of required
    soil parameters are difficult, if at all possible, to obtain. Therefore the SWATprepR
    package includes a function, get_usersoil_table(), that simplifies the process
    of generating the complete set of soil parameters needed for the SWAT + model,
    i.e., moist bulk density, available water capacity, saturated hydraulic conductivity,
    moist albedo and Universal Soil Loss Equation soil erodibility factor. These parameters
    are derived automatically from commonly available soil datasets by pedotransfer
    functions and other equations available from the literature [39,40,41,42,43,44].
    To populate the SWAT + user soil parameter table, the modeler needs the following
    information for each soil layer within distinct soil types: Depth of layer; Percentage
    of clay defined as particles < 2 μm; Percentage of silt—2–50 μm; Percentage of
    sand—50–2000 μm; Soil organic carbon content in %. For assigning Hydrologic Soil
    Groups, tile drainage, depth to groundwater level and impervious layer data are
    needed. The theoretical documentation for the pedotransfer functions integrated
    in this function is presented in the Sect. “Lacking crop rotation data” of the
    OPTAIN SWAT + modeling protocol [28]. Crop rotation data The SWAT + model’s popularity
    can be partly attributed to the capabilities to simulate the impacts of agricultural
    land management on water resources and water quality. To fully utilize this functionality,
    the modeler is required to supply information about agricultural activities representative
    of the study area, with one of the most crucial pieces being crop rotation data.
    Such information is rarely available or freely accessible, especially in large
    or transboundary watersheds. In such cases, remote sensing data could be utilized
    to generate information about crop rotations for the selected time period. Although
    the SWATprepR package itself does not have a function for directly extracting
    remote sensing data, it does offer functions to work with results of open source
    scripts that perform this task. Google Earth Engine-based (GEE) remote sensing
    data extraction scripts were developed by Mészáros and Szabó [45] and described
    in a report of [46]. The script predicts crop types with a random forest method
    based on time series reflectance data of Sentinel 1A and 1B satellite radar images.
    The script generates a sequence of crop maps for each year as its output. To run
    this GEE script, the modeler needs the following input data: the shape of catchment
    boundary, continental or local crop data with coordinates as training points,
    and optionally, the boundaries of parcels or fields, if such data are available,
    which are added to the time series radar images selected based on the user-defined
    time period. The SWATprepR package provides two functions related to preprocessing
    (i) input data for the abovementioned GEE script and (ii) the derived time series
    crop map for the modeling. The first function, get_lu_points(), generates a set
    of training points for the remote sensing scripts. The second function, extract_rotation(),
    is designed for the extraction of crop rotation sequences per field. This data
    can be used with the SWATfarmR R package [26] to generate SWAT + model management
    input files. Observation data Observation data used for model calibration and
    validation are indispensable in most environmental model applications. Even though
    not strictly considered as model input data, they are required in the model preparation
    process for assessing and fine-tuning model performance. These data are usually
    collected and prepared along with other input data, and SWATprepR includes functions
    to quickly load, assess, plot, and clean monitoring data in the R environment
    (Fig. 3). It is important to emphasize that the SWATprepR functions were originally
    tailored for SWAT model users, but their versatility makes it straightforward
    to customize these functions for different variables or models as needed. Fig.
    3 Functions related to calibration/validation data in the SWATprepR package Full
    size image The load_template() function is used to load the data from the Excel
    template. A different template, named calibration_data.xlsx, is used to format
    the calibration data. After loading, the plot_cal_data() function can be applied
    to examine calibration and/or validation data time series for single or all the
    available gauge stations. Additionally, the plot_monthly() and plot_fractions()
    functions are designed for examining monthly aggregates and changes between ratios
    of different constituents. The plot_map() function is used to display the time
    series variables and station locations on interactive maps for assessing data
    availability, quality and variability in space. Two basic functions are included
    in the package to aid with identifying and correcting errors and inconsistencies
    in the time series data. The clean_wq() function can fix most common issues related
    to water quality observation data, such as addressing comma-dot misuse, converting
    units by applying molecular weight conversion factors (e.g., converting to active
    substance weight as NH4 to N-NH4), handling negative values, removing missing
    values, and updating zero-concentrations to minimum positive values (or defined
    part of it), etc. Another function, clean_outliers(), allows the user to identify
    and remove data outliers in the time series. Outliers are identified as values
    outside the defined range (as mean ± n * standard deviation) of values. Point
    source data Point sources are generally considered to represent municipal or industrial
    wastewater treatment plants’ discharge of treated sewage into the stream network.
    Discharge locations as well as the volume of effluents and the chemical characteristics
    of discharged water are typically needed to accurately represent the anthropogenic
    point source influence in a water quality model. The load_template() function
    in the SWATprepR package is used to load the data from an Excel template. An example
    template is included as a pnt_data.xlsx file. Once loaded into the R environment,
    the data can be examined for spatial and temporal consistency (using functions
    of ggplot2 or similar packages). The prepare_ps() function can be used to transfer
    the point source data into SWAT + model input file format. This function only
    requires a loaded point source object and a path to the model setup text files.
    SWATprepR demonstration case The developed SWATprepR tool was applied in a test
    case study of the Upper Zgłowiączka catchment. This catchment spans an area of
    150 km2 and is situated in central Poland. According to observation data for 2021,
    approximately 89% of this catchment is covered by arable land, while pastures
    account for 2%, forests for 5.5%, and urban and water areas for 3.5%. About 59%
    of the catchment''s territory is equipped with tile drains. The case study site
    featured two point sources, 14 meteorological stations situated both within and
    around the catchment, as well as 21 water quality and flow measurement stations.
    This specific case study site was selected as one of the 14 sites within the OPTAIN
    project [47]. Within the scope of the project, a fine scale SWAT + model is set
    up and used to facilitate the evaluation of environmental effectiveness of Natural/Small
    Water Retention Measures (NSWRMs). The detailed setup of the SWAT + model necessitated
    the collection of various types of data, aimed at providing comprehensive environmental
    insights into the local conditions. Figure 4 illustrates a subset of the geospatial
    data that were gathered for the selected catchment. The process of collecting
    this detailed data and preparing the models input data posed several challenges,
    all of which were successfully addressed through the utilization of functions
    within the SWATprepR package. Below, we provide several illustrative examples.
    Fig. 4 Upper Zgłowiączka GIS catchment data. a Water flow, water quality (Q/WQ),
    meteorological stations, point source locations, reaches and catchment boundary;
    b soil type map; c DEM map; d land use map with crop type specification for 2021,
    classes defined as in land cover/plant growth database [48] Full size image Scarce
    weather data Only one meteorological station was located inside the catchment.
    Yet it had data only for around 8 years, which was not enough for the foreseen
    modeling purposes. Additional meteorological data were collected from 13 stations
    in the vicinity of the catchment (within 40 km radius). All the collected meteorological
    data have been loaded with the SWATprepR load_template() function. Following that,
    the interpolate() function was applied to create a series of virtual weather stations
    within the catchment and the interpolation process was conducted for each day
    throughout the time series. This approach is a fast way to prepare a consistent
    spatially distributed meteorological data set for the catchment. Additionally,
    the actual meteorological stations had multiple gaps in the observation time series,
    which could be addressed with the interpolation procedure. Figure 5 provides an
    average percentage of available time series data within the catchment for all
    required variables for each meteorological station within the period of 1998–2022.
    By using the SWATprepR package, we generated a 2-km spaced grid that resulted
    in 38 virtual stations, which had a 100% data coverage for the selected time period
    and were located in the catchment. Fig. 5 Meteorological stations selected for
    data collection with the evaluation of data coverage in percentage for each variable
    (a) and virtual stations created with data coverage in percentage for all variables
    (b) Full size image Insufficient soil parameter information Collecting soil parameters
    necessary for the SWAT + model at a detailed level can pose challenges. For the
    Upper Zgłowiączka catchment soil type map with values of sand, silt, clay, and
    soil organic carbon content were available for each soil type with characteristic
    soil layering. The availability of basic soil information allowed the parameterization
    of required SWAT + soil parameters by the get_usersoil_table() function. This
    function utilizes sand, silt, clay, and carbon content to parameterize moist bulk
    density, available water capacity, saturated hydraulic conductivity, moist soil
    albedo, and the Universal Soil Loss Equation soil erodibility factor. Figures
    6 and 7 show the derived soil parameters of the function for the studied catchment.
    Additionally, hydrologic soil groups could be computed based on the data available
    on tile drainage, groundwater depth, and depth to water-impermeable layer. Detailed
    description of pedotransfer functions and methodologies applied to the calculation
    of parameters is presented in the SWAT + modeling protocol pages 81–92 [28]. Fig.
    6 Distribution of soil parameters values across three soil layers. Clay, silt,
    sand and soil organic carbon content (SOL_CBN) are used as get_usersoil_table()
    function input. Moist bulk density (SOL_BD), available water capacity (SOL_AWC),
    saturated hydraulic conductivity (SOL_K), moist soil albedo (SOL_ALB) and Universal
    Soil Loss Equation soil erodibility factor (USLE_K) calculated by the get_usersoil_table()
    function Full size image Fig. 7 Maps of calculated soil parameters for all three
    soil layers: a clay, b silt, c sand, and d soil organic carbon content, e moist
    bulk density, f available water capacity, g saturated hydraulic conductivity,
    h moist soil albedo, i Universal Soil Loss Equation (USLE) soil erodibility factor
    Full size image Unavailable atmospheric deposition data No locally collected atmospheric
    deposition data were available for the Upper Zgłowiączka catchment. SWATprepR
    the get_atmo_dep() function was used to retrieve atmospheric deposition data for
    the catchment and another package’s function the add_atmo_dep() was used for incorporating
    downloaded data into the SWAT + model setup. Figure 8 illustrates an example of
    atmospheric deposition data extracted and plotted for the case study area, where
    previously no data was available. Fig. 8 Atmospheric deposition data for the Upper
    Zgłowiączka catchment. NH4_DRY ammonia dry deposition (kg/ha/year), NH4_RF ammonia
    in rainfall (mg/l), NO3_DRY nitrate dry deposition (kg/ha/year), NO3_RF nitrate
    in rainfall (mg/l), NH4_DRY ammonia dry deposition (kg/ha/year), NO3_DRY nitrate
    dry deposition (kg/ha/year) Full size image Lacking crop rotation data In the
    case of the Upper Zgłowiączka catchment, crop data were accessible only for a
    single year—2021. This proved insufficient for generating the required crop rotation
    sequences for the foreseen modeling task. To address this issue, a Google Earth
    Engine-based script [45] was applied to identify crops for field parcels for previous
    years using Sentinel 1A and 1B satellite radar images. For the training of the
    crop classification model, local crop data were used. The crop maps were generated
    with a tool developed and validated as a part of the OPTAIN project [46]. Next,
    the SWATprepR extract_rotation() function was applied and field-based annual crop
    sequences were extracted. The obtained results are presented in Fig. 9. The catchment
    is predominantly characterized by winter wheat cultivation, accounting for approximately
    20% of all rotations, while winter wheat with corn represents another 13%, and
    winter wheat with sugar beets accounts for 7%. Additionally, corn-to-corn rotations
    make up 6%, winter wheat to barley—5.5%. These outputs were then utilized in conjunction
    with the SWATfarmR tool [26] to develop crop management schedules tailored to
    the specifics of the SWAT + model. Fig. 9 Generated crop maps for each year and
    each field in the catchment. Training data are based on farmers’ declarations
    for the year 2021. Meaning of crop codes available in SWAT model databases documentation
    available on https://swat.tamu.edu/media/69419/Appendix-A.pdf Full size image
    Limitations There are several limitations of the SWATprepR package that users
    should be aware of. It is challenging to list all potential limitations due to
    the diverse application cases users might have in mind. Nonetheless, we would
    like to highlight some examples to provide users with a better understanding of
    the current shortcomings in the SWATprepR version. The current version (as of
    January 2024) of interpolate() function only incorporates the IDW interpolation
    technique for weather data. Its effectiveness depends on factors such as location,
    topography, variable, and data gap length. The IDW method is widely used in meteorology
    because it is fast and easy to implement. However, ancillary data, such as elevation,
    cannot be incorporated and the method tends to generate “bull’s eye patterns”
    [49]. Moreover, as there is no extrapolation, all interpolated values are within
    the range of the data points [50]. Customization for alternative (e.g., probabilistic)
    methods, such as kriging [49], may be necessary, as IDW interpolation might not
    be suitable in certain conditions. The atmospheric deposition function get_atmo_dep()prepares
    inputs for the EMEP data domain for Europe, parts of northern Africa, and western
    Asia. It is not applicable to regions outside of this domain. The generated data
    are based on outputs of the MSC-W model [51] and thus afflicted with uncertainties.
    Additionally, EMEP updates its calculations yearly, providing information under
    new server links with slightly different coding, making it impossible to obtain
    the latest data without adjusting the function. Therefore, the get_atmo_dep()
    function is tailored to the last available version of EMEP data at the time of
    article preparation. Users should be aware of this if they intend to use the latest
    EMEP data, and tailor the functionality according to their needs. Users should
    also mind that the current version of the get_atmo_dep() function generates basin-averaged
    single time series without distinguishing between regions or stations, which may
    be an important shortcoming for large-scale model applications. The organic carbon
    content for each soil layer, required for the get_usersoil_table() function, can
    be challenging to obtain in many regions. Information needed for preparing soil
    hydrologic groups, such as impervious layer depth, depth to the high water table,
    or drainage status of soils, could be even more challenging to collect. Proxy
    data may be used, introducing potential inaccuracies and uncertainties. Based
    on soil texture and organic carbon content, the get_usersoil_table() function
    predicts hydrologically effective soil parameters, such as available water capacity
    and saturated hydraulic conductivity. The root mean squared error of the built-in
    pedotransfer functions were 0.048 cm3 cm−3 for available water capacity and 1.48
    cm day−1 for logarithmic ten transformed saturated hydraulic conductivity [52]
    on the test sets of the European Hydropedological Data Inventory point dataset
    [53]. This dataset includes temperate soils, the uncertainty of the pedotransfer
    functions in other regions is therefore unknown. Functions related to crop rotations
    are linked to application of the GEE-based remote sensing script [45] and SWATfarmR
    [26], potentially limiting utility for users with other types of land use maps,
    who will not make use of the SWATfarmR functionality. In case the GEE-based crop
    classification is utilized, users should be aware of possible classification errors,
    which can be reduced by incorporating a sufficient amount of local training data.
    Other technical limitations include interactive functions connected to plotting
    observation data, reliant on the R plotly package [51], which may encounter issues
    displaying data of large datasets. The current SWATprepR version''s point source
    data preparation function, prepare_ps(), does not load data with multi-annual
    averages, potentially posing challenges when constant point source loads are required
    in the model. The functions for data cleaning, clean_wq() or clear_outliers(),
    only include the most elementary techniques, without providing options for more
    advanced data cleaning methods, especially with regard to outlier detection [54,
    55]. These examples do not cover all limitations of the SWATprepR package, as
    there are many possible use cases. Yet, they provide users with an understanding
    of potential methodological, data-related, or technical constraints they might
    encounter. Conclusions and future work The SWATprepR package offers valuable tools
    and incorporates effective techniques to assist SWAT + modelers in preparing their
    models. One of the primary challenges in creating a comprehensive model setup
    lies in the sheer volume of high-resolution spatial and temporal data required
    to feed into the model. With numerous variables, parameters and processes to adjust,
    modelers often find themselves overwhelmed, especially when data availability
    is not straightforward. Another challenge is the number of different file formats
    (spreadsheets, text files, relational databases, NetCDF, etc.), which all require
    different tools or approaches to manipulate. Errors or biases in the input data
    may make the entire available datasets untrustworthy or unusable. Consequently,
    modelers may accidently introduce errors into their setup or choose to omit critical
    information, significantly diminishing model reliability. Unfortunately, such
    shortcomings are often masked through subsequent parameterization of processes
    in calibration, leading to unreliable simulations that can have far-reaching implications
    for decision-making by end-users of the model. The use of scripted workflows like
    SWATprepR offers significant advantages. One of them is quick and easy error correction.
    When errors or inaccuracies are identified in the setup of SWAT models, scripted
    workflows make it straightforward to implement corrections. Instead of manually
    retracing and repeating steps, modelers can easily modify and rerun the scripts
    to ensure that the model setup aligns with the desired specifications. This saves
    time and reduces the likelihood of human errors during the correction process.
    Another advantage is easy adaptation to new, updated datasets. As new or updated
    datasets become available or as the project requirements change, scripted workflows
    prove invaluable. Modelers can efficiently integrate new data sources into the
    existing model setup by updating scripts. Furthermore, scripted workflows could
    also facilitate collaboration among modelers. When workflows are documented and
    shared, other researchers or modelers can readily understand the processes and
    parameters used in the SWAT modeling. This ease of comprehension allows for efficient
    collaboration, peer review, and the potential for others to build upon or extend
    the existing models. In addition, scripted workflows can be managed with version
    control systems like Git, ensuring a history of changes, easy tracking of modifications,
    and the ability to revert to previous states, if needed. This enhances the reproducibility
    and traceability of the modeling process. As well as automation through scripting
    ensures a high level of consistency across different runs of the SWAT model. This
    consistency is essential for producing reliable and comparable results in scientific
    research or environmental assessments. The model setup insights and parameter
    estimation methods discussed in this paper should prove invaluable to any modeler
    embarking on a journey to establish a dependable case study analysis using SWAT +.
    The SWATprepR package is open-source and will continue to undergo active development
    and enhancement in the foreseeable future, reducing its current limitations mentioned
    in the previous section. We extend an invitation to the modeling community to
    contribute to the evolution of these tools, adapt our proposed methods to strengthen
    their own models, perform rigorous quality checks, and ultimately, contribute
    to a more transparent and informed decision-making process through the utilization
    of the SWAT + model. Availability of data and materials The SWATprepR package,
    its documentation, and test data are freely available on https://github.com/biopsichas/SWATprepR.
    Notes The link to website https://swat.tamu.edu/software/ Excel format was chosen
    as the most commonly used format for handling spreadsheet data. References Pullin
    AS, Knight TM (2009) Doing more good than harm—building an evidence-base for conservation
    and environmental management. Biol Conserv 142:931–934. https://doi.org/10.1016/j.biocon.2009.01.010
    Article   Google Scholar   Schmolke A, Thorbek P, DeAngelis DL, Grimm V (2010)
    Ecological models supporting environmental decision making: a strategy for the
    future. Trends Ecol Evol 25:479–486. https://doi.org/10.1016/j.tree.2010.05.001
    Article   PubMed   Google Scholar   Özkundakci D, Wallace P, Jones HFE et al (2018)
    Building a reliable evidence base: legal challenges in environmental decision-making
    call for a more rigorous adoption of best practices in environmental modelling.
    Environ Sci Policy 88:52–62. https://doi.org/10.1016/j.envsci.2018.06.018 Article   Google
    Scholar   Vos MG de, Janssen SJC, Bussel LGJ van, et al (2011) Are environmental
    models transparent and reproducible enough? MODSIM2011. In: 19th International
    Congress on Modelling and Simulation. https://doi.org/10.36334/modsim.2011.g7.devos
    Chawanda CJ, George C, Thiery W et al (2020) User-friendly workflows for catchment
    modelling: towards reproducible SWAT+ model studies. Environ Modell Softw 134:104812.
    https://doi.org/10.1016/j.envsoft.2020.104812 Article   Google Scholar   Hutton
    C, Wagener T, Freer J et al (2016) Most computational hydrology is not reproducible,
    so is it really science? Water Resour Res 52:7548–7555. https://doi.org/10.1002/2016wr019285
    Article   ADS   Google Scholar   Coon ET, Shuai P (2022) Watershed workflow: a
    toolset for parameterizing data-intensive, integrated hydrologic models. Environ
    Modell Softw 157:105502. https://doi.org/10.1016/j.envsoft.2022.105502 Article   Google
    Scholar   Python Software Foundation (2023) Python Language Reference, version
    3.11. http://www.python.org. Accessed 10 Jan 2023 R Foundation (2023) The R Project
    for Statistical Computing 4.2. https://www.r-project.org/. Accessed 10 Jan 2023
    Arnold JG, Srinivasan R, Muttiah RS, Williams JR (1998) Large area hydrologic
    modeling and assessment part I: model development. JAWRA J Am Water Resour Assoc
    34:73–89. https://doi.org/10.1111/j.1752-1688.1998.tb05961.x Article   ADS   CAS   Google
    Scholar   Tan ML, Gassman P, Yang X, Haywood J (2020) A review of SWAT applications,
    performance and future needs for simulation of hydro-climatic extremes. Adv Water
    Resour 143:103662. https://doi.org/10.1016/j.advwatres.2020.103662 Article   Google
    Scholar   Gassman PW, Reyes MR, Green CH, Arnold JG (2007) The soil and water
    assessment tool: historical development, applications, and future research directions.
    Trans ASABE 50:1211–1250 Article   CAS   Google Scholar   Gassman PW, Yingkuan
    W (2015) IJABE SWAT special issue: innovative modeling solutions for water resource
    problems. Int J Agric Biol Eng 8:1–8 Google Scholar   Akoko G, Le TH, Gomi T,
    Kato T (2021) A review of SWAT model application in Africa. Water-sui 13:1313.
    https://doi.org/10.3390/w13091313 Article   Google Scholar   CARD&ISU (2023) SWAT
    literature database for peer-reviewed journal articles. https://www.card.iastate.edu/swat_articles/.
    Accessed 10 Jan 2023 Bieger K, Arnold JG, Rathjens H et al (2017) Introduction
    to SWAT+, a completely restructured version of the soil and water assessment tool.
    JAWRA J Am Water Resour Assoc 53:115–130. https://doi.org/10.1111/1752-1688.12482
    Article   ADS   Google Scholar   Ferreira DB (2019) PySWAT: a Python application
    for Input/Output analysis for the Soil and Water Assessment Tool (SWAT). https://github.com/davidbispo/PySWAT.
    Accessed 10 Jan 2023 Kmoch A (2022) swatpy: A set of python modules to work with
    SWAT2012 models. https://doi.org/10.5281/zenodo.6322023 Houska T, Kraft P, Chamorro-Chavez
    A, Breuer L (2015) SPOTting model parameters using a ready-made python package.
    PLoS ONE 10:e0145180. https://doi.org/10.1371/journal.pone.0145180 Article   CAS   PubMed   PubMed
    Central   Google Scholar   Schürz C (2019) SWATrunR: running SWAT2012 and SWAT+
    Projects in R. https://doi.org/10.5281/zenodo.3373859 Nguyen TV, Dietrich J, Dang
    TD et al (2022) An interactive graphical interface tool for parameter calibration,
    sensitivity analysis, uncertainty analysis, and visualization for the Soil and
    Water Assessment Tool. Environ Modell Softw 156:105497. https://doi.org/10.1016/j.envsoft.2022.105497
    Article   Google Scholar   Musyoka FK, Strauss P, Zhao G et al (2021) Multi-step
    calibration approach for SWAT model using soil moisture and crop yields in a small
    agricultural catchment. Water-sui 13:2238. https://doi.org/10.3390/w13162238 Article   Google
    Scholar   Maref N, Baahmed D, Bemmoussat K, Mahfoud Z (2022) SWAT model application
    for sediment yield modeling and parameters analysis in Wadi K’sob (Northeast of
    Algeria). https://doi.org/10.21203/rs.3.rs-2069353/v1 Yang C, Xu M, Fu C, et al
    (2022) Glacier hydrological process modeling based on improved SWAT+: a case study
    in the Upper Yarkant River Basin. https://doi.org/10.22541/au.164512280.00856493/v1
    Plunge S, Schürz C, Čerkasova N et al (2023) SWAT+ model setup verification tool:
    SWATdoctR. Environ Model Softw 171:105878. https://doi.org/10.1016/j.envsoft.2023.105878
    Article   Google Scholar   Schürz C (2023) SWATfarmR: Simple rule based scheduling
    of management operations in SWAT. https://github.com/chrisschuerz/SWATfarmR. Accessed
    10 Jan 2023 Schürz C (2022) SWATbuildR. https://git.ufz.de/optain/wp4-integrated-assessment/swat/bildr_script.
    Accessed 10 Jan 2023 Schürz C, Čerkasova N, Farkas C et al (2022) SWAT+ modeling
    protocol for the assessment of water and nutrient retention measures in small
    agricultural catchments. Zenodo. https://doi.org/10.5281/zenodo.7463395 ASABE
    N-21 H committee of (2017) Guidelines for calibrating, validating, and evaluating
    hydrologic and water quality (H/WQ) models. ASABE SWAT+ Website (2023) SWAT+ Documentation.
    https://swatplus.gitbook.io/io-docs/. Accessed 27 Dec 2023 Unidata (2023) Network
    Common Data Form (NetCDF). https://www.unidata.ucar.edu/software/netcdf/. Accessed
    29 Dec 2023 Shepard D (1968) A two-dimensional interpolation function for irregularly-spaced
    data. In: Proc 1968 23rd ACM Natl Conf. pp 517–524. https://doi.org/10.1145/800186.810616
    Ma YZ (2019) Geostatistical estimation methods: kriging. In: Ma YZ (ed) Quantitative
    geosciences: data analytics, geostatistics, reservoir characterization and modeling.
    Springer International Publishing, Cham, pp 373–401 Chapter   Google Scholar   Ozelkan
    E, Bagis S, Ozelkan EC et al (2015) Spatial interpolation of climatic variables
    using land surface temperature and modified inverse distance weighting. Int J
    Remote Sens 36:1000–1025. https://doi.org/10.1080/01431161.2015.1007248 Article   Google
    Scholar   Essenfelder AH (2016) SWAT weather database: a quick guide. https://doi.org/10.13140/rg.2.1.4329.1927
    Boisrame G (2011) WGNmaker4.xlsm manual. https://swat.tamu.edu/media/41586/wgen-excel.pdf.
    Accessed 10 Jan 2023 Liersch S (2003) The Programs dew.exe and dew02.exe User’s
    Manual. https://swat.tamu.edu/media/83105/dewpoint.zip. Accessed 20 Sep 2023 MSC-W,
    CCC, CEIP, CIAM (2022) Transboundary particulate matter, photo-oxidants, acidifying
    and eutrophying components. https://emep.int/publ/reports/2022/EMEP_Status_Report_1_2022.pdf.
    Accessed 31 July 2023 Alexander EB (1980) Bulk densities of California soils in
    relation to other soil properties. Soil Sci Soc Am J 44:689–692. https://doi.org/10.2136/sssaj1980.03615995004400040005x
    Article   Google Scholar   Assouline S, Or D (2014) The concept of field capacity
    revisited: defining intrinsic static and dynamic criteria for soil internal drainage
    dynamics. Water Resour Res 50:4787–4802. https://doi.org/10.1002/2014wr015475
    Article   ADS   Google Scholar   Gascoin S, Ducharne A, Ribstein P et al (2009)
    Sensitivity of bare soil albedo to surface soil moisture on the moraine of the
    Zongo glacier (Bolivia). Geophys Res Lett 36:L02405. https://doi.org/10.1029/2008gl036377
    Article   ADS   Google Scholar   Sharpley AN, Williams JR (1990) EPIC—erosion/productivity
    impact calculator: 1. Model documentation Szabó B, Weynants M, Weber TKD (2020)
    Updated European hydraulic pedotransfer functions with communicated uncertainties
    in the predicted variables (euptfv2). Geosci Model Dev 14:151–175. https://doi.org/10.5194/gmd-14-151-2021
    Article   ADS   CAS   Google Scholar   Wessolek G (2009) Bodenphysikalische Kennwerte
    und Berechnungsverfahren für die Praxis Mészáros J, Szabó B (2022) Script to derive
    and apply crop classification based on Sentinel 1 satellite radar images in Google
    Earth Engine platform. https://doi.org/10.5281/zenodo.6700122 Szabó B, Mészáros
    J, Kassai P, et al (2022) Solutions to overcome data scarcity. Deliverable D3.2
    EU Horizon 2020 OPTAIN Project, Grant agreement No. 862756 OPTAIN (2023) Upper
    Zglowiaczka. https://www.optain.eu/case-studies-and-actors-involvement/upper-zglowiaczka.
    Accessed 21 Sep 2023 Arnold JG, Kiniry JR, Srinivasan R, et al (2012) Soil & water
    assessment tool input/output documentation version 2012 Sluiter R (2009) Interpolation
    methods for climate data—literature review. KNMI De Smith MJ, Goodchild MF, Longley
    P (2018) Geospatial analysis: a comprehensive guide to principles, techniques
    and software tools, 6th edn. Troubador publishing Ltd., Market Harborough Google
    Scholar   Simpson D, Benedictow A, Berge H et al (2012) The EMEP MSC-W chemical
    transport model–technical description. Atmos Chem Phys 12:7825–7865. https://doi.org/10.5194/acp-12-7825-2012
    Article   ADS   CAS   Google Scholar   Szabó B, Kassai P, Plunge S, et al (2024)
    Addressing soil data needs and data-gaps in catchment scale environmental modelling:
    the European perspective. Manuscript submitted for publication Weynants M, Montanarella
    L, Toth G, et al (2013) European hydropedological data inventory (EU-HYDI). https://doi.org/10.2788/5936
    Thériault R, Ben-Shachar MS, Patil I et al (2023) Check your outliers! An introduction
    to identifying statistical outliers in R with easystats. PsyArXiv. https://doi.org/10.31234/osf.io/bu6nt
    Article   Google Scholar   Jamshidi EJ, Yusup Y, Kayode JS, Kamaruddin MA (2022)
    Detecting outliers in a univariate time series dataset using unsupervised combined
    statistical methods: a case study on surface water temperature. Eco Inform 69:101672.
    https://doi.org/10.1016/j.ecoinf.2022.101672 Article   Google Scholar   Download
    references Software availability Name of the software: SWATprepR. Developer: Svajunas
    Plunge. Contact information: svajunas_plunge@sggw.edu.pl. Year first available:
    2022. Program language: R. Cost: free. Software availability: https://biopsichas.github.io/SWATprepR/.
    Program size: 163 kB. Funding This work was accomplished within the OPTAIN project
    (OPtimal strategies to reTAIN and re-use water and nutrients in small agricultural
    catchments across different soil-climatic regions in Europe, cordis.europa.eu)
    which has received funding from the European Union’s Horizon 2020 research and
    innovation programme under grant agreement No. 862756. Author information Authors
    and Affiliations Department of Hydrology, Meteorology and Water Management, Warsaw
    University of Life Sciences, Nowoursynowska st. 159, 02-776, Warsaw, Poland Svajunas
    Plunge & Mikołaj Piniewski Institute of Water Resources Engineering, Vytautas
    Magnus University, Universiteto st. 10, 53361, Akademija, Lithuania Svajunas Plunge
    Institute for Soil Sciences, Centre for Agricultural Research, Herman Ottó út
    15, Budapest, 1022, Hungary Brigitta Szabó Department Computational Landscape
    Ecology, Helmholtz Centre for Environmental Research GmbH—UFZ, Permoserstraße
    15, 04318, Leipzig, Germany Michael Strauch & Christoph Schürz Marine Research
    Institute, Klaipėda University, Universiteto Ave. 17, 92294, Klaipėda, Lithuania
    Natalja Čerkasova Texas A&M AgriLife Research, Blackland Research and Extension
    Center, Temple, TX, USA Natalja Čerkasova Contributions SP software, formal analysis,
    visualization, writing—original draft, writing—review and editing. BS software,
    writing—original draft, writing—review and editing, methodology. MS software,
    writing—original draft, writing—review and editing. NČ writing—original draft,
    writing—review and editing, methodology. CS conceptualization, methodology. MP
    writing—original draft, writing—review and editing. Corresponding author Correspondence
    to Svajunas Plunge. Ethics declarations Ethics approval and consent to participate
    Not applicable. Competing interests The authors declare no competing interests.
    Additional information Publisher''s Note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Supplementary Information Additional file 1: SWATprepR package manual. Rights
    and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article''s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article''s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Plunge, S., Szabó, B., Strauch,
    M. et al. SWAT + input data preparation in a scripted workflow: SWATprepR. Environ
    Sci Eur 36, 53 (2024). https://doi.org/10.1186/s12302-024-00873-1 Download citation
    Received 03 October 2023 Accepted 20 February 2024 Published 11 March 2024 DOI
    https://doi.org/10.1186/s12302-024-00873-1 Share this article Anyone you share
    the following link with will be able to read this content: Get shareable link
    Provided by the Springer Nature SharedIt content-sharing initiative Keywords SWAT + model
    Input data processing R package Workflow Reproducibility Download PDF Sections
    Figures References Abstract Introduction SWATprepR package features SWATprepR
    demonstration case Limitations Conclusions and future work Availability of data
    and materials Notes References Funding Author information Ethics declarations
    Additional information Supplementary Information Rights and permissions About
    this article Advertisement Support and Contact Jobs Language editing for authors
    Scientific editing for authors Leave feedback Terms and conditions Privacy statement
    Accessibility Cookies Follow SpringerOpen By using this website, you agree to
    our Terms and Conditions, Your US state privacy rights, Privacy statement and
    Cookies policy. Your privacy choices/Manage cookies we use in the preference centre.
    © 2024 BioMed Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: Environmental Sciences Europe
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'SWAT + input data preparation in a scripted workflow: SWATprepR'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hou L.
  - Latif J.
  - Mehryar P.
  - Withers S.
  - Plastropoulos A.
  - Shen L.
  - Ali Z.
  citation_count: '0'
  description: Assistive powered wheelchairs will bring patients and elderly the ability
    of remain mobile without the direct intervention from caregivers. Vital signs
    from users can be collected and analyzed remotely to allow better disease prevention
    and proactive management of health and chronic conditions. This research proposes
    an autonomous wheelchair prototype system integrated with biophysical sensors
    based on Internet of Thing (IoT). A powered wheelchair system was developed with
    three biophysical sensors to collect, transmit and analysis users’ four vital
    signs to provide real-time feedback to users and clinicians. A user interface
    software embedded with the cloud artificial intelligence (AI) algorithms was developed
    for the data visualization and analysis. An improved data compression algorithm
    Minimalist, Adaptive and Streaming R-bit (O-MAS-R) was proposed to achieve a higher
    compression ratio with minimum 7.1%, maximum 45.25% compared with MAS algorithm
    during the data transmission. At the same time, the prototype wheelchair, accompanied
    with a smart-chair app, assimilates data from the onboard sensors and characteristics
    features within the surroundings in real-time to achieve the functions including
    obstruct laser scanning, autonomous localization, and point-to-point route planning
    and moving within a predefined area. In conclusion, the wheelchair prototype uses
    AI algorithms and navigation technology to help patients and elderly maintain
    their independent mobility and monitor their healthcare information in real-time.
  doi: 10.1038/s41598-024-56357-y
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    11 March 2024 An autonomous wheelchair with health monitoring system based on
    Internet of Thing Lei Hou, Jawwad Latif, Pouyan Mehryar, Stephen Withers, Angelos
    Plastropoulos, Linlin Shen & Zulfiqur Ali  Scientific Reports  14, Article number:
    5878 (2024) Cite this article 441 Accesses 1 Altmetric Metrics Abstract Assistive
    powered wheelchairs will bring patients and elderly the ability of remain mobile
    without the direct intervention from caregivers. Vital signs from users can be
    collected and analyzed remotely to allow better disease prevention and proactive
    management of health and chronic conditions. This research proposes an autonomous
    wheelchair prototype system integrated with biophysical sensors based on Internet
    of Thing (IoT). A powered wheelchair system was developed with three biophysical
    sensors to collect, transmit and analysis users’ four vital signs to provide real-time
    feedback to users and clinicians. A user interface software embedded with the
    cloud artificial intelligence (AI) algorithms was developed for the data visualization
    and analysis. An improved data compression algorithm Minimalist, Adaptive and
    Streaming R-bit (O-MAS-R) was proposed to achieve a higher compression ratio with
    minimum 7.1%, maximum 45.25% compared with MAS algorithm during the data transmission.
    At the same time, the prototype wheelchair, accompanied with a smart-chair app,
    assimilates data from the onboard sensors and characteristics features within
    the surroundings in real-time to achieve the functions including obstruct laser
    scanning, autonomous localization, and point-to-point route planning and moving
    within a predefined area. In conclusion, the wheelchair prototype uses AI algorithms
    and navigation technology to help patients and elderly maintain their independent
    mobility and monitor their healthcare information in real-time. Similar content
    being viewed by others Reshaping healthcare with wearable biosensors Article Open
    access 27 March 2023 A smart device for non-invasive ADL estimation through multi-environmental
    sensor fusion Article Open access 11 October 2023 AI-enabled photonic smart garment
    for movement analysis Article Open access 08 March 2022 Introduction An electric-powered
    wheelchair (EPW) is an assistive technology solution for people with motor disabilities,
    which gives them independent mobility. An estimated 65 million people worldwide
    need a wheelchair1, and the number of people who are in need of a wheelchair is
    estimated to increase over 22% in the next decade2. There is a high level of demand
    for wheelchair services for the elderly that is difficult to meet. The research
    on EPW started around the 1980s. The prototype wheelchair allowed a person to
    maneuver within an office building3. Since then, many EPWs have been developed
    and commercialized, such as TinMan4, NavChair5, Maid6, and SPAM7 to provide users
    indoor mobility. However, the traditional type of EPW was controlled by a joystick
    and was difficult to maneuver by patients with complicated disabilities and mobility
    impairment due to cerebral palsy, cognitive impairment, and fatigue8. For example,
    patients with Parkinson’s disease often lack the cognitive and physical skills
    to maneuver the EPW due to perceptual impairments. A study of 65 clinicians reported
    that between 10 and 40% of their patients could not be equipped with EPW due to
    sensory disabilities, impaired mobility, or cognitive deficits. These impairments
    made it difficult to operate a wheelchair safely with the current control functions9.
    Consequently, those individuals who cannot maneuver an EPW independently and safely
    must be seated in a manual wheelchair and pushed by a caregiver. To solve these
    problems, academics improved the design of the EPW in three main areas: the assistive
    technology mechanics, physical interface, and power shared control between the
    user and the wheelchair10,11. Currently, most autonomous wheelchairs are modified
    by existing commercially available EPW, with additional facilities to improve
    maneuverability, locomotion, localization, navigation, and control interface12.
    The smart autonomous wheelchairs have been trialled in hospitals and airports.
    In 2016, two prototype autonomous wheelchairs developed by the Singapore-MIT Alliance
    for Research and Technology Centre were tested in a hospital of Singapore to navigate
    the hospital’s hallways13. The prototype wheelchair created a path map using data
    from three Lidar sensors. The location of the wheelchair on the map is determined
    using a localization algorithm. In 2017, an autonomous wheelchair embedded with
    LIDAR sensors was proposed by Harkishan14. This wheelchair can navigate to predefined
    locations in an unstructured environment. Another model WHILL autonomous wheelchair
    was developed in 2017 by Panasonic and Whill13. This type of wheelchair was premiered
    at Haneda Airport in Tokyo with further trials in Amsterdam’s Schiphol airport,
    Abu Dhabi airport and north American airports since 201815. However, these prototypes
    can only take passengers to predefined locations within the airport or hospital.
    The maximum luggage carrying capacity of four kilograms cannot fulfill the baggage
    requirements for most passengers. In addition to autonomous driving, assistive
    biophysical sensors can be integrated into the wheelchair to check passengers’
    vital signs before use. A robot operating system was used in an autonomous wheelchair
    for individuals who have difficulty in controlling movements by Grewal16. He employed
    only 2D laser scanners to design a mapping system that enabled the wheelchair
    to move autonomously. The same approach was used by Wang17, but the sensor offered
    large degree measurements in a narrow space. On the other hand, Surmann utilized
    a rotatory mechanism and a 2D LiDAR scanner to create a 3D environment map for
    anti-collision system. Nonetheless, the solution may be insufficient to ensure
    the safety of the wheelchair user18. Furthermore, a wheelchair system developed
    by Andre can transport inpatients autonomously to their departments by integrating
    with the hospital information system19. However, using this system for private
    transportation may be challenging, as it requires specific location and path information
    for departments in the hospital. Electrically powered wheelchairs should not only
    provide mobility for advanced stages of disability but also integrate with assistive
    technology to offer better clinical care. Chronic diseases, such as arthritis,
    asthma and coronary heart disease, are becoming more prevalent among the elderly
    and place a high demand for healthcare services20. A wheelchair health monitoring
    system with routine tests can be a cost-effective way for clinicians and caregivers
    to manage chronic conditions in their patients21. The remote monitoring system
    can improve the management of chronic condition transparency and quality of care
    for patients while reducing the burden on healthcare facilities, emergency situations,
    and re-admissions. For example, a biomedical sensing system was integrated into
    a prototype wheelchair to record users’ pulse rate, respiratory rate and motion
    states22. However, the signal communication and autonomous system were limited
    by Wi-Fi signals and not viable for outdoor scenarios. Based on that prototype,
    a home healthcare system for wheelchair users was proposed to connect more sensors
    in a prototype wheelchair. Similar work was proposed to develop an Intelligent
    Robotic Wheelchair (iRW)23 that integrates telehealth systems to collect vital
    signs of users in real time. However, there is no effective analysis of these
    healthcare signals which can be used for remote diagnosis by doctors. One of the
    significant limitations for the autonomous telehealth wheelchair is the battery
    life. The operating of biophysical sensors embedded in the wheelchair is limited
    by various resources, such as power supply, memory storage and processing capabilities24,25.
    Continuous monitoring sensors produce a large amount of data and consume significant
    storage memory and transmission power. According to a survey26, nearly 80% of
    the power is consumed during the transmission of data in each sensor node. Therefore,
    it is essential to develop a lower power design to make the battery last longer.
    Data compression in sensor nodes before the data transmission provides an adequate
    method to reduce the size of data. The performance of various data compression
    algorithms is evaluated based on dataset types. Lempel–Ziv–Welch (S-LZW) data
    compression algorithm uses structured data to reduce substantial energy consumption27.
    However, S-LZW is a dictionary-based algorithm that occupies memory for calculation,
    so it is not suitable for sensors with restricted RAM25. Another compression algorithm
    of Run Length Encoding (RLE) works by removing duplicate data values. Based on
    RLE, K-RLE was developed to achieve a better compression ratio28. Meanwhile, because
    it concentrates on computing floating-point data, the Minimalist Adaptive and
    Streaming (MAS) method was recommended as resource efficient29. Among them, MAS
    and S-LZW algorithms have been widely applied in real-time sensing applications,
    such as monitoring wind speed, rainfall, temperature, humidity, soil moisture,
    pressure, and battery level24,30. The reduction of power consumption during data
    transmission of the MAS algorithm is between 53.55 and 55.95%, while that of the
    S-LZW is between 23.41 and 33.97%. To further improve the data compression ratio
    during transmission, the Minimalist, Adaptive and Streaming R-bit (O-MAS-R) algorithm
    was proposed. In this paper we propose an intelligent autonomous wheelchair (iChair)
    integrated with telemedicine sensors based on IoT, and the architecture of the
    wheelchair system is shown in Fig. 1. Various sensors including wireless location,
    position accelerometer, seat cushion sensors, and biophysical sensors are embedded
    in the wheelchair to collect users’ physiological and behavioral data in real
    time. At the same time, an improved data compression algorithm Minimalist, Adaptive
    and Streaming R-bit (O-MAS-R), is also proposed to achieve a higher compression
    ratio during the data transmission. To visualize and analyze the data, a user
    interface was developed to provide telediagnosis, advice and alert to users and
    caregivers using artificial intelligence algorithms. Figure 1 The architecture
    of the smart wheelchair system. A portable wheelchair is equipped with sensors,
    cameras, and screens. The data acquisition system processes, compresses and uploads
    the measurements from biophysical sensors. A MATLAB graphic user interface allows
    users and doctors access and diagnose the health information in real-time31. Full
    size image Results Wheelchair monitoring interface The handrail of the wheelchair
    system included three biophysical sensors: pulse oxygen (SpO2), blood pressure,
    and temperature sensors to collect and transmit four kinds of vital signs from
    users (blood oxygen levels, pulse rate, blood pressure and temperature)31, as
    depicted in Fig. 2. Figure 2 The prototype of the smart wheelchair consists of
    a controller box, laser sensors, power system, screens, and biophysical sensors31.
    Full size image On the wheelchair as shown in Fig. 2, there are two monitoring
    interfaces to provide feedback to users: the large screen interface and the handrail
    screen as depicted in Fig. 2. The screen installed on the handrail of the wheelchair
    and the remote GUI are for data classification, visualization, and analysis. The
    information includes data initialization, measurement, upload status to the cloud,
    and transmission completion. The duration for each process results in a 40-s cycle,
    with each set lasting 10 s. The display shows a countdown for each phase, and
    the timing allows the data from all three biophysical sensors to finish transmitting.
    The GUI, developed in MATLAB and shown in Fig. 3 allows users to download, inspect,
    and analyze the cloud-stored data once it finishes uploading. In the user interface,
    access to users’ healthcare data requires a unique Patient Identity number (PID)
    assigned to each user before experiments. The warning system uses three colors
    to flag conditions: red, yellow, and blue. The red indicates that the gathered
    data is above the upper threshold, the yellow shows the data is below the lower
    threshold, and the blue indicates the measured data is within the thresholds.
    Figure 3 The iChair monitoring interface. The GUI comprises four main sections:
    patient information, last update, vital signs, and inspection. It allows users
    and doctors to download and analyze cloud-stored data as well as inspect the data
    being recorded in real-time. Full size image Figure 3 shows the iChair monitoring
    interface comprising four main sections: patient information, last update, vital
    signs, and inspection. The last update section shows the most recent collecting
    date and time from the user, and the users’ vital signs appear in the vital signs
    section. In the inspection section, users can see an aggregated display of their
    specific vital sign’s information in the past. Data compression algorithm Both
    MAS and O-MAS-R compression algorithms were applied to five ECG datasets, twelve
    EMG datasets, and three accelerometer datasets to evaluate the approaches effectiveness.
    Figure 4 depicts the compression ratio performance. Figure 4 The compression ratio
    results of MAS and O-MAS-R algorithms are shown in (a–c), in each figure, x-axis
    shows the group number, and y-axis is the compression ratio. The average ratio
    increase of the O-MAS-R algorithm over MAS is shown in (d). The compression ratio
    of (a) five ECG datasets, (b) twelve EMG datasets and (c) three accelerometer
    datasets are demonstrated. Full size image In Fig. 4a, the compression results
    of MAS and O-MAS-R algorithms applied to five ECG datasets are demonstrated. The
    data in ECG datasets is assigned integer type with two bytes per sample. Each
    ECG dataset comprises 3,600 samples that occupy 7,200 bytes of memory. Among the
    simulation results, the group three of O-MAS-R algorithm shows the greatest compression
    ratio of 20.54%, while the MAS algorithm is 12.47%. For each group, the O-MAS-R
    method achieves compression ratios of 19.86%, 19.13%, 20.54%, 18.78%, and 18.26%
    respectively. Meanwhile, the MAS algorithm demonstrates compression ratios of
    11.9%, 11.57%, 12.47%, 12.32% and 12.28% respectively. In Fig. 4b, EMG data of
    twelve muscle activities during treadmill walking have been compressed by the
    MAS and O-MAS-R algorithms. The EMG values are float type that contains 4 bytes
    per sample. Each EMG dataset comprises 15,000 samples that occupy 60,000 bytes
    of memory. The RF activity shows the highest O-MAS-R compression ratio of 39.85%,
    while the MAS is 31.26%. For each group, the O-MAS-R algorithm achieves compression
    ratios of 39.85%, 35.44%, 34.74%, 39.5%, 35.58%, 36.4%, 33.21%, 36.01%, 39.33%,
    35.71%, 35.86% and 37.87% respectively. Meanwhile, the MAS algorithm demonstrates
    compression ratios of 31.26%, 26.41%, 26.07%, 30.8%, 27.07%, 27.62%, 25.95%, 28.53%,
    31.18%, 28.27%, 28.41% and 28.95% respectively. In Fig. 4c, the compression algorithms
    have been applied to three accelerometer datasets. The data type in the dataset
    is float type and contains 4 bytes per sample. Each Accelerometer dataset has
    15,000 samples that take 60,000 bytes of memory. For each group, the O-MAS-R algorithm
    achieves compression ratios of 84%, 83.83%, and 83.76% respectively. Meanwhile,
    the MAS algorithm demonstrates compression ratios of 38.83%, 38.28%, and 38.77%
    respectively. For all the datasets, O-MAS-R compression algorithm demonstrates
    a better performance. The average increase of O-MAS-R over MAS is shown in Fig.
    4d. The accelerometer datasets of O-MAS-R algorithm shows the greatest increase
    of 45.25% over the MAS algorithm. The average increases of compression ratios
    for ECG, EMG, and Acc datasets are 7.21%, 8.26%, and 45.25%, respectively. According
    to the Spyder platform''s profiler tool, the encoding function of the MAS and
    O-MAS-R algorithms in compressing ECG dataset values took 20.28 µs and 25.69 µs,
    respectively. However, the repetition of data, on the other hand, resulted in
    fewer calls to the encoding function in the O-MAS-R algorithm, which decreased
    the overall run time of the O-MAS-R algorithm. The total run time for the MAS
    and O-MAS-R algorithms applied in ECG dataset were 79.37 ms and 73.04 ms, respectively.
    Similarly, the encoding function of the MAS and O-MAS-R algorithms in compressing
    Accelerometer dataset values took 18.90us and 19.25 µs, respectively. However,
    due to high frequency of repetitions of data in accelerometer dataset, the total
    run time for O-MAS-R encoding algorithm is significantly reduced from 283.53 to
    71.67 ms25. MATLAB graphic user interface (GUI) This paper discusses the smart
    wheelchair prototype and the three integrated biophysical sensors used to collect
    four vital health indicators from users. It also discusses the MATLAB GUI software
    designed to synchronize and download the patients’ healthcare data for diagnosis
    and analysis. The preliminary experiments, five participants were involved in
    the clinical trials, and healthcare data was collected for 5–10 mins for each
    user. Figure 5a–d demonstrates the results. Figure 5 Four types of vital signs
    from five participants were monitored: (a) finger temperature, (b) pulse rate,
    (c) blood oxygen levels, and (d) blood pressure. Each column represents a single
    measurement, and the group of columns represents the results from a single participant.
    The gap between each column is the time spent uploading the measurements. Full
    size image Figure 5a documents the five participants whose finger temperatures
    were measured and recorded. The x-axis is the measurement time, and the y-axis
    is the measured temperature in Celsius (°C). Before taking the measurements, participants
    were advised to place their forefinger on their wrist for a minute to equalize
    the temperature. An upper threshold of 37 °C was set as it was considered as the
    average normal body temperature. Among the participants, users four and five had
    a slightly higher temperature than normal, and thus the column automatically turned
    red following the three-color system. As seen in Fig. 5b, the five participants’
    pulse rate were recorded with the upper threshold set to 120 bpm. The results
    revealed one participant had a higher average pulse rate than the other participants.
    Figure 5c depicts the blood oxygen saturation level (SpO2) for each participant.
    The lower limit of SpO2 was set at 90%, as any number below that represents hypoxemia,
    and poses a variety of complications32. Therefore, the level of SpO2 is a highly
    useful approach for measuring health conditions32. Figure 5d shows the participants’
    systolic and diastolic blood pressures in the top and bottom rows, respectively.
    The upper threshold for systolic blood pressure is 120 mmHg, while the upper threshold
    for diastolic blood pressure is 80 mmHg. The results indicate that participant
    three had unreasonably high systolic blood pressure on certain tests, and participant
    five had high systolic blood pressure and diastolic blood pressure. The three-color
    system automatically marked the column for high blood pressure data in red. iChair
    autonomous driving The autonomous driving experiments were conducted in the factory
    testing area33. We described the smart wheelchair safety and obstacle detection
    system in our previously published papers31. Based on that system, the wheelchair
    was improved to travel autonomously from point to point inside a lager and obstacle
    completed area. An Android-based smartphone app iChair was developed to control
    and tracks the entire driving progress depicted in Fig. 6. Figure 6 The smart
    wheelchair autonomous driving and control. (a) An engineer sits in the wheelchair
    and controls it using the iChair app. (b) The navigation panel with the iChair
    app control information, while (c) depicts the mapping information of the enclosed
    area. Full size image There are three main sections in the iChair app: bio-medical,
    navigation and mapping. The biomedical section displays the collected bio-sensory
    data, the navigation section links the wheelchair to the app and controls its
    movement, and the mapping section displays the wheelchair''s real-time location.
    In Fig. 6a, an engineer sits in the wheelchair and controls it using the iChair
    app. To perform autonomous driving well, the iChair must be in a pre-scanned,
    enclosed environment, achieved by recording the surrounding information into the
    map using the data from LIDAR sensors. As shown in Fig. 6c, the app remembers
    its scanned path of the office, the start and stop coordinates, and the blue dots
    provides the position of the wheelchair. The red and grey dots, in addition to
    the lines, are the LIDAR sensors reflecting signals that represent the barriers
    along the path. Once the scanned map saves, the iChair will link with the app
    to perform the autonomous driving as shown in Fig. 6b. As a result, the user can
    enter the start and stop coordinates from the Android app or directly through
    the ROS network as separate position names. By clicking different positions in
    the app panel, the wheelchair will drive to the location autonomously. During
    the reliability tests, the iChair navigated to various predetermined locations
    using automated driving scripts. It successfully operated for five hours until
    the battery ran out of power. Wooden boards were used to modify the configuration
    of the path during the mobility tests to determine the maximum capacity of the
    system to maneuver. The results show that the iChair could pass through a minimal
    gap of 0.85 m and can operate in at least 1.2 m wide corridors. The maximum speed
    that the wheelchair could move in an unmapped area while accounting for unknown
    obstacles was 0.2 m/s. Discussion Patients who cannot safely and independently
    operate an Electric Powered Wheelchair (EPW) must be seated in a manual wheelchair
    and pushed by a caregiver. An autonomous telemedicine wheelchair is one solution
    to overcoming the cognitive and physical challenges and improve independence for
    those users34. It not only takes people to their desired location but also assesses
    their physical location, status conditions and vital bio-signs in real-time. This
    data will help them manage and prevent chronic diseases in the long term. The
    paper proposes a smart wheelchair equipped with three biophysical sensors and
    a novel Internet of Thing (IoT) compression algorithm that monitors and assesses
    users'' physiological and behavioral data in real-time. The iChair design should
    prioritize simplicity in control to minimize usage barriers, especially for patients
    who require assistance. They may initially struggle with or forget to use some
    of the features. To address the issue, the wheelchair controls should be similar
    to EPWs on the market, facilitating their usage habits. The central screen can
    serve as a user-friendly dashboard, displaying the patient''s current status,
    providing prompts for necessary measurements, and offering easy navigation to
    desired locations. It serves as an interface for users to interact with the iChair
    smoothly. Due to the wheelchair being integrated with advanced components, algorithms,
    and sensors, if it is deployed in the market on a large scale, maintenance may
    require specific technical skills. To mitigate this issue, the system should support
    remote monitoring and diagnostic tools for spotting issues early. It also provides
    detailed documents with best practices and maintenance guidelines. Lastly, regular
    training for maintenance staff can be conducted to ensure they can handle any
    problems effectively. The smart wheelchair can further develop as a proprietary
    medical device for autonomous health monitoring and navigation. For example, it
    will offer those affected by Parkinson’s disease the ability to proactively manage
    their chronic condition, and help them avoid fainting, which are considered the
    most common diagnosis for patients attending emergency departments. It will also
    help maintain their mobility. The artificial intelligence algorithms incorporated
    into the wheelchair will analyze sensor data and provide feedback in real-time
    to the user and clinicians on any potential risks to the patient, such as the
    experience of a sharp and unexpected drop in blood pressure, causing dizziness
    and an increased risk of fainting. With the assistant of the smart wheelchiar,
    the ratio of carers to patients can be increased from 1:2.5 to 1:4 or 1:5 for
    completely disabled people, allowing the cost of carers to be reduced by up to
    CNY 15–18 k per year. The wheelchair system is estimated to be priced at CNY 8000
    (~ £920), and the retrofitted system is priced at CNY 3000 (~ £345). In the UK
    market, the cost of the systems will be £2500 and £500, respectively. During the
    trials, the system could only process up to three sensors simultaneously, because
    of the microcontroller’s restrictions in supporting concurrent sensor readings
    from one group of sensors (analog, UART, Bluetooth) to one interface (TFT, Bluetooth,
    Wi-Fi)35. The constraint may limit the system''s coverage of health conditions,
    especially when managing chronic diseases that involve monitoring multiple health
    indicators. These problems could be optimized by implementing intelligent algorithms
    that prioritize and cycle through different sets of sensors over time, ensuring
    continuous monitoring of key health parameters relevant to chronic conditions.
    Additionally, adapting the system to support sensor modularity and sensor fusion
    technologies would enable the integration of more sensors. The detecting sensors
    integrated into the microcontroller could expand to eighteen different functions,
    including features such as snore monitoring, temperature readings, glucometer
    readings, ECGs, EMGs, breath monitoring, SpO2, blood pressure, airflow, body position,
    emergency alarms, and room thermometer, providing a more comprehensive view of
    the patient''s health status. Health monitoring sensors, such as heart rate, blood
    pressure and temperature sensors, need to be strategically placed for accurate
    readings while considering user comfort. Integrating sensors without interfering
    with wheelchair controls is critical. Thus, for the convenience of our wheelchair
    design, the temperature sensor was placed on the handrail to detect users’ fingers,
    palm and wrist temperature. However, we acknowledge that environmental factors
    influencing temperature in these areas may cause variations in sensor readings.
    Further improvements involve implementing adaptive calibration algorithms that
    dynamically adjust temperature readings based on environmental conditions. Additionally,
    to extend the functionality of the wheelchair, certain sensors can be integrated
    as conformable and wearable patches on the body and be easily removable modular
    elements. The integration of multiple sensors, including non-contact sensors on
    the screen, could be applied to offer a comprehensive approach. However, the effectiveness
    of the O-MAS-R compression algorithm may be specific to the types of data used
    in the study. The performance might vary when applied to different types of datasets
    beyond the scope of the initial experiments. Additionally, the study demonstrated
    positive results under controlled conditions, but real-world scenarios can be
    more complex. Factors such as signal interference, hardware malfunctions, or variations
    in environmental conditions could affect the actual performance of the proposed
    model. Further research can focus on optimizing the compression algorithm for
    diverse sensor data types, ensuring it maintains efficiency across a wide range
    of physiological parameters. Extensive validation studies can be conducted in
    diverse healthcare settings, such as different patient demographics, environmental
    conditions, and healthcare practices. Moreover, the algorithm can be further integrated
    with advanced healthcare AI models for automated monitoring and forecasting of
    users'' physiological conditions and diseases. To explore the EPW with other sensors
    for more functionalities, previous work by Shen et.al.,36 extend  the scope of
    the work. This extension includes a face-recognition screen with a camera on the
    left handrail of the wheelchair. This innovative approach aims to evaluate users''
    long-term cardiovascular conditions based on facial information, utilizing a CHD
    evaluation algorithm published by Shen36. First, sixty-eight face feature points
    and ears from patients’ face images were collected. Based on their coordinates,
    six regions of interests (ROI) were extracted: left canthus, right canthus, left
    crowsfeet, right crowsfeet, nose bridge and forehead36. Then, a gray-level co-occurrence
    matrix algorithm was applied to the ROIs to extract and analyze their texture
    features. Lastly, the random forest and decision tree classification methods were
    applied to predict the risk of CHD. In the paper, 1528 facial images were captured
    from 309 subjects, comprising 226 males and 83 females36. Among them, 195 patients
    have coronary heart disease. Each patient had at least three face images collected:
    front, left, and right faces. By adopting features into the models, the random
    forest algorithm had a maximum accuracy of 72.73% in identifying patients with
    CHD, while the decision tree model had a maximum accuracy of 70.45%. The results
    demonstrated that facial images can be an effective method of detecting patients
    with CHD, with an accuracy rate of above 70%. The algorithm will be embedded into
    the wheelchair''s screen to monitor the user’s coronary health condition over
    time. In the paper, we demonstrated that the proposed use of the O-MAS-R compression
    algorithm maintained a greater compression ratio than the MAS algorithm at a 53%
    reduction in data transmission power consumption24. As the compression ratio is
    directly proportional to data transmission power usage, implementing the O-MAS-R
    algorithm in wireless sensor network sensor nodes will result in even lower data
    transmission power consumption25. This approach uses the least amount of memory
    to store and transmit data by reducing consecutively repeated data values. This
    functionality is particularly useful in dealing with healthcare data. However,
    the effectiveness of the O-MAS-R compression algorithm may be specific to the
    types of data used in the study. The performance might vary when applied to different
    types of datasets beyond the scope of the initial experiments. Additionally, the
    study demonstrated positive results under controlled conditions, but real-world
    scenarios can be more complex. Factors such as signal interference, hardware malfunctions,
    or variations in environmental conditions could affect the actual performance
    of the proposed model. This paper documents and evaluates the obstacle avoidance,
    human–machine interaction, and point-to-point autonomous driving of the smart
    wheelchair. Currently, the intelligent wheelchair can only drive autonomously
    in a pre-scanned enclosed area because the only way to calculate the optimal route
    between any two locations requires the system to store localized data from the
    laser sensors. However, once scanned, the stored maps and routes can be shared
    with other wheelchairs for collaborative driving. For wheelchair users with limited
    mobility, safety is the top priority. Unmapped areas may have construction zones,
    temporary obstacles, changes in road conditions, lacking lane markings and road
    signs, which can cause severe dangers to the wheelchair''s autonomous driving.
    Therefore, the autonomous driving function will be deactivated in unmapped areas.
    Users have to rely on the manual control of the wheelchair to ensure safety. Additionally,
    to ensure safety for wheelchair users, we conduct thorough testing to validate
    the system''s performance under different conditions, ensuring robustness and
    safety. We implement redundant sensor systems, the obstacle avoidance system,
    to ensure the vehicle can rely on multiple sources of information, mitigating
    the risk of sensor failures and avoiding collisions. A software filter that used
    LIDAR sensor data successfully hid the user’s legs from the scan data to minimize
    blind spots. Increasing the use of obstacle detection over a wider range reduced
    the remaining blind spots discovered around the four corners of the wheelchair.
    The smart autonomous wheelchair will assist disabled and elderly patients by allowing
    them to pick locations on their phones and drive independently and autonomously.
    It will reduce their dependency on caregivers and family members while also eliciting
    feelings of self-reliance. Therefore, the wheelchair has potential uses in nursing
    homes, hospitals, communities, airports, and shopping malls. In hospitals and
    nursing homes, the wheelchair will work in conjunction with the other infrastructure,
    such as elevators, ward doors, and automated doors to complete easy point-to-point
    and ward-to-ward mobility. The telemedicine diagnosis from the wheelchair will
    complete the initial evaluation of vital sign measurements at the hospital’s entrance
    and then continually monitor those patients. Conclusion In this paper we proposed
    a smart autonomous wheelchair (iChair) that integrates with telemedicine sensors
    based on IoT. The wheelchair, controlled by a mobile app, achieved point-to-point
    autonomous driving within a predefined area with and without obstructions. Various
    sensors, including wireless location, position accelerometer, seat cushion sensors,
    and biophysical sensors embedded in the wheelchair, collected users’ physiological
    and behavioral data in real-time. This comprehensive data was extracted, transformed,
    and uploaded to a cloud platform for storage. An improved data compression algorithm,
    Minimalist, Adaptive and Streaming R-bit (O-MAS-R) will likely achieve a higher
    compression ratio during the data transmission. Performance of MAS and O-MAS-R
    was evaluated in healthcare applications such as ECG, EMG, and accelerometer datasets.
    The designed user interface allowed users and their caretakers or doctors to see
    and analyze the data using the artificial intelligence algorithm to receive telediagnosis,
    advice and alerts. The interface also allowed users to track and diagnose long-term
    health issues with similar algorithms and makes it easier for medical professionals
    to diagnose probable health conditions in the patients. Methods System architecture
    The robotic wheelchair system was designed based on the research of our previously
    published papers31. The wheelchair prototype modified and improved upon the Titan-LTE
    powered wheelchair37 and integrated with the DMC60C digital motor controllers38
    to allow wheelchair manipulation both manually and autonomously. The new components
    include DC motor controllers, a Jetson Nano developer kit, an Inertial Measurement
    Unit (IMU), a joystick module, two light detection and ranging sensors (LIDAR),
    and a 3D printed shield were incorporated into the wheelchair and allowed users
    to operate the wheelchair via a mobile app. These integrated components communicated
    with each other by a central Controller Area Network (CAN). The joystick module
    was a custom-made unit that used a potentiometer joystick with access to the CAN
    enabled microcontroller. The Jetson module included Wi-Fi capability, which allowed
    the entire wheelchair system to be linked to a wireless Android application. The
    software that enabled mobility assistance and autonomous driving was written in
    C++. The sensors connected to the Jetson Nano development kit used Robot Operating
    System middleware (ROS). It implemented a navigation stack and custom configurations
    for obstacle avoidance. The stack consisted of specially developed modules, including
    a localization module and a mapping module. The packages for reading the joystick,
    movement aid, and motor control were developed while the autonomous movement was
    powered by an open-source navigation. The Timed Elastic-Band (TEB) route planner30
    enabled path planning optimization to ensure smooth and safe mobility in the iChair
    system. It also included two laser sensors31 mounted on the front of the wheelchair
    to help ensure obstruct avoidance. The microcontroller used by the data acquisition
    unit was an Arduino component39, while the biophysical sensors were MySignals
    packages35. Consequently, we designed a converter microcontroller to resolve the
    incompatibility between the Arduino and MySignals system. The ThingSpeak40 cloud
    platform was used to allow users to view, download and analyze the stored data.
    We also developed a new MATLAB graphic user interface (GUI) to help users and
    doctors access and diagnose health information in real-time. Data communication
    and compression We introduced the proposed Minimalist, Adaptive and Streaming
    R-bit (O-MAS-R) data compression algorithm in our previously published papers25,31.
    The improvements made to the MAS algorithm allowed for a decrease in the sequential
    repeating of data values, which lead to a higher compression ratio. Equation (1)
    represents the floating data format of the O-MAS-R data compression algorithm.
    $${\\text{nnn}}/{\\text{eee}}/{\\text{ns}}/ \\ldots {\\text{input data}} \\ldots
    /{\\text{R}}/ \\ldots$$ (1) where nnn is the length of the input data in binary
    format, eee represents the position of the decimal point for the input data from
    left to right. Additionally, ns shows whether the input value is positive or negative,
    and the proposed R-bit represents the consecutive repetition of input digits.
    The algorithm calculates up to seven input digits. The repetition input value
    from the subsequent input data sets the R digit to 1. When there is no repetition,
    R is 0. The number of R-bits increases as the number of consecutive repetitions
    of input data increases. The decoding process outputs the same value until it
    reads 0. Similarly, Eq. (2) represents the O-MAS-R encoding format for the integer
    value. $$000/{\\text{nnn}}/{\\text{ns}}/ \\ldots {\\text{input data}} \\ldots
    /{\\text{R}}/ \\ldots$$ (2) To distinguish between integer and floating-point
    data, the first three digits 000 indicated the input data is integer and eee bit
    is removed. The repetition digit R-bit indicates if the following data is the
    same as the current value. The following describes the detailed encoding and decoding
    process of data. When the sensor nodes send out data, the algorithm determines
    if the value is an integer or float number. When the value is a float number,
    the data value compresses using the float encoding format described in Eq. (1).
    In contrast, when the value is an integer, the integer encoding format [Eq. (2)]
    compresses the data. Following data encoding, an R-bit will append to the end
    of the format dependent on the repetition of the next data value. If the value
    is the same as the present value, the R-bit is 1. If not, the R-bit value is 0.
    For the data decoding progress, the software reads the first data value and examines
    the R-bit to determine whether the upcoming value is the same as the current value.
    If the R-bit is 1, the upcoming value is treated as the same as the current one.
    The method keeps reading R-bit until it equals 0. Both the MAS and O-MAS-R were
    implemented across three healthcare datasets: electrocardiography (ECG), surface
    electromyography (sEMG), and accelerometer-based events (Acc) to assess the efficacy
    of the data compression methods. Scripts for data compression algorithms were
    simulated in Spyder (Python 3.7). The compression ratio was determined by dividing
    the dataset’s compressed size by its original size, as indicated in Eq. (3). The
    higher the compression ratio, the better the data compression algorithm would
    perform. $${\\text{Compression}}\\;{\\text{ratio }} = \\, \\left( {{\\text{Compressed}}\\;{\\text{size}}}
    \\right) \\, / \\, \\left( {{\\text{Original}}\\;{\\text{size}}} \\right) \\,
    \\times { 1}00\\%$$ (3) Five ECG datasets41, twelve EMG datasets42, and three
    accelerometer datasets43 were obtained from the MIT-BIH Arrhythmia Database44,
    with a sampling frequency of 360 samples per second and an 11-bit resolution.
    Additionally, sEMG datasets were recorded at 1.5 kHz, corresponding to 12 lower
    limb muscles in a healthy subject during treadmill walking. These muscles include
    rectus gemoris (RF), vastus lateralis (VL), gracilis (GR), biceps femoris long
    head (BFLH), tensor fasciae latae (TFL), Vastus medialis (VM), Tibialis Anterior
    (TA), Soleus (SOL), Gluteus Medius (GMD), gastrocnemius lateralis (GL), gastrocnemius
    medialis (GM) and semitendinosus (SEM)44. Lastly, datasets from three-axis accelerometers
    were selected and evaluated at a frequency of 120 Hz. Ethical approval This study
    was approved by the Innovative Technology and Science Ltd on 2020.06. We affirm
    that all experiments were conducted in compliance with the experimental guidelines
    and regulations established by the Innovative Technology and Science Ltd. Data
    availability The data that support the findings of this study are available from
    the corresponding author, upon reasonable request. References Labbé, D., Ben Mortenson,
    W., Rushton, P. W., Demers, L. & Miller, W. C. Mobility and participation among
    ageing powered wheelchair users: Using a lifecourse approach. Ageing Soc. https://doi.org/10.1017/S0144686X18001228
    (2020). Article   Google Scholar   Kaye, H. S., Kang, T. & LaPlante, M. P. Mobility
    device use in the United States. Disabil. Stat. Rep. 14, 1–10 (2000). Google Scholar   Madarasz,
    R. L., Heiny, L. C., Cromp, R. F. & Mazur, N. M. The design of an autonomous vehicle
    for the disabled. IEEE J. Robot. Autom. https://doi.org/10.1109/JRA.1986.1087052
    (1986). Article   Google Scholar   Miller, D. P. & Slack, M. G. Design and testing
    of a low-cost robotic wheelchair prototype. Auton. Robots https://doi.org/10.1007/BF00735440
    (1995). Article   Google Scholar   Levine, S. P. et al. The navchair assistive
    wheelchair navigation system. IEEE Trans. Rehabil. Eng. https://doi.org/10.1109/86.808948
    (1999). Article   PubMed   Google Scholar   Prassler, E., Scholz, J. & Fiorini,
    P. A robotic wheelchair for crowded public environments. IEEE Robot. Autom. Mag.
    https://doi.org/10.1109/100.924358 (2001). Article   Google Scholar   Simpson,
    R. et al. A prototype power assist wheelchair that provides for obstacle detection
    and avoidance for those with visual impairments. J. Neuroeng. Rehabil. https://doi.org/10.1186/1743-0003-2-30
    (2005). Article   PubMed   PubMed Central   Google Scholar   Simpson, R. C., LoPresti,
    E. F. & Cooper, R. A. How many people would benefit from a smart wheelchair?.
    J. Rehabil. Res. Dev. https://doi.org/10.1682/JRRD.2007.01.0015 (2008). Article   PubMed   Google
    Scholar   Fehr, L., Langbein, W. E. & Skaar, S. B. Adequacy of power wheelchair
    control interfaces for persons with severe disabilities: A clinical survey. J.
    Rehabil. Res. Dev. 37, 3 (2000). Google Scholar   Simpson, R. C. Smart wheelchairs:
    A literature review. J. Rehabil. Res. Dev. https://doi.org/10.1682/JRRD.2004.08.0101
    (2005). Article   PubMed   Google Scholar   Tomari, M. R. M., Kobayashi, Y. &
    Kuno, Y. Development of smart wheelchair system for a user with severe motor impairment.
    Procedia Eng. https://doi.org/10.1016/j.proeng.2012.07.209 (2012). Article   Google
    Scholar   Prassler, E., Scholz, J. & Fiorini, P. A robotic wheelchair for crowded
    public environments. IEEE Robot. Autom. Mag. 8(1), 38–45. https://doi.org/10.1109/100.924358
    (2001). Article   Google Scholar   Scudellari, M. Self-driving wheelchairs debut
    in hospitals and airports [news]. IEEE Spectr. https://doi.org/10.1109/mspec.2017.8048827
    (2017). Article   Google Scholar   Grewal, H., Matthews, A., Tea, R. & George,
    K. LIDAR-based autonomous wheelchair. in SAS 2017 IEEE Sensors Applications Symposium,
    Proceedings. https://doi.org/10.1109/SAS.2017.7894082 (2017). Fisher, R. WHILL
    Has Brought Autonomous Wheelchairs to Airports in North America. (2019). https://globalshakers.com/whill-has-brought-autonomous-wheelchairs-to-airports-in-north-america/.
    Accessed 28 Sept 2023. Grewal, H., Matthews, A., Tea, R. & George, K. LIDAR-based
    autonomous wheelchair. in Proceedings of the 2017 IEEE Sensors Applications Symposium
    (SAS) (2017). Wang, Y., Ramezani, M. & Fallon, M. Actively mapping industrial
    structures with information gain-based planning on a quadruped robot. in Proceedings
    of the 2020 IEEE International Conference on Robotics and Automation (ICRA), 8609–8615
    (2020). Surmann, H., Nüchter, A. & Hertzberg, J. An autonomous mobile robot with
    a 3D laser range finder for 3D exploration and digitalization of indoor environments.
    Robot. Autonom. Syst. 45, 181–198. https://doi.org/10.1016/j.robot.2003.09.004
    (2003). Article   Google Scholar   Baltazar, A. R., Petry, M. R., Silva, M. F.
    & Moreira, A. P. Autonomous wheelchair for patient’s transportation on healthcare
    institutions. SN Appl. Sci. 3, 354 (2021). Article   PubMed   PubMed Central   Google
    Scholar   Schmidt, H. Chronic Disease Prevention and Health Promotion 137–176
    (Springer, 2016). Google Scholar   Glasziou, P., Irwig, L. & Mant, D. Monitoring
    in chronic disease: A rational approach. Br. Med. J. 330, 7492. https://doi.org/10.1136/bmj.330.7492.644
    (2005). Article   Google Scholar   Postolache, O., Girao, P. S., Mendes, J. &
    Postolache, G. Unobstrusive heart rate and respiratory rate monitor embedded on
    a wheelchair. in 2009 IEEE International Workshop on Medical Measurements and
    Applications, MeMeA 2009, 83–88. https://doi.org/10.1109/MEMEA.2009.5167960 (2009)
    Hsu, P. E., Hsu, Y. L., Chang, K. W. & Geiser, C. Mobility assistance design of
    the intelligent robotic wheelchair. Int. J. Adv. Robot. Syst. https://doi.org/10.5772/54819
    (2012). Article   Google Scholar   Abuda, C. F. P., Caya, M. V. S., Cruz, F. R.
    G. & Uy, F. A. A. Compression of wireless sensor node data for transmission based
    on minimalist, adaptive, and streaming compression algorithm. in 2018 IEEE 10th
    International Conference on Humanoid, Nanotechnology, Information Technology,
    Communication and Control, Environment and Management, HNICEM 2018. https://doi.org/10.1109/HNICEM.2018.8666320
    (2019). Latif, J., Mehryar, P., Hou, L. & Ali, Z. An efficient data compression
    algorithm for real-time monitoring applications in healthcare. in 2020 5th International
    Conference on Computer and Communication Systems, ICCCS 2020. https://doi.org/10.1109/ICCCS49078.2020.9118600
    (2020). Kimura, N. & Latifi, S. A survey on data compression in wireless sensor
    networks. Int. Conf. Inf. Technol. Cod. Comput. ITCC 2, 8–13. https://doi.org/10.1109/itcc.2005.43
    (2005). Article   Google Scholar   Sadler, C. M. & Martonosi, M. Data compression
    algorithms for energy-constrained devices in delay tolerant networks. in SenSys’06:
    Proceedings of the Fourth International Conference on Embedded Networked Sensor
    Systems, 265–278. https://doi.org/10.1145/1182807.1182834 (2006). Capo-Chichi,
    E. P., Guyennet, H. & Friedt, J. M. K-RLE: A new data compression algorithm for
    wireless sensor network. in Proceedings—2009 3rd International Conference on Sensor
    Technologies and Applications, SENSORCOMM 2009, 502–507. https://doi.org/10.1109/SENSORCOMM.2009.84
    (2009). El Assi, M., Ghaddar, A., Tawbi, S. & Fadi, G. Resource-efficient floating-point
    data compression using MAS in WSN. Int. J. Ad hoc Sens. Ubiquit. Comput. 4(5),
    13–28. https://doi.org/10.5121/ijasuc.2013.4502 (2013). Article   Google Scholar   Monjardin,
    C. E. F., Uy, F. A. A., Tan, F. J. & Cruz, F. R. G. Automated real-time monitoring
    system (ARMS) of hydrological parameters for Ambuklao, Binga and San Roque dams
    cascade in Luzon Island, Philippines. in 2017 IEEE Conference on Technologies
    for Sustainability, SusTech 2017, vol. 2018, 1–7. https://doi.org/10.1109/SusTech.2017.8333532
    (2018). Hou, L. et al. IoT Based Smart Wheelchair for Elderly Healthcare Monitoring.
    https://doi.org/10.1109/icccs52626.2021.9449273 (2021). Hersh, M. Overcoming barriers
    and increasing independence: Service robots for elderly and disabled people. Int.
    J. Adv. Robot. Syst. https://doi.org/10.5772/59230 (2015). Article   Google Scholar   InnoTecUK.
    AI (Artificial Intelligence) Based Healthcare System for Elderly People. (2019).
    https://projects.innotecuk.com/key-projects/ichair/. Accessed 08 Dec 2023. Sarkar,
    M., Niranjan, N. & Banyal, P. K. Mechanisms of hypoxemia. Lung India https://doi.org/10.4103/0970-2113.197116
    (2017). Article   PubMed   PubMed Central   Google Scholar   MySignals—eHealth
    and Medical IoT Development Platform. http://www.my-signals.com/. Accessed 22
    Aug 2023. Lin, S. et al. Face analysis for coronary heart disease diagnosis. in
    Proceedings 2019 12th International Congress on Image and Signal Processing, BioMedical
    Engineering and Informatics, CISP-BMEI 2019. https://doi.org/10.1109/CISP-BMEI48845.2019.8966020
    (2019). Titan LTE. https://drivedevilbiss.co.uk/products/titan-lte. Accessed 22
    Aug 2023. Digilent. DMC60C Digital Motor Controller for FIRST Robotics. (2019).
    https://www.digikey.co.uk/en/product-highlight/d/digilent/dmc60c-digital-motor-controller.
    Accessed 29 Sept 2023. Arduino Sensor Kit—Base. https://store.arduino.cc/sensor-kit-base?_gl=1*1hnfgac*_ga*MzQxNzY1MjczLjE2Mjk2NDI1MDI.*_ga_NEXN8H46L5*MTYyOTY0MjUwMS4xLjEuMTYyOTY0MjUzMi4w.
    Accessed 22 Aug 2021. IoT Analytics—ThingSpeak Internet of Things. https://thingspeak.com/.
    Accessed 22 Aug 2023. Goldberger, A. L. et al. PhysioBank, PhysioToolkit, and
    PhysioNet: Components of a new research resource for complex physiologic signals.
    Circulation 101(23), e215–e220 (2000). Article   CAS   PubMed   Google Scholar   Hunter,
    I. et al. EMG activity during positive-pressure treadmill running. J. Electromyogr.
    Kinesiol. 24(3), 348–352 (2014). Article   PubMed   Google Scholar   Amin, M.
    R., Wickramasuriya, D. S. & Faghih, R. T. A wearable exam stress dataset for predicting
    grades using physiological signals. in 2022 IEEE Healthcare Innovations and Point
    of Care Technologies (HI-POCT), IEEE (2022). MIT-BIH Arrhythmia Database v1.0.0.
    https://physionet.org/content/mitdb/1.0.0/. Accessed 22 Aug 2023. Download references
    Funding This work was supported in part by the UK Research and Innovation under
    Grant 104312, as well as the Horizon Europe EC SusFE project under grant agreement
    No. 101070477, and in part by the Science and Technology Project of Guangdong
    Province, China under Grant 2018A050501014. Author information These authors contributed
    equally: Lei Hou and Jawwad Latif. Authors and Affiliations Healthcare Innovation
    Centre, School of Health & Life Sciences, Teesside University, Middlesbrough,
    TS1 BX, UK Lei Hou, Jawwad Latif, Pouyan Mehryar & Zulfiqur Ali Zhejiang Lab,
    Research Center for Frontier Fundamental Studies, Hangzhou, 311121, China Lei
    Hou Innovative Technology and Science Ltd, Hildersham Road, Cambridge, CB21 6DR,
    UK Stephen Withers & Angelos Plastropoulos College of Computer Science and Software
    Engineering, Shenzhen University, Shenzhen, 518060, China Linlin Shen Contributions
    Z.A. and L.L.S. conceived and supervised the project. L.H. performed the experiments.
    A.P. and J.L. performed the algorithm. P.M. designed the sensing experiments.
    All authors analyzed the data. L.H., and S.W. wrote the manuscript. All authors
    discussed the results and commented on the paper. The authors affirm that human
    research participants provided informed consent for publication of the images.
    Corresponding authors Correspondence to Lei Hou or Zulfiqur Ali. Ethics declarations
    Competing interests The authors declare no competing interests. Additional information
    Publisher''s note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Open Access This article is licensed under a Creative Commons Attribution 4.0
    International License, which permits use, sharing, adaptation, distribution and
    reproduction in any medium or format, as long as you give appropriate credit to
    the original author(s) and the source, provide a link to the Creative Commons
    licence, and indicate if changes were made. The images or other third party material
    in this article are included in the article''s Creative Commons licence, unless
    indicated otherwise in a credit line to the material. If material is not included
    in the article''s Creative Commons licence and your intended use is not permitted
    by statutory regulation or exceeds the permitted use, you will need to obtain
    permission directly from the copyright holder. To view a copy of this licence,
    visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions About
    this article Cite this article Hou, L., Latif, J., Mehryar, P. et al. An autonomous
    wheelchair with health monitoring system based on Internet of Thing. Sci Rep 14,
    5878 (2024). https://doi.org/10.1038/s41598-024-56357-y Download citation Received
    26 December 2023 Accepted 05 March 2024 Published 11 March 2024 DOI https://doi.org/10.1038/s41598-024-56357-y
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Autonomous driving Healthcare monitoring Internet of Things
    Smart wheelchair Subjects Engineering Health care Comments By submitting a comment
    you agree to abide by our Terms and Community Guidelines. If you find something
    abusive or that does not comply with our terms or guidelines please flag it as
    inappropriate. Download PDF Sections Figures References Abstract Introduction
    Results Discussion Conclusion Methods Data availability References Funding Author
    information Ethics declarations Additional information Rights and permissions
    About this article Comments Advertisement Scientific Reports (Sci Rep) ISSN 2045-2322
    (online) About Nature Portfolio About us Press releases Press office Contact us
    Discover content Journals A-Z Articles by subject Protocol Exchange Nature Index
    Publishing policies Nature portfolio policies Open access Author & Researcher
    services Reprints & permissions Research data Language editing Scientific editing
    Nature Masterclasses Research Solutions Libraries & institutions Librarian service
    & tools Librarian portal Open research Recommend to library Advertising & partnerships
    Advertising Partnerships & Services Media kits Branded content Professional development
    Nature Careers Nature Conferences Regional websites Nature Africa Nature China
    Nature India Nature Italy Nature Japan Nature Korea Nature Middle East Privacy
    Policy Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility
    statement Terms & Conditions Your US state privacy rights © 2024 Springer Nature
    Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: An autonomous wheelchair with health monitoring system based on Internet
    of Thing
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kowadlo G.
  - Mittelberg Y.
  - Ghomlaghi M.
  - Stiglitz D.K.
  - Kishore K.
  - Guha R.
  - Nazareth J.
  - Weinberg L.
  citation_count: '0'
  description: 'Background: Pre-operative risk assessment can help clinicians prepare
    patients for surgery, reducing the risk of perioperative complications, length
    of hospital stay, readmission and mortality. Further, it can facilitate collaborative
    decision-making and operational planning. Objective: To develop effective pre-operative
    risk assessment algorithms (referred to as Patient Optimizer or POP) using Machine
    Learning (ML) that predict the development of post-operative complications and
    provide pilot data to inform the design of a larger prospective study. Methods:
    After institutional ethics approval, we developed a base model that encapsulates
    the standard manual approach of combining patient-risk and procedure-risk. In
    an automated process, additional variables were included and tested with 10-fold
    cross-validation, and the best performing features were selected. The models were
    evaluated and confidence intervals calculated using bootstrapping. Clinical expertise
    was used to restrict the cardinality of categorical variables (e.g. pathology
    results) by including the most clinically relevant values. The models were created
    with logistic regression (LR) and extreme gradient-boosted trees using XGBoost
    (Chen and Guestrin, 2016). We evaluated performance using the area under the receiver
    operating characteristic curve (AUROC) and the area under the precision-recall
    curve (AUPRC). Data was obtained from a metropolitan university teaching hospital
    from January 2015 to July 2020. Data collection was restricted to adult patients
    undergoing elective surgery. Results: A total of 11,475 adult admissions were
    included. The performance of XGBoost and LR was very similar across endpoints
    and metrics. For predicting the risk of any post-operative complication, kidney
    failure and length-of-stay (LOS), POP with XGBoost achieved an AUROC (95%CI) of
    0.755 (0.744, 0.767), 0.869 (0.846, 0.891) and 0.841 (0.833, 0.847) respectively
    and AUPRC of 0.651 (0.632, 0.669), 0.336 (0.282, 0.390) and 0.741 (0.729, 0.753)
    respectively. For 30-day readmission and in-patient mortality, POP with XGBoost
    achieved an AUROC (95%CI) of 0.610 (0.587, 0.635) and 0.866 (0.777, 0.943) respectively
    and AUPRC of 0.116 (0.104, 0.132) and 0.031 (0.015, 0.072) respectively. Conclusion:
    The POP algorithms effectively predicted any post-operative complication, kidney
    failure and LOS in the sample population. A larger study is justified to improve
    the algorithm to better predict complications and length of hospital stay. A larger
    dataset may also improve the prediction of additional specific complications,
    readmission and mortality.'
  doi: 10.1186/s12911-024-02463-w
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Explore
    journals Get published About BMC Login BMC Medical Informatics and Decision Making
    Home About Articles Submission Guidelines Collections Join The Board Submit manuscript
    Research Open access Published: 11 March 2024 Development and validation of ‘Patient
    Optimizer’ (POP) algorithms for predicting surgical risk with machine learning
    Gideon Kowadlo, Yoel Mittelberg, Milad Ghomlaghi, Daniel K. Stiglitz, Kartik Kishore,
    Ranjan Guha, Justin Nazareth & Laurence Weinberg  BMC Medical Informatics and
    Decision Making  24, Article number: 70 (2024) Cite this article 383 Accesses
    5 Altmetric Metrics Abstract Background Pre-operative risk assessment can help
    clinicians prepare patients for surgery, reducing the risk of perioperative complications,
    length of hospital stay, readmission and mortality. Further, it can facilitate
    collaborative decision-making and operational planning. Objective To develop effective
    pre-operative risk assessment algorithms (referred to as Patient Optimizer or
    POP) using Machine Learning (ML) that predict the development of post-operative
    complications and provide pilot data to inform the design of a larger prospective
    study. Methods After institutional ethics approval, we developed a base model
    that encapsulates the standard manual approach of combining patient-risk and procedure-risk.
    In an automated process, additional variables were included and tested with 10-fold
    cross-validation, and the best performing features were selected. The models were
    evaluated and confidence intervals calculated using bootstrapping. Clinical expertise
    was used to restrict the cardinality of categorical variables (e.g. pathology
    results) by including the most clinically relevant values. The models were created
    with logistic regression (LR) and extreme gradient-boosted trees using XGBoost
    (Chen and Guestrin, 2016). We evaluated performance using the area under the receiver
    operating characteristic curve (AUROC) and the area under the precision-recall
    curve (AUPRC). Data was obtained from a metropolitan university teaching hospital
    from January 2015 to July 2020. Data collection was restricted to adult patients
    undergoing elective surgery. Results A total of 11,475 adult admissions were included.
    The performance of XGBoost and LR was very similar across endpoints and metrics.
    For predicting the risk of any post-operative complication, kidney failure and
    length-of-stay (LOS), POP with XGBoost achieved an AUROC (95%CI) of 0.755 (0.744,
    0.767), 0.869 (0.846, 0.891) and 0.841 (0.833, 0.847) respectively and AUPRC of
    0.651 (0.632, 0.669), 0.336 (0.282, 0.390) and 0.741 (0.729, 0.753) respectively.
    For 30-day readmission and in-patient mortality, POP with XGBoost achieved an
    AUROC (95%CI) of 0.610 (0.587, 0.635) and 0.866 (0.777, 0.943) respectively and
    AUPRC of 0.116 (0.104, 0.132) and 0.031 (0.015, 0.072) respectively. Conclusion
    The POP algorithms effectively predicted any post-operative complication, kidney
    failure and LOS in the sample population. A larger study is justified to improve
    the algorithm to better predict complications and length of hospital stay. A larger
    dataset may also improve the prediction of additional specific complications,
    readmission and mortality. Peer Review reports Introduction The adoption and deployment
    of electronic health records (EHRs) has facilitated the accessibility of large
    patient datasets. Machine learning (ML) has succeeded in diverse arenas, demonstrating
    an ability to operate on large and complex datasets. At the intersection of EHR
    data and the progress of ML, is an opportunity to develop tools for personalised
    medicine. Currently, the most common ML applications in medicine are in imaging
    [1, 2]. An upcoming frontier is surgical risk prediction [3]. Surgery is often
    the only option to alleviate disability and reduce the risk of death from common
    conditions. Millions of people annually undergo surgical treatment, and surgical
    interventions account for an estimated 13% of the world’s total disability-adjusted
    life years (DALYs). Even in the most advanced hospital systems, there is a high
    mortality and complication rate [4, 5], risks of direct harm to patients and high
    financial costs. The WHO recognises these issues as major worldwide health burdens
    [6]. Fortunately, a substantial number of these complications are preventable
    [7]. Pre-operative risk assessment allows clinicians to mitigate adverse outcomes,
    better inform patients and their families about surgical outcomes and risks and
    plan post-operative care [8, 9]. The first generation of risk calculators exists,
    such as the American College of Surgeons National Surgical Quality Improvement
    Program (NSQIP) [10] and the Surgical Outcome Risk Tool (SORT) [11]. They are
    based on linear statistical techniques and are designed to use a low number of
    input parameters to be convenient for manual data entry. These approaches do not
    exploit the data available in modern EHR systems. Additionally, most provide mortality
    risk only. There are also manual risk assessments such as American Society of
    Anesthesiologists (ASA) Physical Status Classification [12] that are effective
    but subjective. It is often difficult for clinicians to find the data and calculate
    the score manually; therefore, they are rarely used [13]. In recent years, more
    sophisticated algorithms have been developed using ML. They typically predict
    a wider range of outcomes than traditional risk calculators and incorporate a
    larger set of input features made available by EHR data. The most common prediction
    outcomes are mortality and post-surgical complications such as acute kidney injury,
    delirium, deep vein thrombosis, pulmonary embolism and pneumonia. ML can be more
    effective than traditional methods [13] such as ASA, CCI, POSSUM [14] and NSQIP
    [15] and can be more effective than human experts [16]. Various techniques have
    been used such as deep learning [17, 18], logistic regression [19, 20], generalised
    additive models (GAMs) [5] and decision trees [15, 21,22,23,24,25]. Further, some
    studies focus on harmonising EHR data [17], testing existing approaches on suitability
    for local populations [3, 19] or predicting the use of the readmission prevention
    clinic [21]. Most of the studies in the literature cited above, focus on the prediction
    of mortality and complications; however, additional endpoints are clinically meaningful.
    Some studies such as [17, 25], utilise sequences of vital sign measurements, unstructured
    notes and radiological images, but in many hospitals, those data are not practically
    obtainable. Study aims This study aims to use readily available EHR data to develop
    interpretable ML risk prediction algorithms to standardise and improve clinical
    decision-making. The target endpoints are length-of-stay (LOS), complications,
    unplanned 30-day readmission and in-patient mortality. Our definition of readily
    available EHR data is patient history, excluding unstructured notes and radiological
    imaging. The algorithms should be interpretable as the ultimate objective is to
    provide information that is understandable, actionable and trusted in a clinical
    setting. Method Study design This was a single-centre cohort study with retrospective
    data collection in adult patients (aged years) who underwent any elective surgical
    procedure at Austin Health between 1st January 2015 and 31st July 2020. Austin
    Health is a university teaching hospital in Australia, with a high volume of surgeries
    across multiple sub-specialities that are performed annually. We restricted cases
    to elective surgery, which comprises 85% of surgical cases in Australia [26] and
    where there is the greatest opportunity to mitigate risk, based on perioperative
    risk prediction. Currently risk predictors such as ASA and NSQIP are standard
    tools used by perioperative physicians to assess risk in elective surgery patients.
    First and foremost, elective surgery affords time for a thorough pre-operative
    evaluation and optimisation of the patient and the opportunity to choose many
    factors that influence their care, such as theatre location (ICU availability),
    blood availability, and many others. In addition, predictions have operational
    utility, for example for planning and scheduling to ensure higher patient throughput.
    We developed risk assessment models for the target endpoints following the Transparent
    Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis
    (TRIPOD) guidelines for risk prediction [27]. Our approach was to begin with a
    base model that emulates a standard approach internationally for surgical risk
    assessment, exploiting two key dimensions: patient-risk and procedure-risk. Each
    was derived from clinical expertise provided by perioperative clinicians with
    at least 10-years of postgraduate experience and familiarity with risk stratification
    for surgical morbidity. The next step was to iteratively add features to the base
    model, resulting in a unique set of features for each model. Model development
    We evaluated our method with two model types. First, logistic regression (LR)
    (using scikit-learn [28]), a widely accepted and relatively straightforward linear
    approach, which served as a baseline. Second, we compared to extreme gradient-boosted
    decision trees (XGBoost, using the XGBoost package [29]), a more complex algorithm
    that is capable of capturing non-linear decision boundaries and interactions between
    features. XGBoost models are interpretable and among the best performing for tabular
    data. There are four main stages to the method. The Data source provides data
    for Pre-processing that reduces dimensionality and transforms relational data
    into a tabular form suitable for algorithm consumption. Feature selection selects
    a subset of features to optimise prediction scores for each endpoint. Finally,
    an Evaluation of the models is performed with bootstrapping. The pipeline is illustrated
    in Fig. 1 and elaborated below. Fig. 1 Data processing pipeline Full size image
    Data source The Data Analytics and Research Evaluation (DARE) Centre provided
    a data extract from the Austin Health Cerner EHR system. A total of 11,475 unique
    admissions were included, covering all elective adult surgical procedures. The
    raw data includes: patient demographic details (age, weight, height, gender) procedures
    performed (primary/scheduled and other) other procedural information (including
    details of the admission and episode) pathology results medications prescribed
    during admission comorbidities: diagnoses using the International Classification
    of Diseases (ICD-10-AM, 9th Edition) Charlson comorbidity index (CCI) derived
    from the ICD codes complications, indicated by ICD codes Pre-processing After
    filtering and cleansing the data, we derived features from raw values: body mass
    index (BMI) and the two features used for the base model, namely estimates of
    patient-risk and procedure-risk (“Study design” section). The process is illustrated
    in Fig. 2. Fig. 2 Dimensionality reduction and calculated features Full size image
    Patient-risk is a proxy for ASA [12]. It is an ordinal numerical value calculated
    through the patient’s diagnoses (using ICD codes). Each ICD code was scored by
    a clinician as either included or excluded from patient risk (score of 0 or 1).
    In addition, codes that represented ‘cancerous’ or ‘cardiopulmonary and vascular’
    were scored higher (score of 2). The total of the ICD risk scores attributable
    to a patient admission was used to calculate the patient-risk. Procedure-risk
    is an ordinal categorical value calculated using a clinically determined risk
    rating (low, medium or high) of the scheduled surgical procedure, which was estimated
    as the earliest non-preparatory procedure. We reduced dimensionality where possible
    to reduce overfitting and improve interpretability. We grouped procedure names
    by anatomical region. Although all patient episodes are elective, the individual
    procedures can be varying levels of elective or emergency, referred to as procedure
    type. The procedure type was reduced to a binary category. For laboratory results,
    we selected a priori eight clinically relevant variables, namely haemoglobin,
    albumin, creatinine, urea, international normalised ration, platelet count, activated
    partial thromboplastin time, and estimated glomerular filtration rate. We grouped
    patient medications by therapeutic class. Finally, very infrequent categories
    were grouped into an ‘other’ bucket. Dimensionality reduction is summarised in
    Fig. 2. Categorical data was one-hot encoded. Where there were one-to-many relationships
    (such as admission to medications) the reduction methods were chosen to provide
    the most clinically relevant summary. Missing categorical data were treated as
    legitimate input by creating a ‘missing’ category. Missing numerical data were
    imputed with XGBoost’s in-built mechanism and by using the median for LR. For
    LR, numerical data was standardized (using scikit-learn’s StandardScaler). Class
    balancing was achieved by applying a higher weight to under-represented classes.
    See Supplementary Materials, Section 1 for more details on pre-processing. Feature
    selection Highly correlated (or collinear) features were removed due to their
    redundancy. We used the variance inflation factor method for multi-collinearity
    analysis with a threshold of 10 [30, 31]. Variables with very low variance were
    removed by detecting features where the ratio between the highest occurring value
    and the second highest was greater than 19, a large threshold to avoid losing
    valuable information [32]. After training and scoring the base model consisting
    of patient-risk and procedure-risk (“Study design” section), an automated iterative
    process added and tested new features. Each available feature was individually
    added to the model and evaluated using area under the receiver operating characteristic
    curve (AUROC) with 10-fold cross-validation. The feature that achieved the highest
    gain in score was added to the selected feature set and the search restarted.
    The remaining features were re-tested in subsequent iterations, after which the
    composition of the selected feature-set had changed. The process continued until
    all features were used and then the model with the highest score was selected.
    Hyperparameter tuning then took place to optimise results (see Supplementary Materials,
    Section 2). Predicted outcomes Length-of-stay (LOS) was framed as multiclass classification.
    We identified three dominant groupings through visual inspection of the distribution
    (see Fig. 3) and defined them by ordering and then segmenting the data into three
    equally sized buckets. The resulting groups were ‘low’ ( 31 hours), ‘medium’ (31
    – 117 hours), and ‘high’ ( 117 hours), equating to one night, two to four nights
    and five nights or more. The ranges were validated through clinical review. There
    was a classifier for each bucket and the prediction was the classifier with the
    highest confidence. The labels (low, medium and high) do not describe clinical
    significance, which depends on the procedure type. For example, ‘medium’ duration
    may be considered prolonged for a simple procedure, whereas ‘medium’ may be expected
    for a more complex procedure. Fig. 3 Length-of-stay: Training data are segmented
    into 3 classes, to cast predicting length-of-stay as multiclass classification.
    There is a clear periodicity around whole days. The x-axis is truncated at 200
    hours to provide detail in the most interesting range. The trend continues past
    200 hours with a steady monotonic decrease in magnitude Full size image Unplanned
    30-day readmission (hereafter abbreviated to ‘30-day readmission’), in-patient
    mortality and the presence of complications (as indicated by the ICD code) were
    explicitly labelled in the dataset. Evaluation The final score and confidence
    intervals were calculated with non-parametric bootstrapping using 1,000 iterations.
    For each iteration the training set size was the same as the whole dataset. As
    bootstrapping involves sampling with replacement, this resulted in approximately
    70% unique samples for training, leaving the left-out 30% for testing. Performance
    metrics Several metrics were used to assess and measure performance: area under
    the receiver operating characteristic curve (AUROC), area under the precision-recall
    curve (AUPRC) and F1 (FBeta, where beta = 1). AUROC is most common in related
    literature. A drawback of AUROC is that it can be misleading on extremely rare
    classes such as mortality and readmission. In such cases, it can achieve an artificially
    high score because the true negatives dwarf the false positives Footnote 1. AUPRC
    is more informative with extremely rare labels [14, 33]. It indicates the trade-off
    between precision (the proportion of true positives of all predicted positives),
    also referred to as positive predictive value (PPV), and recall (the proportion
    of true positives of all positives). F1, the harmonic mean of precision and recall,
    is also suitable for rare classes. We used micro-averaging to calculate the area
    under the curve for multiclass predictions. In addition to the single metric derived
    from the ‘area’ under the respective curves AUROC and AUPRC, we also inspected
    the profiles of the curves, showing how they perform at different operating points.
    Interpretability To visualise the relative importance of features for each model,
    we utilised two methods. The first was XGBoost feature importance, based on the
    average gain of splits per feature. The other was SHapley Additive exPlanations
    (SHAP) [34] which uses cooperative game theory to assign partial credit to the
    input variables for the model’s output. Both methods indicate feature importance
    from different perspectives. XGBoost feature importance gives direct insight into
    the internal structure of the learned trees and provides a single absolute value
    for importance. SHAP treats the model as a black box and bases the importance
    on the observed behaviour of the model. The plots are more informative, showing
    the distribution of observed values and the corresponding directionality of the
    impact on the model. To visualise the features’ influence on specific predictions
    for individual patients, we used SHAP. We plotted typical true positives for each
    of the effective models to demonstrate how SHAP can be used to help make specific
    predictions actionable. The predictions combined with visualisations could allow
    clinicians to understand the most important features in general, while providing
    per-patient feedback on the key features contributing to a prediction. Consequently,
    clinicians can take appropriate actions to address patient or procedure factors
    to minimise risk. In addition, the predictions together with visualisations could
    enable hospitals to improve decision making, such as pre-admission patient optimization
    or capacity planning i.e. booking theatres or hospital resources. For example,
    if a patient is expected to have a longer than expected length-of-stay, the hospital
    administration could anticipate that they would take up a hospital bed for a period
    longer than typical for the relevant procedure. Results Data characteristics A
    total of 11,475 adults were included. There were 41 (0.36%) occurrences of in-patient
    mortality and 941 (8.2%) occurrences of 30-day readmissions. There were 4,351
    (37.92%) complications. The number of occurrences of low, medium and high LOS
    were 3,868 (33.7%), 3,790 (33.0%) and 3,817 (33.3%), respectively. The data characteristics
    are presented in Table 1. Table 1 Data characteristics: The first column shows
    the number of affirmative cases for binary fields and the number of unique values
    for multivalue categorical fields. The second column shows the number of admissions
    with a valid value (e.g., if height is missing, it is deemed invalid). Empty cells
    denote N/A Full size table Accuracy The results are summarised in Tables 2 and
    3, and the ROC and PR curves are shown in Figs. 4 and 5. We selected only those
    specific complications with an adequate number of positive examples to make training
    feasible (above a threshold of 100 (0.8%), including kidney failure, arrhythmia,
    delirium and heart failure. Other complications such as cardiac or respiratory
    arrest, liver failure, pulmonary embolism, and respiratory failure did not have
    sufficient data points. Table 2 Performance of risk models Full size table Table
    3 Performance of risk models for complications Full size table Fig. 4 Receiver
    operating characteristic and precision-recall curves – readmission, mortality
    and length-of-stay. The mortality curves appear stepped due to the fact that there
    are only 7 positive samples Full size image Fig. 5 Receiver operating characteristic
    and precision-recall curves – complications. The mortality curves appear stepped
    due to the fact that there are only 7 positive samples Full size image For predicting
    the risk of any post-operative complication, kidney failure and LOS, XGBoost achieved
    an AUROC (95%CI) of 0.755 (0.744, 0.767), 0.869 (0.846, 0.891) and 0.841 (0.833,
    0.847) respectively and AUPRC of 0.651 (0.632, 0.669), 0.336 (0.282, 0.390) and
    0.741 (0.729, 0.753), respectively; LR achieved an AUROC (95%CI) of 0.747 (0.735,
    0.76), 0.883 (0.863, 0.901) and 0.822 (0.815, 0.829) respectively and AUPRC of
    0.646 (0.628, 0.665), 0.308 (0.258, 0.363) and 0.719 (0.707, 0.73), respectively.
    Refer to the table for full results of other specific complications. For 30-day
    readmission and in-patient mortality, XGBoost achieved an AUROC (95%CI) of 0.61
    (0.587, 0.635) and 0.866 (0.777, 0.943), respectively and AUPRC of 0.116 (0.104,
    0.132) and 0.031 (0.015, 0.072), respectively; LR achieved an AUROC (95%CI) of
    0.622 (0.599, 0.645) and 0.914 (0.811, 0.956), respectively and AUPRC of 0.13
    (0.113, 0.149) and 0.044 (0.019, 0.114), respectively. On visual inspection, the
    ROC curves provide reasonable operating points for all models. Inspection of the
    precision-recall (PR) curves also shows some models have effective operating points;
    although the endpoints with extremely rare positive examples do not, including
    readmission, mortality, and the specific complications other than kidney failure.
    For LOS, accuracy was consistently higher for the two ends of the spectrum (low
    and high) compared to medium which experienced more class overlap than low or
    high. The performance of XGBoost was very similar to LR, and there was no clear
    winner across metrics or endpoints. Interpretability For simplicity, we used one
    model type to explore interpretability. We chose XGBoost, as the performance of
    XGBoost and LR was comparable, and XGBoost is capable of finding more complex
    relationships which may be relevant for other datasets. The selected features
    and their importance are shown for the effective models: complications in Fig.
    6, kidney failure in Fig. 7 and LOS in Fig. 8. For terminology used in the figures,
    please refer to Table 4. Fig. 6 Feature importance for any complication: XGBoost
    gain (left) and SHAP (right), where each dot represents one sample, the colour
    indicates the value and the position on the x-axis indicates the impact (positive
    or negative) on model output. Refer to Table 4 for terminology Full size image
    Fig. 7 Feature importance for kidney failure: XGBoost gain (left) and SHAP (right),
    where each dot represents one sample, the colour indicates the value and the position
    on the x-axis indicates the impact (positive or negative) on model output. Refer
    to Table 4 for terminology Full size image Fig. 8 Feature importance for length-of-stay
    using XGBoost gain. Refer to Table 4 for terminology Full size image Table 4 Feature-name
    terminology Full size table For all the models and visualisation methods, procedure-risk
    and features representing the patient’s health (CCI summaries and patient-risk)
    are amongst the top factors. Patient-risk and CCI represent the patient’s overall
    health. Although patient-risk is derived from more specific and diverse comorbidities
    than CCI, the feature importance plots showed that across the cohort, CCI was
    an important factor particularly in the age-adjusted CCI [35], and comparable
    to patient-risk. However, patient-risk and CCI are both valuable as they contain
    different information, as illustrated in the example of a specific patient high
    LOS, Fig. 10, where patient-risk and CCI have an opposing influence. In addition
    to procedure-risk and patient health, there are other important features. For
    any complication (Fig. 6), XGBoost shows significant tree splits for some specific
    procedure groups: diabetes, total medication dosages and use of analgesics. The
    SHAP features are largely aligned, with differences in the relative values. For
    kidney failure (Fig. 7), related morbidities (diabetes, cancer) and pathology
    results (albumin, creatinine, urea, activated partial thromboplastin time and
    haemoglobin) are also important. For length-of-stay (Fig. 8), the features had
    differing importance to the individual models (low, medium and high), although
    many features are unimportant for all models. Compared to the other models, specific
    procedure groups are relatively important. Feature importance in specific predictions
    using SHAP plots is shown for correct predictions of a) kidney failure (Fig. 9)
    and b) ‘high’ LOS for a procedure that typically has a medium-term LOS (Fig. 10).
    The purpose is to show how SHAP can provide a convenient interpretation of the
    important factors for a given prediction. Fig. 9 SHAP visualisation for a specific
    patient’s risk of kidney failure. This is a True Positive (TP) prediction with
    a probability of 0.87. The length of the bar indicates the influence of that feature
    on the prediction. The colour indicates whether the influence is positive (red)
    or negative (blue). The grey value to the left of the feature name is the value
    of that feature for this patient Full size image Fig. 10 SHAP visualisation for
    a specific patient’s length-of-stay. This is a True Positive (TP) prediction of
    a high length-of-stay, with a probability of 0.72. The length of the bar indicates
    the influence of that feature on the prediction. The colour indicates whether
    the influence is positive (red) or negative (blue). The grey value to the left
    of the feature name is the value of that feature for this patient Full size image
    Discussion Key findings In this single-centre cohort study in adult surgical patients,
    we developed effective pre-operative risk assessment algorithms (POP) using machine
    learning, providing pilot data to inform the design of a larger prospective study.
    We found that POP algorithms were effective for predicting post-operative complications
    and LOS. However, a larger study is justified to further improve the algorithm
    for predicting specific complications, readmission and mortality. The performance
    of logistic regression (LR) and XGBoost models was similar, with no clear winner
    across endpoints and metrics; suggesting that for this set of features, a linear
    decision boundary is sufficient and there are no significant relationships between
    features. It is possible that with more patients and/or more features (such as
    patient notes and imaging), non-linear methods such as XGBoost would outperform
    LR. For simplicity, we focus on one model type, XGBoost, in the interpretability
    results and for the remainder of the Discussion. We chose XGBoost as it is more
    capable and has superior ease-of-use. Comparison to other methods Comparing accuracy
    to other models in the literature is very difficult for several reasons. The quality
    and structure of different datasets vary greatly, cohort differences can influence
    results [36] and endpoints are often defined differently (e.g., 24 hours after
    admission compared to immediately before surgery). Moreover, the choice of performance
    metrics also varies. However, considering the difficulties, it can be useful to
    compare results to provide some context. One of the studies that we compared to
    is Rajkomar et al. [17]. Beyond tabular EHR data, they used additional data sources
    including radiological imaging, unstructured notes, vital sign measurements, time-series
    embedding to handle these data streams, as well as ensembling of complementary
    models. While we consider it to be the ‘gold-standard’, and therefore present
    it as context, we do not aim to match their scores. Our study investigates the
    feasibility of risk predictions with more limited and commonly available data
    sets. In studies with similar objectives to ours, authors compared LR and variations
    of Boosted Decision Trees such as XGBoost (as well as other algorithms) [14, 15,
    25]. In general, the Boosted Decision Tree algorithms were superior to LR, although
    similarly to our results, Corey et al. [15] found LR and XGBoost had very similar
    performance. Where Boosted Decision Trees had an advantage, it could be due to
    the difference in available features or in the methodology used in feature selection.
    The model development pipeline in our study selected the most suitable features
    for each algorithm, and as a result they may use different features to predict
    any particular label. In addition, there were differences in the set of predicted
    outcomes; compared to mortality, ICU admission, and complications [14, 15, 25]
    our successful models also included length-of-stay. Regardless of which algorithm
    performed the best, these studies supported our findings that Machine Learning
    models can provide useful – and in the case of [25], interpretable – predictions,
    to assist clinical decision making. Comparison to standard risk predictors Standard
    risk predictors, such as ASA or CCI provide one score that captures patient risk.
    Such a score can be used to predict ‘general’ outcomes, such as the use of ASA
    to predict mortality and ICU Admission, but has been found to be less effective
    than ML approaches [14]. Moreover, it is not clear how to directly translate the
    score into more specific outcomes such as length-of-stay or specific complications.
    Such outcomes could provide additional administrative or clinical insights to
    assist with patient management and decision making. However, standard risk predictors
    can be used as features, as we did in this study. Feature importance analysis
    showed that CCI and patient-risk (a proxy metric for ASA) are indeed significant
    contributors to model prediction. Other features, such as pathology results or
    medication classes, are also identified by the model as adding predictive value.
    Evaluation metrics The standard practice for evaluating risk prediction algorithms
    is the ROC curve. Using ROC, all of our models appear to be effective. They have
    a good profile with viable operating points, and relatively good AUROC. However
    the results using the PR curves reveal a different story. AUPRC for readmission
    and mortality is very low, and there are no satisfactory operating points on the
    profile. The results confirm that ROC can be misleading for rare classes as suggested
    by [14] (and discussed in “Evaluation” section). They demonstrate the importance
    of metrics that are insensitive to rare classes, such as AUPRC or FBeta for clinical
    algorithms. We used a relatively small dataset (see “Limitations” section). With
    more data and therefore more positive examples, the performance is likely to improve,
    as measured by both AUROC and AUPRC. LOS prediction LOS classification was very
    effective. There is LOS data for every admission, providing ample training signal,
    which is reflected in the ROC and PR curves. LOS predictions have both clinical
    and operational decision making benefits. From a clinical perspective, a prolonged
    or ‘longer than expected’ stay prediction could prompt closer attention. From
    an operational perspective, these predictions could be used for scheduling to
    optimise for ward utilisation and selection of appropriate sites. To the best
    of our knowledge, other ML risk predictors did not consider LOS, except [17].
    They predicted ‘prolonged length-of-stay’, defined as ‘at least 7 days’, whereas
    POP predicts multivalue LOS: low, medium or high. Predicting multivalue LOS makes
    it possible to have a dynamic definition of ‘prolonged’ that depends on factors
    such as procedure and patient. For example, a medium stay (two to four nights)
    prediction could trigger ‘prolonged’ for short-stay surgery (1 night) and healthy
    patients. Secondly, a more granular prediction allows better operational planning.
    Our accuracy, measured using AUROC, was comparable to [17], 0.841 compared to
    0.85 and 0.86 (for two hospital sites respectively), despite fewer data types
    and a much smaller dataset. Unfortunately AUPRC was unavailable for comparison
    to gain a fuller picture. Complication prediction Results for predicting any complication
    were promising, with both AUROC and AUPRC having viable operating points. Of all
    the complications, four had adequate positive examples to train the models. These
    had reasonable ROC curves, but precision and recall showed that only kidney failure,
    which is less rare than the others, was a viable model. In a clinical setting,
    positive predictions could be used as a general indicator that there is morbidity,
    and investigations are warranted. An example of an operating point for kidney
    failure is approximate recall of 12%, and precision of 62%. Out of 100 patients
    with kidney failure, the model will identify approximately 12. Of those, approximately
    62% (7.4) will actually develop kidney failure (true positives). If the information
    is presented so that it doesn’t give a false sense of security if not shown, then
    it can pick up when there is a case, aiding clinical care. The results compare
    favourably to similar studies, despite a much smaller dataset (“Limitations” section).
    Across specific complications, and using AUROC, POP XGBoost models scored 0.794
    – 0.869 compared to 0.820 – 0.940 [5], 0.772 – 0.909 [15] and 0.88 – 0.89 [17].
    For any complication, POP XGBoost scored 0.755 compared to 0.829 – 0.836 in [15].
    Again, PR results are unavailable for a more complete comparison. Precision (referred
    to as PPV or positive predictive value) was reported in [5], which showed the
    same pattern as POP with rare classes (i.e., the rarer classes generally have
    lower precision). Interpretability The introduction of ML often leads to improved
    performance, but it can come at the cost of interpretability. We used XGBoost
    and SHAP feature importance plots. They are intuitive and build trust in the model,
    helping to make it understandable and actionable. The relative importance of features
    learned by the algorithm aligns with clinical practice. For example, the high
    importance of procedure information combined with patient health is commonly used
    to assess the risk of surgery. Alignment with clinical practice provides confidence
    that learning is effective and generalisable. Additionally, the relative weighting
    of feature importances can provide new insights into the relationship between
    features and outcomes. Although not causative, it indicates a relationship, and
    warrants further investigation. A better understanding of the factors, especially
    modifiable ones, could impact clinical practice. The first type of visualisation
    is the feature importance of the model in general, indicating systematic relationships
    across samples in the dataset. The other type of visualisation was feature importance
    for specific predictions, which highlights factors for individual patients. This
    information can provide an opportunity for more personalised risk mitigation.
    We now explore kidney failure as a case study. The model highlighted comorbidities
    (Fig. 7) that align with current knowledge. Several pathology results are also
    considered important; for example, some known to be related to renal function
    such as creatinine and urea, and others that are generally indicative of post-operative
    outcomes such as albumin [4], pathology related to coagulation (INR, APTT, PLT)
    [37, 38] and heamaglobin (Hb) [39, 40]. Some procedure groups were protective:
    ‘trans-urethral resection of the prostate’ (TURP), likely because it improves
    renal function; and ‘nose and facial sinus surgery’, likely because it is very
    low risk. The importance of ‘total knee replacement’ is unexpected, and warrants
    further investigation; for example, the underlying cause may be tourniquet time,
    length of surgery or even anaesthetic type. Surprisingly, diabetes is protective.
    We hypothesise that patients with this condition are more actively managed, so
    it is not picked up by the model, which learns from raw correlations. Alternatively,
    it could be due to conflating factors, which may have a higher than usual impact
    on results due to the small number of positive samples. It is also the likely
    explanation for a similarly protective effect in the model for any complication
    (Fig. 6). It would be beneficial to repeat the study after gathering a larger
    sample size, and a more thorough investigation that includes causal analysis is
    an important topic for future work. Understanding the expected and unexpected
    features may allow for patient-specific pre-operative intervention to minimise
    post-operative complications. For example, by optimising HbA1c in diabetics, being
    aggressive in comorbid management such as blood pressure optimisation, and shortening
    tourniquet time in knee replacements. It is possible that the patient with kidney
    failure (Fig. 9) could have been missed, because they do not have diabetes and
    it was a low-risk procedure. However, the patient suffered post-operative kidney
    failure and POP XGBoost predicted it with 84% confidence. High creatinine and
    comorbidity burden are the most significant contributions. The high creatinine
    confirm that this patient likely has impaired renal function, and the prediction
    could lead to pre-surgical intervention including more intensive management of
    medications, ensuring the patient is well hydrated, selecting more appropriate
    anaesthesia type, and optimisation and monitoring of renal perfusion. Another
    case study is LOS. The patient underwent a knee-replacement procedure, which is
    usually a medium LOS. However, POP XGBoost correctly identified this patient as
    having a high LOS (above five nights) and the SHAP plot (Fig. 10) provides visibility
    into the reasons. The most significant indicators are comorbidities, a high number
    of prescribed medications and the procedure itself. As a result of the prediction
    and indicators, the patient could be booked in for a longer stay and more intensive
    management. Most of the studies reviewed, consider interpretability of models
    to be important for clinical practice, chose algorithms that support it [15] and
    additionally investigated and reported interpretability results [5, 13, 14, 21,
    25, 41, 42]. Lee et al. [42] used a GAM-NN for the benefit of neural networks
    and the interpretability of GAMs–there is a neural network for each input feature
    (or group of features), and they are linearly combined for classification. However,
    most studies did not consider which features contributed to specific predictions.
    Bihorac et al. [5] used an approach, where the feature importance was “based on
    how different she or he is from the patient with an ‘average’ risk”. The reason
    for the prediction must be inferred indirectly, but the method could be applied
    to any model. Rajkomar et al. [17] used deep learning neural networks, where interpretability
    is more of a challenge. They showed a proof-of-concept of how it can be done.
    Active research is taking place to improve interpretability of deep learning models
    [43]. The SHAP plots that were demonstrated here, can be used with any model.
    Limitations The dataset was relatively small for this type of algorithm. For context,
    other studies cited in this paper range between 51,457 patients [16] to 99,755
    [15] admissions and [17] 216,221 admission. We expect the performance to improve
    with more data, particularly for specific complications, readmission and mortality,
    as there were very few positive examples to learn from in our study. In addition
    to the small dataset, a possible contributor to rare cases is missing or incorrect
    classifications. For example, ICD codes, which represented complications, are
    known to be incomplete. The outcomes may be captured in unstructured data, such
    as patient notes, but these data were unavailable. The booked procedure is an
    important factor for predicting risk, according to both clinical practice and
    the models’ feature importance. However, the booked procedure was not explicitly
    labelled and was therefore estimated (see “Pre-processing” section), resulting
    in errors that were difficult to quantify. Data for patient height and weight
    were sparse, but these fields are considered to be important patient health factors.
    Likewise, there were many cases of missing medication therapeutic class, leading
    to information loss when grouping medications by this variable. Obtaining additional
    data in these respects is likely to improve performance. The dataset did not extend
    beyond discharge, restricting mortality to in-patient mortality. In comparison,
    most risk calculators predict mortality at various stages after discharge such
    as 30-day and 60-day mortality. This is clinically important and there would be
    more examples which would improve the model. To the authors’ knowledge, there
    were no shifts to clinical practice over the data collection period (5 years,
    from 2015 to 2020). However, it is possible that there were subtle changes that
    would influence the results, in particular the length-of-stay. Future work In
    future, well-known applied ML techniques for medical risk prediction could be
    used to improve the initial results; for example, class balancing and model ensembling
    [44] and data augmentation [18]. There is also scope to explore alternative feature
    engineering, such as using additional derived features regarding previous admissions,
    other encoding methods for categorical variables, learning a lower dimensional
    space for categorical features using decision trees [5], and including additional
    categories for tests and medications that were ignored in this study. Another
    major area of interest is continual risk assessment throughout the admission,
    including in the post-operative period up until discharge. Only a few related
    studies considered risk assessment after surgery [17, 21, 42]. It is important
    because decisions are made throughout the admission and post-surgical care also
    has the potential to help avoid complications, readmission and mortality. In future
    work, the length-of-stay could be converted to an assessment of ‘prolonged’ relative
    to expectations for specific procedures for additional clinical and operational
    benefits (“LOS prediction” section). Also, there is a possibility of including
    the type of procedure as a variable when segmenting LOS to derive the predicted
    ranges. A key part of our method was to encapsulate clinical expertise by feature
    engineering patient-risk and procedure-risk, derived in a manual process. It would
    be interesting to learn those features in an additional pre-processing step with
    the potential benefits of time-saving, adaptability and accuracy. One possibility
    for patient-risk is to use predicted ASA (as done in [45]), provided that ASA
    targets are available in the training data. Likewise, the patient’s disease state
    could be learnt from other variables such as lab results, thus augmenting the
    use of ICD codes alone; which are subject to human error and can be incomplete,
    reducing training quality. This paper is focussed on elective surgery, as described
    in “Study design” section. Risk prediction is also beneficial for emergency cases;
    for planning post-surgical care and assisting with selection of intra-operative
    monitoring. Our method could be tested in this setting. Conclusions In this study,
    we developed novel algorithms (POP) that exploit tabular EHR data to predict surgical
    patient outcomes. The algorithms were effective for post-operative complications
    and LOS in this patient population, but ineffective for predicting readmission
    and mortality due to extremely rare cases. The results reinforce the importance
    of using metrics that are suitable for rare cases, which is uncommon in other
    surgical risk prediction studies. A larger study is justified to improve the algorithms
    in better predicting complications and length of hospital stay. A larger dataset
    may also improve the prediction of readmissions and mortality, which were extremely
    rare. Together with interpretable feature importance plots, surgical risk predictions
    provide clinically relevant information, that may help to mitigate risks and improve
    patient outcomes. Availability of data and materials The datasets generated and/or
    analysed during the current study are not publicly available as there was no data
    sharing as part of the ethics approval (and raw data is potentially re-identifiable)
    but are available from the corresponding author on reasonable request. Notes False
    Positive Rate is FP/(FP+TN). If FP is high, but TN is very large, the denominator
    remains high and the rate low. References Swarna SR, Boyapati S, Dutt V, Bajaj
    K. Deep learning in dynamic modeling of medical imaging: a review study. Proceedings
    of the 3rd International Conference on Intelligent Sustainable Systems, ICISS
    2020. 2020. p. 745–749. https://doi.org/10.1109/ICISS49785.2020.9315990. Suzuki
    K. Overview of deep learning in medical imaging. Radiol Phys Technol. 2017;10(3):257–73.
    https://doi.org/10.1007/S12194-017-0406-5. Article   PubMed   Google Scholar   Reilly
    JR, Gabbe BJ, Brown WA, Hodgson CL, Myles PS. Systematic review of perioperative
    mortality risk prediction models for adults undergoing inpatient non-cardiac surgery.
    ANZ J Surg. 2021;91(5):860–70. https://doi.org/10.1111/ans.16255. Article   PubMed   Google
    Scholar   Story DA. Postoperative complications in Australia and New Zealand (the
    REASON study). Perioper Med. 2013;2(1):2–4. https://doi.org/10.1186/2047-0525-2-16.
    Article   MathSciNet   Google Scholar   Bihorac A, Ozrazgat-Baslanti T, Ebadi
    A, Motaei A, Madkour M, Pardalos PM, et al. MySurgeryRisk: development and validation
    of a machine-learning risk algorithm for major complications and death after surgery.
    Ann Surg. 2019;269(4):652–62. https://doi.org/10.1097/SLA.0000000000002706. Article   PubMed   Google
    Scholar   Weiser TG, Regenbogen SE, Thompson KD, Haynes AB, Lipsitz SR, Berry
    WR, et al. An estimation of the global volume of surgery: a modelling strategy
    based on available data. Lancet. 2008;372(9633):139–44. https://doi.org/10.1016/S0140-6736(08)60878-8.
    Article   PubMed   Google Scholar   Duckett S, Jorm C, Moran G, Parsonage H. Safer
    care saves money. 2018. http://www.grattan.edu.au/. Accessed 2 Jan 2022. Wijeysundera
    DN. Predicting outcomes: Is there utility in risk scores? Can J Anaesth. 2016;63(2):148–58.
    https://doi.org/10.1007/S12630-015-0537-2. Article   PubMed   Google Scholar   Swart
    M, Carlisle JB, Goddard J. Using predicted 30 day mortality to plan postoperative
    colorectal surgery care: A cohort study. Br J Anaesth. 2017;118(1):100–4. https://doi.org/10.1093/bja/aew402.
    Article   CAS   PubMed   Google Scholar   Khuri SF. The NSQIP: A new frontier
    in surgery. Surgery. 2005;138(5):837–43. https://doi.org/10.1016/J.SURG.2005.08.016.
    Article   PubMed   Google Scholar   Protopapa KL, Simpson JC, Smith NCE, Moonesinghe
    SR. Development and validation of the Surgical Outcome Risk Tool (SORT). Br J
    Surg. 2014;101(13):1774–83. https://doi.org/10.1002/bjs.9638. Article   CAS   PubMed   Google
    Scholar   Wolters U, Wolf T, Stützer H, Schröder T. ASA classification and perioperative
    variables as predictors of postoperative outcome. Br J Anaesth. 1996;77(2):217–22.
    https://doi.org/10.1093/BJA/77.2.217. Article   CAS   PubMed   Google Scholar   Hill
    BL, Brown R, Gabel E, Rakocz N, Lee C, Cannesson M, et al. An automated machine
    learning-based model predicts postoperative mortality using readily-extractable
    preoperative electronic health record data. Br J Anaesth. 2019;123(6):877–86.
    https://doi.org/10.1016/j.bja.2019.07.030. Article   PubMed   PubMed Central   Google
    Scholar   Chiew CJ, Liu N, Wong TH, Sim YE, Abdullah HR. Utilizing Machine Learning
    Methods for Preoperative Prediction of Postsurgical Mortality and Intensive Care
    Unit Admission. Ann Surg. 2020;272(6):1133–9. https://doi.org/10.1097/SLA.0000000000003297.
    Article   PubMed   Google Scholar   Corey KM, Kashyap S, Lorenzi E, Lagoo-Deenadayalan
    SA, Heller K, Whalen K, et al. Development and validation of machine learning
    models to identify high-risk surgical patients using automatically curated electronic
    health record data (Pythia): A retrospective, single-site study. PLoS Med. 2018;15(11):e1002701.
    https://doi.org/10.1371/JOURNAL.PMED.1002701. Article   PubMed   PubMed Central   Google
    Scholar   Brennan M, Puri S, Ozrazgat-Baslanti T, Feng Z, Ruppert M, Hashemighouchani
    H, et al. Comparing clinical judgment with the MySurgeryRisk algorithm for preoperative
    risk assessment: A pilot usability study. Surgery. 2019;165(5):1035–45. https://doi.org/10.1016/J.SURG.2019.01.002.
    Article   PubMed   Google Scholar   Rajkomar A, Oren E, Chen K, Dai AM, Hajaj
    N, Hardt M, et al. Scalable and accurate deep learning with electronic health
    records. NPJ Digit Med. 2018;1(1):1–10. https://doi.org/10.1038/s41746-018-0029-1.
    Article   Google Scholar   Wang L, Tong L, Davis D, Arnold T, Esposito T. The
    application of unsupervised deep learning in predictive models using electronic
    health records. BMC Med Res Methodol. 2020;20(1):1–9. https://doi.org/10.1186/s12874-020-00923-1.
    Article   Google Scholar   Campbell D, Boyle L, Soakell-Ho M, Hider P, Wilson
    L, Koea J, et al. National risk prediction model for perioperative mortality in
    non-cardiac surgery. Br J Surg. 2019;106(11):1549–57. https://doi.org/10.1002/BJS.11232.
    Article   CAS   PubMed   Google Scholar   Le Manach Y, Collins G, Rodseth R, Le
    Bihan-Benjamin C, Biccard B, Riou B, et al. Preoperative Score to Predict Postoperative
    Mortality (POSPOM)Derivation and Validation. Anesthesiology. 2016;124(3):570–9.
    https://doi.org/10.1097/ALN.0000000000000972. Article   PubMed   Google Scholar   Flaks-Manov
    N, Topaz M, Hoshen M, Balicer RD, Shadmi E. Identifying patients at highest-risk:
    the best timing to apply a readmission predictive model. BMC Med Inform Decis
    Mak. 2019;19(1):1–9. https://doi.org/10.1186/s12911-019-0836-6. Article   Google
    Scholar   Flaks-Manov N, Srulovici E, Yahalom R, Perry-Mezre H, Balicer R, Shadmi
    E. Preventing hospital readmissions: healthcare providers’ perspectives on “impactibility’’
    beyond EHR 30-day readmission risk prediction. J Gen Intern Med. 2020;35(5):1484–9.
    https://doi.org/10.1007/s11606-020-05739-9. Article   PubMed   PubMed Central   Google
    Scholar   Mišić VV, Gabel E, Hofer I, Rajaram K, Mahajan A. Machine Learning Prediction
    of Postoperative Emergency Department Hospital Readmission. Anesthesiology. 2020;132(5):968–80.
    https://doi.org/10.1097/ALN.0000000000003140. Article   PubMed   Google Scholar   Flaks-Manov
    N, Shadmi E, Yahalom R, Perry-Mezre H, Balicer RD, Srulovici E. Identification
    of elderly patients at risk for 30-day readmission: clinical insight beyond big
    data prediction. J Nurs Manag. 2021:1–11. https://doi.org/10.1111/jonm.13495.
    Xue B, Li D, Lu C, King CR, Wildes T, Avidan MS, et al. Use of Machine Learning
    to Develop and Evaluate Models Using Preoperative and Intraoperative Data to Identify
    Risks of Postoperative Complications. JAMA Netw Open. 2021;4(3):e212240. https://doi.org/10.1001/jamanetworkopen.2021.2240.
    Hospitals at a glance 2017–18, Surgery in Australia’s hospitals. 2019. https://www.aihw.gov.au/reports/hospitals/hospitals-at-a-glance-2017-18/contents/surgery-in-australia-s-hospitals.
    Accessed 29 Jan 2024. Collins GS, Reitsma JB, Altman DG, Moons KGM. Transparent
    reporting of a multivariable prediction model for individual prognosis or diagnosis
    (TRIPOD): The TRIPOD Statement. BMC Med. 2015;13(1):1–10. https://doi.org/10.1186/S12916-014-0241-Z/TABLES/1.
    Article   PubMed   PubMed Central   Google Scholar   Pedregosa F, Varoquaux G,
    Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: Machine Learning
    in Python. J Mach Learn Res. 2011;12:2825–30. MathSciNet   Google Scholar   Chen
    T, Guestrin C. XGBoost: a scalable tree boosting system. In: Proceedings of the
    22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
    New York: ACM; 2016. https://doi.org/10.1145/2939672. http://dx.doi.org/10.1145/2939672.2939785.
    Multicollinearity Alin A. Wiley Interdiscip Rev. Comput Stat. 2010;2(3):370–4.
    https://doi.org/10.1002/wics.84. Article   Google Scholar   Midi H, Sarkar SK,
    Rana S. Collinearity diagnostics of binary logistic regression model. 2013;13(3):253–67.
    https://doi.org/10.1080/09720502.2010.10700699. Article   Google Scholar   Kuhn
    M, Johnson K. Feature engineering and selection: a practical approach for predictive
    models. 2019. http://www.feat.engineering/index.html. Accessed 13 Mar 2022. Saito
    T, Rehmsmeier M. The Precision-Recall Plot Is More Informative than the ROC Plot
    When Evaluating Binary Classifiers on Imbalanced Datasets. PLoS ONE. 2015;10(3):e0118432.
    https://doi.org/10.1371/JOURNAL.PONE.0118432. Article   PubMed   PubMed Central   Google
    Scholar   Štrumbelj E, Kononenko I. Explaining prediction models and individual
    predictions with feature contributions. Knowl Inf Syst. 2014;41(3):647–65. https://doi.org/10.1007/S10115-013-0679-X.
    Article   Google Scholar   Qu WF, Zhou PY, Liu WR, Tian MX, Jin L, Jiang XF, et
    al. Age-adjusted Charlson Comorbidity Index predicts survival in intrahepatic
    cholangiocarcinoma patients after curative resection. Ann Transl Med. 2020;8:487.
    https://doi.org/10.21037/ATM.2020.03.23. Walsh C, Hripcsak G. The Effects of Data
    Sources, Cohort Selection, and Outcome Definition on a Predictive Model of Risk
    of Thirty-Day Hospital Readmissions. J Biomed Inform. 2014;52:418. https://doi.org/10.1016/J.JBI.2014.08.006.
    Article   PubMed   PubMed Central   Google Scholar   Meesters MI, von Heymann
    C. Optimizing Perioperative Blood and Coagulation Management During Cardiac Surgery.
    Anesthesiol Clin. 2019;37:713–28. https://doi.org/10.1016/j.anclin.2019.08.006.
    Article   PubMed   Google Scholar   Lang Z, Wu Y, Bao M. Coagulation Status and
    Surgical Approach as Predictors of Postoperative Anemia in Patients Undergoing
    Thoracic Surgery: A Retrospective Study. Front Surg. 2021;8:416. https://doi.org/10.3389/FSURG.2021.744810/XML/NLM.
    Article   ADS   Google Scholar   Musallam KM, Tamim HM, Richards T, Spahn DR,
    Rosendaal FR, Habbal A, et al. Preoperative anaemia and postoperative outcomes
    in non-cardiac surgery: a retrospective cohort study. Lancet. 2011;378:1396–407.
    https://doi.org/10.1016/S0140-6736(11)61381-0. Article   PubMed   Google Scholar   Gombotz
    H. Patient blood management is key before elective surgery. Lancet. 2011;378:1362–3.
    https://doi.org/10.1016/S0140-6736(11)61552-3. Article   PubMed   Google Scholar   Morawski
    K, Dvorkis Y, Monsen CB. Predicting hospitalizations from electronic health record
    data. Am J Manage Care. 2020;26(1). https://doi.org/10.37765/ajmc.2020.42147.
    Lee CK, Samad M, Hofer I, Cannesson M, Baldi P. Development and validation of
    an interpretable neural network for prediction of postoperative in-hospital mortality.
    NPJ Dig Med. 2021;4(1):1–9. https://doi.org/10.1038/s41746-020-00377-1. Article   Google
    Scholar   Suresh H, Hunt N, Johnson A, Celi LA, Szolovits P, Ghassemi M. Clinical
    Intervention Prediction and Understanding with Deep Neural Networks. In: Doshi-Velez
    F, Fackler J, Kale D, Ranganath R, Wallace B, Wiens J, editors. Proceedings of
    the 2nd Machine Learning for Healthcare Conference. Proceedings of Machine Learning
    Research, vol. 68. PMLR; 2017. p. 322–337. https://proceedings.mlr.press/v68/suresh17a.html.
    Akselrod-Ballin A, Chorev M, Shoshan Y, Spiro A, Hazan A, Melamed R, et al. Predicting
    breast cancer by applying deep learning to linked health records and mammograms.
    Radiology. 2019;292:331–42. https://doi.org/10.1148/radiol.2019182622. Article   PubMed   Google
    Scholar   Mannion AF, Bianchi G, Mariaux F, Fekete TF, Reitmeir R, Moser B, et
    al. Can the Charlson Comorbidity Index be used to predict the ASA grade in patients
    undergoing spine surgery? Eur Spine J. 2020;29(12):2941–2952. https://doi.org/10.1007/s00586-020-06595-1.
    Download references Acknowledgements Thanks to David Gyorki and Dave Rawlinson
    for valuable discussions and advice at every stage. Funding The research was supported
    by a grant from the Victorian Medical Research Acceleration Fund, Round 4 (March
    2020). The funding bodies played no role in the design of the study and collection,
    analysis, and interpretation of data and in writing the manuscript. Author information
    Authors and Affiliations Atidia Health, Melbourne, Australia Gideon Kowadlo, Yoel
    Mittelberg, Milad Ghomlaghi & Daniel K. Stiglitz Department of Anaesthesiology
    and Perioperative Medicine, Alfred Health, Melbourne, Australia Daniel K. Stiglitz
    Data Analytics Research and Evaluation Centre, Austin Health, Melbourne, Australia
    Kartik Kishore Department of Anaesthesia, Austin Health, Heidelberg, Australia
    Ranjan Guha, Justin Nazareth & Laurence Weinberg Department of Critical Care,
    The University of Melbourne, Austin Health, Heidelberg, Australia Laurence Weinberg
    Contributions GK designed the study and drafted the manuscript. YM helped to design
    the methodology and led the implementation and analysis. MG contributed to implementation
    and analysis. KK extracted the data and assisted with data interpretation and
    analysis. DS helped to design the study and analyse the results from a clinical
    perspective. DS, RG, JN and LW provided clinical expertise for development of
    the methodology. LW helped to write the manuscript. All authors reviewed the manuscript.
    Corresponding author Correspondence to Gideon Kowadlo. Ethics declarations Ethics
    approval and consent to participate Ethics approval was received from Austin Health
    Human Research Ethics Committee. The need for written informed consent was waived
    by Austin Health Human Research Ethics Committee because de-identified retrospective
    data were used. All methods were carried out in accordance with relevant guidelines
    and regulations in the Declaration of Helsinki. Consent for publication Not applicable.
    Competing interests The authors declare no competing interests. Additional information
    Publisher’s Note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Supplementary Information
    Supplementary material 1. Rights and permissions Open Access This article is licensed
    under a Creative Commons Attribution 4.0 International License, which permits
    use, sharing, adaptation, distribution and reproduction in any medium or format,
    as long as you give appropriate credit to the original author(s) and the source,
    provide a link to the Creative Commons licence, and indicate if changes were made.
    The images or other third party material in this article are included in the article''s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article''s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/)
    applies to the data made available in this article, unless otherwise stated in
    a credit line to the data. Reprints and permissions About this article Cite this
    article Kowadlo, G., Mittelberg, Y., Ghomlaghi, M. et al. Development and validation
    of ‘Patient Optimizer’ (POP) algorithms for predicting surgical risk with machine
    learning. BMC Med Inform Decis Mak 24, 70 (2024). https://doi.org/10.1186/s12911-024-02463-w
    Download citation Received 21 February 2023 Accepted 20 February 2024 Published
    11 March 2024 DOI https://doi.org/10.1186/s12911-024-02463-w Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Post-operative complications Pre-operative care Risk prediction Risk
    assessment Machine learning Download PDF Sections Figures References Abstract
    Introduction Method Results Discussion Conclusions Availability of data and materials
    Notes References Acknowledgements Funding Author information Ethics declarations
    Additional information Supplementary Information Rights and permissions About
    this article Advertisement BMC Medical Informatics and Decision Making ISSN: 1472-6947
    Contact us General enquiries: journalsubmissions@springernature.com Read more
    on our blogs Receive BMC newsletters Manage article alerts Language editing for
    authors Scientific editing for authors Policies Accessibility Press center Support
    and Contact Leave feedback Careers Follow BMC By using this website, you agree
    to our Terms and Conditions, Your US state privacy rights, Privacy statement and
    Cookies policy. Your privacy choices/Manage cookies we use in the preference centre.
    © 2024 BioMed Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: BMC Medical Informatics and Decision Making
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Development and validation of ‘Patient Optimizer’ (POP) algorithms for predicting
    surgical risk with machine learning
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Oprea S.V.
  - Bâra A.
  citation_count: '0'
  description: Given the current pace of technological advancement and its pervasive
    impact on society, understanding public sentiment is essential. The usage of AI
    in social media, facial recognition, and driverless cars has been scrutinized
    using the data collected by a complex survey. To extract insights from data, a
    descriptive-prescriptive hybrid data processing method is proposed. It includes
    graphical visualization, cross-tabulation to identify patterns and correlations,
    clustering using K-means, principal component analysis (PCA) enabling 3D cluster
    representation, analysis of variance (ANOVA) of clusters, and forecasting potential
    leveraged by Random Forest to predict clusters. Three well-separated clusters
    with a silhouette score of 0.828 provide the profile of the respondents. The affiliation
    of a respondent to a particular cluster is assessed by an F1 score of 0.99 for
    the test set and 0.98 for the out-of-sample set. With over 5000 respondents answering
    over 120 questions, the dataset reveals interesting opinions and concerns regarding
    AI technologies that have to be handled to facilitate AI acceptance and adoption.
    Its findings have the potential to shape meaningful dialog and policy, ensuring
    that the evolution of technology aligns with the values and needs of the people.
  doi: 10.1057/s41599-024-02926-5
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature humanities and social sciences communications articles article
    Article Open access Published: 11 March 2024 Exploring excitement counterbalanced
    by concerns towards AI technology using a descriptive-prescriptive data processing
    method Simona-Vasilica Oprea & Adela Bâra  Humanities and Social Sciences Communications  11,
    Article number: 388 (2024) Cite this article 344 Accesses 1 Altmetric Metrics
    Abstract Given the current pace of technological advancement and its pervasive
    impact on society, understanding public sentiment is essential. The usage of AI
    in social media, facial recognition, and driverless cars has been scrutinized
    using the data collected by a complex survey. To extract insights from data, a
    descriptive-prescriptive hybrid data processing method is proposed. It includes
    graphical visualization, cross-tabulation to identify patterns and correlations,
    clustering using K-means, principal component analysis (PCA) enabling 3D cluster
    representation, analysis of variance (ANOVA) of clusters, and forecasting potential
    leveraged by Random Forest to predict clusters. Three well-separated clusters
    with a silhouette score of 0.828 provide the profile of the respondents. The affiliation
    of a respondent to a particular cluster is assessed by an F1 score of 0.99 for
    the test set and 0.98 for the out-of-sample set. With over 5000 respondents answering
    over 120 questions, the dataset reveals interesting opinions and concerns regarding
    AI technologies that have to be handled to facilitate AI acceptance and adoption.
    Its findings have the potential to shape meaningful dialog and policy, ensuring
    that the evolution of technology aligns with the values and needs of the people.
    Similar content being viewed by others Attitudes towards AI: measurement and associations
    with personality Article Open access 05 February 2024 A megastudy on the predictability
    of personal information from facial images: Disentangling demographic and non-demographic
    signals Article Open access 29 November 2023 Lessons for artificial intelligence
    from the study of natural stupidity Article 09 April 2019 Introduction The current
    society increasingly depends on AI technologies. Most people possess at least
    one social media account, exposing them to diverse information sources. When traveling,
    we depend on security measures like facial recognition. Furthermore, automated
    transportation, such as driverless cars, has been in use for some time; for example,
    Narita Airport Footnote 1 has operated such commuting trains for over 20 years.
    Other assistants in our homes are tasked with performing mundane activities, gathering
    data, and consequently enhancing our quality of life. Developments in AI technologies
    have the potential to reshape and enhance society in the upcoming decades. Recently,
    the promise of these technologies in enhancing daily life and human capabilities
    has been acknowledged (Choung et al., 2023; Kopalle et al., 2022; Wang et al.,
    2023a; Yu and Helwig, 2022). Nonetheless, public perspectives are heavily influenced
    by considerations such as the intended use of these technologies, the regulation
    in place, and the beneficiaries or potential losses when AI advancements become
    implemented on a large scale. As expected, a sense of caution is prevalent in
    public attitudes toward AI and human enhancement applications, often revolving
    around concerns related to personal autonomy, lack of human connections, loss
    of jobs, unintended consequences, and the substantial societal changes that these
    innovations might entail (Querci et al., 2022; Lambert and Stevens, 2023). People
    might be apprehensive that economic inequalities may worsen as certain advancements
    emerge, and they also express concerns about the potential for increased surveillance
    through technologies like facial recognition software and driverless cars. According
    to a recent Pew Research Center Footnote 2 (PRC) survey applied in February 2021
    to over 10,000 Americans, it revealed that they perceived potential in how AI
    technologies could enhance life and society capabilities. The survey delves into
    a wide spectrum of scientific and technological advancements, testing the perceptions
    of respondents. It was mainly focused on public opinions regarding three AI developments
    that are frequently debated in social media: (1) The utilization of facial recognition
    technology (Smith and Miller, 2022), (Andrejevic and Selwyn, 2020); (2) The use
    of algorithms by social media companies to detect false information (Tseng et
    al., 2023); (3) The advancement of driverless passenger vehicles (Karmakar et
    al., 2021). The dataset is a subset of a larger survey that approached both technological
    and science-related issues. The investigated survey aims to gather opinions on
    the impact of technology on society, specifically looking at perceptions of fairness
    related to algorithms (Giovanola and Tiribelli, 2023) and the interplay between
    technology and issues of discrimination (Kim, 2022). It also seeks to understand
    how these views correlate with demographic factors such as income, political affiliation,
    personal ideologies, etc. The survey addresses several themes, including the overall
    impact of technology on daily life and societal well-being, concerns and excitement
    about future technological developments and their implications, views on the possibility
    of algorithms functioning in a fair and unbiased manner, etc. The distribution
    of responses suggests that the survey targeted a diverse adult population, likely
    with a broad age range, covering various income levels, political affiliations,
    and ideological beliefs. The findings extracted from answers could be used to
    inform policymakers and business leaders about public concerns and expectations
    regarding AI and its governance, guide the development of more equitable and fair
    technology policies and practice, enhance understanding of the relationship between
    socioeconomic status and perspectives on AI (van Noordt and Misuraca, 2022). The
    survey has been structured to provide quantitative data through a series of closed-ended
    questions. This approach allows for a clear analysis of trends and patterns within
    the responses, potentially supporting a wide range of statistical and comparative
    analyses. Given the analysis of the dataset and the prevalence of topics related
    to technology and algorithms, the survey may have sought to capture public sentiment
    on AI and its acceptance. The survey data we have analyzed offers a window into
    this complex landscape, capturing the multifaceted attitudes toward AI and technology
    as a whole. Respondents were asked to qualify the impact of AI technologies (question
    code: TECH1_W99) as mostly positive, negative, or equally mixed. The distribution
    of responses indicates a general optimism towards technology, with many participants
    recognizing its benefits. However, a significant portion of the populace retains
    a level of skepticism or concern, highlighting the need for ongoing dialog about
    AI’s role in society. The nuanced balance of concern and excitement regarding
    technological advances (CNCEXC_W99) suggests that while the public is hopeful
    about the potential of AI, there is an underlying caution about its implications,
    ethical, privacy-related, and socioeconomic. This duality is a clear sign that
    acceptance of AI is not unconditional and is tempered by realistic appraisals
    of its potential downsides. Questions regarding the fairness of algorithms (ALGFAIR_W99)
    delve into the heart of AI ethics. The responses reflect a divided opinion, with
    a notable portion of participants unsure about whether algorithms can be truly
    fair. This uncertainty underscores a pivotal aspect of AI acceptance: trust in
    the systems’ design and the transparency of their decision-making processes. The
    dataset incorporates demographic variables like political affiliation (F_PARTYSUM_FINAL)
    and income (F_INC_SDT1), which could be correlated with AI acceptance levels.
    For instance, one might hypothesize that individuals with higher incomes, who
    may benefit more from AI services, could show greater acceptance, or that political
    leanings might influence trust in technology providers and regulatory approaches.
    The columns that start with POSNEGAI relate to questions where respondents were
    asked about their levels of concern or excitement. These questions are designed
    to measure the respondents’ attitudes towards certain issues, gauging their emotional
    responses on a spectrum from concern to excitement. The SMALG columns in the codification
    file correspond to questions about social media, algorithms, and their impact
    or perceptions among the respondents. The FACEREC prefix in these column names
    indicates that they are related to questions about facial recognition technology.
    These questions cover a range of issues related to facial recognition technology,
    including public sentiment, expected outcomes, societal impacts, and regulatory
    opinions. The available responses suggest that respondents can express a variety
    of opinions, from concerns about privacy and fairness to support for the potential
    benefits of the technology in areas such as law enforcement and crime prevention.
    The DCARS prefix in the column names indicates that these questions are related
    to the topic of driverless cars (autonomous vehicles) and the issues surrounding
    them. These questions cover a broad spectrum of issues related to autonomous vehicles
    (general feeling, societal and economic impact, regulation, and standards), from
    their potential societal and economic impacts to ethical considerations and personal
    comfort levels with the technology. This nuanced perspective on AI acceptance
    is important for stakeholders in the AI ecosystem. As AI continues to evolve,
    monitoring and understanding public sentiment is significant in navigating its
    integration into the fabric of society. In the current research, we propose a
    combined descriptive and prescriptive model that integrates cross-tabulation,
    K-means clustering, 3D graphical visualization using PCA, analysis of variance
    (ANOVA) of clusters, and the predictive capabilities of Random Forest for cluster
    prediction. The proposed model combines supervised and unsupervised Machine Learning
    (ML) algorithms and contributes to the state of the art in the field of survey
    data processing, gaining insights that are essential for promoting the acceptance
    and adoption of AI. Our primary objective is to conduct a comprehensive analysis
    of survey data on the excitement and concerns surrounding AI technology in areas
    like social media, facial recognition, and driverless cars, using a descriptive-prescriptive
    method. Our research motivation consists of understanding relevant groups of individuals,
    identifying their concerns, and predicting new instances of individuals into a
    cluster using prediction algorithms. This approach will enable us to delve deeply
    into the public perception and attitudes towards these technologies. Thus, the
    contribution of this paper lies in extracting meaningful insights from this data,
    which may lead to advancements in both humanity and social sciences. Our research
    questions are “How do different groups of individuals perceive AI technologies
    such as social media, facial recognition, and driverless cars, and what are their
    primary concerns, and how can these perceptions and concerns be predicted for
    new instances of individuals?” This paper is structured as follows: in the “Literature
    review” section, a brief literature survey is presented focusing on similar research,
    the “Methodology” section is dedicated to the proposed methodology, and in the
    section “Results”, the results are presented. Section “Discussion” is designed
    for discussions, whereas section “Conclusion” is designed for conclusions. Literature
    review The acceptance and adoption of AI have been investigated in many fields,
    such as: agriculture (Mohr and Kühl, 2021), manufacturing, health, education,
    transportation, social media (Chung et al., 2021), security, etc. Moreover, the
    ML and AI-induced technology-related stress in organizations was analyzed in (Kumar
    et al., 2023). The study on the practice of AI in manufacturing firms in Malaysia,
    focusing on its role in driving digital transformation, revealed significant insights
    (Ahmad et al., 2022). Integration of digital technologies into business processes
    is essential for digital transformation, a trend underscored by the emergence
    of “Industry 4.0”. In this context, AI is a key component in enhancing performance
    and stimulating demand and productivity in the manufacturing sector. The research
    also highlighted several barriers and challenges faced by manufacturing firms
    in Malaysia, primarily due to the nascent stage of AI development. These include
    a lack of skilled talent, insufficient incentives, and a dearth of innovation.
    To gather data, managers from various manufacturing companies were selected as
    respondents. From the 93 answers, descriptive analysis was employed to pinpoint
    the specific barriers and challenges. The findings indicate that the most significant
    barrier to AI implementation in these firms is the lack of talent. Furthermore,
    the primary challenge identified is the absence of in-house experts. This study
    underscores the need for focused strategies to overcome these issues, such as
    investing in education and training, fostering a culture of innovation, and creating
    incentives to attract and retain AI experts. The researchers focused on exploring
    the mediating role of perceptions on the relationship between technology readiness
    and the adoption of AI in the field of accounting (Damerji and Salimi, 2021).
    Specifically, it aimed to determine how Perceived Ease Of Use (PEOU) and Perceived
    Usefulness (PU) influence the correlation between the technology readiness of
    accounting students and their inclination to adopt AI technologies. This quantitative
    research involved assessing individual accounting students’ perceptions of their
    readiness for technology and their attitudes toward adopting it. To collect data,
    a questionnaire with 31 items was used. This questionnaire also gathered demographic
    information. The findings of the study revealed that technology readiness significantly
    impacts technology adoption. However, more nuanced insights were obtained through
    mediation analysis using hierarchical regression. This analysis showed that the
    link between technology readiness and the adoption of AI is significantly influenced
    by PEOU and PU. Patients’ views on the implementation of AI in radiology were
    investigated by a questionnaire (Ongena et al., 2020). The study aimed to gauge
    patients’ perspectives on the implementation of AI in radiology. A questionnaire
    was developed based on insights from previous qualitative research. The questionnaire
    was administered to 150 patients scheduled for various radiographies. The primary
    analytical tool used in this research was exploratory factor analysis (EFA) with
    principal axis factoring and oblique promax rotation. This approach helped to
    uncover latent variables underlying patients’ responses. To ensure the reliability
    of these factors, internal consistency was measured using Cronbach’s alpha and
    composite reliability. The EFA identified 5 distinct factors concerning patients’
    views on AI in radiology: distrust and accountability, procedural knowledge, personal
    interaction, efficiency, and being informed. The internal consistency of the responses
    was good for the first three factors and acceptable for the last two. Moreover,
    in (Xuan et al., 2023), the readiness towards AI among undergraduate medical students
    in Malaysia was analyzed using frequency tables, percentages, standard deviation,
    unpaired t-tests, and ANOVA. Significant correlations were found between the age
    and academic year of participants and their readiness in the ability, vision,
    and ethics domains of medical AI. Additionally, a notable association was identified
    between prior training and all four domains of medical AI readiness. Additionally,
    other studies investigated AI impact and adoption factors in health systems (Kim
    et al., 2022; Kosan et al., 2022; Patrzyk et al., 2022; Vorisek et al., 2023).
    Researchers explored the use of natural language processing (NLP) to analyze patient
    experiences (van Buchem et al., 2022). The data was primarily gathered through
    5 open-ended questions, allowing for a more nuanced understanding of patient perspectives.
    An NLP pipeline, incorporating sentiment analysis and topic modeling, was employed
    to process the responses, and a visualization tool was developed to help physicians
    navigate the results. The research involved an iterative process of developing
    and validating both the questionnaire and the NLP pipeline within a clinical setting.
    The questionnaire focused on various aspects of patient care, including the information
    provided, personal approach, collaboration among healthcare professionals, organization
    of care, and other experiences. A total of 534 patients participated by responding
    to this questionnaire. The performance of the sentiment analysis model was notable,
    achieving an F1 score of 0.97 for identifying positive texts and 0.63 for negative
    texts. The use of NLP proved efficient in reducing the time required by healthcare
    professionals to assess and prioritize patient experiences. This approach offers
    a significant advantage over traditional methods that rely on closed-ended questions,
    as it captures a broader and more detailed range of patient feedback. The researchers
    explored the implementation of AI in talent acquisition by human resource managers
    (Pillai and Sivathanu, 2020). A survey involving 562 managers was conducted. The
    collected data was analyzed through Partial Least Squares Structural Equation
    Modeling (PLS-SEM). The findings indicate that factors such as cost-effectiveness,
    relative advantage, support from top management, HR readiness, competitive pressure,
    and backing from AI vendors have a positive impact on the adoption of AI for talent
    acquisition. Conversely, concerns regarding security and privacy serve as deterrents.
    The research also identified that the characteristics of tasks and technology
    affect the suitability of AI for talent acquisition tasks. Furthermore, the adoption
    and appropriateness of AI technology are linked to its actual usage in talent
    acquisition. Interestingly, the study uncovers that a preference for traditional
    methods of talent acquisition can weaken the relationship between AI adoption
    and its actual usage in this field. Almarashda et al. (2021) focused on identifying
    factors influencing the implementation of AI in the United Arab Emirates’ energy
    sector. A comprehensive analysis was conducted using data from 350 respondents.
    The study found that the primary drivers for adopting AI in organizations are
    its potential to foster effective business innovation, align with business strategies,
    and enhance production levels. Key aspects of AI, such as user-friendliness, the
    ability to improve work quality, and compatibility with existing tasks, were identified
    as critical factors for its adoption. Additionally, the research highlighted the
    importance of human resource management factors, including the need for a knowledgeable
    referral person when encountering difficulties with AI and expert support in utilizing
    AI technology. The factors influencing the adoption of AI in banking services
    were examined (Rahman et al., 2022). A survey was conducted, yielding 302 opinions
    from Malaysian banking customers. This research aimed to identify the key determinants
    of customers’ intentions to adopt AI in banking services. From the qualitative
    analysis, it was found that AI is essential for detecting fraud and preventing
    risks. However, concerns about data privacy and security, lack of regulatory frameworks,
    and deficiencies in relevant skills and IT infrastructure pose significant challenges
    to AI adoption. The quantitative analysis revealed that several factors significantly
    impact customers’ intentions to adopt AI. These include their attitudes towards
    AI, PU, perceived risk, perceived trust, and subjective norms. Interestingly,
    factors like PEOU and awareness were not found to influence this intention. Moreover,
    the study highlighted the role of attitude towards AI as a mediator in the relationship
    between PU and the intention to adopt AI. Another research presented an innovative
    approach to exploring the dynamics between digital technology and the adoption
    of AI in the context of electronic manufacturing enterprises (Binsaeed et al.,
    2023). Specifically, the study aimed to (a) investigate the direct relationship
    between digital technology and AI adoption; (b) examine the mediating role of
    Knowledge Sharing (KS) in this relationship; (c) assess how privacy and security
    considerations might moderate the link between digital technology and AI adoption.
    A questionnaire survey was distributed among electronic firms in Saudi Arabia.
    The analysis is based on data from 298 respondents. The research methods include
    multi-level correlation and regression analysis to test the hypotheses. The findings
    from this analysis are noteworthy. There is a positive direct influence of digital
    technology on AI adoption within electronics manufacturing enterprises. KS acts
    as a significant mediator in the relationship between digital technology and AI
    adoption. Privacy and security considerations play a moderating role in this relationship.
    Moreover, (Qiu et al., 2022) delved into the influence of AI-enabled services
    in the hospitality industry, particularly focusing on how AI technology supports
    Frontline Employees (FE) physically, mentally, and emotionally, thereby enhancing
    their ability to provide hospitable service. A total of 342 valid questionnaires
    were collected to examine various aspects related to AI-enabled services. The
    study employed factor analyses and measurement model evaluation to understand
    the construct of AI-enabled service better. Four key factors were identified as
    central to this construct: anthropomorphic attributes, entertainment attributes,
    functional attributes, and information attributes. While entertainment does not
    reduce the physical and mental fatigue experienced by FE, it has a notable positive
    impact on their emotional well-being. The adoption of AI and related technologies
    by public and academic librarians in North America was investigated (Yoon et al.,
    2022). Quantitative analysis of the survey responses was conducted using various
    chi-square tests and crosstab analyses. It was observed that academic librarians
    reported higher usage and awareness of AI and related technologies. Conversely,
    public librarians exhibited more positive attitudes towards AI technologies. Overall,
    67% of the participants believed that AI would significantly change library functions,
    and 68% of the librarians expressed interest in receiving training in AI. These
    findings suggest a growing recognition of the transformative potential of AI in
    the library sector, coupled with a strong interest in training and development
    to leverage these technologies effectively. Another research contributed to the
    field of AI education by introducing a novel AI curriculum framework (Chiu et
    al., 2022). The effectiveness of this curriculum in enhancing AI learning was
    established through a multifactorial pre-post-test evaluation, focusing on students’
    perceptions of AI learning. The curriculum’s development involved a collaborative
    co-creation process, with a team of 14 professors working alongside 17 principals
    and teachers from 6 secondary schools. The study participants included 335 students
    and 8 teachers from these schools. The research employed a mixed-methods approach,
    combining quantitative data from pre- and post-questionnaires with qualitative
    insights highlighting teachers’ views on the co-creation process. Data analysis
    was conducted using paired t-tests and ANCOVAs for quantitative data and thematic
    analysis for qualitative data. Post-intervention, students reported an increased
    sense of competence in AI and a more positive attitude towards learning AI. Additionally,
    other studies investigated AI in education (Rauf et al., 2021; Wang et al., 2023b;
    Nouraldeen, 2023) in Lebanese universities. Another study investigated an integrated
    model designed to predict elementary school students’ acceptance of AI robots
    serving as teachers (Chen et al., 2023). This research area has garnered significant
    interest due to AI teachers’ potential to address the global teacher shortage.
    Participants were students from Chinese elementary schools. The study utilized
    descriptive statistics and SEM. The research identified several key factors influencing
    the acceptance of AI teachers, including Robot Use Anxiety (RUA), PU, PEOU, and
    Robot Instructional Task Difficulty (RITD). The findings indicated that students
    had positive attitudes towards AI teachers, and these attitudes could be predicted
    based on their perceptions of the AI teachers’ PU, PEOU, and the difficulty of
    the tasks they were programmed to perform. Furthermore, the study discovered that
    the relationship between RITD and the acceptance of AI teachers was mediated by
    factors such as RUA, PEOU, and PU. This indicated that the difficulty of tasks
    assigned to AI teachers, the anxiety students feel about using robots, and their
    PEOU of AI teachers all play important roles in determining how well these AI
    teachers are accepted by students. The study (Amichai-Hamburger et al., 2020)
    focused on the adoption of personal autonomous cars, particularly how tailoring
    the vehicle’s operations to the user’s personality could improve user experience.
    A questionnaire was created, targeting various features of a proposed information
    system for autonomous cars. Analysis of the responses identified two key factors
    influencing user preferences: the willingness to share personal information and
    the desire to maintain control over the vehicle. Additionally, a regression analysis
    examining the relationship between preferences for autonomous car features and
    factors such as personality traits, gender, and age revealed that traits like
    openness, conscientiousness, and age significantly influenced these preferences.
    Another study (Rowthorn, 2019) investigated the ethical decision-making processes
    in autonomous vehicles, examining aspects of machine ethics, AI in driving, and
    algorithms used during crash scenarios. Utilizing data from multiple sources,
    including the Pew Research Center, the researcher conducted a series of analyses
    and estimations. These included assessing the percentage of U.S. adults willing
    or not willing to travel in a driverless car, evaluating how safe these adults
    feel as pedestrians in cities where Autonomous Vehicles (AVs) operate, determining
    which countries are best equipped for the advent of autonomous vehicles, and identifying
    the key data infrastructure needs in smart cities that are essential for the testing
    and implementation of AVs. More studies focused on identifying the behavioral
    factors influencing the adoption of AVs (Acheampong and Cugurullo, 2019). The
    primary objective was to develop conceptual frameworks and measurement models
    to forecast trends in public transportation, sharing, and ownership of self-driving
    cars. These proposed frameworks incorporated socio-demographic variables along
    with key latent behavioral factors. They included PEOU of AVs, public concerns
    and apprehensions about AVs, subjective norms, perceived behavioral control, and
    attitudes related to the environment, technology, shared consumption, public transport,
    and car ownership. Utilizing survey data with CFA, the research aimed to establish
    and validate the reliability of scale indicators. This process involved confirming
    both the convergent and discriminant validity of the relationships among the latent
    variables. As a result of this comprehensive analysis, four distinct measurement
    models were developed and presented. Passengers’ perceptions and satisfaction
    levels with the digital technology implemented by airlines during the COVID-19
    pandemic, focusing specifically on the Chinese market, were explored (Shiwakoti
    et al., 2022). An online survey was conducted to assess passengers’ views on 11
    different digital technology-based services provided by airlines, including facial
    recognition. A total of 365 valid responses were collected and analyzed using
    ANOVA tests and stepwise multiple linear regression analysis. The findings revealed
    a generally positive attitude among passengers towards the adoption of new technologies
    by airlines. From the regression analysis, 6 technologies were identified as having
    a statistically significant impact on passenger satisfaction. These include AI
    customer service, electronic luggage tags, cleaning robots, ultraviolet light
    and antimicrobial cabin cleaning, an app-controlled in-flight entertainment system,
    and an electronic library. Conversely, three technologies were found to be less
    favorable among passengers: facial recognition, digital documentation, and AI
    customer service. Other recent research focused on public sentiment extracted
    from texts from social platforms such as Twitter (Liu, Zhou, et al., 2023b) and
    emotions classification involving ML algorithms and complex data processing pipelines
    (Liu et al., 2023a). Autonomous cars, driving, and traffic issues were debated
    in (Xiao et al., 2023), addressing the safety challenges posed by occlusions in
    autonomous driving by proposing a multi-tier perception task offloading framework
    that leverages collaborative computing between autonomous vehicles and roadside
    units. Federated learning in autonomous cars (Fu et al., 2023), technology in
    of moving trajectory of autonomous cars (Ding et al., 2023), and human activity
    recognition (Jannat et al., 2023) were further examined by exploring the use of
    Wi-Fi Channel State Information (CSI) signals for recognizing human activities,
    leveraging the insight that human movement alters Wi-Fi signal propagation, evident
    in CSI signal variations. Other intriguing aspects such as management in traffic
    with autonomous vehicles (Yue et al., 2023) and policies related to technological
    innovations in vehicles and their manufacturers (Jiang and Xu, 2023) were investigated.
    Comparing the brief findings extracted in Table 1, we can conclude that AI acceptance
    and adoption were investigated in various countries (Malaysia, U.S.A., Netherlands,
    Malaysia, India, United Arab Emirates, and Saudi Arabia. North America, China
    (Hong-Kong), China-other regions, Israel, Ireland (Dublin). The number of respondents
    varied from 93 to 5,400, showing a wide range of cases. Furthermore, the methods
    used in these studies vary widely, ranging from descriptive analysis, quantitative
    research, and hierarchical regression in the Malaysian industry ([17]) and U.S.
    accounting fields ([18]), to EFA and CFA in health-related studies in the Netherlands
    ([19], [25]), PLS-SEM in human resources and banking system research in India
    and Malaysia ([26], [28]), and more advanced techniques like NLP and multi-level
    regression analysis in health and manufacturing sectors in the Netherlands and
    Saudi Arabia ([25], [29]). Other methods include OLS regression in transportation
    studies in Israel ([37]), and a mix of ANOVA, chi-square tests, and thematic analysis
    in various fields across different countries. This diversity in methods reflects
    the unique requirements and complexities of each field and research question.
    Table 1 Brief comparative analysis of previous research. Full size table Methodology
    Proposed data processing method Survey data can be analyzed in various ways to
    gain insights and make informed decisions: descriptive analysis, cross-tabulation
    (Warwas et al., 2022), hypothesis testing (t-tests and ANOVA, Chi-square tests,
    regression analysis) (Ko et al., 2021), (Sampurna et al., 2023), factor analysis
    (Bâra and Oprea, 2023), (Oprea and Bâra, 2022), cluster analysis, sentiment analysis
    (Lepelaar et al., 2022), principal component analysis (PCA) to reduce the dimensionality
    of the data and identify the most important components; latent class analysis
    (LCA), Bayesian analysis, text mining and topic modeling like latent dirichlet
    allocation (LDA) or word cloud visualizations. To investigate the responses, we
    propose a descriptive-prescriptive hybrid model that includes cross-tabulation,
    clustering using K-means, analysis of variance (ANOVA), and prediction ensemble
    algorithm (Random Forest) to estimate clusters. The process flow of the methodology
    is described in Fig. 1. Fig. 1 Methodology flow diagram. Full size image Algorithms
    and statistical approach background K-means is an unsupervised ML algorithm used
    for clustering similar data points into groups or clusters. The algorithm aims
    to partition a dataset into K distinct, non-overlapping clusters, with each data
    point belonging to the cluster with the nearest mean value. The number of clusters
    is the most important parameter in K-Means. Thus, it is important to determine
    the optimal number of clusters using the elbow method or silhouette score. It
    represents the number of centroids (cluster centers) to be generated. K-means
    starts by randomly initializing K cluster centroids. These centroids serve as
    the initial cluster centers. Each data point is assigned to the nearest centroid
    based on a distance metric, commonly Euclidean distance. The data points are grouped
    into clusters based on their proximity to the centroids. After assigning data
    points to clusters, the algorithm calculates new centroids for each cluster. The
    new centroids are computed as the mean of all data points within the cluster.
    The assignment and update steps are repeated iteratively until one of the stopping
    criteria is met. K-Means is guaranteed to converge to a solution, but it may not
    always find the global optimum. The final clustering result depends on the initial
    random centroid selection. It has several advantages, such as: simple and easy
    to understand, scales well to large datasets, and can be used for various types
    of data, including numerical and categorical. Nonetheless, it has also disadvantages,
    such as: requires specifying the number of clusters (K) in advance; being sensitive
    to the initial placement of centroids, which leads to different results; assuming
    that clusters are spherical and equally sized, which may not be the case in real
    complex datasets. There are several applications where K-means can be applied
    alone or in combination with other ML algorithms providing solutions for customer
    segmentation for marketing, image compression, anomaly detection, document classification,
    genomic data analysis, recommendation systems, etc. The first step consists of
    random initialization of K cluster centroids μ1, μ2,…,μK, where K is the number
    of clusters. Then, the assignment of the data points to clusters is performed.
    For each data point xi, where i = 1,2, …,N (N is the number of data points), the
    clusters are selected based on Eq. (1): $${c}_{i}=\\mathop{{\\arg}\\,{\\min}}\\limits_{k}{\\|{x}_{i}-{\\mu
    }_{k}\\|}^{2}$$ (1) Each data point xi is assigned to the cluster with the nearest
    centroid, where ci is the cluster assignment for xi. After each assignment, the
    cluster centroids are updated. For each cluster k = 1,2,…,K, the cluster centroids
    are calculated as in Eq. (2): $${\\mu }_{k}=\\frac{1}{|{C}_{k}|}\\sum _{i\\in
    {C}_{k}}{x}_{i}$$ (2) The centroid μk is updated as the mean of all data points
    xi assigned to cluster k, where |Ck| is the number of data points in cluster k.
    The assignment and update steps are repeated until a stopping criterion is met.
    Common stopping criteria include a maximum number of iterations reached or centroids
    no longer changing significantly (leading to convergence). Finally, the clusters
    are evaluated in terms of size, centroids, distributions, and other characteristics.
    Therefore, the K-means algorithm involves iteratively assigning data points to
    clusters and updating cluster centroids until convergence. The algorithm aims
    to minimize the sum of squared distances between data points and their assigned
    centroids. The choice of distance metric and the initialization method (e.g.,
    random initialization or K-means) are essential aspects of the K-means algorithm.
    Additionally, the algorithm’s performance is sensitive to the choice of K, the
    number of clusters, which requires evaluation techniques. ANOVA is a statistical
    technique used to analyze and compare the means of two or more groups or populations
    to determine whether there are statistically significant differences among them.
    ANOVA is a tool for understanding the variability within and between groups. It
    can be combined with K-means to further analyze the quality of clusters. Two-way
    ANOVA extends the one-way ANOVA to handle situations with two independent categorical
    variables or factors. It allows us to simultaneously investigate the effects of
    two factors on a response variable. To formalize the two-way ANOVA, let’s consider
    two independent factors A and B, where I is the number of levels or categories
    in the first factor (Factor A) and J is the number of levels or categories in
    the second factor (Factor B), N is the total number of observations, nij is the
    number of observations in the i-th level of Factor A and the j-th level of Factor
    B. \\({\\bar{x}}_{{ij}}\\) is the mean of observations in the i-th level of Factor
    A and the j-th level of Factor B. \\(\\bar{\\bar{x}}\\) is the overall mean, SST
    is the total sum of squares (variation of all data points from the overall mean),
    SSA is the sum of squares for Factor A (variation due to Factor A), SSB is the
    sum of squares for Factor B (variation due to Factor B), SSAB is the sum of squares
    for the interaction between Factors A and B (joint effect of A and B) and SSW
    is the within-group sum of squares (residual or error variation). The total sum
    of squares is defined in Eq. (3): $${{SS}}_{T}=\\mathop{\\sum }\\limits_{i=1}^{I}\\mathop{\\sum
    }\\limits_{j=1}^{J}\\mathop{\\sum }\\limits_{k=1}^{{n}_{{ij}}}{\\left({x}_{{ijk}}-\\overline{\\bar{x}}\\right)}^{2}$$
    (3) It calculates the total variability in the data, measuring how far individual
    data points are from the overall mean. The sum of squares for Factor A is formulated
    in Eq. (4): $${{SS}}_{A}=\\mathop{\\sum }\\limits_{i=1}^{I}{{n}_{i}\\left({\\bar{x}}_{i}-\\overline{\\bar{x}}\\right)}^{2}$$
    (4) The variation due to Factor A, measuring how much the group means (across
    levels of Factor A) differ from the overall mean, is calculated. The sum of squares
    for Factor B is defined in Eq. (5): $${{SS}}_{B}=\\mathop{\\sum }\\limits_{j=1}^{J}{{n}_{j}\\left({\\bar{x}}_{j}-\\overline{\\bar{x}}\\right)}^{2}$$
    (5) The variation due to Factor B, measuring how much the group means (across
    levels of Factor B) differ from the overall mean, is calculated. The sum of squares
    for interaction is provided in Eq. (6): $${{SS}}_{{AB}}=\\mathop{\\sum }\\limits_{i=1}^{I}\\mathop{\\sum
    }\\limits_{j=1}^{J}{{n}_{{ij}}\\left({\\bar{x}}_{{ij}}-{{\\bar{x}}_{i}-\\bar{x}}_{j}-\\overline{\\bar{x}}\\right)}^{2}$$
    (6) Equation (6) calculates the variation due to the interaction between Factors
    A and B, measuring the combined effect of A and B. Within-group sum of squares
    is provided in Eq. (7): $${{SS}}_{W}=\\mathop{\\sum }\\limits_{i=1}^{I}\\mathop{\\sum
    }\\limits_{j=1}^{J}\\mathop{\\sum }\\limits_{k=1}^{{n}_{{ij}}}{\\left({x}_{{ijk}}-{\\bar{x}}_{{ij}}\\right)}^{2}$$
    (7) Equation (7) calculates the variation within each group, measuring how far
    individual data points are from their respective group means. The total degrees
    of freedom is defined in Eq. (8): $${{df}}_{T}=N-1$$ (8) The degrees of freedom
    for Factor A and for Factor B are formulated in Eq. (9): $$\\begin{array}{cc}{{df}}_{A}=I-1
    & {{df}}_{B}=J-1\\end{array}$$ (9) The degrees of freedom for the interaction
    and within-group are defined as in Eq. (10): $$\\begin{array}{cc}{{df}}_{{AB}}=\\left(I-1\\right)\\times
    \\left(J-1\\right) & {{df}}_{W}=N-I\\times J\\end{array}$$ (10) The mean squares
    for Factor A and for Factor B are provided in Eq. (11): $$\\begin{array}{cc}{{MS}}_{A}=\\frac{{{SS}}_{A}}{{{df}}_{A}}
    & {{MS}}_{B}=\\frac{{{SS}}_{B}}{{{df}}_{B}}\\end{array}$$ (11) The mean squares
    for the interaction and within groups are formulated in Eq. (12): $$\\begin{array}{cc}{{MS}}_{{AB}}=\\frac{{{SS}}_{{AB}}}{{{df}}_{{AB}}}
    & {{MS}}_{W}=\\frac{{{SS}}_{W}}{{{df}}_{W}}\\end{array}$$ (12) F-statistics for
    Factor A, for Factor B, and for the interaction are provided in Eq. (13): $$\\begin{array}{ccc}{F}_{A}=\\frac{{{MS}}_{A}}{{{MS}}_{W}}
    & {F}_{B}=\\frac{{{MS}}_{B}}{{{MS}}_{W}} & {F}_{{AB}}=\\frac{{{MS}}_{{AB}}}{{{MS}}_{W}}\\end{array}$$
    (13) The p-values are calculated based on the respective F-statistics and the
    F-distribution. They indicate the probability of observing such extreme F-statistic
    values under the null hypothesis for each factor and the interaction. Null Hypotheses
    (H0) implies that H0A: No significant effect on Factor A, H0B: No significant
    effect of Factor B, and H0AB: No significant interaction between Factors A and
    B. Alternative Hypotheses (H1): implies that H1A Significant effect on Factor
    A, \\({H}_{1A}\\) a significant effect on Factor B and \\({H}_{1{AB}}\\): Significant
    interaction between Factors A and B. If the p-value for a factor or interaction
    is smaller than a chosen significance level (e.g., 0.05), we reject the null hypothesis
    for that factor or interaction. Otherwise, we fail to reject it. These equations
    formalize the process of conducting a two-way ANOVA to analyze the effects of
    two categorical factors (Factor A and Factor B) and their interaction on a response
    variable. Two-way ANOVA helps determine whether variations in the response variable
    are influenced by one or both factors, as well as their interaction. Variation
    analysis is applied to the groups obtained with K-means to estimate the performance
    of the clustering process. The aim of cluster prediction is to categorize a new
    instance within a dataset into a predefined cluster. For instance, when an individual
    responds to survey questions, they can be assigned to a specific cluster based
    on their answers. While it is acknowledged that perceptions and attitudes in technology
    and innovation are subject to change, such allocation can still be effectively
    done using existing classifications. However, to ensure accuracy and relevance
    in understanding technology adoption, it is necessary to periodically repeat the
    survey, thereby updating the responses and refining the calibration of adoption
    measures. Classification methods focus on identifying variables that take on discrete
    values, which are represented as categories or classes (in our particular case
    cluster number). These classes can be binary, such as y ∈ {0,1}, or multi-class,
    such as y ∈{0,1,2,3,4,…,p}, where p represents the total number of distinct classes.
    In a binary classification scenario, the value 1 typically signifies the “positive”
    class, while 0 indicates the “negative” class. The core of classification techniques
    lies in the construction of a function that predicts the probability that a given
    instance belongs to a particular class. This is achieved by modeling a function
    based on independent variables x and computing the likelihood of belonging to
    a specific class. For instance, in binary classification, if the function \\({h}_{\\theta
    }(x)\\) yields a value equal to or greater than 0.5, it is predicted that \\(\\hat{y}=1\\),
    meaning the instance is classified into the positive class. Conversely, if \\({h}_{\\theta
    }(x)\\) is less than 0.5, \\(\\hat{y}=0\\) is predicted, placing the instance
    in the negative class. The function \\({h}_{\\theta }(x)\\), which ranges from
    0 to 1 (i.e., \\(0\\le {h}_{\\theta }(x)\\le 1\\)), is used to model this classification
    decision. To effectively model the function \\({h}_{\\theta }(x)\\), specific
    notations and methods are employed as in Eq. (14): $$\\begin{array}{c}q={\\theta
    }^{T}\\times X\\\\ q=\\left[\\begin{array}{ccc}{\\theta }_{0} & \\ldots & {\\theta
    }_{n}\\end{array}\\right]\\times \\left[\\begin{array}{ccc}{x}_{1}^{\\left(1\\right)}
    & \\ldots & {x}_{n}^{\\left(1\\right)}\\\\ \\ldots & \\ldots & \\ldots \\\\ {x}_{1}^{(m)}
    & \\ldots & {x}_{n}^{\\left(m\\right)}\\end{array}\\right]\\end{array}$$ (14)
    Where \\({\\rm{\\theta }}=\\left[\\begin{array}{ccc}{\\theta }_{0} & \\ldots &
    {\\theta }_{n}\\end{array}\\right]\\)- the vector of weights that is multiplied
    by X; X—the matrix of input variables; n—number of variables; m—number of observations;
    \\(x=\\left[\\begin{array}{ccc}{x}_{1}^{\\left(i\\right)} & \\ldots & {x}_{n}^{\\left(i\\right)}\\end{array}\\right]\\)—an
    observation. The following probability function is calculated as in Eq. (15):
    $${h}_{\\theta }\\left(x\\right)=\\sigma \\left(q\\right)=\\frac{1}{1+{e}^{-q}}$$
    (15) The sigmoid function approximates: \\(\\hat{y}=1\\), if \\({h}_{\\theta }\\left(x\\right)\\ge
    0.5\\) (or \\(q\\ge 0\\)) and \\(\\hat{y}=0\\), if \\({h}_{\\theta }\\left(x\\right)
    < 0.5\\) (or \\(q < 0\\)), whereas the function \\({h}_{\\theta }\\left(x\\right)\\)
    estimates the probability that \\(y=1\\) as in Eq. (16): $${h}_{\\theta }\\left(x\\right)=P\\left(y=1{\\rm{|}}x{\\rm{;}}\\theta
    \\right)$$ (16) The relation between the two probabilities \\(P\\left(y=1{|x;}\\theta
    \\right)\\) and \\(P\\left(y=0{|x;}\\theta \\right)\\) is defined in Eq. (17):
    $$\\begin{array}{ll}P\\left(y=1{{|}}x{{;}}\\theta \\right)+P\\left(y=0{\\rm{|}}x{{;}}\\theta
    \\right)=1 & P\\left(y=0{\\rm{|}}x{{;}}\\theta \\right)\\\\\\qquad\\,=1-P\\left(y=1{{|}}x{{;}}\\theta
    \\right)\\end{array}$$ (17) Let’s consider \\({q=\\theta }_{0}+{\\theta }_{1}{x}_{1}+{\\theta
    }_{2}{x}_{2}\\), where \\({\\theta }_{0}=-2,{\\theta }_{1}=1\\) and \\({\\theta
    }_{2}=1\\). Thus, \\(q=-2+{x}_{1}+{x}_{2}\\) and \\({h}_{\\theta }\\left(x\\right)=\\sigma
    \\left(q\\right)=\\frac{1}{1+{e}^{-(-2+{x}_{1}+{x}_{2})}}\\). \\(y=1\\), if \\(-2+{x}_{1}+{x}_{2}\\ge
    0\\) or \\({x}_{1}+{x}_{2}\\ge 2\\), creating a decision boundary (as in Fig.
    2). Fig. 2 Boundary between the two classes (p = 2). Full size image The cost
    function for classification depends on the dependent variable y as it can be either
    \\(y=1\\) or \\(y=0\\) as in Eq. (18): $$\\begin{array}{ll}{Cost}\\left(\\theta
    \\right)=\\frac{1}{m}\\mathop{\\sum}\\limits_{i=1}^{m}-\\log {\\rm{}}\\left({h}_{\\theta
    }\\left({x}^{\\left(i\\right)}\\right)\\right)\\\\{Cost}\\left(\\theta \\right)
    =\\frac{1}{m}\\mathop{\\sum}\\limits_{i=1}^{m}-\\log {\\rm{}}\\left(1-{h}_{\\theta
    }\\left({x}^{\\left(i\\right)}\\right)\\right)\\end{array}$$ (18) If \\(y=1\\),
    when \\({h}_{\\theta }\\left(x\\right)\\) tends 1, the \\({Cost}\\left(\\theta
    \\right)\\) is very small and tends to 0, whereas when \\({h}_{\\theta }\\left(x\\right)\\)
    tends 0, the cost is very high \\({Cost}\\left(\\theta \\right)\\) and tends to
    infinite. Furthermore, if \\(y=0\\), when \\({h}_{\\theta }\\left(x\\right)\\)
    is closer to 0, the cost tends to 0, then \\({Cost}\\left(\\theta \\right)\\to
    0\\), whereas if \\({h}_{\\theta }\\left(x\\right)\\) tends to 1, then the cost
    is rocketing to infinite: \\({Cost}\\left(\\theta \\right)\\to \\infty\\) as the
    estimation is far from the target and it has to be significantly penalized (as
    in Fig. 3). Fig. 3 Cost function. Full size image In the same way as with linear
    regression, where overfitting is a potential issue, a regularization term denoted
    by γ can be incorporated into the cost function. This addition aims to mitigate
    the problem of overfitting by decreasing the variance. Consequently, the cost
    function is modified to include this regularization term as in eq. (19). $$\\begin{array}{l}{Cost}\\left(\\theta
    \\right)=-\\frac{1}{m}\\mathop{\\sum}\\limits_{i=1}^{m}\\left[{y}^{(i)}\\log \\left({h}_{\\theta}\\left({x}^{\\left(i\\right)}\\right)\\right)+(1-{y}^{\\left(i\\right)})\\left(\\log
    \\left(\\right.1-{h}_{\\theta}\\left({x}^{\\left(i\\right)}\\right)\\right)\\right]\\\\\\qquad\\qquad\\quad
    +\\frac{\\gamma}{2}\\mathop{\\sum}\\limits_{j=1}^{n}{\\theta }_{j}^{2}\\end{array}$$
    (19) The goal is to fine-tune the model’s weights to achieve the lowest possible
    value of the cost function. The gradient descent algorithm is a commonly used
    method for tackling this optimization challenge. By iteratively updating the weights
    θ, the algorithm works towards minimizing the overall cost, defined in Eq. (20).
    $${\\theta }_{j}\\,{{:= }}\\,{\\theta }_{j}-\\alpha \\frac{\\partial {Cost}(\\theta
    )}{\\partial {\\theta }_{j}}$$ (20) The partial derivative of the cost function
    is calculated to obtain Eq. (21): $$\\begin{array}{cc}{\\theta }_{j}\\,{{:= }}\\,{\\theta
    }_{j}-\\alpha \\frac{1}{m}\\mathop{\\sum}\\limits_{i=1}^{m}\\left({h}_{\\theta
    }\\left({x}^{(i)}\\right)-{y}^{(i)}\\right){x}_{j}^{(i)} & \\left(\\forall \\right)j=\\bar{0,n}\\,{\\rm{and}}\\,{x}_{0}=1\\end{array}$$
    (21) If the classification has multiple classes, \\(y\\epsilon \\left\\{\\mathrm{0,1,2},\\ldots
    ,p\\right\\}\\), then the classification problem can be decomposed in p + 1 problems
    as in Eq. (22): $$\\begin{array}{lll}{h}_{\\theta }^{0}\\left(x\\right)=P\\left(y=0{\\rm{|}}x{\\rm{;}}\\theta
    \\right) & {h}_{\\theta }^{1}\\left(x\\right)=P\\left(y=1{\\rm{|}}x{\\rm{;}}\\theta
    \\right)\\ldots\\\\ {h}_{\\theta }^{r}\\left(x\\right)=P\\left(y=p{\\rm{|}}x{\\rm{;}}\\theta
    \\right)\\end{array}$$ (22) To evaluate the performance of classification, Accuracy
    (A), Precision (P), Recall (R), F1 score, and AUC are calculated as in Eq. (23):
    $$A\\left(y,\\hat{y}\\right)=\\frac{1}{m}\\mathop{\\sum}\\limits_{i=1}^{m}1\\left(\\hat{y}=y\\right)$$
    (23) Where y – real value, \\(\\hat{y}\\) – estimated value. Recall (R) or True
    Positive Rate (TPR) is the ratio between True Positive (TP) and the sum of TP
    and False negative (FN). Precision (P) is the ratio between TP and the sum of
    TP and False Positive (FP). Usually, F1score is calculated mostly as it represents
    a combination of R and P. They are defined in Eq. (24): $$\\begin{array}{ccc}R=\\frac{{TP}}{{TP}+{FN}}
    & P=\\frac{{TP}}{{TP}+{FP}} & {Fscore}=\\frac{P\\times R}{P+R}\\end{array}$$ (24)
    Area under the curve (AUC) represents the performance metric of a classifier.
    This metric ranges from 0.5 to 1, where 1 represents the ideal score for excellent
    model performance. An AUC significantly greater than 0.5, typically above 0.7,
    indicates strong predictive accuracy. Results Input data This research examines
    a subset of a comprehensive, publicly accessible survey, the details of which
    are available in a GitHub repository and on the Pew Research Center (PRC) website.
    The analyzed data originates from a survey panel that took place between November
    1st and November 7th, 2021. Out of the 11,492 individuals selected for the survey,
    10,260 completed their responses, resulting in an 89% participation rate. Therefore,
    the survey was designed mainly by PCR in consultation with Ipsos and responses
    were collected online and pre-processed (including the data quality checks) by
    the same institution. According to PRC methodology, when considering non-responses
    to initial recruitment efforts and subsequent dropouts, the overall response rate
    was 3%. Among those panelists who began the survey and answered at least one question,
    the break-off rate was 1%. For the entire sample of 10,260 respondents, the margin
    of error in sampling was estimated to be ±1.6 percentage points. This survey aimed
    to gather data from non-institutionalized adults aged 18 and over who reside in
    the United States, encompassing those living in Alaska and Hawaii. Within each
    household, the survey methodology involved selecting the adult who had the nearest
    upcoming birthday to participate by completing the survey online. Each participant
    was provided with a pre-arranged reward for their involvement in the survey. While
    the original survey explored both technological and scientific impacts, our analysis
    in this paper is confined to the technological aspects (that refer to 5,153 out
    of the 10,260 records of the dataset). This decision is due to the dataset’s complexity
    and its extensive range of variables. For reader convenience and to support data
    transparency, we included a link to the dataset on GitHub in the data availability
    section. This allows for the replication of our analysis. Addressing potential
    biases in survey data is essential for the validity of a study’s outcomes. Concerns
    such as response bias, where respondents may provide socially desirable answers,
    can be mitigated by ensuring anonymity and using neutral, non-leading questions.
    Sampling bias, which occurs when the sample is not representative of the population,
    can be minimized through randomized sampling methods and, if necessary, stratified
    sampling to include underrepresented groups. We used data from the PRC. It is
    an organization renowned for conducting surveys, sampling, data analysis, and
    pre-processing. We considered their pre-processed survey data, which comes with
    an array of explanatory read-me files. PRC employs a rigorous survey methodology
    and provides a wealth of resources for each survey, including files in both CSV
    and PDF formats. Their surveys feature questions labeled with full descriptions
    and answers presented in various formats, ranging from all numeric to mixed formats.
    Additionally, the PRC offers detailed information on its methodology and pre-processing
    stages. The answers were not linked to the respondents’ names being pre-processed
    by the PRC. The exploratory data analysis (EDA) provides some initial insights
    into the dataset sample: The QKEY column is a unique identifier for respondents,
    ranging from 2 to 10,260, with a total count of 5,153 entries. This indicates
    that the dataset is a subset of a larger survey that approached both technological
    and science-related issues. TECH1_W99: Responses suggest that the majority of
    participants (2627 out of 5153) perceive the impact of technology as “Mostly positive”,
    with a significant number (2036) seeing equal positive and negative effects. A
    smaller group (482) views it as “Mostly negative”. CNCEXC_W99: The sentiment about
    concern vs. excitement is fairly even, with 2,267 respondents being “Equally concerned
    and excited” and 1,981 being “More concerned than excited”. A smaller portion
    of 889 is “More excited than concerned”. ALGFAIR_W99: A plurality of respondents
    (2046) is “Not sure” about the fairness of algorithms, with almost equal numbers
    finding it “Not possible” (1580) and “Possible” (1480) for algorithms to be fair.
    F_PARTYSUM_FINAL: Political affiliations are split closely between “Dem/Lean Dem”
    (2589) and “Rep/Lean Rep” (2445), with a small number (119) not leaning towards
    any party or refusing to disclose. F_INC_SDT1: Income distribution varies, with
    the highest number of respondents (1427) reporting an income of “$100,000 or more”.
    The next largest group earns “Less than $30,000” (812), followed by a fairly even
    distribution across other income ranges. F_IDEO: Ideological self-identification
    is mostly “Moderate” (1,853), with “Conservative” (1,344) and “Liberal” (913)
    also represented. Fewer respondents identify as “Very conservative” (541) or “Very
    liberal” (424). This dataset is designed to capture a diverse array of opinions
    on technology, societal concerns, perceptions of fairness, and personal beliefs,
    alongside demographic information such as political affiliation and income. The
    dataset in focus offers a compiled array of survey responses, designed to capture
    a snapshot of public opinion on a spectrum of contemporary issues. The data is
    organized into 126 columns, each representing a unique question or data point,
    and spans 5153 rows, corresponding to individual participants. The survey’s breadth
    covers topics as varied as technological impact (TECH1_W99), concerns and excitement
    about future developments (CNCEXC_W99), and perceptions of algorithmic fairness
    (ALGFAIR_W99). Additionally, it delves into personal beliefs and demographic details,
    including political leanings (F_PARTYSUM_FINAL), income brackets (F_INC_SDT1),
    and self-identified ideology (F_IDEO). The columns cover responses to questions
    regarding technology, fairness of algorithms, discrimination, and personal beliefs.
    Moreover, the dataset includes demographic information such as political affiliation,
    income brackets, and frequency of internet use, among other variables. Given the
    presence of certain columns such as TECH1_W99 and CNCEXC_W99, it can be inferred
    that the survey explores attitudes towards technological advancements and their
    perceived impact on society. Additionally, columns like DISCRIM1_a_W99 to DISCRIM1_f_W99
    suggest an assessment of perceived discrimination across various facets of life.
    The survey data is rich with individual viewpoints that are categorized to facilitate
    a nuanced analysis of public opinion. The diversity of topics within the dataset
    indicates a multifaceted approach to understanding complex societal issues, ranging
    from technology and privacy to socio-political dynamics and personal ideology.
    An EDA was performed, and it revealed a dataset that is complete and free from
    common data entry errors such as duplicates or missing values. This level of data
    integrity allows for a robust analysis of the survey responses. The data showcases
    a diversity of viewpoints, with no single perspective dominating the responses.
    For instance, opinions on technology’s impact range broadly from “Mostly positive”
    to “Mostly negative,” reflecting a spectrum of individual experiences and attitudes.
    The demographic information suggests a balanced representation of political affiliations
    and income levels, providing a rich ground for examining correlations between
    socioeconomic status and opinions on the surveyed topics. Similarly, the distribution
    of ideological self-identification points to a nuanced political landscape, which
    could be pivotal in interpreting the subtleties of the participant’s views on
    the fairness of algorithms and other technology-related issues. Americans’ attitudes
    toward the large-scale use of facial recognition technology by law enforcement
    to monitor crowds and search for potential crime suspects lean more toward the
    positive side. Specifically, 46% of U.S. adults view this as a beneficial concept
    for society, while 27% consider it unfavorable, and another 27% are uncertain
    about it. Regarding the use of computer algorithms by social media companies to
    detect false information on their platforms, public opinion slightly favors it
    being a good idea for society (38%) rather than a bad one (31%). Ambivalence is
    another notable theme in the survey data: 45% indicate they feel both excitement
    and concern about the increased use of AI programs in daily life, while 37% express
    more concern than excitement, and 18% report feeling more excited than concerned.
    The greatest concern relates to the loss of jobs, followed by the loss of privacy
    (as in Fig. 4). Top of FormBottom of Form This discussion contextualizes the survey
    data within the broader topic of AI acceptance, highlighting the complexity of
    public sentiment and the factors that influence it. The dataset encompasses a
    comprehensive collection of survey responses, potentially aimed at gauging public
    sentiment on a variety of topics. Fig. 4: Concerns and excitement distributions.
    a Concern distribution—includes the list of concerns related to AI technologies
    and their frequencies; b excitement distribution—includes the list of excitements
    related to AI technologies and their frequencies. Full size image Distributions
    and cross-tabulation Education and AI technologies To focus on the education level
    of the respondents and analyze its relationship with the acceptance of AI and
    technology, the dataset contains two columns that relate to the education of the
    respondents: ‘F_EDUCCAT’ and ‘F_EDUCCAT2’. These columns contain categorical data
    reflecting the highest level of education attained by the survey participants.
    We analyze the responses in these columns and cross-reference them with other
    columns related to attitudes toward technology and AI. This helps us understand
    if there are any discernible patterns or correlations between education levels
    and opinions on these topics. We start by examining the distribution of educational
    attainment among the respondents and then look at the relationship with their
    views on AI technology. The analysis of the dataset with respect to the education
    of the respondents reveals the following education distribution: College Graduate
    or Higher 2620 respondents; Some College 1640 respondents; High School Graduate
    or Less 877 respondents; Refused 16 respondents. When broken down further (‘F_EDUCCAT2’),
    we notice: College Graduate/Some Postgraduate 1443 respondents; Postgraduate 1177
    respondents; Some College, No Degree 1083 respondents; High School Graduate 717
    respondents; Associate’s Degree 557 respondents; Less than High School 160 respondents;
    Refused 16 respondents (as in Fig. 5). Fig. 5 Education distribution of respondents.
    Full size image To determine opinion on technology’s impact (TECH1_W99), we cross-tabulate
    education levels with opinions on the impact of technology. Respondents with a
    higher education level (College Graduate+) tend to have a more positive view of
    technology, with 1,527 considering it “Mostly positive”. Those with a high school
    education or less have a greater tendency to see technology’s effects as “Equally
    positive and negative” (359) or “Mostly negative” (124). Some college education
    respondents also exhibit a balanced perspective, with significant numbers viewing
    technology’s impact as both positive (736) and mixed (724). Looking at perceptions
    of algorithm fairness (ALGFAIR_W99), college graduates and postgraduates show
    a higher degree of skepticism or uncertainty, with 976 “Not sure” about the fairness
    of algorithms and 839 finding it “Not possible” for algorithms to be fair. Those
    with less education also show uncertainty but with fewer respondents (398 “Not
    sure” for high school education or less). Across all education levels, a significant
    number of respondents are unsure about the fairness of algorithms, indicating
    a general ambiguity regarding this aspect of AI. As education often plays a role
    in technological acceptance and understanding, a bar plot for the opinion on algorithm
    fairness by the highest level of education attained is showcased in Fig. 6. Fig.
    6 Algorithm fairness and education. Full size image Figure 6 shows the perception
    of algorithm fairness by the respondents’ education level. The education levels
    are on the x-axis, and the number of respondents is on the y-axis. The colors
    differentiate the respondents’ views on whether they believe it is possible for
    algorithms to be fair, not possible, or if they are not sure. These provide an
    image of how education correlates with opinions on technology’s impact and perceptions
    of algorithm fairness, which are key indicators of AI acceptance. Age and AI technologies
    To focus on the age of the respondents, we analyze the distribution of respondents’
    ages and examine how different age groups’ opinions vary on technology’s impact,
    excitement and concerns about future developments, and perceptions of algorithmic
    fairness. The column ‘F_AGECAT’ represents the age categories of the respondents.
    We further analyze the distribution of respondents’ ages and then look at how
    these age groups relate to their views on technology’s impact (TECH1_W99), excitement
    and concerns about future developments (CNCEXC_W99) and perceptions of algorithmic
    fairness (ALGFAIR_W99). Age distribution is as follows: Ages 30–49 1,642 respondents;
    Ages 65 + 1,527 respondents; Ages 50–64 1,509 respondents; Ages 18–29 457 respondents;
    Refused to specify 18 respondents. Analyzing these age segments and opinions on
    technology’s impact (TECH1_W99), younger respondents (ages 18–29) show a more
    positive view of technology’s impact, with the majority (237) considering it “Mostly
    positive”. The 30–49 age group also leans toward a “Mostly positive” view (774)
    but with a substantial number seeing equal positive and negative effects (702).
    Older age groups (50–64 and 65+) show a balanced perspective, with a significant
    number viewing technology’s impact as both positive (754 and 857, respectively)
    and mixed (603 and 546, respectively). Furthermore, analyzing the age groups and
    the concern vs. excitement about future developments (CNCEXC_W99), the youngest
    cohort (18–29) exhibits a higher level of excitement than concern, with a notable
    proportion equally concerned and excited (215). As age increases, the level of
    concern seems to rise, with the 50–64 and 65+ groups showing the highest number
    of respondents more concerned than excited (643 and 651, respectively). Moreover,
    considering the perception of algorithm fairness (ALGFAIR_W99), the following
    aspects were extracted. There is a fair degree of uncertainty across all age groups
    regarding the fairness of algorithms, with “Not sure” being a common response.
    The perception of algorithms as “Not possible” to be fair is consistently high
    across all age groups, indicating a widespread skepticism about AI fairness. Gender,
    race, and AI technologies To focus on the gender and race of the respondents and
    their relationship with attitudes toward AI and technology, we analyze how opinions
    on technology’s impact, excitement and concerns about future developments, and
    perceptions of algorithmic fairness vary by these demographic factors. The columns
    ‘F_GENDER’, ‘F_RACECMB’, and ‘F_RACETHNMOD’ correspond to the respondents’ gender
    and race/ethnicity. First, we analyze the distribution of respondents’ gender
    and race and then look at how these demographic groups relate to their views on
    technology’s impact (TECH1_W99), excitement and concerns about future developments
    (CNCEXC_W99), and perceptions of algorithmic fairness (ALGFAIR_W99). The gender
    distribution reveals: Women 2,840 respondents; Men: 2,270 respondents; Other/Non-binary:
    31 respondents; Refused: 12 respondents, while the race distribution distinguished
    among White: 4,072 respondents; Black or African-American 436 respondents; Mixed
    Race: 186 respondents; Asian or Asian-American 180 respondents; Other Races 177
    respondents; Refused: 102 respondents. Their opinions on technology’s impact (TECH1_W99)
    reflect that both men and women predominantly see technology’s impact as either
    “Mostly positive” or “Equal positive and negative effects.” However, women are
    slightly more likely than men to see the impact as “Mostly negative”. Respondents
    who identify as Asian or Asian-American are most likely to view technology’s impact
    as “Mostly positive,” followed by White respondents. Regarding the concern vs.
    excitement about future developments (CNCEXC_W99), women are slightly more likely
    than men to be “More concerned than excited” about future technological developments.
    Black or African-American respondents are the most likely to be “More concerned
    than excited,” whereas Asian or Asian-American respondents are more likely to
    be “More excited than concerned.” The perception of algorithm fairness (ALGFAIR_W99)
    that a higher proportion of women than men believe it is “Not possible” for algorithms
    to be fair, whereas men are more likely to believe it is “Possible”. White respondents
    show the greatest skepticism about the possibility of fair algorithms, with the
    highest count of respondents who believe it is “Not possible” (as in Fig. 7).
    Fig. 7 AI technology’s impact on gender and race. Full size image Religion, ideology,
    and AI technologies To analyze the survey data based on the religion and ideology
    of the respondents, we explore how different religious and ideological groups
    perceive the impact of technology, their concerns and excitement about future
    developments, and their views on the fairness of algorithms. The columns ‘F_RELIG’
    and ‘F_RELIGCAT1’ correspond to the respondents’ religious affiliations, and ‘F_PARTYSUMIDEO_FINAL’
    and ‘F_IDEO’ to their ideological leanings. We analyze the distribution of respondents’
    religion and ideology and then look at how these demographic groups relate to
    their views on technology’s impact (TECH1_W99), excitement and concerns about
    future developments (CNCEXC_W99), and perceptions of algorithmic fairness (ALGFAIR_W99).
    Examining the distribution of religion among the respondents, we find that the
    largest group of respondents identified as Protestant (2,204), followed by Roman
    Catholic (1,075), and those with no particular religious affiliation (826). Smaller
    religious groups included Agnostics (307), Atheists (279), and other faiths like
    Jewish (108), Mormon (100), Buddhist (34), Hindu (26), and Muslim (24). Regarding
    ideology distribution, Moderates represent the largest ideological group (1,853),
    followed by Conservatives (1,344), Liberals (913), Very Conservatives (541), and
    Very Liberals (424). A small number refused to specify their ideology (78). Their
    opinions on technology’s impact (TECH1_W99) reveal that across most religious
    groups, there’s a balance between seeing technology’s effects as equally positive
    and negative. However, Atheists and Agnostics tend to have a more positive view.
    Ideologically, Moderates and Conservatives are more inclined to see technology’s
    impact as equally positive and negative, while Liberals and Very Liberals are
    more likely to see it as mostly positive. Concerns and excitement about future
    technological developments (CNCEXC_W99) are distributed relatively evenly across
    religious affiliations, with no significant outliers. Ideologically, Moderates
    and Conservatives tend to be more concerned than excited, while Liberals and Very
    Liberals show more excitement than concern. The perception of algorithm fairness
    (ALGFAIR_W99) reflects that skepticism about the possibility of algorithms being
    fair is high across all religious groups, with Protestants and Roman Catholics
    having the highest number of respondents who think it is not possible. Ideologically,
    Conservatives and Very Conservatives are more likely to believe that it is not
    possible for algorithms to be fair, while Liberals and Very Liberals are more
    divided between “Not possible” and “Not sure.” Income and AI technologies To analyze
    the survey data based on the income levels of the respondents, represented by
    the columns ‘F_INC_SDT1’ and ‘F_INC_TIER2’, we look into how income correlates
    with attitudes toward technology, concerns about future developments, and views
    on the fairness of algorithms. A summary of the analyses focusing on the income
    levels (‘F_INC_SDT1’ and ‘F_INC_TIER2’) of the respondents is provided. The highest
    number of respondents have an income of $100,000 or more (1427). The next largest
    group earns less than $30,000 (812). Other income ranges have smaller counts,
    with the least being those earning $80,000 to less than $90,000 (277). Some respondents
    refused to disclose their income (244). Middle income is the largest group (2560).
    Upper income is next (1222), followed by lower income (1090). A number of respondents
    refused to disclose their income tier (281). Their opinions on technology’s impact
    (TECH1_W99) show that in the ‘F_INC_SDT1’ category, those earning $100,000 or
    more have the highest number of respondents viewing technology’s impact as “Mostly
    positive” (846), while also having a considerable portion seeing it as “Equal
    positive and negative effects” (481). The ‘F_INC_TIER2’ category shows similar
    patterns, with upper-income respondents more likely to view technology’s impact
    as “Mostly positive” compared to other income tiers. Regarding concern vs. excitement
    about future developments (CNCEXC_W99), for ‘F_INC_SDT1’, lower-income respondents
    have a higher count of being “More concerned than excited” (445) compared to those
    excited (176), whereas upper-income individuals are more balanced with a considerable
    number being “More excited than concerned” (268). ‘F_INC_TIER2’ analysis shows
    that middle-income respondents are more evenly split between concern and excitement,
    with a notable number being “Equally concerned and excited” (1119). Concerning
    the perception of algorithm fairness (ALGFAIR_W99), respondents across all income
    levels in the ‘F_INC_SDT1’ category show skepticism about the possibility of algorithms
    being fair, with high-income earners being the most doubtful (440 believe it’s
    “Not possible”). The ‘F_INC_TIER2’ category also reflects skepticism across all
    tiers, with middle-income individuals showing the highest number of respondents
    who think it is “Not possible” for algorithms to be fair (812). In Fig. 8, the
    opinion on AI technology’s impact on income is showcased. Fig. 8 Opinions on technology’s
    impact categorized by income tier. Full size image The income tiers are represented
    on the x-axis, and the number of respondents is on the y-axis. Different colors
    indicate whether respondents view technology’s impact as mostly positive, mostly
    negative, or equally positive and negative. Clustering using K-means The columns
    QKEY, SMALG3_W99, F_REG, and F_INTFREQ that contain identifiers or strings are
    removed and the values are scaled using a standard scaler, which centers the data
    around zero with a standard deviation of one. The majority of data points are
    grouped into Cluster 4 (using 5 clusters setting) and Cluster 1 (using 3 clusters),
    while the other clusters contain significantly fewer points. After rerunning the
    K-means clustering with 5 and 3 clusters, the respondents’ distribution is presented
    in Table 2. Table 2 Respondents’ distribution for 3 and 5 clusters. Full size
    table With 3 clusters, the majority of the respondents are in Cluster 1, while
    Clusters 0 and 2 are much smaller. To visualize the clusters, a dimensionality
    reduction technique such as Principal Component Analysis (PCA) is applied to reduce
    the dataset to 2 dimensions for a 2D plot, or to 3 dimensions for a 3D plot (as
    in Fig. 9). The silhouette score, with the clusters assigned by K-Means, is approximately
    0.828. This score suggests a good separation between the clusters, indicating
    that the clustering configuration is well-defined for the sampled data. In our
    research, we explored various datasets that contained both numeric and non-numeric
    variables. We applied multiple clustering techniques, including K-prototypes,
    DBSCAN, and hierarchical and spectral clustering. However, we found that the K-means
    clustering algorithm yielded the most effective results in terms of segmentation
    quality. Fig. 9 Clusters 3D visualization after using PCA. Full size image Each
    color represents one of the three clusters: Red: Cluster 0; Green: Cluster 1;
    Blue: Cluster 2. ANOVA is performed for each feature across the three clusters.
    For most features, the p-values are significantly low (often below 0.05), suggesting
    that there are statistically significant differences in the mean values of those
    features across the clusters. This indicates that the clustering has effectively
    separated the respondents into groups with distinct characteristics. ANOVA is
    useful for identifying features that contribute significantly to clustering, which
    can help in interpreting the clusters or in feature selection for further modeling.
    The centroids of the clusters represent the mean value for each feature within
    the cluster. These are the centroids in the original feature space for each of
    the three clusters: Cluster 0: This cluster has higher average values for several
    features compared to Cluster 1 but is generally lower than Cluster 2. Features
    like TECH1_W99, CNCEXC_W99, and ALGFAIR_W99 are notably higher than Cluster 1
    but less than Cluster 2. This cluster might represent a middle ground between
    the other two clusters in terms of features; Cluster 1: This cluster tends to
    have the lowest average values across most features. It suggests that this cluster
    may represent a group with less pronounced characteristics in terms of the variables
    measured or a baseline group in comparison to the other clusters; Cluster 2: This
    cluster has the highest average values for the features, particularly for TECH1_W99,
    CNCEXC_W99, ALGFAIR_W99, DISCRIM1_a_W99, and DISCRIM1_b_W99. This indicates that
    this cluster’s characteristics are quite distinct and more pronounced in comparison
    to the other two clusters. These interpretations are general and based on the
    average values of the features; individual cluster members may vary. The centroids
    can help in understanding the profiles of the respondents in each cluster, which
    could be useful for targeted analysis, marketing, or further study. The feature
    with the most variation across the centroids of the three clusters is DCARS11_b_W99:
    A feature with high variance among the clusters, suggesting that respondents have
    significantly different values for this feature across clusters; DCARS11_a_W99:
    Similar to the previous feature, indicating that this feature strongly differentiates
    the clusters. The feature-specific analysis for the selected features shows the
    following mean responses for each cluster are presented in Table 3. Table 3 Feature-specific
    analysis by cluster. Full size table From these observations, we notice that Cluster
    2 is characterized by a strong engagement with the survey’s topics, particularly
    concerning technology and societal issues. Cluster 0 appears to have moderate
    views, while Cluster 1 seems less engaged or concerned about these topics. To
    understand the profiles of the respondents in each cluster, we can look at the
    centroids for each cluster and interpret them based on the features that stand
    out. A general profile summary based on the centroids provided earlier is provided:
    Cluster 0 - The Moderates. It has moderate average values for most features, neither
    extremely high nor low. This could indicate a more balanced or moderate perspective
    on various issues. Respondents in this cluster might not hold extreme views and
    could represent a general population in terms of the survey’s topics; Cluster
    1—The Baseline. It shows the lowest average values for most of the features. This
    suggests that respondents in this cluster may be less engaged or have less intense
    opinions on the surveyed topics. This could be the most common profile, possibly
    representing the average respondent with no strong inclinations toward the topics
    surveyed; Cluster 2—The Intensives. It exhibits the highest values across nearly
    all features, indicating strong opinions, higher engagement, or more pronounced
    behaviors in the topics covered by the survey. Respondents in this group might
    be more passionate, involved, or affected by the survey’s topics, which could
    make them stand out in specific areas. For a more detailed profile, we map the
    questions’ code with their description. With that information, we can interpret
    the clusters’ profiles more precisely in the context of the survey and refine
    our understanding of the three clusters: Cluster 0 - The Cautious Moderates. This
    group appears to be moderate in their acceptance of new technologies like driverless
    vehicles and facial recognition, possibly favoring some regulations like dedicated
    lanes for driverless cars (‘DCARS11_a_W99’) and identifiable labels (‘DCARS11_b_W99’).
    They might hold moderate views on the role of companies and federal agencies in
    setting standards for facial recognition technology used by the police (‘FACEREC6_b_W99’
    and ‘FACEREC6_a_W99’). Their opinions on social media’s role in filtering false
    information (‘SMALG4_d_W99’ and ‘SMALG12_W99’) might be balanced, suggesting they
    see both benefits and drawbacks in technology’s role in public discourse. Cluster
    1—The Baseline or Technology Skeptics. Members of this cluster could be less inclined
    towards the adoption of new technologies, perhaps less excited about the prospect
    of enhancements like advanced vision (‘POSNEGHE_e_W99’), or less accepting of
    driverless vehicles under various conditions (‘DCARS11’ series). They might also
    be more skeptical of the effectiveness of computer programs in filtering false
    information on social media (‘SMALG4_d_W99’ and ‘SMALG12_W99’). This cluster could
    be more conservative in its views on how facial recognition technology is regulated
    and used by law enforcement (‘FACEREC6’ series). Cluster 2—The Technologically
    Enthusiastic or Concerned. This group likely has strong opinions about technology,
    possibly showing a high level of acceptance for driverless vehicles (‘DCARS11’
    series) and new human abilities through technological advancements (‘POSNEGHE_e_W99’).
    They may advocate for a significant role of both companies and government agencies
    in setting standards for the use of facial recognition technology by the police
    (‘FACEREC6_b_W99’ and ‘FACEREC6_a_W99’). Concerns about racial bias in the use
    of facial recognition technology by the police (‘FACEREC3_c_W99’) and strong views
    on the role of technology in combating false information on social media (‘SMALG4_d_W99’
    and ‘SMALG12_W99’) could also be defining characteristics. These profiles are
    inferred from the centroids, which represent the average values for each cluster,
    and are based on the assumption that higher values indicate greater acceptance
    or concern, as suggested by the descriptions. Each cluster seems to have a unique
    stance on technology and its implications for society. Corroborating the below
    features specifically with their description in the survey, significant differences
    suggest that they are the most influential in distinguishing the groups defined
    by the clustering algorithm. FACEREC6_b_W99: Another distinctive feature among
    the clusters, possibly relating to a respondent’s recognition or attitude towards
    something specific. FACEREC6_a_W99: As with the “b” variant of this feature, it
    seems to be a strong differentiator between clusters. FACEREC3_c_W99: This feature
    also shows considerable variance and may represent a unique aspect of the respondent’s
    profile. SMALG4_d_W99: Indicates a distinctive characteristic or opinion among
    the clusters. POSNEGHE_e_W99: Could represent a particular positive or negative
    health aspect that differs significantly among clusters. DCARS11_c_W99: Like the
    other “DCARS11” features, this one is also a strong differentiator. SMALG12_W99:
    Another feature that significantly varies among clusters. The features with the
    most variation across clusters relate to opinions on emerging technologies such
    as driverless vehicles, facial recognition, and computer programs to detect false
    information. They can be used to understand the clusters’ general attitudes toward
    technology and regulation. For example, a cluster with high values in DCARS11_b_W99
    and DCARS11_a_W99 may consist of respondents who are more accepting of driverless
    vehicles under certain conditions. Conversely, a cluster with low values in FACEREC3_c_W99
    might be less concerned about potential racial biases in the use of facial recognition
    technology by the police. For a more nuanced understanding, it is useful to visualize
    the distribution of these key features within clusters to see how the responses
    vary (as in Fig. 10). Figure 10 and Table 3 show the differences in mean responses
    between the clusters for the selected features. As of TECH1_W99 (Opinions on technology),
    the mean response for Cluster 1 is approximately 1.5 points lower than that of
    Cluster 0, indicating less enthusiasm or engagement with technology. Cluster 2’s
    mean response is significantly higher than Cluster 1’s by over 16 points, reflecting
    a much stronger sentiment towards technology among respondents in Cluster 2. Regarding
    CNCEXC_W99 (Views on the concept of excellence), the mean response for Cluster
    1 is about 2.6 points lower than Cluster 0’s, suggesting different standards or
    priorities regarding excellence. Cluster 2’s mean response is dramatically higher
    than Cluster 1’s by about 40 points, indicating that the concept of excellence
    is significantly more important to respondents in Cluster 2. In terms of ALGFAIR_W99
    (Views on the fairness of algorithms), Cluster 1 has a mean response that is about
    0.78 points lower than Cluster 0, suggesting less concern or awareness of algorithmic
    fairness. The mean response for Cluster 2 is about 23.8 points higher than Cluster
    1, which could indicate a strong opinion on the fairness of algorithms, possibly
    perceiving them as often unfair. Fig. 10: Mean response for selected features
    by clusters (up). Differences in mean response for selected features by clusters
    (down). Full size image As of DISCRIM1_a_W99 (Views on discrimination), Cluster
    1 shows a mean response that is about 6.6 points lower than Cluster 0, indicating
    that discrimination might not be perceived as a significant issue among this group.
    Cluster 2 has a mean response that is about 39.6 points higher than Cluster 1,
    which could signify a heightened awareness or concern regarding discrimination.
    Related to FACEREC3_c_W99 (Opinions on potential racial bias in the use of facial
    recognition), the mean response for Cluster 1 is about 41.9 points lower than
    Cluster 0, suggesting significantly less concern about potential racial bias.
    Cluster 2’s mean response is about 87.4 points higher than Cluster 1, indicating
    strong concerns about racial bias in facial recognition technology. These differences
    underline the distinct profiles of each cluster: Cluster 0: Moderate views on
    technology and social issues; Cluster 1: Generally lower concern or engagement
    with the issues. Cluster 2: Strongly engaged and often with pronounced concern
    or optimism regarding technology and its societal implications. The correlation
    matrices for each cluster are displayed above in Fig. 11. Regarding Cluster 0,
    there is a moderate positive correlation between TECH1_W99 (Opinions on technology)
    and ALGFAIR_W99 (Views on the fairness of algorithms). This suggests that within
    this cluster, individuals who have positive sentiments toward technology may also
    perceive algorithms as fair. A moderate positive correlation is also observed
    between ALGFAIR_W99 and FACEREC3_c_W99 (Opinions on potential racial bias in the
    use of facial recognition), indicating that those concerned with algorithm fairness
    might also be concerned about racial bias in facial recognition. Cluster 1 shows
    a strong positive correlation between TECH1_W99 and ALGFAIR_W99, suggesting that
    attitudes towards technology are strongly linked to perceptions of algorithm fairness
    within this group. DISCRIM1_a_W99 (Views on discrimination) also shows a moderate
    correlation with ALGFAIR_W99, indicating that those who are concerned about discrimination
    also tend to be concerned about the fairness of algorithms. As for Cluster 2,
    a significant positive correlation exists between TECH1_W99 and ALGFAIR_W99, and
    between TECH1_W99 and DISCRIM1_a_W99, which suggests that in this cluster, individuals
    who are more optimistic or concerned about technology are also more likely to
    be concerned about algorithm fairness and discrimination. FACEREC3_c_W99 shows
    moderate to strong correlations with all other features, highlighting that concerns
    about racial bias in facial recognition are associated with more general concerns
    about technology, discrimination, and the fairness of algorithms. Fig. 11: Correlation
    matrices for the three clusters. Correlation matrix for each cluster. Full size
    image These correlations provide insights into the shared sentiments and concerns
    within each cluster, which can be especially valuable for understanding the nuances
    of each group’s attitudes toward technology and social issues. ANOVA We proceed
    with the ANOVA test, considering the gender variable. The ‘DISCRIM1_a_W99’ through
    ‘DISCRIM1_f_W99’ columns contain ordinal responses that are suitable for an ANOVA
    analysis when encoded numerically. These columns represent perceptions on various
    aspects of discrimination and can be encoded as follows: ‘Very well’ = 4, ‘Somewhat
    well’ = 3, ‘Not too well’ = 1, ‘Not at all well’ = 0. We select one of these columns,
    encode the responses, and then perform the ANOVA test considering gender. We encode
    the ‘DISCRIM1_a_W99’ responses. The responses in the ‘DISCRIM1_a_W99’ variable
    are investigated by comparing across the different gender groups as indicated
    by the ‘F_GENDER’ column. The result of the ANOVA test yielded an F-statistic
    of approximately 6.017 and a p-value of approximately 0.000439. The p-value is
    less than 0.05, which is the common threshold for statistical significance. This
    suggests that there are statistically significant differences in the perceptions
    of discrimination (as indicated by ‘DISCRIM1_a_W99’) across different gender groups
    in the dataset. The F-statistic from the ANOVA test indicates the ratio of the
    variance between the group means to the variance within the groups. A higher F-statistic
    generally suggests a greater disparity between the groups. In our case, the F-statistic
    of approximately 6.017 suggests that there is a difference between the means of
    the groups. The p-value tells us about the significance of the differences that
    we observe between the groups’ means. A p-value less than 0.05 (which is commonly
    used as a threshold for statistical significance) suggests that it is very unlikely
    that the observed differences in group means are due to random chance alone. In
    the context of our analysis, with a p-value of approximately 0.000439, we can
    conclude with high confidence that there is a significant difference in the way
    different genders perceive discrimination (‘DISCRIM1_a_W99’). To perform ANOVA
    considering the ‘TECH1_W99’ variable, which refers to the respondents’ views on
    whether technology has had a mostly positive or mostly negative effect on society,
    alongside the ideology of respondents, we perform an ANOVA to see if there is
    a significant difference in the views on technology’s impact on society across
    different ideological groups. The ANOVA test reflects the respondents’ views on
    technology’s impact on society, considering their ideological orientation (‘F_IDEO’).
    The results yield an F-statistic of approximately 15.674 and a p-value of approximately
    9.43 × 10−13. The p-value is much lower than the standard threshold of 0.05, indicating
    that there is a statistically significant difference in how respondents of different
    ideological backgrounds perceive the impact of technology on society. With such
    a low p-value, we can confidently reject the null hypothesis that there are no
    differences among the ideological groups’ opinions regarding technology’s impact.
    In other words, people’s political ideology appears to have a significant association
    with their perceptions of whether technology has a mostly positive or mostly negative
    effect on society. The results of the Tukey’s HSD test comparing the means of
    the ‘TECH1_W99_encoded’ variable across different ideological groups (‘F_IDEO’)
    provide the following insights: 1. Very Conservative vs. Conservative: There is
    a statistically significant difference, with a mean difference of 0.1078. The
    confidence interval does not include zero (0.0171 to 0.1984), indicating that
    Very Conservative respondents are slightly more likely to view technology’s impact
    on society as more negative compared to Conservative respondents. 2. Very Conservative
    vs. Moderate: The difference is larger and significant, with a mean difference
    of 0.1473. This suggests that Moderate respondents are more likely to view technology’s
    impact more positively than Very Conservative respondents. 3. Very Conservative
    vs. Liberal and Very Conservative vs. Very Liberal: Both comparisons show significant
    differences with Liberals and Very Liberals viewing technology’s impact even more
    positively than Very Conservative respondents. 4. Conservative vs. Moderate: There
    is no statistically significant difference in their views. 5. Conservative vs.
    Liberal and Conservative vs. Very Liberal: Conservatives view technology’s impact
    as more negative compared to Liberal and Very Liberal respondents, with significant
    mean differences. 6. Moderate vs. Liberal and Moderate vs. Very Liberal: Moderates
    are slightly more negative in their views compared to Liberals and Very Liberals,
    with significant mean differences. 7. Liberal vs. Very Liberal: There is no statistically
    significant difference between these two groups in their views of technology’s
    impact. From these results, it shows that ideology is a strong factor in how respondents
    perceive the impact of technology on society. Generally, as we move from Very
    Conservative to Very Liberal, the perception of technology’s impact becomes increasingly
    more positive. To perform ANOVA with the variables ‘CNCEXC_W99’ (which relate
    to a specific concept or concern/excitement), ‘F_IDEO’ (ideology), and income
    features ‘F_INC_SDT1’ and ‘F_INC_TIER2’, we grouped the variables and obtained
    the following results: for the ‘F_INC_SDT1’ income variable combined with ‘F_IDEO’:
    F-statistic: 4.9488, p-value: 3.63 × 10−24. For the ‘F_INC_TIER2’ income tier
    variable combined with ‘F_IDEO’: F-statistic: 13.3624, p-value: 1.13 × 10−31.
    The p-values are significantly below the 0.05 threshold, indicating that there
    are statistically significant differences in respondents’ excitement or concern
    regarding the issue in question across different combinations of income levels
    and ideological orientations. This suggests that both a person’s income and their
    ideological beliefs have a significant association with their level of excitement
    or concern regarding technology. Given the magnitude of the p-values, this effect
    is very unlikely to be due to chance. Predicting clusters The clustering process
    introduced an additional column in the dataset, which serves as the target variable
    for predictive analysis. To perform clustering, we utilized the original survey
    dataset. Following this, for the predictive modeling, we incorporated the newly
    added column and then executed a splitting procedure. This procedure divided the
    dataset into three subsets: training, testing, and validation. These subsets facilitate
    the development and evaluation of predictive models. The Random Forest classifier
    was trained on the training set (70%) and evaluated on both the testing (15%)
    and out-of-sample (15%) sets. The accuracy, F1 score, and AUC score results are
    showcased in Table 4. These results show that the classifier performs very well
    on both unseen parts of the original dataset, indicating good generalization.
    The F1 scores for the Random Forest classifier on the testing and out-of-sample
    sets are consistent with the high accuracy we observed, indicating a strong performance
    of the classifier in predicting the clusters across both sets. Table 4 Performance
    metrics. Full size table The F1 score considers both precision and recall, so
    these high values suggest that the classifier has a good balance between these
    metrics. These AUC scores are very close to 1, which indicates excellent performance
    in distinguishing between the different clusters. The model has strong classification
    capabilities across all classes in both the testing and out-of-sample sets. For
    the purpose of predicting clusters, we opted for the Random Forest algorithm due
    to its renowned efficacy and the impressive results it delivered for our classification
    issue. Although we also experimented with Linear Regression (F1 score 0.87 for
    out-of-sample set) and Decision Tree algorithms (F1 score 0.89 for out-of-sample
    set), the F1 score achieved with RF was superior to those obtained with these
    algorithms. Given the excellent performance of the Random Forest model, we chose
    not to explore additional advanced algorithms like XGBoost (XGB) or LightGBM (LGBM),
    which are known to potentially yield good results as well as our primary goal
    was to derive insights from the data and achieve accurate cluster predictions,
    which we successfully accomplished with Random Forest. Addressing the limitations
    and challenges of using Random Forest in predictive modeling is significant for
    maintaining a robust analytical approach. One of the primary concerns with Random
    Forest, as with any machine learning algorithm, is the risk of overfitting. This
    occurs when the model becomes too complex, capturing noise in the data rather
    than underlying patterns. It performs well on the training data but poorly on
    unseen data. To mitigate overfitting, we assessed the model’s performance on different
    subsets of the data, ensuring that it generalizes well to new data. Discussion
    Coming back to our research questions “How do different groups of individuals
    perceive AI technologies such as social media, facial recognition, and driverless
    cars, and what are their primary concerns, and how can these perceptions and concerns
    be predicted for new instances of individuals?”, we discovered that the primary
    worries among respondents centered on job loss and privacy breaches due to heightened
    surveillance. Conversely, the most significant source of enthusiasm was the potential
    for technology to enhance the quality of life. From cross-tabulation the following
    aspects are relevant. There is a correlation between higher education levels and
    a more positive perception of technology’s impact. This could suggest that education
    plays a role in how individuals understand and appreciate the complexities and
    benefits of technology. Education also seems to correlate with skepticism about
    the fairness of algorithms, with more educated respondents showing higher levels
    of doubt or uncertainty. This may be due to a greater awareness of the issues
    surrounding AI and technology among those with more education. Across the board,
    there is a notable level of uncertainty about the fairness of algorithms, which
    might reflect the public’s overall wariness of AI decision-making processes and
    the need for greater transparency and education on this topic. The younger respondents
    are more optimistic about the impact of technology, while there is a more cautious
    view among older respondents. This could be due to generational differences in
    technology adoption and familiarity. Concerns about technological developments
    increase with age. This could reflect the greater potential impact of technology
    on the livelihoods and lifestyles of older generations, or a more conservative
    approach to new developments. Skepticism about algorithmic fairness is prevalent
    across all age groups. However, younger respondents appear slightly more optimistic
    about the potential for algorithms to be fair. This age-based analysis of the
    survey data indicates that while there is general optimism about technology among
    the younger demographic, there is also a tangible concern about the implications
    of technological advancements, particularly regarding the fairness of algorithms.
    The data also suggests that education efforts around technology and AI might need
    to be tailored to address the specific concerns and levels of familiarity within
    different age groups. When analyzing technology concern levels based on gender,
    our findings indicated a modest gender disparity in perceptions towards technology.
    Women, in particular, tended to express greater concern about the adverse effects
    of technology and the fairness of algorithms. This variation in perspective could
    be attributed to different experiences with technology between genders, or a heightened
    awareness among women about issues such as algorithmic bias, which might impact
    women disproportionately. Racial differences in perceptions of technology are
    also evident. Asian or Asian-American respondents tend to have a more positive
    view of technology’s impact and future developments, while Black or African-American
    respondents show more concern. This may be influenced by socioeconomic factors
    or different lived experiences with technology. Across genders, there is a level
    of skepticism about the fairness of algorithms, with a higher proportion of women
    expressing doubt. When broken down by race, White respondents are the most skeptical,
    which might suggest a heightened awareness or concern about the ethical implications
    of AI within this demographic. Regarding religion and AI technologies, religious
    affiliation does not show a strong correlation with views on technology’s impact,
    but there is a tendency among non-religious groups (Atheists and Agnostics) to
    view technology more positively, while ideological orientation appears to influence
    perceptions of technology, with Moderates displaying a balanced view, and Liberals
    leaning towards a more positive outlook. Conservatives are more skeptical about
    the fairness of algorithms, which might reflect a broader caution towards technology
    governance and ethical considerations. Across both religious and ideological lines,
    there’s a notable skepticism about the fairness of algorithms, suggesting a common
    concern about AI ethics and the potential for bias. These findings indicate that
    demographic factors such as gender and race can influence perceptions of technology.
    Tailoring communication to address the unique concerns of different religious
    and ideological groups could foster greater understanding and acceptance of AI
    technologies. Regarding income and views on technology, higher-income respondents
    tend to have a more positive view of technology’s impact. This could be related
    to greater access to technology and its benefits among those with higher incomes.
    Concerns about future developments tend to decrease as income increases, with
    lower-income individuals more worried than their higher-income counterparts. Financial
    stability might influence the level of concern regarding the implications of technological
    changes. Across all income levels, there is skepticism about the fairness of algorithms.
    However, this skepticism is most pronounced among the middle and upper-income
    tiers, which might indicate a higher awareness of algorithmic biases and fairness
    issues among these groups. Income level is a significant factor in shaping individuals’
    perceptions of technology. Those with higher incomes may feel more secure and
    optimistic about the benefits of technology, while those with lower incomes may
    have more immediate concerns related to financial security and job displacement
    due to technological advancements. Addressing these disparities could be key to
    fostering a more inclusive technological future. Conclusion In this paper, we
    propose a descriptive-predictive hybrid model that includes a) EDA, checking and
    handling missing values, graphically visualizing, cross-tabulation to understand
    patterns in data and correlations between demographics and the main features,
    identifying the most significant features, b) clustering, analyzing clusters and
    respondents’ profile with PCA and ANOVA including statistical tests (Tukey’s HSD
    test), and finally c) classification to predict clusters. The proposed model was
    applied to a complex survey that consists of 5153 records and 126 columns that
    represent the encoded questions with their answers provided by people from U.S.A.
    Questions refer to the AI technology impact, concerns, and excitement regarding
    AI, positive and negative perceptions, fairness of algorithms, discrimination
    issues, face recognition, and social media usage of AI, and driverless cars. The
    findings from this survey (presented at length in the results and discussion sections)
    resonate with the current discourse on AI, where enthusiasm for technological
    progress contends with valid concerns over privacy, job displacement, and the
    opacity of AI systems. The data signifies a society that is at a crossroads with
    AI acceptance, eager to harness its potential while acutely aware of the need
    for ethical guardrails. The practical implications derived from the cluster analysis
    of survey respondents’ engagement with technology issues may inform targeted communication
    strategies, policy development, and educational initiatives. The following four
    practical applications are identified: (1) Communication strategies—understanding
    the distinct profiles of Cluster 0 (Moderates), Cluster 1 (Baseline), and Cluster
    2 (Intensives) allows for the development of customized messages that resonate
    with each group’s level of engagement and concern. For instance, awareness campaigns
    about AI’s impact could be nuanced to address the specific interests and concerns
    of each cluster, thereby increasing the relevance and effectiveness of the communication;
    (2) Policy development—insights into the different clusters guide policymakers
    in crafting policies and initiatives that reflect the diverse perspectives and
    engagement levels within the population. For example, policies aimed at addressing
    privacy concerns and job displacement due to AI could be prioritized, considering
    the strong engagement of Cluster 2 (Intensives) with these issues; (3) Educational
    programs - the findings of this analysis help in designing educational and outreach
    programs tailored to the varying levels of interest and concern among the clusters.
    Programs aimed at Cluster 2 (Intensives) might delve deeper into the nuances of
    AI technology and ethical considerations, while those targeting Cluster 1 (Baseline)
    might focus on raising awareness and providing basic information to increase engagement;
    (4) Market segmentation for technology products—companies developing AI-related
    products or services may use these insights for market segmentation, creating
    offerings that cater to the specific needs and concerns of each cluster. For example,
    products designed with enhanced privacy features might appeal more to Cluster
    2 (Intensives), while user-friendly and introductory AI tools could be more suitable
    for Cluster 1 (Baseline). One of the limitations of our study is that it draws
    its conclusions from a survey conducted among U.S. participants, potentially leading
    to a skewed representation of worldwide attitudes toward AI technology. Despite
    this limitation, it is important to acknowledge that the United States is at the
    forefront of AI technology development, making it a potentially representative
    sample for understanding individual opinions on this matter. The advanced state
    of AI technology in the U.S. offers insights into how perceptions and attitudes
    could evolve in other countries as they progress in AI adoption and integration.
    This context suggests that, while the study’s findings are primarily reflective
    of U.S. perspectives, they could still provide valuable foresight into emerging
    global trends and attitudes towards AI technology. This narrow demographic focus
    may limit the broader applicability of our findings. Thus, it is important to
    consider the consequences of relying on this U.S.-centric dataset, as it might
    not encompass the diverse perspectives present in a global context. To enhance
    the relevance and applicability of our research on a global scale, future studies
    will aim to incorporate a more varied demographic, thereby broadening the scope
    and improving the generalizability of the results. Data availability The data
    is available on the GitHub repository: https://github.com/simonavoprea/Human_enhancements.
    Notes https://www.naa.jp/en/airnarita/automation.html https://www.pewresearch.org/
    References Acheampong RA, Cugurullo F (2019) Capturing the behavioural determinants
    behind the adoption of autonomous vehicles: Conceptual frameworks and measurement
    models to predict public transport, sharing and ownership trends of self-driving
    cars. Transp Res Part F Traffic Psychol Behav 62:349–375. https://doi.org/10.1016/j.trf.2019.01.009
    Article   Google Scholar   Ahmad MF, Husin NAA, Ahmad ANA, Abdullah H, Wei CS,
    Nawi MNM (2022) Digital transformation: an exploring barriers and challenges practice
    of artificial intelligence in manufacturing firms in Malaysia. J Adv Res Appl
    Sci Eng Technol 29:110–117. https://doi.org/10.37934/araset.29.1.110117 Article   Google
    Scholar   Almarashda HAHA, Baba IB, Ramli AA, Memon AH, Rahman IA (2021) Human
    resource management and technology development in artificial intelligence adoption
    in the UAE energy sector. J Appl Eng Sci 11:69–76. https://doi.org/10.2478/jaes-2021-0010
    Article   Google Scholar   Amichai-Hamburger Y, Mor Y, Wellingstein T, Landesman
    T, Ophir Y (2020) The personal autonomous car: personality and the driverless
    car. Cyberpsychol Behav Soc Netw 23:242–245. https://doi.org/10.1089/cyber.2019.0544
    Article   PubMed   Google Scholar   Andrejevic M, Selwyn N (2020) Facial recognition
    technology in schools: critical questions and concerns. Learn Media Technol 45:115–128.
    https://doi.org/10.1080/17439884.2020.1686014 Article   Google Scholar   Bâra
    A, Oprea SV (2023) What makes electricity consumers change their behavior? Influence
    of attitude and perceived impact of DR programs on awareness. Kybernetes. https://doi.org/10.1108/K-01-2023-0032
    Binsaeed RH, Yousaf Z, Grigorescu A, Samoila A, Chitescu RI, Nassani AA (2023)
    Knowledge sharing key issue for digital technology and artificial intelligence
    adoption. Systems 11:316. https://doi.org/10.3390/systems11070316 Article   Google
    Scholar   Chen S, Qiu S, Li H, Zhang J, Wu X, Zeng W, Huang F (2023) An integrated
    model for predicting pupils’ acceptance of artificially intelligent robots as
    teachers. Educ Inf Technol 28:11631–11654. https://doi.org/10.1007/s10639-023-11601-2
    Article   Google Scholar   Chiu TKF, Meng H, Chai CS, King I, Wong S, Yam Y (2022)
    Creation and evaluation of a pretertiary artificial intelligence (AI) curriculum.
    IEEE Trans Educ 65:30–39. https://doi.org/10.1109/TE.2021.3085878 Article   Google
    Scholar   Choung H, David P, Ross A (2023) Trust in AI and its role in the acceptance
    of AI technologies. Int J Hum-Comput Interact 39:1727–1739. https://doi.org/10.1080/10447318.2022.2050543
    Article   Google Scholar   Chung KC, Chen CH, Tsai HH, Chuang YH (2021) Social
    media privacy management strategies: a SEM analysis of user privacy behaviors.
    Comput Commun 174:122–130. https://doi.org/10.1016/j.comcom.2021.04.012 Article   Google
    Scholar   Damerji H, Salimi A (2021) Mediating effect of use perceptions on technology
    readiness and adoption of artificial intelligence in accounting. Account Educ
    30:107–130. https://doi.org/10.1080/09639284.2021.1872035 Article   Google Scholar   Ding
    C, Li C, Xiong Z, Li Z, Liang Q (2024) Intelligent identification of moving trajectory
    of autonomous vehicle based on friction nano-generator. IEEE Trans Intell Trans
    Syst 1–8. https://doi.org/10.1109/TITS.2023.3303267 Fu Y, Li C, Yu FR, Luan TH,
    Zhao P (2023) An incentive mechanism of incorporating supervision game for federated
    learning in autonomous driving. IEEE Trans Intell Trans Syst 24:14800–14812. https://doi.org/10.1109/TITS.2023.3297996
    Article   Google Scholar   Giovanola B, Tiribelli S (2023) Beyond bias and discrimination:
    redefining the AI ethics principle of fairness in healthcare machine-learning
    algorithms. AI Soc 38:549–563. https://doi.org/10.1007/s00146-022-01455-6 Article   PubMed   Google
    Scholar   Jannat MKA, Islam MS, Yang SH, Liu H (2023) Efficient wi-fi-based human
    activity recognition using adaptive antenna elimination. IEEE Access 11:105440–105454.
    https://doi.org/10.1109/ACCESS.2023.3320069 Article   Google Scholar   Jiang Z,
    Xu C (2023) Policy incentives, government subsidies, and technological innovation
    in new energy vehicle enterprises: evidence from China. Energy Policy 177:113527.
    https://doi.org/10.1016/j.enpol.2023.113527 Article   Google Scholar   Karmakar
    G, Chowdhury A, Das R, Kamruzzaman J, Islam S (2021) Assessing trust level of
    a driverless car using deep learning. IEEE Trans Intell Trans Syst 22:4457–4466.
    https://doi.org/10.1109/TITS.2021.3059261 Article   Google Scholar   Kim KH, Hong
    KJ, Shin SD, Ro YS, Song KJ, Kim TH, Park JH (2022) How do people think about
    the implementation of speech and video recognition technology in emergency medical
    practice? PLoS ONE 17:e0275280. https://doi.org/10.1371/journal.pone.0275280 Article   CAS   PubMed   PubMed
    Central   Google Scholar   Kim PT (2022) Race-aware algorithms: fairness, nondiscrimination
    and affirmative action. Calif Law Rev. https://doi.org/10.15779/Z387P8TF1W Ko
    E, Kim H, Lee J (2021) Survey data analysis on intention to use shared mobility
    services. J Adv Trans 2021:1–10. https://doi.org/10.1155/2021/5585542 Article   Google
    Scholar   Kopalle PK, Gangwar M, Kaplan A, Ramachandran D, Reinartz W, Rindfleisch
    A (2022) Examining artificial intelligence (AI) technologies in marketing via
    a global lens: current trends and future research opportunities. Int J Res Market
    39:522–540. https://doi.org/10.1016/j.ijresmar.2021.11.002 Article   Google Scholar   Kosan
    E, Krois J, Wingenfeld K, Deuter CE, Gaudin R, Schwendicke F (2022) Patients’
    perspectives on artificial intelligence in dentistry: a controlled study. J Clin
    Med 11:2143. https://doi.org/10.3390/jcm11082143 Article   PubMed   PubMed Central   Google
    Scholar   Kumar A, Krishnamoorthy B, Bhattacharyya SS (2023) Machine learning
    and artificial intelligence-induced technostress in organizations: a study on
    automation-augmentation paradox with socio-technical systems as coping mechanisms.
    Int J Organ Anal. https://doi.org/10.1108/IJOA-01-2023-3581 Lambert J, Stevens
    M (2023) ChatGPT and generative AI technology: a mixed bag of concerns and new
    opportunities. Comput School. 1–25 https://doi.org/10.1080/07380569.2023.2256710
    Lepelaar M, Wahby A, Rossouw M, Nikitin L, Tibble K, Ryan PJ, Watson RB (2022)
    Sentiment analysis of social survey data for local city councils. J Sens Actuat
    Netw 11:7. https://doi.org/10.3390/jsan11010007 Article   Google Scholar   Liu
    X, Shi T, Zhou G, Liu M, Yin Z, Yin L, Zheng W (2023a) Emotion classification
    for short texts: an improved multi-label method. Humanit Soc Sci Commun. https://doi.org/10.1057/s41599-023-01816-6
    Liu X, Zhou G, Kong M, Yin Z, Li X, Yin L, Zheng W (2023b) Developing multi-labelled
    corpus of Twitter short texts: a semi-automatic method. Systems. https://doi.org/10.3390/systems11080390
    Mohr S, Kühl R (2021) Acceptance of artificial intelligence in German agriculture:
    an application of the technology acceptance model and the theory of planned behavior.
    Precis Agri 22:1816–1844. https://doi.org/10.1007/s11119-021-09814-x Article   Google
    Scholar   Nouraldeen RM (2023) The impact of technology readiness and use perceptions
    on students’ adoption of artificial intelligence: the moderating role of gender.
    Dev Learn Organ 37:7–10. https://doi.org/10.1108/DLO-07-2022-0133 Article   Google
    Scholar   Ongena YP, Haan M, Yakar D, Kwee TC (2020) Patients’ views on the implementation
    of artificial intelligence in radiology: development and validation of a standardized
    questionnaire. Eur Radiol 30:1033–1040. https://doi.org/10.1007/s00330-019-06486-0
    Article   PubMed   Google Scholar   Oprea S-V, Bâra A (2022) A measurement model
    for electricity Consumers’ awareness with covariance structure Analyses. A solid
    pillar for boosting demand response programs. Sustain Energy Technol Assess 53:102738.
    https://doi.org/10.1016/j.seta.2022.102738 Article   Google Scholar   Patrzyk
    S, Bielecki W, Woźniacka A (2022) A study of attitudes among polish dermatologists
    and dermatology trainees regarding modern technologies in medicine. Postepy Dermatologii
    i Alergologii 39:531–537. https://doi.org/10.5114/ada.2022.117738 Article   PubMed   PubMed
    Central   Google Scholar   Pillai R, Sivathanu B (2020) Adoption of artificial
    intelligence (AI) for talent acquisition in IT/ITeS organizations. Benchmarking
    27:2599–2629. https://doi.org/10.1108/BIJ-04-2020-0186 Article   Google Scholar   Qiu
    H, Li M, Bai B, Wang N, Li Y (2022) The impact of AI-enabled service attributes
    on service hospitableness: the role of employee physical and psychological workload.
    Int J Contemp Hosp Manag 34:1374–1398. https://doi.org/10.1108/IJCHM-08-2021-0960
    Article   Google Scholar   Querci I, Barbarossa C, Romani S, Ricotta F (2022)
    Explaining how algorithms work reduces consumers’ concerns regarding the collection
    of personal data and promotes AI technology adoption. Psychol Market 39:1888–1901.
    https://doi.org/10.1002/mar.21705 Article   Google Scholar   Rahman M, Ming TH,
    Baigh TA, Sarker M (2021) Adoption of artificial intelligence in banking services:
    an empirical analysis. Int J Emerg Market 18:4270–4300. https://doi.org/10.1108/IJOEM-06-2020-0724
    Article   Google Scholar   Rauf MA, Ashfaq M, Hasan R, Manju MA (2021) A comparative
    study on the impact of artificial intelligence on employment opportunities for
    university graduates in Germany and the Netherlands: AI opportunities and risks.
    Int J Environ Workplace Employment 6:185. https://doi.org/10.1504/IJEWE.2021.119679
    Article   Google Scholar   Rowthorn M (2019) How should autonomous vehicles make
    moral decisions? Machine ethics, artificial driving intelligence, and crash algorithms.
    Contemp Read Law Soc Just 11:9. https://doi.org/10.22381/CRLSJ11120191 Article   Google
    Scholar   Sampurna MTA, Handayani KD, Utomo MT, Angelika D, Etika R, Harianto
    A, Mapindra MP (2023) Determinants of neonatal deaths in Indonesia: a national
    survey data analysis of 10,838 newborns. Heliyon 9:e12980. https://doi.org/10.1016/j.heliyon.2023.e12980
    Article   PubMed   PubMed Central   Google Scholar   Shiwakoti N, Hu Q, Pang MK,
    Cheung TM, Xu Z, Jiang H (2022) Passengers’ perceptions and satisfaction with
    digital technology adopted by airlines during COVID-19 pandemic. Future Transp
    2:988–1009. https://doi.org/10.3390/futuretransp2040055 Article   Google Scholar   Smith
    M, Miller S (2022) The ethical application of biometric facial recognition technology.
    AI Soc 37:167–175. https://doi.org/10.1007/s00146-021-01199-9 Article   PubMed   Google
    Scholar   Tseng FH, Zeng JY, Cho HH, Yeh KH, Chen, CY (2024) Detecting adversarial
    examples of fake news via the neurons activation state. IEEE Trans Comput Soc
    Syst 1–11. https://doi.org/10.1109/TCSS.2023.3293718 van Buchem MM, Neve OM, Kant
    IMJ, Steyerberg EW, Boosman H, Hensen EF (2022) Analyzing patient experiences
    using natural language processing: development and validation of the artificial
    intelligence patient reported experience measure (AI-PREM). BMC Med Inf Decis
    Making 22:183. https://doi.org/10.1186/s12911-022-01923-5 Article   Google Scholar   van
    Noordt C, Misuraca G (2022) Exploratory insights on artificial intelligence for
    government in Europe. Soc Sci Comput Rev 40:426–444. https://doi.org/10.1177/0894439320980449
    Article   Google Scholar   Vorisek CN, Stellmach C, Mayer PJ, Klopfenstein SAI,
    Bures DM, Diehl A, Henningsen M (2023) Artificial intelligence bias in health
    care: web-based survey. J Med Intern Res 25:e41089. https://doi.org/10.2196/41089
    Article   Google Scholar   Wang J, Xing Z, Zhang R (2023a) AI technology application
    and employee responsibility. Humanit Soc Sci Commun https://doi.org/10.1057/s41599-023-01843-3
    Wang S, Sun Z, Chen Y (2023b) Effects of higher education institutes’ artificial
    intelligence capability on students’ self-efficacy, creativity and learning performance.
    Educ Inf Technol. https://doi.org/10.1007/s10639-022-11338-4 Warwas I, Podgórniak-Krzykacz
    A, Wiktorowicz J, Górniak J (2022) Demographic and generational determinants of
    Poles’ participation in the sharing economy: findings from a survey data analysis.
    PLoS ONE 17:e0265341. https://doi.org/10.1371/journal.pone.0265341 Article   CAS   PubMed   PubMed
    Central   Google Scholar   Xiao Z, Shu J, Jiang H, Min G, Chen H, Han Z (2023)
    Perception task offloading with collaborative computation for autonomous driving.
    IEEE J Selected Areas Commun 41:457–473. https://doi.org/10.1109/JSAC.2022.3227027
    Article   Google Scholar   Xuan PY, Fahumida MIF, Al Nazir Hussain MI, Jayathilake
    NT, Khobragade S, Soe HHK, Moe S (2023) Readiness towards artificial intelligence
    among undergraduate medical students in Malaysia. Educ Med J 15:49–60. https://doi.org/10.21315/eimj2023.15.2.4
    Article   Google Scholar   Yoon J, Andrews JE, Ward HL (2022) Perceptions on adopting
    artificial intelligence and related technologies in libraries: public and academic
    librarians in North America. Library Hi Tech 40:1893–1915. https://doi.org/10.1108/LHT-07-2021-0229
    Article   Google Scholar   Yu C, Helwig EJ (2022) The role of AI technology in
    prediction, diagnosis and treatment of colorectal cancer. Artif Intell Rev 55:323–343.
    https://doi.org/10.1007/s10462-021-10034-y Article   PubMed   Google Scholar   Yue
    W, Li C, Wang S, Xue N, Wu J (2023) Cooperative incident management in mixed traffic
    of CAVs and human-driven vehicles. IEEE Trans Intell Trans Syst 24:12462–12476.
    https://doi.org/10.1109/TITS.2023.3289983 Article   Google Scholar   Download
    references Acknowledgements This work was supported by a grant from the Ministry
    of Research, Innovation and Digitization, CNCS- UEFISCDI, project number PN-III-P4-PCE-2021-0334,
    within PNCDI III. Author information Authors and Affiliations Bucharest University
    of Economic Studies, Bucharest, Romania Simona-Vasilica Oprea & Adela Bâra Contributions
    Simona-Vasilica Oprea: methodology, formal analysis, investigation, writing—original
    draft, writing—review and editing, visualization, project administration. Adela
    Bâra: Conceptualization, validation, formal analysis, investigation, resources,
    data curation, writing—original draft, writing—review and editing, visualization,
    supervision. Corresponding author Correspondence to Simona-Vasilica Oprea. Ethics
    declarations Ethical approval Ethical approval was not required as the study did
    not involve designated human participants. Informed consent Informed consent was
    not required as this research did not involve designated human participants. Competing
    interests The authors declare no competing interests. Additional information Publisher’s
    note Springer Nature remains neutral with regard to jurisdictional claims in published
    maps and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article’s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article’s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Oprea, SV., Bâra,
    A. Exploring excitement counterbalanced by concerns towards AI technology using
    a descriptive-prescriptive data processing method. Humanit Soc Sci Commun 11,
    388 (2024). https://doi.org/10.1057/s41599-024-02926-5 Download citation Received
    13 November 2023 Accepted 05 March 2024 Published 11 March 2024 DOI https://doi.org/10.1057/s41599-024-02926-5
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Subjects Science, technology and society Sociology Download PDF Sections
    Figures References Abstract Introduction Literature review Methodology Results
    Discussion Conclusion Data availability Notes References Acknowledgements Author
    information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Humanities and Social Sciences Communications
    (Humanit Soc Sci Commun) ISSN 2662-9992 (online) About Nature Portfolio About
    us Press releases Press office Contact us Discover content Journals A-Z Articles
    by subject Protocol Exchange Nature Index Publishing policies Nature portfolio
    policies Open access Author & Researcher services Reprints & permissions Research
    data Language editing Scientific editing Nature Masterclasses Research Solutions
    Libraries & institutions Librarian service & tools Librarian portal Open research
    Recommend to library Advertising & partnerships Advertising Partnerships & Services
    Media kits Branded content Professional development Nature Careers Nature Conferences
    Regional websites Nature Africa Nature China Nature India Nature Italy Nature
    Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy
    choices/Manage cookies Legal notice Accessibility statement Terms & Conditions
    Your US state privacy rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Humanities and Social Sciences Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Exploring excitement counterbalanced by concerns towards AI technology using
    a descriptive-prescriptive data processing method
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Qiu M.
  - Wu Y.
  citation_count: '0'
  description: 'Although family caregiving is a demanding task, it has the potential
    to increase the significance and satisfaction of the caregiving process, while
    also enabling better response to the patient’s condition. This study aims to qualitatively
    explore, describe and analyze the experiences of family caregivers who are taking
    care of patients with leukemia. With an inductive thematic analysis, data collected
    from 997 online blog posts generated by 32 Chinese family caregivers of both adult
    and pediatric patients with leukemia were thoroughly examined. Through a detailed
    analysis of the blog posts, three main themes were identified: (1) family caregivers’
    (FCGs’) reactions to the leukemia diagnosis, (2) challenges faced in caregiving,
    and (3) effective coping strategies. The study reveals significant implications
    for improving the quality of life for family caregivers and enhancing health services
    for patients with leukemia. Gaining a comprehensive understanding of challenges
    and hardships faced by FCGs can shed light on enhancing the current medical services.
    A detailed account of the financial and psychological burdens experienced by FCGs
    offers valuable insights that can influence the decision-making of healthcare
    institutions and policymakers, facilitating the implementation of effective medical
    reforms. The findings underscore the importance of addressing the psycho-social
    needs of family caregivers, thereby suggesting the need of improved implementation
    of effective psycho-social support to enhance their overall quality of life. It
    is recommended that future research focus on developing more socially integrated
    approaches specifically tailored for family caregivers of patients with leukemia.
    While this study is primarily exploratory and descriptive, it forms a foundation
    for further investigation and understanding of the comprehensive family caregiving
    system for patients with leukemia.'
  doi: 10.1057/s41599-024-02830-y
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature humanities and social sciences communications articles article
    Article Open access Published: 08 March 2024 Understanding the experience of family
    caregivers of patients with leukemia: a qualitative analysis of online blogs Mengying
    Qiu & Yijin Wu   Humanities and Social Sciences Communications  11, Article number:
    374 (2024) Cite this article 423 Accesses 1 Altmetric Metrics Abstract Although
    family caregiving is a demanding task, it has the potential to increase the significance
    and satisfaction of the caregiving process, while also enabling better response
    to the patient’s condition. This study aims to qualitatively explore, describe
    and analyze the experiences of family caregivers who are taking care of patients
    with leukemia. With an inductive thematic analysis, data collected from 997 online
    blog posts generated by 32 Chinese family caregivers of both adult and pediatric
    patients with leukemia were thoroughly examined. Through a detailed analysis of
    the blog posts, three main themes were identified: (1) family caregivers’ (FCGs’)
    reactions to the leukemia diagnosis, (2) challenges faced in caregiving, and (3)
    effective coping strategies. The study reveals significant implications for improving
    the quality of life for family caregivers and enhancing health services for patients
    with leukemia. Gaining a comprehensive understanding of challenges and hardships
    faced by FCGs can shed light on enhancing the current medical services. A detailed
    account of the financial and psychological burdens experienced by FCGs offers
    valuable insights that can influence the decision-making of healthcare institutions
    and policymakers, facilitating the implementation of effective medical reforms.
    The findings underscore the importance of addressing the psycho-social needs of
    family caregivers, thereby suggesting the need of improved implementation of effective
    psycho-social support to enhance their overall quality of life. It is recommended
    that future research focus on developing more socially integrated approaches specifically
    tailored for family caregivers of patients with leukemia. While this study is
    primarily exploratory and descriptive, it forms a foundation for further investigation
    and understanding of the comprehensive family caregiving system for patients with
    leukemia. Similar content being viewed by others Adults who microdose psychedelics
    report health related motivations and lower levels of anxiety and depression compared
    to non-microdosers Article Open access 18 November 2021 Negativity drives online
    news consumption Article Open access 16 March 2023 Persistent interaction patterns
    across social media platforms and over time Article Open access 20 March 2024
    Introduction Leukemia is one of the most common cancers (Bray et al., 2018; H.
    Wang et al., 2016). In 2020, 474,519 new cases of leukemia were diagnosed worldwide,
    with an incidence rate of 2.5% and 311,594 new deaths of this disease were reported
    in the same year, with a mortality rate of 3% (Sung et al., 2021). Despite the
    high incidence and morbidity, there has been substantial advancement in the clinical
    treatment of leukemia during the past few decades regarding the patients’ five-year
    survival rate. The five-year survival rate of patients with leukemia was 19.6%
    in 2003–2005, and it increased to 25.4% in 2012–2015 in China (H. Wang et al.,
    2016). Family caregivers (FCGs) are seen as a crucial component of the survivorship
    process, which takes into account the physical, emotional, and financial aspects
    of cancer (National Cancer Institute, 2019). Although FCGs are important sources
    of support for patients, they must also deal with the burden and stress that come
    with providing care. Studies revealed that marital status (Arab et al., 2020)
    and psychological status (Grover et al., 2019) were contributing factors of the
    caregiver burden. Family function, social support, household income and the type
    of leukemia were important predicators of FCGs’ quality of life (C. Wang et al.,
    2020; Yu et al., 2018). Excessive burden and psychological stress would negatively
    affect family caregivers’ quality of life (Jia et al., 2015; Li et al., 2019;
    M. Wang et al., 2022; Y. Wang et al., 2021). However, those FCGs had insufficient
    resources to manage the stressors and underwent negative psychological, behavioral,
    and physiological effects, resulting in negative impacts on family relationships
    (Yucel et al., 2021). FCGs experienced great burden and psychological distress
    in caring for their family members diagnosed with leukemia. To offset these experiences,
    a growing number of FCGs have been sharing their experience of coping with illness
    online by keeping blogs. Online blog narratives can offer space for both patients
    and their family caregivers to share knowledge and information of the disease
    and its treatment, exchange information regarding various aspects of illness and
    feeling, and form new relationships regardless of temporal or geographic presence
    (Heilferty, 2009; Shah and Robinson, 2011). The blog provides a low-cost, worldwide,
    and immediate data collection method (Hookway, 2008; Hookway and Snee, 2017).
    As a consecutive, unique and rich source of data, blogs have been used by empirical
    health researchers for gathering information about caregivers’ experiences, perceptions,
    and feelings (E. Wilson et al., 2015). As a result, there has been an increasing
    number of studies based on blogs written by caregivers of geriatric patients with
    chronic diseases such as dementia (Anderson et al., 2017, 2019; Kannaley et al.,
    2019; McLennon et al., 2021). However, there is a lack of research on illness
    blogs written by family caregivers of patients with leukemia. Furthermore, prior
    research on FCGs of patients with leukemia tends to focus more on the negative
    effects of caregiving. Although caregiving is a demanding task, it can increase
    sense of meaning and satisfaction in the caregiving process and help FCGs better
    cope with the patient’s condition (Deshields et al., 2016; Papastavrou et al.,
    2012). Existing studies have primarily examined the quality of life and burden
    of FCGs, but there is a significant gap in understanding their emotions, feelings,
    concerns, needs, advice and other meaningful experiences among Chinese FCGs of
    patients with leukemia. Moreover, Chinese traditional culture and Confucianism
    prioritize family care over other forms of care (Ge et al., 2011; Tang and Chen,
    2002). In China, family members have a strong sense of obligation (Yu et al.,
    2018). The care provided by Chinese FCGs for patients with leukemia, both at home
    and in hospitals, far exceeds that delivered by nurses in Western countries (Ge
    et al., 2011; Tang and Chen, 2002; Yu et al., 2017, 2018). To bridge the research
    gap and effectively address the concerns and social needs of Chinese family caregivers
    of patients with leukemia, we deem it is crucial to gain a comprehensive understanding
    of this particular group and their caregiving experiences. Therefore, by describing
    and analyzing the narratives shared by Chinese family caregivers of patients with
    leukemia in their blogs, this study aims to delve into their unique caregiving
    experiences and, in turn, amplify the often-overlooked voices of this vulnerable
    group. The unsolicited nature of the blog itself eliminates the potential of recall
    bias (Anderson et al., 2017). This distinctive characteristic sets blogs apart
    from other more conventional methods, such as interview, focus group, or survey,
    commonly used to explore personal experiences with illness (Heilferty, 2009).
    Additionally, we refrained from providing a pre-set topics for the narratives
    to ensure that they accurately and genuinely reflect the real daily experience
    of leukemia caregiving, devoid of any researcher influence or bias (Kannaley et
    al., 2019). The research questions investigated in this study are listed as follows:
    How do FCGs of patients with leukemia describe their caregiving experience? What
    discernible patterns can be drawn from their narratives? Method Study design By
    focusing on how blog authors used their self-initiated narratives being created
    online, this study employs thematic analysis to examine the perspectives and experience
    of Chinese family caregivers of patients with leukemia. Data collection A purposive
    sampling method was employed to collect data from blog texts created online by
    Chinese family caregivers of patients with leukemia. Blog texts were selected
    from Red (Xiaohongshu in Chinese, literally “little red book”), a smartphone application
    oriented to posting, discovering and sharing experiences of daily life through
    a wide variety of media such as images, texts, videos, and livestreaming. The
    search was conducted on the application between January 4, 2022 and February 18,
    2022. Using key words such as “leukemia”, “leukemia diary”, “leukemia blog”, “fighting
    against leukemia”, and “fighting leukemia diary”, all researchers collected the
    samples eligible for the research objective. Embracing as much relevant content
    as possible, the data were collected based on the inclusion and exclusion criteria,
    as displayed in Table 1. Table 1 Inclusion and exclusion criteria of blog posts
    by family caregivers of patients with leukemia. Full size table Due to the smartphone-based
    nature of Red (Xiaohongshu) and its personalized search results, the search results
    page does not display a fixed quantity of information. Instead, it would dynamically
    display the relevant content based on the key words used for search. Thus, we
    could only access images on the application after conducting the initial search
    using the aforementioned keywords. To obtain eligible data, we took the following
    steps: (1) Initially, we captured the screenshots of search results pages and
    put them into a Microsoft word file. (2) Then, we searched the homepages of bloggers
    by clicking their publicly displayed user names, which were gleaned from the file
    of captured screens. Consequently, we identified a total of 53 bloggers. (3) Next,
    since the targeted group was the family caregiver of patients with leukemia, we
    perused the blog contents to exclude 21 bloggers who were not family caregivers
    of patients with leukemia. This left us with a final sample of 32 bloggers. (4)
    Finally, following the inclusion and exclusion criteria, we identified 997 blog
    texts from the 32 bloggers. Ethical issues Institutional review board approval
    was requested and the protocol was exempted because informed consent is not required
    when it applies to public and published material (Bruckman, 2002; Hookway and
    Snee, 2017). There is a growing consensus among researchers that if Internet data
    are freely and publicly accessible, then they can be used for considered research
    without prior approval (Anderson et al., 2017). To protect confidentiality and
    ensure anonymity, we removed all personal identifiers such as real or nick names,
    locations of the patients with leukemia and their family caregivers within the
    blog posts. Each blog author was coded using letters and numbers such as Bloggers
    1, 2, 3. Data analysis The data were analyzed with a thematic analysis method.
    Thematic analysis is a valuable approach for analyzing naturally occurring data
    (Clarke and Braun, 2018). Its primary objective is to employ a data-driven approach
    to understand events, experiences, opinions, viewpoints, and other salient aspects
    from the participants’ own perspective and determine what matters to them (Kiyimba
    et al., 2019). In the data pre-processing stage, all data from selected blogs
    were saved as html files along with the capture of the “about me” pages of bloggers.
    Then, data from the html files were saved as a single file in Microsoft word.
    The text of all blog postings was analyzed according to the six phases of thematic
    analysis under Braun and Clarke’s guidelines (Braun and Clarke, 2006, 2022). In
    phase 1, all researchers familiarized themselves with the data by perusing the
    whole dataset. Phase 2 was the coding process, in which each researcher worked
    independently to embark on line-by-line open coding of basic units in the data.
    Due to the unsolicited nature of blog narratives, the coding process was driven
    by an inductive orientation to data (Braun and Clarke, 2022). All researchers
    then compared their code lists to identify similar codes, and discrepancies were
    discussed and revised until consensus was reached to guarantee consistency. No
    more analysis was furthered until new codes were identified. In phase 3, codes
    were grouped into meaningful themes. In thematic analysis approach, themes are
    defined by meaning-unity and conceptual coherence, and each theme has its own
    distinct central organizing concept (Braun et al., 2014), which might be evidenced
    at a semantic or latent level (Braun and Clarke, 2006). In phase 4, all researchers
    checked the candidate themes against the coded information and the whole dataset,
    to confirm that they recounted a persuading and coherent story regarding the data
    and the research objective. In phase 5, all researchers fine-tuned the analysis
    by sorting out the extension of each theme, making sure that each theme was distinct
    from the others and was based on a compelling central idea. Themes were thus further
    developed, which sometimes involved them being split, combined, or discarded.
    The last phase was an integral phase, requiring the writing-up of a thematic map
    for reporting purposes as well as a final assessment of the codes and themes.
    Since the process of thematic analysis is not strictly linear, but recursive (Braun
    and Clarke, 2006), the codes and themes were iteratively evaluated and frequently
    discussed throughout the analytical process to compare the findings. Rigor Credibility,
    dependability, confirmability, and transferability were addressed to ensure the
    study was conducted in a rigorous manner (Houghton et al., 2013; Lincoln and Guba,
    1985). Credibility was controlled by peer debriefing (Lincoln and Guba, 1985)
    across the processes of initial coding, theme identifying, theme reviewing, theme
    naming and writing up (Nowell et al. 2017). Audit trail and reflexivity are used
    to determine dependability and confirmability (Houghton et al., 2013). Audit trail
    was maintained through a detailed and comprehensive chronology of research activities
    to ensure the analytical process was coherent and transparent, for example, taking
    analytical memos related to the contextual background of the blog data and notes
    of the themes (Morrow, 2005). Reflexivity was maintained through keeping a reflective
    journal that included the researchers’ views on the daily logistics of the study,
    and the impetus and rationale for all methodological decisions (Houghton et al.,
    2013; Nowell et al., 2017). Transferability was ensured by the purposive sampling
    with inclusion and exclusion criteria to obtain a homogeneous sample. Results
    Demographic profile of family caregivers and patients with leukemia All researchers
    independently examined the posts from the smartphone application Red (Xiaohongshu)
    and identified 997 posts from 32 bloggers that met the selection criteria. The
    posts included in the study were authored by 31 women and one man. Although demographic
    information is not compulsory for bloggers when signing up for their accounts
    of social media, some of the information can be gleaned from the “about me” pages
    or within the post texts. Table 2 reveals the demographic characteristics of family
    caregivers of patients with leukemia. Information with respect to the relationship
    of family caregivers to the patients with leukemia whom they supported can be
    found in Table 2. Table 3 displays the demographic characteristics of patients
    with leukemia. Table 2 Demographic characteristics of family caregivers of patients
    with leukemia. Full size table Table 3 Demographic characteristics of patients
    with leukemia. Full size table Qualitative findings The key themes derived from
    inductive thematic analysis centered on the caregiving experience of family caregivers
    of patients with leukemia. The themes identified were (1) FCGs’ reactions to the
    leukemia diagnosis, (2) challenges faced in caregiving, and (3) effective coping
    strategies, as displayed in Table 4. Table 4 Key themes and subthemes derived
    from family caregivers’ blogs. Full size table Theme 1: FCGs’ reactions to the
    leukemia diagnosis In the early stages of their caregiving journey, FCGs experienced
    three distinct emotional responses when faced with their care recipients’ leukemia
    diagnosis. Initially, they found it hard to come to terms with the diagnosis and
    accept it as reality. Then, they engaged in self-reflection to understand the
    reasons behind their care recipients’ leukemia. Finally, they reached a point
    of acceptance. Failure to confront the reality of being diagnosed The initial
    difficulty in accepting the diagnosis had a profound impact on FCGs, leaving them
    feeling desperate and struggling to face the reality. The failure to confront
    the reality contributed to FCGs’ experience of emotional breakdown and trauma
    during the early stage of the diagnosis. FCGs described the diagnosis as “nightmare”
    or “thunderstorm” [Blogger 1]. Blogger 10 recalled “I couldn’t accept the reality
    and felt it was the work of the devil.” Blogger 17 wrote, “Even after 47 days,
    I still couldn’t accept the reality.” Reflection upon causes of leukemia To gain
    a deeper understanding of the health status of their loved ones and improve their
    prevention and management efforts, FCGs tended to self-examine the possible causes
    of leukemia. Relentless in their pursuit of the possible causes, many FCGs listed
    a series of factors, including the newly renovated houses, new furniture, water
    supplies, plush toys, green plants and flowers, food, nail polish, hair dye and
    X-ray, but still had no idea. As Blogger 15 described, “We didn’t live in a new
    house after the birth of my elder son, so it was not due to formaldehyde or the
    alike. But I still have no idea, so I really don’t know why it is my son.” Blogger
    23 even blamed herself, “Which step was not done properly, making you get infected?”
    A handful of FCGs managed to identify gene mutation as the exact cause after consulting
    with the physicians. For example, Blogger 1, a family caregiver of a patient with
    Acute Lymphoblastic Leukemia Type B, attributed the condition to “mutation in
    E2A-pBX1 gene.” Acceptance of the reality FCGs, regardless of their knowledge
    of leukemia causes, eventually experienced a shift in their mindset from resistance
    to acceptance of the harsh reality. As Blogger 3 said, “though it was like a dream,
    I had to accept the fact.” Blogger 29 mentioned, “The initial sorrow and anger
    turned into acceptance of the fact, and we tried to persuade each other that everything
    would be fine!” Blogger 6 embraced the reality without resentment, “I become more
    open to communicate with fellow patients in the ward, I believe it’s a different
    way of approaching life.” Theme 2: challenges faced in caregiving The rapid acceptance
    of the diagnosis was closely intertwined with the prompt assumption of new responsibilities.
    FCGs took more proactive steps to assist their care recipients in managing and
    coordinating medical care. However, the journey of FCGs’ caregiving life was marked
    with numerous trials and tribulations. Difficulties in seeking medical services,
    a desperate shortage of support, the deteriorating health condition of FCGs and
    the adversities brought by COVID-19 pandemic were persistent challenges for caregiving.
    In addition, FCGs also experienced excessive financial and psychological burdens
    and endured considerable pressure as a result of their numerous caregiving responsibilities
    throughout the course of leukemia. Difficulties in seeking medical treatment FCGs
    experienced untold and unparalleled hardships in seeking affordable medical treatment.
    The process of contacting the hospital was not smooth. “Good baby, I heard from
    the doctor that there were two rehabilitation hospitals in City H that would accept
    seriously ill babies. I contacted one of them, but it would not accept babies
    as young as you, and there was another one whose phone had been out of order”
    [Blogger 23]. On certain occasions, FCGs had to travel long distance for better
    treatment provided by hospitals with higher qualifications, as Blogger 32 related,
    “…luckily, I can still surf the Internet, ask around, and do my best to send you
    here! From City Z, to City J, from City J, to City L, and finally to the last
    stop for leukemia – Hospital Y.” Consequently, referrals and transfers from one
    place to another were not rare. Seeking medical treatment in different locations
    had further compounded the problems of medical insurance reimbursement, as Blogger
    11 articulated, “When I consulted medical insurance issue over the phone, I was
    asked to do out-of-pocket payment first and then go back to my home city for reimbursement.
    Not to say that the reimbursement amount in my home city will be less, but the
    point is that the waiting time is too long, at least two to three months. We have
    already spent 490,000 yuan (67,855 USD) on hospitalization, and we look forward
    to quicker reimbursement of this sum of money so as to pay for the ensuing treatment.”
    Desperate shortage of support A desperate lack of emotional and social support
    was observed from FCGs’ online narratives. Some FCGs reported a lack of emotional
    support from family members because family members showed no concern. Feeling
    abandoned, FCGs told their tales of woes where there was a lack of communication
    and mutual understanding and they nursed grievance towards such indifference.
    “The elders of the family don’t care ……not a word of comfort, but they say it’s
    none of their business…their attitude is breaking me down, and our wardmates are
    supported by their families, but we have to fight alone. When it comes to the
    so-called family it’s really bitterly disappointing” [Blogger 12]. In some cases,
    in-laws of FCGs even tried to persuade FCGs to give up treatment, as Blogger 19
    reported, “My child’s grandparents didn’t support for further treatment, my father-in-law
    said we couldn’t let a child affect the life of their later years.” A lack of
    support from the patient’s school was also observed, as Blogger 24 remarked, “To
    treat my son, we lost all our fortune, and my child was suffering every day, but
    the school didn’t show any concern, just talking about the procedures of withdrawal
    from school.” FCGs’ deteriorating health condition Long-term caregiving required
    intensive and attentive nursing, bringing FCGs physical pains, fatigue and sleep
    deprivation, which had worsened their physical health condition. Blogger 22 articulated
    how she was debilitated by the care recipient’s suffering, “These days have been
    really tough. My kid has been throwing tantrums and crying, we had quite enough
    of this. My heart has been feeling uncomfortable for the past two days… my husband
    has let me go home to have a good rest. While sitting in the car, I suddenly felt
    a terrible pain in my chest.” Blogger 13 wrote how she suffered from insomnia,
    “Now it’s 2:05 am, I’ve been holding you like this for three nights. On average,
    I can only manage to have about two hours of sleep in sporadic increments within
    a 24-h period. I’m already exhausted, on the verge of going crazy.” Adversities
    brought by COVID-19 pandemic The COVID-19 pandemic led to a shortage of medical
    resources, bringing unprecedentedly formidable challenges. As Blogger 15 wrote,
    “Because of the pandemic, there had been a blood bank shortage, causing a long
    delay for blood transfusion, but it couldn’t be waited for a child who needed
    to be saved.” Blogger 11 shared the experience of how her husband managed to buy
    ice packs to reduce the patient’s fever, “Ice packs were out of stock at hospital.
    Due to the pandemic, it was so difficult to buy ice packs that Papa went to three
    pharmacies to get only three.” During the pandemic, the paper report of nucleic
    acid within 48 h was required for hospitalization. FCGs stated that treatment
    thus became more time-consuming than ever before, leading to the results of “delaying
    the transplant” [Blogger 28] and “keeping families from reuniting” [Blogger 24].
    Heavy financial burden As the treatment course progressed, the medical expenses,
    including costs of examination, hospitalization, chemotherapy, transplantation,
    and out-of-town travel, became ever higher or even unaffordable for FCGs. Particularly,
    they noted that Caspofungin, Propecia, PICC membranes, and targeted drugs were
    self-funded and could not be reimbursed by health insurance. “The targeted drug
    is just launched in April this year, not yet included in the medical insurance,
    it’s a self-funded drug, one dose for 70,000 yuan (9791 USD) and can only be taken
    for 21 days. My kid would take it for at least one year. Then, the drug alone
    would cost 1.2 million yuan (168,161 USD). Besides, if we calculate the expense
    on transplant rejection, maintenance treatment, intravenous immunoglobulin (IVIG)
    and multiple examinations, we would be homeless under considerable strain” [Blogger
    17]. Pressure to raise money for medical bills had left FCGs and their families
    in straitened circumstances. Some of them even sold their fixed assets. Being
    almost penniless, Blogger 24 described her added stress, “every day I’m worrying
    about how to raise all the deposit, my only car is also sold… the large sum of
    money for transplant surgery has not been settled. Then, I consider loans, to
    get mortgage on the house, but the only house of mine has no real estate license,
    how should I settle the matter?” Overwhelming psychological burden Enduring persistent
    anxieties and struggling with mental conflicts, FCGs were overwhelmed by a variety
    of psychological burdens. FCGs were particularly concerned about the patient’s
    physical health condition, including the ongoing fear of heightened infection
    risk, adverse physical reactions, intolerable pain, and leukemia relapse. On life-threatening
    situations, FCGs even dreaded their care recipients “would die someday” [Bloggers
    7, 23, 27]. FCGs also worried about the psychological health of the patients.
    Meanwhile, FCGs hoped their care recipients could feel more comfortable so as
    to improve the quality of caregiving, but on the other hand, they considered to
    submit to the demanding treatment requirements which would render the patients
    uncomfortable: “When I see you becoming better, I desperately want to make every
    effort to give you better treatment so that you can stay by my side, but when
    I see you in immense suffering, I would break down and want to make you suffer
    less” [Blogger 23]. FCGs expressed a growing concern about explicitly showing
    their suppressed negative emotions to others. As a result, FCGs opted to internally
    keep their sorrows hidden in order to protect others from distress. “Every day,
    I’m preoccupied with caring for her in the hospital…I just pretend to be relaxed
    when I chat with other patients, I’m constantly anxious about releasing negative
    feelings” [Blogger 11]. Pressure from multiple caregiving roles In addition to
    work, FCGs noted that they juggled the family commitments of caring between the
    patients with leukemia and other family members. With intense caregiving responsibility,
    FCGs’ life became more stressful. Apart from taking care of the patient, Blogger
    15 depicted, “Being overloaded, we have pressure to take care of aging parents,
    meanwhile, we have to live our own life, but we can never exhaust our family’s
    fortune.” Seventeen days after her younger son’s transplant, Blogger 24 recalled
    how anxious she was to go home to see her elder son, “I just wish for my baby
    to get better soon so that I can go back home to take care of your elder brother.”
    When other family members were diagnosed with refractory diseases, for example,
    uremia, cerebral infarction, bladder cancer, prostate cancer, high blood pressure,
    and heart disease, FCGs felt they were under considerable role strain. “We still
    don’t have enough money for transplant surgery……Dad has uremia and can’t take
    care of himself, grandpa and grandma have cerebral infarction and can’t take care
    of themselves, either. You have misfortune and so do I, all the pressure is on
    me, what should I do?” [Blogger 32]. Theme 3: effective coping strategies This
    theme captured how FCGs navigated the barriers and adapted to the strains to effectively
    cope with caregiving. They strove to reduce their financial and psychological
    burdens. Despite constant exposure to hardships, FCGs demonstrated resilience
    by adapting positively, expressing their specific needs and actively seeking support.
    Meanwhile, they also received social support from various sources. Efforts to
    reduce financial burden With rigor in their control of expenses on living and
    medicine, FCGs exerted themselves to the utmost to reduce financial burden. “To
    be honest, I seldom eat fruit now, the food price here is really high, so I’ll
    save whenever possible” [Blogger 11]. Quite a few FCGs revealed that they had
    purchased health insurance for their care recipients and this had remarkably alleviated
    their financial burden. As Blogger 28 described, “Had it not been for the commercial
    insurance purchased in January, we really wouldn’t have been able to approach
    leukemia in a calm mindset as a normal family! Anyhow, although my husband suffered
    from this disease, the combination of medical insurance, commercial insurance
    and critical illness compensation basically will not add too much to my own burden.”
    By means of communicating effectively with healthcare providers, FCGs endeavored
    to optimize options for treatment, contributing to saving medical expenses. On
    the 15th day after transplant, Blogger 24 wrote, “Yesterday, the number of cells
    just increased a little bit, so I contacted the doctor. After negotiation, I quickly
    completed the discharge procedures. By doing so, I could save the expense for
    one day.” Efforts to reduce psychological burden FCGs attempted to buffer the
    stress on psychological health on their own and with others’ assistance. Because
    of the pains brought by treatments, many young patients with leukemia were often
    crying in anguish, leading to FCGs’ emotional turmoil. Through cathartic experiences,
    either writing these experiences in words and posting them online publicly or
    bursting into tears privately, FCGs felt themselves freed from the emotional burden.
    Turning negative feelings into words to share them online via the smartphone-based
    application Red (Xiaohongshu) was a common way to vent grievances, as Blogger
    23 wrote in the blog, “But baby do you know, every day when you have gastrostomy
    and gas incision to change gauze, the pains are unbearable not just to you but
    also to me. I’m sorry, baby, when I was changing the gauze for you today, my emotions
    came to a head, I hid in the bathroom and turned the tap, burying my head into
    the running water, weeping bitter tears uncontrollably.” Communicating with others
    was reported to be an effective way to handle the emotional disruptions, from
    which FCGs found inner peace. “For these days in the hospital, I have already
    started to relieve. After talking to mothers of other patients here, I feel as
    if this disease is not as horrible as imagined” [Blogger 10]. Spiritual practice
    Relying on spiritual practices such as maintaining religious beliefs, reflection
    on past and prospects for future, FCGs gained will power for further treatment.
    FCGs and their families often prayed to Buddha or other gods in temples, in the
    hope that their care recipients would be blessed with excellent health. “When
    I became energetic, I changed my name. In the temple, I was given the Dharma name
    C.Y., meaning ‘long and good’. I believe, to change the name is to change the
    fate. Mom will promise you, praying for your peace and health” [Blogger 17]. FCGs’
    vivid recollections revived at the sight of familiar places or photos and possessions
    of patients with leukemia, which evoked the good times they spent together and
    brought them a joyful mood. Gazing at the photos taken before being diagnosed,
    Blogger 8 noted, “I really love you. I miss the scenes when I was busy cooking
    in the kitchen and you were waiting for me.” Sustaining a sense of hope, FCGs
    also looked forward to the promising future of care recipients, imagining the
    days “when you (the patient) would get well and come back home to have a reunion”
    [Blogger 5]. Cautious optimism about caring and life Optimism was expressed by
    FCGs as actively learning nursing skills, finding the meaning of life through
    adversity and encouraging the care recipients. Inasmuch as patients with leukemia
    were exceedingly vulnerable to various infections, FCGs took extreme caution in
    caring with scrupulous attention to detail. The desire to enhance the quality
    of daily care stimulated FCGs’ motivation to acquire specialized knowledge of
    nursing and pharmacology regarding leukemia, which was obtained online, or gained
    from health professionals or wardmates. Blogger 23 wrote how she taught herself
    the nursing skills, “In the past two days, I also learned much about nursing skills
    of gastrostomy, tracheotomy tube and long-term bed-ridden care, I believe I’ll
    take good care of you.” Blogger 24 described how she meticulously attended to
    the patient’s diet based on the strict requirements of the hospital, “After transplant,
    I have a particular fear for my baby’s dietary issues. I’m more than careful in
    high-temperature sterilization of the bowls and chopsticks which will be brought
    to the hospital.” FCGs also plucked up themselves in navigating the meaning of
    life from the bittersweetness of their caregiving experience. They believed everything
    would be fine if they took an optimistic outlook towards life. Blogger 4 emphasized
    the significance of optimism, “If parents have prolonged self-doubt, it will instead
    affect the child’s emotions …and the effectiveness of the treatment. So, what
    parents need to do is have better self-management of their own emotions, and live
    a positive life.” FCGs not only nerved themselves to live positively but also
    encouraged the care recipients to persevere in grappling with leukemia. Blogger
    9 expressed her encouragement to her care recipient, “Baby, you are cherished
    by so many people, hang in there!” Proactively reporting needs and seeking support
    With various difficulties and excessive burdens, a sense of helplessness was often
    expressed by FCGs when delivering care due to their limited knowledge of therapeutic
    techniques and their inability to plan or make informed decisions. FCGs wrote
    about the challenges regarding the knowledge and sought support to address their
    needs. Primarily, they reported informational needs about treatment, including
    information of transplant, nursing, cross-province treatment, chemotherapy, and
    prevention for infection. They often sought medical advice from their blog readers.
    For example, Blogger 5 asked other blog users for advice of methods to lower a
    fever, “During myelosuppression period, it’s impossible to avoid fever or diarrhea.
    After medication, there’s no sign of the fever going down. It’s difficult for
    me to let such a little baby take montmorillonite powder. Do other mommies have
    any brilliant ideas?” FCGs also expressed emotional needs. “I’m too sentimental
    and fragile. I hope I could have a shoulder to lean on” [Blogger 1]. In an effort
    to relieve the financial burden, FCGs often posted their fund-raising needs via
    social networks, including smartphone applications like Red (Xiaohongshu), WeChat,
    Alipay, and fund-raising platforms such as Shuidichou and Red Cross. “I put the
    donation channel on the top of my homepage. Thank you all for a small favor for
    saving such an ordinary life” [Blogger 8]. Support from family, friends and peers
    The tireless efforts of FCGs played a crucial role in overcoming barriers and
    relieving stress. In addition, seeking financial, emotional, and technical support
    from wider social context proved to be a significant contributing factor in overcoming
    these challenges. FCGs received support from various sources, including family
    members, friends, peers, health care professionals, strangers, and cybercitizens.
    FCGs’ family members shared the burden of caregiving, preparing and delivering
    meals, purchasing drugs, and providing assistance in moving things around between
    home and hospital. Often, family members took turns to attend to the patients.
    Some even stopped work temporarily to share the caregiving responsibility. Many
    FCGs received financial support primarily from their parents or parents-in-law.
    “My father-in-law has always put me at ease. I fight against the disease in the
    frontline together with my partner and my father-in-law would provide funding
    for us” [Blogger 31]. Emotionally, FCGs were backed up by their spouses or partners
    via mutual understanding and support, as Blogger 14 related, “The sadness in my
    heart defies description. As a mom, my husband and I rely on mutual encouragement
    and support.” Friends and peers were the source of emotional support as well.
    Blogger 23 described how she was heartened by the cheerful words of one wardmate,
    “She said to me, you should have the conviction that your baby will be fine. Because
    of the special bond between mother and child, as long as you have the conviction,
    he’ll definitely be fine.” Humanistic care from healthcare providers Out of compassion,
    by offering humanistic care, many healthcare providers would make things easier
    for both patients and FCGs whenever possible. According to FCGs, physicians “helped
    to reach out to rehabilitation hospitals and gave advice on rehabilitation” [Blogger
    23], and “helped to reserve beds when healthcare resources were limited” [Blogger
    21]. FCGs also received technical support from nurses who sacrificed their own
    breaks to teach skills about nursing. “The nurse said she would teach me some
    nursing skills tomorrow when she could’ve been off from work and she also told
    me to contact her if I had any questions after transferal” [Blogger 23]. The succor
    healthcare professionals offered was not limited to technical support, but also
    encouraging words and comforts. “The doctor said to other doctors, ‘But I really
    hope he’ll get better soon’. Before I showed my gratitude, I’d already been moved
    to tears welling up” [Blogger 23]. Blogger 7 described how she was encouraged
    by the consolation of a physician, “It turned out to be a relapse of leukemia,
    and I was just dumbfounded, I kept asking the doctor the reason of relapse to
    see if I hadn’t taken good care of my kid. I really blamed myself. Then, I was
    comforted by the doctor and I was able to pick myself up again.” Goodwill from
    strangers and cybercitizens Some strangers and cybercitizens offered timely help
    to treat the disease. FCGs often felt moved or touched by these people’s charitable
    deeds such as rendering free goods or services. Blogger 3 received “benefit of
    free cleaning offered by housekeeping personnel.” Blogger 1 reported “receiving
    a discount of the grocery offered by a vendor.” Blogger 11 shared the experience
    of being helped by staff from furniture removal carriers, “After moving, he helped
    us to search for house till late at night. We wanted to pay him higher service
    fee but was declined. I was so touched.” FCGs also felt indebted to financial
    support offered by other blog users. “Quite a few mothers added me as a contact
    on the app Red to make donations” [Blogger 8]. Discussion In this study, we make
    a significant contribution to the existing body of knowledge by employing qualitative
    thematic analysis of unsolicited online narratives. This approach allows for a
    detailed examination of the nuanced experience encountered by family caregivers
    of both adult and pediatric persons with leukemia. Our qualitative analysis highlights
    the diversity of family caregivers’ perspectives, capturing the complexity of
    family caregiving for patients with leukemia. It is found that in the initial
    stage of their caregiving journey, FCGs often struggled to confront the reality
    of their care recipients’ initial diagnosis, and it was the same with FCGs who
    had experienced their care recipients’ leukemia relapse. This news could bring
    about severe emotional shock for FCGs. The initial devastation found in our study
    corroborates the results that the news of being diagnosed with leukemia can be
    a difficult and traumatic experience for family caregivers (Peterson et al., 2020;
    M. Wang et al., 2022; M. Wilson et al., 2009). This is due to the life-threatening
    nature of leukemia (Bozo et al., 2010). Notably, our study has made an important
    distinction from prior research by highlighting a unique aspect of FCGs’ initial
    response to the leukemia diagnosis. We have identified that FCGs also contemplated
    the potential causes of leukemia. Their eagerness to understand the exact causes
    of leukemia may arise from their concerns regarding future disease management,
    preventive awareness and genetic counseling, and even family planning considerations
    among young parent bloggers. These findings have implications for designing effective
    educational interventions that provide information about the causes of leukemia.
    The heavy financial burden reported by FCGs of patients with leukemia found in
    this study is consistent with findings from prior studies (Arab et al., 2020;
    Grover et al., 2019; Yucel et al., 2021; Zeidan et al., 2016). Patients with leukemia
    need prolonged treatment and intensive care, leading to frequent hospitalizations
    that further increase the burden faced by families, potentially disrupting the
    overall family equilibrium (Panganiban-Corales and Medina, 2011; Perricone et
    al., 2012). As the results show, out-of-pocket (OOP) medicines also placed an
    undue burden on families, which aligns with the finding from previous research
    that the share of OOPs still accounts for more than 30% of total health expenditure
    (Mao et al., 2017). In China, the drugs required to increase the survival rates
    of patients with leukemia are frequently imported and necessitate long-term usage.
    However, these medications are not covered by medical insurance (Kong et al.,
    2022). Low reimbursement rates and coverage further increased OOP expenses for
    caregivers, substantiating the conclusions drawn from previous research (Zhang
    and Zhang, 2022). It is our expectation that the government will exert more efforts
    to provide significant contribution to reducing the OOP medical payments for patients
    with leukemia. Another significant finding pertains to the inconvenience caused
    by the interregional medical insurance reimbursement. Obstacles such as reduced
    reimbursement rates, ineligibility for immediate reimbursement, and prolonged
    waiting time as a result of cross-regional reimbursement hindered medical efficiency
    for patients with leukemia, which aligns with the finding of a previous study
    (Zhan et al., 2022). The establishment of instant reimbursement mechanism for
    cross-regional medical services occurred gradually between 2009 and 2014 (Zhang
    and Zhang, 2022). It is worth noting that due to variations in healthcare insurance
    systems across different regions in China, when an individual participates in
    medical insurance in one province but seeks medical services in another, it necessitates
    interregional medical insurance reimbursement. This process entails coordination
    and information sharing between different regions and encompasses two aspects:
    (1) reimbursement for hospitalization expenses in a different province, and (2)
    reimbursement for outpatient expenses in a different province. Families of patients
    with leukemia often travel considerable distance across various regions in search
    of better medical services. However, the procedures involved in interregional
    medical insurance reimbursement can be relatively complex and time-consuming,
    especially for patients with leukemia who require prolonged care. Previous research
    has found that immediate reimbursement significantly increases the likelihood
    of patients seeking outpatient treatment in China (Zhong, 2011). Therefore, simplifying
    procedures and improving the efficiency of interregional medical insurance reimbursement
    are vital for enhancing the rate of health service utilization by patients with
    leukemia who seek cross-regional healthcare. These efforts aim to improve treatment
    outcomes for these individuals. FCGs were found to have experienced tremendous
    psychological stress, which aligns with findings from previous investigations
    into the emotional experiences of family caregivers of patients with leukemia
    (Dionne-Odom et al. 2019; Malpert et al., 2015; Sannes et al., 2019). The immense
    psychological pressure has adverse effects on the physical health of FCGs (M.
    Wang et al., 2022), thereby affecting their overall quality of life (Malpert et
    al., 2015; Yu et al., 2017). The demanding nature of caregiving tasks further
    contributes to the psychological burden that FCGs bear. In addition to the physical
    challenges, FCGs also experienced distress while witnessing the patients’ physical
    suffering, especially during bone morrow transplant surgeries. This often led
    to increased stress, depression, and burnout among family caregivers, which confirms
    an earlier study suggesting caregiving can be particularly intense during hematopoietic
    stem cell transplantation, with FCGs assuming many responsibilities to take on
    a variety of medical tasks and struggling to adapt their schedules to the patient’s
    care needs (Dionne-Odom et al., 2019). Although psychological distress was commonly
    observed in previous research (Pailler et al., 2015; Peterson et al., 2020), our
    investigation draws attention to an underappreciated aspect of FCGs’ mental struggle:
    self-denial of overt displaying emotions. This indicates their compassion and
    considerations of maintaining harmonious family relationships even in times of
    crisis. This finding highlights the importance of knowing about the impact of
    FCGs’ emotion on family system, contributing to future interventions on providing
    better psycho-social care. The research findings indicate that FCGs not only experienced
    psychological burden but also faced role strain, which has been consistently reported
    in prior studies on family caregiving for cancer patients (Cooke et al., 2011;
    Eldredge et al., 2006; Muriuki et al., 2023). FCGs prioritized the various needs
    of patients over their own ambitions and aspirations to ensure adequate care.
    We deem that FCGs’ perception of their caregiving roles that they would sacrifice
    their own goals for the sake of their care recipients’ needs, is influenced by
    traditional Chinese family culture. In this culture, self-sacrifice for betterment
    of family members is regarded as a crucial aspect of filial piety. Filial piety
    is highly praised and inherited as a fundamental moral precept that remains profoundly
    ingrained in Confucianism (Yiu et al., 2021). Furthermore, the findings highlight
    that FCGs actively engaged in efforts to reduce financial and psychological burdens,
    keeping a positive attitude towards caregiving and seeking assistance from others.
    These coping strategies align with previous studies that have reported positive
    coping mechanisms among family caregivers (Albrecht et al., 2022; Jacobs et al.,
    2020; M. Wang et al., 2022; M. Wilson et al., 2009). Our research also presents
    unique ways of effective coping mechanisms. For instance, FCGs’ proactive engagement
    in active communication with healthcare providers resulted in reduced medical
    payments; FCGs’ tireless efforts in acquiring medical knowledge played a crucial
    role in reducing their sense of helplessness; practices like writing and shedding
    tears served as avenues for catharsis, providing solace for FCGs and relieving
    their psychological stress to a certain extent. Our findings on FCGs’ positive
    coping strategies to relieve their emotional burden differ from a study conducted
    in the US, which identified maladaptive strategies such as overeating or self-medication
    with psychoactive substances to provide comfort (Albrecht et al., 2022). We postulate
    that the underlying reason for the distinctions of the findings can be associated
    with Chinese traditional social values that emphasize self-esteem and self-respect.
    Among the various sources of support, the primary source identified in this study
    was family, friends and peers, which is consistent with prior research (Bozo et
    al., 2010; Dionne-Odom et al., 2019). However, what distinguishes this study is
    the observation of the support from strangers and cybercitizens, which is less
    commonly seen in prior research on family caregiving for patients with leukemia.
    We believe that this can be attributed to the blogger identity of the FCGs. By
    writing blogs and interacting with their readers, they were able to receive financial
    and psychological support online. Our research brings attention to a particular
    dimension that is important yet underexamined in China: peer support. As the findings
    show, FCGs were encouraged by the emotional support from their wardmates who had
    similar experience of caregiving. By virtue of sharing firsthand information,
    peers offer FCGs health advantages such as increased psychological well-being
    and enhanced coping, promoting hope and adjustment (Sannes et al., 2019; Yu et
    al., 2017). One-on-one volunteer peer supporting delivered in one recent study
    conducted in Denmark (Husted Nielsen et al., 2022) has been proved to be a feasible
    and safe intervention with high satisfaction. We believe it is essential to explore
    and adapt this successful peer support model in Chinese context. FCGs of this
    study reported they received humanistic care from healthcare providers, which
    supports the finding from prior studies highlighting the support provided by formal
    healthcare providers to informal caregivers in order to relieve their psychological
    burden (Laudenslager et al., 2015, 2019). It is crucial to recognize that FCGs’
    caregiving experience affects the entire family system rather than solely the
    patient with leukemia. Although the family has been considered as the optimal
    source of care for patients with leukemia (Karimjejad et al., 2021), relying solely
    on the family members is insufficient. Formal care providers, who are strangers
    but possess authority, serve as an extension of the caregiving system (Waldrop,
    2006). Enhanced care coordination is crucial and family caregivers highly appreciate
    continuous connections with healthcare professionals (Harrison et al., 2022).
    Therefore, it is necessary for the leukemia clinicians to understand the impact
    on the family system. This requires effective communication between families and
    healthcare professionals to address FCGs’ unmet needs, easing their sufferings.
    Although FCGs’ positive adaptation and the social support they received have reduced
    their financial and psychological burdens to a certain extent, they still face
    challenges (Schulz et al., 2018). Therefore, it is essential for policy makers
    and healthcare organizations to gain a comprehensive understanding of these FCGs
    and to formulate interventions and support programs that can effectively alleviate
    their stress levels and improve their overall well-being. Implications Blogs play
    a pivotal role in enhancing our comprehension of the impact of leukemia on patients
    and family caregivers. The findings in this study can be effectively implemented
    in real-world scenarios by adopting a comprehensive and multi-faceted approach.
    The intractable economic challenges, surrounding high medical costs and the inefficient
    cross-regional medical reimbursement system, hold significant importance in guiding
    future healthcare institutions and policymakers towards enacting impactful and
    efficient medical reforms. It is important to advocate for financial assistance
    measures, such as reducing out-of-pocket medical payments and establishing caregiver-friendly
    policies, in order to alleviate the economic burdens faced by patients and caregivers.
    Additionally, incorporating family caregiver assessments into standard healthcare
    practices is recommended to identify and address their specific needs and challenges.
    The expressed anxieties and pressure faced by family caregivers have practical
    implications for implementation of effective psychological support, including
    targeted psychological counseling and relief services aimed at enhancing coping
    skills and resilience among family caregivers. The wide range of support sources
    received by FCGs highlights the need for future research on effective communication
    between informal and formal caregivers. Healthcare providers are encouraged to
    engage with bloggers of FCGs through blogs as a means of online communication.
    This facilitates improved health literacy, empowers them to make personalized
    treatment decisions and predict care recipient outcomes. Online blogs can also
    offer education and training programs for FCGs and their families to increase
    awareness and understanding of the trajectory of leukemia. By implementing these
    strategies, healthcare systems and policymakers can effectively support the well-being
    of family caregivers and improve the overall quality of care provided to patients
    with leukemia. Strengths and limitations Like all methods of qualitative social
    research, blogs as data have both benefits and pitfalls. Illness blogging provides
    an innovative and valuable method of documenting the authentic experience of family
    caregiving. This research utilized naturalistic data, which included the unfiltered,
    in-depth experiences of family caregivers of patients with leukemia. A rich investigation
    of delicate matters of this vulnerable group is made possible by the perceived
    anonymity of the Internet, which permits family caregivers to freely and openly
    communicate their emotions and thoughts that may be sensitive and challenging
    to share in clinical settings. The in-depth exploration of online blogs using
    thematic analysis has generated understanding of the plights and challenges faced
    by family caregivers of patients with leukemia and how they employed effective
    coping strategies to resolve those problems. Limitations to this study should
    also be noted. Only Chinese family caregivers were included, which would limit
    the scope of this study and leave out some significant global nuances of family
    caregivers’ perspectives towards caregiving for patients with leukemia. Furthermore,
    the data were extracted from only one social networking mobile application. Meanwhile,
    a set of key words used to identify an eligible sample of blog narratives in data
    collection may make it possible that some blogs and some issues pertinent to family
    caregiving for patients with leukemia would have gone unnoticed. The low diversity
    in participants (only one man) and some missing demographic information not presented
    by the bloggers were another limitation of the study. These factors limit the
    generalizability and transferability of the present study’s findings. Conclusion
    This qualitative study employed a thematic approach to explore the experiences
    shared in online blogs by Chinese family caregivers of patients with leukemia.
    The findings indicate that despite the financial and psychological burdens they
    faced, family caregivers coped with the challenges positively with their own endeavors
    and social support from others. The process of coping and psychological adaptation
    for caregivers was complex, as FCGs grappled with various concerns regarding the
    overall family system. This research highlights the need for the healthcare system
    reforms as well as a deeper understanding of the functioning of informal caregivers
    in order to address their needs. It also calls upon healthcare professionals to
    offer appropriate support. Further studies in this field should systematically
    examine the development of interventions for informal caregiving, with the aim
    of enhancing healthcare outcomes for family caregivers and ultimately improving
    the quality of life for patients with leukemia. Data availability Datasets were
    derived from public resources. The data of the study were extracted from the mobile
    phone application Red (Xiaohongshu) accounts identified as Chinese family caregivers
    of patients with leukemia. References Albrecht TA, Hoppe R, Winter MA (2022) How
    caregivers cope and adapt when a family member is diagnosed with a hematologic
    malignancy: informing supportive care needs. Cancer Nurs 45(6):E849–E855. https://doi.org/10.1097/NCC.0000000000001063
    Article   PubMed   Google Scholar   Anderson J, Eppes A, O’Dwyer S (2019) “Like
    death is near”: expressions of suicidal and homicidal ideation in the blog posts
    of family caregivers of people with dementia. Behav Sci 9(3):22. https://doi.org/10.3390/bs9030022
    Anderson JG, Hundt E, Dean M, Keim-Malpass J, Lopez RP (2017) \"The church of
    online support\": examining the use of blogs among family caregivers of persons
    with dementia. J Fam Nurs 23(1):34–54. https://doi.org/10.1177/1074840716681289
    Article   PubMed   Google Scholar   Arab M, Bernstein C, Haghshenas A, Ranjbar
    H (2020) Factors associated with caregiver burden for mothers of children undergoing
    Acute Lymphocytic Leukemia (ALL) treatment. Palliat Support Care 18(4):405–412.
    https://doi.org/10.1017/S1478951519000853 Article   PubMed   Google Scholar   Bozo
    Ö, Anahar S, Ateş G, Etel E (2010) Effects of illness representation, perceived
    quality of information provided by the health-care professional, and perceived
    social support on depressive symptoms of the caregivers of children with leukemia.
    J Clin Psychol Med Settings 17(1):23–30. https://doi.org/10.1007/s10880-009-9177-4
    Article   PubMed   Google Scholar   Braun V, Clarke V (2006) Using thematic analysis
    in psychology. Qual Res Psychol 3(2):77–101. https://doi.org/10.1191/1478088706qp063oa
    Article   Google Scholar   Braun V, Clarke V (2022) Thematic analysis: a practical
    guide. Sage, Los Angeles Book   Google Scholar   Braun V, Clarke V, Rance N (2014)
    How to use thematic analysis with interview data. In: Vossler A, Moller N (eds)
    The counselling and psychotherapy research handbook. Sage, London, p 183–197 Google
    Scholar   Bray F, Ferlay J, Soerjomataram I, Siegel RL, Torre LA, Jemal A(2018)
    Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide
    for 36 cancers in 185 countries CA Cancer J Clin 68(6):394–424. https://doi.org/10.3322/caac.21492
    Article   PubMed   Google Scholar   Bruckman A (2002) Studying the amateur artist:
    a perspective on disguising data collected in human subjects research on the Internet.
    Ethics Inf Technol 4(3):217–231. https://doi.org/10.1023/A:1021316409277 Article   Google
    Scholar   Clarke V, Braun V (2018) Using thematic analysis in counselling and
    psychotherapy research: a critical reflection. Couns Psychother Res 18(2):107–110.
    https://doi.org/10.1002/capr.12165 Article   Google Scholar   Cooke L, Grant M,
    Eldredge DH, Maziarz RT, Nail LM (2011) Informal caregiving in hematopoietic blood
    and marrow transplant patients. Eur J Oncol Nurs 15(5):500–507. https://doi.org/10.1016/j.ejon.2011.01.007
    Article   PubMed   PubMed Central   Google Scholar   Deshields TL, Heiland MF,
    Kracen AC, Dua P (2016) Resilience in adults with cancer: development of a conceptual
    model. Psychooncology 25(1):11–18. https://doi.org/10.1002/pon.3800 Article   PubMed   Google
    Scholar   Dionne-Odom JN, Currie ER, Johnston EE, Rosenberg AR (2019) Supporting
    family caregivers of adult and pediatric persons with leukemia. Semin Oncol Nurs
    35(6):150954. https://doi.org/10.1016/j.soncn.2019.150954 Article   PubMed   Google
    Scholar   Eldredge DH, Nail LM, Maziarz RT, Hansen LK, Ewing D, Archbold PG (2006)
    Explaining family caregiver role strain following autologous blood and marrow
    transplantation. J Psychosoc Oncol 24(3):53–74. https://doi.org/10.1300/J077v24n03_03
    Article   PubMed   Google Scholar   Ge C, Yang X, Fu J, Chang Y, Wei J, Zhang
    F, Nutifafa AE, Wang L (2011) Reliability and validity of the Chinese version
    of the caregiver reaction assessment. Psychiatry Clin Neurosci 65(3):254–263.
    https://doi.org/10.1111/j.1440-1819.2011.02200.x Article   PubMed   Google Scholar   Grover
    S, Rina K, Malhotra P, Khadwal A (2019) Caregiver burden in the patients of acute
    myeloblastic leukemia. Indian J Hematol Blood Transfus 35(3):437–445. https://doi.org/10.1007/s12288-018-1048-4
    Article   PubMed   PubMed Central   Google Scholar   Harrison M, Darlison L, Gardiner
    C (2022) Understanding the experiences of end of life care for patients with mesothelioma
    from the perspective of bereaved family caregivers in the UK: a qualitative analysis.
    J Palliat Care 37(2):197–203. https://doi.org/10.1177/08258597221079235 Article   PubMed   PubMed
    Central   Google Scholar   Heilferty CM (2009) Toward a theory of online communication
    in illness: concept analysis of illness blogs. J Adv Nurs 65(7):1539–1547. https://doi.org/10.1111/j.1365-2648.2009.04996.x
    Article   PubMed   Google Scholar   Hookway N (2008) ‘Entering the blogosphere’:
    some strategies for using blogs in social research. Qual Res 8(1):91–113. https://doi.org/10.1177/1468794107085298
    Article   Google Scholar   Hookway N, Snee H (2017) The blogosphere. In: Fielding
    NG, Lee RM, Blank G (eds) The sage handbook of online research methods, 2nd edn.
    Sage, London, p 380–398 Chapter   Google Scholar   Houghton C, Casey D, Shaw D,
    Murphy K (2013) Rigour in qualitative case-study research. Nurse Res 20(4):12–17.
    https://doi.org/10.7748/nr2013.03.20.4.12.e326 Article   PubMed   Google Scholar   Husted
    Nielsen I, Piil K, Tolver A, Grønbæk K, Kjeldsen L, Jarden M (2022) Family caregiver
    ambassador support for caregivers of patients with newly diagnosed hematological
    cancer: a feasibility study. Support Care Cancer 30(8):6923–6935. https://doi.org/10.1007/s00520-022-07089-0
    Article   PubMed   PubMed Central   Google Scholar   Jacobs JM, Nelson AM, Traeger
    L, Waldman L, Nicholson S, Jagielo AD, D’Alotto J, Greer JA, Temel JS, El-Jawahri
    A (2020) Enhanced coping and self‐efficacy in caregivers of stem cell transplant
    recipients: identifying mechanisms of a multimodal psychosocial intervention.
    Cancer 126(24):5337–5346. https://doi.org/10.1002/cncr.33191 Article   PubMed   Google
    Scholar   Jia M, Li J, Chen C, Cao F (2015) Post-traumatic stress disorder symptoms
    in family caregivers of adult patients with acute leukemia from a dyadic perspective.
    Psychooncology 24(12):1754–1760. https://doi.org/10.1002/pon.3851 Article   PubMed   Google
    Scholar   Kannaley K, Mehta S, Yelton B, Friedman DB (2019) Thematic analysis
    of blog narratives written by people with Alzheimer’s disease and other dementias
    and care partners. Dementia 18(7–8):3071–3090. https://doi.org/10.1177/1471301218768162
    Article   PubMed   Google Scholar   Karimjejad H, Ghaljaei F, Kiani F (2021) The
    effect of education training intervention on the caregiver burden among mothers
    of children with leukemia: a quasi-experimental study. Oncol Clin Pr 17(5):194–199.
    https://doi.org/10.5603/OCP.2021.0018 Article   Google Scholar   Kiyimba N, Lester
    JN, O’Reilly M (2019) Using naturally occurring data in qualitative health research:
    a practical guide. Springer International Publishing, Cham Kong H, Chang S, Jin
    Y, Xing Y, Wang Y, Kou J (2022) Research progress on family caregiver burden of
    children with leukemia. Chin Gen Pr Nurs 20(18):2475–2478 Google Scholar   Laudenslager
    ML, Simoneau TL, Kilbourn K, Natvig C, Philips S, Spradley J, Benitez P, McSweeney
    P, Mikulich-Gilbertson SK (2015) A randomized control trial of a psychosocial
    intervention for caregivers of allogeneic hematopoietic stem cell transplant patients:
    effects on distress. Bone Marrow Transpl 50(8):1110–1118. https://doi.org/10.1038/bmt.2015.104
    Article   CAS   Google Scholar   Laudenslager ML, Simoneau TL, Mikulich‐Gilbertson
    SK, Natvig C, Brewer BW, Sannes TS, Kilbourn K, Gutman J, McSweeneym P (2019)
    A randomized control trial of stress management for caregivers of stem cell transplant
    patients: effect on patient quality of life and caregiver distress. Psychooncology
    28(8):1614–1623. https://doi.org/10.1002/pon.5126 Article   PubMed   Google Scholar   Li
    L, Liu C, Cai X, Yu H, Zeng X, Sui M, Zheng E, Li Y, Xu J, Zhou J, Huang W (2019)
    Validity and reliability of the EQ-5D-5L in family caregivers of leukemia patients.
    BMC Cancer 19(1):522. https://doi.org/10.1186/s12885-019-5721-2 Article   CAS   PubMed   PubMed
    Central   Google Scholar   Lincoln YS, Guba EG (1985) Naturalistic inquiry. Sage
    Publications, Beverly Hills, Calif Book   Google Scholar   Malpert AV, Kimberg
    C, Luxton J, Mullins LL, Pui C-H, Hudson MM, Krull KR, Brinkman TM (2015) Emotional
    distress in parents of long-term survivors of childhood acute lymphoblastic leukemia.
    Psychooncology 24(9):1116–1123. https://doi.org/10.1002/pon.3732 Article   PubMed   Google
    Scholar   Mao W, Tang S, Zhu Y, Xie Z, Chen W (2017) Financial burden of healthcare
    for cancer patients with social medical insurance: a multi-centered study in urban
    China. Int J Equity Health 16(1):180. https://doi.org/10.1186/s12939-017-0675-y
    Article   PubMed   PubMed Central   Google Scholar   McLennon SM, Davis A, Covington
    S, Anderson JG (2021) “At the end we feel forgotten”: needs, concerns, and advice
    from blogs of dementia family caregivers. Clin Nurs Res 30(1):82–88. https://doi.org/10.1177/1054773819865871
    Morrow SL (2005) Quality and trustworthiness in qualitative research in counseling
    psychology. J Couns Psychol 52(2):250–260. https://doi.org/10.1037/0022-0167.52.2.250
    Article   Google Scholar   Muriuki MM, Oluchina S, Mbithi BW (2023) Assessment
    of role strain among family caregivers of adult patients with cancer at a national
    referral hospital in Kenya. Psychooncology 32(1):42–46. https://doi.org/10.1002/pon.5868
    Article   PubMed   Google Scholar   National Cancer Institute (2019) Survivorship.
    https://www.cancer.gov/publications/dictionaries/cancer-terms/def/survivorship.
    Accessed 7 Aug 2022 Nowell LS, Norris JM, White DE, Moules NJ (2017) Thematic
    analysis: striving to meet the trustworthiness criteria. Int J Qual Methods 16(1):1–13.
    https://doi.org/10.1177/1609406917733847 Article   Google Scholar   Pailler ME,
    Johnson TM, Zevon MA, Kuszczak S, Griffiths E, Thompson J, Wang ES, Wetzler M
    (2015) Acceptability, feasibility, and efficacy of a supportive group intervention
    for caregivers of newly diagnosed leukemia patients. J Psychosoc Oncol 33(2):163–177.
    https://doi.org/10.1080/07347332.2014.992086 Article   PubMed   Google Scholar   Panganiban-Corales
    AT, Medina MF (2011) Family resources study: part 1: family resources, family
    function and caregiver strain in childhood cancer. Asia Pac Fam Med 10(1):14.
    https://doi.org/10.1186/1447-056X-10-14 Article   PubMed   PubMed Central   Google
    Scholar   Papastavrou E, Charalambous A, Tsangari H (2012) How do informal caregivers
    of patients with cancer cope: a descriptive study of the coping strategies employed.
    Eur J Oncol Nurs 16(3):258–263. https://doi.org/10.1016/j.ejon.2011.06.001 Article   PubMed   Google
    Scholar   Perricone G, Polizzi C, Morales MR, Marino S, Scacco CF (2012) Functioning
    of family system in pediatric oncology during treatment phase. Pediatr Hematol
    Oncol 29(7):652–662. https://doi.org/10.3109/08880018.2012.695439 Article   PubMed   Google
    Scholar   Peterson RK, Chung J, Barrera M (2020) Emotional symptoms and family
    functioning in caregivers of children with newly diagnosed leukemia/lymphomas
    and solid tumors: short-term changes and related demographic factors. Pediatr
    Blood Cancer 67(2):e28059. https://doi.org/10.1002/pbc.28059 Article   PubMed   Google
    Scholar   Sannes TS, Simoneau TL, Mikulich-Gilbertson SK, Natvig CL, Brewer BW,
    Kilbourn K, Laudenslager ML (2019) Distress and quality of life in patient and
    caregiver dyads facing stem cell transplant: identifying overlap and unique contributions.
    Support Care Cancer 27(6):2329–2337. https://doi.org/10.1007/s00520-018-4496-3
    Article   PubMed   Google Scholar   Schulz R, Beach SR, Friedman EM, Martsolf
    GR, Rodakowski J, James AE (2018) Changing structures and processes to support
    family caregivers of seriously ill patients. J Palliat Med 21(S2):S-36–S-42. https://doi.org/10.1089/jpm.2017.0437
    Article   Google Scholar   Shah SGS, Robinson I (2011) Patients’ perspectives
    on self-testing of oral anticoagulation therapy: content analysis of patients’
    internet blogs. BMC Health Serv Res 11(1):25. https://doi.org/10.1186/1472-6963-11-25
    Article   PubMed   PubMed Central   Google Scholar   Sung H, Ferlay J, Siegel
    RL, Laversanne M, Soerjomataram I, Jemal A, Bray F (2021) Global cancer statistics
    2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in
    185 countries. CA Cancer J Clin 71(3):209–249. https://doi.org/10.3322/caac.21660
    Article   PubMed   Google Scholar   Tang Y-Y, Chen S-P (2002) Health promotion
    behaviors in Chinese family caregivers of patients with stroke. Health Promot
    Int 17(4):329–339. https://doi.org/10.1093/heapro/17.4.329 Article   PubMed   Google
    Scholar   Waldrop DP (2006) Caregiving systems at the end of life: how informal
    caregivers and formal providers collaborate. Fam Soc J Contemp Soc Serv 87(3):427–437.
    https://doi.org/10.1606/1044-3894.3548 Article   Google Scholar   Wang C, Yan
    J, Chen J, Wang Y, Lin YC, Hu R, Wu Y (2020) Factors associated with quality of
    life of adult patients with acute leukemia and their family caregivers in China:
    a cross-sectional study. Health Qual Life Outcomes 18(1):8. https://doi.org/10.1186/s12955-020-1269-8
    Article   CAS   PubMed   PubMed Central   Google Scholar   Wang H, Naghavi M,
    Allen C, Barber RM, Bhutta ZA, Carter A, Casey DC, Charlson FJ, Chen AZ, Coates
    MM, Coggeshall M, Dandona L, Dicker DJ, Erskine HE, Ferrari AJ, Fitzmaurice C,
    Foreman K, Forouzanfar MH, Fraser MS, Murray CJL (2016) Global, regional, and
    national life expectancy, all-cause mortality, and cause-specific mortality for
    249 causes of death, 1980–2015: a systematic analysis for the Global Burden of
    Disease Study 2015. Lancet 388(10053):1459–1544. https://doi.org/10.1016/S0140-6736(16)31012-1
    Wang M, Chang M, Cheng M, Zhang R (2022) The psychological adaptation process
    in Chinese parent caregivers of pediatric leukemia patients: a qualitative analysis.
    Cancer Nurs 45(5):E835–E842. https://doi.org/10.1097/NCC.0000000000001034 Article   PubMed   Google
    Scholar   Wang Y, Yan J, Chen J, Wang C, Lin Y, Wu Y, Hu R (2021) Comparison of
    the anxiety, depression and their relationship to quality of life among adult
    acute leukemia patients and their family caregivers: a cross-sectional study in
    China. Qual Life Res 30(7):1891–1901. https://doi.org/10.1007/s11136-021-02785-6
    Article   PubMed   Google Scholar   Wilson E, Kenny A, Dickson-Swift V (2015)
    Using blogs as a qualitative health research tool: a scoping review. Int J Qual
    Methods 14(5):160940691561804. https://doi.org/10.1177/1609406915618049 Article   Google
    Scholar   Wilson ME, Eilers J, Heermann JA, Million R (2009) The experience of
    spouses as informal caregivers for recipients of hematopoietic stem cell transplants.
    Cancer Nurs 32(3):E15–E23. https://doi.org/10.1097/NCC.0b013e31819962e0 Article   PubMed   Google
    Scholar   Yiu HC, Zang Y, Chew JHS, Chau JPC (2021) The influence of Confucianism
    on the perceptions and process of caring among family caregivers of persons with
    dementia: a qualitative study. J Transcult Nurs 32(2):153–160. https://doi.org/10.1177/1043659620905891
    Article   PubMed   Google Scholar   Yu H, Li L, Liu C, Huang W, Zhou J, Fu W,
    Ma Y, Li S, Chang Y, Liu G, Wu Q (2017) Factors associated with the quality of
    life of family caregivers for leukemia patients in China. Health Qual Life Outcomes
    15(1):55. https://doi.org/10.1186/s12955-017-0628-6 Article   PubMed   PubMed
    Central   Google Scholar   Yu H, Zhang H, Yang J, Liu C, Lu C, Yang H, Huang W,
    Zhou J, Fu W, Shi L, Yan Y, Liu G, Li L (2018) Health utility scores of family
    caregivers for leukemia patients measured by EQ-5D-3L: a cross-sectional survey
    in China. BMC Cancer 18(1):950. https://doi.org/10.1186/s12885-018-4855-y Article   PubMed   PubMed
    Central   Google Scholar   Yucel E, Zhang S, Panjabi S (2021) Health-related and
    economic burden among family caregivers of patients with acute myeloid leukemia
    or hematological malignancies. Adv Ther 38(10):5002–5024. https://doi.org/10.1007/s12325-021-01872-x
    Article   PubMed   PubMed Central   Google Scholar   Zeidan AM, Mahmoud D, Kucmin-Bemelmans
    IT, Alleman CJ, Hensen M, Skikne B, Smith BD (2016) Economic burden associated
    with acute myeloid leukemia treatment. Expert Rev Hematol 9(1):79–89. https://doi.org/10.1586/17474086.2016.1112735
    Article   CAS   PubMed   Google Scholar   Zhan C, Wu Z, Yang L, Yu L, Deng J,
    Luk K, Duan C, Zhang L (2022) Disparities in economic burden for children with
    leukemia insured by resident basic medical insurance: evidence from real-world
    data 2015–2019 in Guangdong, China. BMC Health Serv Res 22(1):229. https://doi.org/10.1186/s12913-022-07564-8
    Article   PubMed   PubMed Central   Google Scholar   Zhang X, Zhang L (2022) The
    impact of instant reimbursement of cross-regional medical services on hospitalization
    costs incurred by the floating population—evidence from China. Healthcare 10(6):1099.
    https://doi.org/10.3390/healthcare10061099 Article   PubMed   PubMed Central   Google
    Scholar   Zhong H (2011) Effect of patient reimbursement method on health-care
    utilization: evidence from China. Health Econ 20(11):1312–1329. https://doi.org/10.1002/hec.1670
    Article   PubMed   Google Scholar   Download references Author information Authors
    and Affiliations School of English Studies, Sichuan International Studies University,
    33 Zhuangzhi Road, Chongqing, 400031, PR China Mengying Qiu Center for Geriatric
    Healthcare Services and Health Education, Qufu Normal University, 80 Yantai North
    Road, Rizhao, 276825, PR China Yijin Wu Contributions All authors made a significant
    contribution to the work reported, whether that is in the conception, study design,
    execution, acquisition of data, analysis and interpretation, or in all these areas;
    took part in drafting, revising or critically reviewing the article; gave final
    approval of the version to be published; have agreed on the journal to which the
    article has been submitted; and agree to be accountable for all aspects of the
    work. Corresponding author Correspondence to Yijin Wu. Ethics declarations Competing
    interests The authors declare no competing interests. Ethical approval Ethical
    approval was not required as the study did not involve human participants. Informed
    consent In line with standard practice for analysis of online social media, informed
    consent from individual authors was deemed neither necessary nor practicable.
    Authors posted the blogs available under terms of service, which permit re-use
    by researchers for academic purposes. Blogs were collected and used in line with
    the terms of service. Data presentation has been anonymized to avoid linking particular
    authors to particular blogs. Additional information Publisher’s note Springer
    Nature remains neutral with regard to jurisdictional claims in published maps
    and institutional affiliations. Rights and permissions Open Access This article
    is licensed under a Creative Commons Attribution 4.0 International License, which
    permits use, sharing, adaptation, distribution and reproduction in any medium
    or format, as long as you give appropriate credit to the original author(s) and
    the source, provide a link to the Creative Commons licence, and indicate if changes
    were made. The images or other third party material in this article are included
    in the article’s Creative Commons licence, unless indicated otherwise in a credit
    line to the material. If material is not included in the article’s Creative Commons
    licence and your intended use is not permitted by statutory regulation or exceeds
    the permitted use, you will need to obtain permission directly from the copyright
    holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Qiu, M., Wu, Y.
    Understanding the experience of family caregivers of patients with leukemia: a
    qualitative analysis of online blogs. Humanit Soc Sci Commun 11, 374 (2024). https://doi.org/10.1057/s41599-024-02830-y
    Download citation Received 26 August 2023 Accepted 15 February 2024 Published
    08 March 2024 DOI https://doi.org/10.1057/s41599-024-02830-y Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Subjects Health humanities Medical humanities Download PDF Sections References
    Abstract Introduction Method Results Discussion Implications Strengths and limitations
    Conclusion Data availability References Author information Ethics declarations
    Additional information Rights and permissions About this article Advertisement
    Humanities and Social Sciences Communications (Humanit Soc Sci Commun) ISSN 2662-9992
    (online) About Nature Portfolio About us Press releases Press office Contact us
    Discover content Journals A-Z Articles by subject Protocol Exchange Nature Index
    Publishing policies Nature portfolio policies Open access Author & Researcher
    services Reprints & permissions Research data Language editing Scientific editing
    Nature Masterclasses Research Solutions Libraries & institutions Librarian service
    & tools Librarian portal Open research Recommend to library Advertising & partnerships
    Advertising Partnerships & Services Media kits Branded content Professional development
    Nature Careers Nature Conferences Regional websites Nature Africa Nature China
    Nature India Nature Italy Nature Japan Nature Korea Nature Middle East Privacy
    Policy Use of cookies Your privacy choices/Manage cookies Legal notice Accessibility
    statement Terms & Conditions Your US state privacy rights © 2024 Springer Nature
    Limited"'
  inline_citation: '>'
  journal: Humanities and Social Sciences Communications
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Understanding the experience of family caregivers of patients with leukemia:
    a qualitative analysis of online blogs'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Li S.
  - Chu L.
  - Wang J.
  - Zhang Y.
  citation_count: '0'
  description: 'This paper constructs a two-layer road data asset revenue allocation
    model based on a modified Shapley value approach. The first layer allocates revenue
    to three roles in the data value realization process: the original data collectors,
    the data processors, and the data product producers. It fully considers and appropriately
    adjusts the revenue allocation to each role based on data risk factors. The second
    layer determines the correction factors for different roles to distribute revenue
    among the participants within those roles. Finally, the revenue values of the
    participants within each role are synthesized to obtain a consolidated revenue
    distribution for each participant. Compared to the traditional Shapley value method,
    this model establishes a revenue allocation evaluation index system, uses entropy
    weighting and rough set theory to determine the weights, and adopts a fuzzy comprehensive
    evaluation and numerical analysis to assess the degree of contribution of participants.
    It fully accounts for differences in both the qualitative and quantitative contributions
    of participants, enabling a fairer and more reasonable distribution of revenues.
    This study provides new perspectives and methodologies for the benefit distribution
    mechanism in road data assets, which aid in promoting the market-based use of
    road data assets, and it serves as an important reference for the application
    of data assetization in the road transportation industry.'
  doi: 10.1038/s41598-024-55819-7
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    02 March 2024 A road data assets revenue allocation model based on a modified
    Shapley value approach considering contribution evaluation Shiwei Li, Lei Chu,
    Jisen Wang & Yuzhao Zhang  Scientific Reports  14, Article number: 5179 (2024)
    Cite this article 318 Accesses Metrics Abstract This paper constructs a two-layer
    road data asset revenue allocation model based on a modified Shapley value approach.
    The first layer allocates revenue to three roles in the data value realization
    process: the original data collectors, the data processors, and the data product
    producers. It fully considers and appropriately adjusts the revenue allocation
    to each role based on data risk factors. The second layer determines the correction
    factors for different roles to distribute revenue among the participants within
    those roles. Finally, the revenue values of the participants within each role
    are synthesized to obtain a consolidated revenue distribution for each participant.
    Compared to the traditional Shapley value method, this model establishes a revenue
    allocation evaluation index system, uses entropy weighting and rough set theory
    to determine the weights, and adopts a fuzzy comprehensive evaluation and numerical
    analysis to assess the degree of contribution of participants. It fully accounts
    for differences in both the qualitative and quantitative contributions of participants,
    enabling a fairer and more reasonable distribution of revenues. This study provides
    new perspectives and methodologies for the benefit distribution mechanism in road
    data assets, which aid in promoting the market-based use of road data assets,
    and it serves as an important reference for the application of data assetization
    in the road transportation industry. Similar content being viewed by others Brownfield
    redevelopment evaluation based on structure-process-outcome theory and continuous
    ordered weighted averaging operator-topology method Article Open access 16 October
    2023 A quantitative methodology for measuring the social sustainability of pavement
    deterioration Article Open access 24 January 2024 Comprehensive strength evaluation
    system of commercial centres based on multi-source data: a case of Hefei central
    city Article Open access 10 October 2023 Introduction Data collection and processing
    have become more convenient and intelligent with the development of information
    technologies like the Internet of Things and artificial intelligence. As a result,
    all industries now have massive amounts of data. Data, as a new production factor,
    harbors immense potential value. However, disorganized data is worthless. Its
    true value can only be realized by transforming raw data into standardized, complete,
    and accurate data resources through data governance. The term \"data asset\" was
    first coined by Richard Peterson in 1974. Later, the 2012 World Economic Forum
    report considered data as a new category of economic asset. In 2021, the China
    State Administration for Market Regulation (CSAMR), together with the China Standardization
    Administration (CSA), issued a national standard document (GB/T 40685-2021) defining
    data assets as measurable and legally sourced data resources capable of generating
    economic and social value1. Since its inception, the transport industry has been
    generating massive amounts of data, which are huge in scale and high in precision
    and quality. Road transport, as an important part of the transport industry, generates
    a large amount of road data during its operation. The data assets formed after
    sorting and processing will provide support for the optimal design of the road
    network, the emergency response of the road network, and the optimization of vehicle
    routes to comprehensively guarantee the construction of smart roads. Therefore,
    accelerating the circulation and trading of road data assets to enable effective
    excavation and commercialization of data value has become crucial. In contrast
    to the banking, Internet, pharmaceutical, and other industries that can complete
    the entire data value chain within the enterprise, the resources and technologies
    mastered by enterprise subjects in the road transport industry vary greatly, making
    it difficult to complete the entire process of data generation, collection, and
    processing independently. Table 1 compares several key characteristics of data
    asset evaluation across four major industries: road transport, banking, internet,
    and pharmaceutical. Table 1 Comparison of data asset evaluation characteristics
    across industries. Full size table The road transport industry contends with unique
    challenges in asset evaluation compared to other sectors for several reasons.
    Firstly, road transport data remains fragmented across proprietary silos in the
    value chain, whereas banking has established unified data systems and infrastructure.
    Secondly, road transport corporate data is highly dispersed unlike consolidated
    user data assets held by banks and internet platforms. Pharmaceutical firms also
    possess proprietary R&D datasets2. Thirdly, road transport has relatively limited
    computational capabilities contrasted with the abundant scalable cloud resources
    of banks and internet companies. Pharmaceuticals likewise require significant
    computing power for R&D2. Road transport faces more stringent regulatory oversight
    than sectors like internet companies, which contributes to its disadvantages in
    data and analytics capabilities. However, regulations alone do not fully explain
    these gaps. Banking and pharmaceuticals also operate under strict supervision,
    yet boast stronger data and computational resources than transport. To address
    the data development challenges in the road transport industry, it is imperative
    to propose innovative solutions within the regulatory framework. Overcoming limitations
    in data access and analytics is critical for realizing the enormous value of road
    transport data. However, the substantial differences in resources and technologies
    possessed by different road transport companies often result in mismatches between
    data ownership and data processing capabilities, highlighting the need for cross-enterprise
    collaboration. From the perspective of the data value chain, collaborative enterprises
    can be divided into three roles: original data collectors, data processors, and
    data product producers. Given the scope, complexity, and diversity of road data
    and participants, it is vital to investigate fair and reasonable revenue distribution
    mechanisms. At the moment, the market-based trading of road data assets is in
    its early stages, with little research on the revenue allocation mechanism of
    data assets and nearly no special studies on the distribution mechanism of road
    data assets. This paper develops a two-layer road data asset revenue allocation
    model based on the modified Shapley value method. The first layer allocates revenues
    to three types of roles, namely, original data collectors, data processors, and
    data product producers, and corrects them using data risk factors. The second
    layer determines the respective correction factors for different roles to realize
    the distribution of revenues to participating enterprises under different roles,
    and finally synthesizes the revenues of the participating enterprises under each
    role to obtain the final revenue of each participating enterprise. The purpose
    of this paper is to conduct an exploratory investigation on the subject of revenue
    allocation of road data assets to fill gaps in related research in this field.
    The results of the study will, to a certain extent, promote the marketed circulation
    and application of road data assets and help the development of smart road construction.
    The main contributions of this paper are as follows: (1) We expand the research
    in the field of road data assets from the perspective of revenue allocation, applying
    the Shapley value method in cooperative game theory to achieve unique and fair
    revenue allocation so that relevant participants in the data value chain can get
    due reward. (2) For situations with many cross-cutting participants in road data
    assets, we divide them into three roles based on the process of realizing data
    value: original data collectors, data processors, and data product producers.
    We then construct a two-layer revenue allocation model from roles to participating
    enterprises. (3) Considering the limitations of the traditional Shapley value
    method, we establish a revenue distribution evaluation index system for revision,
    using entropy weighting and rough set theory to determine index weights. We adopt
    fuzzy evaluation and numerical analysis to comprehensively calculate participants''
    contributions across qualitative and quantitative aspects. This paper constructs
    a fair and reasonable revenue allocation model for road data assets, providing
    methods and suggestions for market-based trading of such assets with certain reference
    values for promoting data assetization in road transport. This paper is structured
    as follows: In Section “Literature review”, we review the literature related to
    data asset trading, road digitization, and revenue allocation methods. In Section
    “Model building”, we analyze the participating subjects of road data assets and
    divide them into three roles. We construct a two-layer revenue allocation model
    for the roles and participating enterprises within them and correct the initial
    allocation of the traditional Shapley value through a revenue evaluation index
    system. In Section “Case study”, we verify the proposed model''s effectiveness
    through a case study of road data assets. Finally, in Section “Conclusions”, we
    summarize the research process for this paper. Literature review Road digitalization
    As a technological advancement, digitalization has permeated various aspects of
    the economy and society. The academics describe the essence of digitalization
    from the perspectives of product and value creation, stating that it facilitates
    the transition from one-way to two-way product design, enabling interactive and
    configurable products, and promoting the co-creation of the product value. Among
    them, \"road digitalization\" is based on collecting data through various types
    of sensing equipment, relying on the multi-network convergence of communication
    facilities to transmit data, and through intelligent analysis and processing,
    to achieve highway control guidance, intelligent decision-making, personalized
    services, etc. This effectively improves the safety level of the transportation
    system, traffic efficiency, and management effectiveness. Some scholars have already
    conducted research in the field of road digitalization. Singh et al.3 extensively
    examined the significance of road digitalization from various aspects such as
    intelligent lighting systems, smart emergency management systems, and renewable
    energy. They also described the architectures of intelligent lighting systems
    and smart emergency management systems. Lu et al.4 constructed a real-time digital
    model of traffic scenes based on vision, which supports the development of digital
    twins of road traffic to a certain extent. With the support of these digitalization
    technologies for roads, the application value of road data assets can be fully
    explored. Road data assets refer to various digital resources related to roads,
    including dynamic data such as technical indicators, traffic flow, weather conditions,
    and vehicle routes. These data can be used for traffic monitoring and prediction5,
    signal control, and road condition feedback6. For example, the Beijing Municipal
    Commission of Transport has opened up traffic-related data to travel service platforms
    such as Amap, Baidu Maps, and Meituan, enabling these platforms to provide new
    features such as bus occupancy rate query, comprehensive comparison of travel
    plans, and estimated travel time, which comprehensively improves the level of
    traffic and travel services. The digitalization of roads provides a data foundation
    for road data assets through various sensing devices that collect dynamic road
    data. The application of road data assets elevates roads from static construction
    to \"networked, sensed, and intelligent\" dynamic management, which is the key
    foundation for smart transportation development. Data asset trading provides an
    opportunity for the open sharing of road data, creating revenue for relevant transportation
    enterprises and further enhancing the value and influence of road digitization.
    This process advances the scientific, intelligent, and efficient development of
    road management. Data assets trading The transition of data from being perceived
    as mere objects to being regarded as valuable assets signifies its significant
    contribution to economic development7. There are two main ways to realize the
    economic value of data assets: one is to bring economic benefits indirectly by
    optimizing business processes and assisting decision-making within the enterprise;
    the other is to sell the data assets directly to the outside world in the data
    trading market so that more enterprises can benefit from them and fully activate
    the value of these data assets. Data transactions are typically facilitated by
    three parties: data consumers, data providers, and data markets. Data providers
    package and submit their data to the data market, which then matches the appropriate
    data providers with the needs of data consumers. Finally, data providers and consumers
    interact to finalize the transaction8. Acting as intermediaries, data markets
    primarily provide services such as data legality examination, quality assessment,
    and value evaluation. Europe and the United States have explored data trading
    earlier, and currently, active big data trading platforms include Dawex (France),
    Streamr (Sweden), Advaneo (UK), Otonomo (Israel), and so on. In 2015, China began
    implementing its big data strategy and established the first domestic big data
    exchange institution, the Guizhou Big Data Exchange. In 2019, China further proposed
    participating in the distribution of data as a factor of production, and data
    trading organizations were established one after another in Beijing, Shanghai,
    and Shenzhen, marking that data trading has entered a period of rapid development
    in China. Data products sold by data trading platforms mainly include different
    forms such as data packages, API interfaces, and data analysis reports. Differences
    in the form of data products affect the formulation of pricing strategies. Existing
    data pricing strategies can be classified into six categories: free data, usage-based
    pricing, package pricing, uniform pricing, freemium pricing, and two-part pricing
    combining bundle and uniform pricing9. Data trading cannot be realized unless
    the precise selling price of the data set is established. Data pricing needs to
    meet the requirements of revenue maximization, fairness, arbitrage-free pricing,
    computational efficiency, etc.9, and more scholars have explored and researched
    data pricing methods. Liang et al.10 explored the factors affecting the data price
    based on the feature price model in terms of the data object, the data seller,
    and the data buyer. Tian et al.11 focused on the data seller as the main entity
    and designed optimal contract mechanisms considering privacy protection in various
    market scenarios, aiming to achieve individual rationality and incentive compatibility.
    Oh et al.12 designed a competitive Internet of Things (IoT) data trading environment
    consisting of data providers, data brokers, data service providers, and data consumers.
    They also proposed a unified method for pricing data sets to compare the competitiveness
    of different data brokers. In addition to considering the transaction scenario
    and market supply and demand conditions, data pricing also entails focusing on
    the inherent value and potential contributions of the data. Some scholars have
    conducted research on the valuation of data assets from the perspective of data
    intrinsic characteristics. Yu et al.13 proposed a data pricing model that takes
    into account data quality and versioning strategies, enabling data quality assessment
    and market segmentation. Liao et al.14 quantified user privacy choices and constructed
    a multi-scenario data property bilateral trading model. Chellappa et al.15 conducted
    a detailed analysis of version control strategies for data products and derived
    the optimal version of data products along with corresponding prices. In the process
    of data transactions, various technical means need to be applied to protect data
    security and the rights of data rights holders. Currently, technologies such as
    privacy computing16, blockchain17, and digital watermarking18,19 can support the
    platform''s data protection efforts, set up the platform''s data protection system,
    and consider data security and compliance when conducting transactions. In summary,
    as a new type of strategic resource, data assets have data trading as one of the
    important means to realize their commercial value. Promoting the transaction and
    utilization of data assets is an important development trend nowadays, which can
    create value for data participants. Data trading platforms should activate the
    value of data while maintaining data security and complying with regulatory requirements
    to promote the orderly circulation of data resources. Revenue allocation is the
    primary task following data asset transactions, serving as a key motivator to
    stimulate the active participation of enterprises. A fair and equitable revenue
    allocation mechanism can promote and incentivize deep open sharing and the value
    creation of data assets. Revenue allocation methods For the revenue allocation
    of data assets, there is a lack of mature allocation methods, and the Shapley
    value method based on cooperative game theory is widely used in revenue allocation
    problems in various fields. The Shapley value method can achieve a unique and
    fair distribution of asset benefits by calculating the marginal contributions
    of each participant in different combination scenarios. Luo et al.20 proposed
    a rapid calculation method of accurate Shapley value under the independent utility
    for multi-source datasets, but this method only considers the data owner''s benefit
    allocation and does not cover other participants in the data value chain. The
    basic Shapley value method treats all participants as equal in status and distributes
    benefits based solely on the average marginal contribution, without considering
    the differentiated contributions of the participants. To overcome this shortcoming
    and make the revenue allocation reasonable, many scholars have tried to introduce
    factors such as input cost, risk-taking, and urgent demand based on the Shapley
    value method to reflect the asymmetric contributions of the participants. Wang
    et al.21 established a modified Shapley value method based on cloud gravity, taking
    into account risk, inputs, and service quality, and applied it to the revenue
    allocation of a private charging pile-sharing project, which significantly improves
    the effect of multi-party cooperation. Yang et al.22 constructed a modified Shapley
    value-based integrated energy system revenue-sharing model based on operational
    risk factors, which can reflect the actual operational risk and the degree of
    contribution of participants. Zheng et al.23 introduced five non-cooperative and
    cooperative models for a remanufacturing closed-loop supply chain. They considered
    the bargaining power of alliances as the game''s bottom line and proposed a method
    of variable-weighted Shapley value to achieve profit distribution in the supply
    chain. The roles and tasks performed by different parties in a cooperative alliance
    differ, and the Shapley value based on contribution alone cannot fully account
    for other key factors, such as resource input and risk-taking by the participants,
    making the benefit distribution scheme unfair to some extent. Therefore, to achieve
    reasonable benefit distribution and stable cooperation in the data asset value
    chain, the basic Shapley value method needs to be improved by selecting appropriate
    modifying factors for the specific conditions of the value chain to take into
    account the contributions, inputs, risks, and other factors of the participants
    in a fair manner. In conclusion, for the problem of data asset revenue allocation,
    the method based on the Shapley value method has the advantage of being uniquely
    fair, but it also has the defect of considering only the average contribution
    and ignoring the differentiated contribution. To achieve fairness and efficiency
    in revenue allocation, it is necessary to follow the principle of fair distribution
    of the Shapley value method, fully consider the differentiated characteristics
    of each participant in the value chain, and use appropriate modifying factors
    to design an improved scheme that can take into account both the fairness of revenue
    allocation and the stability of alliance cooperation. The study of revenue distribution
    of road data assets by modifying the Shapley value method can achieve fair and
    reasonable revenue sharing among the participants, stimulate data sharing, cooperation,
    and innovation, and further optimize road construction and management decisions.
    Ethical declaration The research described in this paper focuses on developing
    a revenue allocation model for road data assets using a modified Shapley value
    approach. The data referenced is simulated and does not contain any real or private
    information about individuals or organizations. All data of the revenue distribution
    model discussed are entirely fictitious and fabricated for the sole purpose of
    demonstrating the proposed approach. No actual road assets or transportation systems
    data has been accessed or analyzed without appropriate consent. This research
    does not involve the collection of any confidential data or infringement on privacy
    rights. The study does not aim to cause harm or unfairly benefit any entities.
    As this is theoretical research for academic purposes only, it does not have any
    current real-world implications. The research methodology and proposed model strive
    to maintain ethical standards, avoid conflicts of interest, and uphold principles
    of fairness and integrity. Model building Players This paper divides the process
    of realizing the value of highway data assets into three key stages: original
    data collection, data processing, and data product development. Based on this
    process, the main stakeholders in distributing revenues from highway data assets
    can be divided into three roles: First, the original data collectors, namely the
    initial holders of road data, who complete the original collection of road data
    and own these data; second, the data processors, who add value to the original
    data through cleansing, integration, analysis, mining, and other means; third,
    the data product producers, who utilize the processed data for product design,
    development, and operation, realizing the full commercial value of the data assets.
    Original data collectors They obtain revenues by collecting road original data,
    which are primarily generated from enterprises'' activities in road construction
    and operation management. For example, traffic volume and speed data collected
    by road authorities through fixed monitoring devices; toll station traffic volume
    and toll data acquired by toll road operators; real-time traffic conditions and
    route data collected by map service companies using navigation devices; vehicle
    status and road condition data gathered by automakers through onboard devices.
    Data processors They obtain revenues by processing the lawfully acquired original
    road data using methods like standardization, cleansing, integration, mining,
    etc. This process requires building road data warehouses, establishing analytical
    models, and discovering correlations in the data to extract value from the data.
    Since the original road data has a large volume but low-value density, it cannot
    be directly used for knowledge discovery and decision support. Only by improving
    data quality and discovering potential value through processing can more valuable
    highway data assets formed. For example, road research institutes analyze and
    integrate data collected by road authorities to support transportation planning;
    intelligent connected vehicle companies develop data models, leveraging data gathered
    by onboard devices to forecast traffic volume; mobility service platforms fuse
    user feedback with driving data to enhance traffic condition judgment and vehicle
    dispatching capabilities. Data product producers Based on the processed datasets,
    they obtain revenues by developing data products with practical value, marketing,
    and maintaining these products. Major data product formats include data packages,
    API interfaces, data analytical reports, etc. These road data products require
    continuous development and maintenance by data product operators. For example,
    road monitoring systems developed by transportation authorities for government
    users to improve road safety; ETC systems developed by new infrastructure operators,
    providing services like toll payment inquiries; usage-based auto insurance products
    developed by insurance companies using vehicle driving data to charge premiums
    based on mileage. It should be emphasized that since road data rights can be separated
    and shared, different interests can be allocated to different stakeholders as
    needed, and the same participant may also simultaneously take on roles in multiple
    stages of the data value chain. For example, some transportation operators are
    responsible for both original data collection and participation in data processing
    and product design. Therefore, the distribution of revenues from road data assets
    should be reasonably determined based on the contributions made by each participant
    at different stages. Evaluation indicator system for revenue allocation The traditional
    Shapley value method only allocates revenues based on marginal contributions,
    while participants in the same role may have significant differences in costs,
    risks, and other aspects. These differences need to be fully considered in the
    revenue distribution process. In addition, generating road data assets requires
    the participation of original data collectors, data processors, and data product
    producers. In practice, some participants may simultaneously take on multiple
    roles. The revenue distribution needs to comprehensively consider their contributions
    across different roles. To address these issues, this paper proposes a two-layer
    allocation mechanism based on the traditional Shapley value method to reasonably
    distribute revenues from road data assets. The first layer determines the revenue
    shares for the three roles based on their contributions in the value chain; the
    second layer further distributes the revenues of each role to the actual participants.
    Compared to the Shapley value method, which only considers marginal contributions,
    this two-layer allocation mechanism is more comprehensive and reasonable, as it
    additionally takes into full consideration the differences in costs and risk sharing
    among different participants, as well as the contributions of the same participant
    under different roles. By considering both role contributions and participant
    efforts, the two-layer allocation mechanism achieves fair and effective revenue
    distribution. First layer: role revenue allocation During the lifecycle of road
    data assets, all participants face various risks, and those taking on more risks
    expect higher returns. Therefore, this paper takes the data risk factor as a correction
    factor for role benefit allocation. According to the sources, data risks are divided
    into external risks and internal risks. External risks mainly include policy risks
    and legal risks, as changes in relevant policies and the enactment of laws regarding
    data assets can significantly impact participants'' operations. Internal risks
    refer to those arising from equipment failures, data security, and other factors
    during the generation of road data assets, which can be prevented and controlled.
    Second layer: revenue allocation among participants of the same role Based on
    the characteristics of the three roles—original data collectors, data processors,
    and data product producers—specific indicators that influence revenue distribution
    among participants within each role are constructed respectively. For original
    data collectors, their contribution lies in planning the collection of high-value
    original data. This paper employs three indicators—construction cost, data demand,
    and data characteristics—to adjust the revenue of the original data collection
    participants. Construction cost covers the major costs involved in the production
    process of original data, including sub-indicators of data planning, data collection,
    and data storage. Data demand is assessed by examining the scarcity and application
    value of the data in the market and is further divided into two sub-indicators:
    demand extent and scarcity level. Considering the large volume but low-value density
    of original road data, data coverage and timeliness of updates are included as
    sub-indicators under data characteristics. For the data processors, their contribution
    lies in transforming the original data into high-quality data with application
    value. Two indicators—data cleansing and data analysis—can be adopted to evaluate
    the contribution of data processing participants. Data cleansing is considered
    the fundamental task in data processing, and its effectiveness can be assessed
    using the sub-indicators of data volume and data quality. As the core of extracting
    data value, data analysis can be evaluated based on the quality of analysis methods
    and analysis utility as sub-indicators. For the data product producers, their
    contribution lies in developing products and services for end users based on processed
    data, as well as managing the operation of the products. Two indicators—product
    development and product maintenance—can be employed to assess the contribution
    of data product producers. Product development is evaluated based on the workload
    and difficulty coefficient as sub-indicators, while product maintenance is evaluated
    based on stability and update frequency as sub-indicators. Overall, the evaluation
    indicator system for road data asset revenue allocation constructed in this paper
    comprehensively considers the contributions of different roles and participants
    in the road data asset value chain. The specific indicators are illustrated in
    Fig. 1, and Table 2 provides detailed explanations of the definitions, calculation
    methods, and value ranges for each indicator. Figure 1 Evaluation indicator system
    for road data asset revenue allocation. Full size image Table 2 Description of
    evaluation indicators for road data asset revenue allocation. Full size table
    Conventional Shapley value The Shapley value method is a cooperative game approach
    used to solve the problem of profit distribution in multi-party cooperation. It
    determines the allocation of profits for each participant based on their marginal
    contributions. It is known for its characteristics of simple model construction,
    easy solvability, and unique solutions, allowing for a balance between efficiency
    and fairness in the distribution process. First layer: role revenue allocation
    Suppose in a road data asset revenue distribution, the three roles of original
    data collectors, data processors, and data product producers are represented by
    the set \\(R = \\{ 1,2,3\\}\\). For any subset (representing any combination of
    roles in the role set), there exists a real-valued function \\(v(s)\\), satisfying:
    $$v(\\emptyset ) = 0$$ $$v(s_{1} \\cup s_{2} ) \\ge v(s_{1} ) + v(s_{2} ),\\begin{array}{*{20}c}
    {} & {s_{1} \\cap s_{2} } \\\\ \\end{array} = \\emptyset ,\\begin{array}{*{20}c}
    {} & {s_{1} ,s_{2} \\subset R} \\\\ \\end{array}$$ (1) \\([R,v]\\) is termed the
    cooperation strategy of the three roles, and \\(v\\) represents the characteristic
    function of the cooperation strategy. \\(x_{i}\\) denotes the fraction of the
    maximum revenue \\(v(R)\\) from the road data asset that role \\(i\\) receives.
    Based on the cooperative strategies \\([R,v]\\), the income distribution among
    the three roles is represented by \\(x = (x_{1} ,x_{2} ,x_{3} )\\). A successful
    cooperative strategy must satisfy the following conditions: $$\\begin{array}{*{20}c}
    {x_{1} + x_{2} + x_{3} = v(R)} & {i = 1,2,3} \\\\ \\end{array}$$ $$x_{i} \\ge
    v(i),\\begin{array}{*{20}c} {} & {i = 1,2,3} \\\\ \\end{array}$$ (2) where \\(\\varphi_{i}
    (v)\\) represents the distribution obtained by role \\(i\\) under the cooperative
    strategy \\([R,v]\\). The Shapley value for each role''s income distribution under
    the cooperative strategies is given by \\(\\Phi (v) = (\\varphi_{1} (v),\\varphi_{2}
    (v),\\varphi_{3} (v))\\): $$\\varphi_{i} (v) = \\sum\\limits_{{s \\in s_{i} }}
    {w(\\left| s \\right|)[v(s) - v(s\\backslash i)]\\begin{array}{*{20}c} {} & {i
    = 1,2,3} \\\\ \\end{array} }$$ $$w(\\left| s \\right|) = \\frac{(3 - \\left| s
    \\right|)!(\\left| s \\right| - 1)!}{{3!}}$$ $$\\varphi_{1} (v) + \\varphi_{2}
    (v) + \\varphi_{3} (v) = v(R)$$ (3) where \\(s_{i}\\) is a set containing all
    subsets of \\(R\\) that include role \\(i\\), \\(\\left| s \\right|\\) is the
    number of elements in subset \\(s\\), \\(w(\\left| s \\right|)\\) is the weighting
    factor, \\(v(s)\\) is the revenue for subset \\(s\\), and \\(v(s\\backslash i)\\)
    represents the revenue that can be obtained by removing role \\(i\\) from subset
    \\(s\\). Therefore, the Shapley value method is applied to evaluate the contributions
    of the three roles in the road data asset, and the calculations for revenue allocation
    are presented in Table 3. Table 3 Road data asset role revenue allocation. Full
    size table Second layer: revenue allocation among participants of the same role
    Once the Shapley values \\(\\Phi (v) = (\\varphi_{1} (v),\\varphi_{2} (v),\\varphi_{3}
    (v))\\) for revenue allocation among the three roles in a road data asset are
    determined, it is necessary to determine the specific distribution of benefits
    to the participants under the same role based on the \\(\\varphi_{i} (v)\\) values
    of each role, to realize the distribution of benefits from the road data asset
    to each participant. Assuming that there are \\(n\\) participants in a road data
    asset revenue allocation, the number of participants with the roles of original
    data collectors, data processors, and data product producers is \\(n_{i}\\)(\\(i
    = 1,2,3\\)), and it is clear that \\(n_{1} + n_{2} + n_{3} \\ge n\\). Denote \\(\\varphi_{{_{j}
    }}^{i} (v)\\) as the profit obtained by \\(j{\\text{ th}}\\) participant when
    distributing the profit \\(\\varphi_{i} (v)\\) of role \\(i\\): $$\\varphi_{{_{j}
    }}^{i} (v) = \\sum\\limits_{{s \\in s_{j}^{i} }} {w(\\left| s \\right|)[v(s) -
    v(s\\backslash j)]\\begin{array}{*{20}c} {} & {j = 1,2, \\ldots ,n_{i} } \\\\
    \\end{array} }$$ $$w(\\left| s \\right|) = \\frac{{(n_{i} - \\left| s \\right|)!(\\left|
    s \\right| - 1)!}}{{n_{i} !}}$$ $$\\sum\\limits_{j = 1}^{{n_{i} }} {\\varphi_{j}^{i}
    } (v) = \\varphi_{i} (v)\\begin{array}{*{20}c} {} & {i = 1,2,3} \\\\ \\end{array}$$
    (4) where \\(s_{{_{j} }}^{i}\\) represents the set of all subsets of participants
    within role \\(i\\) that includes participant \\(j\\), \\(\\left| s \\right|\\)
    is the number of elements in subset \\(s\\), \\(w(\\left| s \\right|)\\) is the
    weighting factor, \\(v(s)\\) is the profit for subset \\(s\\), and \\(v(s\\backslash
    j)\\) denotes the profit that can be obtained by excluding participant \\(j\\)
    from subset \\(s\\). Synthesis of revenue allocation among participants After
    calculating the revenue distribution for each participant within each role, it
    is necessary to synthesize the revenue distribution among participants under different
    roles, taking into account their contributions at different stages. Let \\(N =
    \\{ 1,2, \\ldots ,n\\}\\) be the set of participants, and \\(N_{i} = \\{ 1,2,
    \\ldots ,n_{i} \\}\\)(\\(i = 1,2,3\\)) represents the set of participants for
    the roles of original data collectors, data processors, and data product producers,
    respectively. Clearly \\(N_{i} \\subset N\\), due to the different sizes and order
    of elements in sets \\(N\\) and \\(N_{i}\\), we define a function \\(f_{i} :N
    \\to N_{i}\\) that, for each element \\(x\\)(\\(x = 1,2, \\ldots n\\)) in set
    \\(N\\), maps it to the corresponding element in set \\(N_{i}\\), if there exists
    an element \\(j \\in N_{i}\\) such that \\(f_{i} (x) = j\\), otherwise there is
    no corresponding element in set \\(N_{i}\\). Therefore, the profit distribution
    for each participant \\(x\\) in different roles \\(i\\) can be represented as
    \\(\\hat{\\varphi }_{{_{x} }}^{i} (v)\\), where: $$\\hat{\\varphi }_{{_{x} }}^{i}
    (v) = \\left\\{ {\\begin{array}{*{20}r} \\hfill {\\varphi_{{_{j} }}^{i} (v),\\begin{array}{*{20}c}
    {} & {if} \\\\ \\end{array} \\begin{array}{*{20}c} {} & {f_{i} (x) = j} \\\\ \\end{array}
    } \\\\ \\hfill {0,\\begin{array}{*{20}c} {} & {} & {if\\begin{array}{*{20}c} {}
    & {f_{i} (x) \\ne j} \\\\ \\end{array} } \\\\ \\end{array} } \\\\ \\end{array}
    } \\right.\\begin{array}{*{20}c} {} & {i = 1,2,3} \\\\ \\end{array}$$ (5) To synthesize
    the profit values for each participant in different roles, we obtain the total
    profit distribution \\(\\overline{\\varphi }_{x} (v)\\) for the participant in
    the road data asset, denoted as: $$\\overline{\\varphi }_{x} (v) = \\hat{\\varphi
    }_{{_{x} }}^{1} (v) + \\hat{\\varphi }_{{_{x} }}^{2} (v) + \\hat{\\varphi }_{{_{x}
    }}^{3} (v)$$ (6) The modified Shapley value The traditional Shapley value method
    only determines revenue allocation based on marginal contributions, without considering
    differences among participants in terms of costs and risks. To achieve fair profit
    distribution of road data assets, it is essential to comprehensively evaluate
    the differences among roles and participants in terms of input costs, risk allocation,
    and other aspects. In this paper, based on the traditional Shapley value method,
    a revenue allocation evaluation indicator system for the road data asset, as depicted
    in Fig. 1, is established. This indicator-driven two-layer allocation correction
    scheme is used to modify the revenue allocation among different roles and participants.
    By doing so, a more equitable and reasonable revenue allocation model for the
    road data asset is developed. The architecture of the improved revenue allocation
    model for the road data asset is illustrated in Fig. 2. Figure 2 Architecture
    of the improved revenue allocation model for road data assets. Full size image
    Calculation of weights for evaluating revenue allocation of road data assets Calculation
    of primary indicator weight This study utilizes the entropy weighting method to
    calculate the weights of primary evaluation indicators in Fig. 1. It is assumed
    that \\(m\\) expert will be invited to evaluate the importance of \\(I\\) primary
    indicators and obtain a scoring matrix \\(S = (s_{ij} )_{m \\times I}\\), \\(i
    = 1,2, \\ldots ,m\\), \\(j = 1,2, \\ldots ,I\\), where \\(s_{ij}\\) represents
    the rating provided by the \\(i\\) expert for the \\(j\\) indicator. If \\(j\\)
    denotes a profit-related indicator, normalization is performed according to Eq.
    (7): $$\\hat{s}_{ij} = \\frac{{s_{ij} - \\mathop {\\min }\\limits_{i} \\{ s_{ij}
    \\} }}{{\\mathop {\\max }\\limits_{i} \\{ s_{ij} \\} - \\mathop {\\min }\\limits_{i}
    \\{ s_{ij} \\} }}$$ (7) If \\(j\\) denotes a cost-related indicator, normalization
    is performed according to Eq. (8): $$\\hat{s}_{ij} = \\frac{{\\mathop {\\max }\\limits_{i}
    \\{ s_{ij} \\} - s_{ij} }}{{\\mathop {\\max }\\limits_{i} \\{ s_{ij} \\} - \\mathop
    {\\min }\\limits_{i} \\{ s_{ij} \\} }}$$ (8) The weights \\(p_{ij}\\) of the scores
    given by different experts to each indicator are calculated using the entropy
    weighting method, as shown in Eq. (9): $$p_{ij} = \\frac{{\\hat{s}_{ij} }}{{\\sum\\limits_{i
    = 1}^{m} {\\hat{s}_{ij} } }}$$ (9) The information entropy value \\(e_{j}\\) is
    calculated separately for each indicator \\(j\\) according to \\(p_{ij}\\): $$e_{j}
    = - \\frac{1}{\\ln m}\\sum\\limits_{i = 1}^{m} {p_{ij} \\ln p_{ij} }$$ (10) To
    ensure that the entropy value \\(e_{j}\\) holds numerical significance, we set
    \\(\\ln p_{ij} = 0\\) when \\(p_{ij} = 0\\). The entropy weight \\(\\omega_{j}\\)
    for each indicator is then calculated based on the entropy value \\(e_{j}\\),
    as follows: $$\\omega_{j} = \\frac{{1 - e_{j} }}{{\\sum\\limits_{j = 1}^{I} {(1
    - e_{j} )} }}$$ (11) Calculation of secondary indicator weight For the secondary
    evaluation indicators in Fig. 1, the rough set theory is employed in this study
    to calculate their indicator weights. It is assumed that \\(m\\) experts are invited
    to assess the importance of \\(I_{j}\\) secondary indicators under the \\(j\\)(\\(j
    = 1,2, \\ldots ,I\\)) primary indicator, leading to the construction of an evaluation
    information system \\(S_{j} = (U,A_{j} ,V_{j} ,f)\\), where: the universe of discourse
    \\(U = \\{ 1,2, \\ldots ,m\\}\\), a non-empty finite attribute set \\(A_{j} =
    \\{ a_{1} ,a_{2} , \\ldots ,a_{{I_{j} }} \\}\\), and the attribute value domain
    \\(V_{j}\\) are obtained through expert assessment using a percentage-based scoring
    system. Moreover, \\(f\\) represents the relationship set between \\(U\\) and
    \\(A_{j}\\), also referred to as the information function set. Definition 1 Let
    \\(R\\) be an equivalence relation on \\(U\\), denoted as: $$ind(R) = \\{ (x,y)
    \\in U \\times U|\\forall a \\in A_{j} ,f(x,a) = f(y,a)\\}$$ (12) \\(U/ind(R)\\)
    is referred to as the partition of \\(U\\), and each element \\(a\\) is called
    an equivalence class. In an information system \\(S_{j}\\), different attributes
    have varying effects, and some attributes may even be redundant. Therefore, it
    is necessary to eliminate irrelevant or unimportant knowledge from the information
    system while maintaining its classification ability. This process is known as
    knowledge reduction. Knowledge reduction is divided into attribute reduction and
    attribute value reduction. However, since attribute value reduction is relatively
    straightforward, knowledge reduction generally refers to attribute reduction in
    most cases. Definition 2 If \\(ind(R) = ind(R - \\{ r\\} )\\), it is referred
    to \\(r\\) as reducible knowledge in the information system \\(R\\). If \\(P =
    R - \\{ r\\}\\) is independent, then \\(P\\) is a knowledge reduction in \\(R\\).
    In the information system \\(S_{j}\\), the set of secondary indicators for the
    primary indicator \\(j\\) is denoted as \\(A_{j} = \\{ a_{1} ,a_{2} , \\ldots
    ,a_{{I_{j} }} \\}\\). Assume that there are \\(l_{j}\\) sets of \\(A_{j}\\) divisions
    over \\(U\\), represented as \\(U/ind(A) = \\{ X_{1} ,X_{2} , \\ldots ,X_{{l_{j}
    }} \\}\\). The information quantity of \\(A_{j}\\) is calculated as: $$I(A_{j}
    ) = \\sum\\limits_{i = 1}^{{l_{j} }} {\\frac{{\\left| {X_{i} } \\right|}}{\\left|
    U \\right|}\\left[ {1 - \\frac{{\\left| {X_{i} } \\right|}}{\\left| U \\right|}}
    \\right]} = 1 - \\frac{1}{{\\left| U \\right|^{2} }}\\sum\\limits_{i = 1}^{{l_{j}
    }} {\\left| {X_{i} } \\right|^{2} }$$ (13) where \\(\\left| U \\right|\\) represents
    the number of elements in the universe of discourse \\(U\\), and \\(\\left| {X_{i}
    } \\right|\\) denotes the number of elements in the \\(i{\\text{ th}}\\) set.
    In the information system \\(S_{j}\\), for the knowledge reduction \\(ind(A_{j}
    - \\{ a\\} )\\) of \\(\\forall a \\in A_{j}\\), let there exist \\(l_{a}\\) sets
    of the partition of \\(U\\) after reduction, denoted as \\(U/ind(A_{j} - \\{ a\\}
    ) = \\{ X_{1} ,X_{2} , \\ldots ,X_{{l_{a} }} \\}\\). The information quantity
    of \\(A_{j} - \\{ a\\}\\) is given by: $$I(A_{j} - \\{ a\\} ) = \\sum\\limits_{i
    = 1}^{{l_{a} }} {\\frac{{\\left| {X_{i} } \\right|}}{\\left| U \\right|}\\left[
    {1 - \\frac{{\\left| {X_{i} } \\right|}}{\\left| U \\right|}} \\right]} = 1 -
    \\frac{1}{{\\left| U \\right|^{2} }}\\sum\\limits_{i = 1}^{{l_{a} }} {\\left|
    {X_{i} } \\right|^{2} }$$ (14) Therefore, the importance of \\(a\\) in \\(A_{j}\\)
    can be expressed as: $$Sig_{{A_{j} }} (a) = I(A_{j} ) - I(A_{j} - \\{ a\\} )$$
    (15) The weights of secondary indicators \\(A_{j} = \\{ a_{1} ,a_{2} , \\ldots
    ,a_{{I_{j} }} \\}\\) under the primary indicator \\(j\\) can be calculated based
    on their importance using the equation: $$\\omega_{{A_{j} }} (a) = \\frac{{Sig_{{A_{j}
    }} (a)}}{{\\sum\\limits_{a = 1}^{{I_{j} }} {Sig_{{A_{j} }} (a)} }}$$ (16) By incorporating
    the entropy weight \\(\\omega_{j}\\) of the primary indicator \\(j\\), the final
    weights of the secondary indicators \\(A_{j} = \\{ a_{1} ,a_{2} , \\ldots ,a_{{I_{j}
    }} \\}\\) can be determined as: $$\\tilde{\\omega }_{{A_{j} }} (a) = \\omega_{j}
    \\times \\omega_{{A_{j} }} (a),\\;\\;\\;\\;\\;a = 1,2, \\cdots I_{j} ,j = 1,2,
    \\ldots ,I$$ (17) Evaluation of revenue allocation indicators for road data assets
    Once the weights of the revenue allocation evaluation indicators for road data
    assets are determined, it is necessary to numerically evaluate different schemes
    under the relevant indicators. As some indicators involve subjective measures
    and others are objective numerical metrics, different methods are required to
    quantify both subjective and objective factors for an effective assessment of
    revenue allocation indicators for road data assets. Defining a scheme as a collective
    term for subjects involved in revenue allocation across different layers, the
    scheme represents roles at the first layer and participants within each role at
    the second layer. Assuming that there are \\(D\\) schemes involved in the distribution
    of a road data asset, scheme \\(d\\)(\\(d = 1,2, \\ldots ,D\\)), requires a comprehensive
    evaluation of all secondary indicators under \\(I\\) primary indicators. Let there
    be \\(I_{j}\\) secondary indicators under the \\(j{\\text{ th}}\\)(\\(j = 1,2,
    \\ldots ,I\\)) primary indicator, and the set of indicators is \\(A_{j} = \\{
    a_{1} ,a_{2} , \\ldots ,a_{{I_{j} }} \\}\\), of which there are \\(\\dot{I}_{j}\\)
    subjective indicators and \\(\\ddot{I}_{j}\\) objective indicators, and \\(\\dot{I}_{j}
    + \\ddot{I}_{j} = I_{j}\\), let the set of subjective secondary indicators under
    the \\(j{\\text{ th}}\\) primary indicator be \\(A^{\\prime}_{j} = \\{ a^{\\prime}_{1}
    ,a^{\\prime}_{2} , \\ldots ,a^{\\prime}_{{\\dot{I}_{j} }} \\}\\), \\(a^{\\prime}_{i}
    \\in A_{j} ,i = 1,2, \\ldots ,\\dot{I}_{j}\\), and the set of objective secondary
    indicators be \\(A^{\\prime\\prime}_{j} = \\{ a^{\\prime\\prime}_{1} ,a^{\\prime\\prime}_{2}
    , \\ldots ,a^{\\prime\\prime}_{{\\ddot{I}_{j} }} \\}\\), \\(a^{\\prime\\prime}_{i}
    \\in A_{j} ,i = 1,2, \\ldots ,\\ddot{I}_{j}\\), and \\(A^{\\prime}_{j} \\cup A^{\\prime\\prime}_{j}
    = A_{j}\\), \\(A^{\\prime}_{j} \\cap A^{\\prime\\prime}_{j} = \\emptyset\\). Subjective
    evaluation Suppose \\(m\\) experts are invited to assess scheme \\(d\\) based
    on the subjective indicator set \\(A^{\\prime}_{j} = \\{ a^{\\prime}_{1} ,a^{\\prime}_{2}
    , \\ldots ,a^{\\prime}_{{\\dot{I}_{j} }} \\}\\) for indicator \\(j\\). Based on
    the comment set \\(V =\\){low, moderately low, moderate, moderately high, high},
    a fuzzy evaluation is conducted to obtain the fuzzy relationship matrix: $$R_{j}
    (d) = \\left[ {\\begin{array}{*{20}c} {r_{{a^{\\prime}_{1} }}^{1} (d)} & {r_{{a^{\\prime}_{1}
    }}^{2} (d)} & {r_{{a^{\\prime}_{1} }}^{3} (d)} & {r_{{a^{\\prime}_{1} }}^{4} (d)}
    & {r_{{a^{\\prime}_{1} }}^{5} (d)} \\\\ {r_{{a^{\\prime}_{2} }}^{1} (d)} & {r_{{a^{\\prime}_{2}
    }}^{2} (d)} & {r_{{a^{\\prime}_{2} }}^{3} (d)} & {r_{{a^{\\prime}_{2} }}^{4} (d)}
    & {r_{{a^{\\prime}_{2} }}^{5} (d)} \\\\ \\vdots & \\vdots & \\vdots & \\vdots
    & \\vdots \\\\ {r_{{a^{\\prime}_{{\\dot{I}_{j} }} }}^{1} (d)} & {r_{{a^{\\prime}_{{\\dot{I}_{j}
    }} }}^{2} (d)} & {r_{{a^{\\prime}_{{\\dot{I}_{j} }} }}^{3} (d)} & {r_{{a^{\\prime}_{{\\dot{I}_{j}
    }} }}^{4} (d)} & {r_{{a^{\\prime}_{{\\dot{I}_{j} }} }}^{5} (d)} \\\\ \\end{array}
    } \\right]$$ (18) Among them, \\(r_{{a^{\\prime}_{i} }}^{1} (d)\\), \\(r_{{a^{\\prime}_{i}
    }}^{2} (d)\\), \\(r_{{a^{\\prime}_{i} }}^{3} (d)\\), \\(r_{{a^{\\prime}_{i} }}^{4}
    (d)\\), and \\(r_{{a^{\\prime}_{i} }}^{5} (d)\\) respectively represent the frequency
    distribution of indicator \\(a^{\\prime}_{i}\\)(\\(i = 1,2, \\ldots ,\\dot{I}_{j}\\))
    under the five comments of low, moderately low, moderate, moderately high, and
    high. Based on the indicator weights calculated according to Eq. (17), the subjective
    indicator weight vector for Indicator \\(A^{\\prime}_{j}\\) is denoted as \\(\\tilde{\\omega
    }_{{A^{\\prime}_{j} }} = [\\tilde{\\omega }_{{A^{\\prime}_{j} }} (a^{\\prime}_{1}
    ),\\tilde{\\omega }_{{A^{\\prime}_{j} }} (a^{\\prime}_{2} ), \\ldots ,\\tilde{\\omega
    }_{{A^{\\prime}_{j} }} (a^{\\prime}_{{\\dot{I}_{j} }} )]\\). Using this weight
    vector, the fuzzy evaluation vector is obtained as: $$T_{j} (d) = \\tilde{\\omega
    }_{{A^{\\prime}_{j} }} \\times R_{j} (d)$$ (19) where \\(T_{j} (d)\\) is referred
    to as the fuzzy evaluation vector. Using the membership degree of the comment
    set \\(V =\\){low, moderately low, moderate, moderately high, high}, the membership
    degree vector \\(\\overline{V} = [0.1,0.3,0.5,0.7,0.9]\\) can be determined. From
    this, the evaluation value \\(L^{\\prime}_{j} (d)\\) of the subjective component
    for indicator \\(j\\) can be calculated as: $$L^{\\prime}_{j} (d) = T_{j} (d)
    \\times \\overline{V}^{T}$$ (20) The subjective evaluation values for \\(I\\)
    primary indicators are synthesized as: $$L^{\\prime}(d) = \\sum\\limits_{j = 1}^{I}
    {L^{\\prime}_{j} (d)}$$ (21) where \\(L^{\\prime}(d)\\) is termed as the subjective
    evaluation value of scheme \\(d\\). Objective evaluation In the set of objective
    indicators \\(A^{\\prime\\prime}_{j} = \\{ a^{\\prime\\prime}_{1} ,a^{\\prime\\prime}_{2}
    , \\ldots ,a^{\\prime\\prime}_{{\\ddot{I}_{j} }} \\}\\), the numerical value for
    each indicator of Scheme \\(d\\) is represented by a vector, denoted as \\(f_{d}
    (A^{\\prime\\prime}_{j} ) = [f_{d} (a^{\\prime\\prime}_{1} ),f_{d} (a^{\\prime\\prime}_{2}
    ), \\ldots ,f_{d} (a^{\\prime\\prime}_{{\\ddot{I}_{j} }} )]\\). For the indicator
    \\(a^{\\prime\\prime}_{i}\\)(\\(i = 1,2, \\ldots ,\\ddot{I}_{j}\\)), if it is
    a revenue indicator, it is normalized on the scheme \\(D\\) according to Eq. (22),
    and if it is a cost indicator, the values are normalized using Eq. (23). This
    normalization process yields the normalized value vector, denoted as \\(\\tilde{f}_{d}
    (A^{\\prime\\prime}_{j} ) = [\\tilde{f}_{d} (a^{\\prime\\prime}_{1} ),\\tilde{f}_{d}
    (a^{\\prime\\prime}_{2} ), \\ldots ,\\tilde{f}_{d} (a^{\\prime\\prime}_{{\\ddot{I}_{j}
    }} )]\\), where it is evident that \\(\\sum\\limits_{d = 1}^{D} {\\tilde{f}_{d}
    (a^{\\prime\\prime}_{i} } ) = 1\\). $$\\tilde{f}_{d} (a^{\\prime\\prime}_{i} )
    = \\frac{{f_{d} (a^{\\prime\\prime}_{i} )}}{{\\sum\\limits_{d = 1}^{D} {f_{d}
    (a^{\\prime\\prime}_{i} } )}}$$ (22) $$\\tilde{f}_{d} (a^{\\prime\\prime}_{i}
    ) = 1 - \\frac{{f_{d} (a^{\\prime\\prime}_{i} )}}{{\\sum\\limits_{d = 1}^{D} {f_{d}
    (a^{\\prime\\prime}_{i} } )}}$$ (23) According to the weights of the indicators
    calculated in Eq. (17), the weight vector \\(\\tilde{\\omega }_{{A^{\\prime\\prime}_{j}
    }} = [\\tilde{\\omega }_{{A^{\\prime\\prime}_{j} }} (a^{\\prime\\prime}_{1} ),\\tilde{\\omega
    }_{{A^{\\prime\\prime}_{j} }} (a^{\\prime\\prime}_{2} ), \\ldots ,\\tilde{\\omega
    }_{{A^{\\prime\\prime}_{j} }} (a^{\\prime\\prime}_{{\\ddot{I}_{j} }} )]\\) of
    the objective indicator \\(A^{\\prime\\prime}_{j}\\) can be obtained, and based
    on the vector of normalized values \\(\\tilde{f}_{d} (A^{\\prime\\prime}_{j} )\\),
    the evaluation value of the objective part of the indicator \\(j\\) is calculated
    as \\(L^{\\prime\\prime}_{j} (d)\\): $$L^{\\prime\\prime}_{j} (d) = \\tilde{\\omega
    }_{{A^{\\prime\\prime}_{j} }} \\times \\tilde{f}_{d} (A^{\\prime\\prime}_{j} )^{T}$$
    (24) Synthesize the assessed value of the objective component of the \\(I\\) primary
    indicators indicator: $$L^{\\prime\\prime}(d) = \\sum\\limits_{j = 1}^{I} {L^{\\prime\\prime}_{j}
    (d)}$$ (25) where \\(L^{\\prime\\prime}(d)\\) is called the objective evaluation
    value of scheme \\(d\\). Integration of objective and subjective evaluations Combine
    the subjective and objective evaluation values for scheme \\(d\\) to obtain the
    composite evaluation value. $$L(d) = \\alpha L^{\\prime}(d) + (1 - \\alpha )L^{\\prime\\prime}(d),d
    = 1,2, \\ldots ,D$$ (26) where \\(L(d)\\) is the composite evaluated value of
    scheme \\(d\\) and \\(\\alpha\\)(\\(0 \\le \\alpha \\le 1\\)) is the weighting
    factor, allowing for the adjustment of the importance of subjective and objective
    evaluation values in the composite evaluated value. Normalize the composite evaluated
    value \\(L(d)\\) of scheme \\(d\\): $$\\tilde{L}(d) = \\frac{L(d)}{{\\sum\\limits_{d
    = 1}^{D} {L(d)} }},d = 1,2, \\ldots ,D$$ (27) The modification of road data asset
    revenue allocation Role revenue allocation modification Based on Eq. (3), the
    initial allocations for the roles \\(R = \\{ 1,2,3\\}\\) of the original data
    collectors, data processors, and data product producers can be computed, denoted
    as \\(\\Phi (v) = (\\varphi_{1} (v),\\varphi_{2} (v),\\varphi_{3} (v))\\). Additionally,
    it is known that \\(\\varphi_{1} (v) + \\varphi_{2} (v) + \\varphi_{3} (v) = v(R)\\),
    where \\(v(R)\\) represents the maximum revenue for the road data asset. As illustrated
    in Fig. 2a, based on the model in Section \"Evaluation of revenue allocation indicators
    for road data assets\", the comprehensive evaluation values \\(\\tilde{L}_{R}
    (1)\\), \\(\\tilde{L}_{R} (2)\\), and \\(\\tilde{L}_{R} (3)\\) for the roles of
    the original data collectors, data processors, and data product producers can
    be calculated. Next, compute the role revenue allocation modification factor:
    $$\\Delta \\theta_{i} = \\tilde{L}_{R} (i) - \\frac{1}{3},i = 1,2,3$$ (28) The
    modified value of the role''s revenue allocation is: $$\\tilde{\\varphi }_{i}
    (v) = \\varphi_{i} (v) + \\Delta \\theta_{i} \\times v(R),i = 1,2,3$$ (29) Participant
    revenue allocation modification within the same role Suppose there are \\(n\\)
    participants involved in the distribution of road data asset profits, and the
    number of participants in the roles of data collectors, data processors, and data
    product producers is denoted as \\(n_{i}\\)(\\(i = 1,2,3\\)). According to Eq.
    (4), we determine the initial distribution scheme \\(\\Phi^{i} (v) = (\\varphi_{{_{1}
    }}^{i} (v),\\varphi_{{_{2} }}^{i} (v), \\ldots ,\\varphi_{{_{{n_{i} }} }}^{i}
    (v))\\) of participants within the role \\(i\\) based on the role revenue allocation
    modified value \\(\\tilde{\\varphi }_{i} (v)\\), where \\(\\sum\\limits_{j = 1}^{{n_{i}
    }} {\\varphi_{j}^{i} (v)} = \\tilde{\\varphi }_{i} (v),i = 1,2,3\\). As shown
    in Fig. 2b, applying the model in Section \"Evaluation of revenue allocation indicators
    for road data assets\", we can calculate the comprehensive evaluation value \\([\\tilde{L}^{1}
    (1),\\tilde{L}^{1} (2), \\ldots ,\\tilde{L}^{1} (n_{1} )]\\) for participants
    within the data collectors. To modify the revenue allocation for participants
    within the data collectors, we compute the participant modification factor as
    follows: $$\\Delta \\theta_{j}^{1} = \\tilde{L}^{1} (j) - \\frac{1}{{n_{1} }},j
    = 1,2, \\ldots ,n_{1}$$ (30) The modified values for participant revenue allocation
    within the data collectors are then given by: $$\\tilde{\\varphi }_{j}^{1} (v)
    = \\varphi_{j}^{1} (v) + \\Delta \\theta_{j}^{1} \\times \\tilde{\\varphi }_{1}
    (v)$$ (31) Similarly, using the model in Section \"Evaluation of revenue allocation
    indicators for road data assets\", we can calculate the comprehensive evaluation
    value \\([\\tilde{L}^{2} (1),\\tilde{L}^{2} (2), \\ldots ,\\tilde{L}^{2} (n_{2}
    )]\\) for participants within the data processors. For the data processors, the
    participant revenue allocation modification factor is calculated as follows: $$\\Delta
    \\theta_{j}^{2} = \\tilde{L}^{2} (j) - \\frac{1}{{n_{2} }},j = 1,2, \\ldots ,n_{2}$$
    (32) The modified values for participant revenue allocation within the data processors
    are then obtained as: $$\\tilde{\\varphi }_{j}^{2} (v) = \\varphi_{j}^{2} (v)
    + \\Delta \\theta_{j}^{2} \\times \\tilde{\\varphi }_{2} (v)$$ (33) Likewise,
    considering the model in Section \"Evaluation of revenue allocation indicators
    for road data assets\", we can compute the comprehensive evaluation value \\([\\tilde{L}^{3}
    (1),\\tilde{L}^{3} (2), \\ldots ,\\tilde{L}^{3} (n_{3} )]\\) for participants
    within the data product producers. To modify the revenue allocation for participants
    within the data product producers, we calculate the participant revenue allocation
    modification factor as follows: $$\\Delta \\theta_{j}^{3} = \\tilde{L}^{3} (j)
    - \\frac{1}{{n_{3} }},j = 1,2, \\ldots ,n_{3}$$ (34) Finally, the modified values
    for participant revenue allocation within the data product producers are given
    by: $$\\tilde{\\varphi }_{j}^{3} (v) = \\varphi_{j}^{3} (v) + \\Delta \\theta_{j}^{3}
    \\times \\tilde{\\varphi }_{3} (v)$$ (35) Final revenue allocation scheme for
    participants As depicted in Fig. 2(c), using Eq. (5), we determine the revenue
    allocation modified values for the \\(n\\) participants across the three roles:
    $$\\hat{\\varphi }_{{_{x} }}^{i} (v) = \\left\\{ {\\begin{array}{*{20}r} \\hfill
    {\\tilde{\\varphi }_{{_{j} }}^{i} (v),\\begin{array}{*{20}c} {} & {if} \\\\ \\end{array}
    \\begin{array}{*{20}c} {} & {f_{i} (x) = j} \\\\ \\end{array} } \\\\ \\hfill {0,\\begin{array}{*{20}c}
    {} & {} & {if\\begin{array}{*{20}c} {} & {f_{i} (x) \\ne j} \\\\ \\end{array}
    } \\\\ \\end{array} } \\\\ \\end{array} } \\right.\\begin{array}{*{20}c} {} &
    {x = 1,2, \\ldots ,n;\\begin{array}{*{20}c} {} \\\\ \\end{array} i = 1,2,3} \\\\
    \\end{array}$$ (36) By synthesizing the profit values for each participant across
    the different roles, we obtain the final revenue allocation values for each participant
    involved in the road data asset: $$\\overline{\\varphi }_{x} (v) = \\hat{\\varphi
    }_{{_{x} }}^{1} (v) + \\hat{\\varphi }_{{_{x} }}^{2} (v) + \\hat{\\varphi }_{{_{x}
    }}^{3} (v)\\begin{array}{*{20}c} {} & {x = 1,2, \\ldots ,n} \\\\ \\end{array}$$
    (37) Case study Assuming that the sale of a road data asset obtains total proceeds
    of 960,000 RMB, the revenue need to be allocated to the five enterprises \\(N
    = \\{ 1,2,3,4,5\\}\\) involved in data collection, processing and production.
    According to the process of realizing the value of road data, enterprises can
    be divided into three types of roles \\(R = \\{ 1,2,3\\}\\): the original data
    collectors, the data processors and the data product producers, and the set of
    participating enterprises under the three types of roles are \\(N_{1} = \\{ 1,2\\}\\),
    \\(N_{2} = \\{ 2,3,4\\}\\), and \\(N_{3} = \\{ 4,5\\}\\), respectively. Based
    on our investigation, we found that selling the original data directly can generate
    revenue of 300,000 RMB while processing the original data and selling it can bring
    in revenue of 420,000 RMB. Developing the original data into data products and
    selling them can yield revenue of 660,000 RMB. Without the original data, neither
    the data processors nor the data product producers can generate any revenue, regardless
    of whether they operate individually or in cooperation. The income values and
    indicator values for the participating enterprises in each role are reasonably
    assumed, as shown in Tables 4, 5, 6 and 7. Table 4 Revenue situation of participating
    enterprise combinations under each role (unit: ten thousand RMB). Full size table
    Table 5 Indicator values for participating enterprises under the original data
    collectors. Full size table Table 6 Indicator values for participating enterprises
    under the data processors. Full size table Table 7 Indicator values for participating
    enterprises under the data product producers. Full size table Role revenue allocation
    With reference to the revenue data in Table 3, the initial revenue allocation
    for the original data collectors, the data processors, and the data product producers
    is calculated using the traditional Shapley value method, as presented in Table
    8. Table 8 Initial revenue allocation for the three roles. Full size table Determine
    the weights of the evaluation indexes for the role revenue allocation. Ten experts
    in the field of road data assets were asked to evaluate the importance of two
    primary indicators, external risk, and internal risk, using a 1–9 scale. The weights
    of these indicators were then determined using the entropy weight method. The
    scoring results provided by the experts are presented in Table 9. Table 9 Scoring
    results of the role''s primary indicators. Full size table According to Eqs. (7)
    and (9), the scoring results were normalized and the weights \\(p_{ij}\\) were
    calculated as shown in Table 10. Table 10 Scoring weights for the role''s primary
    indicators. Full size table The information entropy values and entropy weights
    of the indicators were calculated according to Eqs. (10) and (11), as shown in
    Table 11. The weights for the primary evaluation indicators of the roles, denoted
    as \\(\\omega_{1} = 0.444\\) and \\(\\omega_{2} = 0.556\\), were obtained. Table
    11 Process of calculating entropy weights for the role''s primary indicators.
    Full size table The secondary evaluation indicators for the roles were scored
    on a percentage scale, with higher scores indicating greater importance of the
    indicators. The scoring results are presented in Table 12. Table 12 Scoring results
    of the roles secondary indicators. Full size table To facilitate further analysis
    and capture more common features in the sample data, it is necessary to abstract
    the indicator scores into higher-level data. Considering the simplification of
    the model, an unsupervised distance-based method was employed in this study to
    classify the expert scoring results into three categories, as shown in Table 13.
    In future research, more scientifically designed and applicable classification
    methods can be developed based on the characteristics of the scoring data to enhance
    effectiveness and reliability. Table 13 Classification results of secondary indicator
    scores for roles. Full size table The weights of the secondary indicators under
    external risks and internal risks were calculated according to Eqs. (13)–(16),
    as shown in Table 14. It is worth noting that the elements within the sets in
    Table 14 correspond to the indices of the scoring experts in Table 13. Table 14
    Process of calculating weights for the role''s secondary indicators. Full size
    table Using Eq. (17), the final weights for the secondary indicators under external
    risks and internal risks are \\(\\tilde{\\omega }_{{A_{1} }} = [0.280,0.164]\\)
    and \\(\\tilde{\\omega }_{{A_{2} }} = [0.234,0.322]\\), respectively. The evaluation
    of the secondary indicators under external risks and internal risks for each role
    is subjective. The evaluation process for the indicators of each role is shown
    in Table 15. Table 15 The risk indicator evaluation process for each role. Full
    size table Since the evaluation indicators for the original data collectors, data
    processors, and data product producers are all subjective indicators, according
    to Eqs. (26) and (27), in this case, we take \\(\\alpha = 1\\) and calculate the
    normalized comprehensive correction values for the three categories of roles as
    \\(\\tilde{L}_{R} (1) = 0.38{5}\\), \\(\\tilde{L}_{R} (2) = 0.{295}\\), and \\(\\tilde{L}_{R}
    (3) = 0.32{0}\\). Furthermore, we can calculate the modified revenue allocation
    values for the three categories of roles as \\(\\tilde{\\varphi }_{1} (v) = 64.{960}\\),
    \\(\\tilde{\\varphi }_{2} (v) = 8.{320}\\), and \\(\\tilde{\\varphi }_{3} (v)
    = 22.72{0}\\). Revenue allocation among participants of the same role Revenue
    allocation among participants of the original data collectors According to Eq.
    (4), the modified revenue allocation from the original data collectors to Enterprise
    1 and Enterprise 2 is calculated as \\(\\varphi_{1}^{1} (v) = 29.{48}\\) and \\(\\varphi_{2}^{1}
    (v) = 35.{48}\\). Using the entropy weight method to calculate the weights of
    the primary indicators that influence the revenue allocation for the participants
    under the original data collectors, similar to Sect. 4.1, the weights assigned
    by experts are shown in Table 16. Table 16 The weighting of primary indicator
    scores for the original data collectors. Full size table The information entropy
    values and entropy weights of the primary indicators for the original data collectors
    are calculated, resulting in indicator weights of \\(\\omega_{3} = 0.238\\), \\(\\omega_{4}
    = 0.337\\), and \\(\\omega_{5} = 0.425\\), as shown in Table 17. Table 17 Process
    of calculating entropy weights for the primary indicators of the original data
    collectors. Full size table The weights of the secondary indicators for the original
    data collectors are determined, and the classification results of the expert scores
    are shown in Table 18. Table 18 Classification results of secondary indicator
    scores for the original data collectors. Full size table The weights of the secondary
    indicators that influence the participants under the original data collectors
    are calculated according to Eqs. (13)–(16), and the specific process is displayed
    in Table 19. It is worth noting that the elements within the sets in Table 19
    correspond to the indices of the scoring experts in Table 18. Table 19 Process
    of calculating weights for the secondary indicators of the original data collectors.
    Full size table Considering the weights of the primary indicators, the final weights
    for each indicator under construction cost, data demand, and data characteristics
    are \\(\\tilde{\\omega }_{{A_{3} }} = [0.033,0.135,0.069]\\), \\(\\tilde{\\omega
    }_{{A_{4} }} = [0.169,0.169]\\), and \\(\\tilde{\\omega }_{{A_{5} }} = [0.132,0.293]\\),
    respectively. Evaluation of the participating enterprises under the original data
    collectors is conducted. Data cost is an objective indicator where higher costs
    result in higher allocation values. Therefore, using Eq. (22), the costs of Enterprise
    1 and Enterprise 2 are normalized, resulting in \\(\\tilde{f}_{1} (A_{3}^{2} )
    = [0.600,0.600,0.375]\\) and \\(\\tilde{f}_{2} (A_{3}^{2} ) = [0.400,0.400,0.625]\\).
    Furthermore, according to Eq. (24), the evaluation values for Enterprise 1 and
    Enterprise 2 under data cost are calculated as \\(L_{3}^{\\prime \\prime } (1)
    = 0.127\\) and \\(L_{3}^{\\prime \\prime } (2) = 0.110\\), respectively. Data
    demand is a subjective indicator, and the evaluation values for the participating
    enterprises are determined using the fuzzy comprehensive evaluation method. The
    calculation process is shown in Table 20. Table 20 The evaluation process of data
    demand indicators for enterprises participating under the original data collectors.
    Full size table The evaluation of data characteristics for the participating enterprises
    is conducted. The secondary indicators \" data coverage \" and \" data timeliness
    \" correspond to profit-related and cost-related indicators, respectively. Using
    Eq. (22) and (23), the normalization results for Enterprise 1 and Enterprise 2
    are \\(\\tilde{f}_{1} (A_{5}^{1} ) = [0.400,0.250]\\) and \\(\\tilde{f}_{2} (A_{5}^{1}
    ) = [0.600,0.750]\\), respectively. Subsequently, the evaluation values are calculated
    as \\(L_{5}^{\\prime \\prime } (1) = 0.126\\) and \\(L_{5}^{\\prime \\prime }
    (2) = 0.299\\). The subjective evaluation values and objective evaluation values
    for Enterprise 1 and Enterprise 2 under the original data collectors are obtained
    by summing the subjective and objective evaluation values, resulting in subjective
    evaluation values of \\(L^{1\\prime } (1) = 0.193\\) and \\(L^{1\\prime } (2)
    = 0.196\\) and objective evaluation values of \\(L^{1\\prime \\prime } (1) = 0.253\\)
    and \\(L^{1\\prime \\prime } (2) = 0.409\\). According to Eqs. (26) and (27),
    this study takes \\(\\alpha = 0.5\\) (the value of \\(\\alpha\\) can be adjusted
    according to the actual situation), resulting in normalized comprehensive evaluation
    values for Enterprise 1 and Enterprise 2 of \\(\\tilde{L}^{1} (1) = 0.424\\) and
    \\(\\tilde{L}^{1} (2) = 0.576\\). Further calculations using Eqs. (30) and (31)
    yield the modified revenue allocation values for Enterprise 1 and Enterprise 2
    under the original data collectors as \\(\\tilde{\\varphi }_{1}^{1} (v) = 24.543\\)
    and \\(\\tilde{\\varphi }_{2}^{1} (v) = 40.417\\), respectively. Revenue allocation
    among participants of the data processors Using the traditional Shapley value
    method, the adjusted profit distribution for the data processing party is allocated
    to Enterprise 2, Enterprise 3, and Enterprise 4, resulting in \\(\\varphi_{1}^{2}
    (v) = 1.874\\), \\(\\varphi_{2}^{2} (v) = 3.673\\), and \\(\\varphi_{3}^{2} (v)
    = 2.773\\). Calculate the weighting of primary indicators among participants under
    the data processors, with individual expert rating weights as shown in Table 21.
    According to Eqs. (10) and (11), the entropy weights for the primary indicators
    of the data processing party are calculated as \\(\\omega_{6} = 0.630\\) and \\(\\omega_{7}
    = 0.370\\). Table 21 The weighting of primary indicator scores for the data processors.
    Full size table Determine the weights of secondary indicators for the data processors,
    with the classification results of expert ratings shown in Table 22. The calculation
    process for the secondary indicator weights is presented in Table 23. It is worth
    noting that the elements within the sets in Table 23 correspond to the indices
    of the scoring experts in Table 22. Table 22 Classification results of secondary
    indicator scores for the data processors. Full size table Table 23 Process of
    calculating weights for the secondary indicators of the data processors. Full
    size table Taking into account the entropy weights of the primary indicators,
    the final weights for each indicator under data cleansing and data analysis are
    obtained as \\(\\tilde{\\omega }_{{A_{6} }} = [0.296,0.334]\\) and \\(\\tilde{\\omega
    }_{{A_{7} }} = [0.148,0.222]\\). The participating enterprises under the data
    processors are then evaluated based on the indicators. The secondary indicators
    under data cleansing are all objective indicators, with the evaluation for the
    data quality indicator based on the improvement in the proportion of data that
    meets the criteria of completeness, validity, and consistency compared to the
    original data. The normalized evaluation vectors for Enterprise 2, Enterprise
    3, and Enterprise 4 under the data cleansing indicators are \\(\\tilde{f}_{1}
    (A_{6}^{2} ) = [0.250,0.311]\\), \\(\\tilde{f}_{2} (A_{6}^{2} ) = [0.450,0.331]\\),
    and \\(\\tilde{f}_{3} (A_{6}^{2} ) = [0.300,0.358]\\), respectively, resulting
    in evaluation values of \\(L_{6}^{\\prime \\prime } (1) = 0.178\\), \\(L_{6}^{\\prime
    \\prime } (2) = 0.244\\), and \\(L_{6}^{\\prime \\prime } (3) = 0.208\\). The
    data analysis situation of the participating enterprises is evaluated using fuzzy
    evaluation, as shown in Table 24. Table 24 The evaluation process of data analysis
    indicators for enterprises participating under the data processors. Full size
    table The subjective evaluation values for Enterprise 2, Enterprise 3, and Enterprise
    4 under data analysis are \\(L^{2\\prime } (1) = 0.185\\), \\(L^{2\\prime } (2)
    = 0.203\\), and \\(L^{2\\prime } (3) = 0.207\\), respectively, while the objective
    evaluation values are \\(L^{2\\prime \\prime } (1) = 0.178\\), \\(L^{2\\prime
    \\prime } (2) = 0.244\\), and \\(L^{2\\prime \\prime } (3) = 0.208\\). By synthesizing
    both subjective and objective evaluation values and normalizing them, we obtain
    \\(\\tilde{L}^{2} (1) = 0.296\\), \\(\\tilde{L}^{2} (2) = 0.365\\), \\(\\tilde{L}^{2}
    (3) = 0.339\\). Consequently, the modified revenue allocation values for Enterprise
    2, Enterprise 3, and Enterprise 4 under the data processors are \\(\\tilde{\\varphi
    }_{1}^{2} (v) = 1.564\\), \\(\\tilde{\\varphi }_{2}^{2} (v) = 3.936\\), \\(\\tilde{\\varphi
    }_{3}^{2} (v) = 2.820\\). Revenue allocation among participants of the data product
    producers According to Eq. (4), the initial revenue allocation for Enterprise
    4 and Enterprise 5 under the data product producers is calculated as \\(\\varphi_{1}^{{3}}
    (v) = {9}{\\text{.360}}\\) and \\(\\varphi_{{2}}^{{3}} (v) = {13}{\\text{.360}}\\).
    Using Eqs. (7)–(11), the weights of the primary indicators under the data product
    producers are calculated as \\(\\omega_{8} = 0.440\\) and \\(\\omega_{9} = 0.560\\),
    as shown in Table 25. Table 25 Process of calculating entropy weights for the
    primary indicators of the data product producers. Full size table Experts are
    invited to evaluate the factors influencing the data product producers, and their
    ratings are used to determine the weights based on rough set theory. The classification
    results of the expert ratings are presented in Table 26. By applying Eqs. (13)–(16),
    the weights for the secondary indicators under product development are calculated
    as \\(\\omega_{{A_{8} }} (a_{1} ) = 0.690\\) and \\(\\omega_{{A_{8} }} (a_{2}
    ) = 0.310\\), while the weights for the secondary indicators under product maintenance
    are calculated as \\(\\omega_{{A_{9} }} (a_{1} ) = 0.530\\) and \\(\\omega_{{A_{9}
    }} (a_{2} ) = 0.470\\). Combining the entropy weights of the primary indicators
    for the data product producers, the final weights for the secondary indicators
    are obtained as \\(\\tilde{\\omega }_{{A_{8} }} = [0.304,0.136]\\) and \\(\\tilde{\\omega
    }_{{A_{9} }} = [0.297,0.263]\\). Table 26 Classification results of secondary
    indicator scores for the data product producers. Full size table The normalized
    numerical vectors for Enterprise 4 regarding product development and product maintenance
    indicators are \\(\\tilde{f}_{1} (A_{8}^{2} ) = [0.457,0.429]\\) and \\(\\tilde{f}_{1}
    (A_{9}^{2} ) = [0.518,0.400]\\), while for Enterprise 5, they are \\(\\tilde{f}_{2}
    (A_{8}^{2} ) = [0.523,0.571]\\), \\(\\tilde{f}_{2} (A_{9}^{2} ) = [0.482,0.600]\\).Considering
    the weights of the secondary indicators, the evaluation values for Enterprise
    4 under the data product producers are calculated as \\(L^{\\prime\\prime}_{8}
    (1) = 0.197\\) and \\(L^{\\prime\\prime}_{9} (1) = 0.259\\), while for Enterprise
    5, they are \\(L^{\\prime\\prime}_{8} (2) = 0.237\\) and \\(L^{\\prime\\prime}_{9}
    (2) = 0.301\\). Combining the evaluation values of the two primary objective indicators
    yields \\(L^{3\\prime \\prime } (1) = 0.456\\) and \\(L^{3\\prime \\prime } (2)
    = 0.538\\). Since all the evaluation indicators for the data product producers
    are objective, the term \\(\\alpha\\) in Eq. (26) is set to 0, resulting in a
    normalized comprehensive evaluation value of \\(\\tilde{L}^{3} (1) = 0.459\\)
    and \\(\\tilde{L}^{3} (2) = 0.531\\). Therefore, the modified revenue allocation
    values for Enterprise 4 and Enterprise 5 under the data product producers are
    \\(\\tilde{\\varphi }_{1}^{3} (v) = 8.428\\) and \\(\\tilde{\\varphi }_{2}^{3}
    (v) = 14.292\\). Final revenue allocation scheme for participants After obtaining
    the modified revenue allocation values for each participant under each role, it
    is necessary to synthesize the profit values for the actual participants since
    there is an intersection of participants across roles. In this case, the set of
    all participating enterprises is denoted as \\(N = \\{ 1,2,3,4,5\\}\\), and the
    participating enterprise sets for the original data collectors, the data processors,
    and the data product producers are \\(N_{1} = \\{ 1,2\\}\\), \\(N_{2} = \\{ 2,3,4\\}\\),
    and \\(N_{3} = \\{ 4,5\\}\\), respectively. The final profit values for the participating
    enterprises are calculated using Eqs. (36) and (37), as shown in Table 27. Table
    27 The final revenue allocation scheme for participating enterprises (unit: ten
    thousand RMB). Full size table Conclusions The circulation and trading of road
    data assets contribute to enhancing data value and promoting digital transportation
    applications. A fair and reasonable revenue allocation mechanism is key to achieving
    this goal. This paper proposes a two-layer revenue allocation model for road data
    assets based on a modified Shapley value. The model first categorizes participating
    companies into three roles: original data collectors, data processors, and data
    product producers, based on the process of realizing data value. Subsequently,
    a revenue allocation evaluation index system is established, considering the characteristics
    of different roles. At the first layer, the model allocates revenues among the
    three roles and introduces risk indicators for adjustment purposes. At the second
    layer, the model redistributes the adjusted revenues to participating companies
    under each role, while designing evaluation indicators specific to each role to
    modify the initial revenue allocation for each company. Finally, the profits of
    participating companies under each role are synthesized to obtain the final profit
    allocation for each company. This two-layer approach, combining the Shapley value
    with modifications, achieves a fair and effective distribution of road data asset
    profits. Case studies verify that the model effectively addresses the revenue
    allocation issues among multiple roles in the road data asset value chain, achieving
    fair and reasonable allocation results. The innovation of this model lies in the
    role categorization and two-layer revenue allocation mechanism, which fully considers
    the characteristics and contributions of different roles, as well as the differences
    among participating companies within the same role, thereby achieving a fair and
    reasonable profit allocation. Specifically, the evaluation index system can be
    flexibly adjusted according to the actual situation. This research provides new
    ideas and methods for the revenue allocation of road data assets, offering important
    references for promoting the utilization and circulation of road data assets.
    Data availability The datasets used and/or analyzed during the current study are
    available from the corresponding author upon reasonable request. References China
    State Administration for Market Regulation & China Standardization Administration.
    Information Technology Service: Data Asset: Management Requirements (Standards
    Press of China, 2021). Google Scholar   Wu, L. C. & Wu, L. H. Pharmaceutical patent
    evaluation and licensing using a stochastic model and Monte Carlo simulations.
    Nat. Biotechnol. 29, 789–801 (2011). Article   Google Scholar   Singh, R., Rohit,
    S. & Shaik, V. A. Highway 4.0: Digitalization of highways for vulnerable road
    safety development with intelligent IoT sensors and machine learning. Saf. Sci.
    143, 105407 (2021). Article   Google Scholar   Lu, L. & Dai, F. Digitalization
    of traffic scenes in support of intelligent transportation applications. J. Comput.
    Civ. Eng. 37, 04023019 (2023). Article   Google Scholar   Chen, Y., Wang, W. &
    Chen, X. M. Bibliometric methods in traffic flow prediction based on artificial
    intelligence. Expert Syst. Appl. 228, 120421 (2023). Article   Google Scholar   Tu,
    B. et al. Research on intelligent calculation method of intelligent traffic flow
    index based on big data mining. Int. J. Intell. Syst. 37, 1186–1203 (2022). Article   Google
    Scholar   Leonelli, S. Data: From objects to assets. Nature. 574, 317–320 (2019).
    Article   ADS   CAS   PubMed   Google Scholar   Zhao, Y. et al. Machine learning
    based privacy-preserving fair data trading in big data market. Inf. Sci. 478,
    449–460 (2019). Article   Google Scholar   Pei, J. A survey on data pricing: From
    economics to data science. IEEE Trans. Knowl. Data Eng. 34, 4586–4608 (2022).
    Article   Google Scholar   Liang, J. & Yuan, C. Data price determinants based
    on a hedonic pricing model. Big Data Res. 25, 100249 (2021). Article   Google
    Scholar   Tian, L. et al. Optimal contract-based mechanisms for online data trading
    markets. IEEE Internet Things J. 6, 780–7810 (2019). Article   Google Scholar   Oh,
    H. et al. Competitive data trading model with privacy valuation for multiple stakeholders
    in IoT data markets. IEEE Internet Things J. 7, 3623–3639 (2020). Article   Google
    Scholar   Yu, H. & Zhang, M. Data pricing strategy based on data quality. Comput.
    Ind. Eng. 112, 1–10 (2017). Article   ADS   Google Scholar   Liao, J. & Li, R.
    Establishing a two-way transaction pricing model of “platform-individual” co-creation
    data property rights. J. Innov. Knowl. 8, 100427 (2023). Article   Google Scholar   Chellappa,
    R. K. & Mehra, A. Cost drivers of versioning: Pricing and product line strategies
    for information goods. Manag. Sci. 64, 2164–2180 (2017). Article   Google Scholar   Li,
    F. et al. Privacy computing: Concept, computing framework, and future development
    trends. Engineering. 5, 1179–1192 (2019). Article   Google Scholar   Wei, Q. et
    al. A survey of blockchain data management systems. ACM Trans. Embed. Comput.
    Syst. 21, 1–28 (2022). Article   Google Scholar   Podilchuk, C. I. & Delp, E.
    J. Digital watermarking: Algorithms and applications. IEEE Signal Process. Mag.
    18, 33–46 (2001). Article   ADS   Google Scholar   Kadian, P., Arora, S. M. &
    Arora, N. Robust digital watermarking techniques for copyright protection of digital
    data: A survey. Wirel. Pers. Commun. 118, 3225–3249 (2021). Article   Google Scholar   Luo,
    X. et al. On shapley value in data assemblage under independent utility. Proc.
    VLDB Endow. 15, 2761–2773 (2022). Article   Google Scholar   Wang, Y., Zhao, Z.
    & Baležentis, T. Benefit distribution in shared private charging pile projects
    based on modified Shapley value. Energy. 263, 125720 (2023). Article   Google
    Scholar   Yang, S. et al. Operation optimization and income distribution model
    of park integrated energy system with power-to-gas technology and energy storage.
    J. Clean. Prod. 247, 119090 (2020). Article   Google Scholar   Zheng, X. X. et
    al. Coordinating a closed-loop supply chain with fairness concerns through variable-weighted
    Shapley values. Transp. Res. E. 126, 227–253 (2019). Article   Google Scholar   Download
    references Acknowledgements This paper was supported by the National Natural Science
    Foundation of China (Grant No. 71761025), \"Double first-class initiative\" key
    scientific research projects in Gansu Province (Grant No. GSSYLXM-04), Lanzhou
    Jiaotong University and Tianjin University Joint Innovation Fund Project of China
    (Grant No. 2019058). Author information Authors and Affiliations School of Traffic
    and Transportation, Lanzhou Jiaotong University, Lanzhou, 730070, China Shiwei
    Li, Lei Chu, Jisen Wang & Yuzhao Zhang Key Laboratory of Railway Industry on Plateau
    Railway Transportation Intelligent Management and Control, Lanzhou, 730070, China
    Shiwei Li & Yuzhao Zhang Contributions S.L.: Conceptualization (lead); Methodology
    (lead); Writing—original draft (lead); Writing—review and editing (equal). L.C.:
    Conceptualization (equal); Methodology (equal); Writing—original draft (supporting);
    Writing—review and editing (equal). J.W.: Conceptualization (supporting); Writing—review
    and editing (supporting). Y.Z.: Methodology (supporting); Writing—review and editing
    (equal) Corresponding author Correspondence to Shiwei Li. Ethics declarations
    Competing interests The authors declare no competing interests. Additional information
    Publisher''s note Springer Nature remains neutral with regard to jurisdictional
    claims in published maps and institutional affiliations. Rights and permissions
    Open Access This article is licensed under a Creative Commons Attribution 4.0
    International License, which permits use, sharing, adaptation, distribution and
    reproduction in any medium or format, as long as you give appropriate credit to
    the original author(s) and the source, provide a link to the Creative Commons
    licence, and indicate if changes were made. The images or other third party material
    in this article are included in the article''s Creative Commons licence, unless
    indicated otherwise in a credit line to the material. If material is not included
    in the article''s Creative Commons licence and your intended use is not permitted
    by statutory regulation or exceeds the permitted use, you will need to obtain
    permission directly from the copyright holder. To view a copy of this licence,
    visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions About
    this article Cite this article Li, S., Chu, L., Wang, J. et al. A road data assets
    revenue allocation model based on a modified Shapley value approach considering
    contribution evaluation. Sci Rep 14, 5179 (2024). https://doi.org/10.1038/s41598-024-55819-7
    Download citation Received 28 November 2023 Accepted 28 February 2024 Published
    02 March 2024 DOI https://doi.org/10.1038/s41598-024-55819-7 Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Road data asset Two-layer revenue allocation Index evaluation Modified
    Shapley value Rough set Subjects Computational science Scientific data Comments
    By submitting a comment you agree to abide by our Terms and Community Guidelines.
    If you find something abusive or that does not comply with our terms or guidelines
    please flag it as inappropriate. Download PDF Sections Figures References Abstract
    Introduction Literature review Model building Case study Conclusions Data availability
    References Acknowledgements Author information Ethics declarations Additional
    information Rights and permissions About this article Comments Advertisement Scientific
    Reports (Sci Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press
    releases Press office Contact us Discover content Journals A-Z Articles by subject
    Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open
    access Author & Researcher services Reprints & permissions Research data Language
    editing Scientific editing Nature Masterclasses Research Solutions Libraries &
    institutions Librarian service & tools Librarian portal Open research Recommend
    to library Advertising & partnerships Advertising Partnerships & Services Media
    kits Branded content Professional development Nature Careers Nature Conferences
    Regional websites Nature Africa Nature China Nature India Nature Italy Nature
    Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy
    choices/Manage cookies Legal notice Accessibility statement Terms & Conditions
    Your US state privacy rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: A road data assets revenue allocation model based on a modified Shapley value
    approach considering contribution evaluation
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Kassahun E.A.
  - Gebreyesus S.H.
  - Tesfamariam K.
  - Endris B.S.
  - Roro M.A.
  - Getnet Y.
  - Hassen H.Y.
  - Brusselaers N.
  - Coenen S.
  citation_count: '0'
  description: 'Preterm birth is one of the most common obstetric complications in
    low- and middle-income countries, where access to advanced diagnostic tests and
    imaging is limited. Therefore, we developed and validated a simplified risk prediction
    tool to predict preterm birth based on easily applicable and routinely collected
    characteristics of pregnant women in the primary care setting. We used a logistic
    regression model to develop a model based on the data collected from 481 pregnant
    women. Model accuracy was evaluated through discrimination (measured by the area
    under the Receiver Operating Characteristic curve; AUC) and calibration (via calibration
    graphs and the Hosmer–Lemeshow goodness of fit test). Internal validation was
    performed using a bootstrapping technique. A simplified risk score was developed,
    and the cut-off point was determined using the “Youden index” to classify pregnant
    women into high or low risk for preterm birth. The incidence of preterm birth
    was 19.5% (95% CI:16.2, 23.3) of pregnancies. The final prediction model incorporated
    mid-upper arm circumference, gravidity, history of abortion, antenatal care, comorbidity,
    intimate partner violence, and anemia as predictors of preeclampsia. The AUC of
    the model was 0.687 (95% CI: 0.62, 0.75). The calibration plot demonstrated a
    good calibration with a p-value of 0.713 for the Hosmer–Lemeshow goodness of fit
    test. The model can identify pregnant women at high risk of preterm birth. It
    is applicable in daily clinical practice and could contribute to the improvement
    of the health of women and newborns in primary care settings with limited resources.
    Healthcare providers in rural areas could use this prediction model to improve
    clinical decision-making and reduce obstetrics complications.'
  doi: 10.1038/s41598-024-55627-z
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement View all journals
    Search Log in Explore content About the journal Publish with us Sign up for alerts
    RSS feed nature scientific reports articles article Article Open access Published:
    28 February 2024 Development and validation of a simplified risk prediction model
    for preterm birth: a prospective cohort study in rural Ethiopia Eskeziaw Abebe
    Kassahun, Seifu Hagos Gebreyesus, Kokeb Tesfamariam, Bilal Shikur Endris, Meselech
    Assegid Roro, Yalemwork Getnet, Hamid Yimam Hassen, Nele Brusselaers & Samuel
    Coenen  Scientific Reports  14, Article number: 4845 (2024) Cite this article
    632 Accesses 1 Altmetric Metrics Abstract Preterm birth is one of the most common
    obstetric complications in low- and middle-income countries, where access to advanced
    diagnostic tests and imaging is limited. Therefore, we developed and validated
    a simplified risk prediction tool to predict preterm birth based on easily applicable
    and routinely collected characteristics of pregnant women in the primary care
    setting. We used a logistic regression model to develop a model based on the data
    collected from 481 pregnant women. Model accuracy was evaluated through discrimination
    (measured by the area under the Receiver Operating Characteristic curve; AUC)
    and calibration (via calibration graphs and the Hosmer–Lemeshow goodness of fit
    test). Internal validation was performed using a bootstrapping technique. A simplified
    risk score was developed, and the cut-off point was determined using the “Youden
    index” to classify pregnant women into high or low risk for preterm birth. The
    incidence of preterm birth was 19.5% (95% CI:16.2, 23.3) of pregnancies. The final
    prediction model incorporated mid-upper arm circumference, gravidity, history
    of abortion, antenatal care, comorbidity, intimate partner violence, and anemia
    as predictors of preeclampsia. The AUC of the model was 0.687 (95% CI: 0.62, 0.75).
    The calibration plot demonstrated a good calibration with a p-value of 0.713 for
    the Hosmer–Lemeshow goodness of fit test. The model can identify pregnant women
    at high risk of preterm birth. It is applicable in daily clinical practice and
    could contribute to the improvement of the health of women and newborns in primary
    care settings with limited resources. Healthcare providers in rural areas could
    use this prediction model to improve clinical decision-making and reduce obstetrics
    complications. Similar content being viewed by others Clinical risk models for
    preterm birth less than 28 weeks and less than 32 weeks of gestation using a large
    retrospective cohort Article 10 June 2021 Modeling clinical and non-clinical determinants
    of intrapartum stillbirths in singletons in six public hospitals in the Greater
    Accra Region of Ghana: a case–control study Article Open access 18 January 2023
    Validation of a prognostic model for adverse perinatal health outcomes Article
    Open access 09 July 2020 Introduction Preterm birth, as defined by the World Health
    Organization (WHO), is birth before 37 completed weeks of gestation and poses
    a significant global health challenge1. A substantial proportion, approximately
    65–70% of preterm births, occur spontaneously2, and its associated deaths are
    more common in low and middle-income countries (LMICs) 3,4. An estimated 15 million
    infants are born prematurely each year, constituting a considerable public health
    concern5. In 2014, 14.8 million (10.6%) live births were preterm, with 80% of
    cases concentrated in Asia and Sub-Saharan Africa. The rate of preterm birth varies
    between countries, from 8.7% in Europe to 13.4% in North Africa. India, China,
    Nigeria, Bangladesh, and Indonesia account for 44.6% (6.6 million) of preterm
    births worldwide6. within Ethiopia alone, 320,000 neonates are born preterm annually,
    reflecting a prevalence rate of approximately 10.5% 7,8. The burden of preterm
    birth is a serious public health problem that contributes to significant neonatal
    morbidity and mortality9. Preterm birth is associated with short and long-term
    morbidities for survivors, incurring high costs for the healthcare system and
    psychological and financial consequences on the family10,11. Preterm birth is
    one of the most common obstetric complications12. In 2016, prematurity was the
    leading cause of neonatal death during the first weeks of life for children under
    five years13. Approximately 35% of neonatal deaths14 and 18% of all deaths of
    under-five children4 were attributable to preterm birth. In Ethiopia, preterm
    birth complications account for 10% of all deaths of children under five years7.
    Maternal age less than 20 years15, short stature (≤ 155 cm)16, smoking17, anemia,
    malaria infection18, intimate partner violence19, multiple pregnancies, pre-existing
    chronic conditions20, rural residence, short birth interval, history of abortion21,
    history of preterm birth8 and household food insecurity22 are the potential risk
    factors of preterm birth. Stress is also a widespread psychological health problem
    among pregnant women, and it contributes to preterm birth23,24. Despite preterm
    birth being a global public health priority, success in reducing adverse outcomes
    through evidence-based policies during antenatal care has been limited9,20. Early
    identification and quantification of individual women at risk of preterm birth
    could help to improve the quality of care during pregnancy, ensuring that all
    women have a positive pregnancy experience and outcomes5. Predicting preterm birth
    would allow earlier intervention to reduce infant morbidity and mortality, benefiting
    families, society, and healthcare. The potential preterm birth risk factors have
    been identified, but the predictive value of their combination remains unclear
    in rural settings. Prediction models are vital for healthcare providers to estimate
    probabilities of preterm birth and allow for timely intervention to reduce adverse
    outcomes25. Ultrasound examinations26,27 and biomarker tests28,29 have been utilized
    to predict preterm birth, but these methods are less practical in resource-limited
    settings due to a lack of specialized medical equipment and trained health care
    providers30,31. Furthermore, existing studies on the prediction of preterm birth
    often exhibit limitations, such as lack of inclusivity of important prognostic
    factors32,33,34, focused on hospitalized women 35,36,37,38 or exclusively on multiple
    pregnancies39,40 and reliance on high-level health care settings33,34. However,
    none of these studies were accurate enough to be applied in daily clinical practice
    in the primary care setting due to disparities in the potential predictors, domains,
    and levels of healthcare settings. We, therefore, set out to develop a simple
    prediction tool to identify pregnant women at higher risk of preterm birth in
    early pregnancy in resource-limited primary care settings. The current study aimed
    to develop and validate a simplified risk prediction model for preterm birth and
    evaluate the added value of maternal stress in predicting preterm birth using
    the routine characteristics of pregnant women in the rural area of Ethiopia. Methods
    Study setting and design We used data from the Butajira Nutrition, Mental Health,
    and Pregnancy (BUNMAP) project among pregnant women and their newborns in Southern
    Ethiopia. The BUNMAP project was established in 2016 under the Butajira Health
    and Demographic Surveillance Site (BHDSS), consisting of nine rural and one urban
    dweller association. The BHDSS is one of the oldest Demographic and Health Survey
    sites in the Southern Ethiopia region41. The BUNMAP project was an open, prospective
    cohort of pregnant women and their offspring (up to 59 months) collected between
    2017 and 2019. We developed a prediction model for preterm birth using the baseline
    characteristics of pregnant women and birth history. The theoretical design was
    the incidence of preterm birth as a function of multiple predictors of ambulatory
    pregnant women. Health extension workers in the BHDSS identified pregnant women
    aged 15 to 49 through house-to-house surveillance. Study population At baseline,
    all pregnant women aged 15–49 years with gestational age between 8 and 24 completed
    weeks, living in the Butajira HDSS, and planning to deliver in the study area
    were included in the study. Pregnant women who were severely malnourished or had
    a mid-upper arm circumference (MUAC) of less than 17.5 cm and severely anemic
    women with hemoglobin (Hgb) levels less than seven g/dL were excluded from the
    cohort. Our study included pregnant women enrolled in the cohort who met the inclusion
    criteria and had a well-determined gestational age at birth (Fig. 1). Figure 1
    Flow chart of the study design for preterm births in the Butajira Nutrition, Mental
    Health, and Pregnancy cohort. Full size image Predictors and outcome assessment
    After enrolment, all women were requested to travel to the nearest health facility
    for a comprehensive baseline assessment. An experienced sonographer used transabdominal,
    portable diagnostic imaging, and full-color flow mapping ultrasound to confirm
    gestational age at baseline. The primary outcome, preterm birth, was defined as
    birth before 37 completed weeks of gestation. The study questionnaire was adopted
    and developed from the validated tools and Ethiopian demography and health survey
    to collect data on the following predictors: Intimate partner violence (IPV),
    maternal stress, maternal age, marital status, educational status, pregnancy type,
    substance use, Mid-upper arm circumference (MUAC), Comorbidity, history of previous
    and current Antenatal care (ANC) visits, history of abortion, history of preterm
    birth, gravidity, contraceptive use, deworming, and comorbidity. Then, potential
    predictors were categorized by considering the clinical relevance thresholds for
    adverse birth outcomes. Mid-upper arm circumference was measured using a standard
    MUAC tape, and less than 23 cm was considered maternal malnutrition42. Intimate
    partner violence was assessed using the Hurt, Insult, Threaten, and Scream (HITS)
    screening tool. This four-item questionnaire asks respondents how frequently their
    partner physically hurt, insulted, threatened harm, and screamed at them during
    pregnancy using a five-point Likert scale ranging from \"never\" (coded as 1)
    to \"frequently\" (coded as 5). The total score ranges from 4 to a maximum of
    20. Positive or exposure to IPV was considered if the HITS score was more significant
    than ten43. Maternal stress was assessed based on the ten-item classic perceived
    stress scale (PSS) assessment instrument for one month before data collection
    day using a five-point Likert scale ranging from \"never\" (coded as 0) to \"frequently\"
    (coded as 4). The individual score on the PSS can range from 0 to a maximum of
    40. Exposure to low, moderate, or high maternal stress was considered if the PSS
    score was 0–13, 14–26, or 27–40, respectively44. Maternal anemia was assessed
    by measuring Hgb concentration in red blood cells by taking a finger-prick blood
    sample using a Hemo-Cue (Hb-201) instrument. Pregnant women with Hgb concentration < 11
    g/dl were considered anemic45. Maternal comorbidity was considered when one or
    more of the following medical conditions exist: cardiac disease, diabetes, thyroid
    disease, chronic hypertension, HIV infection, malaria, typhoid, or renal disease
    in the baseline assessment46. Substance use was examined in pregnant women who
    consumed local alcohol or beer or chewed khat at least once a week during pregnancy.
    Statistical analysis The data were collected using the Open Data Kit (ODK) platforms
    and were exported to the R statistical programming software version 4.2.047. The
    baseline characteristics of the women were summarised in a table with frequencies
    and proportions. The distribution was assessed using histograms for maternal age,
    gestational age, and MUAC at baseline. The median and interquartile range (IQR)
    for the pregnant women’s age, gestational age, and MUAC were presented. The mean
    and standard deviation (SD) were also used to present the baseline Hgb level of
    women. We performed Little’s missing completely at random (MCAR) test and checked
    the pattern of missing values. The p-value (< 0.001) indicated that the missing
    was not MCAR. However, the test result is insufficient to indicate whether the
    missing is not missing at random (NMAR) or missing at random (MAR)48. We then
    performed multiple imputations by chained equation using the “mice” package with
    ten imputations and 20 iterations49. Sensitivity analyses were performed to determine
    whether the MAR assumption was valid. The MAR assumption was valid, and the complete
    case and imputed data analysis results were comparable (Supplementary Table S1).
    Model development Individual predictors that significantly contribute to the risk
    of preterm birth were examined using univariable logistic regression analysis.
    The Likelihood Ratio Test (LRT) was used to determine the p-value for each model.
    Variables with a p-value for the LRT less than 0.25 were considered eligible to
    be included in the multivariable logistic regression analysis. The backward stepwise
    elimination technique with a p-value ≤ 0.15 for the LRT was fitted to build a
    final multivariable logistic regression model. The predictive accuracy of the
    final model was checked using discrimination (AUC) and calibration (calibration
    graphs and Hosmer–Lemeshow goodness of fit test) parameters. The Hosmer–Lemeshow
    goodness of fit test with a p-value greater than 0.05 indicates good calibration,
    which means that the probability of preterm birth estimated by the model is similar
    to the observed probability. An AUC of 0.5 indicates no discrimination ability,
    while an AUC of 1 indicates perfect discrimination. Moreover, the added value
    of maternal stress in predicting preterm birth was assessed using logistic regression
    analysis. The model’s performance, including maternal stress, was evaluated using
    discrimination and calibration. The AUC of this model was compared with the reduced
    model to evaluate the improvement in prediction performance. Internal validation
    The model was internally validated using a bootstrap technique50 to estimate the
    degree of over-optimism of the final model when applied to a similar population.
    Internal validation was performed on the regression coefficient with a 95% confidence
    interval (CI) and the AUC of the model using 2,000 random bootstrap samples. The
    AUC difference between the bootstrap and the original full sample measured the
    optimism of the predictive model. Simplified risk score development Based on the
    final model''s regression coefficients hierarchy, a simplified risk score was
    computed to provide an easily applicable prediction model. Each regression coefficient
    of the predictors in the final model was divided by the smallest regression coefficient,
    and the result was rounded to the nearest integers. The risk score performance
    was assessed and compared with the original regression coefficient model using
    the AUC. The simplified risk score is also arbitrarily classified based on the
    size of each interval and its potential public health relevance. The TRIPOD (transparent
    reporting of a multivariable prediction model for individual prognosis or diagnosis)
    checklist51 was used to guide the development and validation of the prediction
    model and reporting (Supplementary Table S2). Ethics declarations The study has
    been reviewed, and ethical clearance was obtained from the Institutional Review
    Boards of Addis Ababa University, College of Health Sciences (code: 099/17/SPH).
    Written informed consent and parental assent were obtained from study participants.
    This manuscript was approved by the Faculty of Medicine and Health Sciences, University
    of Antwerp, Belgium, and all methods were carried out in accordance with relevant
    guidelines and regulations. Result Characteristics of pregnant women Of the 618
    pregnant women, 137 were excluded from the analysis due to absence or inappropriate
    determination of gestational age at birth. At enrolment, the median age was 26
    with an IQR of 6 years, and 38 (8.1%) were younger than 20 years (Table 1). All
    women (100%) were married, and 205 (42.7%) did not receive formal education. Table
    1 Baseline sociodemographic, obstetric, and clinical characteristics of study
    participants (n = 481). Full size table Prediction model Preterm birth occurred
    in 94 (19.5%) of the 481 women who gave birth in the BHDSS. The multivariable
    logistic regression analysis included variables with a p-value of less than 0.25
    for the LRT in the univariable logistic regression analysis, such as MUAC, gravidity,
    history of abortion, anemia, comorbidity, IPV, and history of ANC for the current
    pregnancy. The final prediction model combined seven predictors, including MUAC,
    gravidity, history of abortion, comorbidity, IPV, anemia, and history of ANC for
    the current pregnancy (Table 2). Table 2 Association between predictors and preterm
    birth in South Ethiopia (n = 481). Full size table Model performance and validation
    The AUC of the final model was 0.687 (95%CI: 0.620, 0.753). The calibration test
    of the model had a p-value of 0.7134, indicating good agreement between the predicted
    and observed probability of preterm birth (Fig. 2). Figure 2 Model performance
    of the prediction model for preterm birth. (A) The area under the receiver operating
    characteristics curve to evaluate the discrimination. The AUC suggests that the
    model has a 68.7% (95%CI:62.0%,75.3%) chance to correctly distinguish a high risk
    for preterm birth from normal pregnancy based on the characteristics of pregnant
    women in resource limited setting. (B) Calibration plot to evaluate the calibration
    of the prediction model. The visual calibration plot between the observed and
    prediction risk in different percentiles of the predicted values. The p-value
    of the calibration plot is 0.713. Full size image The bootstrapping technique
    showed that we expect low optimism when applied to newly pregnant women in a similar
    population. The adjusted AUC of the model was 0.689 (95% CI: 0.622, 0.755) with
    an AUC overoptimism coefficient of 0.002 (p-value = 0.796) (Fig. 3). Figure 3
    Area under the receiver operating characteristic curve of an internal validation
    of the prediction model. Full size image Clinical importance of maternal stress
    In the univariable and multivariable analysis, the maternal stress had a p-value
    for the LRT of 0.031 and 0.008, respectively. The calibration plot indicates that
    the model, including maternal stress, had good calibration (p-value = 0.825) (Fig.
    4). The AUC increased from 0.687 (95%CI: 0.620, 0.753) to 0.693 (95% CI: 0.620–0.766)
    with a p-value of 0.784. Therefore, the addition of maternal stress to the prediction
    model has not made a significant difference in the discrimination performance
    of the model. Figure 4 Model performance after adding maternal stress in the prediction
    model. (A) Area under the receiver operating characteristic curve of the final
    model with and without maternal stress. (B) Calibration plot to evaluate the calibration
    of the addition of maternal stress to the final model. The added value maternal
    stress was assessed through a calibration plot, revealing a good calibration (a
    p-value of 0.825). Full size image Simplified risk score per individual A MUAC
    less than 23 cm had the smallest regression coefficient and was weighted as 1.
    The total risk score ranged from 0 to 13. The simplified risk score had an AUC
    of 0.678 (95% CI: 0.612–0.743) and a p-value of 0.08 compared to the original
    regression coefficient model. Total risk score formula = (1*MUAC < 23 cm + 1*primigravida + 2*being
    anemic + 2*positive IPV + 3*had a history of abortion + 2* had no ANC history
    for the current pregnancy + 2*presence of comorbidity) (Table 3). Table 3 The
    simplified risk score and rounded weight to predict preterm birth. Full size table
    Table 4 indicates that the risk of preterm birth increased as the simplified risk
    score increased. The proportion of preterm births was 244 (13.9%) with a simplified
    risk score of ≤ 1 and 18 (50%) with a simplified risk score of ≥ 5. Table 4 Risk
    of preterm birth per individual risk score to predict high risk for preterm birth.
    Full size table Table 5 presents results from the likelihood ratio of each risk
    score interval. The likelihood ratio increased as the simplified risk score for
    the prediction model increased. The simplified risk score between 5 and 13 increased
    the probability of having a preterm birth in pregnant women by 3.8 times compared
    to those who did not have a preterm birth. Table 5 Interval likelihood ratio of
    risk score intervals for the prediction model. Full size table Based on Youden’s
    index52, the optimal cut-off point was a probability of ≥ 0.221 with a sensitivity
    of 41% (95% CI: 30, 53), a specificity of 82% (95%CI: 77, 86), and a positive
    predictive value (PPV) of 36% (95%CI: 26, 48), and a negative predictive value
    (NPV) of 84% (95%CI: 80, 88). The positive likelihood ratio (LR+) was 2.24 (1.56,
    3.21), and the negative likelihood ratio (LR−) was 0.72 (95% CI: 0.60, 0.88).
    Similarly, the optimal cut-off for the simplified risk score was ≥ 2 using Youden''s
    index. One hundred twenty-seven (34.2%) pregnant women were classified as high
    risk for preterm birth, while 244 (65.8%) were classified as low risk, with a
    sensitivity of 55% and specificity of 71% (Table 6). Table 6 Performance of simplified
    risk sore at different cut-off points to predict preterm birth. Full size table
    Discussion In the present study, one-fifth of the babies were born prematurely.
    We developed a prediction model for preterm birth based on the baseline characteristics
    of pregnant women in rural settings. The model combined MUAC, gravidity, comorbidity,
    ANC follow-up, history of abortion, IPV, and anemia to predict preterm birth.
    We developed a simplified risk score to improve the practical applicability of
    the prediction model in clinical practice in primary healthcare settings without
    requiring advanced diagnostic tests and imaging. Previous studies developed a
    model for predicting preterm birth during pregnancy using ultrasound examination26,27
    and biomarker tests28,29. However, these procedures are limited by their expense
    and complexity, requiring trained healthcare professionals and specialized equipment.
    In LMICs, such as Ethiopia, most primary care settings lack access to these advanced
    laboratory and imaging procedures. Consequently, estimating the risk of preterm
    birth in resource-limited settings using these methods is challenging. Hence,
    we developed a model using easily obtained and routinely collected maternal characteristics
    applicable in primary care settings in Ethiopia. Prediction models were developed
    for women with signs of preterm birth in tertiary care centers35,36,38. The potential
    predictors of preterm birth in hospitalized women with preterm birth symptoms
    may differ from those in non-hospitalized women. The healthcare system in tertiary
    healthcare is different, and the diagnostic procedures are more advanced and expensive.
    However, the current study estimated the risk of preterm birth using available
    characteristics of women in the primary care setting. Prediction models for preterm
    birth were initially developed in comprehensive specialized hospitals in northern
    Ethiopia, utilizing a retrospective study design33,34. These studies, however,
    overlooked fundamental predictors recommended by WHO, such as IPV, Substance use,
    MUAC, history of contraceptive use, deworming, and maternal stress. The exclusive
    use of a specialized hospital may have limited the generalizability of findings,
    potentially reflecting specific patient characteristics or a more selective group
    seeking specialized care. In contrast, our prediction model was deployed in southern
    Ethiopia using a prospective study design considering the basic WHO recommendations
    for potential preterm birth predictors. This was crucial in minimizing recall
    bias and strengthening model accuracy. Additionally, our research was conducted
    in a rural community setting, where diagnostic methods and healthcare professionals
    are more limited compared to specialized hospitals. Consequently, the previously
    developed model may not be applicable for predicting preterm birth in primary
    care settings. Schaaf et al.53 predicted the risk of preterm birth with poor calibration
    and an AUC of 0.63 using a combined 13 potential predictors of preterm birth,
    including fetal sex and vaginal bleeding before 20 weeks of gestation. However,
    confirmation of fetal sex requires ultrasound examination, and vaginal bleeding
    before 20 weeks of pregnancy necessitates advanced diagnostic procedures. Therefore,
    the study is less applicable in clinical settings with scarce resources. Huang
    et al.32 also developed a prediction model for preterm birth by combining stress
    and metabolic predictors. Maternal stress biomarkers (cortisol) and metabolites
    were measured in the serum samples to predict preterm birth. The prediction model
    yielded an optimum AUC value of 0.895. The current study assessed maternal stress
    using a perceived stress scale. The addition of maternal stress in the prediction
    model increases the AUC from 0.687 to 0.693, but the difference was insignificant
    compared to the original model. Schaaf et al.53 calculated the predictive model
    at two arbitrary cut-off points of the predictive probability. The incidence of
    preterm birth has occurred in 3.8% of pregnancies. At a predictive probability
    of 0.1, the sensitivity was 4.2%, and the specificity was 99.3%. At this cut-off
    point, the PPV was 19.4%, and the NPV was 96.3%. While in our study, we developed
    different cut-off points using the Youden index. The optimal predicted probability
    cut-off point was 0.221, with a sensitivity of 41%, a specificity of 82%, a PPV
    of 36%, and an NPV of 84%. The LR + value was 2.24, and the LR− value was 0.72.
    The simplified risk score is highly applicable and easier to use in daily clinical
    practice. Our study suggests that a prediction model using the characteristics
    of pregnant women might be useful in identifying pregnant women at high risk of
    preterm birth in resource-limited settings. Women categorized as high-risk could
    be referred for further assessment and therapeutic intervention to prevent preterm
    birth. A simplified risk score would help produce relevant information for the
    community, policymakers, and clinical interventions to reduce neonatal morbidity
    and mortality rates. The current study included highly applicable and routinely
    collected characteristics of pregnant women in LMICs. This can be used in daily
    clinical practice in primary care settings to identify high-risk pregnant women.
    We further used an internal validation based on a bootstrapping technique to provide
    unbiased estimates of the high risk for preterm birth. We developed different
    optimal cut-off points based on the coefficient and risk score, which helps to
    choose various cut-off points depending on the program goal and the availability
    of resources. Our study is subject to limitations. The missingness data, mainly
    the history of abortion, preterm birth, and ANC for the previous pregnancy, had
    high missing values. However, we tried to minimize the risk of bias by using multiple
    imputations to develop the estimated regression coefficient of the prediction
    model. When pregnant women experience risk or complication during their pregnancy,
    healthcare providers may intervene to induce preterm birth, affecting the prediction
    model and implying a computing risk. Future researchers should consider such issues
    using advanced analysis techniques. Potential predictors of preterm birth and
    complications might be different in the first and second trimesters. Developing
    a prediction model in each trimester thus improves the model''s practical applicability
    and predictive capacity. Future research should consider conducting an extensive
    and multicentre study to improve the prediction model''s generalization and external
    validation. Conclusions In this study, we developed a prediction model and a score
    for preterm birth risk stratification in rural Ethiopia. The model can identify
    pregnant women at high risk of preterm birth. Prediction of high-risk women based
    on individual characteristics could help to strengthen the clinical decision-making
    of the primary health care providers to reduce obstetric complications. It would
    also help produce relevant information for the community, policymakers, and clinical
    interventions on preventing and treating preterm birth and reducing neonatal morbidity
    and mortality rates. Data availability Data is available from the corresponding
    author at a reasonable request. References World Health Organization (WHO). Recommended
    definitions, terminology, and format for statistical tables related to the perinatal
    period and use of a new certificate for the cause of perinatal deaths. Modifications
    recommended by FIGO as amended October 14, 1976. Acta Obstetr. Gynecol. Scand.
    56(3), 247–253 (1977). Google Scholar   Howson, C., Kinney, M. & Lawn. J. March
    of dimes, PMNCH, save the children, WHO. Born Too Soon: The Global Action Report
    on Preterm Birth (World Health Organization, 2012). World Health Organization(WHO).
    Born Too Soon: The Global Action Report on Preterm Birth (Springer, 2012). Walani,
    S. R. Global burden of preterm birth. Int. J. Gynaecol. Obstetr. 150(1), 31–33
    (2020). Article   Google Scholar   World Health Organization (WHO). Preterm birth
    (2018, accessed 19 Feb 2018). https://www.who.int/news-room/fact-sheets/detail/preterm-birth.
    Chawanpaiboon, S. et al. Global, regional, and national estimates of levels of
    preterm birth in 2014: a systematic review and modelling analysis. Lancet Glob.
    Health 7(1), e37–e46 (2019). Article   PubMed   Google Scholar   EVERY PREEMIE
    SCALE (USAID and PCI). Profile of preterm and low birth weight prevention and
    care. https://reliefweb.int/report/ethiopia/ethiopia-profile-preterm-and-low-birth-weight-prevention-and-care
    (2015). Muchie, K. F. et al. Epidemiology of preterm birth in Ethiopia: systematic
    review and meta-analysis. BMC Pregn. Childbirth 20(1), 574 (2020). Article   Google
    Scholar   Been, J. V. & Millett, C. Reducing the global burden of preterm births.
    Lancet Glob. Health 7(4), e414 (2019). Article   PubMed   Google Scholar   World
    Health Organization (WHO). Preterm birth. https://www.who.int/news-room/fact-sheets/detail/preterm-birth
    (2018). Howe, T.-H., Sheu, C.-F., Wang, T.-N. & Hsu, Y.-W. Parenting stress in
    families with very low birth weight preterm infants in early infancy. Res. Dev.
    Disabil. 35(7), 1748–1756 (2014). Article   PubMed   Google Scholar   Vogel, J.
    P. et al. The global epidemiology of preterm birth. Best Pract. Res. Clin. Obstet.
    Gynaecol. 52, 3–12 (2018). Article   PubMed   Google Scholar   Liu, L. et al.
    Global, regional, and national causes of under-5 mortality in 2000–15: An updated
    systematic analysis with implications for the Sustainable Development Goals. The
    Lancet. 388(10063), 3027–3035 (2016). Article   Google Scholar   UN Inter-Agency
    Group for Child Mortality Estimation. Levels and trends in child mortality: Available
    on https://www.un.org/en/development/desa/population/publications/mortality/child-mortality-report-2017.asp.
    New York: United Nations Children’s Fund (2017). Wagura, P., Wasunna, A., Laving,
    A., Wamalwa, D. & Nganga, P. Prevalence and factors associated with preterm birth
    at kenyatta national hospital. BMC Pregn. Childbirth 18(1), 107 (2018). Article   Google
    Scholar   Derraik, J. G. B., Lundgren, M., Cutfield, W. S. & Ahlsson, F. Maternal
    height and preterm birth: A study on 192,432 Swedish women. PLOS ONE 11(4), e0154304
    (2016). Article   PubMed   PubMed Central   Google Scholar   Delnord, M., Blondel,
    B. & Zeitlin, J. What contributes to disparities in the preterm birth rate in
    European countries?. Curr. Opin. Obstetr. Gynecol. 27(2), 133–142 (2015). Article   Google
    Scholar   van den Broek, N. R., Jean-Baptiste, R. & Neilson, J. P. Factors associated
    with preterm, early preterm and late preterm birth in Malawi. PLOS ONE. 9(3),
    e90128 (2014). Article   PubMed   PubMed Central   Google Scholar   Sigalla, G.
    N. et al. Intimate partner violence during pregnancy and its association with
    preterm birth and low birth weight in Tanzania: A prospective cohort study. PLOS
    ONE. 12(2), e0172540 (2017). Article   PubMed   PubMed Central   Google Scholar   Requejo,
    J. et al. Born Too Soon: Care during pregnancy and childbirth to reduce preterm
    deliveries and improve health outcomes of the preterm baby. Reprod. Health 10(1),
    S4 (2013). Article   PubMed   PubMed Central   Google Scholar   Muhumed, I. I.,
    Kebira, J. Y. & Mabalhin, M. O. Preterm birth and associated factors among mothers
    who gave birth in Fafen Zone Public Hospitals, Somali Regional State, Eastern
    Ethiopia. Res. Rep. Neonatol. 11, 23–33 (2021). Google Scholar   Richterman, A.
    et al. Food insecurity as a risk factor for preterm birth: A prospective facility-based
    cohort study in rural Haiti. BMJ Glob. Health 5(7), e002341 (2020). Article   PubMed   PubMed
    Central   Google Scholar   Eick, S. M. et al. Relationships between psychosocial
    factors during pregnancy and preterm birth in Puerto Rico. PLOS ONE 15(1), e0227976
    (2020). Article   CAS   PubMed   PubMed Central   Google Scholar   Staneva, A.,
    Bogossian, F., Pritchard, M. & Wittkowski, A. The effects of maternal depression,
    anxiety, and perceived stress during pregnancy on preterm birth: A systematic
    review. Women Birth 28(3), 179–193 (2015). Article   PubMed   Google Scholar   Moons,
    K. G., Altman, D. G., Vergouwe, Y. & Royston, P. Prognosis and prognostic research:
    application and impact of prognostic models in clinical practice. BMJ 2009, 338
    (2009). Google Scholar   Włodarczyk, T. et al. Machine learning methods for preterm
    birth prediction: A review. Electronics 10(5), 586 (2021). Article   Google Scholar   Gomez,
    R. et al. Cervicovaginal fibronectin improves the prediction of preterm delivery
    based on sonographic cervical length in patients with preterm uterine contractions
    and intact membranes. Am. J. Obstetr. Gynecol. 192(2), 350–359 (2005). Article   CAS   Google
    Scholar   Saade, G. R. et al. Development and validation of a spontaneous preterm
    delivery predictor in asymptomatic women. Am. J. Obstetr. Gynecol. 214(5), 633
    (2016). Article   Google Scholar   Ngo, T. T. et al. Noninvasive blood tests for
    fetal development predict gestational age and preterm delivery. Science 360(6393),
    1133–1136 (2018). Article   ADS   CAS   PubMed   PubMed Central   Google Scholar   Katz,
    J. et al. Mortality risk in preterm and small-for-gestational-age infants in low-income
    and middle-income countries: A pooled country analysis. The Lancet 382(9890),
    417–425 (2013). Article   Google Scholar   Hosny, A. & Aerts, H. Artificial intelligence
    for global health. Science 366(6468), 955–956 (2019). Article   ADS   CAS   PubMed   PubMed
    Central   Google Scholar   Huang, D. et al. Stress and metabolomics for prediction
    of spontaneous preterm birth: A prospective nested case-control study in a tertiary
    Hospital. Front. Pediatr. 2021, 949 (2021). Google Scholar   Feleke, S. F., Anteneh,
    Z. A., Wassie, G. T., Yalew, A. K. & Dessie, A. M. Developing and validating a
    risk prediction model for preterm birth at Felege Hiwot Comprehensive Specialized
    Hospital, North-West Ethiopia: A retrospective follow-up study. BMJ Open 12(9),
    e061061 (2022). Article   PubMed   PubMed Central   Google Scholar   Fente, B.
    M., Asaye, M. M., Tesema, G. A. & Gudayu, T. W. Development and validation of
    a prognosis risk score model for preterm birth among pregnant women who had antenatal
    care visit, Northwest, Ethiopia, retrospective follow-up study. BMC Pregn. Childbirth
    23(1), 732 (2023). Article   Google Scholar   Lee, K. J. et al. The clinical usefulness
    of predictive models for preterm birth with potential benefits: A KOrean Preterm
    collaboratE Network (KOPEN) registry-linked data-based cohort study. Int. J. Med.
    Sci. 17(1), 1 (2020). Article   PubMed   PubMed Central   Google Scholar   Mailath-Pokorny,
    M. et al. Individualized assessment of preterm birth risk using two modified prediction
    models. Eur. J. Obstetr. Gynecol. Reprod. Biol. 186, 42–48 (2015). Article   Google
    Scholar   Stock, S. J. et al. Development and validation of a risk prediction
    model of preterm birth for women with preterm labour symptoms (the QUIDS study):
    A prospective cohort study and individual participant data meta-analysis. PLoS
    Med. 18(7), e1003686 (2021). Article   PubMed   PubMed Central   Google Scholar   Allouche,
    M., Huissoud, C., Guyard-Boileau, B., Rouzier, R. & Parant, O. Development and
    validation of nomograms for predicting preterm delivery. Am. J. Obstetr. Gynecol.
    204(3), 242 (2011). Article   Google Scholar   van de Mheen, L. et al. Prediction
    of preterm birth in multiple pregnancies: development of a multivariable model
    including cervical length measurement at 16 to 21 weeks’ gestation. J. Obstetr.
    Gynaecol. Can. 36(4), 309–319 (2014). Article   Google Scholar   Zhang, J. et
    al. Development and external validation of a nomogram for predicting preterm birth
    at< 32 weeks in twin pregnancy. Sci. Rep. 11(1), 1–13 (2021). Google Scholar   Molla
    M. Butajira Butajira Rural Health Program (HDSS), Ethiopia. http://www.indepth-network.org/Profiles/butajira_hdss_2013.pdf
    (2013). Ververs, M.-T., Antierens, A., Sackl, A., Staderini, N. & Captier, V.
    Which anthropometric indicators identify a pregnant woman as acutely malnourished
    and predict adverse birth outcomes in the humanitarian context?. PLoS Curr. 2013,
    5 (2013). Google Scholar   Sherin, K. M., Sinacore, J. M., Li, X. Q., Zitter,
    R. E. & Shakil, A. HITS: A short domestic violence screening tool for use in a
    family practice setting. Fam. Med. 30(7), 508–512 (1998). CAS   PubMed   Google
    Scholar   Roberti, J. W., Harrington, L. N. & Storch, E. A. Further psychometric
    support for the 10-item version of the perceived stress scale. J. Coll. Counsel.
    9(2), 135–147 (2006). Article   Google Scholar   World Health Organization. Haemoglobin
    concentrations for the diagnosis of anaemia and assessment of severity. World
    Health Organization (2011). Organization WH. WHO antenatal care randomized trial:
    manual for the implementation of the new model. In World Health Organization.
    Report No.: 9241546298 (2002). R Core Team. R: A language and environment for
    statistical computing. In R Foundation for Statistical Computing, Vienna, Austria
    https://www.R-project.org/ (2021). Little, R. J. A test of missing completely
    at random for multivariate data with missing values. J. Am. Stat. Assoc. 83(404),
    1198–1202 (1988). Article   MathSciNet   Google Scholar   White, I. R., Royston,
    P. & Wood, A. M. Multiple imputation using chained equations: issues and guidance
    for practice. Stat. Med. 30(4), 377–399 (2011). Article   MathSciNet   PubMed   Google
    Scholar   Tibshirani, R. J. & Efron, B. An introduction to the bootstrap. Monogr.
    Stat. Appl. Prob. 57, 1–436 (1993). MathSciNet   Google Scholar   Collins, G.
    S., Reitsma, J. B., Altman, D. G. & Moons, K. G. Transparent reporting of a multivariable
    prediction model for individual prognosis Or Diagnosis (TRIPOD): The TRIPOD Statement.
    Br. J. Surg. 102(3), 148–158 (2015). Article   CAS   PubMed   Google Scholar   Youden,
    W. J. Index for rating diagnostic tests. Cancer 3(1), 32–35 (1950). Article   CAS   PubMed   Google
    Scholar   Schaaf, J. M., Ravelli, A. C., Mol, B. W. J. & Abu-Hanna, A. Development
    of a prognostic model for predicting spontaneous singleton preterm birth. Eur.
    J. Obstetr. Gynecol. Reprod. Biol. 164(2), 150–155 (2012). Article   Google Scholar   Download
    references Acknowledgements We would like to thank the VLIR-OUS ICP program and
    the University of Antwerp for providing a scholarship for this master''s degree
    program and a travel grant to visit the study site in Ethiopia. Funding The BUNMAP
    project was partially funded by Addis Abeba University, Ethiopia. However, there
    was no funding available for the current research. Author information Authors
    and Affiliations Department of Family Medicine & Population Health, Faculty of
    Medicine and Health Sciences, University of Antwerp, Antwerp, Belgium Eskeziaw
    Abebe Kassahun & Hamid Yimam Hassen Departmentof of Nutrition and Dietetics, School
    of Public Health, Addis Ababa University, Addis Ababa, Ethiopia Seifu Hagos Gebreyesus,
    Bilal Shikur Endris & Yalemwork Getnet Department of Food Technology, Safety,
    and Health, Faculty of Bioscience Engineering, Ghent University, Ghent, Belgium
    Kokeb Tesfamariam Department of Reproductive Health and Health Service Management,
    School of Public Health, Addis Ababa University, Addis Ababa, Ethiopia Meselech
    Assegid Roro Global Health Institute, Department of Family Medicine & Population
    Health, Antwerp University, Antwerp, Belgium Nele Brusselaers Centre for Translational
    Microbiome Research, Department of Microbiology, Tumour and Cell Biology, Karolinska
    Institute, Stockholm, Sweden Nele Brusselaers Centre for General Practice, Department
    of Family Medicine & Population Health, Faculty of Medicine and Health Sciences,
    University of Antwerp, 2000, Antwerp, Belgium Samuel Coenen Contributions E.A.K
    comprehended the study. E.A.K, S.H.G, K.H, B.S.E, M.A.R, Y.G and H.Y.H involved
    in the study design, supervision of the data collection and data curation, E.A.K
    analyzed and drafted the manuscript under the supervision of S.C. E.A.K, S.H.G,
    K.H, B.S.E, M.A.R, Y.G, H.Y.H, N.B and S.C interpreted the data and involved in
    the final version of the manuscript. All authors critically reviewed and approved
    the final manuscript. Corresponding author Correspondence to Eskeziaw Abebe Kassahun.
    Ethics declarations Competing interests The authors declare no competing interests.
    Additional information Publisher''s note Springer Nature remains neutral with
    regard to jurisdictional claims in published maps and institutional affiliations.
    Supplementary Information Supplementary Tables. Rights and permissions Open Access
    This article is licensed under a Creative Commons Attribution 4.0 International
    License, which permits use, sharing, adaptation, distribution and reproduction
    in any medium or format, as long as you give appropriate credit to the original
    author(s) and the source, provide a link to the Creative Commons licence, and
    indicate if changes were made. The images or other third party material in this
    article are included in the article''s Creative Commons licence, unless indicated
    otherwise in a credit line to the material. If material is not included in the
    article''s Creative Commons licence and your intended use is not permitted by
    statutory regulation or exceeds the permitted use, you will need to obtain permission
    directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Kassahun, E.A.,
    Gebreyesus, S.H., Tesfamariam, K. et al. Development and validation of a simplified
    risk prediction model for preterm birth: a prospective cohort study in rural Ethiopia.
    Sci Rep 14, 4845 (2024). https://doi.org/10.1038/s41598-024-55627-z Download citation
    Received 21 February 2023 Accepted 26 February 2024 Published 28 February 2024
    DOI https://doi.org/10.1038/s41598-024-55627-z Share this article Anyone you share
    the following link with will be able to read this content: Get shareable link
    Provided by the Springer Nature SharedIt content-sharing initiative Keywords Prediction
    model Preterm birth Risk score Pregnant women Ethiopia Subjects Epidemiology Health
    care Medical research Preclinical research Pregnancy outcome Comments By submitting
    a comment you agree to abide by our Terms and Community Guidelines. If you find
    something abusive or that does not comply with our terms or guidelines please
    flag it as inappropriate. Download PDF Sections Figures References Abstract Introduction
    Methods Result Discussion Conclusions Data availability References Acknowledgements
    Funding Author information Ethics declarations Additional information Supplementary
    Information Rights and permissions About this article Comments Advertisement Scientific
    Reports (Sci Rep) ISSN 2045-2322 (online) About Nature Portfolio About us Press
    releases Press office Contact us Discover content Journals A-Z Articles by subject
    Protocol Exchange Nature Index Publishing policies Nature portfolio policies Open
    access Author & Researcher services Reprints & permissions Research data Language
    editing Scientific editing Nature Masterclasses Research Solutions Libraries &
    institutions Librarian service & tools Librarian portal Open research Recommend
    to library Advertising & partnerships Advertising Partnerships & Services Media
    kits Branded content Professional development Nature Careers Nature Conferences
    Regional websites Nature Africa Nature China Nature India Nature Italy Nature
    Japan Nature Korea Nature Middle East Privacy Policy Use of cookies Your privacy
    choices/Manage cookies Legal notice Accessibility statement Terms & Conditions
    Your US state privacy rights © 2024 Springer Nature Limited"'
  inline_citation: '>'
  journal: Scientific Reports
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Development and validation of a simplified risk prediction model for preterm
    birth: a prospective cohort study in rural Ethiopia'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Gonullu I.
  - Bayazit A.
  - Erden S.
  citation_count: '0'
  description: 'Background: Virtual Patients are computer-based simulations used to
    teach and evaluate patient interviews, medical diagnoses, and treatment of medical
    conditions. It helps develop clinical reasoning skills, especially in undergraduate
    medical education. This study aimed to and investigate the medical students’ perceptions
    of individual and group-based clinical reasoning and decision-making processes
    by using Virtual Patients. Methods: The study group comprised 24 third-year medical
    students. Body Interact® software was utilized as a VP tool. The students’ readiness
    and the courses’ learning goals were considered when choosing the scenarios. Semi-structured
    interview forms were employed for data collection. MAXQDA 2020 qualitative analysis
    software was used to analyze the data. The students’ written answers were analyzed
    using content analysis. Results: The participants perceived individual applications
    as beneficial when making clinical decisions with Virtual Patients, but they suggested
    that group-based applications used with the same cases immediately following individual
    applications were a more appropriate decision-making method. The results indicated
    that students learn to make decisions through trial and error, based on software
    scoring priorities, or using clinical reasoning protocols. Conclusion: In group-based
    reasoning, the discussion-conciliation technique is utilized. The students stated
    that the individual decision-making was advantageous because it provided students
    with the freedom to make choices and the opportunity for self-evaluation. On the
    other hand, they stated that the group based decision-making process activated
    their prior knowledge, assisted in understanding misconceptions, and promoted
    information retention. Medical educators need to determine the most appropriate
    method when using Virtual Patients, which can be structured as individual and/or
    group applications depending on the competency sought.'
  doi: 10.1186/s12909-024-05121-x
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Explore
    journals Get published About BMC Login BMC Medical Education Home About Articles
    Submission Guidelines Collections Join The Board Submit manuscript Research Open
    access Published: 25 February 2024 Exploring medical students’ perceptions of
    individual and group-based clinical reasoning with virtual patients: a qualitative
    study Ipek Gonullu , Alper Bayazit & Sengul Erden   BMC Medical Education  24,
    Article number: 189 (2024) Cite this article 722 Accesses 1 Altmetric Metrics
    Abstract Background Virtual Patients are computer-based simulations used to teach
    and evaluate patient interviews, medical diagnoses, and treatment of medical conditions.
    It helps develop clinical reasoning skills, especially in undergraduate medical
    education. This study aimed to and investigate the medical students’ perceptions
    of individual and group-based clinical reasoning and decision-making processes
    by using Virtual Patients. Methods The study group comprised 24 third-year medical
    students. Body Interact® software was utilized as a VP tool. The students’ readiness
    and the courses’ learning goals were considered when choosing the scenarios. Semi-structured
    interview forms were employed for data collection. MAXQDA 2020 qualitative analysis
    software was used to analyze the data. The students’ written answers were analyzed
    using content analysis. Results The participants perceived individual applications
    as beneficial when making clinical decisions with Virtual Patients, but they suggested
    that group-based applications used with the same cases immediately following individual
    applications were a more appropriate decision-making method. The results indicated
    that students learn to make decisions through trial and error, based on software
    scoring priorities, or using clinical reasoning protocols. Conclusion In group-based
    reasoning, the discussion-conciliation technique is utilized. The students stated
    that the individual decision-making was advantageous because it provided students
    with the freedom to make choices and the opportunity for self-evaluation. On the
    other hand, they stated that the group based decision-making process activated
    their prior knowledge, assisted in understanding misconceptions, and promoted
    information retention. Medical educators need to determine the most appropriate
    method when using Virtual Patients, which can be structured as individual and/or
    group applications depending on the competency sought. Peer Review reports Background
    Virtual patients (VPs) are computer-based simulations used to teach and evaluate
    patient interviews, medical diagnoses, and treatment of medical condition [1,2,3].
    VP activities can be designed based on individuals or groups, and their effectiveness
    depends on the activity design and not just on the virtual patients they employ
    [2]. VPs also instruct students in procedural and team skills [3], as well as
    help them develop communication, decision-making, situational awareness, leadership,
    and professionalism skills [4]. In terms of measurement and evaluation, they can
    be applied to both formative assessment of student performance and summative assessment
    of student success [5]. A study found that VP evaluations elicited similar results
    for those from clinical instructor evaluations and during the interactive learning
    process, VPs also can be employed to evaluate students’ reasoning and decision-making
    abilities objectively [6]. Unlike traditional simulation methodologies, VPs can
    generate continuous and predictable improvement in user performance through feedback
    and algorithms [7]. VPs contribute to the development of clinical reasoning skills,
    particularly in undergraduate medical education [8]. Clinical reasoning can be
    defined as skills, processes, or outcomes in which physicians observe, collect,
    and interpret data to diagnose and treat patients [9]. The process includes taking
    a medical history, performing a physical examination, confirming medical records,
    and providing a conclusive diagnosis. Clinical reasoning education aims to enable
    students to determine effectively which history-taking questions and examination
    methods should be used to make a correct diagnosis [10]. Gamification theory in
    education emphasizes illustrating goals and their relevance, nudging users through
    guided paths, giving users immediate feedback, and reinforcing good performance,
    which can offer an approach to enhance clinical reasoning [11]. Integrating clinical
    reasoning through case-based discussions and gamification creates a more engaging
    and interactive learning experience [12]. This integration starts by framing clinical
    scenarios as missions or challenges, creating game-like environments and interventions
    [13]. Points, levels, and badges are introduced to give users immediate feedback
    and a sense of accomplishment. The VP software, which provides real-time reactions,
    can be used to offer positive feedback, such as earning a badge when the virtual
    patient’s symptoms decrease or show improvement, indicated by the normalization
    of vital signs and improvement in breathing. This progress can motivate the learners,
    help track their progress, and improvement in learning performance [14]. Furthermore,
    gamification integrates aspects of storytelling and role-playing, allowing learners
    to take on the role of a healthcare professional navigating clinical cases. This
    immersive experience improves clinical reasoning and decision-making skills by
    applying theoretical knowledge to practical scenarios similar to real-world settings.
    Using gamification through individual clinical reasoning in medical education
    can foster an engaging and interactive learning experience. It can encourage self-directed
    learning and enhance the practical application of theoretical knowledge [15].
    Collaborative clinical reasoning emphasizes the value of teamwork in achieving
    optimal clinical outcomes and patient safety [16]. Social Learning Theory (SLT)
    emphasizes the importance of collaborative interaction and observational participation
    [17]. It highlights learning through observation, imitation, and modeling, providing
    valuable insights into how collaborative learning can be optimized for educational
    purposes. During group-based clinical reasoning, the SLT emphasizes the significance
    of observational learning. Observing peers’ reasoning and decision-making processes
    allows students to learn from their experiences and the mistakes of others [18].
    The theory highlights the importance of discussions and interactions in the learning
    process. This experience can enhance students’ understanding, activate prior learning,
    and help them identify critical queries for reaching a differential diagnosis.
    In addition, social learning also plays a crucial role in developing essential
    skills like communication, teamwork, and leadership. Individuals participating
    in small group discussions can share and discuss ideas. This process can enhance
    their communication skills and ability to express and understand diverse perspectives
    [19]. Group decision-making fosters teamwork as members coordinate, delegate,
    and support each other [20] to treat and cure virtual patients. Leadership training
    in medical education, methods like small group teaching, project-based learning,
    mentoring, and coaching were commonly employed [21], fostering the emergence of
    leadership skills as individuals learn to motivate, guide others, and assume responsibility
    in group settings. As group-work has been an important context for self-regulation,
    it has been suggested that these regulatory processes could have an interpersonal
    level in group-work. The researchers consider that group cognition is the result
    of ‘aggregation’ of minds and the different individual cognitive systems interact
    to achieve the learning outcomes of group-work [22]. According to a group of researchers,
    self-regulated learning is an important predictor of socially shared regulation
    of learning and should be considered when designing small group activities and
    their environments [23]. Collaborative learning environments like small group-work
    can enable learners to engage in activities that are valuable for facilitating
    the learning process, like self-directed learning, justifications, and reflections
    or developing arguments [24]. Students can practice their clinical reasoning skills
    individually or in groups with VPs [25, 26]. Individually or as a group, VPs can
    be used for various purposes as an educational practice tool. For example, at
    the individual level, they might be used to determine the progression or order
    of a learning session [27, 28]. At the group level, they can be utilized to bring
    together individuals with diverse perspectives and abilities, and help them progress
    toward a shared goal [26]. Before selecting whether individual or group-based
    applications are appropriate for developing instructional activities with VPs,
    instructors must consider the student competency desired. By integrating both
    individual and group clinical reasoning processes in line with the VP scenario,
    learners can be aware of their knowledge and skill shortcomings, as well as gain
    the ability to make group decisions. Identifying student experiences in group
    and individual practices can guide medical educators and researchers. Methods
    Aim of the study This qualitative study aimed toinvestigate the medical students’’
    perceptions towards individual and group-based clinical reasoning and decision-making
    with VPs. The research questions were as follows: 1) How do the students view
    differently their experience of individual and group-based decision making in
    VP simulations? 2) How do the students approach group-based decision-making processes
    with VP simulations? 3) What do the students think about VP simulation and its
    contribution to their professionaldevelopment? Study setting The “Clinical Reasoning
    with Virtual Patient” elective course is held one day each week for two hours.
    The course’s learning outcomes include patient-centered clinical evaluation and
    patient management, as well as arranging interventional procedures for diagnosis
    and/or treatment. Based on the course’s objectives, after taking the course, students
    should be able to: Recognize the importance of basic life support for patients
    in the emergency department. Formulate a primary diagnosis based on findings from
    the anamnesis and physical examination. Select diagnostic tests to evaluate for
    pre-diagnoses. Formulate differential diagnoses by integrating anamnesis, physical
    examination, and diagnostic findings with physiopathology and clinical science
    knowledge. During the sessions, Body Interact® software was utilized as a VP tool.
    During the first session, students were informed about the lesson’s teaching methodology,
    the VP program, and its application. Furthermore, an explanation of the study
    was provided, and students’ written informed consent was collected. Applications
    were conducted in five stages (Fig. 1). Fig. 1 Application flow Full size image
    I. Briefing: A summary of the scenario and the following steps is provided. II.
    Individual Decision Making: At this stage, students were given 10 min to think
    about the scenario and apply clinical reasoning procedures. Taking the Body Interact®
    scripts into consideration, anamnesis, physical examination, and test questions
    were transformed into a checklist for the scenarios (Appendix 1). Students were
    asked to rank the anamnesis, physical examination, and test questions individually
    based on their clinical reasoning and priorities (1. must be done/asked/requested
    immediately, 2. can be done/asked/requested, or 3. not required at this time).
    III. Group Decision Making: Students were assigned randomly to groups of three
    or four at this stage and instructed to complete the checklist form once again
    via group discussions. Students then managed their group dynamics and produced
    a final group decision through collaboration and clinical reasoning processes.
    They presented this conclusion as a collaborative decision on a single checklist
    form (the same as the second stage). IV. Group Decision Making with a VP: At this
    stage, each group used the simulations to implement the group decisions they made.
    In the VP simulation, they performed priority query operations depending on the
    form they completed during the third stage, and the patient interface provided
    real-time interaction results (patient responses, physical examination responses,
    and test results). V. Debriefing: Upon completion of the application, the groups’
    software performance scores were displayed on-screen. After all groups practiced
    with the VP, the students were invited to verbalize their emotions and thoughts.
    The instructors then conducted group discussions regarding students’ incomplete
    and incorrect choices made during the application phase. Some of the VP applications
    were completed face-to-face, while others were completed through distance education
    due to the COVID-19 pandemic. In this study, eight scenarios were used altogether,
    including two (one myocardial infarction case and one pneumonia case) in face-to-face
    education and six (five COVID-19 cases and one hypoglycemia case) in distance
    education. The students’ readiness and the courses’ learning goals were considered
    when choosing the scenarios. While the courses were taught through distance education,
    the VP program was made available to students online. In this process, the scenario
    names, and forms to be entered were sent to the students, who were asked to complete
    the forms based on the scenarios and send them back. However, the students who
    could not make group decisions made only individual decisions. Virtual patient
    simulation Body Interact® is a platform that offers education and training through
    VPs that respond in real time to medical interventions (Fig. 2). When a case is
    completed or the simulation time is up, a dashboard with performance ratings is
    presented. These ratings are calculated based on the users’ interactions and queries.
    The software is available for PCs (with a desktop application), PDAs, the web
    (through browsers), and multi-touchscreen devices. In this study, we used both
    the PC and web-based (https://bodyinteract.com) versions during the data collection
    processes. Fig. 2 Body interact® user interface Full size image Participants Students
    were selected for the study through “criterion sampling,” which is one of the
    purposive sampling methods with 2019–2020 academic year Spring Semester and 3rd
    term pre-clinical medical students. The criteria for participation in the study
    comprised taking the “Clinical Reasoning with Virtual Patient” course and volunteering
    to participate in the study. When selection was completed, the sample comprised
    24 students (10 females and 14 males). Data collection procedures To collect data
    in the study, the researchers prepared a semi-structured interview form (Appendix
    2), but due to the pandemic, which required that courses be held online, the interviews
    could not be conducted face-to-face, and the interview form was sent to the students
    via e-mail. Data analysis MAXQDA 2020 qualitative analysis software was used to
    analyze the data. In this framework, students’ written answers were imported to
    the qualitative analysis software, and content analysis was used. During the content
    analysis phase, the students’ responses were read, and the first codes were revealed.
    The themes then were created from the codes and finalized by checking whether
    the themes and codes formed a suitable pattern. During the data analysis, three
    researchers coded the data set separately, then discussed the codes and themes
    to reach a consensus. Altogether, 68 coded structures and 440 code sections were
    created in 24 text documents. Results RQ 1. 1 how do the students view differently
    their experience of individual and group-based decision making in VP simulations?
    The analysis found that students have varying views on individual and group-based
    discussions in decision-making processes. Two themes were detected for the first
    research question. Theme 1.1: views on the individual decision-making process
    The students reported positive aspects from the individual decision-making process,
    e.g., giving them the freedom to make decisions, observe consequences from individual
    mistakes, and conduct self-evaluations: S21, M: I think it would be more useful
    if this application was individual and online rather than in groups. … I think
    that (the) online and individual application will be more beneficial for us because
    we see our own mistakes when we do it individually (sub-theme: observing consequences
    from individual mistakes). S17, F: Individually, we learn to trust our own knowledge
    and opinions (sub-theme: conducting self-evaluations). Theme 1.2: views on integrating
    processes The students stated that in the decision-making process with VP scenarios,
    integrating the process by first applying it individually, then through group
    discussion, made them conscious of being team members, as well as recognizing
    their faulty or incomplete information, thereby bringing different views together
    on common ground. Furthermore, they said that this approach supports active learning,
    provides new knowledge and perspective, and increases learning permanence. However,
    they also stated that this approach can cause difficulties in communication and
    decision-making processes: S3, M: … I think it reinforces the team-thinking approaches
    and my spirit (as) being (part of) a team (sub-theme: making them conscious of
    being team members). S11, M: In group decisions, everyone has a different approach
    to a subject, and a more comprehensive decision is made by combining these approaches.
    In individual activity, these comprehensive issues may be disregarded (sub-theme:
    bringing different views together on common ground). S19, F: I can say that discussing
    and brainstorming with my group friends after individual decisions gave me different
    perspectives. It also helped me remember things I didn’t know or forgot (sub-theme:
    providing new knowledge and perspective; recognizing their faulty or incomplete
    information). S7, M: … With individual and group decision-making formats being
    integrated, we first questioned ourselves and revealed our information as much
    as possible, then different information from friends…. I think it provides more
    careful and active learning (sub-theme: supporting active learning). S21, M: We
    could not communicate well as a group, so everyone’s decisions could not be considered.
    All my friends experienced clinical reasoning but remained passive because they
    could not express themselves. Likewise, sometimes I couldn’t make the group accept
    the decisions I wanted (sub-theme: causing difficulties in communication and decision-making
    processes). To sum up, while the students found individual applications useful
    in clinical decision-making processes with VPs, they stated that integration of
    the processes (group discussions for the same scenario immediately after the individual
    decision-making) provided a more useful decision-making process. RQ 2.2 how do
    the students approach group-based decision-making processes with VP simulations?
    It was observed that some students changed some of their individual decisions
    after group discussions. Three themes emerged from this research question. Theme
    2.1: approaches in the individual decision-making process It was determined that
    the students developed different approaches in individual decision-making processes
    with the VP: (1) trial and error; (2) using the VP software’s scoring priorities;
    and/or (3) following the clinical reasoning steps: S21, M: Due (to) my weaknesses
    in the treatment phase, I made more progress through trial and error (sub-theme:
    trial error). S11, M: After a few uses, I was able to comprehend the simulation’s
    expectations (of) me, and I fulfilled its requirements (sub-theme: using the VP
    software’s scoring priorities). S14, M: In the first applications, I gave priority
    to questions and physical examinations directly related to the patient’s complaints.
    However, when I saw that in some scenarios, unrelated situations also affected
    the treatment, I expanded my questions and examinations in the next scenarios
    (sub-theme: following the clinical reasoning steps). S15, F: I have always done
    these choices in an orderly manner, first, I took anamnesis, and I acted according
    to which question I should ask first. I did the same in physical examination,
    tests, medications, and diagnosis, respectively (sub-theme: following the clinical
    reasoning steps). Theme 2.2: approaches in decision-making processes with the
    group The students employed two basic approaches when deciding through group discussion:
    (1) exchanging ideas and agreeing on the information they view as correct, (2)
    following predetermined steps: S3, M: In fact, we applied the reasoning and decision-making
    processes that I applied individually in a similar way with the group. We determined
    the diagnosis, necessary tests, (and) findings by brainstorming. Listening to
    other ideas in the group, we agreed on these methods and applied them in the same
    way (sub-theme: exchanging ideas and agreeing on the information they view as
    correct). S14, M: We realized that we selected too many queries when in the individual
    decision-making phase. When we came together as a group, we tried to prioritize
    the more necessary ones. We tried to (reach) a common decision by taking everyone’s
    opinion on every question and every item. (With) the items where we selected different
    items, we tried to learn from each other what we did not know (sub-theme: exchanging
    ideas and agreeing on the information they view as correct). S7, M: I can say
    that we have an approximate standard for anamnesis and physical examination. …
    we reached a synthesis result by approaching different information and ideas rationally
    and questioningly in diagnosis and treatment (sub-theme: following predetermined
    steps). Theme 2.3: reasons for changing decisions as a result of group discussion
    The reasons why students changed their decisions from the individual process after
    participating in-group discussions included: (1) awareness of lack of knowledge
    and misconceptions; (2) awareness of lack of knowledge about the scenario; (3)
    agreeing with their peers’ arguments; and (4) considering the software’s scoring
    priorities. They were found to receive: S16, F: The explanations and views of
    my group mates regarding their decisions did make sense to me. It brought to my
    attention several details I had ignored. I also gained knowledge in topics I did
    not know well. These reasons caused me to change my mind (sub-theme: awareness
    of lack of knowledge and misconceptions). S8, M: After explaining my decision,
    as a result of the discussion, the more logical and grounded ideas of my group
    mates were enough to change my decision. For example, I learned that glucagon
    should be given to a hypoglycemic patient with diabetes (sub-theme: awareness
    of lack of knowledge and misconceptions; agreeing with their peers’ arguments).
    S14, M: Another reason is that we changed it by considering (to) which criteria
    the system might have given priority. For example, maybe I will not apply the
    Glasgow Coma Scale in real applications, but we changed it by considering that
    the system probably wants it (sub-theme: considering the software’s scoring priorities).
    To sum up, students followed clinical reasoning steps in VP applications, but
    some students who gained experience with different scenarios using the software
    noted its scoring features, and then made decisions based on its priorities. During
    group discussions, students changed their minds by noticing their misconceptions
    or lack of information. RQ 3. 3 what do the students think about VP simulation
    and its contribution to their professionaldevelopment? The students stated that
    VP applications helped them in many ways, which were grouped under three themes.
    Theme 3.1: contributing to cognitive domain The students stated that decision-making
    processes with VPs: (1) ensured the permanence of what has been learned; (2) provided
    clinical reasoning skills; (3) provided the ability to interpret test results;
    and (4) provided an opportunity to learn medication: S3, M: In my opinion, it
    is an application that is similar to PBL lessons, but thanks to simulation, it
    is much more memorable, realistic, and we can see the results of our mistakes.
    When we give the interventions to a real patient, we can realize the results of
    our mistakes, and we can learn the right decisions (sub-theme: ensuring the permanence
    of what has been learned). S19, F: … The persistence of observing the results
    of my interventions during the right or wrong implementation process. When I think
    about it, I was able to put a few things in my mind that I heard in the theoretical
    lessons, but could not understand, thanks to these applications (sub-theme: ensuring
    the permanence of what has been learned). S8, M: I found it very successful, especially
    in terms of learning clinical reasoning, and therefore, I strongly recommend it
    to be used in medical school courses (providing clinical reasoning skills). S13,
    M: During the treatment process, I think that as I apply more to the work, I give
    more normal doses of medication (providing an opportunity to learn medication).
    Theme 3.2: contributing to affective domain Students stated that VP practices
    helped them by (1) reducing the level of anxiety toward clinical applications,
    (2) increasing their motivation and professional self-confidence, and (3) helping
    them deal with feelings of panic, particularly in emergency cases: S14, M: Interventions
    in emergencies seemed difficult and complex to me. I realized that thanks to this
    simulation, it became easier as I practiced. I can say that I have partially overcome
    my fear…. I can say that seeing the reflection of the information we have learned
    in the clinic has increased my motivation toward the lessons, partially (sub-theme:
    reducing the level of anxiety toward clinical applications). S10, M: The virtual
    patient application made a significant contribution to the increase (in) my self-confidence.
    Taking responsibility by calmly responding to an emergency or life-threatening
    patient (situations) increases self-confidence and enables us to be prepared for
    such situations in the future (sub-theme: increasing their motivation and professional
    self-confidence). S19, F: I think it helps us to learn to control our panic, excitement,
    and stress…. I tried to keep calm to save the patient as much as I could in sudden
    situations (sub-theme: helping them deal with feelings of panic, particularly
    in emergency cases). Theme 3.3: contributing to gaining clinical experience Medical
    students stated that they can gained a sense of clinical experience in decision-making
    processes through VPs, allowing them to realize the difficulties experienced in
    clinical practice, as well as learn from their mistakes: S3, M: The virtual patient
    simulation has given me familiarity on how I should approach patients before I
    pass to clinical phases of my education (sub-theme: gaining sense of clinical
    experience in decision-making processes). S8, M: Being able to practice is very
    important; it is very easy to access this opportunity thanks to the virtual patient
    application (sub-theme: gaining sense of clinical experience in decision-making
    processes). S4, F: In short, we have taken a lesson that is a risk-free simulation
    of our professional life for us and what we have lived has remained a sweet experience
    for all of us (sub-theme: gaining sense of clinical experience in decision-making
    processes). S18, M: Since there are no actual patients, we can act and learn with
    high flexibility. It makes us aware of the gaps in our knowledge (sub-theme: learning
    from their mistakes). In summary, the decision-making processes used during the
    VP simulation led preclinical students to appreciate its professional benefits.
    They found that it allowed them to experience the feeling of interacting with
    a real patient without the risk of harming one. Table 1 summarizes the subthemes
    for individual and group-based clinical reasoning according to our results. Table
    1 Sub-themes obtain form student answers Full size table Discussion This study
    investigated students’ perceptions in clinical reasoning processes with VPs and
    their views on the contribution of this practice to their professional development.
    With the first research question, in which they were asked for their opinions
    on individual and group-discussion decision-making processes, the students stated
    that the individual decision process was useful in terms of allowing the freedom
    to decide, demonstrating the consequences of mistakes, and providing opportunities
    for self-evaluation. While a single participant is sufficient for computer-based
    educational tasks that do not require divided attention because they do not require
    interpersonal coordination, groups of two or three are more successful than an
    individual participant as task complexity increases [29]. Accordingly, the number
    of people in the group can be determined by the task’s complexity, and in some
    cases, individual practices may be more beneficial than group practices. This
    can be explained by the ease of VP scenario levels in students’ positive perception
    towards individual clinical reasoning. However, group practices were beneficial
    in terms of getting feedback from students and gaining new ideas to improve their
    clinical performance, but students viewed talking about mistakes in these groups
    negatively [30]. This can be viewed as another factor affecting the opinions of
    the students who participated in our study and found the individual VP assessment
    more beneficial than group discussions. Some students stated that group discussions
    held immediately after individual decision making provided a more useful decision-making
    process. Conducting group discussions during sessions with VPs to retain information
    [31]. As a result of students sharing their perspectives, observations, and prior
    experiences while completing the application in a group, the information can be
    reanalyzed, reformed, and restructured. SLT suggests that people can learn new
    information and behaviors by watching others. In our study, students engaged in
    group discussions after individual decision-making, which allowed them to observe
    their peers’ perspectives, observations, and experiences [17]. Some students who
    participated in our study also supported this view and stated that they gained
    new knowledge and perspective through group discussions, and that the permanence
    of what was learned increased. SLT posits that social interaction plays a critical
    role in the learning process. Through group discussions, students actively engage
    with it, discussing and sharing insights, a key component of learning in social
    contexts [26]. With the second research question, students’ clinical reasoning
    processes were discussed. It has been determined that they manage the process
    through trial and error, consider software scoring priorities, and/or follow clinical
    reasoning steps in clinical decision-making processes. Students feel more comfortable
    trying different strategies in a gamified setting without fear of real-world consequences.
    Case-based discussion and gamification strategies engage [12] students and foster
    a trial-and-error approach, where students can learn from their mistakes and refine
    their clinical reasoning skills towards a common goal [11]. For instance, in our
    study, students made an incorrect diagnosis or intervention, and the virtual patient
    provided instant feedback such as dizziness, bruising, and sweating, allowing
    the student to understand their error and try a different approach. On the other
    hand, students with moderate self-regulation skills in particular can demonstrate
    gaming-system behavior [32], which can be explained by the fact that learners
    try to be successful or get a high score in an educational environment by utilizing
    the help, results, or feedback features of the system instead of trying to learn
    the material [33]. In this study, instead of transforming students’ theoretical
    knowledge into practice processes through clinical reasoning steps, the choice
    to progress by considering trial-and-error or software-scoring priorities can
    be viewed as “gaming the system.” The students reached common decisions by exchanging
    ideas in their approaches to the decision-making process with the group, and they
    followed the clinical reasoning steps. Decisions that individuals made alone in
    decision-making processes were investigated with decisions that individuals made
    through group discussion-conciliation [20]. The results indicated that the students
    who used group discussion-conciliation made more accurate decisions than those
    who employed individual decision-making. In our study, students reported that
    they used group discussion-conciliation processes while making decisions with
    the group in VP applications, and that their decisions were more accurate compared
    with the individual decision-making process. Another study found that using VPs
    in a collaborative learning activity was more effective in improving students’
    knowledge and retaining treatment decisions than in an independent learning activity
    [25]. Another finding that emerged from the study was that after using the individual
    approach, students sometimes changed their decisions after working with a group.
    It has been determined that being aware of theoretical or scenario-knowledge deficiencies,
    accepting their peers’ ideas, or considering software scoring priorities affect
    these decision changes. With the third research question, which examined VPs’
    contribution to professional development, we found that these practices benefitted
    students in terms of gaining cognitive, affective, and clinical experience. Among
    these contributions, VPs provided learning permanence, boosted clinical reasoning
    skills, provided experience in treatment processes, reduced anxiety levels, increased
    motivation and professional self-confidence, and created a sense of treating real
    patients without the risk of harming them. These results supported the outcomes
    of previous studies [6, 8, 34,35,36]. Although VPs elicited little effect on knowledge
    acquisition, VP users prepared themselves for clinical experience and viewed them
    as a good resource to help them reinforce their skills [37]. This was a qualitative
    study in which 24 medical school students participated, with the results limited
    to those who participated. Furthermore, the sessions that initially were held
    face-to-face had to move online due to the COVID-19 pandemic; therefore, the applications
    were made individually in the online course process, and the decision-making process
    could not be realized with the group. Thus, students’ group decision-making experiences
    comprised face-to-face sessions, another limitation of the study. Conclusions
    VP is a simulation method that provides an opportunity to evaluate the stages
    – e.g., data collection, diagnosis, and patient management – used in the clinical
    reasoning process, monitor changes in student performance, and provide feedback.
    The effective use of VPs in medical education plays an essential role in achieving
    learning goals and permanence in learned knowledge. For this reason, medical educators
    must determine the most appropriate method when using VPs, which can be structured
    as individual and/or group applications depending on the competency sought. The
    students perceived that the individual decision-making process is beneficial in
    terms of giving freedom to make decisions, as well as the opportunity for self-evaluation.
    They also stated that the group decision-making process can be beneficial in terms
    of activating students’ prior knowledge, as well as helping them recognize knowledge
    deficits and gain learning permanence. Based on the research findings, the following
    are nine tips for educators: 1. Facilitate Post-Individual Decision-Making Discussions:
    After individual clinical decision-making, encourage group discussions to enable
    students to share information and reanalyze it, leading to more informed and refined
    decisions. 2. Integrate Group Discussions with VP Sessions: During virtual patient
    sessions, incorporate group discussions to improve retention and practical application.
    3. Encourage Sharing of Student Perspectives: Promote a comfortable environment
    where students can share observations and prior experiences, improve their communication
    skills, and exchange information. 4. Focus on Clinical Reasoning Processes: Guide
    students to effectively manage clinical reasoning processes through structured
    steps and trial and error to make decisions. 5. Address “Gaming the System” Behavior:
    Be aware that students are not demonstrating gaming the system behavior. Guide
    them towards focusing on the clinical reasoning process rather than simply aiming
    for high scores. 6. Promote Group Discussion-Conciliation: Encourage group discussion
    and conciliation to achieve shared decisions, leading to more accurate outcomes.
    7. Acknowledge the Role of Peer Influence and Knowledge Gaps: It’s important to
    acknowledge that students may change their decisions after group discussions due
    to the awareness of knowledge gaps or the influence of peers’ ideas. Peer influence
    can play a crucial role in the learning process. 8. Emphasize VPs’ Contribution
    to Professional Development: Highlight how working with VPs contributes to cognitive,
    affective, and clinical experience, enhancing skills like clinical reasoning,
    reducing anxiety, and increasing motivation and professional confidence. 9. Use
    VPs for Realistic Practice: Utilizing VPs provides a low-risk way for students
    to gain experience in treatment processes and prepare for real clinical situations
    while reinforcing their skills in a safe environment. In future studies, to support
    the findings and identify the difficulties experienced in these processes, a detailed
    examination will be made by including retrospective thinking, in-group interaction
    metrics, and lecturer observation reports. Data availability The datasets generated
    during and/or analysed during the current study are available from the corresponding
    author on reasonable request. Abbreviations VP: Virtual Patient SLT: Social Learning
    Theory References Ellaway R, et al. Building a virtual patient commons. Med Teach.
    2008;30(2):170–4. Article   PubMed   Google Scholar   Ellaway R, et al. Virtual
    patient activity patterns for clinical learning. Clin Teach. 2015;12(4):267–71.
    Article   PubMed   Google Scholar   Kononowicz AA, et al. Virtual Patient Simulations
    in Health Professions Education: systematic review and Meta-analysis by the Digital
    Health Education Collaboration. J Med Internet Res. 2019;21(7):e14676. Article   PubMed   PubMed
    Central   Google Scholar   Peddle M, et al. Development of non-technical skills
    through virtual patients for undergraduate nursing students: an exploratory study.
    Nurse Educ Today. 2019;73:94–101. Article   PubMed   Google Scholar   Hamdy H,
    et al. Virtual clinical encounter examination (VICEE): a novel approach for assessing
    medical students’ non-psychomotor clinical competency. Med Teach. 2021;43(10):1203–9.
    Article   PubMed   Google Scholar   Setrakian J, et al. Comparison of Assessment
    by a virtual patient and by clinician-educators of Medical Students’ history-taking
    skills: exploratory descriptive study. JMIR Med Educ. 2020;6(1):e14428. Article   PubMed   PubMed
    Central   Google Scholar   Cant RP, Cooper SJ. Simulation in the internet age:
    the place of web-based simulation in nursing education. An integrative review.
    Nurse Educ Today. 2014;34(12):1435–42. Article   PubMed   Google Scholar   Plackett
    R, et al. The effectiveness of using virtual patient educational tools to improve
    medical students’ clinical reasoning skills: a systematic review. BMC Med Educ.
    2022;22(1):365. Article   PubMed   PubMed Central   Google Scholar   Cooper N,
    et al. Consensus statement on the content of clinical reasoning curricula in undergraduate
    medical education. Med Teach. 2021;43(2):152–9. Article   PubMed   Google Scholar   Schmidt
    HG, Mamede S. How to improve the teaching of clinical reasoning: a narrative review
    and a proposal. Med Educ. 2015;49(10):961–73. Article   PubMed   Google Scholar   Krath
    J, Schürmann L, von Korflesch HFO. Revealing the theoretical basis of gamification:
    a systematic review and analysis of theory in research on gamification, serious
    games and game-based learning. Comput Hum Behav. 2021;125:106963. Article   Google
    Scholar   Mackavey C, Cron S. Innovative strategies: increased engagement and
    synthesis in online advanced practice nursing education. Nurse Educ Today. 2019;76:85–8.
    Article   PubMed   Google Scholar   Kuipers DA, et al. The role of transfer in
    designing games and simulations for health: systematic review. JMIR Serious Games.
    2017;5(4):e7880. Article   Google Scholar   Bai S, Hew KF, Huang B. Does gamification
    improve student learning outcome? Evidence from a meta-analysis and synthesis
    of qualitative data in educational contexts. Educational Res Rev. 2020;30:100322.
    Article   Google Scholar   Abdul Ghani AS, et al. Developing an interactive PBL
    environment via persuasive gamify elements: a scoping review. Res Pract Technol
    Enhanced Learn. 2022;17(1):21. Article   Google Scholar   Lee C-Y et al. Collaborative
    clinical reasoning: a scoping review. medRxiv, 2023: p. 2023.02. 09.23285741.
    Li S, Hong Y-C, Craig SD. A systematic literature review of Social Learning Theory
    in Online Learning environments. Educational Psychol Rev. 2023;35(4):108. Article   Google
    Scholar   Tutticci N, et al. Exploring the observer role and clinical reasoning
    in simulation: a scoping review. Nurse Educ Pract. 2022;59:103301. Article   PubMed   Google
    Scholar   Smith S, et al. Teaching patient communication skills to medical students:
    a review of randomized controlled trials. Eval Health Prof. 2007;30(1):3–21. Article   PubMed   Google
    Scholar   DiPierro K, et al. Groupthink among health professional teams in patient
    care: a scoping review. Med Teach. 2022;44(3):309–18. Article   PubMed   Google
    Scholar   Sadowski B, et al. Leadership Training in Graduate Medical Education:
    a systematic review. J Graduate Med Educ. 2018;10(2):134–48. Article   Google
    Scholar   Grau V, Whitebread D. Self and social regulation of learning during
    collaborative activities in the classroom: the interplay of individual and group
    cognition. Learn Instruction. 2012;22(6):401–12. Article   Google Scholar   Panadero
    E, et al. How individual self-regulation affects Group Regulation and Performance:
    A Shared Regulation intervention. Small Group Res. 2015;46(4):431–54. Article   Google
    Scholar   Kirschner F, Paas F, Kirschner PA. Individual and group-based learning
    from complex cognitive tasks: effects on retention and transfer efficiency. Comput
    Hum Behav. 2009;25(2):306–14. Article   Google Scholar   Marei HF, Donkers J,
    Van Merrienboer JJG. The effectiveness of integration of virtual patients in a
    collaborative learning activity. Med Teach. 2018;40(sup1):S96–S103. Article   PubMed   Google
    Scholar   Mestre A, et al. The impact of small-group virtual patient simulator
    training on perceptions of individual learning process and curricular integration:
    a multicentre cohort study of nursing and medical students. BMC Med Educ. 2022;22(1):375.
    Article   PubMed   PubMed Central   Google Scholar   Adefila A, et al. Students’
    engagement and learning experiences using virtual patient simulation in a computer
    supported collaborative learning environment. Innovations Educ Teach Int. 2020;57(1):50–61.
    Google Scholar   Zheng J, Li S, Lajoie SP. Diagnosing virtual patients in a technology-rich
    learning environment: a sequential mining of students’ efficiency and behavioral
    patterns. Education and Information Technologies; 2021. Klein J. Individual and
    group performance of computerized educational tasks. Educ Inform Technol. 2013;18(3):443–58.
    Article   Google Scholar   Parish SJ, et al. APPLIED RESEARCH: Teaching Clinical
    skills through Videotape Review: a Randomized Trial of Group Versus Individual
    Reviews. Teach Learn Med. 2006;18(2):92–8. Article   PubMed   Google Scholar   Mardani
    M, et al. Effectiveness of virtual patients in teaching clinical decision-making
    skills to dental students. J Dent Educ. 2020;84(5):615–23. Article   PubMed   Google
    Scholar   Verstege S, et al. Relations between students’ perceived levels of self-regulation
    and their corresponding learning behavior and outcomes in a virtual experiment
    environment. Comput Hum Behav. 2019;100:325–34. Article   Google Scholar   Baker
    R, et al. Why students engage in Gaming the System Behavior in interactive learning
    environments. J Interact Learn Res. 2008;19(2):185–224. Google Scholar   Isaza-Restrepo
    A, et al. The virtual patient as a learning tool: a mixed quantitative qualitative
    study. BMC Med Educ. 2018;18(1):297. Article   PubMed   PubMed Central   Google
    Scholar   Verkuyl M, Betts L, Sivaramalingam S. Nursing students’ perceptions
    using an interactive Digital Simulation table: a usability study. Volume 50. Simulation
    & Gaming; 2019. pp. 202–13. 2. Woodham LA, et al. Virtual patients designed for
    training against medical error: exploring the impact of decision-making on learner
    motivation. PLoS ONE. 2019;14(4):e0215597. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Jeimy S, Wang JY, Richardson L. Evaluation of virtual patient cases
    for teaching diagnostic and management skills in internal medicine: a mixed methods
    study. BMC Res Notes. 2018;11(1):357. Article   PubMed   PubMed Central   Google
    Scholar   Download references Acknowledgements We sincerely thank BERTAŞ A.Ş for
    their generous support through the provision of Body Interact® software and hardware,
    which played a crucial role in the data collection process of our study. Funding
    This article has not been granted by any institution or organization. The authors
    declared that this study has received no financial support. Author information
    Authors and Affiliations Faculty of Medicine, Department of Medical Education
    and Informatics, Ankara University, Cebeci, Ankara, Turkey Ipek Gonullu, Alper
    Bayazit & Sengul Erden Contributions All authors contributed to the study’s commencement
    and coordination, collected data, and drafted the manuscript. Conception, design,
    development of the Virtual Patient and first conception of the research protocol
    (IG, AB). Course design (IG, AB, SE). Design, development, collection of data,
    results analyses, discussion, article writing and critical review (IG, AB, SE).
    All authors read and approved the final manuscript. Corresponding author Correspondence
    to Ipek Gonullu. Ethics declarations Ethical approval This study was conducted
    in accordance with the Declaration of Helsinki and approved by the Institutional
    Ethics Committee of Ankara University Faculty of Medicine (protocol code: İ4-145-19,
    date of approval: 10 October 2019) for studies involving humans. Informed written
    consent to participate was obtained from all students. Students participated in
    the study voluntarily, and their scores remain confidential. Consent for publication
    Not applicable. Competing interests The authors declare no competing interests.
    Additional information Publisher’s Note Springer Nature remains neutral with regard
    to jurisdictional claims in published maps and institutional affiliations. Electronic
    supplementary material Below is the link to the electronic supplementary material.
    Appendix 1: Scenario Checklist Appendix 2: Semi-Structured Interview Form Rights
    and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article’s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article’s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. The Creative
    Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/)
    applies to the data made available in this article, unless otherwise stated in
    a credit line to the data. Reprints and permissions About this article Cite this
    article Gonullu, I., Bayazit, A. & Erden, S. Exploring medical students’ perceptions
    of individual and group-based clinical reasoning with virtual patients: a qualitative
    study. BMC Med Educ 24, 189 (2024). https://doi.org/10.1186/s12909-024-05121-x
    Download citation Received 25 September 2023 Accepted 31 January 2024 Published
    25 February 2024 DOI https://doi.org/10.1186/s12909-024-05121-x Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Clinical reasoning Individual vs. group-based Virtual patient Qualitative
    study Medical students Download PDF Sections Figures References Abstract Background
    Methods Results Discussion Conclusions Data availability Abbreviations References
    Acknowledgements Funding Author information Ethics declarations Additional information
    Electronic supplementary material Rights and permissions About this article Advertisement
    BMC Medical Education ISSN: 1472-6920 Contact us Submission enquiries: bmcmedicaleducation@biomedcentral.com
    General enquiries: ORSupport@springernature.com Read more on our blogs Receive
    BMC newsletters Manage article alerts Language editing for authors Scientific
    editing for authors Policies Accessibility Press center Support and Contact Leave
    feedback Careers Follow BMC By using this website, you agree to our Terms and
    Conditions, Your US state privacy rights, Privacy statement and Cookies policy.
    Your privacy choices/Manage cookies we use in the preference centre. © 2024 BioMed
    Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: BMC Medical Education
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Exploring medical students’ perceptions of individual and group-based clinical
    reasoning with virtual patients: a qualitative study'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Al-Yaseen W.
  - Nanjappa S.
  - Jindal-Snape D.
  - Innes N.
  citation_count: '0'
  description: 'Background: This longitudinal study using qualitative methodology
    aims to investigate the perceptions, and implementation, of evidence-based guidelines
    into practice among new dental graduates (NDGs) during their transition from university
    into professional practice, by identifying factors that influence the adoption
    of evidence-based practice (EBP) in dental practice. Methods: The study invited
    NDGs from one UK dental school (N = 66) and employed longitudinal, multiple qualitative
    methodologies for data collection, throughout the participants’ Vocational Dental
    Training (VDT) year. Initial interviews (Interview 1) conducted upon graduation
    and follow-up interviews (Interview 2) carried out between six and nine months
    into professional practice were combined with participants longitudinal audio
    diaries (LADs) recorded between the interviews. The study. Results: A total of
    12 NDGs agreed to participate. For Interview 1, twelve participants were interviewed,
    seven of whom agreed to participate in Interview 2 and six recorded the LADs.
    Interview 1 exposed diverse views among NDGs about EBP, acknowledging its significance
    but facing obstacles in implementation due to time and financial constraints.
    They intended to use evidence selectively, often aligning with trainers’ or NHS
    treatment options, while hesitating to fully embrace EBP in a busy dental practice.
    During VDT, LAD entries showed initial enthusiasm for EBP, but integrating evidence-based
    guidelines within the NHS system led to pragmatic treatment decisions, balancing
    gold-standard and cost-effective options. Over time, NDGs became more comfortable
    with alternative treatments, considering patients’ financial constraints, yet
    they expressed frustration with external pressures limiting their clinical decision-making
    autonomy. In Interview 2, after six to nine months in practice, NDGs exhibited
    mixed attitudes towards EBP. Some actively used dental guidelines like SDCEP,
    others associated EBP with hi-tech or expensive materials, while others would
    thought to rely on colleagues’ recommendations. None consistently sought direct
    evidence for treatment decisions. Conclusion: NDGs’ attitudes towards EBP changed
    and became more negative over their first year in professional practice, leading
    to challenges in their applying it. It questions the assumption that teaching
    EBP during undergraduate education ensures its implementation. Further understanding
    the influences on the development of attitudinal challenges will help to devise
    effective strategies for fostering lifelong learning and supporting evidence-based
    practice in dentistry.'
  doi: 10.1186/s12909-024-05182-y
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Explore
    journals Get published About BMC Login BMC Medical Education Home About Articles
    Submission Guidelines Collections Join The Board Submit manuscript Research Open
    access Published: 26 February 2024 New dental graduates transition into UK professional
    practice; a longitudinal study of changes in perceptions and behaviours through
    the lens of evidence-based dentistry Waraf Al-Yaseen, Sucharita Nanjappa, Divya
    Jindal-Snape & Nicola Innes  BMC Medical Education  24, Article number: 195 (2024)
    Cite this article 402 Accesses 4 Altmetric Metrics Abstract Background This longitudinal
    study using qualitative methodology aims to investigate the perceptions, and implementation,
    of evidence-based guidelines into practice among new dental graduates (NDGs) during
    their transition from university into professional practice, by identifying factors
    that influence the adoption of evidence-based practice (EBP) in dental practice.
    Methods The study invited NDGs from one UK dental school (N = 66) and employed
    longitudinal, multiple qualitative methodologies for data collection, throughout
    the participants’ Vocational Dental Training (VDT) year. Initial interviews (Interview
    1) conducted upon graduation and follow-up interviews (Interview 2) carried out
    between six and nine months into professional practice were combined with participants
    longitudinal audio diaries (LADs) recorded between the interviews. The study.
    Results A total of 12 NDGs agreed to participate. For Interview 1, twelve participants
    were interviewed, seven of whom agreed to participate in Interview 2 and six recorded
    the LADs. Interview 1 exposed diverse views among NDGs about EBP, acknowledging
    its significance but facing obstacles in implementation due to time and financial
    constraints. They intended to use evidence selectively, often aligning with trainers’
    or NHS treatment options, while hesitating to fully embrace EBP in a busy dental
    practice. During VDT, LAD entries showed initial enthusiasm for EBP, but integrating
    evidence-based guidelines within the NHS system led to pragmatic treatment decisions,
    balancing gold-standard and cost-effective options. Over time, NDGs became more
    comfortable with alternative treatments, considering patients’ financial constraints,
    yet they expressed frustration with external pressures limiting their clinical
    decision-making autonomy. In Interview 2, after six to nine months in practice,
    NDGs exhibited mixed attitudes towards EBP. Some actively used dental guidelines
    like SDCEP, others associated EBP with hi-tech or expensive materials, while others
    would thought to rely on colleagues’ recommendations. None consistently sought
    direct evidence for treatment decisions. Conclusion NDGs’ attitudes towards EBP
    changed and became more negative over their first year in professional practice,
    leading to challenges in their applying it. It questions the assumption that teaching
    EBP during undergraduate education ensures its implementation. Further understanding
    the influences on the development of attitudinal challenges will help to devise
    effective strategies for fostering lifelong learning and supporting evidence-based
    practice in dentistry. Peer Review reports Introduction The transition from being
    an undergraduate student at Dental School to becoming an independent professional
    dental practitioner is a time of change and can also be a time of stress when
    trying to establish long-term clinical practice habits [1]. The new graduate needs
    to adapt to new working conditions, establish relationships with patients and
    colleagues, and navigate new clinical and administrative processes [2]. This transition
    can be stressful and challenging, influencing dentists’ decision-making and clinical
    practice in the long term. Vocational Dental Training (VDT) (the equivalent scheme
    is called Dental Foundation Training in England and Wales and Northern Ireland)
    is a supervised training period for newly graduating dentists, serving as a transitional
    stage from undergraduate education to independent practice. It provides a gradual
    transition for dental graduates, towards independent clinical decision-making
    and patient care and completes the shift that generally occurs towards the end
    of the undergraduate dental programme, towards holistic care rather than single-discipline
    focussed treatment. VDT is overseen by the UK Committee of Postgraduate Dental
    Deans and Directors and requires completion of a set of competencies outlined
    in the A Curriculum for UK Dental Foundation Programme Training [3]. The recruitment
    process in Scotland begins in September when dental students are at the start
    of the final year of their BDS programme. Interviews for the post take place around
    May, and the results are usually available in June. The VDT is expected to commence
    at the start of August. The rest of the UK follows a slightly different timeline,
    but it is similar in terms of the milestones for new dental graduates on their
    journey through the application and securing a post. Evidence-based practice (EBP)
    is an approach to oral healthcare decision-making that involves integrating scientific
    evidence, clinical expertise, and patient preferences. Its underpinning principle
    is to improve health outcome for patients on various levels [4, 5]. Although the
    evidence for this downstream effect might be weak, it is a fundamental part of
    lifelong learning. In the UK, EBP is a fundamental competency expected to be attained
    upon graduation from dental schools across the UK. Integrated into the General
    Dental Council (GDC) “Preparing for Practice” standards [6]. The teaching approaches
    of EBP can vary between dental schools, leading to some variability in how it’s
    taught and assessed. However, in general, EBP education is introduced early in
    the dental curriculum starts early and continues throughout the degree, incorporating
    various educational strategies [7]. Teaching usually covers critical appraisal,
    literature review, research methodology, and the systematic five-step process
    for applying evidence in clinical practice. The teaching methods include didactic
    lectures, interactive problem-based learning, and immersive clinical training.
    EBP principles are integrated into clinical experiences to encourage practical
    application, and students are assessed through written assignments, oral presentations,
    and Objective Structured Clinical Examinations (OSCEs). In Dentistry, treatments
    and materials change as well as the clinical environment, remuneration and patient
    expectations. Seeking and critically appraising evidence is a crucial aspect of
    EBP as it supports decision-making to ensure safe and high-quality care. However,
    the implementation of EBP among dental practitioners varies. with General Dental
    Practitioners’ (GDPs) perceptions and behaviour towards EBP being mixed, and some
    evidence of resistance to consulting evidence in practice [8, 9]. Several factors
    influencing GDPs’ adoption of EBP have been identified. Internal elements include
    a lack of familiarity or confidence in using EBP and time constraints, while external
    factors include financial resources and practice policies. EBP is a significant
    component of undergraduate training in UK dental schools, as it is a competence
    that must be demonstrated to satisfy the requirements of the UK Professional Statutory
    Regulatory Body, the General Dental Council (GDC) [10]. Despite this emphasis
    on EBP, little is known about the behaviour of new dental graduates (NDGs) related
    to EBP and how it may change during their transitions to professional practice.
    Understanding the potential factors that influence the adoption of EBP among NDGs,
    may provide insights to improve the integration of EBP in dental practice and
    enhance the quality of patient care. Therefore, this study’s aim was to explore
    whether NDGs’ behaviour, including their perceptions of, intentions to use, and
    implementation of, EBP, changed during this transition. Method Ethics approval
    and consent The study protocol was approved by the University of Dundee, Schools
    of Nursing & Health Sciences and Dentistry Ethics Committee (Application number
    2,018,009). Written informed consent was obtained from all study participants.
    Overview of study design This research was part of a larger programme of research.
    This research was part of a larger programme of research (See Fig. 1) [11, 12].
    A multi-method, longitudinal study was conducted using a qualitative longitudinal
    methodology, with two data collection methods. Data collection spanned from participants’
    graduation until six to nine months into their Vocational Dental Training (VDT)
    (Foundation Dental Training in England, Wales, and Northern Ireland) [3]. Fig.
    1 Dental Student Transition to Practice project overview of the three data collection
    methods. Findings from the questionnaire arm and the qualitative arm related to
    the exploration of NDGs transition experience has been published [9, 10] Full
    size image Semi-structured interviews were conducted at two time points. The first
    interview (Interview 1) was conducted shortly after participants had graduated
    as dentists but before starting their VDT placement. The second interview (Interview
    2) took place six to nine months after the NDGs began their 12-month VDT placement.
    Longitudinal Audio Diaries (LADs) were recorded by the NDGs during the first six
    to nine months of their VDT placement. Target population and recruitment The target
    population for this study was NDGs of the academic year 2018–2019 in the UK. Recruitment
    took place in one dental school with all NDGs being invited (N = 66). Recruitment
    was initiated by contacting final year dental students who had just completed
    their final year examinations but had not officially graduated. They were invited
    to participate via their university email. The invitation email aimed to recruit
    participants for all three project components and included an invitation letter,
    a link to a questionnaire asking about evidence-based practice (previously published),
    a participant information sheet (Appendix 1a), and the consent form (Appendix
    1b) for the interviews and LADs. The contact details of the Principal Investigator
    were included in the email and the NDGs were asked to contact them if they were
    interested in participating, to schedule a time and location for the interviews.
    The recruitment process for Interview 1 was open-ended, starting with the invitation
    email and continuing with a snowball sampling strategy which involved participants
    helping to recruit other final year students through their informal and social
    networks. Snowball sampling was deemed to be the most suitable method due to the
    unique circumstances of the target sample. Our participants, newly graduated dental
    professionals entering their Vocational Dental Training, were undergoing a transitional
    phase characterised by significant life changes, such as relocating to different
    cities. This posed a challenge when recruiting participants through purposive
    sampling, which typically relies on predefined criteria. Snowball sampling enabled
    us to leverage existing connections and networks, making participant recruitment
    more feasible during a period of geographic mobility and life transition. No sample
    size estimation was conducted prior to the study. Recruitment was stopped when
    no new topics were discussed in the interviews. Recruitment for Interview 2 was
    conducted by inviting Interview 1 participants to take part in the follow-up interview.
    Participants for the LAD component of the project was also recruited through the
    initial invitation email, and recruitment continued through a snowball sampling
    strategy until the start of the VDT placement. Methods of data collection Semi-structured
    interviews Before the interview, NDGs reviewed and signed an informed consent
    form, with the opportunity to ask questions. Participants were offered face-to-face
    interviews in a quiet and convenient place within the dental school premises or
    online via Skype. A topic guide was used to outline discussion areas (Appendix
    2); participants were asked to freely expand on other topics that were important
    to them. The interview interactions were conversational in nature and were recorded
    using a digital audio recorder. Longitudinal audio diaries (LADs) LADs were also
    employed to ensure that participants could capture incidents and experiences most
    relevant to them, contemporaneously. For their convenience and ease of availability,
    participants were asked to record their LADs using their smartphones and send
    the recordings to the Principal Investigator through end-to-end encrypted messages
    via WhatsApp. Written diaries were also offered to those who preferred to submit
    their data in a written format via email. Prior to starting recording, there was
    discussion with the participants about the importance of recording meaningful
    occurrences to them during their daily practice. Meaningful in this context was
    defined as any event/interaction, whether at work or outside work, that the participants
    remembered or that left an impact on their feelings or behaviours. Fortnightly
    reminders were sent to the participants (solicited entries). However, they were
    also encouraged to send their reflections whenever they felt they had something
    to say (unsolicited entries). Data handling The interviews and LADs were transcribed
    using a non-verbatim approach, focusing on capturing the essential meaning of
    the spoken words while rectifying grammar errors and omitting non-essential elements.
    This method ensured accurate and concise transcriptions for efficient data analysis.
    Recordings and transcripts were stored in a password protected online folder in
    the dental school’s secure OneDrive cloud. Transcripts were anonymised and any
    personal details within the transcripts that would have identified the participants
    were replaced with pseudonyms or omitted if it did not influence the transcript
    context. Data analysis Thematic analysis [13] was employed for data analysis with
    the aid of NVivo (Version 11.0, Lumivero, Melbourne, Australia) software [14].
    Data from Interview 1, Interview 2 and the LADs were analysed separately to look
    at patterns at particular time points. Then, findings from each timepoint were
    compiled to allow understanding around the longitudinal aspects of the participants’
    transitions. Data analysis was conducted by one member of the research team. To
    assure validity of the results, a subset of the analysed transcripts were reviewed
    by a more experienced researcher in qualitative research. For the interviews,
    the analysis process was carried out using an inductive approach using a codebook.
    An initial coding frame of the main topics that were discussed by the participants
    was created based on the interview questions (main themes). The analysis process
    was then carried out inductively using reflective analysis approach to develop
    emerging subthemes according to participants’ inputs and views. For the LAD data,
    analyses were carried out using a reflexive thematic analysis approach where themes
    and subthemes were developed based on participants’ views by iteratively reviewing
    each text throughout the data collection and analyses process. Reflections from
    the LADs from one participant with the largest amount of LAD entries will be presented
    in the results as a case study. At the data analysis stage, triangulation of the
    data from semi-structured interviews and LADs was performed. Within each theme,
    the findings were combined and presented longitudinally at three different times
    to illustrate the NDGs’ transition journey: (1) upon graduation (Interview 1);
    (2) during VDT (LADs); and (3) after spending at least six to nine months in practice
    (Interview 2). Results Participants: Project participants’ demographic information
    had been previously reported [12]. Briefly, in Interview 1, eleven participants
    were interviewed for a total of 759 min. The average interview duration was 69
    min, with a median of 61 min and ranged from 38 to 129 min. In Interview 2, seven
    NDGs participated, with a total interview time of 487 min. The interviews had
    an average duration of 70 min, a median of 68 min, and ranged from 47 to 97 min.
    Four interviews were conducted in person, and three were conducted over Skype.
    There were, six NDGs who took part in the LADs, providing a combination of audio
    diaries (five participants) and a written diary (one participant). Overall, there
    were 47 entries (42 recorded, five written), with each participant contributing
    between two and 24 entries. The total duration of the entries was 378 min. Main
    findings The findings from interviews and LADs are reported chronologically to
    express its longitudinal nature. The main topic summaries from the interviews
    are presented. To provide a longitudinal narrative of NDGs’ transitions over time,
    the LAD data from one participant is presented (See Fig. 2 and Appendix 3). Fig.
    2 Main themes developed from the collected data (interviews + LADs) Full size
    image Upon graduation and before starting professional practice, findings from
    interview 1 Theme 1- expectations Participants’ perceptions and expectations to
    practice in line with available evidence were explored during Interview 1. The
    opinions were mixed, although there was a general belief that the process of practising
    in line with the evidence-base would be more challenging in a general practice
    setting. Participants expected they would consult the evidence during their practice
    when the circumstances were favourable; mainly if it was in line with the practise
    philosophy of their trainer or what would be available within the NHS provided
    treatment. I don’t really expect to go by the book. I think I’ll sort of be doing
    a treatment that would be recognised as common and accepted practice and then
    proper consent would protect you. NDG 11 -Interview 1. Theme 2: Attitude towards
    EBP: Some participants expressed reluctance due to time constraints or their perception
    of the value of evidence-based practice in a busy general practice setting. They
    believed that reasonable compromises are acceptable if patients were satisfied
    with treatment outcomes, regardless of guideline recommendations. Financial constraints
    were also frequently mentioned. I’m starting off VDT, my practice doesn’t do Hall
    crowns and that’s fair enough. They’re ludicrously expensive… Even though the
    clinical outcomes for Hall crowns are consistently better. Doesn’t matter, can’t
    afford them. NDG 6 -Interview 1. There was a generally negative perception towards
    the process of finding and accessing evidence, as it was considered to be complicated
    and challenging. Evidence should be given to you in a way that covers the causes,
    signs, symptoms, and treatments. It shouldn’t be necessary for me to hunt on Google
    Scholar or type in weird combinations of words on PubMed. NDG 4- Interview 1.
    Changes during professional practice, a case study from LADs findings The diaries
    of six participants were received with varying lengths and frequencies. From these
    diaries, the journey of one participant, Tom, will be presented as he provided
    the most extensive input among the participants, offering deeper insights into
    his experiences as a new dental graduate starting his professional life. At the
    time of data collection, Tom was a 23-year-old dental graduate with no previous
    degree. He provided 41 diary entries totalling approximately 93 min from (August
    2018- June 2019). Theme 3- Embarking on professional life: independence and learning
    curve: Tom’s initial entries reflected positive and enthusiastic first impressions
    of his VDT experience. Tom felt a sense of clinical independence and highlighted
    the opportunity to learn new clinical skills and work with new materials. He also
    noticed a difference in the involvement of patients in the treatment planning
    process, which he attributed to the fact that patients paid for their treatments.
    However, Tom struggled with the NHS payment reimbursement system and found the
    software difficult to navigate, with complicated treatment coding. Theme 4- Diving
    into professional practice: Feeling conflicted: The first two months of Tom’s
    VDT were busy for him. His LAD submissions were filled with examples of clinical
    cases and reflections on the changes in his practicing philosophy, and growing
    awareness of treatment costs. Tom was cautious about certain treatments he considered
    affordable, like dental pins, which weren’t the recommended best practice. He
    expressed his anxiousness regarding making these compromises at the beginning
    and how he aimed to do what is best for his patients. I feel anxious about providing
    these treatments. We’re taught the gold standard, and that’s what I want to provide
    for my patients. (5 weeks into VDT) Tom also found discussing treatment options
    with patients to be an unpleasant experience. His frustration stemmed from being
    aware of the gold standard treatment but being unable to offer it under the NHS
    system. He felt restricted to providing budget-type treatments, which he didn’t
    appreciate. When it comes to providing treatment in practice and knowing what
    the gold standards are, but then only being able to offer sort of NHS budget type
    treatment, I really don’t like it. (6 weeks into VDT) Tom further reflected on
    this topic two weeks later after encountering situations where he needed to incorporate
    financial considerations into his treatment planning. That’s me done it a couple
    of times now. It doesn’t get any easier necessarily. You still want to go ahead
    and do the gold standard, and I think when patients opt for the cheaper of the
    two options, I feel a sense of pressure to be able to get the same results. (7
    weeks into VDT) Tom reflected on his new management approaches and shared instances
    where his trainer suggested alternative treatments that may not be considered
    best practices. A patient comes in with scores of 3 on their BPE (Basic Periodontal
    Examination)… I knew we should do full periodontal charts and Root Surface Debridement
    (RSD) and all this. But then my trainer says, ‘What you’re going to do is a deep
    scrape and polish.’ I struggled with that. It felt like cutting corners because
    I was fully aware that it wouldn’t effectively address the patient’s gingival
    condition, and in fact, it might even make it worse. (9 weeks into VDT). Theme
    5- Patient expectations and treatment constraints: As the demands of professional
    practice increased, Tom encountered more challenges in his daily practice. He
    felt frustrated that he needed to refer patients who needed complex treatments
    rather than treating them himself because he did not have the time to deal with
    such cases. It’s actually a very frustrating attitude, I really don’t like thinking
    that, ‘Okay this patient isn’t suitable in general practice because I don’t have
    time for it.’ I really would much prefer to have the time to be able to treat
    all my patients. But I guess that’s just the way it’s got to be at the moment.
    (10 weeks into VDT) Tom was also warned about the legal liability of carrying
    out less commonplace treatments offered in general practice and was advised to
    refer such cases to avoid complains. …My supervisor says: ‘Well, you can just
    try it yourself now but when you become an Associate, you might not want to do
    this and the success is so poor and then if it fails, they’ll blame you’. (8 weeks
    into VDT) Time management was another area of struggle for Tom. He faced challenges
    with dealing with the NHS payment system that he was not trained to use during
    his undergraduate training, which added stress to his daily practice as this affected
    the time available for patient consultations. Tom also believed that he was not
    fast enough. Tom’s trainer’s inputs about delivering dental education while doing
    scaling to optimise time management was positively highlighted. My trainer has
    been really helpful, he suggested when you’re doing a scale, you can give oral
    hygiene instruction. (10 weeks into VDT) Similar pieces of advice on time management
    were evident in other logs. My trainer told me that I just need to cut how much
    I talk at the end of the appointment and trying to wrap things up, because I just
    generate even more questions. (Tom/11 weeks into VDT) Theme 6- Internalising professional
    practice norms: As weeks passed, Tom gradually aligned his professional identity
    with the expectations and practices of the general practice. His perspective on
    treatment decisions became increasingly pragmatic, recognising their higher costs
    that patients might struggle to afford. This led him to consider alternative options.
    The better treatments are not as cheap for the patient, simply put. Therefore,
    we offer the pin-retained direct restoration as an alternative, and the patients
    seem happy with that. (13 weeks into VDT) Also… We are taught in Dental School
    that pinned amalgams are the worst thing to do because they can cause fractures.
    But, in reality, patients don’t want that. They just want you to place a filling
    and see how many more years they can get out of the tooth without spending too
    much money. (14 weeks into VDT) As the year progressed, Tom became more comfortable
    with treatment planning that accounted for the financial and time restrains associated
    with the NHS system and patients’ opinions. Dental school taught me how to do
    gold-standard treatment planning. However, now I need to think more realistically
    in terms of what is available on the NHS and via the SDR (Statement of Dental
    Remuneration). Also, I need to consider managing my time in general practice.
    (16 weeks into VDT) Tom’s practice started to settle down with alternative treatments
    like pin-retained amalgam because it was a popular treatment option for his patients,
    especially for those with low expectations or budget constraints. It works best
    in situations where you have a patient come in, and they say, ‘I want something
    done quickly,’ or ‘I don’t want to pay too much,’ or even when they say, ‘Oh,
    I want to keep it for as long as I can. (20 weeks into VDT) He also became more
    comfortable with referring patients who required more complex treatments. I am
    getting into the groove of regular referrals and knowing where to send them now.
    (22 weeks into VDT) After more than six months in VDT, Tom’s practice became a
    routine. However, he didn’t view this routine positively. He expressed disappointment
    in how dentists often conform to avoid criticism or patient complaints, and he
    felt this was due to pressure from external stakeholders. It’s a bit disappointing
    to see how many dentists put themselves in a rut because they all try to do the
    same as each other to avoid criticism, and they also follow instructions to avoid
    appearing incompetent. This situation makes me feel like the health board or the
    GDC has more control over dentists than the dentists themselves. (29 weeks into
    VDT) In one of his final logs, Tom highlighted his frustration with the funding
    system and costs covered by the NHS system. If I were willing to go the extra
    mile under the NHS, then my trainer and my practice would be losing money because
    of the amount that we’d get back from the Statement of Dental Remuneration (SDR).
    It wouldn’t cover my time, and it wouldn’t cover my materials. I just felt really
    frustrated and really let down by the NHS. (31 weeks into VDT) NDGs behaviour
    spending six or more months into practice, findings from interview 2 Theme 7-Current
    use of EPD: 7a) Ambiguity about the concept: Interview 2 presented mixed and varied
    views regarding the use of EBP among NDGs. While some participants actively implemented
    EBP in their routine practice, their understanding of EBP differed. Most associated
    EBP strictly with using dental guidelines, particularly relying on the Scottish
    Dental Clinical Effectiveness Programme (SDCEP) guidelines. Some participants
    believed their practice was evidence-based due to using hi-tech equipment and
    expensive materials that were recommended by colleagues rather than being used
    on the basis of evidence of their clinical benefits. I would say so, for the most
    part. We’re always trying out new materials or instruments that have been recommended
    and giving them a go. NDG 1- Interview 2. When asked if they tried to look for
    evidence to inform their treatment decisions, none replied positively. Instead,
    they relied on alternative sources like peers and VDT trainers, considering them
    reliable and equivalent to finding and appraising evidence themselves. 7b) reluctance
    to use: On the other hand, some participants explicitly showed a lack of interest
    in using EBP or staying updated with the latest evidence. Although they attributed
    this to a number of reasons, the NHS system was, by far, the main topic that was
    discussed and blamed for them not keeping their practice focussed on an evidence-based
    approach. The NHS system was perceived as unfair to both dentists and patients
    with all participants expressing feelings of stress over their perceived ethical
    dilemma of providing NHS treatment versus meets the standards of care. The NHS
    is unfairly telling us, ‘Well you’ve got to provide the one that’s the better
    one. But we’re not going to pay you anything more for it’. NDG 6- Interview 2.
    For patients, EBP was seen as a treatment option exclusive to private dentistry,
    creating a moral dilemma as patients made economic choices compromising their
    healthcare. It’s creating a moral hazard by steering people in that direction
    because patients will make economic choices about their healthcare, and they’ll
    say, ‘Well, the best treatment for me would cost £60, but I don’t really have
    £60, so I’m going to have to go make a compromise of a treatment for £10’. NDG
    4-Interview 2. Another limitation highlighted regarding the NHS payment system
    was bureaucracy. Participants found navigating through the system and process
    of obtaining approvals for treatment planning as time-consuming and difficult.
    If a treatment plan went over a certain amount of money, then I had to request
    prior approval before I could continue the patient’s treatment plan. That generally
    can take quite a long time, and sometimes is quite difficult. NDG 7-Interview
    2. Another reason for reluctance in adopting EBP, was the perceived disconnect
    between high standards of care and the business aspects of dentistry. EBP was
    also often highlighted as a treatment option to be exclusively offered within
    the private dentistry realm. Making speed is a way of compensating for the fact
    that the standards are poor. You either have high standards or high turnover.
    That is how you make a business work. NDG 4-Interview. Also, guidelines were criticised
    for being repetitive and lengthy. Others questioned the concept of EBP itself,
    finding academic papers time-consuming without significant value. Many participants
    emphasised their professional identity as clinicians rather than academics, explaining
    their reluctance to engage in the steps of the EBP process. No, definitely no,
    the others as well. This is not our job. NDG 3-Interview 2. A lack of shared vision
    among stakeholders was highlighted by participants as leading them to feel conflicted
    between what they could offer and what they perceived as gold-standard treatment.
    I think that each party is just not paying attention to the others. The GDC wants
    to ensure that everything is happening in a way that protects the patients. Dental
    schools, on the other hand, focus on teaching us gold standard treatments. But
    once we start practicing, we have to navigate the realities of working within
    the NHS and truly understand what we can and cannot accomplish. NDG 1-Interview
    2. Discussion This study provides valuable insights into the changes in perception
    and adoption of EBP among newly graduated dental professionals, as well as identifying
    the factors that influence their adoption of evidence-based practices. These insights
    may help inform strategies to improve the adoption of evidence-based dentistry
    through teaching methods in dental school curricula and in dental practice. Study
    design and data collection Our study employed a longitudinal design, gathering
    data at various time points. The initial interview occurred upon graduation when
    participants had completed their degree and obtained professional registration
    with the GDC. This allowed us to explore their perceptions of EBP without the
    influence of the general practice environment. The subsequent interviews and diary
    entries were conducted after the participants had spent six to nine months in
    VDT roles within NHS dental practices, offering insights into the evolution of
    their attitudes and behaviours toward EBP in their new professional roles. Two
    data collection methods, semi-structured interviews and LADs, were employed to
    ensure a comprehensive understanding of the phenomenon. While the interviews provided
    focused exploration and opportunities for participants to share their experiences,
    they generated a substantial amount of complex data. The LADs, though initially
    challenging to maintain which was reported in similar studies, [15, 16] became
    a valuable tool for participants to express their feelings and reflect on their
    experiences, particularly during challenging periods. Some participants also reported
    a moderate therapeutic effect of recording diaries, similar to findings from other
    studies using longitudinal diaries [17]. The diaries offered a safe space for
    reflection on negative experiences and self-doubts. However, the therapeutic effect
    of LADs can be a limiting factor as it shifts the purpose of diaries from being
    solely an investigative method to serving as an intervention, potentially influencing
    participants’ experiences and behaviours [18]. Both methods are qualitative in
    nature, introducing a degree of subjectivity into the data [19]. We took measures
    to ensure the quality of analysis, including consulting the literature, expert
    oversight, and internal coding checks. We employed a codebook and reflexive thematic
    analysis, recognising that the findings are primarily interpreted through the
    researcher’s lens but remained open to alternative interpretations. Main findings
    The main findings of this study provide valuable insights into the attitudes and
    behaviours of dental graduates towards EBP, while also highlighting the challenges
    they encounter in implementing it. The results reveal a complex and multifaceted
    picture regarding the incorporation of EBP into the professional work of dental
    graduates. During the initial interview (Interview 1), it became evident that
    dental graduates were not well-prepared to integrate evidence-based practices
    into their work within the general practice environment. The primary obstacle
    was the new graduates’ expectation that the workplace environment would differ
    significantly from dental school in terms of policies, time constraints, and available
    resources. Additionally, an attitudinal barrier surfaced, as many participants
    expressed a lack of belief in the role of evidence-based practice in delivering
    high-quality care. They perceived the use of evidence as either limited to specific
    topics or as an optional feature that could be implemented only under favourable
    circumstances. These findings are in line with the evidence available in the literature
    [20,21,22,23]. It is not possible to know where the negativity towards implementing
    EBP stems from. However, it seems likely that it is a combination of undergraduate
    education, where the new graduates were not fully equipped with the necessary
    skills to implement EBP knowledge or the dental practice environment and culture
    they enter into, as part of VDP, which is not conducive to its implementation.
    The findings from this support assert that they did not feel in a position to
    deliver care to patients according to available evidence and recommendations for
    best treatment approaches, even though they had fulfilled their academic requirements
    and were deemed competent in this regard. For the education side of this, the
    problem may lie in the unobserved dimension of the teaching process. This dimension
    pertains to the behaviour of teaching staff towards EBP, known as the “Hidden
    Curriculum.” [24] The “Hidden Curriculum” suggests that educators inadvertently
    communicate their values and attitudes, which then trickle down the hierarchical
    chain of the teaching process. This communication can occur through subtle cues
    that conveys a certain stance. Research supports the notion that learners are
    more susceptible to being influenced by this implicit side of teaching when they
    lack the confidence to develop their own values and attitudes toward certain issues.
    Consequently, they adopt the attitude of their senior figures. Thus, if students
    witness educators disregarding research evidence in favour of personal opinions
    or anecdotal evidence, they are more likely to adopt a similar approach towards
    EBP. This misalignment in values and attitudes may explain the mixed views on
    EBP held by dental graduates in this study, despite receiving formal education
    on the subject during their dental training. While the concept of the hidden curriculum
    has been extensively investigated in other medical disciplines, [25, 26] there
    remains a scarcity of evidence related to the dental context, highlighting the
    need for further investigation. Notably, the influence of this concept may also
    extend to the attitudes of VDT trainers, adding another layer to the complexities
    surrounding EBP in dental education and practice. In addition to the attitudinal
    barriers, the environment of general dental practice presented significant challenges
    to the adoption of EBP. The fast-paced nature of dental practice, coupled with
    perceived financial constraints, seemed to create additional hurdles. The participants
    highlighted the NHS system as a consistently frustrating element in the decision-making
    process. The rigidity of the system and its policies negatively impacted the ability
    of dental graduates to implement what they considered to be best for their patients,
    creating ethical dilemmas for them. The findings of the study also revealed concerns
    regarding legal issues, leading to excessive referrals and fostering a culture
    of “defensive dentistry.” These challenges were not only stressful for the participants
    of this study but for the GDPs in general practice, as evidenced in previous studies
    [27, 28]. The diary logs provided further insight into how participants adapted
    their practice to conform to NHS dentistry. For example, Tom expressed increased
    anxiety about time management and the financial situation. Hence, he adapted his
    practice to align with the requirements of NHS dentistry. These adaptations are
    influenced by the guidance and support of their VDT trainers. The participant
    perceived this support positively as it prepared them for the real-world dental
    profession. These views are based on the assumption that EBP is only associated
    with sophisticated, expensive and time-consuming approaches, which is not necessarily
    true. Recent evidence supports minimally invasive dentistry and the use of simple,
    cost-effective strategies to manage dental caries (e.g. non-restorative cavity
    control) [29,30,31]. A pilot study was conducted in North Ireland to assess changes
    in GDPs’ clinical activity when the NHS remuneration system was modified to incentivise
    prevention treatment planning (along with other positive outcomes) [32]. It found
    a general reduction in clinical activity including prevention procedures, and
    although the changes in the payment system were beneficial in various ways, GDPs’
    treatment choices quickly returned to the baseline once the incentives were stopped.
    There are also examples from developed countries where dental care is chiefly
    offered through private channels and there is still emerging evidence from those
    countries reporting issues regarding dentists’ behaviour and attitude towards
    EBP [33,34,35]. These findings challenge the prevailing notion of the participants
    views, that more time or better financing would lead to an increase in adoption
    of EBP. Strategies to enhance EBP adoptions Importance is placed on EBP by the
    General Dental Council, the Professional Statutory Regulatory Body who oversee
    dentistry, undergraduate curricula content and delivery by Dental Schools. Our
    study highlights the complexities of the challenges in EBP adoption among dental
    graduates, prompting the need actionable strategies for improvement. These strategies
    could encompass faculty training, which is a critical step in integrating EBP
    principles into curricula. Training for VDT trainers is equally vital, as they
    significantly influence the professional identity of new graduates during their
    early years in practice. To achieve more ambitious goals, establishing an EBP-friendly
    environment is vital. This involves ensuring convenient access to evidence-based
    resources, including online libraries and databases, and user-friendly platforms.
    Introducing incentives and rewards for actively implementing EBP in clinical practice
    can serve as a catalyst for widespread adoption [36]. Empowering patients with
    information is a crucial step to enhance EBP adoption. By fostering a transparent
    and open dialogue during consultations, dentists can provide clear insights into
    various treatment options, associated risks, and expected outcomes. Actively encouraging
    patients to ask questions and involving them in decision-making not only builds
    trust but also promotes a culture of shared decision-making rooted in evidence
    [37]. These initiatives necessitate substantial changes in the dental practice
    landscape, requiring collective efforts amongst regulatory bodies, dental associations,
    policymakers, and educators to collectively facilitate EPD implementation. Study
    limitations, mitigation strategies, and transferability While this study provides
    valuable insights, it is essential to address its limitations and propose strategies
    to mitigate them. The focus on dental graduates from a single institution during
    their VDT phase may raise concerns about the generalisability of the findings.
    Variations in curricula and teaching approaches across dental schools in the UK
    can also influence dental graduates’ attitudes and behaviours toward EBP. To address
    these limitations, future research should involve a more diverse sample of dental
    schools, investigating variations in EBP education and its impact on graduates’
    practices. Another limitation of this study is the relatively short-term follow-up
    of the new dental graduates on their EBP journey. To address this limitation,
    future research should consider conducting longitudinal studies that follow graduates
    over an extended period. Such studies will assess the long-lasting impact of EBP
    teaching and training on dentists’ routine practice. Both used of the data collection
    methods used are qualitative in nature, introducing a degree of subjectivity into
    the data [34]. We took measures to ensure the quality of analysis, including consulting
    the literature, expert oversight, and internal coding checks and triangulation
    with the the quantitative data collected as part of the project [11]. We employed
    a codebook and reflexive thematic analysis, recognising that the findings are
    primarily interpreted through the researcher’s lens but remained open to alternative
    interpretations. This research is transferable to other contexts as researchers
    and educators can draw upon the evidence presented here by adapting our strategies
    and recommendations. For instance, this study highlighted the importance of faculty
    training in EBP which is likely to be relevant to multiple contexts. extend it
    to their specific educational settings. This involves equipping educators with
    the necessary knowledge and skills to intentionally teach and communicate EBP
    principles. Moreover, training should be extended to immediate educators, such
    as VDT trainers, who play a critical role in shaping the professional identity
    of new graduates. Conclusion This study sheds light on the gap between the expected
    incorporation of EBP values and the actual professional behaviour of new dental
    graduates. It challenges the belief that undergraduate education alone can guarantee
    the active use and application of EBP by new dental graduates. Various factors
    impede the process of EBP implementation. The findings suggest that time and financial
    constraints within the NHS system, coupled with the views of the trainer and other
    staff and misconceptions about EBP, serve as barriers to its explicit use in dental
    practice settings. To address these challenges, further investigation is required
    to uncover the hidden aspects of dental education and training that contribute
    to attitudinal obstacles. By gaining a comprehensive understanding of these factors,
    effective strategies can be developed to cultivate lifelong learning and foster
    the integration of EBP among dental graduates. Data availability The datasets
    generated during the current study are available upon reasonable request from
    the corresponding author. References Ali K, Tredwin C, Kay E, Slade A. Transition
    of new dental graduates into practice: a qualitative study. Eur J Dent Educ. 2016;20(2):65–72.
    Article   CAS   PubMed   Google Scholar   Gordon L, Jindal-Snape D, Morrison J,
    Muldoon J, Needham G, Siebert S, et al. Multiple and multidimensional transitions
    from trainee to trained doctor: a qualitative longitudinal study in the UK. BMJ
    Open. 2017;7(11):e018583. Article   PubMed   PubMed Central   Google Scholar   UK
    Committee of Postgraduate Dental Deans and Directors. Dental Foundation Training
    Curriculum (COPDEND). 2015 [Available from: https://www.copdend.org/postgraduate-training/dental-foundation-training/872-2/.
    Kishore M, Panat SR, Aggarwal A, Agarwal N, Upadhyay N, Alok A. Evidence based
    dental care: integrating clinical expertise with systematic research. J Clin Diagn
    Res. 2014;8(2):259–62. PubMed   PubMed Central   Google Scholar   Sackett DL,
    Rosenberg WMC, Gray JAM, Haynes RB, Richardson WS. Evidence based medicine: what
    it is and what it isn’t. BMJ. 1996;312(7023):71–2. Article   CAS   PubMed   PubMed
    Central   Google Scholar   General Dental Council. Preparing for practice: Dental
    team learning outcomes for registration. London; 2015. Hong B, Plugge E. Critical
    appraisal skills teaching in UK dental schools. Br Dent J. 2017;222(3):209–13.
    Article   CAS   PubMed   Google Scholar   Iqbal A, Glenny AM. General dental practitioners’
    knowledge of and attitudes towards evidence based practice. Br Dent J. 2002;193(10):587–91.
    discussion 3. Article   CAS   PubMed   Google Scholar   Farook SA, Davis AK, Khawaja
    N, Sheikh AM. NICE guideline and current practice of antibiotic prophylaxis for
    high risk cardiac patients (HRCP) among dental trainers and trainees in the United
    Kingdom (UK). Br Dent J. 2012;213(4):E6. Article   CAS   PubMed   Google Scholar   General
    Dental Council. Preparing for Practice: Dental team learning outcomes for registration
    London2015 [Available from: https://www.gdc-uk.org/docs/default-source/quality-assurance/preparing-for-practice-(revised-2015).pdf?sfvrsn=81d58c49_2.
    Al-Yaseen W, Nanjappa S, Jindal-Snape D, Innes N. A longitudinal study of changes
    in new dental graduates’ engagement with evidence-based practice during their
    transition to professional practice. Br Dent J. 2022. Al-Yaseen W, Nanjappa S,
    Jindal-Snape D, Innes N. A longitudinal qualitative multi-methods study of new
    dental graduates’ transition journey from undergraduate studies to professional
    practice. Eur J Dent Educ. 2023. Braun V, Clarke V, Hayfield N, Terry G. Thematic
    analysis. In: Liamputtong P, editor. Handbook of Research Methods in Health Social
    Sciences. Springer Singapore; 2019. pp. 843–60. QSR International Pty Ltd. NVivo
    (Version 12). 2018. p. https://www.qsrinternational.com/nvivo-qualitative-data-analysis-software/home.
    Creswell J. Research Design: Qualitative, Quantitative, and Mixed-Method Approaches.
    2009. Gibson BE, Mistry B, Smith B, Yoshida KK, Abbott D, Lindsay S, et al. The
    Integrated Use of Audio Diaries, Photography, and Interviews in Research with
    Disabled Young men. Int J Qualitative Methods. 2013;12:382–402. Article   Google
    Scholar   Hackett J, Godfrey M, Bennett MI. Patient and caregiver perspectives
    on managing pain in advanced cancer: a qualitative longitudinal study. Palliat
    Med. 2016;30(8):711–9. Article   PubMed   Google Scholar   Teece A, Baker J. Thematic
    analysis: how do patient diaries affect survivors’ psychological recovery? Intensive
    Crit Care Nurs. 2017;41:50–6. Article   PubMed   Google Scholar   Pannucci CJ,
    Wilkins EG. Identifying and avoiding bias in research. Plast Reconstr Surg. 2010;126(2):619.
    Article   CAS   PubMed   PubMed Central   Google Scholar   Feres MFN, Albuini
    ML, de Araújo Castro Santos RP, de Almeida-Junior LA, Flores-Mir C, Roscoe MG.
    Dentists’ awareness and knowledge of evidence- based dentistry principles, methods
    and practices: a systematic review. Evid-Based Dent. 2022. Sin M, Butt S, Barber
    SK. Assessing dentist and dental student knowledge of and attitudes towards shared
    decision-making in the United Kingdom. Eur J Dent Educ. 2021;25(4):768–77. Article   PubMed   Google
    Scholar   Wong G, Print M, Gerzina T. Understanding the impact of an evidence-based
    practice curriculum on oral health graduates. 2019;20:55. Santiago V, Cardenas
    M, Charles A, Hernandez E, Oyoyo U, Kwon SR. Evidence-based practice knowledge,
    attitude, Access and confidence: a comparison of dental hygiene and dental students.
    J Dent Hygiene: JDH. 2018;92:31–7. Google Scholar   Brooks KC. A piece of my mind.
    A silent curriculum. JAMA. 2015;313(19):1909–10. Article   PubMed   Google Scholar   Rothlind
    E, Fors U, Salminen H, Wändell P, Ekblad S. The informal curriculum of family
    medicine– what does it entail and how is it taught to residents? A systematic
    review. BMC Fam Pract. 2020;21(1):49. Article   PubMed   PubMed Central   Google
    Scholar   Hafferty FW. Beyond curriculum reform: confronting medicine’s hidden
    curriculum. Acad Med. 1998;73(4):403–7. Article   CAS   PubMed   Google Scholar   Holmes
    RD, Steele JG, Donaldson C, Exley C. Learning from contract change in primary
    care dentistry: a qualitative study of stakeholders in the north of England. Health
    Policy. 2015;119(9):1218–25. Article   PubMed   PubMed Central   Google Scholar   Rossi
    M. NHS dentistry: UDA disaster. Br Dent J. 2016;221(6):279. Article   CAS   PubMed   Google
    Scholar   Schwendicke F, Krois J, Robertson M, Splieth C, Santamaria R, Innes
    N. Cost-effectiveness of the Hall technique in a Randomized Trial. J Dent Res.
    2019;98(1):61–7. Article   CAS   PubMed   Google Scholar   Tellez M, Gray SL,
    Gray S, Lim S, Ismail AI. Sealants and dental caries: dentists’ perspectives on
    evidence-based recommendations. J Am Dent Association. 2011;142(9):1033–40. Article   Google
    Scholar   Govindaiah S, Bhoopathi V. Dentists’ levels of evidence-based clinical
    knowledge and attitudes about using pit-and-fissure sealants. J Am Dent Association.
    2014;145(8):849–55. Article   Google Scholar   Brocklehurst P, Tickle M, Birch
    S, McDonald R, Walsh T, Goodwin TL, Health Services and Delivery Research. Impact
    of changing provider remuneration on NHS general dental practitioner services
    in Northern Ireland: a mixed-methods study. Southampton (UK): NIHR Journals Library
    Copyright © Queen’s Printer and Controller of HMSO 2020. This work was produced
    by Brocklehurst under the terms of a commissioning contract issued by the Secretary
    of State for Health and Social Care. This issue may be freely reproduced for the
    purposes of private research and study and extracts (or indeed, the full report)
    may be included in professional journals provided that suitable acknowledgement
    is made and the reproduction is not associated with any form of advertising. Applications
    for commercial reproduction should be addressed to: NIHR Journals Library, National
    Institute for Health Research, Evaluation, Trials and Studies Coordinating Centre,
    Alpha House, University of Southampton Science Park, Southampton SO16 7NS, UK.;
    2020. O’Donnell JA, Modesto A, Oakley M, Polk DE, Valappil B, Spallek H. Sealants
    and dental caries: insight into dentists’ behaviors regarding implementation of
    clinical practice recommendations. J Am Dent Assoc. 2013;144(4):e24–30. Article   PubMed   PubMed
    Central   Google Scholar   San Martin L, Castaño A, Bravo M, Tavares M, Niederman
    R, Ogunbodede EO. Dental sealant knowledge, opinion, values and practice of Spanish
    dentists. BMC Oral Health. 2013;13:1–8. Article   Google Scholar   Spallek H,
    Song M, Polk DE, Bekhuis T, Frantsve-Hawley J, Aravamudhan K. Barriers to implementing
    evidence-based clinical guidelines: a survey of early adopters. J Evid Based Dent
    Pract. 2010;10(4):195–206. Article   PubMed   PubMed Central   Google Scholar   Kueny
    A, Shever LL, Lehan Mackin M, Titler MG. Facilitating the implementation of evidence-
    based practice through contextual support and nursing leadership. J Healthc Leadersh.
    2015;7:29–39. Article   PubMed   PubMed Central   Google Scholar   Engle RL, Mohr
    DC, Holmes SK, Seibert MN, Afable M, Leyson J, et al. Evidence-based practice
    and patient-centered care: doing both well. Health Care Manage Rev. 2021;46(3):174–84.
    Article   PubMed   Google Scholar   Download references Acknowledgements We extend
    our gratitude to the new dental graduates for generously contributing their time
    and valuable insights, which greatly informed our research findings. Funding No
    external funding was sought for this project. Author information Authors and Affiliations
    School of Dentistry, College of Biomedical & Life Sciences, Cardiff University,
    Heath Park, CF14 4XY, Cardiff, UK Waraf Al-Yaseen & Nicola Innes School of Dentistry,
    University of Dundee, Park Place, DD1 4HR, Dundee, UK Sucharita Nanjappa School
    of Humanities, Social Sciences and Law, University of Dundee, Old Medical School,
    DD1 4HN, Dundee, UK Divya Jindal-Snape Contributions Waraf Al-Yaseen led data
    collection, primary analysis, and manuscript drafting. Sucharita Nanjappa, Divya
    Jindal-Snape, and Nicola Innes collectively contributed to research design, result
    analysis, interpretation, and manuscript composition. Nicola Innes additionally
    oversaw the study’s planning and execution, while Waraf Al-Yaseen provided data-driven
    insights during the drafting process. Corresponding author Correspondence to Waraf
    Al-Yaseen. Ethics declarations Ethics approval and consent to participate Written
    consent was obtained from all study participants (Appendix). Consent for publication
    All participants have granted permission to include their direct inputs, including
    direct quotations, in the publication when applicable. Consent for publication
    Not applicable. Competing interests Authors declare no conflicts of interest.
    Additional information Publisher’s Note Springer Nature remains neutral with regard
    to jurisdictional claims in published maps and institutional affiliations. Electronic
    supplementary material Below is the link to the electronic supplementary material.
    Supplementary Material 1 Rights and permissions Open Access This article is licensed
    under a Creative Commons Attribution 4.0 International License, which permits
    use, sharing, adaptation, distribution and reproduction in any medium or format,
    as long as you give appropriate credit to the original author(s) and the source,
    provide a link to the Creative Commons licence, and indicate if changes were made.
    The images or other third party material in this article are included in the article’s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article’s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/)
    applies to the data made available in this article, unless otherwise stated in
    a credit line to the data. Reprints and permissions About this article Cite this
    article Al-Yaseen, W., Nanjappa, S., Jindal-Snape, D. et al. New dental graduates
    transition into UK professional practice; a longitudinal study of changes in perceptions
    and behaviours through the lens of evidence-based dentistry. BMC Med Educ 24,
    195 (2024). https://doi.org/10.1186/s12909-024-05182-y Download citation Received
    06 August 2023 Accepted 15 February 2024 Published 26 February 2024 DOI https://doi.org/10.1186/s12909-024-05182-y
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Transitions New dental graduates Evidence-based practice University
    programme Guidelines Dentists Education Download PDF Sections Figures References
    Abstract Introduction Method Results Discussion Conclusion Data availability References
    Acknowledgements Funding Author information Ethics declarations Additional information
    Electronic supplementary material Rights and permissions About this article Advertisement
    BMC Medical Education ISSN: 1472-6920 Contact us Submission enquiries: bmcmedicaleducation@biomedcentral.com
    General enquiries: ORSupport@springernature.com Read more on our blogs Receive
    BMC newsletters Manage article alerts Language editing for authors Scientific
    editing for authors Policies Accessibility Press center Support and Contact Leave
    feedback Careers Follow BMC By using this website, you agree to our Terms and
    Conditions, Your US state privacy rights, Privacy statement and Cookies policy.
    Your privacy choices/Manage cookies we use in the preference centre. © 2024 BioMed
    Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: BMC Medical Education
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: New dental graduates transition into UK professional practice; a longitudinal
    study of changes in perceptions and behaviours through the lens of evidence-based
    dentistry
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Sun Y.
  citation_count: '0'
  description: This paper investigates the integration of relay-assisted Internet
    of Things (IoT) systems, focusing on the use of multiple relays to enhance the
    system performance. The central metric of interest in this study is system outage
    probability, evaluated in terms of latency. Our research provides a comprehensive
    analysis of system outage probability, considering different relay selection criteria
    to optimize the system’s transmission performance. Three relay selection strategies
    are employed to enhance the system transmission performance. Specifically, the
    first strategy, optimal relay selection, aims to identify the relay that minimizes
    the latency and maximizes the data transmission reliability. The second approach,
    partial relay selection, focuses on selecting a subset of relays strategically
    to balance the system resources and achieve the latency reduction. The third strategy,
    random relay selection, explores the potential of opportunistic relay selection
    without prior knowledge. Through a rigorous investigation, our paper evaluates
    the impact of these relay selection criteria on the performance of relay-assisted
    edge computing systems. By assessing the system outage probability in relation
    to latency, we provide valuable insights into the trade-offs and advantages associated
    with each selection strategy. Our findings contribute to the design and optimization
    of reliable and efficient edge computing systems, with implications for various
    applications, including the IoT and intelligent data processing.
  doi: 10.1186/s13634-024-01123-5
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Get published
    Explore Journals Books About Login EURASIP Journal on Advances in Signal Processing
    About Articles Submission Guidelines Submit manuscript Research Open access Published:
    24 February 2024 Distributed transmission and optimization of relay-assisted space-air-ground
    IoT systems Ying Sun   EURASIP Journal on Advances in Signal Processing  2024,
    Article number: 27 (2024) Cite this article 506 Accesses Metrics Abstract This
    paper investigates the integration of relay-assisted Internet of Things (IoT)
    systems, focusing on the use of multiple relays to enhance the system performance.
    The central metric of interest in this study is system outage probability, evaluated
    in terms of latency. Our research provides a comprehensive analysis of system
    outage probability, considering different relay selection criteria to optimize
    the system’s transmission performance. Three relay selection strategies are employed
    to enhance the system transmission performance. Specifically, the first strategy,
    optimal relay selection, aims to identify the relay that minimizes the latency
    and maximizes the data transmission reliability. The second approach, partial
    relay selection, focuses on selecting a subset of relays strategically to balance
    the system resources and achieve the latency reduction. The third strategy, random
    relay selection, explores the potential of opportunistic relay selection without
    prior knowledge. Through a rigorous investigation, our paper evaluates the impact
    of these relay selection criteria on the performance of relay-assisted edge computing
    systems. By assessing the system outage probability in relation to latency, we
    provide valuable insights into the trade-offs and advantages associated with each
    selection strategy. Our findings contribute to the design and optimization of
    reliable and efficient edge computing systems, with implications for various applications,
    including the IoT and intelligent data processing. 1 Introduction The trajectory
    of information technology development, particularly within Internet of Things
    (IoT) networks, has been marked by significant progress [1,2,3,4]. From the early
    days with limited coverage and low data rates leading to concerns about transmission
    outage probability, to the subsequent emergence of cellular IoT and low-power
    wide-area network (LPWAN) technologies that offered improved reliability and power
    efficiency, and finally, the advent of fifth-generation (5 G), which introduced
    high data rates, ultra-low latency, and minimized transmission outage probability,
    IoT networks have evolved significantly [5,6,7,8]. These advances have enabled
    a wide range of applications, with varying data rate requirements and stringent
    reliability demands, across industries, heralding a transformative era of real-time
    connectivity and operational efficiency while continually enhancing key performance
    metrics such as data rate and symbol error rate (SER) [2, 9,10,11]. Relaying is
    an effective technique employed in IoT networks to bolster wireless transmission
    performance [12,13,14,15]. It has a direct impact on key metrics such as outage
    probability, data rate, and SER. Relaying involves the use of intermediate devices
    or relay nodes to assist in the transmission of data between the source and destination
    [16,17,18,19]. By strategically positioning relays, it can extend communication
    range and mitigate the likelihood of transmission failures, thereby reducing outage
    probability [20,21,22,23]. Additionally, relaying can enhance data rates by amplifying
    signals and facilitating efficient data transmission over extended distances.
    However, the usage of relays may introduce latency, which could potentially affect
    data rates, and the quality of relay placement can influence SER either positively
    by improving signal quality or negatively if noise is introduced. Consequently,
    judicious relay deployment and signal amplification strategies are crucial to
    optimize the performance of wireless transmission in IoT networks. Edge computing
    is another critical technique in IoT networks aimed at expediting computing tasks
    by decentralizing data processing and analysis closer to the data source or the
    network edge, effectively minimizing the system latency, energy consumption, and
    enhancing overall performance metrics [1]. By moving computation closer to IoT
    devices, edge computing reduces the round-trip time for data to travel to centralized
    cloud servers, significantly decreasing the system latency, and enhancing real-time
    processing capabilities. This approach, in turn, leads to a lower energy consumption
    as data transmission is minimized [24,25,26,27]. In further, it helps mitigate
    outage probability by ensuring that even if the central cloud server experiences
    downtime, essential computing tasks can continue at the edge. While edge computing
    often involves lightweight data processing, it can also optimize the data rate
    and SER by intelligently filtering, aggregating, or compressing data at the edge,
    reducing the volume of data transmitted and enhancing the accuracy of communication
    while conserving network resources. Consequently, edge computing plays a pivotal
    role in advancing the efficiency and reliability of IoT networks. Motivated by
    the above literature review, this paper studies the integration of relay-assisted
    edge computing systems, with a specific emphasis on leveraging multiple relays
    to enhance the system performance. The central focal point of this investigation
    is the system outage probability, assessed within the context of latency. Our
    research undertakes a thorough and encompassing examination of the system outage
    probability, encompassing of three relay selection criteria. Specifically, the
    first strategy, known as the optimal relay selection, strives to pinpoint the
    relay that minimizes the latency and maximizes the data transmission reliability.
    In contrast, the second approach, referred to as partial relay selection, concentrates
    on a judicious selection of relays to harmonize system resources and achieve latency
    reduction. The third strategy, denoted as random relay selection, explores the
    potential of a serendipitous relay selection approach, devoid of prior knowledge.
    Through a comprehensive analysis, this paper scrutinizes the repercussions of
    these relay selection strategies on the performance of relay-assisted edge computing
    systems. By evaluating the system outage probability in the context of latency,
    it offers invaluable insights into the trade-offs and advantages inherent in each
    selection strategy, ultimately contributing to the refinement and optimization
    of dependable and efficient edge computing systems, with far-reaching implications
    across applications such as the IoT and real-time data processing. 2 System and
    computing models 2.1 System model Figure 1 shows the system model of an edge computing
    system from source node S to destination node D, assisted by N decode-and-forward
    (DF) relays denoted as . In this system model, data are transmitted from the source
    node S to the destination node D via a set of N relay nodes, each labeled as .
    These relay nodes serve as intermediaries to facilitate the data transmission.
    The data transmission process includes various parameters such as transmission
    latency, computation latency, task size, CPU-cycle frequencies, the required CPU
    cycles for computation, wireless bandwidth, transmit power, additive white Gaussian
    noise (AWGN), and wireless channel parameters. Fig. 1 System model of relay-assisted
    edge computing systems Full size image The task processing latency for each relay
    , denoted as , is the sum of the transmission latency ( ) and computation latency
    ( ). Transmission latency is influenced by wireless parameters like channel conditions,
    bandwidth, and transmit power, while computation latency is determined by CPU-cycle
    frequencies and the complexity of the computation tasks. The task size, denoted
    as L, represents the amount of data to be processed. The processing latency must
    meet a predefined threshold, denoted as , for successful operation. If the processing
    latency exceeds this threshold, it leads to an outage scenario. The outage probability
    (OP) is defined as the probability of the processing latency exceeding the threshold
    . This probability is affected by various factors, including the transmission
    parameters and the wireless channel characteristics, which follow specific distributions
    such as Rayleigh fading. The system model considers the use of multiple relay
    nodes to enhance the data transmission and computation, optimizing performance
    while ensuring that the processing latency remains within acceptable limits. 2.2
    Communication and computing process The task processing latency through relay
    is written as (1) (2) where L is the task size, is the CPU-cycle frequencies of
    D, and indicates the required CPU cycles to compute each bit. Moreover, W is the
    wireless bandwidth, P is the transmit power, denotes variance of the additive
    white Gaussian noise (AWGN), is the wireless channel parameter from S to relay
    , and is the wireless channel parameter from relay to D. In practice, the processing
    latency needs to be within a threshold , given by (3) Once the processing latency
    exceeds a given threshold , the system should be in outage, and the outage probability
    (OP) is (4) (5) (6) We can further write as, (7) (8) where is applied, and therefore,
    the lower bound on the outage probability is (9) (10) As Rayleigh fading environments
    are considered, the channel gain follows the distribution as (11) (12) The CDFs
    of and are (13) (14) When and , we can obtain the asymptotic expression of (13)
    and (14) as (15) (16) 3 Performance analysis 3.1 Outage analysis for optimal relay
    selection strategy Optimal relay selection in a network with N relays is a technique
    used to maximize the data transmission performance by systematically choosing
    the relay that provides the best channel conditions based on specific criteria
    such as signal-to-noise ratio or path loss. This selection process reduces the
    likelihood of transmission failures, enhancing reliability and signal quality.
    Optimal relay selection can be dynamic or static, adaptable to changing channel
    conditions, and its complexity varies with the number of relays and measurement
    requirements. It is a powerful strategy when the performance is critical, as it
    minimizes the outage probabilities by selecting the most advantageous relay, given
    by (17) According to (9) and (17), the analytical solution of the outage probability
    is (18) (19) We can further write as, (20) (21) In further, is derived as, (22)
    (23) (24) According to (15) and (16), we can obtain the asymptotic solution as
    (25) (26) 3.2 Outage analysis for partial relay selection strategy Partial relay
    selection in relaying networks with N relays is a strategy that strikes a balance
    between the performance optimization and implementation simplicity. Instead of
    selecting a single relay as in the case of optimal relay selection, partial relay
    selection involves choosing a subset of the available relays for each transmission.
    The selection can be based on criteria like signal strength, channel quality,
    or distance, with the aim of improving reliability without the computational complexity
    associated with optimal selection. By utilizing only a subset of relays, partial
    relay selection can enhance the diversity and reduce outage probability, making
    it a practical compromise when the number of relays is large or when real-time
    decision-making requirements are stringent. This approach simplifies implementation
    while still offering improved performance compared to random relay selection,
    given by (27) According to (9) and (27), the analytical solution of the outage
    probability is (28) (29) We can further write as, (30) (31) In further, is derived
    as, (32) (33) can be further derived as, (34) (35) According to (15) and (16),
    we can obtain the asymptotic solution as (36) (37) (38) 3.3 Outage analysis for
    random relay selection strategy Random relay selection in relaying networks with
    N relays is a straightforward but less sophisticated approach where relays are
    chosen without considering their channel conditions or specific criteria. In this
    strategy, the selection of a relay is entirely based on chance, which can be achieved
    using methods like lottery or a random number generator. While simple to implement,
    random relay selection lacks the ability to optimize the performance by considering
    channel quality, leading to less predictable and generally lower overall system
    performance, as it cannot adapt to changing channel conditions. It is often used
    in scenarios where computational resources and decision-making complexity are
    limited and where the performance trade-off is acceptable, making it a cost-effective
    choice in relatively stable communication environments. According to (9), we can
    obtain the analytical solution of the outage probability as (39) (40) We can further
    write as, (41) (42) (43) According to (15) and (16), we can obtain the asymptotic
    solution as (44) (45) Note that the trade-off among optimal relay selection, partial
    relay selection, and random relay selection in cooperative communication systems
    revolves around balancing outage probability performance and implementation complexity.
    Optimal relay selection, while offering the best outage probability performance,
    is complex and computationally intensive, making it suitable for scenarios where
    performance is paramount. Partial relay selection strikes a balance between the
    performance and complexity, involving less computation but still requiring some
    decision-making. In contrast, random relay selection is the simplest to implement
    but provides the least favorable performance due to its lack of channel quality
    consideration. The choice of relay selection depends on the specific system requirements,
    available resources, and the trade-offs that best align with the application’s
    objectives. 4 Simulation results and discussions In this part, we provide some
    numerical results to illustrate the impact of network parameters on the system
    performance for three relay selection strategies. If not specified, we set the
    number of relays to five. In addition, we set W, MHz, , , and . Moreover, the
    task size L is set to 3Mbits, computational capability is 1GHz, , and the latency
    threshold s. Figure 2 and Table 1 present the outage probability versus P for
    three strategies, where P varies within the range of 1W to 5W. Observing Fig.
    2 and Table 1, we find that the asymptotic results of three strategies become
    convergent to the analytical ones as P increases. This convergence is attributed
    to the fact that the increasing P aligns the asymptotic solution more closely
    with the analytical solution, which thereby verifies the derivation of the analytical
    and asymptotic expressions of the OP. Moreover, it becomes evident that the OP
    of the three relay selection strategies experiences a decline as P increases.
    This is because that the increasing P results in a high transmit signal-to-noise
    ratio (SNR), subsequently reducing the transmit latency and consequentially reducing
    the OP. In further, it is noteworthy that the OP of the optimal strategy exhibits
    a superiority over that of the other strategies. Specifically, when W, the OP
    of the optimal method reaches 0, which is 100% lower than that of the other methods.
    This disparity accentuates the effectiveness and superiority of the optimal strategy.
    Table 1 Numerical outage probability versus P for three strategies Full size table
    Fig. 2 Outage probability versus P for three strategies Full size image Figure
    3 and Table 2 illustrate the impact of W on the outage probability for three strategies,
    where W changes from 3MHz to 7MHz. We can find from the figure and Table that
    the OP of the asymptotic result gradually converges to that of the analytical
    result when W increases, which verifies the effectiveness of the derived analytical
    and asymptotic expressions for all strategies. Moreover, it is observed that the
    OP associated with the three relay selection strategies exhibits a decreasing
    trend as W increases. The reason is that the increasing W results in a high transmit
    SNR, thereby reducing the OP. In further, the result in the figure shows that
    the performance of the optimal strategy is better than those of the other strategies.
    Specifically, when MHz, the OP of the optimal method reaches 0, which is 100%
    lower than that of the other methods. This verifies the superiority of the optimal
    strategy. Table 2 Numerical impact of W on the outage probability of the three
    strategies Full size table Fig. 3 Impact of W on the outage probability of the
    three strategies Full size image Figure 4 and Table 3 depict the influence of
    on the outage probability for three strategies, where varies from 0.1s to 0.5s.
    Observing from the figure and table, we can find that the asymptotic result gradually
    converges to that of the analytical one in the high region. This convergence substantiates
    the effectiveness of the derived analytical and asymptotic solutions. Moreover,
    it is evident that the OP of the three relay selection strategies decreases as
    increases, which is attributed to the fact that a larger value of signifies a
    greater permissible latency within the system, consequently leading to a lower
    OP. In further, the OP of the optimal strategy is always lower than that of the
    other strategies. Specifically, when s, the OP of the optimal method reaches 0,
    which is 100% lower than that of the other methods. This attests the superiority
    of the optimal strategy. Table 3 Numerical influence of on the outage probability
    of the three strategies Full size table Fig. 4 Influence of on the outage probability
    of the three strategies Full size image Figure 5 and Table 4 illustrate the impact
    of on the outage probability for three strategies, where varies from 0.2GHz to
    1GHz. From this figure and table, we can find that the asymptotic solution is
    close to the analytical one, which validates the effectiveness of the derived
    analytical and asymptotic expressions of the outage probability for three strategies.
    Moreover, we find that the OP of the three relay selection strategies decreases
    with the increasing . This is because that a larger results in a lower computational
    latency, thereby reducing the OP. In further, the optimal strategy is always superior
    to the other strategies. Specifically, when GHz, the optimal method is 98.96%
    superior to the other methods. This attests the superiority of the optimal strategy.
    Table 4 Numerical impact of on the outage probability of the three strategies
    Full size table Fig. 5 Impact of on the outage probability of the three strategies
    Full size image Figure 6 and Table 5 depict the impact of L on the outage probability
    for three strategies, where L varies from 1Mbits to 5Mbits. As observed from this
    figure and table, we can find that the asymptotic result becomes convergent to
    the exact one in the low region of L, which validates the effectiveness of the
    derived analytical and asymptotic expressions of the outage probability for all
    strategies. Moreover, we find that the OP of the three relay selection strategies
    increases with the increasing L. This is because that a larger L results in a
    higher computational latency, thereby increasing the OP. In further, the performance
    of the optimal strategy is always better than those of the other strategies. Specifically,
    when Mbits, the OP of the optimal method reaches 0, which is 100% better than
    that of the other methods. This validates the superiority of the optimal strategy.
    Table 5 Data for Fig. 6 Full size table Fig. 6 The impact of L on the outage probability
    for three strategies Full size image 5 Conclusions In conclusion, the investigation
    into relay-assisted edge computing systems was completed in this work. The use
    of multiple relays was found to be helpful in enhancing the system performance,
    particularly in reducing the system outage probability and latency. The comprehensive
    analysis of system outage probability, involving various relay selection criteria,
    was undertaken to optimize the system transmission performance. The three relay
    selection strategies, including optimal relay selection, partial relay selection,
    and random relay selection, were employed, revealing their respective impacts
    on enhancing the system transmission. The results demonstrated that optimal relay
    selection excelled in minimizing the latency and maximizing the data transmission
    reliability. In contrast, partial relay selection strategically balanced resources
    and reduced latency, while random relay selection explored opportunistic relay
    selection without prior knowledge. These findings have contributed to the design
    and optimization of reliable and efficient edge computing systems, with broad
    implications for applications, including the IoT and real-time data processing.
    Availability of data and materials The data for this study can be acquired by
    emailing the authors. Abbreviations IoT: Internet of Things LPWAN: Low-power wide-area
    network SER: Symbol error rate DF: Decode-and-forward AWGN: Additive white Gaussian
    noise OP: Outage probability SNR: Signal-to-noise ratio References Wu, Y., Tang,
    S., Zhang, L.: Resilient machine learning based semantic-aware MEC networks for
    sustainable next-G consumer electronics. IEEE Trans. Consumer Electron. PP(99),
    1–10 (2023) Z. Na, Y. Liu, J. Shi, C. Liu, Z. Gao, UAV-supported clustered NOMA
    for 6G-enabled internet of things: trajectory planning and resource allocation.
    IEEE Internet Things J. 8(20), 15041–15048 (2021) Article   Google Scholar   W.
    Xu, Z. Yang, D.W.K. Ng, M. Levorato, Y.C. Eldar, M. Debbah, Edge learning for
    B5G networks with distributed signal processing: semantic communication, edge
    computing, and wireless sensing. IEEE J. Sel. Top. Signal Process. 17(1), 9–39
    (2023) Article   ADS   Google Scholar   X. Liu, C. Sun, M. Zhou, C. Wu, B. Peng,
    P. Li, Reinforcement learning-based multislot double-threshold spectrum sensing
    with Bayesian fusion for industrial big spectrum data. IEEE Trans. Ind. Informatics
    17(5), 3391–3400 (2021) Article   Google Scholar   He, L., Tang, X.: Learning-based
    MIMO detection with dynamic spatial modulation. IEEE Transactions on Cognitive
    Communications and Networking PP(99), 1–12 (2023) Z. Na, B. Li, X. Liu, J. Wan,
    M. Zhang, Y. Liu, B. Mao, UAV-based wide-area internet of things: an integrated
    deployment architecture. IEEE Netw. 35(5), 122–128 (2021) Article   Google Scholar   X.
    Li, Q. Wang, M. Zeng, Y. Liu, S. Dang, T.A. Tsiftsis, O.A. Dobre, Physical-layer
    authentication for ambient backscatter-aided NOMA symbiotic systems. IEEE Trans.
    Commun. 71(4), 2288–2303 (2023) Article   Google Scholar   X. Liu, Q. Sun, W.
    Lu, C. Wu, H. Ding, Big-data-based intelligent spectrum sensing for heterogeneous
    spectrum communications in 5G. IEEE Wirel. Commun. 27(5), 67–73 (2020) Article   Google
    Scholar   Tang, S., Yang, Q., Fan, L.: Contrastive learning based semantic communications.
    IEEE Trans. Commun. PP(99), 1–12 (2024) J. Zhao, X. Sun, X. Ma, H. Zhang, F.R.
    Yu, Y. Hu, Online distributed optimization for energy-efficient computation offloading
    in air-ground integrated networks. IEEE Trans. Veh. Technol. 72(4), 5110–5124
    (2023) Article   Google Scholar   X. Liu, B. Lai, B. Lin, V.C.M. Leung, Joint
    communication and trajectory optimization for multi-UAV enabled mobile internet
    of vehicles. IEEE Trans. Intell. Transp. Syst. 23(9), 15354–15366 (2022) Article   Google
    Scholar   He, L., Lei, X.: Towards general edge intelligence: Visions, challenges,
    and enabling technologies. IEEE Wireless Commun. PP(99), 1–10 (2023) X. Liu, H.
    Ding, S. Hu, Uplink resource allocation for noma-based hybrid spectrum access
    in 6G-enabled cognitive internet of things. IEEE Internet Things J. 8(20), 15049–15058
    (2021) Article   Google Scholar   W. Xu, Y. Huang, W. Wang, F. Zhu, X. Ji, Toward
    ubiquitous and intelligent 6G networks: from architecture to technology. Sci.
    China Inf. Sci. 66(3), 130300 (2023) Article   Google Scholar   T. Ma, Y. Xiao,
    X. Lei, W. Xiong, M. Xiao, Distributed reconfigurable intelligent surfaces assisted
    indoor positioning. IEEE Trans. Wirel. Commun. 22(1), 47–58 (2023) Article   Google
    Scholar   K.N. Ramamohan, S.P. Chepuri, D.F. Comesaña, G. Leus, Self-calibration
    of acoustic scalar and vector sensor arrays. IEEE Trans. Signal Process. 71, 61–75
    (2023) Article   MathSciNet   ADS   Google Scholar   X. Li, M. Zhao, M. Zeng,
    S. Mumtaz, V.G. Menon, Z. Ding, O.A. Dobre, Hardware impaired ambient backscatter
    NOMA systems: reliability and security. IEEE Trans. Commun. 69(4), 2723–2736 (2021)
    Article   Google Scholar   J. Zhao, Y. Nie, H. Zhang, F.R. Yu, A UAV-aided vehicular
    integrated platooning network for heterogeneous resource management. IEEE Trans.
    Green Commun. Netw. 7(1), 512–521 (2023) Article   Google Scholar   H. Hou, Y.S.
    Han, P.P.C. Lee, Y. Wu, G. Han, M. Blaum, A generalization of array codes with
    local properties and efficient encoding/decoding. IEEE Trans. Inf. Theory 69(1),
    107–125 (2023) Article   MathSciNet   Google Scholar   W. Wu, F. Zhou, R.Q. Hu,
    B. Wang, Energy-efficient resource allocation for secure noma-enabled mobile edge
    computing networks. IEEE Trans. Commun. 68(1), 493–505 (2020) Article   Google
    Scholar   H. Huang, G. Gui, H. Gacanin, C. Yuen, H. Sari, F. Adachi, Deep regularized
    waveform learning for beam prediction with limited samples in non-cooperative
    mmWave systems. IEEE Trans. Veh. Technol. 72(7), 9614–9619 (2023) Article   Google
    Scholar   C. Zeng, J. Wang, C. Ding, M. Lin, J. Wang, MIMO unmanned surface vessels
    enabled maritime wireless network coexisting with satellite network: beamforming
    and trajectory design. IEEE Trans. Commun. 71(1), 83–100 (2023) Article   Google
    Scholar   S. Arya, Y.H. Chung, Fault-tolerant cooperative signal detection for
    petahertz short-range communication with continuous waveform wideband detectors.
    IEEE Trans. Wirel. Commun. 22(1), 88–106 (2023) Article   Google Scholar   Y.
    Liu, Z. Tan, A.W.H. Khong, H. Liu, An iterative implementation-based approach
    for joint source localization and association under multipath propagation environments.
    IEEE Trans. Signal Process. 71, 121–135 (2023) Article   MathSciNet   ADS   Google
    Scholar   K. Ma, S. Du, H. Zou, W. Tian, Z. Wang, S. Chen, Deep learning assisted
    mmWave beam prediction for heterogeneous networks: A dual-band fusion approach.
    IEEE Trans. Commun. 71(1), 115–130 (2023) Article   Google Scholar   B. Banerjee,
    R.C. Elliott, W.A. Krzymien, H. Farmanbar, Downlink channel estimation for FDD
    massive MIMO using conditional generative adversarial networks. IEEE Trans. Wirel.
    Commun. 22(1), 122–137 (2023) Article   Google Scholar   S. Liu, L. Ji, Double
    multilevel constructions for constant dimension codes. IEEE Trans. Inf. Theory
    69(1), 157–168 (2023) Article   MathSciNet   Google Scholar   Download references
    Acknowledgements This work was supported in part by the 2022 information special
    research project of China Southern Power Grid Corporation “Research on the theory
    of Digital Power Grid”. Funding This work was supported in part by the 2022 information
    special research project of China Southern Power Grid Corporation “Research on
    the theory of Digital Power Grid”. Author information Authors and Affiliations
    Guangdong Power Grid Co., Ltd, China Southern Power Grid, Guangzhou, China Ying
    Sun Contributions YS was responsible for designing the proposed approach, performing
    the simulations, and the writing in the manuscript. Corresponding author Correspondence
    to Ying Sun. Ethics declarations Ethics approval and consent to participate Not
    applicable. Consent for publication All authors of this paper agree to publish
    the work in this paper. Competing interests The authors declare that they have
    no competing interests. Additional information Publisher''s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Open Access This article is licensed under
    a Creative Commons Attribution 4.0 International License, which permits use, sharing,
    adaptation, distribution and reproduction in any medium or format, as long as
    you give appropriate credit to the original author(s) and the source, provide
    a link to the Creative Commons licence, and indicate if changes were made. The
    images or other third party material in this article are included in the article''s
    Creative Commons licence, unless indicated otherwise in a credit line to the material.
    If material is not included in the article''s Creative Commons licence and your
    intended use is not permitted by statutory regulation or exceeds the permitted
    use, you will need to obtain permission directly from the copyright holder. To
    view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
    Reprints and permissions About this article Cite this article Sun, Y. Distributed
    transmission and optimization of relay-assisted space-air-ground IoT systems.
    EURASIP J. Adv. Signal Process. 2024, 27 (2024). https://doi.org/10.1186/s13634-024-01123-5
    Download citation Received 08 November 2023 Accepted 06 February 2024 Published
    24 February 2024 DOI https://doi.org/10.1186/s13634-024-01123-5 Share this article
    Anyone you share the following link with will be able to read this content: Get
    shareable link Provided by the Springer Nature SharedIt content-sharing initiative
    Keywords Space-air-ground networks IoT networks Distributed transmission Relaying
    Download PDF Collection AI Enabled Signal Processing for Space-Air-Ground Integrated
    Internet of Things Sections Figures References Abstract Introduction System and
    computing models Performance analysis Simulation results and discussions Conclusions
    Availability of data and materials Abbreviations References Acknowledgements Funding
    Author information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Support and Contact Jobs Language editing for
    authors Scientific editing for authors Leave feedback Terms and conditions Privacy
    statement Accessibility Cookies Follow SpringerOpen By using this website, you
    agree to our Terms and Conditions, Your US state privacy rights, Privacy statement
    and Cookies policy. Your privacy choices/Manage cookies we use in the preference
    centre. © 2024 BioMed Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: Eurasip Journal on Advances in Signal Processing
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Distributed transmission and optimization of relay-assisted space-air-ground
    IoT systems
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Liu Z.
  - Yuan Y.
  - Zhang C.
  - Zhu Q.
  - Xu X.
  - Yuan M.
  - Tan W.
  citation_count: '0'
  description: 'Purpose: Early-stage lung cancer is typically characterized clinically
    by the presence of isolated lung nodules. Thousands of cases are examined each
    year, and one case usually contains numerous lung CT slices. Detecting and classifying
    early microscopic lung nodules is demanding due to their diminutive dimensions
    and restricted characterization capabilities. Therefore, a lung nodule classification
    model that performs well and is sensitive to microscopic lung nodules is needed
    to accurately classify lung nodules. Methods: This paper uses the Resnet34 network
    as a basic classification model. A new cascade lung nodule classification method
    is proposed to classify lung nodules into 6 classes instead of the traditional
    2 or 4 classes. It can effectively classify six different nodule types including
    ground-glass and solid nodules, benign and malignant nodules, and nodules with
    predominantly ground-glass or solid components. Results: In this paper, the traditional
    multi-classification method and the cascade classification method proposed in
    this paper were tested using real lung nodule data collected in the clinic. The
    test results demonstrate that the cascade classification method in this study
    achieves an accuracy of 80.04%, outperforming the conventional multi-classification
    approach. Conclusions: Different from the existing methods for categorizing the
    benign and malignant nature of lung nodules, the approach presented in this paper
    can classify lung nodules into 6 categories more accurately. At the same time,
    This paper proposes a rapid, precise, and dependable approach for classifying
    six distinct categories of lung nodules, which increases the accuracy categorization
    compared with the traditional multivariate categorization method.'
  doi: 10.1007/s13755-024-00273-y
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Log in Find a journal Publish
    with us Track your research Search Cart Home Health Information Science and Systems
    Article Hierarchical classification of early microscopic lung nodule based on
    cascade network Research Published: 23 February 2024 Volume 12, article number
    13, (2024) Cite this article Health Information Science and Systems Aims and scope
    Submit manuscript Ziang Liu , Ye Yuan, Cui Zhang, Quan Zhu, Xinfeng Xu, Mei Yuan
    & Wenjun Tan  196 Accesses Explore all metrics Abstract Purpose Early-stage lung
    cancer is typically characterized clinically by the presence of isolated lung
    nodules. Thousands of cases are examined each year, and one case usually contains
    numerous lung CT slices. Detecting and classifying early microscopic lung nodules
    is demanding due to their diminutive dimensions and restricted characterization
    capabilities. Therefore, a lung nodule classification model that performs well
    and is sensitive to microscopic lung nodules is needed to accurately classify
    lung nodules. Methods This paper uses the Resnet34 network as a basic classification
    model. A new cascade lung nodule classification method is proposed to classify
    lung nodules into 6 classes instead of the traditional 2 or 4 classes. It can
    effectively classify six different nodule types including ground-glass and solid
    nodules, benign and malignant nodules, and nodules with predominantly ground-glass
    or solid components. Results In this paper, the traditional multi-classification
    method and the cascade classification method proposed in this paper were tested
    using real lung nodule data collected in the clinic. The test results demonstrate
    that the cascade classification method in this study achieves an accuracy of 80.04\\(\\%\\),
    outperforming the conventional multi-classification approach. Conclusions Different
    from the existing methods for categorizing the benign and malignant nature of
    lung nodules, the approach presented in this paper can classify lung nodules into
    6 categories more accurately. At the same time, This paper proposes a rapid, precise,
    and dependable approach for classifying six distinct categories of lung nodules,
    which increases the accuracy categorization compared with the traditional multivariate
    categorization method. This is a preview of subscription content, log in via an
    institution to check access.  Similar content being viewed by others Machine learning
    for risk stratification of thyroid cancer patients: a 15-year cohort study Article
    30 October 2023 Lung cancer identification: a review on detection and classification
    Article 09 June 2020 Deep learning for lung Cancer detection and classification
    Article 02 January 2020 Data availability The data that support the findings of
    this study are available from the corresponding author upon reasonable request.
    References Prabhu S, Prasad K, Robels-Kelly A, Lu X. AI-based carcinoma detection
    and classification using histopathological images: a systematic review. Comput
    Biol Med. 2022;142: 105209. Article   PubMed   Google Scholar   Monkam P, Qi S,
    Ma H, Gao W, Yao Y, Qian W. Detection and classification of pulmonary nodules
    using convolutional neural networks: a survey. IEEE Access. 2019;7:78075–91. Article   Google
    Scholar   Naik A, Edla DR. Lung nodule classification on computed tomography images
    using deep learning. Wirel Pers Commun. 2021;116(1):655–90. Article   Google Scholar   Winkels
    M, Cohen TS. Pulmonary nodule detection in CT scans with equivariant CNNs. Med
    Image Anal. 2019;55:15–26. Article   PubMed   Google Scholar   Cao W, Wu R, Cao
    G, He Z. A comprehensive review of computer-aided diagnosis of pulmonary nodules
    based on computed tomography scans. IEEE Access. 2020;8:154007–23. Article   Google
    Scholar   Sori WJ, Feng J, Godana AW, Liu S, Gelmecha DJ. DFD-net: lung cancer
    detection from denoised CT scan image using deep learning. Front Comput Sci. 2021;15(2):1–13.
    Article   Google Scholar   Pang S, Fan M, Wang X, Wang J, Song T, Wang X, Cheng
    X. VGG16-T: a novel deep convolutional neural network with boosting to identify
    pathological type of lung cancer in early stage by CT images. Int J Comput Intell
    Syst. 2020;13(1):771. Article   Google Scholar   Shen S, Han SX, Aberle DR, Bui
    AA, Hsu W. An interpretable deep hierarchical semantic convolutional neural network
    for lung nodule malignancy classification. Expert Syst Appl. 2019;128:84–95. Article   PubMed   PubMed
    Central   Google Scholar   Shengdong Nie, Xiwen Sun, Zhaoxue Chen. Progress in
    computer-aided detection for pulmonary nodule using CT image. Chin J Med Phys.
    2009;26(02):1075–9. Google Scholar   Tan Wenjun, Liu Pan, Li Xiaoshuo, Shaoxun
    Xu, Chen Yufei, Yang Jinzhu. Segmentation of lung airways based on deep learning
    methods. IET Image Process. 2022;16(5):1444–56. Article   Google Scholar   Tan
    Wenjun, Zhou Luyu, Li Xiaoshuo, Yang Xiaoyu, Chen Yufei, Yang Jinzhu. Automated
    vessel segmentation in lung CT and CTA images via deep neural networks. J X-Ray
    Sci Technol. 2021;1:1123–37. Google Scholar   Tan W, Huang P, Li X, et al. Analysis
    of segmentation of lung parenchyma based on deep learning methods. J X-ray Sci
    Technol. 2021;29(6):945–59. Google Scholar   Bojiang CHEN, Weimin LI. Comparative
    values of different imaging methods in lung cancer screening. Chin J Lung Cancer.
    2010;13(10):992–8. Google Scholar   Sahiner B, Chan HP, Hadjiiski LM, et al. Effect
    of CAD on radiologists detection of lung nodules on thoracic CT scans: analysis
    of an observer performance study by nodule size. Acad Radiol. 2009;16(12):1518–30.
    Article   PubMed   PubMed Central   Google Scholar   Shen S, Han SX, Aberle DR,
    Bui AA, Hsu W. An interpretable deep hierarchical semantic convolutional neural
    network for lung nodule malignancy classification. Expert Syst Appl. 2019;128:84–95.
    Article   PubMed   PubMed Central   Google Scholar   Rubin GD. Lung nodule and
    cancer detection in CT screening. J Thorac Imag. 2015;30(2):130. Article   Google
    Scholar   Gu Y, Lu X, Yang L, Zhang B, Yu D, Zhao Y, Gao L, Wu L, Zhou T. Automatic
    lung nodule detection using a 3D deep convolutional neural network combined with
    a multi-scale prediction strategy in chest CTs. Comput Biol Med. 2018;103:220–31.
    Article   PubMed   Google Scholar   Winkels M, Cohen TS. Pulmonary nodule detection
    in CT scans with equivariant CNNs. Med Image Anal. 2019;55:15–26. Article   PubMed   Google
    Scholar   Zhao B, Tan Y, Bell DJ, Marley SE, Guo P, Mann H, Scott ML, Schwartz
    LH, Ghiorghiu DC. Exploring intra-and inter-reader variability in unidimensional,
    bi-dimensional, and volumetric measurements of solid tumors on CT scans reconstructed
    at different slice intervals. Eur J Radiol. 2013;82(6):959–68. Article   PubMed   Google
    Scholar   Pinsky PF, Gierada DS, Nath PH, Kazerooni E, Amorosa J. National lung
    screening trial: variability in nodule detection rates in chest CT studies. Radiology.
    2013;268(3):865–73. Article   PubMed   PubMed Central   Google Scholar   Isensee
    F, Petersen J, Klein A, et al. nnu-net: Self-adapting framework for u-net-based
    medical image segmentation. arXiv preprint 2018. arXiv:1809.10486, https://doi.org/10.48550/arXiv.1809.10486.
    Dubray B, Thureau S, Nkhali L, Modzelewski R, Doyeux K, Ruan S, Vera P. FDG-PET
    imaging for radiotherapy target volume definition in lung cancer. IRBM. 2014;35(1):41–5.
    Article   Google Scholar   Sun J, Liu Q, Wang Y, Wang L, Song X, Zhao X. Five-year
    prognosis model of esophageal cancer based on genetic algorithm improved deep
    neural network. IRBM. 2023;44(3): 100748. Article   Google Scholar   Armato SG,
    Giger ML, Moran CJ. Automated detection of pulmonary nodules in helical computed
    tomography images of the thorax. SPIE. 1998;3338:916–9. ADS   Google Scholar   Armato
    SG, Giger ML, MacMahon H. Analysis of a three-dimensional lung nodule detection
    method for thoracic CT scans. SPIE. 2000;3979:103–9. ADS   Google Scholar   Armato
    SG, Giger ML, MacMahon H. Automated detection of lung nodules in CT scans: preliminary
    results. Med Phys. 2001;28(8):1552–61. Article   PubMed   Google Scholar   Wiemker
    R, Rogalla P, et al. Computer aided lung nodule detection on high resolution CT
    data.SPIE 2002:677-688. Wang H, Guo XH, Jia ZW, et al. Multilevel binomial logistic
    prediction model for malignant pulmonary nodules based on texture features of
    CT image. Eur J Radiol. 2010;74(1):124–9. Article   PubMed   Google Scholar   Hua
    KL, Hsu CH, Hidayati SC, et al. Computer-aided classification of lung nodules
    on computed tomography images via deep learning technique. Oncotargets Ther. 2015;8:2015–22.
    CAS   Google Scholar   Lin PL, Huang PW, Lee CH, et al. Automatic classification
    for solitary pulmonary nodule in CT image by fractal analysis based on fractional
    Brownian motion model. Pattern Recognit. 2013;46(12):3279–87. Article   ADS   Google
    Scholar   Shen W, Zhou M, Yang F, et al. Multi-scale Convolutional Neural Networks
    for Lung Nodule Classification[C]//Information Processing in Medical Imaging.
    Springer International Publishing, 2015:588-599. Causey JL, Zhang J, Ma S, et
    al. Highly accurate model for prediction of lung nodule malignancy with CT scans.
    Sci Rep. 2018;8(1):1–12. Article   ADS   Google Scholar   Al-Shabi M, Lee HK,
    Tan M. Gated-dilated networks for lung nodule classification in CT scans. IEEE
    Access. 2019;7:178827–38. Article   Google Scholar   Jena SR, George ST, Ponraj
    DN. Lung cancer detection and classification with DGMM-RBCNN technique, Neural
    Comput. Appl. 2021;1-17. Morales Pinzón A, Orkisz M, Richard J-C, Hernández Hoyos
    M. Lung segmentation by cascade registration. IRBM. 2017;38(5):266–80. Article   Google
    Scholar   He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C].
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016:770-778. Alex Krizhevsky, Ilya Sutskever, Hinton Geoffrey E. ImageNet classification
    with deep convolutional neural networks. Commun ACM. 2017;60(6):84–90. https://doi.org/10.1145/3065386.
    Article   Google Scholar   Szegedy, Christian et al. Going deeper with convolutions.
    2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2014:1-9.
    Karen Simonyan, Zisserman Andrew. Very Deep Convolutional Networks for Large-Scale
    Image Recognition. CoRR abs/1409.1556 : n. pag 2014. Download references Acknowledgements
    This work is supported by the National Natural Science Foundation of China (61971118),
    Science and Technology Plan of Liaoning Province (2021JH1/10400051), Fundamental
    Research Funds for the Central Universities (N2216014). Author information Authors
    and Affiliations Key Laboratory of Intelligent Computing in Medical Image, Ministry
    of Education, Northeastern University, Shenyang, 110189, China Ziang Liu, Ye Yuan,
    Cui Zhang & Wenjun Tan College of Computer Science and Engineering, Northeastern
    University, Shenyang, 110189, China Ziang Liu, Ye Yuan, Cui Zhang & Wenjun Tan
    Department of Thoracic Surgery, The First Affiliated Hospital of Nanjing Medical
    University, Nanjing, 210029, China Quan Zhu, Xinfeng Xu & Mei Yuan Corresponding
    authors Correspondence to Quan Zhu or Wenjun Tan. Ethics declarations Competing
    Interests The authors have no pertinent financial or non-financial conflicts of
    interest to declare. Additional information Publisher''s Note Springer Nature
    remains neutral with regard to jurisdictional claims in published maps and institutional
    affiliations. Rights and permissions Springer Nature or its licensor (e.g. a society
    or other partner) holds exclusive rights to this article under a publishing agreement
    with the author(s) or other rightsholder(s); author self-archiving of the accepted
    manuscript version of this article is solely governed by the terms of such publishing
    agreement and applicable law. Reprints and permissions About this article Cite
    this article Liu, Z., Yuan, Y., Zhang, C. et al. Hierarchical classification of
    early microscopic lung nodule based on cascade network. Health Inf Sci Syst 12,
    13 (2024). https://doi.org/10.1007/s13755-024-00273-y Download citation Received
    27 May 2023 Accepted 08 January 2024 Published 23 February 2024 DOI https://doi.org/10.1007/s13755-024-00273-y
    Keywords Lung nodules Convolutional neural network Resnet34 CT image Cascade classification
    method Access this article Log in via an institution Buy article PDF USD 39.95
    Price excludes VAT (USA) Tax calculation will be finalised during checkout. Instant
    access to the full article PDF. Rent this article via DeepDyve Institutional subscriptions
    Sections Figures References Abstract Data availability References Acknowledgements
    Author information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Discover content Journals A-Z Books A-Z Publish
    with us Publish your research Open access publishing Products and services Our
    products Librarians Societies Partners and advertisers Our imprints Springer Nature
    Portfolio BMC Palgrave Macmillan Apress Your privacy choices/Manage cookies Your
    US state privacy rights Accessibility statement Terms and conditions Privacy policy
    Help and support 129.93.161.219 Big Ten Academic Alliance (BTAA) (3000133814)
    - University of Nebraska-Lincoln (3000134173) © 2024 Springer Nature"'
  inline_citation: '>'
  journal: Health Information Science and Systems
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: Hierarchical classification of early microscopic lung nodule based on cascade
    network
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
- analysis: '>'
  authors:
  - Hamad F.
  - Shehata A.
  - Al Hosni N.
  citation_count: '0'
  description: The shift toward electronic learning due to the COVID-19 pandemic has
    created many opportunities to shape Oman’s learning styles. This study explores
    the factors that affect students’ acceptance of blended learning (BL) in higher
    education institutions in developing countries, focusing on Oman. The study examines
    the impact of demographic and social factors, attitude, subjective norms, perceived
    behavioral control, self-efficacy, beliefs, behavioral intention, and actual use
    of BL among students. The Theory of Planned Behavior (TPB) was used as a theoretical
    framework to understand the decision-making processes surrounding BL adoption.
    Hypotheses are formulated and tested using statistical analysis of survey results.
    The questionnaire was distributed to students from Sultan Qaboos University in
    Oman. The data collected were analyzed using inferential predictive modeling methods
    such as multiple regression analysis and Pearson correlation. The findings indicate
    that students have a positive attitude toward BL and are likely to choose it in
    the future. The study also reveals that demographic characteristics and various
    dimensions, such as attitude, subjective norms, perceived behavioral control,
    self-efficacy, beliefs, behavioral intention, and actual usage, influence students’
    acceptance and utilization of BL. The results contribute to the existing literature
    and provide insights into the factors that affect BL adoption in developing countries.
  doi: 10.1186/s41239-024-00443-8
  full_citation: '>'
  full_text: '>

    "Your privacy, your choice We use essential cookies to make sure the site can
    function. We also use optional cookies for advertising, personalisation of content,
    usage analysis, and social media. By accepting optional cookies, you consent to
    the processing of your personal data - including transfers to third parties. Some
    third parties are outside of the European Economic Area, with varying standards
    of data protection. See our privacy policy for more information on the use of
    your personal data. Manage preferences for further information and to change your
    choices. Accept all cookies Skip to main content Advertisement Search Get published
    Explore Journals Books About Login International Journal of Educational Technology
    in Higher Education About Articles Article Collections Submission Guidelines Videos
    Infographics Submit manuscript Research article Open access Published: 19 February
    2024 Predictors of blended learning adoption in higher education institutions
    in Oman: theory of planned behavior Faten Hamad , Ahmed Shehata & Noura Al Hosni  International
    Journal of Educational Technology in Higher Education  21, Article number: 13
    (2024) Cite this article 1155 Accesses 3 Altmetric Metrics Abstract The shift
    toward electronic learning due to the COVID-19 pandemic has created many opportunities
    to shape Oman’s learning styles. This study explores the factors that affect students’
    acceptance of blended learning (BL) in higher education institutions in developing
    countries, focusing on Oman. The study examines the impact of demographic and
    social factors, attitude, subjective norms, perceived behavioral control, self-efficacy,
    beliefs, behavioral intention, and actual use of BL among students. The Theory
    of Planned Behavior (TPB) was used as a theoretical framework to understand the
    decision-making processes surrounding BL adoption. Hypotheses are formulated and
    tested using statistical analysis of survey results. The questionnaire was distributed
    to students from Sultan Qaboos University in Oman. The data collected were analyzed
    using inferential predictive modeling methods such as multiple regression analysis
    and Pearson correlation. The findings indicate that students have a positive attitude
    toward BL and are likely to choose it in the future. The study also reveals that
    demographic characteristics and various dimensions, such as attitude, subjective
    norms, perceived behavioral control, self-efficacy, beliefs, behavioral intention,
    and actual usage, influence students’ acceptance and utilization of BL. The results
    contribute to the existing literature and provide insights into the factors that
    affect BL adoption in developing countries. Introduction Blended learning (BL),
    which combines face-to-face instruction with online learning activities (Graham
    et al., 2013), has evolved as a popular pedagogical strategy in higher education
    institutions around the world. Combining the benefits of traditional and online
    learning (Poon, 2014) and with a strong influence on students’ awareness of the
    teaching style and learning background, BL improves learning outcomes, improves
    student engagement and experience, and overcomes the limits of traditional classroom-based
    instruction (Edward et al., 2018; Ghazal et al., 2018). It also develops constructive,
    logical skills, enhances teaching characteristics, and establishes social order
    (Subramaniam & Muniandy, 2019). Students can become more engaged and excited about
    the learning process when the focus is shifted from teaching to learning thanks
    to BL (Ismail et al. 2018), increasing their tenacity and dedication. Integrating
    the qualities of online and traditional in-class learning can lead to overcoming
    their shortcomings, resulting in blended learning (Azizan, 2010; Sabah, 2020).
    For BL to be successful, teachers and students must share responsibility for the
    learning process (Zhao, 2022). BL involves combining various delivery strategies,
    learning philosophies, and instructional paradigms. BL involves the integration
    efforts of teachers, students and administration (Kaur, 2013). It is accomplished
    by mixing 30% in-class interaction with 70% IT-facilitated education (Anthony
    et al., 2019). According to Owston et al., (2019) effective BL delivery should
    combine 20% of classroom instruction with 80% of high-quality online learning.
    Previous research has shown positive student attitudes and comprehension towards
    blended learning, potentially affecting the future of teaching paradigms (Lazar
    et al., 2020; Sabah, 2020). Researchers Wai and Seng (2015) and Wang et al. (2021)
    argue that by empowering students with enhanced autonomy over their educational
    journey, higher education fosters learning through improved control and unrestricted
    availability of both physical and online course materials. Consequently, students
    gain the freedom to access academic resources online conveniently and engage with
    professors and peers in virtual environments while participating in traditional
    in-person classroom settings (Salonen et al., 2021). Similarly, Miniaoui and Kaur
    (2014) elaborated that the BL method promotes students’ learning autonomy. The
    authors claimed that blending face-to-face and online instruction can transform
    students’ academic experience. This is because learners can benefit from participating
    in a learning community, whether in person or electronically (Bokolo, 2019). Additionally,
    Lin and Wang (2012) showed BL’s lower withdrawal rates increase student satisfaction
    compared to conventional F2F courses. While blended learning has many advantages,
    implementing it successfully in higher education institutions in developing nations
    presents unique difficulties and necessitates careful consideration of several
    issues. Many learning theories have been applied and newly introduced to build
    up a proper base of the technological approach; however, many researchers, such
    as Han and Wang (2019), Bouilheres et al. (2020), and Felipede et al. (2021) argued
    that the theory should integrate constructivism, connectionism, cognitivism, humanism,
    educational technology and other learning theories to build up the proper understanding.
    Wong et al. (2014) and Zhu et al. (2016) emphasized the need for more investigative
    research on BL adoption to identify the influencing factors. Moreover, Sabah (2020)
    explains students’ behavioral attitudes, motivations, and barriers to continuing
    to use BL. Based on this, more research is needed, according to the study’s recommendations,
    to investigate the nature of usage-context factors and document the connections
    between various motives to fully comprehend how these elements collectively encourage
    students to use blended learning. For efficient implementation and adoption of
    this teaching strategy, it is crucial to comprehend the factors that affect the
    deployment of blended learning in developing nations. The Theory of Planned Behavior
    (TPB) offers a useful framework to examine the elements influencing the adoption
    of blended learning in higher education institutions (Jnr et al., 2020). The TPB
    created by Ajzen (1991) contends that three major constructs impact behavioral
    intentions and subsequent conduct: attitudes, subjective norms, and perceived
    behavioral control. Subjective norms reflect the perceived social pressure and
    influence from important others regarding the activity, whereas attitudes refer
    to people’s favorable or negative judgments of a specific behavior. Their perceived
    behavioural control reflects individuals’ perceptions of their capacity to carry
    out the behavior successfully. BL is commonly used in higher education; however,
    assessing its effectiveness is challenging since BL’s components can be very diverse.
    Through the lens of the TPB, the predictors of BL deployment in developing countries
    may be examined, allowing for a thorough understanding of the decision-making
    procedures and variables that influence the adoption of this cutting-edge instructional
    strategy. Gawande (2015) affirmed that Oman is in the early stages of implementing
    BL. There is uncertainty over how concepts for delivery systems like eLearning
    and BL are being developed. Therefore, models need to explore actual usage to
    encourage the adoption of the blended learning concept in Oman. Accordingly, this
    study aims to contribute to the existing literature by examining the predictors
    of blended learning deployment in developing countries using the TPB as a theoretical
    framework. Through an empirical investigation, it seeks to identify the attitudes,
    subjective norms, and perceived behavioral control factors that influence the
    decision-making processes surrounding BL adoption in higher education institutions
    in developing countries. Study objectives The study’s main objective is to explore
    the factors that affect students’ acceptance of BL in higher education in Oman.
    To achieve the study’s objective, several sub-objectives are set: 1. Explore the
    impact of demographic and social factors in determining their learning approach.
    2. Examine the attitude of the students toward the blended learning approach.
    3. Explore the role of subjective norms in students’ perception of blended learning.
    4. Identify how personal beliefs affect students’ acceptance of blended learning.
    5. Explore the relationship between students’ self-efficacy and their acceptance
    of blended learning. 6. Explore the relationship between students’ behavioral
    intention and the actual use of blended learning. Literature review Blended learning
    in higher education Many scholars and practitioners from an educational perspective
    have recognized and researched the importance of the conducted learning tactics,
    whether to deliver them through traditional means, as face-to-face classrooms
    or through digitizing the process through semi-learning conduction of blended
    learning or whole learning via online, electronic and mobile learning. Numerous
    studies have focused on how new technologies affect the educational sector, specifically
    how they alter the traditional face-to-face classroom setting. Being trapped between
    the inflexibility of traditional learning and the limitation of complete electronic
    learning, BL was developed as an approach to cover the weakness of traditional,
    in class and electronic learning (Azizan, 2010; Sabah, 2020), which creates a
    shared responsibility for the learning process from both instructors and students
    (Zhao, 2022). Since blended learning is viewed as the future teaching model, many
    prior studies showed positive students’ intention and understanding of the method
    (Lazar et al., 2020; Sabah, 2020). Though the presence of digital learning was
    recognized many years back, the occurrence of the COVID-19 pandemic has deepened
    the use of and dependency on technology, especially between 2020 and 2021, when
    the catastrophe forced all governments around the world toward the closure of
    schools and colleges to contain the global symptoms of the pandemic (Lerma et
    al., 2022; UNESCO, 2020; Yang et al., 2022). The realization of integrating digital
    approaches into the learning and teaching framework was converted and widespread,
    exceptionally involving both instructors and students as essential learning assets
    in the educational system (Lerma et al., 2022). Therefore, practicing BL in higher
    education institutions was extensively adopted, promoting computational thinking
    abilities and higher-order thinking among students. Thus, those students have
    been encouraged to participate actively in BL and online courses on a continuing
    basis even after the announcement of the pandemic solution (Gong et al., 2020;
    Yang et al., 2022). A systematic review by Balakrishnan et al. (2021) revealed
    that only four of the 26 studies were conducted in developing nations, perhaps
    as a result of unreliable Internet connectivity, a lack of resources, instructors’
    lack of training in new technologies, interruptions in power supplies, low bandwidth,
    affordable internet connections, and a lack of trust. Anthony et al. (2022) indicated
    a rise in BL studies from 2004 to 2020, with 2018 being the highest number of
    publications due to the increased awareness toward BL, especially in Malaysia,
    where the initiative and procedures of conducting BL in higher education had been
    established since 2015. The United States of America, Australia and the United
    Kingdom followed next. However, Arab countries had minimal BL research interest
    compared to other contexts, with only two resulting studies conducted in UAE and
    KSA (Anthony et al., 2022). Moreover, their findings inflated the presence of
    these studies in higher education institutions by almost 62% compared with other
    settings. Furthermore, Ashraf et al. (2021), using PRISMA (Preferred Reporting
    Items for Systematic Reviews and Meta-Analyses) guidelines, conducted a systematic
    review of systematic reviews on BL to identify BL trends, gaps, and future directions.
    The results showed that BL research was mostly conducted in higher education and
    initially focused on students. Most BL research also comes from developed countries,
    indicating a gap in research from developing countries. Teachers, students, and
    institutions frequently face issues related to a lack of ICT infrastructure and
    expertise. Despite the bright picture of BL, various challenges popped up, motivating
    many scholars to search for the best substitutions to overcome these barriers.
    For instance, Sabah (2020), who identified and evaluated the stimulated factors
    and barriers that forced students’ decisions to continue using BL via the Moodle
    platform. Conducting a multigroup analysis of three different universities in
    Palestine, the impact of individual differences, BL system’s features, students’
    perceptions and involvement were the most significant stimulating antecedents
    toward students’ attitudes, while perceived behavioral control and subjective
    norm were found to be the primary factors toward student decisions to continue
    using the approach. Similarly, Bamoallem and Altarteer (2022), focusing on Saudi
    students, pointed out that the idea of BL is very new in Saudi Arabia, with only
    a few colleges providing such blended educational programs. As a result, little
    is currently known about how students now perceive this approach. According to
    Keržič et al. (2019), BL should not be seen as a replacement for traditional face-to-face
    classrooms or online learning environments but rather as an approach that can
    maximize both benefits. BL can facilitate easier and more efficient interaction
    between students and teachers, personalized learning experiences, and overcome
    limitations of time and space (Keržič et al., 2019; Lerma et al., 2022; Zhao,
    2022). Anthony et al. (2022) and Poon (2014) have questioned the focus of current
    literature on whether higher education should adopt BL. Instead, they argue that
    the discussion should be centred around the practical aspects and strategies for
    successfully implementing BL. Furthermore, BL has been found to increase students’
    curiosity, enhance their academic skills, improve social communication, and foster
    self-reliance (Al-shami et al., 2018; Anthony et al., 2022). Students perceive
    BL positively, as it facilitates interaction through group discussions, online
    chats, and chapter reviews, leading to increased engagement and satisfaction (Sabah,
    2020). Van Laer and Elen (2017) emphasize the importance of BL in developing learners’
    self-regulation behavior and boosting their confidence in managing the learning
    process. However, several factors hinder the adoption of BL, particularly in Arab
    countries. One critical factor is the availability and quality of technological
    infrastructure. Insufficient access to reliable internet connectivity, limited
    digital devices, and inadequate technical support pose challenges to the effective
    implementation of BL initiatives (Alqudah et al., 2022). Arab countries need to
    invest in improving their technological infrastructure to facilitate the widespread
    adoption of BL. Additionally, Onah et al. (2022) highlight students’ difficulty
    in self-regulation and using learning devices as a major challenge when implementing
    BL. On the other hand, teachers often struggle with technology competencies, while
    educational institutions face issues related to the supply of adequate instructional
    technologies and effective support for teacher preparation (Rasheed et al., 2020).
    In Oman, there is currently no effective BL policy in place. However, with the
    outbreak of the COVID-19 virus in 2020, the Ministry of Education began promoting
    the concept. Moreover, His Majesty Sultan Haitham al-Tariq of Oman recognized
    the development and introduction of BL as an effective pedagogy in education in
    2020/2021 (Nair, 2020). Al-Musawi et al. (2020) have highlighted several benefits
    of BL, including time and effort savings, teaching skills development, and teaching
    practice facilitation. Overall, BL enhances students’ knowledge retention. BL
    has been considered an optimal and novel practice in higher education institutions,
    with various implementations across primary, middle, and high schools (Keržič
    et al., 2019; Lazar et al., 2020). However, there is a scarcity of investigative
    studies on students’ perceptions of BL in universities, with most existing studies
    focusing on the potential benefits and challenges of the approach (Anthony et
    al., 2022; Joo et al., 2017). As BL continues to evolve, it will be integrated
    with new technological advances, creating new BL environments that should be assessed
    based on learners’ perceptions (Nadlifatin et al., 2020). Theory of planned behaviour
    (TPB) Various studies have utilized technology acceptance theories to establish
    the foundation for blended learning (BL) with appropriate infrastructure. Yang
    et al. (2022) employed the Expectation-Confirmation Model of Information System
    Continuance (ECM-ISC) integrated with intrinsic motivation and academic self-efficacy
    as key personal factors. The ECM-ISC model incorporates perceived usefulness,
    confirmation, satisfaction, and information system continuance intention, which
    have been supported as effective measures for e-learning continuance intention
    (Roca & Gagne, 2008; Sorebo et al., 2009). The Theory of Planned Behavior (TPB)
    has been widely used to explain various learning behaviors and individuals’ choices
    regarding leisure activities, health decisions, and technology adoption. Lerma
    et al. (2022) integrated TPB with other context-relevant variables to predict
    e-learning success during the pandemic. In their theoretical review, Anthony et
    al. (2022) noted that the Technology Acceptance Model (TAM) was the most frequently
    used theory among their selected studies, accounting for 13% of the cases, followed
    by the Unified Theory of Acceptance and Use of Technology (UTAUT) with 7% and
    the Diffusion of Innovations (DoI) with 5% of the studies. Thus, this study contributes
    to the literature by applying TPB to BL technologies in higher education institutions.
    TPB has been employed to investigate various technological learning approaches.
    For instance, Cheon et al. (2012) explored college students’ perceptions and readiness
    for mobile learning in higher education in the United States, using TPB to demonstrate
    that students’ attitudes, subjective norms, and behavioral control positively
    influenced their acceptance of mobile learning. Valtonen et al. (2015) also used
    TPB to investigate pre-service teachers’ intentions to use ICT for teaching and
    learning. The model showed that self-efficacy and subjective norms were the primary
    factors affecting instructors’ adoption behavior. Furthermore, Nyasulu and Chawinga
    (2019) used the TPB model to examine how the WhatsApp messaging service was used
    as an e-learning tool in Malawi. Their conclusions showed that quick information
    sharing, academic collaboration, and the opportunity to learn outside of traditional
    class times benefited students. They did note several difficulties, though, including
    the price of mobile devices, frequent power outages, and erratic Internet connectivity
    from mobile network service providers. In a comparative study, Nadlifatin et al.
    (2020) compared the behavioral intentions of Taiwanese and Indonesian students
    toward modern learning technologies using the TAM-TPB model. The model yielded
    an explanatory power of 41.2% for Taiwanese students and 28.1% for Indonesian
    students. They recommended further research to address technical and material
    aspects of BL, such as content, curriculum, and facilities. Moreover, Azizi et
    al. (2020) employed UTAUT2 to identify factors influencing students’ intention
    to use BL in Iran, conducting a cross-sectional correlational study with a sample
    of 225 Iranian medical sciences students. The results revealed that all UTAUT2
    factors significantly influenced students’ behavioral intentions. Furthermore,
    Anthony et al. (2022) utilized TPB to evaluate predictors determining students’
    acceptance of BL in Malaysian higher education institutions. Predictors of blended
    learning While information systems theories and models have played a significant
    role in explaining technology users’ intentions, acceptance, and usage, many scholars
    and researchers argue for the inclusion of various personal, social, and technical
    factors to understand the dynamics influencing users comprehensively. Theoretical
    models used to identify students’ behavior towards the simultaneous use of technologies
    are still in the developmental stage, and the integration of additional constructs
    is necessary, especially as current empirical research focuses on individual technologies
    like e-learning and m-learning (Williamson et al., 2020). In the same vein, Hamad
    et al. (2022) affirmed the importance to investigate behaviors and attitudes of
    individuals for adopting new trends. For example, Sabah (2020) proposed a conceptual
    model based on the Technology Acceptance Model (TAM), Theory of Planned Behavior
    (TPB), Self-Determination Theory (SDT) of motivation, and other factors. This
    model integrates critical factors such as individual characteristics, extrinsic
    and intrinsic motivations, emotional affect, cultural elements, and features of
    blended learning systems. The study investigated the influential factors that
    drive students’ behavioral attitudes towards adopting and continuously using blended
    learning systems. The results indicated that students exhibited high levels of
    self-motivation, self-efficacy, behavioral control, and favorable attitudes toward
    blended learning. Bouilheres et al. (2020) identified engagement, flexibility
    of learning, online learning experience, and self-confidence as factors influencing
    blended learning adoption in Vietnam. Their findings align with the notion that
    the university’s blended learning environment positively influenced students’
    perceptions of their educational experiences, as well as their engagement with
    classmates, professors, and course materials. Anthony et al. (2022) found that
    attitude, subjective norm, perceived behavioral control, and self-efficacy were
    predictors of students’ acceptance of blended learning in Malaysia. The study
    also highlighted that students’ intention to accept blended learning was significantly
    influenced by the actual implementation of the approach. This study contributes
    to the limited body of research investigating students’ behavioral intentions
    toward blended learning deployment in Malaysia and enhances our understanding
    of the predictors that influence students’ intention to accept and adopt blended
    learning in educational institutions. Furthermore, in their theoretical and systematic
    investigation, Anthony et al. (2022) identified several key constructs considered
    by scholars, such as perceived ease of use, attitude, actual use, self-efficacy,
    emotional engagement, satisfaction, perceived usefulness, continuance intention,
    frequency of use, enjoyment, hedonic motivation, habit, age, sex, social influence,
    and flexibility. Other studies have highlighted factors, such as e-learning adaptability,
    on-time teacher feedback, outcome expectancy, facilitating conditions, computer
    self-efficacy, learning atmosphere, perceived enjoyment, system performance, social
    interaction, content specificity, and performance expectation as important factors
    influencing learner satisfaction and acceptance of blended learning (García et
    al., 2014; Wu & Liu, 2013; Yeou, 2016; Zhao & Yuan, 2010). These studies collectively
    contribute to understanding the multifaceted factors that shape students’ perceptions,
    attitudes, and intentions toward blended learning. Blended learning research in
    Oman Blended learning has become more prevalent worldwide, including in Oman,
    where educational institutions are considering using it. Enhancing student involvement
    is one important benefit of BL in Oman. It provides students with a more dynamic
    and interactive learning environment by incorporating online components like multimedia
    and discussion forums. Student retention, student achievement, attendance, satisfaction,
    and exam performance have all improved at Arab Open University in Oman (Muthuraman,
    2018). The learning management system significantly aids in promoting BL, and
    students have a positive attitude toward it. The study used a six-dimension Hexagonal
    E-Learning Assessment Model (HELAM) created by Ozkan and Koseler (2009) to determine
    factors affecting overall university achievement and student satisfaction of BL.
    Al-Busaidi (2013) instigated the role of Learning Management Systems (LMS) in
    higher education to promote student adoption of LMS in BL. The study indicates
    that innovativeness, perceived usefulness, and satisfaction with LMS significantly
    influence students’ intention to engage in full e-learning courses. Al-Ani (2013)
    investigated the (BL) perspectives of 283 students from different colleges at
    Sultan Qaboos. The study results offered proof in favor of switching from a traditional
    learning environment to a mixed learning environment. According to students, Moodle’s
    online learning platform benefited their motivation, accomplishments, collaboration,
    and communication skills. Additionally, the results showed that by minimizing
    the time spent in conventional face-to-face learning environments, BL enabled
    increased self-regulation and self-direction among students. At Sur College of
    Applied Sciences (Oman), Gawande (2015) investigated the relationship between
    behavior intention and user acceptance of technology (BL adoption). It was discovered
    that elements, including interaction, flexibility, student efficiency, instructor
    leadership, training, and technical support, influenced BL adoption among students.
    At Ibra College of Technology (Oman), Siraj and Maskari (2019) find that students
    favour BL programs. The study emphasized the need for appropriate infrastructure,
    staff and student training and development programs, and a transition to more
    targeted practical assessment techniques to gauge graduate qualities. The BL course
    appeared to positively affect student engagement, learner autonomy, connection
    of learning to real-world situations, and flexibility. Al Musawi and Ammar (2021)
    examined the impact of two BL (BL) methodologies compared to conventional approaches
    within Sultan Qaboos University’s College of Education. The experiment set out
    with the lofty goal of determining the ideal BL ratio that would boost students’
    academic competence, particularly in comprehension and critical thinking. Theoretical
    framework The TPB considers three types of human actions: behavioural beliefs,
    normative beliefs, and control beliefs (Ajzen, 1991). The theoretical framework
    of the current research has been developed based on the research objectives and
    questions and a review of the related literatures i.e. Yeou (2016); Dakduk et
    al. (2018), Nadlifatin et al. (2020), Bouilheres et al. (2020), Anthony et al.
    (2019), Anthony et al. (2022) (Fig. 1). Fig. 1 Shows the research framework, including
    the independent, mediator, and dependent variables Full size image Hypotheses
    Using statistical analysis of survey results and based on the findings of the
    literature review above, the study expects to provide evidence to the following
    hypotheses: Attitude: a measurement of a student’s positive or negative opinion
    of the behavior in question (Ajzen, 1991). As a result, attitude affects students’
    willingness to accept BL, which affects how BL is used (Valtonen et al., 2022).
    According to earlier research (Dakduk et al., 2018), student attitudes significantly
    predicted their intention to accept blended e-learning. H1: Attitude significantly
    predicts students’ intention to accept BL. Subjective norm (SN): represents the
    influence of social pressure or expectations on an individual’s intention to engage
    in a behavior. Previous studies have suggested that SN is associated with students’
    perceptions of the expectations from others, such as peers, who encourage them
    to adopt or accept blended learning (Cheon et al., 2012; Dakduk et al., 2018).
    When students observe their peers endorsing blended learning, they are more likely
    to embrace it (Dakduk et al., 2018; Valtonen et al., 2022; Yeou, 2016). H2: Subjective
    norm significantly predicts students’ intention to accept blended learning. Perceived
    behavioral control (PBC): reflects an individual’s belief in their ability to
    perform a behavior. PBC is closely linked to an individual’s perception of control,
    subsequently affecting their behavioral intentions and actual usage (Ajzen, 2002;
    Cheon et al., 2012; Raza et al., 2020). As individuals become more confident in
    their capability to carry out the behavior in question, their perceived behavioral
    control increases (Ajzen, 1985). H3: Perceived behavioral control significantly
    predicts tudents’ intention to accept BL. Self-efficacy: is related to perceived
    behavioral control and defined as an individual’s belief in their capability to
    execute a specific behavior successfully (Ajzen, 1991). Self-efficacy discusses
    how students use BL and how their level of self-assurance affects their conduct
    (Tagoe and Abakah, 2014; Yeou, 2016). According to Jnr et al., (2020a), self-efficacy
    in BL is correlated with how students evaluate student confidence in their abilities
    to carry out BL activities which positively reflests on their intention to adopt
    BL (Anthony et al., 2019). H4: Self-efficacy significantly predicts students’
    intention to accept BL. Beliefs: about the outcomes or advantages of performing
    a behavior. H5: Beliefs significantly predict students’ intention to accept BL.
    Intentions: according to Ajzen (1991), intentions determine how motivated individuals
    are to attempt something new or how much effort they intend to put into engaging
    in the behavior. According to Revythi and Tselios (2019), the theory of planned
    behavior is based on the idea that users’ intentions drive behavior. Almulla (2022)
    stated that students’ intentions to adopt and use BL are directly impacted by
    how valuable they believe BL to be. H6: Students’ intention to accept BL positively
    influences the actual use of BL. Testing these hypotheses will provide an answer
    to the research question that supports this study: “What are the main predictors
    of BL use among university students in the Sultanante of Oman?”. Methodology This
    research investigates the factors that affect students’ acceptance of BL in higher
    education in Oman. Sultan Qaboos University students were the population of this
    study (N ≈ 17,000). Quantitative research methods using a questionnaire were applied
    based on the research aims and objectives. A convenient sampling technique was
    used in this study. The questionnaire was distributed to all students from social
    sciences schools. Social science students were targeted in 2022 where the number
    of those students is approximately 6000 students based on the SQU admission office.
    362 students were targeted to participate in the survey after calculating the
    sample size for the total number of both cohorts with 95% confidence level and
    +−5% margin of error. The survey was professionally and electronically designed,
    with well-prepared and simple English language that the targeted students could
    handle. Moreover, several channels were used to reach these students, including
    emails, social media and in-class participation by scanning the QR code of the
    survey URL. However, only 182 returned surveys were obtained from students, resulting
    in a 50.3% response rate, which is sufficient to rely on their responses. As such,
    a limitation of this study that could be considered for future research is to
    have a higher response rate since the current one is limited with improper selected
    data collection timing. Students were busy with their exams and project submission
    periods, as well as competing with those students’s priorities at that period
    of time. And this is one of the reasons to exclude scientific schools. Data collection
    tool The data collection method was a questionnaire administered to all social
    science students during the academic year 2022–2023. The questionnaire facilitates
    a better outreach and a comprehensive exploration of students’ attitudes and perceptions
    of their BL beliefs. The questionnaire of this research was developed based on
    the research objectives and aligned with related literature, such as Yeou (2016),
    Dakduk et al. (2018), Nadlifatin et al. (2020), Bouilheres et al. (2020), Anthony
    et al. (2022). The instrument was developed based on the theoretical framework.
    For example, for Attitude, items measure students'' general feelings towards BL,
    based on Valtonen et al. (2022) and Dakduk et al., 2018). Furthermore, studies
    show that the endorsement of BL by peers can significantly influence students''
    decisions toward adopting it (Cheon et al., 2012; Dakduk et al., 2018; Valtonen
    et al., 2022; Yeou, 2016). PBC captures the individual''s belief in their capability
    to effectively engage in BL (Ajzen, 1985, 2002; Cheon et al., 2012; Raza et al.,
    2020). SE is correlated with students’ assessment of their capacity to perform
    BL-related activities, impacting their intention to adopt BL (Anthony et al.,
    2019; Yeou, 2016). Students'' perceptions of the outcomes or benefits of engaging
    in BL and their intention to use it are seen as a key driver of behavior and are
    influenced by the perceived value of the behavior (Almulla, 2022; Rahman et al.,
    2019; Revythi & Tselios, 2019). The questionnaire was emailed to all students
    from different social science departments, and 182 students responded. The first
    part of the questionnaire collected students’ profiles (sex, academic year level,
    and technology skills level). Data about students’ enrollment in BL courses were
    also collected to understand the respondent characteristics and variation among
    the responding groups. The second part of the questionnaire consisted of items
    (ranked questions). Sections two to eight of the questionnaire are dedicated to
    testing the significance level of the given hypothesis of the current study using
    a 5-point Likert scale (1 = Strongly Disagree, 2 = Disagree, 3 = Neutral, 4 = Agree
    and 5 = Strongly Agree). Attitude (ATT) is the first independent variable covered
    by section two questions (6–14). Subjective Norms (SN) is the second independent
    variable covered by section 3 questions (15–19). Perceived Behavioral Control
    (PBC), Self-Efficacy (SE) and Belief (B) are also independent variables covered
    in sections 4, 5 and 6, questions (20–34). Behavioral Intention (BI) is considered
    the dependent variable of the pre-mentioned variables covered in section 7, questions
    (35–38). BI is an independent variable for the dependent variable Actual Use (AU),
    covered in section 8 questions (39–43). Instrument validity To ensure validity,
    the questionnaire was reviewed by a group of referees (N = 6) from information
    studies, management information systems and education professionals to check for
    clarity and grammatical errors and to identify the suitability of the questionnaire
    items to provide answers to the research main questions. Moreover, a Cronbach’s
    Alpha test was conducted to ensure the reliability of the questionnaire. The Cronbach’s
    Alpha value was high, scoring 0.90, as indicated in Table 1. Table 1 The result
    of Cronbach’s Alpha test Full size table To ensure validity, the questionnaire
    was reviewed by a group of referees from the faculty of Educational Sciences and
    library professionals to check for clarity and grammatical errors and identify
    the suitability of the questionnaire items to provide answers to the research
    main questions. For example, one of the recommendations made was to rephrase the
    item on the competencies form “Designing data infrastructure (metadata creation)”
    to be “Designing data infrastructure (metadata creation) to enable AI.” (item
    no. 23). Other recommendations were to add items, such as an item on the “chatbots”
    in library services (item no. 7), another on the capability of using data analytics
    tools for data mining (item no. 28) and another one about data encryption and
    security (item no. 31). All reviewer recommendations (N = 12) were considered,
    and the questionnaire was revised accordingly. Descriptive statistics of the sample
    Descriptive statistics were computed to understand the structure and description
    of the sample. The distribution of the acquired answers was dominated by female
    students with 60.4%, compared to the proportion of male students who formed 39.6%
    of overall targeted participants, as shown in Table 2. Additionally, the layout
    of the study sample was covered by different academic years. Third-year students
    in the university topped with 26.4% followed by Second-year students who shared
    a similar proportion of 20.9% (Table 2). Among these students, most rated their
    skills in using technology as intermediate (44%) and advanced intermediate (34.6%)
    levels, with 78.6% of overall students, as indicated in Table 2. As a real practice
    of BL courses, (48.4) of respondents indicated they are enrolled in BL courses
    (Table 2). 19.8% response (maybe) indicated a lack of understanding of the term
    BL. Table 2 Participant profile Full size table Statistical analysis Inferential,
    predictive modelling statistical methods, such as multiple and multiple regression
    analyses were also applied to test the formulated hypothesis at significance levels
    (α = 0.05) and (α = 0.01) to determine the relative correlation between the independent
    variables and dependent variables. Pearson correlation was used to determine the
    correlation at significance levels (α = 0.05) and (α = 0.01). Results and data
    analysis Descriptive statistics of the variables Attitude (ATT) Statistically,
    students’ responses indicated a positive attitude toward using BL courses, as
    displayed in Table 3 with (M = 3.77, SD = 0.691). Table 3 Students’ attitudes
    toward BL courses Full size table Subjective norms (SN) Students’ responses indicated
    that they are positively affected by their peers’ opinion of BL. The results showed
    that other opinions encouraged students to take these courses with (AVG = 3.42,
    SD = 0.813) (Table 4). Table 4 Students’ subjective norms toward BL courses Full
    size table Perceived behavioral control (PBC) Regarding students’ ability to easily
    learn using BL, their responses indicated that most of them agreed on their control
    over behavior of interest, BL, with (AVG = 3.42, SD = 0.914), as displayed in
    Table 5. Table 5 Students’ perceived behavioral control toward BL courses Full
    size table Self-efficacy (SE) Most of the students believe they are confident
    about their abilities to enroll in BL courses, as (AVG = 3.58, SD = 0.754) demonstrated
    in Table 6. Table 6 Students’ self-efficacy toward BL courses Full size table
    Belief (B) Based on the opinions collected by respondents, they believe and have
    confidence in the advantages of delivering BL courses at the university with (AVG = 3.64,
    SD = 0.815), as shown in Table 7. Table 7 Students’ belief toward BL courses Full
    size table Behavioral intention (BI) Students at the university showed an agreement
    of their positive intention toward enrolling in BL courses as (AVG = 3.64, SD = 0.807),
    as shown in Table 8. Table 8 Students’ behavioral intention toward BL courses
    Full size table Actual use (AU) Students who participated in the questionnaire
    have already enrolled in courses prescribed as BL courses and showed an intention
    to enrol in similar courses in the future with (AVG = 3.63, SD = 0.807), as shown
    in Table 9. Table 9 Students’ actual use of BL courses Full size table Predictive
    modeling: Independent variable effect on behavioral intention The P–P Plot in
    Fig. 2, indicates the goodness-of-fit of the model. It shows that assumptions
    of regression model is correct and the model’s predictions are reliable. Fig.
    2 The P–P Plot for goodness-of-fit of the model Full size image Multiple regression
    modeling was applied to investigate the prediction power of the independent variable:
    Attitude, Subjective Norm, Perceived Behavioral Control, Self Efficacy, and Belief
    on the dependent variable; Behavioral Intention (BI). Based on R-Square value
    of 0.703 (70.3%), 70.3% of the independent variables of the proposed model can
    explain BI (Table 10). Table 10 Model summary Full size table As indicated in
    Table 11, the tolerance values are above 0.2, and variation inflation factor (VIF)
    values are less than 5 is greater than the usual threshold of 5, meaning no multicollinearity
    exists in the regression model. When independent variables are correlated, this
    is referred to as multicollinearity. It may affect the stability and accuracy
    of the regression coefficients and how the results are to be interpreted. Table
    11 Regression’s results of coefficients Full size table The Regression equation
    values in Table 11 indicate that students have more intention to enroll in BL
    courses as attitude (ATT), subjective norms (SN), self-efficacy (SE), and belief
    (B) increase. In contrast, the lower their perceived control (PBC) in these courses,
    the lower their intention to enroll in BL courses. The Regression Equation that
    demonstrates the predictive power of each independent variable is as the following:
    From the Coefficients table (Table 11), and as per the resulting t-values and
    p-values, Subjective Norms (t-value = 3.101; p-value = 0.002), Self-Efficacy (t-value = 4.431;
    p-value < 0.001) and Belief (t-value = 5.904 & p-value < 0.001) are the only significant
    variables and have a positive relationship with belief having the strongest effect.
    Students’ attitude (t-value = 1.542 & p-value = 0.125) and Perceived Behavioral
    Control (t-value = -0.897 & p-value = 0.371) are not significant and have no influences
    on Behavioral Intention. Behavioral intention effect on actual use The P–P Plot
    in Fig. 3, indicates the goodness-of-fit of the model. It shows that regression
    model assumptions are correct and the model’s predictions are reliable. Fig. 3
    P–P Plot for the goodness-of-fit of the model Full size image Linear regression
    modeling was applied to investigate the prediction power of the dependent variable
    Behavioral Intention–Independent variable or predictor of Actual Use (dependent
    variable). The regression equation: As students’ behavioral intention increases,
    they will engage in more BL courses. This means that students more interested
    in enrolling in BL courses will enroll in BL courses in the future (Table 12).
    Table 12 Regression’s results of coefficients Full size table Table 12 indicates
    that the tolerance values are above 0.2 and variation inflation factor (VIF) values
    are less than 5 is greater than the usual threshold of 5, meaning no multicollinearity
    exists in the regression model. The results of coefficients (Table 12) and correlation
    (Table 13), indicate a very strong relationship between the two variables (BI
    and AU). As per the t-values, which is highly significant (p < 0.001), (Table
    12), Behavioral Intention (t-value = 11.893) significantly predicts student Actual
    Use. Table 13 further confirms the positive impact of BI on AU. Table 13 Correlation
    between AU and BI Full size table Based on R-Square value of 0.440 (Table 14).
    Accordingly, Behavioral Intention significantly explains (44%) of the variance
    in Actual Use (44% of students’ BI can explain their AU). Although the model appears
    to be reasonably good at forecasting Actual Use, 56% of the variance in Actual
    Use remains unaccounted for, indicating that additional factors considered by
    the model may also be impacting Actual Use. Table 14 Model summary Full size table
    Testing hypothesis Discussion The study’s findings make some intriguing observations
    regarding how blended learning (BL) is used and accepted by Sultan Qaboos University
    students in Oman. Using established scales for attitude (ATT), subjective norms
    (SN), perceived behavioral control (PBC), self-efficacy (SE), belief (B), behavioral
    intention (BI), and actual usage (AU), the students’ responses have been carefully
    quantified. These dimensions significantly influence the level of acceptance and
    utilization of BL, which combines traditional and digital approaches, at Sultan
    Qaboos University. Students showed a positive attitude toward BL and are likely
    to choose it in the future because they believe it can help them learn more effectively
    and freely. Students’ desire to adopt BL is influenced by their demographic characteristics.
    Firstly, the demographics of the sample population reveal a higher proportion
    of female students and a broad distribution of academic years, with most students
    having an intermediary to above intermediary level of technological skills. This
    sex distribution could be related to the specific setting of the study or could
    also reflect a gender-related trend in attitudes toward blended learning. Similar
    conclusions have been noted in prior literature. The sex difference in the sample
    reflects the trend identified in previous studies, such as Zhang et al. (2020),
    which found that female students generally show higher acceptance rates for BL.
    Also, Adams et al. (2021) affirmed that female students are more likely to participate
    and feel at ease in BL contexts. This suggests that female students may be more
    adept than male students at time management, taking the initiative to learn new
    things, and participating in the learning process. Interestingly, the distribution
    reveals that almost half of the respondents were already enrolled in BL courses.
    Third-year students comprised the largest percentage, followed by second and fourth-year
    students; both came second. The nature of courses might explain this during the
    early years and some advanced years, where most courses in social sciences are
    mostly theory and can be easily adapted to BL mode. This information is essential
    because it gives a general picture of the sample’s academic composition and enables
    an evaluation of how academic standing may affect attitudes toward BL. The majority
    of students ranked their technological proficiency at intermediate levels, which
    may indicate that they are at ease using technology and may have an impact on
    how they feel about BL. The fact that almost half of the students said they were
    taking BL courses points to the widespread use of BL in this academic environment.
    The academic year and skill level distribution highlight the significance of these
    variables in BL acceptance, which is consistent with Al-Azawei et al. (2017) and
    Alqurashi’s (2019) findings. Nearly half of the students (48.4%) indicated they
    were enrolled in BL courses, suggesting a significant interest and application
    of blended learning in this academic setting. This suggests that many students
    have chosen to enroll in BL courses. There might be several reasons for this:
    flexibility of learning and personalized learning. BL can also improve the learning
    environment, which frequently includes interactive digital tools, multimedia resources,
    and group projects, which can improve the overall learning environment. Several
    researches supported the explanation provided above. For example, a study by Allen
    and Seaman (2016) found that flexibility, access to resources, and personalized
    learning experiences were among the primary factors motivating students to enroll
    in online or blended courses. Other studies, such as those by Serrano et al. (2019)
    and Sahni (2019), have also highlighted the benefits of BL courses, including
    enhanced engagement, improved student outcomes, and increased access to learning
    opportunities. On the other hand, the 31.9% who did not enroll in BL courses might
    have issues related to lack of access to technology, mainly the Internet (Dey
    & Bandyopadhyay, 2019; Rasheed et al., 2020). It also can be attributed to the
    limited awareness or understanding of the benefits and opportunities provided
    by BL courses (Cannon et al., 2023), leading them to choose traditional courses.
    Also, BL courses may not be available for certain programs or specific courses,
    limiting the enrollment options for students. Secondly, analysis of the student’s
    attitudes (ATT), subjective norms (SN), perceived behavioral control (PBC), self-efficacy
    (SE), belief (B), behavioral intention (BI), and actual use (AU) toward blended
    learning yielded insightful information. The mean scores of these constructs are
    all above the neutral point of 3, indicating positive responses toward BL, a finding
    supported by literature such as Al-Maroof et al. (2021); Anthony et al. (2022).
    Also, Owston et al., (2019) indicated that students showed positive attitudes
    toward blended learning due to its flexibility and diverse learning experiences.
    A worthy point is the relatively high mean score of the self-efficacy construct,
    which aligns with Bandura’s self-efficacy theory (Bandura & Adams, 1977) argues
    that the greater one’s self-efficacy, the more likely they are to undertake a
    task or behavior. Similarly, Bandura (1986) argued that self-efficacy plays a
    crucial role in setting challenges and overcoming obstacles, such as those potentially
    faced in a blended learning environment. The inferential predictive modeling analysis
    explores the relationships between these constructs, particularly how independent
    variables predict behavioral intention (BI), and, in turn, how BI predicts AU.
    The R-Square value of 70.3% shows that the independent variables, including ATT,
    SN, PBC, SE, and B, together can explain 70.3% of the variance in BI towards BL.
    The regression model revealed that students’ subjective norms, self-efficacy,
    and belief were statistically significant in predicting their behavioral intention
    toward blended learning, which echoes Venkatesh et al.’s (2003) findings in their
    UTAUT model. Several previous studies also support this. Numerous studies have
    found that perceived social norms significantly predict behavioral intention (Brouwer
    et al., 2009; Rivis & Sheeran, 2003). Ajzen’s (1991) theory of planned behavior
    suggested that subjective norms significantly influence behavioral intentions.
    People are more likely to engage in a behavior if they perceive that others, such
    as friends, family, or society, expect them to do so. The statistically significant
    relationship between subjective norm and behavioral intention in the provided
    analysis aligns with these findings. Studies have consistently reported a positive
    association between self-efficacy and behavioral intention across various domains,
    including health behaviors, academic performance, and career choices (Bandura
    & Adams, 1997; Judge & Bono, 2001). Higher levels of self-efficacy are typically
    associated with stronger intentions to engage in a behavior. The highly statistically
    significant relationship between self-efficacy and behavioral intention in the
    provided analysis is consistent with these findings. Beliefs about the outcomes
    or advantages of performing a behavior have been found to be strong predictors
    of behavioral intention (Eccles et al., 1998). Positive beliefs about the benefits
    or advantages associated with a behavior are generally associated with a higher
    intention to engage in that behavior. The highly statistically significant relationship
    between belief and behavioral intention in the provided analysis aligns with the
    existing literature. Interestingly, the study found that attitude and perceived
    behavioral control did not significantly influence behavioral intention, contrary
    to what the theory of planned behavior (Ajzen, 1991) would suggest. The relationship
    between attitude and behavioral intention has been extensively studied in the
    field of psychology and behavior change. According to the Theory of Planned Behavior
    (Ajzen, 1991), attitude is one of the key determinants of behavioral intention.
    A positive attitude toward a behavior is generally associated with a higher intention
    to engage in that behavior. However, the lack of statistical significance in the
    provided analysis suggests that the relationship may be weak or not present in
    the given sample. Various factors, such as the specific context, deferent level
    of culture, measurement scales, or sample characteristics might influence this
    finding. Furthermore, Bervell et al. (2020) argued that sometimes attitude must
    be investigated in relation to other factors/dimensions, including technology-related,
    societal-related, or environmental-related dimensions, to understand or predict
    BI. Also, the literature has a significant technological character, performance
    expectancy, and effort expectancy social influence as factors that need further
    investigation to understand attitude (Davis, 1989; Venkatesh & Bala, 2008; Venkatesh
    et al., 2003). Furthermore, research has consistently shown that higher levels
    of perceived behavioral control are associated with stronger intentions to engage
    in a behavior (Armitage & Conner, 2004; Sheeran, 2002). However, in the provided
    analysis, the lack of statistical significance suggests that the relationship
    between perceived behavioral control and behavioral intention might not be present
    in the given sample or may be weak. This discrepancy calls for further research
    to understand the unique contexts and variables that may be at play in this setting.
    Based on the results, four out of the six hypotheses (H3, H4 and H5) were accepted,
    and the remaining three (H1 and H2) were rejected (Table 15). This implies that
    while subjective norms, beliefs, and self-efficacy significantly predict behavioral
    intentions toward blended learning, attitudes and perceived behavioral control
    do not significantly influence this behavioral intention. The Ajzen (1991) theory
    of planned behavior, which contends that attitudes and perceived behavioral control
    have a major impact on behavioral intentions, contradicts this result. Table 15
    Final resulted hypothesis testing Full size table The study also discovered that
    behavioral intention substantially determines whether blended learning courses
    would actually be used. The technological acceptance model (TAM) (Silva, 2015)
    contends that behavioral desire to use a technology greatly precedes actual use,
    and a large portion of the literature concurs with this. According to the linear
    regression model, the usage of integrated learning in practice and behavioral
    intention are positively correlated. As a result, students who aspire to employ
    blended learning are more likely to do so, according to numerous studies (e.g.,
    Silva, 2015; Venkatesh et al., 2003). According to the findings of studies like
    Ajzen (1991); Venkatesh et al. (2003) and Silva (2015), this evidence is consistent
    with the notion that Behavioral Intention positively influences Actual Use. Although
    behavioral intention is a significant component in predicting actual use, the
    correlation of 0.663 shows that other factors may also be at work, accounting
    for the share of the variance in actual use that is not explained by behavioral
    intention. It would take more investigation to determine what these potential
    influences might be. In conclusion, the analysis and discussion of the results
    are robust and shed light on the acceptance and utilization of BL among students
    in higher education in Oman. The study, however, does hint at unexplained variance
    in actual use, suggesting that additional factors might influence the actual usage
    of blended learning that were not captured in the current study. This presents
    an interesting avenue for future research to explore. Also, attitude and perceived
    behavioral control were found not to significantly influence behavioral intention,
    contrary to the TPB, which calls for further investigation to understand other
    factors that might have a mediation effect on this. Conclusion In conclusion,
    blended learning (BL) has emerged as a popular pedagogical approach in higher
    education institutions worldwide, combining the benefits of traditional face-to-face
    instruction with online learning activities. BL has shown to improve learning
    outcomes, enhance student engagement and experience, and overcome the limitations
    of traditional classroom-based instruction. By integrating online and in-class
    learning qualities, BL can create a transformative academic experience for students,
    promoting learning autonomy and providing access to a wide range of educational
    resources. The Theory of Planned Behavior (TPB) provided a useful framework for
    understanding the factors that influence the adoption of BL in higher education
    institutions. Attitudes, subjective norms, and perceived behavioral control play
    significant roles in students’ intention to accept and use BL. Positive attitudes
    toward BL, social pressure from peers, and a belief in one’s ability to engage
    in BL activities contribute to the intention to adopt and use BL. Additionally,
    students’ self-efficacy and beliefs about the outcomes of using BL also influence
    their acceptance of this instructional strategy. The study contributes to the
    existing literature by providing insights into the factors influencing the adoption
    and use of BL in developing countries. The findings can inform educational institutions
    and policymakers in Oman and similar contexts about the importance of considering
    students’ attitudes, social norms, perceived control, self-efficacy, and beliefs
    when implementing BL initiatives. Understanding these factors can help optimize
    the design and delivery of BL programs, leading to improved student engagement
    and learning outcomes. It is important to note that this study focused on one
    specific institution and may not be generalizable to all higher education settings.
    Further research is needed to validate and extend these findings in other developing
    countries. Additionally, qualitative research methods can be employed better to
    understand students’ experiences and perceptions regarding BL. Moreover, investigating
    the impact of faculty readiness and support for BL implementation would provide
    valuable insights into the successful integration of this instructional strategy.
    Blended learning has the potential to revolutionize higher education by combining
    the strengths of traditional and online learning. By considering the factors influencing
    students’ acceptance and use of BL, educational institutions can effectively implement
    and optimize this approach, leading to enhanced student engagement, improved learning
    outcomes, and a more flexible and inclusive learning environment. Availability
    of data and materials Data will be available and shared upon request. Abbreviations
    AU: Actual use B: Belief BI: Behavioral intention BL: Blended learning ECM-ISC:
    Expectation-Confirmation Model of Information System Continuance HELAM: Hexagonal
    E-Learning Assessment Model LMS: Learning Management Systems PBC: Perceived behavioral
    control PRISMA: Preferred Reporting Items for Systematic Reviews and Meta-Analyses
    SDT: Self-Determination Theory SE: Self-Efficacy SN: Subjective norm TPB: Theory
    of Planned Behavior UTAUT: Unified Theory of Acceptance and Use of Technology
    TAM: Technology Acceptance Model References Adams, D., Tan, M. H. J., & Sumintono,
    B. (2021). Students’ readiness for blended learning in a leading Malaysian private
    higher education institution. Interactive Technology and Smart Education, 18(4),
    515–534. Article   Google Scholar   Ajzen, I. (1985). From intentions to actions:
    A theory of planned behavior. In Action control: From cognition to behavior (pp.
    11–39). Berlin, Heidelberg: Springer Berlin Heidelberg. Ajzen, I. (1991). The
    theory of planned behavior. Organizational Behavior and Human Decision Processes,
    50(2), 179–211. Article   Google Scholar   Ajzen, I. (2002). Perceived behavioral
    control, self-efficacy, locus of control, and the theory of planned behavior 1.
    Journal of Applied Social Psychology, 32(4), 665–683. Article   Google Scholar   Al
    Musawi, A. S., & Ammar, M. E. (2021). The effect of different blending levels
    of traditional and E-learning delivery on academic achievement and students’ attitudes
    towards blended learning at Sultan Qaboos University. Turkish Online Journal of
    Educational Technology-TOJET, 20(2), 127–139. Google Scholar   Al-Ani, W. T. (2013).
    Blended learning approach using moodle and student’s achievement at Sultan Qaboos
    University in Oman. Journal of Education and Learning, 2(3), 96–110. Article   Google
    Scholar   Al-Azawei, A., Parslow, P., & Lundqvist, K. (2017). Investigating the
    effect of learning styles in a blended e-learning system: An extension of the
    technology acceptance model (TAM). Australasian Journal of Educational Technology,
    33(2). Al-Busaidi, K. A. (2013). An empirical investigation linking learners’
    adoption of blended learning to their intention of full e-learning. Behaviour
    & Information Technology, 32(11), 1168–1176. Article   Google Scholar   Allen,
    I. E., & Seaman, J. (2016). Online report card: Tracking online education in the
    United States. Babson Survey Research Group. Babson College, 231 Forest Street,
    Babson Park, MA 02457. Al-Maroof, R., Al-Qaysi, N., Salloum, S. A., & Al-Emran,
    M. (2021). Blended learning acceptance: A systematic review of information systems
    models. Technology, Knowledge and Learning, 1–36. Almulla, M. A. (2022). Investigating
    important elements that affect students’ readiness for and practical use of teaching
    methods in Higher Education. Sustainability, 15(1), 653. Article   Google Scholar   Al-Musawi,
    A.S., El Shourbagi, S.A., & Al Saddi, B.K. (2020). Effects of software on gifted
    students achievement and activities in elementary education: Cross-cultural investigation.
    In Handbook of research on software for gifted and talented school activities
    in K-12 classrooms (pp. 65–93). IGI Global. Alqudah, H., Alwaely, S. A., Lahiani,
    H., & Aljarrah, H. (2022, November). Perspectives on E-Learning in Universities
    of the Arab Countries. In 2022 International Arab Conference on Information Technology
    (ACIT) (pp. 1–6). IEEE. Alqurashi, E. (2019). Predicting student satisfaction
    and perceived learning within online learning environments. Distance Education,
    40(1), 133–148. Article   Google Scholar   Al-shami, S. A., Aziz, H., & Rashid,
    N. (2018). The adoption of MOOC utilization among undergraduate students in UniversitiTeknikal
    Malaysia Melaka (UTEM). Journal of Fundamental and Applied Sciences, 10(6S), 2634–2654.
    Google Scholar   Anthony, B., Kamaludin, A., Romli, A., Raffei, A. F. M., Eh Phon,
    D. N. A. L., Abdullah, A., Ming, G. L., Shukor, N. A., Nordin, M. S., & Baba,
    S. (2019). Exploring the role of blended learning for teaching and learning effectiveness
    in institutions of higher learning: An empirical investigation. Education and
    Information Technologies, 24, 3433–3466. Article   Google Scholar   Anthony, B.,
    Kamaludin, A., Romli, A., Raffei, A. F. M., Phon, D. N. A. E., Abdullah, A., &
    Ming, G. L. (2022). Blended learning adoption and implementation in higher education:
    A theoretical and systematic review. Technology, Knowledge and Learning, 1–48.
    Armitage, C. J., & Conner, M. (2004). The effects of attitudinal ambivalence on
    attitude-intention-behavior relations. Contemporary Perspectives on the Psychology
    of Attitudes, 3(2), 121–143. Google Scholar   Ashraf, M. A., Yang, M., Zhang,
    Y., Denden, M., Tlili, A., Liu, J., Huang, R., & Burgos, D. (2021). A systematic
    review of systematic reviews on blended learning: Trends, gaps and future directions.
    Psychology Research and Behavior Management, 14, 1525–1541. Article   PubMed   PubMed
    Central   Google Scholar   Azizan, F. Z. (2010). “Blended Learning in Higher Education
    Institution in Malaysia.” In Proceedings of Regional Conference on Knowledge Integration
    in Information & Communication Technology (ICT), 454–466. Selangor: Malaysia.
    Azizi, S. M., Roozbahani, N., & Khatony, A. (2020). Factors affecting the acceptance
    of blended learning in medical education: Application of UTAUT2 model. BMC Medical
    Education, 20, 1–9. Article   Google Scholar   Balakrishnan, A., Puthean, S.,
    Satheesh, G., Unnikrishnan, M. K., Rashid, M., Nair, S., & Thunga, G. (2021).
    Effectiveness of blended learning in pharmacy education: A systematic review and
    meta-analysis. PLoS ONE, 16(6), e0252461. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Bamoallem, B., & Altarteer, S. (2022). Remote emergency learning during
    COVID-19 and its impact on university students perception of blended learning
    in KSA. Education and Information Technologies, 27(1), 157–179. Article   PubMed   Google
    Scholar   Bandura, A. (1986). Social foundations of thought and action. Englewood
    Cliffs, NJ, 1986(23–28). Bandura, A., & Adams, N. E. (1977). Analysis of self-efficacy
    theory of behavioral change. Cognitive Therapy and Research, 1(4), 287–310. Article   Google
    Scholar   Bervell, B., Nyagorme, P., & Arkorful, V. (2020). LMS-enabled blended
    learning use intentions among distance education tutors: Examining the mediation
    role of attitude based on technology-related stimulus-response theoretical framework.
    Contemporary Educational Technology, 12(2), ep273. Article   Google Scholar   Bokolo,
    A. J. (2019). Exploring the role of blended learning for teaching and learning
    effectiveness in institutions of higher learning: An empirical investigation.
    Bouilheres, F., Le, L. T. V. H., McDonald, S., Nkhoma, C., & Jandug-Montera, L.
    (2020). Defining student learning experience through blended learning. Education
    and Information Technologies, 25, 3049–3069. https://doi.org/10.1007/s10639-020-10100-y
    Article   Google Scholar   Brouwer, S., Krol, B., Reneman, M. F., Bültmann, U.,
    Franche, R. L., van der Klink, J. J., & Groothoff, J. W. (2009). Behavioral determinants
    as predictors of return to work after long-term sickness absence: An application
    of the theory of planned behavior. Journal of Occupational Rehabilitation, 19,
    166–174. Article   PubMed   Google Scholar   Cannon, J. P., Lohtia, R., & Paulich,
    B. J. (2023). Blended learning in principles of marketing: The effects of student
    differences on student performance. Journal of Marketing Education, 45(1), 70–90.
    Article   Google Scholar   Cheon, J., Lee, S., Crooks, S. M., & Song, J. (2012).
    An investigation of mobile learning readiness in higher education based on the
    theory of planned behavior. Computers & Education, 59(3), 1054–1064. Article   Google
    Scholar   Dakduk, S., Santalla-Banderali, Z., & van der Woude, D. (2018). Acceptance
    of blended learning in executive education. SAGE Open, 8(3), 1–16. Article   Google
    Scholar   Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and
    user acceptance of information technology. MIS Quarterly, 13, 319–340. Article   Google
    Scholar   Dey, P., & Bandyopadhyay, S. (2019). Blended learning to improve quality
    of primary education among underprivileged school children in India. Education
    and Information Technologies, 24(3), 1995–2016. Article   Google Scholar   Eccles,
    J. S., Wigfield, A., & Schiefele, U. (1998). Motivation to succeed. In W. Damon
    & N. Eisenberg (Eds.), Handbook of child psychology: Social, emotional, and personality
    development (pp. 1017–1095). John Wiley & Sons Inc. Google Scholar   Edward, C.
    N., Asirvatham, D., & Johar, M. G. M. (2018). Effect of blended learning and learners’
    characteristics on students’ competence: An empirical evidence in learning oriental
    music. Education and Information Technologies, 23, 2587–2606. Article   Google
    Scholar   Felipede, B. L., Sintria, L., & Alex, S. G. (2021). Contrasting levels
    of student engagement in blended and non-blended learning scenarios. Computer
    & Education, 172, 1–13. https://doi.org/10.1016/j.compedu.2021.104241 Article   Google
    Scholar   García, A. V. M., del Dujo, Á. G., & Rodríguez, J. M. M. (2014). Factores
    determinantes de adopción de blended learning en educación superior. Adaptación
    del modelo UTAUT. Educación., 17(2), 217–240. Google Scholar   Gawande, V. (2015).
    Development of blended learning model based on the perceptions of students at
    higher education institutes in Oman. International Journal of Computer Applications,
    114(1). Ghazal, S., Aldowah, H., & Umar, I. (2018). Critical factors to learning
    management system acceptance and satisfaction in a blended learning environment.
    In Recent Trends in Information and Communication Technology: Proceedings of the
    2nd International Conference of Reliable Information and Communication Technology
    (IRICT 2017) (pp. 688–698). Springer International Publishing. Gong, D., Yang,
    H. H., & Cai, J. (2020). Exploring the key influencing factors on college students’
    computational thinking skills through flipped-classroom instruction. International
    Journal of Educational Technology in Higher Education, 17(1), 19. https://doi.org/10.1186/s41239-020-00196-0
    Article   Google Scholar   Graham, C. R., Woodfield, W., & Harrison, J. B. (2013).
    A framework for institutional adoption and implementation of blended learning
    in higher education. The Internet and Higher Education, 18, 4–14. Article   Google
    Scholar   Hamad, F., Al-Fadel, M., & Fakhouri, H. (2022). The role of academic
    libraries and information specialists during times of health crises in Jordan:
    The COVID-19 pandemic case. Digital Library Perspectives, 38(4), 476–492. Article   Google
    Scholar   Han, X., & Wang, Y. (2019). Towards a framework for an institution-wide
    quantitative assessment of teachers’ online participation in blended learning
    implementation. The Internet & Higher Education, 42, 1–12. https://doi.org/10.1016/j.iheduc.2019.03.003
    Article   CAS   Google Scholar   Ismail, A. O., Mahmood, A. K., & Abdelmaboud,
    A. (2018). Factors influencing academic performance of students in blended and
    traditional domains. International Journal of Emerging Technologies in Learning
    (online), 13(2), 170. Article   Google Scholar   Jnr, B. A., Kamaludin, A., Romli,
    A., Raffei, A. F. M., Phon, D. N. A. E., Abdullah, A., Ming, G. L., Shukor, N.
    A., & NordinBaba, M. S. S. (2020a). Predictors of blended learning deployment
    in institutions of higher learning: Theory of planned behavior perspective. The
    International Journal of Information and Learning Technology, 37(4), 179–196.
    Article   Google Scholar   Joo, Y. J., Park, S., & Shin, E. K. (2017). Students’
    expectation, satisfaction, and continuance intention to use digital textbooks.
    Computers in Human Behavior., 2017(69), 83–90. https://doi.org/10.1016/j.chb.2016.12.025
    Article   Google Scholar   Judge, T. A., & Bono, J. E. (2001). Relationship of
    core self-evaluations traits—self-esteem, generalized self-efficacy, locus of
    control, and emotional stability—with job satisfaction and job performance: A
    meta-analysis. Journal of Applied Psychology, 86(1), 80. Article   CAS   PubMed   Google
    Scholar   Kaur, M. (2013). Blended learning-its challenges and future. Procedia-Social
    and Behavioral Sciences, 93, 612–617. Article   Google Scholar   Keržič, D., Tomaževič,
    N., Aristovnik, A., & Umek, L. (2019). Exploring critical factors of the perceived
    usefulness of blended learning for higher education students. PLoS ONE, 14(11),
    e0223767. Article   PubMed   PubMed Central   Google Scholar   Lazar, I. M., Panisoara,
    G., & Panisoara, I. O. (2020). Digital technology adoption scale in the blended
    learning context in higher education: Development, validation and testing of a
    specific tool. PLoS ONE, 15(7), e0235957. Article   CAS   PubMed   PubMed Central   Google
    Scholar   Lerma, D. F. P., Nwaiwu, F., Afful-dadzie, E., Ntsiful, A., & Kwarteng,
    M. A. (2022, October). A Conceptual Framework for Integrating TPB With Context-Relevant
    Variables to Predict e-Learning Success During the Covid-19 Pandemic. In European
    Conference on e-Learning (Vol. 21, No. 1, pp. 365–372). Academic Conferences International
    Limited. Lin, W. S., & Wang, C. H. (2012). Antecedences to continued intentions
    of adopting e-learning system in blended learning instruction: A contingency framework
    based on models of information system success and task-technology fit. Computers
    & Education, 58(1), 88–99. Article   Google Scholar   Miniaoui, H., & Kaur, A.
    (2014). ‘A discussion forum’: A blended learning assessment tool to enhance students’
    learning. International Journal of Innovation and Learning, 16(3), 277–290. Article   Google
    Scholar   Muthuraman, S. (2018). Quality of blended learning education in higher
    education. The Online Journal of Distance Education and e-Learning, 6(4), 48.
    Google Scholar   Nadlifatin, R., Miraja, B., Persada, S., Belgiawan, P., Redi,
    A. A. N., & Lin, S. C. (2020). The measurement of University students’ intention
    to use blended learning system through technology acceptance model (TAM) and theory
    of planned behavior (TPB) at developed and developing regions: Lessons learned
    from Taiwan and Indonesia. International Journal of Emerging Technologies in Learning
    (iJET), 15(9), 219–230. Article   Google Scholar   Nair, V. (2020). Schools begin
    offline, online in Oman. Oman Daily Observer, 1st November. https://www.omanobserver.om/ministry-of-education-okays-guidelines-for-school-reopening/.
    Nyasulu, C., & Chawinga, D. (2019). Using the decomposed theory of planned behaviour
    to understand university students’ adoption of WhatsApp in learning. E-Learning
    and Digital Media., 16(5), 413–429. https://doi.org/10.1177/2042753019835906 Article   Google
    Scholar   Onah, D. F., Pang, E. L., & Sinclair, J. E. (2022). Investigating self-regulation
    in the context of a blended learning computing course. The International Journal
    of Information and Learning Technology, 39(1), 50–69. Article   Google Scholar   Owston,
    R., York, D. N., & Malhotra, T. (2019). Blended learning in large enrolment courses:
    Student perceptions across four different instructional models. Australasian Journal
    of Educational Technology, 35(5), 29–45. Google Scholar   Ozkan, S., & Koseler,
    R. (2009). Multi-dimensional students’ evaluation of e-learning systems in the
    higher education context: An empirical investigation. Computers & Education, 53(4),
    1285–1296. Article   Google Scholar   Poon, J. (2014). A cross-country comparison
    on the use of blended learning in property education. Property Management, 32(2),
    154–175. Article   Google Scholar   Rahman, N. S. A., Raffei, A. F. M., & Al-Rahmi,
    W. (2019). Understanding university students’ behavioral intention to use social
    media for teaching and learning. In International Conference on E-Learning (pp.
    60–73). Universiti Malaysia Sarawak 21–22 August 2019. Rasheed, R. A., Kamsin,
    A., & Abdullah, N. A. (2020). Challenges in the online component of blended learning:
    A systematic review. Computers & Education, 144, 103701. Article   Google Scholar   Raza,
    S. A., Qazi, W., Shah, N., Qureshi, M. A., Qaiser, S., & Ali, R. (2020). Drivers
    of intensive Facebook usage among university students: An implications of U&G
    and TPB theories. Technology in Society, 62, 101331. Article   Google Scholar   Revythi,
    A., & Tselios, N. (2019). Extension of technology acceptance model by using system
    usability scale to assess behavioral intention to use e-learning. Education and
    Information Technologies, 24, 2341–2355. Article   Google Scholar   Rivis, A.,
    & Sheeran, P. (2003). Social influences and the theory of planned behaviour: Evidence
    for a direct relationship between prototypes and young people’s exercise behaviour.
    Psychology and Health, 18(5), 567–583. Article   Google Scholar   Roca, J. C.,
    & Gagne, M. (2008). Understanding e-learning continuance intention in the workplace:
    A self-determination theory perspective. Computers in Human Behavior, 24(4), 1585–1604.
    https://doi.org/10.1016/j.chb.2007.06.001 Article   Google Scholar   Sabah, N.
    M. (2020). Motivation factors and barriers to the continuous use of blended learning
    approach using Moodle: Students’ perceptions and individual differences. Behaviour
    & Information Technology, 39(8), 875–898. Article   Google Scholar   Sahni, J.
    (2019). Does blended learning enhance student engagement? Evidence from higher
    education. Journal of E-Learning and Higher Education, 2019(2019), 1–14. Article   ADS   Google
    Scholar   Salonen, A. O., Tapani, A., & Suhonen, S. (2021). Student online activity
    in blended learning: A learning analytics perspective of professional teacher
    education studies in Finland. SAGE Open, 11(4), 21582440211056612. Article   Google
    Scholar   Serrano, D. R., Dea-Ayuela, M. A., Gonzalez-Burgos, E., Serrano-Gil,
    A., & Lalatsa, A. (2019). Technology-enhanced learning in higher education: How
    to enhance student engagement through blended learning. European Journal of Education,
    54(2), 273–286. Article   Google Scholar   Sheeran, P. (2002). Intention—behavior
    relations: A conceptual and empirical review. European Review of Social Psychology,
    12(1), 1–36. Article   Google Scholar   Silva, P. (2015). Davis'' technology acceptance
    model (TAM) (1989). Information seeking behavior and technology adoption: Theories
    and trends, 205–219. Siraj, K. K., & Maskari, A. A. (2019). Student engagement
    in blended learning instructional design: An analytical study. Learning and Teaching
    in Higher Education: Gulf Perspectives, 15(2), 61–79. Google Scholar   Sorebo,
    O., Halvari, H., Gulli, V. F., & Kristiansen, R. (2009). The role of self-determination
    theory in explaining teachers’ motivation to continue to use e-learning technology.
    Computers & Education, 53(4), 1177–1187. https://doi.org/10.1016/j.compedu.2009.06.001
    Article   Google Scholar   Subramaniam, S. R., & Muniandy, B. (2019). The effect
    of flipped classroom on students’ engagement. Technology, Knowledge and Learning,
    24(3), 355–372. Article   Google Scholar   Tagoe, M. A., & Abakah, E. (2014).
    Determining distance education students’ readiness for mobile learning at University
    of Ghana using the theory of planned behavior. International Journal of Education
    and Development using Information and Communication Technology, 10(1), 91–106.
    Google Scholar   UNESCO. (2020). COVID-19 Educational disruption and response.
    Unesco.Org. Valtonen, T., Kukkonen, J., Kontkanen, S., Sormunen, K., Dillon, P.,
    & Sointu, E. (2015). The impact of authentic learning experiences with ICT on
    pre-service teachers’ intentions to use ICT for teaching and learning. Computers
    & Education, 81, 49–58. Article   Google Scholar   Valtonen, T., López-Pernas,
    S., Saqr, M., Vartiainen, H., Sointu, E. T., & Tedre, M. (2022). The nature and
    building blocks of educational technology research. Computers in Human Behavior,
    128, 107123. Article   Google Scholar   Van Laer, S., & Elen, J. (2017). In search
    of attributes that support self-regulation in blended learning environments. Education
    and Information Technologies, 22(4), 1395–1454. Article   Google Scholar   Venkatesh,
    V., & Bala, H. (2008). Technology acceptance model 3 and a research agenda on
    interventions. Decision Sciences, 39(2), 273–315. Article   Google Scholar   Venkatesh,
    V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information
    technology: Toward a unified view. MIS Quarterly, 27, 425–478. Article   Google
    Scholar   Wai, C. C., & Seng, E. L. K. (2015). Measuring the effectiveness of
    blended learning environment: A case study in Malaysia. Education and Information
    Technologies, 20, 429–443. Article   Google Scholar   Wang, N., Chen, J., Tai,
    M., & Zhang, J. (2021). Blended learning for Chinese university EFL learners:
    Learning environment and learner perceptions. Computer Assisted Language Learning,
    34(3), 297–323. Article   Google Scholar   Williamson, B., Eynon, R., & Potter,
    J. (2020). Pandemic politics, pedagogies and practices: digital technologies and
    distance education during the coronavirus emergency. Learning, Media and Technology.,
    45(2), 107–114. https://doi.org/10.1080/17439884.2020.1761641 Article   Google
    Scholar   Wong, L., Tatnall, A., & Burgess, S. (2014). A framework for investigating
    blended learning effectiveness. Education Training, 56(2/3), 233–251. Article   Google
    Scholar   Wu, J., & Liu, W. (2013). An empirical investigation of the critical
    factors affecting students’ satisfaction in EFL blended learning. J Language Teach
    Res., 4(1), 176–185. Google Scholar   Yang, H., Cai, J., Yang, H. H., & Wang,
    X. (2022). Examining key factors of beginner’s continuance intention in blended
    learning in higher education. Journal of Computing in Higher Education, 35, 1–18.
    Google Scholar   Yeou, M. (2016). An investigation of students’ acceptance of
    Moodle in a blended learning setting using technology acceptance model. Journal
    of Educational Technology Systems, 44(3), 300–318. Article   Google Scholar   Zhang,
    Y., Chen, T., & Wang, C. (2020). Factors influencing students’ willingness to
    choose blended learning in higher education. In Blended Learning. Education in
    a Smart Learning Environment: 13th International Conference, ICBL 2020, Bangkok,
    Thailand, August 24–27, 2020, Proceedings 13 (pp. 289–302). Springer International
    Publishing. Zhao G, Yuan S. (2010). Key factors of effecting blended learning
    satisfaction: a study on Peking University students. International Conference
    on Hybrid Learning. Berlin: Springer; 2010. pp. 282–95. Zhao, W. (2022). An empirical
    study on blended learning in higher education in “internet+” era. Education and
    Information Technologies, 27(6), 8705–8722. Article   MathSciNet   Google Scholar   Zhu,
    Y., Au, W., & Yates, G. (2016). University students’ self-control and self-regulated
    learning in a blended course. The Internet and Higher Education, 30, 54–62. Article   Google
    Scholar   Download references Acknowledgements This research and the works behind
    it has been supported by Sultan Qaboos University to enhance scientific research
    in Oman and also to enhance teaching and learning environment in the Sultanate.
    Funding The research leading to these results has received funding from the Faculty
    of Arts and social sciences, Sultan Qaboos University, under the internal research
    fund program. Funding Agreement No [IG/ART/INFO/22/01]. Author information Authors
    and Affiliations Sultan Qaboos University, Seeb, Oman Faten Hamad, Ahmed Shehata
    & Noura Al Hosni University of Jordan, Amman, Jordan Faten Hamad Minya University,
    Minya, Egypt Ahmed Shehata Contributions All authors have equally contributed
    in preparing this research. Corresponding author Correspondence to Faten Hamad.
    Ethics declarations Ethics approval and consent to participate This research has
    been approved by rscientific ommettii at Sultan Qaboos University and is in accordance
    with rules and regulations in force at Sultan Qaboos University DVC-PSR approval.
    Consent for publication Not applicable. Competing interests There is no competing
    interest. Additional information Publisher''s Note Springer Nature remains neutral
    with regard to jurisdictional claims in published maps and institutional affiliations.
    Rights and permissions Open Access This article is licensed under a Creative Commons
    Attribution 4.0 International License, which permits use, sharing, adaptation,
    distribution and reproduction in any medium or format, as long as you give appropriate
    credit to the original author(s) and the source, provide a link to the Creative
    Commons licence, and indicate if changes were made. The images or other third
    party material in this article are included in the article''s Creative Commons
    licence, unless indicated otherwise in a credit line to the material. If material
    is not included in the article''s Creative Commons licence and your intended use
    is not permitted by statutory regulation or exceeds the permitted use, you will
    need to obtain permission directly from the copyright holder. To view a copy of
    this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and
    permissions About this article Cite this article Hamad, F., Shehata, A. & Al Hosni,
    N. Predictors of blended learning adoption in higher education institutions in
    Oman: theory of planned behavior. Int J Educ Technol High Educ 21, 13 (2024).
    https://doi.org/10.1186/s41239-024-00443-8 Download citation Received 28 August
    2023 Accepted 15 January 2024 Published 19 February 2024 DOI https://doi.org/10.1186/s41239-024-00443-8
    Share this article Anyone you share the following link with will be able to read
    this content: Get shareable link Provided by the Springer Nature SharedIt content-sharing
    initiative Keywords Blended learning Theory of Planned Behavior (TPB) e-Learning
    Oman Download PDF Sections Figures References Abstract Introduction Literature
    review Methodology Results and data analysis Discussion Conclusion Availability
    of data and materials Abbreviations References Acknowledgements Funding Author
    information Ethics declarations Additional information Rights and permissions
    About this article Advertisement Support and Contact Jobs Language editing for
    authors Scientific editing for authors Leave feedback Terms and conditions Privacy
    statement Accessibility Cookies Follow SpringerOpen By using this website, you
    agree to our Terms and Conditions, Your US state privacy rights, Privacy statement
    and Cookies policy. Your privacy choices/Manage cookies we use in the preference
    centre. © 2024 BioMed Central Ltd unless otherwise stated. Part of Springer Nature."'
  inline_citation: '>'
  journal: International Journal of Educational Technology in Higher Education
  limitations: '>'
  relevance_score1: 0
  relevance_score2: 0
  title: 'Predictors of blended learning adoption in higher education institutions
    in Oman: theory of planned behavior'
  verbatim_quote1: '>'
  verbatim_quote2: '>'
  verbatim_quote3: '>'
